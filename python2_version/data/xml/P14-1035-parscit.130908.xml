<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.961659">
A Bayesian Mixed Effects Model of Literary Character
</title>
<author confidence="0.996214">
David Bamman
</author>
<affiliation confidence="0.887476333333333">
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.998294">
dbamman@cs.cmu.edu
</email>
<author confidence="0.99585">
Ted Underwood
</author>
<affiliation confidence="0.857766666666667">
Department of English
University of Illinois
Urbana, IL 61801, USA
</affiliation>
<email confidence="0.998531">
tunder@illinois.edu
</email>
<author confidence="0.958188">
Noah A. Smith
</author>
<affiliation confidence="0.886167">
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.998708">
nasmith@cs.cmu.edu
</email>
<sectionHeader confidence="0.994796" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999376">
We consider the problem of automatically
inferring latent character types in a collec-
tion of 15,099 English novels published
between 1700 and 1899. Unlike prior
work in which character types are assumed
responsible for probabilistically generat-
ing all text associated with a character,
we introduce a model that employs mul-
tiple effects to account for the influence
of extra-linguistic information (such as au-
thor). In an empirical evaluation, we find
that this method leads to improved agree-
ment with the preregistered judgments of a
literary scholar, complementing the results
of alternative models.
</bodyText>
<sectionHeader confidence="0.998777" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999964862745098">
Recent work in NLP has begun to exploit the
potential of entity-centric modeling for a vari-
ety of tasks: Chambers (2013) places entities at
the center of probabilistic frame induction, show-
ing gains over a comparable event-centric model
(Cheung et al., 2013); Bamman et al. (2013) ex-
plicitly learn character types (or “personas”) in a
dataset of Wikipedia movie plot summaries; and
entity-centric models form one dominant approach
in coreference resolution (Durrett et al., 2013;
Haghighi and Klein, 2010).
One commonality among all of these very dif-
ferent probabilistic approaches is that each learns
statistical regularities about how entities are de-
picted in text (whether for the sake of learning
a set of semantic roles, character types, or link-
ing anaphora to the entities to which they refer).
In each case, the text we observe associated with
an entity in a document is directly dependent on
the class of entity—and only that class. This re-
lationship between entity and text is a theoreti-
cal assumption, with important consequences for
learning: entity types learned in this way will
be increasingly similar the more similar the do-
main, author, and other extra-linguistic effects are
between them.1 While in many cases the topi-
cally similar types learned under this assumption
may be desirable, we explore here the alterna-
tive, in which entity types are learned in a way
that controls for such effects. In introducing a
model based on different assumptions, we provide
a method that complements past work and pro-
vides researchers with more flexible tools to infer
different kinds of character types.
We focus here on the literary domain, exploring
a large collection of 15,099 English novels pub-
lished in the 18th and 19th centuries. By account-
ing for the influence of individual authors while in-
ferring latent character types, we are able to learn
personas that cut across different authors more ef-
fectively than if we learned types conditioned on
the text alone. Modeling the language used to de-
scribe a character as the joint result of that charac-
ter’s latent type and of other formal variables al-
lows us to test multiple models of character and
assess their value for different interpretive prob-
lems. As a test case, we focus on separating char-
acter from authorial diction, but this approach can
readily be generalized to produce models that pro-
visionally distinguish character from other factors
(such as period, genre, or point of view) as well.
</bodyText>
<sectionHeader confidence="0.974948" genericHeader="introduction">
2 Literary Background
</sectionHeader>
<bodyText confidence="0.999873166666667">
Inferring character is challenging from a liter-
ary perspective partly because scholars have not
reached consensus about the meaning of the term.
It may seem obvious that a “character” is a repre-
sentation of a (real or imagined) person, and many
humanists do use the term that way. But there is
</bodyText>
<footnote confidence="0.99101075">
1For example, many entities in Early Modern English
texts may be judged to be more similar to each other than
to entities from later texts simply by virtue of using hath and
other archaic verb forms.
</footnote>
<page confidence="0.929259">
370
</page>
<note confidence="0.8328535">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 370–379,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999988051282051">
an equally strong critical tradition that treats char-
acter as a formal dimension of narrative. To de-
scribe a character as a “blocking figure” or “first-
person narrator,” for instance, is a statement less
about the attributes of an imagined person than
about a narrative function (Keen, 2003). Charac-
ters are in one sense collections of psychological
or moral attributes, but in another sense “word-
masses” (Forster, 1927). This tension between
“referential” and “formalist” models of character
has been a centrally “divisive question in ... liter-
ary theory” (Woloch, 2003).
Considering primary source texts (as distinct
from plot summaries) forces us to confront new
theoretical questions about character. In a plot
summary (such as those explored by Bamman et
al., 2013), a human reader may already have used
implicit models of character to extract high-level
features. To infer character types from raw narra-
tive text, researchers need to explicitly model the
relationship of character to narrative form. This is
not a solved problem, even for human readers.
For instance, it has frequently been remarked
that the characters of Charles Dickens share
certain similarities—including a reliance on tag
phrases and recurring tics. A referential model
of character might try to distinguish this common
stylistic element from underlying “personalities.”
A strictly formalist model might refuse to separate
authorial diction from character at all. In prac-
tice, human readers can adopt either perspective:
we recognize that characters have a “Dickensian”
quality but also recognize that a Dickens villain is
(in one sense) more like villains in other authors
than like a Dickensian philanthropist. Our goal is
to show that computational methods can support
the same range of perspectives—allowing a provi-
sional, flexible separation between the referential
and formal dimensions of narrative.
</bodyText>
<sectionHeader confidence="0.994957" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999844111111111">
The dataset for this work consists of 15,099 dis-
tinct narratives drawn from HathiTrust Digital Li-
brary.2 From an initial collection of 469,200 vol-
umes written in English and published between
1700 and 1899 (including poetry, drama, and non-
fiction as well as prose narrative), we extract
32,209 volumes of prose fiction, remove dupli-
cates and fuse multi-volume works to create the fi-
nal dataset. Since the original texts were produced
</bodyText>
<footnote confidence="0.926639">
2http://www.hathitrust.org
</footnote>
<bodyText confidence="0.999918916666667">
by scanning and running OCR on physical books,
we automatically correct common OCR errors and
trim front and back matter from the volumes using
the page-level classifiers and HMM of Underwood
et al. (2013)
Many aspects of this process would be sim-
pler if we used manually-corrected texts, such as
those drawn from Project Gutenberg. But we hope
to produce research that has historical as well as
computational significance, and doing so depends
on the provenance of a collection. Gutenberg’s
decentralized selection process tends to produce
exceptionally good coverage of currently-popular
genres like science fiction, whereas HathiTrust ag-
gregates university libraries. Library collections
are not guaranteed to represent the past perfectly,
but they are larger, and less strongly shaped by
contemporary preferences.
The goal of this work is to provide a method to
infer a set of character types in an unsupervised
fashion from the data. As with prior work (Bam-
man et al., 2013), we define this target, a character
persona, as a distribution over several categories
of typed dependency relations:3
</bodyText>
<listItem confidence="0.992036384615385">
1. agent: the actions of which a character is
the agent (i.e., verbs for which the character
holds an nsubj or agent relation).
2. patient: the actions of which a character is
the patient (i.e., verbs for which the character
holds a dobj or nsubjpass relation).
3. possessive: the objects that a character pos-
sesses (i.e., all words for which the character
holds a poss relation).
4. predicative: attributes predicated of a char-
acter (i.e., adjectives or nouns holding an
nsubj relation to the character, with an inflec-
tion of be as a child).
</listItem>
<bodyText confidence="0.999698545454545">
This set captures the constellation of what a
character does and has done to them, what they
possess, and what they are described as being.
While previous work uses the Stanford
CoreNLP toolkit to identify characters and extract
typed dependencies for them, we found this
approach to be too slow for the scale of our data (a
total of 1.8 billion tokens); in particular, syntactic
parsing, with cubic complexity in sentence length,
and out-of-the-box coreference resolution (with
thousands of potential antecedents) prove to be
</bodyText>
<footnote confidence="0.992903666666667">
3All categories are described using the Stanford typed de-
pendencies (de Marneffe and Manning, 2008), but any syn-
tactic formalism is equally applicable.
</footnote>
<page confidence="0.998589">
371
</page>
<bodyText confidence="0.998515583333333">
the biggest bottlenecks.
Before addressing character inference, we
present here a prerequisite NLP pipeline that
scales well to book-length documents.4 This
pipeline uses the Stanford POS tagger (Toutanova
et al., 2003), the linear-time MaltParser (Nivre et
al., 2007) for dependency parsing (trained on Stan-
ford typed dependencies), and the Stanford named
entity recognizer (Finkel et al., 2005). It includes
the following components for clustering charac-
ter name mentions, resolving pronominal corefer-
ence, and reducing vocabulary dimensionality.
</bodyText>
<subsectionHeader confidence="0.999047">
3.1 Character Clustering
</subsectionHeader>
<bodyText confidence="0.99994294117647">
First, let us terminologically distinguish between a
character mention in a text (e.g., the token Tom on
page 141 of The Adventures of Tom Sawyer) and a
character entity (e.g., TOM SAWYER the character,
to which that token refers). To resolve the former
to the latter, we largely follow Davis et al. (2003)
and Elson et al. (2010): we define a set of initial
characters corresponding to each unique charac-
ter name that is not a subset of another (e.g., Mr.
Tom Sawyer) and deterministically create a set of
allowable variants for each one (Mr. Tom Sawyer
—* Tom, Sawyer, Tom Sawyer, Mr. Sawyer, and
Mr. Tom); then, from the beginning of the book
to the end, we greedily assign each mention to the
most recently linked entity for whom it is a vari-
ant. The result constitutes our set of characters,
with all mentions partitioned among them.
</bodyText>
<subsectionHeader confidence="0.999428">
3.2 Pronominal Coreference Resolution
</subsectionHeader>
<bodyText confidence="0.99992">
While the character clustering stage is essentially
performing proper noun coreference resolution,
approximately 74% of references to characters in
books come in the form of pronouns.5 To resolve
this more difficult class at the scale of an entire
book, we train a log-linear discriminative classifier
only on the task of resolving pronominal anaphora
(i.e., ignoring generic noun phrases such as the
paint or the rascal).
For this task, we annotated a set of 832 coref-
erence links in 3 books (Pride and Prejudice, The
Turn of the Screw, and Heart of Darkness) and fea-
turized coreference/antecedent pairs with:
</bodyText>
<footnote confidence="0.9896452">
4All code is available at http://www.ark.cs.cmu.
edu/literaryCharacter
5Over all 15,099 narratives, the average number of char-
acter proper name mentions is 1,673; the average number of
gendered singular pronouns (he, she, him, his, her) is 4,641.
</footnote>
<listItem confidence="0.986414571428571">
1. The syntactic dependency path from a
pronoun to its potential antecedent (e.g.,
dobjTpred—*JpredJnsubj (where —* de-
notes movement across sentence boundaries).
2. The salience of the antecedent character (de-
fined as the count of that character’s named
mentions in the previous 500 words).
3. The antecedent part of speech.
4. Whether or not the pronoun and antecedent
appear in the same quotation scope (false if
one appears in a quotation and one outside).
5. Whether or not the two agree for gender.
6. The syntactic tree distance between the two.
7. The linear (word) distance between the two.
</listItem>
<bodyText confidence="0.999865181818182">
With this featurization and training data, we train
a binary logistic regression classifier with Ei regu-
larization (where negative examples are comprised
of all character entities in the previous 100 words
not labeled as the true antecedent). In a 10-fold
cross-validation on predicting the true nearest an-
tecedent for a pronominal anaphor, this method
achieves an average accuracy of 82.7%.
With this trained model, we then select the
highest-scoring antecedent within 100 words for
each pronominal anaphor in our data.
</bodyText>
<subsectionHeader confidence="0.998972">
3.3 Dimensionality Reduction
</subsectionHeader>
<bodyText confidence="0.99999319047619">
To manage the degrees of freedom in the model
described in §4, we perform dimensionality reduc-
tion on the vocabulary by learning word embed-
dings with a log-linear continuous skip-gram lan-
guage model (Mikolov et al., 2013) on the entire
collection of 15,099 books. This method learns a
low-dimensional real-valued vector representation
of each word to predict all of the words in a win-
dow around it; empirically, we find that with a suf-
ficient window size (we use n = 10), these word
embeddings capture semantic similarity (placing
topically similar words near each other in vector
space).6 We learn a 100-dimensional embedding
for each of the 512,344 words in our vocabulary.
To create a partition over the vocabulary, we
use hard K-means clustering (with Euclidean dis-
tance) to group the 512,344 word types into 1,000
clusters. We then agglomeratively cluster those
1,000 groups to assign bitstring representations to
each one, forming a balanced binary tree by only
merging existing clusters at equal levels in the hi-
</bodyText>
<footnote confidence="0.867731333333333">
6In comparison, Brown et al. (1992) clusters learned from
the same data capture syntactic similarity (placing function-
ally similar words in the same cluster).
</footnote>
<page confidence="0.99387">
372
</page>
<figureCaption confidence="0.995039333333333">
Figure 1: Bitstring representations of neural agglomerative clusters, illustrating the leaf nodes in a binary tree rooted in the
prefix 01110011. Bitstring encodings of intermediate nodes and terminal leaves result by following the left (0) and right (1)
branches of the merge tree created through agglomerative clustering.
</figureCaption>
<figure confidence="0.996092777777778">
0111001111: pair boots shoes gloves leather
1 0 0111001110: hat coat cap cloak handkerchief
1
01110011 →
0111001101: dress clothes wore worn wear
0111001100: dressed costume uniform clad clothed
1
0
0
</figure>
<bodyText confidence="0.983595">
erarchy. We use Euclidean distance as a funda-
mental metric and a group-average similarity func-
tion for calculating the distance between groups.
Fig. 1 illustrates four of the 1,000 learned clusters.
</bodyText>
<sectionHeader confidence="0.995" genericHeader="method">
4 Model
</sectionHeader>
<bodyText confidence="0.99991976">
In order to separate out the effects that a charac-
ter’s persona has on the words that are associated
with them (as opposed to other factors, such as
time period, genre, or author), we adopt a hierar-
chical Bayesian approach in which the words we
observe are generated conditional on a combina-
tion of different effects captured in a log-linear (or
“maximum entropy”) distribution.
Maximum entropy approaches to language
modeling have been used since Rosenfeld (1996)
to incorporate long-distance information, such as
previously-mentioned trigger words, into n-gram
language models. This work has since been ex-
tended to a Bayesian setting by applying both
a Gaussian prior (Chen and Rosenfeld, 2000),
which dampens the impact of any individual fea-
ture, and sparsity-inducing priors (Kazama and
Tsujii, 2003; Goodman, 2004), which can drive
many feature weights to 0. The latter have been
applied specifically to the problem of estimating
word probabilities with sparse additive generative
(SAGE) models (Eisenstein et al., 2011), where
sparse extra-linguistic effects can influence a word
probability in a larger generative setting.
In contrast to previous work in which the prob-
ability of a word linked to a character is depen-
dent entirely on the character’s latent persona, in
our model, we see the probability of a word as
dependent on: (i) the background likelihood of
the word, (ii) the author, so that a word becomes
more probable if a particular author tends to use it
more, and (iii) the character’s persona, so that a
word is more probable if appearing with a partic-
ular persona. Intuitively, if the author Jane Austen
is associated with a high weight for the word man-
ners, and all personas have little effect for this
word, then manners will have little impact on de-
ciding which persona a particular Austen character
embodies, since its presence is explained largely
by Austen having penned the word. While we ad-
dress only the author as an observed effect, this
model is easily extended to other features as well,
including period, genre, point of view, and others.
The generative story runs as follows (Figure 2
depicts the full graphical model): Let there be
M unique authors in the data, P latent personas
(a hyperparameter to be set), and V words in
the vocabulary (in the general setting these may
be word types; in our data the vocabulary is the
set of 1,000 unique cluster IDs). Each role type
r E {agent, patient, possessive, predicative}
and vocabulary word v (here, a cluster ID)
is associated with a real-valued vector ηr,v =
[ηmeta pers 0]
r,v ,ηr,v ,ηr, v of length M + P + 1. The first
M + P elements are drawn from a Laplace prior
with mean µ = 0 and scale A = 1; the last el-
ement η0r,v is an unregularized bias term account-
ing for the background. Each element in this vec-
tor captures the log-additive effect of each author,
persona, and the background distribution on the
word’s probability (Eq. 1, below).
Much like latent Dirichlet allocation (Blei et al.,
2003), each document d in our dataset draws a
multinomial distribution Bd over personas from a
shared Dirichlet prior α, which captures the pro-
portion of each character type in that particular
document. Every character c in the document
draws its persona p from this document-specific
multinomial. Given document metadata m (here,
one of a set of M authors) and persona p, each tu-
ple of a role r with word w is assumed to be drawn
from Eq. 1 in Fig. 3. This SAGE model can be
understood as a log-linear distribution with three
kinds of features (metadata, persona, and back-
</bodyText>
<page confidence="0.940675">
373
</page>
<equation confidence="0.9199712">
V
meta ] + 1Pws [] + , ntaP(w  |m, p, r,η) = exp (ηrwm ,w) exp (?7rv[m] + ηrpv, s [p] + η0v) (1)
/v=1
P(b  |m, p, r, η) = n−1H { logit−1 (ηr7eta [m] + ηP vl:j[p] + η�,b1:j) if bj+1 = 1 (2)
j=0 1 − logit−1 (ηmeta [m] + ηP vlsj [p] + η� b1:j) otherwise
</equation>
<figureCaption confidence="0.99896">
Figure 3: Parameterizations of the SAGE word distribution. Eq. 1 is a
</figureCaption>
<bodyText confidence="0.628118">
multinomial logistic regression with one 77-vector
per role and word. Eq. 2 uses the hierarchical softmax formulation, with one 77-vector per role and node in the binary
</bodyText>
<equation confidence="0.4531906">
“flat”
tree of
word clusters, giving a distribution over bit strings (b) with the same number of parameters as Eq. 1.
m, p, and r (as above) we let b = b1b2
bn de-
</equation>
<bodyText confidence="0.976648222222222">
note the bitstring representation of a word cluster,
and the distribution is given by Eq. 2 in Fig. 3.
In this paramaterization, rather than one
for each role and vocabulary term, we have
one
for each role and conditional binary
decision in the tree (each bitstring prefix). Since
the tree is binary with V leaves, this yields the
same total number of parameters. As Goodman
(2001) points out, while this reparameterization is
exact for true probabilities, it remains an approx-
imation for estimated models (with generalization
behavior dependent on how well the class hierar-
chy is supported by the data). In addition to en-
abling faster inference, one advantage of the bit-
string representation and the hierarchical softmax
parameterization is that we can easily calculate
probabilities of clusters at different gran
</bodyText>
<equation confidence="0.8079172">
···
η-
vector
η-vector
ularities.
</equation>
<bodyText confidence="0.968949533333333">
nce
are p (the personas for each character) and
the
effects that each author and persona have on the
probability of a word. Rather than adopting a fully
Bayesian approach (e.g., sampling all variables),
we infer these values using stochastic EM, alter-
nating between collapsed Gibbs sampling for each
p and maximizing with respect to
η,
η.
At each step,
the required quantity is the probability that char-
acter c in document d has persona z, given ev-
erything else. This is proportional to the number
</bodyText>
<equation confidence="0.8100715">
personas.8
pd,c = z) of all of the words
</equation>
<subsectionHeader confidence="0.921751">
4.1 Hierarchical Softmax
</subsectionHeader>
<bodyText confidence="0.794960166666667">
The partition function in Eq. 1 can lead to slow
inference for any reasonably-sized vocabulary. To
address this, we reparameterize the model by ex-
ploiting the structure of the agglomerative clus-
tering in
to perform a hierarchical softmax,
following Goodman (2001), Morin and Bengio
(2005) and Mikolov et al. (2013).
The bitstring representations by which we en-
code each word in the vocabulary serve as natural,
and inherently meaningful, intermediate classes
that correspond to semantically related subsets of
the vocabulary, with each bitstring prefix denoting
one such class. Longer bitstrings correspond to
more fine-grained classes. In the example shown
in Figure 1,
is one such intermediate
class, containing the union of pair, boots, shoes,
gloves leather and hat, coat, cap cloak, handker-
chief. Because these classes recursively partition
the vocabulary, they offer a convenient way to
reparameterize the model through the chain rule
of probability.
Consider, for example, a word represented as
</bodyText>
<equation confidence="0.850622461538462">
the bitstring c =
calculating P(c =
01011)—we suppress conditioning variables for
clarity—involves the product: P(c1 = 0)
= 1
= 0) X
= 0
=
X
= 1
= 010) X
= 1
c1:4 = 0101).
</equation>
<table confidence="0.715830739130435">
Since each multiplicand involves a binary pre-
diction, we can avoid partition functions and use
the classic binary logistic
We have
converted the V-way multiclass logistic regression
problem of Eq. 1 into a sequence of log V evalua-
tions (assuming a perfectly balan
§3.3
011100111
01011;
XP(c2
|c1
P(c3
|c1:2
01)
P(c4
|c1:3
P(c5
|
regression.7
ced tree). Given
that logistic regression lets PLR(y =
= 1/(1 +exp
</table>
<footnote confidence="0.698852">
for binary dependent
variable y, independent variables x, an
7Recall
</footnote>
<equation confidence="0.6542445">
1 |x, Q)=logit−1(xTQ)
−xTQ)
</equation>
<bodyText confidence="0.835510333333333">
d coefficients Q.
perparameter which acts as a smoother) times the
probability (under
</bodyText>
<subsectionHeader confidence="0.693421">
4.2 Infere
</subsectionHeader>
<bodyText confidence="0.749846666666667">
Our primary quantities of interest in this model
assume the reader is familiar with collapsed Gibbs
sampling as
</bodyText>
<footnote confidence="0.545366666666667">
8We
used in latent-variable NLP models.
ground bias).
</footnote>
<page confidence="0.909236">
374
</page>
<bodyText confidence="0.794062333333333">
Collapsed Gibbs for
of other characters in document d who also (cur-
rently) have that persona (plus the Dirichlet hy-
</bodyText>
<figure confidence="0.777160846153846">
P
D
Cd
Wd,c
md
Bd
pd,c
j
wj
rj
η
µ, λ
α
</figure>
<figureCaption confidence="0.999026333333333">
Figure 2: Above: Probabilistic graphical model. Observed
variables are shaded, latent variables are clear, and collapsed
variables are dotted. Below: Definition of variables.
</figureCaption>
<bodyText confidence="0.998796066666667">
(3)
The metadata features (like author, etc.) influence
this probability by being constant for all choices
of z; e.g., if the coefficient learned for Austen for
vocabulary term manners is high and all coeffi-
cients for all z are close to zero, then the proba-
bility of manners will change little under different
choices of z. Eq. 3 contains one multiplicand for
every word associated with a character, and only
one term reflecting the influence of the shared doc-
ument multinomial. The implication is that, for
major characters with many observed words, the
words will dominate the choice of persona; where
the document influence would have a bigger effect
is with characters for whom we don’t have much
data. In that case, it can act as a kind of informed
background; given what little data we have for that
character, it would nudge us toward the character
types that the other characters in the book embody.
Given an assignment of all p, we choose q
to maximize the conditional log-likelihood of the
words, as represented by their bitstring cluster IDs,
given the observed author and background effects
and the sampled personas. This equates to solving
4V `1-regularized logistic regressions (see Eq. 2
in Figure 3), one for each role type and bitstring
prefix, each with M + P + 1 parameters. We ap-
ply OWL-QN (Andrew and Gao, 2007) to mini-
mize the `1-regularized objective with an absolute
convergence threshold of 10−5.
</bodyText>
<sectionHeader confidence="0.998698" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.99935428">
While standard NLP and machine learning prac-
tice is to evaluate the performance of an algorithm
on a held-out gold standard, articulating what a
true “persona” might be for a character is inher-
ently problematic. Rather, we evaluate the perfor-
mance and output of our model by preregistering
a set of 29 hypotheses of varying scope and diffi-
culty and comparing the performance of different
models in either confirming, or failing to confirm,
those hypotheses. This kind of evaluation was pre-
viously applied to a subjective text measurement
problem by Sim et al. (2013).
All hypotheses were created by a literary
scholar with specialization in the period to not
only give an empirical measure of the strengths
and weaknesses of different models, but also to
help explore exactly what the different models
may, or may not, be learning. All preregistered hy-
potheses establish the degrees of similarity among
three characters, taking the form: “character X is
more similar to character Y than either X or Y is
to a distractor character Z”; for a given model and
definition of distance under that model, each hy-
pothesis yields two yes/no decisions that we can
evaluate:
</bodyText>
<listItem confidence="0.9993115">
• distance(X, Y ) &lt; distance(X, Z)
• distance(X, Y ) &lt; distance(Y, Z)
</listItem>
<bodyText confidence="0.972979">
To tease apart the different kinds of similarities
we hope to explore, we divide the hypotheses into
four classes:
</bodyText>
<figure confidence="0.977884827586207">
w
a
p
q
r
W
C
m
D
tl
l
µ
Number of personas (hyperparameter)
Number of documents
Number of characters in document d
Number of (cluster, role) tuples for character c
Metadata for document d (ranges over M authors)
Document d’s distribution over personas
Character c’s persona
An index for a (r, w) tuple in the data
Word cluster ID for tuple j
Role for tuple j E {agent, patient, poss, pred}
Coefficients for the log-linear language model
Laplace mean and scale (for regularizing η)
Dirichlet concentration parameter
observed in each role r for that character:
(count(z; pd,−c) + αz)x R H P(bj  |m, p, r, q)
H j:rj=r
r=1
</figure>
<page confidence="0.98736">
375
</page>
<listItem confidence="0.976651710526316">
A. This class constitutes sanity checks: charac-
ter X and Y are more similar to each other
in every way than to character Z. E.g.: Eliz-
abeth Bennet in Pride and Prejudice resem-
bles Elinor Dashwood in Sense and Sensibil-
ity (Jane Austen) more than either character
resembles Allen Quatermain in Allen Quater-
main (H. Rider Haggard). (Austenian protag-
onists should resemble each other more than
they resemble a grizzled hunter.)
B. This class captures our ability to identify two
characters in the same author as being more
similar to each other than to a closely re-
lated character in a different author. E.g.:
Wickham in Pride and Prejudice resembles
Willoughby in Sense and Sensibility (Jane
Austen) more than either character resem-
bles Mr. Rochester in Jane Eyre (Charlotte
Bront¨e).
C. This class captures our ability to discrimi-
nate among similar characters in the same au-
thor. In these hypotheses, two characters X
and Y from the same author are more simi-
lar to each other than to a third character Z
from that same author. E.g.: Wickham in
Pride and Prejudice (Jane Austen) resembles
Willoughby in Sense and Sensibility more
than either character resembles Mr. Darcy in
Pride and Prejudice.
D. This class constitutes more difficult, ex-
ploratory hypotheses, including differences
among point of view. E.g.: Montoni in
Mysteries of Udolpho (Radcliffe) resem-
bles Heathcliff in Wuthering Heights (Emily
Bront¨e) more than either resembles Mr. Ben-
net in Pride and Prejudice. (Testing our
model’s ability to discern similarities in spite
of elapsed time.)
</listItem>
<bodyText confidence="0.99927525">
All 29 hypotheses can be found in a supplemen-
tary technical report (Bamman et al., 2014). We
emphasize that the full set of hypotheses was
locked before the model was estimated.
</bodyText>
<sectionHeader confidence="0.998892" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999761615384615">
Part of the motivation of our mixed effects model
is to be able to tackle hypothesis class C—by fac-
toring out the influence of a particular author on
the learning of personas, we would like to be able
to discriminate between characters that all have
a common authorial voice. In contrast, the Per-
sona Regression model of Bamman et al. (2013),
which uses metadata variables (like authorship)
to encourage entities with similar covariates to
have similar personas, reflects an assumption that
makes it likely to perform well at class B.
To judge their respective strengths on different
hypothesis classes, we evaluate three models:
</bodyText>
<listItem confidence="0.766791923076923">
1. The mixed-effects Author/Persona model
(described above), which includes author in-
formation as a metadata effect; here, each
q-vector (of length M + P + 1) contains a
parameter for each of the distinct authors in
our data, a parameter for each persona, and a
background parameter.
2. A Basic persona model, which ablates au-
thor information but retains the same log-
linear architecture; here, the q-vector is of
size P + 1 and does not model author effects.
3. The Persona Regression model of Bam-
man et al. (2013).
</listItem>
<bodyText confidence="0.989857451612903">
All models are run with P ∈ {10, 25, 50, 100,
250} personas; Persona Regression addition-
ally uses K = 25 latent topics. All configura-
tions use the full dataset of 15,099 novels, and all
characters with at least 25 total roles (a total of
257,298 entities). All experiments are run with
50 iterations of Gibbs sampling to collect samples
for the personas p, alternating with maximization
steps for q. The value of α is optimized using slice
sampling (with a non-informative prior) every 5
iterations. The value of A is held constant at 1.
At the end of inference, we calculate the posterior
distributions over personas for all characters as the
sampling probability of the final iteration.
To formally evaluate “similarity” between two
characters, we measure the Jensen-Shannon diver-
gence between personas (calculated as the average
JS distance over the cluster distributions for each
role type), marginalizing over the characters’ pos-
terior distributions over personas; two characters
with a lower JS divergence are judged to be more
similar than two characters with a higher one.
As a Baseline, we also evaluate all hypotheses
on a model with no latent variables whatsoever,
which instead measures similarity as the average
JS divergence between the empirical word distri-
butions over each role type.
Table 1 presents the results of this compari-
son; for all models with latent variables, we re-
port the average of 5 sampling runs with different
random initializations. Figure 4 provides a syn-
</bodyText>
<page confidence="0.996529">
376
</page>
<table confidence="0.999950555555555">
P Model Hypothesis Class
A B C D
Author/Persona 1.00 0.58 0.75 0.42
250 Basic Persona 1.00 0.73 0.58 0.53
Persona Reg. 0.90 0.70 0.58 0.44
Author/Persona 0.98 0.68 0.70 0.46
100 Basic Persona 0.95 0.73 0.53 0.47
Persona Reg. 0.93 0.78 0.63 0.49
Author/Persona 0.95 0.73 0.63 0.50
50 Basic Persona 0.98 0.75 0.48 0.53
Persona Reg. 1.00 0.75 0.65 0.38
Author/Persona 1.00 0.63 0.65 0.50
25 Basic Persona 1.00 0.63 0.50 0.50
Persona Reg. 0.90 0.78 0.60 0.39
Author/Persona 0.95 0.63 0.70 0.51
10 Basic Persona 0.78 0.80 0.48 0.46
Persona Reg. 0.90 0.73 0.43 0.41
Baseline 1.00 0.63 0.58 0.37
</table>
<tableCaption confidence="0.994314">
Table 1: Agreement rates with preregistered hypotheses, av-
eraged over 5 sampling runs with different initializations.
</tableCaption>
<figure confidence="0.40827">
Hypothesis class
</figure>
<figureCaption confidence="0.778822666666667">
Figure 4: Synopsis of table 1: average accuracy across all P.
Persona regression is best able to judge characters in one
author to be more similar to each other than to characters in
another (B), while our mixed-effects Author/Persona model
outperforms other models at discriminating characters in the
same author (C).
</figureCaption>
<bodyText confidence="0.99998424137931">
opsis of this table by illustrating the average ac-
curacy across all choice of P. All models, in-
cluding the baseline, perform well on the sanity
checks (A). As expected, the Persona Regres-
sion model performs best at hypothesis class B
(correctly judging two characters from the same
author to be more similar to each other than to a
character from a different author); this behavior is
encouraged in this model by allowing an author (as
an external metadata variable) to directly influence
the persona choice, which has the effect of push-
ing characters from the same author to embody
the same character type. Our mixed effects Au-
thor/Persona model, in contrast, outperforms the
other models at hypothesis class C (correctly dis-
criminating different character types present in the
same author). By discounting author-specific lexi-
cal effects during persona inference, we are better
able to detect variation among the characters of a
single author that we are not able to capture oth-
erwise. While these different models complement
each other in this manner, we note that there is
no absolute separation among them, which may be
suggestive of the degree to which the formal and
referential dimensions are fused in novels. Nev-
ertheless, the strengths of these different models
on these different hypothesis classes gives us flex-
ible alternatives to use depending on the kinds of
character types we are looking to infer.
</bodyText>
<sectionHeader confidence="0.995688" genericHeader="method">
7 Analysis
</sectionHeader>
<bodyText confidence="0.999971133333334">
The latent personas inferred from this model will
support further exploratory analysis of literary his-
tory. Figure 2 illustrates this with a selection of
three character types learned, displaying charac-
teristic clusters for all role types, along with the
distribution of that persona’s use across time and
the gender distribution of characters embodying
that persona. In general, the personas learned so
far do not align neatly with character types known
to literary historians. But they do have legible as-
sociations both with literary genres and with social
categories. Even though gender is not an observ-
able variable known to the model during inference,
personas tend to be clearly gendered. This is not
in itself surprising (since literary scholars know
that assumptions about character are strongly gen-
dered), but it does suggest that diachronic analysis
of latent character types might cast new light on
the history of gender in fiction. This is especially
true since the distribution of personas across the
time axis similarly reveals coherent trends.
Table 3 likewise illustrates what our model
learns by presenting a sample of the fixed effects
learned for a set of five major 19th-century au-
thors. These are clusters that are conditionally
more likely to appear associated with a character
in a work by the given author than they are in the
overall data; by factoring this information out of
the inference process for learning character types
(by attributing its presence in a text to the author
</bodyText>
<figure confidence="0.995229090909091">
Accuracy
100
25
75
50
0
Author/Persona
Baseline
Basic
Persona Reg.
A B C D
</figure>
<page confidence="0.911886">
377
</page>
<table confidence="0.989195928571429">
1800 1820 1840 1860 1880 1900 1800 1820 1840 1860 1880 1900 1800 1820 1840 1860 1880 1900
Agent carried ran threw sent received arrived turns begins returns
rose fell suddenly appeared struck showed thinks loves calls
is seems returned immediately waiting does knows comes
Patient wounded killed murdered wounded killed murdered thinks loves calls
suffer yield acknowledge destroy bind crush love hope true
free saved unknown attend haste proceed turn hold show
Poss death happiness future army officers troops lips cheek brow
lips cheek brow soldiers band armed eyes face eye
mouth fingers tongue party join camp table bed chair
Pred crime guilty murder king emperor throne beautiful fair fine
youth lover hers general officer guard good kind ill
dead living died soldier knight hero dead living died
% Female 12.2 3.7 54.7
</table>
<tableCaption confidence="0.997494333333333">
Table 2: Snapshots of three personas learned from the P = 50, Author/Persona model. Gender and time proportions are
calculated by summing and normalizing the posterior distributions over all characters with that feature. We truncate time series
at 1800 due to data sparsity before that date; the y-axis illustrates the frequency of its use in a given year, relative to its lifetime.
</tableCaption>
<table confidence="0.9994688125">
Author clusters
Jane Austen praise gift consolation
letter read write
character natural taste
Charlotte Bront¨e lips cheek brow
book paper books
hat coat cap
Charles Dickens hat coat cap
table bed chair
hand head hands
Herman Melville boat ship board
hat coat cap
feet ground foot
Jules Verne journey travel voyage
master company presence
success plan progress
</table>
<tableCaption confidence="0.9946565">
Table 3: Characteristic possessive clusters in a sample of
major 19th-century authors.
</tableCaption>
<bodyText confidence="0.99998325">
rather than the persona), we are able to learn per-
sonas that cut across different topics more effec-
tively than if a character type is responsible for
explaining the presence of these terms as well.
</bodyText>
<sectionHeader confidence="0.997499" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.9999898">
Our method establishes the possibility of repre-
senting the relationship between character and nar-
rative form in a hierarchical Bayesian model. Pos-
tulating an interaction between authorial diction
and character allows models that consider the ef-
fect of the author to more closely reproduce a hu-
man reader’s judgments, especially by learning to
distinguish different character types within a sin-
gle author’s oeuvre. This opens the door to con-
sidering other structural and formal dimensions of
narration. For instance, representation of charac-
ter is notoriously complicated by narrative point of
view (Booth, 1961); and indeed, comparisons be-
tween first-person narrators and other characters
are a primary source of error for all models tested
above. The strategy we have demonstrated sug-
gests that it might be productive to address this by
modeling the interaction of character and point of
view as a separate effect analogous to authorship.
It is also worth noting that the models tested
above diverge from many structuralist theories of
narrative (Propp, 1998) by allowing multiple in-
stances of the same persona in a single work.
Learning structural limitations on the number of
“protagonists” likely to coexist in a single story,
for example, may be another fruitful area to ex-
plore. In all cases, the machinery of hierarchical
models gives us the flexibility to incorporate such
effects at will, while also being explicit about the
theoretical assumptions that attend them.
</bodyText>
<sectionHeader confidence="0.997209" genericHeader="acknowledgments">
9 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999258">
We thank the reviewers for their helpful com-
ments. The research reported here was supported
by a National Endowment for the Humanities
start-up grant to T.U., U.S. National Science Foun-
dation grant CAREER IIS-1054319 to N.A.S., and
an ARCS scholarship to D.B. This work was made
possible through the use of computing resources
made available by the Pittsburgh Supercomputing
Center. Eleanor Courtemanche provided advice
about the history of narrative theory.
</bodyText>
<page confidence="0.998236">
378
</page>
<sectionHeader confidence="0.993878" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997563958762887">
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of ll-regularized log-linear models. In Proc. of
ICML.
David Bamman, Brendan O’Connor, and Noah A.
Smith. 2013. Learning latent personas of film char-
acters. Proc. of ACL.
David Bamman, Ted Underwood, and Noah A. Smith.
2014. Appendix to ‘A Bayesian mixed effects
model of literary character’. Technical report,
Carnegie Mellon University, University of Illinois-
Urbana Champaign.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Wayne Booth. 1961. The Rhetoric of Fiction. Univer-
sity of Chicago Press, Chicago.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467–479.
Nathanael Chambers. 2013. Event schema induction
with a probabilistic entity-driven model. In Proc. of
EMNLP, Seattle, Washington, USA.
Stanley F. Chen and Roni Rosenfeld. 2000. A
survey of smoothing techniques for me models.
IEEE Transactions on Speech and Audio Process-
ing, 8(1):37–50.
Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-
derwende. 2013. Probabilistic frame induction. In
Proc. of NAACL.
Peter T. Davis, David K. Elson, and Judith L. Klavans.
2003. Methods for precise named entity matching in
digital collections. In Proc. of JCDL, Washington,
DC, USA.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed dependencies manual.
Technical report, Stanford University.
Greg Durrett, David Hall, and Dan Klein. 2013.
Decentralized entity-level modeling for coreference
resolution. In Proc. of ACL.
Jacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011.
Sparse additive generative models of text. In Proc.
of ICML.
David K. Elson, Nicholas Dames, and Kathleen R.
McKeown. 2010. Extracting social networks from
literary fiction. In Proc. of ACL, Stroudsburg, PA,
USA.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proc. of ACL.
E. M. Forster. 1927. Aspects of the Novel. Harcourt,
Brace &amp; Co.
Joshua Goodman. 2001. Classes for fast maximum
entropy training. In Proc. of ICASSP.
Joshua Goodman. 2004. Exponential priors for maxi-
mum entropy models. In Proc. of NAACL.
Aria Haghighi and Dan Klein. 2010. Coreference reso-
lution in a modular, entity-centered model. In Proc.
of NAACL.
Jun’ichi Kazama and Jun’ichi Tsujii. 2003. Evaluation
and extension of maximum entropy models with in-
equality constraints. In Proc. of EMNLP.
Suzanne Keen. 2003. Narrative Form. Palgrave
Macmillan, Basingstoke.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. Proc. of ICLR.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proc. of AISTATS.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13:95–135, 5.
Vladimir Propp. 1998. Morphology of the Folktale.
University of Texas Press, 2nd edition.
Roni Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modelling. Com-
puter Speech and Language, 10(3):187 – 228.
Yanchuan Sim, Brice D. L. Acree, Justin H. Gross, and
Noah A. Smith. 2013. Measuring ideological pro-
portions in political speeches. In Proc. of EMNLP,
Seattle, Washington, USA.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proc. of NAACL.
Ted Underwood, Michael L Black, Loretta Auvil, and
Boris Capitanu. 2013. Mapping mutable genres in
structurally complex volumes. In Proc. of IEEE In-
ternational Conference on Big Data.
Alex Woloch. 2003. The One vs. the Many: Minor
Characters and the Space of the Protagonist in the
Novel. Princeton University Press, Princeton NJ.
</reference>
<page confidence="0.999246">
379
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.895903">
<title confidence="0.999962">A Bayesian Mixed Effects Model of Literary Character</title>
<author confidence="0.999991">David Bamman</author>
<affiliation confidence="0.9998625">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.99895">Pittsburgh, PA 15213, USA</address>
<email confidence="0.999672">dbamman@cs.cmu.edu</email>
<author confidence="0.971613">Ted</author>
<affiliation confidence="0.9991145">Department of University of</affiliation>
<address confidence="0.933394">Urbana, IL 61801,</address>
<email confidence="0.999692">tunder@illinois.edu</email>
<author confidence="0.999935">Noah A Smith</author>
<affiliation confidence="0.9999575">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.997887">Pittsburgh, PA 15213, USA</address>
<email confidence="0.999656">nasmith@cs.cmu.edu</email>
<abstract confidence="0.9995743125">We consider the problem of automatically inferring latent character types in a collection of 15,099 English novels published between 1700 and 1899. Unlike prior work in which character types are assumed responsible for probabilistically generatassociated with a character, we introduce a model that employs multiple effects to account for the influence of extra-linguistic information (such as author). In an empirical evaluation, we find that this method leads to improved agreement with the preregistered judgments of a literary scholar, complementing the results of alternative models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Galen Andrew</author>
<author>Jianfeng Gao</author>
</authors>
<title>Scalable training of ll-regularized log-linear models.</title>
<date>2007</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="23462" citStr="Andrew and Gao, 2007" startWordPosition="3827" endWordPosition="3830">hat case, it can act as a kind of informed background; given what little data we have for that character, it would nudge us toward the character types that the other characters in the book embody. Given an assignment of all p, we choose q to maximize the conditional log-likelihood of the words, as represented by their bitstring cluster IDs, given the observed author and background effects and the sampled personas. This equates to solving 4V `1-regularized logistic regressions (see Eq. 2 in Figure 3), one for each role type and bitstring prefix, each with M + P + 1 parameters. We apply OWL-QN (Andrew and Gao, 2007) to minimize the `1-regularized objective with an absolute convergence threshold of 10−5. 5 Evaluation While standard NLP and machine learning practice is to evaluate the performance of an algorithm on a held-out gold standard, articulating what a true “persona” might be for a character is inherently problematic. Rather, we evaluate the performance and output of our model by preregistering a set of 29 hypotheses of varying scope and difficulty and comparing the performance of different models in either confirming, or failing to confirm, those hypotheses. This kind of evaluation was previously </context>
</contexts>
<marker>Andrew, Gao, 2007</marker>
<rawString>Galen Andrew and Jianfeng Gao. 2007. Scalable training of ll-regularized log-linear models. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Bamman</author>
<author>Brendan O’Connor</author>
<author>Noah A Smith</author>
</authors>
<title>Learning latent personas of film characters.</title>
<date>2013</date>
<booktitle>Proc. of ACL.</booktitle>
<marker>Bamman, O’Connor, Smith, 2013</marker>
<rawString>David Bamman, Brendan O’Connor, and Noah A. Smith. 2013. Learning latent personas of film characters. Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Bamman</author>
<author>Ted Underwood</author>
<author>Noah A Smith</author>
</authors>
<title>Appendix to ‘A Bayesian mixed effects model of literary character’.</title>
<date>2014</date>
<tech>Technical report,</tech>
<institution>Carnegie Mellon University, University of IllinoisUrbana Champaign.</institution>
<contexts>
<context position="27176" citStr="Bamman et al., 2014" startWordPosition="4452" endWordPosition="4455">hor. E.g.: Wickham in Pride and Prejudice (Jane Austen) resembles Willoughby in Sense and Sensibility more than either character resembles Mr. Darcy in Pride and Prejudice. D. This class constitutes more difficult, exploratory hypotheses, including differences among point of view. E.g.: Montoni in Mysteries of Udolpho (Radcliffe) resembles Heathcliff in Wuthering Heights (Emily Bront¨e) more than either resembles Mr. Bennet in Pride and Prejudice. (Testing our model’s ability to discern similarities in spite of elapsed time.) All 29 hypotheses can be found in a supplementary technical report (Bamman et al., 2014). We emphasize that the full set of hypotheses was locked before the model was estimated. 6 Experiments Part of the motivation of our mixed effects model is to be able to tackle hypothesis class C—by factoring out the influence of a particular author on the learning of personas, we would like to be able to discriminate between characters that all have a common authorial voice. In contrast, the Persona Regression model of Bamman et al. (2013), which uses metadata variables (like authorship) to encourage entities with similar covariates to have similar personas, reflects an assumption that makes</context>
</contexts>
<marker>Bamman, Underwood, Smith, 2014</marker>
<rawString>David Bamman, Ted Underwood, and Noah A. Smith. 2014. Appendix to ‘A Bayesian mixed effects model of literary character’. Technical report, Carnegie Mellon University, University of IllinoisUrbana Champaign.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="17217" citStr="Blei et al., 2003" startWordPosition="2758" endWordPosition="2761">00 unique cluster IDs). Each role type r E {agent, patient, possessive, predicative} and vocabulary word v (here, a cluster ID) is associated with a real-valued vector ηr,v = [ηmeta pers 0] r,v ,ηr,v ,ηr, v of length M + P + 1. The first M + P elements are drawn from a Laplace prior with mean µ = 0 and scale A = 1; the last element η0r,v is an unregularized bias term accounting for the background. Each element in this vector captures the log-additive effect of each author, persona, and the background distribution on the word’s probability (Eq. 1, below). Much like latent Dirichlet allocation (Blei et al., 2003), each document d in our dataset draws a multinomial distribution Bd over personas from a shared Dirichlet prior α, which captures the proportion of each character type in that particular document. Every character c in the document draws its persona p from this document-specific multinomial. Given document metadata m (here, one of a set of M authors) and persona p, each tuple of a role r with word w is assumed to be drawn from Eq. 1 in Fig. 3. This SAGE model can be understood as a log-linear distribution with three kinds of features (metadata, persona, and back373 V meta ] + 1Pws [] + , ntaP(</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wayne Booth</author>
</authors>
<title>The Rhetoric of Fiction.</title>
<date>1961</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="36453" citStr="Booth, 1961" startWordPosition="5970" endWordPosition="5971"> Our method establishes the possibility of representing the relationship between character and narrative form in a hierarchical Bayesian model. Postulating an interaction between authorial diction and character allows models that consider the effect of the author to more closely reproduce a human reader’s judgments, especially by learning to distinguish different character types within a single author’s oeuvre. This opens the door to considering other structural and formal dimensions of narration. For instance, representation of character is notoriously complicated by narrative point of view (Booth, 1961); and indeed, comparisons between first-person narrators and other characters are a primary source of error for all models tested above. The strategy we have demonstrated suggests that it might be productive to address this by modeling the interaction of character and point of view as a separate effect analogous to authorship. It is also worth noting that the models tested above diverge from many structuralist theories of narrative (Propp, 1998) by allowing multiple instances of the same persona in a single work. Learning structural limitations on the number of “protagonists” likely to coexist</context>
</contexts>
<marker>Booth, 1961</marker>
<rawString>Wayne Booth. 1961. The Rhetoric of Fiction. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="13370" citStr="Brown et al. (1992)" startWordPosition="2119" endWordPosition="2122">ficient window size (we use n = 10), these word embeddings capture semantic similarity (placing topically similar words near each other in vector space).6 We learn a 100-dimensional embedding for each of the 512,344 words in our vocabulary. To create a partition over the vocabulary, we use hard K-means clustering (with Euclidean distance) to group the 512,344 word types into 1,000 clusters. We then agglomeratively cluster those 1,000 groups to assign bitstring representations to each one, forming a balanced binary tree by only merging existing clusters at equal levels in the hi6In comparison, Brown et al. (1992) clusters learned from the same data capture syntactic similarity (placing functionally similar words in the same cluster). 372 Figure 1: Bitstring representations of neural agglomerative clusters, illustrating the leaf nodes in a binary tree rooted in the prefix 01110011. Bitstring encodings of intermediate nodes and terminal leaves result by following the left (0) and right (1) branches of the merge tree created through agglomerative clustering. 0111001111: pair boots shoes gloves leather 1 0 0111001110: hat coat cap cloak handkerchief 1 01110011 → 0111001101: dress clothes wore worn wear 01</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
</authors>
<title>Event schema induction with a probabilistic entity-driven model.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<location>Seattle, Washington, USA.</location>
<contexts>
<context position="1125" citStr="Chambers (2013)" startWordPosition="164" endWordPosition="165">ed between 1700 and 1899. Unlike prior work in which character types are assumed responsible for probabilistically generating all text associated with a character, we introduce a model that employs multiple effects to account for the influence of extra-linguistic information (such as author). In an empirical evaluation, we find that this method leads to improved agreement with the preregistered judgments of a literary scholar, complementing the results of alternative models. 1 Introduction Recent work in NLP has begun to exploit the potential of entity-centric modeling for a variety of tasks: Chambers (2013) places entities at the center of probabilistic frame induction, showing gains over a comparable event-centric model (Cheung et al., 2013); Bamman et al. (2013) explicitly learn character types (or “personas”) in a dataset of Wikipedia movie plot summaries; and entity-centric models form one dominant approach in coreference resolution (Durrett et al., 2013; Haghighi and Klein, 2010). One commonality among all of these very different probabilistic approaches is that each learns statistical regularities about how entities are depicted in text (whether for the sake of learning a set of semantic r</context>
</contexts>
<marker>Chambers, 2013</marker>
<rawString>Nathanael Chambers. 2013. Event schema induction with a probabilistic entity-driven model. In Proc. of EMNLP, Seattle, Washington, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Roni Rosenfeld</author>
</authors>
<title>A survey of smoothing techniques for me models.</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="14926" citStr="Chen and Rosenfeld, 2000" startWordPosition="2360" endWordPosition="2363">the words that are associated with them (as opposed to other factors, such as time period, genre, or author), we adopt a hierarchical Bayesian approach in which the words we observe are generated conditional on a combination of different effects captured in a log-linear (or “maximum entropy”) distribution. Maximum entropy approaches to language modeling have been used since Rosenfeld (1996) to incorporate long-distance information, such as previously-mentioned trigger words, into n-gram language models. This work has since been extended to a Bayesian setting by applying both a Gaussian prior (Chen and Rosenfeld, 2000), which dampens the impact of any individual feature, and sparsity-inducing priors (Kazama and Tsujii, 2003; Goodman, 2004), which can drive many feature weights to 0. The latter have been applied specifically to the problem of estimating word probabilities with sparse additive generative (SAGE) models (Eisenstein et al., 2011), where sparse extra-linguistic effects can influence a word probability in a larger generative setting. In contrast to previous work in which the probability of a word linked to a character is dependent entirely on the character’s latent persona, in our model, we see th</context>
</contexts>
<marker>Chen, Rosenfeld, 2000</marker>
<rawString>Stanley F. Chen and Roni Rosenfeld. 2000. A survey of smoothing techniques for me models. IEEE Transactions on Speech and Audio Processing, 8(1):37–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jackie Chi Kit Cheung</author>
<author>Hoifung Poon</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Probabilistic frame induction.</title>
<date>2013</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="1263" citStr="Cheung et al., 2013" startWordPosition="183" endWordPosition="186">t associated with a character, we introduce a model that employs multiple effects to account for the influence of extra-linguistic information (such as author). In an empirical evaluation, we find that this method leads to improved agreement with the preregistered judgments of a literary scholar, complementing the results of alternative models. 1 Introduction Recent work in NLP has begun to exploit the potential of entity-centric modeling for a variety of tasks: Chambers (2013) places entities at the center of probabilistic frame induction, showing gains over a comparable event-centric model (Cheung et al., 2013); Bamman et al. (2013) explicitly learn character types (or “personas”) in a dataset of Wikipedia movie plot summaries; and entity-centric models form one dominant approach in coreference resolution (Durrett et al., 2013; Haghighi and Klein, 2010). One commonality among all of these very different probabilistic approaches is that each learns statistical regularities about how entities are depicted in text (whether for the sake of learning a set of semantic roles, character types, or linking anaphora to the entities to which they refer). In each case, the text we observe associated with an enti</context>
</contexts>
<marker>Cheung, Poon, Vanderwende, 2013</marker>
<rawString>Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Vanderwende. 2013. Probabilistic frame induction. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter T Davis</author>
<author>David K Elson</author>
<author>Judith L Klavans</author>
</authors>
<title>Methods for precise named entity matching in digital collections.</title>
<date>2003</date>
<booktitle>In Proc. of JCDL,</booktitle>
<location>Washington, DC, USA.</location>
<contexts>
<context position="9738" citStr="Davis et al. (2003)" startWordPosition="1531" endWordPosition="1534">, 2007) for dependency parsing (trained on Stanford typed dependencies), and the Stanford named entity recognizer (Finkel et al., 2005). It includes the following components for clustering character name mentions, resolving pronominal coreference, and reducing vocabulary dimensionality. 3.1 Character Clustering First, let us terminologically distinguish between a character mention in a text (e.g., the token Tom on page 141 of The Adventures of Tom Sawyer) and a character entity (e.g., TOM SAWYER the character, to which that token refers). To resolve the former to the latter, we largely follow Davis et al. (2003) and Elson et al. (2010): we define a set of initial characters corresponding to each unique character name that is not a subset of another (e.g., Mr. Tom Sawyer) and deterministically create a set of allowable variants for each one (Mr. Tom Sawyer —* Tom, Sawyer, Tom Sawyer, Mr. Sawyer, and Mr. Tom); then, from the beginning of the book to the end, we greedily assign each mention to the most recently linked entity for whom it is a variant. The result constitutes our set of characters, with all mentions partitioned among them. 3.2 Pronominal Coreference Resolution While the character clusterin</context>
</contexts>
<marker>Davis, Elson, Klavans, 2003</marker>
<rawString>Peter T. Davis, David K. Elson, and Judith L. Klavans. 2003. Methods for precise named entity matching in digital collections. In Proc. of JCDL, Washington, DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>Stanford typed dependencies manual.</title>
<date>2008</date>
<tech>Technical report,</tech>
<institution>Stanford University.</institution>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008. Stanford typed dependencies manual. Technical report, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>David Hall</author>
<author>Dan Klein</author>
</authors>
<title>Decentralized entity-level modeling for coreference resolution.</title>
<date>2013</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1483" citStr="Durrett et al., 2013" startWordPosition="216" endWordPosition="219">to improved agreement with the preregistered judgments of a literary scholar, complementing the results of alternative models. 1 Introduction Recent work in NLP has begun to exploit the potential of entity-centric modeling for a variety of tasks: Chambers (2013) places entities at the center of probabilistic frame induction, showing gains over a comparable event-centric model (Cheung et al., 2013); Bamman et al. (2013) explicitly learn character types (or “personas”) in a dataset of Wikipedia movie plot summaries; and entity-centric models form one dominant approach in coreference resolution (Durrett et al., 2013; Haghighi and Klein, 2010). One commonality among all of these very different probabilistic approaches is that each learns statistical regularities about how entities are depicted in text (whether for the sake of learning a set of semantic roles, character types, or linking anaphora to the entities to which they refer). In each case, the text we observe associated with an entity in a document is directly dependent on the class of entity—and only that class. This relationship between entity and text is a theoretical assumption, with important consequences for learning: entity types learned in </context>
</contexts>
<marker>Durrett, Hall, Klein, 2013</marker>
<rawString>Greg Durrett, David Hall, and Dan Klein. 2013. Decentralized entity-level modeling for coreference resolution. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Amr Ahmed</author>
<author>Eric P Xing</author>
</authors>
<title>Sparse additive generative models of text.</title>
<date>2011</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="15255" citStr="Eisenstein et al., 2011" startWordPosition="2409" endWordPosition="2412">ches to language modeling have been used since Rosenfeld (1996) to incorporate long-distance information, such as previously-mentioned trigger words, into n-gram language models. This work has since been extended to a Bayesian setting by applying both a Gaussian prior (Chen and Rosenfeld, 2000), which dampens the impact of any individual feature, and sparsity-inducing priors (Kazama and Tsujii, 2003; Goodman, 2004), which can drive many feature weights to 0. The latter have been applied specifically to the problem of estimating word probabilities with sparse additive generative (SAGE) models (Eisenstein et al., 2011), where sparse extra-linguistic effects can influence a word probability in a larger generative setting. In contrast to previous work in which the probability of a word linked to a character is dependent entirely on the character’s latent persona, in our model, we see the probability of a word as dependent on: (i) the background likelihood of the word, (ii) the author, so that a word becomes more probable if a particular author tends to use it more, and (iii) the character’s persona, so that a word is more probable if appearing with a particular persona. Intuitively, if the author Jane Austen </context>
</contexts>
<marker>Eisenstein, Ahmed, Xing, 2011</marker>
<rawString>Jacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011. Sparse additive generative models of text. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David K Elson</author>
<author>Nicholas Dames</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Extracting social networks from literary fiction.</title>
<date>2010</date>
<booktitle>In Proc. of ACL,</booktitle>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9762" citStr="Elson et al. (2010)" startWordPosition="1536" endWordPosition="1539">arsing (trained on Stanford typed dependencies), and the Stanford named entity recognizer (Finkel et al., 2005). It includes the following components for clustering character name mentions, resolving pronominal coreference, and reducing vocabulary dimensionality. 3.1 Character Clustering First, let us terminologically distinguish between a character mention in a text (e.g., the token Tom on page 141 of The Adventures of Tom Sawyer) and a character entity (e.g., TOM SAWYER the character, to which that token refers). To resolve the former to the latter, we largely follow Davis et al. (2003) and Elson et al. (2010): we define a set of initial characters corresponding to each unique character name that is not a subset of another (e.g., Mr. Tom Sawyer) and deterministically create a set of allowable variants for each one (Mr. Tom Sawyer —* Tom, Sawyer, Tom Sawyer, Mr. Sawyer, and Mr. Tom); then, from the beginning of the book to the end, we greedily assign each mention to the most recently linked entity for whom it is a variant. The result constitutes our set of characters, with all mentions partitioned among them. 3.2 Pronominal Coreference Resolution While the character clustering stage is essentially p</context>
</contexts>
<marker>Elson, Dames, McKeown, 2010</marker>
<rawString>David K. Elson, Nicholas Dames, and Kathleen R. McKeown. 2010. Extracting social networks from literary fiction. In Proc. of ACL, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="9254" citStr="Finkel et al., 2005" startWordPosition="1456" endWordPosition="1459">ce resolution (with thousands of potential antecedents) prove to be 3All categories are described using the Stanford typed dependencies (de Marneffe and Manning, 2008), but any syntactic formalism is equally applicable. 371 the biggest bottlenecks. Before addressing character inference, we present here a prerequisite NLP pipeline that scales well to book-length documents.4 This pipeline uses the Stanford POS tagger (Toutanova et al., 2003), the linear-time MaltParser (Nivre et al., 2007) for dependency parsing (trained on Stanford typed dependencies), and the Stanford named entity recognizer (Finkel et al., 2005). It includes the following components for clustering character name mentions, resolving pronominal coreference, and reducing vocabulary dimensionality. 3.1 Character Clustering First, let us terminologically distinguish between a character mention in a text (e.g., the token Tom on page 141 of The Adventures of Tom Sawyer) and a character entity (e.g., TOM SAWYER the character, to which that token refers). To resolve the former to the latter, we largely follow Davis et al. (2003) and Elson et al. (2010): we define a set of initial characters corresponding to each unique character name that is </context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Forster</author>
</authors>
<date>1927</date>
<journal>Aspects of the Novel. Harcourt, Brace &amp; Co.</journal>
<contexts>
<context position="4600" citStr="Forster, 1927" startWordPosition="733" endWordPosition="734">roceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 370–379, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics an equally strong critical tradition that treats character as a formal dimension of narrative. To describe a character as a “blocking figure” or “firstperson narrator,” for instance, is a statement less about the attributes of an imagined person than about a narrative function (Keen, 2003). Characters are in one sense collections of psychological or moral attributes, but in another sense “wordmasses” (Forster, 1927). This tension between “referential” and “formalist” models of character has been a centrally “divisive question in ... literary theory” (Woloch, 2003). Considering primary source texts (as distinct from plot summaries) forces us to confront new theoretical questions about character. In a plot summary (such as those explored by Bamman et al., 2013), a human reader may already have used implicit models of character to extract high-level features. To infer character types from raw narrative text, researchers need to explicitly model the relationship of character to narrative form. This is not a </context>
</contexts>
<marker>Forster, 1927</marker>
<rawString>E. M. Forster. 1927. Aspects of the Novel. Harcourt, Brace &amp; Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Classes for fast maximum entropy training.</title>
<date>2001</date>
<booktitle>In Proc. of ICASSP.</booktitle>
<contexts>
<context position="18821" citStr="Goodman (2001)" startWordPosition="3062" endWordPosition="3063">oftmax formulation, with one 77-vector per role and node in the binary “flat” tree of word clusters, giving a distribution over bit strings (b) with the same number of parameters as Eq. 1. m, p, and r (as above) we let b = b1b2 bn denote the bitstring representation of a word cluster, and the distribution is given by Eq. 2 in Fig. 3. In this paramaterization, rather than one for each role and vocabulary term, we have one for each role and conditional binary decision in the tree (each bitstring prefix). Since the tree is binary with V leaves, this yields the same total number of parameters. As Goodman (2001) points out, while this reparameterization is exact for true probabilities, it remains an approximation for estimated models (with generalization behavior dependent on how well the class hierarchy is supported by the data). In addition to enabling faster inference, one advantage of the bitstring representation and the hierarchical softmax parameterization is that we can easily calculate probabilities of clusters at different gran ··· η- vector η-vector ularities. nce are p (the personas for each character) and the effects that each author and persona have on the probability of a word. Rather t</context>
<context position="20116" citStr="Goodman (2001)" startWordPosition="3271" endWordPosition="3272">hese values using stochastic EM, alternating between collapsed Gibbs sampling for each p and maximizing with respect to η, η. At each step, the required quantity is the probability that character c in document d has persona z, given everything else. This is proportional to the number personas.8 pd,c = z) of all of the words 4.1 Hierarchical Softmax The partition function in Eq. 1 can lead to slow inference for any reasonably-sized vocabulary. To address this, we reparameterize the model by exploiting the structure of the agglomerative clustering in to perform a hierarchical softmax, following Goodman (2001), Morin and Bengio (2005) and Mikolov et al. (2013). The bitstring representations by which we encode each word in the vocabulary serve as natural, and inherently meaningful, intermediate classes that correspond to semantically related subsets of the vocabulary, with each bitstring prefix denoting one such class. Longer bitstrings correspond to more fine-grained classes. In the example shown in Figure 1, is one such intermediate class, containing the union of pair, boots, shoes, gloves leather and hat, coat, cap cloak, handkerchief. Because these classes recursively partition the vocabulary, t</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>Joshua Goodman. 2001. Classes for fast maximum entropy training. In Proc. of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Exponential priors for maximum entropy models.</title>
<date>2004</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="15049" citStr="Goodman, 2004" startWordPosition="2380" endWordPosition="2381">Bayesian approach in which the words we observe are generated conditional on a combination of different effects captured in a log-linear (or “maximum entropy”) distribution. Maximum entropy approaches to language modeling have been used since Rosenfeld (1996) to incorporate long-distance information, such as previously-mentioned trigger words, into n-gram language models. This work has since been extended to a Bayesian setting by applying both a Gaussian prior (Chen and Rosenfeld, 2000), which dampens the impact of any individual feature, and sparsity-inducing priors (Kazama and Tsujii, 2003; Goodman, 2004), which can drive many feature weights to 0. The latter have been applied specifically to the problem of estimating word probabilities with sparse additive generative (SAGE) models (Eisenstein et al., 2011), where sparse extra-linguistic effects can influence a word probability in a larger generative setting. In contrast to previous work in which the probability of a word linked to a character is dependent entirely on the character’s latent persona, in our model, we see the probability of a word as dependent on: (i) the background likelihood of the word, (ii) the author, so that a word becomes</context>
</contexts>
<marker>Goodman, 2004</marker>
<rawString>Joshua Goodman. 2004. Exponential priors for maximum entropy models. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Coreference resolution in a modular, entity-centered model.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="1510" citStr="Haghighi and Klein, 2010" startWordPosition="220" endWordPosition="223">with the preregistered judgments of a literary scholar, complementing the results of alternative models. 1 Introduction Recent work in NLP has begun to exploit the potential of entity-centric modeling for a variety of tasks: Chambers (2013) places entities at the center of probabilistic frame induction, showing gains over a comparable event-centric model (Cheung et al., 2013); Bamman et al. (2013) explicitly learn character types (or “personas”) in a dataset of Wikipedia movie plot summaries; and entity-centric models form one dominant approach in coreference resolution (Durrett et al., 2013; Haghighi and Klein, 2010). One commonality among all of these very different probabilistic approaches is that each learns statistical regularities about how entities are depicted in text (whether for the sake of learning a set of semantic roles, character types, or linking anaphora to the entities to which they refer). In each case, the text we observe associated with an entity in a document is directly dependent on the class of entity—and only that class. This relationship between entity and text is a theoretical assumption, with important consequences for learning: entity types learned in this way will be increasing</context>
</contexts>
<marker>Haghighi, Klein, 2010</marker>
<rawString>Aria Haghighi and Dan Klein. 2010. Coreference resolution in a modular, entity-centered model. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Evaluation and extension of maximum entropy models with inequality constraints.</title>
<date>2003</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="15033" citStr="Kazama and Tsujii, 2003" startWordPosition="2376" endWordPosition="2379"> we adopt a hierarchical Bayesian approach in which the words we observe are generated conditional on a combination of different effects captured in a log-linear (or “maximum entropy”) distribution. Maximum entropy approaches to language modeling have been used since Rosenfeld (1996) to incorporate long-distance information, such as previously-mentioned trigger words, into n-gram language models. This work has since been extended to a Bayesian setting by applying both a Gaussian prior (Chen and Rosenfeld, 2000), which dampens the impact of any individual feature, and sparsity-inducing priors (Kazama and Tsujii, 2003; Goodman, 2004), which can drive many feature weights to 0. The latter have been applied specifically to the problem of estimating word probabilities with sparse additive generative (SAGE) models (Eisenstein et al., 2011), where sparse extra-linguistic effects can influence a word probability in a larger generative setting. In contrast to previous work in which the probability of a word linked to a character is dependent entirely on the character’s latent persona, in our model, we see the probability of a word as dependent on: (i) the background likelihood of the word, (ii) the author, so tha</context>
</contexts>
<marker>Kazama, Tsujii, 2003</marker>
<rawString>Jun’ichi Kazama and Jun’ichi Tsujii. 2003. Evaluation and extension of maximum entropy models with inequality constraints. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suzanne Keen</author>
</authors>
<title>Narrative Form.</title>
<date>2003</date>
<publisher>Palgrave Macmillan,</publisher>
<location>Basingstoke.</location>
<contexts>
<context position="4471" citStr="Keen, 2003" startWordPosition="713" endWordPosition="714">ore similar to each other than to entities from later texts simply by virtue of using hath and other archaic verb forms. 370 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 370–379, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics an equally strong critical tradition that treats character as a formal dimension of narrative. To describe a character as a “blocking figure” or “firstperson narrator,” for instance, is a statement less about the attributes of an imagined person than about a narrative function (Keen, 2003). Characters are in one sense collections of psychological or moral attributes, but in another sense “wordmasses” (Forster, 1927). This tension between “referential” and “formalist” models of character has been a centrally “divisive question in ... literary theory” (Woloch, 2003). Considering primary source texts (as distinct from plot summaries) forces us to confront new theoretical questions about character. In a plot summary (such as those explored by Bamman et al., 2013), a human reader may already have used implicit models of character to extract high-level features. To infer character ty</context>
</contexts>
<marker>Keen, 2003</marker>
<rawString>Suzanne Keen. 2003. Narrative Form. Palgrave Macmillan, Basingstoke.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>Proc. of ICLR.</booktitle>
<contexts>
<context position="12537" citStr="Mikolov et al., 2013" startWordPosition="1983" endWordPosition="1986">sed of all character entities in the previous 100 words not labeled as the true antecedent). In a 10-fold cross-validation on predicting the true nearest antecedent for a pronominal anaphor, this method achieves an average accuracy of 82.7%. With this trained model, we then select the highest-scoring antecedent within 100 words for each pronominal anaphor in our data. 3.3 Dimensionality Reduction To manage the degrees of freedom in the model described in §4, we perform dimensionality reduction on the vocabulary by learning word embeddings with a log-linear continuous skip-gram language model (Mikolov et al., 2013) on the entire collection of 15,099 books. This method learns a low-dimensional real-valued vector representation of each word to predict all of the words in a window around it; empirically, we find that with a sufficient window size (we use n = 10), these word embeddings capture semantic similarity (placing topically similar words near each other in vector space).6 We learn a 100-dimensional embedding for each of the 512,344 words in our vocabulary. To create a partition over the vocabulary, we use hard K-means clustering (with Euclidean distance) to group the 512,344 word types into 1,000 cl</context>
<context position="20167" citStr="Mikolov et al. (2013)" startWordPosition="3278" endWordPosition="3281"> between collapsed Gibbs sampling for each p and maximizing with respect to η, η. At each step, the required quantity is the probability that character c in document d has persona z, given everything else. This is proportional to the number personas.8 pd,c = z) of all of the words 4.1 Hierarchical Softmax The partition function in Eq. 1 can lead to slow inference for any reasonably-sized vocabulary. To address this, we reparameterize the model by exploiting the structure of the agglomerative clustering in to perform a hierarchical softmax, following Goodman (2001), Morin and Bengio (2005) and Mikolov et al. (2013). The bitstring representations by which we encode each word in the vocabulary serve as natural, and inherently meaningful, intermediate classes that correspond to semantically related subsets of the vocabulary, with each bitstring prefix denoting one such class. Longer bitstrings correspond to more fine-grained classes. In the example shown in Figure 1, is one such intermediate class, containing the union of pair, boots, shoes, gloves leather and hat, coat, cap cloak, handkerchief. Because these classes recursively partition the vocabulary, they offer a convenient way to reparameterize the mo</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. Proc. of ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederic Morin</author>
<author>Yoshua Bengio</author>
</authors>
<title>Hierarchical probabilistic neural network language model.</title>
<date>2005</date>
<booktitle>In Proc. of AISTATS.</booktitle>
<contexts>
<context position="20141" citStr="Morin and Bengio (2005)" startWordPosition="3273" endWordPosition="3276">g stochastic EM, alternating between collapsed Gibbs sampling for each p and maximizing with respect to η, η. At each step, the required quantity is the probability that character c in document d has persona z, given everything else. This is proportional to the number personas.8 pd,c = z) of all of the words 4.1 Hierarchical Softmax The partition function in Eq. 1 can lead to slow inference for any reasonably-sized vocabulary. To address this, we reparameterize the model by exploiting the structure of the agglomerative clustering in to perform a hierarchical softmax, following Goodman (2001), Morin and Bengio (2005) and Mikolov et al. (2013). The bitstring representations by which we encode each word in the vocabulary serve as natural, and inherently meaningful, intermediate classes that correspond to semantically related subsets of the vocabulary, with each bitstring prefix denoting one such class. Longer bitstrings correspond to more fine-grained classes. In the example shown in Figure 1, is one such intermediate class, containing the union of pair, boots, shoes, gloves leather and hat, coat, cap cloak, handkerchief. Because these classes recursively partition the vocabulary, they offer a convenient wa</context>
</contexts>
<marker>Morin, Bengio, 2005</marker>
<rawString>Frederic Morin and Yoshua Bengio. 2005. Hierarchical probabilistic neural network language model. In Proc. of AISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<pages>5</pages>
<contexts>
<context position="9126" citStr="Nivre et al., 2007" startWordPosition="1437" endWordPosition="1440">f 1.8 billion tokens); in particular, syntactic parsing, with cubic complexity in sentence length, and out-of-the-box coreference resolution (with thousands of potential antecedents) prove to be 3All categories are described using the Stanford typed dependencies (de Marneffe and Manning, 2008), but any syntactic formalism is equally applicable. 371 the biggest bottlenecks. Before addressing character inference, we present here a prerequisite NLP pipeline that scales well to book-length documents.4 This pipeline uses the Stanford POS tagger (Toutanova et al., 2003), the linear-time MaltParser (Nivre et al., 2007) for dependency parsing (trained on Stanford typed dependencies), and the Stanford named entity recognizer (Finkel et al., 2005). It includes the following components for clustering character name mentions, resolving pronominal coreference, and reducing vocabulary dimensionality. 3.1 Character Clustering First, let us terminologically distinguish between a character mention in a text (e.g., the token Tom on page 141 of The Adventures of Tom Sawyer) and a character entity (e.g., TOM SAWYER the character, to which that token refers). To resolve the former to the latter, we largely follow Davis e</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007. Maltparser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13:95–135, 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Propp</author>
</authors>
<title>Morphology of the Folktale.</title>
<date>1998</date>
<publisher>University of Texas Press,</publisher>
<note>2nd edition.</note>
<contexts>
<context position="36902" citStr="Propp, 1998" startWordPosition="6042" endWordPosition="6043"> other structural and formal dimensions of narration. For instance, representation of character is notoriously complicated by narrative point of view (Booth, 1961); and indeed, comparisons between first-person narrators and other characters are a primary source of error for all models tested above. The strategy we have demonstrated suggests that it might be productive to address this by modeling the interaction of character and point of view as a separate effect analogous to authorship. It is also worth noting that the models tested above diverge from many structuralist theories of narrative (Propp, 1998) by allowing multiple instances of the same persona in a single work. Learning structural limitations on the number of “protagonists” likely to coexist in a single story, for example, may be another fruitful area to explore. In all cases, the machinery of hierarchical models gives us the flexibility to incorporate such effects at will, while also being explicit about the theoretical assumptions that attend them. 9 Acknowledgments We thank the reviewers for their helpful comments. The research reported here was supported by a National Endowment for the Humanities start-up grant to T.U., U.S. Na</context>
</contexts>
<marker>Propp, 1998</marker>
<rawString>Vladimir Propp. 1998. Morphology of the Folktale. University of Texas Press, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roni Rosenfeld</author>
</authors>
<title>A maximum entropy approach to adaptive statistical language modelling.</title>
<date>1996</date>
<journal>Computer Speech and Language,</journal>
<volume>10</volume>
<issue>3</issue>
<pages>228</pages>
<contexts>
<context position="14694" citStr="Rosenfeld (1996)" startWordPosition="2328" endWordPosition="2329">ric and a group-average similarity function for calculating the distance between groups. Fig. 1 illustrates four of the 1,000 learned clusters. 4 Model In order to separate out the effects that a character’s persona has on the words that are associated with them (as opposed to other factors, such as time period, genre, or author), we adopt a hierarchical Bayesian approach in which the words we observe are generated conditional on a combination of different effects captured in a log-linear (or “maximum entropy”) distribution. Maximum entropy approaches to language modeling have been used since Rosenfeld (1996) to incorporate long-distance information, such as previously-mentioned trigger words, into n-gram language models. This work has since been extended to a Bayesian setting by applying both a Gaussian prior (Chen and Rosenfeld, 2000), which dampens the impact of any individual feature, and sparsity-inducing priors (Kazama and Tsujii, 2003; Goodman, 2004), which can drive many feature weights to 0. The latter have been applied specifically to the problem of estimating word probabilities with sparse additive generative (SAGE) models (Eisenstein et al., 2011), where sparse extra-linguistic effects</context>
</contexts>
<marker>Rosenfeld, 1996</marker>
<rawString>Roni Rosenfeld. 1996. A maximum entropy approach to adaptive statistical language modelling. Computer Speech and Language, 10(3):187 – 228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanchuan Sim</author>
<author>Brice D L Acree</author>
<author>Justin H Gross</author>
<author>Noah A Smith</author>
</authors>
<title>Measuring ideological proportions in political speeches.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<location>Seattle, Washington, USA.</location>
<contexts>
<context position="24131" citStr="Sim et al. (2013)" startWordPosition="3936" endWordPosition="3939">solute convergence threshold of 10−5. 5 Evaluation While standard NLP and machine learning practice is to evaluate the performance of an algorithm on a held-out gold standard, articulating what a true “persona” might be for a character is inherently problematic. Rather, we evaluate the performance and output of our model by preregistering a set of 29 hypotheses of varying scope and difficulty and comparing the performance of different models in either confirming, or failing to confirm, those hypotheses. This kind of evaluation was previously applied to a subjective text measurement problem by Sim et al. (2013). All hypotheses were created by a literary scholar with specialization in the period to not only give an empirical measure of the strengths and weaknesses of different models, but also to help explore exactly what the different models may, or may not, be learning. All preregistered hypotheses establish the degrees of similarity among three characters, taking the form: “character X is more similar to character Y than either X or Y is to a distractor character Z”; for a given model and definition of distance under that model, each hypothesis yields two yes/no decisions that we can evaluate: • d</context>
</contexts>
<marker>Sim, Acree, Gross, Smith, 2013</marker>
<rawString>Yanchuan Sim, Brice D. L. Acree, Justin H. Gross, and Noah A. Smith. 2013. Measuring ideological proportions in political speeches. In Proc. of EMNLP, Seattle, Washington, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network. In</title>
<date>2003</date>
<booktitle>Proc. of NAACL.</booktitle>
<contexts>
<context position="9077" citStr="Toutanova et al., 2003" startWordPosition="1430" endWordPosition="1433">h to be too slow for the scale of our data (a total of 1.8 billion tokens); in particular, syntactic parsing, with cubic complexity in sentence length, and out-of-the-box coreference resolution (with thousands of potential antecedents) prove to be 3All categories are described using the Stanford typed dependencies (de Marneffe and Manning, 2008), but any syntactic formalism is equally applicable. 371 the biggest bottlenecks. Before addressing character inference, we present here a prerequisite NLP pipeline that scales well to book-length documents.4 This pipeline uses the Stanford POS tagger (Toutanova et al., 2003), the linear-time MaltParser (Nivre et al., 2007) for dependency parsing (trained on Stanford typed dependencies), and the Stanford named entity recognizer (Finkel et al., 2005). It includes the following components for clustering character name mentions, resolving pronominal coreference, and reducing vocabulary dimensionality. 3.1 Character Clustering First, let us terminologically distinguish between a character mention in a text (e.g., the token Tom on page 141 of The Adventures of Tom Sawyer) and a character entity (e.g., TOM SAWYER the character, to which that token refers). To resolve th</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Underwood</author>
<author>Michael L Black</author>
<author>Loretta Auvil</author>
<author>Boris Capitanu</author>
</authors>
<title>Mapping mutable genres in structurally complex volumes.</title>
<date>2013</date>
<booktitle>In Proc. of IEEE International Conference on Big Data.</booktitle>
<contexts>
<context position="6738" citStr="Underwood et al. (2013)" startWordPosition="1059" endWordPosition="1062">9 distinct narratives drawn from HathiTrust Digital Library.2 From an initial collection of 469,200 volumes written in English and published between 1700 and 1899 (including poetry, drama, and nonfiction as well as prose narrative), we extract 32,209 volumes of prose fiction, remove duplicates and fuse multi-volume works to create the final dataset. Since the original texts were produced 2http://www.hathitrust.org by scanning and running OCR on physical books, we automatically correct common OCR errors and trim front and back matter from the volumes using the page-level classifiers and HMM of Underwood et al. (2013) Many aspects of this process would be simpler if we used manually-corrected texts, such as those drawn from Project Gutenberg. But we hope to produce research that has historical as well as computational significance, and doing so depends on the provenance of a collection. Gutenberg’s decentralized selection process tends to produce exceptionally good coverage of currently-popular genres like science fiction, whereas HathiTrust aggregates university libraries. Library collections are not guaranteed to represent the past perfectly, but they are larger, and less strongly shaped by contemporary </context>
</contexts>
<marker>Underwood, Black, Auvil, Capitanu, 2013</marker>
<rawString>Ted Underwood, Michael L Black, Loretta Auvil, and Boris Capitanu. 2013. Mapping mutable genres in structurally complex volumes. In Proc. of IEEE International Conference on Big Data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Woloch</author>
</authors>
<title>The One vs. the Many: Minor Characters and the Space of the Protagonist in the Novel.</title>
<date>2003</date>
<publisher>Princeton University Press,</publisher>
<location>Princeton NJ.</location>
<contexts>
<context position="4751" citStr="Woloch, 2003" startWordPosition="755" endWordPosition="756"> Association for Computational Linguistics an equally strong critical tradition that treats character as a formal dimension of narrative. To describe a character as a “blocking figure” or “firstperson narrator,” for instance, is a statement less about the attributes of an imagined person than about a narrative function (Keen, 2003). Characters are in one sense collections of psychological or moral attributes, but in another sense “wordmasses” (Forster, 1927). This tension between “referential” and “formalist” models of character has been a centrally “divisive question in ... literary theory” (Woloch, 2003). Considering primary source texts (as distinct from plot summaries) forces us to confront new theoretical questions about character. In a plot summary (such as those explored by Bamman et al., 2013), a human reader may already have used implicit models of character to extract high-level features. To infer character types from raw narrative text, researchers need to explicitly model the relationship of character to narrative form. This is not a solved problem, even for human readers. For instance, it has frequently been remarked that the characters of Charles Dickens share certain similarities</context>
</contexts>
<marker>Woloch, 2003</marker>
<rawString>Alex Woloch. 2003. The One vs. the Many: Minor Characters and the Space of the Protagonist in the Novel. Princeton University Press, Princeton NJ.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>