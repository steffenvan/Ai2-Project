<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.092005">
<title confidence="0.9987495">
Semantic Role Labelling with minimal resources:
Experiments with French
</title>
<author confidence="0.94756">
Rasoul Kaljahi†‡, Jennifer Foster†, Johann Roturier‡†NCLT, School of Computing, Dublin City University, Ireland
</author>
<email confidence="0.812636">
{rkaljahi, jfoster}@computing.dcu.ie
</email>
<author confidence="0.482059">
‡Symantec Research Labs, Dublin, Ireland
</author>
<email confidence="0.983876">
johann roturier@symantec.com
</email>
<sectionHeader confidence="0.99323" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999811571428571">
This paper describes a series of French se-
mantic role labelling experiments which
show that a small set of manually anno-
tated training data is superior to a much
larger set containing semantic role labels
which have been projected from a source
language via word alignment. Using uni-
versal part-of-speech tags and dependen-
cies makes little difference over the orig-
inal fine-grained tagset and dependency
scheme. Moreover, there seems to be no
improvement gained from projecting se-
mantic roles between direct translations
than between indirect translations.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9995314">
Semantic role labelling (SRL) (Gildea and Juraf-
sky, 2002) is the task of identifying the predicates
in a sentence, their semantic arguments and the
roles these arguments take. The last decade has
seen considerable attention paid to statistical SRL,
thanks to the existence of two major hand-crafted
resources for English, namely, FrameNet (Baker
et al., 1998) and PropBank (Palmer et al., 2005).
Apart from English, only a few languages have
SRL resources and these resources tend to be of
limited size compared to the English datasets.
French is one of those languages which suffer
from a scarcity of hand-crafted SRL resources.
The only available gold-standard resource is a
small set of 1000 sentences taken from Europarl
(Koehn, 2005) and manually annotated with Prop-
bank verb predicates (van der Plas et al., 2010b).
This dataset is then used by van der Plas et al.
(2011) to evaluate their approach to projecting the
SRLs of English sentences to their translations
</bodyText>
<footnote confidence="0.9060525">
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.999172314285715">
in French. They additionally build a large, “ar-
tificial” or automatically labelled dataset of ap-
proximately 1M Europarl sentences by projecting
the SRLs from English sentences to their French
translations and use it for training an SRL system.
We build on the work of van der Plas et al.
(2010b) by answering the following questions: 1)
How much artificial data is needed to train an
SRL system? 2) Is it better to use direct trans-
lations than indirect translations, i.e. is it better
to use for projection a source-target pair where
the source represents the original sentence and the
target represents its direct translation as opposed
to a source-target pair where the source and tar-
get are both translations of an original sentence
in a third language? 3) Is it better to use coarse-
grained syntactic information (in the form of uni-
versal part-of-speech tags and universal syntactic
dependencies) than to use fine-grained syntactic
information? We find that SRL performance lev-
els off after only 5K training sentences obtained
via projection and that direct translations are no
more useful than indirect translations. We also
find that it makes very little difference to French
SRL performance whether we use universal part-
of-speech tags and syntactic dependencies or more
fine-grained tags and dependencies.
The surprising result that SRL performance lev-
els off after just 5K training sentences leads us
to directly compare the small hand-crafted set of
1K sentences to the larger artificial training set.
We use 5-fold cross-validation on the small dataset
and find that the SRL performance is substantially
higher (&gt;10 F1 in identification and classification)
when the hand-crafted annotations are used.
</bodyText>
<sectionHeader confidence="0.999803" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9998205">
There has been relatively few works in French
SRL. Lorenzo and Cerisara (2012) propose a clus-
tering approach for verb predicate and argument
labelling (but not identification). They choose
</bodyText>
<page confidence="0.991961">
87
</page>
<note confidence="0.761042">
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 87–92,
Dublin, Ireland, August 23-24 2014.
</note>
<bodyText confidence="0.999958738095238">
VerbNet style roles (Schuler, 2006) and manu-
ally annotate sentences with them for evaluation,
achieving an F1 of 78.5.
Gardent and Cerisara (2010) propose a method
for semi-automatically annotating the French de-
pendency treebank (Candito et al., 2010) with
Propbank core roles (no adjuncts). They first
manually augment TreeLex (Kup´s´c and Abeill´e,
2008), a syntactic lexicon of French, with seman-
tic roles of syntactic arguments of verbs (i.e. verb
subcategorization). They then project this anno-
tation to verb instances in the dependency trees.
They evaluate their approach by performing error
analysis on a small sample and suggest directions
for improvement. The annotation work is however
at its preliminary stage and no data is published.
As mentioned earlier, van der Plas et al. (2011)
use word alignments to project the SRLs of the
English side of EuroParl to its French side result-
ing in a large artificial dataset. This idea is based
on the Direct Semantic Transfer hypothesis which
assumes that a semantic relationship between two
words in a sentence can be transferred to any
two words in the translation which are aligned to
these source-side words. Evaluation on their 1K
manually-annotated dataset shows that a syntactic-
semantic dependency parser trained on this artifi-
cial data set performs significantly better than di-
rectly projecting the labelling from its English side
– a promising result because, in a real-world sce-
nario, the English translations of the French data
to be annotated do not necessarily exist.
Pad´o and Lapata (2009) also make use of word
alignments to project SRLs from English to Ger-
man. The word alignments are used to compute
the semantic similarity between syntactic con-
stituents. In order to determine the extent of se-
mantic correspondence between English and Ger-
man, they manually annotate a set of parallel sen-
tences and find that about 72% of the frames and
92% of the argument roles exist in both sides, ig-
noring their lexical correspondence.
</bodyText>
<sectionHeader confidence="0.975135" genericHeader="method">
3 Datasets, SRL System and Evaluation
</sectionHeader>
<bodyText confidence="0.999973868421053">
We use the two datasets described in (van der Plas
et al., 2011) and the delivery report of the Clas-
sic project (van der Plas et al., 2010a). These
are the gold standard set of 1K sentences which
was annotated by manually identifying each verb
predicate, finding its equivalent English frameset
in PropBank and identifying and labelling its ar-
guments based on the description of the frame-
set (henceforth known as Classic1K), and the syn-
thetic dataset consisting of more than 980K sen-
tences (henceforth known as Classic980K), which
was created by word aligning an English-French
parallel corpus (Europarl) using GIZA++ (Och
and Ney, 2003) and projecting the French SRLs
from the English SRLs via the word alignments.
The joint syntactic-semantic parser described in
(Titov et al., 2009) was used to produce the En-
glish SRLs and the dependency parses of the
French side were produced using the ISBN parser
described in (Titov and Henderson, 2007).
We use LTH (Bj¨orkelund et al., 2009), a
dependency-based SRL system, in all of our ex-
periments. This system was among the best-
performing systems in the CoNLL 2009 shared
task (Hajiˇc et al., 2009) and is straightforward to
use. It comes with a set of features tuned for each
shared task language (English, German, Japanese,
Spanish, Catalan, Czech, Chinese). We compared
the performance of the English and Spanish fea-
ture sets on French and chose the former due to its
higher performance (by 1 F1 point).
To evaluate SRL performance, we use the
CoNLL 2009 shared task scoring script1, which
assumes a semantic dependency between the argu-
ment and predicate and the predicate and a dummy
root node and then calculates the precision (P), re-
call (R) and F1 of identification of these dependen-
cies and classification (labelling) of them.
</bodyText>
<sectionHeader confidence="0.999192" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998575">
4.1 Learning Curve
</subsectionHeader>
<bodyText confidence="0.999983266666667">
The ultimate goal of SRL projection is to build a
training set which partially compensates for the
lack of hand-crafted resources. van der Plas et
al. (2011) report encouraging results showing that
training on their projected data is beneficial over
directly obtaining the annotation via projection
which is not always possible. Although the quality
of such automatically-generated training data may
not be comparable to the manual one, the possi-
bility of building much bigger data sets may pro-
vide some advantages. Our first experiment inves-
tigates the extent to which the size of the synthetic
training set can improve performance.
We randomly select 100K sentences from Clas-
sic980K, shuffle them and split them into 20 sub-
</bodyText>
<footnote confidence="0.9975545">
1https://ufal.mff.cuni.cz/
conll2009-st/eval09.pl
</footnote>
<page confidence="0.99852">
88
</page>
<figureCaption confidence="0.9851495">
Figure 1: Learning curve with 100K training data
of projected annotations
Figure 2: Learning curve with 100K training data
of projected annotations on only direct translations
</figureCaption>
<bodyText confidence="0.999382166666667">
sets of 5K sentences. We then split the first 5K into
10 sets of 500 sentences. We train SRL models
on the resulting 29 subsets using LTH. The per-
formance of the models evaluated on Classic1K
is presented in Fig. 1. Surprisingly, the best F1
(58.7) is achieved by only 4K sentences, and af-
ter that the recall (and consequently F1) tends to
drop though precision shows a positive trend, sug-
gesting that the additional sentences bring little in-
formation. The large gap between precision and
recall is also interesting, showing that the projec-
tions do not have wide semantic role coverage.2
</bodyText>
<subsectionHeader confidence="0.994775">
4.2 Direct Translations
</subsectionHeader>
<bodyText confidence="0.970210904761905">
Each sentence in Europarl was written in one of
the official languages of the European Parliament
and translated to all of the other languages. There-
fore both sides of a parallel sentence pair can be in-
direct translations of each other. van der Plas et al.
(2011) suggest that translation divergence may af-
2Note that our results are not directly comparable with
(van der Plas et al., 2011) because they split Classic1K into
development and test sets, while we use the whole set for
testing. We do not have access to their split.
fect automatic projection of semantic roles. They
therefore select for their experiments only those
276K sentences from the 980K which are direct
translations between English and French. Moti-
vated by this idea, we replicate the learning curve
in Fig. 1 with another set of 100K sentences ran-
domly selected from only the direct translations.
The curve is shown in Fig. 2. There is no no-
ticeable difference between this and the graph in
Fig. 1, suggesting that the projections obtained via
direct translations are not of higher quality.
</bodyText>
<subsectionHeader confidence="0.999517">
4.3 Impact of Syntactic Annotation
</subsectionHeader>
<bodyText confidence="0.999822394736842">
Being a dependency-based semantic role labeller,
LTH employs a large set of features based on syn-
tactic dependency structure. This inspires us to
compare the impact of different types of syntactic
annotations on the performance of this system.
Based on the observations from the previous
sections, we choose two different sizes of training
sets. The first set contains the first 5K sentences
from the original 100K, as we saw that more than
this amount tends to diminish performance. The
second set contains the first 50K from the original
100K, the purpose of which is to check if changing
the parses affects the usefulness of adding more
data. We will call these data sets ClassicSK and
ClassicS0K respectively.
Petrov et al. (2012) create a set of 12 univer-
sal part-of-speech (POS) tags which should in the-
ory be applicable to any natural language. It is
interesting to know whether these POS tags are
more useful for SRL than the original set of the 29
more fine-grained POS tags used in French Tree-
bank which we have used so far. To this end, we
convert the original POS tags of the data to uni-
versal POS tags and retrain and evaluate the SRL
models. The results are given in the second row of
Table 1 (OrgDep+UniPOS). The first row of the
table (Original) shows the performance using
the original annotation. Even though the scores
increase in most cases – due mostly to a rise in
recall – the changes are small. It is worth noting
that identification seems to benefit more from the
universal POS tags.
Similar to universal POS tags, McDonald et al.
(2013) introduce a set of 40 universal dependency
types which generalize over the dependency struc-
ture specific to several languages. For French, they
provide a new treebank, called uni-dep-tb,
manually annotating 16,422 sentences from vari-
</bodyText>
<figure confidence="0.998376214285715">
80
70
60
50
20
10000
0
60000
90000
80000
70000
50000
30000
20000
40000
100000
Precision
Recall
F1
40
30
80
70
60
50
40
30
20
Precision
Recall
F1
100000
0
10000
20000
90000
80000
70000
60000
50000
40000
30000
</figure>
<page confidence="0.997015">
89
</page>
<table confidence="0.999262857142857">
5K 50K
Identification Classification Identification Classification
P R F1 P R F1 P R F1 P R F1
Original 85.95 59.64 70.42 71.34 49.50 58.45 86.67 58.07 69.54 72.44 48.54 58.13
OrgDep+UniPOS 86.71 60.46 71.24 71.11 49.58 58.43 86.82 58.71 70.05 72.30 48.90 58.34
StdUniDep+UniPOS 86.14 59.76 70.57 70.60 48.98 57.84 86.38 58.90 70.04 71.61 48.83 58.07
CHUniDep+UniPOS 85.98 59.21 70.13 70.66 48.66 57.63 86.47 58.26 69.61 71.74 48.34 57.76
</table>
<tableCaption confidence="0.999958">
Table 1: SRL performance using different syntactic parses with Classic 5K and 50K training sets
</tableCaption>
<bodyText confidence="0.982072736842105">
ous domains. We now explore the utility of this
new dependency scheme in SRL.
The French universal dependency treebank
comes in two versions, the first using the stan-
dard dependency structure based on basic Stanford
dependencies (de Marneffe and Manning, 2008)
where content words are the heads except in cop-
ula and adposition constructions, and the second
which treats content words as the heads for all
constructions without exemption. We use both
schemes in order to verify their effect on SRL.
In order to obtain universal dependencies for
our data, we train parsing models with Malt-
Parser (Nivre et al., 2006) using the entire
uni-dep-tb.3 We then parse our data us-
ing these MaltParser models. The input POS
tags to the parser are the universal POS tags
used in OrgDep+UniPOS. We train and evalu-
ate new SRL models on these data. The results
are shown in the third and fourth rows of Table
1. StdUniDept+UniPOS is the setting using
standard dependencies and CHUDep+UPOS using
content-head dependencies.
According to the third and fourth rows in Table
1, content-head dependencies are slightly less use-
ful than standard dependencies. The general ef-
fect of universal dependencies can be compared to
those of original ones by comparing these results
to OrgDep+UniPOS - the use of universal de-
pendencies appears to have only a modest (nega-
tive) effect. However, we must be careful of draw-
ing too many conclusions because in addition to
the difference in dependency schemes, the training
data used to train the parsers as well as the parsers
themselves are different.
Overall, we observe that the universal annota-
tions can be reliably used when the fine-grained
annotation is not available. This can be especially
</bodyText>
<footnote confidence="0.99703375">
3Based on our preliminary experiments on the pars-
ing performance, we use LIBSVM as learning algorithm,
nivreeager as parsing algorithm for the standard depen-
dency models and stackproj for the content-head ones.
</footnote>
<table confidence="0.999607333333333">
Identification Classification
P R F1 P R F1
1K 83.76 83.00 83.37 68.40 67.78 68.09
5K 85.94 59.62 70.39 71.30 49.47 58.40
1K+5K 85.74 66.53 74.92 71.48 55.46 62.46
SelfT 83.82 83.66 83.73 67.91 67.79 67.85
</table>
<tableCaption confidence="0.97461">
Table 2: Average scores of 5-fold cross-validation
with Classic 1K (1K), 5K (5K), 1K plus 5K
(1K+5K) and self-training with 1K seed and 5K
unlabeled data (SelfT)
</tableCaption>
<bodyText confidence="0.909431333333333">
useful for languages which lack such resources
and require techniques such as cross-lingual trans-
fer to replace them.
</bodyText>
<subsectionHeader confidence="0.998286">
4.4 Quality vs. Quantity
</subsectionHeader>
<bodyText confidence="0.99996892">
In Section 4.1, we saw that adding more data an-
notated through projection did not elevate SRL
performance. In other words, the same perfor-
mance was achieved using only a small amount
of data. This is contrary to the motivation for cre-
ating synthetic training data, especially when the
hand-annotated data already exist, albeit in a small
size. In this section, we compare the performance
of SRL models trained using manually-annotated
data with SRL models trained using 5K of artifi-
cial or synthetic training data. We use the original
syntactic annotations for both datasets.
To this end, we carry out a 5-fold cross-
validation on Classic1K. We then evaluate the
Classic5K model, on each of the 5 test sets gen-
erated in the cross-validation. The average scores
of the two evaluation setups are compared. The
results are shown in Table 2.
While the 5K model achieves higher precision,
its recall is far lower resulting in dramatically
lower F1. This high precision and low recall is due
to the low confidence of the model trained on pro-
jected data suggesting that a considerable amount
of information is not transferred during the projec-
tion. This issue can be attributed to the fact that the
</bodyText>
<page confidence="0.995153">
90
</page>
<bodyText confidence="0.999977580645161">
Classic projection uses intersection of alignments
in the two translation directions, which is the most
restrictive setting and leaves many source predi-
cates and arguments unaligned.
We next add the Classic5K projected data to
the manually annotated training data in each fold
of another cross-validation setting and evaluate
the resulting models on the same test sets. The
results are reported in the third row of the Ta-
ble 2 (1K+5K). As can be seen, the low qual-
ity of the projected data significantly degrades the
performance compared to when only manually-
annotated data are used for training.
Finally, based on the observation that the qual-
ity of labelling using manually annotated data is
higher than using the automatically projected data,
we replicate 1K+5K with the 5K data labelled us-
ing the model trained on the training subset of 1K
at each cross-validation fold. In other words, we
perform a one-round self-training with this model.
The performance of the resulting model evaluated
in the same cross-validation setting is given in the
last row of Table 2 (SelfT).
As expected, the labelling obtained by mod-
els trained on manual annotation are more useful
than the projected ones when used for training new
models. It is worth noting that, unlike with the
1K+5K setting, the balance between precision and
recall follows that of the 1K model. In addition,
some of the scores are the highest among all re-
sults, although the differences are not significant.
</bodyText>
<subsectionHeader confidence="0.996968">
4.5 How little is too little?
</subsectionHeader>
<bodyText confidence="0.999465">
In the previous section we saw that using a manu-
ally annotated dataset with as few as 800 sentences
resulted in significantly better SRL performance
than using projected annotation with as many as
5K sentences. This unfortunately indicates the
need for human labour in creating such resources.
It is interesting however to know the lower bound
of this requirement. To this end, we reverse our
cross-validation setting and train on 200 and test
on 800 sentences. We then compare to the 5K
models evaluated on the same 800 sentence sets
at each fold. The results are presented in Table 3.
Even with only 200 manually annotated sentences,
the performance is considerably higher than with
5K sentences of projected annotations. However,
as one might expect, compared to when 800 sen-
tences are used for training, this small model per-
forms significantly worse.
</bodyText>
<table confidence="0.997718">
Identification Classification
P R F1 P R F1
1K 82.34 79.61 80.95 64.14 62.01 63.06
5K 85.95 59.64 70.42 71.34 49.50 58.45
</table>
<tableCaption confidence="0.992253666666667">
Table 3: Average scores of 5-fold cross-validation
with Classic 1K (1K) and 5K (5K) using 200 sen-
tences for training and 800 for testing at each fold
</tableCaption>
<sectionHeader confidence="0.996513" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999877181818182">
We have explored the projection-based approach
to SRL by carrying out experiments with a large
set of French semantic role labels which have been
automatically transferred from English. We have
found that increasing the number of these artificial
projections that are used in training an SRL sys-
tem does not improve performance as might have
been expected when creating such a resource. In-
stead it is better to train directly on what little gold
standard data is available, even if this dataset con-
tains only 200 sentences. We suspect that the dis-
appointing performance of the projected dataset
originates in the restrictive way the word align-
ments have been extracted. Only those alignments
that are in the intersection of the English-French
and French-English word alignment sets are re-
tained resulting in low SRL recall. Recent prelim-
inary experiments show that less restrictive align-
ment extraction strategies including extracting the
union of the two sets or source-to-target align-
ments lead to a better recall and consequently F1
both when used for direct projection to the test
data or for creating the training data and then ap-
plying the resulting model to the test data.
We have compared the use of universal POS
tags and dependency labels to the original, more
fine-grained sets and shown that there is only a
little difference. However, it remains to be seen
whether this finding holds for other languages or
whether it will still hold for French when SRL per-
formance can be improved. It might also be in-
teresting to explore the combination of universal
dependencies with fine-grained POS tags.
</bodyText>
<sectionHeader confidence="0.998285" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.989391166666667">
This research has been supported by the Irish
Research Council Enterprise Partnership Scheme
(EPSPG/2011/102) and the computing infrastruc-
ture of the CNGL at DCU. We thank Lonneke van
der Plas for providing us the Classic data. We also
thank the reviewers for their helpful comments.
</bodyText>
<page confidence="0.998126">
91
</page>
<sectionHeader confidence="0.989316" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999700373737374">
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th ACL, pages 86–90.
Anders Bj¨orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 43–48.
Marie Candito, Benot Crabb´e, and Pascal Denis. 2010.
Statistical french dependency parsing: treebank
conversion and first results. In Proceedings of
LREC’2010.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies
representation. In Proceedings of the COLING
Workshop on Cross-Framework and Cross-Domain
Parser Evaluation.
Claire Gardent and Christophe Cerisara. 2010. Semi-
Automatic Propbanking for French. In TLT9 -
The Ninth International Workshop on Treebanks and
Linguistic Theories.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245–288.
Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies
in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1–18.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Sum-
mit, pages 79–86.
Anna Kup´s´c and Anne Abeill´e. 2008. Growing
treelex. In Proceedings of the 9th International Con-
ference on Computational Linguistics and Intelli-
gent Text Processing, CICLing’08, pages 28–39.
Alejandra Lorenzo and Christophe Cerisara. 2012.
Unsupervised frame based semantic role induction:
application to french and english. In Proceedings of
the ACL 2012 Joint Workshop on Statistical Parsing
and Semantic Processing of Morphologically Rich
Languages, pages 30–35.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
man Ganchev, Keith Hall, Slav Petrov, Hao
Zhang, Oscar T¨ackstr¨om, Claudia Bedini, N´uria
Bertomeu Castell´o, and Jungmee Lee. 2013. Uni-
versal dependency annotation for multilingual pars-
ing. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 92–97.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In In Proceedings of LREC.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Sebastian Pad´o and Mirella Lapata. 2009. Cross-
lingual annotation projection of semantic roles. J.
Artif. Int. Res., 36(1):307–340.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–106.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC, May.
Karin Kipper Schuler. 2006. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. the-
sis, University of Pennsylvania.
Ivan Titov and James Henderson. 2007. A latent vari-
able model for generative dependency parsing. In
Proceedings of the 10th International Conference on
Parsing Technologies, pages 144–155.
Ivan Titov, James Henderson, Paola Merlo, and
Gabriele Musillo. 2009. Online projectivisation for
synchronous parsing of semantic and syntactic de-
pendencies. In In Proceedings of the Internation
Joint Conference on Artificial Intelligence (IJCAI),
pages 1562–1567.
Lonneke van der Plas, James Henderson, and Paola
Merlo. 2010a. D6. 2: Semantic role annotation of a
french-english corpus.
Lonneke van der Plas, Tanja Samardˇzi´c, and Paola
Merlo. 2010b. Cross-lingual validity of propbank
in the manual annotation of french. In Proceedings
of the Fourth Linguistic Annotation Workshop, LAW
IV ’10, pages 113–117.
Lonneke van der Plas, Paola Merlo, and James Hen-
derson. 2011. Scaling up automatic cross-lingual
semantic role annotation. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 299–304.
</reference>
<page confidence="0.995948">
92
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.364242">
<title confidence="0.995186">Semantic Role Labelling with minimal</title>
<author confidence="0.41174">Experiments with French</author>
<affiliation confidence="0.630109">Jennifer Johann School of Computing, Dublin City University,</affiliation>
<address confidence="0.77258">Research Labs, Dublin,</address>
<email confidence="0.989014">johannroturier@symantec.com</email>
<abstract confidence="0.999759733333333">This paper describes a series of French semantic role labelling experiments which show that a small set of manually annotated training data is superior to a much larger set containing semantic role labels which have been projected from a source language via word alignment. Using universal part-of-speech tags and dependencies makes little difference over the original fine-grained tagset and dependency scheme. Moreover, there seems to be no improvement gained from projecting semantic roles between direct translations than between indirect translations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th ACL,</booktitle>
<pages>86--90</pages>
<contexts>
<context position="1231" citStr="Baker et al., 1998" startWordPosition="172" endWordPosition="175">d dependencies makes little difference over the original fine-grained tagset and dependency scheme. Moreover, there seems to be no improvement gained from projecting semantic roles between direct translations than between indirect translations. 1 Introduction Semantic role labelling (SRL) (Gildea and Jurafsky, 2002) is the task of identifying the predicates in a sentence, their semantic arguments and the roles these arguments take. The last decade has seen considerable attention paid to statistical SRL, thanks to the existence of two major hand-crafted resources for English, namely, FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Apart from English, only a few languages have SRL resources and these resources tend to be of limited size compared to the English datasets. French is one of those languages which suffer from a scarcity of hand-crafted SRL resources. The only available gold-standard resource is a small set of 1000 sentences taken from Europarl (Koehn, 2005) and manually annotated with Propbank verb predicates (van der Plas et al., 2010b). This dataset is then used by van der Plas et al. (2011) to evaluate their approach to projecting the SRLs of English sentences to their t</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The berkeley framenet project. In Proceedings of the 36th ACL, pages 86–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Love Hafdell</author>
<author>Pierre Nugues</author>
</authors>
<title>Multilingual semantic role labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>43--48</pages>
<marker>Bj¨orkelund, Hafdell, Nugues, 2009</marker>
<rawString>Anders Bj¨orkelund, Love Hafdell, and Pierre Nugues. 2009. Multilingual semantic role labeling. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task, pages 43–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie Candito</author>
<author>Benot Crabb´e</author>
<author>Pascal Denis</author>
</authors>
<title>Statistical french dependency parsing: treebank conversion and first results.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC’2010.</booktitle>
<marker>Candito, Crabb´e, Denis, 2010</marker>
<rawString>Marie Candito, Benot Crabb´e, and Pascal Denis. 2010. Statistical french dependency parsing: treebank conversion and first results. In Proceedings of LREC’2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>In Proceedings of the COLING Workshop on Cross-Framework and Cross-Domain Parser Evaluation.</booktitle>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The stanford typed dependencies representation. In Proceedings of the COLING Workshop on Cross-Framework and Cross-Domain Parser Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Gardent</author>
<author>Christophe Cerisara</author>
</authors>
<title>SemiAutomatic Propbanking for French.</title>
<date>2010</date>
<booktitle>In TLT9 -The Ninth International Workshop on Treebanks and Linguistic Theories.</booktitle>
<contexts>
<context position="4262" citStr="Gardent and Cerisara (2010)" startWordPosition="648" endWordPosition="651"> SRL performance is substantially higher (&gt;10 F1 in identification and classification) when the hand-crafted annotations are used. 2 Related Work There has been relatively few works in French SRL. Lorenzo and Cerisara (2012) propose a clustering approach for verb predicate and argument labelling (but not identification). They choose 87 Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 87–92, Dublin, Ireland, August 23-24 2014. VerbNet style roles (Schuler, 2006) and manually annotate sentences with them for evaluation, achieving an F1 of 78.5. Gardent and Cerisara (2010) propose a method for semi-automatically annotating the French dependency treebank (Candito et al., 2010) with Propbank core roles (no adjuncts). They first manually augment TreeLex (Kup´s´c and Abeill´e, 2008), a syntactic lexicon of French, with semantic roles of syntactic arguments of verbs (i.e. verb subcategorization). They then project this annotation to verb instances in the dependency trees. They evaluate their approach by performing error analysis on a small sample and suggest directions for improvement. The annotation work is however at its preliminary stage and no data is published.</context>
</contexts>
<marker>Gardent, Cerisara, 2010</marker>
<rawString>Claire Gardent and Christophe Cerisara. 2010. SemiAutomatic Propbanking for French. In TLT9 -The Ninth International Workshop on Treebanks and Linguistic Theories.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="929" citStr="Gildea and Jurafsky, 2002" startWordPosition="124" endWordPosition="128">s paper describes a series of French semantic role labelling experiments which show that a small set of manually annotated training data is superior to a much larger set containing semantic role labels which have been projected from a source language via word alignment. Using universal part-of-speech tags and dependencies makes little difference over the original fine-grained tagset and dependency scheme. Moreover, there seems to be no improvement gained from projecting semantic roles between direct translations than between indirect translations. 1 Introduction Semantic role labelling (SRL) (Gildea and Jurafsky, 2002) is the task of identifying the predicates in a sentence, their semantic arguments and the roles these arguments take. The last decade has seen considerable attention paid to statistical SRL, thanks to the existence of two major hand-crafted resources for English, namely, FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Apart from English, only a few languages have SRL resources and these resources tend to be of limited size compared to the English datasets. French is one of those languages which suffer from a scarcity of hand-crafted SRL resources. The only available gold-sta</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajiˇc</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Ant`onia Mart´ı</author>
<author>Llu´ıs M`arquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
<author>Jan ˇStˇep´anek</author>
<author>Pavel Straˇn´ak</author>
<author>Mihai Surdeanu</author>
<author>Nianwen Xue</author>
<author>Yi Zhang</author>
</authors>
<title>The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>1--18</pages>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Mart´ı, M`arquez, Meyers, Nivre, Pad´o, ˇStˇep´anek, Straˇn´ak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Conference Proceedings: the tenth Machine Translation Summit,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="1610" citStr="Koehn, 2005" startWordPosition="236" endWordPosition="237">tic arguments and the roles these arguments take. The last decade has seen considerable attention paid to statistical SRL, thanks to the existence of two major hand-crafted resources for English, namely, FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Apart from English, only a few languages have SRL resources and these resources tend to be of limited size compared to the English datasets. French is one of those languages which suffer from a scarcity of hand-crafted SRL resources. The only available gold-standard resource is a small set of 1000 sentences taken from Europarl (Koehn, 2005) and manually annotated with Propbank verb predicates (van der Plas et al., 2010b). This dataset is then used by van der Plas et al. (2011) to evaluate their approach to projecting the SRLs of English sentences to their translations This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http: //creativecommons.org/licenses/by/4.0/ in French. They additionally build a large, “artificial” or automatically labelled dataset of approximately 1M Europarl sentences by projecting the SRLs f</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Conference Proceedings: the tenth Machine Translation Summit, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Kup´s´c</author>
<author>Anne Abeill´e</author>
</authors>
<title>Growing treelex.</title>
<date>2008</date>
<booktitle>In Proceedings of the 9th International Conference on Computational Linguistics and Intelligent Text Processing, CICLing’08,</booktitle>
<pages>28--39</pages>
<marker>Kup´s´c, Abeill´e, 2008</marker>
<rawString>Anna Kup´s´c and Anne Abeill´e. 2008. Growing treelex. In Proceedings of the 9th International Conference on Computational Linguistics and Intelligent Text Processing, CICLing’08, pages 28–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alejandra Lorenzo</author>
<author>Christophe Cerisara</author>
</authors>
<title>Unsupervised frame based semantic role induction: application to french and english.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACL 2012 Joint Workshop on Statistical Parsing and Semantic Processing of Morphologically Rich Languages,</booktitle>
<pages>30--35</pages>
<contexts>
<context position="3859" citStr="Lorenzo and Cerisara (2012)" startWordPosition="588" endWordPosition="591">ench SRL performance whether we use universal partof-speech tags and syntactic dependencies or more fine-grained tags and dependencies. The surprising result that SRL performance levels off after just 5K training sentences leads us to directly compare the small hand-crafted set of 1K sentences to the larger artificial training set. We use 5-fold cross-validation on the small dataset and find that the SRL performance is substantially higher (&gt;10 F1 in identification and classification) when the hand-crafted annotations are used. 2 Related Work There has been relatively few works in French SRL. Lorenzo and Cerisara (2012) propose a clustering approach for verb predicate and argument labelling (but not identification). They choose 87 Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 87–92, Dublin, Ireland, August 23-24 2014. VerbNet style roles (Schuler, 2006) and manually annotate sentences with them for evaluation, achieving an F1 of 78.5. Gardent and Cerisara (2010) propose a method for semi-automatically annotating the French dependency treebank (Candito et al., 2010) with Propbank core roles (no adjuncts). They first manually augment TreeLex (Kup´s´c and Ab</context>
</contexts>
<marker>Lorenzo, Cerisara, 2012</marker>
<rawString>Alejandra Lorenzo and Christophe Cerisara. 2012. Unsupervised frame based semantic role induction: application to french and english. In Proceedings of the ACL 2012 Joint Workshop on Statistical Parsing and Semantic Processing of Morphologically Rich Languages, pages 30–35.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Yvonne QuirmbachBrundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar T¨ackstr¨om, Claudia Bedini, N´uria Bertomeu Castell´o, and Jungmee Lee.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>92--97</pages>
<marker>McDonald, Nivre, 2013</marker>
<rawString>Ryan McDonald, Joakim Nivre, Yvonne QuirmbachBrundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar T¨ackstr¨om, Claudia Bedini, N´uria Bertomeu Castell´o, and Jungmee Lee. 2013. Universal dependency annotation for multilingual parsing. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 92–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Maltparser: A data-driven parser-generator for dependency parsing. In</title>
<date>2006</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="13782" citStr="Nivre et al., 2006" startWordPosition="2227" endWordPosition="2230">sets ous domains. We now explore the utility of this new dependency scheme in SRL. The French universal dependency treebank comes in two versions, the first using the standard dependency structure based on basic Stanford dependencies (de Marneffe and Manning, 2008) where content words are the heads except in copula and adposition constructions, and the second which treats content words as the heads for all constructions without exemption. We use both schemes in order to verify their effect on SRL. In order to obtain universal dependencies for our data, we train parsing models with MaltParser (Nivre et al., 2006) using the entire uni-dep-tb.3 We then parse our data using these MaltParser models. The input POS tags to the parser are the universal POS tags used in OrgDep+UniPOS. We train and evaluate new SRL models on these data. The results are shown in the third and fourth rows of Table 1. StdUniDept+UniPOS is the setting using standard dependencies and CHUDep+UPOS using content-head dependencies. According to the third and fourth rows in Table 1, content-head dependencies are slightly less useful than standard dependencies. The general effect of universal dependencies can be compared to those of orig</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Maltparser: A data-driven parser-generator for dependency parsing. In In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="6785" citStr="Och and Ney, 2003" startWordPosition="1058" endWordPosition="1061">sets described in (van der Plas et al., 2011) and the delivery report of the Classic project (van der Plas et al., 2010a). These are the gold standard set of 1K sentences which was annotated by manually identifying each verb predicate, finding its equivalent English frameset in PropBank and identifying and labelling its arguments based on the description of the frameset (henceforth known as Classic1K), and the synthetic dataset consisting of more than 980K sentences (henceforth known as Classic980K), which was created by word aligning an English-French parallel corpus (Europarl) using GIZA++ (Och and Ney, 2003) and projecting the French SRLs from the English SRLs via the word alignments. The joint syntactic-semantic parser described in (Titov et al., 2009) was used to produce the English SRLs and the dependency parses of the French side were produced using the ISBN parser described in (Titov and Henderson, 2007). We use LTH (Bj¨orkelund et al., 2009), a dependency-based SRL system, in all of our experiments. This system was among the bestperforming systems in the CoNLL 2009 shared task (Hajiˇc et al., 2009) and is straightforward to use. It comes with a set of features tuned for each shared task lan</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Crosslingual annotation projection of semantic roles.</title>
<date>2009</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>36</volume>
<issue>1</issue>
<marker>Pad´o, Lapata, 2009</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2009. Crosslingual annotation projection of semantic roles. J. Artif. Int. Res., 36(1):307–340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="1266" citStr="Palmer et al., 2005" startWordPosition="178" endWordPosition="181">ence over the original fine-grained tagset and dependency scheme. Moreover, there seems to be no improvement gained from projecting semantic roles between direct translations than between indirect translations. 1 Introduction Semantic role labelling (SRL) (Gildea and Jurafsky, 2002) is the task of identifying the predicates in a sentence, their semantic arguments and the roles these arguments take. The last decade has seen considerable attention paid to statistical SRL, thanks to the existence of two major hand-crafted resources for English, namely, FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Apart from English, only a few languages have SRL resources and these resources tend to be of limited size compared to the English datasets. French is one of those languages which suffer from a scarcity of hand-crafted SRL resources. The only available gold-standard resource is a small set of 1000 sentences taken from Europarl (Koehn, 2005) and manually annotated with Propbank verb predicates (van der Plas et al., 2010b). This dataset is then used by van der Plas et al. (2011) to evaluate their approach to projecting the SRLs of English sentences to their translations This work is licensed u</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2012</date>
<booktitle>In Proceedings of LREC,</booktitle>
<contexts>
<context position="11360" citStr="Petrov et al. (2012)" startWordPosition="1812" endWordPosition="1815"> structure. This inspires us to compare the impact of different types of syntactic annotations on the performance of this system. Based on the observations from the previous sections, we choose two different sizes of training sets. The first set contains the first 5K sentences from the original 100K, as we saw that more than this amount tends to diminish performance. The second set contains the first 50K from the original 100K, the purpose of which is to check if changing the parses affects the usefulness of adding more data. We will call these data sets ClassicSK and ClassicS0K respectively. Petrov et al. (2012) create a set of 12 universal part-of-speech (POS) tags which should in theory be applicable to any natural language. It is interesting to know whether these POS tags are more useful for SRL than the original set of the 29 more fine-grained POS tags used in French Treebank which we have used so far. To this end, we convert the original POS tags of the data to universal POS tags and retrain and evaluate the SRL models. The results are given in the second row of Table 1 (OrgDep+UniPOS). The first row of the table (Original) shows the performance using the original annotation. Even though the sco</context>
</contexts>
<marker>Petrov, Das, McDonald, 2012</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of LREC, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper Schuler</author>
</authors>
<title>VerbNet: A BroadCoverage, Comprehensive Verb Lexicon.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="4151" citStr="Schuler, 2006" startWordPosition="632" endWordPosition="633">ger artificial training set. We use 5-fold cross-validation on the small dataset and find that the SRL performance is substantially higher (&gt;10 F1 in identification and classification) when the hand-crafted annotations are used. 2 Related Work There has been relatively few works in French SRL. Lorenzo and Cerisara (2012) propose a clustering approach for verb predicate and argument labelling (but not identification). They choose 87 Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 87–92, Dublin, Ireland, August 23-24 2014. VerbNet style roles (Schuler, 2006) and manually annotate sentences with them for evaluation, achieving an F1 of 78.5. Gardent and Cerisara (2010) propose a method for semi-automatically annotating the French dependency treebank (Candito et al., 2010) with Propbank core roles (no adjuncts). They first manually augment TreeLex (Kup´s´c and Abeill´e, 2008), a syntactic lexicon of French, with semantic roles of syntactic arguments of verbs (i.e. verb subcategorization). They then project this annotation to verb instances in the dependency trees. They evaluate their approach by performing error analysis on a small sample and sugges</context>
</contexts>
<marker>Schuler, 2006</marker>
<rawString>Karin Kipper Schuler. 2006. VerbNet: A BroadCoverage, Comprehensive Verb Lexicon. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
</authors>
<title>A latent variable model for generative dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the 10th International Conference on Parsing Technologies,</booktitle>
<pages>144--155</pages>
<contexts>
<context position="7092" citStr="Titov and Henderson, 2007" startWordPosition="1109" endWordPosition="1112">ying and labelling its arguments based on the description of the frameset (henceforth known as Classic1K), and the synthetic dataset consisting of more than 980K sentences (henceforth known as Classic980K), which was created by word aligning an English-French parallel corpus (Europarl) using GIZA++ (Och and Ney, 2003) and projecting the French SRLs from the English SRLs via the word alignments. The joint syntactic-semantic parser described in (Titov et al., 2009) was used to produce the English SRLs and the dependency parses of the French side were produced using the ISBN parser described in (Titov and Henderson, 2007). We use LTH (Bj¨orkelund et al., 2009), a dependency-based SRL system, in all of our experiments. This system was among the bestperforming systems in the CoNLL 2009 shared task (Hajiˇc et al., 2009) and is straightforward to use. It comes with a set of features tuned for each shared task language (English, German, Japanese, Spanish, Catalan, Czech, Chinese). We compared the performance of the English and Spanish feature sets on French and chose the former due to its higher performance (by 1 F1 point). To evaluate SRL performance, we use the CoNLL 2009 shared task scoring script1, which assume</context>
</contexts>
<marker>Titov, Henderson, 2007</marker>
<rawString>Ivan Titov and James Henderson. 2007. A latent variable model for generative dependency parsing. In Proceedings of the 10th International Conference on Parsing Technologies, pages 144–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
<author>Paola Merlo</author>
<author>Gabriele Musillo</author>
</authors>
<title>Online projectivisation for synchronous parsing of semantic and syntactic dependencies. In</title>
<date>2009</date>
<booktitle>In Proceedings of the Internation Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<pages>1562--1567</pages>
<contexts>
<context position="6933" citStr="Titov et al., 2009" startWordPosition="1081" endWordPosition="1084">ard set of 1K sentences which was annotated by manually identifying each verb predicate, finding its equivalent English frameset in PropBank and identifying and labelling its arguments based on the description of the frameset (henceforth known as Classic1K), and the synthetic dataset consisting of more than 980K sentences (henceforth known as Classic980K), which was created by word aligning an English-French parallel corpus (Europarl) using GIZA++ (Och and Ney, 2003) and projecting the French SRLs from the English SRLs via the word alignments. The joint syntactic-semantic parser described in (Titov et al., 2009) was used to produce the English SRLs and the dependency parses of the French side were produced using the ISBN parser described in (Titov and Henderson, 2007). We use LTH (Bj¨orkelund et al., 2009), a dependency-based SRL system, in all of our experiments. This system was among the bestperforming systems in the CoNLL 2009 shared task (Hajiˇc et al., 2009) and is straightforward to use. It comes with a set of features tuned for each shared task language (English, German, Japanese, Spanish, Catalan, Czech, Chinese). We compared the performance of the English and Spanish feature sets on French a</context>
</contexts>
<marker>Titov, Henderson, Merlo, Musillo, 2009</marker>
<rawString>Ivan Titov, James Henderson, Paola Merlo, and Gabriele Musillo. 2009. Online projectivisation for synchronous parsing of semantic and syntactic dependencies. In In Proceedings of the Internation Joint Conference on Artificial Intelligence (IJCAI), pages 1562–1567.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lonneke van der Plas</author>
<author>James Henderson</author>
<author>Paola Merlo</author>
</authors>
<title>2: Semantic role annotation of a french-english corpus.</title>
<date>2010</date>
<marker>van der Plas, Henderson, Merlo, 2010</marker>
<rawString>Lonneke van der Plas, James Henderson, and Paola Merlo. 2010a. D6. 2: Semantic role annotation of a french-english corpus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lonneke van der Plas</author>
<author>Tanja Samardˇzi´c</author>
<author>Paola Merlo</author>
</authors>
<title>Cross-lingual validity of propbank in the manual annotation of french.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourth Linguistic Annotation Workshop, LAW IV ’10,</booktitle>
<pages>113--117</pages>
<marker>van der Plas, Samardˇzi´c, Merlo, 2010</marker>
<rawString>Lonneke van der Plas, Tanja Samardˇzi´c, and Paola Merlo. 2010b. Cross-lingual validity of propbank in the manual annotation of french. In Proceedings of the Fourth Linguistic Annotation Workshop, LAW IV ’10, pages 113–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lonneke van der Plas</author>
<author>Paola Merlo</author>
<author>James Henderson</author>
</authors>
<title>Scaling up automatic cross-lingual semantic role annotation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>299--304</pages>
<marker>van der Plas, Merlo, Henderson, 2011</marker>
<rawString>Lonneke van der Plas, Paola Merlo, and James Henderson. 2011. Scaling up automatic cross-lingual semantic role annotation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 299–304.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>