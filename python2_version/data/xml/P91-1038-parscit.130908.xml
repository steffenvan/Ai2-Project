<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000059">
<title confidence="0.979040666666667">
A Preference-first Language Processor
Integrating the Unification Grammar and Markov Language Model
for Speech Recognition Applications
</title>
<author confidence="0.998435">
Lee-Feng Chien**, K. J. Chen** and Lin-Shan Lee*
</author>
<affiliation confidence="0.916234666666667">
* Dept. of Computer Science and Information Engineering,
National Taiwan University,Taipei, Taiwan, Rep. of China, Tel: (02) 362-2444.
** The Institute of Information Science, Academia Sinica, Taipei, Taiwan, Rep. of Chma.
</affiliation>
<sectionHeader confidence="0.923484" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999272333333333">
A language processor is to find out a most promising
sentence hypothesis for a given word lattice obtained
from acoustic signal recognition. In this paper a new
language processor is proposed, in which unification
grammar and Markov language model are integrated in a
word lattice parsing algorithm based on an augmented
chart, and the island-driven parsing concept is combined
with various preference-first parsing strategies defined by
different construction principles and decision rules. Test
results&apos;show that significant improvements in both
correct rate of recognition and computation speed can be
achieved.
</bodyText>
<sectionHeader confidence="0.999014" genericHeader="keywords">
1. Introduction
</sectionHeader>
<bodyText confidence="0.993565863636364">
In many speech recognition applications, a word
lattice is a partially ordered set of possible word
hypotheses obtained from an acoustic signal
processor. The purpose of a language processor is
then, for an input word lattice, to find the most
promising word sequence or sentence hypothesis as
the output (Hayes, 1986; Tomita, 1986;
O&apos;Shaughnessy, 1989). Conventionally either
grammatical or statistical approaches were used in
such language processors. However, the high degree
of ambiguity and large number of noisy word
hypotheses in the word lattices usually make the
search space huge and correct identification of the
output sentence hypothesis difficult, and the
capabilities of a language processor based on either
grammatical or statistical approaches alone were very
often limited. Because the features of these two
approaches are basically complementary, Derouault
and Merialdo (Derouault, 1986) first proposed a
unified model to combine them. But in this model
these two approaches were applied primarily
separately, selecting the output sentence hypothesis
based on the product of two probabilities
independently obtained from these two approaches.
In this paper a new language processor based on a
recently proposed augmented chart parsing algorithm
(Chien, 1990a) is presented, in which the
grammatical approach of unification grammar
(Sheiber, 1986) and the statistical approach of
Markov language model (Jelinek, 1976) are properly
integrated in a preference-first word lattice parsing
algorithm. The augmented chart (Chien, 1990b) was
extended from the conventional chart. It can represent
a very complicated word lattice, so that the difficult
word lattice parsing problem can be reduced to
essentially a well-known chart parsing problem.
Unification grammars, compared with other
grammatical approaches, are more declarative and can
better integrate syntactic and semantic information to
eliminate illegal combinations; while Markov
language models are in general both effective and
simple. The new language processor proposed in this
paper actually integrates the unification grammar and
the Markov language model by a new preference-first
parsing algorithm with various preference-first
parsing strategies defined by different constituent
construction principles and decision rules, such that
the constituent selection and search directions in the
parsing process can be more appropriately determined
by Markovian probabilities, thus rejecting most
noisy word hypotheses and significantly reducing the
search space. Therefore the global structural synthesis
capabilities of the unification grammar and the local
relation estimation capabilities of the Markov
language model are properly integrated. This makes
the present language processor not sensitive at all to
the increased number of noisy word hypotheses in a
very large vocabulary environment. An experimental
system for Mandarin speech recognition has been
implemented (Lee, 1990) and tested, in which a very
high correct rate of recognition (93.8%) was obtained
at a very high processing speed (about 5 sec per
sentence on an IBM PC/AT). This indicates
significant improvements as compared to previously
proposed models. The details of this new language
processor will be presented in the following sections.
</bodyText>
<page confidence="0.999035">
293
</page>
<sectionHeader confidence="0.995239" genericHeader="introduction">
2. The Proposed
</sectionHeader>
<subsectionHeader confidence="0.981789">
Language Processor
</subsectionHeader>
<bodyText confidence="0.999929722222222">
The language processor proposed in this paper
is shown in Fig. 1, where an acoustic signal
preprocessor is included to form a complete speech
recognition system. The language processor consists
of a language model and a parser. The language model
properly integrates the unification grammar and the
Markov language model, while the parser is defined
based on the augmented chart and the preference-first
parsing algorithm. The input speech signal is first
processed by the acoustic signal preprocessor; the
corresponding word lattice will thus be generated and
constructed onto the augmented chart The parser will
then proceed to build possible constituents from the
word lattice on the augmented chart in accordance
with the language model and the preference-first
parsing algorithm. Below, except the preference-first
parsing algorithm presented in detail in the next
section, all of other elements are briefly summarized.
</bodyText>
<subsectionHeader confidence="0.980666">
The Languagc_Model
</subsectionHeader>
<bodyText confidence="0.99914568">
The goal of the language model is to participate
in the selection of candidate constituents for a
sentence to be identified. The proposed language
model is composed of a PATR-II-like unification
grammar (Sheiber, 1986; Chien, 1990a) and a
first-order Markov language model (Jelinek, 1976) and
thus, combines many features of the grammatical and
statistical language modeling approaches. The
PATR-II-like unification grammar is used primarily
to distinguish between well-formed, acceptable word
sequences against ill-formed ones, and then to
represent the structural phrases and categories, or to
fmd the intended meaning depending on different
applications. The first-order Markov language model,
on the other hand, is used to guide the parser toward
correct search directions, such that many noisy word
hypotheses can be rejected and many unnecessary
constituents can be avoided, and the most promising
sentence hypothesis can thus be easily found. In this
way the weakness in either the PATR-II-like
unification grammar (Sheiber, 1986), e.g., the heavy
reliance on rigid linguistic information, or the
first-order Markov language model (Jelinek, 1976),
e.g., the need for a large training corpus and the local
prediction scope can also be effectively remedied.
</bodyText>
<sectionHeader confidence="0.714577" genericHeader="method">
IhcAugnunimiLhatiand
ihrilordlathoTarsingSacnic
</sectionHeader>
<subsectionHeader confidence="0.694084">
Chart is an efficient and widely used working
</subsectionHeader>
<bodyText confidence="0.999977041666667">
structure in many natural language processing
systems (Kay, 1980; Thompson, 1984), but it is
basically designed to parse a sequence of fixed and
known words instead of an ambiguous word lattice.
The concept of the augmented chart has recently been
successfully developed such that it can be used to
represent and parse a word lattice (Chien, 1990b).
Any given input word lattice for parsing can be
represented by the augmented chart through a
mapping procedure, in which a minimum number of
vertices are used to indicate the end points for all word
hypotheses in the lattice, and an inactive edge is used
to represent every word hypotheses. Also, specially
designed jump edges are constructed to link some
edges whose corresponding word hypotheses can
possibly be connected but themselves are physically
separated in the chart. In this way the basic operation
of a chart parser can thus be properly performed on a
word lattice. The difference is that two separated edges
linked by a jump edge can also be combined as long
as the required condition is satisfied. Note that in such
a scheme, every constituents (edge) will be
constructed only once, regardless of the fact that it
may be shared by many different sentence hypotheses.
</bodyText>
<figure confidence="0.6392545">
A speech recognition system
The
sente
ost promising
cc hypothesis
Acoustic signal
preprocessor
Speech-input
</figure>
<figureCaption confidence="0.762359">
Fig. 1 An abstract diagram of the proposed language processor.
</figureCaption>
<page confidence="0.872326">
294
</page>
<figure confidence="0.53999">
Example Construction principles:
</figure>
<figureCaption confidence="0.5967566">
faa=11.111:inCi011: at any time randomly select a candidate constituent to be consuucted
probahiliZleleCtitalMbellge: at any time the candidate constituent with the highest
probability will be constructed first
length_seleainalltliattle: at any time the candidate constituent with the largest numt
component word hypotheses will be constructed first
</figureCaption>
<bodyText confidence="0.992208666666667">
jengildorobabilitv selection minciole: at any time the candidate constituent with the
highest probability among those with the largest number of component word
hypotheses will be constructed lust
</bodyText>
<subsectionHeader confidence="0.892144">
Example Decision rules:
</subsectionHeader>
<bodyText confidence="0.9612095">
highellialthlatiMIXIC: after all grammatical sentence constituents have been found,
one with the highest probability is taken as the result
run. I rule: the fust grammatical sentence constituent obtained during the course of
parsing is taken as the result
firn-k rule: the sentence constituent with the highest probability among the first k
constructed grammatical sentence constituents obtained during the course of parsii:,
is taken as the result
The performance of these various construction
principles and decision rules will be discussed in
Sections 5 and 6 based on experimental results.
</bodyText>
<sectionHeader confidence="0.768253" genericHeader="method">
4. The Experimental System
</sectionHeader>
<bodyText confidence="0.982618911764706">
An experimental system based on the proposed
language processor has been developed and tested on a
small lexicon, a Markov language model, and a simple
set of unification grammar rules for the Chinese
language, although the present model is in fact
language independent. The system is written in C
language and performed on an IBM PC/AT.
The lexicon used has a total of 1550 words. They
are extracted from the primary school Chinese text
books currently used in Taiwan area, which arta
believed to cover the most frequently used words and
most of the syntactic and semantic structures in the
everyday Chinese sentences. Each word stored in the
lexicon (word entry) contains such information as the.
word name, the pronunciations (the phonemes), the
lexical categories and the corresponding feature
structures. Information contained in each word entry is
relatively simple except for the verb words, because
verbs have complicated behavior and will play a central
role in syntactic analysis. The unification grammar
constructed includes about 60 rules. It is believed that
these rules cover almost all of the sentences used in the
primary school Chinese text books. The Markov
language model is trained using the primary school
Chinese text books as training corpus. Since there are
no boundary markers between adjacent words in written
Chinese sentences, each sentence in the corpus was
first segmented into a corresponding word string before
used in the model training. Moreover, the test data
include 200 sentences randomly selected from 20
articles taken from several different magazines,
newspapers and books published in Taiwan area. All
295 the words used in the test sentences are included in the
lexicon.
</bodyText>
<figure confidence="0.4613945">
3. The Preference-first Parsing
Algorithm
</figure>
<bodyText confidence="0.987023263157895">
The preference-first parsing algorithm is
developed based on the augmented chart summarized
above, so that the difficult word lattice parsing
problem is reduced to essentially a well-known chart
parsing problem. This parsing algorithm is a general
algorithm, in which various preference-first parsing
strategies defined by different construction principles
and decision rules can be combined with the
island-driven parsing concept, so that the constituent
selection and search directions can be appropriately
determined by Markovian probabilities, thus rejecting
many noisy word hypotheses and significantly
reducing the search space. In this way, not only can
the features of the grammatical and statistical
approaches be combined, but the effects of the two
different approaches are reflected and integrated in a
single algorithm such that overall performance can be
appropriately optimized. Below, more details about
the algorithm will be given.
Probability Estimation
for Constructed Constituents
In order to make the unification-based parsing
algorithm also capable of handling the Markov
language model, every constructed constituent has to
be assigned a probability. In general, for each given
constituent C a probability P(C) = P(We) is assigned,
where We is the component word hypothesis sequence
of C and P(We) can be evaluated from the Markov
language model. Now, when_ an active constituent A
and an inactive constituent I form a new constituent
N, the probability P(N) can be evaluated from
probabilities P(A) and P(I). Let Wn, Wa, Wi be the
component word hypothesis sequences of N, A, and I
respectively. Without loss of generality, assume A is
to the left of I, thereby Wn = WaWi =
wal,-- -wam where wak is the k-th word
hypothesis of Wa and wik the k-th word hypothesis
of Wi. Then,
</bodyText>
<equation confidence="0.8823856">
P(Wn) = P(VVaWi)
=P(wa 1) * TC P(waklwak- 1) * P(wi 1 lwarn) *It P(wilciwik-1)
25tn 25-1c.n
P(Wa)*NWir (Kwillviam)IP (wi 1) ) â€¢
This can be easily evaluated in each parsing step.
</equation>
<table confidence="0.3294545">
The Preference-first Construction
Principles and Decision Rules
</table>
<bodyText confidence="0.992710636363636">
Since P(C) is assigned to every constituent C in
the augmented chart, various parsing strategies can be
developed for the preference-first parsing algorithm for
different applications. For example, there can be
various construction principles to determine the order
of constituent construction for all possible candidate
constituents. There can also be various decision rules
to choose the output sentence among all of the
constructed sentence constituents. Some examples for
such construction principles and decision rules are
listed in the following.
</bodyText>
<sectionHeader confidence="0.490036666666667" genericHeader="method">
5. Test Results (I) -- Initial
Preference-first Parsing
Strategies
</sectionHeader>
<bodyText confidence="0.999266416666667">
The present preference-first language
processor is a general model on which different
parsing strategies defined by different construction
principles and decision rules can be implemented. In
this and the next sections, several attractive parsing
strategies are proposed, tested and discussed under the
test conditions presented above. Two initial tests,
test I and II, were first performed to be used as the
baseline for comparison in the following. In test I,
the conventional unification-based grammatical
analysis alone is used, in which all the sentence
hypotheses obtained from the word lattice were
parsed exhaustively and a grammatical sentence
constituent was selected randomly as the result;
while in test II the first-order Markov modeling
approach alone is used, and a sentence hypothesis
with the highest probability was selected as the
result regardless of the grammatical structure. The
correct rate of recognition is defined as the averaged
percentage of the correct words in the output
sentences. The correct rate of recognition and the
approximated average time required are found to be
73.8% and 25 sec for Test I , as well as 82.2% and
3 sec for Test II, as indicated in the first two rows of
Table 1. In all the following parsing strategies, both
the unification grammar and the Markov language
model will be integrated in the language model to
obtain better results.
The parsing strategy 1 uses the random
selection principle and the highest probability rule (
as listed in Section 3), and the entire word lattice
will be parsed exhaustively. The total number of
constituents constructed during the course of parsing
for each test sentence are also recorded. The results
show that the correct rate of recognition can be as
high as 98.3%. This indicates that the language
processor based on the integration of the unification
grammar and the Markov language model can in fact
be very reliable. That is, most of the interferences
due to the noisy word hypotheses are actually
rejected by such an integration. However, the
computation load required for such an exhaustive
parsing strategy turns out to be very high (similar to
that in Test I), i.e., for each test sentence in average
305.9 constituents have to be constructed and it
takes about 25 sec to process a sentence on the IBM
PC/AT. Such computation requirements will make
this strategy practically difficult for many
applications. All these test data together with the
results for the other three parsing strategies 2-4 are
listed in Table 1 for comparison.
The basic concept of parsing strategy 2
(using the probability selection principle and the
first-1 rule, as listed in Section 3 ) is to use the
probabilities of the constituents to select the search
direction such that significant reduction in
computation requirements can be achieved. The test
results (in the fourth row of Table 1) show that with
this strategy for each test sentence in average only
152.4 constituents are constructed and it takes only
about 12 sec to process a sentence on the PC/AT,
and the high correct rate of recognition of parsing
strategy 1 is almost preserved, i.e., 96.0%. Therefore
this strategy represents a very good tradeoff, i.e., the
computation requirements are reduced by a factor of
0.50 ( the constituent reduction ratio in the last
second column of Table 1 is the ration of the average
number of built constituents to that of Strategy 1),
while the correct rate is only degraded by 2.3%.
However, such a speed (12 sac for a sentence) is still
very low especially if real-time operation is
considered.
</bodyText>
<subsectionHeader confidence="0.852226333333333">
6. Test Results (II) --
Improved Best-first Parsing
Strategies
</subsectionHeader>
<bodyText confidence="0.999922178571429">
In a further analysis all of the constituents
constructed by parsing strategy 1 were first divided
into two classes: correct constituents and noisy
constituents. A correct constituent is a constituent
without any component noisy word hypothesis;
while a noisy constituent is a constituent which is
not correct. These two classes of constituents were
then categorized according to their length (number of
word hypotheses in the constituents). The average
probability values for each category of correct and
noisy constituents were then evaluated. The results
are plotted in Fig. 2, where the vertical axis shows
the average probability values and the horizontal axis
denotes the length of the constituent. Some
observations can be made as in the following. First,
it can be seen that the two curves in Fig. 2 apparently
diverge, especially for longer constituents, which
implies that the Markovian probabilities can
effectively discriminate the noisy constituents against
the correct constituents (note that all of those
constituents are grammatical), especially for longer
constituents. This is exactly why parsing strategr
and 2 can provide very high correct rams.
Furthermore, Fig. 2 also shows that in genervl
the probabilities for shorter constituents wo-A.ad
usually be much higher than those for longer
constituents. This means with parsing strategy 2
almost all short constituents, no matter noisy or
</bodyText>
<page confidence="0.995262">
296
</page>
<bodyText confidence="0.99993370967742">
correct, would be constructed first, and only those
long noisy constituents with lower probability values
can be rejected by the parsing strategy 2. This thus
leads to the parsing strategies 3 and 4 discussed
below.
In parsing strategy 3 (using the
length/probability selection principle and First-1 rule,
as listed in Section 3), the length of a constituent is
considered first, because it is found that the correct
constituents have much better chance to be obtained
very quickly by means of the Markovian probabilities
for longer constituents than shorter correct
constituents, as discussed in the above. In this way,
the construction of the desired constituents would be
much more faster and very significant reduction in
computation requirements can be achieved. The test
results in the fifth row of Table 1 show that with this
strategy in average only 70.2 constituents were
constructed for a sentence, a constituent reduction
ratio of 0.27 is found, and it takes only about 4 sec to
process a sentence on PC/AT, which is now very
close to real-time. However, the correct rate of
recognition is seriously degraded to as low as 85.8%,
apparently because some correct constituents have
been missed due to the high speed construction
principle. Fortunately, after a series of experiments, it
was found that in this case the correct sentences very
often appeared as the second or the third constructed
sentences, if not the first. Therefore, the parsing
strategy 4 is proposed below, in which everything is
the same as parsing strategy 3 except that the first-1
decision rule is replaced by the first-3 decision rule. In
other words, those missed correct constituents can
very possibly be picked up in the next few steps, if
the final decision can be slightly delayed.
The test results for parsing strategy 4 listed in
the sixth row of Table 1 show that with this strategy
the correct rate of recognition has been improved to
93.8% and the computation complexity is still close
to that of parsing strategy 3, i.e., the average number
of constructed constituents for a sentence is 91.0, it
takes about 5 sec to process a sentence, and a
constituent reduction ratio of 0.29 is achieved. This is
apparently a very attractive approach considering both
the accuracy and the computation complexity. In
fact, with the parsing strategy 4, only those noisy
word hypotheses which both have relatively high
probabilities and can be unified with their
neighboring word hypotheses can cause interferences.
This is why the noisy word hypothesis interferences
can be reduced, and the present approach is therefore
not sensitive at all to the increased number of noisy
word hypotheses in a very large vocabulary
environment. Note that although intuitively the
integration of grammatical and statistical approaches
would imply more computation requirements, but
here in fact the preference-first algorithm provides
correct directions of search such that many noisy
constituents are simply rejected and the reduction of
the computation complexity makes such an
integration also very attractive in terms of
computation requirements.
</bodyText>
<sectionHeader confidence="0.873251" genericHeader="method">
7. Concluding Remarks
</sectionHeader>
<bodyText confidence="0.997769">
In this paper, we have proposed an efficient
language processor for speech recognition
applications, in which the unification grammar and
the Markov language model are properly integrated in
</bodyText>
<table confidence="0.938694052631579">
construction decision Correct rates o. Number of Constituent Approximated eve
principles rules recognition built constituent: reduction ratio.; rage time required
(Sec/Sentence)
Test I 73.8 % 305.9 1.00 25
(Unification gram
mar only)
Test II 82.2 % 3
(Markov language
model only)
parsing strategy 1 the random the highest 98.3 % 305.9 1.00 25
selection principle probability
parsing strategy 2 the probability First-1 96.0% 152.4 0.50 12
selection principle rule .
Parsing strategy 3 the length/pro- First-1 85.8 cfo 70.2 0.27 4
bability selection rule
principle
parsing strategy 4 the length/pro- First-3 93.8 % 91.0 0.29
bability selection rule 5
principle
</table>
<tableCaption confidence="0.996109">
Table 1 Test results for the two initial tests and four parsing strategies.
</tableCaption>
<page confidence="0.994226">
297
</page>
<bodyText confidence="0.9996536">
a preference-first parsing algorithm defined on an
augmented chart. Because the unification-based
analysis eliminates all illegal combinations and the
Markovian probabilities of constituents indicates the
correct direction of processing, a very high correct rate
of recognition can be obtained. Meanwhile, many
unnecessary computations can be effectively
eliminated and very high processing speed obtained
due to the significant reduction of the huge search
space. This preference-first language processor is
quite general, in which many different parsing
strategies defined by appropriately chosen
construction principles and decision rules can be
easily implemented for different speech recognition
applications.
</bodyText>
<sectionHeader confidence="0.997506" genericHeader="method">
References:
</sectionHeader>
<reference confidence="0.99488514">
Chien, L. F., Chen, K. J. and Lee, L. S. (1990b). An
Augmented Chart Parsing Algorithm Integrating
Unification Grammar and Markov Language Model
for Continuous Speech Recognition. Proceedings of
the IEEE 990 International Conference on Acoustics,
Speech and Signal Processing, Albuquerque, NM,
USA, Apr. 1990.
Chien, L. F., Chen, K. J. and Lee, L. S. (1990a). An
Augmented Chart Data Structure with Efficient Word
Lattice Parsing Scheme in Speech Recognition
Applications. To appear on Speech Communication.,
also in Proceedings of the 13th International
Conference on Computational Linguistics, July
1990, pp. 60-65.
Derouault A. and Merialdo B. (1986). Natural
Language Modeling for Phoneme-to-Text
Transcription, IEEE Trans. on PAMI, Vol. PAMI-8,
pp. 742-749.
Computational Linguistics, University of Bonn, pp.
587-592.
Jelinek, F. (1976). Continuous Speech Recognition
by Statistical Methods, Proc. IEEE, Vol. 64(4), pp.
532-556, Apr. 1976.
Kay M. (1980). Algorithm Schemata and Data
Structures in Syntactic Processing. Xerox Report
CSL-80-12, pp. 35-70, Pala Alto.
Lee, L. S. et al. (1990). A Mandarin Dictation
Machine Based Upon A Hierarchical Recognition
Approach and Chinese Natural Language Analysis,
IEEE Trans. on Pattern Analysis and Machine
Intelligence, Vol. 12, No. 7. July 1990, pp. 695-704.
O&apos;Shaughnessy, D. (1989). Using Syntactic
Information to Improve Large Vocabulary Word
Recognition, ICASSP&apos;89 pp. 715-718.
Sheiber, S. M. (1986). An Introduction to
Unification-Based Approaches to Grammar.
University of Chicago Press, Chicago.
Thompson, H. and Ritchie, G. (1984). Implementing
Natural Language Parsers, in Artificial Intelligence,
Tools, Techniques, and Applications, O&apos;shea, T. and
Elsenstadt, M. (eds), Harper8zRow, Publishers, Inc.
Tomita. M. (1986). An Efficient Word Lattice
Parsing Algorithm for Continuous Speech
Recognition. Proceedings of the 1986 International
Conference on Acoustic, Speech and Signal
Processing, pp. 1569-1572.
Hayes, P. 3. et al. (1986). Parsing Spoken
Language:A Semantic Caseframe Approach.
Proceedings of the 11th International Conference on
Constituent length
</reference>
<figureCaption confidence="0.751626">
Fig. 2 The average probability values for the correct and noisy constituents with different
</figureCaption>
<bodyText confidence="0.954321">
lengths construCted by parsing strategy 1.
</bodyText>
<page confidence="0.934878">
298
</page>
<figure confidence="0.995942428571429">
Average
Probability
values
Correct constituents
-1
10
Noisy constituents
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.324467">
<title confidence="0.999024666666667">A Preference-first Language Processor Integrating the Unification Grammar and Markov Language Model for Speech Recognition Applications</title>
<author confidence="0.995936">Lee-Feng Chien</author>
<author confidence="0.995936">K J Chen</author>
<author confidence="0.995936">Lin-Shan Lee</author>
<affiliation confidence="0.961418">Dept. of Computer Science and Information Engineering,</affiliation>
<abstract confidence="0.925252266666667">National Taiwan University,Taipei, Taiwan, Rep. of China, Tel: (02) 362-2444. ** The Institute of Information Science, Academia Sinica, Taipei, Taiwan, Rep. of Chma. ABSTRACT A language processor is to find out a most promising sentence hypothesis for a given word lattice obtained from acoustic signal recognition. In this paper a new language processor is proposed, in which unification grammar and Markov language model are integrated in a word lattice parsing algorithm based on an augmented chart, and the island-driven parsing concept is combined with various preference-first parsing strategies defined by different construction principles and decision rules. Test results&apos;show that significant improvements in both correct rate of recognition and computation speed can be achieved.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L F Chien</author>
<author>K J Chen</author>
<author>L S Lee</author>
</authors>
<title>An Augmented Chart Parsing Algorithm Integrating Unification Grammar and Markov Language Model for Continuous Speech Recognition.</title>
<date>1990</date>
<booktitle>Proceedings of the IEEE 990 International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<location>Albuquerque, NM, USA,</location>
<marker>Chien, Chen, Lee, 1990</marker>
<rawString>Chien, L. F., Chen, K. J. and Lee, L. S. (1990b). An Augmented Chart Parsing Algorithm Integrating Unification Grammar and Markov Language Model for Continuous Speech Recognition. Proceedings of the IEEE 990 International Conference on Acoustics, Speech and Signal Processing, Albuquerque, NM, USA, Apr. 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L F Chien</author>
<author>K J Chen</author>
<author>L S Lee</author>
</authors>
<title>An Augmented Chart Data Structure with Efficient Word Lattice Parsing Scheme in Speech Recognition Applications. To appear on Speech Communication., also</title>
<date>1990</date>
<booktitle>in Proceedings of the 13th International Conference on Computational Linguistics,</booktitle>
<pages>60--65</pages>
<marker>Chien, Chen, Lee, 1990</marker>
<rawString>Chien, L. F., Chen, K. J. and Lee, L. S. (1990a). An Augmented Chart Data Structure with Efficient Word Lattice Parsing Scheme in Speech Recognition Applications. To appear on Speech Communication., also in Proceedings of the 13th International Conference on Computational Linguistics, July 1990, pp. 60-65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Derouault</author>
<author>B Merialdo</author>
</authors>
<title>Natural Language Modeling for Phoneme-to-Text Transcription,</title>
<date>1986</date>
<journal>IEEE Trans. on PAMI,</journal>
<volume>8</volume>
<pages>742--749</pages>
<marker>Derouault, Merialdo, 1986</marker>
<rawString>Derouault A. and Merialdo B. (1986). Natural Language Modeling for Phoneme-to-Text Transcription, IEEE Trans. on PAMI, Vol. PAMI-8, pp. 742-749.</rawString>
</citation>
<citation valid="false">
<pages>587--592</pages>
<institution>Computational Linguistics, University of Bonn,</institution>
<marker></marker>
<rawString>Computational Linguistics, University of Bonn, pp. 587-592.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Continuous Speech Recognition by Statistical Methods,</title>
<date>1976</date>
<booktitle>Proc. IEEE,</booktitle>
<volume>64</volume>
<issue>4</issue>
<pages>532--556</pages>
<contexts>
<context position="2483" citStr="Jelinek, 1976" startWordPosition="355" endWordPosition="356"> two approaches are basically complementary, Derouault and Merialdo (Derouault, 1986) first proposed a unified model to combine them. But in this model these two approaches were applied primarily separately, selecting the output sentence hypothesis based on the product of two probabilities independently obtained from these two approaches. In this paper a new language processor based on a recently proposed augmented chart parsing algorithm (Chien, 1990a) is presented, in which the grammatical approach of unification grammar (Sheiber, 1986) and the statistical approach of Markov language model (Jelinek, 1976) are properly integrated in a preference-first word lattice parsing algorithm. The augmented chart (Chien, 1990b) was extended from the conventional chart. It can represent a very complicated word lattice, so that the difficult word lattice parsing problem can be reduced to essentially a well-known chart parsing problem. Unification grammars, compared with other grammatical approaches, are more declarative and can better integrate syntactic and semantic information to eliminate illegal combinations; while Markov language models are in general both effective and simple. The new language process</context>
<context position="5609" citStr="Jelinek, 1976" startWordPosition="809" endWordPosition="810"> proceed to build possible constituents from the word lattice on the augmented chart in accordance with the language model and the preference-first parsing algorithm. Below, except the preference-first parsing algorithm presented in detail in the next section, all of other elements are briefly summarized. The Languagc_Model The goal of the language model is to participate in the selection of candidate constituents for a sentence to be identified. The proposed language model is composed of a PATR-II-like unification grammar (Sheiber, 1986; Chien, 1990a) and a first-order Markov language model (Jelinek, 1976) and thus, combines many features of the grammatical and statistical language modeling approaches. The PATR-II-like unification grammar is used primarily to distinguish between well-formed, acceptable word sequences against ill-formed ones, and then to represent the structural phrases and categories, or to fmd the intended meaning depending on different applications. The first-order Markov language model, on the other hand, is used to guide the parser toward correct search directions, such that many noisy word hypotheses can be rejected and many unnecessary constituents can be avoided, and the</context>
</contexts>
<marker>Jelinek, 1976</marker>
<rawString>Jelinek, F. (1976). Continuous Speech Recognition by Statistical Methods, Proc. IEEE, Vol. 64(4), pp. 532-556, Apr. 1976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>Algorithm Schemata and Data Structures in Syntactic Processing.</title>
<date>1980</date>
<tech>Xerox Report CSL-80-12,</tech>
<pages>35--70</pages>
<location>Pala Alto.</location>
<contexts>
<context position="6742" citStr="Kay, 1980" startWordPosition="970" endWordPosition="971">es can be rejected and many unnecessary constituents can be avoided, and the most promising sentence hypothesis can thus be easily found. In this way the weakness in either the PATR-II-like unification grammar (Sheiber, 1986), e.g., the heavy reliance on rigid linguistic information, or the first-order Markov language model (Jelinek, 1976), e.g., the need for a large training corpus and the local prediction scope can also be effectively remedied. IhcAugnunimiLhatiand ihrilordlathoTarsingSacnic Chart is an efficient and widely used working structure in many natural language processing systems (Kay, 1980; Thompson, 1984), but it is basically designed to parse a sequence of fixed and known words instead of an ambiguous word lattice. The concept of the augmented chart has recently been successfully developed such that it can be used to represent and parse a word lattice (Chien, 1990b). Any given input word lattice for parsing can be represented by the augmented chart through a mapping procedure, in which a minimum number of vertices are used to indicate the end points for all word hypotheses in the lattice, and an inactive edge is used to represent every word hypotheses. Also, specially designe</context>
</contexts>
<marker>Kay, 1980</marker>
<rawString>Kay M. (1980). Algorithm Schemata and Data Structures in Syntactic Processing. Xerox Report CSL-80-12, pp. 35-70, Pala Alto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Lee</author>
</authors>
<title>A Mandarin Dictation Machine Based Upon A Hierarchical Recognition Approach and Chinese Natural Language Analysis,</title>
<date>1990</date>
<journal>IEEE Trans. on Pattern Analysis and Machine Intelligence,</journal>
<volume>12</volume>
<pages>695--704</pages>
<contexts>
<context position="4008" citStr="Lee, 1990" startWordPosition="565" endWordPosition="566">h directions in the parsing process can be more appropriately determined by Markovian probabilities, thus rejecting most noisy word hypotheses and significantly reducing the search space. Therefore the global structural synthesis capabilities of the unification grammar and the local relation estimation capabilities of the Markov language model are properly integrated. This makes the present language processor not sensitive at all to the increased number of noisy word hypotheses in a very large vocabulary environment. An experimental system for Mandarin speech recognition has been implemented (Lee, 1990) and tested, in which a very high correct rate of recognition (93.8%) was obtained at a very high processing speed (about 5 sec per sentence on an IBM PC/AT). This indicates significant improvements as compared to previously proposed models. The details of this new language processor will be presented in the following sections. 293 2. The Proposed Language Processor The language processor proposed in this paper is shown in Fig. 1, where an acoustic signal preprocessor is included to form a complete speech recognition system. The language processor consists of a language model and a parser. The</context>
</contexts>
<marker>Lee, 1990</marker>
<rawString>Lee, L. S. et al. (1990). A Mandarin Dictation Machine Based Upon A Hierarchical Recognition Approach and Chinese Natural Language Analysis, IEEE Trans. on Pattern Analysis and Machine Intelligence, Vol. 12, No. 7. July 1990, pp. 695-704.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D O&apos;Shaughnessy</author>
</authors>
<title>Using Syntactic Information to Improve Large Vocabulary Word Recognition,</title>
<date>1989</date>
<pages>715--718</pages>
<contexts>
<context position="1401" citStr="O&apos;Shaughnessy, 1989" startWordPosition="200" endWordPosition="201">t is combined with various preference-first parsing strategies defined by different construction principles and decision rules. Test results&apos;show that significant improvements in both correct rate of recognition and computation speed can be achieved. 1. Introduction In many speech recognition applications, a word lattice is a partially ordered set of possible word hypotheses obtained from an acoustic signal processor. The purpose of a language processor is then, for an input word lattice, to find the most promising word sequence or sentence hypothesis as the output (Hayes, 1986; Tomita, 1986; O&apos;Shaughnessy, 1989). Conventionally either grammatical or statistical approaches were used in such language processors. However, the high degree of ambiguity and large number of noisy word hypotheses in the word lattices usually make the search space huge and correct identification of the output sentence hypothesis difficult, and the capabilities of a language processor based on either grammatical or statistical approaches alone were very often limited. Because the features of these two approaches are basically complementary, Derouault and Merialdo (Derouault, 1986) first proposed a unified model to combine them</context>
</contexts>
<marker>O&apos;Shaughnessy, 1989</marker>
<rawString>O&apos;Shaughnessy, D. (1989). Using Syntactic Information to Improve Large Vocabulary Word Recognition, ICASSP&apos;89 pp. 715-718.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Sheiber</author>
</authors>
<title>An Introduction to Unification-Based Approaches to Grammar.</title>
<date>1986</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="2413" citStr="Sheiber, 1986" startWordPosition="345" endWordPosition="346">pproaches alone were very often limited. Because the features of these two approaches are basically complementary, Derouault and Merialdo (Derouault, 1986) first proposed a unified model to combine them. But in this model these two approaches were applied primarily separately, selecting the output sentence hypothesis based on the product of two probabilities independently obtained from these two approaches. In this paper a new language processor based on a recently proposed augmented chart parsing algorithm (Chien, 1990a) is presented, in which the grammatical approach of unification grammar (Sheiber, 1986) and the statistical approach of Markov language model (Jelinek, 1976) are properly integrated in a preference-first word lattice parsing algorithm. The augmented chart (Chien, 1990b) was extended from the conventional chart. It can represent a very complicated word lattice, so that the difficult word lattice parsing problem can be reduced to essentially a well-known chart parsing problem. Unification grammars, compared with other grammatical approaches, are more declarative and can better integrate syntactic and semantic information to eliminate illegal combinations; while Markov language mod</context>
<context position="5538" citStr="Sheiber, 1986" startWordPosition="799" endWordPosition="800">enerated and constructed onto the augmented chart The parser will then proceed to build possible constituents from the word lattice on the augmented chart in accordance with the language model and the preference-first parsing algorithm. Below, except the preference-first parsing algorithm presented in detail in the next section, all of other elements are briefly summarized. The Languagc_Model The goal of the language model is to participate in the selection of candidate constituents for a sentence to be identified. The proposed language model is composed of a PATR-II-like unification grammar (Sheiber, 1986; Chien, 1990a) and a first-order Markov language model (Jelinek, 1976) and thus, combines many features of the grammatical and statistical language modeling approaches. The PATR-II-like unification grammar is used primarily to distinguish between well-formed, acceptable word sequences against ill-formed ones, and then to represent the structural phrases and categories, or to fmd the intended meaning depending on different applications. The first-order Markov language model, on the other hand, is used to guide the parser toward correct search directions, such that many noisy word hypotheses ca</context>
</contexts>
<marker>Sheiber, 1986</marker>
<rawString>Sheiber, S. M. (1986). An Introduction to Unification-Based Approaches to Grammar. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Thompson</author>
<author>G Ritchie</author>
</authors>
<title>Implementing Natural Language Parsers,</title>
<date>1984</date>
<booktitle>in Artificial Intelligence, Tools, Techniques, and</booktitle>
<publisher>Harper8zRow, Publishers, Inc.</publisher>
<marker>Thompson, Ritchie, 1984</marker>
<rawString>Thompson, H. and Ritchie, G. (1984). Implementing Natural Language Parsers, in Artificial Intelligence, Tools, Techniques, and Applications, O&apos;shea, T. and Elsenstadt, M. (eds), Harper8zRow, Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M</author>
</authors>
<title>An Efficient Word Lattice Parsing Algorithm for Continuous Speech Recognition.</title>
<date>1986</date>
<booktitle>Proceedings of the 1986 International Conference on Acoustic, Speech and Signal Processing,</booktitle>
<pages>1569--1572</pages>
<marker>M, 1986</marker>
<rawString>Tomita. M. (1986). An Efficient Word Lattice Parsing Algorithm for Continuous Speech Recognition. Proceedings of the 1986 International Conference on Acoustic, Speech and Signal Processing, pp. 1569-1572.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hayes</author>
</authors>
<title>Parsing Spoken Language:A Semantic Caseframe Approach.</title>
<date>1986</date>
<booktitle>Proceedings of the 11th International Conference on Constituent length</booktitle>
<contexts>
<context position="1365" citStr="Hayes, 1986" startWordPosition="196" endWordPosition="197">sland-driven parsing concept is combined with various preference-first parsing strategies defined by different construction principles and decision rules. Test results&apos;show that significant improvements in both correct rate of recognition and computation speed can be achieved. 1. Introduction In many speech recognition applications, a word lattice is a partially ordered set of possible word hypotheses obtained from an acoustic signal processor. The purpose of a language processor is then, for an input word lattice, to find the most promising word sequence or sentence hypothesis as the output (Hayes, 1986; Tomita, 1986; O&apos;Shaughnessy, 1989). Conventionally either grammatical or statistical approaches were used in such language processors. However, the high degree of ambiguity and large number of noisy word hypotheses in the word lattices usually make the search space huge and correct identification of the output sentence hypothesis difficult, and the capabilities of a language processor based on either grammatical or statistical approaches alone were very often limited. Because the features of these two approaches are basically complementary, Derouault and Merialdo (Derouault, 1986) first prop</context>
</contexts>
<marker>Hayes, 1986</marker>
<rawString>Hayes, P. 3. et al. (1986). Parsing Spoken Language:A Semantic Caseframe Approach. Proceedings of the 11th International Conference on Constituent length</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>