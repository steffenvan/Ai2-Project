<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.978333">
The CoNLL-2009 Shared Task:
Syntactic and Semantic Dependencies in Multiple Languages
</title>
<author confidence="0.989304333333333">
Jan Hajifc⋆ Massimiliano Ciaramita† Richard Johansson‡ Daisuke Kawahara⋄
Maria Ant`onia Marti⋆⋆ Lluis M`arquez⋆† Adam Meyers⋆‡ Joakim Nivre⋆⋄ Sebastian Pad´o⋄⋆
Jan fStfep´anek⋆ Pavel Strafn´ak⋆ Mihai Surdeanu‡⋆ Nianwen Xue‡‡ Yi Zhang‡⋄
</author>
<affiliation confidence="0.954578416666667">
⋆: Charles University in Prague, {hajic,stepanek,stranak}@ufal.mff.cuni.cz
t: Google Inc., massi@google.com
t: University of Trento, johansson@disi.unitn.it
o: National Institute of Information and Communications Technology, dk@nict.go.jp
⋆⋆: University of Barcelona, amarti@ub.edu
⋆t: Technical University of Catalonia, Barcelona, lluism@lsi.upc.edu
⋆t: New York University, meyers@cs.nyu.edu
⋆o: Uppsala University and V¨axj¨o University, joakim.nivre@lingfil.uu.se
o⋆: Stuttgart University, pado@ims.uni-stuttgart.de
t⋆: Stanford University, mihais@stanford.edu
tt: Brandeis University, xuen@brandeis.edu
to: Saarland University, yzhang@coli.uni-sb.de
</affiliation>
<sectionHeader confidence="0.982851" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999874125">
For the 11th straight year, the Conference
on Computational Natural Language Learn-
ing has been accompanied by a shared task
whose purpose is to promote natural language
processing applications and evaluate them in
a standard setting. In 2009, the shared task
was dedicated to the joint parsing of syntac-
tic and semantic dependencies in multiple lan-
guages. This shared task combines the shared
tasks of the previous five years under a unique
dependency-based formalism similar to the
2008 task. In this paper, we define the shared
task, describe how the data sets were created
and show their quantitative properties, report
the results and summarize the approaches of
the participating systems.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999507">
Every year since 1999, the Conference on Com-
putational Natural Language Learning (CoNLL)
launches a competitive, open “Shared Task”. A
common (“shared”) task is defined and datasets are
provided for its participants. In 2004 and 2005, the
shared tasks were dedicated to semantic role label-
ing (SRL) in a monolingual setting (English). In
</bodyText>
<page confidence="0.844931">
1
</page>
<bodyText confidence="0.99996894117647">
2006 and 2007 the shared tasks were devoted to
the parsing of syntactic dependencies, using corpora
from up to 13 languages. In 2008, the shared task
(Surdeanu et al., 2008) used a unified dependency-
based formalism, which modeled both syntactic de-
pendencies and semantic roles for English. The
CoNLL-2009 Shared Task has built on the 2008 re-
sults by providing data for six more languages (Cata-
lan, Chinese, Czech, German, Japanese and Span-
ish) in addition to the original English1. It has thus
naturally extended the path taken by the five most
recent CoNLL shared tasks.
As in 2008, the CoNLL-2009 shared task com-
bined dependency parsing and the task of identify-
ing and labeling semantic arguments of verbs (and
other parts of speech whenever available). Partici-
pants had to choose from two tasks:
</bodyText>
<listItem confidence="0.9509245">
• Joint task (syntactic dependency parsing and
semantic role labeling), or
• SRL-only task (syntactic dependency parses
have been provided by the organizers, using
state-of-the art parsers for the individual lan-
guages).
</listItem>
<footnote confidence="0.996012">
1There are some format changes and deviations from the
2008 task data specification; see Sect. 2.3
</footnote>
<note confidence="0.9943975">
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 1–18,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999549375">
In contrast to the previous year, the evaluation data
indicated which words were to be dealt with (for the
SRL task). In other words, (predicate) disambigua-
tion was still part of the task, whereas the identi-
fication of argument-bearing words was not. This
decision was made to compensate for the significant
differences between languages and between the an-
notation schemes used.
The “closed” and “open” challenges have been
kept from last year as well; participants could have
chosen one or both. In the closed challenge, systems
had to be trained strictly with information contained
in the given training corpus; in the open challenge,
systems could have been developed making use of
any kind of external tools and resources.
This paper is organized as follows. Section 2 de-
fines the task, including the format of the data, the
evaluation metrics, and the two challenges. A sub-
stantial portion of the paper (Section 3) is devoted
to the description of the conversion and develop-
ment of the data sets in the additional languages.
Section 4 shows the main results of the submitted
systems in the Joint and SRL-only tasks. Section 5
summarizes the approaches implemented by partic-
ipants. Section 6 concludes the paper. In all sec-
tions, we will mention some of the differences be-
tween last year’s and this year’s tasks while keeping
the text self-contained whenever possible; for details
and observations on the English data, please refer to
the overview paper of the CoNLL-2008 Shared Task
(Surdeanu et al., 2008) and to the references men-
tioned in the sections describing the other languages.
</bodyText>
<sectionHeader confidence="0.992634" genericHeader="introduction">
2 Task Definition
</sectionHeader>
<bodyText confidence="0.998376">
In this section we provide the definition of the shared
task; after introducing the two challenges and the
two tasks the participants were to choose, we con-
tinue with the format of the shared task data, fol-
lowed by a description of the evaluation metrics
used.
For three of the languages (Czech, English and
German), out-of-domain data (OOD) have also been
prepared for the final evaluation, following the same
guidelines and formats.
</bodyText>
<subsectionHeader confidence="0.954081">
2.1 Closed and Open Challenges
</subsectionHeader>
<bodyText confidence="0.999929666666667">
Similarly to the CoNLL-2005 and CoNLL-2008
shared tasks, this shared task evaluation is separated
into two challenges:
Closed Challenge The aim of this challenge was to
compare performance of the participating systems in
a fair environment. Systems had to be built strictly
with information contained in the given training cor-
pus, and tuned with the development section. In
addition, the lexical frame files (such as the Prop-
Bank and NomBank for English, the valency dictio-
nary PDT-Vallex for Czech etc.) were provided and
may have been used. These restrictions mean that
outside parsers (not trained by the participants’ sys-
tems) could not be used. However, we did provide
the output of a single, state-of-the-art dependency
parser for each language so that participants could
build a SRL-only system (using the provided parses
as inputs) within the closed challenge (as opposed to
the 2008 shared task).
Open Challenge Systems could have been devel-
oped making use of any kind of external tools and
resources. The only condition was that such tools or
resources must not have been developed with the an-
notations of the test set, both for the input and output
annotations of the data. In this challenge, we were
interested in learning methods which make use of
any tools or resources that might improve the per-
formance. The comparison of different systems in
this setting may not be fair, and thus ranking of sys-
tems is not necessarily important.
</bodyText>
<subsectionHeader confidence="0.992405">
2.2 Joint and SRL-only tasks
</subsectionHeader>
<bodyText confidence="0.999944923076923">
In 2008, systems participating in the open challenge
could have used state-of-the-art parsers for the syn-
tactic dependency part of the task. This year, we
have provided the output of these parsers for all the
languages in an uniform way, thus allowing an or-
thogonal combination of the two tasks and the two
challenges. For the SRL-only task, participants in
the closed challenge simply had to use the provided
parses only.
Despite the provisions for the SRL-only task, we
are more interested in the approaches and results of
the Joint task. Therefore, primary system ranking is
provided for the Joint task while additional measures
</bodyText>
<page confidence="0.988812">
2
</page>
<bodyText confidence="0.8651265">
are computed for various combinations of parsers
and SRL methods across the tasks and challenges.
</bodyText>
<subsectionHeader confidence="0.99405">
2.3 Data Format
</subsectionHeader>
<bodyText confidence="0.999975333333333">
The data format used in this shared task has been
based on the CoNLL-2008 shared task, with some
differences. The data follows these general rules:
</bodyText>
<listItem confidence="0.947475666666667">
• The files contain sentences separated by a blank
line.
• A sentence consists of one or more tokens and
the information for each token is represented on
a separate line.
• A token consists of at least 14 fields. The fields
are separated by one or more whitespace char-
acters (spaces or tabs). Whitespace characters
are not allowed within fields.
</listItem>
<bodyText confidence="0.999656739130435">
The data is thus a large table with whitespace-
separated fields (columns). The fields provided in
the data are described in Table 1. They are identical
for all languages, but they may differ in contents;
for example, some fields might not be filled for all
the languages provided (such as the FEAT or PFEAT
fields).
For the SRL-only task, participants have been
provided will all the data but the PRED and
APREDs, which they were supposed to fill in with
their correct values. However, they did not have
to determine which tokens are predicates (or more
precisely, which are the argument-bearing tokens),
since they were marked by ‘Y’ in the FILLPRED
field.
For the Joint task, participants could not (in ad-
dition to the PRED and APREDs) see the gold-
standard nor the predicted syntactic dependencies
(HEAD, PHEAD) and their labels (DEPREL, PDE-
PREL). These syntactic dependencies were also to
be filled by participants’ systems.
In both tasks, participants have been free to
use any other data (columns) provided, except the
LEMMA, POS and FEAT columns (to get more ‘re-
alistic’ results using only their automatically pre-
dicted variants PLEMMA, PPOS and PFEAT).
Besides the corpus proper, predicate dictionaries
have been provided to participants in order to be able
to properly match the predicates to the tokens in the
corpus; their contents could have been used e.g. as
features for the PRED/APREDs predictions (or even
for the syntactic dependencies, i.e., for filling in the
PHEAD and PDEPREL fields).
The system of filling-in the APREDs follows
the 2008 pattern; for each argument-bearing token
(predicate), a new APREDn column is created in the
order in which the predicate token is encountered
within the sentence (i.e., based on its ID seen as a
numerical value). Then, for each token in the sen-
tence, the value in the intersection of the APREDn
column and the token row is either left unfilled
(if the token is not an argument), or a predicate-
argument label(s) is(are) filled in.
The differences between the English-only 2008
task and this year’s multilingual task can be briefly
summarized as follows:
</bodyText>
<listItem confidence="0.9966659375">
• only “split”2 lemmas and forms have been pro-
vided in the English datasets (for the other lan-
guages, original tokenization from the respec-
tive treebanks has been used);
• rich morphological features have been added
wherever available;
• syntactic dependencies by state-of-the-art
parsers have been provided (for the SRL-only
task);
• multiple semantic labels for a single token have
been allowed (and properly evaluated) in the
APREDs columns;
• predicates have been pre-identified and marked
in both the training and test data;
• some of the fields (e.g. the APREDx) and val-
ues (ARG0 → A0 etc.) have been renamed.
</listItem>
<subsectionHeader confidence="0.994259">
2.4 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.999676833333333">
It was required that participants submit results in all
seven languages in the chosen task and in any of (or
both) the challenges. Submission of out-of-domain
data files has been optional.
The main evaluation measure, according to which
systems are primarily compared, is the Joint task,
</bodyText>
<footnote confidence="0.934637333333333">
2Splitting of forms and lemmas in English has been intro-
duced in the 2008 shared task to match the tokenization con-
vention for the arguments in NomBank.
</footnote>
<page confidence="0.996092">
3
</page>
<table confidence="0.9840351875">
Field # Name Description
1 ID Token counter, starting at 1 for each new sentence
2 FORM Form or punctuation symbol (the token; “split” for English)
3 LEMMA Gold-standard lemma of FORM
4 PLEMMA Automatically predicted lemma of FORM
5 POS Gold-standard POS (major POS only)
6 PPOS Automatically predicted major POS by a language-specific tagger
7 FEAT Gold-standard morphological features (if applicable)
8 PFEAT Automatically predicted morphological features (if applicable)
9 HEAD Gold-standard syntactic head of the current token (ID or 0 if root)
10 PHEAD Automatically predicted syntactic head
11 DEPREL Gold-standard syntactic dependency relation (to HEAD)
12 PDEPREL Automatically predicted dependency relation to PHEAD
13 FILLPRED Contains ‘Y’ for argument-bearing tokens
14 PRED (sense) identifier of a semantic “predicate” coming from a current token
15... APREDn Columns with argument labels for each semantic predicate (in the ID order)
</table>
<tableCaption confidence="0.92933">
Table 1: Description of the fields (columns) in the data provided. The values of columns 9, 11 and 14 and above are
not provided in the evaluation data; for the Joint task, columns 9–12 are also empty in the evaluation data.
</tableCaption>
<bodyText confidence="0.751250333333333">
closed challenge, Macro F1 score. However, scores
can also be computed for a number of other condi-
tions:
</bodyText>
<listItem confidence="0.98392175">
• Task: Joint or SRL-only
• Challenge: open or closed
• Domain: in-domain data (IDD, separated from
training corpus) or out-of-domain data (OOD)
</listItem>
<bodyText confidence="0.999306764705883">
Joint task participants are also evaluated separately
on the syntactic dependency task (labeled attach-
ment score, LAS). Finally, systems competing in
both tasks are compared on semantic role labeling
alone, to assess the impact of the the joint pars-
ing/SRL task compared to an SRL-only task on pre-
parsed data.
Finally, as an explanatory measure, precision and
recall of the semantic labeling task have been com-
puted and tabulated.
We have decided to omit several evaluation fig-
ures that were reported in previous years, such as the
percentage of completely correct sentences (“Exact
Match”), unlabeled scores, etc. With seven lan-
guages, two tasks (plus two challenges, and the
IDD/OOD distinction), there are enough results to
get lost even as it is.
</bodyText>
<subsectionHeader confidence="0.93384">
2.4.1 Syntactic Dependency Measures
</subsectionHeader>
<bodyText confidence="0.999941636363636">
The LAS score is defined similarly as in the pre-
vious shared tasks, as the percentage of tokens for
which a system has predicted the correct HEAD and
DEPREL columns. The unlabeled attachment score
(UAS), i.e., the percentage of tokens with correct
HEAD regardless if the DEPREL is correct, has not
been officially computed this year. No precision and
recall measures are applicable, since all systems are
supposed to output a single dependency with a single
label (see also below the footnote to the description
of the combined score).
</bodyText>
<subsectionHeader confidence="0.985291">
2.4.2 Semantic Labeling Measures
</subsectionHeader>
<bodyText confidence="0.999978133333333">
The semantic propositions are evaluated by con-
verting them to semantic dependencies, i.e., we cre-
ate n semantic dependencies from every predicate
to its n arguments. These dependencies are labeled
with the labels of the corresponding arguments. Ad-
ditionally, we create a semantic dependency from
each predicate to a virtual ROOT node. The latter
dependencies are labeled with the predicate senses.
This approach guarantees that the semantic depen-
dency structure conceptually forms a single-rooted,
connected (but not necessarily acyclic) graph. More
importantly, this scoring strategy implies that if a
system assigns the incorrect predicate sense, it still
receives some points for the arguments correctly as-
signed. For example, for the correct proposition:
</bodyText>
<footnote confidence="0.519828666666667">
verb.01: A0, A1, AM-TMP
the system that generates the following output for
the same argument tokens:
</footnote>
<page confidence="0.958141">
4
</page>
<equation confidence="0.388774">
verb.02: A0, A1, AM-LOC
</equation>
<bodyText confidence="0.99636025">
receives a labeled precision score of 2/4 because two
out of four semantic dependencies are incorrect: the
dependency to ROOT is labeled 02 instead of 01
and the dependency to the AM-TMP is incorrectly la-
beled AM-LOC. Using this strategy we compute pre-
cision, recall, and F1 scores for semantic dependen-
cies (labeled only).
For some languages (Czech, Japanese) there may
be more than one label in a given argument position;
for example, this happens in Czech in special cases
of reciprocity when the same token serves as two or
more arguments to the same predicate. The scorer
takes this into account and considers such cases to
be (as if) multiple predicate-argument relations for
the computation of the evaluation measures.
For example, for the correct proposition:
</bodyText>
<sectionHeader confidence="0.596892" genericHeader="method">
v1f1: ACT|EFF, ADDR
</sectionHeader>
<bodyText confidence="0.88857">
the system that generates the following output for
the same argument tokens:
</bodyText>
<sectionHeader confidence="0.54732" genericHeader="method">
v1f1: ACT, ADDR|PAT
</sectionHeader>
<bodyText confidence="0.9907256">
receives a labeled precision score of 3/4 because
the PAT is incorrect and labeled recall 3/4 be-
cause the EFF is missing (should the ACT|EFF and
ADDR|PAT be taken as atomic values, the scores
would then be zero).
</bodyText>
<subsectionHeader confidence="0.945027">
2.4.3 Combined Syntactic and Semantic Score
</subsectionHeader>
<bodyText confidence="0.986486833333334">
We combine the syntactic and semantic measures
into one global measure using macro averaging. We
compute macro precision and recall scores by aver-
aging the labeled precision and recall for semantic
dependencies with the LAS for syntactic dependen-
cies:3
</bodyText>
<equation confidence="0.999846">
LMP = Wsem * LPsem + (1 − Wsem) * LAS (1)
LMR = Wsem * LRsem + (1 − Wsem) * LAS (2)
</equation>
<bodyText confidence="0.9982746">
where LMP is the labeled macro precision and
LPsem is the labeled precision for semantic depen-
dencies. Similarly, LMR is the labeled macro re-
call and LRsem is the labeled recall for semantic
dependencies. Wsem is the weight assigned to the
</bodyText>
<footnote confidence="0.749585">
3We can do this because the LAS for syntactic dependen-
cies is a special case of precision and recall, where the predicted
number of dependencies is equal to the number of gold depen-
dencies.
</footnote>
<bodyText confidence="0.9979515">
semantic task.4 The macro labeled F1 score, which
was used for the ranking of the participating sys-
tems, is computed as the harmonic mean of LMP
and LMR.
</bodyText>
<sectionHeader confidence="0.995219" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999951777777778">
The unification of the data formats for the various
languages appeared to be a challenge in itself. We
will briefly describe the processes of the conversion
of the existing treebanks in the seven languages of
the CoNLL-2009 shared task. In many instances,
the original treebanks had to be not only converted
format-wise, but also merged with other resources in
order to generate useful training and testing data that
fit the task description.
</bodyText>
<subsectionHeader confidence="0.995338">
3.1 The Input Corpora
</subsectionHeader>
<bodyText confidence="0.993373642857143">
The data used as the input for the transformations
aimed at arriving at the data contents and format de-
scribed in Sect. 2.3 are described in (Taul´e et al.,
2008), (Xue and Palmer, 2009), (Hajiˇc et al., 2006),
(Surdeanu et al., 2008), (Burchardt et al., 2006) and
(Kawahara et al., 2002).
In the subsequent sections, the procedures for the
data conversion for the individual languages are de-
scribed. The data has been collected by the main
organization site and checked for format errors, and
repackaged for distribution.
There were three packages of the data distributed
to the participants: Trial, Training plus Develop-
ment, and Evaluation. The Trial data were rather
small, just to give the feeling of the format and
languages involved. A visual representation of the
Trial data was also created to make understanding
of the data easier. Any data in the same format
can be transformed and displayed in the Tree Editor
TrEd5 (Pajas and ˇStˇep´anek, 2008) with the CoNLL
2009 Shared Task extension that can be installed
from within the editor. A sample visualization of an
English sentence after its conversion to the shared
task format (Sect. 2.3) is in Fig. 1.
Due to licensing requirements, every package of
the data had to be split into two portions. One
portion (Catalan, German, Japanese, and Spanish
data) was published on the task’s webpage for down-
</bodyText>
<footnote confidence="0.9999345">
4We assign equal weight to the two tasks, i.e., W,,. = 0.5.
5http://ufal.mff.cuni.cz/∼pajas/tred
</footnote>
<page confidence="0.981797">
5
</page>
<figure confidence="0.872763">
Al
</figure>
<figureCaption confidence="0.981612857142857">
Figure 1: Visualisation of the English sentence “And sometimes a reputable charity with a houshold name gets used
and doesn’t even know it.” (Penn Treebank, wsj 0559) showing jointly the labeled syntactic and semantic depen-
dencies. The basic tree shape comes from the syntactic dependencies; syntactic labels and POS tags are on the 2nd
line at each node. Semantic dependencies which do not follow the syntactic ones use dotted lines. Predicate senses
in parentheses (use:01, ...) follow the word label. SRLs (A0, AM-TMP, ...) are on the last line. Please note that
multiple semantic dependencies (e.g., there are four for charity: A0 ← know, A1 ← gets, A1 ← used, A1 ← name)
and self-dependencies (name) appear in this sentence.
</figureCaption>
<figure confidence="0.999015731707317">
household
NMOD NN
it
OBJ PRP
a
NMOD DT
gets ( get.03)
ROOT VBZ
And
DEP CC
sometimes
TMP RB
charity
SBJ NN
used use.0�
( )
VC VBN
and
COORD CC
P .
does
CONJ VBZ
with
NMOD IN
n&apos;t
ADV RB
even
ADV RB
name name.0�
( )
PMOD NN
know know.0�
( )
VC VB
A2
AM-TMP AM-TMP AM-TMP A0 Al Al Al
reputable
NMOD JJ
a
NMOD DT
A2 AM-NEG AM-ADV
</figure>
<bodyText confidence="0.9765005">
load, the other portion (Czech, English, and Chinese
data) was invoiced and distributed by the Linguistic
Data Consortium under a special agreement free of
charge.
Distribution of the Evaluation package was a bit
more complicated, because there were two types of
the packages - one for the Joint task and one for the
SRL-only task. Every participant had to subscribe
to one of the two tasks; subsequently, they obtained
the appropriate data (again, from the webpage and
LDC).
Prior to release, each data file was checked to
eliminate errors. The following test were carried
out:
</bodyText>
<listItem confidence="0.9993565">
• For every sentence, number of PREDs rows
matches the number of APREDs columns.
• The first line of each file is never empty, while
the last line always is.
• The first character on a non-empty line is al-
ways a digit, the last one is never a whitespace.
• The number of empty lines (i.e. the number
of sentences) equals the number of lines begin-
ning with “1”.
• The data contain no spaces nor double tabs.
</listItem>
<bodyText confidence="0.9986355">
Some statistics on the data can be seen in Ta-
bles 2, 3 and 4. Whereas the training sizes of the
data have not been that different as they were e.g.
for the 2007 shared task on multilingual dependency
parsing (Nivre et al., 2007)6, substantial differences
existed in the distribution of the predicates and ar-
guments, the input features, the out-of-vocabulary
rates, and other statistical characteristics of the data.
Data sizes have been relatively uniform in all the
datasets, with Japanese having the smallest dataset
</bodyText>
<footnote confidence="0.98821">
6http://nextens.uvt.nl/depparse-wiki/
DataOverview
</footnote>
<page confidence="0.998407">
6
</page>
<bodyText confidence="0.999979516129033">
containing data for SRL annotation training. To
compensate at least for the dependency parsing part,
an additional, large Japanese corpus with syntactic
dependency annotation has been provided.
The average sentence length, the vocabulary sizes
for FORM and LEMMA fields and the OOV rates
characterize quite naturally the properties of the re-
spective languages (in the domain of the training and
evaluation data). It is no surprise that the FORM
OOV rate is the highest for Czech, a highly inflec-
tional language, and that the LEMMA OOV rate is
the highest for German (as a consequence of keeping
compounds as a single lemma). The other statistics
also reflect (to a large extent) the annotation speci-
fication and conventions used for the original tree-
banks and/or the result of the conversion process to
the unified CoNLL-2009 Shared Task format.
Starting with the POS and FEAT fields, it can be
seen that Catalan, Czech and Spanish use only the
12 major part-of-speech categories as values of the
POS field (with richly populated FEAT field); En-
glish and Chinese are the opposite extreme, disre-
garding the use of the FEAT field completely and
coding everything as a POS value. While for Chi-
nese this is quite understandable, English follows the
PTB tradition in this respect. German and Japanese
use relatively rich set of values in both the POS and
FEAT fields.
For the dependency relations (DEPREL), all
the languages use a similarly-sized set except for
Japanese, which only encodes the distinction be-
tween a root and a dependent node (and some in-
frequent special ones).
Evaluation data are over 10% of the size of the
training data for Catalan, Chinese, Czech, Japanese
and Spanish and roughly 5% for English and Ger-
man.
Table 3 shows the distribution of the five most fre-
quent dependency relations (determined as part of
the subtask of syntactic parsing). With the exception
of Japanese, which essentially does not label depen-
dency relations at this level, all the other languages
show little difference in this distribution. For exam-
ple, the unconditioned probability of “subjects” is
almost the same for all the six other languages (be-
tween 6 and 8 percent). The probability mass cov-
ered by the first five most frequent DEPRELs is also
almost the same (again, except for Japanese), sug-
gesting that the labeling task might have similar dif-
ficulty7. The most skewed one is for Czech (after
Japanese).
Table 4 shows similar statistics for the argument
labels (PRED/APREDs); it also adds the average
number of arguments per “predicate” token, since
this is part of the SRL task8. It is apparent from the
comparison of the “Total” rows in this table and Ta-
ble 3 that the first five argument labels cover more
that their syntactic counterparts. For example, the
arguments A0-A4 account for all but 3% of all ar-
guments labels, whereas Spanish and Catalan have
much more rich set of argument labels, with a high
entropy of the most-frequent-label distribution.
</bodyText>
<subsectionHeader confidence="0.999041">
3.2 Catalan and Spanish
</subsectionHeader>
<bodyText confidence="0.972212483870968">
The Catalan and Spanish datasets (Taul´e et al., 2008)
were generated from the AnCora corpora9 through
an automatic conversion process from a constituent-
based formalism to dependencies (Civit et al., 2006).
AnCora corpora contain about half million words
for Catalan and Spanish annotated with syntactic
and semantic information. Text sources for the Cata-
lan corpus are EFE news agency (-75Kw), ACN
Catalan news agency (-225Kw), and ‘El Peri´odico’
newspaper (-200Kw). The Spanish corpus comes
from the Lexesp Spanish balanced corpus (-75Kw),
the EFE Spanish news agency (-225Kw), and the
Spanish version of ‘El Peri´odico’ (-200Kw). The
subset from ‘El Peri´odico’ corresponds to the same
news in Catalan and Spanish, spanning from January
to December 2000.
Linguistic annotation is the same in both lan-
guages and includes: PoS tags with morphologi-
cal features (gender, number, person, etc.), lemma-
tization, syntactic dependencies (syntactic func-
tions), semantic dependencies (arguments and the-
matic roles), named entities and predicate semantic
classes (Lexical Semantic Structure, LSS). Tag sets
are shared by the two languages.
If we take into account the complete PoS tags,
7Yes, this is overgeneralization since this distribution does
not condition on the features, dependencies etc. But as a rough
measure, it often correlates well with the results.
8A number below 1 means there are some argument-bearing
words (often nouns) which have no arguments in the particular
sentence in which they appear.
</bodyText>
<footnote confidence="0.957244">
9http://clic.ub.edu/ancora
</footnote>
<page confidence="0.999054">
7
</page>
<table confidence="0.999557357142857">
Characteristic Catalan Chinese Czech English German Japanese Spanish
Training data size (sentences) 13200 22277 38727 39279 36020 4393a 14329
Training data size (tokens) 390302 609060 652544 958167 648677 112555a 427442
Avg. sentence length (tokens) 29.6 27.3 16.8 24.4 18.0 25.6 29.8
Tokens with argumentsb (%) 9.6 16.9 63.5 18.7 2.7 22.8 10.3
DEPREL types 50 41 49 69 46 5 49
POS types 12 41 12 48 56 40 12
FEAT types 237 1 1811 1 267 302 264
FORM vocabulary size 33890 40878 86332 39782 72084 36043 40964
LEMMA vocabulary size 24143 40878 37580 28376 51993 30402 26926
Evaluation data size (sent.) 1862 2556 4213 2399 2000 500 1725
Evaluation data size (tokens) 53355 73153 70348 57676 31622 13615 50630
Evaluation FORM OOVc 5.40 3.92 7.98/8.62d 1.58/3.76d 7.93/7.57d 6.07 5.63
Evaluation LEMMA OOVc 4.14 3.92 3.03/4.29d 1.08/2.30d 5.83/7.36d 5.21 3.69
</table>
<tableCaption confidence="0.999023">
Table 2: Elementary data statistics for the CoNLL-2009 Shared Task languages. The data themselves, the original
treebanks they were derived from and the conversion process are described in more detail in sections 3.2-3.7. All
evaluation data statistics are derived from the in-domain evaluation data.
</tableCaption>
<bodyText confidence="0.6729864">
aThere were additional 33257 sentences (839947 tokens) available for syntactic dependency parsing of Japanese; the type and
vocabulary statistics are computed using this larger dataset.
bPercentage of tokens with FILLPRED=‘Y’.
cPercentage of FORM/LEMMA tokens not found in the respective vocabularies derived solely from the training data.
dOOV percentage for in-domain/out-of-domain data.
</bodyText>
<table confidence="0.916409142857143">
DEPREL Catalan Chinese Czech English German Japanese Spanish
sn 0.16 COMP 0.21 Atr 0.26 NMOD 0.27 NK 0.31 D 0.93 sn 0.16
spec 0.15 NMOD 0.14 AuxP 0.10 P 0.11 PUNC 0.14 ROOT 0.04 spec 0.15
Labels f 0.11 ADV 0.10 Adv 0.10 PMOD 0.10 MO 0.12 P 0.03 f 0.12
sp 0.09 UNK 0.09 Obj 0.07 SBJ 0.07 SB 0.07 A 0.00 sp 0.08
suj 0.07 SBJ 0.08 Sb 0.06 OBJ 0.06 ROOT 0.06 I 0.00 suj 0.08
Total 0.58 0.62 0.59 0.61 0.70 1.00 0.59
</table>
<tableCaption confidence="0.9927815">
Table 3: Unigram probability for the five most frequent DEPREL labels in the training data of the CoNLL-2009
Shared Task is shown. Total is the probability mass covered by the five dependency labels shown.
</tableCaption>
<table confidence="0.99976225">
APRED Catalan Chinese Czech English German Japanese Spanish
arg1-pat 0.22 A1 0.30 RSTR 0.30 A1 0.37 A0 0.40 GA 0.33 arg1-pat 0.20
arg0-agt 0.18 A0 0.27 PAT 0.18 A0 0.25 A1 0.39 WO 0.15 arg0-agt 0.19
Labels arg1-tem 0.15 ADV 0.20 ACT 0.17 A2 0.12 A2 0.12 NO 0.15 arg1-tem 0.15
argM-tmp 0.08 TMP 0.07 APP 0.06 AM-TMP 0.06 A3 0.06 NI 0.09 arg2-atr 0.08
arg2-atr 0.08 DIS 0.04 LOC 0.04 AM-MNR 0.03 A4 0.01 DE 0.06 argM-tmp 0.08
Total 0.71 0.91 0.75 0.83 0.97 0.78 0.70
Avg. 2.25 2.26 0.88 2.20 1.97 1.71 2.26
</table>
<tableCaption confidence="0.91972075">
Table 4: Unigram probability for the five most frequent APRED labels in the training data of the CoNLL-2009
Shared Task is shown. Total is the probability mass covered by the five argument labels shown. The “Avg.” line
shows the average number of arguments per predicate or other argument-bearing token (i.e. for those marked by
FILLPRED=‘Y’).
</tableCaption>
<page confidence="0.995474">
8
</page>
<bodyText confidence="0.999521474358975">
AnCora has 280 different labels. Considering only
the main syntactic categories, the tag set is reduced
to 47 tags. The syntactic tag set consists of 50 dif-
ferent syntactic functions. Regarding semantic ar-
guments, we distinguish Arg0, Arg1, Arg2, Arg3,
Arg4, ArgM, and ArgL. The first five tags are num-
bered from less to more obliqueness with respect
to the verb, ArgM corresponds to adjuncts. The
list of thematic roles consists of 20 different labels:
AGT (Agent), AGI (Induced Agent), CAU (Cause),
EXP (Experiencer), SCR (Source), PAT (Patient),
TEM (Theme), ATR (Attribute), BEN (Beneficiary),
EXT (Extension), INS (Instrument), LOC (Loca-
tive), TMP (Time), MNR (Manner), ORI (Origin),
DES (Goal), FIN (Purpose), EIN (Initial State), EFI
(Final State), and ADV (Adverbial). Each argument
position can map onto specific thematic roles. By
way of example, Arg1 can be PAT, TEM or EXT. For
Named Entities, we distinguish six types: Organiza-
tion, Person, Location, Date, Number, and Others.
An incremental process guided the annotation of
AnCora, since semantics depends on morphosyntax,
and syntax relies on morphology. This procedure
made it possible to check, correct, and complete
the previous annotations, thus guaranteeing the final
quality of the corpora and minimizing the error rate.
The annotation process was carried out sequentially
from lower to upper layers of linguistic description.
All resulting layers are independent of each other,
thus making easier the data management. The ini-
tial annotation was performed manually for syntax,
semiautomatically in the case of arguments and the-
matic roles, and fully automatically for PoS (Martiet al., 2007; M`arquez et al., 2007).
The Catalan and Spanish AnCora corpora were
straightforwardly translated into the CoNLL-2009
shared task formatting (information about named
entities was skipped in this process). The resulting
Catalan corpus (including training, development and
test partitions) contains 16,786 sentences with an av-
erage length of 29.59 lexical tokens per sentence.
Long sentences abound in this corpus. For instance,
10.73% of the sentences are longer than 50 tokens,
and 4.42% are longer than 60. The corpus con-
tains 47,537 annotated predicates (2.83 predicates
per sentence, on average) with 107,171 arguments
(2.25 arguments per predicate, on average). From
the latter, 73.89% correspond to core arguments and
26.11% to adjuncts. Numbers for the Spanish cor-
pus are comparable in all aspects: 17,709 sentences
with 29.84 lexical tokens on average (11.58% of the
sentences longer than 50 tokens, 4.07% longer than
60); 54,075 predicates (3.05 per sentence, on aver-
age) and 122,478 arguments (2.26 per predicate, on
average); 73.34% core arguments and 26.66% ad-
juncts.
The following are important features of the Cata-
lan and Spanish corpora in the CoNLL-2009 shared
task setting: (1) all dependency trees are projective;
(2) no word can be the argument of more than one
predicate in a sentence; (3) semantic dependencies
completely match syntactic dependency structures
(i.e., no new edges are introduced by the semantic
structure); (4) only verbal predicates are annotated
(with exceptional cases referring to words that can
be adjectives and past participles); (5) the corpus is
segmented so multi-words, named entities, temporal
expressions, compounds, etc. are grouped together;
and (6) segmentation also accounts for elliptical pro-
nouns (there are marked as empty lexical tokens ‘_’
with a pronoun POS tag).
Finally, the predicted columns (PLEMMA,
PPOS, and PFEAT) have been generated with the
FreeLing Open source suite of Language Analyz-
ers10. Accuracy in PLEMMA and PPOS columns
is above 95% for the two languages. PHEAD
and PDEPREL columns have been generated using
MaltParser11. Parsing accuracy (LAS) is above 86%
for the the two languages.
</bodyText>
<subsectionHeader confidence="0.991184">
3.3 Chinese
</subsectionHeader>
<bodyText confidence="0.99974025">
The Chinese Corpus for the 2009 CoNLL Shared
Task was generated by merging the Chinese Tree-
bank (Xue et al., 2005) and the Chinese Proposition
Bank (Xue and Palmer, 2009) and then converting
the constituent structure to a dependency formalism
as specified in the CoNLL Shared Task. The Chi-
nese data used in the shared task is based on Chinese
Treebank 6.0 and the Chinese Proposition Bank 2.0,
both of which are publicly available via the Linguis-
tic Data Consortium.
The Chinese Treebank Project originated at Penn
and was later moved to University of Colorado at
</bodyText>
<footnote confidence="0.9989865">
10http://www.lsi.upc.es/—nlp/freeling
11http://w3.msi.vxu.se/—jha/maltparser
</footnote>
<page confidence="0.993988">
9
</page>
<bodyText confidence="0.997202384615385">
Boulder. Now it is the process of being to moved
to Brandeis University. The data sources of the Chi-
nese Treebank range from Xinhua newswire (main-
land China), Hong Kong news, and Sinorama Maga-
zine (Taiwan). More recently under DARPA GALE
funding it has been expanded to include broadcast
news, broadcast conversation, news groups and web
log data. It currently has over one million words
and is fully segmented, POS-tagged and annotated
with phrase structure. The version of the Chinese
Treebank used in this shared task, CTB 6.0, includes
newswire, magazine articles, and transcribed broad-
cast news 12. The training set has 609,060 tokens,
the development set has 49,620 tokens, and the test
set has 73,153 tokens.
The Chinese Proposition Bank adds a layer of se-
mantic annotation to the syntactic parses in the Chi-
nese Treebank. This layer of semantic annotation
mainly deals with the predicate-argument structure
of Chinese verbs and their nominalizations. Each
major sense (called frameset) of a predicate takes a
number of core arguments annotated with numeri-
cal labels Arg0 through Arg5 which are defined in
a predicate-specific manner. The Chinese Proposi-
tion Bank also annotates adjunctive arguments such
as locative, temporal and manner modifiers of the
predicate. The version of the Chinese Propbank used
in this CoNLL Shared Task is CPB 2.0, but nominal
predicates are excluded because the annotation is in-
complete.
Since the Chinese Treebank is annotated with
constituent structures, the conversion and merging
procedure converts the constituent structures to de-
pendencies by identifying the head for each con-
stituent in a parse tree and making its sisters its de-
pendents. The Chinese Propbank pointers are then
shifted from the entire constituent to the head of that
constituent. The conversion procedure identifies the
head by first exploiting the structural information
in the syntactic parse and detecting six broad cate-
gories of syntactic relations that hold between the
head and its dependents (predication, modification,
complementation, coordination, auxiliary, and flat)
and then designating the head based on these rela-
tions. In particular, the first conjunct of a coordina-
12A small number of files were taken out of the CoNLL
shared task data due to conversion problems and time con-
straints to fix them.
tion structure is designated as the head and the heads
of the other conjuncts are the conjunctions preced-
ing them. The conjunctions all “modify” the first
conjunct.
</bodyText>
<subsectionHeader confidence="0.947546">
3.4 Czech
</subsectionHeader>
<bodyText confidence="0.999911975">
For the training, development and evaluation data,
Prague Dependency Treebank 2.0 was used (Hajiˇc
et al., 2006). For the out-of-domain evaluation data,
part of the Czech side of the Prague Czech-English
Dependency Treebank (version 2, under construc-
tion) was used13, see also ( ˇCmejrek et al., 2004). For
the OOD data, no manual annotation of LEMMA,
POS, and FEAT existed, so the predicted values
were used. The same conversion procedure has been
applied to both sources.
The FORM column was created from the form
element of the morphological layer, not from the
“token” from the word-form layer. Therefore, most
typos, errors in word segmentation and tokenization
are corrected and numerals are normalized.
The LEMMA column was created from the
lemma element of the morphological layer. Only
the initial string of the element was used, so there is
no distinction between homonyms. However, some
components of the detailed lemma explanation were
incorporated into the FEAT column (see below).
The POS column was created form the morpho-
logical tag element, its first character more pre-
cisely.
The FEAT column was created from the remain-
ing characters of the tag element. In addition, the
special feature “Sem” corresponds to a semantic fea-
ture of the lemma.
For the HEAD and DEPREL columns, the PDT
analytical layer was used. The DEPREL was taken
from the analytic function (the afun node at-
tribtue). There are 27 possible values for afun el-
ement: Pred, Pnom, AuxV, Sb, Obj, Atr, Adv,
Atv, AtvV, Coord, Apos, ExD, and a number
of auxiliary and “double-function” labels. The first
nine of these are the “most interesting” from the
point of view of the shared task, since they relate to
semantics more closely than the rest (at least from
the linguistic point of view). The HEAD is a pointer
to its parent, which means the PDT’s ord attribute
</bodyText>
<footnote confidence="0.849125">
13http://ufal.mff.cuni.cz/pedt
</footnote>
<page confidence="0.995834">
10
</page>
<bodyText confidence="0.997567555555555">
(within-sentence ID / word position number) of the
parent. If a node is a member of a coordination
or apposition (is_member element), its DEPREL
obtains the _M suffix. The parenthesis annotation
(is_parenthesis_root element) was ignored.
The PRED and APREDs columns were created
from the tectogrammatical layer of PDT 2.0 and the
valency lexicon PDT-Vallex according to the follow-
ing rules:
</bodyText>
<listItem confidence="0.979209095238095">
• Every line corresponding to an analytical node
referenced by a lexical reference (a/lex.rf)
from the tectogrammatical layer has a PRED
value filled. If the referring non-generated
tectogrammatical node (is_generated not
equal to 1) has a valency frame assigned
(val_frame.rf), the value of PRED is the
identifier of the frame. Otherwise, it is set to
the same value as the LEMMA column.
• For every tectogrammatical node, a corre-
sponding analytical node is searched for:
1. If the tectogrammatical node is not
generated and has a lexical reference
(a/lex.rf), the referenced node is
taken.
2. Otherwise, if the tectogrammatical node
has a coreference (coref_text.rf or
coref_gram.rf) or complement refer-
ence (compl.rf) to a node that has an
analytical node assigned (by 1. or 2.), the
assigned node is taken.
</listItem>
<bodyText confidence="0.99906525">
APRED columns are filled with respect to the
following correspondence: for a tectogrammatical
node P and its effective child C with functor F, the
column for P’s corresponding analytical node at the
row for C’s corresponding analytical node is filled
with F. Some nodes can thus have several functors
in one APRED column, separated by a vertical bar
(see Sect. 2.4.2).
PLEMMA, PPOS and PFEAT were gener-
ated by the (cross-trained) morphological tagger
MORCE (Spoustov´a et al., 2009), which gives full
combined accuracy (PLEMMA+PPOS+PFEAT)
slightly under 96%.
PHEAD and PDEPREL were generated by
the (cross-trained) MST parser for Czech (Chu–
Liu/Edmonds algorithm, (McDonald et al., 2005)),
which has typical dependency accuracy around
85%.
The valency lexicon, converted from (Hajiˇc et al.,
2003), has four columns:
</bodyText>
<listItem confidence="0.996281833333333">
1. lemma (can occur several times in the lexicon,
with different frames)
2. frame identifier (as found in the PRED column)
3. list of space-separated actants and obligatory
members of the frame
4. example(s)
</listItem>
<bodyText confidence="0.9999185">
The source of the out-of-domain data uses an
extended valency lexicon (because of out-of-
vocabulary entries). For simplicity, the extended
lexicon was not provided; instead, such words were
not marked as predicates in the OOD data (their
FILLPRED was set to ‘_’) and thus not evaluated.
</bodyText>
<subsectionHeader confidence="0.700043">
3.5 English
</subsectionHeader>
<bodyText confidence="0.998783875">
The English corpus is almost identical to the cor-
pus used in the closed challenge in the CoNLL-2008
shared task evaluation (Surdeanu et al., 2008). This
corpus was generated through a process that merges
several input corpora and converts them from the
constituent-based formalism to dependencies. The
following corpora were used as input to the merging
procedure:
</bodyText>
<listItem confidence="0.996989857142857">
• Penn Treebank 3 – The Penn Treebank 3 cor-
pus (Marcus et al., 1994) consists of hand-
coded parses of the Wall Street Journal (test,
development and training) and a small subset
of the Brown corpus (W. N. Francis and H.
Kucera, 1964) (test only).
• BBN Pronoun Coreference and Entity Type
</listItem>
<bodyText confidence="0.9750662">
Corpus – BBN’s NE annotation of the Wall
Street Journal corpus (Weischedel and Brun-
stein, 2005) takes the form of SGML inline
markup of text, tokenized to be completely
compatible with the Penn Treebank annotation.
For the CoNLL-2008 shared task evaluation,
this corpus was extended by the task organizers
to cover the subset of the Brown corpus used as
a secondary testing dataset. From this corpus
we only used NE boundaries to derive NAME
</bodyText>
<page confidence="0.998236">
11
</page>
<bodyText confidence="0.986922333333333">
dependencies between NE tokens, e.g., we cre-
ate a NAME dependency from Mary to Smith
given the NE mention Mary Smith.
</bodyText>
<listItem confidence="0.939416291666667">
• Proposition Bank I (PropBank) – The Prop-
Bank annotation (Palmer et al., 2005) classifies
the arguments of all the main verbs in the Penn
Treebank corpus, other than be. Arguments are
numbered (Arg0, Arg1, ...) based on lexical
entries or frame files. Different sets of argu-
ments are assumed for different rolesets. De-
pendent constituents that fall into categories in-
dependent of the lexical entries are classified as
various types of adjuncts (ArgM-TMP, -ADV,
etc.).
• NomBank – NomBank annotation (Meyers et
al., 2004) uses essentially the same framework
as PropBank to annotate arguments of nouns.
Differences between PropBank and NomBank
stem from differences between noun and verb
argument structure; differences in treatment of
nouns and verbs in the Penn Treebank; and dif-
ferences in the sophistication of previous re-
search about noun and verb argument structure.
Only the subset of nouns that take arguments
are annotated in NomBank and only a subset of
the non-argument siblings of nouns are marked
as ArgM.
</listItem>
<bodyText confidence="0.999935647058824">
The complete merging process and the conversion
from the constituent representation to dependencies
is detailed in (Surdeanu et al., 2008).
The main difference between the 2008 and 2009
version of the corpora is the generation of word lem-
mas. In the 2008 version the only lemmas pro-
vided were predicted using the built-in lemmatizer
in WordNet (Fellbaum, 1998) based on the most fre-
quent sense for the form and the predicted part-of-
speech tag. These lemmas are listed in the 2009
corpus under the PLEMMA column. The LEMMA
column in the 2009 version of the corpus contains
lemmas generated using the same algorithm but us-
ing the correct Treebank part-of-speech tags. Addi-
tionally, the PHEAD and PDEPREL columns were
generated using MaltParser14, similarly to the open
challenge corpus in the CoNLL 2008 shared task.
</bodyText>
<footnote confidence="0.736119">
14http://w3.msi.vxu.se/∼nivre/research/
MaltParser.html
</footnote>
<subsectionHeader confidence="0.922055">
3.6 German
</subsectionHeader>
<bodyText confidence="0.999978136363637">
The German in-domain dataset is based on the an-
notated verb instances of the SALSA corpus (Bur-
chardt et al., 2006), a total of around 40k sen-
tences15. SALSA provides manual semantic role
annotation on top of the syntactically annotated
TIGER newspaper corpus, one of the standard Ger-
man treebanks. The original SALSA corpus uses se-
mantic roles in the FrameNet paradigm. We con-
structed mappings between FrameNet frame ele-
ments and PropBank argument positions at the level
of frame-predicate pairs semi-automatically. For the
frame elements of each frame-predicate pair, we first
identified the semantically defined PropBank Arg-
0 and Arg-1 positions. To do so, we annotated a
small number of very abstract frame elements with
these labels (Agent, Actor, Communicator as Arg-
0, and Theme, Effect, Message as Arg-1) and per-
colated these labels through the FrameNet hierar-
chy, adding further manual labels where necessary.
Then, we used frequency and grammatical realiza-
tion information to map the remaining roles onto
higher-numbered Arg roles. We considerably sim-
plified the annotations provided by SALSA, which
use a rather complex annotation scheme. In partic-
ular, we removed annotation for multi-word expres-
sions (which may be non-contiguous), annotations
involving multiple frames for the same predicate
(metaphors, underspecification), and inter-sentence
roles.
The out-of-domain dataset was taken from a study
on the multi-lingual projection of FrameNet annota-
tion (Pado and Lapata, 2005). It is sampled from
the EUROPARL corpus and was chosen to maxi-
mize the lexical coverage, i.e., it contains of a large
number of infrequent predicates. Both syntactic and
semantic structure were annotated manually, in the
TIGER and SALSA format, respectively. Since it
uses a simplified annotation schemes, we did not
have to discard any annotation.
For both datasets, we converted the syntactic
TIGER (Brants et al., 2002) representations into de-
pendencies with a similar set of head-finding rules
used for the preparation of the CoNLL-X shared task
German dataset. Minor modifications (for the con-
</bodyText>
<footnote confidence="0.7890005">
15Note, however, that typically not all predicates in each sen-
tence are annotated (cf. Table 2).
</footnote>
<page confidence="0.998505">
12
</page>
<bodyText confidence="0.999902">
version of person names and coordinations) were
made to achieve better consistency with datasets
of other languages. Since the TIGER annotation
allows non-contiguous constituents, the resulting
dependencies can be non-projective. Secondary
edges were discarded in the conversion. As for the
automatically constructed features, we used Tree-
Tagger (Schmid, 1994) to produce the PLEMMA
and PPOS columns, and the Morphisto morphol-
ogy (Zielinski and Simon, 2008) for PFEAT.
</bodyText>
<subsectionHeader confidence="0.959615">
3.7 Japanese
</subsectionHeader>
<bodyText confidence="0.999933128205128">
For Japanese, we used the Kyoto University Text
Corpus (Kawahara et al., 2002), which consists of
approximately 40k sentences taken from Mainichi
Newspapers. Out of them, approximately 5k sen-
tences are annotated with syntactic and semantic de-
pendencies, and are used the training, development
and test data of this year’s shared task. The remain-
ing sentences, which are annotated with only syntac-
tic dependencies, are provided for the training cor-
pus of syntactic dependency parsers.
This corpus adopts a dependency structure repre-
sentation, and thus the conversion to the CoNLL-
2009 format was relatively straightforward. How-
ever, since the original dependencies are annotated
on the basis of phrases (Japanese bunsetsu), we
needed to automatically convert the original annota-
tions to word-based ones using several criteria. We
used the following basic criteria: the words except
the last word in a phrase depend on the next (right)
word, and the last word in a phrase basically depends
on the head word of the governing phrase.
Semantic dependencies are annotated for both
verbal predicates and nominal predicates. The se-
mantic roles (APRED columns) consist of 41 sur-
face cases, many of which are case-marking post-
positions such as ga (nominative), wo (accusative)
and ni (dative). Semantic frame discrimination is not
annotated, and so the PRED column is the same as
the LEMMA column. The original corpus contains
coreference annotations and inter-sentential seman-
tic dependencies, such as inter-sentential zero pro-
nouns and bridging references, but we did not use
these annotations, which are not the target of this
year’s shared task.
To produce the PLEMMA, PPOS and PFEAT
columns, we used the morphological analyzer JU-
MAN 16 and the dependency and case structure an-
alyzer KNP 17. To produce the PHEAD and PDE-
PREL columns, we used the MSTParser 18.
</bodyText>
<sectionHeader confidence="0.906427" genericHeader="method">
4 Submissions and Results
</sectionHeader>
<bodyText confidence="0.999767483870968">
Participants uploaded the results through the shared
task website, and the official evaluation was per-
formed centrally. Feedback was provided if any for-
mal problems were encountered (for a list of checks,
see the previous section). One submission had to
be rejected because only English results were pro-
vided. After the evaluation period had passed, the
results were anonymized and published on the web.
A total of 20 systems participated in the closed
challenge; 13 of them in the Joint task and seven in
the SRL-only task. Two systems participated in the
open challenge (Joint task). Moreover, 17 systems
provided output in the out-of-domain part of the task
(11 in the OOD Joint task and six in the OOD SRL-
only task).
The main results for the core task - the Joint task
(dependency syntax and semantic relations) in the
context of the closed challenge - are summarized and
ranked in Table 5.
The largest number of systems can be compared
in the SRL results table (Table 6), where all the sys-
tems have been evaluated solely on the SRL perfor-
mance regardless whether they participated in the
Joint or SRL-only task. However, since the results
might have been influenced by the supplied parser,
separate ranking is provided for both types of the
systems.
Additional breakdown of the results (open chal-
lenge, precision and recall tables for the semantic
labeling task, etc.) are available from the CoNLL-
2009 Shared Task website19.
</bodyText>
<sectionHeader confidence="0.991471" genericHeader="method">
5 Approaches
</sectionHeader>
<bodyText confidence="0.5862075">
Table 7 summarizes the properties of the systems
that participated in the closed the open challenges.
</bodyText>
<footnote confidence="0.998098">
16http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/juman-e.html
17http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp-e.html
18http://sourceforge.net/projects/
mstparser
19http://ufal.mff.cuni.cz/conll2009-st
</footnote>
<page confidence="0.99481">
13
</page>
<table confidence="0.999188428571428">
Rank System Average Catalan Chinese Czech English German Japanese Spanish
1 Che 82.64 81.84 76.38 83.27 87.00 82.44 85.65 81.90
2 Chen 82.52 83.01 76.23 80.87 87.69 81.22 85.28 83.31
3 Merlo 82.14 82.66 76.15 83.21 86.03 79.59 84.91 82.43
4 Bohnet 80.85 80.44 75.91 79.57 85.14 81.60 82.51 80.75
5 Asahara 78.43 75.91 73.43 81.43 86.40 69.84 84.86 77.12
6 Brown 77.27 77.40 72.12 75.66 83.98 77.86 76.65 77.21
7 Zhang 76.49 75.00 73.42 76.93 82.88 73.76 78.17 75.25
8 Dai 73.98 72.09 72.72 67.14 81.89 75.00 80.89 68.14
9 Lu Li 73.97 71.32 65.53 75.85 81.92 70.93 80.49 71.72
10 Lluis 71.49 56.64 66.18 75.95 81.69 72.31 81.76 65.91
11 Vallejo 70.81 73.75 67.16 60.50 78.19 67.51 77.75 70.78
12 Ren 67.81 59.42 75.90 60.18 77.83 65.77 77.63 57.96
13 Zeman 51.07 49.61 43.50 57.95 50.27 49.57 57.69 48.90
</table>
<tableCaption confidence="0.9835615">
Table 5: Official results of the Joint task, closed challenge. Teams are denoted by the last name (first name added
only where needed) of the author who registered for the evaluation data. Results are sorted in descending order of the
language-averaged macro F1 score on the closed challenge Joint task. Bold numbers denote the best result for a given
language.
</tableCaption>
<table confidence="0.731264">
Rank Rank in task System Average Catalan Chinese Czech English German Japanese Spanish
1 1 (SRLonly) Zhao 80.47 80.32 77.72 85.19 85.44 75.99 78.15 80.46
2 2 (SRLonly) Nugues 80.31 80.01 78.60 85.41 85.63 79.71 76.30 76.52
</table>
<figure confidence="0.966137666666667">
3 1 (Joint) Chen 79.96 80.10 76.77 82.04 86.15 76.19 78.17 80.29
4 2 (Joint) Che 79.94 77.10 77.15 86.51 85.51 78.61 78.26 76.47
5 3 (Joint) Merlo 78.42 77.44 76.05 86.02 83.24 71.78 77.23 77.19
6 3 (SRLonly) Meza-Ruiz 77.46 78.00 77.73 75.75 83.34 73.52 76.00 77.91
7 4 (Joint) Bohnet 76.00 74.53 75.29 79.02 80.39 75.72 72.76 74.31
8 5 (Joint) Asahara 75.65 72.35 74.17 84.69 84.26 63.66 77.93 72.50
9 6 (Joint) Brown 72.85 72.18 72.43 78.02 80.43 73.40 61.57 71.95
10 7 (Joint) Dai 70.78 66.34 71.57 75.50 78.93 67.43 71.02 64.64
11 8 (Joint) Zhang 70.31 67.34 73.20 78.28 77.85 62.95 64.71 67.81
12 9 (Joint) Lu Li 69.72 66.95 67.06 79.08 77.17 61.98 69.58 66.23
13 4 (SRLonly) Baoli Li 69.26 74.06 70.37 57.46 69.63 67.76 72.03 73.54
14 10 (Joint) Vallejo 68.95 70.14 66.71 71.49 75.97 61.01 68.82 68.48
15 5 (SRLonly) Moreau 66.49 65.60 67.37 71.74 72.14 66.50 57.75 64.33
16 11 (Joint) Lluis 63.06 46.79 59.72 76.90 75.86 62.66 71.60 47.88
17 6 (SRLonly) T¨ackstr¨om 61.27 57.11 63.41 71.05 67.64 53.42 54.74 61.51
18 7 (SRLonly) Lin 57.18 61.70 70.33 60.43 65.66 59.51 23.78 58.87
19 12 (Joint) Ren 56.69 41.00 72.58 62.82 67.56 54.31 58.73 39.80
20 13 (Joint) Zeman 32.14 24.19 34.71 58.13 36.05 16.44 30.13 25.36
</figure>
<figureCaption confidence="0.552189">
Table 6: Official results of the semantic labeling, closed challenge, all systems. Teams are denoted by the last name
(first name added only where needed) of the author who registered for the evaluation data. Results are sorted in
descending order of the semantic labeled F1 score (closed challenge). Bold numbers denote the best result for a given
language. Separate ranking is provided for SRL-only systems.
</figureCaption>
<bodyText confidence="0.999721375">
The second column of the table highlights the over-
all architectures. We used + to indicate that the
components are sequentially connected. The lack of
a + sign indicates that the corresponding tasks are
performed jointly.
It is perhaps not surprising that most of the obser-
vations from the 2008 shared task still hold; namely,
the best systems overall do not use joint learning or
optimization (the best such system was placed third
in the Joint task, and there were only four systems
where the learning methodology can be considered
“joint”).
Therefore, most of the observations and conclu-
sions from 2008 shared task hold as well for the
current results. For details, we will leave it to the
reader to interpret the architectures and methods
</bodyText>
<page confidence="0.998971">
14
</page>
<tableCaption confidence="0.564692">
Table 7: Summary of system architectures for the CoNLL-2009 shared task; all systems are included. SRL-only systems do not have the D columns and the Joint Learing/Opt.
</tableCaption>
<bodyText confidence="0.737317833333333">
columns filled in. The systems are sorted by the semantic labeled Fl score averaged over all the languages (same as in Table 6). Only the systems that have a corresponding paper in
the proceedings are included. Acronyms used: D - syntactic dependencies, P - predicate, A - argument, I - identification, C - classification. Overall arch. stands for the complete
system architecture; D Arch. stands for the architecture of the syntactic parser; D Comb. indicates if the final parser output was generated using parser combination; D Inference
stands for the type of inference used for syntactic parsing; PA Arch. stands the type of architecture used for PAIC; PA Comb. indicates if the PA output was generated through
system combination; PA Inference stands for the the type of inference used for PAIC; Joint Learning/Opt. indicates if some form of joint learning or optimization was implemented
for the syntactic + semantic global task; ML Methods lists the ML methods used throughout the complete system.
</bodyText>
<figure confidence="0.989867142180095">
SVM (Malt), ME
MIRA
MIRA, ME
MBL
perceptron
Avg. Perceptron
SVM, kNN, ME
ME
SVM, ME
SVM (MIRA)
ME
ME
SVM
L2-regularized
lin. regression
ISBN
CRF
ME
cooccurrence
aAuthors of two systems: “Brown” and “Lin” didn’t submit a paper, so their systems’ architectures are unknown.
bThe symbol +indicates sequential processing (otherwise, parallel/joint). The  ||means that several different architectures spanning multiple subtasks ran in parallel.
cMSTCL/E as used by McDonald (2005), MSTC by Carreras (2007), MSTE by Eisner (2000), MSTHOE =MSTE with higher-order features (siblings +all grandchildren).
dThe system unifies the syntactic and semantic labels into one label, and trains classifiers over them. Itis thus difficult to split the system characteristic into a “D”/“PA” part.
Methods
ML
(SRL-only)
no
unified labels
no
no
(SRL-only)
yes, MSTE
iterative
no
no
no
no
no
(SRL-only)
(SRL-only)
(SRL-only)
synchronized
derivation
(SRL-only)
Learning/Opt.
Joint
Cutting Plane
greedy
classification
reranking
n-best relax.
MSTE
prob
ILP
greedy
greedy
greedy
greedy (?)
CRF
beam search
beam search +
reranking
greedy
greedy/global
search
greedy
Inference
PA
no
no
no
no
no
no
no
no
no
no
no
no
no
no
no
no
no
no
Comb.
PA
Markov LN
class
class
class
class
class
class
class
graph
class
class
class
class
class
class
trans
class
class
Arch.
PA
(SRL-only)
greedy
MSTE
reranking
(SRL-only)
MSTE
MSTC
MSTCL/E,MSTE
(SRL-only)
beam search
(SRL-only)
(SRL-only)
greedy with
heuristics
(SRL-only)
MSTC+rearrange
Inferencec
MSTCL/E
MSTHOE
MSTC
D
(SRL-only)
no
no
no
no
no
(SRL-only)
no
no
no
partially
for each lang.
(SRL-only)
no
(SRL-only)
(SRL-only)
(SRL-only)
no
Comb.
D
(SRL-only)
trans
graph
graph
class
graph
graph
(SRL-only)
graph
graph
graph
graph
trans
(SRL-only)
generative,
trans
(SRL-only)
(SRL-only)
(SRL-only)
Arch.
D
PAIC
D+PC+AIC
[D+P+A]C + DI
D+AI+AC+PC
D + PIC + AIC
D+PC+AIC
PC + AIC
D+DAIC+PC
D+PC+AC
D+AI+AC+PC
D + (PC AIC)
P+PC+AI+AC
DPAIC+D
D+PI+AI+AC+
Constraint Satisfaction
D + PI + Clustering +
AI + AC
PAIC
(PC+AI+AC) + AIC
DI+DC+PC+AI+AC
Overall
Arch.b
Meza-Ruiz
Ren
Asahara
Zhang
Vallejod
Dai
Lluis
Baoli Li
Che
Bohnet
Lu Li
Chen
Nugues
T¨ackstr¨om
Zhao
Moreau
Merlo
Zeman
Systema
</figure>
<page confidence="0.89746">
15
</page>
<bodyText confidence="0.903954">
when comparing Table 7 with the Tables 5 and 6).
</bodyText>
<sectionHeader confidence="0.9869" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.972763611111111">
This year’s task has been demanding in several re-
spects, but certainly the most difficulty came from
the fact that participants had to tackle all seven lan-
guages. It is encouraging that despite this added af-
fort the number of participating systems has been
almost the same as last year (20 vs. 22 in 2008).
There are several positive outcomes from this
year’s enterprise:
• we have prepared a unified format and data for
several very different lanaguages, as a basis
for possible extensions towards other languages
and unified treatment of syntactic depenndecies
and semantic role labeling across natural lan-
guages;
• 20 participants have produced SRL results for
all seven languages, using several different
methods, giving hope for a combined system
with even substantially better performance;
</bodyText>
<listItem confidence="0.552378">
• initial results have been provided for three lan-
guages on out-of-domain data (being in fact
quite close to the in-domain results).
</listItem>
<bodyText confidence="0.999960140350877">
Only four systems tried to apply what can be de-
scribed as joint learning for the syntactic and seman-
tic parts of the task. (Morante et al., 2009) use a true
joint learning formulation that phrases syntactico-
semantic parsing as a series of classification where
the class labels are concatenations of syntactic and
semantic edge labels. They predict (a), the set of
syntactico-semantic edge labels for each pair of to-
kens; (b), the set of incoming syntactico-semantic
edge labels for each individual token; and (c), the
existence of an edge between each pair of tokens.
Subsequently, they combine the (possibly conflict-
ing) output of the three classifiers by a ranking ap-
proach to determine the most likely structure that
meets all well-formedness constraints. (Lluis et al.,
2009) present a joint approach based on an exten-
sion of Eisner’s parser to accommodate also seman-
tic dependency labels. This architecture is similar
to the one presented by the same authors in the past
edition, with the extension to a second-order syn-
tactic parsing and a particular setting for Catalan
and Spanish. (Gesmundo et al., 2009) use an in-
cremental parsing model with synchronous syntac-
tic and semantic derivations and a joint probability
model for syntactic and semantic dependency struc-
tures. The system uses a single input queue but two
separate stacks and synchronizes syntactic and se-
mantic derivations at every word. The synchronous
derivations are modeled with an Incremental Sig-
moid Belief Network that has latent variables for
both syntactic and semantic states and connections
from syntax to semantics and vice versa. (Dai et
al., 2009) designed an iterative system to exploit
the inter-connections between the different subtasks
of the CoNLL shared task. The idea is to decom-
pose the joint learning problem into four subtasks
– syntactic dependency identification, syntactic de-
pendency labeling, semantic dependency identifica-
tion and semantic dependency labeling. The initial
step is to use a pipeline approach to use the input of
one subtask as input to the next, in the order speci-
fied. The iterative steps then use additional features
that are not available in the initial step to improve the
accuracy of the overall system. For example, in the
iterative steps, semantic information becomes avail-
able as features to syntactic parsing, so on and so
forth.
Despite these results, it is still not clear whether
joint learning has a significant advantage over other
approaches (and if yes, then for what languages). It
is thus necessary to carefully plan the next shared
tasks; it might be advantageous to bring up a sim-
ilar task in the future once again, and/or couple it
with selected application(s). There, (we hope) the
benefits of the dependency representation combined
with semantic roles the way we have formulated it
in 2008 and 2009 will really show up.
</bodyText>
<sectionHeader confidence="0.998362" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999864444444444">
We would like to thank the Linguistic Data Consor-
tium, mainly to Denise DiPersio, Tony Casteletto
and Christopher Cieri for their help and handling
of invoicing and distribution of the data for which
LDC has a license. For all of the trial, training and
evaluation data they had to act a very short notice.
All the data has been at the participants’ disposal
(again) free of charge. We are grateful to all of them
for LDC’s continuing support of the CoNLL Shared
</bodyText>
<page confidence="0.987942">
16
</page>
<bodyText confidence="0.947956514285714">
Tasks.
We would also like to thank organizers of the pre-
vious four shared tasks: Sabine Buchholz, Xavier
Carreras, Ryan McDonald, Amit Dubey, Johan Hall,
Yuval Krymolowski, Sandra K¨ubler, Erwin Marsi,
Jens Nilsson, Sebastian Riedel and Deniz Yuret.
This shared task would not have been possible with-
out their previous effort.
We also acknowledge the support of the M ˇSMT
of the Czech Republic, projects MSM0021620838
and LC536; the Grant Agency of the Academy of
sciences of the Czech Republic 1ET201120505 (for
Jan Hajiˇc, Jan ˇStˇep´anek and Pavel Straˇn´ak).
Lluis M`arquez and M. Ant`onia Martipartici-
pation was supported by the Spanish Ministry of
Education and Science, through the OpenMT and
TextMess research projects (TIN2006-15307-C03-
02, TIN2006-15265-C06-06).
The following individuals directly contributed to
the Chinese Treebank (in alphabetic order): Meiyu
Chang, Fu-Dong Chiou, Shizhe Huang, Zixin Jiang,
Tony Kroch, Martha Palmer, Mitch Marcus, Fei
Xia, Nianwen Xue. The contributors to the Chi-
nese Proposition Bank include (in alphabetic order):
Meiyu Chang, Gang Chen, Helen Chen, Zixin Jiang,
Martha Palmer, Zhiyi Song, Nianwen Xue, Ping Yu,
Hua Zhong. The Chinese Treebank and the Chinese
Proposition Bank were funded by DOD, NSF and
DARPA.
Adam Meyers’ work on the shared task has been
supported by the NSF Grant IIS-0534700 “Structure
Alignment-based MT.”
We thank the Mainichi Newspapers for the per-
mission of distributing the sentences of the Kyoto
University Text Corpus for this shared task.
</bodyText>
<sectionHeader confidence="0.999248" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999923968253968">
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pad´o, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings of
EMNLP-CoNLL 2007, pages 957–961, June. Prague,
Czech Republic.
Montserrat Civit, M. Ant`onia Marti, and N´uria Buf´ı.
2006. Cat3LB and Cast3LB: from constituents to
dependencies. In Proceedings of the 5th Interna-
tional Conference on Natural Language Processing,
FinTAL, pages 141–153, Turku, Finland. Springer Ver-
lag, LNAI 4139.
Qifeng Dai, Enhong Chen, and Liu Shi. 2009. An it-
erative approach for joint dependency parsing and se-
mantic role labeling. In Proceedings of the 13th Con-
ference on Computational Natural Language Learning
(CoNLL-2009), June 4-5, Boulder, Colorado, USA.
June 4-5.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Harry Bunt and Anton
Nijholt, editors, Advances in Probabilistic and Other
Parsing Tehcnologies, pages 29–62. Kluwer Academic
Publishers.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press, Cambridge.
Andrea Gesmundo, James Henderson, Paola Merlo, and
Ivan Titov. 2009. A latent variable model of syn-
chronous syntactic-semantic parsing for multiple lan-
guages. In Proceedings of the 13th Conference on
Computational Natural Language Learning (CoNLL-
2009), June 4-5, Boulder, Colorado, USA. June 4-5.
Jan Hajiˇc, Jarmila Panevov´a, Zdeˇnka Ureˇsov´a, Alevtina
B´emov´a, Veronika Kol´aˇrov´a-ˇRezniˇckov´a‘, and Petr
Pajas. 2003. PDT-VALLEX: Creating a Large-
coverage Valency Lexicon for Treebank Annotation.
In J. Nivre and E. Hinrichs, editors, Proceedings of The
Second Workshop on Treebanks and Linguistic Theo-
ries, pages 57–68, Vaxjo, Sweden. Vaxjo University
Press.
Jan Hajiˇc, Jarmila Panevov´a, Eva Hajiˇcov´a, Petr
Sgall, Petr Pajas, Jan ˇStˇep´anek, Jiˇr´ı Havelka, Marie
Mikulov´a, and Zdenˇek ˇZabokrtsk´y. 2006. Prague De-
pendency Treebank 2.0.
Daisuke Kawahara, Sadao Kurohashi, and Kˆoiti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008–2013, Las Palmas, Canary
Islands.
Xavier Lluis, Stefan Bott, and Lluis M`arquez. 2009.
A second-order joint eisner model for syntactic and
semantic dependency parsing. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA. June 4-5.
</reference>
<page confidence="0.987744">
17
</page>
<reference confidence="0.999425895833334">
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19(2):313–330.
Lluis M`arquez, Luis Villarejo, M. Ant`onia Marti, and
Mariona Taul´e. 2007. SemEval-2007 Task 09: Mul-
tilevel semantic annotation of catalan and spanish.
In Proceedings of the 4th International Workshop on
Semantic Evaluations (SemEval-2007), pages 42–47,
Prague, Czech Republic.
M. Ant`onia Marti, Mariona Taul´e, Lluis M`arquez, and
Manu Bertran. 2007. Anotaci´on semiautom´atica
con papeles tem´aticos de los corpus CESS-ECE.
Procesamiento del Lenguaje Natural, SEPLNJournal,
38:67–76.
Ryan McDonald, Fernando Pereira, Jan Hajiˇc, and Kiril
Ribarov. 2005. Non-projective dependency parsing
using spanning tree algortihms. In Proceedings of
NAACL-HLT’05, Vancouver, Canada, pages 523–530.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The Nom-
Bank Project: An Interim Report. In NAACL/HLT
2004 Workshop Frontiers in Corpus Annotation,
Boston.
Roser Morante, Vincent Van Asch, and Antal van den
Bosch. 2009. A simple generative pipeline approach
to dependency parsing and semantic role labeling. In
Proceedings of the 13th Conference on Computational
Natural Language Learning (CoNLL-2009), Boulder,
Colorado, USA. June 4-5.
Joakim Nivre, Johann Hall, Sandra K¨ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The conll 2007 shared task on depen-
dency parsing. In Proceedings of the EMNLP-CoNLL
2007 Conference, pages 915–932, Prague, Czech Re-
public.
Sebastian Pado and Mirella Lapata. 2005. Cross-lingual
projection of role-semantic information. In Proceed-
ings of the Human Language Technology Conference
and Conference on Empirical Methods in Natural Lan-
guage Processing (HLT/EMNLP-2005), pages 859–
866, Vancouver, BC.
Petr Pajas and Jan ˇStˇep´anek. 2008. Recent advances in
a feature-rich framework for treebank annotation. In
The 22nd International Conference on Computational
Linguistics - Proceedings of the Conference (COL-
ING’08), pages 673–680, Manchester.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71–106.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing.
Drahomira ”Johanka” Spoustov´a, Jan Hajiˇc, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised train-
ing for the averaged perceptron POS tagger. In Pro-
ceedings of the European ACL Cenference EACL’09,
Athens, Greece.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Lluis M`arquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008), pages 159–177.
Mariona Taul´e, Maria Ant`onia Marti, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Martin ˇCmejrek, Jan Cuˇrin, Jan Hajiˇc, Jiˇr´ı Havelka,
and Vladislav Kuboˇn. 2004. Prague Czech-English
Dependency Treebank: Syntactically Anntoated Re-
sources for Machine Translation. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation (LREC-2004), pages 1597–
1600, Lisbon, Portugal.
W. N. Francis and H. Kucera. 1964. Brown Corpus Man-
ual of Information to accompany A Standard Corpus
of Present-Day Edited American English, for use with
Digital Computers. Revised 1971, Revised and Am-
plified 1979, available at www.clarinet/brown.
R. Weischedel and A. Brunstein. 2005. BBN pronoun
coreference and entity type corpus. Technical report,
Lin- guistic Data Consortium.
Nianwen Xue and Martha Palmer. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143–172.
Nianwen Xue, Fei Xia, Fu Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural Lan-
guage Engineering, 11(2):207–238.
Andrea Zielinski and Christian Simon. 2008. Morphisto:
An open-source morphological analyzer for german.
In Proceedings ofthe Conference on Finite State Meth-
ods in Natural Language Processing.
</reference>
<page confidence="0.999281">
18
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.733384">
<title confidence="0.995966">The CoNLL-2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Languages</title>
<author confidence="0.91457">Massimiliano Ant`onia Lluis Sebastian Pavel Mihai Nianwen</author>
<affiliation confidence="0.997464416666667">Charles University in Prague, Google Inc., University of Trento, National Institute of Information and Communications Technology, University of Barcelona, Technical University of Catalonia, Barcelona, New York University, Uppsala University and V¨axj¨o University, Stuttgart University, Stanford University, Brandeis University, Saarland University,</affiliation>
<abstract confidence="0.999792882352941">For the 11th straight year, the Conference on Computational Natural Language Learning has been accompanied by a shared task whose purpose is to promote natural language processing applications and evaluate them in a standard setting. In 2009, the shared task was dedicated to the joint parsing of syntactic and semantic dependencies in multiple languages. This shared task combines the shared tasks of the previous five years under a unique dependency-based formalism similar to the 2008 task. In this paper, we define the shared task, describe how the data sets were created and show their quantitative properties, report the results and summarize the approaches of the participating systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sabine Brants</author>
<author>Stefanie Dipper</author>
<author>Silvia Hansen</author>
<author>Wolfgang Lezius</author>
<author>George Smith</author>
</authors>
<title>The TIGER treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop on Treebanks and Linguistic Theories,</booktitle>
<location>Sozopol.</location>
<contexts>
<context position="45556" citStr="Brants et al., 2002" startWordPosition="7330" endWordPosition="7333"> predicate (metaphors, underspecification), and inter-sentence roles. The out-of-domain dataset was taken from a study on the multi-lingual projection of FrameNet annotation (Pado and Lapata, 2005). It is sampled from the EUROPARL corpus and was chosen to maximize the lexical coverage, i.e., it contains of a large number of infrequent predicates. Both syntactic and semantic structure were annotated manually, in the TIGER and SALSA format, respectively. Since it uses a simplified annotation schemes, we did not have to discard any annotation. For both datasets, we converted the syntactic TIGER (Brants et al., 2002) representations into dependencies with a similar set of head-finding rules used for the preparation of the CoNLL-X shared task German dataset. Minor modifications (for the con15Note, however, that typically not all predicates in each sentence are annotated (cf. Table 2). 12 version of person names and coordinations) were made to achieve better consistency with datasets of other languages. Since the TIGER annotation allows non-contiguous constituents, the resulting dependencies can be non-projective. Secondary edges were discarded in the conversion. As for the automatically constructed feature</context>
</contexts>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. The TIGER treebank. In Proceedings of the Workshop on Treebanks and Linguistic Theories, Sozopol.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Katrin Erk</author>
<author>Anette Frank</author>
<author>Andrea Kowalski</author>
<author>Sebastian Pad´o</author>
<author>Manfred Pinkal</author>
</authors>
<title>The SALSA corpus: a German corpus resource for lexical semantics.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC-2006),</booktitle>
<location>Genoa, Italy.</location>
<marker>Burchardt, Erk, Frank, Kowalski, Pad´o, Pinkal, 2006</marker>
<rawString>Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea Kowalski, Sebastian Pad´o, and Manfred Pinkal. 2006. The SALSA corpus: a German corpus resource for lexical semantics. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC-2006), Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
</authors>
<title>Experiments with a higherorder projective dependency parser.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL 2007,</booktitle>
<pages>957--961</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="55379" citStr="Carreras (2007)" startWordPosition="8922" endWordPosition="8923">nted for the syntactic + semantic global task; ML Methods lists the ML methods used throughout the complete system. SVM (Malt), ME MIRA MIRA, ME MBL perceptron Avg. Perceptron SVM, kNN, ME ME SVM, ME SVM (MIRA) ME ME SVM L2-regularized lin. regression ISBN CRF ME cooccurrence aAuthors of two systems: “Brown” and “Lin” didn’t submit a paper, so their systems’ architectures are unknown. bThe symbol +indicates sequential processing (otherwise, parallel/joint). The ||means that several different architectures spanning multiple subtasks ran in parallel. cMSTCL/E as used by McDonald (2005), MSTC by Carreras (2007), MSTE by Eisner (2000), MSTHOE =MSTE with higher-order features (siblings +all grandchildren). dThe system unifies the syntactic and semantic labels into one label, and trains classifiers over them. Itis thus difficult to split the system characteristic into a “D”/“PA” part. Methods ML (SRL-only) no unified labels no no (SRL-only) yes, MSTE iterative no no no no no (SRL-only) (SRL-only) (SRL-only) synchronized derivation (SRL-only) Learning/Opt. Joint Cutting Plane greedy classification reranking n-best relax. MSTE prob ILP greedy greedy greedy greedy (?) CRF beam search beam search + reranki</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>Xavier Carreras. 2007. Experiments with a higherorder projective dependency parser. In Proceedings of EMNLP-CoNLL 2007, pages 957–961, June. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Montserrat Civit</author>
<author>M Ant`onia Marti</author>
<author>N´uria Buf´ı</author>
</authors>
<title>Cat3LB and Cast3LB: from constituents to dependencies.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Natural Language Processing, FinTAL,</booktitle>
<pages>141--153</pages>
<publisher>Springer Verlag,</publisher>
<location>Turku, Finland.</location>
<marker>Civit, Marti, Buf´ı, 2006</marker>
<rawString>Montserrat Civit, M. Ant`onia Marti, and N´uria Buf´ı. 2006. Cat3LB and Cast3LB: from constituents to dependencies. In Proceedings of the 5th International Conference on Natural Language Processing, FinTAL, pages 141–153, Turku, Finland. Springer Verlag, LNAI 4139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qifeng Dai</author>
<author>Enhong Chen</author>
<author>Liu Shi</author>
</authors>
<title>An iterative approach for joint dependency parsing and semantic role labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009),</booktitle>
<pages>4--5</pages>
<location>Boulder, Colorado, USA.</location>
<contexts>
<context position="59698" citStr="Dai et al., 2009" startWordPosition="9610" endWordPosition="9613">nd-order syntactic parsing and a particular setting for Catalan and Spanish. (Gesmundo et al., 2009) use an incremental parsing model with synchronous syntactic and semantic derivations and a joint probability model for syntactic and semantic dependency structures. The system uses a single input queue but two separate stacks and synchronizes syntactic and semantic derivations at every word. The synchronous derivations are modeled with an Incremental Sigmoid Belief Network that has latent variables for both syntactic and semantic states and connections from syntax to semantics and vice versa. (Dai et al., 2009) designed an iterative system to exploit the inter-connections between the different subtasks of the CoNLL shared task. The idea is to decompose the joint learning problem into four subtasks – syntactic dependency identification, syntactic dependency labeling, semantic dependency identification and semantic dependency labeling. The initial step is to use a pipeline approach to use the input of one subtask as input to the next, in the order specified. The iterative steps then use additional features that are not available in the initial step to improve the accuracy of the overall system. For ex</context>
</contexts>
<marker>Dai, Chen, Shi, 2009</marker>
<rawString>Qifeng Dai, Enhong Chen, and Liu Shi. 2009. An iterative approach for joint dependency parsing and semantic role labeling. In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009), June 4-5, Boulder, Colorado, USA. June 4-5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Bilexical grammars and their cubictime parsing algorithms.</title>
<date>2000</date>
<booktitle>Advances in Probabilistic and Other Parsing Tehcnologies,</booktitle>
<pages>29--62</pages>
<editor>In Harry Bunt and Anton Nijholt, editors,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="55402" citStr="Eisner (2000)" startWordPosition="8926" endWordPosition="8927">semantic global task; ML Methods lists the ML methods used throughout the complete system. SVM (Malt), ME MIRA MIRA, ME MBL perceptron Avg. Perceptron SVM, kNN, ME ME SVM, ME SVM (MIRA) ME ME SVM L2-regularized lin. regression ISBN CRF ME cooccurrence aAuthors of two systems: “Brown” and “Lin” didn’t submit a paper, so their systems’ architectures are unknown. bThe symbol +indicates sequential processing (otherwise, parallel/joint). The ||means that several different architectures spanning multiple subtasks ran in parallel. cMSTCL/E as used by McDonald (2005), MSTC by Carreras (2007), MSTE by Eisner (2000), MSTHOE =MSTE with higher-order features (siblings +all grandchildren). dThe system unifies the syntactic and semantic labels into one label, and trains classifiers over them. Itis thus difficult to split the system characteristic into a “D”/“PA” part. Methods ML (SRL-only) no unified labels no no (SRL-only) yes, MSTE iterative no no no no no (SRL-only) (SRL-only) (SRL-only) synchronized derivation (SRL-only) Learning/Opt. Joint Cutting Plane greedy classification reranking n-best relax. MSTE prob ILP greedy greedy greedy greedy (?) CRF beam search beam search + reranking greedy greedy/global</context>
</contexts>
<marker>Eisner, 2000</marker>
<rawString>Jason Eisner. 2000. Bilexical grammars and their cubictime parsing algorithms. In Harry Bunt and Anton Nijholt, editors, Advances in Probabilistic and Other Parsing Tehcnologies, pages 29–62. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>The MIT Press,</publisher>
<location>Cambridge.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. The MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Gesmundo</author>
<author>James Henderson</author>
<author>Paola Merlo</author>
<author>Ivan Titov</author>
</authors>
<title>A latent variable model of synchronous syntactic-semantic parsing for multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL2009),</booktitle>
<pages>4--5</pages>
<location>Boulder, Colorado, USA.</location>
<contexts>
<context position="59181" citStr="Gesmundo et al., 2009" startWordPosition="9529" endWordPosition="9532">vidual token; and (c), the existence of an edge between each pair of tokens. Subsequently, they combine the (possibly conflicting) output of the three classifiers by a ranking approach to determine the most likely structure that meets all well-formedness constraints. (Lluis et al., 2009) present a joint approach based on an extension of Eisner’s parser to accommodate also semantic dependency labels. This architecture is similar to the one presented by the same authors in the past edition, with the extension to a second-order syntactic parsing and a particular setting for Catalan and Spanish. (Gesmundo et al., 2009) use an incremental parsing model with synchronous syntactic and semantic derivations and a joint probability model for syntactic and semantic dependency structures. The system uses a single input queue but two separate stacks and synchronizes syntactic and semantic derivations at every word. The synchronous derivations are modeled with an Incremental Sigmoid Belief Network that has latent variables for both syntactic and semantic states and connections from syntax to semantics and vice versa. (Dai et al., 2009) designed an iterative system to exploit the inter-connections between the differen</context>
</contexts>
<marker>Gesmundo, Henderson, Merlo, Titov, 2009</marker>
<rawString>Andrea Gesmundo, James Henderson, Paola Merlo, and Ivan Titov. 2009. A latent variable model of synchronous syntactic-semantic parsing for multiple languages. In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL2009), June 4-5, Boulder, Colorado, USA. June 4-5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
</authors>
<title>Jarmila Panevov´a, Zdeˇnka Ureˇsov´a, Alevtina B´emov´a, Veronika Kol´aˇrov´a-ˇRezniˇckov´a‘, and Petr Pajas.</title>
<date>2003</date>
<booktitle>Proceedings of The Second Workshop on Treebanks and Linguistic Theories,</booktitle>
<pages>57--68</pages>
<editor>In J. Nivre and E. Hinrichs, editors,</editor>
<publisher>Vaxjo University Press.</publisher>
<marker>Hajiˇc, 2003</marker>
<rawString>Jan Hajiˇc, Jarmila Panevov´a, Zdeˇnka Ureˇsov´a, Alevtina B´emov´a, Veronika Kol´aˇrov´a-ˇRezniˇckov´a‘, and Petr Pajas. 2003. PDT-VALLEX: Creating a Largecoverage Valency Lexicon for Treebank Annotation. In J. Nivre and E. Hinrichs, editors, Proceedings of The Second Workshop on Treebanks and Linguistic Theories, pages 57–68, Vaxjo, Sweden. Vaxjo University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Jarmila Panevov´a</author>
<author>Eva Hajiˇcov´a</author>
<author>Petr Sgall</author>
<author>Petr Pajas</author>
<author>Jan ˇStˇep´anek</author>
<author>Jiˇr´ı Havelka</author>
<author>Marie Mikulov´a</author>
<author>Zdenˇek ˇZabokrtsk´y</author>
</authors>
<date>2006</date>
<journal>Prague Dependency Treebank</journal>
<volume>2</volume>
<marker>Hajiˇc, Panevov´a, Hajiˇcov´a, Sgall, Pajas, ˇStˇep´anek, Havelka, Mikulov´a, ˇZabokrtsk´y, 2006</marker>
<rawString>Jan Hajiˇc, Jarmila Panevov´a, Eva Hajiˇcov´a, Petr Sgall, Petr Pajas, Jan ˇStˇep´anek, Jiˇr´ı Havelka, Marie Mikulov´a, and Zdenˇek ˇZabokrtsk´y. 2006. Prague Dependency Treebank 2.0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
<author>Kˆoiti Hasida</author>
</authors>
<title>Construction of a Japanese relevance-tagged corpus.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC-2002),</booktitle>
<pages>2008--2013</pages>
<location>Las Palmas, Canary Islands.</location>
<contexts>
<context position="17824" citStr="Kawahara et al., 2002" startWordPosition="2854" endWordPosition="2857">ocesses of the conversion of the existing treebanks in the seven languages of the CoNLL-2009 shared task. In many instances, the original treebanks had to be not only converted format-wise, but also merged with other resources in order to generate useful training and testing data that fit the task description. 3.1 The Input Corpora The data used as the input for the transformations aimed at arriving at the data contents and format described in Sect. 2.3 are described in (Taul´e et al., 2008), (Xue and Palmer, 2009), (Hajiˇc et al., 2006), (Surdeanu et al., 2008), (Burchardt et al., 2006) and (Kawahara et al., 2002). In the subsequent sections, the procedures for the data conversion for the individual languages are described. The data has been collected by the main organization site and checked for format errors, and repackaged for distribution. There were three packages of the data distributed to the participants: Trial, Training plus Development, and Evaluation. The Trial data were rather small, just to give the feeling of the format and languages involved. A visual representation of the Trial data was also created to make understanding of the data easier. Any data in the same format can be transformed</context>
<context position="46392" citStr="Kawahara et al., 2002" startWordPosition="7456" endWordPosition="7459">l predicates in each sentence are annotated (cf. Table 2). 12 version of person names and coordinations) were made to achieve better consistency with datasets of other languages. Since the TIGER annotation allows non-contiguous constituents, the resulting dependencies can be non-projective. Secondary edges were discarded in the conversion. As for the automatically constructed features, we used TreeTagger (Schmid, 1994) to produce the PLEMMA and PPOS columns, and the Morphisto morphology (Zielinski and Simon, 2008) for PFEAT. 3.7 Japanese For Japanese, we used the Kyoto University Text Corpus (Kawahara et al., 2002), which consists of approximately 40k sentences taken from Mainichi Newspapers. Out of them, approximately 5k sentences are annotated with syntactic and semantic dependencies, and are used the training, development and test data of this year’s shared task. The remaining sentences, which are annotated with only syntactic dependencies, are provided for the training corpus of syntactic dependency parsers. This corpus adopts a dependency structure representation, and thus the conversion to the CoNLL2009 format was relatively straightforward. However, since the original dependencies are annotated o</context>
</contexts>
<marker>Kawahara, Kurohashi, Hasida, 2002</marker>
<rawString>Daisuke Kawahara, Sadao Kurohashi, and Kˆoiti Hasida. 2002. Construction of a Japanese relevance-tagged corpus. In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC-2002), pages 2008–2013, Las Palmas, Canary Islands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Lluis</author>
<author>Stefan Bott</author>
<author>Lluis M`arquez</author>
</authors>
<title>A second-order joint eisner model for syntactic and semantic dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009),</booktitle>
<pages>4--5</pages>
<location>Boulder, Colorado, USA.</location>
<marker>Lluis, Bott, M`arquez, 2009</marker>
<rawString>Xavier Lluis, Stefan Bott, and Lluis M`arquez. 2009. A second-order joint eisner model for syntactic and semantic dependency parsing. In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009), June 4-5, Boulder, Colorado, USA. June 4-5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="40960" citStr="Marcus et al., 1994" startWordPosition="6599" endWordPosition="6602">r simplicity, the extended lexicon was not provided; instead, such words were not marked as predicates in the OOD data (their FILLPRED was set to ‘_’) and thus not evaluated. 3.5 English The English corpus is almost identical to the corpus used in the closed challenge in the CoNLL-2008 shared task evaluation (Surdeanu et al., 2008). This corpus was generated through a process that merges several input corpora and converts them from the constituent-based formalism to dependencies. The following corpora were used as input to the merging procedure: • Penn Treebank 3 – The Penn Treebank 3 corpus (Marcus et al., 1994) consists of handcoded parses of the Wall Street Journal (test, development and training) and a small subset of the Brown corpus (W. N. Francis and H. Kucera, 1964) (test only). • BBN Pronoun Coreference and Entity Type Corpus – BBN’s NE annotation of the Wall Street Journal corpus (Weischedel and Brunstein, 2005) takes the form of SGML inline markup of text, tokenized to be completely compatible with the Penn Treebank annotation. For the CoNLL-2008 shared task evaluation, this corpus was extended by the task organizers to cover the subset of the Brown corpus used as a secondary testing datase</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1994. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lluis M`arquez</author>
<author>Luis Villarejo</author>
<author>M Ant`onia Marti</author>
<author>Mariona Taul´e</author>
</authors>
<title>SemEval-2007 Task 09: Multilevel semantic annotation of catalan and spanish.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>42--47</pages>
<location>Prague, Czech Republic.</location>
<marker>M`arquez, Villarejo, Marti, Taul´e, 2007</marker>
<rawString>Lluis M`arquez, Luis Villarejo, M. Ant`onia Marti, and Mariona Taul´e. 2007. SemEval-2007 Task 09: Multilevel semantic annotation of catalan and spanish. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 42–47, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ant`onia Marti</author>
<author>Mariona Taul´e</author>
<author>Lluis M`arquez</author>
<author>Manu Bertran</author>
</authors>
<title>Anotaci´on semiautom´atica con papeles tem´aticos de los corpus CESS-ECE.</title>
<date>2007</date>
<booktitle>Procesamiento del Lenguaje Natural, SEPLNJournal,</booktitle>
<pages>38--67</pages>
<marker>Marti, Taul´e, M`arquez, Bertran, 2007</marker>
<rawString>M. Ant`onia Marti, Mariona Taul´e, Lluis M`arquez, and Manu Bertran. 2007. Anotaci´on semiautom´atica con papeles tem´aticos de los corpus CESS-ECE. Procesamiento del Lenguaje Natural, SEPLNJournal, 38:67–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Jan Hajiˇc</author>
<author>Kiril Ribarov</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algortihms.</title>
<date>2005</date>
<booktitle>In Proceedings of NAACL-HLT’05,</booktitle>
<pages>523--530</pages>
<location>Vancouver, Canada,</location>
<marker>McDonald, Pereira, Hajiˇc, Ribarov, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Jan Hajiˇc, and Kiril Ribarov. 2005. Non-projective dependency parsing using spanning tree algortihms. In Proceedings of NAACL-HLT’05, Vancouver, Canada, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Meyers</author>
<author>R Reeves</author>
<author>C Macleod</author>
<author>R Szekely</author>
<author>V Zielinska</author>
<author>B Young</author>
<author>R Grishman</author>
</authors>
<title>The NomBank Project: An Interim Report.</title>
<date>2004</date>
<booktitle>In NAACL/HLT 2004 Workshop Frontiers in Corpus Annotation,</booktitle>
<location>Boston.</location>
<contexts>
<context position="42264" citStr="Meyers et al., 2004" startWordPosition="6818" endWordPosition="6821">n NE tokens, e.g., we create a NAME dependency from Mary to Smith given the NE mention Mary Smith. • Proposition Bank I (PropBank) – The PropBank annotation (Palmer et al., 2005) classifies the arguments of all the main verbs in the Penn Treebank corpus, other than be. Arguments are numbered (Arg0, Arg1, ...) based on lexical entries or frame files. Different sets of arguments are assumed for different rolesets. Dependent constituents that fall into categories independent of the lexical entries are classified as various types of adjuncts (ArgM-TMP, -ADV, etc.). • NomBank – NomBank annotation (Meyers et al., 2004) uses essentially the same framework as PropBank to annotate arguments of nouns. Differences between PropBank and NomBank stem from differences between noun and verb argument structure; differences in treatment of nouns and verbs in the Penn Treebank; and differences in the sophistication of previous research about noun and verb argument structure. Only the subset of nouns that take arguments are annotated in NomBank and only a subset of the non-argument siblings of nouns are marked as ArgM. The complete merging process and the conversion from the constituent representation to dependencies is </context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielinska, B. Young, and R. Grishman. 2004. The NomBank Project: An Interim Report. In NAACL/HLT 2004 Workshop Frontiers in Corpus Annotation, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Vincent Van Asch</author>
<author>Antal van den Bosch</author>
</authors>
<title>A simple generative pipeline approach to dependency parsing and semantic role labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009),</booktitle>
<pages>4--5</pages>
<location>Boulder, Colorado, USA.</location>
<marker>Morante, Van Asch, van den Bosch, 2009</marker>
<rawString>Roser Morante, Vincent Van Asch, and Antal van den Bosch. 2009. A simple generative pipeline approach to dependency parsing and semantic role labeling. In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009), Boulder, Colorado, USA. June 4-5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johann Hall</author>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>The conll</title>
<date>2007</date>
<booktitle>In Proceedings of the EMNLP-CoNLL 2007 Conference,</booktitle>
<pages>915--932</pages>
<location>Prague, Czech Republic.</location>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johann Hall, Sandra K¨ubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The conll 2007 shared task on dependency parsing. In Proceedings of the EMNLP-CoNLL 2007 Conference, pages 915–932, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pado</author>
<author>Mirella Lapata</author>
</authors>
<title>Cross-lingual projection of role-semantic information.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP-2005),</booktitle>
<pages>859--866</pages>
<location>Vancouver, BC.</location>
<contexts>
<context position="45133" citStr="Pado and Lapata, 2005" startWordPosition="7262" endWordPosition="7265">y, adding further manual labels where necessary. Then, we used frequency and grammatical realization information to map the remaining roles onto higher-numbered Arg roles. We considerably simplified the annotations provided by SALSA, which use a rather complex annotation scheme. In particular, we removed annotation for multi-word expressions (which may be non-contiguous), annotations involving multiple frames for the same predicate (metaphors, underspecification), and inter-sentence roles. The out-of-domain dataset was taken from a study on the multi-lingual projection of FrameNet annotation (Pado and Lapata, 2005). It is sampled from the EUROPARL corpus and was chosen to maximize the lexical coverage, i.e., it contains of a large number of infrequent predicates. Both syntactic and semantic structure were annotated manually, in the TIGER and SALSA format, respectively. Since it uses a simplified annotation schemes, we did not have to discard any annotation. For both datasets, we converted the syntactic TIGER (Brants et al., 2002) representations into dependencies with a similar set of head-finding rules used for the preparation of the CoNLL-X shared task German dataset. Minor modifications (for the con1</context>
</contexts>
<marker>Pado, Lapata, 2005</marker>
<rawString>Sebastian Pado and Mirella Lapata. 2005. Cross-lingual projection of role-semantic information. In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP-2005), pages 859– 866, Vancouver, BC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Pajas</author>
<author>Jan ˇStˇep´anek</author>
</authors>
<title>Recent advances in a feature-rich framework for treebank annotation.</title>
<date>2008</date>
<booktitle>In The 22nd International Conference on Computational Linguistics - Proceedings of the Conference (COLING’08),</booktitle>
<pages>673--680</pages>
<location>Manchester.</location>
<marker>Pajas, ˇStˇep´anek, 2008</marker>
<rawString>Petr Pajas and Jan ˇStˇep´anek. 2008. Recent advances in a feature-rich framework for treebank annotation. In The 22nd International Conference on Computational Linguistics - Proceedings of the Conference (COLING’08), pages 673–680, Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The Proposition Bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="41822" citStr="Palmer et al., 2005" startWordPosition="6747" endWordPosition="6750">tation of the Wall Street Journal corpus (Weischedel and Brunstein, 2005) takes the form of SGML inline markup of text, tokenized to be completely compatible with the Penn Treebank annotation. For the CoNLL-2008 shared task evaluation, this corpus was extended by the task organizers to cover the subset of the Brown corpus used as a secondary testing dataset. From this corpus we only used NE boundaries to derive NAME 11 dependencies between NE tokens, e.g., we create a NAME dependency from Mary to Smith given the NE mention Mary Smith. • Proposition Bank I (PropBank) – The PropBank annotation (Palmer et al., 2005) classifies the arguments of all the main verbs in the Penn Treebank corpus, other than be. Arguments are numbered (Arg0, Arg1, ...) based on lexical entries or frame files. Different sets of arguments are assumed for different rolesets. Dependent constituents that fall into categories independent of the lexical entries are classified as various types of adjuncts (ArgM-TMP, -ADV, etc.). • NomBank – NomBank annotation (Meyers et al., 2004) uses essentially the same framework as PropBank to annotate arguments of nouns. Differences between PropBank and NomBank stem from differences between noun a</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>M. Palmer, D. Gildea, and P. Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of International Conference on New Methods in Language Processing.</booktitle>
<contexts>
<context position="46192" citStr="Schmid, 1994" startWordPosition="7425" endWordPosition="7426">ependencies with a similar set of head-finding rules used for the preparation of the CoNLL-X shared task German dataset. Minor modifications (for the con15Note, however, that typically not all predicates in each sentence are annotated (cf. Table 2). 12 version of person names and coordinations) were made to achieve better consistency with datasets of other languages. Since the TIGER annotation allows non-contiguous constituents, the resulting dependencies can be non-projective. Secondary edges were discarded in the conversion. As for the automatically constructed features, we used TreeTagger (Schmid, 1994) to produce the PLEMMA and PPOS columns, and the Morphisto morphology (Zielinski and Simon, 2008) for PFEAT. 3.7 Japanese For Japanese, we used the Kyoto University Text Corpus (Kawahara et al., 2002), which consists of approximately 40k sentences taken from Mainichi Newspapers. Out of them, approximately 5k sentences are annotated with syntactic and semantic dependencies, and are used the training, development and test data of this year’s shared task. The remaining sentences, which are annotated with only syntactic dependencies, are provided for the training corpus of syntactic dependency par</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of International Conference on New Methods in Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Drahomira ”Johanka” Spoustov´a</author>
<author>Jan Hajiˇc</author>
<author>Jan Raab</author>
<author>Miroslav Spousta</author>
</authors>
<title>Semi-supervised training for the averaged perceptron POS tagger.</title>
<date>2009</date>
<booktitle>In Proceedings of the European ACL Cenference EACL’09,</booktitle>
<location>Athens, Greece.</location>
<marker>Spoustov´a, Hajiˇc, Raab, Spousta, 2009</marker>
<rawString>Drahomira ”Johanka” Spoustov´a, Jan Hajiˇc, Jan Raab, and Miroslav Spousta. 2009. Semi-supervised training for the averaged perceptron POS tagger. In Proceedings of the European ACL Cenference EACL’09, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Lluis M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The CoNLL2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008),</booktitle>
<pages>159--177</pages>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Lluis M`arquez, and Joakim Nivre. 2008. The CoNLL2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008), pages 159–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mariona Taul´e</author>
<author>Maria Ant`onia Marti</author>
<author>Marta Recasens</author>
</authors>
<title>AnCora: Multilevel Annotated Corpora for Catalan and Spanish.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC-2008),</booktitle>
<location>Marrakesh, Morroco.</location>
<marker>Taul´e, Marti, Recasens, 2008</marker>
<rawString>Mariona Taul´e, Maria Ant`onia Marti, and Marta Recasens. 2008. AnCora: Multilevel Annotated Corpora for Catalan and Spanish. In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC-2008), Marrakesh, Morroco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin ˇCmejrek</author>
<author>Jan Cuˇrin</author>
<author>Jan Hajiˇc</author>
<author>Jiˇr´ı Havelka</author>
<author>Vladislav Kuboˇn</author>
</authors>
<title>Prague Czech-English Dependency Treebank: Syntactically Anntoated Resources for Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC-2004),</booktitle>
<pages>1597--1600</pages>
<location>Lisbon, Portugal.</location>
<marker>ˇCmejrek, Cuˇrin, Hajiˇc, Havelka, Kuboˇn, 2004</marker>
<rawString>Martin ˇCmejrek, Jan Cuˇrin, Jan Hajiˇc, Jiˇr´ı Havelka, and Vladislav Kuboˇn. 2004. Prague Czech-English Dependency Treebank: Syntactically Anntoated Resources for Machine Translation. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC-2004), pages 1597– 1600, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W N Francis</author>
<author>H Kucera</author>
</authors>
<title>Brown Corpus Manual of Information to accompany A Standard Corpus of Present-Day Edited American English, for use with Digital Computers. Revised 1971, Revised and Amplified 1979, available at www.clarinet/brown.</title>
<date>1964</date>
<marker>Francis, Kucera, 1964</marker>
<rawString>W. N. Francis and H. Kucera. 1964. Brown Corpus Manual of Information to accompany A Standard Corpus of Present-Day Edited American English, for use with Digital Computers. Revised 1971, Revised and Amplified 1979, available at www.clarinet/brown.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Weischedel</author>
<author>A Brunstein</author>
</authors>
<title>BBN pronoun coreference and entity type corpus.</title>
<date>2005</date>
<tech>Technical report, Lin- guistic Data Consortium.</tech>
<contexts>
<context position="41275" citStr="Weischedel and Brunstein, 2005" startWordPosition="6652" endWordPosition="6656">ation (Surdeanu et al., 2008). This corpus was generated through a process that merges several input corpora and converts them from the constituent-based formalism to dependencies. The following corpora were used as input to the merging procedure: • Penn Treebank 3 – The Penn Treebank 3 corpus (Marcus et al., 1994) consists of handcoded parses of the Wall Street Journal (test, development and training) and a small subset of the Brown corpus (W. N. Francis and H. Kucera, 1964) (test only). • BBN Pronoun Coreference and Entity Type Corpus – BBN’s NE annotation of the Wall Street Journal corpus (Weischedel and Brunstein, 2005) takes the form of SGML inline markup of text, tokenized to be completely compatible with the Penn Treebank annotation. For the CoNLL-2008 shared task evaluation, this corpus was extended by the task organizers to cover the subset of the Brown corpus used as a secondary testing dataset. From this corpus we only used NE boundaries to derive NAME 11 dependencies between NE tokens, e.g., we create a NAME dependency from Mary to Smith given the NE mention Mary Smith. • Proposition Bank I (PropBank) – The PropBank annotation (Palmer et al., 2005) classifies the arguments of all the main verbs in th</context>
</contexts>
<marker>Weischedel, Brunstein, 2005</marker>
<rawString>R. Weischedel and A. Brunstein. 2005. BBN pronoun coreference and entity type corpus. Technical report, Lin- guistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Adding semantic roles to the Chinese Treebank.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<volume>15</volume>
<issue>1</issue>
<contexts>
<context position="17722" citStr="Xue and Palmer, 2009" startWordPosition="2837" endWordPosition="2840">rmats for the various languages appeared to be a challenge in itself. We will briefly describe the processes of the conversion of the existing treebanks in the seven languages of the CoNLL-2009 shared task. In many instances, the original treebanks had to be not only converted format-wise, but also merged with other resources in order to generate useful training and testing data that fit the task description. 3.1 The Input Corpora The data used as the input for the transformations aimed at arriving at the data contents and format described in Sect. 2.3 are described in (Taul´e et al., 2008), (Xue and Palmer, 2009), (Hajiˇc et al., 2006), (Surdeanu et al., 2008), (Burchardt et al., 2006) and (Kawahara et al., 2002). In the subsequent sections, the procedures for the data conversion for the individual languages are described. The data has been collected by the main organization site and checked for format errors, and repackaged for distribution. There were three packages of the data distributed to the participants: Trial, Training plus Development, and Evaluation. The Trial data were rather small, just to give the feeling of the format and languages involved. A visual representation of the Trial data was</context>
<context position="33151" citStr="Xue and Palmer, 2009" startWordPosition="5356" endWordPosition="5359">unts for elliptical pronouns (there are marked as empty lexical tokens ‘_’ with a pronoun POS tag). Finally, the predicted columns (PLEMMA, PPOS, and PFEAT) have been generated with the FreeLing Open source suite of Language Analyzers10. Accuracy in PLEMMA and PPOS columns is above 95% for the two languages. PHEAD and PDEPREL columns have been generated using MaltParser11. Parsing accuracy (LAS) is above 86% for the the two languages. 3.3 Chinese The Chinese Corpus for the 2009 CoNLL Shared Task was generated by merging the Chinese Treebank (Xue et al., 2005) and the Chinese Proposition Bank (Xue and Palmer, 2009) and then converting the constituent structure to a dependency formalism as specified in the CoNLL Shared Task. The Chinese data used in the shared task is based on Chinese Treebank 6.0 and the Chinese Proposition Bank 2.0, both of which are publicly available via the Linguistic Data Consortium. The Chinese Treebank Project originated at Penn and was later moved to University of Colorado at 10http://www.lsi.upc.es/—nlp/freeling 11http://w3.msi.vxu.se/—jha/maltparser 9 Boulder. Now it is the process of being to moved to Brandeis University. The data sources of the Chinese Treebank range from Xi</context>
</contexts>
<marker>Xue, Palmer, 2009</marker>
<rawString>Nianwen Xue and Martha Palmer. 2009. Adding semantic roles to the Chinese Treebank. Natural Language Engineering, 15(1):143–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The Penn Chinese TreeBank: Phrase Structure Annotation of a Large Corpus.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="33095" citStr="Xue et al., 2005" startWordPosition="5347" endWordPosition="5350">are grouped together; and (6) segmentation also accounts for elliptical pronouns (there are marked as empty lexical tokens ‘_’ with a pronoun POS tag). Finally, the predicted columns (PLEMMA, PPOS, and PFEAT) have been generated with the FreeLing Open source suite of Language Analyzers10. Accuracy in PLEMMA and PPOS columns is above 95% for the two languages. PHEAD and PDEPREL columns have been generated using MaltParser11. Parsing accuracy (LAS) is above 86% for the the two languages. 3.3 Chinese The Chinese Corpus for the 2009 CoNLL Shared Task was generated by merging the Chinese Treebank (Xue et al., 2005) and the Chinese Proposition Bank (Xue and Palmer, 2009) and then converting the constituent structure to a dependency formalism as specified in the CoNLL Shared Task. The Chinese data used in the shared task is based on Chinese Treebank 6.0 and the Chinese Proposition Bank 2.0, both of which are publicly available via the Linguistic Data Consortium. The Chinese Treebank Project originated at Penn and was later moved to University of Colorado at 10http://www.lsi.upc.es/—nlp/freeling 11http://w3.msi.vxu.se/—jha/maltparser 9 Boulder. Now it is the process of being to moved to Brandeis University</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Nianwen Xue, Fei Xia, Fu Dong Chiou, and Martha Palmer. 2005. The Penn Chinese TreeBank: Phrase Structure Annotation of a Large Corpus. Natural Language Engineering, 11(2):207–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Zielinski</author>
<author>Christian Simon</author>
</authors>
<title>Morphisto: An open-source morphological analyzer for german.</title>
<date>2008</date>
<booktitle>In Proceedings ofthe Conference on Finite State Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="46289" citStr="Zielinski and Simon, 2008" startWordPosition="7439" endWordPosition="7442"> CoNLL-X shared task German dataset. Minor modifications (for the con15Note, however, that typically not all predicates in each sentence are annotated (cf. Table 2). 12 version of person names and coordinations) were made to achieve better consistency with datasets of other languages. Since the TIGER annotation allows non-contiguous constituents, the resulting dependencies can be non-projective. Secondary edges were discarded in the conversion. As for the automatically constructed features, we used TreeTagger (Schmid, 1994) to produce the PLEMMA and PPOS columns, and the Morphisto morphology (Zielinski and Simon, 2008) for PFEAT. 3.7 Japanese For Japanese, we used the Kyoto University Text Corpus (Kawahara et al., 2002), which consists of approximately 40k sentences taken from Mainichi Newspapers. Out of them, approximately 5k sentences are annotated with syntactic and semantic dependencies, and are used the training, development and test data of this year’s shared task. The remaining sentences, which are annotated with only syntactic dependencies, are provided for the training corpus of syntactic dependency parsers. This corpus adopts a dependency structure representation, and thus the conversion to the Co</context>
</contexts>
<marker>Zielinski, Simon, 2008</marker>
<rawString>Andrea Zielinski and Christian Simon. 2008. Morphisto: An open-source morphological analyzer for german. In Proceedings ofthe Conference on Finite State Methods in Natural Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>