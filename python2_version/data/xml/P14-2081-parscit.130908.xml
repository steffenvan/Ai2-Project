<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006054">
<title confidence="0.991526">
Two-Stage Hashing for Fast Document Retrieval
</title>
<author confidence="0.99574">
Hao Li* Wei Liu† Heng Ji*
</author>
<affiliation confidence="0.9884365">
*Computer Science Department,
Rensselaer Polytechnic Institute, Troy, NY, USA
</affiliation>
<email confidence="0.954078">
{lih13,jih}@rpi.edu
</email>
<note confidence="0.949608">
†IBM T. J. Watson Research Center, Yorktown Heights, NY, USA
</note>
<email confidence="0.992989">
weiliu@us.ibm.com
</email>
<sectionHeader confidence="0.99732" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999961739130435">
This work fulfills sublinear time Near-
est Neighbor Search (NNS) in massive-
scale document collections. The primary
contribution is to propose a two-stage
unsupervised hashing framework which
harmoniously integrates two state-of-the-
art hashing algorithms Locality Sensitive
Hashing (LSH) and Iterative Quantization
(ITQ). LSH accounts for neighbor candi-
date pruning, while ITQ provides an ef-
ficient and effective reranking over the
neighbor pool captured by LSH. Further-
more, the proposed hashing framework
capitalizes on both term and topic similar-
ity among documents, leading to precise
document retrieval. The experimental re-
sults convincingly show that our hashing
based document retrieval approach well
approximates the conventional Informa-
tion Retrieval (IR) method in terms of re-
trieving semantically similar documents,
and meanwhile achieves a speedup of over
one order of magnitude in query time.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9947216">
A Nearest Neighbor Search (NNS) task aims at
searching for top K objects (e.g., documents)
which are most similar, based on pre-defined sim-
ilarity metrics, to a given query object in an ex-
isting dataset. NNS is essential in dealing with
many search related tasks, and also fundamen-
tal to a broad range of Natural Language Pro-
cessing (NLP) down-stream problems including
person name spelling correction (Udupa and Ku-
mar, 2010), document translation pair acquisition
(Krstovski and Smith, 2011), large-scale similar
noun list generation (Ravichandran et al., 2005),
lexical variants mining (Gouws et al., 2011), and
large-scale first story detection (Petrovic et al.,
2010).
Hashing has recently emerged to be a popular
solution to tackling fast NNS, and been success-
fully applied to a variety of non-NLP problems
such as visual object detection (Dean et al., 2013)
and recognition (Torralba et al., 2008a; Torralba
et al., 2008b), large-scale image retrieval (Kulis
and Grauman, 2012; Liu et al., 2012; Gong et al.,
2013), and large-scale machine learning (Weiss et
al., 2008; Liu et al., 2011; Liu, 2012). However,
hashing has received limited attention in the NLP
field to the date. The basic idea of hashing is to
represent each data object as a binary code (each
bit of a code is one digit of “0” or “1”). When
applying hashing to handle NLP problems, the ad-
vantages are two-fold: 1) the capability to store
a large quantity of documents in the main mem-
ory. for example, one can store 250 million doc-
uments with 1.9G memory using only 64 bits for
each document while a large news corpus such as
the English Gigaword fifth edition1 stores 10 mil-
lion documents in a 26G hard drive; 2) the time
efficiency of manipulating binary codes, for ex-
ample, computing the hamming distance between
a pair of binary codes is several orders of magni-
tude faster than computing the real-valued cosine
similarity over a pair of document vectors.
The early explorations of hashing focused on
using random permutations or projections to con-
struct randomized hash functions, e.g., the well-
known Min-wise Hashing (MinHash) (Broder et
al., 1998) and Locality Sensitive Hashing (LSH)
(Andoni and Indyk, 2008). In contrast to such
data-independent hashing schemes, recent re-
search has been geared to studying data-dependent
hashing through learning compact hash codes
from a training dataset. The state-of-the-art unsu-
pervised learning-based hashing methods include
Spectral Hashing (SH) (Weiss et al., 2008), An-
chor Graph Hashing (AGH) (Liu et al., 2011),
and Iterative Quantization (ITQ) (Gong et al.,
</bodyText>
<footnote confidence="0.944506">
1http://catalog.ldc.upenn.edu/LDC2011T07
</footnote>
<page confidence="0.984754">
495
</page>
<bodyText confidence="0.971445615384615">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 495–500,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
2013), all of which endeavor to make the learned
hash codes preserve or reveal some intrinsic struc-
ture, such as local neighborhood structure, low-
dimensional manifolds, or the closest hypercube,
underlying the training data. Despite achieving
data-dependent hash codes, most of these “learn-
ing to hash” methods cannot guarantee a high suc-
cess rate of looking a query code up in a hash ta-
ble (referred to as hash table lookup in literature),
which is critical to the high efficacy of exploit-
ing hashing in practical uses. It is worth noting
that we choose to use ITQ in the proposed two-
stage hashing framework for its simplicity and ef-
ficiency. ITQ has been found to work better than
SH by Gong et al. (2013), and be more efficient
than AGH in terms of training time by Liu (2012).
To this end, in this paper we propose a novel
two-stage unsupervised hashing framework to si-
multaneously enhance the hash lookup success
rate and increase the search accuracy by integrat-
ing the advantages of both LSH and ITQ. Further-
more, we make the hashing framework applicable
to combine different similarity measures in NNS.
2 Background and Terminology
</bodyText>
<listItem confidence="0.997220666666667">
• Binary Codes: A bit (a single bit is “0” or
“1”) sequence assigned to represent a data
object. For example, represent a document
as a 8-bit code “11101010”.
• Hash Table: A linear table in which all bi-
nary codes of a data set are arranged to be
table indexes, and each table bucket contains
the IDs of the data items sharing the same
code.
• Hamming Distance: The number of bit po-
sitions in which bits of the two codes differ.
• Hash Table Lookup: Given a query q with
its binary code hq, find the candidate neigh-
bors in a hash table such that the Hamming
distances from their codes to hq are no more
than a small distance threshold E. In practice
E is usually set to 2 to maintain the efficiency
of table lookups.
• Hash Table Lookup Success Rate: Given a
query q with its binary code hq, the probabil-
ity to find at least one neighbor in the table
buckets whose corresponding codes (i.e., in-
dexes) are within a Hamming ball of radius E
centered at hq.
• Hamming Ranking: Given a query q with
its binary code hq, rank all data items accord-
ing to the Hamming distances between their
</listItem>
<bodyText confidence="0.9314385">
codes and hq; the smaller the Hamming dis-
tance, the higher the data item is ranked.
</bodyText>
<sectionHeader confidence="0.980131" genericHeader="method">
3 Document Retrieval with Hashing
</sectionHeader>
<bodyText confidence="0.999970166666667">
In this section, we first provide an overview of ap-
plying hashing techniques to a document retrieval
task, and then introduce two unsupervised hash-
ing algorithms: LSH acts as a neighbor-candidate
filter, while ITQ works towards precise reranking
over the candidate pool returned by LSH.
</bodyText>
<subsectionHeader confidence="0.999695">
3.1 Document Retrieval
</subsectionHeader>
<bodyText confidence="0.999974">
The most traditional way of retrieving nearest
neighbors for documents is to represent each docu-
ment as a term vector of which each element is the
tf-idf weight of a term. Given a query document
vector q, we use the Cosine similarity measure to
evaluate the similarity between q and a document
x in a dataset:
Then the traditional document retrieval method
exhaustively scans all documents in the dataset
and returns the most similar ones. However, such
a brute-force search does not scale to massive
datasets since the search time complexity for each
query is O(n); additionally, the computational
cost spent on Cosine similarity calculation is also
nontrivial.
</bodyText>
<subsectionHeader confidence="0.999777">
3.2 Locality Sensitive Hashing
</subsectionHeader>
<bodyText confidence="0.99841625">
The core idea of LSH is that if two data points are
close, then after a “projection” operation they will
remain close. In other words, similar data points
are more likely to be mapped into the same bucket
with a high collision probability. In a typical LSH
setting of k bits and L hash tables, a query point
q E Rd and a dataset point x E Rd collide if and
only if
</bodyText>
<equation confidence="0.876604">
hij(q) - hij(x), i E [1 : L], j E [1 : k], (2)
</equation>
<bodyText confidence="0.994188">
where the hash function hij(�) is defined as
</bodyText>
<equation confidence="0.994838">
hij(x) = sgn(w�ijx), (3)
</equation>
<bodyText confidence="0.999904333333333">
in which wij E Rd is a random projection di-
rection with components being independently and
identically drawn from a normal distribution, and
the sign function sgn(x) returns 1 if x &gt; 0 and -1
otherwise. Note that we use “1/-1” bits for deriva-
tions and training, and “1/0” bits for the hashing
</bodyText>
<equation confidence="0.990626">
sim(q, x) = qTx
JJ
qJJJJxJJ. (1)
</equation>
<page confidence="0.988053">
496
</page>
<bodyText confidence="0.999761666666667">
implementation including converting data to bi-
nary codes, arranging binary codes into hash ta-
bles, and hash table lookups.
</bodyText>
<subsectionHeader confidence="0.996867">
3.3 Iterative Quantization
</subsectionHeader>
<bodyText confidence="0.998127375">
The central idea of ITQ is to learn the binary codes
achieving the lowest quantization error that en-
coding raw data to binary codes incurs. This is
pursued by seeking a rotation of the zero-centered
projected data. Suppose that a set of n data points
X = {xi ∈ Rd}ni=1 are provided. The data matrix
is defined as X = [x1, x2, · · · , xn]&gt; ∈ Rn×d.
In order to reduce the data dimension from d to
the desired code length c &lt; d, Principal Compo-
nent Analysis (PCA) or Latent Semantic Analy-
sis (LSA) is first applied to X. We thus obtain
the zero-centered projected data matrix as V =
(I− 1n11&gt;)XU where U ∈ Rd×c is the projec-
tion matrix.
After the projection operation, ITQ minimizes
the quantization error as follows
</bodyText>
<equation confidence="0.872036">
Q(B, R) = kB − VRk2F, (4)
</equation>
<bodyText confidence="0.999755357142857">
where B ∈ {1, −1}n×c is the code matrix each
row of which contains a binary code, R ∈ Rc×c
is the target orthogonal rotation matrix, and k · kF
denotes the Frobenius norm. Finding a local min-
imum of the quantization error in Eq. (4) begins
with a random initialization of R, and then em-
ploys a K-means clustering like iterative proce-
dure. In each iteration, each (projected) data point
is assigned to the nearest vertex of the binary hy-
percube, and R always satisfying RR&gt; = I is
subsequently updated to minimize the quantiza-
tion loss given the current assignment; the two
steps run alternatingly until a convergence is en-
countered. Concretely, the two updating steps are:
</bodyText>
<listItem confidence="0.993101">
1. Fix R and update B: minimize the follow-
ing quantization loss
</listItem>
<equation confidence="0.899846">
Q(B,R) = kBk2F + kVRk2F − 2tr(R&gt;V&gt;B)
= nc + kVk2F − 2tr(R&gt;V&gt;B)
</equation>
<bodyText confidence="0.300647">
= constant − 2tr(R&gt;V&gt;B),
</bodyText>
<listItem confidence="0.8447058">
(5)
achieving B = sgn(VR);
2. Fix B and update R: perform the SVD of
the _matrix V&gt;B ∈ Rc×c to obtain V&gt;B =
SΩ§T, and then set R = S�S&gt;.
</listItem>
<figureCaption confidence="0.998665">
Figure 1: The two-stage hashing framework.
</figureCaption>
<subsectionHeader confidence="0.816469">
3.4 Two-Stage Hashing
</subsectionHeader>
<bodyText confidence="0.999954885714286">
There are three main merits of LSH. (1) It tries to
preserve the Cosine similarity of the original data
with a probabilistic guarantee (Charikar, 2002).
(2) It is training free, and thus very efficient in
hashing massive databases to binary codes. (3) It
has a very high hash table lookup success rate. For
example, in our experiments LSH with more than
one hash table is able to achieve a perfect 100%
hash lookup success rate. Unfortunately, its draw-
back is the low search precision that is observed
even with long hash bits and multiple hash tables.
ITQ tries to minimize the quantization error of
encoding data to binary codes, so its advantage
is the high quality (potentially high precision of
Hamming ranking) of the produced binary codes.
Nevertheless, ITQ frequently suffers from a poor
hash lookup success rate when longer bits (e.g.,
≥ 48) are used (Liu, 2012). For example, in
our experiments ITQ using 384 bits has a 18.47%
hash lookup success rate within Hamming radius
2. Hence, Hamming ranking (costing O(n) time)
must be invoked for the queries for which ITQ
fails to return any neighbors via hash table lookup,
which makes the searches inefficient especially on
very large datasets.
Taking into account the above advantages and
disadvantages of LSH and ITQ, we propose a two-
stage hashing framework to harmoniously inte-
grate them. Fig. 1 illustrates our two-stage frame-
work with a toy example where identical shapes
denote ground-truth nearest neighbors.
In this framework, LSH accounts for neigh-
bor candidate pruning, while ITQ provides an ef-
ficient and effective reranking over the neighbor
pool captured by LSH. To be specific, the pro-
</bodyText>
<page confidence="0.992696">
497
</page>
<bodyText confidence="0.868265">
posed framework enjoys two advantages:
</bodyText>
<listItem confidence="0.9723426">
1. Provide a simple solution to accomplish both
a high hash lookup success rate and high precision,
which does not require scanning the whole list of
the ITQ binary codes but scanning the short list
returned by LSH hash table lookup. Therefore, a
high hash lookup success rate is attained by the
LSH stage, while maintaining high search preci-
sion due to the ITQ reranking stage.
2. Enable a hybrid hashing scheme combining
two similarity measures. The term similarity is
</listItem>
<bodyText confidence="0.907605272727273">
used during the LSH stage that directly works
on document tf-idf vectors; during the ITQ stage,
the topic similarity is used since ITQ works on
the topic vectors obtained by applying Latent se-
mantic analysis (LSA) (Deerwester et al., 1990)
to those document vectors. LSA (or PCA), the
first step in running ITQ, can be easily acceler-
ated via a simple sub-selective sampling strategy
which has been proven theoretically and empiri-
cally sound by Li et al. (2014). As a result, the
nearest neighbors returned by the two-stage hash-
ing framework turns out to be both lexically and
topically similar to the query document. To sum-
marize, the proposed two-stage hashing frame-
work works in an unsupervised manner, achieves a
sublinear search time complexity due to LSH, and
attains high search precision thanks to ITQ. After
hashing all data (documents) to LSH and ITQ bi-
nary codes, we do not need to save the raw data in
memory. Thus, our approach can scale to gigan-
tic datasets with compact storage and fast search
speed.
</bodyText>
<sectionHeader confidence="0.999935" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.972297">
Data and Evaluations
</subsectionHeader>
<bodyText confidence="0.999962333333333">
For the experiments, we use the English portion
of the standard TDT-5 dataset, which consists of
278,109 documents from a time spanning April
2003 to September 2003. 126 topics are anno-
tated with an average of 51 documents per topic,
and other unlabeled documents are irrelevant to
them. We select six largest topics for the top-K
NNS evaluation, with each including more than
250 documents. We randomly select 60 docu-
ments from each of the six topics for testing. The
six topics are (1). Bombing in Riyadh, Saudi Ara-
bia (2). Mad cow disease in North America (3).
Casablanca bombs (4). Swedish Foreign Minister
killed (5). Liberian former president arrives in ex-
ile and (6). UN official killed in attack. For each
document, we apply the Stanford Tokenizer 2 for
tokenization; remove stopwords based on the stop
list from InQuery (Callan et al., 1992), and apply
Porter Stemmer (Porter, 1980) for stemming.
If one retrieved document shares the same topic
label with the query document, they are true neigh-
bors. We evaluate the precision of the top-K candi-
date documents returned by each method and cal-
culate the average precision across all queries.
</bodyText>
<sectionHeader confidence="0.902595" genericHeader="evaluation">
Results
</sectionHeader>
<bodyText confidence="0.999852375">
We first evaluate the quality of term vectors and
ITQ binary codes by conducting the whole list
Cosine similarity ranking and hamming distance
ranking, respectively. For each query document,
the top-K candidate documents with highest Co-
sine similarity scores and shortest hamming dis-
tances are returned, then we calculate the average
precision for each K. Fig. 2(a) demonstrates that
ITQ binary codes could preserve document simi-
larities as traditional term vectors. It is interesting
to notice that ITQ binary codes are able to outper-
form traditional term vectors. It is mainly because
some documents are topically related but share
few terms thus their relatedness can be captured by
LSA. Fig. 2(a) also shows that the NNS precision
keep increasing as longer ITQ code length is used
and is converged when ITQ code length equals to
384 bits. Therefore we set ITQ code length as 384
bits in the rest of the experiments.
Fig. 2(b) - Fig. 2(e) show that our two-stage
hashing framework surpasses LSH with a large
margin for both small K (e.g., K ≤ 10) and
large K (e.g., K ≥ 100) in top-K NNS. It also
demonstrates that our hashing based document re-
trieval approach with only binary codes from LSH
and ITQ well approximates the conventional IR
method. Another crucial observation is that with
ITQ reranking, a small number of LSH hash ta-
bles is needed in the pruning step. For example,
LSH(40bits) + ITQ(384bits) and LSH(48bits) +
ITQ(384bits) are able to reach convergence with
only four LSH hash tables. In that case, we can
alleviate one main drawback of LSH as it usually
requires a large number of hash tables to maintain
the hashing quality.
Since the LSH pruning time can be ignored,
the search time of the two-stage hashing scheme
equals to the time of hamming distance rerank-
ing in ITQ codes for all candidates produced from
LSH pruning step, e.g., LSH(48bits, 4 tables) +
</bodyText>
<footnote confidence="0.950463">
2http://nlp.stanford.edu/software/corenlp.shtml
</footnote>
<page confidence="0.995499">
498
</page>
<figureCaption confidence="0.972495">
Figure 2: (a) ITQ code quality for different code length, (b) LSH Top-10 Precision, (c) LSH +
ITQ(384bits) Top-10 Precision, (d) LSH Top-100 Precision, (e) LSH + ITQ(384bits) Top-100 Precision,
(f) The percentage of visited data samples by LSH hash lookup.
</figureCaption>
<figure confidence="0.999917009345794">
0.95
0.9
0.85
Precision
0.8
0.75
0.7
0.65
(a)
Traditional IR
ITQ(448bits)
ITQ(384bits)
ITQ(320bits)
ITQ(256bits)
ITQ(192bits)
0 50 100 150
Top−100 Precision
(d)
number of top−K returned documents
0.18
0.16
0.14
0.12
0.08
0.06
0.04
0.02
0.2
0.1
0
1 2 3 4 5 6 7 8 9 1
LSH(64bits)
LSH(56bits)
LSH(48bits)
LSH(40bits)
number of hash tables
(b)
0.5
LSH(64bits)
LSH(56bits)
LSH(48bits)
LSH(40bits)
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
1 2 3 4 5 6 7 8 910
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
number of hash tables
(c)
0.8
0.75
0.7
0.65
0.6
0.55
0.5
0.45
1 2 3 4 5 6 7 8 9 1
0.9
LSH(40bits)
LSH(48bits)
LSH(56bits)
LSH(64bits)
0.6
0.5
0.4
0.3
0.2
0.1
0
1 2 3 4 5 6 7 8 9 1
number of hash tables
Tcp−10 Precisicn
Tcp−10 Precisicn
0.45
0.4
Top−100 Precision
Traditional IR
LSH(40bits)+ITQ(384bits)
LSH(48bits)+ITQ(384bits)
LSH(56bits)+ITQ(384bits)
LSH(64bits)+ITQ(384bits)
1 2 3 4 5 6 7 8 910
Traditional IR
LSH(40bits)+ITQ(384bits)
LSH(48bits)+ITQ(384bits)
LSH(56bits)+ITQ(384bits)
LSH(64bits)+ITQ(384bits)
Percentage
0.8
0.7
number of hash tables
number of hash tables
(e)
(f)
</figure>
<bodyText confidence="0.989187608695652">
ITQ(384bits) takes only one thirtieth of the search
time as the traditional IR method. Fig. 2 (f)
shows the ITQ data reranking percentage for dif-
ferent LSH bit lengths and table numbers. As the
LSH bit length increases or the hash table num-
ber decreases, a lower percentage of the candidates
will be selected for reranking, and thus costs less
search time.
The percentage of visited data samples by LSH
hash lookup is a key factor that influence the
NNS precision in the two-stage hashing frame-
work. Generally, higher rerank percentage results
in better top-K NNS Precision. Further more, by
comparing Fig. 2 (c) and (e), it shows that our
framework works better for small K than for large
K. For example, scanning 5.52% of the data is
enough for achieving similar top-10 NNS result
as the traditional IR method while 36.86% of the
data is needed for top-100 NNS. The reason of the
lower performance with large K is that some true
neighbors with the same topic label do not share
high term similarities and may be filtered out in
the LSH step when the rerank percentage is low.
</bodyText>
<sectionHeader confidence="0.999542" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999974833333333">
In this paper, we proposed a novel two-stage un-
supervised hashing framework for efficient and ef-
fective nearest neighbor search in massive docu-
ment collections. The experimental results have
shown that this framework achieves not only com-
parable search accuracy with the traditional IR
method in retrieving semantically similar docu-
ments, but also an order of magnitude speedup in
search time. Moreover, our approach can com-
bine two similarity measures in a hybrid hashing
scheme, which is beneficial to comprehensively
modeling the document similarity. In our future
work, we plan to design better data representa-
tion which can well fit into the two-stage hash-
ing theme; we also intend to apply the proposed
hashing approach to more informal genres (e.g.,
tweets) and other down-stream NLP applications
(e.g., first story detection).
</bodyText>
<sectionHeader confidence="0.996491" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999952545454545">
This work was supported by the U.S. ARL
No. W911NF-09-2-0053 (NSCTA), NSF IIS-
0953149, DARPA No. FA8750- 13-2-0041, IBM,
Google and RPI. The views and conclusions con-
tained in this document are those of the authors
and should not be interpreted as representing the
official policies, either expressed or implied, of the
U.S. Government. The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernment purposes notwithstanding any copyright
notation here on.
</bodyText>
<sectionHeader confidence="0.998367" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999698671428571">
A. Andoni and P. Indyk. 2008. Near-optimal hash-
ing algorithms for approximate nearest neighbor in
high dimensions. Communications of the ACM,
51(1):117–122.
A. Z. Broder, M. Charikar, A. M. Frieze, and
M. Mitzenmacher. 1998. Min-wise independent
permutations. In Proc. STOC.
J. P. Callan, W. B. Croft, and S. M. Harding. 1992. The
inquery retrieval system. In Proc. the Third Interna-
tional Conference on Database and Expert Systems
Applications.
M. Charikar. 2002. Similarity estimation techniques
from rounding algorithms. In Proc. STOC.
T. Dean, M. A. Ruzon, M. Segal, J. Shlens, S. Vijaya-
narasimhan, and J. Yagnik. 2013. Fast, accurate
detection of 100,000 object classes on a single ma-
chine. In Proc. CVPR.
S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. 1990. Indexing by
latent semantic analysis. JASIS, 41(6):391–407.
Y. Gong, S. Lazebnik, A. Gordo, and F. Perronnin.
2013. Iterative quantization: A procrustean ap-
proach to learning binary codes for large-scale im-
age retrieval. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 35(12):2916–2929.
S. Gouws, D. Hovy, and D. Metzler. 2011. Unsuper-
vised mining of lexical variants from noisy text. In
Proc. EMNLP.
K. Krstovski and D. A. Smith. 2011. A minimally su-
pervised approach for detecting and ranking docu-
ment translation pairs. In Proc. the sixth ACL Work-
shop on Statistical Machine Translation.
B. Kulis and K. Grauman. 2012. Kernelized locality-
sensitive hashing. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 34(6):1092–
1104.
Y. Li, C. Chen, W. Liu, and J. Huang. 2014. Sub-
selective quantization for large-scale image search.
In Proc. AAAI Conference on Artificial Intelligence
(AAAI).
W. Liu, J. Wang, S. Kumar, and S.-F. Chang. 2011.
Hashing with graphs. In Proc. ICML.
W. Liu, J. Wang, R. Ji, Y.-G. Jiang, and S.-F. Chang.
2012. Supervised hashing with kernels. In Proc.
CVPR.
W. Liu. 2012. Large-scale machine learning for classi-
fication and search. In PhD Thesis, Graduate School
of Arts and Sciences, Columbia University.
S. Petrovic, M. Osborne, and V. Lavrenko. 2010.
Streaming first story detection with application to
twitter. In Proc. HLT-NAACL.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130–137.
D. Ravichandran, P. Pantel, and E. H. Hovy. 2005.
Randomized algorithms and nlp: Using locality sen-
sitive hash functions for high speed noun clustering.
In Proc. ACL.
A. Torralba, R. Fergus, and W. T. Freeman. 2008a. 80
million tiny images: A large data set for nonpara-
metric object and scene recognition. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
30(11):1958–1970.
A. Torralba, R. Fergus, and Y. Weiss. 2008b. Small
codes and large image databases for recognition. In
Proc. CVPR.
R. Udupa and S. Kumar. 2010. Hashing-based ap-
proaches to spelling correction of personal names.
In Proc. EMNLP.
Y. Weiss, A. Torralba, and R. Fergus. 2008. Spectral
hashing. In NIPS 21.
</reference>
<page confidence="0.995607">
500
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.448638">
<title confidence="0.996831">Two-Stage Hashing for Fast Document Retrieval</title>
<author confidence="0.983449">Wei</author>
<affiliation confidence="0.7831175">Science Rensselaer Polytechnic Institute, Troy, NY,</affiliation>
<author confidence="0.86423">T J Watson Research Center</author>
<author confidence="0.86423">Yorktown Heights</author>
<author confidence="0.86423">NY</author>
<email confidence="0.999856">weiliu@us.ibm.com</email>
<abstract confidence="0.991837041666667">This work fulfills sublinear time Nearest Neighbor Search (NNS) in massivescale document collections. The primary contribution is to propose a two-stage unsupervised hashing framework which harmoniously integrates two state-of-theart hashing algorithms Locality Sensitive Hashing (LSH) and Iterative Quantization (ITQ). LSH accounts for neighbor candidate pruning, while ITQ provides an efficient and effective reranking over the neighbor pool captured by LSH. Furthermore, the proposed hashing framework capitalizes on both term and topic similarity among documents, leading to precise document retrieval. The experimental results convincingly show that our hashing based document retrieval approach well approximates the conventional Information Retrieval (IR) method in terms of retrieving semantically similar documents, and meanwhile achieves a speedup of over one order of magnitude in query time.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Andoni</author>
<author>P Indyk</author>
</authors>
<title>Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions.</title>
<date>2008</date>
<journal>Communications of the ACM,</journal>
<volume>51</volume>
<issue>1</issue>
<contexts>
<context position="3364" citStr="Andoni and Indyk, 2008" startWordPosition="524" endWordPosition="527">while a large news corpus such as the English Gigaword fifth edition1 stores 10 million documents in a 26G hard drive; 2) the time efficiency of manipulating binary codes, for example, computing the hamming distance between a pair of binary codes is several orders of magnitude faster than computing the real-valued cosine similarity over a pair of document vectors. The early explorations of hashing focused on using random permutations or projections to construct randomized hash functions, e.g., the wellknown Min-wise Hashing (MinHash) (Broder et al., 1998) and Locality Sensitive Hashing (LSH) (Andoni and Indyk, 2008). In contrast to such data-independent hashing schemes, recent research has been geared to studying data-dependent hashing through learning compact hash codes from a training dataset. The state-of-the-art unsupervised learning-based hashing methods include Spectral Hashing (SH) (Weiss et al., 2008), Anchor Graph Hashing (AGH) (Liu et al., 2011), and Iterative Quantization (ITQ) (Gong et al., 1http://catalog.ldc.upenn.edu/LDC2011T07 495 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 495–500, Baltimore, Maryland, USA, June 23-25 2014</context>
</contexts>
<marker>Andoni, Indyk, 2008</marker>
<rawString>A. Andoni and P. Indyk. 2008. Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. Communications of the ACM, 51(1):117–122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Z Broder</author>
<author>M Charikar</author>
<author>A M Frieze</author>
<author>M Mitzenmacher</author>
</authors>
<title>Min-wise independent permutations.</title>
<date>1998</date>
<booktitle>In Proc. STOC.</booktitle>
<contexts>
<context position="3302" citStr="Broder et al., 1998" startWordPosition="515" endWordPosition="518">ents with 1.9G memory using only 64 bits for each document while a large news corpus such as the English Gigaword fifth edition1 stores 10 million documents in a 26G hard drive; 2) the time efficiency of manipulating binary codes, for example, computing the hamming distance between a pair of binary codes is several orders of magnitude faster than computing the real-valued cosine similarity over a pair of document vectors. The early explorations of hashing focused on using random permutations or projections to construct randomized hash functions, e.g., the wellknown Min-wise Hashing (MinHash) (Broder et al., 1998) and Locality Sensitive Hashing (LSH) (Andoni and Indyk, 2008). In contrast to such data-independent hashing schemes, recent research has been geared to studying data-dependent hashing through learning compact hash codes from a training dataset. The state-of-the-art unsupervised learning-based hashing methods include Spectral Hashing (SH) (Weiss et al., 2008), Anchor Graph Hashing (AGH) (Liu et al., 2011), and Iterative Quantization (ITQ) (Gong et al., 1http://catalog.ldc.upenn.edu/LDC2011T07 495 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Pap</context>
</contexts>
<marker>Broder, Charikar, Frieze, Mitzenmacher, 1998</marker>
<rawString>A. Z. Broder, M. Charikar, A. M. Frieze, and M. Mitzenmacher. 1998. Min-wise independent permutations. In Proc. STOC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J P Callan</author>
<author>W B Croft</author>
<author>S M Harding</author>
</authors>
<title>The inquery retrieval system.</title>
<date>1992</date>
<booktitle>In Proc. the Third International Conference on Database and Expert Systems Applications.</booktitle>
<contexts>
<context position="14158" citStr="Callan et al., 1992" startWordPosition="2390" endWordPosition="2393">ic, and other unlabeled documents are irrelevant to them. We select six largest topics for the top-K NNS evaluation, with each including more than 250 documents. We randomly select 60 documents from each of the six topics for testing. The six topics are (1). Bombing in Riyadh, Saudi Arabia (2). Mad cow disease in North America (3). Casablanca bombs (4). Swedish Foreign Minister killed (5). Liberian former president arrives in exile and (6). UN official killed in attack. For each document, we apply the Stanford Tokenizer 2 for tokenization; remove stopwords based on the stop list from InQuery (Callan et al., 1992), and apply Porter Stemmer (Porter, 1980) for stemming. If one retrieved document shares the same topic label with the query document, they are true neighbors. We evaluate the precision of the top-K candidate documents returned by each method and calculate the average precision across all queries. Results We first evaluate the quality of term vectors and ITQ binary codes by conducting the whole list Cosine similarity ranking and hamming distance ranking, respectively. For each query document, the top-K candidate documents with highest Cosine similarity scores and shortest hamming distances are</context>
</contexts>
<marker>Callan, Croft, Harding, 1992</marker>
<rawString>J. P. Callan, W. B. Croft, and S. M. Harding. 1992. The inquery retrieval system. In Proc. the Third International Conference on Database and Expert Systems Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Charikar</author>
</authors>
<title>Similarity estimation techniques from rounding algorithms.</title>
<date>2002</date>
<booktitle>In Proc. STOC.</booktitle>
<contexts>
<context position="10240" citStr="Charikar, 2002" startWordPosition="1733" endWordPosition="1734">; the two steps run alternatingly until a convergence is encountered. Concretely, the two updating steps are: 1. Fix R and update B: minimize the following quantization loss Q(B,R) = kBk2F + kVRk2F − 2tr(R&gt;V&gt;B) = nc + kVk2F − 2tr(R&gt;V&gt;B) = constant − 2tr(R&gt;V&gt;B), (5) achieving B = sgn(VR); 2. Fix B and update R: perform the SVD of the _matrix V&gt;B ∈ Rc×c to obtain V&gt;B = SΩ§T, and then set R = S�S&gt;. Figure 1: The two-stage hashing framework. 3.4 Two-Stage Hashing There are three main merits of LSH. (1) It tries to preserve the Cosine similarity of the original data with a probabilistic guarantee (Charikar, 2002). (2) It is training free, and thus very efficient in hashing massive databases to binary codes. (3) It has a very high hash table lookup success rate. For example, in our experiments LSH with more than one hash table is able to achieve a perfect 100% hash lookup success rate. Unfortunately, its drawback is the low search precision that is observed even with long hash bits and multiple hash tables. ITQ tries to minimize the quantization error of encoding data to binary codes, so its advantage is the high quality (potentially high precision of Hamming ranking) of the produced binary codes. Neve</context>
</contexts>
<marker>Charikar, 2002</marker>
<rawString>M. Charikar. 2002. Similarity estimation techniques from rounding algorithms. In Proc. STOC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dean</author>
<author>M A Ruzon</author>
<author>M Segal</author>
<author>J Shlens</author>
<author>S Vijayanarasimhan</author>
<author>J Yagnik</author>
</authors>
<title>Fast, accurate detection of 100,000 object classes on a single machine. In</title>
<date>2013</date>
<booktitle>Proc. CVPR.</booktitle>
<contexts>
<context position="2040" citStr="Dean et al., 2013" startWordPosition="299" endWordPosition="302">rch related tasks, and also fundamental to a broad range of Natural Language Processing (NLP) down-stream problems including person name spelling correction (Udupa and Kumar, 2010), document translation pair acquisition (Krstovski and Smith, 2011), large-scale similar noun list generation (Ravichandran et al., 2005), lexical variants mining (Gouws et al., 2011), and large-scale first story detection (Petrovic et al., 2010). Hashing has recently emerged to be a popular solution to tackling fast NNS, and been successfully applied to a variety of non-NLP problems such as visual object detection (Dean et al., 2013) and recognition (Torralba et al., 2008a; Torralba et al., 2008b), large-scale image retrieval (Kulis and Grauman, 2012; Liu et al., 2012; Gong et al., 2013), and large-scale machine learning (Weiss et al., 2008; Liu et al., 2011; Liu, 2012). However, hashing has received limited attention in the NLP field to the date. The basic idea of hashing is to represent each data object as a binary code (each bit of a code is one digit of “0” or “1”). When applying hashing to handle NLP problems, the advantages are two-fold: 1) the capability to store a large quantity of documents in the main memory. fo</context>
</contexts>
<marker>Dean, Ruzon, Segal, Shlens, Vijayanarasimhan, Yagnik, 2013</marker>
<rawString>T. Dean, M. A. Ruzon, M. Segal, J. Shlens, S. Vijayanarasimhan, and J. Yagnik. 2013. Fast, accurate detection of 100,000 object classes on a single machine. In Proc. CVPR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C Deerwester</author>
<author>S T Dumais</author>
<author>T K Landauer</author>
<author>G W Furnas</author>
<author>R A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>JASIS,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="12497" citStr="Deerwester et al., 1990" startWordPosition="2107" endWordPosition="2110">gh precision, which does not require scanning the whole list of the ITQ binary codes but scanning the short list returned by LSH hash table lookup. Therefore, a high hash lookup success rate is attained by the LSH stage, while maintaining high search precision due to the ITQ reranking stage. 2. Enable a hybrid hashing scheme combining two similarity measures. The term similarity is used during the LSH stage that directly works on document tf-idf vectors; during the ITQ stage, the topic similarity is used since ITQ works on the topic vectors obtained by applying Latent semantic analysis (LSA) (Deerwester et al., 1990) to those document vectors. LSA (or PCA), the first step in running ITQ, can be easily accelerated via a simple sub-selective sampling strategy which has been proven theoretically and empirically sound by Li et al. (2014). As a result, the nearest neighbors returned by the two-stage hashing framework turns out to be both lexically and topically similar to the query document. To summarize, the proposed two-stage hashing framework works in an unsupervised manner, achieves a sublinear search time complexity due to LSH, and attains high search precision thanks to ITQ. After hashing all data (docum</context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman. 1990. Indexing by latent semantic analysis. JASIS, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Gong</author>
<author>S Lazebnik</author>
<author>A Gordo</author>
<author>F Perronnin</author>
</authors>
<title>Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval.</title>
<date>2013</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>35</volume>
<issue>12</issue>
<contexts>
<context position="2197" citStr="Gong et al., 2013" startWordPosition="324" endWordPosition="327">(Udupa and Kumar, 2010), document translation pair acquisition (Krstovski and Smith, 2011), large-scale similar noun list generation (Ravichandran et al., 2005), lexical variants mining (Gouws et al., 2011), and large-scale first story detection (Petrovic et al., 2010). Hashing has recently emerged to be a popular solution to tackling fast NNS, and been successfully applied to a variety of non-NLP problems such as visual object detection (Dean et al., 2013) and recognition (Torralba et al., 2008a; Torralba et al., 2008b), large-scale image retrieval (Kulis and Grauman, 2012; Liu et al., 2012; Gong et al., 2013), and large-scale machine learning (Weiss et al., 2008; Liu et al., 2011; Liu, 2012). However, hashing has received limited attention in the NLP field to the date. The basic idea of hashing is to represent each data object as a binary code (each bit of a code is one digit of “0” or “1”). When applying hashing to handle NLP problems, the advantages are two-fold: 1) the capability to store a large quantity of documents in the main memory. for example, one can store 250 million documents with 1.9G memory using only 64 bits for each document while a large news corpus such as the English Gigaword f</context>
<context position="4722" citStr="Gong et al. (2013)" startWordPosition="734" endWordPosition="737">nsic structure, such as local neighborhood structure, lowdimensional manifolds, or the closest hypercube, underlying the training data. Despite achieving data-dependent hash codes, most of these “learning to hash” methods cannot guarantee a high success rate of looking a query code up in a hash table (referred to as hash table lookup in literature), which is critical to the high efficacy of exploiting hashing in practical uses. It is worth noting that we choose to use ITQ in the proposed twostage hashing framework for its simplicity and efficiency. ITQ has been found to work better than SH by Gong et al. (2013), and be more efficient than AGH in terms of training time by Liu (2012). To this end, in this paper we propose a novel two-stage unsupervised hashing framework to simultaneously enhance the hash lookup success rate and increase the search accuracy by integrating the advantages of both LSH and ITQ. Furthermore, we make the hashing framework applicable to combine different similarity measures in NNS. 2 Background and Terminology • Binary Codes: A bit (a single bit is “0” or “1”) sequence assigned to represent a data object. For example, represent a document as a 8-bit code “11101010”. • Hash Ta</context>
</contexts>
<marker>Gong, Lazebnik, Gordo, Perronnin, 2013</marker>
<rawString>Y. Gong, S. Lazebnik, A. Gordo, and F. Perronnin. 2013. Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(12):2916–2929.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gouws</author>
<author>D Hovy</author>
<author>D Metzler</author>
</authors>
<title>Unsupervised mining of lexical variants from noisy text.</title>
<date>2011</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="1785" citStr="Gouws et al., 2011" startWordPosition="257" endWordPosition="260">troduction A Nearest Neighbor Search (NNS) task aims at searching for top K objects (e.g., documents) which are most similar, based on pre-defined similarity metrics, to a given query object in an existing dataset. NNS is essential in dealing with many search related tasks, and also fundamental to a broad range of Natural Language Processing (NLP) down-stream problems including person name spelling correction (Udupa and Kumar, 2010), document translation pair acquisition (Krstovski and Smith, 2011), large-scale similar noun list generation (Ravichandran et al., 2005), lexical variants mining (Gouws et al., 2011), and large-scale first story detection (Petrovic et al., 2010). Hashing has recently emerged to be a popular solution to tackling fast NNS, and been successfully applied to a variety of non-NLP problems such as visual object detection (Dean et al., 2013) and recognition (Torralba et al., 2008a; Torralba et al., 2008b), large-scale image retrieval (Kulis and Grauman, 2012; Liu et al., 2012; Gong et al., 2013), and large-scale machine learning (Weiss et al., 2008; Liu et al., 2011; Liu, 2012). However, hashing has received limited attention in the NLP field to the date. The basic idea of hashin</context>
</contexts>
<marker>Gouws, Hovy, Metzler, 2011</marker>
<rawString>S. Gouws, D. Hovy, and D. Metzler. 2011. Unsupervised mining of lexical variants from noisy text. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Krstovski</author>
<author>D A Smith</author>
</authors>
<title>A minimally supervised approach for detecting and ranking document translation pairs.</title>
<date>2011</date>
<booktitle>In Proc. the sixth ACL Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="1669" citStr="Krstovski and Smith, 2011" startWordPosition="241" endWordPosition="244">rieving semantically similar documents, and meanwhile achieves a speedup of over one order of magnitude in query time. 1 Introduction A Nearest Neighbor Search (NNS) task aims at searching for top K objects (e.g., documents) which are most similar, based on pre-defined similarity metrics, to a given query object in an existing dataset. NNS is essential in dealing with many search related tasks, and also fundamental to a broad range of Natural Language Processing (NLP) down-stream problems including person name spelling correction (Udupa and Kumar, 2010), document translation pair acquisition (Krstovski and Smith, 2011), large-scale similar noun list generation (Ravichandran et al., 2005), lexical variants mining (Gouws et al., 2011), and large-scale first story detection (Petrovic et al., 2010). Hashing has recently emerged to be a popular solution to tackling fast NNS, and been successfully applied to a variety of non-NLP problems such as visual object detection (Dean et al., 2013) and recognition (Torralba et al., 2008a; Torralba et al., 2008b), large-scale image retrieval (Kulis and Grauman, 2012; Liu et al., 2012; Gong et al., 2013), and large-scale machine learning (Weiss et al., 2008; Liu et al., 2011</context>
</contexts>
<marker>Krstovski, Smith, 2011</marker>
<rawString>K. Krstovski and D. A. Smith. 2011. A minimally supervised approach for detecting and ranking document translation pairs. In Proc. the sixth ACL Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Kulis</author>
<author>K Grauman</author>
</authors>
<title>Kernelized localitysensitive hashing.</title>
<date>2012</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>34</volume>
<issue>6</issue>
<pages>1104</pages>
<contexts>
<context position="2159" citStr="Kulis and Grauman, 2012" startWordPosition="316" endWordPosition="319"> including person name spelling correction (Udupa and Kumar, 2010), document translation pair acquisition (Krstovski and Smith, 2011), large-scale similar noun list generation (Ravichandran et al., 2005), lexical variants mining (Gouws et al., 2011), and large-scale first story detection (Petrovic et al., 2010). Hashing has recently emerged to be a popular solution to tackling fast NNS, and been successfully applied to a variety of non-NLP problems such as visual object detection (Dean et al., 2013) and recognition (Torralba et al., 2008a; Torralba et al., 2008b), large-scale image retrieval (Kulis and Grauman, 2012; Liu et al., 2012; Gong et al., 2013), and large-scale machine learning (Weiss et al., 2008; Liu et al., 2011; Liu, 2012). However, hashing has received limited attention in the NLP field to the date. The basic idea of hashing is to represent each data object as a binary code (each bit of a code is one digit of “0” or “1”). When applying hashing to handle NLP problems, the advantages are two-fold: 1) the capability to store a large quantity of documents in the main memory. for example, one can store 250 million documents with 1.9G memory using only 64 bits for each document while a large news</context>
</contexts>
<marker>Kulis, Grauman, 2012</marker>
<rawString>B. Kulis and K. Grauman. 2012. Kernelized localitysensitive hashing. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(6):1092– 1104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Li</author>
<author>C Chen</author>
<author>W Liu</author>
<author>J Huang</author>
</authors>
<title>Subselective quantization for large-scale image search.</title>
<date>2014</date>
<booktitle>In Proc. AAAI Conference on Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="12718" citStr="Li et al. (2014)" startWordPosition="2145" endWordPosition="2148">intaining high search precision due to the ITQ reranking stage. 2. Enable a hybrid hashing scheme combining two similarity measures. The term similarity is used during the LSH stage that directly works on document tf-idf vectors; during the ITQ stage, the topic similarity is used since ITQ works on the topic vectors obtained by applying Latent semantic analysis (LSA) (Deerwester et al., 1990) to those document vectors. LSA (or PCA), the first step in running ITQ, can be easily accelerated via a simple sub-selective sampling strategy which has been proven theoretically and empirically sound by Li et al. (2014). As a result, the nearest neighbors returned by the two-stage hashing framework turns out to be both lexically and topically similar to the query document. To summarize, the proposed two-stage hashing framework works in an unsupervised manner, achieves a sublinear search time complexity due to LSH, and attains high search precision thanks to ITQ. After hashing all data (documents) to LSH and ITQ binary codes, we do not need to save the raw data in memory. Thus, our approach can scale to gigantic datasets with compact storage and fast search speed. 4 Experiments Data and Evaluations For the ex</context>
</contexts>
<marker>Li, Chen, Liu, Huang, 2014</marker>
<rawString>Y. Li, C. Chen, W. Liu, and J. Huang. 2014. Subselective quantization for large-scale image search. In Proc. AAAI Conference on Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Liu</author>
<author>J Wang</author>
<author>S Kumar</author>
<author>S-F Chang</author>
</authors>
<title>Hashing with graphs.</title>
<date>2011</date>
<booktitle>In Proc. ICML.</booktitle>
<contexts>
<context position="2269" citStr="Liu et al., 2011" startWordPosition="336" endWordPosition="339">and Smith, 2011), large-scale similar noun list generation (Ravichandran et al., 2005), lexical variants mining (Gouws et al., 2011), and large-scale first story detection (Petrovic et al., 2010). Hashing has recently emerged to be a popular solution to tackling fast NNS, and been successfully applied to a variety of non-NLP problems such as visual object detection (Dean et al., 2013) and recognition (Torralba et al., 2008a; Torralba et al., 2008b), large-scale image retrieval (Kulis and Grauman, 2012; Liu et al., 2012; Gong et al., 2013), and large-scale machine learning (Weiss et al., 2008; Liu et al., 2011; Liu, 2012). However, hashing has received limited attention in the NLP field to the date. The basic idea of hashing is to represent each data object as a binary code (each bit of a code is one digit of “0” or “1”). When applying hashing to handle NLP problems, the advantages are two-fold: 1) the capability to store a large quantity of documents in the main memory. for example, one can store 250 million documents with 1.9G memory using only 64 bits for each document while a large news corpus such as the English Gigaword fifth edition1 stores 10 million documents in a 26G hard drive; 2) the ti</context>
<context position="3710" citStr="Liu et al., 2011" startWordPosition="574" endWordPosition="577">ument vectors. The early explorations of hashing focused on using random permutations or projections to construct randomized hash functions, e.g., the wellknown Min-wise Hashing (MinHash) (Broder et al., 1998) and Locality Sensitive Hashing (LSH) (Andoni and Indyk, 2008). In contrast to such data-independent hashing schemes, recent research has been geared to studying data-dependent hashing through learning compact hash codes from a training dataset. The state-of-the-art unsupervised learning-based hashing methods include Spectral Hashing (SH) (Weiss et al., 2008), Anchor Graph Hashing (AGH) (Liu et al., 2011), and Iterative Quantization (ITQ) (Gong et al., 1http://catalog.ldc.upenn.edu/LDC2011T07 495 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 495–500, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics 2013), all of which endeavor to make the learned hash codes preserve or reveal some intrinsic structure, such as local neighborhood structure, lowdimensional manifolds, or the closest hypercube, underlying the training data. Despite achieving data-dependent hash codes, most of these “learning t</context>
</contexts>
<marker>Liu, Wang, Kumar, Chang, 2011</marker>
<rawString>W. Liu, J. Wang, S. Kumar, and S.-F. Chang. 2011. Hashing with graphs. In Proc. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Liu</author>
<author>J Wang</author>
<author>R Ji</author>
<author>Y-G Jiang</author>
<author>S-F Chang</author>
</authors>
<title>Supervised hashing with kernels.</title>
<date>2012</date>
<booktitle>In Proc. CVPR.</booktitle>
<contexts>
<context position="2177" citStr="Liu et al., 2012" startWordPosition="320" endWordPosition="323">elling correction (Udupa and Kumar, 2010), document translation pair acquisition (Krstovski and Smith, 2011), large-scale similar noun list generation (Ravichandran et al., 2005), lexical variants mining (Gouws et al., 2011), and large-scale first story detection (Petrovic et al., 2010). Hashing has recently emerged to be a popular solution to tackling fast NNS, and been successfully applied to a variety of non-NLP problems such as visual object detection (Dean et al., 2013) and recognition (Torralba et al., 2008a; Torralba et al., 2008b), large-scale image retrieval (Kulis and Grauman, 2012; Liu et al., 2012; Gong et al., 2013), and large-scale machine learning (Weiss et al., 2008; Liu et al., 2011; Liu, 2012). However, hashing has received limited attention in the NLP field to the date. The basic idea of hashing is to represent each data object as a binary code (each bit of a code is one digit of “0” or “1”). When applying hashing to handle NLP problems, the advantages are two-fold: 1) the capability to store a large quantity of documents in the main memory. for example, one can store 250 million documents with 1.9G memory using only 64 bits for each document while a large news corpus such as th</context>
</contexts>
<marker>Liu, Wang, Ji, Jiang, Chang, 2012</marker>
<rawString>W. Liu, J. Wang, R. Ji, Y.-G. Jiang, and S.-F. Chang. 2012. Supervised hashing with kernels. In Proc. CVPR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Liu</author>
</authors>
<title>Large-scale machine learning for classification and search. In</title>
<date>2012</date>
<tech>PhD Thesis,</tech>
<institution>Graduate School of Arts and Sciences, Columbia University.</institution>
<contexts>
<context position="2281" citStr="Liu, 2012" startWordPosition="340" endWordPosition="341">large-scale similar noun list generation (Ravichandran et al., 2005), lexical variants mining (Gouws et al., 2011), and large-scale first story detection (Petrovic et al., 2010). Hashing has recently emerged to be a popular solution to tackling fast NNS, and been successfully applied to a variety of non-NLP problems such as visual object detection (Dean et al., 2013) and recognition (Torralba et al., 2008a; Torralba et al., 2008b), large-scale image retrieval (Kulis and Grauman, 2012; Liu et al., 2012; Gong et al., 2013), and large-scale machine learning (Weiss et al., 2008; Liu et al., 2011; Liu, 2012). However, hashing has received limited attention in the NLP field to the date. The basic idea of hashing is to represent each data object as a binary code (each bit of a code is one digit of “0” or “1”). When applying hashing to handle NLP problems, the advantages are two-fold: 1) the capability to store a large quantity of documents in the main memory. for example, one can store 250 million documents with 1.9G memory using only 64 bits for each document while a large news corpus such as the English Gigaword fifth edition1 stores 10 million documents in a 26G hard drive; 2) the time efficienc</context>
<context position="4794" citStr="Liu (2012)" startWordPosition="750" endWordPosition="751">or the closest hypercube, underlying the training data. Despite achieving data-dependent hash codes, most of these “learning to hash” methods cannot guarantee a high success rate of looking a query code up in a hash table (referred to as hash table lookup in literature), which is critical to the high efficacy of exploiting hashing in practical uses. It is worth noting that we choose to use ITQ in the proposed twostage hashing framework for its simplicity and efficiency. ITQ has been found to work better than SH by Gong et al. (2013), and be more efficient than AGH in terms of training time by Liu (2012). To this end, in this paper we propose a novel two-stage unsupervised hashing framework to simultaneously enhance the hash lookup success rate and increase the search accuracy by integrating the advantages of both LSH and ITQ. Furthermore, we make the hashing framework applicable to combine different similarity measures in NNS. 2 Background and Terminology • Binary Codes: A bit (a single bit is “0” or “1”) sequence assigned to represent a data object. For example, represent a document as a 8-bit code “11101010”. • Hash Table: A linear table in which all binary codes of a data set are arranged</context>
<context position="10960" citStr="Liu, 2012" startWordPosition="1856" endWordPosition="1857">very high hash table lookup success rate. For example, in our experiments LSH with more than one hash table is able to achieve a perfect 100% hash lookup success rate. Unfortunately, its drawback is the low search precision that is observed even with long hash bits and multiple hash tables. ITQ tries to minimize the quantization error of encoding data to binary codes, so its advantage is the high quality (potentially high precision of Hamming ranking) of the produced binary codes. Nevertheless, ITQ frequently suffers from a poor hash lookup success rate when longer bits (e.g., ≥ 48) are used (Liu, 2012). For example, in our experiments ITQ using 384 bits has a 18.47% hash lookup success rate within Hamming radius 2. Hence, Hamming ranking (costing O(n) time) must be invoked for the queries for which ITQ fails to return any neighbors via hash table lookup, which makes the searches inefficient especially on very large datasets. Taking into account the above advantages and disadvantages of LSH and ITQ, we propose a twostage hashing framework to harmoniously integrate them. Fig. 1 illustrates our two-stage framework with a toy example where identical shapes denote ground-truth nearest neighbors.</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>W. Liu. 2012. Large-scale machine learning for classification and search. In PhD Thesis, Graduate School of Arts and Sciences, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrovic</author>
<author>M Osborne</author>
<author>V Lavrenko</author>
</authors>
<title>Streaming first story detection with application to twitter.</title>
<date>2010</date>
<booktitle>In Proc. HLT-NAACL.</booktitle>
<contexts>
<context position="1848" citStr="Petrovic et al., 2010" startWordPosition="266" endWordPosition="269">ching for top K objects (e.g., documents) which are most similar, based on pre-defined similarity metrics, to a given query object in an existing dataset. NNS is essential in dealing with many search related tasks, and also fundamental to a broad range of Natural Language Processing (NLP) down-stream problems including person name spelling correction (Udupa and Kumar, 2010), document translation pair acquisition (Krstovski and Smith, 2011), large-scale similar noun list generation (Ravichandran et al., 2005), lexical variants mining (Gouws et al., 2011), and large-scale first story detection (Petrovic et al., 2010). Hashing has recently emerged to be a popular solution to tackling fast NNS, and been successfully applied to a variety of non-NLP problems such as visual object detection (Dean et al., 2013) and recognition (Torralba et al., 2008a; Torralba et al., 2008b), large-scale image retrieval (Kulis and Grauman, 2012; Liu et al., 2012; Gong et al., 2013), and large-scale machine learning (Weiss et al., 2008; Liu et al., 2011; Liu, 2012). However, hashing has received limited attention in the NLP field to the date. The basic idea of hashing is to represent each data object as a binary code (each bit o</context>
</contexts>
<marker>Petrovic, Osborne, Lavrenko, 2010</marker>
<rawString>S. Petrovic, M. Osborne, and V. Lavrenko. 2010. Streaming first story detection with application to twitter. In Proc. HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="14199" citStr="Porter, 1980" startWordPosition="2398" endWordPosition="2399"> to them. We select six largest topics for the top-K NNS evaluation, with each including more than 250 documents. We randomly select 60 documents from each of the six topics for testing. The six topics are (1). Bombing in Riyadh, Saudi Arabia (2). Mad cow disease in North America (3). Casablanca bombs (4). Swedish Foreign Minister killed (5). Liberian former president arrives in exile and (6). UN official killed in attack. For each document, we apply the Stanford Tokenizer 2 for tokenization; remove stopwords based on the stop list from InQuery (Callan et al., 1992), and apply Porter Stemmer (Porter, 1980) for stemming. If one retrieved document shares the same topic label with the query document, they are true neighbors. We evaluate the precision of the top-K candidate documents returned by each method and calculate the average precision across all queries. Results We first evaluate the quality of term vectors and ITQ binary codes by conducting the whole list Cosine similarity ranking and hamming distance ranking, respectively. For each query document, the top-K candidate documents with highest Cosine similarity scores and shortest hamming distances are returned, then we calculate the average </context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>M. F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ravichandran</author>
<author>P Pantel</author>
<author>E H Hovy</author>
</authors>
<title>Randomized algorithms and nlp: Using locality sensitive hash functions for high speed noun clustering.</title>
<date>2005</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="1739" citStr="Ravichandran et al., 2005" startWordPosition="250" endWordPosition="253">up of over one order of magnitude in query time. 1 Introduction A Nearest Neighbor Search (NNS) task aims at searching for top K objects (e.g., documents) which are most similar, based on pre-defined similarity metrics, to a given query object in an existing dataset. NNS is essential in dealing with many search related tasks, and also fundamental to a broad range of Natural Language Processing (NLP) down-stream problems including person name spelling correction (Udupa and Kumar, 2010), document translation pair acquisition (Krstovski and Smith, 2011), large-scale similar noun list generation (Ravichandran et al., 2005), lexical variants mining (Gouws et al., 2011), and large-scale first story detection (Petrovic et al., 2010). Hashing has recently emerged to be a popular solution to tackling fast NNS, and been successfully applied to a variety of non-NLP problems such as visual object detection (Dean et al., 2013) and recognition (Torralba et al., 2008a; Torralba et al., 2008b), large-scale image retrieval (Kulis and Grauman, 2012; Liu et al., 2012; Gong et al., 2013), and large-scale machine learning (Weiss et al., 2008; Liu et al., 2011; Liu, 2012). However, hashing has received limited attention in the N</context>
</contexts>
<marker>Ravichandran, Pantel, Hovy, 2005</marker>
<rawString>D. Ravichandran, P. Pantel, and E. H. Hovy. 2005. Randomized algorithms and nlp: Using locality sensitive hash functions for high speed noun clustering. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Torralba</author>
<author>R Fergus</author>
<author>W T Freeman</author>
</authors>
<title>80 million tiny images: A large data set for nonparametric object and scene recognition.</title>
<date>2008</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>30</volume>
<issue>11</issue>
<contexts>
<context position="2079" citStr="Torralba et al., 2008" startWordPosition="305" endWordPosition="308">tal to a broad range of Natural Language Processing (NLP) down-stream problems including person name spelling correction (Udupa and Kumar, 2010), document translation pair acquisition (Krstovski and Smith, 2011), large-scale similar noun list generation (Ravichandran et al., 2005), lexical variants mining (Gouws et al., 2011), and large-scale first story detection (Petrovic et al., 2010). Hashing has recently emerged to be a popular solution to tackling fast NNS, and been successfully applied to a variety of non-NLP problems such as visual object detection (Dean et al., 2013) and recognition (Torralba et al., 2008a; Torralba et al., 2008b), large-scale image retrieval (Kulis and Grauman, 2012; Liu et al., 2012; Gong et al., 2013), and large-scale machine learning (Weiss et al., 2008; Liu et al., 2011; Liu, 2012). However, hashing has received limited attention in the NLP field to the date. The basic idea of hashing is to represent each data object as a binary code (each bit of a code is one digit of “0” or “1”). When applying hashing to handle NLP problems, the advantages are two-fold: 1) the capability to store a large quantity of documents in the main memory. for example, one can store 250 million do</context>
</contexts>
<marker>Torralba, Fergus, Freeman, 2008</marker>
<rawString>A. Torralba, R. Fergus, and W. T. Freeman. 2008a. 80 million tiny images: A large data set for nonparametric object and scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 30(11):1958–1970.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Torralba</author>
<author>R Fergus</author>
<author>Y Weiss</author>
</authors>
<title>Small codes and large image databases for recognition.</title>
<date>2008</date>
<booktitle>In Proc. CVPR.</booktitle>
<contexts>
<context position="2079" citStr="Torralba et al., 2008" startWordPosition="305" endWordPosition="308">tal to a broad range of Natural Language Processing (NLP) down-stream problems including person name spelling correction (Udupa and Kumar, 2010), document translation pair acquisition (Krstovski and Smith, 2011), large-scale similar noun list generation (Ravichandran et al., 2005), lexical variants mining (Gouws et al., 2011), and large-scale first story detection (Petrovic et al., 2010). Hashing has recently emerged to be a popular solution to tackling fast NNS, and been successfully applied to a variety of non-NLP problems such as visual object detection (Dean et al., 2013) and recognition (Torralba et al., 2008a; Torralba et al., 2008b), large-scale image retrieval (Kulis and Grauman, 2012; Liu et al., 2012; Gong et al., 2013), and large-scale machine learning (Weiss et al., 2008; Liu et al., 2011; Liu, 2012). However, hashing has received limited attention in the NLP field to the date. The basic idea of hashing is to represent each data object as a binary code (each bit of a code is one digit of “0” or “1”). When applying hashing to handle NLP problems, the advantages are two-fold: 1) the capability to store a large quantity of documents in the main memory. for example, one can store 250 million do</context>
</contexts>
<marker>Torralba, Fergus, Weiss, 2008</marker>
<rawString>A. Torralba, R. Fergus, and Y. Weiss. 2008b. Small codes and large image databases for recognition. In Proc. CVPR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Udupa</author>
<author>S Kumar</author>
</authors>
<title>Hashing-based approaches to spelling correction of personal names.</title>
<date>2010</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="1602" citStr="Udupa and Kumar, 2010" startWordPosition="232" endWordPosition="236"> conventional Information Retrieval (IR) method in terms of retrieving semantically similar documents, and meanwhile achieves a speedup of over one order of magnitude in query time. 1 Introduction A Nearest Neighbor Search (NNS) task aims at searching for top K objects (e.g., documents) which are most similar, based on pre-defined similarity metrics, to a given query object in an existing dataset. NNS is essential in dealing with many search related tasks, and also fundamental to a broad range of Natural Language Processing (NLP) down-stream problems including person name spelling correction (Udupa and Kumar, 2010), document translation pair acquisition (Krstovski and Smith, 2011), large-scale similar noun list generation (Ravichandran et al., 2005), lexical variants mining (Gouws et al., 2011), and large-scale first story detection (Petrovic et al., 2010). Hashing has recently emerged to be a popular solution to tackling fast NNS, and been successfully applied to a variety of non-NLP problems such as visual object detection (Dean et al., 2013) and recognition (Torralba et al., 2008a; Torralba et al., 2008b), large-scale image retrieval (Kulis and Grauman, 2012; Liu et al., 2012; Gong et al., 2013), and</context>
</contexts>
<marker>Udupa, Kumar, 2010</marker>
<rawString>R. Udupa and S. Kumar. 2010. Hashing-based approaches to spelling correction of personal names. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Weiss</author>
<author>A Torralba</author>
<author>R Fergus</author>
</authors>
<title>Spectral hashing.</title>
<date>2008</date>
<booktitle>In NIPS 21.</booktitle>
<contexts>
<context position="2251" citStr="Weiss et al., 2008" startWordPosition="332" endWordPosition="335">uisition (Krstovski and Smith, 2011), large-scale similar noun list generation (Ravichandran et al., 2005), lexical variants mining (Gouws et al., 2011), and large-scale first story detection (Petrovic et al., 2010). Hashing has recently emerged to be a popular solution to tackling fast NNS, and been successfully applied to a variety of non-NLP problems such as visual object detection (Dean et al., 2013) and recognition (Torralba et al., 2008a; Torralba et al., 2008b), large-scale image retrieval (Kulis and Grauman, 2012; Liu et al., 2012; Gong et al., 2013), and large-scale machine learning (Weiss et al., 2008; Liu et al., 2011; Liu, 2012). However, hashing has received limited attention in the NLP field to the date. The basic idea of hashing is to represent each data object as a binary code (each bit of a code is one digit of “0” or “1”). When applying hashing to handle NLP problems, the advantages are two-fold: 1) the capability to store a large quantity of documents in the main memory. for example, one can store 250 million documents with 1.9G memory using only 64 bits for each document while a large news corpus such as the English Gigaword fifth edition1 stores 10 million documents in a 26G har</context>
<context position="3663" citStr="Weiss et al., 2008" startWordPosition="565" endWordPosition="568"> real-valued cosine similarity over a pair of document vectors. The early explorations of hashing focused on using random permutations or projections to construct randomized hash functions, e.g., the wellknown Min-wise Hashing (MinHash) (Broder et al., 1998) and Locality Sensitive Hashing (LSH) (Andoni and Indyk, 2008). In contrast to such data-independent hashing schemes, recent research has been geared to studying data-dependent hashing through learning compact hash codes from a training dataset. The state-of-the-art unsupervised learning-based hashing methods include Spectral Hashing (SH) (Weiss et al., 2008), Anchor Graph Hashing (AGH) (Liu et al., 2011), and Iterative Quantization (ITQ) (Gong et al., 1http://catalog.ldc.upenn.edu/LDC2011T07 495 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 495–500, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics 2013), all of which endeavor to make the learned hash codes preserve or reveal some intrinsic structure, such as local neighborhood structure, lowdimensional manifolds, or the closest hypercube, underlying the training data. Despite achieving data-</context>
</contexts>
<marker>Weiss, Torralba, Fergus, 2008</marker>
<rawString>Y. Weiss, A. Torralba, and R. Fergus. 2008. Spectral hashing. In NIPS 21.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>