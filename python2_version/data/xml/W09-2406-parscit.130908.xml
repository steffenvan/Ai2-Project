<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004291">
<title confidence="0.741651">
Large-scale Semantic Networks: Annotation and Evaluation
</title>
<author confidence="0.985607">
V´aclav Nov´ak
</author>
<affiliation confidence="0.9853815">
Institute of Formal and Applied Linguistics
Charles University in Prague, Czech Republic
</affiliation>
<email confidence="0.9575">
novak@ufal.mff.cuni.cz
</email>
<author confidence="0.998417">
Sven Hartrumpf Keith Hall∗
</author>
<affiliation confidence="0.9987935">
Computer Science Department Google Research
University of Hagen, Germany Z¨urich, Switzerland
</affiliation>
<email confidence="0.991014">
Sven.Hartrumpf@FernUni-Hagen.de kbhall@google.com
</email>
<sectionHeader confidence="0.998511" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999310714285714">
We introduce a large-scale semantic-network
annotation effort based on the MutliNet for-
malism. Annotation is achieved via a pro-
cess which incorporates several independent
tools including a MultiNet graph editing tool,
a semantic concept lexicon, a user-editable
knowledge-base for semantic concepts, and a
MultiNet parser. We present an evaluation
metric for these semantic networks, allowing
us to determine the quality of annotations in
terms of inter-annotator agreement. We use
this metric to report the agreement rates for a
pilot annotation effort involving three annota-
tors.
</bodyText>
<sectionHeader confidence="0.99952" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999569">
In this paper we propose an annotation frame-
work which integrates the MultiNet semantic net-
work formalism (Helbig, 2006) and the syntactico-
semantic formalism of the Prague Dependency Tree-
bank (Hajiˇc et al., 2006) (PDT). The primary goal of
this task is to increase the interoperability of these
two frameworks in order to facilitate efforts to an-
notate at the semantic level while preserving intra-
sentential semantic and syntactic annotations as are
found in the PDT.
The task of annotating text with global semantic
interactions (e.g., semantic interactions within some
discourse) presents a cognitively demanding prob-
lem. As with many other annotation formalisms,
</bodyText>
<footnote confidence="0.509590666666667">
∗Part of this work was completed while at the Johns Hop-
kins University Center for Language and Speech Processing in
Baltimore, MD USA.
</footnote>
<bodyText confidence="0.999822">
we propose a technique that builds from cognitively
simpler tasks such as syntactic and semantic anno-
tations at the sentence level including rich morpho-
logical analysis. Rather than constraining the se-
mantic representations to those compatible with the
sentential annotations, our procedure provides the
syntacitco-semantic tree as a reference; the annota-
tors are free to select nodes from this tree to create
nodes in the network. We do not attempt to measure
the influence this procedure has on the types of se-
mantic networks generated. We believe that using a
soft-constraint such as the syntactico-semantic tree,
allows us to better generate human labeled seman-
tic networks with links to the interpretations of the
individual sentence analyses.
In this paper, we present a procedure for com-
puting the annotator agreement rate for MultiNet
graphs. Note that a MultiNet graph does not rep-
resent the same semantics as a syntactico-semantic
dependency tree. The nodes of the MultiNet graph
are connected based on a corpus-wide interpretation
of the entities referred to in the corpus. These global
connections are determined by the intra-sentential
interpretation but are not restricted to that inter-
pretation. Therefore, the procedure for computing
annotator agreement differs from the standard ap-
proaches to evaluating syntactic and semantic de-
pendency treebanks (e.g., dependency link agree-
ment, label agreement, predicate-argument structure
agreement).
As noted in (Bos, 2008), “Even though the de-
sign of annotation schemes has been initiated for
single semantic phenomena, there exists no anno-
tation scheme (as far as I know) that aims to inte-
</bodyText>
<page confidence="0.994252">
37
</page>
<note confidence="0.99216">
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 37–45,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999332210526316">
grate a wide range of semantic phenomena all at
once. It would be welcome to have such a resource
at ones disposal, and ideally a semantic annotation
scheme should be multi-layered, where certain se-
mantic phenomena can be properly analysed or left
simply unanalysed.”
In Section 1 we introduce the theoretical back-
ground of the frameworks on which our annotation
tool is based: MultiNet and the Tectogrammatical
Representation (TR) of the PDT. Section 2 describes
the annotation process in detail, including an intro-
duction to the encyclopedic tools available to the an-
notators. In Section 3 we present an evaluation met-
ric for MultiNet/TR labeled data. We also present an
evaluation of the data we have had annotated using
the proposed procedure. Finally, we conclude with
a short discussion of the problems observed during
the annotation process and suggest improvements as
future work.
</bodyText>
<subsectionHeader confidence="0.994362">
1.1 MultiNet
</subsectionHeader>
<bodyText confidence="0.999739409090909">
The representation of the Multilayered Extended
Semantic Networks (MultiNet), which is described
in (Helbig, 2006), provides a universal formalism
for the treatment of semantic phenomena of natu-
ral language. To this end, they offer distinct ad-
vantages over the use of the classical predicate
calculus and its derivatives. For example, Multi-
Net provides a rich ontology of semantic-concept
types. This ontology has been constructed to be
language independent. Due to the graphical inter-
pretation of MultiNets, we believe manual anno-
tation and interpretation is simpler and thus more
cognitively compatible. Figure 1 shows the Multi-
Net annotation of a sentence from the WSJ corpus:
“Stephen Akerfeldt, currently vice president fi-
nance, will succeed Mr. McAlpine.”
In this example, there are a few relationships that il-
lustrate the representational power of MultiNet. The
main predicate succeed is a ANTE dependent of the
node now, which indicates that the outcome of the
event described by the predicate occurs at some time
later than the time of the statement (i.e., the succes-
sion is taking place after the current time as captured
by the future tense in the sentence). Intra-sentential
coreference is indicated by the EQU relationship.
From the previous context, we know that the vice
president is related to a particular company, Magna
International Inc. The pragmatically defined rela-
tionship between Magna International Inc. and vice
president finance is captured by the ATTCH (con-
ceptual attachment) relationship. This indicates that
there is some relationship between these entities for
which one is a member of the other (as indicated by
the directed edge). Stephen Akerfeldt is the agent of
the predicate described by this sub-network.
The semantic representation of natural language
expressions by means of MultiNet is generally in-
dependent of the considered language. In contrast,
the syntactic constructs used in different languages
to express the same content are obviously not iden-
tical. To bridge the gap between different languages
we employ the deep syntactico-semantic representa-
tion available in the Functional Generative Descrip-
tion framework (Sgall et al., 1986).
</bodyText>
<subsectionHeader confidence="0.992819">
1.2 Prague Dependency Treebank
</subsectionHeader>
<bodyText confidence="0.999808576923077">
The Prague Dependency Treebank (PDT) presents a
language resource containing a deep manual analy-
sis of texts(Sgall et al., 2004). The PDT contains
annotations on three layers:
Morphological A rich morphological annotation is
provided when such information is available in
the language. This includes lemmatization and
detailed morphological tagging.
Analytical The analytical layer is a dependency
analysis based purely on the syntactic interpre-
tation.
Tectogrammatical The tectogrammatical annota-
tion provides a deep-syntactic (syntactico-
semantic) analysis of the text. The formal-
ism abstracts away from word-order, function
words (syn-semantic words), and morphologi-
cal variation.
The units of each annotation level are linked with
corresponding units on the preceding level. The
morphological units are linked directly with the
original tokenized text. Linking is possible as most
of these interpretations are directly tied to the words
in the original sentence. In MultiNet graphs, addi-
tional nodes are added and nodes are removed.
The PDT 2.0 is based on the long-standing
Praguian linguistic tradition, adapted for the current
</bodyText>
<page confidence="0.999364">
38
</page>
<figureCaption confidence="0.936140666666667">
Figure 1: MultiNet annotation of sentence “Stephen Akerfeldt, currently vice president finance, will succeed Mr.
McAlpine.” Nodes C4 and C8 are re-used from previous sentences. Node C2 is an unexpressed (not explicitly stated
in the text) annotator-created node used in previous annotations.
</figureCaption>
<bodyText confidence="0.999444142857143">
computational-linguistics research needs. The theo-
retical basis of the tectogrammatical representation
lies in the Functional Generative Description of lan-
guage systems (Sgall et al., 1986). Software tools
for corpus search, lexicon retrieval, annotation, and
language analysis are included. Extensive documen-
tation in English is provided as well.
</bodyText>
<sectionHeader confidence="0.995488" genericHeader="method">
2 Integrated Annotation Process
</sectionHeader>
<bodyText confidence="0.9999968">
We propose an integrated annotation procedure
aimed at acquiring high-quality MultiNet semantic
annotations. The procedure is based on a combi-
nation of annotation tools and annotation resources.
We present these components in the this section.
</bodyText>
<subsectionHeader confidence="0.998788">
2.1 Annotation Tool
</subsectionHeader>
<bodyText confidence="0.999860166666667">
The core annotation is facilitated by the cedit
tool1, which uses PML (Pajas and ˇStˇep´anek, 2005),
an XML file format, as its internal representa-
tion (Nov´ak, 2007). The annotation tool is an
application with a graphical user interface imple-
mented in Java (Sun Microsystems, Inc., 2007). The
</bodyText>
<footnote confidence="0.936071">
1The cedit annotation tool can be downloaded from
http://ufal.mff.cuni.cz/∼novak/files/cedit.zip.
</footnote>
<bodyText confidence="0.999434444444444">
cedit tool is platform independent and directly con-
nected to the annotators’ wiki (see Section 2.4),
where annotators can access the definitions of indi-
vidual MultiNet semantic relations, functions and at-
tributes; as well as examples, counterexamples, and
discussion concerning the entity in question. If the
wiki page does not contain the required information,
the annotator is encouraged to edit the page with
his/her questions and comments.
</bodyText>
<subsectionHeader confidence="0.997877">
2.2 Online Lexicon
</subsectionHeader>
<bodyText confidence="0.999976538461539">
The annotators in the semantic annotation project
have the option to look up examples of MultiNet
structures in an online version of the semantically
oriented computer lexicon HaGenLex (Hartrumpf et
al., 2003). The annotators can use lemmata (instead
of reading IDs formed of the lemma and a numer-
ical suffix) for the query, thus increasing the recall
of related structures. English and German input is
supported with outputs in English and/or German;
there are approximately 3,000 and 25,000 seman-
tic networks, respectively, in the lexicon. An exam-
ple sentence for the German verb “borgen.1.1” (“to
borrow”) plus its automatically generated and val-
</bodyText>
<page confidence="0.998864">
39
</page>
<figureCaption confidence="0.9873985">
Figure 2: HaGenLex entry showing an example sentence
for the German verb “borgen.1.1” (“to borrow”). The
sentence is literally “The man borrows himself money
from the friend.”
</figureCaption>
<bodyText confidence="0.9947335">
idated semantic representation is displayed in Fig-
ure 2. The quality of example parses is assured by
comparing the marked-up complements in the ex-
ample to the ones in the semantic network. In the
rare case that the parse is not optimal, it will not be
visible to annotators.
</bodyText>
<subsectionHeader confidence="0.996548">
2.3 Online Parser
</subsectionHeader>
<bodyText confidence="0.999888428571429">
Sometimes the annotator needs to look up a phrase
or something more general than a particular noun
or verb. In this case, the annotator can use
the workbench for (MultiNet) knowledge bases
(MWR (Gn¨orlich, 2000)), which provides conve-
nient and quick access to the parser that translates
German sentences or phrases into MultiNets.
</bodyText>
<subsectionHeader confidence="0.997017">
2.4 Wiki Knowledge Base
</subsectionHeader>
<bodyText confidence="0.9999828">
A wiki (Leuf and Cunningham, 2001) is used collab-
oratively to create and maintain the knowledge base
used by all the annotators. In this project we use
Dokuwiki (Badger, 2007). The entries of individ-
ual annotators in the wiki are logged and a feed of
changes can be observed using an RSS reader. The
cedit annotation tool allows users to display appro-
priate wiki pages of individual relation types, func-
tion types and attributes directly from the tool using
their preferred web browser.
</bodyText>
<sectionHeader confidence="0.99814" genericHeader="method">
3 Network Evaluation
</sectionHeader>
<bodyText confidence="0.999464227272727">
We present an evaluation which has been carried
out on an initial set of annotations of English arti-
cles from The Wall Street Journal (covering those
annotated at the syntactic level in the Penn Tree-
bank (Marcus et al., 1993)). We use the annotation
from the Prague Czech-English Dependency Tree-
bank (Cuˇr´ın et al., 2004), which contains a large por-
tion of the WSJ Treebank annotated according to the
PDT annotation scheme (including all layers of the
FGD formalism).
We reserved a small set of data to be used to train
our annotators and have excluded these articles from
the evaluation. Three native English-speaking anno-
tators were trained and then asked to annotate sen-
tences from the corpus. We have a sample of 67
sentences (1793 words) annotated by two of the an-
notators; of those, 46 sentences (1236 words) were
annotated by three annotators.2 Agreement is mea-
sured for each individual sentences in two steps.
First, the best match between the two annotators’
graphs is found and then the F-measure is computed.
In order to determine the optimal graph match be-
tween two graphs, we make use of the fact that
the annotators have the tectogrammatical tree from
which they can select nodes as concepts in the Multi-
Net graph. Many of the nodes in the annotated
graphs remain linked to the tectogrammatical tree,
therefore we have a unique identifier for these nodes.
When matching the nodes of two different annota-
tions, we assume a node represents an identical con-
cept if both annotators linked the node to the same
tectogrammatical node. For the remaining nodes,
we consider all possible one-to-one mappings and
construct the optimal mapping with respect to the F-
measure.
Formally, we start with a set of tectogrammatical
trees containing a set of nodes N. The annotation is
a tuple G = (V, E, T, A), where V are the vertices,
E C_ V x V x P are the directed edges and their la-
bels (e.g., agent of an action: AGT E P), T C_ V xN
is the mapping from vertices to the tectogrammati-
cal nodes, and finally A are attributes of the nodes,
which we ignore in this initial evaluation.3 Analo-
gously, G&apos; = (V &apos;, E&apos;, T&apos;, A&apos;) is another annotation
</bodyText>
<footnote confidence="0.999364714285714">
2The data associated with this experiment can be down-
loaded from http://ufal.mff.cuni.cz/∼novak/files/data.zip. The
data is in cedit format and can be viewed using the cedit editor
at http://ufal.mff.cuni.cz/∼novak/files/cedit.zip.
3We simplified the problem also by ignoring the mapping
from edges to tectogrammatical nodes and the MultiNet edge
attribute knowledge type.
</footnote>
<page confidence="0.997415">
40
</page>
<bodyText confidence="0.995503">
of the same sentence and our goal is to measure the
similarity s(G, G0) E [0, 1] of G and G0.
To measure the similarity we need a set Φ of ad-
missible one-to-one mappings between vertices in
the two annotations. A mapping is admissible if
it connects vertices which are indicated by the an-
notators as representing the same tectogrammatical
node:
</bodyText>
<equation confidence="0.998825777777778">
( φ C V x V 0~~~
Φ = (1)
V
n∈N
v∈V
n v VV
v0,w0∈V 0
v0 ∈SV/0 (((v,v0)∈φ∧(v,w0)∈φ)→(v0=w0) I
(((v,v0)∈φ∧(w,v0)∈φ)→(v=w)))
</equation>
<bodyText confidence="0.9995824">
In Equation 1, the first condition ensures that Φ is
constrained by the mapping induced by the links to
the tectogrammatical layer. The remaining two con-
ditions guarantee that Φ is a one-to-one mapping.
We define the annotation agreement s as:
</bodyText>
<equation confidence="0.988345">
s OE�
17(G, G&apos;) =max (F(G, G0, φ))
where F is the F1-measure:
2 � m(φ)
Fm(G, G0, φ) =
�E� + �E0�
</equation>
<bodyText confidence="0.9821205">
where m(φ) is the number of edges that match given
the mapping φ.
We use four versions of m, which gives us four
versions of F and consequently four scores s for ev-
</bodyText>
<equation confidence="0.974412318181818">
ery sentence:
Directed unlabeled: mdu(φ) =
����� n� �v0, w0, ρ0� E E0
(v,w,ρ)∈E ���Iv0,w0∈V 0,ρ0∈P
~o~~~~~
n (v, v0) E φ n (w, w0) E φ
Undirected unlabeled: muu(φ) =
����� n(v,w,ρ)∈E ~ ~Iv0,w0∈V 0,ρ0∈P
((v0, w0, ρ0) E E0 V (w0, v0, ρ0) E E0)
�o�����
n (v, v0) E φ n (w, w0) E φ
Directed labeled: mdl(φ) =
n��v0, w0, ρ� E E0
(v,w,ρ)∈E ���Iv0,w0∈V 0
~o~~~~~
n (v, v0) E φ n (w, w0) E φ
Undirected labeled: mul(φ) =
����� n �
(v,w,ρ)∈E ���Iv0,w0∈V 0
((v0, w0, ρ) E E0 V (w0, v0, ρ) E E0)
�o�����
n (v, v0) E φ n (w, w0) E φ
</equation>
<bodyText confidence="0.999279222222222">
These four m(φ) functions give us four possible
Fm measures, which allows us to have four scores
for every sentence: sdu, suu, sdl and sul.
Figure 3 shows that the inter-annotator agreement
is not significantly correlated with the position of the
sentence in the annotation process. This suggests
that the annotations for each annotator had achieved
a stable point (primarily due to the annotator training
process).
</bodyText>
<figure confidence="0.451537">
Sentence length
</figure>
<figureCaption confidence="0.99903325">
Figure 4: Inter-annotator agreement depending on the
sentence length. Each point represents a sentence.
Figure 4 shows that the agreement is not corre-
lated with the sentence length. It means that longer
</figureCaption>
<figure confidence="0.998570666666667">
10 20 30 40 50
Inter−annotator F−measure − Undirected Unlabeled
0.2 0.4 0.6 0.8 1.0
Annotators
CB−CW
SM−CW
SM−CB
�((v,n)∈T∧(v0,n)∈T0)→(v,v0)∈φJ
n �
v,w∈V
v0∈V 0
�����
</figure>
<page confidence="0.575882">
41
</page>
<figure confidence="0.9997804375">
Undirected Unlabeled F−measure
Undirected Labeled F−measure
Index
Index
0 10 20 30 40
0.2 0.4 0.6 0.8 1.0
Annotators
CB−CW
SM−CW
SM−CB
0 10 20 30 40
0.0 0.2 0.4 0.6
Annotators
CB−CW
SM−CW
SM−CB
</figure>
<figureCaption confidence="0.9973225">
Figure 3: Inter-annotator agreement over time. Left: unlabeled, right: labeled. Each point represents a sentence; CB,
CW, and SM are the annotators’ IDs.
</figureCaption>
<bodyText confidence="0.998959863636364">
sentences are not more difficult than short sentences.
The variance decreases with the sentence length as
expected.
In Figure 5 we show the comparison of directed
and labeled evaluations with the undirected unla-
beled case. By definition the undirected unlabeled
score is the upper bound for all the other scores.
The directed score is well correlated and not very
different from the undirected score, indicating that
the annotators did not have much trouble with de-
termining the correct direction of the edges. This
might be, in part, due to support from the formal-
ism and its tool cedit: each relation type is speci-
fied by a semantic-concept type signature; a relation
that violates its signature is reported immediately to
the annotator. On the other hand, labeled score is
significantly lower than the unlabeled score, which
suggests that the annotators have difficulties in as-
signing the correct relation types. The correlation
coefficient between s,,,, and s,,l (approx. 0.75) is
also much lower than than the correlation coefficient
between s,,,, and sd,, (approx. 0.95).
</bodyText>
<figureCaption confidence="0.659931666666667">
Figure 6 compares individual annotator pairs. The
scores are similar to each other and also have a sim-
ilar distribution shape.
</figureCaption>
<figure confidence="0.9141305">
Density
Undirected Unlabeled F−measure
</figure>
<figureCaption confidence="0.999889">
Figure 6: Comparison of individual annotator pairs.
</figureCaption>
<bodyText confidence="0.99638">
A more detailed comparison of individual anno-
tator pairs is depicted in Figure 7. The graph shows
that there is a significant positive correlation be-
tween scores, i.e. if two annotators can agree on the
</bodyText>
<figure confidence="0.985718333333333">
0.2 0.4 0.6 0.8 1.0
2.5
2.0
0.5
0.0
2.5
2.0
0.5
0.0
1.5
1.0
1.5
1.0
SM − CW
CB − CW
SM − CB
2.5
2.0
0.5
0.0
1.5
1.0
42
Directed Unlabeled F−measure
Undirected Labeled F−measure
Undirected Unlabeled F−measure
Undirected Unlabeled F−measure
0.2 0.4 0.6 0.8 1.0
Annotators
CB−CW
SM−CW
SM−CB
0.2 0.4 0.6 0.8
0.2 0.4 0.6 0.8 1.0
Annotators
CB−CW
SM−CW
SM−CB
0.0 0.2 0.4 0.6
</figure>
<figureCaption confidence="0.9854925">
Figure 5: Left: Directed vs. undirected inter-annotator agreement. Right: Labeled vs. unlabeled inter-annotator agree-
ment. Each point represents a sentence.
</figureCaption>
<bodyText confidence="0.999723">
annotation, the third is likely to also agree, but this
correlation is not a very strong one. The actual cor-
relation coefficients are shown under the main diag-
onal of the matrix.
</bodyText>
<table confidence="0.999555428571429">
Sample Annotators Agreement F-measure
Suu Sdu Sul Sdl
Smaller CB-CW 61.0 56.3 37.1 35.0
Smaller SM-CB 54.9 48.5 27.1 25.7
Smaller SM-CW 58.5 50.7 31.3 30.2
Smaller average 58.1 51.8 31.8 30.3
Larger CB-CW 64.6 59.8 40.1 38.5
</table>
<tableCaption confidence="0.913405666666667">
Table 1: Inter-annotator agreement in percents. The re-
sults come from the two samples described in the first
paragraph of Section 3.
</tableCaption>
<bodyText confidence="0.999852333333333">
Finally, we summarize the raw result in Table 1.
Note that we report simple annotator agreement
here.
</bodyText>
<sectionHeader confidence="0.993539" genericHeader="evaluation">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999937652173913">
We have presented a novel framework for the anno-
tation of semantic network for natural language dis-
course. Additionally we present a technique to eval-
uate the agreement between the semantic networks
annotated by different annotators.
Our evaluation of an initial dataset reveals that
given the current tools and annotation guidelines, the
annotators are able to construct the structure of the
semantic network (i.e., they are good at building the
directed graph). They are not, however, able to con-
sistently label the semantic relations between the se-
mantic nodes. In our future work, we will investigate
the difficulty in labeling semantic annotations. We
would like to determine whether this is a product of
the annotation guidelines, the tool, or the formalism.
Our ongoing research include the annotation of
inter-sentential coreference relationships between
the semantic concepts within the sentence-based
graphs. These relationships link the local structures,
allowing for a complete semantic interpretation of
the discourse. Given the current level of consistency
in structural annotation, we believe the data will be
useful in this analysis.
</bodyText>
<page confidence="0.999709">
43
</page>
<figure confidence="0.888959666666667">
Undirected Unlabeled F−measure with Correlation Coefficients
0.2 0.4 0.6 0.8
0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8
</figure>
<figureCaption confidence="0.9989068">
Figure 7: Undirected, unlabeled F-measure correlation of annotator pairs. Each cell represents two different pairs of
annotators; cells with graphs show scatter-plots of F-scores for the annotator pairs along with the optimal linear fit;
cells with values show the correlation coefficient (each point in the plot corresponds to a sentence). For example,
the top row, right-most column, we are comparing the F-score agreement of annotators CB and CW with that of the
F-score agreement of annotators SM and CB. This should help identify an outlier in the consistency of the annotations.
</figureCaption>
<figure confidence="0.999790485436894">
0.55
0.56
SM_CB
CB_CW
0.34
SM_CW
0.2 0.4 0.6 0.8 1.0
0.2 0.4 0.6 0.8
●
●
●
●
●
●
●●
●
●● ●
● ● ●
●
● ● ●
●
●
●
●
● ● ●
●
●
●
●
●
●
●
●
● ●●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
● ● ● ●
●
●●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
● ●●● ●●
●
●
●
●
●
●●
●
●
●
●
● ●
●●●
●
●●
●
●
●
0.2 0.4 0.6 0.8
</figure>
<sectionHeader confidence="0.952474" genericHeader="conclusions">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.999944375">
This work was partially supported by Czech
Academy of Science grants 1ET201120505 and
1ET101120503; by Czech Ministry of Educa-
tion, Youth and Sports projects LC536 and
MSM0021620838; and by the US National Science
Foundation under grant OISE–0530118. The views
expressed are not necessarily endorsed by the spon-
sors.
</bodyText>
<sectionHeader confidence="0.999646" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998315">
Mike Badger. 2007. Dokuwiki – A Practical Open
Source Knowledge Base Solution. Enterprise Open
Source Magazine.
Johan Bos. 2008. Let’s not Argue about Semantics. In
European Language Resources Association (ELRA),
editor, Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC’08), Mar-
rakech, Morocco, may.
Jan Cuˇr´ın, Martin ˇCmejrek, Jiˇr´ı Havelka, and Vladislav
Kuboˇn. 2004. Building parallel bilingual syntacti-
cally annotated corpus. In Proceedings of The First
International Joint Conference on Natural Language
Processing, pages 141–146, Hainan Island, China.
Carsten Gn¨orlich. 2000. MultiNet/WR: A Knowledge
Engineering Toolkit for Natural Language Informa-
tion. Technical Report 278, University Hagen, Hagen,
Germany.
Jan Hajiˇc, Jarmila Panevov´a, Eva Hajiˇcov´a, Petr Sgall,
Petr Pajas, Jan ˇStˇep´anek, Jiˇr´ı Havelka, and Marie
Mikulov´a. 2006. Prague Dependency Treebank 2.0.
</reference>
<page confidence="0.979557">
44
</page>
<reference confidence="0.99978056097561">
CD-ROM, Linguistic Data Consortium, LDC Catalog
No.: LDC2006T01, Philadelphia, Pennsylvania.
Sven Hartrumpf, Hermann Helbig, and Rainer Osswald.
2003. The Semantically Based Computer Lexicon Ha-
GenLex – Structure and Technological Environment.
TraitementAutomatique des Langues, 44(2):81–105.
Hermann Helbig. 2006. Knowledge Representation and
the Semantics of Natural Language. Springer, Berlin,
Germany.
Bo Leuf and Ward Cunningham. 2001. The Wiki Way.
Quick Collaboration on the Web. Addison-Wesley,
Reading, Massachusetts.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn treebank. Computational
Linguistics, 19(2):313–330.
V´aclav Nov´ak. 2007. Cedit – semantic networks man-
ual annotation tool. In Proceedings of Human Lan-
guage Technologies: The Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL-HLT), pages 11–12,
Rochester, New York, April. Association for Compu-
tational Linguistics.
Petr Pajas and Jan ˇStˇep´anek. 2005. A Generic XML-
Based Format for Structured Linguistic Annotation
and Its Application to Prague Dependency Treebank
2.0. Technical Report 29, UFAL MFF UK, Praha,
Czech Republic.
Petr Sgall, Eva Hajiˇcov´a, and Jarmila Panevov´a. 1986.
The Meaning of the Sentence in Its Semantic and Prag-
matic Aspects. D. Reidel, Dordrecht, The Netherlands.
Petr Sgall, Jarmila Panevov´a, and Eva Hajiˇcov´a. 2004.
Deep syntactic annotation: Tectogrammatical repre-
sentation and beyond. In Adam Meyers, editor, Pro-
ceedings of the HLT-NAACL 2004 Workshop: Fron-
tiers in Corpus Annotation, pages 32–38, Boston,
Massachusetts, May. Association for Computational
Linguistics.
Sun Microsystems, Inc. 2007. Java Platform, Standard
Edition 6. http://java.sun.com/javase/6/webnotes/
README.html.
</reference>
<page confidence="0.999388">
45
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.229563">
<title confidence="0.99944">Large-scale Semantic Networks: Annotation and Evaluation</title>
<author confidence="0.930668">V´aclav</author>
<affiliation confidence="0.7010745">Institute of Formal and Applied Charles University in Prague, Czech</affiliation>
<email confidence="0.860986">novak@ufal.mff.cuni.cz</email>
<author confidence="0.799602">Hartrumpf Keith</author>
<affiliation confidence="0.9636355">Computer Science Department Google Research University of Hagen, Germany Z¨urich, Switzerland</affiliation>
<email confidence="0.960223">Sven.Hartrumpf@FernUni-Hagen.dekbhall@google.com</email>
<abstract confidence="0.987718666666667">We introduce a large-scale semantic-network annotation effort based on the MutliNet formalism. Annotation is achieved via a process which incorporates several independent tools including a MultiNet graph editing tool, a semantic concept lexicon, a user-editable knowledge-base for semantic concepts, and a MultiNet parser. We present an evaluation metric for these semantic networks, allowing us to determine the quality of annotations in terms of inter-annotator agreement. We use this metric to report the agreement rates for a pilot annotation effort involving three annotators.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mike Badger</author>
</authors>
<title>Dokuwiki – A Practical Open Source Knowledge Base Solution. Enterprise Open Source Magazine.</title>
<date>2007</date>
<contexts>
<context position="11307" citStr="Badger, 2007" startWordPosition="1706" endWordPosition="1707"> the parse is not optimal, it will not be visible to annotators. 2.3 Online Parser Sometimes the annotator needs to look up a phrase or something more general than a particular noun or verb. In this case, the annotator can use the workbench for (MultiNet) knowledge bases (MWR (Gn¨orlich, 2000)), which provides convenient and quick access to the parser that translates German sentences or phrases into MultiNets. 2.4 Wiki Knowledge Base A wiki (Leuf and Cunningham, 2001) is used collaboratively to create and maintain the knowledge base used by all the annotators. In this project we use Dokuwiki (Badger, 2007). The entries of individual annotators in the wiki are logged and a feed of changes can be observed using an RSS reader. The cedit annotation tool allows users to display appropriate wiki pages of individual relation types, function types and attributes directly from the tool using their preferred web browser. 3 Network Evaluation We present an evaluation which has been carried out on an initial set of annotations of English articles from The Wall Street Journal (covering those annotated at the syntactic level in the Penn Treebank (Marcus et al., 1993)). We use the annotation from the Prague C</context>
</contexts>
<marker>Badger, 2007</marker>
<rawString>Mike Badger. 2007. Dokuwiki – A Practical Open Source Knowledge Base Solution. Enterprise Open Source Magazine.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Let’s not Argue about Semantics.</title>
<date>2008</date>
<booktitle>In European Language Resources Association (ELRA), editor, Proceedings of the Sixth International Language Resources and Evaluation (LREC’08),</booktitle>
<location>Marrakech, Morocco,</location>
<contexts>
<context position="3243" citStr="Bos, 2008" startWordPosition="474" endWordPosition="475">t graph does not represent the same semantics as a syntactico-semantic dependency tree. The nodes of the MultiNet graph are connected based on a corpus-wide interpretation of the entities referred to in the corpus. These global connections are determined by the intra-sentential interpretation but are not restricted to that interpretation. Therefore, the procedure for computing annotator agreement differs from the standard approaches to evaluating syntactic and semantic dependency treebanks (e.g., dependency link agreement, label agreement, predicate-argument structure agreement). As noted in (Bos, 2008), “Even though the design of annotation schemes has been initiated for single semantic phenomena, there exists no annotation scheme (as far as I know) that aims to inte37 Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 37–45, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics grate a wide range of semantic phenomena all at once. It would be welcome to have such a resource at ones disposal, and ideally a semantic annotation scheme should be multi-layered, where certain semantic phenomena can be properly </context>
</contexts>
<marker>Bos, 2008</marker>
<rawString>Johan Bos. 2008. Let’s not Argue about Semantics. In European Language Resources Association (ELRA), editor, Proceedings of the Sixth International Language Resources and Evaluation (LREC’08), Marrakech, Morocco, may.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Cuˇr´ın</author>
<author>Martin ˇCmejrek</author>
<author>Jiˇr´ı Havelka</author>
<author>Vladislav Kuboˇn</author>
</authors>
<title>Building parallel bilingual syntactically annotated corpus.</title>
<date>2004</date>
<booktitle>In Proceedings of The First International Joint Conference on Natural Language Processing,</booktitle>
<pages>141--146</pages>
<location>Hainan Island, China.</location>
<marker>Cuˇr´ın, ˇCmejrek, Havelka, Kuboˇn, 2004</marker>
<rawString>Jan Cuˇr´ın, Martin ˇCmejrek, Jiˇr´ı Havelka, and Vladislav Kuboˇn. 2004. Building parallel bilingual syntactically annotated corpus. In Proceedings of The First International Joint Conference on Natural Language Processing, pages 141–146, Hainan Island, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carsten Gn¨orlich</author>
</authors>
<title>MultiNet/WR: A Knowledge Engineering Toolkit for Natural Language Information.</title>
<date>2000</date>
<tech>Technical Report 278,</tech>
<location>University Hagen, Hagen, Germany.</location>
<marker>Gn¨orlich, 2000</marker>
<rawString>Carsten Gn¨orlich. 2000. MultiNet/WR: A Knowledge Engineering Toolkit for Natural Language Information. Technical Report 278, University Hagen, Hagen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Jarmila Panevov´a</author>
<author>Eva Hajiˇcov´a</author>
<author>Petr Sgall</author>
<author>Petr Pajas</author>
<author>Jan ˇStˇep´anek</author>
<author>Jiˇr´ı Havelka</author>
<author>Marie Mikulov´a</author>
</authors>
<date>2006</date>
<journal>Prague Dependency Treebank</journal>
<volume>2</volume>
<marker>Hajiˇc, Panevov´a, Hajiˇcov´a, Sgall, Pajas, ˇStˇep´anek, Havelka, Mikulov´a, 2006</marker>
<rawString>Jan Hajiˇc, Jarmila Panevov´a, Eva Hajiˇcov´a, Petr Sgall, Petr Pajas, Jan ˇStˇep´anek, Jiˇr´ı Havelka, and Marie Mikulov´a. 2006. Prague Dependency Treebank 2.0.</rawString>
</citation>
<citation valid="false">
<authors>
<author>CD-ROM</author>
</authors>
<booktitle>Linguistic Data Consortium, LDC Catalog No.: LDC2006T01,</booktitle>
<location>Philadelphia, Pennsylvania.</location>
<marker>CD-ROM, </marker>
<rawString>CD-ROM, Linguistic Data Consortium, LDC Catalog No.: LDC2006T01, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven Hartrumpf</author>
<author>Hermann Helbig</author>
<author>Rainer Osswald</author>
</authors>
<date>2003</date>
<booktitle>The Semantically Based Computer Lexicon HaGenLex – Structure and Technological Environment. TraitementAutomatique des Langues,</booktitle>
<volume>44</volume>
<issue>2</issue>
<contexts>
<context position="9865" citStr="Hartrumpf et al., 2003" startWordPosition="1467" endWordPosition="1470">onnected to the annotators’ wiki (see Section 2.4), where annotators can access the definitions of individual MultiNet semantic relations, functions and attributes; as well as examples, counterexamples, and discussion concerning the entity in question. If the wiki page does not contain the required information, the annotator is encouraged to edit the page with his/her questions and comments. 2.2 Online Lexicon The annotators in the semantic annotation project have the option to look up examples of MultiNet structures in an online version of the semantically oriented computer lexicon HaGenLex (Hartrumpf et al., 2003). The annotators can use lemmata (instead of reading IDs formed of the lemma and a numerical suffix) for the query, thus increasing the recall of related structures. English and German input is supported with outputs in English and/or German; there are approximately 3,000 and 25,000 semantic networks, respectively, in the lexicon. An example sentence for the German verb “borgen.1.1” (“to borrow”) plus its automatically generated and val39 Figure 2: HaGenLex entry showing an example sentence for the German verb “borgen.1.1” (“to borrow”). The sentence is literally “The man borrows himself money</context>
</contexts>
<marker>Hartrumpf, Helbig, Osswald, 2003</marker>
<rawString>Sven Hartrumpf, Hermann Helbig, and Rainer Osswald. 2003. The Semantically Based Computer Lexicon HaGenLex – Structure and Technological Environment. TraitementAutomatique des Langues, 44(2):81–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Helbig</author>
</authors>
<title>Knowledge Representation and the Semantics of Natural Language.</title>
<date>2006</date>
<publisher>Springer,</publisher>
<location>Berlin, Germany.</location>
<contexts>
<context position="1081" citStr="Helbig, 2006" startWordPosition="143" endWordPosition="144">n is achieved via a process which incorporates several independent tools including a MultiNet graph editing tool, a semantic concept lexicon, a user-editable knowledge-base for semantic concepts, and a MultiNet parser. We present an evaluation metric for these semantic networks, allowing us to determine the quality of annotations in terms of inter-annotator agreement. We use this metric to report the agreement rates for a pilot annotation effort involving three annotators. 1 Introduction In this paper we propose an annotation framework which integrates the MultiNet semantic network formalism (Helbig, 2006) and the syntacticosemantic formalism of the Prague Dependency Treebank (Hajiˇc et al., 2006) (PDT). The primary goal of this task is to increase the interoperability of these two frameworks in order to facilitate efforts to annotate at the semantic level while preserving intrasentential semantic and syntactic annotations as are found in the PDT. The task of annotating text with global semantic interactions (e.g., semantic interactions within some discourse) presents a cognitively demanding problem. As with many other annotation formalisms, ∗Part of this work was completed while at the Johns H</context>
<context position="4628" citStr="Helbig, 2006" startWordPosition="690" endWordPosition="691">mmatical Representation (TR) of the PDT. Section 2 describes the annotation process in detail, including an introduction to the encyclopedic tools available to the annotators. In Section 3 we present an evaluation metric for MultiNet/TR labeled data. We also present an evaluation of the data we have had annotated using the proposed procedure. Finally, we conclude with a short discussion of the problems observed during the annotation process and suggest improvements as future work. 1.1 MultiNet The representation of the Multilayered Extended Semantic Networks (MultiNet), which is described in (Helbig, 2006), provides a universal formalism for the treatment of semantic phenomena of natural language. To this end, they offer distinct advantages over the use of the classical predicate calculus and its derivatives. For example, MultiNet provides a rich ontology of semantic-concept types. This ontology has been constructed to be language independent. Due to the graphical interpretation of MultiNets, we believe manual annotation and interpretation is simpler and thus more cognitively compatible. Figure 1 shows the MultiNet annotation of a sentence from the WSJ corpus: “Stephen Akerfeldt, currently vice</context>
</contexts>
<marker>Helbig, 2006</marker>
<rawString>Hermann Helbig. 2006. Knowledge Representation and the Semantics of Natural Language. Springer, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Leuf</author>
<author>Ward Cunningham</author>
</authors>
<title>The Wiki Way. Quick Collaboration on the Web.</title>
<date>2001</date>
<publisher>Addison-Wesley,</publisher>
<location>Reading, Massachusetts.</location>
<contexts>
<context position="11166" citStr="Leuf and Cunningham, 2001" startWordPosition="1680" endWordPosition="1683"> The quality of example parses is assured by comparing the marked-up complements in the example to the ones in the semantic network. In the rare case that the parse is not optimal, it will not be visible to annotators. 2.3 Online Parser Sometimes the annotator needs to look up a phrase or something more general than a particular noun or verb. In this case, the annotator can use the workbench for (MultiNet) knowledge bases (MWR (Gn¨orlich, 2000)), which provides convenient and quick access to the parser that translates German sentences or phrases into MultiNets. 2.4 Wiki Knowledge Base A wiki (Leuf and Cunningham, 2001) is used collaboratively to create and maintain the knowledge base used by all the annotators. In this project we use Dokuwiki (Badger, 2007). The entries of individual annotators in the wiki are logged and a feed of changes can be observed using an RSS reader. The cedit annotation tool allows users to display appropriate wiki pages of individual relation types, function types and attributes directly from the tool using their preferred web browser. 3 Network Evaluation We present an evaluation which has been carried out on an initial set of annotations of English articles from The Wall Street </context>
</contexts>
<marker>Leuf, Cunningham, 2001</marker>
<rawString>Bo Leuf and Ward Cunningham. 2001. The Wiki Way. Quick Collaboration on the Web. Addison-Wesley, Reading, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="11865" citStr="Marcus et al., 1993" startWordPosition="1800" endWordPosition="1803">the annotators. In this project we use Dokuwiki (Badger, 2007). The entries of individual annotators in the wiki are logged and a feed of changes can be observed using an RSS reader. The cedit annotation tool allows users to display appropriate wiki pages of individual relation types, function types and attributes directly from the tool using their preferred web browser. 3 Network Evaluation We present an evaluation which has been carried out on an initial set of annotations of English articles from The Wall Street Journal (covering those annotated at the syntactic level in the Penn Treebank (Marcus et al., 1993)). We use the annotation from the Prague Czech-English Dependency Treebank (Cuˇr´ın et al., 2004), which contains a large portion of the WSJ Treebank annotated according to the PDT annotation scheme (including all layers of the FGD formalism). We reserved a small set of data to be used to train our annotators and have excluded these articles from the evaluation. Three native English-speaking annotators were trained and then asked to annotate sentences from the corpus. We have a sample of 67 sentences (1793 words) annotated by two of the annotators; of those, 46 sentences (1236 words) were anno</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: the Penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V´aclav Nov´ak</author>
</authors>
<title>Cedit – semantic networks manual annotation tool.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT),</booktitle>
<pages>11--12</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<marker>Nov´ak, 2007</marker>
<rawString>V´aclav Nov´ak. 2007. Cedit – semantic networks manual annotation tool. In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT), pages 11–12, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Pajas</author>
<author>Jan ˇStˇep´anek</author>
</authors>
<title>A Generic XMLBased Format for Structured Linguistic Annotation and Its Application to Prague Dependency Treebank 2.0.</title>
<date>2005</date>
<tech>Technical Report 29, UFAL MFF</tech>
<location>UK, Praha, Czech Republic.</location>
<marker>Pajas, ˇStˇep´anek, 2005</marker>
<rawString>Petr Pajas and Jan ˇStˇep´anek. 2005. A Generic XMLBased Format for Structured Linguistic Annotation and Its Application to Prague Dependency Treebank 2.0. Technical Report 29, UFAL MFF UK, Praha, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Sgall</author>
<author>Eva Hajiˇcov´a</author>
<author>Jarmila Panevov´a</author>
</authors>
<title>The Meaning of the Sentence in Its Semantic and Pragmatic Aspects.</title>
<date>1986</date>
<location>D. Reidel, Dordrecht, The Netherlands.</location>
<marker>Sgall, Hajiˇcov´a, Panevov´a, 1986</marker>
<rawString>Petr Sgall, Eva Hajiˇcov´a, and Jarmila Panevov´a. 1986. The Meaning of the Sentence in Its Semantic and Pragmatic Aspects. D. Reidel, Dordrecht, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Sgall</author>
<author>Jarmila Panevov´a</author>
<author>Eva Hajiˇcov´a</author>
</authors>
<title>Deep syntactic annotation: Tectogrammatical representation and beyond.</title>
<date>2004</date>
<booktitle>Proceedings of the HLT-NAACL 2004 Workshop: Frontiers in Corpus Annotation,</booktitle>
<pages>32--38</pages>
<editor>In Adam Meyers, editor,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Boston, Massachusetts,</location>
<marker>Sgall, Panevov´a, Hajiˇcov´a, 2004</marker>
<rawString>Petr Sgall, Jarmila Panevov´a, and Eva Hajiˇcov´a. 2004. Deep syntactic annotation: Tectogrammatical representation and beyond. In Adam Meyers, editor, Proceedings of the HLT-NAACL 2004 Workshop: Frontiers in Corpus Annotation, pages 32–38, Boston, Massachusetts, May. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<date>2007</date>
<journal>Java Platform, Standard Edition</journal>
<volume>6</volume>
<institution>Sun Microsystems, Inc.</institution>
<note>http://java.sun.com/javase/6/webnotes/ README.html.</note>
<marker>2007</marker>
<rawString>Sun Microsystems, Inc. 2007. Java Platform, Standard Edition 6. http://java.sun.com/javase/6/webnotes/ README.html.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>