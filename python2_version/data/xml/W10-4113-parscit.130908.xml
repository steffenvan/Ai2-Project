<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013176">
<title confidence="0.987092">
Selecting Optimal Feature Template Subset for CRFs
</title>
<author confidence="0.9910215">
Xingjun Xu1 and Guanglu Sun2 and Yi Guan1 and
Xishuang Dong1 and Sheng Li1
</author>
<affiliation confidence="0.936833333333333">
1: School of Computer Science and Technology,
Harbin Institute of Technology,
150001, Harbin, China
2: School of Computer Science and Technology,
Harbin University of Science and Technology
150080, Harbin, China
</affiliation>
<email confidence="0.972259333333333">
xxjroom@163.com; guanglu.sun@gmail.com
guanyi@hit.edu.cn; dongxishuang@gmail.com
lisheng@hit.edu.cn
</email>
<sectionHeader confidence="0.995354" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99716275">
Conditional Random Fields (CRFs) are the
state-of-the-art models for sequential labe-
ling problems. A critical step is to select
optimal feature template subset before em-
ploying CRFs, which is a tedious task. To
improve the efficiency of this step, we pro-
pose a new method that adopts the maxi-
mum entropy (ME) model and maximum
entropy Markov models (MEMMs) instead
of CRFs considering the homology be-
tween ME, MEMMs, and CRFs. Moreover,
empirical studies on the efficiency and ef-
fectiveness of the method are conducted in
the field of Chinese text chunking, whose
performance is ranked the first place in
task two of CIPS-ParsEval-2009.
</bodyText>
<sectionHeader confidence="0.999088" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999010605263158">
Conditional Random Fields (CRFs) are the state-
of-the-art models for sequential labeling problem.
In natural language processing, two aspects of
CRFs have been investigated sufficiently: one is to
apply it to new tasks, such as named entity recog-
nition (McCallum and Li, 2003; Li and McCallum,
2003; Settles, 2004), part-of-speech tagging (Laf-
ferty et al., 2001), shallow parsing (Sha and Perei-
ra, 2003), and language modeling (Roark et al.,
2004); the other is to exploit new training methods
for CRFs, such as improved iterative scaling (Laf-
ferty et al., 2001), L-BFGS (McCallum, 2003) and
gradient tree boosting (Dietterich et al., 2004).
One of the critical steps is to select optimal fea-
ture subset before employing CRFs. McCallum
(2003) suggested an efficient method of feature
induction by iteratively increasing conditional log-
likelihood for discrete features. However, since
there are millions of features and feature selection
is an NP problem, this is intractable when search-
ing optimal feature subset. Therefore, it is neces-
sary that selects feature at feature template level,
which reduces input scale from millions of fea-
tures to tens or hundreds of candidate templates.
In this paper, we propose a new method that
adopts ME and MEMMs instead of CRFs to im-
prove the efficiency of selecting optimal feature
template subset considering the homology between
ME, MEMMs, and CRFs, which reduces the train-
ing time from hours to minutes without loss of
performance.
The rest of this paper is organized as follows.
Section 2 presents an overview of previous work
for feature template selection. We propose our op-
timal method for feature template selection in Sec-
tion 3. Section 4 presents our experiments and re-
sults. Finally, we end this paper with some con-
cluding remarks.
</bodyText>
<sectionHeader confidence="0.99957" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999635916666667">
Feature selection can be carried out from two le-
vels: feature level (feature selection, or FS), or
feature template level (feature template selection,
or FTS). FS has been sufficiently investigated and
share most concepts with FTS. For example, the
target of FS is to select a subset from original fea-
ture set, whose optimality is measured by an eval-
uation criterion (Liu and Yu, 2005). Similarly, the
target of FTS is to select a subset from original
feature template set. To achieve optimal feature
subset, two problems in original set must be elimi-
nated: irrelevance and redundancy (Yu and Liu,
2004). The only difference between FS and FTS is
that the number of elements in feature template set
is much less than that in feature set.
Liu and Yu (2005) classified FS models into
three categories: the filter model, the wrapper
model, and the hybrid model. The filter model
(Hall 2000; Liu and Setiono, 1996; Yu and Liu,
2004) relies on general characteristics of the data
to evaluate and select feature subsets without any
machine learning model. The wrapper model (Dy
and Brodley, 2000; Kim et al., 2000; Kohavi and
John, 1997) requires one predetermined machine
learning model and uses its performance as the
evaluation criterion. The hybrid model (Das, 2001)
attempts to take advantage of the two models by
exploiting their different evaluation criteria in dif-
ferent search stages.
There are two reasons to employ the wrapper
model to accomplish FTS: (1) The wrapper model
tends to achieve better effectiveness than that of
the filter model with respect of a more direct eval-
uation criterion; (2) The computational cost is trac-
table because it can reduce the number of subsets
sharply by heuristic algorithm according to the
human knowledge. And our method belongs to
this type.
Lafferty (2001) noticed the homology between
MEMMs and CRFs, and chose optimal MEMMs
parameter vector as a starting point for training the
corresponding CRFs. And the training process of
CRFs converges faster than that with all zero pa-
rameter vectors.
On the other hand, the general framework that
processes sequential labeling with CRFs has also
been investigated well, which can be described as
follows:
</bodyText>
<listItem confidence="0.993500333333333">
1. Converting the new problem to sequential
labeling problem;
2. Selecting optimal feature template subset for
CRFs;
3. Parameter estimation for CRFs;
4. Inference for new data.
</listItem>
<bodyText confidence="0.9998934">
In the field of English text chunking (Sha and
Pereira, 2003), the step 1, 3, and 4 have been stu-
died sufficiently, whereas the step 2, how to select
optimal feature template subset efficiently, will be
the main topic of this paper.
</bodyText>
<sectionHeader confidence="0.937402" genericHeader="method">
3 Feature Template Selection
</sectionHeader>
<subsectionHeader confidence="0.999184">
3.1 The Wrapper Model for FTS
</subsectionHeader>
<bodyText confidence="0.9997335">
The framework of FTS based on the wrapper
model for CRFs can be described as:
</bodyText>
<listItem confidence="0.9772585">
1. Generating the new feature template subset;
2. Training a CRFs model;
3. Updating optimal feature template subset if the
new subset is better;
4. Repeating step 1, 2, 3 until there are no new
feature template subsets.
</listItem>
<bodyText confidence="0.962183">
Let N denote the number of feature templates,
the number of non-empty feature template subsets
will be (2N-1). And the wrapper model is unable to
deal with such case without heuristic methods,
which contains:
</bodyText>
<listItem confidence="0.976390411764706">
1. Atomic feature templates are firstly added to
feature template subset, which is carried out by:
Given the position i, the current word Wi and the
current part-of-speech Pi are firstly added to cur-
rent feature template subset, and then Wi-1 and Pi-1,
or Wi+1 and Pi+1, and so on, until the effectiveness
is of no improvement. Taking the Chinese text
chunking as example, optimal atomic feature tem-
plate subset is {Wi-3—Wi+3, Pi-3—Pi+3};
2. Adding combined feature templates properly
to feature template set will be helpful to improve
the performance, however, too many combined
feature templates will result in severe data sparse-
ness problem. Therefore, we present three restric-
tions for combined feature templates: (1) A com-
bined feature template that contains more than
three atomic templates are not allowable; (2) If a
combined feature template contains three atomic
feature template, it can only contain at most one
atomic word template; (3) In a combined template,
at most one word is allowable between the two
most adjacent atomic templates; For example, the
combined feature templates, such as {Pi-1, Pi, Pi+1,
Pi+2}, {Wi, Wi+1, Pi}, and {Pi-1, Pi+2}, are not al-
lowable, whereas the combined templates, such as
{Pi, Pi+1, Pi+2}, {Pi-1, Wi, Pi+1}, and {Pi-1, Pi+1}, are
allowable.
3. After atomic templates have been added, {Wi-
1, Wi}, or {Wi, Wi+1}, or {Pi-1, Pi}, or {Pi, Pi+1} are
firstly added to feature template subset. The tem-
plate window is moved forward, and then back-
ward. Such process will repeat with expanding
template window, until the effectiveness is of no
improvement.
</listItem>
<bodyText confidence="0.999925214285714">
Tens or hundreds of training processes are still
needed even if the heuristic method is introduced.
People usually employ CRFs model to estimate the
effectiveness of template subset However, this is
more tedious than that we use ME or MEMMs
instead. The idea behind this lie in three aspects:
first, in one iteration, the Forward-Backward Al-
gorithm adopted in CRFs training is time-
consuming; second, CRFs need more iterations
than that of ME or MEMMs to converge because
of larger parameter space; third, ME, MEMMs,
and CRFs, are of the same type (log-linear models)
and based on the same principle, as will be dis-
cussed in detail as follows.
</bodyText>
<subsectionHeader confidence="0.998711">
3.2 Homology of ME, MEMMs and CRFs
</subsectionHeader>
<bodyText confidence="0.978356">
ME, MEMMs, and CRFs are all based on the Prin-
ciple of Maximum Entropy (Jaynes, 1957). The
mathematical expression for ME model is as for-
mula (1):
</bodyText>
<equation confidence="0.902552">
(1)
</equation>
<bodyText confidence="0.985648652173913">
, and Z(x) is the normalization factor.
MEMMs can be considered as a sequential ex-
tension to the ME model. In MEMMs, the HMM
transition and observation functions are replaced
by a single function P(Yi|Yi-1, Xi). There are three
kinds of implementations of MEMMs (McCallum
et al., 2000) in which we realized the second type
for its abundant expressiveness. In implementation
two, which is denoted as MEMMs_2 in this paper,
a distributed representation for the previous state
Yi-1 is taken as a collection of features with
weights set by maximum entropy, just as we have
done for the observations Xi. However, label bias
problem (Lafferty et al., 2001) exists in MEMMs,
since it makes a local normalization of random
field models. CRFs overcome the label bias prob-
lem by global normalization.
Considering the homology between CRFs and
MEMMs_2 (or ME), it is reasonable to suppose
that a useful template for MEMMs_2 (or ME) is
also useful for CRFs, and vice versa. And this is a
necessary condition to replace CRFs with ME or
MEMMs for FTS.
</bodyText>
<subsectionHeader confidence="0.993128">
3.3 A New Framework for FTS
</subsectionHeader>
<bodyText confidence="0.999783526315789">
Besides the homology of these models, the other
necessary condition to replace CRFs with ME or
MEMMs for FTS is that all kinds of feature tem-
plates in CRFs can also be expressed by ME or
MEMMs. There are two kinds of feature templates
for CRFs: one is related to Yi-1, which is denoted
as g(Yi-1, Yi, Xi); the other is not related to Yi-1,
which is denoted as f(Yi, Xi). Both of them can be
expressed by MEMMs_2. If there is only the
second kind of feature templates in the subset, it
can also be expressed by ME. For example, the
feature function f(Yi, Pi) in CRFs can be expressed
by feature template {Pi} in MEMMs_2 or ME; and
g(Yi-1, Yi, Pi) can be expressed by feature template
{Yi-1, Pi} in MEMM_2.
Therefore, MEMMs_2 or ME can be employed
to replace CRFs as machine learning model for
improving the efficiency of FTS.
Then the new framework for FTS will be:
</bodyText>
<listItem confidence="0.989494833333333">
1. Generating the new feature template subset;
2. Training an MEMMs_2 or ME model;
3. Updating optimal feature template subset
if the new subset is better;
4. Repeating step 1, 2, 3 until there are no
new feature template subsets.
</listItem>
<bodyText confidence="0.99980475">
The wrapper model evaluates the effectiveness
of feature template subset by evaluating the model
on testing data. However, there is a serious effi-
ciency problem when decoding a sequence by
MEMMs_2. Given N as the length of a sentence,
C as the number of candidate labels, the time
complexity based on MEMMs_2 is O(NC2) when
decoding by viterbi algorithm. Considering the C
different Yi-1 for every word in a sentence, we
need compute P(Yi|Yi-1, Xi) (N.C) times for
MEMMs_2.
Reducing the average number of candidate label
C can help to improve the decoding efficiency.
And in most cases, the Yi-1 in P(Yi|Yi-1, Xi) is not
necessary (Koeling, 2000; Osbome, 2000). There-
fore, to reduce the average number of candidate
labels C, it is reasonable to use an ME model to
filter the candidate label. Given a threshold T (0
&lt;= T &lt;= 1), the candidate label filtering algorithm
is as follows:
</bodyText>
<listItem confidence="0.997530333333333">
1. CP = 0;
2. While CP &lt;= T
a) Add the most probable candidate label Y’
to viterbi algorithm;
b) Delete Y’ from the candidate label set;
c) CP = P(Y’|Xi) + CP.
</listItem>
<bodyText confidence="0.9985495">
If the probability of the most probable candidate
label has surpassed T, other labels are discarded.
Otherwise, more labels need be added to viterbi
algorithm.
</bodyText>
<sectionHeader confidence="0.990175" genericHeader="evaluation">
4 Evaluation and Result
</sectionHeader>
<subsectionHeader confidence="0.809429">
4.1 Evaluation
</subsectionHeader>
<bodyText confidence="0.998580392857143">
We evaluate the effectiveness and efficiency of the
new framework by the data set in the task two of
CIPS-ParsEval-2009 (Zhou and Li, 2010). The
effectiveness is supported by high F-1 measure in
the task two of CIPS-ParsEval-2009 (see Figure 1),
which shows that optimal feature template subset
driven by ME or MEMMs is also optimal for
CRFs. The efficiency is shown by significant de-
cline in training time (see Figure 3), where the
baseline is CRFs, and comparative methods are
ME or MEMMs.
We design six subsets of feature template set
and six experiments to show the effectiveness and
efficiency of the new framework. As shown in
Table 1 and Table 2, the 1~3 experiments shows
the influence of the feature templates, which are
unrelated to Yi-1, for both ME and CRFs. And the
4~6 experiments show the influence of the feature
templates, which are related to Yi-1, for both
MEMMs_2 and CRFs. In table 1, six template
subsets can be divided into two sets by relevance
of previous label: 1, 2, 3 and 4, 5, 6. Moreover, the
first set can be divided into 1, 2, and 3 by distances
between features with headwords; the second set
can be divided into 4, 5 and 6 by relevance of ob-
served value. In order to ensure the objectivity of
comparative experiments, candidate label filtering
algorithm is not adopted.
</bodyText>
<figureCaption confidence="0.991929">
Figure 1: the result in the task two of CIPS-
</figureCaption>
<table confidence="0.959440785714286">
ParsEval-2009
1 Wi, Wi-1, Wi-2, Wi+1, Wi+2, Pi, Pi-1, Pi-2, Pi+1,
Pi+2, Wi-1_Wi, Wi_Wi+1, Wi-1_Wi+1, Pi-1_Pi,
Pi-2_Pi-1, Pi_Pi+1, Pi-1_Pi+1, Pi-1_Pi_Pi+1, Pi-
2_Pi-1_Pi, Pi_Pi+1_Pi+2, Wi_Pi+1, Wi_Pi+2,
Pi_Wi-1, Wi-2_Pi-1_Pi, Pi_Wi+1_Pi+1, Pi-
1_Wi_Pi, Pi_Wi+1
2 Wi-3, Wi+3, Pi-3, Pi+3, Wi-3_Wi-2, Wi+2_Wi+3,
Pi-3_Pi-2, Pi+2_Pi+3
3 Wi-4, Wi+4, Pi-4, Pi+4, Wi-4_Wi-3, Wi+3_Wi+4,
Pi-4_Pi-3, Pi+3_Pi+4
4 Yi-1
5 Yi-1_Pi_Pi+1, Yi-1_Pi, Yi-1_Pi-1_Pi
6 Yi-1_Pi-4, Yi-1_Pi+4
</table>
<tableCaption confidence="0.999808">
Table 1: six subsets of feature template set
</tableCaption>
<table confidence="0.993374142857143">
id Model FT subset
1 ME vs. CRFs 1
2 ME vs. CRFs 1, 2
3 ME vs. CRFs 1, 2, 3
4 MEMMs vs. CRFs 1, 2, 4
5 MEMMs vs. CRFs 1, 2, 4, 5
6 MEMMs vs. CRFs 1, 2, 4, 5, 6
</table>
<tableCaption confidence="0.999515">
Table 2: six experiments
</tableCaption>
<subsectionHeader confidence="0.99692">
4.2 Empirical Results
</subsectionHeader>
<bodyText confidence="0.9994398">
The F-measure curve is shown in Figure 2. For the
same and optimal feature template subset, the F-1
measure of CRFs is superior to that of ME because
of global normalization; and it is superior to that of
MEMMs since it overcomes the label bias.
</bodyText>
<figureCaption confidence="0.9999815">
Figure 2: the F-measure curve
Figure 3: the training time curve
</figureCaption>
<bodyText confidence="0.9931541">
The significant decline in training time of the
new framework is shown in Figure 3, while the
testing time curve in Figure 4 and the total time
curve in Figure 5. The testing time of ME is more
than that of CRFs because of local normalization;
and the testing time of MEMMs_2 is much more
than that of CRFs because of N.C times of P(YijYi-
1, Xi) computation.
computation amount. Our research proves to be a
successful attempt.
</bodyText>
<note confidence="0.595558">
References
</note>
<figureCaption confidence="0.991375">
Figure 5: the total time curve
</figureCaption>
<bodyText confidence="0.999696">
All results of ME and MEMMs in figures are
represented by the same line because perfor-
mances of these two models are the same when
features are only related to observed values.
</bodyText>
<sectionHeader confidence="0.999567" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999909">
In this paper, we propose a new optimal feature
template selection method for CRFs, which is car-
ried out by replacing the CRFs with MEMM_2
(ME) as the machine learning model to address the
efficiency problem according to the homology of
these models. Heuristic method and candidate la-
bel filtering algorithm, which can improve the ef-
ficiency of FTS further, are also introduced. The
effectiveness and efficiency of the new method is
confirmed by the experiments on Chinese text
chunking.
Two problems deserve further study: one is to
prove the homology of ME, MEMMs, and CRFs
theoretically; the other is to expand the method to
other fields.
For any statistical machine learning model, fea-
ture selection or feature template selection is a
computation-intensive step. This work can be ade-
quately reduced by means of analyzing the homol-
ogy between models and using the model with less
</bodyText>
<reference confidence="0.963800291666666">
Das Sanmay. 2001. Filters, wrappers and a boosting-
based hybrid for feature selection. In Proceedings of
the Eighteenth International Conference on Machine
Learning, pages 74–81.
Dietterich Thomas G., Adam Ashenfelter, Yaroslav
Bulatov. 2004. Training Conditional Random Fields
via Gradient Tree Boosting. In Proc. of the 21th In-
ternational Conference on Machine Learning
(ICML).
Dy Jennifer G., and Carla E. Brodley. 2000. Feature
subset selection and order identification for unsuper-
vised learning. In Proceedings of the Seventeenth In-
ternational Conference on Machine Learning, pages
247–254.
Hall Mark A.. 2000. Correlation-based feature selection
for discrete and numeric class machine learning. In
Proceedings of the Seventeenth International Confe-
rence on Machine Learning, pages 359–366.
Jaynes, Edwin T.. 1957. Information Theory and Statis-
tical Mechanics. Physical Review 106(1957), May.
No.4, pp. 620-630.
Kim YongSeog, W. Nick Street and Filippo Menczer.
2000. Feature Selection in Unsupervised Learning
via Evolutionary Search. In Proceedings of the Sixth
ACM SIGKDD International Conference on Know-
ledge Discovery and Data Mining, pages 365–369.
Koeling Rob. 2000. Chunking with Maximum Entropy
Models. In Proceeding of CoNLL-2000 and LLL-
2000, Lisbon, Portugal, 2000, pp. 139-141.
Kohavi Ron, and George H. John. 1997. Wrappers for
feature subset selection. Artificial Intelligence, 97(1-
2):273–324.
Lafferty John, Andrew McCallum, and Fernando Perei-
ra. 2001. Conditional Random Fields: Probabilistic
Models for Segmenting and Labeling Sequence Data.
Proceedings of the Eighteenth International Confe-
rence on Machine Learning.
Li Wei, and Andrew McCallum. 2003. Rapid Devel-
opment of Hindi Named Entity Recognition using
Conditional Random Fields and Feature Induction.
ACM Transactions on Asian Language Information
Processing (TALIP).
Liu Huan, and Lei Yu. 2005. Toward Integrating Fea-
ture Selection Algorithms for Classification and
Clustering. IEEE Transactions on knowledge and
Data Engineering, v.17 n.4, p.491-502.
Liu Huan, and Rudy Setiono. 1996. A probabilistic ap-
proach to feature selection - a filter solution. In Pro-
</reference>
<figureCaption confidence="0.981848">
Figure 4: the testing time curve
</figureCaption>
<reference confidence="0.998518558139535">
ceedings of the Thirteenth International Conference
on Machine Learning, pages 319–327.
McCallum Andrew. 2003. Efficiently Inducing Features
of Conditional Random Fields. In Proceedings of the
Nineteenth Conference on Uncertainty in Artificial
Intelligence.
McCallum Andrew, DAyne Freitag, Fernando Pereira.
2000. Maximum Entropy Markov Models for Infor-
mation Extraction and Segmentation. In Proceedings
of ICML&apos;2000, Stanford, CA, USA, 2000, pp. 591-
598.
McCallum Andrew, and Wei Li. 2003. Early Results for
Named Entity Recognition with Conditional Random
Fields, Feature Induction and Web-Enhanced Lex-
icons. In Proceedings of The Seventh Conference on
Natural Language Learning (CoNLL-2003), Edmon-
ton, Canada.
Osbome Miles. 2000. Shallow Parsing as Part-of-
speech Tagging. In Proceeding of CoNLL-2000 and
LLL-2000, Lisbon, Portugal, 2000,pp. 145-147.
Roark Brian, Murat Saraclar, Michael Collins, and
Mark Johnson. 2004. Discriminative language mod-
eling with conditional random fields and the percep-
tron algorithm. Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics.
Settles Burr. 2004. Biomedical Named Entity Recogni-
tion Using Conditional Random Fields and Rich Fea-
ture Sets. COLING 2004 International Joint Work-
shop on Natural Language Processing in Biomedi-
cine and its Applications (NLPBA).
Sha Fei, and Fernando Pereira. 2003. Shallow Parsing
with Conditional Random Fields. Proceedings of the
2003 conference of the North American Chapter of
the Association for Computational Linguistics on
Human Language Technology, Edmonton, Canada.
Yu Lei, and Huan Liu. 2004. Feature selection for high-
dimensional data: a fast correlation-based filter solu-
tion. In Proceedings of the twentieth International
Conference on Machine Learning, pages 856–863.
Zhou Qiang, and Yumei Li. 2010. Chinese Chunk Pars-
ing Evaluation Tasks. Journal of Chinese Informa-
tion Processing.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.216574">
<title confidence="0.7370085">Selecting Optimal Feature Template Subset for CRFs and and</title>
<affiliation confidence="0.97686">1: School of Computer Science and Harbin Institute of</affiliation>
<address confidence="0.99952">150001, Harbin, China</address>
<affiliation confidence="0.9775685">2: School of Computer Science and Harbin University of Science and</affiliation>
<address confidence="0.998292">150080, Harbin,</address>
<email confidence="0.920606">xxjroom@163.com;guanglu.sun@gmail.comguanyi@hit.edu.cn;dongxishuang@gmail.comlisheng@hit.edu.cn</email>
<abstract confidence="0.981569470588235">Conditional Random Fields (CRFs) are the state-of-the-art models for sequential labeling problems. A critical step is to select optimal feature template subset before employing CRFs, which is a tedious task. To the step, we propose a new method that adopts the maximum entropy (ME) model and maximum entropy Markov models (MEMMs) instead of CRFs considering the homology between ME, MEMMs, and CRFs. Moreover, empirical studies on the efficiency and effectiveness of the method are conducted in the field of Chinese text chunking, whose performance is ranked the first place in task two of CIPS-ParsEval-2009.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Das Sanmay</author>
</authors>
<title>Filters, wrappers and a boostingbased hybrid for feature selection.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning,</booktitle>
<pages>74--81</pages>
<marker>Sanmay, 2001</marker>
<rawString>Das Sanmay. 2001. Filters, wrappers and a boostingbased hybrid for feature selection. In Proceedings of the Eighteenth International Conference on Machine Learning, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dietterich Thomas G</author>
<author>Adam Ashenfelter</author>
<author>Yaroslav Bulatov</author>
</authors>
<title>Training Conditional Random Fields via Gradient Tree Boosting.</title>
<date>2004</date>
<booktitle>In Proc. of the 21th International Conference on Machine Learning (ICML).</booktitle>
<marker>G, Ashenfelter, Bulatov, 2004</marker>
<rawString>Dietterich Thomas G., Adam Ashenfelter, Yaroslav Bulatov. 2004. Training Conditional Random Fields via Gradient Tree Boosting. In Proc. of the 21th International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dy Jennifer G</author>
<author>Carla E Brodley</author>
</authors>
<title>Feature subset selection and order identification for unsupervised learning.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth International Conference on Machine Learning,</booktitle>
<pages>247--254</pages>
<marker>G, Brodley, 2000</marker>
<rawString>Dy Jennifer G., and Carla E. Brodley. 2000. Feature subset selection and order identification for unsupervised learning. In Proceedings of the Seventeenth International Conference on Machine Learning, pages 247–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hall Mark A</author>
</authors>
<title>Correlation-based feature selection for discrete and numeric class machine learning.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth International Conference on Machine Learning,</booktitle>
<pages>359--366</pages>
<marker>A, 2000</marker>
<rawString>Hall Mark A.. 2000. Correlation-based feature selection for discrete and numeric class machine learning. In Proceedings of the Seventeenth International Conference on Machine Learning, pages 359–366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edwin T Jaynes</author>
</authors>
<title>Information Theory and Statistical Mechanics.</title>
<date>1957</date>
<journal>Physical Review</journal>
<volume>106</volume>
<issue>1957</issue>
<pages>620--630</pages>
<contexts>
<context position="8419" citStr="Jaynes, 1957" startWordPosition="1370" endWordPosition="1371">effectiveness of template subset However, this is more tedious than that we use ME or MEMMs instead. The idea behind this lie in three aspects: first, in one iteration, the Forward-Backward Algorithm adopted in CRFs training is timeconsuming; second, CRFs need more iterations than that of ME or MEMMs to converge because of larger parameter space; third, ME, MEMMs, and CRFs, are of the same type (log-linear models) and based on the same principle, as will be discussed in detail as follows. 3.2 Homology of ME, MEMMs and CRFs ME, MEMMs, and CRFs are all based on the Principle of Maximum Entropy (Jaynes, 1957). The mathematical expression for ME model is as formula (1): (1) , and Z(x) is the normalization factor. MEMMs can be considered as a sequential extension to the ME model. In MEMMs, the HMM transition and observation functions are replaced by a single function P(Yi|Yi-1, Xi). There are three kinds of implementations of MEMMs (McCallum et al., 2000) in which we realized the second type for its abundant expressiveness. In implementation two, which is denoted as MEMMs_2 in this paper, a distributed representation for the previous state Yi-1 is taken as a collection of features with weights set b</context>
</contexts>
<marker>Jaynes, 1957</marker>
<rawString>Jaynes, Edwin T.. 1957. Information Theory and Statistical Mechanics. Physical Review 106(1957), May. No.4, pp. 620-630.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kim YongSeog</author>
<author>W Nick Street</author>
<author>Filippo Menczer</author>
</authors>
<title>Feature Selection in Unsupervised Learning via Evolutionary Search.</title>
<date>2000</date>
<booktitle>In Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>365--369</pages>
<marker>YongSeog, Street, Menczer, 2000</marker>
<rawString>Kim YongSeog, W. Nick Street and Filippo Menczer. 2000. Feature Selection in Unsupervised Learning via Evolutionary Search. In Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 365–369.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koeling Rob</author>
</authors>
<title>Chunking with Maximum Entropy Models.</title>
<date>2000</date>
<booktitle>In Proceeding of CoNLL-2000 and LLL2000,</booktitle>
<pages>139--141</pages>
<location>Lisbon, Portugal,</location>
<marker>Rob, 2000</marker>
<rawString>Koeling Rob. 2000. Chunking with Maximum Entropy Models. In Proceeding of CoNLL-2000 and LLL2000, Lisbon, Portugal, 2000, pp. 139-141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kohavi Ron</author>
<author>George H John</author>
</authors>
<title>Wrappers for feature subset selection.</title>
<date>1997</date>
<journal>Artificial Intelligence,</journal>
<pages>97--1</pages>
<marker>Ron, John, 1997</marker>
<rawString>Kohavi Ron, and George H. John. 1997. Wrappers for feature subset selection. Artificial Intelligence, 97(1-2):273–324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lafferty John</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>Proceedings of the Eighteenth International Conference on Machine Learning.</booktitle>
<marker>John, McCallum, Pereira, 2001</marker>
<rawString>Lafferty John, Andrew McCallum, and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. Proceedings of the Eighteenth International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Wei</author>
<author>Andrew McCallum</author>
</authors>
<title>Rapid Development of Hindi Named Entity Recognition using Conditional Random Fields and Feature Induction.</title>
<date>2003</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP).</journal>
<marker>Wei, McCallum, 2003</marker>
<rawString>Li Wei, and Andrew McCallum. 2003. Rapid Development of Hindi Named Entity Recognition using Conditional Random Fields and Feature Induction. ACM Transactions on Asian Language Information Processing (TALIP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liu Huan</author>
<author>Lei Yu</author>
</authors>
<title>Toward Integrating Feature Selection Algorithms for Classification and Clustering.</title>
<date>2005</date>
<journal>IEEE Transactions on knowledge and Data Engineering,</journal>
<volume>17</volume>
<pages>491--502</pages>
<marker>Huan, Yu, 2005</marker>
<rawString>Liu Huan, and Lei Yu. 2005. Toward Integrating Feature Selection Algorithms for Classification and Clustering. IEEE Transactions on knowledge and Data Engineering, v.17 n.4, p.491-502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liu Huan</author>
<author>Rudy Setiono</author>
</authors>
<title>A probabilistic approach to feature selection - a filter solution.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth International Conference on Machine Learning,</booktitle>
<pages>319--327</pages>
<marker>Huan, Setiono, 1996</marker>
<rawString>Liu Huan, and Rudy Setiono. 1996. A probabilistic approach to feature selection - a filter solution. In Proceedings of the Thirteenth International Conference on Machine Learning, pages 319–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>McCallum Andrew</author>
</authors>
<title>Efficiently Inducing Features of Conditional Random Fields.</title>
<date>2003</date>
<booktitle>In Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence.</booktitle>
<marker>Andrew, 2003</marker>
<rawString>McCallum Andrew. 2003. Efficiently Inducing Features of Conditional Random Fields. In Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>McCallum Andrew</author>
<author>DAyne Freitag</author>
<author>Fernando Pereira</author>
</authors>
<title>Maximum Entropy Markov Models for Information Extraction and Segmentation.</title>
<date>2000</date>
<booktitle>In Proceedings of ICML&apos;2000,</booktitle>
<pages>591--598</pages>
<location>Stanford, CA, USA,</location>
<marker>Andrew, Freitag, Pereira, 2000</marker>
<rawString>McCallum Andrew, DAyne Freitag, Fernando Pereira. 2000. Maximum Entropy Markov Models for Information Extraction and Segmentation. In Proceedings of ICML&apos;2000, Stanford, CA, USA, 2000, pp. 591-598.</rawString>
</citation>
<citation valid="true">
<authors>
<author>McCallum Andrew</author>
<author>Wei Li</author>
</authors>
<title>Early Results for Named Entity Recognition with Conditional Random Fields, Feature Induction and Web-Enhanced Lexicons.</title>
<date>2003</date>
<booktitle>In Proceedings of The Seventh Conference on Natural Language Learning (CoNLL-2003),</booktitle>
<location>Edmonton, Canada.</location>
<marker>Andrew, Li, 2003</marker>
<rawString>McCallum Andrew, and Wei Li. 2003. Early Results for Named Entity Recognition with Conditional Random Fields, Feature Induction and Web-Enhanced Lexicons. In Proceedings of The Seventh Conference on Natural Language Learning (CoNLL-2003), Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Osbome Miles</author>
</authors>
<title>Shallow Parsing as Part-ofspeech Tagging.</title>
<date>2000</date>
<booktitle>In Proceeding of CoNLL-2000 and LLL-2000,</booktitle>
<pages>145--147</pages>
<location>Lisbon, Portugal,</location>
<marker>Miles, 2000</marker>
<rawString>Osbome Miles. 2000. Shallow Parsing as Part-ofspeech Tagging. In Proceeding of CoNLL-2000 and LLL-2000, Lisbon, Portugal, 2000,pp. 145-147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roark Brian</author>
<author>Murat Saraclar</author>
<author>Michael Collins</author>
<author>Mark Johnson</author>
</authors>
<title>Discriminative language modeling with conditional random fields and the perceptron algorithm.</title>
<date>2004</date>
<booktitle>Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Brian, Saraclar, Collins, Johnson, 2004</marker>
<rawString>Roark Brian, Murat Saraclar, Michael Collins, and Mark Johnson. 2004. Discriminative language modeling with conditional random fields and the perceptron algorithm. Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Settles Burr</author>
</authors>
<title>Biomedical Named Entity Recognition Using Conditional Random Fields and Rich Feature Sets.</title>
<date>2004</date>
<booktitle>COLING 2004 International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA).</booktitle>
<marker>Burr, 2004</marker>
<rawString>Settles Burr. 2004. Biomedical Named Entity Recognition Using Conditional Random Fields and Rich Feature Sets. COLING 2004 International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sha Fei</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow Parsing with Conditional Random Fields.</title>
<date>2003</date>
<booktitle>Proceedings of the 2003 conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<location>Edmonton, Canada.</location>
<marker>Fei, Pereira, 2003</marker>
<rawString>Sha Fei, and Fernando Pereira. 2003. Shallow Parsing with Conditional Random Fields. Proceedings of the 2003 conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu Lei</author>
<author>Huan Liu</author>
</authors>
<title>Feature selection for highdimensional data: a fast correlation-based filter solution.</title>
<date>2004</date>
<booktitle>In Proceedings of the twentieth International Conference on Machine Learning,</booktitle>
<pages>856--863</pages>
<marker>Lei, Liu, 2004</marker>
<rawString>Yu Lei, and Huan Liu. 2004. Feature selection for highdimensional data: a fast correlation-based filter solution. In Proceedings of the twentieth International Conference on Machine Learning, pages 856–863.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhou Qiang</author>
<author>Yumei Li</author>
</authors>
<title>Chinese Chunk Parsing Evaluation Tasks.</title>
<date>2010</date>
<journal>Journal of Chinese Information Processing.</journal>
<marker>Qiang, Li, 2010</marker>
<rawString>Zhou Qiang, and Yumei Li. 2010. Chinese Chunk Parsing Evaluation Tasks. Journal of Chinese Information Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>