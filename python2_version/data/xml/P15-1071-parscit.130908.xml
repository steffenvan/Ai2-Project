<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000025">
<title confidence="0.963708">
Unsupervised Cross-Domain Word Representation Learning
</title>
<author confidence="0.557789">
Danushka Bollegala Takanori Maehara Ken-ichi Kawarabayashi
</author>
<bodyText confidence="0.4167025">
danushka.bollegala@ maehara.takanori@ k keniti@
liverpool.ac.uk shizuoka.ac.jp nii.ac.jp
University of Liverpool Shizuoka University National Institute of Informatics
JST, ERATO, Kawarabayashi Large Graph Project.
</bodyText>
<sectionHeader confidence="0.976488" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999953071428571">
Meaning of a word varies from one do-
main to another. Despite this impor-
tant domain dependence in word seman-
tics, existing word representation learning
methods are bound to a single domain.
Given a pair of source-target domains,
we propose an unsupervised method for
learning domain-specific word representa-
tions that accurately capture the domain-
specific aspects of word semantics. First,
we select a subset of frequent words that
occur in both domains as pivots. Next,
we optimize an objective function that
enforces two constraints: (a) for both
source and target domain documents, piv-
ots that appear in a document must accu-
rately predict the co-occurring non-pivots,
and (b) word representations learnt for
pivots must be similar in the two do-
mains. Moreover, we propose a method
to perform domain adaptation using the
learnt word representations. Our proposed
method significantly outperforms compet-
itive baselines including the state-of-the-
art domain-insensitive word representa-
tions, and reports best sentiment classifi-
cation accuracies for all domain-pairs in a
benchmark dataset.
</bodyText>
<sectionHeader confidence="0.998124" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99377422">
Learning semantic representations for words is a
fundamental task in NLP that is required in nu-
merous higher-level NLP applications (Collobert
et al., 2011). Distributed word representations
have gained much popularity lately because of
their accuracy as semantic representations for
words (Mikolov et al., 2013a; Pennington et al.,
2014). However, the meaning of a word often
varies from one domain to another. For exam-
ple, the phrase lightweight is often used in a posi-
tive sentiment in the portable electronics domain
because a lightweight device is easier to carry
around, which is a positive attribute for a portable
electronic device. However, the same phrase has a
negative sentiment assocition in the movie domain
because movies that do not invoke deep thoughts
in viewers are considered to be lightweight (Bol-
legala et al., 2014). However, existing word rep-
resentation learning methods are agnostic to such
domain-specific semantic variations of words, and
capture semantics of words only within a single
domain. To overcome this problem and capture
domain-specific semantic orientations of words,
we propose a method that learns separate dis-
tributed representations for each domain in which
a word occurs.
Despite the successful applications of dis-
tributed word representation learning meth-
ods (Pennington et al., 2014; Collobert et al.,
2011; Mikolov et al., 2013a) most existing ap-
proaches are limited to learning only a single
representation for a given word (Reisinger and
Mooney, 2010). Although there have been some
work on learning multiple prototype representa-
tions (Huang et al., 2012; Neelakantan et al., 2014)
for a word considering its multiple senses, such
methods do not consider the semantics of the do-
main in which the word is being used.
If we can learn separate representations for a
word for each domain in which it occurs, we can
use the learnt representations for domain adapta-
tion tasks such as cross-domain sentiment clas-
sification (Bollegala et al., 2011b), cross-domain
POS tagging (Schnabel and Sch¨utze, 2013), cross-
domain dependency parsing (McClosky et al.,
2010), and domain adaptation of relation extrac-
tors (Bollegala et al., 2013a; Bollegala et al.,
2013b; Bollegala et al., 2011a; Jiang and Zhai,
2007a; Jiang and Zhai, 2007b).
We introduce the cross-domain word represen-
</bodyText>
<note confidence="0.85165325">
730
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 730–740,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.95588506097561">
tation learning task, where given two domains,
(referred to as the source (S) and the target (T))
the goal is to learn two separate representations
wS and wT for a word w respectively from the
source and the target domain that capture domain-
specific semantic variations of w. In this paper,
we use the term domain to represent a collection
of documents related to a particular topic such as
user-reviews in Amazon for a product category
(e.g. books, dvds, movies, etc.). However, a do-
main in general can be a field of study (e.g. biol-
ogy, computer science, law, etc.) or even an entire
source of information (e.g. twitter, blogs, news
articles, etc.). In particular, we do not assume the
availability of any labeled data for learning word
representations.
This problem setting is closely related to unsu-
pervised domain adaptation (Blitzer et al., 2006),
which has found numerous useful applications
such as, sentiment classification and POS tagging.
For example, in unsupervised cross-domain sen-
timent classification (Blitzer et al., 2006; Blitzer
et al., 2007), we train a binary sentiment classifier
using positive and negative labeled user reviews
in the source domain, and apply the trained clas-
sifier to predict sentiment of the target domain’s
user reviews. Although the distinction between the
source and the target domains is not important for
the word representation learning step, it is impor-
tant for the domain adaptation tasks in which we
subsequently evaluate the learnt word representa-
tions. Following prior work on domain adapta-
tion (Blitzer et al., 2006), high-frequent features
(unigrams/bigrams) common to both domains are
referred to as domain-independent features or piv-
ots. In contrast, we use non-pivots to refer to fea-
tures that are specific to a single domain.
We propose an unsupervised cross-domain
word representation learning method that jointly
optimizes two criteria: (a) given a document d
from the source or the target domain, we must ac-
curately predict the non-pivots that occur in d us-
ing the pivots that occur in d, and (b) the source
and target domain representations we learn for piv-
ots must be similar. The main challenge in domain
adaptation is feature mismatch, where the features
that we use for training a classifier in the source
domain do not necessarily occur in the target do-
main. Consequently, prior work on domain adap-
tation (Blitzer et al., 2006; Pan et al., 2010) learn
lower-dimensional mappings from non-pivots to
pivots, thereby overcoming the feature mismatch
problem. Criteria (a) ensures that word represen-
tations for domain-specific non-pivots in each do-
main are related to the word representations for
domain-independent pivots. This relationship en-
ables us to discover pivots that are similar to tar-
get domain-specific non-pivots, thereby overcom-
ing the feature mismatch problem.
On the other hand, criteria (b) captures the prior
knowledge that high-frequent words common to
two domains often represent domain-independent
semantics. For example, in sentiment classifica-
tion, words such as excellent or terrible would ex-
press similar sentiment about a product irrespec-
tive of the domain. However, if a pivot expresses
different semantics in source and the target do-
mains, then it will be surrounded by dissimilar
sets of non-pivots, and reflected in the first crite-
ria. Criteria (b) can also be seen as a regulariza-
tion constraint imposed on word representations to
prevent overfitting by reducing the number of free
parameters in the model.
Our contributions in this paper can be summa-
rized as follows.
• We propose a distributed word representa-
tion learning method that learns separate
representations for a word for each do-
main in which it occurs. To the best
of our knowledge, ours is the first-ever
domain-sensitive distributed word represen-
tation learning method.
</bodyText>
<listItem confidence="0.963379">
• Given domain-specific word representations,
we propose a method to learn a cross-domain
sentiment classifier.
</listItem>
<bodyText confidence="0.999276875">
Although word representation learning meth-
ods have been used for various related
tasks in NLP such as similarity measure-
ment (Mikolov et al., 2013c), POS tag-
ging (Collobert et al., 2011), dependency
parsing (Socher et al., 2011a), machine trans-
lation (Zou et al., 2013), sentiment classifica-
tion (Socher et al., 2011b), and semantic role
labeling (Roth and Woodsend, 2014), to the
best of our knowledge, word representations
methods have not yet been used for cross-
domain sentiment classification.
Experimental results for cross-domain senti-
ment classification on a benchmark dataset show
that the word representations learnt using the pro-
posed method statistically significantly outper-
</bodyText>
<page confidence="0.788126">
731
</page>
<bodyText confidence="0.999976642857143">
form a state-of-the-art domain-insensitive word
representation learning method (Pennington et al.,
2014), and several competitive baselines. In par-
ticular, our proposed cross-domain word represen-
tation learning method is not specific to a par-
ticular task such as sentiment classification, and
in principle, can be in applied to a wide-range
of domain adaptation tasks. Despite this task-
independent nature of the proposed method, it
achieves the best sentiment classification accu-
racies on all domain-pairs, reporting statistically
comparable results to the current state-of-the-art
unsupervised cross-domain sentiment classifica-
tion methods (Pan et al., 2010; Blitzer et al., 2006).
</bodyText>
<sectionHeader confidence="0.999786" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999986244186047">
Representing the semantics of a word using some
algebraic structure such as a vector (more gener-
ally a tensor) is a common first step in many NLP
tasks (Turney and Pantel, 2010). By applying al-
gebraic operations on the word representations, we
can perform numerous tasks in NLP, such as com-
posing representations for larger textual units be-
yond individual words such as phrases (Mitchell
and Lapata, 2008). Moreover, word representa-
tions are found to be useful for measuring se-
mantic similarity, and for solving proportional
analogies (Mikolov et al., 2013c). Two main ap-
proaches for computing word representations can
be identified in prior work (Baroni et al., 2014):
counting-based and prediction-based.
In counting-based approaches (Baroni and
Lenci, 2010), a word w is represented by a vec-
tor w that contains other words that co-occur with
w in a corpus. Numerous methods for selecting
co-occurrence contexts such as proximity or de-
pendency relations have been proposed (Turney
and Pantel, 2010). Despite the numerous suc-
cessful applications of co-occurrence counting-
based distributional word representations, their
high dimensionality and sparsity are often prob-
lematic in practice. Consequently, further post-
processing steps such as dimensionality reduction,
and feature selection are often required when us-
ing counting-based word representations.
On the other hand, prediction-based approaches
first assign each word, for example, with a d-
dimensional real-vector, and learn the elements of
those vectors by applying them in an auxiliary task
such as language modeling, where the goal is to
predict the next word in a given sequence. The
dimensionality d is fixed for all the words in the
vocabulary, and, unlike counting-based word rep-
resentations, is much smaller (e.g. d ∈ [10,1000]
in practice) compared to the vocabulary size. The
neural network language model (NNLM) (Bengio
et al., 2003) uses a multi-layer feed-forward neu-
ral network to predict the next word in a sequence,
and uses backpropagation to update the word vec-
tors such that the prediction error is minimized.
Although NNLMs learn word representations
as a by-product, the main focus on language
modeling is to predict the next word in a sen-
tence given the previous words, and not learn-
ing word representations that capture semantics.
Moreover, training multi-layer neural networks
using large text corpora is time consuming. To
overcome those limitations, methods that specif-
ically focus on learning word representations that
model word co-occurrences in large corpora have
been proposed (Mikolov et al., 2013a; Mnih and
Kavukcuoglu, 2013; Huang et al., 2012; Pen-
nington et al., 2014). Unlike the NNLM, these
methods use all the words in a contextual win-
dow in the prediction task. Methods that use
one or no hidden layers are proposed to improve
the scalability of the learning algorithms. For
example, the skip-gram model (Mikolov et al.,
2013b) predicts the words c that appear in the
local context of a word w, whereas the continu-
ous bag-of-words model (CBOW) predicts a word
w conditioned on all the words c that appear in
w’s local context (Mikolov et al., 2013a). Meth-
ods that use global co-occurrences in the entire
corpus to learn word representations have shown
to outperform methods that use only local co-
occurrences (Huang et al., 2012; Pennington et
al., 2014). Overall, prediction-based methods
have shown to outperform counting-based meth-
ods (Baroni et al., 2014).
Despite their impressive performance, existing
methods for word representation learning do not
consider the semantic variation of words across
different domains. However, as described in Sec-
tion 1, the meaning of a word vary from one do-
main to another, and must be considered. To the
best of our knowledge, the only prior work study-
ing the problem of word representation variation
across domains is due to Bollegala et al. (2014).
Given a source and a target domain, they first se-
lect a set of pivots using pointwise mutual infor-
mation, and create two distributional representa-
</bodyText>
<page confidence="0.486">
732
</page>
<bodyText confidence="0.999992488888889">
tions for each pivot using their co-occurrence con-
texts in a particular domain. Next, a projection
matrix from the source to the target domain feature
spaces is learnt using partial least squares regres-
sion. Finally, the learnt projection matrix is used
to find the nearest neighbors in the source domain
for each target domain-specific features. However,
unlike our proposed method, their method does
not learn domain-specific word representations,
but simply uses co-occurrence counting when cre-
ating in-domain word representations.
Faralli et al. (2012) proposed a domain-driven
word sense disambiguation (WSD) method where
they construct glossaries for several domain us-
ing a pattern-based bootstrapping technique. This
work demonstrates the importance of considering
the domain specificity of word senses. However,
the focus of their work is not to learn representa-
tions for words or their senses in a domain, but to
construct glossaries. It would be an interesting fu-
ture research direction to explore the possibility of
using such domain-specific glossaries for learning
domain-specific word representations.
Neelakantan et al. (2014) proposed a method
that jointly performs WSD and word embedding
learning, thereby learning multiple embeddings
per word type. In particular, the number of senses
per word type is automatically estimated. How-
ever, their method is limited to a single domain,
and does not consider how the representations vary
across domains. On the other hand, our proposed
method learns a single representation for a partic-
ular word for each domain in which it occurs.
Although in this paper we focus on the mono-
lingual setting where source and target domains
belong to the same language, the related setting
where learning representations for words that are
translational pairs across languages has been stud-
ied (Hermann and Blunsom, 2014; Klementiev et
al., 2012; Gouws et al., 2015). Such representa-
tions are particularly useful for cross-lingual in-
formation retrieval (Duc et al., 2010). It will be an
interesting future research direction to extend our
proposed method to learn such cross-lingual word
representations.
</bodyText>
<sectionHeader confidence="0.985135" genericHeader="method">
3 Cross-Domain Representation
</sectionHeader>
<subsectionHeader confidence="0.686485">
Learning
</subsectionHeader>
<bodyText confidence="0.999991333333333">
We propose a method for learning word represen-
tations that are sensitive to the semantic variations
of words across domains. We call this problem
cross-domain word representation learning, and
provide a definition in Section 3.1. Next, in Sec-
tion 3.2, given a set of pivots that occurs in both a
source and a target domain, we propose a method
for learning cross-domain word representations.
We defer the discussion of pivot selection meth-
ods to Section 3.4. In Section 3.5, we propose a
method for using the learnt word representations
to train a cross-domain sentiment classifier.
</bodyText>
<subsectionHeader confidence="0.998711">
3.1 Problem Definition
</subsectionHeader>
<bodyText confidence="0.999979692307693">
Let us assume that we are given two sets of docu-
ments DS and DT respectively for a source (S)
and a target (T) domain. We do not consider
the problem of retrieving documents for a domain,
and assume such a collection of documents to be
given. Then, given a particular word w, we define
cross-domain representation learning as the task of
learning two separate representations wS and wT
capturing w’s semantics in respectively the source
S and the target T domains.
Unlike in domain adaptation, where there is a
clear distinction between the source (i.e. the do-
main on which we train) vs. the target (i.e. the
domain on which we test) domains, for represen-
tation learning purposes we do not make a distinc-
tion between the two domains. In the unsupervised
setting of the cross-domain representation learn-
ing that we study in this paper, we do not assume
the availability of labeled data for any domain for
the purpose of learning word representations. As
an extrinsic evaluation task, we apply the trained
word representations for classifying sentiment re-
lated to user-reviews (Section 3.5). However, for
this evaluation task we require sentiment-labeled
user-reviews from the source domain.
Decoupling of the word representation learn-
ing from any tasks in which those representations
are subsequently used, simplifies the problem as
well as enables us to learn task-independent word
representations with potential generic applicabil-
ity. Although we limit the discussion to a pair of
domains for simplicity, the proposed method can
be easily extended to jointly learn word represen-
tations for more than two domains. In fact, prior
work on cross-domain sentiment analysis show
that incorporating multiple source domains im-
proves sentiment classification accuracy on a tar-
get domain (Bollegala et al., 2011b; Glorot et al.,
2011).
</bodyText>
<page confidence="0.663861">
733
</page>
<subsectionHeader confidence="0.997446">
3.2 Proposed Method
</subsectionHeader>
<bodyText confidence="0.999985473684211">
To describe our proposed method, let us denote a
pivot and a non-pivot feature respectively by c and
w. Our proposed method does not depend on a
specific pivot selection method, and can be used
with all previously proposed methods for selecting
pivots as explained later in Section 3.4. A pivot
c is represented in the source and target domains
respectively by vectors cS E R&apos; and cT E R&apos;.
Likewise, a source specific non-pivot w is repre-
sented by wS in the source domain, whereas a tar-
get specific non-pivot w is represented by wT in
the target domain. By definition, a non-pivot oc-
curs only in a single domain. For notational conve-
nience we use w to denote non-pivots in both do-
mains when the domain is clear from the context.
We use CS, WS, CT , and WT to denote the sets
of word representation vectors respectively for the
source pivots, source non-pivots, target pivots, and
target non-pivots.
Let us denote the set of documents in the source
and the target domains respectively by DS and
DT . Following the bag-of-features model, we as-
sume that a document D is represented by the set
of pivots and non-pivots that occur in D (w E d
and c E d). We consider the co-occurrences
of a pivot c and a non-pivot w within a fixed-
size contextual window in a document. Following
prior work on representation learning (Mikolov et
al., 2013a), in our experiments, we set the win-
dow size to 10 tokens, without crossing sentence
boundaries. The notation (c, w) E d denotes the
co-occurrence of a pivot c and a non-pivot w in a
document d.
We learn domain-specific word representations
by maximizing the prediction accuracy of the non-
pivots w that occur in the local context of a pivot
c. The hinge loss, L(CS, WS), associated with
predicting a non-pivot w in a source document
</bodyText>
<equation confidence="0.97863575">
d E DS that co-occurs with pivots c is given by:
( )
max 0, 1 − csTws + csTws)
(1)
</equation>
<bodyText confidence="0.999707888888889">
Here, w∗S is the source domain representation of
a non-pivot w∗ that does not occur in d. The loss
function given by Eq. 1 requires that a non-pivot
w that co-occurs with a pivot c in the document d
is assigned a higher ranking score as measured by
the inner-product between cS and wS than a non-
pivot w∗ that does not occur in d. We randomly
sample k non-pivots from the set of all source do-
main non-pivots that do not occur in d as w∗.
Specifically, we use the marginal distribution
of non-pivots p(w), estimated from the corpus
counts, as the sampling distribution. We raise
p(w) to the 3/4-th power as proposed by Mikolov
et al. (2013a), and normalize it to unit probabil-
ity mass prior to sampling k non-pivots w∗ per
each co-occurrence of (c, w) E d. Because non-
occurring non-pivots w∗ are randomly sampled,
prior work on noise contrastive estimation has
found that it requires more negative samples than
positive samples to accurately learn a prediction
model (Mnih and Kavukcuoglu, 2013). We exper-
imentally found k = 5 to be an acceptable trade-
off between the prediction accuracy and the num-
ber of training instances.
Likewise, the loss function L(CT , WT ) for pre-
dicting non-pivots using pivots in the target do-
main is given by:
</bodyText>
<equation confidence="0.977383333333333">
max (0, 1
− cT TwT + cTTwT)
(2)
</equation>
<bodyText confidence="0.99929025">
Here, w∗ denotes target domain non-pivots that
do not occur in d, and are randomly sampled
from p(w) following the same procedure as in the
source domain.
The source and target loss functions given re-
spectively by Eqs. 1 and 2 can be used on their own
to independently learn source and target domain
word representations. However, by definition, piv-
ots are common to both domains. We use this
property to relate the source and target word repre-
sentations via a pivot-regularizer, R(CS, CT ), de-
fined as:
</bodyText>
<equation confidence="0.6051022">
�K ||c(Z)
1 S − c(Z)T ||2 (3)
R(CS, CT ) =
2
Z=1
</equation>
<bodyText confidence="0.997986642857143">
Here, ||x ||represents the l2 norm of a vector x,
and c(Z) is the i-th pivot in a total collection of K
pivots. Word representations for non-pivots in the
source and target domains are linked via the pivot
regularizer because, the non-pivots in each domain
are predicted using the word representations for
the pivots in each domain, which in turn are reg-
ularized by Eq. 3. The overall objective function,
L(CS, WS, CT , WT ), we minimize is the sum1 of
1Weighting the source and target loss functions by the re-
spective dataset sizes did not result in any significant increase
in performance. We believe that this is because the bench-
mark dataset contains approximately equal numbers of docu-
ments for each domain.
</bodyText>
<figure confidence="0.9375078125">
E
dEDS
E
w*p(w)
E
(c,w)Ed
E
dEDT
E
w*p(w)
E
(c,w)Ed
734
the source and target loss functions, regularized
via Eq. 3 with coefficient A, and is given by:
L(CS, WS,) + L(CT , WT ) + AR(CS, CT ) (4)
</figure>
<subsectionHeader confidence="0.987025">
3.3 Training
</subsectionHeader>
<bodyText confidence="0.999845">
Word representations of pivots c and non-pivots w
in the source (cS, wS) and the target (cT , wT ) do-
mains are parameters to be learnt in the proposed
method. To derive parameter updates, we compute
the gradients of the overall loss function in Eq. 4
w.r.t. to each parameter as follows:
</bodyText>
<equation confidence="0.9996971875">
aL = (0 if cS&gt; (w − w∗S) &gt; 1 (6)
aw∗ cS otheriwse
S
(
0 if cT &gt;(wT − w∗ T ) &gt; 1
=
(7)
−cT otherwise
aL =JO if cT &gt;(wT − wT∗) &gt; 1 (8)
awT CT otherwise
aL = A(cS − cT ) if cS&gt; (wS − w∗S) &gt;( 1
acS w∗S − wS + A(cS − cT ) otherwise
(9)
aL = (A(cT − cS) if cT &gt;(wT − wT∗) &gt; 1
acT w∗T − wT + A(cT − cS) otherwise
(10)
</equation>
<bodyText confidence="0.99966427027027">
Here, for simplicity, we drop the arguments inside
the loss function and write it as L. We use mini
batch stochastic gradient descent with a batch size
of 50 instances. AdaGrad (Duchi et al., 2011) is
used to schedule the learning rate. All word repre-
sentations are initialized with n dimensional ran-
dom vectors sampled from a zero mean and unit
variance Gaussian. Although the objective in Eq. 4
is not jointly convex in all four representations,
it is convex w.r.t. the representation of a partic-
ular feature (pivot or non-pivot) when the repre-
sentations for all the other features are held fixed.
In our experiments, the training converged in all
cases with less than 100 epochs over the dataset.
The rank-based predictive hinge loss (Eq. 1)
is inspired by the prior work on word represen-
tation learning for a single domain (Collobert et
al., 2011). However, unlike the multilayer neu-
ral network in Collobert et al. (2011), the pro-
posed method uses a computationally efficient sin-
gle layer to reduce the number of parameters that
must be learnt, thereby scaling to large datasets.
Similar to the skip-gram model (Mikolov et al.,
2013a), the proposed method predicts occurrences
of contexts (non-pivots) w within a fixed-size con-
textual window of a target word (pivot) c.
Scoring the co-occurrences of two words c and
w by the bilinear form given by the inner-product
is similar to prior work on domain-insensitive
word-representation learning (Mnih and Hinton,
2008; Mikolov et al., 2013a). However, unlike
those methods that use the softmax function to
convert inner-products to probabilities, we directly
use the inner-products without any further trans-
formations, thereby avoiding computationally ex-
pensive distribution normalizations over the entire
vocabulary.
</bodyText>
<subsectionHeader confidence="0.993074">
3.4 Pivot Selection
</subsectionHeader>
<bodyText confidence="0.984419625">
Given two sets of documents DS, DT respec-
tively for the source and the target domains, we
use the following procedure to select pivots and
non-pivots. First, we tokenize and lemmatize each
document using the Stanford CoreNLP toolkit2.
Next, we extract unigrams and bigrams as features
for representing a document. We remove features
listed as stop words using a standard stop words
list. Stop word removal increases the effective co-
occurrence window size for a pivot. Finally, we
remove features that occur less than 50 times in
the entire set of documents.
Several methods have been proposed in the
prior work on domain adaptation for selecting a
set of pivots from a given pair of domains such
as the minimum frequency of occurrence of a fea-
ture in the two domains, mutual information (MI),
and the entropy of the feature distribution over the
documents (Pan et al., 2010). In our preliminary
experiments, we discovered that a normalized ver-
sion of the PMI (NPMI) (Bouma, 2009) to work
consistently well for selecting pivots from differ-
ent pairs of domains. NPMI between two features
x and y is given by:
</bodyText>
<equation confidence="0.910863">
NPMI(x, y) = logp(x, y) 1
G(x)p(y) − log(p(x, y))
(11)
</equation>
<bodyText confidence="0.9978206">
Here, the joint probability p(x, y), and the
marginal probabilities p(x) and p(y) are estimated
using the number of co-occurrences of x and y in
the sentences in the documents. Eq. 11 normalizes
both the upper and lower bounds of the PMI.
</bodyText>
<figure confidence="0.783590636363636">
2http://nlp.stanford.edu/software/
corenlp.shtml
(
aL 0 if cS&gt;(wS − w∗ S) &gt; 1
=
(5)
−cS otherwise
awS
aL
awT
735
</figure>
<bodyText confidence="0.976073487804878">
We measure the appropriateness of a feature as
a pivot according to the score given by:
score(x) = min (NPMI(x, S), NPMI(x, T)) .
(12)
We rank features that are common to both domains
in the descending order of their scores as given by
Eq. 12, and select the top N? features as pivots.
We rank features x that occur only in the source
domain by NPMI(x, S), and select the top ranked
NS features as source-specific non-pivots. Like-
wise, we rank the features x that occur only in the
target domain by NPMI(x, T), and select the top
ranked NT features as target-specific non-pivots.
The pivot selection criterion described here dif-
fers from that of Blitzer et al. (2006; 2007), where
pivots are defined as features that behave similarly
both in the source and the target domains. They
compute the mutual information between a feature
(i.e. unigrams or bigrams) and the sentiment labels
using source domain labeled reviews. This method
is useful when selecting pivots that are closely as-
sociated with positive or negative sentiment in the
source domain. However, in unsupervised domain
adaptation we do not have labeled data for the tar-
get domain. Therefore, the pivots selected using
this approach are not guaranteed to demonstrate
the same sentiment in the target domain as in the
source domain. On the other hand, the pivot se-
lection method proposed in this paper focuses on
identifying a subset of features that are closely as-
sociated with both domains.
It is noteworthy that our proposed cross-domain
word representation learning method (Section 3.2)
does not assume any specific pivot/non-pivot se-
lection method. Therefore, in principle, our pro-
posed word representation learning method could
be used with any of the previously proposed pivot
selection methods. We defer a comprehensive
evaluation of possible combinations of pivot selec-
tion methods and their effect on the proposed word
representation learning method to future work.
</bodyText>
<subsectionHeader confidence="0.992446">
3.5 Cross-Domain Sentiment Classification
</subsectionHeader>
<bodyText confidence="0.99940846031746">
As a concrete application of cross-domain word
representations, we describe a method for learning
a cross-domain sentiment classifier using the word
representations learnt by the proposed method.
Existing word representation learning methods
that learn from only a single domain are typi-
cally evaluated for their accuracy in measuring se-
mantic similarity between words, or by solving
word analogy problems. Unfortunately, such gold
standard datasets capturing cross-domain seman-
tic variations of words are unavailable. Therefore,
by applying the learnt word representations in a
cross-domain sentiment classification task, we can
conduct an indirect extrinsic evaluation.
The train data available for unsupervised cross-
domain sentiment classification consists of unla-
beled data for both the source and the target do-
mains as well as labeled data for the source do-
main. We train a binary sentiment classifier using
those train data, and apply it to classify sentiment
of the target test data.
Unsupervised cross-domain sentiment classifi-
cation is challenging due to two reasons: feature-
mismatch, and semantic variation. First, the sets
of features that occur in source and target domain
documents are different. Therefore, a sentiment
classifier trained using source domain labeled data
is likely to encounter unseen features during test
time. We refer to this as the feature-mismatch
problem. Second, some of the features that occur
in both domains will have different sentiments as-
sociated with them (e.g. lightweight). Therefore,
a sentiment classifier trained using source domain
labeled data is likely to incorrectly predict simi-
lar sentiment (as in the source) for such features.
We call this the semantic variation problem. Next,
we propose a method to overcome both problems
using cross-domain word representations.
Let us assume that we are given a set
{(x(i)
S , y(i))}ni=1 of n labeled reviews x(i)
S for the
source domain S. For simplicity, let us consider
binary sentiment classification where each review
x(i) is labeled either as positive (i.e. y(i) = 1) or
negative (i.e. y(i) = −1). Our cross-domain bi-
nary sentiment classification method can be eas-
ily extended to multi-class classification. First, we
lemmatize each word in a source domain labeled
review x(i)
S , and extract unigrams and bigrams as
features to represent x(i)
S by a binary-valued fea-
ture vector. Next, we train a binary linear clas-
sifier, θ, using those feature vectors. Any binary
classification algorithm can be used for this pur-
pose. We use θ(z) to denote the weight learnt by
the classifier for a feature z. In our experiments,
we used l2 regularized logistic regression.
At test time, we represent a test target review
by a binary-valued vector h using a the set of un-
igrams and bigrams extracted from that review.
Then, the activation score, ψ(h), of h is defined
</bodyText>
<equation confidence="0.962547615384615">
736
by:
E
,O(h) =
cEh
E E
e(��)f(c� S, cS)+
c&apos;EB
e(w)f(ws, wT )
E
w&apos;EB
wEh
(13)
</equation>
<bodyText confidence="0.999688555555556">
Here, f is a similarity measure between two vec-
tors. If ψ(h) &gt; 0, we classify h as positive, and
negative otherwise. Eq. 13 measures the similarity
between each feature in h against the features in
the classification model θ. For pivots c ∈ h, we
use the the source domain representations to mea-
sure similarity, whereas for the (target-specific)
non-pivots w ∈ h, we use their target domain rep-
resentations. We experimented with several pop-
ular similarity measures for f and found cosine
similarity to perform consistently well. We can in-
terpret Eq. 13 as a method for expanding a test tar-
get document using nearest neighbor features from
the source domain labeled data. It is analogous to
query expansion used in information retrieval to
improve document recall (Fang, 2008). Alterna-
tively, Eq. 13 can be seen as a linearly-weighted
additive kernel function over two feature spaces.
</bodyText>
<sectionHeader confidence="0.992724" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999989923076923">
For train and evaluation purposes, we use the
Amazon product reviews collected by Blitzer et
al. (2007) for the four product categories: books
(B), DVDs (D), electronic items (E), and kitchen
appliances (K). There are 1000 positive and 1000
negative sentiment labeled reviews for each do-
main. Moreover, each domain has on average
17,547 unlabeled reviews. We use the standard
split of 800 positive and 800 negative labeled re-
views from each domain as training data, and the
rest (200+200) for testing. For validation purposes
we use movie (source) and computer (target) do-
mains, which were also collected by Blitzer et al.
(2007), but not part of the train/test domains.
Experiments conducted using this validation
dataset revealed that the performance of the pro-
posed method is relatively insensitive to the value
of the regularization parameter A ∈ [10−3,103].
For the non-pivot prediction task we generate pos-
itive and negative instances using the procedure
described in Section 3.2. As a typical example,
we have 88,494 train instances from the books
source domain and 141, 756 train instances from
the target domain (1:5 ratio between positive and
negative instances in each domain). The number
of pivots and non-pivots are set to NP = NS =
NT = 500.
In Figure 1, we compare the proposed method
against two baselines (NA, InDomain), current
state-of-the-art methods for unsupervised cross-
domain sentiment classification (SFA, SCL),
word representation learning (GloVe), and cross-
domain similarity prediction (CS). The NA (no-
adapt) lower baseline uses a classifier trained on
source labeled data to classify target test data with-
out any domain adaptation. The InDomain base-
line is trained using the labeled data for the target
domain, and simulates the performance we can ex-
pect to obtain if target domain labeled data were
available. Spectral Feature Alignment (SFA) (Pan
et al., 2010) and Structural Correspondence Learn-
ing (SCL) (Blitzer et al., 2007) are the state-of-
the-art methods for cross-domain sentiment clas-
sification. However, those methods do not learn
word representations.
We use Global Vector Prediction (GloVe) (Pen-
nington et al., 2014), the current state-of-the-
art word representation learning method, to learn
word representations separately from the source
and target domain unlabeled data, and use the
learnt representations in Eq. 13 for sentiment clas-
sification. In contrast to the joint word representa-
tions learnt by the proposed method, GloVe sim-
ulates the level of performance we would obtain
by learning representations independently. CS de-
notes the cross-domain vector prediction method
proposed by Bollegala et al. (2014). Although
CS can be used to learn a vector-space transla-
tion matrix, it does not learn word representations.
Vertical bars represent the classification accuracies
(i.e. percentage of the correctly classified test in-
stances) obtained by a particular method on target
domain’s test data, and Clopper-Pearson 95% bi-
nomial confidence intervals are superimposed.
Differences in data pre-processing (tokeniza-
tion/lemmatization), selection (train/test splits),
feature representation (unigram/bigram), pivot se-
lection (MI/frequency), and the binary classifica-
tion algorithms used to train the final classifier
make it difficult to directly compare results pub-
lished in prior work. Therefore, we re-run the orig-
inal algorithms on the same processed dataset un-
der the same conditions such that any differences
reported in Figure 1 can be directly attributable
to the domain adaptation, or word-representation
learning methods compared.
All methods use l2 regularized logistic regres-
sion as the binary sentiment classifier, and the reg-
</bodyText>
<figure confidence="0.996444142857143">
737
Accuracy
80
75
70
65
60
55
E−&gt;B D−&gt;B K−&gt;B
Accuracy
85
80
75
70
65
60
55
50
B−&gt;E D−&gt;E K−&gt;E
90
80
75
80
Accuracy
70
65
60
Accuracy
70
60
55
50
B−&gt;D E−&gt;D K−&gt;D
B−&gt;K E−&gt;K D−&gt;K
NA GloVe SFA SCL CS Proposed
</figure>
<figureCaption confidence="0.999945">
Figure 1: Accuracies obtained by different methods for each source-target pair in cross-domain sentiment classification.
</figureCaption>
<bodyText confidence="0.999984714285714">
ularization coefficients are set to their optimal val-
ues on the validation dataset. SFA, SCL, and CS
use the same set of 500 pivots as used by the pro-
posed method selected using NPMI (Section 3.4).
Dimensionality n of the representation is set to
300 for both GloVe and the proposed method.
From Fig. 1 we see that the proposed method
reports the highest classification accuracies in all
12 domain pairs. Overall, the improvements of the
proposed method over NA, GloVe, and CS are sta-
tistically significant, and is comparable with SFA,
and SCL. The proposed method’s improvement
over CS shows the importance of predicting word
representations instead of counting. The improve-
ment over GloVe shows that it is inadequate to
simply apply existing word representation learn-
ing methods to learn independent word represen-
tations for the source and target domains.
We must consider the correspondences between
the two domains as expressed by the pivots to
jointly learn word representations. As shown in
Fig. 2, the proposed method reports superior ac-
curacies over GloVe across different dimension-
alities. Moreover, we see that when the dimen-
sionality of the representations increases, initially
accuracies increase in both methods and saturates
after 200 − 600 dimensions. However, further
increasing the dimensionality results in unstable
and some what poor accuracies due to overfit-
ting when training high-dimensional representa-
tions. Although our word representations learnt
by the proposed method are not specific to senti-
ment classification, the fact that it clearly outper-
forms SFA and SCL in all domain pairs is encour-
aging, and implies the wider-applicability of the
</bodyText>
<figureCaption confidence="0.998256">
Figure 2: Accuracy vs. dimensionality of the representation.
</figureCaption>
<bodyText confidence="0.994608">
proposed method for domain adaptation tasks be-
yond sentiment classification.
</bodyText>
<sectionHeader confidence="0.998961" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999961529411765">
We proposed an unsupervised method for learning
cross-domain word representations using a given
set of pivots and non-pivots selected from a source
and a target domain. Moreover, we proposed a do-
main adaptation method using the learnt word rep-
resentations.
Experimental results on a cross-domain senti-
ment classification task showed that the proposed
method outperforms several competitive baselines
and achieves best sentiment classification accura-
cies for all domain pairs. In future, we plan to
apply the proposed method to other types of do-
main adaptation tasks such as cross-domain part-
of-speech tagging, named entity recognition, and
relation extraction.
Source code and pre-processed data etc. for this
publication are publicly available3.
</bodyText>
<figure confidence="0.982429133333333">
3www.csc.liv.ac.uk/˜danushka/prj/darep
0 200 400 600 800 1000
Dimensions
Accuracy 74
72
70
68
66
64
62
60
Proposed
GloVe
NA
738
</figure>
<sectionHeader confidence="0.943408" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999872076923077">
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):673 – 721.
Marco Baroni, Georgiana Dinu, and Germ´an
Kruszewski. 2014. Don’t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proc. of
ACL, pages 238–247.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137 – 1155.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proc. of EMNLP, pages 120 –
128.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proc. of ACL, pages 440 – 447.
Danushka Bollegala, Yutaka Matsuo, and Mitsuru
Ishizuka. 2011a. Relation adaptation: Learning to
extract novel relations with minimum supervision.
In Proc. of IJCAI, pages 2205 – 2210.
Danushka Bollegala, David Weir, and John Carroll.
2011b. Using multiple sources to construct a senti-
ment sensitive thesaurus for cross-domain sentiment
classification. In ACL/HLT, pages 132 – 141.
Danushka Bollegala, Mitsuru Kusumoto, Yuichi
Yoshida, and Ken ichi Kawarabayashi. 2013a.
Mining for analogous tuples from an entity-relation
graph. In Proc. of IJCAI, pages 2064 – 2070.
Danushka Bollegala, Yutaka Matsuo, and Mitsuru
Ishizuka. 2013b. Minimally supervised novel re-
lation extraction using latent relational mapping.
IEEE Transactions on Knowledge and Data Engi-
neering, 25(2):419 – 432.
Danushka Bollegala, David Weir, and John Carroll.
2014. Learning to predict distributions of words
across domains. In Proc. of ACL, pages 613 – 623.
Gerlof Bouma. 2009. Normalized (pointwsie) mutual
information in collocation extraction. In Proc. of
GSCL, pages 31 – 40.
Ronan Collobert, Jason Weston, Leon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuska.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493 – 2537.
Nguyen Tuan Duc, Danushka Bollegala, and Mitsuru
Ishizuka. 2010. Using relational similarity between
word pairs for latent relational search on the web. In
IEEE/WIC/ACM International Conference on Web
Intelligence and IntelligentAgent Technology, pages
196 – 199.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12:2121 – 2159, July.
Hui Fang. 2008. A re-examination of query expansion
using lexical resources. In Proc. ofACL, pages 139–
147.
Stefano Faralli and Roberto Navigli. 2012. A new
minimally-supervised framework for domain word
sense disambiguation. In EMNLP, pages 1411 –
1422.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Proc. of
ICML.
Stephan Gouws, Yoshua Bengio, and Greg Corrado.
2015. Bilbowa: Fast bilingual distributed represen-
tations without word alignments. In Proc. of ICML.
Karl Moritz Hermann and Phil Blunsom. 2014. Mul-
tilingual distributed representations without word
alignment. In Proc. of ICLR.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proc. of ACL, pages 873 – 882.
Jing Jiang and ChengXiang Zhai. 2007a. Instance
weighting for domain adaptation in nlp. In ACL
2007, pages 264 – 271.
Jing Jiang and ChengXiang Zhai. 2007b. A two-stage
approach to domain adaptation for statistical classi-
fiers. In CIKM 2007, pages 401–410.
Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed rep-
resentations of words. In Proc. of COLING, pages
1459 – 1474.
David McClosky, Eugene Charniak, and Mark John-
son. 2010. Automatic domain adaptation for pars-
ing. In Proc. of NAACL/HLT, pages 28 – 36.
Tomas Mikolov, Kai Chen, and Jeffrey Dean. 2013a.
Efficient estimation of word representation in vector
space. CoRR.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013b. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In Proc. of NIPS, pages 3111 – 3119.
Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continous space
word representations. In NAACL’13, pages 746 –
751.
</reference>
<page confidence="0.591271">
739
</page>
<reference confidence="0.9990340625">
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proc. of ACL-
HLT, pages 236 – 244.
Andriy Mnih and Geoffrey E. Hinton. 2008. A scal-
able hierarchical distributed language model. In
Proc. of NIPS, pages 1081–1088.
Andriy Mnih and Koray Kavukcuoglu. 2013. Learning
word embeddings efficiently with noise-contrastive
estimation. In Proc. of NIPS.
Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew McCallum. 2014. Efficient non-
parametric estimation of multiple embeddings per
word in vector space. In Proc. of EMNLP, pages
1059–1069.
Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang
Yang, and Zheng Chen. 2010. Cross-domain sen-
timent classification via spectral feature alignment.
In Proc. of WWW, pages 751 – 760.
Jeffery Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: global vectors for
word representation. In Proc. of EMNLP.
Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Proc. of HLT-NAACL, pages 109 – 117.
Michael Roth and Kristian Woodsend. 2014. Com-
position of word representations improves semantic
role labelling. In Proc. of EMNLP, pages 407–413.
Tobias Schnabel and Hinrich Sch¨utze. 2013. Towards
robust cross-domain domain adaptation for part-of-
speech tagging. In Proc. of IJCNLP, pages 198 –
206.
Richard Socher, Cliff Chiung-Yu Lin, Andrew Ng, and
Chris Manning. 2011a. Parsing natural scenes and
natural language with recursive neural networks. In
ICML’11.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011b.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proc. of EMNLP,
pages 151–161.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. Journal of Aritificial Intelligence Research,
37:141 – 188.
Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual word embed-
dings for phrase-based machine translation. In Proc.
of EMNLP’13, pages 1393 – 1398.
</reference>
<page confidence="0.859643">
740
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.235287">
<title confidence="0.999049">Unsupervised Cross-Domain Word Representation Learning</title>
<author confidence="0.972095">Danushka Bollegala Takanori Maehara Ken-ichi Kawarabayashi</author>
<abstract confidence="0.933051303030303">danushka.bollegala@ maehara.takanori@ k keniti@ liverpool.ac.uk shizuoka.ac.jp nii.ac.jp University of Liverpool Shizuoka University National Institute of Informatics JST, ERATO, Kawarabayashi Large Graph Project. Abstract Meaning of a word varies from one domain to another. Despite this important domain dependence in word semantics, existing word representation learning methods are bound to a single domain. a pair of we propose an unsupervised method for learning domain-specific word representations that accurately capture the domainspecific aspects of word semantics. First, we select a subset of frequent words that in both domains as Next, we optimize an objective function that enforces two constraints: (a) for both source and target domain documents, pivots that appear in a document must accurately predict the co-occurring non-pivots, and (b) word representations learnt for pivots must be similar in the two domains. Moreover, we propose a method to perform domain adaptation using the learnt word representations. Our proposed method significantly outperforms competitive baselines including the state-of-theart domain-insensitive word representations, and reports best sentiment classification accuracies for all domain-pairs in a benchmark dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>Distributional memory: A general framework for corpus-based semantics.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<pages>721</pages>
<contexts>
<context position="10109" citStr="Baroni and Lenci, 2010" startWordPosition="1542" endWordPosition="1545">many NLP tasks (Turney and Pantel, 2010). By applying algebraic operations on the word representations, we can perform numerous tasks in NLP, such as composing representations for larger textual units beyond individual words such as phrases (Mitchell and Lapata, 2008). Moreover, word representations are found to be useful for measuring semantic similarity, and for solving proportional analogies (Mikolov et al., 2013c). Two main approaches for computing word representations can be identified in prior work (Baroni et al., 2014): counting-based and prediction-based. In counting-based approaches (Baroni and Lenci, 2010), a word w is represented by a vector w that contains other words that co-occur with w in a corpus. Numerous methods for selecting co-occurrence contexts such as proximity or dependency relations have been proposed (Turney and Pantel, 2010). Despite the numerous successful applications of co-occurrence countingbased distributional word representations, their high dimensionality and sparsity are often problematic in practice. Consequently, further postprocessing steps such as dimensionality reduction, and feature selection are often required when using counting-based word representations. On th</context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Computational Linguistics, 36(4):673 – 721.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>238--247</pages>
<contexts>
<context position="10017" citStr="Baroni et al., 2014" startWordPosition="1532" endWordPosition="1535">algebraic structure such as a vector (more generally a tensor) is a common first step in many NLP tasks (Turney and Pantel, 2010). By applying algebraic operations on the word representations, we can perform numerous tasks in NLP, such as composing representations for larger textual units beyond individual words such as phrases (Mitchell and Lapata, 2008). Moreover, word representations are found to be useful for measuring semantic similarity, and for solving proportional analogies (Mikolov et al., 2013c). Two main approaches for computing word representations can be identified in prior work (Baroni et al., 2014): counting-based and prediction-based. In counting-based approaches (Baroni and Lenci, 2010), a word w is represented by a vector w that contains other words that co-occur with w in a corpus. Numerous methods for selecting co-occurrence contexts such as proximity or dependency relations have been proposed (Turney and Pantel, 2010). Despite the numerous successful applications of co-occurrence countingbased distributional word representations, their high dimensionality and sparsity are often problematic in practice. Consequently, further postprocessing steps such as dimensionality reduction, an</context>
<context position="12804" citStr="Baroni et al., 2014" startWordPosition="1971" endWordPosition="1974">lity of the learning algorithms. For example, the skip-gram model (Mikolov et al., 2013b) predicts the words c that appear in the local context of a word w, whereas the continuous bag-of-words model (CBOW) predicts a word w conditioned on all the words c that appear in w’s local context (Mikolov et al., 2013a). Methods that use global co-occurrences in the entire corpus to learn word representations have shown to outperform methods that use only local cooccurrences (Huang et al., 2012; Pennington et al., 2014). Overall, prediction-based methods have shown to outperform counting-based methods (Baroni et al., 2014). Despite their impressive performance, existing methods for word representation learning do not consider the semantic variation of words across different domains. However, as described in Section 1, the meaning of a word vary from one domain to another, and must be considered. To the best of our knowledge, the only prior work studying the problem of word representation variation across domains is due to Bollegala et al. (2014). Given a source and a target domain, they first select a set of pivots using pointwise mutual information, and create two distributional representa732 tions for each pi</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014. Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proc. of ACL, pages 238–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<pages>1155</pages>
<contexts>
<context position="11250" citStr="Bengio et al., 2003" startWordPosition="1718" endWordPosition="1721">ection are often required when using counting-based word representations. On the other hand, prediction-based approaches first assign each word, for example, with a ddimensional real-vector, and learn the elements of those vectors by applying them in an auxiliary task such as language modeling, where the goal is to predict the next word in a given sequence. The dimensionality d is fixed for all the words in the vocabulary, and, unlike counting-based word representations, is much smaller (e.g. d ∈ [10,1000] in practice) compared to the vocabulary size. The neural network language model (NNLM) (Bengio et al., 2003) uses a multi-layer feed-forward neural network to predict the next word in a sequence, and uses backpropagation to update the word vectors such that the prediction error is minimized. Although NNLMs learn word representations as a by-product, the main focus on language modeling is to predict the next word in a sentence given the previous words, and not learning word representations that capture semantics. Moreover, training multi-layer neural networks using large text corpora is time consuming. To overcome those limitations, methods that specifically focus on learning word representations tha</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137 – 1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>120--128</pages>
<contexts>
<context position="4872" citStr="Blitzer et al., 2006" startWordPosition="738" endWordPosition="741">ure domainspecific semantic variations of w. In this paper, we use the term domain to represent a collection of documents related to a particular topic such as user-reviews in Amazon for a product category (e.g. books, dvds, movies, etc.). However, a domain in general can be a field of study (e.g. biology, computer science, law, etc.) or even an entire source of information (e.g. twitter, blogs, news articles, etc.). In particular, we do not assume the availability of any labeled data for learning word representations. This problem setting is closely related to unsupervised domain adaptation (Blitzer et al., 2006), which has found numerous useful applications such as, sentiment classification and POS tagging. For example, in unsupervised cross-domain sentiment classification (Blitzer et al., 2006; Blitzer et al., 2007), we train a binary sentiment classifier using positive and negative labeled user reviews in the source domain, and apply the trained classifier to predict sentiment of the target domain’s user reviews. Although the distinction between the source and the target domains is not important for the word representation learning step, it is important for the domain adaptation tasks in which we s</context>
<context position="6414" citStr="Blitzer et al., 2006" startWordPosition="985" endWordPosition="988">le domain. We propose an unsupervised cross-domain word representation learning method that jointly optimizes two criteria: (a) given a document d from the source or the target domain, we must accurately predict the non-pivots that occur in d using the pivots that occur in d, and (b) the source and target domain representations we learn for pivots must be similar. The main challenge in domain adaptation is feature mismatch, where the features that we use for training a classifier in the source domain do not necessarily occur in the target domain. Consequently, prior work on domain adaptation (Blitzer et al., 2006; Pan et al., 2010) learn lower-dimensional mappings from non-pivots to pivots, thereby overcoming the feature mismatch problem. Criteria (a) ensures that word representations for domain-specific non-pivots in each domain are related to the word representations for domain-independent pivots. This relationship enables us to discover pivots that are similar to target domain-specific non-pivots, thereby overcoming the feature mismatch problem. On the other hand, criteria (b) captures the prior knowledge that high-frequent words common to two domains often represent domain-independent semantics. F</context>
<context position="9332" citStr="Blitzer et al., 2006" startWordPosition="1420" endWordPosition="1423">on learning method (Pennington et al., 2014), and several competitive baselines. In particular, our proposed cross-domain word representation learning method is not specific to a particular task such as sentiment classification, and in principle, can be in applied to a wide-range of domain adaptation tasks. Despite this taskindependent nature of the proposed method, it achieves the best sentiment classification accuracies on all domain-pairs, reporting statistically comparable results to the current state-of-the-art unsupervised cross-domain sentiment classification methods (Pan et al., 2010; Blitzer et al., 2006). 2 Related Work Representing the semantics of a word using some algebraic structure such as a vector (more generally a tensor) is a common first step in many NLP tasks (Turney and Pantel, 2010). By applying algebraic operations on the word representations, we can perform numerous tasks in NLP, such as composing representations for larger textual units beyond individual words such as phrases (Mitchell and Lapata, 2008). Moreover, word representations are found to be useful for measuring semantic similarity, and for solving proportional analogies (Mikolov et al., 2013c). Two main approaches for</context>
<context position="27194" citStr="Blitzer et al. (2006" startWordPosition="4446" endWordPosition="4449"> a pivot according to the score given by: score(x) = min (NPMI(x, S), NPMI(x, T)) . (12) We rank features that are common to both domains in the descending order of their scores as given by Eq. 12, and select the top N? features as pivots. We rank features x that occur only in the source domain by NPMI(x, S), and select the top ranked NS features as source-specific non-pivots. Likewise, we rank the features x that occur only in the target domain by NPMI(x, T), and select the top ranked NT features as target-specific non-pivots. The pivot selection criterion described here differs from that of Blitzer et al. (2006; 2007), where pivots are defined as features that behave similarly both in the source and the target domains. They compute the mutual information between a feature (i.e. unigrams or bigrams) and the sentiment labels using source domain labeled reviews. This method is useful when selecting pivots that are closely associated with positive or negative sentiment in the source domain. However, in unsupervised domain adaptation we do not have labeled data for the target domain. Therefore, the pivots selected using this approach are not guaranteed to demonstrate the same sentiment in the target doma</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proc. of EMNLP, pages 120 – 128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="5081" citStr="Blitzer et al., 2007" startWordPosition="767" endWordPosition="770">g. books, dvds, movies, etc.). However, a domain in general can be a field of study (e.g. biology, computer science, law, etc.) or even an entire source of information (e.g. twitter, blogs, news articles, etc.). In particular, we do not assume the availability of any labeled data for learning word representations. This problem setting is closely related to unsupervised domain adaptation (Blitzer et al., 2006), which has found numerous useful applications such as, sentiment classification and POS tagging. For example, in unsupervised cross-domain sentiment classification (Blitzer et al., 2006; Blitzer et al., 2007), we train a binary sentiment classifier using positive and negative labeled user reviews in the source domain, and apply the trained classifier to predict sentiment of the target domain’s user reviews. Although the distinction between the source and the target domains is not important for the word representation learning step, it is important for the domain adaptation tasks in which we subsequently evaluate the learnt word representations. Following prior work on domain adaptation (Blitzer et al., 2006), high-frequent features (unigrams/bigrams) common to both domains are referred to as domai</context>
<context position="32455" citStr="Blitzer et al. (2007)" startWordPosition="5288" endWordPosition="5291">epresentations. We experimented with several popular similarity measures for f and found cosine similarity to perform consistently well. We can interpret Eq. 13 as a method for expanding a test target document using nearest neighbor features from the source domain labeled data. It is analogous to query expansion used in information retrieval to improve document recall (Fang, 2008). Alternatively, Eq. 13 can be seen as a linearly-weighted additive kernel function over two feature spaces. 4 Experiments and Results For train and evaluation purposes, we use the Amazon product reviews collected by Blitzer et al. (2007) for the four product categories: books (B), DVDs (D), electronic items (E), and kitchen appliances (K). There are 1000 positive and 1000 negative sentiment labeled reviews for each domain. Moreover, each domain has on average 17,547 unlabeled reviews. We use the standard split of 800 positive and 800 negative labeled reviews from each domain as training data, and the rest (200+200) for testing. For validation purposes we use movie (source) and computer (target) domains, which were also collected by Blitzer et al. (2007), but not part of the train/test domains. Experiments conducted using this</context>
<context position="34310" citStr="Blitzer et al., 2007" startWordPosition="5584" endWordPosition="5587"> current state-of-the-art methods for unsupervised crossdomain sentiment classification (SFA, SCL), word representation learning (GloVe), and crossdomain similarity prediction (CS). The NA (noadapt) lower baseline uses a classifier trained on source labeled data to classify target test data without any domain adaptation. The InDomain baseline is trained using the labeled data for the target domain, and simulates the performance we can expect to obtain if target domain labeled data were available. Spectral Feature Alignment (SFA) (Pan et al., 2010) and Structural Correspondence Learning (SCL) (Blitzer et al., 2007) are the state-ofthe-art methods for cross-domain sentiment classification. However, those methods do not learn word representations. We use Global Vector Prediction (GloVe) (Pennington et al., 2014), the current state-of-theart word representation learning method, to learn word representations separately from the source and target domain unlabeled data, and use the learnt representations in Eq. 13 for sentiment classification. In contrast to the joint word representations learnt by the proposed method, GloVe simulates the level of performance we would obtain by learning representations indepe</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Proc. of ACL, pages 440 – 447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danushka Bollegala</author>
<author>Yutaka Matsuo</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Relation adaptation: Learning to extract novel relations with minimum supervision.</title>
<date>2011</date>
<booktitle>In Proc. of IJCAI,</booktitle>
<pages>2205--2210</pages>
<contexts>
<context position="3428" citStr="Bollegala et al., 2011" startWordPosition="511" endWordPosition="514">l., 2013a) most existing approaches are limited to learning only a single representation for a given word (Reisinger and Mooney, 2010). Although there have been some work on learning multiple prototype representations (Huang et al., 2012; Neelakantan et al., 2014) for a word considering its multiple senses, such methods do not consider the semantics of the domain in which the word is being used. If we can learn separate representations for a word for each domain in which it occurs, we can use the learnt representations for domain adaptation tasks such as cross-domain sentiment classification (Bollegala et al., 2011b), cross-domain POS tagging (Schnabel and Sch¨utze, 2013), crossdomain dependency parsing (McClosky et al., 2010), and domain adaptation of relation extractors (Bollegala et al., 2013a; Bollegala et al., 2013b; Bollegala et al., 2011a; Jiang and Zhai, 2007a; Jiang and Zhai, 2007b). We introduce the cross-domain word represen730 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 730–740, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics tation l</context>
<context position="17982" citStr="Bollegala et al., 2011" startWordPosition="2787" endWordPosition="2790">e domain. Decoupling of the word representation learning from any tasks in which those representations are subsequently used, simplifies the problem as well as enables us to learn task-independent word representations with potential generic applicability. Although we limit the discussion to a pair of domains for simplicity, the proposed method can be easily extended to jointly learn word representations for more than two domains. In fact, prior work on cross-domain sentiment analysis show that incorporating multiple source domains improves sentiment classification accuracy on a target domain (Bollegala et al., 2011b; Glorot et al., 2011). 733 3.2 Proposed Method To describe our proposed method, let us denote a pivot and a non-pivot feature respectively by c and w. Our proposed method does not depend on a specific pivot selection method, and can be used with all previously proposed methods for selecting pivots as explained later in Section 3.4. A pivot c is represented in the source and target domains respectively by vectors cS E R&apos; and cT E R&apos;. Likewise, a source specific non-pivot w is represented by wS in the source domain, whereas a target specific non-pivot w is represented by wT in the target domai</context>
</contexts>
<marker>Bollegala, Matsuo, Ishizuka, 2011</marker>
<rawString>Danushka Bollegala, Yutaka Matsuo, and Mitsuru Ishizuka. 2011a. Relation adaptation: Learning to extract novel relations with minimum supervision. In Proc. of IJCAI, pages 2205 – 2210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danushka Bollegala</author>
<author>David Weir</author>
<author>John Carroll</author>
</authors>
<title>Using multiple sources to construct a sentiment sensitive thesaurus for cross-domain sentiment classification.</title>
<date>2011</date>
<booktitle>In ACL/HLT,</booktitle>
<pages>132--141</pages>
<contexts>
<context position="3428" citStr="Bollegala et al., 2011" startWordPosition="511" endWordPosition="514">l., 2013a) most existing approaches are limited to learning only a single representation for a given word (Reisinger and Mooney, 2010). Although there have been some work on learning multiple prototype representations (Huang et al., 2012; Neelakantan et al., 2014) for a word considering its multiple senses, such methods do not consider the semantics of the domain in which the word is being used. If we can learn separate representations for a word for each domain in which it occurs, we can use the learnt representations for domain adaptation tasks such as cross-domain sentiment classification (Bollegala et al., 2011b), cross-domain POS tagging (Schnabel and Sch¨utze, 2013), crossdomain dependency parsing (McClosky et al., 2010), and domain adaptation of relation extractors (Bollegala et al., 2013a; Bollegala et al., 2013b; Bollegala et al., 2011a; Jiang and Zhai, 2007a; Jiang and Zhai, 2007b). We introduce the cross-domain word represen730 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 730–740, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics tation l</context>
<context position="17982" citStr="Bollegala et al., 2011" startWordPosition="2787" endWordPosition="2790">e domain. Decoupling of the word representation learning from any tasks in which those representations are subsequently used, simplifies the problem as well as enables us to learn task-independent word representations with potential generic applicability. Although we limit the discussion to a pair of domains for simplicity, the proposed method can be easily extended to jointly learn word representations for more than two domains. In fact, prior work on cross-domain sentiment analysis show that incorporating multiple source domains improves sentiment classification accuracy on a target domain (Bollegala et al., 2011b; Glorot et al., 2011). 733 3.2 Proposed Method To describe our proposed method, let us denote a pivot and a non-pivot feature respectively by c and w. Our proposed method does not depend on a specific pivot selection method, and can be used with all previously proposed methods for selecting pivots as explained later in Section 3.4. A pivot c is represented in the source and target domains respectively by vectors cS E R&apos; and cT E R&apos;. Likewise, a source specific non-pivot w is represented by wS in the source domain, whereas a target specific non-pivot w is represented by wT in the target domai</context>
</contexts>
<marker>Bollegala, Weir, Carroll, 2011</marker>
<rawString>Danushka Bollegala, David Weir, and John Carroll. 2011b. Using multiple sources to construct a sentiment sensitive thesaurus for cross-domain sentiment classification. In ACL/HLT, pages 132 – 141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danushka Bollegala</author>
<author>Mitsuru Kusumoto</author>
<author>Yuichi Yoshida</author>
<author>Ken ichi Kawarabayashi</author>
</authors>
<title>Mining for analogous tuples from an entity-relation graph.</title>
<date>2013</date>
<booktitle>In Proc. of IJCAI,</booktitle>
<pages>pages</pages>
<contexts>
<context position="3612" citStr="Bollegala et al., 2013" startWordPosition="537" endWordPosition="540">ultiple prototype representations (Huang et al., 2012; Neelakantan et al., 2014) for a word considering its multiple senses, such methods do not consider the semantics of the domain in which the word is being used. If we can learn separate representations for a word for each domain in which it occurs, we can use the learnt representations for domain adaptation tasks such as cross-domain sentiment classification (Bollegala et al., 2011b), cross-domain POS tagging (Schnabel and Sch¨utze, 2013), crossdomain dependency parsing (McClosky et al., 2010), and domain adaptation of relation extractors (Bollegala et al., 2013a; Bollegala et al., 2013b; Bollegala et al., 2011a; Jiang and Zhai, 2007a; Jiang and Zhai, 2007b). We introduce the cross-domain word represen730 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 730–740, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics tation learning task, where given two domains, (referred to as the source (S) and the target (T)) the goal is to learn two separate representations wS and wT for a word w respectively from the</context>
</contexts>
<marker>Bollegala, Kusumoto, Yoshida, Kawarabayashi, 2013</marker>
<rawString>Danushka Bollegala, Mitsuru Kusumoto, Yuichi Yoshida, and Ken ichi Kawarabayashi. 2013a. Mining for analogous tuples from an entity-relation graph. In Proc. of IJCAI, pages 2064 – 2070.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danushka Bollegala</author>
<author>Yutaka Matsuo</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Minimally supervised novel relation extraction using latent relational mapping.</title>
<date>2013</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>25</volume>
<issue>2</issue>
<pages>432</pages>
<contexts>
<context position="3612" citStr="Bollegala et al., 2013" startWordPosition="537" endWordPosition="540">ultiple prototype representations (Huang et al., 2012; Neelakantan et al., 2014) for a word considering its multiple senses, such methods do not consider the semantics of the domain in which the word is being used. If we can learn separate representations for a word for each domain in which it occurs, we can use the learnt representations for domain adaptation tasks such as cross-domain sentiment classification (Bollegala et al., 2011b), cross-domain POS tagging (Schnabel and Sch¨utze, 2013), crossdomain dependency parsing (McClosky et al., 2010), and domain adaptation of relation extractors (Bollegala et al., 2013a; Bollegala et al., 2013b; Bollegala et al., 2011a; Jiang and Zhai, 2007a; Jiang and Zhai, 2007b). We introduce the cross-domain word represen730 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 730–740, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics tation learning task, where given two domains, (referred to as the source (S) and the target (T)) the goal is to learn two separate representations wS and wT for a word w respectively from the</context>
</contexts>
<marker>Bollegala, Matsuo, Ishizuka, 2013</marker>
<rawString>Danushka Bollegala, Yutaka Matsuo, and Mitsuru Ishizuka. 2013b. Minimally supervised novel relation extraction using latent relational mapping. IEEE Transactions on Knowledge and Data Engineering, 25(2):419 – 432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danushka Bollegala</author>
<author>David Weir</author>
<author>John Carroll</author>
</authors>
<title>Learning to predict distributions of words across domains.</title>
<date>2014</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>613--623</pages>
<contexts>
<context position="2278" citStr="Bollegala et al., 2014" startWordPosition="329" endWordPosition="333">ity lately because of their accuracy as semantic representations for words (Mikolov et al., 2013a; Pennington et al., 2014). However, the meaning of a word often varies from one domain to another. For example, the phrase lightweight is often used in a positive sentiment in the portable electronics domain because a lightweight device is easier to carry around, which is a positive attribute for a portable electronic device. However, the same phrase has a negative sentiment assocition in the movie domain because movies that do not invoke deep thoughts in viewers are considered to be lightweight (Bollegala et al., 2014). However, existing word representation learning methods are agnostic to such domain-specific semantic variations of words, and capture semantics of words only within a single domain. To overcome this problem and capture domain-specific semantic orientations of words, we propose a method that learns separate distributed representations for each domain in which a word occurs. Despite the successful applications of distributed word representation learning methods (Pennington et al., 2014; Collobert et al., 2011; Mikolov et al., 2013a) most existing approaches are limited to learning only a singl</context>
<context position="13235" citStr="Bollegala et al. (2014)" startWordPosition="2042" endWordPosition="2045">methods that use only local cooccurrences (Huang et al., 2012; Pennington et al., 2014). Overall, prediction-based methods have shown to outperform counting-based methods (Baroni et al., 2014). Despite their impressive performance, existing methods for word representation learning do not consider the semantic variation of words across different domains. However, as described in Section 1, the meaning of a word vary from one domain to another, and must be considered. To the best of our knowledge, the only prior work studying the problem of word representation variation across domains is due to Bollegala et al. (2014). Given a source and a target domain, they first select a set of pivots using pointwise mutual information, and create two distributional representa732 tions for each pivot using their co-occurrence contexts in a particular domain. Next, a projection matrix from the source to the target domain feature spaces is learnt using partial least squares regression. Finally, the learnt projection matrix is used to find the nearest neighbors in the source domain for each target domain-specific features. However, unlike our proposed method, their method does not learn domain-specific word representations</context>
<context position="35007" citStr="Bollegala et al. (2014)" startWordPosition="5687" endWordPosition="5690">. However, those methods do not learn word representations. We use Global Vector Prediction (GloVe) (Pennington et al., 2014), the current state-of-theart word representation learning method, to learn word representations separately from the source and target domain unlabeled data, and use the learnt representations in Eq. 13 for sentiment classification. In contrast to the joint word representations learnt by the proposed method, GloVe simulates the level of performance we would obtain by learning representations independently. CS denotes the cross-domain vector prediction method proposed by Bollegala et al. (2014). Although CS can be used to learn a vector-space translation matrix, it does not learn word representations. Vertical bars represent the classification accuracies (i.e. percentage of the correctly classified test instances) obtained by a particular method on target domain’s test data, and Clopper-Pearson 95% binomial confidence intervals are superimposed. Differences in data pre-processing (tokenization/lemmatization), selection (train/test splits), feature representation (unigram/bigram), pivot selection (MI/frequency), and the binary classification algorithms used to train the final classif</context>
</contexts>
<marker>Bollegala, Weir, Carroll, 2014</marker>
<rawString>Danushka Bollegala, David Weir, and John Carroll. 2014. Learning to predict distributions of words across domains. In Proc. of ACL, pages 613 – 623.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerlof Bouma</author>
</authors>
<title>Normalized (pointwsie) mutual information in collocation extraction.</title>
<date>2009</date>
<booktitle>In Proc. of GSCL,</booktitle>
<pages>31--40</pages>
<contexts>
<context position="25993" citStr="Bouma, 2009" startWordPosition="4232" endWordPosition="4233">ard stop words list. Stop word removal increases the effective cooccurrence window size for a pivot. Finally, we remove features that occur less than 50 times in the entire set of documents. Several methods have been proposed in the prior work on domain adaptation for selecting a set of pivots from a given pair of domains such as the minimum frequency of occurrence of a feature in the two domains, mutual information (MI), and the entropy of the feature distribution over the documents (Pan et al., 2010). In our preliminary experiments, we discovered that a normalized version of the PMI (NPMI) (Bouma, 2009) to work consistently well for selecting pivots from different pairs of domains. NPMI between two features x and y is given by: NPMI(x, y) = logp(x, y) 1 G(x)p(y) − log(p(x, y)) (11) Here, the joint probability p(x, y), and the marginal probabilities p(x) and p(y) are estimated using the number of co-occurrences of x and y in the sentences in the documents. Eq. 11 normalizes both the upper and lower bounds of the PMI. 2http://nlp.stanford.edu/software/ corenlp.shtml ( aL 0 if cS&gt;(wS − w∗ S) &gt; 1 = (5) −cS otherwise awS aL awT 735 We measure the appropriateness of a feature as a pivot according </context>
</contexts>
<marker>Bouma, 2009</marker>
<rawString>Gerlof Bouma. 2009. Normalized (pointwsie) mutual information in collocation extraction. In Proc. of GSCL, pages 31 – 40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>Leon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuska</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>12</volume>
<pages>2537</pages>
<contexts>
<context position="1596" citStr="Collobert et al., 2011" startWordPosition="221" endWordPosition="224">dict the co-occurring non-pivots, and (b) word representations learnt for pivots must be similar in the two domains. Moreover, we propose a method to perform domain adaptation using the learnt word representations. Our proposed method significantly outperforms competitive baselines including the state-of-theart domain-insensitive word representations, and reports best sentiment classification accuracies for all domain-pairs in a benchmark dataset. 1 Introduction Learning semantic representations for words is a fundamental task in NLP that is required in numerous higher-level NLP applications (Collobert et al., 2011). Distributed word representations have gained much popularity lately because of their accuracy as semantic representations for words (Mikolov et al., 2013a; Pennington et al., 2014). However, the meaning of a word often varies from one domain to another. For example, the phrase lightweight is often used in a positive sentiment in the portable electronics domain because a lightweight device is easier to carry around, which is a positive attribute for a portable electronic device. However, the same phrase has a negative sentiment assocition in the movie domain because movies that do not invoke </context>
<context position="8147" citStr="Collobert et al., 2011" startWordPosition="1252" endWordPosition="1255"> contributions in this paper can be summarized as follows. • We propose a distributed word representation learning method that learns separate representations for a word for each domain in which it occurs. To the best of our knowledge, ours is the first-ever domain-sensitive distributed word representation learning method. • Given domain-specific word representations, we propose a method to learn a cross-domain sentiment classifier. Although word representation learning methods have been used for various related tasks in NLP such as similarity measurement (Mikolov et al., 2013c), POS tagging (Collobert et al., 2011), dependency parsing (Socher et al., 2011a), machine translation (Zou et al., 2013), sentiment classification (Socher et al., 2011b), and semantic role labeling (Roth and Woodsend, 2014), to the best of our knowledge, word representations methods have not yet been used for crossdomain sentiment classification. Experimental results for cross-domain sentiment classification on a benchmark dataset show that the word representations learnt using the proposed method statistically significantly outper731 form a state-of-the-art domain-insensitive word representation learning method (Pennington et al</context>
<context position="24076" citStr="Collobert et al., 2011" startWordPosition="3925" endWordPosition="3928">ng rate. All word representations are initialized with n dimensional random vectors sampled from a zero mean and unit variance Gaussian. Although the objective in Eq. 4 is not jointly convex in all four representations, it is convex w.r.t. the representation of a particular feature (pivot or non-pivot) when the representations for all the other features are held fixed. In our experiments, the training converged in all cases with less than 100 epochs over the dataset. The rank-based predictive hinge loss (Eq. 1) is inspired by the prior work on word representation learning for a single domain (Collobert et al., 2011). However, unlike the multilayer neural network in Collobert et al. (2011), the proposed method uses a computationally efficient single layer to reduce the number of parameters that must be learnt, thereby scaling to large datasets. Similar to the skip-gram model (Mikolov et al., 2013a), the proposed method predicts occurrences of contexts (non-pivots) w within a fixed-size contextual window of a target word (pivot) c. Scoring the co-occurrences of two words c and w by the bilinear form given by the inner-product is similar to prior work on domain-insensitive word-representation learning (Mnih</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuska, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuska. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493 – 2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nguyen Tuan Duc</author>
<author>Danushka Bollegala</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Using relational similarity between word pairs for latent relational search on the web.</title>
<date>2010</date>
<booktitle>In IEEE/WIC/ACM International Conference on Web Intelligence and IntelligentAgent Technology,</booktitle>
<pages>196--199</pages>
<contexts>
<context position="15401" citStr="Duc et al., 2010" startWordPosition="2375" endWordPosition="2378"> and does not consider how the representations vary across domains. On the other hand, our proposed method learns a single representation for a particular word for each domain in which it occurs. Although in this paper we focus on the monolingual setting where source and target domains belong to the same language, the related setting where learning representations for words that are translational pairs across languages has been studied (Hermann and Blunsom, 2014; Klementiev et al., 2012; Gouws et al., 2015). Such representations are particularly useful for cross-lingual information retrieval (Duc et al., 2010). It will be an interesting future research direction to extend our proposed method to learn such cross-lingual word representations. 3 Cross-Domain Representation Learning We propose a method for learning word representations that are sensitive to the semantic variations of words across domains. We call this problem cross-domain word representation learning, and provide a definition in Section 3.1. Next, in Section 3.2, given a set of pivots that occurs in both a source and a target domain, we propose a method for learning cross-domain word representations. We defer the discussion of pivot se</context>
</contexts>
<marker>Duc, Bollegala, Ishizuka, 2010</marker>
<rawString>Nguyen Tuan Duc, Danushka Bollegala, and Mitsuru Ishizuka. 2010. Using relational similarity between word pairs for latent relational search on the web. In IEEE/WIC/ACM International Conference on Web Intelligence and IntelligentAgent Technology, pages 196 – 199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>12</volume>
<pages>2159</pages>
<contexts>
<context position="23422" citStr="Duchi et al., 2011" startWordPosition="3814" endWordPosition="3817">r updates, we compute the gradients of the overall loss function in Eq. 4 w.r.t. to each parameter as follows: aL = (0 if cS&gt; (w − w∗S) &gt; 1 (6) aw∗ cS otheriwse S ( 0 if cT &gt;(wT − w∗ T ) &gt; 1 = (7) −cT otherwise aL =JO if cT &gt;(wT − wT∗) &gt; 1 (8) awT CT otherwise aL = A(cS − cT ) if cS&gt; (wS − w∗S) &gt;( 1 acS w∗S − wS + A(cS − cT ) otherwise (9) aL = (A(cT − cS) if cT &gt;(wT − wT∗) &gt; 1 acT w∗T − wT + A(cT − cS) otherwise (10) Here, for simplicity, we drop the arguments inside the loss function and write it as L. We use mini batch stochastic gradient descent with a batch size of 50 instances. AdaGrad (Duchi et al., 2011) is used to schedule the learning rate. All word representations are initialized with n dimensional random vectors sampled from a zero mean and unit variance Gaussian. Although the objective in Eq. 4 is not jointly convex in all four representations, it is convex w.r.t. the representation of a particular feature (pivot or non-pivot) when the representations for all the other features are held fixed. In our experiments, the training converged in all cases with less than 100 epochs over the dataset. The rank-based predictive hinge loss (Eq. 1) is inspired by the prior work on word representation</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121 – 2159, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Fang</author>
</authors>
<title>A re-examination of query expansion using lexical resources.</title>
<date>2008</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>139--147</pages>
<contexts>
<context position="32217" citStr="Fang, 2008" startWordPosition="5252" endWordPosition="5253">ture in h against the features in the classification model θ. For pivots c ∈ h, we use the the source domain representations to measure similarity, whereas for the (target-specific) non-pivots w ∈ h, we use their target domain representations. We experimented with several popular similarity measures for f and found cosine similarity to perform consistently well. We can interpret Eq. 13 as a method for expanding a test target document using nearest neighbor features from the source domain labeled data. It is analogous to query expansion used in information retrieval to improve document recall (Fang, 2008). Alternatively, Eq. 13 can be seen as a linearly-weighted additive kernel function over two feature spaces. 4 Experiments and Results For train and evaluation purposes, we use the Amazon product reviews collected by Blitzer et al. (2007) for the four product categories: books (B), DVDs (D), electronic items (E), and kitchen appliances (K). There are 1000 positive and 1000 negative sentiment labeled reviews for each domain. Moreover, each domain has on average 17,547 unlabeled reviews. We use the standard split of 800 positive and 800 negative labeled reviews from each domain as training data,</context>
</contexts>
<marker>Fang, 2008</marker>
<rawString>Hui Fang. 2008. A re-examination of query expansion using lexical resources. In Proc. ofACL, pages 139– 147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefano Faralli</author>
<author>Roberto Navigli</author>
</authors>
<title>A new minimally-supervised framework for domain word sense disambiguation.</title>
<date>2012</date>
<booktitle>In EMNLP,</booktitle>
<pages>1411--1422</pages>
<marker>Faralli, Navigli, 2012</marker>
<rawString>Stefano Faralli and Roberto Navigli. 2012. A new minimally-supervised framework for domain word sense disambiguation. In EMNLP, pages 1411 – 1422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Glorot</author>
<author>Antoine Bordes</author>
<author>Yoshua Bengio</author>
</authors>
<title>Domain adaptation for large-scale sentiment classification: A deep learning approach.</title>
<date>2011</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="18005" citStr="Glorot et al., 2011" startWordPosition="2791" endWordPosition="2794">he word representation learning from any tasks in which those representations are subsequently used, simplifies the problem as well as enables us to learn task-independent word representations with potential generic applicability. Although we limit the discussion to a pair of domains for simplicity, the proposed method can be easily extended to jointly learn word representations for more than two domains. In fact, prior work on cross-domain sentiment analysis show that incorporating multiple source domains improves sentiment classification accuracy on a target domain (Bollegala et al., 2011b; Glorot et al., 2011). 733 3.2 Proposed Method To describe our proposed method, let us denote a pivot and a non-pivot feature respectively by c and w. Our proposed method does not depend on a specific pivot selection method, and can be used with all previously proposed methods for selecting pivots as explained later in Section 3.4. A pivot c is represented in the source and target domains respectively by vectors cS E R&apos; and cT E R&apos;. Likewise, a source specific non-pivot w is represented by wS in the source domain, whereas a target specific non-pivot w is represented by wT in the target domain. By definition, a non</context>
</contexts>
<marker>Glorot, Bordes, Bengio, 2011</marker>
<rawString>Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Domain adaptation for large-scale sentiment classification: A deep learning approach. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Gouws</author>
<author>Yoshua Bengio</author>
<author>Greg Corrado</author>
</authors>
<title>Bilbowa: Fast bilingual distributed representations without word alignments.</title>
<date>2015</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="15296" citStr="Gouws et al., 2015" startWordPosition="2360" endWordPosition="2363">er of senses per word type is automatically estimated. However, their method is limited to a single domain, and does not consider how the representations vary across domains. On the other hand, our proposed method learns a single representation for a particular word for each domain in which it occurs. Although in this paper we focus on the monolingual setting where source and target domains belong to the same language, the related setting where learning representations for words that are translational pairs across languages has been studied (Hermann and Blunsom, 2014; Klementiev et al., 2012; Gouws et al., 2015). Such representations are particularly useful for cross-lingual information retrieval (Duc et al., 2010). It will be an interesting future research direction to extend our proposed method to learn such cross-lingual word representations. 3 Cross-Domain Representation Learning We propose a method for learning word representations that are sensitive to the semantic variations of words across domains. We call this problem cross-domain word representation learning, and provide a definition in Section 3.1. Next, in Section 3.2, given a set of pivots that occurs in both a source and a target domain</context>
</contexts>
<marker>Gouws, Bengio, Corrado, 2015</marker>
<rawString>Stephan Gouws, Yoshua Bengio, and Greg Corrado. 2015. Bilbowa: Fast bilingual distributed representations without word alignments. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>Multilingual distributed representations without word alignment.</title>
<date>2014</date>
<booktitle>In Proc. of ICLR.</booktitle>
<contexts>
<context position="15250" citStr="Hermann and Blunsom, 2014" startWordPosition="2352" endWordPosition="2355">le embeddings per word type. In particular, the number of senses per word type is automatically estimated. However, their method is limited to a single domain, and does not consider how the representations vary across domains. On the other hand, our proposed method learns a single representation for a particular word for each domain in which it occurs. Although in this paper we focus on the monolingual setting where source and target domains belong to the same language, the related setting where learning representations for words that are translational pairs across languages has been studied (Hermann and Blunsom, 2014; Klementiev et al., 2012; Gouws et al., 2015). Such representations are particularly useful for cross-lingual information retrieval (Duc et al., 2010). It will be an interesting future research direction to extend our proposed method to learn such cross-lingual word representations. 3 Cross-Domain Representation Learning We propose a method for learning word representations that are sensitive to the semantic variations of words across domains. We call this problem cross-domain word representation learning, and provide a definition in Section 3.1. Next, in Section 3.2, given a set of pivots th</context>
</contexts>
<marker>Hermann, Blunsom, 2014</marker>
<rawString>Karl Moritz Hermann and Phil Blunsom. 2014. Multilingual distributed representations without word alignment. In Proc. of ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>873--882</pages>
<contexts>
<context position="3043" citStr="Huang et al., 2012" startWordPosition="445" endWordPosition="448">f words only within a single domain. To overcome this problem and capture domain-specific semantic orientations of words, we propose a method that learns separate distributed representations for each domain in which a word occurs. Despite the successful applications of distributed word representation learning methods (Pennington et al., 2014; Collobert et al., 2011; Mikolov et al., 2013a) most existing approaches are limited to learning only a single representation for a given word (Reisinger and Mooney, 2010). Although there have been some work on learning multiple prototype representations (Huang et al., 2012; Neelakantan et al., 2014) for a word considering its multiple senses, such methods do not consider the semantics of the domain in which the word is being used. If we can learn separate representations for a word for each domain in which it occurs, we can use the learnt representations for domain adaptation tasks such as cross-domain sentiment classification (Bollegala et al., 2011b), cross-domain POS tagging (Schnabel and Sch¨utze, 2013), crossdomain dependency parsing (McClosky et al., 2010), and domain adaptation of relation extractors (Bollegala et al., 2013a; Bollegala et al., 2013b; Bol</context>
<context position="11984" citStr="Huang et al., 2012" startWordPosition="1833" endWordPosition="1836">update the word vectors such that the prediction error is minimized. Although NNLMs learn word representations as a by-product, the main focus on language modeling is to predict the next word in a sentence given the previous words, and not learning word representations that capture semantics. Moreover, training multi-layer neural networks using large text corpora is time consuming. To overcome those limitations, methods that specifically focus on learning word representations that model word co-occurrences in large corpora have been proposed (Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013; Huang et al., 2012; Pennington et al., 2014). Unlike the NNLM, these methods use all the words in a contextual window in the prediction task. Methods that use one or no hidden layers are proposed to improve the scalability of the learning algorithms. For example, the skip-gram model (Mikolov et al., 2013b) predicts the words c that appear in the local context of a word w, whereas the continuous bag-of-words model (CBOW) predicts a word w conditioned on all the words c that appear in w’s local context (Mikolov et al., 2013a). Methods that use global co-occurrences in the entire corpus to learn word representatio</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proc. of ACL, pages 873 – 882.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in nlp.</title>
<date>2007</date>
<booktitle>In ACL 2007,</booktitle>
<pages>264--271</pages>
<contexts>
<context position="3685" citStr="Jiang and Zhai, 2007" startWordPosition="549" endWordPosition="552">2014) for a word considering its multiple senses, such methods do not consider the semantics of the domain in which the word is being used. If we can learn separate representations for a word for each domain in which it occurs, we can use the learnt representations for domain adaptation tasks such as cross-domain sentiment classification (Bollegala et al., 2011b), cross-domain POS tagging (Schnabel and Sch¨utze, 2013), crossdomain dependency parsing (McClosky et al., 2010), and domain adaptation of relation extractors (Bollegala et al., 2013a; Bollegala et al., 2013b; Bollegala et al., 2011a; Jiang and Zhai, 2007a; Jiang and Zhai, 2007b). We introduce the cross-domain word represen730 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 730–740, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics tation learning task, where given two domains, (referred to as the source (S) and the target (T)) the goal is to learn two separate representations wS and wT for a word w respectively from the source and the target domain that capture domainspecific semantic variat</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007a. Instance weighting for domain adaptation in nlp. In ACL 2007, pages 264 – 271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>A two-stage approach to domain adaptation for statistical classifiers. In CIKM</title>
<date>2007</date>
<pages>401--410</pages>
<contexts>
<context position="3685" citStr="Jiang and Zhai, 2007" startWordPosition="549" endWordPosition="552">2014) for a word considering its multiple senses, such methods do not consider the semantics of the domain in which the word is being used. If we can learn separate representations for a word for each domain in which it occurs, we can use the learnt representations for domain adaptation tasks such as cross-domain sentiment classification (Bollegala et al., 2011b), cross-domain POS tagging (Schnabel and Sch¨utze, 2013), crossdomain dependency parsing (McClosky et al., 2010), and domain adaptation of relation extractors (Bollegala et al., 2013a; Bollegala et al., 2013b; Bollegala et al., 2011a; Jiang and Zhai, 2007a; Jiang and Zhai, 2007b). We introduce the cross-domain word represen730 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 730–740, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics tation learning task, where given two domains, (referred to as the source (S) and the target (T)) the goal is to learn two separate representations wS and wT for a word w respectively from the source and the target domain that capture domainspecific semantic variat</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007b. A two-stage approach to domain adaptation for statistical classifiers. In CIKM 2007, pages 401–410.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Klementiev</author>
<author>Ivan Titov</author>
<author>Binod Bhattarai</author>
</authors>
<title>Inducing crosslingual distributed representations of words.</title>
<date>2012</date>
<booktitle>In Proc. of COLING,</booktitle>
<pages>1459--1474</pages>
<contexts>
<context position="15275" citStr="Klementiev et al., 2012" startWordPosition="2356" endWordPosition="2359">. In particular, the number of senses per word type is automatically estimated. However, their method is limited to a single domain, and does not consider how the representations vary across domains. On the other hand, our proposed method learns a single representation for a particular word for each domain in which it occurs. Although in this paper we focus on the monolingual setting where source and target domains belong to the same language, the related setting where learning representations for words that are translational pairs across languages has been studied (Hermann and Blunsom, 2014; Klementiev et al., 2012; Gouws et al., 2015). Such representations are particularly useful for cross-lingual information retrieval (Duc et al., 2010). It will be an interesting future research direction to extend our proposed method to learn such cross-lingual word representations. 3 Cross-Domain Representation Learning We propose a method for learning word representations that are sensitive to the semantic variations of words across domains. We call this problem cross-domain word representation learning, and provide a definition in Section 3.1. Next, in Section 3.2, given a set of pivots that occurs in both a sourc</context>
</contexts>
<marker>Klementiev, Titov, Bhattarai, 2012</marker>
<rawString>Alexandre Klementiev, Ivan Titov, and Binod Bhattarai. 2012. Inducing crosslingual distributed representations of words. In Proc. of COLING, pages 1459 – 1474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Automatic domain adaptation for parsing.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL/HLT,</booktitle>
<pages>28--36</pages>
<contexts>
<context position="3542" citStr="McClosky et al., 2010" startWordPosition="526" endWordPosition="529">er and Mooney, 2010). Although there have been some work on learning multiple prototype representations (Huang et al., 2012; Neelakantan et al., 2014) for a word considering its multiple senses, such methods do not consider the semantics of the domain in which the word is being used. If we can learn separate representations for a word for each domain in which it occurs, we can use the learnt representations for domain adaptation tasks such as cross-domain sentiment classification (Bollegala et al., 2011b), cross-domain POS tagging (Schnabel and Sch¨utze, 2013), crossdomain dependency parsing (McClosky et al., 2010), and domain adaptation of relation extractors (Bollegala et al., 2013a; Bollegala et al., 2013b; Bollegala et al., 2011a; Jiang and Zhai, 2007a; Jiang and Zhai, 2007b). We introduce the cross-domain word represen730 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 730–740, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics tation learning task, where given two domains, (referred to as the source (S) and the target (T)) the goal is to learn two</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2010</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2010. Automatic domain adaptation for parsing. In Proc. of NAACL/HLT, pages 28 – 36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representation in vector space.</title>
<date>2013</date>
<publisher>CoRR.</publisher>
<contexts>
<context position="1751" citStr="Mikolov et al., 2013" startWordPosition="242" endWordPosition="245">domain adaptation using the learnt word representations. Our proposed method significantly outperforms competitive baselines including the state-of-theart domain-insensitive word representations, and reports best sentiment classification accuracies for all domain-pairs in a benchmark dataset. 1 Introduction Learning semantic representations for words is a fundamental task in NLP that is required in numerous higher-level NLP applications (Collobert et al., 2011). Distributed word representations have gained much popularity lately because of their accuracy as semantic representations for words (Mikolov et al., 2013a; Pennington et al., 2014). However, the meaning of a word often varies from one domain to another. For example, the phrase lightweight is often used in a positive sentiment in the portable electronics domain because a lightweight device is easier to carry around, which is a positive attribute for a portable electronic device. However, the same phrase has a negative sentiment assocition in the movie domain because movies that do not invoke deep thoughts in viewers are considered to be lightweight (Bollegala et al., 2014). However, existing word representation learning methods are agnostic to </context>
<context position="8107" citStr="Mikolov et al., 2013" startWordPosition="1245" endWordPosition="1248"> of free parameters in the model. Our contributions in this paper can be summarized as follows. • We propose a distributed word representation learning method that learns separate representations for a word for each domain in which it occurs. To the best of our knowledge, ours is the first-ever domain-sensitive distributed word representation learning method. • Given domain-specific word representations, we propose a method to learn a cross-domain sentiment classifier. Although word representation learning methods have been used for various related tasks in NLP such as similarity measurement (Mikolov et al., 2013c), POS tagging (Collobert et al., 2011), dependency parsing (Socher et al., 2011a), machine translation (Zou et al., 2013), sentiment classification (Socher et al., 2011b), and semantic role labeling (Roth and Woodsend, 2014), to the best of our knowledge, word representations methods have not yet been used for crossdomain sentiment classification. Experimental results for cross-domain sentiment classification on a benchmark dataset show that the word representations learnt using the proposed method statistically significantly outper731 form a state-of-the-art domain-insensitive word represen</context>
<context position="9905" citStr="Mikolov et al., 2013" startWordPosition="1514" endWordPosition="1517">ethods (Pan et al., 2010; Blitzer et al., 2006). 2 Related Work Representing the semantics of a word using some algebraic structure such as a vector (more generally a tensor) is a common first step in many NLP tasks (Turney and Pantel, 2010). By applying algebraic operations on the word representations, we can perform numerous tasks in NLP, such as composing representations for larger textual units beyond individual words such as phrases (Mitchell and Lapata, 2008). Moreover, word representations are found to be useful for measuring semantic similarity, and for solving proportional analogies (Mikolov et al., 2013c). Two main approaches for computing word representations can be identified in prior work (Baroni et al., 2014): counting-based and prediction-based. In counting-based approaches (Baroni and Lenci, 2010), a word w is represented by a vector w that contains other words that co-occur with w in a corpus. Numerous methods for selecting co-occurrence contexts such as proximity or dependency relations have been proposed (Turney and Pantel, 2010). Despite the numerous successful applications of co-occurrence countingbased distributional word representations, their high dimensionality and sparsity ar</context>
<context position="11935" citStr="Mikolov et al., 2013" startWordPosition="1825" endWordPosition="1828">xt word in a sequence, and uses backpropagation to update the word vectors such that the prediction error is minimized. Although NNLMs learn word representations as a by-product, the main focus on language modeling is to predict the next word in a sentence given the previous words, and not learning word representations that capture semantics. Moreover, training multi-layer neural networks using large text corpora is time consuming. To overcome those limitations, methods that specifically focus on learning word representations that model word co-occurrences in large corpora have been proposed (Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013; Huang et al., 2012; Pennington et al., 2014). Unlike the NNLM, these methods use all the words in a contextual window in the prediction task. Methods that use one or no hidden layers are proposed to improve the scalability of the learning algorithms. For example, the skip-gram model (Mikolov et al., 2013b) predicts the words c that appear in the local context of a word w, whereas the continuous bag-of-words model (CBOW) predicts a word w conditioned on all the words c that appear in w’s local context (Mikolov et al., 2013a). Methods that use global co-occurrences</context>
<context position="19364" citStr="Mikolov et al., 2013" startWordPosition="3038" endWordPosition="3041">om the context. We use CS, WS, CT , and WT to denote the sets of word representation vectors respectively for the source pivots, source non-pivots, target pivots, and target non-pivots. Let us denote the set of documents in the source and the target domains respectively by DS and DT . Following the bag-of-features model, we assume that a document D is represented by the set of pivots and non-pivots that occur in D (w E d and c E d). We consider the co-occurrences of a pivot c and a non-pivot w within a fixedsize contextual window in a document. Following prior work on representation learning (Mikolov et al., 2013a), in our experiments, we set the window size to 10 tokens, without crossing sentence boundaries. The notation (c, w) E d denotes the co-occurrence of a pivot c and a non-pivot w in a document d. We learn domain-specific word representations by maximizing the prediction accuracy of the nonpivots w that occur in the local context of a pivot c. The hinge loss, L(CS, WS), associated with predicting a non-pivot w in a source document d E DS that co-occurs with pivots c is given by: ( ) max 0, 1 − csTws + csTws) (1) Here, w∗S is the source domain representation of a non-pivot w∗ that does not occu</context>
<context position="24361" citStr="Mikolov et al., 2013" startWordPosition="3972" endWordPosition="3975">t or non-pivot) when the representations for all the other features are held fixed. In our experiments, the training converged in all cases with less than 100 epochs over the dataset. The rank-based predictive hinge loss (Eq. 1) is inspired by the prior work on word representation learning for a single domain (Collobert et al., 2011). However, unlike the multilayer neural network in Collobert et al. (2011), the proposed method uses a computationally efficient single layer to reduce the number of parameters that must be learnt, thereby scaling to large datasets. Similar to the skip-gram model (Mikolov et al., 2013a), the proposed method predicts occurrences of contexts (non-pivots) w within a fixed-size contextual window of a target word (pivot) c. Scoring the co-occurrences of two words c and w by the bilinear form given by the inner-product is similar to prior work on domain-insensitive word-representation learning (Mnih and Hinton, 2008; Mikolov et al., 2013a). However, unlike those methods that use the softmax function to convert inner-products to probabilities, we directly use the inner-products without any further transformations, thereby avoiding computationally expensive distribution normalizat</context>
</contexts>
<marker>Mikolov, Chen, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, and Jeffrey Dean. 2013a. Efficient estimation of word representation in vector space. CoRR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Gregory S Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Proc. of NIPS,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="1751" citStr="Mikolov et al., 2013" startWordPosition="242" endWordPosition="245">domain adaptation using the learnt word representations. Our proposed method significantly outperforms competitive baselines including the state-of-theart domain-insensitive word representations, and reports best sentiment classification accuracies for all domain-pairs in a benchmark dataset. 1 Introduction Learning semantic representations for words is a fundamental task in NLP that is required in numerous higher-level NLP applications (Collobert et al., 2011). Distributed word representations have gained much popularity lately because of their accuracy as semantic representations for words (Mikolov et al., 2013a; Pennington et al., 2014). However, the meaning of a word often varies from one domain to another. For example, the phrase lightweight is often used in a positive sentiment in the portable electronics domain because a lightweight device is easier to carry around, which is a positive attribute for a portable electronic device. However, the same phrase has a negative sentiment assocition in the movie domain because movies that do not invoke deep thoughts in viewers are considered to be lightweight (Bollegala et al., 2014). However, existing word representation learning methods are agnostic to </context>
<context position="8107" citStr="Mikolov et al., 2013" startWordPosition="1245" endWordPosition="1248"> of free parameters in the model. Our contributions in this paper can be summarized as follows. • We propose a distributed word representation learning method that learns separate representations for a word for each domain in which it occurs. To the best of our knowledge, ours is the first-ever domain-sensitive distributed word representation learning method. • Given domain-specific word representations, we propose a method to learn a cross-domain sentiment classifier. Although word representation learning methods have been used for various related tasks in NLP such as similarity measurement (Mikolov et al., 2013c), POS tagging (Collobert et al., 2011), dependency parsing (Socher et al., 2011a), machine translation (Zou et al., 2013), sentiment classification (Socher et al., 2011b), and semantic role labeling (Roth and Woodsend, 2014), to the best of our knowledge, word representations methods have not yet been used for crossdomain sentiment classification. Experimental results for cross-domain sentiment classification on a benchmark dataset show that the word representations learnt using the proposed method statistically significantly outper731 form a state-of-the-art domain-insensitive word represen</context>
<context position="9905" citStr="Mikolov et al., 2013" startWordPosition="1514" endWordPosition="1517">ethods (Pan et al., 2010; Blitzer et al., 2006). 2 Related Work Representing the semantics of a word using some algebraic structure such as a vector (more generally a tensor) is a common first step in many NLP tasks (Turney and Pantel, 2010). By applying algebraic operations on the word representations, we can perform numerous tasks in NLP, such as composing representations for larger textual units beyond individual words such as phrases (Mitchell and Lapata, 2008). Moreover, word representations are found to be useful for measuring semantic similarity, and for solving proportional analogies (Mikolov et al., 2013c). Two main approaches for computing word representations can be identified in prior work (Baroni et al., 2014): counting-based and prediction-based. In counting-based approaches (Baroni and Lenci, 2010), a word w is represented by a vector w that contains other words that co-occur with w in a corpus. Numerous methods for selecting co-occurrence contexts such as proximity or dependency relations have been proposed (Turney and Pantel, 2010). Despite the numerous successful applications of co-occurrence countingbased distributional word representations, their high dimensionality and sparsity ar</context>
<context position="11935" citStr="Mikolov et al., 2013" startWordPosition="1825" endWordPosition="1828">xt word in a sequence, and uses backpropagation to update the word vectors such that the prediction error is minimized. Although NNLMs learn word representations as a by-product, the main focus on language modeling is to predict the next word in a sentence given the previous words, and not learning word representations that capture semantics. Moreover, training multi-layer neural networks using large text corpora is time consuming. To overcome those limitations, methods that specifically focus on learning word representations that model word co-occurrences in large corpora have been proposed (Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013; Huang et al., 2012; Pennington et al., 2014). Unlike the NNLM, these methods use all the words in a contextual window in the prediction task. Methods that use one or no hidden layers are proposed to improve the scalability of the learning algorithms. For example, the skip-gram model (Mikolov et al., 2013b) predicts the words c that appear in the local context of a word w, whereas the continuous bag-of-words model (CBOW) predicts a word w conditioned on all the words c that appear in w’s local context (Mikolov et al., 2013a). Methods that use global co-occurrences</context>
<context position="19364" citStr="Mikolov et al., 2013" startWordPosition="3038" endWordPosition="3041">om the context. We use CS, WS, CT , and WT to denote the sets of word representation vectors respectively for the source pivots, source non-pivots, target pivots, and target non-pivots. Let us denote the set of documents in the source and the target domains respectively by DS and DT . Following the bag-of-features model, we assume that a document D is represented by the set of pivots and non-pivots that occur in D (w E d and c E d). We consider the co-occurrences of a pivot c and a non-pivot w within a fixedsize contextual window in a document. Following prior work on representation learning (Mikolov et al., 2013a), in our experiments, we set the window size to 10 tokens, without crossing sentence boundaries. The notation (c, w) E d denotes the co-occurrence of a pivot c and a non-pivot w in a document d. We learn domain-specific word representations by maximizing the prediction accuracy of the nonpivots w that occur in the local context of a pivot c. The hinge loss, L(CS, WS), associated with predicting a non-pivot w in a source document d E DS that co-occurs with pivots c is given by: ( ) max 0, 1 − csTws + csTws) (1) Here, w∗S is the source domain representation of a non-pivot w∗ that does not occu</context>
<context position="24361" citStr="Mikolov et al., 2013" startWordPosition="3972" endWordPosition="3975">t or non-pivot) when the representations for all the other features are held fixed. In our experiments, the training converged in all cases with less than 100 epochs over the dataset. The rank-based predictive hinge loss (Eq. 1) is inspired by the prior work on word representation learning for a single domain (Collobert et al., 2011). However, unlike the multilayer neural network in Collobert et al. (2011), the proposed method uses a computationally efficient single layer to reduce the number of parameters that must be learnt, thereby scaling to large datasets. Similar to the skip-gram model (Mikolov et al., 2013a), the proposed method predicts occurrences of contexts (non-pivots) w within a fixed-size contextual window of a target word (pivot) c. Scoring the co-occurrences of two words c and w by the bilinear form given by the inner-product is similar to prior work on domain-insensitive word-representation learning (Mnih and Hinton, 2008; Mikolov et al., 2013a). However, unlike those methods that use the softmax function to convert inner-products to probabilities, we directly use the inner-products without any further transformations, thereby avoiding computationally expensive distribution normalizat</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Proc. of NIPS, pages 3111 – 3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continous space word representations.</title>
<date>2013</date>
<booktitle>In NAACL’13,</booktitle>
<pages>746--751</pages>
<contexts>
<context position="1751" citStr="Mikolov et al., 2013" startWordPosition="242" endWordPosition="245">domain adaptation using the learnt word representations. Our proposed method significantly outperforms competitive baselines including the state-of-theart domain-insensitive word representations, and reports best sentiment classification accuracies for all domain-pairs in a benchmark dataset. 1 Introduction Learning semantic representations for words is a fundamental task in NLP that is required in numerous higher-level NLP applications (Collobert et al., 2011). Distributed word representations have gained much popularity lately because of their accuracy as semantic representations for words (Mikolov et al., 2013a; Pennington et al., 2014). However, the meaning of a word often varies from one domain to another. For example, the phrase lightweight is often used in a positive sentiment in the portable electronics domain because a lightweight device is easier to carry around, which is a positive attribute for a portable electronic device. However, the same phrase has a negative sentiment assocition in the movie domain because movies that do not invoke deep thoughts in viewers are considered to be lightweight (Bollegala et al., 2014). However, existing word representation learning methods are agnostic to </context>
<context position="8107" citStr="Mikolov et al., 2013" startWordPosition="1245" endWordPosition="1248"> of free parameters in the model. Our contributions in this paper can be summarized as follows. • We propose a distributed word representation learning method that learns separate representations for a word for each domain in which it occurs. To the best of our knowledge, ours is the first-ever domain-sensitive distributed word representation learning method. • Given domain-specific word representations, we propose a method to learn a cross-domain sentiment classifier. Although word representation learning methods have been used for various related tasks in NLP such as similarity measurement (Mikolov et al., 2013c), POS tagging (Collobert et al., 2011), dependency parsing (Socher et al., 2011a), machine translation (Zou et al., 2013), sentiment classification (Socher et al., 2011b), and semantic role labeling (Roth and Woodsend, 2014), to the best of our knowledge, word representations methods have not yet been used for crossdomain sentiment classification. Experimental results for cross-domain sentiment classification on a benchmark dataset show that the word representations learnt using the proposed method statistically significantly outper731 form a state-of-the-art domain-insensitive word represen</context>
<context position="9905" citStr="Mikolov et al., 2013" startWordPosition="1514" endWordPosition="1517">ethods (Pan et al., 2010; Blitzer et al., 2006). 2 Related Work Representing the semantics of a word using some algebraic structure such as a vector (more generally a tensor) is a common first step in many NLP tasks (Turney and Pantel, 2010). By applying algebraic operations on the word representations, we can perform numerous tasks in NLP, such as composing representations for larger textual units beyond individual words such as phrases (Mitchell and Lapata, 2008). Moreover, word representations are found to be useful for measuring semantic similarity, and for solving proportional analogies (Mikolov et al., 2013c). Two main approaches for computing word representations can be identified in prior work (Baroni et al., 2014): counting-based and prediction-based. In counting-based approaches (Baroni and Lenci, 2010), a word w is represented by a vector w that contains other words that co-occur with w in a corpus. Numerous methods for selecting co-occurrence contexts such as proximity or dependency relations have been proposed (Turney and Pantel, 2010). Despite the numerous successful applications of co-occurrence countingbased distributional word representations, their high dimensionality and sparsity ar</context>
<context position="11935" citStr="Mikolov et al., 2013" startWordPosition="1825" endWordPosition="1828">xt word in a sequence, and uses backpropagation to update the word vectors such that the prediction error is minimized. Although NNLMs learn word representations as a by-product, the main focus on language modeling is to predict the next word in a sentence given the previous words, and not learning word representations that capture semantics. Moreover, training multi-layer neural networks using large text corpora is time consuming. To overcome those limitations, methods that specifically focus on learning word representations that model word co-occurrences in large corpora have been proposed (Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013; Huang et al., 2012; Pennington et al., 2014). Unlike the NNLM, these methods use all the words in a contextual window in the prediction task. Methods that use one or no hidden layers are proposed to improve the scalability of the learning algorithms. For example, the skip-gram model (Mikolov et al., 2013b) predicts the words c that appear in the local context of a word w, whereas the continuous bag-of-words model (CBOW) predicts a word w conditioned on all the words c that appear in w’s local context (Mikolov et al., 2013a). Methods that use global co-occurrences</context>
<context position="19364" citStr="Mikolov et al., 2013" startWordPosition="3038" endWordPosition="3041">om the context. We use CS, WS, CT , and WT to denote the sets of word representation vectors respectively for the source pivots, source non-pivots, target pivots, and target non-pivots. Let us denote the set of documents in the source and the target domains respectively by DS and DT . Following the bag-of-features model, we assume that a document D is represented by the set of pivots and non-pivots that occur in D (w E d and c E d). We consider the co-occurrences of a pivot c and a non-pivot w within a fixedsize contextual window in a document. Following prior work on representation learning (Mikolov et al., 2013a), in our experiments, we set the window size to 10 tokens, without crossing sentence boundaries. The notation (c, w) E d denotes the co-occurrence of a pivot c and a non-pivot w in a document d. We learn domain-specific word representations by maximizing the prediction accuracy of the nonpivots w that occur in the local context of a pivot c. The hinge loss, L(CS, WS), associated with predicting a non-pivot w in a source document d E DS that co-occurs with pivots c is given by: ( ) max 0, 1 − csTws + csTws) (1) Here, w∗S is the source domain representation of a non-pivot w∗ that does not occu</context>
<context position="24361" citStr="Mikolov et al., 2013" startWordPosition="3972" endWordPosition="3975">t or non-pivot) when the representations for all the other features are held fixed. In our experiments, the training converged in all cases with less than 100 epochs over the dataset. The rank-based predictive hinge loss (Eq. 1) is inspired by the prior work on word representation learning for a single domain (Collobert et al., 2011). However, unlike the multilayer neural network in Collobert et al. (2011), the proposed method uses a computationally efficient single layer to reduce the number of parameters that must be learnt, thereby scaling to large datasets. Similar to the skip-gram model (Mikolov et al., 2013a), the proposed method predicts occurrences of contexts (non-pivots) w within a fixed-size contextual window of a target word (pivot) c. Scoring the co-occurrences of two words c and w by the bilinear form given by the inner-product is similar to prior work on domain-insensitive word-representation learning (Mnih and Hinton, 2008; Mikolov et al., 2013a). However, unlike those methods that use the softmax function to convert inner-products to probabilities, we directly use the inner-products without any further transformations, thereby avoiding computationally expensive distribution normalizat</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig. 2013c. Linguistic regularities in continous space word representations. In NAACL’13, pages 746 – 751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proc. of ACLHLT,</booktitle>
<pages>236--244</pages>
<contexts>
<context position="9754" citStr="Mitchell and Lapata, 2008" startWordPosition="1491" endWordPosition="1494">curacies on all domain-pairs, reporting statistically comparable results to the current state-of-the-art unsupervised cross-domain sentiment classification methods (Pan et al., 2010; Blitzer et al., 2006). 2 Related Work Representing the semantics of a word using some algebraic structure such as a vector (more generally a tensor) is a common first step in many NLP tasks (Turney and Pantel, 2010). By applying algebraic operations on the word representations, we can perform numerous tasks in NLP, such as composing representations for larger textual units beyond individual words such as phrases (Mitchell and Lapata, 2008). Moreover, word representations are found to be useful for measuring semantic similarity, and for solving proportional analogies (Mikolov et al., 2013c). Two main approaches for computing word representations can be identified in prior work (Baroni et al., 2014): counting-based and prediction-based. In counting-based approaches (Baroni and Lenci, 2010), a word w is represented by a vector w that contains other words that co-occur with w in a corpus. Numerous methods for selecting co-occurrence contexts such as proximity or dependency relations have been proposed (Turney and Pantel, 2010). Des</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proc. of ACLHLT, pages 236 – 244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model.</title>
<date>2008</date>
<booktitle>In Proc. of NIPS,</booktitle>
<pages>1081--1088</pages>
<contexts>
<context position="24693" citStr="Mnih and Hinton, 2008" startWordPosition="4023" endWordPosition="4026">011). However, unlike the multilayer neural network in Collobert et al. (2011), the proposed method uses a computationally efficient single layer to reduce the number of parameters that must be learnt, thereby scaling to large datasets. Similar to the skip-gram model (Mikolov et al., 2013a), the proposed method predicts occurrences of contexts (non-pivots) w within a fixed-size contextual window of a target word (pivot) c. Scoring the co-occurrences of two words c and w by the bilinear form given by the inner-product is similar to prior work on domain-insensitive word-representation learning (Mnih and Hinton, 2008; Mikolov et al., 2013a). However, unlike those methods that use the softmax function to convert inner-products to probabilities, we directly use the inner-products without any further transformations, thereby avoiding computationally expensive distribution normalizations over the entire vocabulary. 3.4 Pivot Selection Given two sets of documents DS, DT respectively for the source and the target domains, we use the following procedure to select pivots and non-pivots. First, we tokenize and lemmatize each document using the Stanford CoreNLP toolkit2. Next, we extract unigrams and bigrams as fea</context>
</contexts>
<marker>Mnih, Hinton, 2008</marker>
<rawString>Andriy Mnih and Geoffrey E. Hinton. 2008. A scalable hierarchical distributed language model. In Proc. of NIPS, pages 1081–1088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Koray Kavukcuoglu</author>
</authors>
<title>Learning word embeddings efficiently with noise-contrastive estimation.</title>
<date>2013</date>
<booktitle>In Proc. of NIPS.</booktitle>
<contexts>
<context position="11964" citStr="Mnih and Kavukcuoglu, 2013" startWordPosition="1829" endWordPosition="1832">and uses backpropagation to update the word vectors such that the prediction error is minimized. Although NNLMs learn word representations as a by-product, the main focus on language modeling is to predict the next word in a sentence given the previous words, and not learning word representations that capture semantics. Moreover, training multi-layer neural networks using large text corpora is time consuming. To overcome those limitations, methods that specifically focus on learning word representations that model word co-occurrences in large corpora have been proposed (Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013; Huang et al., 2012; Pennington et al., 2014). Unlike the NNLM, these methods use all the words in a contextual window in the prediction task. Methods that use one or no hidden layers are proposed to improve the scalability of the learning algorithms. For example, the skip-gram model (Mikolov et al., 2013b) predicts the words c that appear in the local context of a word w, whereas the continuous bag-of-words model (CBOW) predicts a word w conditioned on all the words c that appear in w’s local context (Mikolov et al., 2013a). Methods that use global co-occurrences in the entire corpus to lear</context>
<context position="20875" citStr="Mnih and Kavukcuoglu, 2013" startWordPosition="3312" endWordPosition="3315"> the set of all source domain non-pivots that do not occur in d as w∗. Specifically, we use the marginal distribution of non-pivots p(w), estimated from the corpus counts, as the sampling distribution. We raise p(w) to the 3/4-th power as proposed by Mikolov et al. (2013a), and normalize it to unit probability mass prior to sampling k non-pivots w∗ per each co-occurrence of (c, w) E d. Because nonoccurring non-pivots w∗ are randomly sampled, prior work on noise contrastive estimation has found that it requires more negative samples than positive samples to accurately learn a prediction model (Mnih and Kavukcuoglu, 2013). We experimentally found k = 5 to be an acceptable tradeoff between the prediction accuracy and the number of training instances. Likewise, the loss function L(CT , WT ) for predicting non-pivots using pivots in the target domain is given by: max (0, 1 − cT TwT + cTTwT) (2) Here, w∗ denotes target domain non-pivots that do not occur in d, and are randomly sampled from p(w) following the same procedure as in the source domain. The source and target loss functions given respectively by Eqs. 1 and 2 can be used on their own to independently learn source and target domain word representations. Ho</context>
</contexts>
<marker>Mnih, Kavukcuoglu, 2013</marker>
<rawString>Andriy Mnih and Koray Kavukcuoglu. 2013. Learning word embeddings efficiently with noise-contrastive estimation. In Proc. of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arvind Neelakantan</author>
<author>Jeevan Shankar</author>
<author>Alexandre Passos</author>
<author>Andrew McCallum</author>
</authors>
<title>Efficient nonparametric estimation of multiple embeddings per word in vector space.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>1059--1069</pages>
<contexts>
<context position="3070" citStr="Neelakantan et al., 2014" startWordPosition="449" endWordPosition="452">a single domain. To overcome this problem and capture domain-specific semantic orientations of words, we propose a method that learns separate distributed representations for each domain in which a word occurs. Despite the successful applications of distributed word representation learning methods (Pennington et al., 2014; Collobert et al., 2011; Mikolov et al., 2013a) most existing approaches are limited to learning only a single representation for a given word (Reisinger and Mooney, 2010). Although there have been some work on learning multiple prototype representations (Huang et al., 2012; Neelakantan et al., 2014) for a word considering its multiple senses, such methods do not consider the semantics of the domain in which the word is being used. If we can learn separate representations for a word for each domain in which it occurs, we can use the learnt representations for domain adaptation tasks such as cross-domain sentiment classification (Bollegala et al., 2011b), cross-domain POS tagging (Schnabel and Sch¨utze, 2013), crossdomain dependency parsing (McClosky et al., 2010), and domain adaptation of relation extractors (Bollegala et al., 2013a; Bollegala et al., 2013b; Bollegala et al., 2011a; Jiang</context>
<context position="14528" citStr="Neelakantan et al. (2014)" startWordPosition="2236" endWordPosition="2239">d representations. Faralli et al. (2012) proposed a domain-driven word sense disambiguation (WSD) method where they construct glossaries for several domain using a pattern-based bootstrapping technique. This work demonstrates the importance of considering the domain specificity of word senses. However, the focus of their work is not to learn representations for words or their senses in a domain, but to construct glossaries. It would be an interesting future research direction to explore the possibility of using such domain-specific glossaries for learning domain-specific word representations. Neelakantan et al. (2014) proposed a method that jointly performs WSD and word embedding learning, thereby learning multiple embeddings per word type. In particular, the number of senses per word type is automatically estimated. However, their method is limited to a single domain, and does not consider how the representations vary across domains. On the other hand, our proposed method learns a single representation for a particular word for each domain in which it occurs. Although in this paper we focus on the monolingual setting where source and target domains belong to the same language, the related setting where le</context>
</contexts>
<marker>Neelakantan, Shankar, Passos, McCallum, 2014</marker>
<rawString>Arvind Neelakantan, Jeevan Shankar, Alexandre Passos, and Andrew McCallum. 2014. Efficient nonparametric estimation of multiple embeddings per word in vector space. In Proc. of EMNLP, pages 1059–1069.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sinno Jialin Pan</author>
<author>Xiaochuan Ni</author>
<author>Jian-Tao Sun</author>
<author>Qiang Yang</author>
<author>Zheng Chen</author>
</authors>
<title>Cross-domain sentiment classification via spectral feature alignment.</title>
<date>2010</date>
<booktitle>In Proc. of WWW,</booktitle>
<pages>751--760</pages>
<contexts>
<context position="6433" citStr="Pan et al., 2010" startWordPosition="989" endWordPosition="992">an unsupervised cross-domain word representation learning method that jointly optimizes two criteria: (a) given a document d from the source or the target domain, we must accurately predict the non-pivots that occur in d using the pivots that occur in d, and (b) the source and target domain representations we learn for pivots must be similar. The main challenge in domain adaptation is feature mismatch, where the features that we use for training a classifier in the source domain do not necessarily occur in the target domain. Consequently, prior work on domain adaptation (Blitzer et al., 2006; Pan et al., 2010) learn lower-dimensional mappings from non-pivots to pivots, thereby overcoming the feature mismatch problem. Criteria (a) ensures that word representations for domain-specific non-pivots in each domain are related to the word representations for domain-independent pivots. This relationship enables us to discover pivots that are similar to target domain-specific non-pivots, thereby overcoming the feature mismatch problem. On the other hand, criteria (b) captures the prior knowledge that high-frequent words common to two domains often represent domain-independent semantics. For example, in sent</context>
<context position="9309" citStr="Pan et al., 2010" startWordPosition="1416" endWordPosition="1419"> word representation learning method (Pennington et al., 2014), and several competitive baselines. In particular, our proposed cross-domain word representation learning method is not specific to a particular task such as sentiment classification, and in principle, can be in applied to a wide-range of domain adaptation tasks. Despite this taskindependent nature of the proposed method, it achieves the best sentiment classification accuracies on all domain-pairs, reporting statistically comparable results to the current state-of-the-art unsupervised cross-domain sentiment classification methods (Pan et al., 2010; Blitzer et al., 2006). 2 Related Work Representing the semantics of a word using some algebraic structure such as a vector (more generally a tensor) is a common first step in many NLP tasks (Turney and Pantel, 2010). By applying algebraic operations on the word representations, we can perform numerous tasks in NLP, such as composing representations for larger textual units beyond individual words such as phrases (Mitchell and Lapata, 2008). Moreover, word representations are found to be useful for measuring semantic similarity, and for solving proportional analogies (Mikolov et al., 2013c). </context>
<context position="25888" citStr="Pan et al., 2010" startWordPosition="4213" endWordPosition="4216">ams and bigrams as features for representing a document. We remove features listed as stop words using a standard stop words list. Stop word removal increases the effective cooccurrence window size for a pivot. Finally, we remove features that occur less than 50 times in the entire set of documents. Several methods have been proposed in the prior work on domain adaptation for selecting a set of pivots from a given pair of domains such as the minimum frequency of occurrence of a feature in the two domains, mutual information (MI), and the entropy of the feature distribution over the documents (Pan et al., 2010). In our preliminary experiments, we discovered that a normalized version of the PMI (NPMI) (Bouma, 2009) to work consistently well for selecting pivots from different pairs of domains. NPMI between two features x and y is given by: NPMI(x, y) = logp(x, y) 1 G(x)p(y) − log(p(x, y)) (11) Here, the joint probability p(x, y), and the marginal probabilities p(x) and p(y) are estimated using the number of co-occurrences of x and y in the sentences in the documents. Eq. 11 normalizes both the upper and lower bounds of the PMI. 2http://nlp.stanford.edu/software/ corenlp.shtml ( aL 0 if cS&gt;(wS − w∗ S)</context>
<context position="34242" citStr="Pan et al., 2010" startWordPosition="5574" endWordPosition="5577">ompare the proposed method against two baselines (NA, InDomain), current state-of-the-art methods for unsupervised crossdomain sentiment classification (SFA, SCL), word representation learning (GloVe), and crossdomain similarity prediction (CS). The NA (noadapt) lower baseline uses a classifier trained on source labeled data to classify target test data without any domain adaptation. The InDomain baseline is trained using the labeled data for the target domain, and simulates the performance we can expect to obtain if target domain labeled data were available. Spectral Feature Alignment (SFA) (Pan et al., 2010) and Structural Correspondence Learning (SCL) (Blitzer et al., 2007) are the state-ofthe-art methods for cross-domain sentiment classification. However, those methods do not learn word representations. We use Global Vector Prediction (GloVe) (Pennington et al., 2014), the current state-of-theart word representation learning method, to learn word representations separately from the source and target domain unlabeled data, and use the learnt representations in Eq. 13 for sentiment classification. In contrast to the joint word representations learnt by the proposed method, GloVe simulates the lev</context>
</contexts>
<marker>Pan, Ni, Sun, Yang, Chen, 2010</marker>
<rawString>Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang Yang, and Zheng Chen. 2010. Cross-domain sentiment classification via spectral feature alignment. In Proc. of WWW, pages 751 – 760.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffery Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: global vectors for word representation.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1778" citStr="Pennington et al., 2014" startWordPosition="246" endWordPosition="249"> the learnt word representations. Our proposed method significantly outperforms competitive baselines including the state-of-theart domain-insensitive word representations, and reports best sentiment classification accuracies for all domain-pairs in a benchmark dataset. 1 Introduction Learning semantic representations for words is a fundamental task in NLP that is required in numerous higher-level NLP applications (Collobert et al., 2011). Distributed word representations have gained much popularity lately because of their accuracy as semantic representations for words (Mikolov et al., 2013a; Pennington et al., 2014). However, the meaning of a word often varies from one domain to another. For example, the phrase lightweight is often used in a positive sentiment in the portable electronics domain because a lightweight device is easier to carry around, which is a positive attribute for a portable electronic device. However, the same phrase has a negative sentiment assocition in the movie domain because movies that do not invoke deep thoughts in viewers are considered to be lightweight (Bollegala et al., 2014). However, existing word representation learning methods are agnostic to such domain-specific semant</context>
<context position="8755" citStr="Pennington et al., 2014" startWordPosition="1337" endWordPosition="1340">rt et al., 2011), dependency parsing (Socher et al., 2011a), machine translation (Zou et al., 2013), sentiment classification (Socher et al., 2011b), and semantic role labeling (Roth and Woodsend, 2014), to the best of our knowledge, word representations methods have not yet been used for crossdomain sentiment classification. Experimental results for cross-domain sentiment classification on a benchmark dataset show that the word representations learnt using the proposed method statistically significantly outper731 form a state-of-the-art domain-insensitive word representation learning method (Pennington et al., 2014), and several competitive baselines. In particular, our proposed cross-domain word representation learning method is not specific to a particular task such as sentiment classification, and in principle, can be in applied to a wide-range of domain adaptation tasks. Despite this taskindependent nature of the proposed method, it achieves the best sentiment classification accuracies on all domain-pairs, reporting statistically comparable results to the current state-of-the-art unsupervised cross-domain sentiment classification methods (Pan et al., 2010; Blitzer et al., 2006). 2 Related Work Repres</context>
<context position="12010" citStr="Pennington et al., 2014" startWordPosition="1837" endWordPosition="1841">ors such that the prediction error is minimized. Although NNLMs learn word representations as a by-product, the main focus on language modeling is to predict the next word in a sentence given the previous words, and not learning word representations that capture semantics. Moreover, training multi-layer neural networks using large text corpora is time consuming. To overcome those limitations, methods that specifically focus on learning word representations that model word co-occurrences in large corpora have been proposed (Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013; Huang et al., 2012; Pennington et al., 2014). Unlike the NNLM, these methods use all the words in a contextual window in the prediction task. Methods that use one or no hidden layers are proposed to improve the scalability of the learning algorithms. For example, the skip-gram model (Mikolov et al., 2013b) predicts the words c that appear in the local context of a word w, whereas the continuous bag-of-words model (CBOW) predicts a word w conditioned on all the words c that appear in w’s local context (Mikolov et al., 2013a). Methods that use global co-occurrences in the entire corpus to learn word representations have shown to outperfor</context>
<context position="34509" citStr="Pennington et al., 2014" startWordPosition="5612" endWordPosition="5616">t) lower baseline uses a classifier trained on source labeled data to classify target test data without any domain adaptation. The InDomain baseline is trained using the labeled data for the target domain, and simulates the performance we can expect to obtain if target domain labeled data were available. Spectral Feature Alignment (SFA) (Pan et al., 2010) and Structural Correspondence Learning (SCL) (Blitzer et al., 2007) are the state-ofthe-art methods for cross-domain sentiment classification. However, those methods do not learn word representations. We use Global Vector Prediction (GloVe) (Pennington et al., 2014), the current state-of-theart word representation learning method, to learn word representations separately from the source and target domain unlabeled data, and use the learnt representations in Eq. 13 for sentiment classification. In contrast to the joint word representations learnt by the proposed method, GloVe simulates the level of performance we would obtain by learning representations independently. CS denotes the cross-domain vector prediction method proposed by Bollegala et al. (2014). Although CS can be used to learn a vector-space translation matrix, it does not learn word represent</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffery Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: global vectors for word representation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Raymond J Mooney</author>
</authors>
<title>Multi-prototype vector-space models of word meaning.</title>
<date>2010</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>109--117</pages>
<contexts>
<context position="2940" citStr="Reisinger and Mooney, 2010" startWordPosition="429" endWordPosition="432">tion learning methods are agnostic to such domain-specific semantic variations of words, and capture semantics of words only within a single domain. To overcome this problem and capture domain-specific semantic orientations of words, we propose a method that learns separate distributed representations for each domain in which a word occurs. Despite the successful applications of distributed word representation learning methods (Pennington et al., 2014; Collobert et al., 2011; Mikolov et al., 2013a) most existing approaches are limited to learning only a single representation for a given word (Reisinger and Mooney, 2010). Although there have been some work on learning multiple prototype representations (Huang et al., 2012; Neelakantan et al., 2014) for a word considering its multiple senses, such methods do not consider the semantics of the domain in which the word is being used. If we can learn separate representations for a word for each domain in which it occurs, we can use the learnt representations for domain adaptation tasks such as cross-domain sentiment classification (Bollegala et al., 2011b), cross-domain POS tagging (Schnabel and Sch¨utze, 2013), crossdomain dependency parsing (McClosky et al., 201</context>
</contexts>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>Joseph Reisinger and Raymond J. Mooney. 2010. Multi-prototype vector-space models of word meaning. In Proc. of HLT-NAACL, pages 109 – 117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Roth</author>
<author>Kristian Woodsend</author>
</authors>
<title>Composition of word representations improves semantic role labelling.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>407--413</pages>
<contexts>
<context position="8333" citStr="Roth and Woodsend, 2014" startWordPosition="1280" endWordPosition="1283"> in which it occurs. To the best of our knowledge, ours is the first-ever domain-sensitive distributed word representation learning method. • Given domain-specific word representations, we propose a method to learn a cross-domain sentiment classifier. Although word representation learning methods have been used for various related tasks in NLP such as similarity measurement (Mikolov et al., 2013c), POS tagging (Collobert et al., 2011), dependency parsing (Socher et al., 2011a), machine translation (Zou et al., 2013), sentiment classification (Socher et al., 2011b), and semantic role labeling (Roth and Woodsend, 2014), to the best of our knowledge, word representations methods have not yet been used for crossdomain sentiment classification. Experimental results for cross-domain sentiment classification on a benchmark dataset show that the word representations learnt using the proposed method statistically significantly outper731 form a state-of-the-art domain-insensitive word representation learning method (Pennington et al., 2014), and several competitive baselines. In particular, our proposed cross-domain word representation learning method is not specific to a particular task such as sentiment classific</context>
</contexts>
<marker>Roth, Woodsend, 2014</marker>
<rawString>Michael Roth and Kristian Woodsend. 2014. Composition of word representations improves semantic role labelling. In Proc. of EMNLP, pages 407–413.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tobias Schnabel</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Towards robust cross-domain domain adaptation for part-ofspeech tagging.</title>
<date>2013</date>
<booktitle>In Proc. of IJCNLP,</booktitle>
<pages>198--206</pages>
<marker>Schnabel, Sch¨utze, 2013</marker>
<rawString>Tobias Schnabel and Hinrich Sch¨utze. 2013. Towards robust cross-domain domain adaptation for part-ofspeech tagging. In Proc. of IJCNLP, pages 198 – 206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff Chiung-Yu Lin</author>
<author>Andrew Ng</author>
<author>Chris Manning</author>
</authors>
<title>Parsing natural scenes and natural language with recursive neural networks.</title>
<date>2011</date>
<booktitle>In ICML’11.</booktitle>
<contexts>
<context position="8188" citStr="Socher et al., 2011" startWordPosition="1258" endWordPosition="1261">d as follows. • We propose a distributed word representation learning method that learns separate representations for a word for each domain in which it occurs. To the best of our knowledge, ours is the first-ever domain-sensitive distributed word representation learning method. • Given domain-specific word representations, we propose a method to learn a cross-domain sentiment classifier. Although word representation learning methods have been used for various related tasks in NLP such as similarity measurement (Mikolov et al., 2013c), POS tagging (Collobert et al., 2011), dependency parsing (Socher et al., 2011a), machine translation (Zou et al., 2013), sentiment classification (Socher et al., 2011b), and semantic role labeling (Roth and Woodsend, 2014), to the best of our knowledge, word representations methods have not yet been used for crossdomain sentiment classification. Experimental results for cross-domain sentiment classification on a benchmark dataset show that the word representations learnt using the proposed method statistically significantly outper731 form a state-of-the-art domain-insensitive word representation learning method (Pennington et al., 2014), and several competitive baselin</context>
</contexts>
<marker>Socher, Lin, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Cliff Chiung-Yu Lin, Andrew Ng, and Chris Manning. 2011a. Parsing natural scenes and natural language with recursive neural networks. In ICML’11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>151--161</pages>
<contexts>
<context position="8188" citStr="Socher et al., 2011" startWordPosition="1258" endWordPosition="1261">d as follows. • We propose a distributed word representation learning method that learns separate representations for a word for each domain in which it occurs. To the best of our knowledge, ours is the first-ever domain-sensitive distributed word representation learning method. • Given domain-specific word representations, we propose a method to learn a cross-domain sentiment classifier. Although word representation learning methods have been used for various related tasks in NLP such as similarity measurement (Mikolov et al., 2013c), POS tagging (Collobert et al., 2011), dependency parsing (Socher et al., 2011a), machine translation (Zou et al., 2013), sentiment classification (Socher et al., 2011b), and semantic role labeling (Roth and Woodsend, 2014), to the best of our knowledge, word representations methods have not yet been used for crossdomain sentiment classification. Experimental results for cross-domain sentiment classification on a benchmark dataset show that the word representations learnt using the proposed method statistically significantly outper731 form a state-of-the-art domain-insensitive word representation learning method (Pennington et al., 2014), and several competitive baselin</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011b. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proc. of EMNLP, pages 151–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Aritificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="9526" citStr="Turney and Pantel, 2010" startWordPosition="1455" endWordPosition="1458">ask such as sentiment classification, and in principle, can be in applied to a wide-range of domain adaptation tasks. Despite this taskindependent nature of the proposed method, it achieves the best sentiment classification accuracies on all domain-pairs, reporting statistically comparable results to the current state-of-the-art unsupervised cross-domain sentiment classification methods (Pan et al., 2010; Blitzer et al., 2006). 2 Related Work Representing the semantics of a word using some algebraic structure such as a vector (more generally a tensor) is a common first step in many NLP tasks (Turney and Pantel, 2010). By applying algebraic operations on the word representations, we can perform numerous tasks in NLP, such as composing representations for larger textual units beyond individual words such as phrases (Mitchell and Lapata, 2008). Moreover, word representations are found to be useful for measuring semantic similarity, and for solving proportional analogies (Mikolov et al., 2013c). Two main approaches for computing word representations can be identified in prior work (Baroni et al., 2014): counting-based and prediction-based. In counting-based approaches (Baroni and Lenci, 2010), a word w is rep</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Aritificial Intelligence Research, 37:141 – 188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Y Zou</author>
<author>Richard Socher</author>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>Bilingual word embeddings for phrase-based machine translation.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP’13,</booktitle>
<pages>1393--1398</pages>
<contexts>
<context position="8230" citStr="Zou et al., 2013" startWordPosition="1265" endWordPosition="1268">d representation learning method that learns separate representations for a word for each domain in which it occurs. To the best of our knowledge, ours is the first-ever domain-sensitive distributed word representation learning method. • Given domain-specific word representations, we propose a method to learn a cross-domain sentiment classifier. Although word representation learning methods have been used for various related tasks in NLP such as similarity measurement (Mikolov et al., 2013c), POS tagging (Collobert et al., 2011), dependency parsing (Socher et al., 2011a), machine translation (Zou et al., 2013), sentiment classification (Socher et al., 2011b), and semantic role labeling (Roth and Woodsend, 2014), to the best of our knowledge, word representations methods have not yet been used for crossdomain sentiment classification. Experimental results for cross-domain sentiment classification on a benchmark dataset show that the word representations learnt using the proposed method statistically significantly outper731 form a state-of-the-art domain-insensitive word representation learning method (Pennington et al., 2014), and several competitive baselines. In particular, our proposed cross-doma</context>
</contexts>
<marker>Zou, Socher, Cer, Manning, 2013</marker>
<rawString>Will Y. Zou, Richard Socher, Daniel Cer, and Christopher D. Manning. 2013. Bilingual word embeddings for phrase-based machine translation. In Proc. of EMNLP’13, pages 1393 – 1398.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>