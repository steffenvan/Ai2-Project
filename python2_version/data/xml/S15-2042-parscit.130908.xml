<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000997">
<title confidence="0.995227">
ECNU: Using Multiple Sources of CQA-based Information for Answer
Selection and YES/NO Response Inference
</title>
<author confidence="0.999894">
Liang Yi, Jianxiang Wang, Man Lan*
</author>
<affiliation confidence="0.985546666666667">
Shanghai Key Laboratory of Multidimensional Information Processing
Department of Computer Science and Technology,
East China Normal University, Shanghai 200241, China
</affiliation>
<email confidence="0.992714">
151121201035, 511412010621@ecnu.cn; mlan@cs.ecnu.edu.cn*
</email>
<sectionHeader confidence="0.998571" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99992094117647">
This paper reports our submissions to com-
munity question answering task in SemEval-
2015, which consists of two subtasks: (1) pre-
dict the quality of answers to given question
as good, bad, or potentially relevant and (2)
identify yes, no or unsure response to a given
YES/NO question based on the good answer-
s identified by subtask 1. For both subtasks,
we adopted supervised classification method
and examined the effects of heterogeneous
features generated from community question
answering data, such as bag-of-words, string
matching, semantic similarity, answerer in-
formation, answer-specific features, question-
specific features, etc. Our submitted primary
systems ranked the forth and the second for
the two subtasks of English data respectively.
</bodyText>
<sectionHeader confidence="0.999456" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9929555">
Community Question Answering (CQA) system-
s such as Yahoo!Answers rely on users to provide
answers (i.e., user generated content) for questions
posted. Generally such systems are quite open and
the answers provided by users are not always of high
quality. For example, a bad answer may present ir-
relevant opinions or issues, contain only URL links
without direct answer, or even be written informally.
Therefore, in order to achieve high-quality user ex-
perience and maintain high levels of adherence, it is
critical to present high-quality answers and provide
direct responses for users.
The CQA task in SemEval-2015 (M`arquez et al.,
2015) provides such a universal platform for re-
</bodyText>
<listItem confidence="0.869423333333333">
searchers to make a comparison between differen-
t approaches. This task consists of two subtasks:
(1) subtask A is to classify the quality of answers
</listItem>
<bodyText confidence="0.940528483870968">
as good, potential or bad, which also refers to the
task of answer quality prediction (Jeon et al., 2006;
Agichtein et al., 2008); (2) subtask B is to infer the
global answer of a YES/NO question to be yes, no
or unsure based on individual good answers.
Most of the previous research on answer quality
prediction has focused on extracting various features
to employ ranking or classification methods (Sur-
deanu et al., 2011; Shah and Pomerantz, 2010), such
as textual features (Agichtein et al., 2008; Blooma
et al., 2010) including the length of an answer, over-
lapped words between a question-answer (QA) pair,
etc. Another kind of widely used feature is extracted
from answerer profile information (Shah and Pomer-
antz, 2010), such as the number of best answers,
the achieved levels and the earned points. However,
such information is not often available in real world.
Moreover, a recent study (Toba et al., 2014) has tak-
en question type into consideration to make the an-
swers quality prediction.
In this paper, we built two classification systems
for the two tasks respectively. For Task A, we ex-
tracted six types of features from multiple sources of
CQA-based information to predict the answer qual-
ity, such as answer-, question-, answerer-specific
information, surface word similarity and semantic
similarity between question-answer pair, ect. For
Task B, the global answer of a YES/NO question is
summarized just from the individual good answer-
s identified by Task A. Specifically, we first built
a classifier to predict Yes/No/Unsure labels for each
</bodyText>
<page confidence="0.974361">
236
</page>
<bodyText confidence="0.882611">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 236–241,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
predicted good answer, then we performed a major-
ity voting to summarize the global answer for each
question.
The rest of this paper is structured as follows.
Section 2 describes our systems, including features,
algorithms, etc. Section 3 shows experiments on
training data and results on test data. Finally, con-
clusions and future work are given in Section 4.
</bodyText>
<sectionHeader confidence="0.948659" genericHeader="method">
2 Our Systems
</sectionHeader>
<bodyText confidence="0.97924675">
For both tasks we adopted supervised classification
methods and extracted various features from mul-
tiple sources to predict answer quality and infer
YES/NO response.
</bodyText>
<subsectionHeader confidence="0.995464">
2.1 Data Extraction
</subsectionHeader>
<bodyText confidence="0.99999036">
English data is extracted from Qatar Living Forum1
and provided with XML-format. Each data file con-
sists of a list of question tags, where each question
is followed by a list of answer tags to this question.
Each question or answer has a subject, a body, and
a list of attributes from which we can extract signifi-
cant features. For example, a question has attributes
of question category (overall 27 categories, e.g., Ed-
ucation, Cars, etc.), identifier of asker, question type
(GENERAL or YES/NO) and an answer also has an-
swerer identifier.
To obtain complete contents of a question or an
answer, we merged the contents extracted from sub-
ject and body. Exceptionally, if subject is substring
of body or subject of an answer starts with “RE:”,
we just extracted the contents from body.
Moreover, to reduce the influence of Not English
answers to the subsequent classification, we filtered
out the Not English answers from data. To discov-
er such answers we found out unusual words for
each answer by comparing word set of this answer
with an English vocabulary with 235,887 words
from NLTK2 words corpus, if the number of unusual
words is over 10 and the ratio over answer length is
above 60% we then regarded it as Not English.
</bodyText>
<subsectionHeader confidence="0.999914">
2.2 Pre-processing
</subsectionHeader>
<bodyText confidence="0.933562">
After data extraction we performed the following
preprocessing operations. Firstly, HTML character
</bodyText>
<footnote confidence="0.992854">
lhttp://www.qatarliving.com/forum
2http://www.nltk.org/
</footnote>
<bodyText confidence="0.999607090909091">
encodings are substituted by the actual characters
(e.g., “&amp;” is converted into whitespace). Then
HTML tags, URLs, emoticons, ending signatures
and repeating punctuation are removed from data.
After that, we collected a slang list from Internet and
replaced the informal words with formal words (e.g.,
“u r” is converted into “you are”). For the processed
data, we performed tokenization and POS tagging
using Penn Treebank tokenizer and POS tagger in
NLTK. The words are lemmatized using WordNet-
based lemmatizer implemented in NLTK.
</bodyText>
<subsectionHeader confidence="0.999725">
2.3 Features of Task A
</subsectionHeader>
<bodyText confidence="0.999965285714286">
We extracted six types of features from multiple
sources of CQA-based information, i.e., bag-of-
words (BoW) and answer-specific features (AS)
from answer, string matching (SM) and semantic
similarity (SS) from QA pair, answerer informa-
tion features (AI) from answerer profile, question-
specific features (QS) from question.
</bodyText>
<subsectionHeader confidence="0.725192">
2.3.1 Bag-of-Words for Answer (BoW)
</subsectionHeader>
<bodyText confidence="0.9999946">
We collected words from training and develop-
ment answer set and adopted binary BoW represen-
tation. To reduce the problem of data sparse, we
selected the words with frequency higher than four,
resulting in 5, 730 words.
</bodyText>
<subsectionHeader confidence="0.783649">
2.3.2 Answer-Specific Features (AS)
</subsectionHeader>
<bodyText confidence="0.999901842105263">
For each question, we extracted three answer-
specific features. The first is answer length, which
is computed at three levels, i.e., word, sentence and
paragraph. We used Li normalization on the glob-
al answer set. To gain insight on the effect of an-
swer length for each individual question, we also de-
signed a length ratio feature to record the ratio of the
length of each answer to the maximal answer length
for the same question.
A good answer is generally supposed to an-
swer a question explicitly instead of starting a new
question or suggesting other consulting approach-
es. Therefore, the second binary feature is to rep-
resent whether an answer contains a question mark
or not. In addition, we manually collected eight
words and phrases from training set, which contain-
s the meaning of suggestion (i.e., “suggest”, “rec-
ommend”, “advise”, “try”, “call”, “you may”, “may
be”, “you could”). Thus the third binary feature is to
</bodyText>
<page confidence="0.977344">
237
</page>
<bodyText confidence="0.995443">
represent if there is at least one of above suggestion
words in a given answer.
</bodyText>
<subsectionHeader confidence="0.802749">
2.3.3 String Matching between QA (SM)
</subsectionHeader>
<bodyText confidence="0.999947970588235">
The above two types of features are both extracted
from answer regardless of the question asked. How-
ever, the string matching features are to consider the
overlapped words from a given QA pair.
Word: This feature group records the proportions
of co-occurred words between a QA pair, which are
calculatedusing six measures: |AnB|/|A|, |AnB|/|B|,
|A−B|/|A|, |B−A|/|B|, |AnB|/|AUB|, 2*|AnB|/(|A|+
|B|), where |A |and |B |denote the number of non-
repeated words of question A and answer B. How-
ever, the same word appearing in different contex-
t could vary in word forms and normalizing words
may obtain more accurate overlapped proportions,
so we computed each measure at three word forms:
original, lemmatized and stem form.
POS: This POS feature is similar to the above
word feature. We use three measures: |A n B|/|A|,
|A n B|/|B|, |A n B|/|A U B |to compute overlapped
proportion of POS tags for nouns, verbs, adjectives
and adverbs.
n-gram: Unlike the above two features measuring
the overlap of single words or POS without consid-
ering multiple continuous words, the n-gram feature
is to calculate the Jaccard similarity of overlapped
n-grams between each QA pair. The n-grams are
obtained at word level (n = 2, 3) and character level
(n = 2, 3, 4). In addition, the n-grams at word lev-
el are obtained from original form and lemmatized
form respectively.
Longest Common Sequence (LCS): The LCS
feature is to measure the LCS similarity for a QA
pair on the original and lemmatized form. It is cal-
culated as the length of the LCS between each QA
pair at word level divided by the length of question.
</bodyText>
<subsubsectionHeader confidence="0.5812">
2.3.4 Semantic Similarity between QA (SS)
</subsubsectionHeader>
<bodyText confidence="0.999895648648649">
The previous string matching feature only consid-
ers the overlapped surface words or substrings in a
QA pair and it may not capture the semantic infor-
mation between a QA pair. Therefore, we presented
the following semantic similarity features, which are
borrowed from previous work.
Determining semantic similarity of sentences
commonly uses measures of semantic similarity be-
tween individual words. We used knowledge-based
and corpus-based word similarity features. The
knowledge-based similarity estimation relies on a
semantic network of words such as WordNet. In
this work, we employed four WordNet-based word
similarity metrics: Path (Banea et al., 2012), WUP
(Wu and Palmer, 1994), LCH (Leacock and Chodor-
ow, 1998) and Lin (Lin, 1998) similarity. Following
(Zhu and Man, 2013), the best alignment strategy
and the aggregation strategy are employed to propa-
gate the word similarity to the text similarity. More-
over, Latent Semantic analysis (LSA) (Landauer et
al., 1997) is a widely used corpus-based measure
when evaluating textual similarity. We used the vec-
tor space sentence similarity proposed by (ˇSari´c et
al., 2012), which represents each sentence as a s-
ingle distributional vector by summing up the LSA
vector of each word in the sentence. In this work,
two corpora are used to compute the LSA vector of
words: New York Times Annotated Corpus (NYT)
and Wikipedia.
Besides, following (Zhao et al., 2014), we adopt-
ed the weighted textual matrix factorization (WTM-
F) (Guo and Diab, 2012) to model the semantics
representations of sentences and then employed the
new representations to calculate the semantic simi-
larity between QA pairs using Cosine, Manhattan,
Euclidean, Person, Spearmanr, Kendalltau measures
respectively.
</bodyText>
<subsubsectionHeader confidence="0.399957">
2.3.5 Answerer Information (AI)
</subsubsectionHeader>
<bodyText confidence="0.999867588235294">
Previous work (Zhou et al., 2012) showed that in-
formation about answerer has great impact on an-
swer ranking in CQA. Inspired by this work, we
designed two answerer-specific features to represent
answerer level and answerer expert domain informa-
tion. To calculate the answerer level feature, we used
the number of answers and the percentage of good
answers for each answerer. For expert domain fea-
ture, we employed the question categories where the
answerer is an expert. Specifically, for each answer-
er, let G be the number of good answers the answer-
er responses and GZ be the number of good answers
to the i-th question category (i&lt;=27). Then we used
GZ/G to measure the answerer’s expert domain. Be-
sides, for each of the 27 question categories (e.g., E-
ducation, Cars), we recorded the maximal value MZ
over all values of GZ from each answerer and then
</bodyText>
<page confidence="0.956816">
238
</page>
<table confidence="0.709805">
calculated the GZ/MZ score to measure expert lev- Algorithm macro-F1(Task A) macro-F1(Task B)
el of an answerer in current domain among all an- SVM (linear) 54.25 58.60
swerers. Totally, we adopted 54 features to indicate SVM (rbf) 29.44 25.05
expert domain for each answerer. GB 49.70 39.05
RF 45.40 27.14
2.3.6 Question-Specific Features (QS)
</table>
<bodyText confidence="0.996229875">
Since the domain of questions may also affect the
performance of answer selection, we considered to
use 27 binary features to indicate the question cate-
gory. In addition, we manually collected 9 question
words (i.e., where, what, when, which, who, whom,
whose, why and how) and used 9 binary features to
indicate if one of these question words occurs in the
question.
</bodyText>
<subsectionHeader confidence="0.995346">
2.4 Features of Task B
</subsectionHeader>
<bodyText confidence="0.99996548">
To address task B, we performed two steps. Firstly,
we extracted features from good answers identified
from task A and trained a classifier to predict the Yes,
No or Unsure label for each good answer. Second-
ly, for each given YES/NO question, we counted the
answer labels of Yes, No or Unsure and used major-
ity voting to obtain the global answer.
We used three types of features for this task,
which are all extracted from answer: (1) Bag-of-
Words from answer (BoW), the same as in Task A;
(2) Semantic Word2Vec (W2V): this feature indi-
cates a vector representation of answer. We used
word2vec tool3 to get word vectors with dimension
d = 300 and then summed up all the word vectors
to obtain the answer vector. (3) Yes/No Word List
(YN): we manually collected 50 affirmative words
and 45 negation words by starting from several seed
words (e.g., “yes”, “sure”, “definitely”, “no”, “sel-
dom”, “never”, etc) and then expanding the list using
snowball with the aid of WordNet synset. Besides,
several phrases are manually added in the list (e.g.,
“beyond a doubt”, “beyond question”, “not at all”,
“only just”, etc). We utilized 2 binary features to
indicate whether an answer contains at least one of
these affirmative and negation words or not.
</bodyText>
<subsectionHeader confidence="0.988896">
2.5 Classification Algorithms
</subsectionHeader>
<bodyText confidence="0.998484666666667">
We explored several widely-used supervised clas-
sification algorithms including Support Vector Ma-
chine (SVM), Random Forest (RF), and Gradien-
</bodyText>
<footnote confidence="0.746076">
3https://code.google.com/p/word2vec/
</footnote>
<tableCaption confidence="0.997971">
Table 1: Results on training data for different algorithms.
</tableCaption>
<bodyText confidence="0.5624965">
t Boosting(GB), which are implemented in scikit-
learn toolkit (Pedregosa et al., 2011).
</bodyText>
<subsectionHeader confidence="0.986924">
2.6 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.9998872">
The official evaluation measures for both tasks is
macro-averaged Fl. For Task A the official score
is calculated on three labels: Good, Bad, Poten-
tial (where Bad includes Dialogue, Not English and
Other).
</bodyText>
<sectionHeader confidence="0.998835" genericHeader="evaluation">
3 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.996712">
3.1 English Data Set
</subsectionHeader>
<bodyText confidence="0.999972090909091">
The English training and development set contain
2,900 questions with 18,186 answers and the test
set contains 329 questions with 1, 976 answers, con-
sisting of around 50% good, 40% bad and 10% po-
tential answers. The YES/NO questions are about
10% of all the questions, which indicates that the
data for Task B is much less than Task A.
For both tasks we used training set with 2,600
questions to build classifiers and validated the per-
formance on development set with 300 questions for
algorithms comparison and features choosing.
</bodyText>
<subsectionHeader confidence="0.999641">
3.2 Algorithm Choosing Experiments
</subsectionHeader>
<bodyText confidence="0.999976166666667">
We performed algorithm choosing experiments us-
ing all designed features. All the parameters of algo-
rithms are set to be default values from scikit-learn
(Pedregosa et al., 2011). Table 1 lists the prelimi-
nary algorithm comparison experimental results. We
found SVM with linear kernel outperforms other al-
gorithm choices for both tasks. Moreover, we tuned
the trade-off parameter c of SVM and when set c to
0.8 we obtained a better score 54.78% and 58.82%
for Task A and B respectively. Therefore, in the fol-
lowing experiments on training and test data, we set
the algorithm to SVM with linear kernel.
</bodyText>
<subsectionHeader confidence="0.999253">
3.3 Feature Comparison Experiments
</subsectionHeader>
<bodyText confidence="0.997621">
We performed a series of experiments for both tasks
to explore the effects of various feature types using
</bodyText>
<page confidence="0.995436">
239
</page>
<bodyText confidence="0.998438428571429">
SVM (linear). In Task B we always chose the pre-
dicted good answers from the system with the best
macro-F1 in Task A. Table 2 shows the results of
different feature combinations where for each time
we selected and added one best feature type. From
this table we found the following interesting obser-
vations.
</bodyText>
<table confidence="0.998875181818182">
Task A BoW AS SM SS AI QS macro-Fl(%)
+ 48.91
+ + 49.73(+0.82)
+ + + 51.85(+2.12)
+ + + + 52.03(+0.18)
+ + + + + 53.22(+1.19)
+ + + + + + 54.25(+1.03)
Task B BoW W2V YN macro-Fl(%)
+ 47.82
+ + 49.54(+1.72)
+ + + 58.60(+9.06)
</table>
<tableCaption confidence="0.836815333333333">
Table 2: Results of feature combinations for Task A and
B, the numbers in the bracket are the performance incre-
ments compared with previous result.
</tableCaption>
<bodyText confidence="0.999947818181818">
First, for both tasks the most effective feature type
is bag-of-words from answer and this feature alone
achieves 48.91% for Task A and 47.82% for Task B,
which both outperforms the baseline system provid-
ed by organizers respectively. The baseline of Task
A which predicts all answers as good just achieves
22.36% and for Task B it achieves 25.0% which pre-
dict all answers as yes. Moreover, in Task A the
performance of other five feature types alone is far
lower than bag-of-words, ranging from 23% to 38%
approximately.
Second, for Task A, when combining all the fea-
tures together the system achieves the best perfor-
mance, which indicates that all types of features
make contribution more or less. Specially, among
the six types of features, answerer information and
semantic similarity between QA pairs make more
contribution than others. This indicates that answer-
er profile information is important, which is consis-
tent with the findings in (Zhou et al., 2012). Be-
sides, the semantic similarity captures deep relation-
ship between Q-A pair than the surface word, which
is helpful for performance improvement. In Task
B, we also observed the similar findings, i.e., the
system using all types of features achieves the best
performance. Moreover, the YES/NO word list fea-
ture makes great contribution to the performance im-
provement. This is consistent with our expectation.
Besides, although in this work the word vector fea-
ture improves the performance, this improvements is
not as much as our expectation. The possible reason
may be the simple way of using the vector by only
summing up.
</bodyText>
<subsectionHeader confidence="0.950074">
3.4 Results on Test Data
</subsectionHeader>
<bodyText confidence="0.995628">
According to the above experiments on training da-
ta, we configured one primary and two contrastive
systems for both tasks. The only difference between
these systems lies in the features and parameters in
SVM. Table 3 lists the configuration of three sys-
tems and their corresponding results on test data.
Besides, we also list the top three results officially
released by organizers.
</bodyText>
<table confidence="0.998846777777778">
Systems Task A Task B
features para. result features para. result
primary all c=0.8 53.47(9) all c=0.8 55.8(3)
contrastive1 all c=1.0 52.55(10) all c=1.0 50.6(4)
contrastive2 all-SS c=0.8 52.27(11) all-W2V c=0.8 53.9(6)
Top Systems Task A Result Task B Result
rank 1st 57.19 63.7
rank 2nd 56.41 55.8
rank 3rd 53.74 53.6
</table>
<tableCaption confidence="0.969522666666667">
Table 3: Configurations and results of our three submitted
systems and top three results, the numbers in bracket are
the official ranking out of all submitted systems.
</tableCaption>
<bodyText confidence="0.999927666666667">
Our primary system ranked the 4th out of 12 par-
ticipants in Task A and the 2nd out of 7 participants
in Task B. For both tasks the performance of the pri-
mary system is higher than the two contrastive sys-
tems, which is consistent with the results on training
data.
</bodyText>
<sectionHeader confidence="0.999613" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999978411764706">
We build two supervised classification systems for
answer selection and YES/NO response inference in
CQA. Specially, we extract heterogeneous features
from various information sources, i.e., answer, ques-
tion, answer-question pair and answerer. Our exper-
iments reveal that our designed features are all ef-
fective and when we combine all types of features
together the system achieves the best performance.
Although multiple features extracted from CQA,
the way of using these features are quite simple. Be-
sides, due to the huge number of bag-of-word fea-
ture, the effects of other specific features are im-
paired. For future work, we may explore other un-
derlying useful features and the advanced way of in-
tegrating these features to further improve the per-
formance, such as the fine-grained semantic rela-
tionship between question and answer, etc.
</bodyText>
<page confidence="0.992648">
240
</page>
<sectionHeader confidence="0.999202" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999226666666667">
This research is supported by grants from Science
and Technology Commission of Shanghai Munici-
pality under research grant no. (14DZ2260800 and
15ZR1410700) and Shanghai Collaborative Innova-
tion Center of Trustworthy Software for Internet of
Things (ZF1213).
</bodyText>
<sectionHeader confidence="0.998946" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999440465909091">
Eugene Agichtein, Carlos Castillo, Debora Donato, Aris-
tides Gionis, and Gilad Mishne. 2008. Finding high-
quality content in social media. In Proceedings of the
2008 International Conference on Web Search and Da-
ta Mining, pages 183–194.
Carmen Banea, Samer Hassan, Michael Mohler, and Ra-
da Mihalcea. 2012. Unt: A supervised synergistic
approach to semantic text similarity. In Proceedings
of the First Joint Conference on Lexical and Compu-
tational Semantics-Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation, pages 635–642.
Mohan John Blooma, Alton Yeow-Kuan Chua, and Dion
Hoe-Lian Goh. 2010. Selection of the best answer in
cqa services. In Information Technology: New Gener-
ations (ITNG), 2010 Seventh International Conference
on, pages 534–539.
Weiwei Guo and Mona Diab. 2012. Modeling sentences
in the latent space. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 864–872.
Jiwoon Jeon, William Bruce Croft, Joon Ho Lee, and
Soyeon Park. 2006. A framework to predict the qual-
ity of answers with non-textual features. In Proceed-
ings of the 29th annual international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 228–235.
Thomas K Landauer, Darrell Laham, Bob Rehder, and
Missy E Schreiner. 1997. How well can passage
meaning be derived without using word order? a com-
parison of latent semantic analysis and humans. In
Proceedings of the 19th annual meeting of the Cog-
nitive Science Society, pages 412–417.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and wordnet similarity for word
sense identification. WordNet: An electronic lexical
database, 49(2):265–283.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In ICML, volume 98, pages 296–304.
Llu´ıs M`arquez, James Glass, Walid Magdy, Alessandro
Moschitti, Preslav Nakov, and Bilal Randeree. 2015.
Semeval-2015 task 3: Answer selection in community
question answering. In SemEval 2015.
Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier Grisel,
Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-
cent Dubourg, et al. 2011. Scikit-learn: Machine
learning in python. The Journal of Machine Learning
Research, 12:2825–2830.
Frane ˇSari´c, Goran Glavaˇs, Mladen Karan, Jan ˇSnajder,
and Bojana Dalbelo Baˇsi´c. 2012. Takelab: Systems
for measuring semantic text similarity. In Proceedings
of the First Joint Conference on Lexical and Compu-
tational Semantics-Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation, pages 441–448.
Chirag Shah and Jefferey Pomerantz. 2010. Evaluat-
ing and predicting answer quality in community qa.
In Proceedings of the 33rd international ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 411–418.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2011. Learning to rank answers to non-
factoid questions from web collections. Computation-
al Linguistics, 37(2):351–383.
Hapnes Toba, Zhao-Yan Ming, Mirna Adriani, and Tat-
Seng Chua. 2014. Discovering high quality answers
in community question answering archives using a hi-
erarchy of classifiers. Information Sciences, 261:101–
115.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational Lin-
guistics, pages 133–138.
Jiang Zhao, Tian Tian Zhu, and Man Lan. 2014. Ecnu:
One stone two birds: Ensemble of heterogenous mea-
sures for semantic relatedness and textual entailment.
SemEval 2014, page 271.
Zhi-Min Zhou, Man Lan, Zheng-Yu Niu, and Yue Lu.
2012. Exploiting user profile information for answer
ranking in cqa. In Proceedings of the 21st internation-
al conference companion on World Wide Web, pages
767–774.
Tian Tian Zhu and LAN Man. 2013. Ecnucs: Measuring
short text semantic equivalence using multiple similar-
ity measurements. Atlanta, Georgia, USA, page 124.
</reference>
<page confidence="0.998094">
241
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.394974">
<title confidence="0.9992705">ECNU: Using Multiple Sources of CQA-based Information for Selection and YES/NO Response Inference</title>
<author confidence="0.960563">Jianxiang Wang Yi</author>
<author confidence="0.960563">Man Shanghai Key Laboratory of Multidimensional Information</author>
<affiliation confidence="0.996087">Department of Computer Science and</affiliation>
<address confidence="0.440326">East China Normal University, Shanghai 200241,</address>
<abstract confidence="0.998513055555555">This paper reports our submissions to community question answering task in SemEval- 2015, which consists of two subtasks: (1) predict the quality of answers to given question or relevant (2) to a given question based on the answers identified by subtask 1. For both subtasks, we adopted supervised classification method and examined the effects of heterogeneous features generated from community question answering data, such as bag-of-words, string matching, semantic similarity, answerer information, answer-specific features, questionspecific features, etc. Our submitted primary systems ranked the forth and the second for the two subtasks of English data respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Carlos Castillo</author>
<author>Debora Donato</author>
<author>Aristides Gionis</author>
<author>Gilad Mishne</author>
</authors>
<title>Finding highquality content in social media.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 International Conference on Web Search and Data Mining,</booktitle>
<pages>183--194</pages>
<contexts>
<context position="2097" citStr="Agichtein et al., 2008" startWordPosition="311" endWordPosition="314"> URL links without direct answer, or even be written informally. Therefore, in order to achieve high-quality user experience and maintain high levels of adherence, it is critical to present high-quality answers and provide direct responses for users. The CQA task in SemEval-2015 (M`arquez et al., 2015) provides such a universal platform for researchers to make a comparison between different approaches. This task consists of two subtasks: (1) subtask A is to classify the quality of answers as good, potential or bad, which also refers to the task of answer quality prediction (Jeon et al., 2006; Agichtein et al., 2008); (2) subtask B is to infer the global answer of a YES/NO question to be yes, no or unsure based on individual good answers. Most of the previous research on answer quality prediction has focused on extracting various features to employ ranking or classification methods (Surdeanu et al., 2011; Shah and Pomerantz, 2010), such as textual features (Agichtein et al., 2008; Blooma et al., 2010) including the length of an answer, overlapped words between a question-answer (QA) pair, etc. Another kind of widely used feature is extracted from answerer profile information (Shah and Pomerantz, 2010), su</context>
</contexts>
<marker>Agichtein, Castillo, Donato, Gionis, Mishne, 2008</marker>
<rawString>Eugene Agichtein, Carlos Castillo, Debora Donato, Aristides Gionis, and Gilad Mishne. 2008. Finding highquality content in social media. In Proceedings of the 2008 International Conference on Web Search and Data Mining, pages 183–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carmen Banea</author>
<author>Samer Hassan</author>
<author>Michael Mohler</author>
<author>Rada Mihalcea</author>
</authors>
<title>Unt: A supervised synergistic approach to semantic text similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,</booktitle>
<pages>635--642</pages>
<contexts>
<context position="10188" citStr="Banea et al., 2012" startWordPosition="1614" endWordPosition="1617">ly considers the overlapped surface words or substrings in a QA pair and it may not capture the semantic information between a QA pair. Therefore, we presented the following semantic similarity features, which are borrowed from previous work. Determining semantic similarity of sentences commonly uses measures of semantic similarity between individual words. We used knowledge-based and corpus-based word similarity features. The knowledge-based similarity estimation relies on a semantic network of words such as WordNet. In this work, we employed four WordNet-based word similarity metrics: Path (Banea et al., 2012), WUP (Wu and Palmer, 1994), LCH (Leacock and Chodorow, 1998) and Lin (Lin, 1998) similarity. Following (Zhu and Man, 2013), the best alignment strategy and the aggregation strategy are employed to propagate the word similarity to the text similarity. Moreover, Latent Semantic analysis (LSA) (Landauer et al., 1997) is a widely used corpus-based measure when evaluating textual similarity. We used the vector space sentence similarity proposed by (ˇSari´c et al., 2012), which represents each sentence as a single distributional vector by summing up the LSA vector of each word in the sentence. In t</context>
</contexts>
<marker>Banea, Hassan, Mohler, Mihalcea, 2012</marker>
<rawString>Carmen Banea, Samer Hassan, Michael Mohler, and Rada Mihalcea. 2012. Unt: A supervised synergistic approach to semantic text similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, pages 635–642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohan John Blooma</author>
<author>Alton Yeow-Kuan Chua</author>
<author>Dion Hoe-Lian Goh</author>
</authors>
<title>Selection of the best answer in cqa services.</title>
<date>2010</date>
<booktitle>In Information Technology: New Generations (ITNG), 2010 Seventh International Conference on,</booktitle>
<pages>534--539</pages>
<contexts>
<context position="2489" citStr="Blooma et al., 2010" startWordPosition="377" endWordPosition="380">pproaches. This task consists of two subtasks: (1) subtask A is to classify the quality of answers as good, potential or bad, which also refers to the task of answer quality prediction (Jeon et al., 2006; Agichtein et al., 2008); (2) subtask B is to infer the global answer of a YES/NO question to be yes, no or unsure based on individual good answers. Most of the previous research on answer quality prediction has focused on extracting various features to employ ranking or classification methods (Surdeanu et al., 2011; Shah and Pomerantz, 2010), such as textual features (Agichtein et al., 2008; Blooma et al., 2010) including the length of an answer, overlapped words between a question-answer (QA) pair, etc. Another kind of widely used feature is extracted from answerer profile information (Shah and Pomerantz, 2010), such as the number of best answers, the achieved levels and the earned points. However, such information is not often available in real world. Moreover, a recent study (Toba et al., 2014) has taken question type into consideration to make the answers quality prediction. In this paper, we built two classification systems for the two tasks respectively. For Task A, we extracted six types of fe</context>
</contexts>
<marker>Blooma, Chua, Goh, 2010</marker>
<rawString>Mohan John Blooma, Alton Yeow-Kuan Chua, and Dion Hoe-Lian Goh. 2010. Selection of the best answer in cqa services. In Information Technology: New Generations (ITNG), 2010 Seventh International Conference on, pages 534–539.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>Modeling sentences in the latent space.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume</booktitle>
<volume>1</volume>
<pages>864--872</pages>
<contexts>
<context position="11028" citStr="Guo and Diab, 2012" startWordPosition="1753" endWordPosition="1756">larity to the text similarity. Moreover, Latent Semantic analysis (LSA) (Landauer et al., 1997) is a widely used corpus-based measure when evaluating textual similarity. We used the vector space sentence similarity proposed by (ˇSari´c et al., 2012), which represents each sentence as a single distributional vector by summing up the LSA vector of each word in the sentence. In this work, two corpora are used to compute the LSA vector of words: New York Times Annotated Corpus (NYT) and Wikipedia. Besides, following (Zhao et al., 2014), we adopted the weighted textual matrix factorization (WTMF) (Guo and Diab, 2012) to model the semantics representations of sentences and then employed the new representations to calculate the semantic similarity between QA pairs using Cosine, Manhattan, Euclidean, Person, Spearmanr, Kendalltau measures respectively. 2.3.5 Answerer Information (AI) Previous work (Zhou et al., 2012) showed that information about answerer has great impact on answer ranking in CQA. Inspired by this work, we designed two answerer-specific features to represent answerer level and answerer expert domain information. To calculate the answerer level feature, we used the number of answers and the p</context>
</contexts>
<marker>Guo, Diab, 2012</marker>
<rawString>Weiwei Guo and Mona Diab. 2012. Modeling sentences in the latent space. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 864–872.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwoon Jeon</author>
<author>William Bruce Croft</author>
<author>Joon Ho Lee</author>
<author>Soyeon Park</author>
</authors>
<title>A framework to predict the quality of answers with non-textual features.</title>
<date>2006</date>
<booktitle>In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>228--235</pages>
<contexts>
<context position="2072" citStr="Jeon et al., 2006" startWordPosition="307" endWordPosition="310">ssues, contain only URL links without direct answer, or even be written informally. Therefore, in order to achieve high-quality user experience and maintain high levels of adherence, it is critical to present high-quality answers and provide direct responses for users. The CQA task in SemEval-2015 (M`arquez et al., 2015) provides such a universal platform for researchers to make a comparison between different approaches. This task consists of two subtasks: (1) subtask A is to classify the quality of answers as good, potential or bad, which also refers to the task of answer quality prediction (Jeon et al., 2006; Agichtein et al., 2008); (2) subtask B is to infer the global answer of a YES/NO question to be yes, no or unsure based on individual good answers. Most of the previous research on answer quality prediction has focused on extracting various features to employ ranking or classification methods (Surdeanu et al., 2011; Shah and Pomerantz, 2010), such as textual features (Agichtein et al., 2008; Blooma et al., 2010) including the length of an answer, overlapped words between a question-answer (QA) pair, etc. Another kind of widely used feature is extracted from answerer profile information (Shah</context>
</contexts>
<marker>Jeon, Croft, Lee, Park, 2006</marker>
<rawString>Jiwoon Jeon, William Bruce Croft, Joon Ho Lee, and Soyeon Park. 2006. A framework to predict the quality of answers with non-textual features. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 228–235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Darrell Laham</author>
<author>Bob Rehder</author>
<author>Missy E Schreiner</author>
</authors>
<title>How well can passage meaning be derived without using word order? a comparison of latent semantic analysis and humans.</title>
<date>1997</date>
<booktitle>In Proceedings of the 19th annual meeting of the Cognitive Science Society,</booktitle>
<pages>412--417</pages>
<contexts>
<context position="10504" citStr="Landauer et al., 1997" startWordPosition="1665" endWordPosition="1668"> semantic similarity between individual words. We used knowledge-based and corpus-based word similarity features. The knowledge-based similarity estimation relies on a semantic network of words such as WordNet. In this work, we employed four WordNet-based word similarity metrics: Path (Banea et al., 2012), WUP (Wu and Palmer, 1994), LCH (Leacock and Chodorow, 1998) and Lin (Lin, 1998) similarity. Following (Zhu and Man, 2013), the best alignment strategy and the aggregation strategy are employed to propagate the word similarity to the text similarity. Moreover, Latent Semantic analysis (LSA) (Landauer et al., 1997) is a widely used corpus-based measure when evaluating textual similarity. We used the vector space sentence similarity proposed by (ˇSari´c et al., 2012), which represents each sentence as a single distributional vector by summing up the LSA vector of each word in the sentence. In this work, two corpora are used to compute the LSA vector of words: New York Times Annotated Corpus (NYT) and Wikipedia. Besides, following (Zhao et al., 2014), we adopted the weighted textual matrix factorization (WTMF) (Guo and Diab, 2012) to model the semantics representations of sentences and then employed the n</context>
</contexts>
<marker>Landauer, Laham, Rehder, Schreiner, 1997</marker>
<rawString>Thomas K Landauer, Darrell Laham, Bob Rehder, and Missy E Schreiner. 1997. How well can passage meaning be derived without using word order? a comparison of latent semantic analysis and humans. In Proceedings of the 19th annual meeting of the Cognitive Science Society, pages 412–417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Combining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database,</title>
<date>1998</date>
<pages>49--2</pages>
<contexts>
<context position="10249" citStr="Leacock and Chodorow, 1998" startWordPosition="1624" endWordPosition="1628">gs in a QA pair and it may not capture the semantic information between a QA pair. Therefore, we presented the following semantic similarity features, which are borrowed from previous work. Determining semantic similarity of sentences commonly uses measures of semantic similarity between individual words. We used knowledge-based and corpus-based word similarity features. The knowledge-based similarity estimation relies on a semantic network of words such as WordNet. In this work, we employed four WordNet-based word similarity metrics: Path (Banea et al., 2012), WUP (Wu and Palmer, 1994), LCH (Leacock and Chodorow, 1998) and Lin (Lin, 1998) similarity. Following (Zhu and Man, 2013), the best alignment strategy and the aggregation strategy are employed to propagate the word similarity to the text similarity. Moreover, Latent Semantic analysis (LSA) (Landauer et al., 1997) is a widely used corpus-based measure when evaluating textual similarity. We used the vector space sentence similarity proposed by (ˇSari´c et al., 2012), which represents each sentence as a single distributional vector by summing up the LSA vector of each word in the sentence. In this work, two corpora are used to compute the LSA vector of w</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Claudia Leacock and Martin Chodorow. 1998. Combining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database, 49(2):265–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In ICML,</booktitle>
<volume>98</volume>
<pages>296--304</pages>
<contexts>
<context position="10269" citStr="Lin, 1998" startWordPosition="1631" endWordPosition="1632">e the semantic information between a QA pair. Therefore, we presented the following semantic similarity features, which are borrowed from previous work. Determining semantic similarity of sentences commonly uses measures of semantic similarity between individual words. We used knowledge-based and corpus-based word similarity features. The knowledge-based similarity estimation relies on a semantic network of words such as WordNet. In this work, we employed four WordNet-based word similarity metrics: Path (Banea et al., 2012), WUP (Wu and Palmer, 1994), LCH (Leacock and Chodorow, 1998) and Lin (Lin, 1998) similarity. Following (Zhu and Man, 2013), the best alignment strategy and the aggregation strategy are employed to propagate the word similarity to the text similarity. Moreover, Latent Semantic analysis (LSA) (Landauer et al., 1997) is a widely used corpus-based measure when evaluating textual similarity. We used the vector space sentence similarity proposed by (ˇSari´c et al., 2012), which represents each sentence as a single distributional vector by summing up the LSA vector of each word in the sentence. In this work, two corpora are used to compute the LSA vector of words: New York Times</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In ICML, volume 98, pages 296–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Llu´ıs M`arquez</author>
<author>James Glass</author>
</authors>
<title>Walid Magdy, Alessandro Moschitti, Preslav Nakov, and Bilal Randeree.</title>
<date>2015</date>
<marker>M`arquez, Glass, 2015</marker>
<rawString>Llu´ıs M`arquez, James Glass, Walid Magdy, Alessandro Moschitti, Preslav Nakov, and Bilal Randeree. 2015.</rawString>
</citation>
<citation valid="true">
<title>Semeval-2015 task 3: Answer selection in community question answering.</title>
<date>2015</date>
<booktitle>In SemEval</booktitle>
<marker>2015</marker>
<rawString>Semeval-2015 task 3: Answer selection in community question answering. In SemEval 2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Alexandre Gramfort Ga¨el Varoquaux</author>
<author>Vincent Michel</author>
<author>Bertrand</author>
</authors>
<title>Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer,</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<location>Ron Weiss, Vincent Dubourg, et</location>
<marker>Pedregosa, Ga¨el Varoquaux, Michel, Bertrand, 2011</marker>
<rawString>Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. 2011. Scikit-learn: Machine learning in python. The Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frane ˇSari´c</author>
<author>Goran Glavaˇs</author>
<author>Mladen Karan</author>
<author>Jan ˇSnajder</author>
<author>Bojana Dalbelo Baˇsi´c</author>
</authors>
<title>Takelab: Systems for measuring semantic text similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,</booktitle>
<pages>441--448</pages>
<marker>ˇSari´c, Glavaˇs, Karan, ˇSnajder, Baˇsi´c, 2012</marker>
<rawString>Frane ˇSari´c, Goran Glavaˇs, Mladen Karan, Jan ˇSnajder, and Bojana Dalbelo Baˇsi´c. 2012. Takelab: Systems for measuring semantic text similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, pages 441–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chirag Shah</author>
<author>Jefferey Pomerantz</author>
</authors>
<title>Evaluating and predicting answer quality in community qa.</title>
<date>2010</date>
<booktitle>In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>411--418</pages>
<contexts>
<context position="2417" citStr="Shah and Pomerantz, 2010" startWordPosition="365" endWordPosition="368">a universal platform for researchers to make a comparison between different approaches. This task consists of two subtasks: (1) subtask A is to classify the quality of answers as good, potential or bad, which also refers to the task of answer quality prediction (Jeon et al., 2006; Agichtein et al., 2008); (2) subtask B is to infer the global answer of a YES/NO question to be yes, no or unsure based on individual good answers. Most of the previous research on answer quality prediction has focused on extracting various features to employ ranking or classification methods (Surdeanu et al., 2011; Shah and Pomerantz, 2010), such as textual features (Agichtein et al., 2008; Blooma et al., 2010) including the length of an answer, overlapped words between a question-answer (QA) pair, etc. Another kind of widely used feature is extracted from answerer profile information (Shah and Pomerantz, 2010), such as the number of best answers, the achieved levels and the earned points. However, such information is not often available in real world. Moreover, a recent study (Toba et al., 2014) has taken question type into consideration to make the answers quality prediction. In this paper, we built two classification systems </context>
</contexts>
<marker>Shah, Pomerantz, 2010</marker>
<rawString>Chirag Shah and Jefferey Pomerantz. 2010. Evaluating and predicting answer quality in community qa. In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval, pages 411–418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Massimiliano Ciaramita</author>
<author>Hugo Zaragoza</author>
</authors>
<title>Learning to rank answers to nonfactoid questions from web collections.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>2</issue>
<contexts>
<context position="2390" citStr="Surdeanu et al., 2011" startWordPosition="360" endWordPosition="364">., 2015) provides such a universal platform for researchers to make a comparison between different approaches. This task consists of two subtasks: (1) subtask A is to classify the quality of answers as good, potential or bad, which also refers to the task of answer quality prediction (Jeon et al., 2006; Agichtein et al., 2008); (2) subtask B is to infer the global answer of a YES/NO question to be yes, no or unsure based on individual good answers. Most of the previous research on answer quality prediction has focused on extracting various features to employ ranking or classification methods (Surdeanu et al., 2011; Shah and Pomerantz, 2010), such as textual features (Agichtein et al., 2008; Blooma et al., 2010) including the length of an answer, overlapped words between a question-answer (QA) pair, etc. Another kind of widely used feature is extracted from answerer profile information (Shah and Pomerantz, 2010), such as the number of best answers, the achieved levels and the earned points. However, such information is not often available in real world. Moreover, a recent study (Toba et al., 2014) has taken question type into consideration to make the answers quality prediction. In this paper, we built </context>
</contexts>
<marker>Surdeanu, Ciaramita, Zaragoza, 2011</marker>
<rawString>Mihai Surdeanu, Massimiliano Ciaramita, and Hugo Zaragoza. 2011. Learning to rank answers to nonfactoid questions from web collections. Computational Linguistics, 37(2):351–383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hapnes Toba</author>
<author>Zhao-Yan Ming</author>
<author>Mirna Adriani</author>
<author>TatSeng Chua</author>
</authors>
<title>Discovering high quality answers in community question answering archives using a hierarchy of classifiers.</title>
<date>2014</date>
<journal>Information Sciences,</journal>
<volume>261</volume>
<pages>115</pages>
<contexts>
<context position="2882" citStr="Toba et al., 2014" startWordPosition="441" endWordPosition="444">ality prediction has focused on extracting various features to employ ranking or classification methods (Surdeanu et al., 2011; Shah and Pomerantz, 2010), such as textual features (Agichtein et al., 2008; Blooma et al., 2010) including the length of an answer, overlapped words between a question-answer (QA) pair, etc. Another kind of widely used feature is extracted from answerer profile information (Shah and Pomerantz, 2010), such as the number of best answers, the achieved levels and the earned points. However, such information is not often available in real world. Moreover, a recent study (Toba et al., 2014) has taken question type into consideration to make the answers quality prediction. In this paper, we built two classification systems for the two tasks respectively. For Task A, we extracted six types of features from multiple sources of CQA-based information to predict the answer quality, such as answer-, question-, answerer-specific information, surface word similarity and semantic similarity between question-answer pair, ect. For Task B, the global answer of a YES/NO question is summarized just from the individual good answers identified by Task A. Specifically, we first built a classifier</context>
</contexts>
<marker>Toba, Ming, Adriani, Chua, 2014</marker>
<rawString>Hapnes Toba, Zhao-Yan Ming, Mirna Adriani, and TatSeng Chua. 2014. Discovering high quality answers in community question answering archives using a hierarchy of classifiers. Information Sciences, 261:101– 115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verbs semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd annual meeting on Association for Computational Linguistics,</booktitle>
<pages>133--138</pages>
<contexts>
<context position="10215" citStr="Wu and Palmer, 1994" startWordPosition="1619" endWordPosition="1622">d surface words or substrings in a QA pair and it may not capture the semantic information between a QA pair. Therefore, we presented the following semantic similarity features, which are borrowed from previous work. Determining semantic similarity of sentences commonly uses measures of semantic similarity between individual words. We used knowledge-based and corpus-based word similarity features. The knowledge-based similarity estimation relies on a semantic network of words such as WordNet. In this work, we employed four WordNet-based word similarity metrics: Path (Banea et al., 2012), WUP (Wu and Palmer, 1994), LCH (Leacock and Chodorow, 1998) and Lin (Lin, 1998) similarity. Following (Zhu and Man, 2013), the best alignment strategy and the aggregation strategy are employed to propagate the word similarity to the text similarity. Moreover, Latent Semantic analysis (LSA) (Landauer et al., 1997) is a widely used corpus-based measure when evaluating textual similarity. We used the vector space sentence similarity proposed by (ˇSari´c et al., 2012), which represents each sentence as a single distributional vector by summing up the LSA vector of each word in the sentence. In this work, two corpora are u</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu and Martha Palmer. 1994. Verbs semantics and lexical selection. In Proceedings of the 32nd annual meeting on Association for Computational Linguistics, pages 133–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Zhao</author>
<author>Tian Tian Zhu</author>
<author>Man Lan</author>
</authors>
<title>Ecnu: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailment. SemEval</title>
<date>2014</date>
<pages>271</pages>
<contexts>
<context position="10946" citStr="Zhao et al., 2014" startWordPosition="1739" endWordPosition="1742">ent strategy and the aggregation strategy are employed to propagate the word similarity to the text similarity. Moreover, Latent Semantic analysis (LSA) (Landauer et al., 1997) is a widely used corpus-based measure when evaluating textual similarity. We used the vector space sentence similarity proposed by (ˇSari´c et al., 2012), which represents each sentence as a single distributional vector by summing up the LSA vector of each word in the sentence. In this work, two corpora are used to compute the LSA vector of words: New York Times Annotated Corpus (NYT) and Wikipedia. Besides, following (Zhao et al., 2014), we adopted the weighted textual matrix factorization (WTMF) (Guo and Diab, 2012) to model the semantics representations of sentences and then employed the new representations to calculate the semantic similarity between QA pairs using Cosine, Manhattan, Euclidean, Person, Spearmanr, Kendalltau measures respectively. 2.3.5 Answerer Information (AI) Previous work (Zhou et al., 2012) showed that information about answerer has great impact on answer ranking in CQA. Inspired by this work, we designed two answerer-specific features to represent answerer level and answerer expert domain information</context>
</contexts>
<marker>Zhao, Zhu, Lan, 2014</marker>
<rawString>Jiang Zhao, Tian Tian Zhu, and Man Lan. 2014. Ecnu: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailment. SemEval 2014, page 271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi-Min Zhou</author>
<author>Man Lan</author>
<author>Zheng-Yu Niu</author>
<author>Yue Lu</author>
</authors>
<title>Exploiting user profile information for answer ranking in cqa.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st international conference companion on World Wide Web,</booktitle>
<pages>767--774</pages>
<contexts>
<context position="11331" citStr="Zhou et al., 2012" startWordPosition="1794" endWordPosition="1797">onal vector by summing up the LSA vector of each word in the sentence. In this work, two corpora are used to compute the LSA vector of words: New York Times Annotated Corpus (NYT) and Wikipedia. Besides, following (Zhao et al., 2014), we adopted the weighted textual matrix factorization (WTMF) (Guo and Diab, 2012) to model the semantics representations of sentences and then employed the new representations to calculate the semantic similarity between QA pairs using Cosine, Manhattan, Euclidean, Person, Spearmanr, Kendalltau measures respectively. 2.3.5 Answerer Information (AI) Previous work (Zhou et al., 2012) showed that information about answerer has great impact on answer ranking in CQA. Inspired by this work, we designed two answerer-specific features to represent answerer level and answerer expert domain information. To calculate the answerer level feature, we used the number of answers and the percentage of good answers for each answerer. For expert domain feature, we employed the question categories where the answerer is an expert. Specifically, for each answerer, let G be the number of good answers the answerer responses and GZ be the number of good answers to the i-th question category (i&lt;</context>
<context position="17723" citStr="Zhou et al., 2012" startWordPosition="2868" endWordPosition="2871">which predict all answers as yes. Moreover, in Task A the performance of other five feature types alone is far lower than bag-of-words, ranging from 23% to 38% approximately. Second, for Task A, when combining all the features together the system achieves the best performance, which indicates that all types of features make contribution more or less. Specially, among the six types of features, answerer information and semantic similarity between QA pairs make more contribution than others. This indicates that answerer profile information is important, which is consistent with the findings in (Zhou et al., 2012). Besides, the semantic similarity captures deep relationship between Q-A pair than the surface word, which is helpful for performance improvement. In Task B, we also observed the similar findings, i.e., the system using all types of features achieves the best performance. Moreover, the YES/NO word list feature makes great contribution to the performance improvement. This is consistent with our expectation. Besides, although in this work the word vector feature improves the performance, this improvements is not as much as our expectation. The possible reason may be the simple way of using the </context>
</contexts>
<marker>Zhou, Lan, Niu, Lu, 2012</marker>
<rawString>Zhi-Min Zhou, Man Lan, Zheng-Yu Niu, and Yue Lu. 2012. Exploiting user profile information for answer ranking in cqa. In Proceedings of the 21st international conference companion on World Wide Web, pages 767–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tian Tian Zhu</author>
<author>LAN Man</author>
</authors>
<title>Ecnucs: Measuring short text semantic equivalence using multiple similarity measurements.</title>
<date>2013</date>
<pages>124</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="10311" citStr="Zhu and Man, 2013" startWordPosition="1635" endWordPosition="1638"> a QA pair. Therefore, we presented the following semantic similarity features, which are borrowed from previous work. Determining semantic similarity of sentences commonly uses measures of semantic similarity between individual words. We used knowledge-based and corpus-based word similarity features. The knowledge-based similarity estimation relies on a semantic network of words such as WordNet. In this work, we employed four WordNet-based word similarity metrics: Path (Banea et al., 2012), WUP (Wu and Palmer, 1994), LCH (Leacock and Chodorow, 1998) and Lin (Lin, 1998) similarity. Following (Zhu and Man, 2013), the best alignment strategy and the aggregation strategy are employed to propagate the word similarity to the text similarity. Moreover, Latent Semantic analysis (LSA) (Landauer et al., 1997) is a widely used corpus-based measure when evaluating textual similarity. We used the vector space sentence similarity proposed by (ˇSari´c et al., 2012), which represents each sentence as a single distributional vector by summing up the LSA vector of each word in the sentence. In this work, two corpora are used to compute the LSA vector of words: New York Times Annotated Corpus (NYT) and Wikipedia. Bes</context>
</contexts>
<marker>Zhu, Man, 2013</marker>
<rawString>Tian Tian Zhu and LAN Man. 2013. Ecnucs: Measuring short text semantic equivalence using multiple similarity measurements. Atlanta, Georgia, USA, page 124.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>