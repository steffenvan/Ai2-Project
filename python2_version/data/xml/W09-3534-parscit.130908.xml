<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000355">
<title confidence="0.986565">
Analysis and Robust Extraction of Changing Named Entities
</title>
<author confidence="0.997613">
Masatoshi Tsuchiya† Shoko Endo$ Seiichi Nakagawa$
</author>
<affiliation confidence="0.9993825">
†Information and Media Center / $Department of Information and Computer Sciences,
Toyohashi University of Technology
</affiliation>
<email confidence="0.989699">
tsuchiya@imc.tut.ac.jp, {shoko,nakagawa}@slp.ics.tut.ac.jp
</email>
<sectionHeader confidence="0.994654" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999873086956522">
This paper focuses on the change of named
entities over time and its influence on the
performance of the named entity tagger.
First, we analyze Japanese named enti-
ties which appear in Mainichi Newspaper
articles published in 1995, 1996, 1997,
1998 and 2005. This analysis reveals that
the number of named entity types and
the number of named entity tokens are
almost steady over time and that 70 —
80% of named entity types in a certain
year occur in the articles published either
in its succeeding year or in its preceding
year. These facts lead that 20 — 30%
of named entity types are replaced with
new ones every year. The experiment
against these texts shows that our propos-
ing semi-supervised method which com-
bines a small annotated corpus and a large
unannotated corpus for training works ro-
bustly although the traditional supervised
method is fragile against the change of
name entity distribution.
</bodyText>
<sectionHeader confidence="0.998884" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999924636363636">
It is widely agreed that extraction of named entity
(henceforth, denoted as NE) is an important sub-
task for various NLP applications, such as infor-
mation retrieval, machine translation, information
extraction and natural language understanding.
Several conferences like Message Understanding
Conference(Grishman and Sundheim, 1996) and
the IREX workshop (Sekine and Eriguchi, 2000)
were conducted to encourage researchers of NE
extraction and to provide its common evaluation
basis.
In Japanese NE extraction, it is quite common
to apply morphological analysis as preprocessing
stage which segments a sentence into a sequence
of morphemes. After that, either a pattern matcher
based on hand-crafted rules or a statistical chun-
ker is employed to extract NEs from a sequence of
morphemes. Various machine learning approaches
such as maximum entropy(Uchimoto et al., 2000),
decision list(Sassano and Utsuro, 2000; Isozaki,
2001), and Support Vector Machine(Yamada et
al., 2002; Isozaki and Kazawa, 2002) were in-
vestigated for extracting NEs. These researches
show that machine learning approaches are more
promising than approaches based on hand-crafted
rules if a large corpus whose NEs are properly an-
notated is available as training data.
However, it is difficult to obtain an enough cor-
pus in the real world because of the increasing
number of NE types and the increasing time gap
between the training corpus and the test corpus.
There is the increasing number of NE types like
personal names and company names in the real
world. For example, a large database of organi-
zation names(Nichigai Associates, 2007) already
contains 171,708 types and is still increasing. Be-
cause annotation work is quite expensive, the an-
notated corpus may become obsolete in a short
period of time. Both of two factors expands the
difference of NE distribution between the training
corpus and the test corpus, and it may decrease the
performance of the NE tagger as shown in (Mota
and Grishman, 2008). Therefore, a robust method
to extract NEs which do not occur or occur few
times in a training corpus is necessary.
This paper focuses on the change of NEs over
time and its influence on the performance of the
NE tagger. First, we annotate NEs in Mainichi
Newspaper articles published in 1996, 1997, 1998
and 2005, and analyze NEs which appear in
these texts and an existing corpus. It consists of
Mainichi Newspaper articles published in 1995,
thus, we get an annotated corpus that spans 10
years. This analysis reveals that the number of
NE types and the number of NE tokens are almost
</bodyText>
<page confidence="0.977793">
161
</page>
<note confidence="0.9929825">
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 161–167,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<tableCaption confidence="0.932873">
Table 1: Statistics of NE categories of IREX cor-
pus
</tableCaption>
<table confidence="0.9959927">
NE Categories Frequency (%)
ARTIFACT 747 (4.0)
DATE 3567 (19.1)
LOCATION 5463 (29.2)
MONEY 390 (2.1)
ORGANIZATION 3676 (19.7)
PERCENT 492 (2.6)
PERSON 3840 (20.6)
TIME 502 (2.7)
Total 18677
</table>
<bodyText confidence="0.998836">
steady over time and that that 70 — 80% of NE
types in a certain year occur in the articles pub-
lished either in its succeeding year or in its preced-
ing year. These facts lead that 20 — 30% of named
entity types are replaced with new ones every year.
The experiment against these corpora shows that
the traditional supervised method is fragile against
the change of NE types and that our proposing
semi-supervised method which combines a small
annotated corpus and a large unannotated corpus
for training is robust against the change of NE
types.
</bodyText>
<subsectionHeader confidence="0.948020333333333">
String overlap
2 Analysis of Changing Named Entities
2.1 Task of the IREX Workshop
</subsectionHeader>
<bodyText confidence="0.99995075">
The task of NE extraction of the IREX work-
shop (Sekine and Eriguchi, 2000) is to recognize
eight NE categories in Table 1. The organizer
of the IREX workshop provided a training corpus
(henceforth, denoted as IREX corpus), which con-
sists of 1,174 Mainichi Newspaper articles pub-
lished from January 1st 1995 to 10th which in-
clude 18,677 NEs. In the Japanese language, no
other corpora whose NEs are annotated are pub-
licly available as far as we know.&apos; Thus, IREX
corpus is referred as a golden sample of NE distri-
bution in this paper.
</bodyText>
<subsectionHeader confidence="0.998059">
2.2 Data Description
</subsectionHeader>
<bodyText confidence="0.99980325">
The most homogeneous texts which are written in
different days are desirable, to explore the influ-
ence of the text time frame on NE distribution. Be-
cause IREX corpus is referred as a golden sample
</bodyText>
<footnote confidence="0.897669">
&apos;The organizer of the IBEX workshop also provides the
testing data to its participants, however, we cannot see it be-
cause we did not join it.
</footnote>
<figureCaption confidence="0.99997">
Figure 1: Distribution of NE categories
</figureCaption>
<figure confidence="0.951653">
—3 —2 —1 0 1 2 3 4
Time pp beween annotated corpus and unannotated corpus
</figure>
<figureCaption confidence="0.999144">
Figure 2: Overlap ratio of NEs over years
</figureCaption>
<bodyText confidence="0.999977">
in this paper, Mainichi Newspaper articles writ-
ten in different years than IREX corpus is suit-
able. Thus, ordinal days of June and October in
1996, 1997, 1998 and 2005 are randomly selected
as sampling days.
Because annotating work is too expensive for
us to annotate all articles published in sampling
days, thirty percent of them are only annotated.
Each article of Mainichi Newspaper belongs into
16 categories like front page articles, international
stories, economical stories, political stories, edito-
rial columns, and human interest stories. Because
these categories may influence to NE distribution,
it is important to keep the proportion of categories
in the sampled texts to the proportion in the whole
newspaper, in order to investigate NE distribution
over the whole newspaper. Therefore, thirty per-
cent articles of each category published at sam-
pling days are randomly selected and annotated in
accordance with the IREX regulation.
</bodyText>
<subsectionHeader confidence="0.999757">
2.3 Analysis of Annotated Samples
</subsectionHeader>
<bodyText confidence="0.9981415">
Table 2 shows the statistics of our annotated cor-
pus. The leftmost column of Table 2 (whose pub-
</bodyText>
<figure confidence="0.999553551724138">
100%
90%
TIME
PERSON
PERCENT
ORGANIZATION
MONEY
LOCATION
DATE
ARTIFACT
10%
0%
80%
70%
80%
50%
40%
s0%
20%
100%
40%
80%
80%
20%
0%
1998
1998
1997
1998
</figure>
<page confidence="0.984709">
162
</page>
<tableCaption confidence="0.998581">
Table 2: Statistics of sampling texts
</tableCaption>
<table confidence="0.999617">
Published date 1995 1996 1997 1998 2005
Jan. 1∼10 Jun. 5 Oct. 15 Jun. 10 Oct. 7 Jun. 8 Oct. 21 Jun. 23 Oct. 12
# of articles 1174 120 133 106 117 96 126 90 99
# of characters 407881 60790 53625 46653 50362 51006 67744 49038 44344
# of NE types 6979 1446 1656 1276 1350 1190 1226 1230 1113
# of NE tokens 18677 2519 2652 2145 2403 2126 2052 1902 2007
# of NE types / # of characters 0.0171 0.0238 0.0309 0.0274 0.0268 0.0233 0.0181 0.0251 0.0251
# of NE tokens / # of characters 0.0458 0.0414 0.0495 0.0460 0.0477 0.0417 0.0303 0.0388 0.0453
</table>
<tableCaption confidence="0.999861">
Table 3: Overlap of NE types between texts published in different years
</tableCaption>
<bodyText confidence="0.750931714285714">
Published date of Published year of unannotated corpus U
annotated corpus A
Jan. 1-10 (1995)
Jun. 6, Oct. 15 (1996)
Jun. 6, Oct. 7 (1997)
Jun. 8, Oct. 21 (1998)
Jun. 23, Oct. 12 (2005)
</bodyText>
<figure confidence="0.997370463414634">
1993
73.2%
67.2%
71.2%
72.5%
62.3%
1994
78.6%
71.7%
73.4%
74.6%
64.1%
1995
72.2%
74.4%
76.2%
66.8%
1996
74.4%
—
78.6%
79.7%
68.7%
1997
65.0%
77.3%
—
82.7%
71.2%
1998
64.4%
76.0%
80.8%
—
72.9%
1999
63.3%
75.1%
78.6%
84.0%
73.8%
</figure>
<bodyText confidence="0.998098875">
lish date is January 1st to 10th in 1995) is corre-
sponding to IREX corpus, and other columns are
corresponding to articles annotated by ourselves.
Table 2 illustrates that the normalized number of
NE types and the normalized number of NE tokens
are almost steady over time. Figure 1 shows the
distributions of NE categories for sampling texts
and that there is no significant difference between
them.
We also investigate the relation of the time gap
between texts and NE types which appear in these
texts. The overlap ratio of NE types between the
annotated corpus A published in the year YA and
the annotated corpus B published in the year YB
was defined in (Mota and Grishman, 2008) as fol-
lows
</bodyText>
<equation confidence="0.991509666666667">
|TA n TB|
type overlap(A, B) =
|TA |+ |TB |- |TA n TB|,
</equation>
<bodyText confidence="0.999952625">
where TA and TB are lists of NE types which ap-
pear in A and B respectively. However, it is im-
possible to compute reliable type overlap in our
research because enough annotated texts are un-
available. As an alternative of type overlap, the
overlap ratio of NE types between the annotated
corpus A and the unannotated corpus U published
in the year YU is defined as follows
</bodyText>
<equation confidence="0.8116625">
∑ ���� S(s, U)
string overlap(A, U) = |TA|
</equation>
<bodyText confidence="0.999861095238095">
where S(s, U) is the binary function to indicate
whether the string s occurs in the string U or not.
Table 3 shows string ratio values of annotated
texts. It shows that 70 - 80% of TA appear in the
preceding year of YA, and that 70 - 80% of TA
appear in the succeeding year of YA.
Figure 2 shows the relation between the time
gap YU - YA and string ratio(A, U). Sup-
pose that all NEs are independent and equiv-
alent on their occurrence probability and that
string ratio(A, U) is equal to 0.8 when the time
gap YU - YA is equal to one. When the time gap
YUP - YA is equal to two years, although this as-
sumption leads that string ratio(A, U&apos;) will be
equal to 0.64, string ratio(A, U&apos;) in Figure 2 is
greater than 0.7. This suggests that NEs are not
equivalent on their occurrence probability. And
more, Table 4 shows that the longer time span
of the annotated text increases the number of NE
types. These facts lead that some NEs are short-
lived and superseded by other new NEs.
</bodyText>
<sectionHeader confidence="0.9469935" genericHeader="method">
3 Robust Extraction of Changing Named
Entities
</sectionHeader>
<bodyText confidence="0.9374207">
It is infeasible to prepare a large annotated cor-
pus which covers all increasing NEs. A semi-
supervised learning approach which combines a
small annotated corpus and a large unannotated
corpus for training is promising to cope this prob-
lem. (Miller et al., 2004) proposed the method
using classes which are assigned to words based
on the class language model built from a large
unannotated corpus. (Ando and Zhang, 2005) pro-
,
</bodyText>
<page confidence="0.999027">
163
</page>
<tableCaption confidence="0.99929">
Table 4: Number of NE types and Time Span of Annotated Text
</tableCaption>
<table confidence="0.8980589">
1995 1995∼1996 1995∼1997 1995∼1998 1995∼2005
ARTIFACT 541 (1.00) 743 (1.37) 862 (1.59) 1025 (1.89) 1169 (2.16)
DATE 950 (1.00) 1147 (1.21) 1326 (1.40) 1461 (1.54) 1583 (1.67)
LOCATION 1403 (1.00) 1914 (1.36) 2214 (1.58) 2495 (1.78) 2692 (1.92)
MONEY 301 (1.00) 492 (1.63) 570 (1.89) 656 (2.18) 749 (2.49)
ORGANIZATION 1487 (1.00) 1890 (1.27) 2280 (1.53) 2566 (1.73) 2893 (1.95)
PERCENT 249 (1.00) 319 (1.28) 353 (1.42) 401 (1.61) 443 (1.78)
PERSON 1842 (1.00) 2540 (1.38) 3175 (1.72) 3683 (2.00) 4243 (2.30)
TIME 206 (1.00) 257 (1.25) 291 (1.41) 314 (1.52) 332 (1.61)
Total 6979 (1.00) 9302 (1.33) 11071 (1.59) 12601 (1.81) 14104 (2.02)
(Values in brackets are rates of increase comparing to 1995.)
Morpheme Feature Similar Morpheme Feature Character Chunk Label
Type
Feature
(English POS (English POS
translation) translation)
(kyou) (today) Noun–Adverbial (kyou) (today) Noun–Adverbial (1, 0, 0, 0, 0, 0) O
(no) gen Particle (no) gen Particle (0, 1, 0, 0, 0, 0) O
(Ishikari) (Ishikari) Noun–Proper (Kantou) (Kantou) Noun–Proper (1, 0, 0, 0, 0, 0) B-LOCATION
(heiya) (plain) Noun–Generic (heiya) (plain) Noun–Generic (1, 0, 0, 0, 0, 0) I-LOCATION
</table>
<figureCaption confidence="0.7573302">
(no) gen Particle (no) gen Particle (0, 1, 0, 0, 0, 0) O
(tenki) (weather) Noun–Generic (tenki) (weather) Noun–Generic (1, 0, 0, 0, 0, 0) O
(ha) top Particle (ha) top Particle (0, 1, 0, 0, 0, 0) O
(hare) (fine) Noun–Generic (hare) (fine) Noun–Generic (1, 1, 0, 0, 0, 0) O
Figure 3: Example of Training Instance for Proposed Method
</figureCaption>
<bodyText confidence="0.990285">
posed the method using thousands of automati-
cally generated auxiliary classification problems
on an unannotated corpus. (?) proposed the semi-
supervised discriminative model whose potential
function can treat both an annotated corpus and an
unannotated corpus.
In this paper, the method proposed by (Tsuchiya
et al., 2008) is employed, because its implementa-
tion is quite easy. It consists of two steps. The
first step is to assign the most similar and famil-
iar morpheme to each unfamiliar morpheme based
on their context vectors calculated from a large
unannotated corpus. The second step is to employ
Conditional Random Fields(CRF)2(Lafferty et al.,
2001) using both features of original morphemes
and features of similar morphemes.
This section gives the detail of this method.
</bodyText>
<subsectionHeader confidence="0.999929">
3.1 Chunking of Named Entities
</subsectionHeader>
<bodyText confidence="0.999921833333333">
It is quite common that the task of extracting
Japanese NEs from a sentence is formalized as
a chunking problem against a sequence of mor-
phemes. For representing proper chunks, we em-
ploy IOB2 representation, one of representations
which have been studied well in various chunking
</bodyText>
<footnote confidence="0.644534">
2http://chasen.org/˜taku/software/CRF+
+/
</footnote>
<bodyText confidence="0.995777333333333">
tasks of NLP (Tjong Kim Sang, 1999). This rep-
resentation uses the following three labels.
B Current token is the beginning of a chunk.
I Current token is a middle or the end of a
chunk consisting of more than one token.
O Current token is outside of any chunk.
Actually, we prepare the 16 derived labels from
the label B and the label I for eight NE categories,
in order to distinguish them.
When the task of extracting Japanese NEs from
a sentence is formalized as a chunking problem
of a sequence of morphemes, the segmentation
boundary problem arises as widely known. For
example, the NE definition of IREX tells that a
Chinese character “ (bei)” must be extracted as
an NE means America from a morpheme “
(hou-bei)” which means visiting America. A naive
chunker using a morpheme as a chunking unit can-
not extract such a kind of NEs. In order to cope
this problem, (Uchimoto et al., 2000) proposed
employing translation rules to modify problematic
morphemes, and (Asahara and Matsumoto, 2003;
Nakano and Hirai, 2004) formalized the task of ex-
tracting NEs as a chunking problem of a sequence
of characters instead of a sequence of morphemes.
In this paper, we keep the naive formalization, be-
cause it is still enough to analyze the influence of
</bodyText>
<page confidence="0.996828">
164
</page>
<bodyText confidence="0.982544">
the text time frame.
</bodyText>
<subsectionHeader confidence="0.999734">
3.2 Assignment of Similar Morpheme
</subsectionHeader>
<bodyText confidence="0.999778333333333">
A context vector Vm of a morpheme m is a vector
consisting of frequencies of all possible unigrams
and bigrams,
</bodyText>
<equation confidence="0.989834714285714">
Vm = � �
� �
� f(m, m0, m0), ··· f(m, mN, mN), �
� �
�f(m0, m), · · · f(mN, m), �
f(m, m0), · · · f(m, mN),
f(m0, m0, m), ··· f(mN, mN, m)
</equation>
<bodyText confidence="0.999857307692308">
where M - {m0, m1, ... ,mN} is a set of all
morphemes of the unannotated corpus, f(mi, mj)
is a frequency that a sequence of a morpheme mi
and a morpheme mj occurs in the unannotated
corpus, and f(mi, mj, mk) is a frequency that a
sequence of morphemes mi, mj and mk occurs in
the unannotated corpus.
Suppose an unfamiliar morpheme mu E M n
MF, where MF is a set of familiar morphemes
that occur frequently in the annotated corpus. The
most similar morpheme rhu to the morpheme mu
measured with their context vectors is given by the
following equation,
</bodyText>
<equation confidence="0.9908905">
rhu = argmax sim(Vmu, Vm), (1)
mEMF
</equation>
<bodyText confidence="0.999935">
where sim(Vi, Vj) is a similarity function between
context vectors. In this paper, the cosine function
is employed as it.
</bodyText>
<subsectionHeader confidence="0.98014">
3.3 Features
</subsectionHeader>
<bodyText confidence="0.983257">
The feature set Fi at i-th position is defined as
a tuple of the morpheme feature MF(mi) of the
i-th morpheme mi, the similar morpheme feature
SF(mi), and the character type feature CF(mi).
Fi = (MF(mi), SF(mi), CF(mi) )
The morpheme feature MF(mi) is a pair of the
surface string and the part-of-speech of mi. The
similar morpheme feature SF (mi) is defined as
where mi is the most similar and familiar mor-
pheme to mi given by Eqn. 1. The character type
feature CF(mi) is a set of six binary flags to in-
dicate that the surface string of mi contains a Chi-
nese character, a hiragana character, a katakana
</bodyText>
<figure confidence="0.467475">
�� Chunking Direction ��
Feature set Fi−2 Fi−1 Fi Fi+1 Fi+2
Chunk label ci−2 ci−1 ci
</figure>
<figureCaption confidence="0.889208125">
Figure 4: Chunking Direction
character, an English alphabet, a number and an
other character respectively.
, When we identify the chunk label ci for the i-
th morpheme mi, the surrounding five feature sets
Fi−2, Fi−1, Fi, Fi+1, Fi+2 and the preceding two
chunk labels ci−2, ci−1 are referred as shown in
Figure 4.
Figure 3 shows an example of training instance
of the proposed method for the sentence “
(kyou) (no) (Ishikari) (heiya) (no)
(tenki) (ha) (hare)” which means “It
is fine at Ishikari-plain, today”. “ (Kantou)”
is assigned as the most similar and familiar mor-
pheme to “ (Ishikari)” which is unfamiliar in
the training corpus.
</figureCaption>
<subsectionHeader confidence="0.911091">
3.4 Experimental Result
</subsectionHeader>
<bodyText confidence="0.999955916666667">
Figure 5 compares performances of the proposed
method and the baseline method over the test texts
which were published in 1996, 1997, 1998 and
2005. The proposed method combines a small an-
notated corpus and a large unannotated corpus as
already described. This experiment refers IREX
corpus as a small annotated corpus, and refers
Mainichi Newspaper articles published from 1993
to the preceding year of the test text published
year as a large unannotated corpus. For example,
when the test text was published in 1998, Mainichi
Newspaper articles published from 1993 to 1997
are used. The baseline method is trained from
IREX corpus with CRF. But, it uses only MF and
CF as features, and does not use SF. Figure 5 il-
lustrates two points: (1) the proposed method out-
performs the baseline method consistently, (2) the
baseline method is fragile to changing of test texts.
Figure 6 shows the relation between the per-
formance of the proposed method and the size of
unannotated corpus against the test corpus pub-
lished in 2005. It reveals that that increasing unan-
notated corpus size improves the performance of
the proposed method.
</bodyText>
<sectionHeader confidence="0.999414" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.987322">
In this paper, we explored the change of NE dis-
tribution over time and its influence on the per-
</bodyText>
<figure confidence="0.9213292">
r
5F(mi) =MF( mi) if mi E M n MF
Sl
MF(mi)
otherwise ,
</figure>
<page confidence="0.804223">
165
</page>
<figureCaption confidence="0.8790128">
Figure 5: Comparison between proposed method
and baseline method
Unannotated text used for training
Figure 6: Relation of performance and unanno-
tated corpus size
</figureCaption>
<bodyText confidence="0.9998915">
formance of the NE tagger. First, we annotated
Mainichi Newspaper articles published in 1996,
1997, 1998 and 2005, and analyzed NEs which
appear in these texts and IREX corpus which con-
sists of Mainichi Newspaper articles published in
1995. This analysis illustrated that the number of
NE types and the number of NE tokens are al-
most steady over time, and that 70 — 80% of NE
types seen in a certain year occur in the texts pub-
lished either in its succeeding year or in its pre-
ceding year. The experiment against these texts
showed that our proposing semi-supervised NE
tagger works robustly although the traditional su-
pervised NE tagger is fragile against the change of
NE types. Based on the results described in this
paper, we will investigate the relation between the
performance of NE tagger and the similarity of its
training corpus and its test corpus.
</bodyText>
<sectionHeader confidence="0.978338" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998650098039215">
Rie Kubota Ando and Tong Zhang. 2005. A high-
performance semi-supervised learning method for
text chunking. In Proc. ofACL ’05, pages 1–9, June.
Masayuki Asahara and Yuji Matsumoto. 2003.
Japanese named entity extraction with redundant
morphological analysis. In Proc. of HLT–NAACL
’03, pages 8–15.
Ralph Grishman and Beth Sundheim. 1996. Mes-
sage understanding conference-6: a brief history. In
Proc. of the 16th COLING, pages 466–471.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient
support vector classifiers for named entity recogni-
tion. In Proc. of the 19th COLING, pages 1–7.
Hideki Isozaki. 2001. Japanese named entity recogni-
tion based on a simple rule generator and decision
tree learning. In Proc. of ACL ’01, pages 314–321.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In Proceedings of ICML, pages 282–
289.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. In Proc. of HLT-NAACL 2004,
pages 337–342, May.
Cristina Mota and Ralph Grishman. 2008. Is this NE
tagger getting old? In Proceedings of the Sixth
International Language Resources and Evaluation
(LREC’08), May.
Keigo Nakano and Yuzo Hirai. 2004. Japanese named
entity extraction with bunsetsu features. Transac-
tions of Information Processing Society of Japan,
45(3):934–941, Mar. (in Japanese).
Nichigai Associates, editor. 2007. DCS Kikan-mei
Jisho. Nichigai Associates. (in Japanese).
Manabu Sassano and Takehito Utsuro. 2000. Named
entity chunking techniques in supervised learning
for japanese named entity recognition. In Proc. of
the 18th COLING, pages 705–711.
Satoshi Sekine and Yoshio Eriguchi. 2000. Japanese
named entity extraction evaluation: analysis of re-
sults. In Proc. of the 18th COLING, pages 1106–
1110.
E. Tjong Kim Sang. 1999. Representing text chunks.
In Proc. of the 9th EACL, pages 173–179.
Masatoshi Tsuchiya, Shinya Hida, and Seiichi Naka-
gawa. 2008. Robust extraction of named entity in-
cluding unfamiliar word. In Proceedings of ACL-
08: HLT, Short Papers, pages 125–128, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
</reference>
<figure confidence="0.99787447826087">
1996 1997 1998 2005
Publishing year of test text
F—measure
0.790
0.780
0.770
0.760
0.750
0.740
0.730
0.720
0.710
0.700
0.690
0.680
Proposed
Baseline
F-msmurs
0.765
0.735
0.705
0.75
0.72
</figure>
<page confidence="0.982185">
166
</page>
<reference confidence="0.998970181818182">
Kiyotaka Uchimoto, Ma Qing, Masaki Murata, Hiromi
Ozaku, Masao Utiyama, and Hitoshi Isahara. 2000.
Named entity extraction based on a maximum en-
tropy model and transformation rules. Journal of
Natural Language Processing, 7(2):63–90, Apr. (in
Japanese).
Hiroyasu Yamada, Taku Kudo, and Yuji Matsumoto.
2002. Japanese named entity extraction using sup-
port vector machine. Transactions of Information
Processing Society of Japan, 43(1):44–53, Jan. (in
Japanese).
</reference>
<page confidence="0.997693">
167
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.519498">
<title confidence="0.984599">Analysis and Robust Extraction of Changing Named Entities</title>
<affiliation confidence="0.764141">and Media Center / of Information and Computer Toyohashi University of Technology</affiliation>
<abstract confidence="0.9995305">This paper focuses on the change of named entities over time and its influence on the performance of the named entity tagger. First, we analyze Japanese named entities which appear in Mainichi Newspaper articles published in 1995, 1996, 1997, 1998 and 2005. This analysis reveals that the number of named entity types and the number of named entity tokens are steady over time and that named entity types in a certain year occur in the articles published either in its succeeding year or in its preceding These facts lead that of named entity types are replaced with new ones every year. The experiment against these texts shows that our proposing semi-supervised method which combines a small annotated corpus and a large unannotated corpus for training works robustly although the traditional supervised method is fragile against the change of name entity distribution.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
<author>Tong Zhang</author>
</authors>
<title>A highperformance semi-supervised learning method for text chunking.</title>
<date>2005</date>
<booktitle>In Proc. ofACL ’05,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="10741" citStr="Ando and Zhang, 2005" startWordPosition="1841" endWordPosition="1844">he longer time span of the annotated text increases the number of NE types. These facts lead that some NEs are shortlived and superseded by other new NEs. 3 Robust Extraction of Changing Named Entities It is infeasible to prepare a large annotated corpus which covers all increasing NEs. A semisupervised learning approach which combines a small annotated corpus and a large unannotated corpus for training is promising to cope this problem. (Miller et al., 2004) proposed the method using classes which are assigned to words based on the class language model built from a large unannotated corpus. (Ando and Zhang, 2005) pro, 163 Table 4: Number of NE types and Time Span of Annotated Text 1995 1995∼1996 1995∼1997 1995∼1998 1995∼2005 ARTIFACT 541 (1.00) 743 (1.37) 862 (1.59) 1025 (1.89) 1169 (2.16) DATE 950 (1.00) 1147 (1.21) 1326 (1.40) 1461 (1.54) 1583 (1.67) LOCATION 1403 (1.00) 1914 (1.36) 2214 (1.58) 2495 (1.78) 2692 (1.92) MONEY 301 (1.00) 492 (1.63) 570 (1.89) 656 (2.18) 749 (2.49) ORGANIZATION 1487 (1.00) 1890 (1.27) 2280 (1.53) 2566 (1.73) 2893 (1.95) PERCENT 249 (1.00) 319 (1.28) 353 (1.42) 401 (1.61) 443 (1.78) PERSON 1842 (1.00) 2540 (1.38) 3175 (1.72) 3683 (2.00) 4243 (2.30) TIME 206 (1.00) 257 (1</context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>Rie Kubota Ando and Tong Zhang. 2005. A highperformance semi-supervised learning method for text chunking. In Proc. ofACL ’05, pages 1–9, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masayuki Asahara</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese named entity extraction with redundant morphological analysis.</title>
<date>2003</date>
<booktitle>In Proc. of HLT–NAACL ’03,</booktitle>
<pages>8--15</pages>
<contexts>
<context position="14417" citStr="Asahara and Matsumoto, 2003" startWordPosition="2447" endWordPosition="2450">der to distinguish them. When the task of extracting Japanese NEs from a sentence is formalized as a chunking problem of a sequence of morphemes, the segmentation boundary problem arises as widely known. For example, the NE definition of IREX tells that a Chinese character “ (bei)” must be extracted as an NE means America from a morpheme “ (hou-bei)” which means visiting America. A naive chunker using a morpheme as a chunking unit cannot extract such a kind of NEs. In order to cope this problem, (Uchimoto et al., 2000) proposed employing translation rules to modify problematic morphemes, and (Asahara and Matsumoto, 2003; Nakano and Hirai, 2004) formalized the task of extracting NEs as a chunking problem of a sequence of characters instead of a sequence of morphemes. In this paper, we keep the naive formalization, because it is still enough to analyze the influence of 164 the text time frame. 3.2 Assignment of Similar Morpheme A context vector Vm of a morpheme m is a vector consisting of frequencies of all possible unigrams and bigrams, Vm = � � � � � f(m, m0, m0), ··· f(m, mN, mN), � � � �f(m0, m), · · · f(mN, m), � f(m, m0), · · · f(m, mN), f(m0, m0, m), ··· f(mN, mN, m) where M - {m0, m1, ... ,mN} is a set</context>
</contexts>
<marker>Asahara, Matsumoto, 2003</marker>
<rawString>Masayuki Asahara and Yuji Matsumoto. 2003. Japanese named entity extraction with redundant morphological analysis. In Proc. of HLT–NAACL ’03, pages 8–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>Beth Sundheim</author>
</authors>
<title>Message understanding conference-6: a brief history.</title>
<date>1996</date>
<booktitle>In Proc. of the 16th COLING,</booktitle>
<pages>466--471</pages>
<contexts>
<context position="1545" citStr="Grishman and Sundheim, 1996" startWordPosition="228" endWordPosition="231">e experiment against these texts shows that our proposing semi-supervised method which combines a small annotated corpus and a large unannotated corpus for training works robustly although the traditional supervised method is fragile against the change of name entity distribution. 1 Introduction It is widely agreed that extraction of named entity (henceforth, denoted as NE) is an important subtask for various NLP applications, such as information retrieval, machine translation, information extraction and natural language understanding. Several conferences like Message Understanding Conference(Grishman and Sundheim, 1996) and the IREX workshop (Sekine and Eriguchi, 2000) were conducted to encourage researchers of NE extraction and to provide its common evaluation basis. In Japanese NE extraction, it is quite common to apply morphological analysis as preprocessing stage which segments a sentence into a sequence of morphemes. After that, either a pattern matcher based on hand-crafted rules or a statistical chunker is employed to extract NEs from a sequence of morphemes. Various machine learning approaches such as maximum entropy(Uchimoto et al., 2000), decision list(Sassano and Utsuro, 2000; Isozaki, 2001), and </context>
</contexts>
<marker>Grishman, Sundheim, 1996</marker>
<rawString>Ralph Grishman and Beth Sundheim. 1996. Message understanding conference-6: a brief history. In Proc. of the 16th COLING, pages 466–471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Hideto Kazawa</author>
</authors>
<title>Efficient support vector classifiers for named entity recognition.</title>
<date>2002</date>
<booktitle>In Proc. of the 19th COLING,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="2214" citStr="Isozaki and Kazawa, 2002" startWordPosition="329" endWordPosition="332"> 2000) were conducted to encourage researchers of NE extraction and to provide its common evaluation basis. In Japanese NE extraction, it is quite common to apply morphological analysis as preprocessing stage which segments a sentence into a sequence of morphemes. After that, either a pattern matcher based on hand-crafted rules or a statistical chunker is employed to extract NEs from a sequence of morphemes. Various machine learning approaches such as maximum entropy(Uchimoto et al., 2000), decision list(Sassano and Utsuro, 2000; Isozaki, 2001), and Support Vector Machine(Yamada et al., 2002; Isozaki and Kazawa, 2002) were investigated for extracting NEs. These researches show that machine learning approaches are more promising than approaches based on hand-crafted rules if a large corpus whose NEs are properly annotated is available as training data. However, it is difficult to obtain an enough corpus in the real world because of the increasing number of NE types and the increasing time gap between the training corpus and the test corpus. There is the increasing number of NE types like personal names and company names in the real world. For example, a large database of organization names(Nichigai Associat</context>
</contexts>
<marker>Isozaki, Kazawa, 2002</marker>
<rawString>Hideki Isozaki and Hideto Kazawa. 2002. Efficient support vector classifiers for named entity recognition. In Proc. of the 19th COLING, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
</authors>
<title>Japanese named entity recognition based on a simple rule generator and decision tree learning.</title>
<date>2001</date>
<booktitle>In Proc. of ACL ’01,</booktitle>
<pages>314--321</pages>
<contexts>
<context position="2139" citStr="Isozaki, 2001" startWordPosition="320" endWordPosition="321"> and Sundheim, 1996) and the IREX workshop (Sekine and Eriguchi, 2000) were conducted to encourage researchers of NE extraction and to provide its common evaluation basis. In Japanese NE extraction, it is quite common to apply morphological analysis as preprocessing stage which segments a sentence into a sequence of morphemes. After that, either a pattern matcher based on hand-crafted rules or a statistical chunker is employed to extract NEs from a sequence of morphemes. Various machine learning approaches such as maximum entropy(Uchimoto et al., 2000), decision list(Sassano and Utsuro, 2000; Isozaki, 2001), and Support Vector Machine(Yamada et al., 2002; Isozaki and Kazawa, 2002) were investigated for extracting NEs. These researches show that machine learning approaches are more promising than approaches based on hand-crafted rules if a large corpus whose NEs are properly annotated is available as training data. However, it is difficult to obtain an enough corpus in the real world because of the increasing number of NE types and the increasing time gap between the training corpus and the test corpus. There is the increasing number of NE types like personal names and company names in the real w</context>
</contexts>
<marker>Isozaki, 2001</marker>
<rawString>Hideki Isozaki. 2001. Japanese named entity recognition based on a simple rule generator and decision tree learning. In Proc. of ACL ’01, pages 314–321.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="12946" citStr="Lafferty et al., 2001" startWordPosition="2200" endWordPosition="2203">f automatically generated auxiliary classification problems on an unannotated corpus. (?) proposed the semisupervised discriminative model whose potential function can treat both an annotated corpus and an unannotated corpus. In this paper, the method proposed by (Tsuchiya et al., 2008) is employed, because its implementation is quite easy. It consists of two steps. The first step is to assign the most similar and familiar morpheme to each unfamiliar morpheme based on their context vectors calculated from a large unannotated corpus. The second step is to employ Conditional Random Fields(CRF)2(Lafferty et al., 2001) using both features of original morphemes and features of similar morphemes. This section gives the detail of this method. 3.1 Chunking of Named Entities It is quite common that the task of extracting Japanese NEs from a sentence is formalized as a chunking problem against a sequence of morphemes. For representing proper chunks, we employ IOB2 representation, one of representations which have been studied well in various chunking 2http://chasen.org/˜taku/software/CRF+ +/ tasks of NLP (Tjong Kim Sang, 1999). This representation uses the following three labels. B Current token is the beginning </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings of ICML, pages 282– 289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Jethran Guinness</author>
<author>Alex Zamanian</author>
</authors>
<title>Name tagging with word clusters and discriminative training.</title>
<date>2004</date>
<booktitle>In Proc. of HLT-NAACL</booktitle>
<pages>337--342</pages>
<contexts>
<context position="10583" citStr="Miller et al., 2004" startWordPosition="1815" endWordPosition="1818">tring ratio(A, U&apos;) in Figure 2 is greater than 0.7. This suggests that NEs are not equivalent on their occurrence probability. And more, Table 4 shows that the longer time span of the annotated text increases the number of NE types. These facts lead that some NEs are shortlived and superseded by other new NEs. 3 Robust Extraction of Changing Named Entities It is infeasible to prepare a large annotated corpus which covers all increasing NEs. A semisupervised learning approach which combines a small annotated corpus and a large unannotated corpus for training is promising to cope this problem. (Miller et al., 2004) proposed the method using classes which are assigned to words based on the class language model built from a large unannotated corpus. (Ando and Zhang, 2005) pro, 163 Table 4: Number of NE types and Time Span of Annotated Text 1995 1995∼1996 1995∼1997 1995∼1998 1995∼2005 ARTIFACT 541 (1.00) 743 (1.37) 862 (1.59) 1025 (1.89) 1169 (2.16) DATE 950 (1.00) 1147 (1.21) 1326 (1.40) 1461 (1.54) 1583 (1.67) LOCATION 1403 (1.00) 1914 (1.36) 2214 (1.58) 2495 (1.78) 2692 (1.92) MONEY 301 (1.00) 492 (1.63) 570 (1.89) 656 (2.18) 749 (2.49) ORGANIZATION 1487 (1.00) 1890 (1.27) 2280 (1.53) 2566 (1.73) 2893 (</context>
</contexts>
<marker>Miller, Guinness, Zamanian, 2004</marker>
<rawString>Scott Miller, Jethran Guinness, and Alex Zamanian. 2004. Name tagging with word clusters and discriminative training. In Proc. of HLT-NAACL 2004, pages 337–342, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristina Mota</author>
<author>Ralph Grishman</author>
</authors>
<title>Is this NE tagger getting old?</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08),</booktitle>
<contexts>
<context position="3193" citStr="Mota and Grishman, 2008" startWordPosition="493" endWordPosition="496">nd the increasing time gap between the training corpus and the test corpus. There is the increasing number of NE types like personal names and company names in the real world. For example, a large database of organization names(Nichigai Associates, 2007) already contains 171,708 types and is still increasing. Because annotation work is quite expensive, the annotated corpus may become obsolete in a short period of time. Both of two factors expands the difference of NE distribution between the training corpus and the test corpus, and it may decrease the performance of the NE tagger as shown in (Mota and Grishman, 2008). Therefore, a robust method to extract NEs which do not occur or occur few times in a training corpus is necessary. This paper focuses on the change of NEs over time and its influence on the performance of the NE tagger. First, we annotate NEs in Mainichi Newspaper articles published in 1996, 1997, 1998 and 2005, and analyze NEs which appear in these texts and an existing corpus. It consists of Mainichi Newspaper articles published in 1995, thus, we get an annotated corpus that spans 10 years. This analysis reveals that the number of NE types and the number of NE tokens are almost 161 Proceed</context>
<context position="8818" citStr="Mota and Grishman, 2008" startWordPosition="1480" endWordPosition="1483">o IREX corpus, and other columns are corresponding to articles annotated by ourselves. Table 2 illustrates that the normalized number of NE types and the normalized number of NE tokens are almost steady over time. Figure 1 shows the distributions of NE categories for sampling texts and that there is no significant difference between them. We also investigate the relation of the time gap between texts and NE types which appear in these texts. The overlap ratio of NE types between the annotated corpus A published in the year YA and the annotated corpus B published in the year YB was defined in (Mota and Grishman, 2008) as follows |TA n TB| type overlap(A, B) = |TA |+ |TB |- |TA n TB|, where TA and TB are lists of NE types which appear in A and B respectively. However, it is impossible to compute reliable type overlap in our research because enough annotated texts are unavailable. As an alternative of type overlap, the overlap ratio of NE types between the annotated corpus A and the unannotated corpus U published in the year YU is defined as follows ∑ ���� S(s, U) string overlap(A, U) = |TA| where S(s, U) is the binary function to indicate whether the string s occurs in the string U or not. Table 3 shows str</context>
</contexts>
<marker>Mota, Grishman, 2008</marker>
<rawString>Cristina Mota and Ralph Grishman. 2008. Is this NE tagger getting old? In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08), May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keigo Nakano</author>
<author>Yuzo Hirai</author>
</authors>
<title>Japanese named entity extraction with bunsetsu features.</title>
<date>2004</date>
<journal>Transactions of Information Processing Society of Japan,</journal>
<volume>45</volume>
<issue>3</issue>
<note>(in Japanese).</note>
<contexts>
<context position="14442" citStr="Nakano and Hirai, 2004" startWordPosition="2451" endWordPosition="2454"> the task of extracting Japanese NEs from a sentence is formalized as a chunking problem of a sequence of morphemes, the segmentation boundary problem arises as widely known. For example, the NE definition of IREX tells that a Chinese character “ (bei)” must be extracted as an NE means America from a morpheme “ (hou-bei)” which means visiting America. A naive chunker using a morpheme as a chunking unit cannot extract such a kind of NEs. In order to cope this problem, (Uchimoto et al., 2000) proposed employing translation rules to modify problematic morphemes, and (Asahara and Matsumoto, 2003; Nakano and Hirai, 2004) formalized the task of extracting NEs as a chunking problem of a sequence of characters instead of a sequence of morphemes. In this paper, we keep the naive formalization, because it is still enough to analyze the influence of 164 the text time frame. 3.2 Assignment of Similar Morpheme A context vector Vm of a morpheme m is a vector consisting of frequencies of all possible unigrams and bigrams, Vm = � � � � � f(m, m0, m0), ··· f(m, mN, mN), � � � �f(m0, m), · · · f(mN, m), � f(m, m0), · · · f(m, mN), f(m0, m0, m), ··· f(mN, mN, m) where M - {m0, m1, ... ,mN} is a set of all morphemes of the </context>
</contexts>
<marker>Nakano, Hirai, 2004</marker>
<rawString>Keigo Nakano and Yuzo Hirai. 2004. Japanese named entity extraction with bunsetsu features. Transactions of Information Processing Society of Japan, 45(3):934–941, Mar. (in Japanese).</rawString>
</citation>
<citation valid="false">
<editor>Nichigai Associates, editor. 2007. DCS Kikan-mei Jisho. Nichigai Associates. (in Japanese).</editor>
<marker></marker>
<rawString>Nichigai Associates, editor. 2007. DCS Kikan-mei Jisho. Nichigai Associates. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manabu Sassano</author>
<author>Takehito Utsuro</author>
</authors>
<title>Named entity chunking techniques in supervised learning for japanese named entity recognition.</title>
<date>2000</date>
<booktitle>In Proc. of the 18th COLING,</booktitle>
<pages>705--711</pages>
<contexts>
<context position="2123" citStr="Sassano and Utsuro, 2000" startWordPosition="316" endWordPosition="319">anding Conference(Grishman and Sundheim, 1996) and the IREX workshop (Sekine and Eriguchi, 2000) were conducted to encourage researchers of NE extraction and to provide its common evaluation basis. In Japanese NE extraction, it is quite common to apply morphological analysis as preprocessing stage which segments a sentence into a sequence of morphemes. After that, either a pattern matcher based on hand-crafted rules or a statistical chunker is employed to extract NEs from a sequence of morphemes. Various machine learning approaches such as maximum entropy(Uchimoto et al., 2000), decision list(Sassano and Utsuro, 2000; Isozaki, 2001), and Support Vector Machine(Yamada et al., 2002; Isozaki and Kazawa, 2002) were investigated for extracting NEs. These researches show that machine learning approaches are more promising than approaches based on hand-crafted rules if a large corpus whose NEs are properly annotated is available as training data. However, it is difficult to obtain an enough corpus in the real world because of the increasing number of NE types and the increasing time gap between the training corpus and the test corpus. There is the increasing number of NE types like personal names and company nam</context>
</contexts>
<marker>Sassano, Utsuro, 2000</marker>
<rawString>Manabu Sassano and Takehito Utsuro. 2000. Named entity chunking techniques in supervised learning for japanese named entity recognition. In Proc. of the 18th COLING, pages 705–711.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
<author>Yoshio Eriguchi</author>
</authors>
<title>Japanese named entity extraction evaluation: analysis of results.</title>
<date>2000</date>
<booktitle>In Proc. of the 18th COLING,</booktitle>
<pages>1106--1110</pages>
<contexts>
<context position="1595" citStr="Sekine and Eriguchi, 2000" startWordPosition="236" endWordPosition="239">osing semi-supervised method which combines a small annotated corpus and a large unannotated corpus for training works robustly although the traditional supervised method is fragile against the change of name entity distribution. 1 Introduction It is widely agreed that extraction of named entity (henceforth, denoted as NE) is an important subtask for various NLP applications, such as information retrieval, machine translation, information extraction and natural language understanding. Several conferences like Message Understanding Conference(Grishman and Sundheim, 1996) and the IREX workshop (Sekine and Eriguchi, 2000) were conducted to encourage researchers of NE extraction and to provide its common evaluation basis. In Japanese NE extraction, it is quite common to apply morphological analysis as preprocessing stage which segments a sentence into a sequence of morphemes. After that, either a pattern matcher based on hand-crafted rules or a statistical chunker is employed to extract NEs from a sequence of morphemes. Various machine learning approaches such as maximum entropy(Uchimoto et al., 2000), decision list(Sassano and Utsuro, 2000; Isozaki, 2001), and Support Vector Machine(Yamada et al., 2002; Isozak</context>
<context position="4867" citStr="Sekine and Eriguchi, 2000" startWordPosition="783" endWordPosition="786">n the articles published either in its succeeding year or in its preceding year. These facts lead that 20 — 30% of named entity types are replaced with new ones every year. The experiment against these corpora shows that the traditional supervised method is fragile against the change of NE types and that our proposing semi-supervised method which combines a small annotated corpus and a large unannotated corpus for training is robust against the change of NE types. String overlap 2 Analysis of Changing Named Entities 2.1 Task of the IREX Workshop The task of NE extraction of the IREX workshop (Sekine and Eriguchi, 2000) is to recognize eight NE categories in Table 1. The organizer of the IREX workshop provided a training corpus (henceforth, denoted as IREX corpus), which consists of 1,174 Mainichi Newspaper articles published from January 1st 1995 to 10th which include 18,677 NEs. In the Japanese language, no other corpora whose NEs are annotated are publicly available as far as we know.&apos; Thus, IREX corpus is referred as a golden sample of NE distribution in this paper. 2.2 Data Description The most homogeneous texts which are written in different days are desirable, to explore the influence of the text time</context>
</contexts>
<marker>Sekine, Eriguchi, 2000</marker>
<rawString>Satoshi Sekine and Yoshio Eriguchi. 2000. Japanese named entity extraction evaluation: analysis of results. In Proc. of the 18th COLING, pages 1106– 1110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Tjong Kim Sang</author>
</authors>
<title>Representing text chunks.</title>
<date>1999</date>
<booktitle>In Proc. of the 9th EACL,</booktitle>
<pages>173--179</pages>
<contexts>
<context position="13458" citStr="Sang, 1999" startWordPosition="2281" endWordPosition="2282">notated corpus. The second step is to employ Conditional Random Fields(CRF)2(Lafferty et al., 2001) using both features of original morphemes and features of similar morphemes. This section gives the detail of this method. 3.1 Chunking of Named Entities It is quite common that the task of extracting Japanese NEs from a sentence is formalized as a chunking problem against a sequence of morphemes. For representing proper chunks, we employ IOB2 representation, one of representations which have been studied well in various chunking 2http://chasen.org/˜taku/software/CRF+ +/ tasks of NLP (Tjong Kim Sang, 1999). This representation uses the following three labels. B Current token is the beginning of a chunk. I Current token is a middle or the end of a chunk consisting of more than one token. O Current token is outside of any chunk. Actually, we prepare the 16 derived labels from the label B and the label I for eight NE categories, in order to distinguish them. When the task of extracting Japanese NEs from a sentence is formalized as a chunking problem of a sequence of morphemes, the segmentation boundary problem arises as widely known. For example, the NE definition of IREX tells that a Chinese char</context>
</contexts>
<marker>Sang, 1999</marker>
<rawString>E. Tjong Kim Sang. 1999. Representing text chunks. In Proc. of the 9th EACL, pages 173–179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masatoshi Tsuchiya</author>
<author>Shinya Hida</author>
<author>Seiichi Nakagawa</author>
</authors>
<title>Robust extraction of named entity including unfamiliar word.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL08: HLT, Short Papers,</booktitle>
<pages>125--128</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="12611" citStr="Tsuchiya et al., 2008" startWordPosition="2146" endWordPosition="2149">ticle (0, 1, 0, 0, 0, 0) O (tenki) (weather) Noun–Generic (tenki) (weather) Noun–Generic (1, 0, 0, 0, 0, 0) O (ha) top Particle (ha) top Particle (0, 1, 0, 0, 0, 0) O (hare) (fine) Noun–Generic (hare) (fine) Noun–Generic (1, 1, 0, 0, 0, 0) O Figure 3: Example of Training Instance for Proposed Method posed the method using thousands of automatically generated auxiliary classification problems on an unannotated corpus. (?) proposed the semisupervised discriminative model whose potential function can treat both an annotated corpus and an unannotated corpus. In this paper, the method proposed by (Tsuchiya et al., 2008) is employed, because its implementation is quite easy. It consists of two steps. The first step is to assign the most similar and familiar morpheme to each unfamiliar morpheme based on their context vectors calculated from a large unannotated corpus. The second step is to employ Conditional Random Fields(CRF)2(Lafferty et al., 2001) using both features of original morphemes and features of similar morphemes. This section gives the detail of this method. 3.1 Chunking of Named Entities It is quite common that the task of extracting Japanese NEs from a sentence is formalized as a chunking proble</context>
</contexts>
<marker>Tsuchiya, Hida, Nakagawa, 2008</marker>
<rawString>Masatoshi Tsuchiya, Shinya Hida, and Seiichi Nakagawa. 2008. Robust extraction of named entity including unfamiliar word. In Proceedings of ACL08: HLT, Short Papers, pages 125–128, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyotaka Uchimoto</author>
<author>Ma Qing</author>
<author>Masaki Murata</author>
<author>Hiromi Ozaku</author>
<author>Masao Utiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Named entity extraction based on a maximum entropy model and transformation rules.</title>
<date>2000</date>
<journal>Journal of Natural Language Processing,</journal>
<volume>7</volume>
<issue>2</issue>
<note>(in Japanese).</note>
<contexts>
<context position="2083" citStr="Uchimoto et al., 2000" startWordPosition="311" endWordPosition="314">veral conferences like Message Understanding Conference(Grishman and Sundheim, 1996) and the IREX workshop (Sekine and Eriguchi, 2000) were conducted to encourage researchers of NE extraction and to provide its common evaluation basis. In Japanese NE extraction, it is quite common to apply morphological analysis as preprocessing stage which segments a sentence into a sequence of morphemes. After that, either a pattern matcher based on hand-crafted rules or a statistical chunker is employed to extract NEs from a sequence of morphemes. Various machine learning approaches such as maximum entropy(Uchimoto et al., 2000), decision list(Sassano and Utsuro, 2000; Isozaki, 2001), and Support Vector Machine(Yamada et al., 2002; Isozaki and Kazawa, 2002) were investigated for extracting NEs. These researches show that machine learning approaches are more promising than approaches based on hand-crafted rules if a large corpus whose NEs are properly annotated is available as training data. However, it is difficult to obtain an enough corpus in the real world because of the increasing number of NE types and the increasing time gap between the training corpus and the test corpus. There is the increasing number of NE t</context>
<context position="14314" citStr="Uchimoto et al., 2000" startWordPosition="2434" endWordPosition="2437">, we prepare the 16 derived labels from the label B and the label I for eight NE categories, in order to distinguish them. When the task of extracting Japanese NEs from a sentence is formalized as a chunking problem of a sequence of morphemes, the segmentation boundary problem arises as widely known. For example, the NE definition of IREX tells that a Chinese character “ (bei)” must be extracted as an NE means America from a morpheme “ (hou-bei)” which means visiting America. A naive chunker using a morpheme as a chunking unit cannot extract such a kind of NEs. In order to cope this problem, (Uchimoto et al., 2000) proposed employing translation rules to modify problematic morphemes, and (Asahara and Matsumoto, 2003; Nakano and Hirai, 2004) formalized the task of extracting NEs as a chunking problem of a sequence of characters instead of a sequence of morphemes. In this paper, we keep the naive formalization, because it is still enough to analyze the influence of 164 the text time frame. 3.2 Assignment of Similar Morpheme A context vector Vm of a morpheme m is a vector consisting of frequencies of all possible unigrams and bigrams, Vm = � � � � � f(m, m0, m0), ··· f(m, mN, mN), � � � �f(m0, m), · · · f(</context>
</contexts>
<marker>Uchimoto, Qing, Murata, Ozaku, Utiyama, Isahara, 2000</marker>
<rawString>Kiyotaka Uchimoto, Ma Qing, Masaki Murata, Hiromi Ozaku, Masao Utiyama, and Hitoshi Isahara. 2000. Named entity extraction based on a maximum entropy model and transformation rules. Journal of Natural Language Processing, 7(2):63–90, Apr. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese named entity extraction using support vector machine.</title>
<date>2002</date>
<journal>Transactions of Information Processing Society of Japan,</journal>
<volume>43</volume>
<issue>1</issue>
<note>(in Japanese).</note>
<contexts>
<context position="2187" citStr="Yamada et al., 2002" startWordPosition="325" endWordPosition="328">(Sekine and Eriguchi, 2000) were conducted to encourage researchers of NE extraction and to provide its common evaluation basis. In Japanese NE extraction, it is quite common to apply morphological analysis as preprocessing stage which segments a sentence into a sequence of morphemes. After that, either a pattern matcher based on hand-crafted rules or a statistical chunker is employed to extract NEs from a sequence of morphemes. Various machine learning approaches such as maximum entropy(Uchimoto et al., 2000), decision list(Sassano and Utsuro, 2000; Isozaki, 2001), and Support Vector Machine(Yamada et al., 2002; Isozaki and Kazawa, 2002) were investigated for extracting NEs. These researches show that machine learning approaches are more promising than approaches based on hand-crafted rules if a large corpus whose NEs are properly annotated is available as training data. However, it is difficult to obtain an enough corpus in the real world because of the increasing number of NE types and the increasing time gap between the training corpus and the test corpus. There is the increasing number of NE types like personal names and company names in the real world. For example, a large database of organizat</context>
</contexts>
<marker>Yamada, Kudo, Matsumoto, 2002</marker>
<rawString>Hiroyasu Yamada, Taku Kudo, and Yuji Matsumoto. 2002. Japanese named entity extraction using support vector machine. Transactions of Information Processing Society of Japan, 43(1):44–53, Jan. (in Japanese).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>