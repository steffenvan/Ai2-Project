<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000121">
<title confidence="0.99336">
Named Entity Recognition for Ukrainian: A Resource-Light Approach
</title>
<author confidence="0.921843">
Sophia Katrenko
</author>
<affiliation confidence="0.901875">
HCSL, University of Amsterdam,
</affiliation>
<address confidence="0.6966875">
Kruislaan 419, 1098VA Amsterdam,
the Netherlands
</address>
<email confidence="0.994153">
katrenko@science.uva.nl
</email>
<author confidence="0.449724">
Pieter Adriaans
</author>
<affiliation confidence="0.538302">
HCSL, University of Amsterdam,
</affiliation>
<address confidence="0.4945">
Kruislaan 419, 1098VA Amsterdam,
the Netherlands
</address>
<email confidence="0.993833">
pitera@science.uva.nl
</email>
<sectionHeader confidence="0.998566" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994996">
Named entity recognition (NER) is a subtask
of information extraction (IE) which can be
used further on for different purposes. In this
paper, we discuss named entity recognition
for Ukrainian language, which is a Slavonic
language with a rich morphology. The ap-
proach we follow uses a restricted number of
features. We show that it is feasible to boost
performance by considering several heuris-
tics and patterns acquired from the Web data.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.978683195652174">
The information extraction task has proved to be dif-
ficult for a variety of domains (Riloff, 1995). The
extracted information can further be used for ques-
tion answering, information retrieval and other ap-
plications. Depending on the final purpose, the ex-
tracted information can be of different type, e.g.,
temporal events, locations, etc. The information cor-
responding to locations and names, is referred to as
the information about named entities. Hence, named
entity recognition constitutes a subtask of the infor-
mation extraction in general.
It is especially challenging to extract the named
entities from the text sources written in languages
other than English which, in practice, is supported
by the results of the shared tasks on the named entity
recognition (Tjong Kim Sang, 2002).
Named entity recognition for the languages with
a rich morphology and a free word order is difficult
because of several reasons. The entropy of texts in
such languages is usually higher than the entropy
of English texts. It is either needed to use such
resources as morphological analyzers to reduce the
data sparseness or to annotate a large amount of data
in order to obtain a good performance. Luckily, the
free word order is not crucial for the named entity
recognition task as the local context of a named en-
tity should be sufficient for its detection. Besides,
a free word order usually implies a free order of
constituents (such as noun phrases or verb phrases)
rather than words as such. For instance, although
(1)1 is grammatically correct and can occur in the
data, it would be less frequent than (2).
The first phrase exemplifies that an adjective
is in a focus, whereas the second reflects
the word order which is more likely to occur. In
terms of named entities, an entity consisting of sev-
eral words is also less likely to be split (consider,
e.g., National saw she Bank where ’National Bank’
represents one named entity of type organization).
In the newspaper corpus we annotated, we have ob-
served no examples of split named entities.
In this paper, we study different data represen-
tation and machine learning methods to extract the
named entities from text. Our goal is two-fold. First,
&apos;all examples in the paper are in Ukrainian, for convenience
translated and sometimes transliterated
</bodyText>
<page confidence="0.984458">
88
</page>
<subsubsectionHeader confidence="0.732297">
Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 88–93,
</subsubsectionHeader>
<bodyText confidence="0.996533090909091">
Prague, June 2007. c�2007 Association for Computational Linguistics
we explore the possibility of using patterns induced
from the data gathered on the Web. We also con-
sider Levenshtein distance to find the most similar
instances in the test data given a training set. Be-
sides, we study the impact of different feature sets
on the resulting classification performance.
We start with the short overview of the methods
for NER proposed by the IE community. After-
wards, the experiments are described. We conclude
with the outlook for the future work.
</bodyText>
<sectionHeader confidence="0.999558" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999305307692308">
The existing NER systems use many sources in or-
der to be able to extract NEs from the text data.
Some of them rely on hand-written rules and pre-
compiled lists of city names, person names and
other NEs in a given language, while others explore
methods to automatically extract NEs without prior
knowledge. In the first case, the gazetteers will in
most cases improve NER results (Carreras et al.,
2002) but, unfortunately, they may not exist for a
language one is working on. Hand-written rules can
also cover more NEs but building such patterns will
be a very time-consuming process.
There have been many methods applied to NER,
varying from the statistical to the memory-based ap-
proaches. Most work on NER has been focused on
English but there are also approaches to other lan-
guages such as Spanish (Kozareva et al., 2005), Ger-
man, or Dutch. In addition, several competitions
have been organized, with a focus on multilingual
NER (Tjong Kim Sang, 2002). While analyzing
the results of these shared tasks, it can be concluded
that the selected features are of a great importance.
In our view, they can be categorized in two types,
i.e. contextual and orthographic 2. The first type
includes words surrounding a given word while the
other contains such features as capitalized letters,
digits contained within the word, etc. Both types
of features contribute to the information extraction
task. Nevertheless, orthographic features can al-
ready be language-specific. For instance, capitaliza-
tion is certainly very important for such languages
as English or Dutch but it might be less useful for
German.
2Sometimes, these types of features are referred to as word-
external and word-internal (Klein et al., 2003)
The feature set of some NER methods (Wu, 2002)
also includes part-of-speech information and/or
word prefixes and suffixes. Although this informa-
tion (and especially lemmas) is very useful for the
languages with rich morphology, it presupposes the
existence of POS taggers for a given language.
Another conclusion which can be drawn relates to
the machine learning approaches. The best results
have been received by applying ensemble methods
(Wu, 2002; Florian, 2002; Carreras et al., 2002).
A very interesting work on named entity recogni-
tion task was reported by Collins et al. (1999) who
used only few named entities to bootstrap more. The
other approach proposed recently makes use of the
data extracted from the Web (Talukdar et al., 2006).
By restricting themselves to the fixed context of the
extracted named entities and by employing grammar
inference techniques, the authors filter out the most
useful patterns. As they show, by applying such ap-
proach precision can already be largely boosted.
Pastra et al. (2002) focused on the applicability
of already existing resources in one language to an-
other. Their case study was based on English and
Romanian, where a system, originally developed for
NER in English was adapted to Romanian. Their
results suggest that such adaptation is easier than
developing a named entity recognition system for
Romanian from scratch. However, the authors also
mention that not all phenomena in Romanian have
been taken into account which resulted in low recall.
</bodyText>
<sectionHeader confidence="0.998348" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.999943">
Ukrainian belongs to the languages where the named
entities are usually capitalized, which makes their
detection relatively easy. In this paper we focus
on using minimal information about the language in
combination with the patterns learnt from the Web
data, features extracted from the corpus and Leven-
shtein similarity measure.
Our hypothesis behind all three components is the
following. We expect orthographic features be use-
ful for a named entity detection but not sufficient
for its classification. Contextual information may
already help but as we do not intend to use lemmas
but words instead, it will likely not boost recall of the
named entity recongnition. To be able to detect more
named enities in the text, we propose to use pat-
</bodyText>
<page confidence="0.99884">
89
</page>
<bodyText confidence="0.999970076923077">
terns collected from the Web and Levenshtein sim-
ilarity measure. Patterns from the Web should pro-
vide more contextual information than can be found
in a corpus. In addition, a similarity measure gives
us an opportunity to detect the named entities which
have the same stem. The latter is especially useful
when the same entity was mentioned in the training
set as well as in the test data but its flections differ.
The intention of our study is, therefore, to start
with a standard set of features (contextual and ortho-
graphic) as used for the many languages in the past
and to add some means which would account for the
fact that Ukrainian is a highly-inflected language.
</bodyText>
<subsectionHeader confidence="0.99479">
3.1 Classification
</subsectionHeader>
<bodyText confidence="0.999836833333333">
First, we consider the features which can be easily
extracted given the data, such as contextual and or-
thographic ones as described below in Table 1. For
each word in a corpus its context (2 tokens to left
and to the right) and its orthographic features are
extracted. Orthographic features are binary features
which, for instance, indicate whether a word is cap-
italized (1 or 0), etc. We have selected the following
machine learning methods: k-nearest neighbor (knn)
and voting and stacking as the ensemble methods
which have been successfully applied to the named
entity recognition task in the past.
</bodyText>
<table confidence="0.459895428571429">
contextual -2/+2 words
orthographic
CAP capitalized
ALLCAP all elements of a token capitalized
BSENT first token in a sentence
NUM contains digits
QUOTE contains quotes
</table>
<tableCaption confidence="0.996724">
Table 1: Features
</tableCaption>
<bodyText confidence="0.999135666666667">
To overcome data sparseness and to increase re-
call, we make use of two techniques. First, we apply
the patterns extracted from the Web.
</bodyText>
<subsectionHeader confidence="0.998405">
3.2 Patterns
</subsectionHeader>
<bodyText confidence="0.999977942857143">
If we wish to collect patterns for a certain category C
of the named entities (e.g.), we first collect all named
entities that fall into it. Then, for each X E C,
we use X as a query term for Google (for this pur-
pose we used the Google API). The queries we con-
structed were mainly based on the locations, such as
etc. For
each of these words we created queries by declin-
ing them (as there are 7 cases in Ukrainian language
which causes the high variability). Consequently,
we get many snippets where X occurs. To extract
patterns from snippets, we fix a context and use 2
words to the left and to the right of X as in the
classification approach above. The patterns which
only consist of a named entity, closed-class words
(e.g., prepositions, conjunctions, etc.) and punctua-
tion are removed as such that do not provide enough
evidence to classify an instance.
Intuitively, if there are many patterns acquired
from the large collection of data on the Web, they
must be sufficient (in some sense even redundant)
to recognize named entities in a text. For instance,
such pattern as was located in X in English can cor-
respond to three patterns in Ukrainian was located
(fem., sing.) in X, was located (masc., sing.) in X,
was located (neut., sing.) in X. Even though these
patterns could be embraced in one, we are rather in-
terested in collecting all possible patterns avoiding
this way stemming and morphological analysis.
As in Talukdar’s approach (Talukdar et al., 2006),
we expect patterns to provide high precision. We
are, however, concerned about the size of Ukrainian
Web which is much smaller than English part of the
Web. As a consequence, it is not clear whether recall
can be improved much by using the Web data.
</bodyText>
<subsectionHeader confidence="0.998695">
3.3 Levenshtein distance
</subsectionHeader>
<bodyText confidence="0.998876">
Yet another approach to address rich morphology of
Ukrainian, is to carry out a matching of probable
named entities in a test set against a list of named
entities in a training set. It can be done by using
string edit distances, such as Levenshtein.
Levenshtein (or edit) distance of two strings, x
and y is measured as the minimal number of in-
sertions, deletions, or substitutions to transform one
string into the other. Levenshtein distance has be-
come popular in the natural language processing
field and was used for the variety of tasks (e.g., se-
mantic role labeling ).
</bodyText>
<construct confidence="0.611064666666667">
Definition 1 (Levenshtein distance) Given two sequences
x = x1x2 ... x. and y = y1y2 ... y,,,, of a length n and m
respectively, Levenshtein distance is defined as follows
</construct>
<equation confidence="0.88067325">
lev(i − 1, j − 1) + d(xi, yj)
lev(i − 1, j) + 1
lev(i, j − 1) + 1
lev(i, j) = min
</equation>
<page confidence="0.943637">
90
</page>
<bodyText confidence="0.9995665">
In the definition above, d(xi, yj) is a cost of sub-
stituting one symbol in x by a symbol from y. The
insertion and deletion costs are equal to 1.
Let A be a candidate named entity and L a list
of all named entities found in the training set. By
computing the Levenshtein distance between A and
each element from L, the nearest neighbor to A will
be a NE with the lowest Levenshtein score. It might,
however, happen that there are no named entities in
a training set that correspond to the candidate in a
test set. Consider, for instance the Levenshtein dis-
tance of two words (George) and
(besides) which is equal to 2. Even though the dis-
tance is low, we do not wish to classify as
a named entity whose type is PERSON because it is
simply a preposition. The problem we described can
be solved in several ways. On the one hand, it is pos-
sible to use a list of stop words with most frequent
prepositions, conjunctions and pronouns listed. On
the other hand, we can also set a threshold for the
Levenshtein distance. In the experiments we present
below, we avoid setting threshold by using a simple
heuristics. We align the first letters of A with its
nearest neighbor. If they do not match (as in exam-
ple above), we conclude that no variants of A belong
to the training set.
</bodyText>
<sectionHeader confidence="0.997184" genericHeader="evaluation">
4 Experiments and Evaluation
</sectionHeader>
<bodyText confidence="0.999758166666667">
We have conducted three types of experiments us-
ing different feature sets, patterns extracted from the
Web and Levenshtein distance. We expect that both
types of experiments can shed a light on usefulness
of the features that we defined for NER on Ukrainian
data.
</bodyText>
<subsectionHeader confidence="0.985356">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.992482111111111">
Initially, several articles of the newspaper Mir-
ror Weekly (year 2005)3 were annotated. During
the annotating process we considered the following
named instances: PERSON (person names), LOC
(location), ORG (organization).In total, there were
10,000 tokens annotated, 514 of which are named
entities. All named entities have been annotated ac-
cording to the IOB annotation scheme (Ramshaw
and Marcus, 1995). The annotated corpus can
Scan be found at http://www.zn.kiev.ua
be downloaded from http://www.science.
uva.nl/˜katrenko/Corpus
The corpus was split into training and test sets of
6,606 and 3,397 tokens, respectively. The corpus is
relatively small but we hope to study whether such
features as orthographic are sufficient for the NER
task alone or it is needed to add more sources to ap-
proach this task.
</bodyText>
<subsectionHeader confidence="0.976073">
4.2 Classification
</subsectionHeader>
<bodyText confidence="0.999420777777778">
The results of our experiments on classification of
named entities are provided in Table 2. Baseline
B1 was defined by the most frequent tag in the data
(ORG). Similarly to Conll shared task (Tjong Kim
Sang, 2002), we also calculated a baseline by tag-
ging all named entities which occurred in the train-
ing set (B2). Although there are many names of or-
ganizations detected, there are only 1,92% of person
names recognized.
</bodyText>
<table confidence="0.999751466666667">
precision recall F-score
B1 0.32 0.32 0.32
B2 0.29 0.18 0.22
M2−knn 0.31 0.44 0.36
ortho 0.38 0.46 0.42
M2−knn 0.47 0.38 0.42
ortho+cont 0.40 0.43 0.41
MV oting 0.46 0.39 0.42
ortho+cont 0.50 0.46 0.48
MStacking
ortho+cont
MV oting
ortho+cont+pat
MV oting
ortho+cont+pat+lev
</table>
<tableCaption confidence="0.999727">
Table 2: Experiments: precision and recall
</tableCaption>
<bodyText confidence="0.92933685">
Since we are interested in how much each type
of the feature sets contributes to the classification
accuracy, we have conducted experiments on con-
textual features only, on orthographic features only
(model M2−knn in Table 2) and on the combina-
ortho
tions of both (model M2−knn in Table 2). When
ortho+cont
used alone, contextual features do not provide a
high performance. However, their combination with
the orthographic features already results in a higher
precision (at expense of recall) and in a higher F-
score. It is worth noting that all results given in
Table 2 were obtained either by using memory-
based learning (in particular, k-nearest neighbor as
in M2−kon and in M2−knnortho+cont) or by ensemble
orthmethods (as in MVoting and MStacking ) The
ortho+cont ortho+cont
latter option was particularly interesting to explore
because it proved to provide accurate results for the
</bodyText>
<page confidence="0.997343">
91
</page>
<bodyText confidence="0.98770725">
named entity recognition task in the past. The re-
sults in Table 2 also seem to support a claim that
the ensemble methods perform better. It can be
seen when comparing M2�knn
ortho+cont, MV oting
ortho+cont and
MStacking
ortho+cont. Despite of using the same feature sets,
Voting (based on Naive Bayes, decision trees and 2-
knn) and Stacking (2-knn as a meta-learner applied
to Naive Bayes and decision tree learner) both pro-
vide higher precision but lower recall.
By using x2 test on the training set, we deter-
mined which attributes are the most informative for
the classification task. The most informative turned
out to be a word itself followed by the surround-
ing context (one token to the right and to the left).
The least informative feature is NUM, apparently
because there have been not many named entities
containing digits.
</bodyText>
<subsectionHeader confidence="0.998098">
4.3 Patterns
</subsectionHeader>
<bodyText confidence="0.996583315789474">
As a next step, we employed the patterns extracted
from the Web data. Some of the patterns accom-
panied with the translation and information on case
are given in Table 3. It can be noticed that not all
of the patterns are accurate. For instance, a pattern
together with a city mayor LOC can also be used to
extract a name of a mayor (hence, PERSON) and
not a location (LOC). Patterns containing preposi-
tions (so, mostly patterns containing a named entity
in locative case) ’in’, ’with’, ’nearby’ are usually
more accurate but they still require additional con-
text (as a word ’town’ in in a little town LOC).
The results we obtained by employing such pat-
terns did not significantly change the overall perfor-
mance (Table 2, model MV oting ) However,
ortho+cont+pat
the performance on some categories such as ORG
or LOC (Table 5 and Table 6, model ALL+P) was
positively affected in terms of F-score.
</bodyText>
<subsectionHeader confidence="0.999664">
4.4 Levenshtein distance
</subsectionHeader>
<bodyText confidence="0.99961115">
Finally, we compare all capitalized words in a test
set against the named entities found in the train-
ing data. The first 6 examples in Table 4 show the
same nouns but in different cases. The distance in
each case is equal 1. Since we did not carry out
the morphological analysis of the corpus, many such
occurrences of the named entities in the test data
were found given the information from the training
set using orthographic and contextual features only
(as they do not match exactly). However, Leven-
shtein distance helps to identify the variants of the
same named entity. The results of applying Leven-
shtein distance (together with the patterns and Vot-
ing model on all features) for each category are
given in Table 5 and Table 6 (model ALL+P+L).
LOC and ORG are two categories whose perfor-
mance is greatly improved by using Levenshtein dis-
tance. In case of PERSON category, recall gets
slightly higher, whereas precision does not change
much.
</bodyText>
<figure confidence="0.76799325">
PATTERN case
locative
in a little town LOC
instrumental
with a city LOC
genitive
a map of LOC
instrumental
together with a city mayor LOC
vocative
my dear/native LOC
locative
in LOC was found
instrumental
travelling in LOC
lives somewhere nearby LOC
</figure>
<tableCaption confidence="0.983094">
Table 3: Patterns for LOC category
</tableCaption>
<bodyText confidence="0.991255">
The last three examples in Table 4 are very in-
teresting. They show that sometimes the nearest
neighbor of the candidates for NEs in the test data
is a named entity of the same category but it can-
not be found by aligning. Having noticed this, we
decided to exclude aligning step and to consider a
nearest neighbor of every capitalized token in the
test set. Although we extracted few novel person
names and locations, performance in terms of preci-
sion dropped significantly. The very last example in
Table 4 demonstrates a case when applying Leven-
shtein measure fails. In this case is of type
ORG (a political party) and are people
who belong to the party. Given the nearest neighbor
and the successful alignment, it is predicted that
belongs to the category ORG but it is
not true. In the other example involving the same
entity is correctly classified as
ORG (it is the same named entity as in the training
data but in dative case).
instrumental
</bodyText>
<page confidence="0.913445">
92
</page>
<bodyText confidence="0.99944025">
It can be concluded that, in general, Leven-
shtein distance helps to identify many named enti-
ties which were either misclassified or not detected
at all. However, it is sometimes unable to distinguish
between the variant of the same named entity and
a true negative. Additional constraints such as the
upper threshold of the Levenshtein distance might
solve this problem.
</bodyText>
<tableCaption confidence="0.681094909090909">
Category Test set Training set L-score
PERSON 1
PERSON 1
ORG 1
LOC 1
LOC 1
PERSON 2
PERSON 3
PERSON 4
WRONG 4
Table 4: The nearest neighbors
</tableCaption>
<bodyText confidence="0.9867922">
As can be seen from Table 2, the best overall per-
formance is achieved by combining contextual and
orthographic features together with the patterns ex-
tracted from the Web and entities classified by em-
ploying the Levenshtein distance.
</bodyText>
<table confidence="0.9987816">
Model PERSON LOC ORG
ORTHO 0.25 0.34 0.52
ALL 0.47 0.37 0.49
ALL+P 0.48 0.31 0.47
ALL+P+L 0.49 0.55 0.51
</table>
<tableCaption confidence="0.998355">
Table 5: Performance on each category: precision
</tableCaption>
<table confidence="0.9997066">
Model PERSON LOC ORG
ORTHO 0.49 0.26 0.42
ALL 0.36 0.15 0.51
ALL+P 0.36 0.27 0.49
ALL+P+L 0.42 0.49 0.56
</table>
<tableCaption confidence="0.999441">
Table 6: Performance on each category: recall
</tableCaption>
<sectionHeader confidence="0.997137" genericHeader="conclusions">
5 Conclusions and Future work
</sectionHeader>
<bodyText confidence="0.999888846153846">
In this paper, we focused on standard features used
for the named entity recognition on the newswire
data which have been used on many languages. To
improve the results that we get by employing ortho-
graphic and contextual features, we add patterns ex-
tracted from the Web and use a similarity measure
to find the named entities similar to the NEs in the
training set. The results we received are, in general,
lower than the performance of NER systems in other
languages but higher than both baselines. The for-
mer might be explained by the size of the corpus we
use and by the characteristics of the language. As
Ukrainian language is a language with a rich mor-
phology, there are several directions we would like
to explore in the future.
From the language-oriented perspective, it would
be useful to determine to which extent stemming and
morphological analysis would boost performance.
The other problem which we have not considered up
to now is the ambiguity of some named entities. For
example, a word ’Ukraine’ can belong to the cate-
gory LOC as well as to the category ORG (as it is a
part of a complex named entity).
In addition, we would also like to explore the
semi-supervised techniques such as co-training and
self-training (Collins and Singer, 1999).
</bodyText>
<sectionHeader confidence="0.998984" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99668596969697">
Carreras et al. 2002. Named Entity Extraction using AdaBoost.
In the Proceedings of CoNLL-2002, Taipei, Taiwan.
Michael Collins and Yoram Singer 1999. Unsupervised Mod-
els for Named Entity Classification. In Proccedings of
EMNLP/VLC-99.
Radu Florian. Named Entity Recognition as a House of Cards:
Classifier Stacking. In the Proceedings of CoNLL-2002,
Taipei, Taiwan, 2002.
Dan Klein et al. Named Entity Recognition with Character-
Level Models. In the Proceedings of CoNLL-2002, Taipei,
Taiwan, 2003.
Zornitsa Kozareva, Boyan Bonev, and Andres Montoyo. 2005.
Self-training and Co-training Applied to Spanish Named En-
tity Recognition. In MICAI 2005: 770-779.
Katerina Pastra, Diana Maynard, Oana Hamza, Hamish Cun-
ningham and Yorick Wilks. 2002. How feasible is the reuse
of grammars for Named Entity Recognition? In LREC’02.
Lance Ramshaw and Mitch Marcus. 1995. Text Chunking Us-
ing Transformation-Based Learning In ACL’95.
Ellen Riloff. 1995. Information Extraction as a Basis for
Portable Text Classification Systems. PhD Thesis. Dept.
of Computer Science Technical Report, University of Mas-
sachusetts Amherst.
P. P. Talukdar, T. Brants, M. Liberman and F. Pereira. 2006. A
Context Pattern Induction Method for Named Entity Extrac-
tion. In the Proceedings of the Tenth Conference on Compu-
tational Natural Language Learning (CoNLL-2006).
Erik Tjong Kim Sang. 2002. Introduction to the CoNLL-2002
Shared Task: Language-Independent Named Entity Recog-
nition. In the Proceedings of CoNLL-2002, Taipei, Taiwan,
155–158.
Dekai Wu et al. 2002. Boosting for Named Entity Recognition.
In the Proceedings of CoNLL-2002, Taipei, Taiwan.
</reference>
<page confidence="0.999132">
93
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.512197">
<title confidence="0.998831">Named Entity Recognition for Ukrainian: A Resource-Light Approach</title>
<author confidence="0.92459">Sophia</author>
<affiliation confidence="0.899165">HCSL, University of</affiliation>
<abstract confidence="0.927621842105263">Kruislaan 419, 1098VA the katrenko@science.uva.nl Pieter HCSL, University of Kruislaan 419, 1098VA the pitera@science.uva.nl Abstract Named entity recognition (NER) is a subtask of information extraction (IE) which can be used further on for different purposes. In this paper, we discuss named entity recognition for Ukrainian language, which is a Slavonic language with a rich morphology. The approach we follow uses a restricted number of features. We show that it is feasible to boost performance by considering several heuristics and patterns acquired from the Web data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Carreras</author>
</authors>
<title>Named Entity Extraction using AdaBoost.</title>
<date>2002</date>
<booktitle>In the Proceedings of CoNLL-2002,</booktitle>
<location>Taipei, Taiwan.</location>
<marker>Carreras, 2002</marker>
<rawString>Carreras et al. 2002. Named Entity Extraction using AdaBoost. In the Proceedings of CoNLL-2002, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised Models for Named Entity Classification.</title>
<date>1999</date>
<booktitle>In Proccedings of EMNLP/VLC-99.</booktitle>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer 1999. Unsupervised Models for Named Entity Classification. In Proccedings of EMNLP/VLC-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
</authors>
<title>Named Entity Recognition as a House of Cards: Classifier Stacking.</title>
<date>2002</date>
<booktitle>In the Proceedings of CoNLL-2002,</booktitle>
<location>Taipei, Taiwan,</location>
<contexts>
<context position="5877" citStr="Florian, 2002" startWordPosition="942" endWordPosition="943"> but it might be less useful for German. 2Sometimes, these types of features are referred to as wordexternal and word-internal (Klein et al., 2003) The feature set of some NER methods (Wu, 2002) also includes part-of-speech information and/or word prefixes and suffixes. Although this information (and especially lemmas) is very useful for the languages with rich morphology, it presupposes the existence of POS taggers for a given language. Another conclusion which can be drawn relates to the machine learning approaches. The best results have been received by applying ensemble methods (Wu, 2002; Florian, 2002; Carreras et al., 2002). A very interesting work on named entity recognition task was reported by Collins et al. (1999) who used only few named entities to bootstrap more. The other approach proposed recently makes use of the data extracted from the Web (Talukdar et al., 2006). By restricting themselves to the fixed context of the extracted named entities and by employing grammar inference techniques, the authors filter out the most useful patterns. As they show, by applying such approach precision can already be largely boosted. Pastra et al. (2002) focused on the applicability of already ex</context>
</contexts>
<marker>Florian, 2002</marker>
<rawString>Radu Florian. Named Entity Recognition as a House of Cards: Classifier Stacking. In the Proceedings of CoNLL-2002, Taipei, Taiwan, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
</authors>
<title>Named Entity Recognition with CharacterLevel Models.</title>
<date>2003</date>
<booktitle>In the Proceedings of CoNLL-2002,</booktitle>
<location>Taipei, Taiwan,</location>
<marker>Klein, 2003</marker>
<rawString>Dan Klein et al. Named Entity Recognition with CharacterLevel Models. In the Proceedings of CoNLL-2002, Taipei, Taiwan, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Boyan Bonev</author>
<author>Andres Montoyo</author>
</authors>
<title>Self-training and Co-training Applied to Spanish Named Entity Recognition. In MICAI</title>
<date>2005</date>
<pages>770--779</pages>
<contexts>
<context position="4528" citStr="Kozareva et al., 2005" startWordPosition="730" endWordPosition="733">in a given language, while others explore methods to automatically extract NEs without prior knowledge. In the first case, the gazetteers will in most cases improve NER results (Carreras et al., 2002) but, unfortunately, they may not exist for a language one is working on. Hand-written rules can also cover more NEs but building such patterns will be a very time-consuming process. There have been many methods applied to NER, varying from the statistical to the memory-based approaches. Most work on NER has been focused on English but there are also approaches to other languages such as Spanish (Kozareva et al., 2005), German, or Dutch. In addition, several competitions have been organized, with a focus on multilingual NER (Tjong Kim Sang, 2002). While analyzing the results of these shared tasks, it can be concluded that the selected features are of a great importance. In our view, they can be categorized in two types, i.e. contextual and orthographic 2. The first type includes words surrounding a given word while the other contains such features as capitalized letters, digits contained within the word, etc. Both types of features contribute to the information extraction task. Nevertheless, orthographic fe</context>
</contexts>
<marker>Kozareva, Bonev, Montoyo, 2005</marker>
<rawString>Zornitsa Kozareva, Boyan Bonev, and Andres Montoyo. 2005. Self-training and Co-training Applied to Spanish Named Entity Recognition. In MICAI 2005: 770-779.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katerina Pastra</author>
<author>Diana Maynard</author>
<author>Oana Hamza</author>
<author>Hamish Cunningham</author>
<author>Yorick Wilks</author>
</authors>
<title>How feasible is the reuse of grammars for Named Entity Recognition?</title>
<date>2002</date>
<booktitle>In LREC’02.</booktitle>
<contexts>
<context position="6434" citStr="Pastra et al. (2002)" startWordPosition="1032" endWordPosition="1035"> received by applying ensemble methods (Wu, 2002; Florian, 2002; Carreras et al., 2002). A very interesting work on named entity recognition task was reported by Collins et al. (1999) who used only few named entities to bootstrap more. The other approach proposed recently makes use of the data extracted from the Web (Talukdar et al., 2006). By restricting themselves to the fixed context of the extracted named entities and by employing grammar inference techniques, the authors filter out the most useful patterns. As they show, by applying such approach precision can already be largely boosted. Pastra et al. (2002) focused on the applicability of already existing resources in one language to another. Their case study was based on English and Romanian, where a system, originally developed for NER in English was adapted to Romanian. Their results suggest that such adaptation is easier than developing a named entity recognition system for Romanian from scratch. However, the authors also mention that not all phenomena in Romanian have been taken into account which resulted in low recall. 3 Methodology Ukrainian belongs to the languages where the named entities are usually capitalized, which makes their dete</context>
</contexts>
<marker>Pastra, Maynard, Hamza, Cunningham, Wilks, 2002</marker>
<rawString>Katerina Pastra, Diana Maynard, Oana Hamza, Hamish Cunningham and Yorick Wilks. 2002. How feasible is the reuse of grammars for Named Entity Recognition? In LREC’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance Ramshaw</author>
<author>Mitch Marcus</author>
</authors>
<title>Text Chunking Using Transformation-Based Learning</title>
<date>1995</date>
<booktitle>In ACL’95.</booktitle>
<contexts>
<context position="13861" citStr="Ramshaw and Marcus, 1995" startWordPosition="2318" endWordPosition="2321">ifferent feature sets, patterns extracted from the Web and Levenshtein distance. We expect that both types of experiments can shed a light on usefulness of the features that we defined for NER on Ukrainian data. 4.1 Data Initially, several articles of the newspaper Mirror Weekly (year 2005)3 were annotated. During the annotating process we considered the following named instances: PERSON (person names), LOC (location), ORG (organization).In total, there were 10,000 tokens annotated, 514 of which are named entities. All named entities have been annotated according to the IOB annotation scheme (Ramshaw and Marcus, 1995). The annotated corpus can Scan be found at http://www.zn.kiev.ua be downloaded from http://www.science. uva.nl/˜katrenko/Corpus The corpus was split into training and test sets of 6,606 and 3,397 tokens, respectively. The corpus is relatively small but we hope to study whether such features as orthographic are sufficient for the NER task alone or it is needed to add more sources to approach this task. 4.2 Classification The results of our experiments on classification of named entities are provided in Table 2. Baseline B1 was defined by the most frequent tag in the data (ORG). Similarly to Co</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Lance Ramshaw and Mitch Marcus. 1995. Text Chunking Using Transformation-Based Learning In ACL’95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
</authors>
<title>Information Extraction as a Basis for Portable Text Classification Systems.</title>
<date>1995</date>
<tech>PhD Thesis.</tech>
<institution>Dept. of Computer Science</institution>
<contexts>
<context position="867" citStr="Riloff, 1995" startWordPosition="124" endWordPosition="125"> Amsterdam, the Netherlands pitera@science.uva.nl Abstract Named entity recognition (NER) is a subtask of information extraction (IE) which can be used further on for different purposes. In this paper, we discuss named entity recognition for Ukrainian language, which is a Slavonic language with a rich morphology. The approach we follow uses a restricted number of features. We show that it is feasible to boost performance by considering several heuristics and patterns acquired from the Web data. 1 Introduction The information extraction task has proved to be difficult for a variety of domains (Riloff, 1995). The extracted information can further be used for question answering, information retrieval and other applications. Depending on the final purpose, the extracted information can be of different type, e.g., temporal events, locations, etc. The information corresponding to locations and names, is referred to as the information about named entities. Hence, named entity recognition constitutes a subtask of the information extraction in general. It is especially challenging to extract the named entities from the text sources written in languages other than English which, in practice, is supported</context>
</contexts>
<marker>Riloff, 1995</marker>
<rawString>Ellen Riloff. 1995. Information Extraction as a Basis for Portable Text Classification Systems. PhD Thesis. Dept. of Computer Science Technical Report, University of Massachusetts Amherst.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P P Talukdar</author>
<author>T Brants</author>
<author>M Liberman</author>
<author>F Pereira</author>
</authors>
<title>A Context Pattern Induction Method for Named Entity Extraction.</title>
<date>2006</date>
<booktitle>In the Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-2006).</booktitle>
<contexts>
<context position="6155" citStr="Talukdar et al., 2006" startWordPosition="988" endWordPosition="991">. Although this information (and especially lemmas) is very useful for the languages with rich morphology, it presupposes the existence of POS taggers for a given language. Another conclusion which can be drawn relates to the machine learning approaches. The best results have been received by applying ensemble methods (Wu, 2002; Florian, 2002; Carreras et al., 2002). A very interesting work on named entity recognition task was reported by Collins et al. (1999) who used only few named entities to bootstrap more. The other approach proposed recently makes use of the data extracted from the Web (Talukdar et al., 2006). By restricting themselves to the fixed context of the extracted named entities and by employing grammar inference techniques, the authors filter out the most useful patterns. As they show, by applying such approach precision can already be largely boosted. Pastra et al. (2002) focused on the applicability of already existing resources in one language to another. Their case study was based on English and Romanian, where a system, originally developed for NER in English was adapted to Romanian. Their results suggest that such adaptation is easier than developing a named entity recognition syst</context>
<context position="10774" citStr="Talukdar et al., 2006" startWordPosition="1765" endWordPosition="1768">an instance. Intuitively, if there are many patterns acquired from the large collection of data on the Web, they must be sufficient (in some sense even redundant) to recognize named entities in a text. For instance, such pattern as was located in X in English can correspond to three patterns in Ukrainian was located (fem., sing.) in X, was located (masc., sing.) in X, was located (neut., sing.) in X. Even though these patterns could be embraced in one, we are rather interested in collecting all possible patterns avoiding this way stemming and morphological analysis. As in Talukdar’s approach (Talukdar et al., 2006), we expect patterns to provide high precision. We are, however, concerned about the size of Ukrainian Web which is much smaller than English part of the Web. As a consequence, it is not clear whether recall can be improved much by using the Web data. 3.3 Levenshtein distance Yet another approach to address rich morphology of Ukrainian, is to carry out a matching of probable named entities in a test set against a list of named entities in a training set. It can be done by using string edit distances, such as Levenshtein. Levenshtein (or edit) distance of two strings, x and y is measured as the</context>
</contexts>
<marker>Talukdar, Brants, Liberman, Pereira, 2006</marker>
<rawString>P. P. Talukdar, T. Brants, M. Liberman and F. Pereira. 2006. A Context Pattern Induction Method for Named Entity Extraction. In the Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Tjong Kim Sang</author>
</authors>
<title>Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition.</title>
<date>2002</date>
<booktitle>In the Proceedings of CoNLL-2002,</booktitle>
<location>Taipei, Taiwan,</location>
<contexts>
<context position="1557" citStr="Sang, 2002" startWordPosition="231" endWordPosition="232">tion retrieval and other applications. Depending on the final purpose, the extracted information can be of different type, e.g., temporal events, locations, etc. The information corresponding to locations and names, is referred to as the information about named entities. Hence, named entity recognition constitutes a subtask of the information extraction in general. It is especially challenging to extract the named entities from the text sources written in languages other than English which, in practice, is supported by the results of the shared tasks on the named entity recognition (Tjong Kim Sang, 2002). Named entity recognition for the languages with a rich morphology and a free word order is difficult because of several reasons. The entropy of texts in such languages is usually higher than the entropy of English texts. It is either needed to use such resources as morphological analyzers to reduce the data sparseness or to annotate a large amount of data in order to obtain a good performance. Luckily, the free word order is not crucial for the named entity recognition task as the local context of a named entity should be sufficient for its detection. Besides, a free word order usually impli</context>
<context position="4658" citStr="Sang, 2002" startWordPosition="753" endWordPosition="754">l in most cases improve NER results (Carreras et al., 2002) but, unfortunately, they may not exist for a language one is working on. Hand-written rules can also cover more NEs but building such patterns will be a very time-consuming process. There have been many methods applied to NER, varying from the statistical to the memory-based approaches. Most work on NER has been focused on English but there are also approaches to other languages such as Spanish (Kozareva et al., 2005), German, or Dutch. In addition, several competitions have been organized, with a focus on multilingual NER (Tjong Kim Sang, 2002). While analyzing the results of these shared tasks, it can be concluded that the selected features are of a great importance. In our view, they can be categorized in two types, i.e. contextual and orthographic 2. The first type includes words surrounding a given word while the other contains such features as capitalized letters, digits contained within the word, etc. Both types of features contribute to the information extraction task. Nevertheless, orthographic features can already be language-specific. For instance, capitalization is certainly very important for such languages as English or</context>
<context position="14499" citStr="Sang, 2002" startWordPosition="2423" endWordPosition="2424">Scan be found at http://www.zn.kiev.ua be downloaded from http://www.science. uva.nl/˜katrenko/Corpus The corpus was split into training and test sets of 6,606 and 3,397 tokens, respectively. The corpus is relatively small but we hope to study whether such features as orthographic are sufficient for the NER task alone or it is needed to add more sources to approach this task. 4.2 Classification The results of our experiments on classification of named entities are provided in Table 2. Baseline B1 was defined by the most frequent tag in the data (ORG). Similarly to Conll shared task (Tjong Kim Sang, 2002), we also calculated a baseline by tagging all named entities which occurred in the training set (B2). Although there are many names of organizations detected, there are only 1,92% of person names recognized. precision recall F-score B1 0.32 0.32 0.32 B2 0.29 0.18 0.22 M2−knn 0.31 0.44 0.36 ortho 0.38 0.46 0.42 M2−knn 0.47 0.38 0.42 ortho+cont 0.40 0.43 0.41 MV oting 0.46 0.39 0.42 ortho+cont 0.50 0.46 0.48 MStacking ortho+cont MV oting ortho+cont+pat MV oting ortho+cont+pat+lev Table 2: Experiments: precision and recall Since we are interested in how much each type of the feature sets contrib</context>
</contexts>
<marker>Sang, 2002</marker>
<rawString>Erik Tjong Kim Sang. 2002. Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition. In the Proceedings of CoNLL-2002, Taipei, Taiwan, 155–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Boosting for Named Entity Recognition.</title>
<date>2002</date>
<booktitle>In the Proceedings of CoNLL-2002,</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="5458" citStr="Wu, 2002" startWordPosition="880" endWordPosition="881">al and orthographic 2. The first type includes words surrounding a given word while the other contains such features as capitalized letters, digits contained within the word, etc. Both types of features contribute to the information extraction task. Nevertheless, orthographic features can already be language-specific. For instance, capitalization is certainly very important for such languages as English or Dutch but it might be less useful for German. 2Sometimes, these types of features are referred to as wordexternal and word-internal (Klein et al., 2003) The feature set of some NER methods (Wu, 2002) also includes part-of-speech information and/or word prefixes and suffixes. Although this information (and especially lemmas) is very useful for the languages with rich morphology, it presupposes the existence of POS taggers for a given language. Another conclusion which can be drawn relates to the machine learning approaches. The best results have been received by applying ensemble methods (Wu, 2002; Florian, 2002; Carreras et al., 2002). A very interesting work on named entity recognition task was reported by Collins et al. (1999) who used only few named entities to bootstrap more. The othe</context>
</contexts>
<marker>Wu, 2002</marker>
<rawString>Dekai Wu et al. 2002. Boosting for Named Entity Recognition. In the Proceedings of CoNLL-2002, Taipei, Taiwan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>