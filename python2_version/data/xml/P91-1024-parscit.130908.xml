<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<sectionHeader confidence="0.84315" genericHeader="abstract">
EXPERIMENTS AND PROSPECTS OF
EXAMPLE-BASED MACHINE TRANSLATION
</sectionHeader>
<note confidence="0.7031375">
Eiichiro SUMITA*
and
</note>
<author confidence="0.351892">
Hitoshi HDA
</author>
<affiliation confidence="0.331909">
ATR Interpreting Telephony Research Laboratories
</affiliation>
<address confidence="0.42396">
Sanpeidani, Inuidani, Seika-cho
Souraku-gun, Kyoto 619-02, JAPAN
</address>
<sectionHeader confidence="0.755748" genericHeader="keywords">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.9697883">
EBMT (Example-Based Machine Translation)
is proposed. EBMT retrieves similar examples
(pairs of source phrases, sentences, or
texts and their translations) from a database of
examples, adapting the examples to translate a new
input. EBMT has the following features: (1) It is
easily upgraded simply by inputting appropriate
examples to the database; (2) It assigns a reliability
factor to the translation result; (3) It is accelerated
effectively by both indexing aid parallel computing;
</bodyText>
<listItem confidence="0.8116415">
(4) It is robust because of best-match reasoning; and
(5) It well utilizes translator expertise. A prototype
</listItem>
<bodyText confidence="0.904201636363636">
system has been implemented to deal with a difficult
translation problem for conventional Rule-Based
Machine Translation (RBMT), i.e., translating
Japanese noun phrases of the form &amp;quot;N, no N,&amp;quot; into
English. The system has achieved about a 78%
success rate on average. This paper explains the basic
idea of EBMT, illustrates the experiment in detail,
explains the Wad applicability of EBMT to several
difficult translation problems for RBMT and
discusses the advantages of integrating EBMT with
RBMT.
</bodyText>
<sectionHeader confidence="0.99983" genericHeader="introduction">
1 INTRODUCTION
</sectionHeader>
<bodyText confidence="0.997741588235294">
Machine Translation requires handcrafted axl
complicated large-scale knowledge (Nirenburg 1987).
Conventional machine translation systems use
rules as the knowledge. This framework is
called Rule-Based Machine Translation
(RBMT). It is difficult to scale up from a toy
program to a practical system because of the problem
of building such a large-scale rule-base. It is also
difficult to improve translation performance because
the effect of adding a new rule is hard to anticipate,
and because translation using a large-scale rule-based
system is time-consuming. Moreover, it is difficult
to make use of situational or domain-specific
information for translation.
In order to conquer these problems in machine
translation, a database of examples (pairs of
source phrases, sentences, or texts and
</bodyText>
<sectionHeader confidence="0.379565" genericHeader="method">
* Currently with Kyoto University
</sectionHeader>
<bodyText confidence="0.988359580645161">
their translations) has been implemented as
the knowledge (Nagao 1984; Sumita and
Tsutsumi 1988; Sato and Nagao 1989; Sadler 1989a;
Sumita et al. 1990a, b). The translation mechanism
retrieves similar examples from the database,
adapting the examples to translate the new source
text. This framework is called Example-Based
Machine Translation (EBMT).
This paper focuses on ATR&apos;s linguistic
database of spoken Japanese with English
translations. The corpus contains conversations about
international conference registration (Ogura et al.
1989). Results of this study indicate that EBMT is a
breakthrough in MT technology.
Our pilot EBMT system translates Japanese
noun phrases of the form &amp;quot;N, no 142&amp;quot; into English
noun phrases. About a 78% success rate on
average has been achieved in the
experiment, which is considered to
outperform RBMT. This rate can be improved as
discussed below.
Section 2 explains the basic idea of EBMT.
Section 3 discusses the broad applicability of EBMT
and the advantages of integrating it with RBMT.
Sections 4 and 5 give a rationale for section 3, i.e.,
section 4 illustrates the experiment of translating
noun phrases of the form &amp;quot;N, no N2&amp;quot; in detail, aid
section 5 studies other phenomena through actual
data from our corpus. Section 6 concludes this paper
with detailed comparisons between RBMT and
EBMT.
</bodyText>
<sectionHeader confidence="0.7552585" genericHeader="method">
2 BASIC IDEA OF EBMT
2.1 BASIC FLOW
</sectionHeader>
<bodyText confidence="0.889649333333333">
In this section, the basic idea of EBMT,
which is general and applicable to many phenomena
dealt with by machine translation, is shown.
Figure 1 shows the basic flow of EBMT
using translation of &amp;quot;Icireru&amp;quot;[cut / be sharp]. From
here on, the literal English translations are
bracketed.
(1) and (2) at examples (pairs of
Japanese sentences and their English
</bodyText>
<page confidence="0.998111">
185
</page>
<bodyText confidence="0.995655666666667">
translations) in the database.
Examples similar to the Japanese
input sentence are retrieved in the following
manner. Syntactically, the input is similar to
Japanese sentences (1) and (2). However,
semantically, &amp;quot;kachou&amp;quot; [chief] is far from &amp;quot;houchou&amp;quot;
[kitchen knife]. But., &amp;quot;kachou&amp;quot; [chief] is semantically
similar to &amp;quot;kanojo&amp;quot; [she] in that both are people. In
other words, the input is similar to example sentence
(2). By mimicking the similar example (2), we
finally get &amp;quot;The chief is sharp&amp;quot;.
Although it is possible to obtain the same
result by a word selection rule using fine-tuned
semantic restriction, note that translation here is
obtained by retrieving similar examples to the input.
</bodyText>
<listItem confidence="0.966391857142857">
• Example Database
(data for &amp;quot;kireru[cut / be sharp])
(1) houchou wa klreru -&gt; The kitchen knife cuts.
(2) kanojo wa klreru -› She Is sharp.
• Input
kachou wa kirsru ?
• Retrieval of similar examples
</listItem>
<figure confidence="0.9262716">
Input (1), (2)
kachou I. houchou
kachou kanojo
Input . (2)
-&gt; The chief! s sharp.
</figure>
<figureCaption confidence="0.983727">
Figure 1 Mimicking Similar Examples
</figureCaption>
<subsectionHeader confidence="0.985108">
2.2 DISTANCE
</subsectionHeader>
<bodyText confidence="0.999943615384615">
Retrieving similar examples to the input is
done by measuring the distance of the input to
each of examples. The smaller a distance is, the
more similar the example is to the input. To define
the best distance metric is a problem of EBMT not
yet completely solved. However, one possible
definition is shown in section 4.2.2.
From similar examples retrieved, EBMT
generates the most likely translation with a
reliability factor based on distance and frequency. If
there is no similar example within the given
threshold, EBMT tells the user that it cannot
translate the input.
</bodyText>
<sectionHeader confidence="0.9999625" genericHeader="method">
3 BROAD APPLICABILITY AND
INTEGRATION
</sectionHeader>
<subsectionHeader confidence="0.99136">
3.1 BROAD APPLICABILITY
</subsectionHeader>
<bodyText confidence="0.994334055555556">
EBMT is applicable to many linguistic
phenomena that are regarded as difficult to translate in
conventional RBMT. Some are well-known among
researchers of natural language processing and others
have recently been given a great deal of attention.
When one of the following conditions holds
true for a linguistic phenomenon, RBMT is less
suitable than EBMT.
(Ca) Translation rule formation is
difficult.
(Cb) The general rule cannot accurately
describe phenomena because it
represents a special case, e.g., idioms.
(Cc) Translation cannot be made in a
compositional way from target words
(Nagao 1984; Nitta 1986; Sadler 1989b).
This is a list (not exhaustive) of phenomena
in J-E translation that are suitable for EBMT:
</bodyText>
<listItem confidence="0.999112714285714">
• optional cases with a case particle
( &amp;quot;— de&amp;quot;, &amp;quot;— ni&amp;quot;,...)
• subordinate conjunction (&amp;quot;— ba —&amp;quot;, &amp;quot;— nagara —&amp;quot;,
&amp;quot;— tara —&amp;quot;,...,&amp;quot;— baai —&amp;quot;,...)
• noun phrases of the form &amp;quot;N, no N2&amp;quot;
• sentences of the form &amp;quot;N, wa N2 da&amp;quot;
• sentences lacking the main verb (eg. sentences of
the form &amp;quot;— o-negaishimasu&amp;quot;)
• fragmental expressions (&amp;quot;hai&amp;quot;, &amp;quot;sou-desu&amp;quot;,
&amp;quot;wakarimashita&amp;quot;,...) (Furuse et al. 1990)
• modality represented by the sentence ending
(&amp;quot;—tainodesuga&amp;quot;, &amp;quot;—seteitadalcimasu&amp;quot;, ...)
(Furuse et al. 1990)
• simple sentences (Sato and Nagao 1989)
</listItem>
<bodyText confidence="0.999960727272727">
This paper discusses a detailed experiment for
&amp;quot;N, no N2&amp;quot; in section 4 and prospects for other
phenomena, &amp;quot;N, wa N2 da&amp;quot; and &amp;quot;— o-negaishimasu&amp;quot;
in section 5.
Similar phenomena in other language
pairs can be found. For example, in Spanish to
English translation, the Spanish preposition &amp;quot;de&amp;quot;,
with its broad usage like Japanese &amp;quot;no&amp;quot;, is also
effectively translated by EBMT. Likewise, in German
to English translation, the German complex noun is
also effectively translated by EBMT.
</bodyText>
<subsectionHeader confidence="0.95438">
3.2 INTEGRATION
</subsectionHeader>
<bodyText confidence="0.999895230769231">
It is not yet clear whether EBMT can or
should deal with the whole process of translation. We
assume that there are many kinds of phenomena.
Some are suitable for EBMT, while others are
suitable for RBMT.
Integrating EBMT with RBMT is
expected to be useful. It would be more
acceptable for users if RBMT were first introduced as
a base system, and then incrementally have its
translation performance improved by attaching
EBMT components. This is in the line with the
proposal in Nagao (1984). Subsequently, we
proposed a practical method of integration in
</bodyText>
<figure confidence="0.78105775">
(Syntax)
(Semantics)
(Total)
• Output
</figure>
<page confidence="0.989695">
186
</page>
<bodyText confidence="0.972891">
previous papers (Sumita et al. 1990a, b).
</bodyText>
<sectionHeader confidence="0.7962865" genericHeader="method">
4 EBMT FOR &amp;quot;N1 no N2&amp;quot;
4.1 THE PROBLEM
</sectionHeader>
<bodyText confidence="0.999281125">
&amp;quot;N, no Ny&amp;quot; is a common Japanese noun
phrase form. &amp;quot;no&amp;quot; in the &amp;quot;N, no N2&amp;quot; is a Japanese
adnominal particle. There are other variants,
including &amp;quot;deno&amp;quot;, &amp;quot;Icarano&amp;quot;, &amp;quot;madeno&amp;quot; and so on.
Roughly speaking, Japanese noun phrases of
the form &amp;quot;N, no N2&amp;quot; correspond to English noun
phrases of the form &amp;quot;N2 of N,&amp;quot; as shown in the
examples at the top of Figure 2.
</bodyText>
<subsubsectionHeader confidence="0.483063">
Japanese English
</subsubsectionHeader>
<bodyText confidence="0.996334111111111">
youka no gogo the afternoon of the 8th
kaigi no mokuteki the object of the conference
kaigi no sankaryou the application fee fo r the conf.
?the application fee of the conf.
kyouto deno kaigi the cord. In Kyoto
?the cont. of Kyoto
isshukan no kyuka a week&apos;s holiday
?the holiday of a week
mittsu no hoteru three hotels
</bodyText>
<figureCaption confidence="0.695466">
*hotels o f three
Figure 2 Variations in Translation of &amp;quot;N1 no N2&amp;quot;
</figureCaption>
<bodyText confidence="0.999345">
expressions). EBMT has the advantage that it can
directly return a translation by adapting examples
without reasoning through a long chain of rules.
</bodyText>
<sectionHeader confidence="0.738923" genericHeader="method">
4.2 IMPLEMENTATION
4.2.1 OVERVIEW
</sectionHeader>
<bodyText confidence="0.9677681">
The EBMT system consists of two databases:
an example database and a thesaurus; and also three
translation modules: analysis, example-based transfer,
and generation (Figure 3).
Examples (pairs of source phrases
and their translations) are extracted from ATR&apos;s
linguistic database of spoken Japanese with English
translations. The corpus contains conversations about
registering for an international conference (Ogura
1989).
</bodyText>
<figure confidence="0.999006428571429">
(1) Analysis
(Example
Database
(2)Example-Based
Transfer
( Thesaurus
(3) Generation
</figure>
<figureCaption confidence="0.998505">
Figure 3 System Configuration
</figureCaption>
<bodyText confidence="0.968243733333333">
However, &amp;quot;N2 of N,&amp;quot; does not always provide
a natural translation as shown in the lower examples
in Figure 2. Some translations are too broad in
meaning to interpret, others am almost
ungrammatical. For example, the fourth one, &amp;quot;the
conference of Kyoto&amp;quot;, could be misconstrued as &amp;quot;the
conference about Kyoto&amp;quot;, ani the last one, &amp;quot;hotels of
three&amp;quot;, is not English. Natural translations often
require prepositions other than &amp;quot;of&amp;quot;, or no
preposition at all. In only about one-fifth of &amp;quot;N, no
N2&amp;quot; occurrences in our domain, &amp;quot;N2 of N,&amp;quot; would be
the most appropriate English translation. We cannot
use any particular preposition as an effective default
value.
No rules for selecting the most appropriate
translation for &amp;quot;N, no N2&amp;quot; have yet been found. In
other words, the condition (Ca) in section 3.1 holds.
Selecting the translation for &amp;quot;N, no N2&amp;quot; is still an
important and complicated problem in J-E
translation.
In contrast with the preceding research
analyzing &amp;quot;N, no N2&amp;quot; (Shimazu et al. 1987; Hirai and
Kitahashi 1986), deep semantic analysis is avoided
because it is assumed that translations appropriate
for given domain can be obtained using
domain-specific examples (pairs of source and target
The thesaurus is used in calculating
the semantic distance between the content
words in the input and those in the
examples. It is composed of a hierarchical structure
in accordance with the thesaurus of everyday Japanese
written by Ohno and Hamanishi (1984).
Analysis
kyouto deno kaigi
Example-Based Transfer
d Japanese English
0.4 toukyou deno taizai the stay in Tokyo
0.4 honkon deno taizai the stay In Hongkong
0.4 toukyou deno go-taizai the stay In Tokyo
the conf. in Osaka
the cord. in Tokyo
Figure 4 illustrates the translation procedure
with an actual sample. First, morphological analysis
is performed for the input phrase,&amp;quot;kyouto[Kyoto]
deno kaigi [conference]&amp;quot;. In this case, syntactical
</bodyText>
<figure confidence="0.7334415">
1.0 oosaka no kaigi
1.0 toukyou no kaigi
Generation
the conf. In Kyoto
</figure>
<figureCaption confidence="0.99366">
Figure 4 Translation Procedure
</figureCaption>
<page confidence="0.986129">
187
</page>
<bodyText confidence="0.998801166666667">
analysis is not necessary. Second, similar examples
are retrieved from the database. The top five similar
examples are shown. Note that the top three
examples have the same distance and that they are all
translated with &amp;quot;in&amp;quot;. Third, using this rationale,
EBMT generates &amp;quot;the conference in Kyoto&amp;quot;.
</bodyText>
<subsectionHeader confidence="0.376442">
4.2.2 DISTANCE CALCULATION
</subsectionHeader>
<bodyText confidence="0.996558176470588">
The distance metric used when retrieving
examples is essential and is explained here in detail.
Here we suppose that the input and examples (I, E)
in the database are represented in the same data
structure, i.e., the list of words&apos; syntactic and
semantic attribute values (referred to as and I , Ei) for
each phrase.
The attributes of the current target, &amp;quot;N1 no
N2&amp;quot;, ate as follows: 1) kr the nouns &amp;quot;NI&amp;quot; and &amp;quot;N2&amp;quot;:
the lexical subcategory of the noun, the existence of
a prefix or suffix, aid its semantic code in the
thesaurus; 2) for the adnominal particle &amp;quot;no&amp;quot;: the
kinds of variants, &amp;quot;deno&amp;quot;, &amp;quot;lcarano&amp;quot;, &amp;quot;madeno&amp;quot; and so
on. Here, for simplicity, only the semantic code and
the kind of adnominal ate considered.
Distances are calculated using the following
two expressions (Stunita et al. 1990a, b):
</bodyText>
<listItem confidence="0.997262333333333">
(1) d(I,E)=E d(II,Ej) *wi
(2) wri E ( freq. oft. p. when Ej=lj 2
t. p.
</listItem>
<bodyText confidence="0.9998568">
The attribute distance, d(li,Ei) and the weight
of attribute, we ate explained in the following
sections. Each translation pattern (t.p.) is abstracted
from an example and is stored with the example in
the example database [see Figure 6].
</bodyText>
<sectionHeader confidence="0.85885" genericHeader="method">
(a) ATTRIBUTE DISTANCE
</sectionHeader>
<bodyText confidence="0.993403">
For the attribute of the adnominal particle
&amp;quot;no&amp;quot;, the distance is 0 or 1 depending on whether or
not they match exactly, for example,
li(&amp;quot;deno&amp;quot;,&amp;quot;deno&amp;quot;)= 0 and dcdeno&amp;quot;, &amp;quot;no&amp;quot;) = 1.
For semantic attributes, however, the distance
varies between 0 and 1. Semantic distance d(0 5
d 5 1) is determined by the Most Specific
Common Abstraction(MSCA) (Kolodner and
Riesbeck 1989) obtained from the thesaurus
abstraction hierarchy. When the thesaurus is (n+1)
layered, (k/n) is assigned to the concepts in the k-th
layer from the bottom. For example, as shown with
the broken line in Figure 5, the MSCACkaigi&amp;quot;
[conference], &amp;quot;tairai&amp;quot; [stay]) is &amp;quot;koudou&amp;quot; [actions] and
the distance is 2/3. Of course, 0 is assigned when the
MSCA is the bottom class, for instance,
MSCA(&amp;quot;kyouto&amp;quot;[Kyoto], &amp;quot;toukyou&amp;quot; [Tokyo]) =
&amp;quot;timer[place], or when nouns are identical (
MSCA(N, N) for any N).
</bodyText>
<figure confidence="0.582273666666667">
Thesaurus Root
• (3/1-11
(b) WEIGHT OF ATTRIBUTE
</figure>
<bodyText confidence="0.924605">
The weight of the attribute is the
degree to which the attribute influences
the selection of the translation
pattern(t.p.). We adopt the expression (2) used
by Stanfill and Waltz (1986) for memory-based
reasoning, to implement the intuition.
t.p. freq. t.p. freq. t.p. freq.
</bodyText>
<figure confidence="0.968358714285714">
B in A 12/27 B in A 3/3 B 9/24
AB 4/27 AB 9/24
B from A 2/27 B in A 2/24
BA 2/27 A&apos;s B 1/24
B to A 1/27 B on A 1/24
(El=timei) (E2=deno) (E3=soudan)
(Place) I fin) I (meetings)
</figure>
<figureCaption confidence="0.977401">
Figure 6 Weight of the i-th attribute
</figureCaption>
<figure confidence="0.989646575757576">
kOU
[actions]
2/3)
touchaku
[arrive]
kaisetsu
[commen-
tary]
kaigi taizai
[conference] [stay]
X
X
chinjutsu ourai
[statements] comings &amp;
goings]
(1/3) 1/3)
Figure 5 Thesaurus(portion)
setsumei
[explana-
tions]
(0)
soudan
[meetings
(0)
tame!
[stays]
(0)
•
hatchaku
[arrivals &amp;
departures;
(0)
4.(2)-$r.
</figure>
<page confidence="0.990792">
188
</page>
<bodyText confidence="0.9991931">
In Figure 6, all the examples whose E2 =
&amp;quot;deno&amp;quot; are translated with the same preposition,
&amp;quot;in&amp;quot;. This implies that when E2= &amp;quot;deno&amp;quot;, E2 is an
attribute which heavily influences the selection of the
translation pattern. In contrast to this, the translation
patterns of examples whose El = &amp;quot;timei&amp;quot;[place], are
varied. This implies that when El = &amp;quot;timer[plaee],
El is an attribute which is less influential on the
selection of the translation pattern.
According to the expression (2) , weights for
</bodyText>
<equation confidence="0.8744014">
attributes, E1, E2 E3are as follows:
wi =1412/27) 244/27) 24- • .41/27) 2 0.49
w2=1/6735-2 = 1.0
w3=149/24) 2449/24) 24 • .024) 2 . 0.54
(c) TOTAL DISTANCE
</equation>
<bodyText confidence="0.941667">
The distance between the input and the first
example shown in Figure 4 is calculated using the
weights in section 4.2.2 (b), attribute distances as
explained in section 4.2.2 (a) and expression (1) at
the beginning of section 4.2.2.
d( &amp;quot;kyoutolKyoto] &amp;quot;dendlin] &amp;quot;kale( conference],
&amp;quot;toukyou&amp;quot;[Tokyo] &amp;quot;denolin] &amp;quot;taizailstayl)
drkyouto&amp;quot;,&amp;quot;toukyou70.49+
d(&amp;quot;deno&amp;quot;,&amp;quot;deno&amp;quot;)&amp;quot;1.0+
d(lraigi&amp;quot;, &amp;quot;taizaiT0.54
0&apos;0.49+01.0+2/30.54 = 0.4
</bodyText>
<sectionHeader confidence="0.589359" genericHeader="method">
4.3 EXPERIMENTS
</sectionHeader>
<bodyText confidence="0.99966675">
The current number of words in the corpus is
about 300,000 and the number of examples is 2,550.
The collection of examples from another domain is
in progress.
</bodyText>
<subsectionHeader confidence="0.588905">
4.3.1 JACKKNIFE TEST
</subsectionHeader>
<bodyText confidence="0.993025166666667">
In cider to roughly estimate translation
performance, a jackknife experiment was conducted.
We partitioned the example database(2,550) in groups
of one hundred, then used one set as input(100) and
translated them with the rest as an example database
(2,450). This was repeated 25 times.
</bodyText>
<figureCaption confidence="0.695710666666667">
Figure 7 shows that the average success
rate is 78%, the minimum 70% and the
maximum 89% [see section 4.3.4].
</figureCaption>
<bodyText confidence="0.998592571428571">
It is difficult to fairly compare this result
with the success rate of the existing MT system.
However, it is believed that current conventional
systems can at best output the most common
translation pattern, for example, &amp;quot;B of A&amp;quot;, as the
default. In this case, the average success rate may
only be about 20%.
</bodyText>
<figure confidence="0.940632166666667">
success(%) MAXIMUM(89%)
100
AVERAGE(78%)
MINIMUM(70%)
1 11 21
test number
</figure>
<figureCaption confidence="0.999924">
Figure 7 Result of Jackknife Test
</figureCaption>
<sectionHeader confidence="0.9619755" genericHeader="method">
4.3.2 SUCCESS RATE PER
NUMBER OF EXAMPLES
</sectionHeader>
<bodyText confidence="0.600970285714286">
Figure 8 shows the relationship between the
success rate and the number of examples. Of the
twenty-five cases in the previous jackknife test, three
are shown: maximum, average, and minimum. This
graph shows that, in general, the more examples
we have, the better the quality [see section
4.3.4].
</bodyText>
<figure confidence="0.9693575">
11 21
no. of examples (x 100)
</figure>
<figureCaption confidence="0.972249">
Figure 8 Success Rate per No. of Examples
</figureCaption>
<figure confidence="0.9975832">
80
60
40
20
0
</figure>
<page confidence="0.992231">
189
</page>
<sectionHeader confidence="0.971911" genericHeader="method">
4.3.3 SUCCESS RATE PER
DISTANCE
</sectionHeader>
<bodyText confidence="0.988618375">
Figure 9 shows the relationship between the
success rate and the distance between the input and
the most similar examples retrieved.
This graph shows that in general, the
smaller the distance, the better the quality.
In other words, EBMT assigns the distance between
the input and the retrieved examples as a reliability
factor.
</bodyText>
<figure confidence="0.991497642857143">
success
2,- 1592/1790
23/37
100/169 w 19/33
• w
• 35/67 7/14
95/162 • •
•
74/148
8/24
•
3 / 56
0 0.2 0.4 0.6 0.8 1
distance
</figure>
<figureCaption confidence="0.998392">
Figure 9 Success Rate per Distance
</figureCaption>
<subsectionHeader confidence="0.739315">
4.3.4 SUCCESSES AND FAILURES
</subsectionHeader>
<bodyText confidence="0.799406083333333">
The following represents successful results:
(1) the noun phrase &amp;quot;kyouto-elci [Kyoto-station] no
o-mise [store]&amp;quot; is translated according to the
translation pattern &amp;quot;B at A&amp;quot; while the similar noun
phrase, &amp;quot;kyouto[Kyoto] no shiten [branch]&amp;quot; is
translated according to the translation pattern &amp;quot;B in
A&amp;quot;; (2) the noun phrase of the form &amp;quot;NI no hou&amp;quot; is
translated according to the translation pattern &amp;quot;A&amp;quot;, in
other words, the second noun is omitted.
We are now studying the results carefully and
are striving to improve the success rate.
(a) About half of the failures are caused by a lack of
similar examples. They are easily solved by adding
appropriate examples.
(b) The rest are caused by the existence of similar
examples: (1) equivalent but different examples are
retrieved, for instance, those of the form, &amp;quot;B of A&amp;quot;
and &amp;quot;AB&amp;quot; for &amp;quot;roku-gatsu [June] no futsu-ka
[second]&amp;quot;. This is one of the main reasons the graphs
(Figure 7 and 8) show an up-and-down pattern. They
can be regarded as a correct translation or the distance
calculation may be changed to handle the problem;
(2) Because the current distance calculation is
inadequate, dissimilar examples are retrieved.
</bodyText>
<sectionHeader confidence="0.8890585" genericHeader="method">
5 PHENOMENA OTHER THAN
&amp;quot;N1 no N,&amp;quot;
</sectionHeader>
<bodyText confidence="0.964494666666667">
This section studies the phenomena, &amp;quot;N1 wa
N2 da&amp;quot; and o-negaishimasu&amp;quot; with the same corpus
used in the previous section.
</bodyText>
<subsectionHeader confidence="0.991624">
5.1 &amp;quot;N1 wa Ni da&amp;quot;
</subsectionHeader>
<bodyText confidence="0.99440535">
A sentence of the form &amp;quot;N1 wa N2 da&amp;quot; is
called a &amp;quot;da&amp;quot; sentence. Here &amp;quot;N,&amp;quot; and &amp;quot;N,&amp;quot; are nouns,
&amp;quot;wa&amp;quot; is a topical particle, awl &amp;quot;da&amp;quot; is a kind of verb
which, roughly speaking, is the English copula &amp;quot;be&amp;quot;.
The correspondences between &amp;quot;da&amp;quot; sentences
and the English equivalents are exemplified in Figure
10. Mainly, &amp;quot;N1 wa N2 da&amp;quot; corresponds to &amp;quot;N1 be N2&amp;quot;
like (a-1) — (a-4).
However, sentences like (b) (e) cannot be
translated according to the translation pattern &amp;quot;N1 be
N2&amp;quot;. In example (d), there is no Japanese counterpart
of &amp;quot;payment should be made by&amp;quot;. The English
sentence has a modal, passive voice, the verb make,
and its object, payment, while the Japanese sentence
has no such correspondences. This translation cannot
be male in a compositional way from the target
words which are selected from a normal dictionary. It
is difficult to formulate rules for the translation and
to explain how the translation is made. The
conditions (Ca) and (Cc) in section 3.1 hold true.
</bodyText>
<tableCaption confidence="0.54428125">
Conventional approaches leaf to the
understanding of &amp;quot;da&amp;quot; sentences using contextual and
extra-linguistic information. However, many
translations exist that are the result of human
translators&apos; understanding. Translation can be made
by mimicking such similar examples.
Example (e) is special, i.e., idiomatic. The
condition (Cb) in section 3.1 holds.
</tableCaption>
<figure confidence="0.955208266666667">
(a) N, be N,
N2
watashi[I] jonson[Johnson]
kochira[this] jimukyoku[secretariat]
denwa-bango[tel-no.] 06-951-0866[06-951-0866]
sanka-hi[fee] 85,000-en[85,000 yen]
(b) N, cost N,
yokoushuu[proc.] 30,000-en[30,000 yen]
(c) for N„ the fee Is N,
kigyou[companies] 85,000-en[85,000 yen]
(d) payment should be made by N,
hiyou[fee] ginnkou-furikomi
[bank-transfer]
(e) the conference will end on N2
saishuu-bi[final day l 10oatsu12nichifOct. 12th]
</figure>
<figureCaption confidence="0.99511">
Figure 1 0 Examples of &amp;quot;NI wa N2 da&amp;quot;
</figureCaption>
<bodyText confidence="0.403488">
The distribution of N, and N2 in the examples
</bodyText>
<figure confidence="0.998642">
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
</figure>
<page confidence="0.99011">
190
</page>
<bodyText confidence="0.999875142857143">
of our corpus vary for each case. Attention should be
given to 2-tuples of nouns, (Ni, N2). N2s of (a-4), (b)
au (c) am similar, i.e., both mean &amp;quot;prices&amp;quot;. However
Nis at not similar to each other. Nis of (a-4) and (d)
are similar, i.e., both mean &amp;quot;fee&amp;quot;. However, the N2s
are not similar to each other. Thus, EBMT is
applicable.
</bodyText>
<subsectionHeader confidence="0.973125">
5.2 &amp;quot;— o-negaishimasu&amp;quot;
</subsectionHeader>
<figureCaption confidence="0.96534">
Figure 11 exemplifies the correspondences
between sentences of the form &amp;quot;- o-negaishimasu&amp;quot;
and the English equivalents.
</figureCaption>
<figure confidence="0.9976705">
(a) may I speak to N jimukyoku[secretariat] o
o-negaishlmasu
(b) please give me N go-juusyo[add rasa] o...
(c) please pay by N genkin[cash] de...
(d) yes, please hal...
je) thank you Anroshiku...
</figure>
<figureCaption confidence="0.999689">
Figure 11 Examples of &amp;quot;- o-negaishimasu&amp;quot;
</figureCaption>
<bodyText confidence="0.999852142857143">
Translations in examples (b) arid (c) am
possible by fmding substitutes in Japanese for give
me and pay by , respectively. The conditions (Ca)
and (Cc) in section 3.1 hold. Usually, this kind of
supplement is done by contextual analysis. However,
the connection between the missing elements and the
noun in the examples is strong enough to reuse,
because it is the product of a combination of
translator expertise and domain specific restriction.
Examples (a), (d) and (e) are idiomatic expressions.
The condition (Cb) holds. The distribution of the
noun and the particle in the examples of our corpus
varies for each case in the same way as in the &amp;quot;da&amp;quot;
sentence. EBMT is applicable.
</bodyText>
<sectionHeader confidence="0.999914" genericHeader="method">
6 CONCLUDING REMARKS
</sectionHeader>
<bodyText confidence="0.999282714285714">
Example-Based Machine Translation (EBMT)
has been proposed. EBMT retrieves similar examples
(pairs of source and target expressions), adapting
them to translate a new source text.
The feasibility of EBMT has been shown by
implementing a system which translates Japanese
noun phrases of the form &amp;quot;Ni no N2&amp;quot; into English
noun phrases. The result of the experiment was
encouraging. Broad applicability of EBMT was
shown by studying the data from the text corpus. The
advantages of integrating EBMT with RBMT were
also discussed. The system has been written in
Common Lisp, and is running on a Genera 7.2
Symbolics Lisp Machine at ATR.
</bodyText>
<sectionHeader confidence="0.998532" genericHeader="method">
(1) IMPROVEMENT
</sectionHeader>
<bodyText confidence="0.998807444444444">
The more elaborate the RBMT becomes, the
less expandable it is. Considerably complex rules
concerning semantics, context, and the real world, are
required in machine translation. This is the notorious
Al bottleneck: not only is it difficult to add a new
rule to the database of rules that are mutually
dependent, but it is also difficult to build such a rule
database itself. Moreover, computation using this
huge and complex rule database is so slow that it
forces a developer to abandon efforts to improve the
system. RBMT is not easily upgraded.
However, EBMT has no rules, all the use of
examples is relatively localized. Improvement is
effected simply by inputting appropriate examples
into the database. EBMT is easily upgraded, which
the experiment in section 4.3.2 has shown: the
more examples we have, the better the
quality.
</bodyText>
<sectionHeader confidence="0.993775" genericHeader="method">
(2) RELIABILITY FACTOR
</sectionHeader>
<bodyText confidence="0.9999558125">
One of the main reasons users dislike RBMT
systems is the so-called &amp;quot;poisoned cookie&amp;quot; problem.
RBMT has no device to compute the reliability of
the result. In other words, users of RBMT cannot
trust any RBMT translation, because it may be
wrong without any such indication from system.
Consider the case where all translation processes have
been completed successfully, yet, the result is
incorrect.
In EBMT, a reliability factor is
assigned to the translation result according
to the distance between the input and the
similar examples found [see the experiment in
section 4.3.3]. In addition to this, retrieved examples
that are similar to the input convince users that the
translation is accurate.
</bodyText>
<sectionHeader confidence="0.99367" genericHeader="method">
(3) TRANSLATION SPEED
</sectionHeader>
<bodyText confidence="0.999889761904762">
RBMT translates slowly in general because it
is really a large-scale rule-based system, which
consists of analysis, transfer, and generation modules
using syntactic rules, semantic restrictions, structural
transfer rules, word selections, generation rules, and
so on. For example, the Mu system has about 2,000
rewriting and word selection rules for about 70,000
lexical items (Nagao et al. 1986).
As recently pointed out (Furuse et al. 1990),
conventional RBMT systems have been biased
toward syntactic, semantic, and contextual analysis,
which consumes considerable computing time.
However, such deep analysis is not always necessary
or useful for translation.
In contrast with this, deep semantic analysis
is avoided in EBMT because it is assumed that
translations appropriate for given domain
can be obtained using domain-specific
examples (pairs of source and target
expressions). EBMT directly returns a translation
without reasoning through a long chain of rules [see
</bodyText>
<page confidence="0.995629">
191
</page>
<bodyText confidence="0.99658025">
sections 2 and 4].
There is fear that retrieval from a large-scale
example database will prove too slow. However, it
can be accelerated effectively by both
indexing (Sumita and Tsutsumi 1988) and
parallel computing (Sumita and Iida 1991).
These processes multiply acceleration. Consequently,
the computation of EBMT is acceptably efficient.
</bodyText>
<sectionHeader confidence="0.996342" genericHeader="evaluation">
(4) ROBUSTNESS
</sectionHeader>
<bodyText confidence="0.8785075">
RBMT works on exact-match reasoning. It
fails to translate when it has no knowledge that
matches the input exactly.
However, EBMT works on best-match
reasoning. It intrinsically translates in a fail-safe way
[see sections 2 and 4].
</bodyText>
<sectionHeader confidence="0.985218" genericHeader="conclusions">
(5) TRANSLATORS EXPERTISE
</sectionHeader>
<bodyText confidence="0.999897125">
Formulating linguistic rules for RBMT is a
difficult job and requires a linguistically trained staff.
Moreover, linguistics does not deal with all
phenomena occurring in real text (Nagao 1988).
However, examples necessary for EBMT are
easy to obtain because a large number of texts and
their translations are available. These are realization
of translator expertise, which deals with all real
phenomena. Moreover, as electronic publishing
increases, more and more texts will be
machine-readable (Sadler 1989b).
EBMT is intrinsically biased toward a
sublanguage: strictly speaking, toward an example
database. This is a good feature because it provides a
way of automatically tuning itself to a
sublanguage.
</bodyText>
<sectionHeader confidence="0.999902" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999831588235294">
Furuse, 0., Sumita, E. and lida, H. 1990 &amp;quot;A Method for
Realizing Transfer-Driven Machine Translation&amp;quot;,
Reprint of WGNL 80-8, IPSJ, (in Japanese).
Hirai, M. and Kitahashi, T. 1986, &amp;quot;A Semantic
Classification of Noun Modifications in Japanese
Sentences and Their Analysis&amp;quot;, Reprint of WGNL
58-1, IPSJ, (in Japanese).
Kolodner, J. and Riesbeck, C. 1989 &amp;quot;Case-Based
Reasoning&amp;quot;, Tutorial Textbook of 11th IJCAI .
Nagao, M. 1984 &amp;quot;A Framework of a Mechanical
Translation Between Japanese and English by
Analogy Principle&amp;quot;, in A. Elithorn and R. Banerji
(ed.), Artificial and Human Intelligence,
North-Holland, 173-180.
Nagao, M. ,Tsujii, J. , Nakamura, J. 1986 &amp;quot;Machine
Translation from Japanese into English&amp;quot;,
Proceedings of the IEEE, 74,7.
Nagao, M.(chair) 1988 &amp;quot;Language Engineering : The
Real Bottleneck of Natural Language Processing&amp;quot;,
Proceedings of the 12th International Conference on
Computational Linguistics.
Nirenburg, S. 1987 Machine Translation, Cambridge
University Press, 350.
Nitta, Y. 1986 &amp;quot;Idiosyncratic Gap: A Tough Problem to
Structure-bound Machine Translation&amp;quot;, Proceedings
of the 11th International Conference on
Computational Linguistics,107 -111.
Ogura, K., Hashimoto, K. , and Morimoto, T. 1989
&amp;quot;Object-Oriented User Interface for Linguistic
Database&amp;quot;, Proceedings of Working Conference on
Data and Knowledge Base Integration, University of
Keele, England.
Ohno, S. and Hamanishi, M. 1984 Ruigo-Shin-Jiten,
Kadokawa, 932, (in Japanese).
Sadler, V. 1989a &amp;quot;Translating with a Simulated
Bilingual Knowledge Bank(BKB)&amp;quot;, BSO/Research.
Sadler, V. 1989b Working with Analogical Semantics,
Foris Publications, 256.
Sato, S. and Nagao, M. 1989 &amp;quot;Memory-Based
Translation&amp;quot;, Reprint of WGNL 70-9, IPSJ, (in
Japanese).
Sato, S. and Nagao, M. 1990 &apos;Toward Memory-Based
Translation&amp;quot;, Proceedings of the 13th International
Conference on Computational Linguistics.
Shirnazu, A. , Naito, S. , and Nomura, H. 1987
&amp;quot;Semantic Structure Analysis of Japanese Noun
Phrases with Adnominal Particles&amp;quot;, Proceedings of
the 25th Annual Meeting of the Association for
Computational Linguistics, 123-130.
Stanfill, C. and Waltz, D. 1986 &apos;Toward Memory-Based
Reasoning&amp;quot;, CACM, 29-12, 1213-1228.
&amp;units, E. and Tsutsumi, Y. 1988 &amp;quot;A Translation Aid
System Using Flexible Text Retrieval Based on
Syntax-Matching&amp;quot;, Proceedings of The Second
International Conference on Theoretical and
Methodological Issues in Machine Translation of
Natural Languages, CMU, Pittsburgh.
Sumita, E., Iida, H. and Kohyama, H. 1990a
&amp;quot;Translating with Examples: A New Approach to
Machine Translation&amp;quot;, Proceedings of The Third
International Conference on Theoretical and
Methodological Issues in Machine Translation of
Natural Languages, Texas, 203-212.
Sumita, E. lids, H. and Kohyama, H. 1990b
&amp;quot;Example-based Approach in Machine Translation&amp;quot;,
Proceedings of InfoJapan &apos;90, Part 2: 65-72.
Sumita, E. and lids, H. 1991 &amp;quot;Acceleration of
Example-Based Machine Translation&amp;quot;, (manuscript).
</reference>
<page confidence="0.99819">
192
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.271963">
<title confidence="0.87364">EXPERIMENTS AND PROSPECTS OF EXAMPLE-BASED MACHINE TRANSLATION and</title>
<author confidence="0.840266">Hitoshi HDA</author>
<affiliation confidence="0.937864">ATR Interpreting Telephony Research Laboratories</affiliation>
<address confidence="0.8437035">Sanpeidani, Inuidani, Seika-cho Souraku-gun, Kyoto 619-02, JAPAN</address>
<abstract confidence="0.989510666666667">EBMT (Example-Based Machine Translation) proposed. EBMT retrieves similar (pairs of source phrases, sentences, or and their translations) from database of examples, adapting the examples to translate a new EBMT has the following features: is easily upgraded simply by inputting appropriate examples to the database; (2) It assigns a reliability factor to the translation result; (3) It is accelerated effectively by both indexing aid parallel computing; (4) It is robust because of best-match reasoning; and (5) It well utilizes translator expertise. A prototype system has been implemented to deal with a difficult translation problem for conventional Rule-Based Machine Translation (RBMT), i.e., translating Japanese noun phrases of the form &amp;quot;N, no N,&amp;quot; into English. The system has achieved about a 78% success rate on average. This paper explains the basic idea of EBMT, illustrates the experiment in detail, explains the Wad applicability of EBMT to several difficult translation problems for RBMT and discusses the advantages of integrating EBMT with RBMT.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Sumita</author>
<author>H lida</author>
</authors>
<title>A Method for Realizing Transfer-Driven Machine Translation&amp;quot;,</title>
<date>1990</date>
<journal>Reprint of WGNL 80-8, IPSJ, (in Japanese).</journal>
<marker>Sumita, lida, 1990</marker>
<rawString>Furuse, 0., Sumita, E. and lida, H. 1990 &amp;quot;A Method for Realizing Transfer-Driven Machine Translation&amp;quot;, Reprint of WGNL 80-8, IPSJ, (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hirai</author>
<author>T Kitahashi</author>
</authors>
<title>A Semantic Classification of Noun Modifications in Japanese Sentences and Their Analysis&amp;quot;,</title>
<date>1986</date>
<journal>Reprint of WGNL 58-1, IPSJ, (in Japanese).</journal>
<contexts>
<context position="10479" citStr="Hirai and Kitahashi 1986" startWordPosition="1656" endWordPosition="1659">quire prepositions other than &amp;quot;of&amp;quot;, or no preposition at all. In only about one-fifth of &amp;quot;N, no N2&amp;quot; occurrences in our domain, &amp;quot;N2 of N,&amp;quot; would be the most appropriate English translation. We cannot use any particular preposition as an effective default value. No rules for selecting the most appropriate translation for &amp;quot;N, no N2&amp;quot; have yet been found. In other words, the condition (Ca) in section 3.1 holds. Selecting the translation for &amp;quot;N, no N2&amp;quot; is still an important and complicated problem in J-E translation. In contrast with the preceding research analyzing &amp;quot;N, no N2&amp;quot; (Shimazu et al. 1987; Hirai and Kitahashi 1986), deep semantic analysis is avoided because it is assumed that translations appropriate for given domain can be obtained using domain-specific examples (pairs of source and target The thesaurus is used in calculating the semantic distance between the content words in the input and those in the examples. It is composed of a hierarchical structure in accordance with the thesaurus of everyday Japanese written by Ohno and Hamanishi (1984). Analysis kyouto deno kaigi Example-Based Transfer d Japanese English 0.4 toukyou deno taizai the stay in Tokyo 0.4 honkon deno taizai the stay In Hongkong 0.4 t</context>
</contexts>
<marker>Hirai, Kitahashi, 1986</marker>
<rawString>Hirai, M. and Kitahashi, T. 1986, &amp;quot;A Semantic Classification of Noun Modifications in Japanese Sentences and Their Analysis&amp;quot;, Reprint of WGNL 58-1, IPSJ, (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kolodner</author>
<author>C Riesbeck</author>
</authors>
<title>Case-Based Reasoning&amp;quot;,</title>
<date>1989</date>
<booktitle>Tutorial Textbook of 11th IJCAI .</booktitle>
<contexts>
<context position="13291" citStr="Kolodner and Riesbeck 1989" startWordPosition="2114" endWordPosition="2117">attribute distance, d(li,Ei) and the weight of attribute, we ate explained in the following sections. Each translation pattern (t.p.) is abstracted from an example and is stored with the example in the example database [see Figure 6]. (a) ATTRIBUTE DISTANCE For the attribute of the adnominal particle &amp;quot;no&amp;quot;, the distance is 0 or 1 depending on whether or not they match exactly, for example, li(&amp;quot;deno&amp;quot;,&amp;quot;deno&amp;quot;)= 0 and dcdeno&amp;quot;, &amp;quot;no&amp;quot;) = 1. For semantic attributes, however, the distance varies between 0 and 1. Semantic distance d(0 5 d 5 1) is determined by the Most Specific Common Abstraction(MSCA) (Kolodner and Riesbeck 1989) obtained from the thesaurus abstraction hierarchy. When the thesaurus is (n+1) layered, (k/n) is assigned to the concepts in the k-th layer from the bottom. For example, as shown with the broken line in Figure 5, the MSCACkaigi&amp;quot; [conference], &amp;quot;tairai&amp;quot; [stay]) is &amp;quot;koudou&amp;quot; [actions] and the distance is 2/3. Of course, 0 is assigned when the MSCA is the bottom class, for instance, MSCA(&amp;quot;kyouto&amp;quot;[Kyoto], &amp;quot;toukyou&amp;quot; [Tokyo]) = &amp;quot;timer[place], or when nouns are identical ( MSCA(N, N) for any N). Thesaurus Root • (3/1-11 (b) WEIGHT OF ATTRIBUTE The weight of the attribute is the degree to which the att</context>
</contexts>
<marker>Kolodner, Riesbeck, 1989</marker>
<rawString>Kolodner, J. and Riesbeck, C. 1989 &amp;quot;Case-Based Reasoning&amp;quot;, Tutorial Textbook of 11th IJCAI .</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nagao</author>
</authors>
<title>A Framework of a Mechanical Translation Between Japanese and English by Analogy Principle&amp;quot;,</title>
<date>1984</date>
<booktitle>Artificial and Human Intelligence, North-Holland,</booktitle>
<pages>173--180</pages>
<editor>in A. Elithorn and R. Banerji (ed.),</editor>
<contexts>
<context position="2230" citStr="Nagao 1984" startWordPosition="321" endWordPosition="322">l system because of the problem of building such a large-scale rule-base. It is also difficult to improve translation performance because the effect of adding a new rule is hard to anticipate, and because translation using a large-scale rule-based system is time-consuming. Moreover, it is difficult to make use of situational or domain-specific information for translation. In order to conquer these problems in machine translation, a database of examples (pairs of source phrases, sentences, or texts and * Currently with Kyoto University their translations) has been implemented as the knowledge (Nagao 1984; Sumita and Tsutsumi 1988; Sato and Nagao 1989; Sadler 1989a; Sumita et al. 1990a, b). The translation mechanism retrieves similar examples from the database, adapting the examples to translate the new source text. This framework is called Example-Based Machine Translation (EBMT). This paper focuses on ATR&apos;s linguistic database of spoken Japanese with English translations. The corpus contains conversations about international conference registration (Ogura et al. 1989). Results of this study indicate that EBMT is a breakthrough in MT technology. Our pilot EBMT system translates Japanese noun </context>
<context position="6153" citStr="Nagao 1984" startWordPosition="951" endWordPosition="952">PPLICABILITY EBMT is applicable to many linguistic phenomena that are regarded as difficult to translate in conventional RBMT. Some are well-known among researchers of natural language processing and others have recently been given a great deal of attention. When one of the following conditions holds true for a linguistic phenomenon, RBMT is less suitable than EBMT. (Ca) Translation rule formation is difficult. (Cb) The general rule cannot accurately describe phenomena because it represents a special case, e.g., idioms. (Cc) Translation cannot be made in a compositional way from target words (Nagao 1984; Nitta 1986; Sadler 1989b). This is a list (not exhaustive) of phenomena in J-E translation that are suitable for EBMT: • optional cases with a case particle ( &amp;quot;— de&amp;quot;, &amp;quot;— ni&amp;quot;,...) • subordinate conjunction (&amp;quot;— ba —&amp;quot;, &amp;quot;— nagara —&amp;quot;, &amp;quot;— tara —&amp;quot;,...,&amp;quot;— baai —&amp;quot;,...) • noun phrases of the form &amp;quot;N, no N2&amp;quot; • sentences of the form &amp;quot;N, wa N2 da&amp;quot; • sentences lacking the main verb (eg. sentences of the form &amp;quot;— o-negaishimasu&amp;quot;) • fragmental expressions (&amp;quot;hai&amp;quot;, &amp;quot;sou-desu&amp;quot;, &amp;quot;wakarimashita&amp;quot;,...) (Furuse et al. 1990) • modality represented by the sentence ending (&amp;quot;—tainodesuga&amp;quot;, &amp;quot;—seteitadalcimasu&amp;quot;, ...) (Fur</context>
<context position="7799" citStr="Nagao (1984)" startWordPosition="1223" endWordPosition="1224">wise, in German to English translation, the German complex noun is also effectively translated by EBMT. 3.2 INTEGRATION It is not yet clear whether EBMT can or should deal with the whole process of translation. We assume that there are many kinds of phenomena. Some are suitable for EBMT, while others are suitable for RBMT. Integrating EBMT with RBMT is expected to be useful. It would be more acceptable for users if RBMT were first introduced as a base system, and then incrementally have its translation performance improved by attaching EBMT components. This is in the line with the proposal in Nagao (1984). Subsequently, we proposed a practical method of integration in (Syntax) (Semantics) (Total) • Output 186 previous papers (Sumita et al. 1990a, b). 4 EBMT FOR &amp;quot;N1 no N2&amp;quot; 4.1 THE PROBLEM &amp;quot;N, no Ny&amp;quot; is a common Japanese noun phrase form. &amp;quot;no&amp;quot; in the &amp;quot;N, no N2&amp;quot; is a Japanese adnominal particle. There are other variants, including &amp;quot;deno&amp;quot;, &amp;quot;Icarano&amp;quot;, &amp;quot;madeno&amp;quot; and so on. Roughly speaking, Japanese noun phrases of the form &amp;quot;N, no N2&amp;quot; correspond to English noun phrases of the form &amp;quot;N2 of N,&amp;quot; as shown in the examples at the top of Figure 2. Japanese English youka no gogo the afternoon of the 8th kaigi</context>
</contexts>
<marker>Nagao, 1984</marker>
<rawString>Nagao, M. 1984 &amp;quot;A Framework of a Mechanical Translation Between Japanese and English by Analogy Principle&amp;quot;, in A. Elithorn and R. Banerji (ed.), Artificial and Human Intelligence, North-Holland, 173-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tsujii</author>
</authors>
<title>Machine Translation from Japanese into English&amp;quot;,</title>
<date>1986</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>74--7</pages>
<marker>Tsujii, 1986</marker>
<rawString>Nagao, M. ,Tsujii, J. , Nakamura, J. 1986 &amp;quot;Machine Translation from Japanese into English&amp;quot;, Proceedings of the IEEE, 74,7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nagao</author>
</authors>
<title>Language Engineering : The Real Bottleneck of Natural Language Processing&amp;quot;,</title>
<date>1988</date>
<booktitle>Proceedings of the 12th International Conference on Computational Linguistics.</booktitle>
<marker>Nagao, 1988</marker>
<rawString>Nagao, M.(chair) 1988 &amp;quot;Language Engineering : The Real Bottleneck of Natural Language Processing&amp;quot;, Proceedings of the 12th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nirenburg</author>
</authors>
<title>Machine Translation,</title>
<date>1987</date>
<pages>350</pages>
<publisher>Cambridge University Press,</publisher>
<contexts>
<context position="1425" citStr="Nirenburg 1987" startWordPosition="200" endWordPosition="201">otype system has been implemented to deal with a difficult translation problem for conventional Rule-Based Machine Translation (RBMT), i.e., translating Japanese noun phrases of the form &amp;quot;N, no N,&amp;quot; into English. The system has achieved about a 78% success rate on average. This paper explains the basic idea of EBMT, illustrates the experiment in detail, explains the Wad applicability of EBMT to several difficult translation problems for RBMT and discusses the advantages of integrating EBMT with RBMT. 1 INTRODUCTION Machine Translation requires handcrafted axl complicated large-scale knowledge (Nirenburg 1987). Conventional machine translation systems use rules as the knowledge. This framework is called Rule-Based Machine Translation (RBMT). It is difficult to scale up from a toy program to a practical system because of the problem of building such a large-scale rule-base. It is also difficult to improve translation performance because the effect of adding a new rule is hard to anticipate, and because translation using a large-scale rule-based system is time-consuming. Moreover, it is difficult to make use of situational or domain-specific information for translation. In order to conquer these prob</context>
</contexts>
<marker>Nirenburg, 1987</marker>
<rawString>Nirenburg, S. 1987 Machine Translation, Cambridge University Press, 350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Nitta</author>
</authors>
<title>Idiosyncratic Gap: A Tough Problem to Structure-bound Machine Translation&amp;quot;,</title>
<date>1986</date>
<booktitle>Proceedings of the 11th International Conference on Computational Linguistics,107 -111.</booktitle>
<contexts>
<context position="6165" citStr="Nitta 1986" startWordPosition="953" endWordPosition="954"> EBMT is applicable to many linguistic phenomena that are regarded as difficult to translate in conventional RBMT. Some are well-known among researchers of natural language processing and others have recently been given a great deal of attention. When one of the following conditions holds true for a linguistic phenomenon, RBMT is less suitable than EBMT. (Ca) Translation rule formation is difficult. (Cb) The general rule cannot accurately describe phenomena because it represents a special case, e.g., idioms. (Cc) Translation cannot be made in a compositional way from target words (Nagao 1984; Nitta 1986; Sadler 1989b). This is a list (not exhaustive) of phenomena in J-E translation that are suitable for EBMT: • optional cases with a case particle ( &amp;quot;— de&amp;quot;, &amp;quot;— ni&amp;quot;,...) • subordinate conjunction (&amp;quot;— ba —&amp;quot;, &amp;quot;— nagara —&amp;quot;, &amp;quot;— tara —&amp;quot;,...,&amp;quot;— baai —&amp;quot;,...) • noun phrases of the form &amp;quot;N, no N2&amp;quot; • sentences of the form &amp;quot;N, wa N2 da&amp;quot; • sentences lacking the main verb (eg. sentences of the form &amp;quot;— o-negaishimasu&amp;quot;) • fragmental expressions (&amp;quot;hai&amp;quot;, &amp;quot;sou-desu&amp;quot;, &amp;quot;wakarimashita&amp;quot;,...) (Furuse et al. 1990) • modality represented by the sentence ending (&amp;quot;—tainodesuga&amp;quot;, &amp;quot;—seteitadalcimasu&amp;quot;, ...) (Furuse et al. 1</context>
</contexts>
<marker>Nitta, 1986</marker>
<rawString>Nitta, Y. 1986 &amp;quot;Idiosyncratic Gap: A Tough Problem to Structure-bound Machine Translation&amp;quot;, Proceedings of the 11th International Conference on Computational Linguistics,107 -111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ogura</author>
<author>K Hashimoto</author>
</authors>
<title>Object-Oriented User Interface for Linguistic Database&amp;quot;,</title>
<date>1989</date>
<booktitle>Proceedings of Working Conference on Data and Knowledge</booktitle>
<institution>Base Integration, University of Keele,</institution>
<location>England.</location>
<marker>Ogura, Hashimoto, 1989</marker>
<rawString>Ogura, K., Hashimoto, K. , and Morimoto, T. 1989 &amp;quot;Object-Oriented User Interface for Linguistic Database&amp;quot;, Proceedings of Working Conference on Data and Knowledge Base Integration, University of Keele, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ohno</author>
<author>M Hamanishi</author>
</authors>
<date>1984</date>
<journal>Ruigo-Shin-Jiten, Kadokawa,</journal>
<volume>932</volume>
<note>(in Japanese).</note>
<contexts>
<context position="10917" citStr="Ohno and Hamanishi (1984)" startWordPosition="1724" endWordPosition="1727">r &amp;quot;N, no N2&amp;quot; is still an important and complicated problem in J-E translation. In contrast with the preceding research analyzing &amp;quot;N, no N2&amp;quot; (Shimazu et al. 1987; Hirai and Kitahashi 1986), deep semantic analysis is avoided because it is assumed that translations appropriate for given domain can be obtained using domain-specific examples (pairs of source and target The thesaurus is used in calculating the semantic distance between the content words in the input and those in the examples. It is composed of a hierarchical structure in accordance with the thesaurus of everyday Japanese written by Ohno and Hamanishi (1984). Analysis kyouto deno kaigi Example-Based Transfer d Japanese English 0.4 toukyou deno taizai the stay in Tokyo 0.4 honkon deno taizai the stay In Hongkong 0.4 toukyou deno go-taizai the stay In Tokyo the conf. in Osaka the cord. in Tokyo Figure 4 illustrates the translation procedure with an actual sample. First, morphological analysis is performed for the input phrase,&amp;quot;kyouto[Kyoto] deno kaigi [conference]&amp;quot;. In this case, syntactical 1.0 oosaka no kaigi 1.0 toukyou no kaigi Generation the conf. In Kyoto Figure 4 Translation Procedure 187 analysis is not necessary. Second, similar examples a</context>
</contexts>
<marker>Ohno, Hamanishi, 1984</marker>
<rawString>Ohno, S. and Hamanishi, M. 1984 Ruigo-Shin-Jiten, Kadokawa, 932, (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Sadler</author>
</authors>
<title>Translating with a Simulated Bilingual Knowledge Bank(BKB)&amp;quot;,</title>
<date>1989</date>
<location>BSO/Research.</location>
<contexts>
<context position="2290" citStr="Sadler 1989" startWordPosition="331" endWordPosition="332">ale rule-base. It is also difficult to improve translation performance because the effect of adding a new rule is hard to anticipate, and because translation using a large-scale rule-based system is time-consuming. Moreover, it is difficult to make use of situational or domain-specific information for translation. In order to conquer these problems in machine translation, a database of examples (pairs of source phrases, sentences, or texts and * Currently with Kyoto University their translations) has been implemented as the knowledge (Nagao 1984; Sumita and Tsutsumi 1988; Sato and Nagao 1989; Sadler 1989a; Sumita et al. 1990a, b). The translation mechanism retrieves similar examples from the database, adapting the examples to translate the new source text. This framework is called Example-Based Machine Translation (EBMT). This paper focuses on ATR&apos;s linguistic database of spoken Japanese with English translations. The corpus contains conversations about international conference registration (Ogura et al. 1989). Results of this study indicate that EBMT is a breakthrough in MT technology. Our pilot EBMT system translates Japanese noun phrases of the form &amp;quot;N, no 142&amp;quot; into English noun phrases. A</context>
<context position="6178" citStr="Sadler 1989" startWordPosition="955" endWordPosition="956">licable to many linguistic phenomena that are regarded as difficult to translate in conventional RBMT. Some are well-known among researchers of natural language processing and others have recently been given a great deal of attention. When one of the following conditions holds true for a linguistic phenomenon, RBMT is less suitable than EBMT. (Ca) Translation rule formation is difficult. (Cb) The general rule cannot accurately describe phenomena because it represents a special case, e.g., idioms. (Cc) Translation cannot be made in a compositional way from target words (Nagao 1984; Nitta 1986; Sadler 1989b). This is a list (not exhaustive) of phenomena in J-E translation that are suitable for EBMT: • optional cases with a case particle ( &amp;quot;— de&amp;quot;, &amp;quot;— ni&amp;quot;,...) • subordinate conjunction (&amp;quot;— ba —&amp;quot;, &amp;quot;— nagara —&amp;quot;, &amp;quot;— tara —&amp;quot;,...,&amp;quot;— baai —&amp;quot;,...) • noun phrases of the form &amp;quot;N, no N2&amp;quot; • sentences of the form &amp;quot;N, wa N2 da&amp;quot; • sentences lacking the main verb (eg. sentences of the form &amp;quot;— o-negaishimasu&amp;quot;) • fragmental expressions (&amp;quot;hai&amp;quot;, &amp;quot;sou-desu&amp;quot;, &amp;quot;wakarimashita&amp;quot;,...) (Furuse et al. 1990) • modality represented by the sentence ending (&amp;quot;—tainodesuga&amp;quot;, &amp;quot;—seteitadalcimasu&amp;quot;, ...) (Furuse et al. 1990) • simple</context>
</contexts>
<marker>Sadler, 1989</marker>
<rawString>Sadler, V. 1989a &amp;quot;Translating with a Simulated Bilingual Knowledge Bank(BKB)&amp;quot;, BSO/Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Sadler</author>
</authors>
<title>Working with Analogical Semantics,</title>
<date>1989</date>
<pages>256</pages>
<publisher>Foris Publications,</publisher>
<contexts>
<context position="2290" citStr="Sadler 1989" startWordPosition="331" endWordPosition="332">ale rule-base. It is also difficult to improve translation performance because the effect of adding a new rule is hard to anticipate, and because translation using a large-scale rule-based system is time-consuming. Moreover, it is difficult to make use of situational or domain-specific information for translation. In order to conquer these problems in machine translation, a database of examples (pairs of source phrases, sentences, or texts and * Currently with Kyoto University their translations) has been implemented as the knowledge (Nagao 1984; Sumita and Tsutsumi 1988; Sato and Nagao 1989; Sadler 1989a; Sumita et al. 1990a, b). The translation mechanism retrieves similar examples from the database, adapting the examples to translate the new source text. This framework is called Example-Based Machine Translation (EBMT). This paper focuses on ATR&apos;s linguistic database of spoken Japanese with English translations. The corpus contains conversations about international conference registration (Ogura et al. 1989). Results of this study indicate that EBMT is a breakthrough in MT technology. Our pilot EBMT system translates Japanese noun phrases of the form &amp;quot;N, no 142&amp;quot; into English noun phrases. A</context>
<context position="6178" citStr="Sadler 1989" startWordPosition="955" endWordPosition="956">licable to many linguistic phenomena that are regarded as difficult to translate in conventional RBMT. Some are well-known among researchers of natural language processing and others have recently been given a great deal of attention. When one of the following conditions holds true for a linguistic phenomenon, RBMT is less suitable than EBMT. (Ca) Translation rule formation is difficult. (Cb) The general rule cannot accurately describe phenomena because it represents a special case, e.g., idioms. (Cc) Translation cannot be made in a compositional way from target words (Nagao 1984; Nitta 1986; Sadler 1989b). This is a list (not exhaustive) of phenomena in J-E translation that are suitable for EBMT: • optional cases with a case particle ( &amp;quot;— de&amp;quot;, &amp;quot;— ni&amp;quot;,...) • subordinate conjunction (&amp;quot;— ba —&amp;quot;, &amp;quot;— nagara —&amp;quot;, &amp;quot;— tara —&amp;quot;,...,&amp;quot;— baai —&amp;quot;,...) • noun phrases of the form &amp;quot;N, no N2&amp;quot; • sentences of the form &amp;quot;N, wa N2 da&amp;quot; • sentences lacking the main verb (eg. sentences of the form &amp;quot;— o-negaishimasu&amp;quot;) • fragmental expressions (&amp;quot;hai&amp;quot;, &amp;quot;sou-desu&amp;quot;, &amp;quot;wakarimashita&amp;quot;,...) (Furuse et al. 1990) • modality represented by the sentence ending (&amp;quot;—tainodesuga&amp;quot;, &amp;quot;—seteitadalcimasu&amp;quot;, ...) (Furuse et al. 1990) • simple</context>
</contexts>
<marker>Sadler, 1989</marker>
<rawString>Sadler, V. 1989b Working with Analogical Semantics, Foris Publications, 256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sato</author>
<author>M Nagao</author>
</authors>
<title>Memory-Based Translation&amp;quot;,</title>
<date>1989</date>
<journal>Reprint of WGNL</journal>
<note>70-9, IPSJ, (in Japanese).</note>
<contexts>
<context position="2277" citStr="Sato and Nagao 1989" startWordPosition="327" endWordPosition="330">lding such a large-scale rule-base. It is also difficult to improve translation performance because the effect of adding a new rule is hard to anticipate, and because translation using a large-scale rule-based system is time-consuming. Moreover, it is difficult to make use of situational or domain-specific information for translation. In order to conquer these problems in machine translation, a database of examples (pairs of source phrases, sentences, or texts and * Currently with Kyoto University their translations) has been implemented as the knowledge (Nagao 1984; Sumita and Tsutsumi 1988; Sato and Nagao 1989; Sadler 1989a; Sumita et al. 1990a, b). The translation mechanism retrieves similar examples from the database, adapting the examples to translate the new source text. This framework is called Example-Based Machine Translation (EBMT). This paper focuses on ATR&apos;s linguistic database of spoken Japanese with English translations. The corpus contains conversations about international conference registration (Ogura et al. 1989). Results of this study indicate that EBMT is a breakthrough in MT technology. Our pilot EBMT system translates Japanese noun phrases of the form &amp;quot;N, no 142&amp;quot; into English no</context>
<context position="6810" citStr="Sato and Nagao 1989" startWordPosition="1057" endWordPosition="1060">a list (not exhaustive) of phenomena in J-E translation that are suitable for EBMT: • optional cases with a case particle ( &amp;quot;— de&amp;quot;, &amp;quot;— ni&amp;quot;,...) • subordinate conjunction (&amp;quot;— ba —&amp;quot;, &amp;quot;— nagara —&amp;quot;, &amp;quot;— tara —&amp;quot;,...,&amp;quot;— baai —&amp;quot;,...) • noun phrases of the form &amp;quot;N, no N2&amp;quot; • sentences of the form &amp;quot;N, wa N2 da&amp;quot; • sentences lacking the main verb (eg. sentences of the form &amp;quot;— o-negaishimasu&amp;quot;) • fragmental expressions (&amp;quot;hai&amp;quot;, &amp;quot;sou-desu&amp;quot;, &amp;quot;wakarimashita&amp;quot;,...) (Furuse et al. 1990) • modality represented by the sentence ending (&amp;quot;—tainodesuga&amp;quot;, &amp;quot;—seteitadalcimasu&amp;quot;, ...) (Furuse et al. 1990) • simple sentences (Sato and Nagao 1989) This paper discusses a detailed experiment for &amp;quot;N, no N2&amp;quot; in section 4 and prospects for other phenomena, &amp;quot;N, wa N2 da&amp;quot; and &amp;quot;— o-negaishimasu&amp;quot; in section 5. Similar phenomena in other language pairs can be found. For example, in Spanish to English translation, the Spanish preposition &amp;quot;de&amp;quot;, with its broad usage like Japanese &amp;quot;no&amp;quot;, is also effectively translated by EBMT. Likewise, in German to English translation, the German complex noun is also effectively translated by EBMT. 3.2 INTEGRATION It is not yet clear whether EBMT can or should deal with the whole process of translation. We assume th</context>
</contexts>
<marker>Sato, Nagao, 1989</marker>
<rawString>Sato, S. and Nagao, M. 1989 &amp;quot;Memory-Based Translation&amp;quot;, Reprint of WGNL 70-9, IPSJ, (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sato</author>
<author>M Nagao</author>
</authors>
<title>Toward Memory-Based Translation&amp;quot;,</title>
<date>1990</date>
<booktitle>Proceedings of the 13th International Conference on Computational Linguistics.</booktitle>
<marker>Sato, Nagao, 1990</marker>
<rawString>Sato, S. and Nagao, M. 1990 &apos;Toward Memory-Based Translation&amp;quot;, Proceedings of the 13th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Naito</author>
</authors>
<title>Semantic Structure Analysis of Japanese Noun Phrases with Adnominal Particles&amp;quot;,</title>
<date>1987</date>
<booktitle>Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>123--130</pages>
<marker>Naito, 1987</marker>
<rawString>Shirnazu, A. , Naito, S. , and Nomura, H. 1987 &amp;quot;Semantic Structure Analysis of Japanese Noun Phrases with Adnominal Particles&amp;quot;, Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics, 123-130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Stanfill</author>
<author>D Waltz</author>
</authors>
<title>Toward Memory-Based Reasoning&amp;quot;,</title>
<date>1986</date>
<journal>CACM,</journal>
<pages>29--12</pages>
<contexts>
<context position="14018" citStr="Stanfill and Waltz (1986)" startWordPosition="2231" endWordPosition="2234">gned to the concepts in the k-th layer from the bottom. For example, as shown with the broken line in Figure 5, the MSCACkaigi&amp;quot; [conference], &amp;quot;tairai&amp;quot; [stay]) is &amp;quot;koudou&amp;quot; [actions] and the distance is 2/3. Of course, 0 is assigned when the MSCA is the bottom class, for instance, MSCA(&amp;quot;kyouto&amp;quot;[Kyoto], &amp;quot;toukyou&amp;quot; [Tokyo]) = &amp;quot;timer[place], or when nouns are identical ( MSCA(N, N) for any N). Thesaurus Root • (3/1-11 (b) WEIGHT OF ATTRIBUTE The weight of the attribute is the degree to which the attribute influences the selection of the translation pattern(t.p.). We adopt the expression (2) used by Stanfill and Waltz (1986) for memory-based reasoning, to implement the intuition. t.p. freq. t.p. freq. t.p. freq. B in A 12/27 B in A 3/3 B 9/24 AB 4/27 AB 9/24 B from A 2/27 B in A 2/24 BA 2/27 A&apos;s B 1/24 B to A 1/27 B on A 1/24 (El=timei) (E2=deno) (E3=soudan) (Place) I fin) I (meetings) Figure 6 Weight of the i-th attribute kOU [actions] 2/3) touchaku [arrive] kaisetsu [commentary] kaigi taizai [conference] [stay] X X chinjutsu ourai [statements] comings &amp; goings] (1/3) 1/3) Figure 5 Thesaurus(portion) setsumei [explanations] (0) soudan [meetings (0) tame! [stays] (0) • hatchaku [arrivals &amp; departures; (0) 4.(2)-$</context>
</contexts>
<marker>Stanfill, Waltz, 1986</marker>
<rawString>Stanfill, C. and Waltz, D. 1986 &apos;Toward Memory-Based Reasoning&amp;quot;, CACM, 29-12, 1213-1228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E &amp;units</author>
<author>Y Tsutsumi</author>
</authors>
<title>A Translation Aid System Using Flexible Text Retrieval Based on Syntax-Matching&amp;quot;,</title>
<date>1988</date>
<booktitle>Proceedings of The Second International Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages, CMU,</booktitle>
<location>Pittsburgh.</location>
<marker>&amp;units, Tsutsumi, 1988</marker>
<rawString>&amp;units, E. and Tsutsumi, Y. 1988 &amp;quot;A Translation Aid System Using Flexible Text Retrieval Based on Syntax-Matching&amp;quot;, Proceedings of The Second International Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages, CMU, Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Sumita</author>
<author>H Iida</author>
<author>H Kohyama</author>
</authors>
<title>Translating with Examples: A New Approach to Machine Translation&amp;quot;,</title>
<date>1990</date>
<booktitle>Proceedings of The Third International Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,</booktitle>
<pages>203--212</pages>
<location>Texas,</location>
<contexts>
<context position="2311" citStr="Sumita et al. 1990" startWordPosition="333" endWordPosition="336"> It is also difficult to improve translation performance because the effect of adding a new rule is hard to anticipate, and because translation using a large-scale rule-based system is time-consuming. Moreover, it is difficult to make use of situational or domain-specific information for translation. In order to conquer these problems in machine translation, a database of examples (pairs of source phrases, sentences, or texts and * Currently with Kyoto University their translations) has been implemented as the knowledge (Nagao 1984; Sumita and Tsutsumi 1988; Sato and Nagao 1989; Sadler 1989a; Sumita et al. 1990a, b). The translation mechanism retrieves similar examples from the database, adapting the examples to translate the new source text. This framework is called Example-Based Machine Translation (EBMT). This paper focuses on ATR&apos;s linguistic database of spoken Japanese with English translations. The corpus contains conversations about international conference registration (Ogura et al. 1989). Results of this study indicate that EBMT is a breakthrough in MT technology. Our pilot EBMT system translates Japanese noun phrases of the form &amp;quot;N, no 142&amp;quot; into English noun phrases. About a 78% success ra</context>
<context position="7941" citStr="Sumita et al. 1990" startWordPosition="1242" endWordPosition="1245">ear whether EBMT can or should deal with the whole process of translation. We assume that there are many kinds of phenomena. Some are suitable for EBMT, while others are suitable for RBMT. Integrating EBMT with RBMT is expected to be useful. It would be more acceptable for users if RBMT were first introduced as a base system, and then incrementally have its translation performance improved by attaching EBMT components. This is in the line with the proposal in Nagao (1984). Subsequently, we proposed a practical method of integration in (Syntax) (Semantics) (Total) • Output 186 previous papers (Sumita et al. 1990a, b). 4 EBMT FOR &amp;quot;N1 no N2&amp;quot; 4.1 THE PROBLEM &amp;quot;N, no Ny&amp;quot; is a common Japanese noun phrase form. &amp;quot;no&amp;quot; in the &amp;quot;N, no N2&amp;quot; is a Japanese adnominal particle. There are other variants, including &amp;quot;deno&amp;quot;, &amp;quot;Icarano&amp;quot;, &amp;quot;madeno&amp;quot; and so on. Roughly speaking, Japanese noun phrases of the form &amp;quot;N, no N2&amp;quot; correspond to English noun phrases of the form &amp;quot;N2 of N,&amp;quot; as shown in the examples at the top of Figure 2. Japanese English youka no gogo the afternoon of the 8th kaigi no mokuteki the object of the conference kaigi no sankaryou the application fee fo r the conf. ?the application fee of the conf. kyouto deno </context>
</contexts>
<marker>Sumita, Iida, Kohyama, 1990</marker>
<rawString>Sumita, E., Iida, H. and Kohyama, H. 1990a &amp;quot;Translating with Examples: A New Approach to Machine Translation&amp;quot;, Proceedings of The Third International Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages, Texas, 203-212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E lids Sumita</author>
<author>H</author>
<author>H Kohyama</author>
</authors>
<title>Example-based Approach in Machine Translation&amp;quot;,</title>
<date>1990</date>
<booktitle>Proceedings of InfoJapan &apos;90, Part</booktitle>
<volume>2</volume>
<pages>65--72</pages>
<contexts>
<context position="2311" citStr="Sumita et al. 1990" startWordPosition="333" endWordPosition="336"> It is also difficult to improve translation performance because the effect of adding a new rule is hard to anticipate, and because translation using a large-scale rule-based system is time-consuming. Moreover, it is difficult to make use of situational or domain-specific information for translation. In order to conquer these problems in machine translation, a database of examples (pairs of source phrases, sentences, or texts and * Currently with Kyoto University their translations) has been implemented as the knowledge (Nagao 1984; Sumita and Tsutsumi 1988; Sato and Nagao 1989; Sadler 1989a; Sumita et al. 1990a, b). The translation mechanism retrieves similar examples from the database, adapting the examples to translate the new source text. This framework is called Example-Based Machine Translation (EBMT). This paper focuses on ATR&apos;s linguistic database of spoken Japanese with English translations. The corpus contains conversations about international conference registration (Ogura et al. 1989). Results of this study indicate that EBMT is a breakthrough in MT technology. Our pilot EBMT system translates Japanese noun phrases of the form &amp;quot;N, no 142&amp;quot; into English noun phrases. About a 78% success ra</context>
<context position="7941" citStr="Sumita et al. 1990" startWordPosition="1242" endWordPosition="1245">ear whether EBMT can or should deal with the whole process of translation. We assume that there are many kinds of phenomena. Some are suitable for EBMT, while others are suitable for RBMT. Integrating EBMT with RBMT is expected to be useful. It would be more acceptable for users if RBMT were first introduced as a base system, and then incrementally have its translation performance improved by attaching EBMT components. This is in the line with the proposal in Nagao (1984). Subsequently, we proposed a practical method of integration in (Syntax) (Semantics) (Total) • Output 186 previous papers (Sumita et al. 1990a, b). 4 EBMT FOR &amp;quot;N1 no N2&amp;quot; 4.1 THE PROBLEM &amp;quot;N, no Ny&amp;quot; is a common Japanese noun phrase form. &amp;quot;no&amp;quot; in the &amp;quot;N, no N2&amp;quot; is a Japanese adnominal particle. There are other variants, including &amp;quot;deno&amp;quot;, &amp;quot;Icarano&amp;quot;, &amp;quot;madeno&amp;quot; and so on. Roughly speaking, Japanese noun phrases of the form &amp;quot;N, no N2&amp;quot; correspond to English noun phrases of the form &amp;quot;N2 of N,&amp;quot; as shown in the examples at the top of Figure 2. Japanese English youka no gogo the afternoon of the 8th kaigi no mokuteki the object of the conference kaigi no sankaryou the application fee fo r the conf. ?the application fee of the conf. kyouto deno </context>
</contexts>
<marker>Sumita, H, Kohyama, 1990</marker>
<rawString>Sumita, E. lids, H. and Kohyama, H. 1990b &amp;quot;Example-based Approach in Machine Translation&amp;quot;, Proceedings of InfoJapan &apos;90, Part 2: 65-72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Sumita</author>
<author>H lids</author>
</authors>
<title>Acceleration of Example-Based Machine Translation&amp;quot;,</title>
<date>1991</date>
<location>(manuscript).</location>
<marker>Sumita, lids, 1991</marker>
<rawString>Sumita, E. and lids, H. 1991 &amp;quot;Acceleration of Example-Based Machine Translation&amp;quot;, (manuscript).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>