<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<note confidence="0.986637666666667">
Proceedings of the Workshop on Effective Tools and Methodologies for Teaching
Natural Language Processing and Computational Linguistics, Philadelphia,
July 2002, pp. 33-38. Association for Computational Linguistics.
</note>
<title confidence="0.989992">
A non-programming introduction to computer science via
NLP, IR, and AI
</title>
<author confidence="0.995093">
Lillian Lee
</author>
<affiliation confidence="0.9904485">
Department of Computer Science
Cornell University
</affiliation>
<address confidence="0.911988">
Ithaca, NY, USA, 14853-7501
</address>
<email confidence="0.999541">
llee@cs.cornell.edu
</email>
<sectionHeader confidence="0.995651" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999871888888889">
This paper describes a new Cornell
University course serving as a non-
programming introduction to com-
puter science, with natural language
processing and information retrieval
forming a crucial part of the syllabus.
Material was drawn from a wide vari-
ety of topics (such as theories of dis-
course structure and random graph
models of the World Wide Web) and
presented at some technical depth, but
was massaged to make it suitable for
a freshman-level course. Student feed-
back from the first running of the class
was overall quite positive, and a grant
from the GE Fund has been awarded
to further support the course’s devel-
opment and goals.
</bodyText>
<sectionHeader confidence="0.999129" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.987793895833333">
Algorithmic concepts and programming tech-
niques from computer science are very useful to
researchers in natural language processing. To
ensure the continued strength of our field, then,
it is important to encourage undergraduates in-
terested in NLP to take courses conveying com-
puter science content. This is especially true for
students not intending to become computer sci-
ence majors.
Usually, one points beginning students inter-
ested in NLP towards the first programming
course (henceforth “CS101”). However, at many
institutions, CS101 is mandatory for a large por-
tion of the undergraduates (e.g., all engineering
students) and is designed primarily to trans-
mit specific programming skills. Experience
suggests that a significant fraction of students
find CS101’s emphasis on skills rather than con-
cepts unstimulating, and therefore decide not to
take further computer science courses. Unfortu-
nately, down the road this results in fewer en-
tering NLP graduate students having been ed-
ucated in important advanced computer-science
concepts. Furthermore, fewer students are intro-
duced to NLP at all, since the subject is often
presented only in upper-level computer-science
classes.
In an attempt to combat these problems, I cre-
ated a new freshman-level course, Computation,
Information, and Intelligence&apos;, designed to in-
troduce entering undergraduates to some of the
ideas and goals of AI (and hence computer sci-
ence). The premise was that if freshmen first
learned something of what artificial intelligence
is about, what the technical issues are, what
has been accomplished, and what remains to be
done, then they would be much more motivated
when taking CS101, because they would under-
stand what they are learning programming for.
Three major design decisions were made at
the outset:
• No programming: Teaching elementary pro-
gramming would be a needless reduplication of
effort, since programming pedagogy is already
well-honed in CS101 and other such classes.
Moreover, it was desirable to attract students
having little or no programming experience: the
new course would offer them an opportunity for
</bodyText>
<footnote confidence="0.903236">
1http://www.cs.cornell.edu/courses/cs172/2001fa
</footnote>
<bodyText confidence="0.997520666666667">
initial exploration at a conceptual level. Indeed,
for the first edition of the class, students with
programming experience were actively discour-
aged from enrolling, in order to ensure a more
level playing field for those without such back-
ground.2
</bodyText>
<listItem confidence="0.859881333333333">
• Emphasis on technically challenging material:
Although no programming would be involved,
the course would nevertheless bring students
face-to-face with substantial technical material
requiring mathematical and abstract reasoning
(indeed, topics from graduate courses in NLP
</listItem>
<bodyText confidence="0.961559133333333">
and machine learning were incorporated). To
achieve this aim, the main coursework would
involve challenging pencil-and-paper problems
with significant mathematical content.3
Of course, one had to be mindful that the in-
tended audience was college freshmen, and thus
one could only assume basic calculus as a pre-
requisite. Even working within this constraint,
though, it was possible to craft problem sets
and exams in which students explored concepts
in some depth; the typical homework problem
asked them not just to demonstrate comprehen-
sion of lecture material but to investigate alter-
native proposals. Sample questions are included
in the appendix.
</bodyText>
<listItem confidence="0.84687">
• Substantial NLP and IR content:4 Because
</listItem>
<bodyText confidence="0.868886833333333">
many students have a lot of experience with
search engines, and, of course, all students have
a great deal of experience with language, NLP
and IR are topics that freshmen can easily relate
to without being introduced to a lot of back-
ground first.
</bodyText>
<footnote confidence="0.964193428571429">
2Students who have programmed previously are more
likely to happily enroll in further computer science
courses, and thus are already well-served by the standard
curriculum.
3An alternative to a technically- and mathematically-
oriented course would have been a “computers and the
humanities” class, but Cornell already offers classes on
the history of computing, the philosophy of AI, and the
social implications of living in an information society.
One of the goals for Computation, Information, and In-
telligence was that students learn what “doing AI” is re-
ally like.
4In this class, I treated information retrieval as a spe-
cial type of NLP for simplicity’s sake.
</footnote>
<sectionHeader confidence="0.824167" genericHeader="introduction">
2 Course content
</sectionHeader>
<bodyText confidence="0.999967025">
The course title, Computation, Information,
and Intelligence, reflects its organization, which
was inspired by Herb Simon’s (1977) statement
that “Knowledge without appropriate proce-
dures for its use is dumb, and procedure without
suitable knowledge is blind.” More specifically,
the first 15 lectures were mainly concerned with
algorithms and computation (game-tree search,
perceptron learning, nearest-neighbor learning,
Turing machines, and the halting problem). For
the purposes of this workshop, though, this pa-
per focuses on the remaining 19 lectures, which
were devoted to information, and in particular,
to IR and NLP. As mentioned above, sample
homework problems for each of the units listed
below can be found in the appendix.
We now outline the major topics of the last
22 lectures. Observe that IR was presented be-
fore NLP, because the former was treated as a
special, simpler case of the latter; that is, we
first treated documents as bags of words before
considering relations between words.
Document retrieval [3 lectures]. Students
were first introduced to the Boolean query re-
trieval model, and hence to the concepts of index
data structures (arrays and B-trees) and binary
search. We then moved on to the vector space
models, and considered simple term-weighting
schemes like tf-idf.
The Web [4 lectures]. After noting how Van-
nevar Bush’s (1945) famous “Memex” article an-
ticipated the development of the Web, we stud-
ied the Web’s global topology, briefly consider-
ing the implications of its so-called “bow-tie”
structure (Broder et al., 2000) for web crawlers
— students were thus introduced to graph-
theoretic notions of strongly-connected compo-
nents and node degrees. Then, we investigated
Kleinberg’s (1998) hubs and authorities algo-
rithm as an alternative to mere in-link counting:
</bodyText>
<footnote confidence="0.986503833333333">
5This does require some linear algebra background in
that one needs to compute inner products, but this was
covered in the section of the course on perceptrons. Since
trigonometry is actually relatively fresh in the minds of
first-year students, their geometric intuitions tended to
serve them fairly well.
</footnote>
<bodyText confidence="0.9998474375">
fortunately, the method is simple enough that
students could engage in hand simulations. Fi-
nally, we looked at the suitability of various ran-
dom graph generation models (e.g., the “rich-
get-richer” (Barab´asi et al., 1999) and “copy-
ing” models (Kumar et al., 2000)) for capturing
the local structure of the Web, such as the phe-
nomenon of in-degree distributions following a
power law — conveniently, these concepts could
be presented in such a way that only required
the students to have intuitive notions of proba-
bility and the ability to take derivatives.
Language structure [7 lectures]. Relying on
students’ instincts about and experience with
language, we considered evidence for the exis-
tence of hidden language structure; such clues
included possible and impossible syntactic and
discourse ambiguities, and movement, prosody
and pause cues for constituents. To describe
this structure, we formally defined context-free
grammars. We then showed how (a tiny frag-
ment of) X-bar theory can be modeled by a
context-free grammar and, using its structural
assignments and the notion of heads of con-
stituents, accounted for some of the ambiguities
and non-ambiguities in the linguistic examples
we previously examined.
The discussion of context-free grammars nat-
urally led us to pushdown automata (which pro-
vided a nice contrast to the Turing machines
we studied earlier in the course). And, hav-
ing thus introduced stacks, we then investigated
the Grosz and Sidner (1986) stack-based theory
of discourse structure, showing that language
structures exist at granularities beyond the sen-
tence level.
Statistical language processing [6 lectures]
We began this unit by considering word fre-
quency distributions, and in particular, Zipf’s
law — note that our having studied power-law
distributions in the Web unit greatly facilitated
this discussion. In fact, because we had pre-
viously investigated generative models for the
Web, it was natural to consider Miller’s (1957)
“monkeys” model which demonstrates that very
simple generative models can account for Zipf’s
law. Next, we looked at methods taking advan-
tage of statistical regularities, including the IBM
Candide statistical machine translation system,
following Knight’s (1999) tutorial and treating
probabilities as weights. It was interesting to
point out parallels with the hubs and authorities
algorithm — both are iterative update proce-
dures with auxiliary information (alignments in
one case, hubs in the other). We also discussed
an intuitive algorithm for Japanese segmenta-
tion drawn from one of my own recent research
collaborations (Ando and Lee, 2000), and how
word statistics were applied to determining the
authorship of the Federalist Papers (Mosteller
and Wallace, 1984). We concluded with an ex-
amination of human statistical learning, focus-
ing on recent evidence indicating that human
infants can use statistics when learning to seg-
ment continuous speech into words (Saffran et
al., 1996).
The Turing test [2 lectures] Finally, we
ended the course with a consideration of intel-
ligence in the large. In particular, we focused
on Turing’s (1950) proposal of the “imitation
game”, which can be interpreted as one of the
first appearances of the claim that natural lan-
guage processing is “AI-complete”, and Searle’s
(1980) “Chinese Room” rebuttal that fluent lan-
guage behavior is not a sufficient indication of
intelligence. Then, we concluded with an exam-
ination of the first running of the Restricted Tur-
ing Test (Shieber, 1994), which served as an ob-
ject lesson as to the importance of careful eval-
uation in NLP, or indeed any science.
</bodyText>
<sectionHeader confidence="0.999379" genericHeader="background">
3 Experience
</sectionHeader>
<bodyText confidence="0.999973212121212">
Twenty-three students enrolled, with only one-
third initially expressing interest in majoring in
computer science. By the end, I was approached
by four students asking if there were research op-
portunities available in the topics we had cov-
ered; interestingly, one of these students had
originally intended to major in electrical engi-
neering. Furthermore, because of the class’s
promise in drawing students into further com-
puter science study, the GE Fund awarded a
grant for the purpose of bringing in a senior out-
side speaker and supporting teaching assistants
in future years.
One issue that remains to be resolved is the
lack, to my knowledge, of a textbook or text-
books that would both cover the syllabus topics
and employ a level of presentation suitable for
freshmen. For first-year students to learn effec-
tively, some sort of reference seems crucial, but
a significant portion of the course material was
drawn from research papers that would proba-
bly be too difficult. In the next edition of the
course, I plan to write up and distributeformal
lecture notes.
Overall, although Computation, Information,
and Intelligence proved quite challenging for the
students, for the most part they felt that they
had learned a lot from the experience, and based
on this evidence and the points outlined in the
previous paragraph, I believe that the course did
make definite progress towards its goal of inter-
esting students in taking further computer sci-
ence courses, especially in AI, IR, and NLP.
</bodyText>
<sectionHeader confidence="0.994756" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999956285714286">
I thank my chair Charles Van Loan for en-
couraging me to develop the course described
in this paper, for discussing many aspects of
the class with me, and for contacting the GE
Fund, which I thank for supplying a grant sup-
porting the future development of the class.
Thanks to Jon Kleinberg for many helpful dis-
cussions, especially regarding curriculum con-
tent, and to the anonymous reviewers for their
feedback. Finally, I am very grateful to my
teaching assistants, Amanda Holland-Minkley,
Milo Polte, and Neeta Rattan, who helped im-
mensely in making the first outing of the course
run smoothly.
</bodyText>
<sectionHeader confidence="0.983833" genericHeader="references">
References
</sectionHeader>
<subsectionHeader confidence="0.691212">
Rie Kubota Ando and Lillian Lee. 2000. Mostly-
</subsectionHeader>
<bodyText confidence="0.5248635">
unsupervised statistical segmentation of Japanese.
In First Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics (NAACL), pages 241–248.
</bodyText>
<note confidence="0.751506625">
Albert-L´aszl´o Barab´asi, R´eka Albert, and Hawoong
Jeong. 1999. Mean-field theory for scale-free ran-
dom networks. Physica, 272:173–187.
Andrei Broder, Ravi Kumar, Farzin Maghoul, Prab-
hakar Raghavan, Sridhar Rajagopalan, Raymie
Stata, Andrew Tomkins, and Janet Wiener. 2000.
Graph structure in the web. In Proceedings of the
Ninth International World Wide Web Conference,
</note>
<reference confidence="0.941245232558139">
pages 309–430.
Vannevar Bush. 1945. As we may think. The At-
lantic Monthly, 176(1):101–108.
Ralph Grishman. 1986. Computational Linguistics:
An Introduction. Studies in Natural Language
Processing. Cambridge.
Barbara J. Grosz and Candace L. Sidner. 1986. At-
tention, intentions, and the structure of discourse.
Computational Linguistics, 12(3):175–204.
Jon Kleinberg. 1998. Authoritative sources in a hy-
perlinked environment. In Proceedings of the 9th
ACM-SIAM Symposium on Discrete Algorithms
(SODA), pages 668–677.
Kevin Knight. 1999. A statistical MT tutorial work-
book. http://www.isi.edu/natural-language/-
mt/wkbk.rtf, August.
Ravi Kumar, Prabhakar Raghavan, Sridhar Ra-
jagopolan, D. Sivakumar, Andrew Tomkins, and
Eli Upfal. 2000. Stochastic models for the web
graph. In Proceedings of the 41st IEEE Sympo-
sium on the Foundations of Computer Science,
pages 57–65.
George A. Miller. 1957. Some effects of intermittent
silence. American Journal of Psychology, 70:311–
313.
Frederick Mosteller and David L. Wallace. 1984. Ap-
plied Bayesian and Classical Inference: The Case
of the Federalist Papers. Springer-Verlag.
Jenny R. Saffran, Richard N. Aslin, and Elissa L.
Newport. 1996. Statistical learning by 8-month-
old infants. Science, 274(5294):1926–1928, De-
cember.
John R. Searle. 1980. Minds, brains, and programs.
Behavioral and Brain Sciences, 3(3):417–457.
Stuart M. Shieber. 1994. Lessons from a re-
stricted Turing test. Communications of the
ACM, 37(6):70–78.
Herb A. Simon. 1977. Artificial intelligence systems
that understand. In Proceedings of the Fifth In-
ternational Joint Conference on Artificial Intelli-
gence, volume 2, pages 1059–1073.
Alan M. Turing. 1950. Computing machinery and
intelligence. Mind, LIX:433–60.
</reference>
<bodyText confidence="0.954824">
Appendix: sample homework they could be). Consider the following set of
problems web pages (all presumably on the same topic):
</bodyText>
<subsectionHeader confidence="0.431428">
IR unit
</subsectionHeader>
<bodyText confidence="0.994139875">
For simplicity, in this question, let the document
vector entries be the term frequencies normal-
ized by vector length.
Suppose someone proposes to you to inte-
grate negative information by converting a query
q = “x1,x2,...,xj,¬y1,¬y2, ... ,¬yk” to an m-
dimensional query vector −q as follows: the ith
entry qi of −q is:
</bodyText>
<equation confidence="0.969735666666667">
{ 0, wi not in the query
1, wi is a positive query term
−1, wi is a negative query term
</equation>
<bodyText confidence="0.999381625">
They claim the -1 entries in the query vector will
prevent documents that contain negative query
terms from being ranked highly.
Show that this claim is incorrect, as follows.
Let the entire three-word vocabulary be w1 =
alligator, w2 = bat, and w3 = cat, and let q =
“alligator, bat, ¬cat”. Give two documents d1
and d2 such that
</bodyText>
<listItem confidence="0.991955">
• d1 and d2 both contain exactly 8 words (ob-
viously, some will be repetitions);
• d1 does not contain the word “cat”;
• d2 does contain the word “cat”; and yet,
• d2 is ranked more highly with respect to q
than d1 is.
</listItem>
<bodyText confidence="0.982441428571429">
Explain your answer; remember to show the
document vectors corresponding to d1 and d2,
the query vector, and how you computed them.
Make sure the documents you choose and corre-
sponding document vectors satisfy all the con-
straints of this problem, including how the doc-
uments get transformed into document vectors.
</bodyText>
<subsectionHeader confidence="0.915855">
Web unit
</subsectionHeader>
<bodyText confidence="0.99445772">
In this question, we engage in some preliminary
explorations as to how many “colluding” web
pages might be needed to “spam” the hubs-and-
authorities algorithm (henceforth HA).
Let m and n be two whole numbers bigger
than 1 (m and n need not be equal, although
That is, all of the m Pi pages point to Q, and all
of the n Yj pages point to all of the n Zk pages.
(a) Let m = 5 and n = 3 (thus, m = 2n − 1,
and in particular m &gt; n), and suppose HA is run
for two iterations. What are the best hub and
authority pages? Explain your answers, show-
ing your computations of the hub and authority
scores of every web page (using the tabular for-
mat from class is fine).
(b) Now, let n be some whole number greater
than 1, and let m = n2. Suppose HA is run
for two iterations in this situation. What are
the best hub and authority pages? Explain your
answers, showing your computations of the hub
and authority scores of every web page. (Note:
in this problem, you don’t get to choose n; we’re
trying to see what happens in general if there are
a quadratic number of colluding web pages. So
treat n as an unknown but fixed constant.)
</bodyText>
<subsectionHeader confidence="0.776908">
Language structure unit
</subsectionHeader>
<bodyText confidence="0.733239">
Recall the Grishman (1986) “next train to
Boston” dialog:
</bodyText>
<listItem confidence="0.997210333333333">
(1) A: Do you know when the next
train to Boston leaves?
(2) B: Yes.
(3) A: I want to know when the
train to Boston leaves.
(4) B: I understand.
</listItem>
<bodyText confidence="0.99195875">
(a) Using the Grosz/Sidner model, analyze
the discourse structure of the entire conversa-
tion from the point of view of speaker A. That
is, give the discourse segments (i.e., “DS1 con-
sists of sentences 1 and 3, and DS2 consists of
sentences 2 and 4”), the corresponding discourse
segment purposes, and the intentional structure
of the conversation. Then, show what the focus
</bodyText>
<figure confidence="0.99426119047619">
P1
P2
...
...
Pm
Y1
Y2
Ya
...
...
...
...
...
...
...
Z1
Z2
Za...
...
Q
qi =
</figure>
<reference confidence="0.723145357142857">
stack is after each sentence is uttered. Explain
how you determined your answers.
(b) Repeat the above subproblem, but from
the point of view of speaker B.
(c) Would your answers to the previous sub-
problem change if sentence (4) had been “Why
are you telling me these things?” Does the
Grosz/Sidner model adequately account for this
case? Explain.
Statistical language processing unit
In this problem, we explicitly derive a type of
power-law behavior in the “Miller’s monkeys”
model (henceforth MMM) from class. First, a
useful fact: for any fixed integers n &gt; 1 and
</reference>
<equation confidence="0.86175675">
k &gt; 0,
nk+1 − n
ni =
n − 1
</equation>
<bodyText confidence="0.927918263157895">
In each of the following subproblems, we rank
the “words” (remember that “zz” is a “word”
in the MMM) by their probability, rather than
by corpus frequency. Also, j refers to some fixed
but unknown integer greater than 1; hence, your
answers should generally be functions of j.
(a) Show mathematically that the number of
words that are shorter than j letters long is
26
�26j−1 − 1� .
25
(b) Compute the maximum possible rank for a
word that is j letters long; explain your answer.
(c) Using your answers to the previous sub-
problems, find the function AR(j), the aver-
age rank of a word that is j letters long, show-
ing your work. (For example, you might say
“AR(j) = 4 − j”.)
(d) The probability of a word of length j is
</bodyText>
<equation confidence="0.502491">
P(j) = 127 x(1 �j (that we aren’t combining like
27
</equation>
<bodyText confidence="0.99979425">
terms is meant to be helpful ...). Show mathe-
matically that the AR(j) function you computed
above and the probability function P(j) have a
particularly simple power-law relationship:
</bodyText>
<equation confidence="0.879569333333333">
1
AR(j) � a x
P(j)
</equation>
<bodyText confidence="0.997535428571429">
for some constant a that doesn’t depend on j.
You may make some reasonable approximations,
for example, saying that n+1
n+2 is close enough to
1 that we can replace it by 1 for argument’s
sake; but please make all such approximations
explicit.
</bodyText>
<equation confidence="0.897558666666667">
k
i=1
.
</equation>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.237316">
<title confidence="0.837717">Proceedings of the Workshop on Effective Tools and Methodologies for Teaching</title>
<affiliation confidence="0.274074">Natural Language Processing and Computational Linguistics, Philadelphia,</affiliation>
<address confidence="0.497336">July 2002, pp. 33-38. Association for Computational Linguistics.</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<pages>309--430</pages>
<marker></marker>
<rawString>pages 309–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vannevar Bush</author>
</authors>
<title>As we may think. The Atlantic Monthly,</title>
<date>1945</date>
<marker>Bush, 1945</marker>
<rawString>Vannevar Bush. 1945. As we may think. The Atlantic Monthly, 176(1):101–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
</authors>
<title>Computational Linguistics: An Introduction.</title>
<date>1986</date>
<booktitle>Studies in Natural Language Processing.</booktitle>
<location>Cambridge.</location>
<marker>Grishman, 1986</marker>
<rawString>Ralph Grishman. 1986. Computational Linguistics: An Introduction. Studies in Natural Language Processing. Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="8941" citStr="Grosz and Sidner (1986)" startWordPosition="1370" endWordPosition="1373">for constituents. To describe this structure, we formally defined context-free grammars. We then showed how (a tiny fragment of) X-bar theory can be modeled by a context-free grammar and, using its structural assignments and the notion of heads of constituents, accounted for some of the ambiguities and non-ambiguities in the linguistic examples we previously examined. The discussion of context-free grammars naturally led us to pushdown automata (which provided a nice contrast to the Turing machines we studied earlier in the course). And, having thus introduced stacks, we then investigated the Grosz and Sidner (1986) stack-based theory of discourse structure, showing that language structures exist at granularities beyond the sentence level. Statistical language processing [6 lectures] We began this unit by considering word frequency distributions, and in particular, Zipf’s law — note that our having studied power-law distributions in the Web unit greatly facilitated this discussion. In fact, because we had previously investigated generative models for the Web, it was natural to consider Miller’s (1957) “monkeys” model which demonstrates that very simple generative models can account for Zipf’s law. Next, </context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara J. Grosz and Candace L. Sidner. 1986. Attention, intentions, and the structure of discourse. Computational Linguistics, 12(3):175–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Kleinberg</author>
</authors>
<title>Authoritative sources in a hyperlinked environment.</title>
<date>1998</date>
<booktitle>In Proceedings of the 9th ACM-SIAM Symposium on Discrete Algorithms (SODA),</booktitle>
<pages>668--677</pages>
<marker>Kleinberg, 1998</marker>
<rawString>Jon Kleinberg. 1998. Authoritative sources in a hyperlinked environment. In Proceedings of the 9th ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 668–677.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
</authors>
<title>A statistical MT tutorial workbook. http://www.isi.edu/natural-language/-mt/wkbk.rtf,</title>
<date>1999</date>
<marker>Knight, 1999</marker>
<rawString>Kevin Knight. 1999. A statistical MT tutorial workbook. http://www.isi.edu/natural-language/-mt/wkbk.rtf, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ravi Kumar</author>
<author>Prabhakar Raghavan</author>
<author>Sridhar Rajagopolan</author>
<author>D Sivakumar</author>
<author>Andrew Tomkins</author>
<author>Eli Upfal</author>
</authors>
<title>Stochastic models for the web graph.</title>
<date>2000</date>
<booktitle>In Proceedings of the 41st IEEE Symposium on the Foundations of Computer Science,</booktitle>
<pages>57--65</pages>
<contexts>
<context position="7735" citStr="Kumar et al., 2000" startWordPosition="1182" endWordPosition="1185">ithm as an alternative to mere in-link counting: 5This does require some linear algebra background in that one needs to compute inner products, but this was covered in the section of the course on perceptrons. Since trigonometry is actually relatively fresh in the minds of first-year students, their geometric intuitions tended to serve them fairly well. fortunately, the method is simple enough that students could engage in hand simulations. Finally, we looked at the suitability of various random graph generation models (e.g., the “richget-richer” (Barab´asi et al., 1999) and “copying” models (Kumar et al., 2000)) for capturing the local structure of the Web, such as the phenomenon of in-degree distributions following a power law — conveniently, these concepts could be presented in such a way that only required the students to have intuitive notions of probability and the ability to take derivatives. Language structure [7 lectures]. Relying on students’ instincts about and experience with language, we considered evidence for the existence of hidden language structure; such clues included possible and impossible syntactic and discourse ambiguities, and movement, prosody and pause cues for constituents.</context>
</contexts>
<marker>Kumar, Raghavan, Rajagopolan, Sivakumar, Tomkins, Upfal, 2000</marker>
<rawString>Ravi Kumar, Prabhakar Raghavan, Sridhar Rajagopolan, D. Sivakumar, Andrew Tomkins, and Eli Upfal. 2000. Stochastic models for the web graph. In Proceedings of the 41st IEEE Symposium on the Foundations of Computer Science, pages 57–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Some effects of intermittent silence.</title>
<date>1957</date>
<journal>American Journal of Psychology,</journal>
<volume>70</volume>
<pages>313</pages>
<marker>Miller, 1957</marker>
<rawString>George A. Miller. 1957. Some effects of intermittent silence. American Journal of Psychology, 70:311– 313.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Mosteller</author>
<author>David L Wallace</author>
</authors>
<title>Applied Bayesian and Classical Inference: The Case of the Federalist Papers.</title>
<date>1984</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="10207" citStr="Mosteller and Wallace, 1984" startWordPosition="1557" endWordPosition="1560">of statistical regularities, including the IBM Candide statistical machine translation system, following Knight’s (1999) tutorial and treating probabilities as weights. It was interesting to point out parallels with the hubs and authorities algorithm — both are iterative update procedures with auxiliary information (alignments in one case, hubs in the other). We also discussed an intuitive algorithm for Japanese segmentation drawn from one of my own recent research collaborations (Ando and Lee, 2000), and how word statistics were applied to determining the authorship of the Federalist Papers (Mosteller and Wallace, 1984). We concluded with an examination of human statistical learning, focusing on recent evidence indicating that human infants can use statistics when learning to segment continuous speech into words (Saffran et al., 1996). The Turing test [2 lectures] Finally, we ended the course with a consideration of intelligence in the large. In particular, we focused on Turing’s (1950) proposal of the “imitation game”, which can be interpreted as one of the first appearances of the claim that natural language processing is “AI-complete”, and Searle’s (1980) “Chinese Room” rebuttal that fluent language behav</context>
</contexts>
<marker>Mosteller, Wallace, 1984</marker>
<rawString>Frederick Mosteller and David L. Wallace. 1984. Applied Bayesian and Classical Inference: The Case of the Federalist Papers. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny R Saffran</author>
<author>Richard N Aslin</author>
<author>Elissa L Newport</author>
</authors>
<title>Statistical learning by 8-monthold infants.</title>
<date>1996</date>
<journal>Science,</journal>
<volume>274</volume>
<issue>5294</issue>
<contexts>
<context position="10426" citStr="Saffran et al., 1996" startWordPosition="1592" endWordPosition="1595">nd authorities algorithm — both are iterative update procedures with auxiliary information (alignments in one case, hubs in the other). We also discussed an intuitive algorithm for Japanese segmentation drawn from one of my own recent research collaborations (Ando and Lee, 2000), and how word statistics were applied to determining the authorship of the Federalist Papers (Mosteller and Wallace, 1984). We concluded with an examination of human statistical learning, focusing on recent evidence indicating that human infants can use statistics when learning to segment continuous speech into words (Saffran et al., 1996). The Turing test [2 lectures] Finally, we ended the course with a consideration of intelligence in the large. In particular, we focused on Turing’s (1950) proposal of the “imitation game”, which can be interpreted as one of the first appearances of the claim that natural language processing is “AI-complete”, and Searle’s (1980) “Chinese Room” rebuttal that fluent language behavior is not a sufficient indication of intelligence. Then, we concluded with an examination of the first running of the Restricted Turing Test (Shieber, 1994), which served as an object lesson as to the importance of car</context>
</contexts>
<marker>Saffran, Aslin, Newport, 1996</marker>
<rawString>Jenny R. Saffran, Richard N. Aslin, and Elissa L. Newport. 1996. Statistical learning by 8-monthold infants. Science, 274(5294):1926–1928, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Searle</author>
</authors>
<title>Minds, brains, and programs.</title>
<date>1980</date>
<journal>Behavioral and Brain Sciences,</journal>
<volume>3</volume>
<issue>3</issue>
<marker>Searle, 1980</marker>
<rawString>John R. Searle. 1980. Minds, brains, and programs. Behavioral and Brain Sciences, 3(3):417–457.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>Lessons from a restricted Turing test.</title>
<date>1994</date>
<journal>Communications of the ACM,</journal>
<volume>37</volume>
<issue>6</issue>
<contexts>
<context position="10964" citStr="Shieber, 1994" startWordPosition="1682" endWordPosition="1683">when learning to segment continuous speech into words (Saffran et al., 1996). The Turing test [2 lectures] Finally, we ended the course with a consideration of intelligence in the large. In particular, we focused on Turing’s (1950) proposal of the “imitation game”, which can be interpreted as one of the first appearances of the claim that natural language processing is “AI-complete”, and Searle’s (1980) “Chinese Room” rebuttal that fluent language behavior is not a sufficient indication of intelligence. Then, we concluded with an examination of the first running of the Restricted Turing Test (Shieber, 1994), which served as an object lesson as to the importance of careful evaluation in NLP, or indeed any science. 3 Experience Twenty-three students enrolled, with only onethird initially expressing interest in majoring in computer science. By the end, I was approached by four students asking if there were research opportunities available in the topics we had covered; interestingly, one of these students had originally intended to major in electrical engineering. Furthermore, because of the class’s promise in drawing students into further computer science study, the GE Fund awarded a grant for the </context>
</contexts>
<marker>Shieber, 1994</marker>
<rawString>Stuart M. Shieber. 1994. Lessons from a restricted Turing test. Communications of the ACM, 37(6):70–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herb A Simon</author>
</authors>
<title>Artificial intelligence systems that understand.</title>
<date>1977</date>
<booktitle>In Proceedings of the Fifth International Joint Conference on Artificial Intelligence,</booktitle>
<volume>2</volume>
<pages>1059--1073</pages>
<marker>Simon, 1977</marker>
<rawString>Herb A. Simon. 1977. Artificial intelligence systems that understand. In Proceedings of the Fifth International Joint Conference on Artificial Intelligence, volume 2, pages 1059–1073.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan M Turing</author>
</authors>
<title>Computing machinery and intelligence.</title>
<date>1950</date>
<location>Mind, LIX:433–60.</location>
<marker>Turing, 1950</marker>
<rawString>Alan M. Turing. 1950. Computing machinery and intelligence. Mind, LIX:433–60.</rawString>
</citation>
<citation valid="false">
<title>stack is after each sentence is uttered. Explain how you determined your answers.</title>
<marker></marker>
<rawString>stack is after each sentence is uttered. Explain how you determined your answers.</rawString>
</citation>
<citation valid="false">
<title>(b) Repeat the above subproblem, but from the point of view of speaker B. (c) Would your answers to the previous subproblem change if sentence (4) had been “Why are you telling me these things?” Does the Grosz/Sidner model adequately account for this case?</title>
<journal>Explain.</journal>
<marker></marker>
<rawString>(b) Repeat the above subproblem, but from the point of view of speaker B. (c) Would your answers to the previous subproblem change if sentence (4) had been “Why are you telling me these things?” Does the Grosz/Sidner model adequately account for this case? Explain.</rawString>
</citation>
<citation valid="false">
<title>Statistical language processing unit In this problem, we explicitly derive a type of power-law behavior in the “Miller’s monkeys” model (henceforth MMM) from class. First, a useful fact: for any fixed integers n</title>
<journal></journal>
<volume>1</volume>
<pages>and</pages>
<marker></marker>
<rawString>Statistical language processing unit In this problem, we explicitly derive a type of power-law behavior in the “Miller’s monkeys” model (henceforth MMM) from class. First, a useful fact: for any fixed integers n &gt; 1 and</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>