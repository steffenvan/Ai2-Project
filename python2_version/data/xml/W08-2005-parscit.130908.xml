<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000018">
<title confidence="0.7719205">
Graph-based Clustering for Semantic Classification of Onomatopoetic
Words
</title>
<author confidence="0.964548">
Kenichi Ichioka
</author>
<affiliation confidence="0.964863333333333">
Interdisciplinary Graduate School of
Medicine and Engineering
University of Yamanashi, Japan
</affiliation>
<email confidence="0.995541">
g07mk001@yamanashi.ac.jp
</email>
<author confidence="0.927463">
Fumiyo Fukumoto
</author>
<affiliation confidence="0.949645666666667">
Interdisciplinary Graduate School of
Medicine and Engineering
University of Yamanashi, Japan
</affiliation>
<email confidence="0.997698">
fukumoto@yamanashi.ac.jp
</email>
<sectionHeader confidence="0.994765" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999896631578947">
This paper presents a method for seman-
tic classification of onomatopoetic words
like “!&amp;quot;#!&amp;quot;# (hum)” and “$%&amp;
&apos;(&amp; (clip clop)” which exist in ev-
ery language, especially Japanese being
rich in onomatopoetic words. We used
a graph-based clustering algorithm called
Newman clustering. The algorithm cal-
culates a simple quality function to test
whether a particular division is meaning-
ful. The quality function is calculated
based on the weights of edges between
nodes. We combined two different sim-
ilarity measures, distributional similarity,
and orthographic similarity to calculate
weights. The results obtained by using
the Web data showed a 9.0% improvement
over the baseline single distributional sim-
ilarity measure.
</bodyText>
<sectionHeader confidence="0.998884" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998964833333333">
Onomatopoeia which we call onomatopoetic word
(ono word) is the formation of words whose sound
is imitative of the sound of the noise or action des-
ignated, such as ‘hiss’ (McLeod, 1991). It is one
of the linguistic features of Japanese. Consider two
sentences from Japanese.
</bodyText>
<equation confidence="0.7706425">
(1) )*+,-./01-234&apos;567-38
9:;&lt;=&gt;
</equation>
<bodyText confidence="0.980473">
“I’m too sleepy because I awoke to the slip-
pers in the hall.”
</bodyText>
<footnote confidence="0.909653">
© 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.990622578947368">
(2) )*+,? @7@7 AB./01-234
&apos;567-389:;&lt;=&gt;
“I’m too sleepy because I awoke to the pit-a-
pat of slippers in the hall.”
Sentences (1) and (2) are almost the same sense.
However, sentence (2) which includes ono word,
“@7@7 (pit-a-pat)” is much better to make
the scene alive, or represents an image clearly.
Therefore large-scale semantic resource of ono
words is indispensable for not only NLP, but
also many semantic-oriented applications such as
Question Answering, Paraphrasing, and MT sys-
tems. Although several machine-readable dictio-
naries which are fine-grained and large-scale se-
mantic knowledge like WordNet, COMLEX, and
EDR dictionary exist, there are none or few ono-
matopoetic thesaurus. Because (i) it is easy to un-
derstand its sense of ono word for Japanese, and
(ii) it is a fast-changing linguistic expressions, as
it is a vogue word. Therefore, considering this re-
source scarcity problem, semantic classification of
ono words which do not appear in the resource but
appear in corpora is very important.
In this paper, we focus on Japanese onomatopo-
etic words, and propose a method for classifying
them into a set with similar meaning. We used
the Web as a corpus to collect ono words, as they
appear in different genres of dialogues including
broadcast news, novels and comics, rather than a
well-edited, balanced corpus like newspaper arti-
cles. The problem using a large, heterogeneous
collection of Web data is that the Web counts are
far more noisy than counts obtained from textual
corpus. We thus used a graph-based clustering al-
gorithm, called Newman clustering for classify-
ing ono words. The algorithm does not simply cal-
culate the number of shortest paths between pairs
of nodes, but instead calculates a quality function
</bodyText>
<page confidence="0.990341">
33
</page>
<note confidence="0.6949855">
Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 33–40
Manchester, August 2008
</note>
<bodyText confidence="0.999976736842105">
of how good a cluster structure found by an algo-
rithm is, and thus makes the computation far more
efficient. The efficacy of the algorithm depends
on a quality function which is calculated by us-
ing the weights of edges between nodes. We com-
bined two different similarity measures, and used
them to calculate weights. One is co-occurrence
based distributional similarity measure. We tested
mutual information (MI) and a χ2 statistic as a
similarity measure. Another is orthographic sim-
ilarity which is based on a feature of ono words
called “sound symbolism”. Sound symbolism in-
dicates that phonemes or phonetic sequences ex-
press their senses. As ono words imitate the sounds
associated with the objects or actions they refer to,
their phonetic sequences provide semantic clues
for classification. The empirical results are encour-
aging, and showed a 9.0% improvement over the
baseline single distributional similarity measure.
</bodyText>
<sectionHeader confidence="0.994778" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999642948275862">
There are quite a lot of work on semantic classifi-
cation of words with corpus-based approach. The
earliest work in this direction are those of (Hindle,
1990), (Lin, 1998), (Dagan et al., 1999), (Chen
and Chen, 2000), (Geffet and Dagan, 2004) and
(Weeds and Weir, 2005). They used distributional
similarity. Similarity measures based on distribu-
tional hypothesis compare a pair of weighted fea-
ture vectors that characterize two words. Features
typically correspond to other words that co-occur
with the characterized word in the same context.
Lin (1998) proposed a word similarity measure
based on the distributional pattern of words which
allows to construct a thesaurus using a parsed cor-
pus. He compared the result of automatically cre-
ated thesaurus with WordNet and Roget, and re-
ported that the result was significantly closer to
WordNet than Roget Thesaurus was.
Graph representations for word similarity have
also been proposed by several researchers (Jan-
nink and Wiederhold,1999; Galley and McKeown,
2003; Muller et al., 2006). Sinha and Mihalcea
(2007) proposed a graph-based algorithm for un-
supervised word sense disambiguation which com-
bines several semantic similarity measures includ-
ing Resnik’s metric (Resnik, 1995), and algorithms
for graph centrality. They reported that the results
using the SENSEVAL-2 and SENSEVAL-3 En-
glish all-words data sets lead to relative error rate
reductions of 5 − 8% as compared to the previous
work (Mihalcea, 2005).
In the context of graph-based clustering of
words, Widdows and Dorow (2002) used a graph
model for unsupervised lexical acquisition. The
graph structure is built by linking pairs of words
which participate in particular syntactic relation-
ships. An incremental cluster-building algorithm
using the graph structure achieved 82% accuracy at
a lexical acquisition task, evaluated against Word-
Net 10 classes, and each class consists of 20 words.
Matsuo et al. (2006) proposed a method of word
clustering based on a word similarity measure by
Web counts. They used Newman clustering for
clustering algorithm. They evaluated their method
using two sets of word classes. One is derived from
the Web data, and another is from WordNet.1 Each
set consists of 90 noun words. They reported that
the results obtained by Newman clustering were
better than those obtained by average-link agglom-
erative clustering. Our work is similar to their
method in the use of Newman clustering. How-
ever, they classified Japanese noun words, while
our work is the first to aim at detecting seman-
tic classification of onomatopoetic words. More-
over, they used only a single similarity metric, co-
occurrence based similarity, while Japanese, espe-
cially “kanji” characters of noun words provide se-
mantic clues for classifying words.
</bodyText>
<sectionHeader confidence="0.988945" genericHeader="method">
3 System Description
</sectionHeader>
<bodyText confidence="0.99979625">
The method consists of three steps: retrieving co-
occurrences using the Web, calculating similarity
between ono words, and classifying ono words by
using Newman clustering.
</bodyText>
<subsectionHeader confidence="0.999835">
3.1 Retrieving Co-occurrence using the Web
</subsectionHeader>
<bodyText confidence="0.99919075">
One criterion for calculating semantic similarity
between ono words is co-occurrence based similar-
ity. We retrieved frequency of two ono words oc-
curring together by using the Web search engine,
Google. The similarity between them is calcu-
lated based on their co-occurrence frequency. Like
much previous work on semantic classification of
the lexicons, our assumption is that semantically
similar words appear in similar contexts. A lot
of strategies for searching words are provided in
Google. Of these we focused on two methods:
Boolean search AND and phrase-based search.
</bodyText>
<footnote confidence="0.713473">
1They used WordNet hypernym information. It consists
of 10 classes. They assigned 90 Japanese noun words to each
class.
</footnote>
<page confidence="0.997652">
34
</page>
<bodyText confidence="0.999983666666667">
When we use AND boolean search, i.e., (Oi Oj)
where Oi and Oj are ono words, we can retrieve
the number of documents which include both Oi
and Oj. In contrast, phrase-based search, i.e.,
(“Oi Oj”) retrieves documents which include two
adjacent words Oi and Oj.
</bodyText>
<subsectionHeader confidence="0.999707">
3.2 Similarity Measures
</subsectionHeader>
<bodyText confidence="0.9999274">
The second step is to calculate semantic similarity
between ono words. We combined two different
similarity measures: the co-occurrence frequency
based similarity and orthographic similarity mea-
sures.
</bodyText>
<subsectionHeader confidence="0.9766485">
3.2.1 Co-occurrence based Similarity
Measure
</subsectionHeader>
<bodyText confidence="0.9999455">
We focused on two popular measures: the mu-
tual information (MI) and χ2 statistics.
</bodyText>
<sectionHeader confidence="0.875237" genericHeader="method">
1. Mutual Information
</sectionHeader>
<bodyText confidence="0.999771333333333">
Church and Hanks (1990) discussed the use
of the mutual information statistics as a way
to identify a variety of interesting linguistic
phenomena, ranging from semantic relations
of the doctor/nurse type (content word/content
word) to lexico-syntactic co-occurrence prefer-
ences between verbs and prepositions (content
word/function word). Let Oi and Oj be ono words
retrieved from the Web. The mutual information
</bodyText>
<equation confidence="0.899925">
MI(Oi, Oj) is defined as:
MI(Oi,Oj) =
�where SOi =
�Sall =
Oi∈Oall
</equation>
<bodyText confidence="0.999738">
In Eq. (1), f(Oi, Oj) refers to the frequency of Oi
and Oj occurring together, and Oall is a set of all
ono words retrieved from the Web.
</bodyText>
<listItem confidence="0.450623">
2. χ2 statistic
</listItem>
<bodyText confidence="0.864261">
The χ2(Oi, Oj) is defined as:
</bodyText>
<subsectionHeader confidence="0.764018">
3.2.2 Orthographic Similarity Measure
</subsectionHeader>
<bodyText confidence="0.961352583333333">
Orthographic similarity has been widely used
in spell checking and speech recognition systems
(Damerau, 1964). Our orthographic similarity
measure is based on a unit of phonetic sequence.
The key steps of the similarity between two ono
words is defined as:
1. Convert each ono word into phonetic se-
quences.
The “hiragana” characters of ono word are
converted into phonetic sequences by a
unique rule. Basically, there are 19 conso-
nants and 5 vowels, as listed in Table 1.
</bodyText>
<tableCaption confidence="0.972338">
Table 1: Japanese consonants and vowels
</tableCaption>
<bodyText confidence="0.976275857142857">
Consonant –, N, Q, h, hy, k, ky, m, my, n,
ny, r, ry, s, sy, t, ty, w, y
Vowel a, i, u, e, o
Consider phonetic sequences “hyu-hyu-” of
ono word “D0—00—” (hum). It is seg-
mented into 4 consonants “hy”, “-”, “hy” and
“-”, and two vowels, “u” and “u”.
</bodyText>
<listItem confidence="0.826765">
2. Form a vector in n-dimensional space.
</listItem>
<bodyText confidence="0.999916666666667">
Each ono word is represented as a vector
of consonants(vowels), where each dimen-
sion of the vector corresponds to each con-
sonant and vowel, and each value of the di-
mension is frequencies of its corresponding
consonant(vowel).
</bodyText>
<listItem confidence="0.934199">
3. Calculate orthographic similarity.
</listItem>
<bodyText confidence="0.999945">
The orthographic similarity between ono
words, Oi and Oj is calculated based on the
consonant and vowel distributions. We used
two popular measures, i.e., the cosine similar-
ity, and α-skew divergence. The cosine mea-
sures the similarity of the two vectors by cal-
culating the cosine of the angle between vec-
tors. α-skew divergence is defined as:
</bodyText>
<equation confidence="0.99374075">
Sall x f(Oi, Oj)
log ,(1)
SOi x SOj
f(Oi,Ok), (2)
k∈Oall
SOi. (3)
x2(Oi,Oj) = f(Oi, Oj) − E(Oi, Oj) (4) αdiv(x, y) = D(y  ||α · x + (1 − α) · y),
E(Oi, Oj)
</equation>
<bodyText confidence="0.859889142857143">
where E(Oi, Oj) = SOi x SS09 oj (5)
all
SOi and Sall in Eq. (5) refer to Eq. (2) and (3),
respectively. A major difference between χ2 and
MI is that the former is a normalized value.
where D(x||y) refers to Kullback-Leibler
and defined as:
</bodyText>
<equation confidence="0.982714666666667">
D(x||y) = �n xi
i=1 xi * log . (6)
yi
</equation>
<page confidence="0.984142">
35
</page>
<bodyText confidence="0.99978625">
Lee (1999) reported the best results with α
= 0.9. We used the same value. We defined a
similarity metric by combining co-occurrence
based and orthographic similarity measures2:
</bodyText>
<equation confidence="0.9935165">
Sim(Oi, Oj) =
MI(Oi,Oj) x (Cos(Oi, Oj) + 1) (7)
</equation>
<subsectionHeader confidence="0.990224">
3.3 The Newman Clustering Algorithm
</subsectionHeader>
<bodyText confidence="0.999980588235294">
We classified ono words collected from the WWW.
Therefore, the clustering algorithm should be effi-
cient and effective even in the very high dimen-
sional spaces. For this purpose, we chose a graph-
based clustering algorithm, called Newman clus-
tering. The Newman clustering is a hierarchical
clustering algorithm which is based on Network
structure (Newman, 2004). The network structure
consists of nodes within which the node-node con-
nections are edges. It produces some division of
the nodes into communities, regardless of whether
the network structure has any natural such divi-
sion. Here, “community” or “cluster” have in com-
mon that they are groups of densely interconnected
nodes that are only sparsely connected with the rest
of the network. To test whether a particular divi-
sion is meaningful a quality function Q is defined:
</bodyText>
<equation confidence="0.964532666666667">
�
Q =
i
</equation>
<bodyText confidence="0.9992758">
where eij is the sum of the weight of edges be-
tween two communities i and j divided by the sum
of the weight of all edges, and ai = Ej eij, i.e., the
expected fraction of edges within the cluster. Here
are the key steps of that algorithm:
</bodyText>
<listItem confidence="0.53506">
1. Given a set of n ono words S = 1O1, · · ·,
O,,,1. Create a network structure which con-
</listItem>
<bodyText confidence="0.989176428571429">
sists of nodes O1, · · ·, O,,,, and edges. Here,
the weight of an edge between Oi and Oj
is a similarity value obtained by Eq. (7). If
the “network density” of ono words is smaller
than the parameter θ, we cut the edge. Here,
“network density” refers to a ratio selected
from the topmost edges. For example, if it
</bodyText>
<footnote confidence="0.94977325">
2When we used x2 statistic as a co-occurrence based sim-
ilarity, MI in Eq. (7) is replaced by x2. In a similar way,
Cos(Oi, Oj) is replaced by max − αdiv(x, y), where max
is the maximum value among all αdiv(x, y) values.
</footnote>
<bodyText confidence="0.980007">
was 0.9, we used the topmost 90% of all
edges and cut the remains, where edges are
sorted in the descending order of their simi-
larity values.
</bodyText>
<listItem confidence="0.963646333333333">
2. Starting with a state in which each ono word
is the sole member of one of n communities,
we repeatedly joined communities together in
pairs, choosing at each step the join that re-
sults in the greatest increase.
3. Suppose that two communities are merged
into one by a join operation. The change in
Q upon joining two communities i and j is
given by:
</listItem>
<equation confidence="0.748328">
AQij = eij + eji − 2aiaj
= 2(eij − aiaj)
</equation>
<listItem confidence="0.975326125">
4. Apply step 3. to every pair of communities.
5. Join two communities such that AQ is maxi-
mum and create one community. If AQ &lt; 0,
go to step 7.
6. Re-calculate eij and ai of the joined commu-
nity, and go to step 3.
7. Words within the same community are re-
garded as semantically similar.
</listItem>
<bodyText confidence="0.929061">
The computational cost of the algorithm is known
as O((m + n)n) or O(n2), where m and n are the
number of edges and nodes, respectively.
</bodyText>
<sectionHeader confidence="0.999948" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.995452">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999989785714286">
The data for the classification of ono words have
been taken from the Japanese ono dictionary (Ono,
2007) that consisted of 4,500 words. Of these, we
selected 273 words, which occurred at least 5,000
in the document URLs from the WWW. The min-
imum frequency of a word was found to be 5,220,
while the maximum was about 26 million. These
words are classified into 10 classes. Word classes
and examples of ono words from the dictionary are
listed in Table 2.
“Id” denotes id number of each class. “Sense”
refers to each sense of ono word within the same
class, and “Num” is the number of words which
should be assigned to each class. Each word
</bodyText>
<equation confidence="0.947515">
2
(eii − ai )
</equation>
<page confidence="0.997398">
36
</page>
<tableCaption confidence="0.995551">
Table 2: Onomatopoetic words and # of words in each class
</tableCaption>
<table confidence="0.999546636363636">
Id Sense Num Onomatopoetic words
1 laugh 63 CD*D* (a,Q,h,a,Q,h,a), C** (a,h,a,h,a), E** (w,a,h,a,h,a)
C*C* (a,h,a,a,h,a), =!! (i,h,i,h,i), FDGDG (u,Q,s,i,Q,s,i), ·· ·
2 cry 34 C#&amp; (a,–,N), FE#&amp; (u,w,a,–,N), C&amp;C&amp; (a,N,a,N), H&amp;H&amp; (e,N,e,N)
FBFB (u,r,u,u,r,u), FBB&amp; (u,r,u,r,u,N), FBD(u,r,u,Q), H#&amp; (e,–,N), · · ·
3 pain 34 =I=I (i,k,a,i,k,a), !J!J (h,i,r,i,h,i,r,i), IKIK (k,a,s,i,k,a,s,i)
I&amp;I&amp; (k,a,N,k,a,N), · · ·
4 anger 33 $#D(k,a,–,Q), $L&amp; (k,a,t,i,N), $M&amp; (k,a,t,u,N), $D(k,a,Q), $D$ (k,a,Q,k,a),
ININ (k,a,m,i,k,a,m,i), $J$J (k,a,r,i,k,a,r,i), $&amp;$&amp; (k,a,N,k,a,N), ·· ·
5 spook 31 CEE (a,w,a,w,a), FOP# (u,ky,a,–), I#&amp; (k,a,–,N), OQ (k,i,k,u)
OQD(k,i,k,u,Q), OQJ (k,i,k,u,r,i), OQ&amp; (k,i,k,u,N), · · ·
6 panic 25 CQRQ (a,k,u,s,e,k,u), C7S7 (a,t,a,h,u,t,a), CDTCDT (a,Q,h,u,a,Q,h,u),
CECE (a,w,a,a,w,a)·· ·
7 bloodless 27 $QD(k,a,k,u,Q), IQD(k,a,k,u,Q), ID$J (k,a,Q,k,a,r,i), IDQJ (k,a,Q,k,u,r,i)
$Q&amp; (k,a,k,u,N), OPS&amp; (ky,a,h,u,N), O&amp;quot;# (ky,u,–), ·· ·
8 deem 13 FD9J (u,Q,t,o,r,i), U&amp;quot;#&amp; (ky,u,–,N), U&amp;quot;&amp; (ky,u,N)
MQVQ (t,u,k,u,t,u,k,u), · · ·
9 feel delight 6 FGFG (u,s,i,u,s,i), UPWUPW (ky,a,h,i,ky,a,h,i)
F*F* (u,–,h,a,–,u,–,h,a), X=X= (h,o,i,h,o,i), B&amp;B&amp; (r,u,N,r,u,N), · · ·
10 balk 7 =K=K (i,s,i,i,s,i), FKFK (u,s,i,u,s,i), YZYZ (o,s,u,o,s,u)
[\[\ (k,u,t,a,k,u,t,a), ;K;K (m,o,s,i,m,o,s,i), · · ·
Total 273
</table>
<bodyText confidence="0.9970606875">
marked with bracket denotes phonetic sequences
consisting of consonants and vowels.
We retrieved co-occurrences of ono words
shown in Table 2 using the search engine, Google.
We applied Newman clustering to the input words.
For comparison, we implemented standard k-
means which is often used as a baseline, as it is
one of the simplest unsupervised clustering algo-
rithms, and compared the results to those obtained
by our method. We used Euclidean distance (L2
norm) as a distance metric used in the k-means.
For evaluation of classification, we used
Precision(Prec), Recall(Rec), and F-measure
which is a measure that balances precision and re-
call (Bilenko et al., 2004). The precise definitions
of these measures are given below:
</bodyText>
<equation confidence="0.996491857142857">
Prec = #PairsCorrectlyPredictedInSamecluster
#TotalPairsPredictedInSameCluster
Rec = #PairsCorrectlyPredictedInSameCluster
#TotalPairsInSameCluster
2 × Prec × Rec
F − measure = (10)
(Prec + Rec)
</equation>
<sectionHeader confidence="0.793223" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.999920461538462">
The results are shown in Table 3. “Co-occ. &amp;
Sounds” in Data refers to the results obtained by
our method. “Co-occ.” denotes the results ob-
tained by a single measure, co-occurrence based
distributional similarity measure, and “Sounds”
shows the results obtained by orthographic sim-
ilarity. “θ” in Table 3 shows a parameter θ
used in the Newman clustering.3 Table 3 shows
best performance of each method against θ val-
ues. The best result was obtained when we used
phrase-based search and a combined measure of
co-occurrence(MI) and sounds (cos), and F-score
was 0.451.
</bodyText>
<subsubsectionHeader confidence="0.688445">
4.2.1 AND vs phrase-based search
</subsubsectionHeader>
<bodyText confidence="0.998897823529412">
Table 3 shows that overall the results using
phrase-based search were better than those of
AND search, and the maximum difference of F-
score between them was 20.6% when we used a
combined measure. We note that AND boolean
search did not consider the position of a word in
a document, while our assumption was that se-
mantically similar words appeared in similar con-
texts. As a result, two ono words which were
not semantically similar were often retrieved by
AND boolean search. For example, consider two
antonymous words, “a,h,a,h,a” (grinning broadly)
and “w,a,–,N” (Wah, Wah). The co-occurrence fre-
quency obtained by AND was 5,640, while that of
phrase-based search was only one. The observa-
tion shows that we find phrase-based search to be
a good choice.
</bodyText>
<footnote confidence="0.9661115">
3In case of k-means, we used the weights which satisfies
network density.
</footnote>
<page confidence="0.999678">
37
</page>
<tableCaption confidence="0.99777">
Table 3: Classification results
</tableCaption>
<table confidence="0.999904129032258">
Data Algo. Sim (Co-occ.) Sim (Sounds) Search method 0 Prec Rec F # of clusters
Co-occ. &amp; Sounds k-means 2 cos AND .050 .134 .799 .229 10
X
Phrase .820 .137 .880 .236 10
MI AND .050 .134 .562 .216 10
Phrase .150 .190 .618 .289 10
2 αdiv AND .680 .134 .801 .229 10
X
Phrase .280 .138 .882 .238 10
MI AND .040 .134 .602 .219 10
Phrase .140 .181 .677 .285 10
Newman X2 cos AND .170 .182 .380 .246 9
Phrase .100 .322 .288 .304 14
MI AND .050 .217 .282 .245 13
Phrase .080 .397 .520 .451 7
X2 αdiv AND .130 .212 .328 .258 9
Phrase .090 .414 .298 .347 17
MI AND .090 .207 .325 .253 6
Phrase .160 .372 .473 .417 8
Co-occ. k-means X2 – AND .460 .138 .644 .227 10
Phrase .110 .136 .870 .236 10
MI AND .040 .134 .599 .219 10
Phrase .150 .191 .588 .286 10
Newman X2 – AND .700 .169 .415 .240 8
Phrase .190 .301 .273 .286 14
MI AND .590 .159 .537 .245 3
Phrase .140 .275 .527 .361 5
Sounds k-means – cos – .050 .145 .321 .199 10
αdiv – .020 .126 .545 .204 10
Newman – cos – .270 .151 .365 .213 4
αdiv – .350 .138 .408 .206 3
</table>
<subsectionHeader confidence="0.512586">
4.2.2 A single vs combined similarity measure
</subsectionHeader>
<bodyText confidence="0.99995402631579">
To examine the effectiveness of the combined
similarity measure, we used a single measure as
a quality function of the Newman clustering, and
compared these results with those obtained by our
method. As shown in Table 3, the results with
combining similarity measures improved overall
performance. In the phrase-based search, for ex-
ample, the F-score using a combined measure “Co-
occ(MI) &amp; Sounds(cos)” was 23.8% better than
the baseline single measure “Sounds(cos)”, and
9.0% better a single measure “Co-occ(MI)”.
Figure 1 shows F-score by “Co-occ(MI) &amp;
Sounds(cos)” and “Co-occ(MI)” against changes
in B. These curves were obtained by phrase-
based search. We can see from Figure 1 that the
F-score by a combined measure “Co-occ(MI) &amp;
Sounds(cos)” was better than “Co-occ(MI)” with
B value ranged from .001 to .25. One possible rea-
son for the difference of F-score between them is
the edges selected by varying B. Figure 2 shows
the results obtained by each single measure, and a
combined measure to examine how the edges se-
lected by varying B affect overall performance, F-
measure. “Precision” in Figure 2 refers to the ratio
of correct ono word pairs (edges) divided by the to-
tal number of edges. Here, correct ono word pairs
were created by using the Japanese ono dictionary,
i.e., we extracted word pairs within the same sense
of the dictionary. Surprisingly, there were no sig-
nificant difference between a combined measure
“Co-occ(MI) &amp; Sounds(cos)” and a single mea-
sure “Co-occ(MI)” curves, while the precision of
a single measure “Sounds” was constantly worse
than that obtained by a combined measure. An-
other possible reason for the difference of F-score
is due to product of MI and Cos in Eq. (7). Fur-
ther work is needed to analyze these results in de-
tail.
</bodyText>
<subsectionHeader confidence="0.970339">
4.2.3 k-means vs Newman algorithms
</subsectionHeader>
<bodyText confidence="0.999955076923077">
We examined the results obtained by standard k-
means and Newman clustering algorithms. As can
be seen clearly from Table 3, the results with New-
man clustering were better than those of the stan-
dard k-means at all search and similarity measures,
especially the result obtained by Newman clus-
tering showed a 16.2 % improvement over the k-
means when we used Co-occ.(MI) &amp; Sounds(cos)
&amp; phrase-based search. We recall that we used
273 ono words for clustering. However, Newman
clustering is applicable for a large number of nodes
and edges without decreasing accuracy too much,
as it does not simply calculate the number of short-
</bodyText>
<page confidence="0.996152">
38
</page>
<figure confidence="0.9936455">
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Lower bound values
</figure>
<figureCaption confidence="0.998934">
Figure 1: F-score against θ values
</figureCaption>
<figure confidence="0.99563">
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Lower bound values
</figure>
<figureCaption confidence="0.999929">
Figure 2: Precision against θ values
</figureCaption>
<bodyText confidence="0.999199">
est paths between pairs of nodes, but instead calcu-
lates a simple quality function. Quantitative eval-
uation by applying the method to larger data from
the Web is worth trying for future work.
</bodyText>
<subsectionHeader confidence="0.99981">
4.3 Qualitative Analysis of Errors
</subsectionHeader>
<bodyText confidence="0.999312142857143">
Finally, to provide feedback for further devel-
opment of our classification approach, we per-
formed a qualitative analysis of errors. Con-
sider the following clusters (the Newman output
for Co-occ.(MI), Sounds(cos) and phrase-based
search), where each parenthetic sequences denotes
ono word:
</bodyText>
<equation confidence="0.500930666666667">
A1: (t,o,Q) (t,o,Q,t,o) (t,o,Q,k,i,N,t,o,Q,k,i,N)
A2: (o,h,o,h,o), (e,h,e,h,e), (h,e,h,e,h,e), (o,-,o,-)
A3: (u,s,i,u,s,i), (m,o,s,i,m,o,s,i), (m,o,s,o,m,o,s,o)
</equation>
<bodyText confidence="0.757802">
Three main error types were identified:
</bodyText>
<listItem confidence="0.545557">
1. Morphological idiosyncrasy: This was
</listItem>
<bodyText confidence="0.935073526315789">
the most frequent error type, exemplified
in A1, where “(t,o,Q,k,i,N,t,o,Q,k,i,N)“
(pain sense) was incorrectly clustered with
other two words (laugh sense) merely be-
cause orthographic similarity between them
was large, as the phonetics sequences of
“(t,o,Q,k,i,N,t,o,Q,k,i,N)” included “t” and
“o”.
2. Sparse data: Many of the low frequency ono
words performed poorly. In A2, “(o,-,o,-)”
(cry sense) was classified with other three
words (laugh sense) because it occurred few
in our data.
3. Problems of polysemy: In A3,
“(m,o,s,o,m,o,s,o)” (pain sense) was
clustered with other two words (balk sense)
of its gold standard class. However, the ono
word has another sense, balk sense when it
co-occurred with action verbs.
</bodyText>
<sectionHeader confidence="0.999152" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999956419354839">
We have focused on onomatopoetic words, and
proposed a method for classifying them into a set
of semantically similar words. We used a graph-
based clustering algorithm, called Newman clus-
tering with a combined different similarity mea-
sures. The results obtained by using the Web
data showed a 9.0% improvement over the base-
line single distributional similarity measure. There
are number of interesting directions for future re-
search.
The distributional similarity measure we used
is the basis of the ono words, while other content
words such as verbs and adverbs are also effective
for classifying ono words. In the future, we plan to
investigate the use of these words and work on im-
proving the accuracy of classification. As shown
in Table 2, many of the ono words consist of du-
plicative character sequences such as “h” and “a”
of “a,h,a,h,a”, and “h” and “i” of “i,h,i,h,i”. More-
over, characters which consist of ono words within
the same class match. For example, the hiragana
character “U” (h,a) frequently appears in laugh
sense class. These observations indicate that in-
tegrating edit-distance and our current similarity
measure will improve overall performance.
Another interesting direction is a problem of
polysemy. It clearly supports the classification
of (Ono, 2007) to insist that some ono words
belong to more than one cluster. For example,
“(i,s,o,i,s,o)” has at least two senses, panic and feel
delight sense. In order to accommodate this, we
</bodyText>
<figure confidence="0.999341923076923">
Co-occ(MI)
Co-occ(MI) &amp; Sounds(cos)
Precision
0.55
0.45
0.35
0.25
0.15
0.6
0.5
0.4
0.3
0.2
0.1
Co-occ(MI)
Co-occ(MI) &amp; Sounds(cos)
Sounds(cos)
F-measure 0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
</figure>
<page confidence="0.996964">
39
</page>
<bodyText confidence="0.929682666666667">
should apply an appropriate soft clustering tech-
nique (Tishby et al., 1999; Reichardt and Born-
holdt, 2006; Zhang et al., 2007).
</bodyText>
<sectionHeader confidence="0.995502" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99997175">
We would like to thank the anonymous reviewers
for their helpful suggestions. This material is sup-
ported in part by the Grant-in-aid for the Japan So-
ciety for the Promotion of Science(JSPS).
</bodyText>
<sectionHeader confidence="0.997851" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999949282608696">
Bilenko, M., S. Basu, and R. J. Mooney. 2004. In-
tegrating Constraints and Metric Learning in Semi-
Supervised Clustering. In Proc. of 21st International
Conference on Machine Learning, pages 81–88.
Chen, K. J. and C. J. Chen. 2000. Automatic Seman-
tic Classification for Chinese Unknown Compound
Nouns. In Proc. of 38th Annual Meeting of the As-
sociation for Computational Linguistics, pages 125–
130.
Church, K. and P. Hunks. 1990. Word Association
Norms, Mutual Information and Lexicography. In In
Proc. of 28th Annual Meeting of the Association for
Computational Linguistics., pages 76–83.
Dagan, I., L. Lee, and F. Pereira. 1999. Similarity-
based Models of Cooccurrence Probabilities. Ma-
chine Learning, 34(1-3):43–69.
Damerau, F. 1964. A Technique for Computer Detec-
tion and Correction of Spelling Errors. Communica-
tions of the ACM, 7:171–176.
Galley, M. and K. McKeown. 2003. Improving Word
Sense Disambiguation in Lexical Chaining. In Proc.
of 19th International Joint Conference on Arti�cial
Intelligence, pages 1486–1488.
Geffet, M. and I. Dagan. 2004. Feature Vector Quality
and Distributional Similarity. In Proc. of 20th Inter-
national Conference on Computational Linguistics,
pages 247–253.
Hindle, D. 1990. Noun Classification from Predicate-
argument Structures. In Proc. of 28th Annual Meet-
ing of the Association for Computational Linguistics,
pages 268–275.
Jannink, J. and G. Wiederhold. 1999. Thesaurus Entry
Extraction from an On-line Dictionary. In Proc. of
Fusion’99.
Lee, L. 1999. Measures of Distributional Similarity. In
Proc. of the 37th Annual Meeting of the Association
for Computational Linguistics, pages 25–32.
Lin, D. 1998. Automatic Retrieval and Clustering
of Similar Words. In Proc. of 36th Annual Meet-
ing of the Association for Computational Linguis-
tics and 17th International Conference on Compu-
tational Linguistics, pages 768–773.
Matsuo, Y., T. Sakaki, K. Uchiyama, and M. Ishizuka.
2006. Graph-based Word Clustering using a Web
Search Engine. In Proc. of 2006 Conference on
Empirical Methods in Natural Language Processing
(EMNLP2006), pages 542–550.
McLeod, W. T. 1991. The COLLINS Dictionary and
Thesaurus. HarperCollinsPublishers.
Mihalcea, R. 2005. Unsupervised Large Vocabulary
Word Sense Disambiguation with Graph-based Al-
gorithms for Sequence Data Labeling. In Proc. of the
Human Language Technology / Empirical Methods
in Natural Language PRocessing Conference, pages
411–418.
Muller, P., N. Hathout, and B. Gaume. 2006. Synonym
Extraction Using a Semantic Distance on a Dictio-
nary. In Proc. of the Workshop on TextGraphs, pages
65–72.
Newman, M. E. J. 2004. Fast algorithm for detecting
community structure in networks. In Physics Review
E, (69, 066133).
Ono, M. 2007. Nihongo Omomatope Jiten (in
Japanese). Shougakukan.
Reichardt, J. and S. Bornholdt. 2006. Statistical Me-
chanics of Community Detection. PHYICAL RE-
VIEW E, (74):1–14.
Resnik, P. 1995. Using Information Content to Eval-
uate Semantic Similarity in a Taxonomy. In Proc.
of 14th International Joint Conference on Arti�cial
Intelligence, pages 448–453.
Sinha, R. and R. Mihalcea. 2007. Unsupervised
Graph-based Word Sense Disambiguation Using
Measures of Word Semantic Similarity. In Proc.
of the IEEE International Conference on Semantic
Computing, pages 46–54.
Tishby, N., F. C. Pereira, and W. Bialek. 1999. The In-
formation Bottleneck Method. In Proc. of 37th An-
nual Allerton Conference on Communication Con-
trol and Computing, pages 368–377.
Weeds, J. and D. Weir. 2005. Co-occurrence Retrieval:
A Flexible Framework for Lexical Distributional
Similarity. Computational Linguistics, 31(4):439–
476.
Widdows, D. and B. Dorow. 2002. A Graph Model for
Unsupervised Lexical Acquisition. In Proc. of 19th
International conference on Computational Linguis-
tics (COLING2002), pages 1093–1099.
Zhang, S., R. S. Wang, and X. S. Zhang. 2007. Iden-
tification of Overlapping Community Structure in
Complex Networks using Fuzzy C-means Cluster-
ing. PHYSICAA, (374):483–490.
</reference>
<page confidence="0.998639">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.072411">
<title confidence="0.991908">Graph-based Clustering for Semantic Classification of Onomatopoetic Words</title>
<author confidence="0.495547">Kenichi</author>
<affiliation confidence="0.776338333333333">Interdisciplinary Graduate School Medicine and University of Yamanashi,</affiliation>
<email confidence="0.914182">g07mk001@yamanashi.ac.jp</email>
<title confidence="0.342753">Fumiyo Interdisciplinary Graduate School</title>
<author confidence="0.289314">Medicine</author>
<affiliation confidence="0.999524">University of Yamanashi,</affiliation>
<email confidence="0.995053">fukumoto@yamanashi.ac.jp</email>
<abstract confidence="0.9955491">This paper presents a method for semantic classification of onomatopoetic words and which exist in every language, especially Japanese being rich in onomatopoetic words. We used a graph-based clustering algorithm called The algorithm calculates a simple quality function to test whether a particular division is meaningful. The quality function is calculated based on the weights of edges between nodes. We combined two different similarity measures, distributional similarity, and orthographic similarity to calculate weights. The results obtained by using the Web data showed a 9.0% improvement over the baseline single distributional similarity measure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Bilenko</author>
<author>S Basu</author>
<author>R J Mooney</author>
</authors>
<title>Integrating Constraints and Metric Learning in SemiSupervised Clustering.</title>
<date>2004</date>
<booktitle>In Proc. of 21st International Conference on Machine Learning,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="16998" citStr="Bilenko et al., 2004" startWordPosition="2756" endWordPosition="2759">nsisting of consonants and vowels. We retrieved co-occurrences of ono words shown in Table 2 using the search engine, Google. We applied Newman clustering to the input words. For comparison, we implemented standard kmeans which is often used as a baseline, as it is one of the simplest unsupervised clustering algorithms, and compared the results to those obtained by our method. We used Euclidean distance (L2 norm) as a distance metric used in the k-means. For evaluation of classification, we used Precision(Prec), Recall(Rec), and F-measure which is a measure that balances precision and recall (Bilenko et al., 2004). The precise definitions of these measures are given below: Prec = #PairsCorrectlyPredictedInSamecluster #TotalPairsPredictedInSameCluster Rec = #PairsCorrectlyPredictedInSameCluster #TotalPairsInSameCluster 2 × Prec × Rec F − measure = (10) (Prec + Rec) 4.2 Results The results are shown in Table 3. “Co-occ. &amp; Sounds” in Data refers to the results obtained by our method. “Co-occ.” denotes the results obtained by a single measure, co-occurrence based distributional similarity measure, and “Sounds” shows the results obtained by orthographic similarity. “θ” in Table 3 shows a parameter θ used in</context>
</contexts>
<marker>Bilenko, Basu, Mooney, 2004</marker>
<rawString>Bilenko, M., S. Basu, and R. J. Mooney. 2004. Integrating Constraints and Metric Learning in SemiSupervised Clustering. In Proc. of 21st International Conference on Machine Learning, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K J Chen</author>
<author>C J Chen</author>
</authors>
<title>Automatic Semantic Classification for Chinese Unknown Compound Nouns.</title>
<date>2000</date>
<booktitle>In Proc. of 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>125--130</pages>
<contexts>
<context position="4676" citStr="Chen and Chen, 2000" startWordPosition="709" endWordPosition="712">ed “sound symbolism”. Sound symbolism indicates that phonemes or phonetic sequences express their senses. As ono words imitate the sounds associated with the objects or actions they refer to, their phonetic sequences provide semantic clues for classification. The empirical results are encouraging, and showed a 9.0% improvement over the baseline single distributional similarity measure. 2 Previous Work There are quite a lot of work on semantic classification of words with corpus-based approach. The earliest work in this direction are those of (Hindle, 1990), (Lin, 1998), (Dagan et al., 1999), (Chen and Chen, 2000), (Geffet and Dagan, 2004) and (Weeds and Weir, 2005). They used distributional similarity. Similarity measures based on distributional hypothesis compare a pair of weighted feature vectors that characterize two words. Features typically correspond to other words that co-occur with the characterized word in the same context. Lin (1998) proposed a word similarity measure based on the distributional pattern of words which allows to construct a thesaurus using a parsed corpus. He compared the result of automatically created thesaurus with WordNet and Roget, and reported that the result was signif</context>
</contexts>
<marker>Chen, Chen, 2000</marker>
<rawString>Chen, K. J. and C. J. Chen. 2000. Automatic Semantic Classification for Chinese Unknown Compound Nouns. In Proc. of 38th Annual Meeting of the Association for Computational Linguistics, pages 125– 130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>P Hunks</author>
</authors>
<title>Word Association Norms, Mutual Information and Lexicography. In</title>
<date>1990</date>
<booktitle>In Proc. of 28th Annual Meeting of the Association for Computational Linguistics.,</booktitle>
<pages>76--83</pages>
<marker>Church, Hunks, 1990</marker>
<rawString>Church, K. and P. Hunks. 1990. Word Association Norms, Mutual Information and Lexicography. In In Proc. of 28th Annual Meeting of the Association for Computational Linguistics., pages 76–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>L Lee</author>
<author>F Pereira</author>
</authors>
<title>Similaritybased Models of Cooccurrence Probabilities.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="4653" citStr="Dagan et al., 1999" startWordPosition="705" endWordPosition="708">ture of ono words called “sound symbolism”. Sound symbolism indicates that phonemes or phonetic sequences express their senses. As ono words imitate the sounds associated with the objects or actions they refer to, their phonetic sequences provide semantic clues for classification. The empirical results are encouraging, and showed a 9.0% improvement over the baseline single distributional similarity measure. 2 Previous Work There are quite a lot of work on semantic classification of words with corpus-based approach. The earliest work in this direction are those of (Hindle, 1990), (Lin, 1998), (Dagan et al., 1999), (Chen and Chen, 2000), (Geffet and Dagan, 2004) and (Weeds and Weir, 2005). They used distributional similarity. Similarity measures based on distributional hypothesis compare a pair of weighted feature vectors that characterize two words. Features typically correspond to other words that co-occur with the characterized word in the same context. Lin (1998) proposed a word similarity measure based on the distributional pattern of words which allows to construct a thesaurus using a parsed corpus. He compared the result of automatically created thesaurus with WordNet and Roget, and reported tha</context>
</contexts>
<marker>Dagan, Lee, Pereira, 1999</marker>
<rawString>Dagan, I., L. Lee, and F. Pereira. 1999. Similaritybased Models of Cooccurrence Probabilities. Machine Learning, 34(1-3):43–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Damerau</author>
</authors>
<title>A Technique for Computer Detection and Correction of Spelling Errors.</title>
<date>1964</date>
<journal>Communications of the ACM,</journal>
<pages>7--171</pages>
<contexts>
<context position="9602" citStr="Damerau, 1964" startWordPosition="1475" endWordPosition="1476">e type (content word/content word) to lexico-syntactic co-occurrence preferences between verbs and prepositions (content word/function word). Let Oi and Oj be ono words retrieved from the Web. The mutual information MI(Oi, Oj) is defined as: MI(Oi,Oj) = �where SOi = �Sall = Oi∈Oall In Eq. (1), f(Oi, Oj) refers to the frequency of Oi and Oj occurring together, and Oall is a set of all ono words retrieved from the Web. 2. χ2 statistic The χ2(Oi, Oj) is defined as: 3.2.2 Orthographic Similarity Measure Orthographic similarity has been widely used in spell checking and speech recognition systems (Damerau, 1964). Our orthographic similarity measure is based on a unit of phonetic sequence. The key steps of the similarity between two ono words is defined as: 1. Convert each ono word into phonetic sequences. The “hiragana” characters of ono word are converted into phonetic sequences by a unique rule. Basically, there are 19 consonants and 5 vowels, as listed in Table 1. Table 1: Japanese consonants and vowels Consonant –, N, Q, h, hy, k, ky, m, my, n, ny, r, ry, s, sy, t, ty, w, y Vowel a, i, u, e, o Consider phonetic sequences “hyu-hyu-” of ono word “D0—00—” (hum). It is segmented into 4 consonants “hy</context>
</contexts>
<marker>Damerau, 1964</marker>
<rawString>Damerau, F. 1964. A Technique for Computer Detection and Correction of Spelling Errors. Communications of the ACM, 7:171–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>K McKeown</author>
</authors>
<title>Improving Word Sense Disambiguation in Lexical Chaining.</title>
<date>2003</date>
<booktitle>In Proc. of 19th International Joint Conference on Arti�cial Intelligence,</booktitle>
<pages>1486--1488</pages>
<contexts>
<context position="5471" citStr="Galley and McKeown, 2003" startWordPosition="830" endWordPosition="833">ed feature vectors that characterize two words. Features typically correspond to other words that co-occur with the characterized word in the same context. Lin (1998) proposed a word similarity measure based on the distributional pattern of words which allows to construct a thesaurus using a parsed corpus. He compared the result of automatically created thesaurus with WordNet and Roget, and reported that the result was significantly closer to WordNet than Roget Thesaurus was. Graph representations for word similarity have also been proposed by several researchers (Jannink and Wiederhold,1999; Galley and McKeown, 2003; Muller et al., 2006). Sinha and Mihalcea (2007) proposed a graph-based algorithm for unsupervised word sense disambiguation which combines several semantic similarity measures including Resnik’s metric (Resnik, 1995), and algorithms for graph centrality. They reported that the results using the SENSEVAL-2 and SENSEVAL-3 English all-words data sets lead to relative error rate reductions of 5 − 8% as compared to the previous work (Mihalcea, 2005). In the context of graph-based clustering of words, Widdows and Dorow (2002) used a graph model for unsupervised lexical acquisition. The graph struc</context>
</contexts>
<marker>Galley, McKeown, 2003</marker>
<rawString>Galley, M. and K. McKeown. 2003. Improving Word Sense Disambiguation in Lexical Chaining. In Proc. of 19th International Joint Conference on Arti�cial Intelligence, pages 1486–1488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Geffet</author>
<author>I Dagan</author>
</authors>
<title>Feature Vector Quality and Distributional Similarity.</title>
<date>2004</date>
<booktitle>In Proc. of 20th International Conference on Computational Linguistics,</booktitle>
<pages>247--253</pages>
<contexts>
<context position="4702" citStr="Geffet and Dagan, 2004" startWordPosition="713" endWordPosition="716">ound symbolism indicates that phonemes or phonetic sequences express their senses. As ono words imitate the sounds associated with the objects or actions they refer to, their phonetic sequences provide semantic clues for classification. The empirical results are encouraging, and showed a 9.0% improvement over the baseline single distributional similarity measure. 2 Previous Work There are quite a lot of work on semantic classification of words with corpus-based approach. The earliest work in this direction are those of (Hindle, 1990), (Lin, 1998), (Dagan et al., 1999), (Chen and Chen, 2000), (Geffet and Dagan, 2004) and (Weeds and Weir, 2005). They used distributional similarity. Similarity measures based on distributional hypothesis compare a pair of weighted feature vectors that characterize two words. Features typically correspond to other words that co-occur with the characterized word in the same context. Lin (1998) proposed a word similarity measure based on the distributional pattern of words which allows to construct a thesaurus using a parsed corpus. He compared the result of automatically created thesaurus with WordNet and Roget, and reported that the result was significantly closer to WordNet </context>
</contexts>
<marker>Geffet, Dagan, 2004</marker>
<rawString>Geffet, M. and I. Dagan. 2004. Feature Vector Quality and Distributional Similarity. In Proc. of 20th International Conference on Computational Linguistics, pages 247–253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Noun Classification from Predicateargument Structures.</title>
<date>1990</date>
<booktitle>In Proc. of 28th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>268--275</pages>
<contexts>
<context position="4618" citStr="Hindle, 1990" startWordPosition="701" endWordPosition="702">arity which is based on a feature of ono words called “sound symbolism”. Sound symbolism indicates that phonemes or phonetic sequences express their senses. As ono words imitate the sounds associated with the objects or actions they refer to, their phonetic sequences provide semantic clues for classification. The empirical results are encouraging, and showed a 9.0% improvement over the baseline single distributional similarity measure. 2 Previous Work There are quite a lot of work on semantic classification of words with corpus-based approach. The earliest work in this direction are those of (Hindle, 1990), (Lin, 1998), (Dagan et al., 1999), (Chen and Chen, 2000), (Geffet and Dagan, 2004) and (Weeds and Weir, 2005). They used distributional similarity. Similarity measures based on distributional hypothesis compare a pair of weighted feature vectors that characterize two words. Features typically correspond to other words that co-occur with the characterized word in the same context. Lin (1998) proposed a word similarity measure based on the distributional pattern of words which allows to construct a thesaurus using a parsed corpus. He compared the result of automatically created thesaurus with </context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>Hindle, D. 1990. Noun Classification from Predicateargument Structures. In Proc. of 28th Annual Meeting of the Association for Computational Linguistics, pages 268–275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jannink</author>
<author>G Wiederhold</author>
</authors>
<title>Thesaurus Entry Extraction from an On-line Dictionary.</title>
<date>1999</date>
<booktitle>In Proc. of Fusion’99.</booktitle>
<marker>Jannink, Wiederhold, 1999</marker>
<rawString>Jannink, J. and G. Wiederhold. 1999. Thesaurus Entry Extraction from an On-line Dictionary. In Proc. of Fusion’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lee</author>
</authors>
<title>Measures of Distributional Similarity.</title>
<date>1999</date>
<booktitle>In Proc. of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="11348" citStr="Lee (1999)" startWordPosition="1799" endWordPosition="1800">d α-skew divergence. The cosine measures the similarity of the two vectors by calculating the cosine of the angle between vectors. α-skew divergence is defined as: Sall x f(Oi, Oj) log ,(1) SOi x SOj f(Oi,Ok), (2) k∈Oall SOi. (3) x2(Oi,Oj) = f(Oi, Oj) − E(Oi, Oj) (4) αdiv(x, y) = D(y ||α · x + (1 − α) · y), E(Oi, Oj) where E(Oi, Oj) = SOi x SS09 oj (5) all SOi and Sall in Eq. (5) refer to Eq. (2) and (3), respectively. A major difference between χ2 and MI is that the former is a normalized value. where D(x||y) refers to Kullback-Leibler and defined as: D(x||y) = �n xi i=1 xi * log . (6) yi 35 Lee (1999) reported the best results with α = 0.9. We used the same value. We defined a similarity metric by combining co-occurrence based and orthographic similarity measures2: Sim(Oi, Oj) = MI(Oi,Oj) x (Cos(Oi, Oj) + 1) (7) 3.3 The Newman Clustering Algorithm We classified ono words collected from the WWW. Therefore, the clustering algorithm should be efficient and effective even in the very high dimensional spaces. For this purpose, we chose a graphbased clustering algorithm, called Newman clustering. The Newman clustering is a hierarchical clustering algorithm which is based on Network structure (Ne</context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>Lee, L. 1999. Measures of Distributional Similarity. In Proc. of the 37th Annual Meeting of the Association for Computational Linguistics, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words.</title>
<date>1998</date>
<booktitle>In Proc. of 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<pages>768--773</pages>
<contexts>
<context position="4631" citStr="Lin, 1998" startWordPosition="703" endWordPosition="704">ased on a feature of ono words called “sound symbolism”. Sound symbolism indicates that phonemes or phonetic sequences express their senses. As ono words imitate the sounds associated with the objects or actions they refer to, their phonetic sequences provide semantic clues for classification. The empirical results are encouraging, and showed a 9.0% improvement over the baseline single distributional similarity measure. 2 Previous Work There are quite a lot of work on semantic classification of words with corpus-based approach. The earliest work in this direction are those of (Hindle, 1990), (Lin, 1998), (Dagan et al., 1999), (Chen and Chen, 2000), (Geffet and Dagan, 2004) and (Weeds and Weir, 2005). They used distributional similarity. Similarity measures based on distributional hypothesis compare a pair of weighted feature vectors that characterize two words. Features typically correspond to other words that co-occur with the characterized word in the same context. Lin (1998) proposed a word similarity measure based on the distributional pattern of words which allows to construct a thesaurus using a parsed corpus. He compared the result of automatically created thesaurus with WordNet and R</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, D. 1998. Automatic Retrieval and Clustering of Similar Words. In Proc. of 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, pages 768–773.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Matsuo</author>
<author>T Sakaki</author>
<author>K Uchiyama</author>
<author>M Ishizuka</author>
</authors>
<title>Graph-based Word Clustering using a Web Search Engine.</title>
<date>2006</date>
<booktitle>In Proc. of 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP2006),</booktitle>
<pages>542--550</pages>
<contexts>
<context position="6384" citStr="Matsuo et al. (2006)" startWordPosition="969" endWordPosition="972"> the SENSEVAL-2 and SENSEVAL-3 English all-words data sets lead to relative error rate reductions of 5 − 8% as compared to the previous work (Mihalcea, 2005). In the context of graph-based clustering of words, Widdows and Dorow (2002) used a graph model for unsupervised lexical acquisition. The graph structure is built by linking pairs of words which participate in particular syntactic relationships. An incremental cluster-building algorithm using the graph structure achieved 82% accuracy at a lexical acquisition task, evaluated against WordNet 10 classes, and each class consists of 20 words. Matsuo et al. (2006) proposed a method of word clustering based on a word similarity measure by Web counts. They used Newman clustering for clustering algorithm. They evaluated their method using two sets of word classes. One is derived from the Web data, and another is from WordNet.1 Each set consists of 90 noun words. They reported that the results obtained by Newman clustering were better than those obtained by average-link agglomerative clustering. Our work is similar to their method in the use of Newman clustering. However, they classified Japanese noun words, while our work is the first to aim at detecting </context>
</contexts>
<marker>Matsuo, Sakaki, Uchiyama, Ishizuka, 2006</marker>
<rawString>Matsuo, Y., T. Sakaki, K. Uchiyama, and M. Ishizuka. 2006. Graph-based Word Clustering using a Web Search Engine. In Proc. of 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP2006), pages 542–550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W T McLeod</author>
</authors>
<title>The COLLINS Dictionary and Thesaurus.</title>
<date>1991</date>
<publisher>HarperCollinsPublishers.</publisher>
<contexts>
<context position="1269" citStr="McLeod, 1991" startWordPosition="176" endWordPosition="177"> a simple quality function to test whether a particular division is meaningful. The quality function is calculated based on the weights of edges between nodes. We combined two different similarity measures, distributional similarity, and orthographic similarity to calculate weights. The results obtained by using the Web data showed a 9.0% improvement over the baseline single distributional similarity measure. 1 Introduction Onomatopoeia which we call onomatopoetic word (ono word) is the formation of words whose sound is imitative of the sound of the noise or action designated, such as ‘hiss’ (McLeod, 1991). It is one of the linguistic features of Japanese. Consider two sentences from Japanese. (1) )*+,-./01-234&apos;567-38 9:;&lt;=&gt; “I’m too sleepy because I awoke to the slippers in the hall.” © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. (2) )*+,? @7@7 AB./01-234 &apos;567-389:;&lt;=&gt; “I’m too sleepy because I awoke to the pit-apat of slippers in the hall.” Sentences (1) and (2) are almost the same sense. However, sentence (2) which includes ono word, “@7@7 (pit-a-pat)” is much b</context>
</contexts>
<marker>McLeod, 1991</marker>
<rawString>McLeod, W. T. 1991. The COLLINS Dictionary and Thesaurus. HarperCollinsPublishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
</authors>
<title>Unsupervised Large Vocabulary Word Sense Disambiguation with Graph-based Algorithms for Sequence Data Labeling.</title>
<date>2005</date>
<booktitle>In Proc. of the Human Language Technology / Empirical Methods in Natural Language PRocessing Conference,</booktitle>
<pages>411--418</pages>
<contexts>
<context position="5921" citStr="Mihalcea, 2005" startWordPosition="901" endWordPosition="902"> Roget Thesaurus was. Graph representations for word similarity have also been proposed by several researchers (Jannink and Wiederhold,1999; Galley and McKeown, 2003; Muller et al., 2006). Sinha and Mihalcea (2007) proposed a graph-based algorithm for unsupervised word sense disambiguation which combines several semantic similarity measures including Resnik’s metric (Resnik, 1995), and algorithms for graph centrality. They reported that the results using the SENSEVAL-2 and SENSEVAL-3 English all-words data sets lead to relative error rate reductions of 5 − 8% as compared to the previous work (Mihalcea, 2005). In the context of graph-based clustering of words, Widdows and Dorow (2002) used a graph model for unsupervised lexical acquisition. The graph structure is built by linking pairs of words which participate in particular syntactic relationships. An incremental cluster-building algorithm using the graph structure achieved 82% accuracy at a lexical acquisition task, evaluated against WordNet 10 classes, and each class consists of 20 words. Matsuo et al. (2006) proposed a method of word clustering based on a word similarity measure by Web counts. They used Newman clustering for clustering algori</context>
</contexts>
<marker>Mihalcea, 2005</marker>
<rawString>Mihalcea, R. 2005. Unsupervised Large Vocabulary Word Sense Disambiguation with Graph-based Algorithms for Sequence Data Labeling. In Proc. of the Human Language Technology / Empirical Methods in Natural Language PRocessing Conference, pages 411–418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Muller</author>
<author>N Hathout</author>
<author>B Gaume</author>
</authors>
<title>Synonym Extraction Using a Semantic Distance on a Dictionary.</title>
<date>2006</date>
<booktitle>In Proc. of the Workshop on TextGraphs,</booktitle>
<pages>65--72</pages>
<contexts>
<context position="5493" citStr="Muller et al., 2006" startWordPosition="834" endWordPosition="837">aracterize two words. Features typically correspond to other words that co-occur with the characterized word in the same context. Lin (1998) proposed a word similarity measure based on the distributional pattern of words which allows to construct a thesaurus using a parsed corpus. He compared the result of automatically created thesaurus with WordNet and Roget, and reported that the result was significantly closer to WordNet than Roget Thesaurus was. Graph representations for word similarity have also been proposed by several researchers (Jannink and Wiederhold,1999; Galley and McKeown, 2003; Muller et al., 2006). Sinha and Mihalcea (2007) proposed a graph-based algorithm for unsupervised word sense disambiguation which combines several semantic similarity measures including Resnik’s metric (Resnik, 1995), and algorithms for graph centrality. They reported that the results using the SENSEVAL-2 and SENSEVAL-3 English all-words data sets lead to relative error rate reductions of 5 − 8% as compared to the previous work (Mihalcea, 2005). In the context of graph-based clustering of words, Widdows and Dorow (2002) used a graph model for unsupervised lexical acquisition. The graph structure is built by linki</context>
</contexts>
<marker>Muller, Hathout, Gaume, 2006</marker>
<rawString>Muller, P., N. Hathout, and B. Gaume. 2006. Synonym Extraction Using a Semantic Distance on a Dictionary. In Proc. of the Workshop on TextGraphs, pages 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E J Newman</author>
</authors>
<title>Fast algorithm for detecting community structure in networks.</title>
<date>2004</date>
<journal>In Physics Review E,</journal>
<volume>69</volume>
<pages>066133</pages>
<contexts>
<context position="11959" citStr="Newman, 2004" startWordPosition="1896" endWordPosition="1897">9) reported the best results with α = 0.9. We used the same value. We defined a similarity metric by combining co-occurrence based and orthographic similarity measures2: Sim(Oi, Oj) = MI(Oi,Oj) x (Cos(Oi, Oj) + 1) (7) 3.3 The Newman Clustering Algorithm We classified ono words collected from the WWW. Therefore, the clustering algorithm should be efficient and effective even in the very high dimensional spaces. For this purpose, we chose a graphbased clustering algorithm, called Newman clustering. The Newman clustering is a hierarchical clustering algorithm which is based on Network structure (Newman, 2004). The network structure consists of nodes within which the node-node connections are edges. It produces some division of the nodes into communities, regardless of whether the network structure has any natural such division. Here, “community” or “cluster” have in common that they are groups of densely interconnected nodes that are only sparsely connected with the rest of the network. To test whether a particular division is meaningful a quality function Q is defined: � Q = i where eij is the sum of the weight of edges between two communities i and j divided by the sum of the weight of all edges</context>
</contexts>
<marker>Newman, 2004</marker>
<rawString>Newman, M. E. J. 2004. Fast algorithm for detecting community structure in networks. In Physics Review E, (69, 066133).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ono</author>
</authors>
<date>2007</date>
<journal>Nihongo Omomatope Jiten (in Japanese). Shougakukan.</journal>
<contexts>
<context position="14404" citStr="Ono, 2007" startWordPosition="2362" endWordPosition="2363"> AQij = eij + eji − 2aiaj = 2(eij − aiaj) 4. Apply step 3. to every pair of communities. 5. Join two communities such that AQ is maximum and create one community. If AQ &lt; 0, go to step 7. 6. Re-calculate eij and ai of the joined community, and go to step 3. 7. Words within the same community are regarded as semantically similar. The computational cost of the algorithm is known as O((m + n)n) or O(n2), where m and n are the number of edges and nodes, respectively. 4 Experiments 4.1 Experimental Setup The data for the classification of ono words have been taken from the Japanese ono dictionary (Ono, 2007) that consisted of 4,500 words. Of these, we selected 273 words, which occurred at least 5,000 in the document URLs from the WWW. The minimum frequency of a word was found to be 5,220, while the maximum was about 26 million. These words are classified into 10 classes. Word classes and examples of ono words from the dictionary are listed in Table 2. “Id” denotes id number of each class. “Sense” refers to each sense of ono word within the same class, and “Num” is the number of words which should be assigned to each class. Each word 2 (eii − ai ) 36 Table 2: Onomatopoetic words and # of words in </context>
<context position="25176" citStr="Ono, 2007" startWordPosition="4124" endWordPosition="4125"> and work on improving the accuracy of classification. As shown in Table 2, many of the ono words consist of duplicative character sequences such as “h” and “a” of “a,h,a,h,a”, and “h” and “i” of “i,h,i,h,i”. Moreover, characters which consist of ono words within the same class match. For example, the hiragana character “U” (h,a) frequently appears in laugh sense class. These observations indicate that integrating edit-distance and our current similarity measure will improve overall performance. Another interesting direction is a problem of polysemy. It clearly supports the classification of (Ono, 2007) to insist that some ono words belong to more than one cluster. For example, “(i,s,o,i,s,o)” has at least two senses, panic and feel delight sense. In order to accommodate this, we Co-occ(MI) Co-occ(MI) &amp; Sounds(cos) Precision 0.55 0.45 0.35 0.25 0.15 0.6 0.5 0.4 0.3 0.2 0.1 Co-occ(MI) Co-occ(MI) &amp; Sounds(cos) Sounds(cos) F-measure 0.5 0.45 0.4 0.35 0.3 0.25 0.2 0.15 0.1 39 should apply an appropriate soft clustering technique (Tishby et al., 1999; Reichardt and Bornholdt, 2006; Zhang et al., 2007). Acknowledgments We would like to thank the anonymous reviewers for their helpful suggestions. T</context>
</contexts>
<marker>Ono, 2007</marker>
<rawString>Ono, M. 2007. Nihongo Omomatope Jiten (in Japanese). Shougakukan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Reichardt</author>
<author>S Bornholdt</author>
</authors>
<date>2006</date>
<booktitle>Statistical Mechanics of Community Detection. PHYICAL REVIEW E,</booktitle>
<pages>74--1</pages>
<marker>Reichardt, Bornholdt, 2006</marker>
<rawString>Reichardt, J. and S. Bornholdt. 2006. Statistical Mechanics of Community Detection. PHYICAL REVIEW E, (74):1–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Using Information Content to Evaluate Semantic Similarity in a Taxonomy.</title>
<date>1995</date>
<booktitle>In Proc. of 14th International Joint Conference on Arti�cial Intelligence,</booktitle>
<pages>448--453</pages>
<contexts>
<context position="5689" citStr="Resnik, 1995" startWordPosition="863" endWordPosition="864"> pattern of words which allows to construct a thesaurus using a parsed corpus. He compared the result of automatically created thesaurus with WordNet and Roget, and reported that the result was significantly closer to WordNet than Roget Thesaurus was. Graph representations for word similarity have also been proposed by several researchers (Jannink and Wiederhold,1999; Galley and McKeown, 2003; Muller et al., 2006). Sinha and Mihalcea (2007) proposed a graph-based algorithm for unsupervised word sense disambiguation which combines several semantic similarity measures including Resnik’s metric (Resnik, 1995), and algorithms for graph centrality. They reported that the results using the SENSEVAL-2 and SENSEVAL-3 English all-words data sets lead to relative error rate reductions of 5 − 8% as compared to the previous work (Mihalcea, 2005). In the context of graph-based clustering of words, Widdows and Dorow (2002) used a graph model for unsupervised lexical acquisition. The graph structure is built by linking pairs of words which participate in particular syntactic relationships. An incremental cluster-building algorithm using the graph structure achieved 82% accuracy at a lexical acquisition task, </context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Resnik, P. 1995. Using Information Content to Evaluate Semantic Similarity in a Taxonomy. In Proc. of 14th International Joint Conference on Arti�cial Intelligence, pages 448–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sinha</author>
<author>R Mihalcea</author>
</authors>
<title>Unsupervised Graph-based Word Sense Disambiguation Using Measures of Word Semantic Similarity.</title>
<date>2007</date>
<booktitle>In Proc. of the IEEE International Conference on Semantic Computing,</booktitle>
<pages>46--54</pages>
<contexts>
<context position="5520" citStr="Sinha and Mihalcea (2007)" startWordPosition="838" endWordPosition="841">Features typically correspond to other words that co-occur with the characterized word in the same context. Lin (1998) proposed a word similarity measure based on the distributional pattern of words which allows to construct a thesaurus using a parsed corpus. He compared the result of automatically created thesaurus with WordNet and Roget, and reported that the result was significantly closer to WordNet than Roget Thesaurus was. Graph representations for word similarity have also been proposed by several researchers (Jannink and Wiederhold,1999; Galley and McKeown, 2003; Muller et al., 2006). Sinha and Mihalcea (2007) proposed a graph-based algorithm for unsupervised word sense disambiguation which combines several semantic similarity measures including Resnik’s metric (Resnik, 1995), and algorithms for graph centrality. They reported that the results using the SENSEVAL-2 and SENSEVAL-3 English all-words data sets lead to relative error rate reductions of 5 − 8% as compared to the previous work (Mihalcea, 2005). In the context of graph-based clustering of words, Widdows and Dorow (2002) used a graph model for unsupervised lexical acquisition. The graph structure is built by linking pairs of words which par</context>
</contexts>
<marker>Sinha, Mihalcea, 2007</marker>
<rawString>Sinha, R. and R. Mihalcea. 2007. Unsupervised Graph-based Word Sense Disambiguation Using Measures of Word Semantic Similarity. In Proc. of the IEEE International Conference on Semantic Computing, pages 46–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Tishby</author>
<author>F C Pereira</author>
<author>W Bialek</author>
</authors>
<title>The Information Bottleneck Method.</title>
<date>1999</date>
<booktitle>In Proc. of 37th Annual Allerton Conference on Communication Control and Computing,</booktitle>
<pages>368--377</pages>
<marker>Tishby, Pereira, Bialek, 1999</marker>
<rawString>Tishby, N., F. C. Pereira, and W. Bialek. 1999. The Information Bottleneck Method. In Proc. of 37th Annual Allerton Conference on Communication Control and Computing, pages 368–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Weeds</author>
<author>D Weir</author>
</authors>
<title>Co-occurrence Retrieval: A Flexible Framework for Lexical Distributional Similarity.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>4</issue>
<pages>476</pages>
<contexts>
<context position="4729" citStr="Weeds and Weir, 2005" startWordPosition="718" endWordPosition="721"> phonemes or phonetic sequences express their senses. As ono words imitate the sounds associated with the objects or actions they refer to, their phonetic sequences provide semantic clues for classification. The empirical results are encouraging, and showed a 9.0% improvement over the baseline single distributional similarity measure. 2 Previous Work There are quite a lot of work on semantic classification of words with corpus-based approach. The earliest work in this direction are those of (Hindle, 1990), (Lin, 1998), (Dagan et al., 1999), (Chen and Chen, 2000), (Geffet and Dagan, 2004) and (Weeds and Weir, 2005). They used distributional similarity. Similarity measures based on distributional hypothesis compare a pair of weighted feature vectors that characterize two words. Features typically correspond to other words that co-occur with the characterized word in the same context. Lin (1998) proposed a word similarity measure based on the distributional pattern of words which allows to construct a thesaurus using a parsed corpus. He compared the result of automatically created thesaurus with WordNet and Roget, and reported that the result was significantly closer to WordNet than Roget Thesaurus was. G</context>
</contexts>
<marker>Weeds, Weir, 2005</marker>
<rawString>Weeds, J. and D. Weir. 2005. Co-occurrence Retrieval: A Flexible Framework for Lexical Distributional Similarity. Computational Linguistics, 31(4):439– 476.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Widdows</author>
<author>B Dorow</author>
</authors>
<title>A Graph Model for Unsupervised Lexical Acquisition.</title>
<date>2002</date>
<booktitle>In Proc. of 19th International conference on Computational Linguistics (COLING2002),</booktitle>
<pages>1093--1099</pages>
<contexts>
<context position="5998" citStr="Widdows and Dorow (2002)" startWordPosition="911" endWordPosition="914"> also been proposed by several researchers (Jannink and Wiederhold,1999; Galley and McKeown, 2003; Muller et al., 2006). Sinha and Mihalcea (2007) proposed a graph-based algorithm for unsupervised word sense disambiguation which combines several semantic similarity measures including Resnik’s metric (Resnik, 1995), and algorithms for graph centrality. They reported that the results using the SENSEVAL-2 and SENSEVAL-3 English all-words data sets lead to relative error rate reductions of 5 − 8% as compared to the previous work (Mihalcea, 2005). In the context of graph-based clustering of words, Widdows and Dorow (2002) used a graph model for unsupervised lexical acquisition. The graph structure is built by linking pairs of words which participate in particular syntactic relationships. An incremental cluster-building algorithm using the graph structure achieved 82% accuracy at a lexical acquisition task, evaluated against WordNet 10 classes, and each class consists of 20 words. Matsuo et al. (2006) proposed a method of word clustering based on a word similarity measure by Web counts. They used Newman clustering for clustering algorithm. They evaluated their method using two sets of word classes. One is deriv</context>
</contexts>
<marker>Widdows, Dorow, 2002</marker>
<rawString>Widdows, D. and B. Dorow. 2002. A Graph Model for Unsupervised Lexical Acquisition. In Proc. of 19th International conference on Computational Linguistics (COLING2002), pages 1093–1099.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Zhang</author>
<author>R S Wang</author>
<author>X S Zhang</author>
</authors>
<date>2007</date>
<booktitle>Identification of Overlapping Community Structure in Complex Networks using Fuzzy C-means Clustering. PHYSICAA,</booktitle>
<pages>374--483</pages>
<marker>Zhang, Wang, Zhang, 2007</marker>
<rawString>Zhang, S., R. S. Wang, and X. S. Zhang. 2007. Identification of Overlapping Community Structure in Complex Networks using Fuzzy C-means Clustering. PHYSICAA, (374):483–490.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>