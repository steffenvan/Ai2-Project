<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.966399">
Summarization-based Query Expansion in Information Retrieval
</title>
<author confidence="0.994454">
Tomek StrzaIkowski, Jin Wang, and Bowden Wise
</author>
<affiliation confidence="0.898725">
GE Corporate Research and Development
1 Research Circle
</affiliation>
<address confidence="0.661871">
Niskayuna, NY 12309
</address>
<email confidence="0.992262">
strzalkowski@crd.ge.com
</email>
<sectionHeader confidence="0.976432" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9908946">
We discuss a semi-interactive approach to infor-
mation retrieval which consists of two tasks per-
formed in a sequence. First, the system assists
the searcher in building a comprehensive statement
of information need, using automatically generated
topical summaries of sample documents. Second,
the detailed statement of information need is auto-
matically processed by a series of natural language
processing routines in order to derive an optimal
search query for a statistical information retrieval
system. In this paper, we investigate the role of au-
tomated document summarization in building effec-
tive search statements. We also discuss the results
of latest evaluation of our system at the annual Text
Retrieval Conference (TREC).
</bodyText>
<sectionHeader confidence="0.519217" genericHeader="method">
Information Retrieval
</sectionHeader>
<bodyText confidence="0.996162222222222">
Information retrieval (IR) is a task of selecting docu-
ments from a database in response to a user&apos;s query,
and ranking them according to relevance. This has
been usually accomplished using statistical methods
(often coupled with manual encoding) that (a) select
terms (words, phrases, and other units) from docu-
ments that are deemed to best represent their con-
tent, and (b) create an inverted index file (or files)
that provide an easy access to documents containing
these terms. A subsequent search process attempts
to match preprocessed user queries against term-
based representations of documents in each case de-
termining a degree of relevance between the two
which depends upon the number and types of match-
ing terms.
A search is successful if it can return as many
as possible documents which are relevant to the
query, with as few as possible non-relevant docu-
ments. In addition, the relevant documents should
be ranked ahead of non-relevant ones. The quanti-
tative text representation methods, predominant in
today&apos;s leading information retrieval systemsi limit
&apos;Representations anchored on words, word or char-
the system&apos;s ability to generate a successful search
because they rely more on the form of a query than
on its content in finding document matches. This
problem is particularly acute in ad-hoc retrieval situ-
ations where the user has only a limited knowledge of
database composition and needs to resort to generic
or otherwise incomplete search statements. In or-
der to overcome this limitation, many IR systems
allow varying degrees of user interaction that facil-
itates query optimization and calibration to closer
match user&apos;s information seeking goals. A popular
technique here is relevance feedback, where the user
or the system judges the relevance of a sample of re-
sults returned from an initial search, and the query is
subsequently rebuilt to reflect this information. Au-
tomatic relevance feedback techniques can lead to
a very close mapping of known relevant documents,
however, they also tend to overfit, which in turn re-
duces their ability of finding new documents on the
same subject. Therefore, a serious challenge for in-
formation retrieval is to devise methods for building
better queries, or in assisting user to do so.
</bodyText>
<subsectionHeader confidence="0.846415">
Building effective search queries
</subsectionHeader>
<bodyText confidence="0.9741233125">
We have been experimenting with manual and auto-
matic natural language query (or topic, in TREC
parlance) building techniques. This differs from
most query modification techniques used in IR in
that our method is to reformulate the user&apos;s state-
ment of information need rather than the search sys-
tem&apos;s internal representation of it, as relevance feed-
back does. Our goal is to devise a method of full-
text expansion that would allow for creating exhaus-
tive search topics such that: (1) the performance
of any system using the expanded topics would be
significantly better than when the system is run us-
ing the original topics, and (2) the method of topic
atter sequences, or some surrogates of these, along with
significance weights derived from their distribution in the
database.
</bodyText>
<page confidence="0.986738">
1258
</page>
<bodyText confidence="0.999961376470588">
expansion could eventually be automated or semi-
automated so as to be useful to a non-expert user.
Note that the first of the above requirements effec-
tively calls for a free text, unstructured, but highly
precise and exhaustive description of user&apos;s search
statement. The preliminary results from TREC
evaluations show that such an approach is indeed
very effective.
One way to view query expansion is to make the
user query resemble more closely the documents it is
expected to retrieve. This may include both content,
as well as some other aspects such as composition,
style, language type, etc. If the query is indeed made
to resemble a &amp;quot;typical&amp;quot; relevant document, then sud-
denly everything about this query becomes a valid
search criterion: words, collocations, phrases, var-
ious relationships, etc. Unfortunately, an average
search query does not look anything like this, most
of the time. It is more likely to be a statement speci-
fying the semantic criteria of relevance. This means
that except for the semantic or conceptual resem-
blance (which we cannot model very well as yet)
much of the appearance of the query (which we can
model reasonably well) may be, and often is, quite
misleading for search purposes. Where can we get
the right queries?
In today&apos;s information retrieval, query expansion
usually is typically limited to adding, deleting or
re-weighting of terms. For example, content terms
from documents judged relevant are added to the
query while weights of all terms are adjusted in or-
der to reflect the relevance information. Thus, terms
occurring predominantly in relevant documents will
have their weights increased, while those occurring
mostly in non-relevant documents will have their
weights decreased. This process can be performed
automatically using a relevance feedback method,
e.g., (Rocchio 1971), with the relevance informa-
tion either supplied manually by the user (Har-
man 1988), or otherwise guessed, e.g. by assum-
ing top 10 documents relevant, etc. (Buckley, et
al. 1995). A serious problem with this term-based
expansion is its limited ability to capture and rep-
resent many important aspects of what makes some
documents relevant to the query, including particu-
lar term co-occurrence patterns, and other hard-to-
measure text features, such as discourse structure or
stylistics. Additionally, relevance-feedback expan-
sion depends on the inherently partial relevance in-
formation, which is normally unavailable, or unre-
liable. Other types of query expansions, including
general purpose thesauri or lexical databases (e.g.,
Wordnet) have been found generally unsuccessful in
information retrieval, (Voorhees 1994).
An alternative to term-only expansion is a full-
text expansion described in (Strzalkowski et al.
1997). In this approach, search topics are expanded
by pasting in entire sentences, paragraphs, and other
sequences directly from any text document. To
make this process efficient, an initial search is per-
formed with the unexpanded queries and the top
N (10-30) returned documents are used for query
expansion. These documents, irrespective of their
overall relevancy to the search topic, are scanned
for passages containing concepts referred to in the
query. The resulting expanded queries undergo fur-
ther text processing steps, before the search is run
again. We need to note that the expansion ma-
terial was found in both relevant and non-relevant
documents, benefiting the final query all the same.
In fact, the presence of such text in otherwise non-
relevant documents underscores the inherent limita-
tions of distribution-based term reweighting used in
relevance feedback.
In this paper, we describe a method of full-text
topic expansion where the expansion passages are
obtained from an automatic text summarizer. A
preliminary examination of TREC-6 results indicate
that this mode of expansion is at least as effective
as the purely manual expansion which requires the
users to read entire documents to select expansion
passages. This brings us a step closer to a fully au-
tomated expansion: the human-decision factor has
been reduced to an accept/reject decision for ex-
panding the search query with a summary.
</bodyText>
<subsectionHeader confidence="0.941971">
Summarization-based query expansion
</subsectionHeader>
<bodyText confidence="0.9995058">
We used our automatic text summarizer to de-
rive query-specific summaries of documents returned
from the first round of retrieval. The summaries
were usually 1 or 2 consecutive paragraphs selected
from the original document text. The initial purpose
was to show to the user, by the way of a quick-read
abstract, why a document has been retrieved. If the
summary appeared relevant and moreover captured
some important aspect of relevant information, then
the user had an option to paste it into the query,
thus increasing the chances of a more successful sub-
sequent search. Note again that it wasn&apos;t important
if the summarized documents were themselves rele-
vant, although they usually were.
The query expansion interaction proceeds as fol-
lows:
1. The initial natural language statement of informa-
tion need is submitted to SMART-based NLIR re-
trieval engine via a Query Expansion Tool (QET)
interface. The statement is converted into an in-
</bodyText>
<page confidence="0.971503">
1259
</page>
<bodyText confidence="0.995394">
ternal search query and run against the TREC
database.&apos;
</bodyText>
<listItem confidence="0.937685647058824">
2. NOR returns top N (=30) documents from the
database that match the search query.
3. The user determines a topic for the summarizer.
By default, it is the title field of the initial search
statement (see below).
4. The summarizer is invoked to automatically sum-
marize each of the N documents with respect to
the selected topic.
5. The user reviews the summaries (spending ap-
prox. 5-15 seconds per summary) and de-selects
these that are not relevant to the search state-
ment.
6. All remaining summaries are automatically at-
tached to the search statement.
7. The expanded search statement is passed through
a series of natural language processing steps and
then submitted for the final retrieval.
</listItem>
<bodyText confidence="0.999833">
A partially expanded TREC Topic 304 is shown
below. The original topic comprises the first four
fields, with the Expanded field added through the
query expansion process. The initial query, while
somewhat lengthy by IR standards (though not by
TREC standards) is still quite generic in form, that
is, it supplies few specifics to guide the search. In
contrast, the Expanded section supplies not only
many concrete examples of relevant concepts (here,
names of endangered mammals) but also the lan-
guage and the style used by others to describe them.
</bodyText>
<listItem confidence="0.67846225">
&lt; lop &gt;
&lt; num &gt; Number: 304
&lt; title &gt; Endangered Species (Mammals)
&lt; desc &gt; Description:
Compile a list of mammals that are considered to be endan-
gered, identify their habitat and, if possible, specify what
threatens them.
&lt; narr &gt; Narrative:
</listItem>
<bodyText confidence="0.86495772972973">
Any document identifying a mammal as endangered is rel-
evant. Statements of authorities disputing the endangered
status would also be relevant. A document containing infor-
mation on habitat and populations of a mammal identified
elsewhere as endangered would also be relevant even if the
document at hand did not identify the species as endan-
gered. Generalized statements about endangered species
without reference to specific mammals would not be rele-
vant.
&lt; expd &gt; Expanded:
2TREC-6 database consisted of approx. 2 GBytes of
documents from Associated Press newswire, Wall Street
Journal, Financial Times, Federal Register, PETS and
other sources (Harman Sz Voorhees 1998).
The Service is responsible for eight species of marine mam-
mals under the jurisdiction of the Department of the Inte-
rior, as assigned by the Marine Mammal Protection Act of
1972. These species are polar bear, sea and marine otters,
walrus, manatees (three species) and dugong. The report
reviews the Service&apos;s marine mammal-related activities dur-
ing the report period.
The U.S. Fish and Wildlife Service had classified the pri-
mate as a &amp;quot;threatened&amp;quot; species, but officials said that more
protection was needed in view of recent studies document-
ing a drastic decline in the populations of wild chimps in
Africa.
The Endangered Species Act was passed in 1973 and has
been used to provide protection to the bald eagle and grizzly
bear, among other animals.
Under the law, a designation of a threatened species means
it is likely to become extinct without protection, whereas
extinction is viewed as a certainty for an endangered
species.
The bear on California&apos;s state flag should remind us of what
we have done to some of our species. It is a grizzly. And
it is extinct in California and in most other states where it
once roamed.
</bodyText>
<equation confidence="0.35782">
&lt; /fop &gt;
</equation>
<bodyText confidence="0.9983145">
In the next section we describe the summarization
process in detail.
</bodyText>
<subsectionHeader confidence="0.612135">
Robust text summarization
</subsectionHeader>
<bodyText confidence="0.999965903225807">
Perhaps the most difficult problem in designing an
automatic text summarization is to define what a
summary is, and how to tell a summary from a non-
summary, or a good summary from a bad one. The
answer depends in part upon who the summary is
intended for, and in part upon what it is meant to
achieve, which in large measure precludes any ob-
jective evaluation. For most of us, a summary is a
brief synopsis of the content of a larger document, an
abstract recounting the main points while suppress-
ing most details. One purpose of having a summary
is to quickly learn some facts, and decide what you
want to do with the entire story. Therefore, one im-
portant evaluation criterion is the tradeoff between
the degree of compression afforded by the summary,
which may result in a decreased accuracy of infor-
mation, and the time required to review that infor-
mation. This interpretations is particularly useful,
though it isn&apos;t the only one acceptable, in summariz-
ing news and other report-like documents. It is also
well suited for evaluating the usefulness of summa-
rization in context of an information retrieval sys-
tem, where the user needs to rapidly and efficiently
review the documents returned from search for an
indication of relevance and, possibly, to see which
aspect of relevance is present.
Our early inspiration, and a benchmark, have
been the Quick Read Summaries, posted daily off
the front page of New York Times on-line edition
(http://www.nytimes.com). These summaries, pro-
duced manually by NYT staff, are assembled out of
</bodyText>
<page confidence="0.94701">
1260
</page>
<bodyText confidence="0.999967225806452">
passages, sentences, and sometimes sentence frag-
ments taken from the main article with very few,
if any, editorial adjustments. The effect is a col-
lection of perfectly coherent tidbits of news: the
who, the what, and when, but perhaps not why.
This kind of summarization, where appropriate pas-
sages are extracted from the original text, is very
efficient, and arguably effective, because it doesn&apos;t
require generation of any new text, and thus low-
ers the risk of misinterpretation. It is also relatively
easier to automate, because we only need to iden-
tify the suitable passages among the other text, a
task that can be accomplished via shallow NLP and
statistical techniques.3
It has been noted, eg., (Rino &amp; Scott 1994),
(Weissberg &amp; Buker 1990), that certain types of
texts, such as news articles, technical reports, re-
search papers, etc., conform to a set of style and or-
ganization constraints, called the Discourse Macro
Structure (DMS) which help the author to achieve
a desired communication effect. News reports, for
example, tend to be built hierarchically out of com-
ponents which fall roughly into one of the two cate-
gories: the what&apos;s-the-news category, and the op-
tional background category. The background, if
present, supplies the context necessary to under-
stand the central story, or to make a follow up story
self-contained. This organization is often reflected
in the summary, as illustrated in the example below
from NYT 10/15/97, where the highlighted portion
provides the background for the main news:
</bodyText>
<subsectionHeader confidence="0.898361">
Spies Just Wouldn&apos;t Come In From Cold War, Files Show
</subsectionHeader>
<bodyText confidence="0.990905">
Terry Sguillacole was a Pentagon lawyer who hated her
job. Kurt Stand was a union leader with an aging beat-
nik&apos;s slouch. Jim Clark was a lonely private investigator.
(A 200-page affidavit filed last week by] the Federal Bureau
of Investigation says the three were out-of-work spies for
East Germany. And after that state withered away, it says,
they desperately reached out for anyone who might want
them as secret agents.
In this example, the two passages are non-
consecutive paragraphs in the original text; the
string in the square brackets at the opening of the
second passage has been omitted in the summary.
Here the human summarizer&apos;s actions appear rela-
tively straightforward, and it would not be difficult
to propose an algorithmic method to do the same.
This may go as follows:
</bodyText>
<listItem confidence="0.6478605">
1. Choose a DMS template for the summary; e.g.,
Background+News.
</listItem>
<footnote confidence="0.978553333333333">
3This approach is contrasted with a far more diffi-
cult method of summarizing text &amp;quot;in your own words.&amp;quot;
Computational attempts at such discourse-level and
knowledge-level summarization include (Ono, Sumita &amp;
Miike 1994), (McKeown &amp; Radev 1995), (DeJong 1982),
and (Lehnert 1981).
</footnote>
<listItem confidence="0.966884">
2. Select appropriate passages from the original text
and fill the DMS template.
3. Assemble the summary in the desired order; delete
extraneous words.
</listItem>
<bodyText confidence="0.99974835">
We have used this method to build our auto-
mated summarizer. We overcome the shortcom-
ings of sentence-based summarization by working on
paragraph level instead.4 The summarizer has been
applied to a variety of documents, including Asso-
ciated Press newswires, articles from the New York
Times, Wall Street Journal, Financial Times, San
Jose Mercury, as well as documents from the Federal
Register, and Congressional Record. The program
is domain independent, and it can be easily adapted
to most European languages. It is also very robust:
we used it to derive summaries of thousands of doc-
uments returned by an information retrieval system.
It can work in two modes: generic and topical. In
the generic mode, it captures the main topic of a
document; in the topical mode, it takes a user sup-
plied statement of interest and derives a summary
related to this topic. The topical summary is usu-
ally different than the generic summary of the same
document.
</bodyText>
<subsectionHeader confidence="0.800399">
Deriving automatic summaries
</subsectionHeader>
<bodyText confidence="0.9984593">
Each component of a summary DMS needs to be in-
stantiated by one or more passages extracted from
the original text. Initially, all eligible passages (i.e.,
explicitly delineated paragraphs) within a document
are potential candidates for the summary. As we
move through text, paragraphs are scored for their
summary-worthiness. The final score for each pas-
sage, normalized for its length, is a weighted sum
of a number of minor scores, using the following
formula:5
</bodyText>
<equation confidence="0.999055">
1
score(paragraph) = •Ewh • Sh (1)
</equation>
<bodyText confidence="0.794917642857143">
where Sh is a minor score calculated using metric h;
wh is the weight reflecting how effective this metric
is in general; 1 is the length of the segment.
The following metrics are used to score passages
considered for the main news section of the summary
DMS. We list here only the criteria which are the
&amp;quot;Refer to (Luhn 1958) (Paice 1990) (Rau, Brandow
&amp; Mitze 1994) (Kupiec, Pedersen &amp; Chen 1995) for
sentence-based summarization approaches.
3The weights wh are trainable in a supervised mode,
given a corpus of texts and their summaries, or in an un-
supervised mode as described in (Strzalkowski &amp; Wang
1996). For the purpose of the experiments described
here, these weights have been set manually.
</bodyText>
<page confidence="0.984368">
1261
</page>
<bodyText confidence="0.9755685">
most relevant for generating summaries in context
of an information retrieval system.
</bodyText>
<listItem confidence="0.961556260869565">
1. Words and phrases frequently occurring in a text
are likely to be indicative of its content, espe-
cially if such words or phrases do not occur often
elsewhere in the database. A weighted frequency
score, similar to tridf used in automatic text in-
dexing is applicable. Here, idf stands for the in-
verted document frequency of a term.
2. Title of a text is often strongly related to its con-
tent. Therefore, words and phrases from the title
repeated in text are considered as important in-
dicators of content concentration within a docu-
ment.
3. Noun phrases occurring in the opening sentences
of multiple paragraphs tend to be indicative of the
content. These phrases, along with words from the
title receive premium scores.
4. In addition, all significant terms in a passage (i.e.,
other than the common stopwords) are ranked
by a passage-level inverted frequency distribution,
e.g., Nlpf, where pf is the number of passages
containing the term and N is the total number of
passages contained in a document.
5. For generic-type summaries, in case of score ties
</listItem>
<bodyText confidence="0.946800181818182">
the passages closer to the beginning of a text are
preferred to those located towards the end.
The process of passage selection as described here
resembles query-based document retrieval. The
&amp;quot;documents&amp;quot; here are the passages, and the &amp;quot;query&amp;quot;
is a set of words and phrases found in the document&apos;s
title and in the openings of some paragraphs. Note
that the summarizer scores both single- and multi-
paragraph passages, which makes it more indepen-
dent from any particular physical paragraph struc-
ture of a document.
</bodyText>
<subsectionHeader confidence="0.926335">
Supplying the background passage
</subsectionHeader>
<bodyText confidence="0.911383913043478">
The background section supplies information that
makes the summary self-contained. For example, a
passage selected from a document may have signif-
icant links, both explicit and implicit, to the sur-
rounding context, which if severed are likely to ren-
der the passage uncomprehensible, or even mislead-
ing. The following passage illustrates the point:
&amp;quot;Once again this demonstrates the substantial influence
Iran holds over terrorist kidnapers,&amp;quot; Redman said, adding
that it is not yet clear what prompted Iran to take the ac-
tion it did.
Adding a background paragraph makes this a far
more informative summary:
Both the French and Iranian governments acknowledged the
Iranian role in the release of the three French hostages,
Jean-Paul Kauffmann, Marcel Carton and Marcel Fontaine.
&amp;quot;Once again this demonstrates the substantial influence
Iran holds over terrorist kidnapers,&amp;quot; Redman said, adding
that it is not yet clear what prompted Iran to take the ac-
tion it did.
Below are three main criteria we consider to decide
if a background passage is required, and if so, how
to get one.
</bodyText>
<listItem confidence="0.713614">
1. One indication that a background information
</listItem>
<bodyText confidence="0.821313857142857">
may be needed is the presence of outgoing refer-
ences, such as anaphors. If an anaphor is detected
within the first N (=6) items (words, phrases) of
the selected passage, the preceding passage is ap-
pended to the summary. Anaphors and other ref-
erences are identified by the presence of pronouns,
definite noun phrases, and quoted expressions.
2. Initially the passages are formed from single physi-
cal paragraphs, but for some texts the required in-
formation may be spread over multiple paragraphs
so that no clear &amp;quot;winner&amp;quot; can be selected. Sub-
sequently, multi-paragraph passages are scored,
starting with pairs of adjacent paragraphs.
3. If the selected main summary passage is shorter
than L characters, then the passage following it is
added to the to the summary. The value of L de-
pends upon the average length of the documents
being summarized, and it was set as 100 charac-
ters for AP newswire articles. This helps avoiding
choppy summaries from texts with a weak para-
graph structure.
</bodyText>
<subsectionHeader confidence="0.711661">
Implementation and evaluation
</subsectionHeader>
<bodyText confidence="0.999849833333334">
The summarizer has been implemented as a demon-
stration system, primarily for news summarization.
In general we are quite pleased with the system&apos;s
performance. The summarizer is domain indepen-
dent, and can effectively process a range of types
of documents. The summaries are quite informative
with excellent readability. They are also quite short,
generally only 5 to 10% of the original text and can
be read and understood very quickly.
As discussed before, we have included the sum-
marizer as a helper application within the user in-
terface to the natural language information retrieval
system. In this application, the summarizer is used
to derive query-related summaries of documents re-
turned from database search. The summarization
method used here is the same as for generic sum-
maries described thus far, with the following excep-
tions:
</bodyText>
<page confidence="0.980049">
1262
</page>
<listItem confidence="0.82055575">
1. The passage-search &amp;quot;query&amp;quot; is derived from the
user&apos;s document search query rather than from
the document title.
2. The distance of a passage from the beginning
</listItem>
<bodyText confidence="0.9923359">
of the document is not considered towards its
summary-worthiness.
The topical summaries are read by the users to
quickly decide their relevance to the search topic
and, if desired, to expand the initial information
search statement in order to produce a significantly
more effective query. The following example shows
a topical (query-guided summary) and compares it
to the generic summary (we abbreviate SGML for
brevity).
</bodyText>
<table confidence="0.797971454545455">
INITIAL SEARCH STATEMENT:
&lt; title &gt; Evidence of Iranian support for Lebanese hostage
takers.
&lt; desc &gt; Document will give data linking Iran to groups
in Lebanon which seize and hold Western hostages.
FIRST RETRIEVED DOCUMENT (TITLE):
Arab Hijackers&apos; Demands Similar To Those of Hostage-
Takers in Lebanon
SUMMARIZER TOPIC:
Evidence of Iranian support for Lebanese hostage takers
TOPICAL SUMMARY (used for expansion):
</table>
<bodyText confidence="0.8548123">
Mugniyeh, 36, is a key figure in the security apparatus of
Hezbollah, or Party of God, an Iranian-backed Shiite move-
ment believed to be the umbrella for factions holding most
of the 22 foreign hostages in Lebanon.
GENERIC SUMMARY (for comparison):
The demand made by hijackers of a Kuwaiti jet is the same
as that made by Moslems holding Americans hostage in
Lebanon - freedom for 17 pro-Iranian extremists jailed in
Kuwait for bombing U.S. and French embassies there in
1983.
</bodyText>
<sectionHeader confidence="0.686095" genericHeader="method">
PARTIALLY EXPANDED SEARCH STATEMENT:
</sectionHeader>
<bodyText confidence="0.8191355">
&lt; title &gt; Evidence of Iranian support for Lebanese hostage
takers.
&lt; desc &gt; Document will give data linking Iran to groups
in Lebanon which seize and hold Western hostages.
&lt; expd &gt; Mugniyeh, 36, is a key figure in the security
apparatus of Hezbollah, or Party of God, an Iranian-backed
Shiite movement believed to be the umbrella for factions
holding most of the 22 foreign hostages in Lebanon.
</bodyText>
<subsectionHeader confidence="0.631742">
Overview of the NLIR System
</subsectionHeader>
<bodyText confidence="0.999897">
The Natural Language Information Retrieval Sys-
tem (NLIR)6 as been designed as a series of par-
allel text processing and indexing &amp;quot;streams&amp;quot;. Each
stream constitutes an alternative representation of
the database obtained using different combination
of natural language processing steps. The purpose
of NL processing is to obtain a more accurate con-
tent representation than that based on words alone,
which will in turn lead to improved performance.
The following term extraction steps correspond to
some of the streams used in our system:
</bodyText>
<footnote confidence="0.9885185">
6For more details, see (Strzalkowski 1995), (Strza-
lkowski et al. 1997)
</footnote>
<listItem confidence="0.9703655">
1. Elimination of stopwords: Documents are indexed
using original words minus selected &amp;quot;stopwords&amp;quot;
that include all closed-class words (determiners,
prepositions, etc.)
2. Morphological stemming: Words are normalized
across morphological variants using a lexicon-
based stemmer.
3. Phrase extraction: Shallow text processing tech-
</listItem>
<bodyText confidence="0.962804913043478">
niques, including part-of-speech tagging, phrase
boundary detection, and word co-occurrence met-
rics are used to identify relatively stable groups of
words, e.g., joint venture.
4. Phrase normalization: Documents are processed
with a syntactic parser, and &amp;quot;Head+Modifier&amp;quot;
pairs are extracted in order to normalize across
syntactic variants and reduce to a common &amp;quot;con-
cept&amp;quot;, e.g., weapon+proliferate.
5. Proper name extraction: Names of people, loca-
tions, organizations, etc. are identified.
Search queries, after appropriate processing, are
run against each stream, i.e., a phrase query against
the phrase stream, a name query against the name
stream, etc. The results are obtained by merging
ranked lists of documents obtained from searching
all streams. This allows for an easy combination
of alternative retrieval methods, creating a meta-
search strategy which maximizes the contribution of
each stream. Different information retrieval systems
can used as indexing and search engines each stream.
In the experiments described here we used Cornell&apos;s
SMART (version 11) (Buckley, et al. 1995).
</bodyText>
<sectionHeader confidence="0.860993" genericHeader="method">
TREC Evaluation Results
</sectionHeader>
<bodyText confidence="0.996045722222222">
Table 1 lists selected runs performed with the
NLIR system on TREC-6 database using 50 queries
(TREC topics) numbered 301 through 350. The
expanded query runs are contrasted with runs ob-
tained using TREC original topics using NLIR as
well as Cornell&apos;s SMART (version 11) which serves
here as a benchmark. The first two columns are
automatic runs, which means that there was no hu-
man intervention in the process at any time. Since
query expansion requires human decision on sum-
mary selection, these runs (columns 3 and 4) are
classified as &amp;quot;manual&amp;quot;, although most of the process
is automatic. As can be seen, query expansion pro-
duces an impressive improvement in precision at all
levels. Recall figures are shown at 1000 retrieved
documents.
Query expansion appears to produce consistently
high gains not only for different sets of queries but
</bodyText>
<page confidence="0.983694">
1263
</page>
<tableCaption confidence="0.9428135">
Table 1: Performance improvement for expanded
queries
</tableCaption>
<table confidence="0.999374769230769">
queries: original original expanded expanded
SYSTEM SMART NLIR SMART NLIR
PRECISION
Average 0.1429 0.1837 0.2672 0.2859
%change +28.5 +87.0 +100.0
At 10 does 0.3000 0.3840 0.5060 0.5200
%change +28.0 +68.6 +73.3
At 30 does 0.2387 0.2747 0.3887 0.3940
%change +15.0 +62.8 +65.0
At 100 doe 0.1600 0.1736 0.2480 0.2574
%change +8.5 +55.0 +60.8
Recall 0.57 0.53 0.61 0.62
%change -7.0 +7.0 +8.7
</table>
<bodyText confidence="0.998534272727273">
also for different systems: we asked other groups
participating in TREC to run search using our ex-
panded queries, and they reported similarly large
improvements.
Finally, we may note that NLP-based indexing has
also a positive effect on overall performance, but the
improvements are relatively modest, particularly on
the expanded queries. A similar effect of reduced ef-
fectiveness of linguistic indexing has been reported
also in connection with improved term weighting
techniques.
</bodyText>
<sectionHeader confidence="0.758707" genericHeader="conclusions">
Conclusions
</sectionHeader>
<bodyText confidence="0.99977952173913">
We have developed a method to derive quick-read
summaries from news-like texts using a number of
shallow NLP and simple quantitative techniques.
The summary is assembled out of passages extracted
from the original text, based on a pre-determined
DMS template. This approach has produced a very
efficient and robust summarizer for news-like texts.
We used the summarizer, via the QET inter-
face, to build effective search queries for an informa-
tion retrieval system. This has been demonstrated
to produce dramatic performance improvements in
TREC evaluations. We believe that this query ex-
pansion approach will also prove useful in searching
very large databases where obtaining a full index
may be impractical or impossible, and accurate sam-
pling will become critical.
Acknowledgements We thank Chris Buckley for
helping us to understand the inner workings of
SMART, and also for providing SMART system re-
sults used here. This paper is based upon work sup-
ported in part by the Defense Advanced Research
Projects Agency under Tipster Phase-3 Contract 97-
F157200-000.
</bodyText>
<sectionHeader confidence="0.996293" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99987770967742">
Buckley, Chris, Amit Singhal, Mandar Mitra, Gerard Salton.
1995. &amp;quot;New Retrieval Approaches Using SMART: TREC 4&amp;quot;.
Proceedings of TREC-4 Conference, NIST Special Publication
500-236.
DeJong, G.G., 1992. An overview of the FRUMP system, Lehn-
ert, W.G. and M.H. Ringle (eds), Strategies for NLP, Lawrence
Erlbaum, Hillsdale, NJ.
Harman, Donna. 1988. &amp;quot;Towards interactive query expansion.&amp;quot;
Proceedings of ACM SIGIR-88, pp. 321-331.
Harman, Donna, and Ellen Voorhees (eds). 1998. The Text Re-
trieval Conference (TREC-6). NIST Special Publication (to ap-
pear).
Kupiec,J., J. Pedersen and F. Chen, 1995. A trainable document
summarizer, Proceedings of ACM SIGIR-95, pp. 68-73.
Lehnert, W.G., 1981. Plots Units and Narrative summarization,
Cognitive Science, 4, pp 293-331.
Luhn, H.P., 1958. The automatic creation of literature abstracts,
IBM Journal, Apr, pp. 159-165.
McKeown, K.R. and D.R. Radev, 1995. Generating Summaries
of Multiple News Articles, Proceedings of ACM SIGIR-95
Proceedings of 5th Message Understanding Conference, San
Francisco, CA:Morgan Kaufman Publishers. 1993.
Ono, K., K. Sumita and S.Miike, 1994. Abstract Generation
based on Rhetorical Structure Extraction, COL/NG94, vol 1,
pp 344-348, Kyoto, Japan.
Paice, C.D., 1990. Constructing literature abstracts by com-
puter: techniques and prospects, Information Processing and
Management, vol 26 (1), pp 171-186.
Rau, L.F., R. Brandow and K. Mitze, 1994. Domain-
independent summarization of news, Summarizing text for in-
telligent communication, page 71-75, Dagstuhl, Gemany.
Rino, L.H.M. and D. Scott, 1994. Content selection in summary
generation, Third International Conference on the Cognitive
Science of NLP, Dublin City University, Ireland.
Rocchio, J. J. 1971. &amp;quot;Relevance Feedback in Informatio Re-
trieval.&amp;quot; In Salton, G. (Ed.), The SMART Retrieval System,
pp. 313-323. Prentice Hall, Inc., Englewood Cliffs, NJ.
Strzalkowski, Tomek, Jin Wang, and Bowden Wise. 1998. &amp;quot;A
Robust Practical Text Summarization.&amp;quot; Proceedings of AAAI
Spring Symposium on Intelligent Text Summarization (to ap-
pear).
Strzalkowski, Tomek, Fang Lin, Jose Perez-Carballo, and Jin
Wang. 1997. &amp;quot;Natural Language Information Retrieval: TREC-
6 Report.&amp;quot; Proceedings of TREC-6 conference.
Strzalkowski, Tomek, Louise Guthrie, Jussi Karlgren, Jim Leis-
tensnider, Fang Lin, Jose Perez-Carballo, Troy Straszheim, Jin
Wang, and Jon Wilding. 1997. &amp;quot;Natural Language Information
Retrieval: TREC-5 Report.&amp;quot; Proceedings of TREC-5 confer-
ence.
Strzalkowski, Tomek. 1995. &amp;quot;Natural Language Information Re-
trieval&amp;quot; Information Processing and Management, Vol. 31, No.
3, pp. 397-417. Pergamon/Elsevier.
Strzalkowski, Tomek. and Jin Wang, 1996. A Self-Learning Uni-
versal Concept Spotter, Proceedings of COLING-96, pp. 931-
936.
Tipsier Text Phase 2: 24 month Conference, Morgan-
Kaufmann. 1996
Voorhees, Ellen M. 1994. &amp;quot;Query Expansion Using Lexical-
Semantic Relations.&amp;quot; Proceedings of ACM SIGIR&apos;94, pp. 61-70.
Weissberg, R. and S. Buker, 1990. Writing up Research: Ex-
perimental Research Report Writing for Student of English,
Prentice Hall, Inc.
</reference>
<page confidence="0.994077">
1264
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.002373">
<title confidence="0.997452">Summarization-based Query Expansion in Information Retrieval</title>
<author confidence="0.999265">Tomek StrzaIkowski</author>
<author confidence="0.999265">Jin Wang</author>
<author confidence="0.999265">Bowden Wise</author>
<affiliation confidence="0.952395">GE Corporate Research and Development 1 Research Circle</affiliation>
<address confidence="0.994194">Niskayuna, NY 12309</address>
<email confidence="0.999686">strzalkowski@crd.ge.com</email>
<abstract confidence="0.998984447513813">We discuss a semi-interactive approach to information retrieval which consists of two tasks performed in a sequence. First, the system assists the searcher in building a comprehensive statement of information need, using automatically generated topical summaries of sample documents. Second, the detailed statement of information need is automatically processed by a series of natural language processing routines in order to derive an optimal search query for a statistical information retrieval system. In this paper, we investigate the role of automated document summarization in building effective search statements. We also discuss the results of latest evaluation of our system at the annual Text Retrieval Conference (TREC). Information Retrieval retrieval a task of selecting docua database in response to a user&apos;s query, and ranking them according to relevance. This has been usually accomplished using statistical methods (often coupled with manual encoding) that (a) select terms (words, phrases, and other units) from documents that are deemed to best represent their content, and (b) create an inverted index file (or files) that provide an easy access to documents containing these terms. A subsequent search process attempts to match preprocessed user queries against termbased representations of documents in each case determining a degree of relevance between the two which depends upon the number and types of matching terms. A search is successful if it can return as many possible documents which are the query, with as few as possible non-relevant documents. In addition, the relevant documents should be ranked ahead of non-relevant ones. The quantitative text representation methods, predominant in leading information retrieval limit anchored on words, word or charsystem&apos;s ability to generate a search they rely more on the a query than its finding document matches. This is particularly acute in situations where the user has only a limited knowledge of database composition and needs to resort to generic or otherwise incomplete search statements. In order to overcome this limitation, many IR systems allow varying degrees of user interaction that facilitates query optimization and calibration to closer match user&apos;s information seeking goals. A popular technique here is relevance feedback, where the user or the system judges the relevance of a sample of results returned from an initial search, and the query is subsequently rebuilt to reflect this information. Automatic relevance feedback techniques can lead to very close mapping of documents, however, they also tend to overfit, which in turn reduces their ability of finding new documents on the same subject. Therefore, a serious challenge for information retrieval is to devise methods for building better queries, or in assisting user to do so. Building effective search queries We have been experimenting with manual and automatic natural language query (or topic, in TREC parlance) building techniques. This differs from most query modification techniques used in IR in that our method is to reformulate the user&apos;s statement of information need rather than the search system&apos;s internal representation of it, as relevance feedback does. Our goal is to devise a method of fulltext expansion that would allow for creating exhaustive search topics such that: (1) the performance of any system using the expanded topics would be significantly better than when the system is run using the original topics, and (2) the method of topic or some surrogates of these, along with significance weights derived from their distribution in the database. 1258 expansion could eventually be automated or semiautomated so as to be useful to a non-expert user. Note that the first of the above requirements effectively calls for a free text, unstructured, but highly precise and exhaustive description of user&apos;s search statement. The preliminary results from TREC evaluations show that such an approach is indeed very effective. One way to view query expansion is to make the user query resemble more closely the documents it is expected to retrieve. This may include both content, as well as some other aspects such as composition, style, language type, etc. If the query is indeed made to resemble a &amp;quot;typical&amp;quot; relevant document, then suddenly everything about this query becomes a valid search criterion: words, collocations, phrases, various relationships, etc. Unfortunately, an average search query does not look anything like this, most of the time. It is more likely to be a statement specifying the semantic criteria of relevance. This means that except for the semantic or conceptual resemblance (which we cannot model very well as yet) much of the appearance of the query (which we can model reasonably well) may be, and often is, quite misleading for search purposes. Where can we get the right queries? In today&apos;s information retrieval, query expansion usually is typically limited to adding, deleting or re-weighting of terms. For example, content terms from documents judged relevant are added to the query while weights of all terms are adjusted in order to reflect the relevance information. Thus, terms occurring predominantly in relevant documents will have their weights increased, while those occurring mostly in non-relevant documents will have their weights decreased. This process can be performed automatically using a relevance feedback method, e.g., (Rocchio 1971), with the relevance information either supplied manually by the user (Harman 1988), or otherwise guessed, e.g. by assuming top 10 documents relevant, etc. (Buckley, et al. 1995). A serious problem with this term-based expansion is its limited ability to capture and represent many important aspects of what makes some documents relevant to the query, including particular term co-occurrence patterns, and other hard-tomeasure text features, such as discourse structure or stylistics. Additionally, relevance-feedback expansion depends on the inherently partial relevance information, which is normally unavailable, or unreliable. Other types of query expansions, including general purpose thesauri or lexical databases (e.g., Wordnet) have been found generally unsuccessful in information retrieval, (Voorhees 1994). An alternative to term-only expansion is a fulltext expansion described in (Strzalkowski et al. 1997). In this approach, search topics are expanded by pasting in entire sentences, paragraphs, and other sequences directly from any text document. To make this process efficient, an initial search is performed with the unexpanded queries and the top N (10-30) returned documents are used for query expansion. These documents, irrespective of their overall relevancy to the search topic, are scanned for passages containing concepts referred to in the query. The resulting expanded queries undergo further text processing steps, before the search is run again. We need to note that the expansion material was found in both relevant and non-relevant documents, benefiting the final query all the same. In fact, the presence of such text in otherwise nonrelevant documents underscores the inherent limitations of distribution-based term reweighting used in relevance feedback. In this paper, we describe a method of full-text topic expansion where the expansion passages are obtained from an automatic text summarizer. A preliminary examination of TREC-6 results indicate that this mode of expansion is at least as effective as the purely manual expansion which requires the users to read entire documents to select expansion passages. This brings us a step closer to a fully automated expansion: the human-decision factor has been reduced to an accept/reject decision for expanding the search query with a summary. query expansion We used our automatic text summarizer to derive query-specific summaries of documents returned from the first round of retrieval. The summaries were usually 1 or 2 consecutive paragraphs selected from the original document text. The initial purpose was to show to the user, by the way of a quick-read abstract, why a document has been retrieved. If the summary appeared relevant and moreover captured some important aspect of relevant information, then the user had an option to paste it into the query, thus increasing the chances of a more successful subsequent search. Note again that it wasn&apos;t important if the summarized documents were themselves relevant, although they usually were. The query expansion interaction proceeds as follows: 1. The initial natural language statement of information need is submitted to SMART-based NLIR retrieval engine via a Query Expansion Tool (QET) The statement is converted into an in- 1259 ternal search query and run against the TREC database.&apos; 2. NOR returns top N (=30) documents from the database that match the search query. 3. The user determines a topic for the summarizer. By default, it is the title field of the initial search statement (see below). 4. The summarizer is invoked to automatically summarize each of the N documents with respect to the selected topic. 5. The user reviews the summaries (spending ap- 5-15 seconds per summary) and that are to the search statement. 6. All remaining summaries are automatically attached to the search statement. 7. The expanded search statement is passed through a series of natural language processing steps and then submitted for the final retrieval. A partially expanded TREC Topic 304 is shown below. The original topic comprises the first four fields, with the Expanded field added through the query expansion process. The initial query, while somewhat lengthy by IR standards (though not by TREC standards) is still quite generic in form, that is, it supplies few specifics to guide the search. In contrast, the Expanded section supplies not only many concrete examples of relevant concepts (here, names of endangered mammals) but also the language and the style used by others to describe them. &lt; lop &gt; &lt; num &gt; Number: 304 title Species (Mammals) desc Compile a list of mammals that are considered to be endangered, identify their habitat and, if possible, specify what threatens them. &lt; narr &gt; Narrative: Any document identifying a mammal as endangered is relevant. Statements of authorities disputing the endangered status would also be relevant. A document containing information on habitat and populations of a mammal identified elsewhere as endangered would also be relevant even if the document at hand did not identify the species as endangered. Generalized statements about endangered species without reference to specific mammals would not be relevant. &lt; expd &gt; Expanded: database consisted of approx. 2 GBytes of documents from Associated Press newswire, Wall Street Journal, Financial Times, Federal Register, PETS and sources (Harman 1998). The Service is responsible for eight species of marine mammals under the jurisdiction of the Department of the Interior, as assigned by the Marine Mammal Protection Act of 1972. These species are polar bear, sea and marine otters, manatees (three species) and dugong. The reviews the Service&apos;s marine mammal-related activities during the report period. The U.S. Fish and Wildlife Service had classified the primate as a &amp;quot;threatened&amp;quot; species, but officials said that more protection was needed in view of recent studies documenting a drastic decline in the populations of wild chimps in Africa. The Endangered Species Act was passed in 1973 and has been used to provide protection to the bald eagle and grizzly bear, among other animals. Under the law, a designation of a threatened species means it is likely to become extinct without protection, whereas extinction is viewed as a certainty for an endangered species. bear on California&apos;s state flag should remind us of we have done to some of our species. It is a grizzly. And it is extinct in California and in most other states where it once roamed. &lt; /fop &gt; In the next section we describe the summarization process in detail. summarization Perhaps the most difficult problem in designing an automatic text summarization is to define what a summary is, and how to tell a summary from a nonsummary, or a good summary from a bad one. The answer depends in part upon who the summary is intended for, and in part upon what it is meant to achieve, which in large measure precludes any obevaluation. For most of us, a is a of the content of a larger document, an abstract recounting the main points while suppressing most details. One purpose of having a summary to learn facts, and decide what you want to do with the entire story. Therefore, one important evaluation criterion is the tradeoff between the degree of compression afforded by the summary, which may result in a decreased accuracy of information, and the time required to review that information. This interpretations is particularly useful, though it isn&apos;t the only one acceptable, in summarizing news and other report-like documents. It is also well suited for evaluating the usefulness of summarization in context of an information retrieval system, where the user needs to rapidly and efficiently review the documents returned from search for an indication of relevance and, possibly, to see which aspect of relevance is present. Our early inspiration, and a benchmark, have been the Quick Read Summaries, posted daily off the front page of New York Times on-line edition (http://www.nytimes.com). These summaries, produced manually by NYT staff, are assembled out of 1260 passages, sentences, and sometimes sentence fragments taken from the main article with very few, if any, editorial adjustments. The effect is a collection of perfectly coherent tidbits of news: the who, the what, and when, but perhaps not why. This kind of summarization, where appropriate passages are extracted from the original text, is very efficient, and arguably effective, because it doesn&apos;t require generation of any new text, and thus lowers the risk of misinterpretation. It is also relatively easier to automate, because we only need to identify the suitable passages among the other text, a task that can be accomplished via shallow NLP and It has been noted, eg., (Rino &amp; Scott 1994), 1990), that certain types of texts, such as news articles, technical reports, research papers, etc., conform to a set of style and organization constraints, called the Discourse Macro Structure (DMS) which help the author to achieve a desired communication effect. News reports, for example, tend to be built hierarchically out of components which fall roughly into one of the two categories: the what&apos;s-the-news category, and the optional background category. The background, if present, supplies the context necessary to understand the central story, or to make a follow up story self-contained. This organization is often reflected in the summary, as illustrated in the example below from NYT 10/15/97, where the highlighted portion provides the background for the main news: Spies Just Wouldn&apos;t Come In From Cold War, Files Show was a Pentagon hated her Kurt Stand was union leader with an beatslouch. Jim Clark was a lonely (A 200-page affidavit filed last week by] the Federal Bureau of Investigation says the three were out-of-work spies for East Germany. And after that state withered away, it says, they desperately reached out for anyone who might want them as secret agents. In this example, the two passages are nonconsecutive paragraphs in the original text; the string in the square brackets at the opening of the second passage has been omitted in the summary. Here the human summarizer&apos;s actions appear relatively straightforward, and it would not be difficult to propose an algorithmic method to do the same. This may go as follows: 1. Choose a DMS template for the summary; e.g., Background+News. approach is contrasted with a far more difficult method of summarizing text &amp;quot;in your own words.&amp;quot; Computational attempts at such discourse-level and knowledge-level summarization include (Ono, Sumita &amp; Miike 1994), (McKeown &amp; Radev 1995), (DeJong 1982), and (Lehnert 1981). 2. Select appropriate passages from the original text and fill the DMS template. 3. Assemble the summary in the desired order; delete extraneous words. We have used this method to build our automated summarizer. We overcome the shortcomings of sentence-based summarization by working on level The summarizer has been applied to a variety of documents, including Asso-</abstract>
<degree confidence="0.69401075">ciated Press newswires, articles from the New York Times, Wall Street Journal, Financial Times, San Jose Mercury, as well as documents from the Federal Register, and Congressional Record. The program</degree>
<abstract confidence="0.998104666666667">is domain independent, and it can be easily adapted to most European languages. It is also very robust: we used it to derive summaries of thousands of documents returned by an information retrieval system. It can work in two modes: generic and topical. In the generic mode, it captures the main topic of a document; in the topical mode, it takes a user supplied statement of interest and derives a summary related to this topic. The topical summary is usually different than the generic summary of the same document. Deriving automatic summaries Each component of a summary DMS needs to be instantiated by one or more passages extracted from the original text. Initially, all eligible passages (i.e., explicitly delineated paragraphs) within a document are potential candidates for the summary. As we move through text, paragraphs are scored for their summary-worthiness. The final score for each passage, normalized for its length, is a weighted sum of a number of minor scores, using the following 1 = • a minor score calculated using metric the weight reflecting how effective this metric in general; the length of the segment. The following metrics are used to score passages considered for the main news section of the summary DMS. We list here only the criteria which are the to 1958) (Paice 1990) (Rau, Brandow &amp; Mitze 1994) (Kupiec, Pedersen &amp; Chen 1995) for sentence-based summarization approaches. weights trainable in a supervised mode, given a corpus of texts and their summaries, or in an unsupervised mode as described in (Strzalkowski &amp; Wang 1996). For the purpose of the experiments described here, these weights have been set manually. 1261 most relevant for generating summaries in context of an information retrieval system. 1. Words and phrases frequently occurring in a text are likely to be indicative of its content, especially if such words or phrases do not occur often elsewhere in the database. A weighted frequency similar to in automatic text inis applicable. Here, for the inverted document frequency of a term. 2. Title of a text is often strongly related to its content. Therefore, words and phrases from the title repeated in text are considered as important indicators of content concentration within a document. 3. Noun phrases occurring in the opening sentences of multiple paragraphs tend to be indicative of the content. These phrases, along with words from the title receive premium scores. 4. In addition, all significant terms in a passage (i.e., other than the common stopwords) are ranked by a passage-level inverted frequency distribution, the number of passages the term and the total number of passages contained in a document. 5. For generic-type summaries, in case of score ties the passages closer to the beginning of a text are preferred to those located towards the end. The process of passage selection as described here resembles query-based document retrieval. The &amp;quot;documents&amp;quot; here are the passages, and the &amp;quot;query&amp;quot; is a set of words and phrases found in the document&apos;s title and in the openings of some paragraphs. Note that the summarizer scores both singleand multiparagraph passages, which makes it more independent from any particular physical paragraph structure of a document. Supplying the background passage The background section supplies information that makes the summary self-contained. For example, a passage selected from a document may have significant links, both explicit and implicit, to the surrounding context, which if severed are likely to render the passage uncomprehensible, or even misleading. The following passage illustrates the point: &amp;quot;Once again this demonstrates the substantial influence Iran holds over terrorist kidnapers,&amp;quot; Redman said, adding that it is not yet clear what prompted Iran to take the action it did. Adding a background paragraph makes this a far more informative summary: Both the French and Iranian governments acknowledged the Iranian role in the release of the three French hostages, Jean-Paul Kauffmann, Marcel Carton and Marcel Fontaine. &amp;quot;Once again this demonstrates the substantial influence Iran holds over terrorist kidnapers,&amp;quot; Redman said, adding that it is not yet clear what prompted Iran to take the action it did. Below are three main criteria we consider to decide if a background passage is required, and if so, how to get one. 1. One indication that a background information may be needed is the presence of outgoing references, such as anaphors. If an anaphor is detected within the first N (=6) items (words, phrases) of the selected passage, the preceding passage is appended to the summary. Anaphors and other references are identified by the presence of pronouns, definite noun phrases, and quoted expressions. 2. Initially the passages are formed from single physical paragraphs, but for some texts the required information may be spread over multiple paragraphs so that no clear &amp;quot;winner&amp;quot; can be selected. Subsequently, multi-paragraph passages are scored, starting with pairs of adjacent paragraphs. 3. If the selected main summary passage is shorter than L characters, then the passage following it is added to the to the summary. The value of L depends upon the average length of the documents being summarized, and it was set as 100 characters for AP newswire articles. This helps avoiding choppy summaries from texts with a weak paragraph structure. Implementation and evaluation The summarizer has been implemented as a demonstration system, primarily for news summarization. In general we are quite pleased with the system&apos;s performance. The summarizer is domain independent, and can effectively process a range of types of documents. The summaries are quite informative with excellent readability. They are also quite short, generally only 5 to 10% of the original text and can be read and understood very quickly. As discussed before, we have included the summarizer as a helper application within the user interface to the natural language information retrieval system. In this application, the summarizer is used to derive query-related summaries of documents returned from database search. The summarization method used here is the same as for generic summaries described thus far, with the following exceptions: 1262 1. The passage-search &amp;quot;query&amp;quot; is derived from the user&apos;s document search query rather than from the document title. 2. The distance of a passage from the beginning of the document is not considered towards its summary-worthiness. The topical summaries are read by the users to quickly decide their relevance to the search topic and, if desired, to expand the initial information search statement in order to produce a significantly more effective query. The following example shows a topical (query-guided summary) and compares it to the generic summary (we abbreviate SGML for brevity). INITIAL SEARCH STATEMENT: title &gt; of Iranian support for Lebanese hostage takers. desc &gt; will give data linking Iran to groups in Lebanon which seize and hold Western hostages.</abstract>
<title confidence="0.7431594">FIRST RETRIEVED DOCUMENT (TITLE): Arab Hijackers&apos; Demands Similar To Those of Hostage- Takers in Lebanon SUMMARIZER TOPIC: Evidence of Iranian support for Lebanese hostage takers</title>
<abstract confidence="0.954103651515152">TOPICAL SUMMARY (used for expansion): Mugniyeh, 36, is a key figure in the security apparatus of Hezbollah, or Party of God, an Iranian-backed Shiite movement believed to be the umbrella for factions holding most of the 22 foreign hostages in Lebanon. GENERIC SUMMARY (for comparison): The demand made by hijackers of a Kuwaiti jet is the same as that made by Moslems holding Americans hostage in Lebanon freedom for 17 pro-Iranian extremists jailed in Kuwait for bombing U.S. and French embassies there in 1983. PARTIALLY EXPANDED SEARCH STATEMENT: title &gt; of Iranian support for Lebanese hostage takers. desc &gt; will give data linking Iran to groups in Lebanon which seize and hold Western hostages. expd &gt; 36, is a key figure in the security apparatus of Hezbollah, or Party of God, an Iranian-backed Shiite movement believed to be the umbrella for factions holding most of the 22 foreign hostages in Lebanon. Overview of the NLIR System The Natural Language Information Retrieval Sysas been designed as a series of parallel text processing and indexing &amp;quot;streams&amp;quot;. Each stream constitutes an alternative representation of the database obtained using different combination of natural language processing steps. The purpose of NL processing is to obtain a more accurate content representation than that based on words alone, which will in turn lead to improved performance. The following term extraction steps correspond to some of the streams used in our system: more details, see (Strzalkowski 1995), (Strzalkowski et al. 1997) 1. Elimination of stopwords: Documents are indexed using original words minus selected &amp;quot;stopwords&amp;quot; that include all closed-class words (determiners, prepositions, etc.) 2. Morphological stemming: Words are normalized across morphological variants using a lexiconbased stemmer. 3. Phrase extraction: Shallow text processing techniques, including part-of-speech tagging, phrase boundary detection, and word co-occurrence metrics are used to identify relatively stable groups of e.g., venture. 4. Phrase normalization: Documents are processed with a syntactic parser, and &amp;quot;Head+Modifier&amp;quot; pairs are extracted in order to normalize across syntactic variants and reduce to a common &amp;quot;cone.g., 5. Proper name extraction: Names of people, locations, organizations, etc. are identified. Search queries, after appropriate processing, are run against each stream, i.e., a phrase query against the phrase stream, a name query against the name stream, etc. The results are obtained by merging ranked lists of documents obtained from searching all streams. This allows for an easy combination of alternative retrieval methods, creating a metasearch strategy which maximizes the contribution of each stream. Different information retrieval systems can used as indexing and search engines each stream. In the experiments described here we used Cornell&apos;s SMART (version 11) (Buckley, et al. 1995). TREC Evaluation Results Table 1 lists selected runs performed with the NLIR system on TREC-6 database using 50 queries (TREC topics) numbered 301 through 350. The expanded query runs are contrasted with runs obtained using TREC original topics using NLIR as well as Cornell&apos;s SMART (version 11) which serves here as a benchmark. The first two columns are automatic runs, which means that there was no human intervention in the process at any time. Since query expansion requires human decision on summary selection, these runs (columns 3 and 4) are classified as &amp;quot;manual&amp;quot;, although most of the process is automatic. As can be seen, query expansion produces an impressive improvement in precision at all levels. Recall figures are shown at 1000 retrieved documents. Query expansion appears to produce consistently high gains not only for different sets of queries but 1263 Table 1: Performance improvement for expanded queries queries: original original expanded expanded SYSTEM SMART NLIR SMART NLIR PRECISION Average 0.1429 0.1837 0.2672 0.2859 %change +28.5 +87.0 +100.0 At 10 does 0.3000 0.3840 0.5060 0.5200 %change +28.0 +68.6 +73.3 At 30 does 0.2387 0.2747 0.3887 0.3940 %change +15.0 +62.8 +65.0 At 100 doe 0.1600 0.1736 0.2480 0.2574 %change +8.5 +55.0 +60.8 Recall 0.57 0.53 0.61 0.62 %change -7.0 +7.0 +8.7 also for different systems: we asked other groups participating in TREC to run search using our expanded queries, and they reported similarly large improvements. Finally, we may note that NLP-based indexing has also a positive effect on overall performance, but the improvements are relatively modest, particularly on the expanded queries. A similar effect of reduced effectiveness of linguistic indexing has been reported also in connection with improved term weighting techniques. Conclusions We have developed a method to derive quick-read summaries from news-like texts using a number of shallow NLP and simple quantitative techniques. The summary is assembled out of passages extracted from the original text, based on a pre-determined DMS template. This approach has produced a very efficient and robust summarizer for news-like texts. We used the summarizer, via the QET interface, to build effective search queries for an information retrieval system. This has been demonstrated to produce dramatic performance improvements in TREC evaluations. We believe that this query expansion approach will also prove useful in searching very large databases where obtaining a full index may be impractical or impossible, and accurate sampling will become critical. thank Chris Buckley for helping us to understand the inner workings of SMART, and also for providing SMART system results used here. This paper is based upon work sup-</abstract>
<note confidence="0.97152478125">ported in part by the Defense Advanced Research Projects Agency under Tipster Phase-3 Contract 97- F157200-000. References Buckley, Chris, Amit Singhal, Mandar Mitra, Gerard Salton. 1995. &amp;quot;New Retrieval Approaches Using SMART: TREC 4&amp;quot;. Proceedings of TREC-4 Conference, NIST Special Publication 500-236. DeJong, G.G., 1992. An overview of the FRUMP system, Lehn- W.G. and M.H. Ringle (eds), for NLP, Erlbaum, Hillsdale, NJ. Harman, Donna. 1988. &amp;quot;Towards interactive query expansion.&amp;quot; Proceedings of ACM SIGIR-88, pp. 321-331. Harman, Donna, and Ellen Voorhees (eds). 1998. The Text Retrieval Conference (TREC-6). NIST Special Publication (to appear). Kupiec,J., J. Pedersen and F. Chen, 1995. A trainable document of ACM SIGIR-95, 68-73. Lehnert, W.G., 1981. Plots Units and Narrative summarization, Science, 4, 293-331. Luhn, H.P., 1958. The automatic creation of literature abstracts, Journal, pp. 159-165. McKeown, K.R. and D.R. Radev, 1995. Generating Summaries Multiple News Articles, of ACM SIGIR-95 of 5th Message Understanding Conference, Francisco, CA:Morgan Kaufman Publishers. 1993. Ono, K., K. Sumita and S.Miike, 1994. Abstract Generation on Rhetorical Structure Extraction, 1, pp 344-348, Kyoto, Japan. Paice, C.D., 1990. Constructing literature abstracts by comtechniques and prospects, Processing and 26 (1), pp 171-186. Rau, L.F., R. Brandow and K. Mitze, 1994. Domainsummarization of news, text for incommunication, 71-75, Dagstuhl, Gemany. Rino, L.H.M. and D. Scott, 1994. Content selection in summary International Conference on the Cognitive of NLP, City University, Ireland. Rocchio, J. J. 1971. &amp;quot;Relevance Feedback in Informatio Re- In Salton, G. (Ed.), SMART Retrieval System, pp. 313-323. Prentice Hall, Inc., Englewood Cliffs, NJ. Strzalkowski, Tomek, Jin Wang, and Bowden Wise. 1998. &amp;quot;A Robust Practical Text Summarization.&amp;quot; Proceedings of AAAI Spring Symposium on Intelligent Text Summarization (to appear). Strzalkowski, Tomek, Fang Lin, Jose Perez-Carballo, and Jin Wang. 1997. &amp;quot;Natural Language Information Retrieval: TREC- 6 Report.&amp;quot; Proceedings of TREC-6 conference. Strzalkowski, Tomek, Louise Guthrie, Jussi Karlgren, Jim Leistensnider, Fang Lin, Jose Perez-Carballo, Troy Straszheim, Jin Wang, and Jon Wilding. 1997. &amp;quot;Natural Language Information Retrieval: TREC-5 Report.&amp;quot; Proceedings of TREC-5 conference. Strzalkowski, Tomek. 1995. &amp;quot;Natural Language Information Re- Processing and Management, Vol. No. 3, pp. 397-417. Pergamon/Elsevier. Strzalkowski, Tomek. and Jin Wang, 1996. A Self-Learning Uni- Concept Spotter, of COLING-96, 931- 936. Phase 2: 24 month Conference, Morgan- Kaufmann. 1996 Voorhees, Ellen M. 1994. &amp;quot;Query Expansion Using Lexical- Semantic Relations.&amp;quot; Proceedings of ACM SIGIR&apos;94, pp. 61-70. R. and S. Buker, 1990. Writing up Ex-</note>
<affiliation confidence="0.668426">perimental Research Report Writing for Student of English,</affiliation>
<address confidence="0.749352">Prentice Hall, Inc. 1264</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Buckley</author>
<author>Amit Singhal</author>
<author>Mandar Mitra</author>
<author>Gerard Salton</author>
</authors>
<title>New Retrieval Approaches Using SMART: TREC 4&amp;quot;.</title>
<date>1995</date>
<booktitle>Proceedings of TREC-4 Conference, NIST Special Publication</booktitle>
<pages>500--236</pages>
<contexts>
<context position="6040" citStr="Buckley, et al. 1995" startWordPosition="951" endWordPosition="954">mple, content terms from documents judged relevant are added to the query while weights of all terms are adjusted in order to reflect the relevance information. Thus, terms occurring predominantly in relevant documents will have their weights increased, while those occurring mostly in non-relevant documents will have their weights decreased. This process can be performed automatically using a relevance feedback method, e.g., (Rocchio 1971), with the relevance information either supplied manually by the user (Harman 1988), or otherwise guessed, e.g. by assuming top 10 documents relevant, etc. (Buckley, et al. 1995). A serious problem with this term-based expansion is its limited ability to capture and represent many important aspects of what makes some documents relevant to the query, including particular term co-occurrence patterns, and other hard-tomeasure text features, such as discourse structure or stylistics. Additionally, relevance-feedback expansion depends on the inherently partial relevance information, which is normally unavailable, or unreliable. Other types of query expansions, including general purpose thesauri or lexical databases (e.g., Wordnet) have been found generally unsuccessful in </context>
<context position="27843" citStr="Buckley, et al. 1995" startWordPosition="4469" endWordPosition="4472">tc. are identified. Search queries, after appropriate processing, are run against each stream, i.e., a phrase query against the phrase stream, a name query against the name stream, etc. The results are obtained by merging ranked lists of documents obtained from searching all streams. This allows for an easy combination of alternative retrieval methods, creating a metasearch strategy which maximizes the contribution of each stream. Different information retrieval systems can used as indexing and search engines each stream. In the experiments described here we used Cornell&apos;s SMART (version 11) (Buckley, et al. 1995). TREC Evaluation Results Table 1 lists selected runs performed with the NLIR system on TREC-6 database using 50 queries (TREC topics) numbered 301 through 350. The expanded query runs are contrasted with runs obtained using TREC original topics using NLIR as well as Cornell&apos;s SMART (version 11) which serves here as a benchmark. The first two columns are automatic runs, which means that there was no human intervention in the process at any time. Since query expansion requires human decision on summary selection, these runs (columns 3 and 4) are classified as &amp;quot;manual&amp;quot;, although most of the proc</context>
</contexts>
<marker>Buckley, Singhal, Mitra, Salton, 1995</marker>
<rawString>Buckley, Chris, Amit Singhal, Mandar Mitra, Gerard Salton. 1995. &amp;quot;New Retrieval Approaches Using SMART: TREC 4&amp;quot;. Proceedings of TREC-4 Conference, NIST Special Publication 500-236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G G DeJong</author>
</authors>
<title>An overview of the FRUMP</title>
<date>1992</date>
<location>Hillsdale, NJ.</location>
<marker>DeJong, 1992</marker>
<rawString>DeJong, G.G., 1992. An overview of the FRUMP system, Lehnert, W.G. and M.H. Ringle (eds), Strategies for NLP, Lawrence Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donna Harman</author>
</authors>
<title>Towards interactive query expansion.&amp;quot;</title>
<date>1988</date>
<booktitle>Proceedings of ACM SIGIR-88,</booktitle>
<pages>321--331</pages>
<contexts>
<context position="5945" citStr="Harman 1988" startWordPosition="936" endWordPosition="938">ion usually is typically limited to adding, deleting or re-weighting of terms. For example, content terms from documents judged relevant are added to the query while weights of all terms are adjusted in order to reflect the relevance information. Thus, terms occurring predominantly in relevant documents will have their weights increased, while those occurring mostly in non-relevant documents will have their weights decreased. This process can be performed automatically using a relevance feedback method, e.g., (Rocchio 1971), with the relevance information either supplied manually by the user (Harman 1988), or otherwise guessed, e.g. by assuming top 10 documents relevant, etc. (Buckley, et al. 1995). A serious problem with this term-based expansion is its limited ability to capture and represent many important aspects of what makes some documents relevant to the query, including particular term co-occurrence patterns, and other hard-tomeasure text features, such as discourse structure or stylistics. Additionally, relevance-feedback expansion depends on the inherently partial relevance information, which is normally unavailable, or unreliable. Other types of query expansions, including general p</context>
</contexts>
<marker>Harman, 1988</marker>
<rawString>Harman, Donna. 1988. &amp;quot;Towards interactive query expansion.&amp;quot; Proceedings of ACM SIGIR-88, pp. 321-331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donna Harman</author>
<author>Ellen Voorhees</author>
</authors>
<date>1998</date>
<booktitle>The Text Retrieval Conference (TREC-6). NIST Special Publication</booktitle>
<note>(to appear).</note>
<marker>Harman, Voorhees, 1998</marker>
<rawString>Harman, Donna, and Ellen Voorhees (eds). 1998. The Text Retrieval Conference (TREC-6). NIST Special Publication (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
<author>J Pedersen</author>
<author>F Chen</author>
</authors>
<title>A trainable document summarizer,</title>
<date>1995</date>
<booktitle>Proceedings of ACM SIGIR-95,</booktitle>
<pages>68--73</pages>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>Kupiec,J., J. Pedersen and F. Chen, 1995. A trainable document summarizer, Proceedings of ACM SIGIR-95, pp. 68-73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W G Lehnert</author>
</authors>
<title>Plots Units and Narrative summarization,</title>
<date>1981</date>
<journal>Cognitive Science,</journal>
<volume>4</volume>
<pages>293--331</pages>
<contexts>
<context position="16861" citStr="Lehnert 1981" startWordPosition="2710" endWordPosition="2711">ing in the square brackets at the opening of the second passage has been omitted in the summary. Here the human summarizer&apos;s actions appear relatively straightforward, and it would not be difficult to propose an algorithmic method to do the same. This may go as follows: 1. Choose a DMS template for the summary; e.g., Background+News. 3This approach is contrasted with a far more difficult method of summarizing text &amp;quot;in your own words.&amp;quot; Computational attempts at such discourse-level and knowledge-level summarization include (Ono, Sumita &amp; Miike 1994), (McKeown &amp; Radev 1995), (DeJong 1982), and (Lehnert 1981). 2. Select appropriate passages from the original text and fill the DMS template. 3. Assemble the summary in the desired order; delete extraneous words. We have used this method to build our automated summarizer. We overcome the shortcomings of sentence-based summarization by working on paragraph level instead.4 The summarizer has been applied to a variety of documents, including Associated Press newswires, articles from the New York Times, Wall Street Journal, Financial Times, San Jose Mercury, as well as documents from the Federal Register, and Congressional Record. The program is domain in</context>
</contexts>
<marker>Lehnert, 1981</marker>
<rawString>Lehnert, W.G., 1981. Plots Units and Narrative summarization, Cognitive Science, 4, pp 293-331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>The automatic creation of literature abstracts,</title>
<date>1958</date>
<journal>IBM Journal, Apr,</journal>
<pages>159--165</pages>
<contexts>
<context position="18820" citStr="Luhn 1958" startWordPosition="3037" endWordPosition="3038">ntial candidates for the summary. As we move through text, paragraphs are scored for their summary-worthiness. The final score for each passage, normalized for its length, is a weighted sum of a number of minor scores, using the following formula:5 1 score(paragraph) = •Ewh • Sh (1) where Sh is a minor score calculated using metric h; wh is the weight reflecting how effective this metric is in general; 1 is the length of the segment. The following metrics are used to score passages considered for the main news section of the summary DMS. We list here only the criteria which are the &amp;quot;Refer to (Luhn 1958) (Paice 1990) (Rau, Brandow &amp; Mitze 1994) (Kupiec, Pedersen &amp; Chen 1995) for sentence-based summarization approaches. 3The weights wh are trainable in a supervised mode, given a corpus of texts and their summaries, or in an unsupervised mode as described in (Strzalkowski &amp; Wang 1996). For the purpose of the experiments described here, these weights have been set manually. 1261 most relevant for generating summaries in context of an information retrieval system. 1. Words and phrases frequently occurring in a text are likely to be indicative of its content, especially if such words or phrases do</context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>Luhn, H.P., 1958. The automatic creation of literature abstracts, IBM Journal, Apr, pp. 159-165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K R McKeown</author>
<author>D R Radev</author>
</authors>
<title>Generating Summaries of Multiple News Articles,</title>
<date>1995</date>
<booktitle>Proceedings of ACM SIGIR-95 Proceedings of 5th Message Understanding Conference,</booktitle>
<location>San Francisco, CA:Morgan Kaufman Publishers.</location>
<contexts>
<context position="16826" citStr="McKeown &amp; Radev 1995" startWordPosition="2703" endWordPosition="2706">ve paragraphs in the original text; the string in the square brackets at the opening of the second passage has been omitted in the summary. Here the human summarizer&apos;s actions appear relatively straightforward, and it would not be difficult to propose an algorithmic method to do the same. This may go as follows: 1. Choose a DMS template for the summary; e.g., Background+News. 3This approach is contrasted with a far more difficult method of summarizing text &amp;quot;in your own words.&amp;quot; Computational attempts at such discourse-level and knowledge-level summarization include (Ono, Sumita &amp; Miike 1994), (McKeown &amp; Radev 1995), (DeJong 1982), and (Lehnert 1981). 2. Select appropriate passages from the original text and fill the DMS template. 3. Assemble the summary in the desired order; delete extraneous words. We have used this method to build our automated summarizer. We overcome the shortcomings of sentence-based summarization by working on paragraph level instead.4 The summarizer has been applied to a variety of documents, including Associated Press newswires, articles from the New York Times, Wall Street Journal, Financial Times, San Jose Mercury, as well as documents from the Federal Register, and Congression</context>
</contexts>
<marker>McKeown, Radev, 1995</marker>
<rawString>McKeown, K.R. and D.R. Radev, 1995. Generating Summaries of Multiple News Articles, Proceedings of ACM SIGIR-95 Proceedings of 5th Message Understanding Conference, San Francisco, CA:Morgan Kaufman Publishers. 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ono</author>
<author>K</author>
</authors>
<title>Sumita and S.Miike,</title>
<date>1994</date>
<booktitle>Abstract Generation based on Rhetorical Structure Extraction, COL/NG94,</booktitle>
<volume>1</volume>
<pages>344--348</pages>
<location>Kyoto, Japan.</location>
<marker>Ono, K, 1994</marker>
<rawString>Ono, K., K. Sumita and S.Miike, 1994. Abstract Generation based on Rhetorical Structure Extraction, COL/NG94, vol 1, pp 344-348, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Paice</author>
</authors>
<title>Constructing literature abstracts by computer: techniques and prospects,</title>
<date>1990</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>26</volume>
<issue>1</issue>
<pages>171--186</pages>
<contexts>
<context position="18833" citStr="Paice 1990" startWordPosition="3039" endWordPosition="3040">ates for the summary. As we move through text, paragraphs are scored for their summary-worthiness. The final score for each passage, normalized for its length, is a weighted sum of a number of minor scores, using the following formula:5 1 score(paragraph) = •Ewh • Sh (1) where Sh is a minor score calculated using metric h; wh is the weight reflecting how effective this metric is in general; 1 is the length of the segment. The following metrics are used to score passages considered for the main news section of the summary DMS. We list here only the criteria which are the &amp;quot;Refer to (Luhn 1958) (Paice 1990) (Rau, Brandow &amp; Mitze 1994) (Kupiec, Pedersen &amp; Chen 1995) for sentence-based summarization approaches. 3The weights wh are trainable in a supervised mode, given a corpus of texts and their summaries, or in an unsupervised mode as described in (Strzalkowski &amp; Wang 1996). For the purpose of the experiments described here, these weights have been set manually. 1261 most relevant for generating summaries in context of an information retrieval system. 1. Words and phrases frequently occurring in a text are likely to be indicative of its content, especially if such words or phrases do not occur of</context>
</contexts>
<marker>Paice, 1990</marker>
<rawString>Paice, C.D., 1990. Constructing literature abstracts by computer: techniques and prospects, Information Processing and Management, vol 26 (1), pp 171-186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L F Rau</author>
<author>R Brandow</author>
<author>K Mitze</author>
</authors>
<title>Domainindependent summarization of news, Summarizing text for intelligent communication,</title>
<date>1994</date>
<pages>71--75</pages>
<location>Dagstuhl, Gemany.</location>
<marker>Rau, Brandow, Mitze, 1994</marker>
<rawString>Rau, L.F., R. Brandow and K. Mitze, 1994. Domainindependent summarization of news, Summarizing text for intelligent communication, page 71-75, Dagstuhl, Gemany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L H M Rino</author>
<author>D Scott</author>
</authors>
<title>Content selection in summary generation,</title>
<date>1994</date>
<booktitle>Third International Conference on the Cognitive Science of NLP,</booktitle>
<institution>Dublin City University,</institution>
<contexts>
<context position="14878" citStr="Rino &amp; Scott 1994" startWordPosition="2387" endWordPosition="2390">, editorial adjustments. The effect is a collection of perfectly coherent tidbits of news: the who, the what, and when, but perhaps not why. This kind of summarization, where appropriate passages are extracted from the original text, is very efficient, and arguably effective, because it doesn&apos;t require generation of any new text, and thus lowers the risk of misinterpretation. It is also relatively easier to automate, because we only need to identify the suitable passages among the other text, a task that can be accomplished via shallow NLP and statistical techniques.3 It has been noted, eg., (Rino &amp; Scott 1994), (Weissberg &amp; Buker 1990), that certain types of texts, such as news articles, technical reports, research papers, etc., conform to a set of style and organization constraints, called the Discourse Macro Structure (DMS) which help the author to achieve a desired communication effect. News reports, for example, tend to be built hierarchically out of components which fall roughly into one of the two categories: the what&apos;s-the-news category, and the optional background category. The background, if present, supplies the context necessary to understand the central story, or to make a follow up sto</context>
</contexts>
<marker>Rino, Scott, 1994</marker>
<rawString>Rino, L.H.M. and D. Scott, 1994. Content selection in summary generation, Third International Conference on the Cognitive Science of NLP, Dublin City University, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Rocchio</author>
</authors>
<title>Relevance Feedback in Informatio Retrieval.&amp;quot;</title>
<date>1971</date>
<booktitle>In Salton, G. (Ed.), The SMART Retrieval System,</booktitle>
<pages>313--323</pages>
<publisher>Prentice Hall, Inc.,</publisher>
<location>Englewood Cliffs, NJ.</location>
<contexts>
<context position="5862" citStr="Rocchio 1971" startWordPosition="923" endWordPosition="924">. Where can we get the right queries? In today&apos;s information retrieval, query expansion usually is typically limited to adding, deleting or re-weighting of terms. For example, content terms from documents judged relevant are added to the query while weights of all terms are adjusted in order to reflect the relevance information. Thus, terms occurring predominantly in relevant documents will have their weights increased, while those occurring mostly in non-relevant documents will have their weights decreased. This process can be performed automatically using a relevance feedback method, e.g., (Rocchio 1971), with the relevance information either supplied manually by the user (Harman 1988), or otherwise guessed, e.g. by assuming top 10 documents relevant, etc. (Buckley, et al. 1995). A serious problem with this term-based expansion is its limited ability to capture and represent many important aspects of what makes some documents relevant to the query, including particular term co-occurrence patterns, and other hard-tomeasure text features, such as discourse structure or stylistics. Additionally, relevance-feedback expansion depends on the inherently partial relevance information, which is normal</context>
</contexts>
<marker>Rocchio, 1971</marker>
<rawString>Rocchio, J. J. 1971. &amp;quot;Relevance Feedback in Informatio Retrieval.&amp;quot; In Salton, G. (Ed.), The SMART Retrieval System, pp. 313-323. Prentice Hall, Inc., Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomek Strzalkowski</author>
<author>Jin Wang</author>
<author>Bowden Wise</author>
</authors>
<title>A Robust Practical Text Summarization.&amp;quot;</title>
<date>1998</date>
<booktitle>Proceedings of AAAI Spring Symposium on Intelligent Text Summarization</booktitle>
<note>(to appear).</note>
<marker>Strzalkowski, Wang, Wise, 1998</marker>
<rawString>Strzalkowski, Tomek, Jin Wang, and Bowden Wise. 1998. &amp;quot;A Robust Practical Text Summarization.&amp;quot; Proceedings of AAAI Spring Symposium on Intelligent Text Summarization (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomek Strzalkowski</author>
<author>Fang Lin</author>
<author>Jose Perez-Carballo</author>
<author>Jin Wang</author>
</authors>
<title>Natural Language Information Retrieval: TREC6 Report.&amp;quot;</title>
<date>1997</date>
<booktitle>Proceedings of TREC-6 conference.</booktitle>
<contexts>
<context position="6781" citStr="Strzalkowski et al. 1997" startWordPosition="1056" endWordPosition="1059">pects of what makes some documents relevant to the query, including particular term co-occurrence patterns, and other hard-tomeasure text features, such as discourse structure or stylistics. Additionally, relevance-feedback expansion depends on the inherently partial relevance information, which is normally unavailable, or unreliable. Other types of query expansions, including general purpose thesauri or lexical databases (e.g., Wordnet) have been found generally unsuccessful in information retrieval, (Voorhees 1994). An alternative to term-only expansion is a fulltext expansion described in (Strzalkowski et al. 1997). In this approach, search topics are expanded by pasting in entire sentences, paragraphs, and other sequences directly from any text document. To make this process efficient, an initial search is performed with the unexpanded queries and the top N (10-30) returned documents are used for query expansion. These documents, irrespective of their overall relevancy to the search topic, are scanned for passages containing concepts referred to in the query. The resulting expanded queries undergo further text processing steps, before the search is run again. We need to note that the expansion material</context>
<context position="26424" citStr="Strzalkowski et al. 1997" startWordPosition="4267" endWordPosition="4271">of the NLIR System The Natural Language Information Retrieval System (NLIR)6 as been designed as a series of parallel text processing and indexing &amp;quot;streams&amp;quot;. Each stream constitutes an alternative representation of the database obtained using different combination of natural language processing steps. The purpose of NL processing is to obtain a more accurate content representation than that based on words alone, which will in turn lead to improved performance. The following term extraction steps correspond to some of the streams used in our system: 6For more details, see (Strzalkowski 1995), (Strzalkowski et al. 1997) 1. Elimination of stopwords: Documents are indexed using original words minus selected &amp;quot;stopwords&amp;quot; that include all closed-class words (determiners, prepositions, etc.) 2. Morphological stemming: Words are normalized across morphological variants using a lexiconbased stemmer. 3. Phrase extraction: Shallow text processing techniques, including part-of-speech tagging, phrase boundary detection, and word co-occurrence metrics are used to identify relatively stable groups of words, e.g., joint venture. 4. Phrase normalization: Documents are processed with a syntactic parser, and &amp;quot;Head+Modifier&amp;quot; p</context>
</contexts>
<marker>Strzalkowski, Lin, Perez-Carballo, Wang, 1997</marker>
<rawString>Strzalkowski, Tomek, Fang Lin, Jose Perez-Carballo, and Jin Wang. 1997. &amp;quot;Natural Language Information Retrieval: TREC6 Report.&amp;quot; Proceedings of TREC-6 conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomek Strzalkowski</author>
<author>Louise Guthrie</author>
<author>Jussi Karlgren</author>
<author>Jim Leistensnider</author>
<author>Fang Lin</author>
<author>Jose Perez-Carballo</author>
<author>Troy Straszheim</author>
<author>Jin Wang</author>
<author>Jon Wilding</author>
</authors>
<title>Natural Language Information Retrieval: TREC-5 Report.&amp;quot;</title>
<date>1997</date>
<booktitle>Proceedings of TREC-5 conference.</booktitle>
<contexts>
<context position="6781" citStr="Strzalkowski et al. 1997" startWordPosition="1056" endWordPosition="1059">pects of what makes some documents relevant to the query, including particular term co-occurrence patterns, and other hard-tomeasure text features, such as discourse structure or stylistics. Additionally, relevance-feedback expansion depends on the inherently partial relevance information, which is normally unavailable, or unreliable. Other types of query expansions, including general purpose thesauri or lexical databases (e.g., Wordnet) have been found generally unsuccessful in information retrieval, (Voorhees 1994). An alternative to term-only expansion is a fulltext expansion described in (Strzalkowski et al. 1997). In this approach, search topics are expanded by pasting in entire sentences, paragraphs, and other sequences directly from any text document. To make this process efficient, an initial search is performed with the unexpanded queries and the top N (10-30) returned documents are used for query expansion. These documents, irrespective of their overall relevancy to the search topic, are scanned for passages containing concepts referred to in the query. The resulting expanded queries undergo further text processing steps, before the search is run again. We need to note that the expansion material</context>
<context position="26424" citStr="Strzalkowski et al. 1997" startWordPosition="4267" endWordPosition="4271">of the NLIR System The Natural Language Information Retrieval System (NLIR)6 as been designed as a series of parallel text processing and indexing &amp;quot;streams&amp;quot;. Each stream constitutes an alternative representation of the database obtained using different combination of natural language processing steps. The purpose of NL processing is to obtain a more accurate content representation than that based on words alone, which will in turn lead to improved performance. The following term extraction steps correspond to some of the streams used in our system: 6For more details, see (Strzalkowski 1995), (Strzalkowski et al. 1997) 1. Elimination of stopwords: Documents are indexed using original words minus selected &amp;quot;stopwords&amp;quot; that include all closed-class words (determiners, prepositions, etc.) 2. Morphological stemming: Words are normalized across morphological variants using a lexiconbased stemmer. 3. Phrase extraction: Shallow text processing techniques, including part-of-speech tagging, phrase boundary detection, and word co-occurrence metrics are used to identify relatively stable groups of words, e.g., joint venture. 4. Phrase normalization: Documents are processed with a syntactic parser, and &amp;quot;Head+Modifier&amp;quot; p</context>
</contexts>
<marker>Strzalkowski, Guthrie, Karlgren, Leistensnider, Lin, Perez-Carballo, Straszheim, Wang, Wilding, 1997</marker>
<rawString>Strzalkowski, Tomek, Louise Guthrie, Jussi Karlgren, Jim Leistensnider, Fang Lin, Jose Perez-Carballo, Troy Straszheim, Jin Wang, and Jon Wilding. 1997. &amp;quot;Natural Language Information Retrieval: TREC-5 Report.&amp;quot; Proceedings of TREC-5 conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomek Strzalkowski</author>
</authors>
<title>Natural Language Information Retrieval&amp;quot;</title>
<date>1995</date>
<journal>Information Processing and Management,</journal>
<volume>31</volume>
<pages>397--417</pages>
<publisher>Pergamon/Elsevier.</publisher>
<contexts>
<context position="26396" citStr="Strzalkowski 1995" startWordPosition="4265" endWordPosition="4266">in Lebanon. Overview of the NLIR System The Natural Language Information Retrieval System (NLIR)6 as been designed as a series of parallel text processing and indexing &amp;quot;streams&amp;quot;. Each stream constitutes an alternative representation of the database obtained using different combination of natural language processing steps. The purpose of NL processing is to obtain a more accurate content representation than that based on words alone, which will in turn lead to improved performance. The following term extraction steps correspond to some of the streams used in our system: 6For more details, see (Strzalkowski 1995), (Strzalkowski et al. 1997) 1. Elimination of stopwords: Documents are indexed using original words minus selected &amp;quot;stopwords&amp;quot; that include all closed-class words (determiners, prepositions, etc.) 2. Morphological stemming: Words are normalized across morphological variants using a lexiconbased stemmer. 3. Phrase extraction: Shallow text processing techniques, including part-of-speech tagging, phrase boundary detection, and word co-occurrence metrics are used to identify relatively stable groups of words, e.g., joint venture. 4. Phrase normalization: Documents are processed with a syntactic p</context>
</contexts>
<marker>Strzalkowski, 1995</marker>
<rawString>Strzalkowski, Tomek. 1995. &amp;quot;Natural Language Information Retrieval&amp;quot; Information Processing and Management, Vol. 31, No. 3, pp. 397-417. Pergamon/Elsevier.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Wang</author>
</authors>
<title>A Self-Learning Universal Concept Spotter,</title>
<date>1996</date>
<booktitle>Proceedings of COLING-96,</booktitle>
<pages>931--936</pages>
<contexts>
<context position="19104" citStr="Wang 1996" startWordPosition="3083" endWordPosition="3084">where Sh is a minor score calculated using metric h; wh is the weight reflecting how effective this metric is in general; 1 is the length of the segment. The following metrics are used to score passages considered for the main news section of the summary DMS. We list here only the criteria which are the &amp;quot;Refer to (Luhn 1958) (Paice 1990) (Rau, Brandow &amp; Mitze 1994) (Kupiec, Pedersen &amp; Chen 1995) for sentence-based summarization approaches. 3The weights wh are trainable in a supervised mode, given a corpus of texts and their summaries, or in an unsupervised mode as described in (Strzalkowski &amp; Wang 1996). For the purpose of the experiments described here, these weights have been set manually. 1261 most relevant for generating summaries in context of an information retrieval system. 1. Words and phrases frequently occurring in a text are likely to be indicative of its content, especially if such words or phrases do not occur often elsewhere in the database. A weighted frequency score, similar to tridf used in automatic text indexing is applicable. Here, idf stands for the inverted document frequency of a term. 2. Title of a text is often strongly related to its content. Therefore, words and ph</context>
</contexts>
<marker>Wang, 1996</marker>
<rawString>Strzalkowski, Tomek. and Jin Wang, 1996. A Self-Learning Universal Concept Spotter, Proceedings of COLING-96, pp. 931-936.</rawString>
</citation>
<citation valid="true">
<date>1996</date>
<booktitle>Tipsier Text Phase 2: 24 month Conference,</booktitle>
<location>MorganKaufmann.</location>
<marker>1996</marker>
<rawString>Tipsier Text Phase 2: 24 month Conference, MorganKaufmann. 1996</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Query Expansion Using LexicalSemantic Relations.&amp;quot;</title>
<date>1994</date>
<booktitle>Proceedings of ACM SIGIR&apos;94,</booktitle>
<pages>61--70</pages>
<contexts>
<context position="6678" citStr="Voorhees 1994" startWordPosition="1042" endWordPosition="1043"> this term-based expansion is its limited ability to capture and represent many important aspects of what makes some documents relevant to the query, including particular term co-occurrence patterns, and other hard-tomeasure text features, such as discourse structure or stylistics. Additionally, relevance-feedback expansion depends on the inherently partial relevance information, which is normally unavailable, or unreliable. Other types of query expansions, including general purpose thesauri or lexical databases (e.g., Wordnet) have been found generally unsuccessful in information retrieval, (Voorhees 1994). An alternative to term-only expansion is a fulltext expansion described in (Strzalkowski et al. 1997). In this approach, search topics are expanded by pasting in entire sentences, paragraphs, and other sequences directly from any text document. To make this process efficient, an initial search is performed with the unexpanded queries and the top N (10-30) returned documents are used for query expansion. These documents, irrespective of their overall relevancy to the search topic, are scanned for passages containing concepts referred to in the query. The resulting expanded queries undergo fur</context>
</contexts>
<marker>Voorhees, 1994</marker>
<rawString>Voorhees, Ellen M. 1994. &amp;quot;Query Expansion Using LexicalSemantic Relations.&amp;quot; Proceedings of ACM SIGIR&apos;94, pp. 61-70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Weissberg</author>
<author>S Buker</author>
</authors>
<title>Writing up Research: Experimental Research Report Writing for Student of English,</title>
<date>1990</date>
<publisher>Prentice Hall, Inc.</publisher>
<contexts>
<context position="14904" citStr="Weissberg &amp; Buker 1990" startWordPosition="2391" endWordPosition="2394">ts. The effect is a collection of perfectly coherent tidbits of news: the who, the what, and when, but perhaps not why. This kind of summarization, where appropriate passages are extracted from the original text, is very efficient, and arguably effective, because it doesn&apos;t require generation of any new text, and thus lowers the risk of misinterpretation. It is also relatively easier to automate, because we only need to identify the suitable passages among the other text, a task that can be accomplished via shallow NLP and statistical techniques.3 It has been noted, eg., (Rino &amp; Scott 1994), (Weissberg &amp; Buker 1990), that certain types of texts, such as news articles, technical reports, research papers, etc., conform to a set of style and organization constraints, called the Discourse Macro Structure (DMS) which help the author to achieve a desired communication effect. News reports, for example, tend to be built hierarchically out of components which fall roughly into one of the two categories: the what&apos;s-the-news category, and the optional background category. The background, if present, supplies the context necessary to understand the central story, or to make a follow up story self-contained. This or</context>
</contexts>
<marker>Weissberg, Buker, 1990</marker>
<rawString>Weissberg, R. and S. Buker, 1990. Writing up Research: Experimental Research Report Writing for Student of English, Prentice Hall, Inc.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>