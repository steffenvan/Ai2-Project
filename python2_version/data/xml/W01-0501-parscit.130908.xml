<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000168">
<title confidence="0.905634333333333">
In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing
Limitations of Co-Training for Natural Language Learning
from Large Datasets
</title>
<author confidence="0.99799">
David Pierce and Claire Cardie
</author>
<affiliation confidence="0.863882">
Department of Computer Science
Cornell University
Ithaca NY 14853
</affiliation>
<email confidence="0.787239">
{pierce, cardie}Ktcs.cornell.edu
</email>
<sectionHeader confidence="0.972444" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999278">
Co-Training is a weakly supervised learning
paradigm in which the redundancy of the learn-
ing task is captured by training two classifiers
using separate views of the same data. This
enables bootstrapping from a small set of la-
beled training data via a large set of unlabeled
data. This study examines the learning behav-
ior of co-training on natural language process-
ing tasks that typically require large numbers
of training instances to achieve usable perfor-
mance levels. Using base noun phrase brack-
eting as a case study, we find that co-training
reduces by 36% the difference in error between
co-trained classifiers and fully supervised clas-
sifiers trained on a labeled version of all avail-
able data. However, degradation in the quality
of the bootstrapped data arises as an obstacle
to further improvement. To address this, we
propose a moderately supervised variant of co-
training in which a human corrects the mistakes
made during automatic labeling. Our analysis
suggests that corrected co-training and similar
moderately supervised methods may help co-
training scale to large natural language learning
tasks.
</bodyText>
<sectionHeader confidence="0.997719" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999838203703704">
Co-Training (Blum and Mitchell, 1998) is a
weakly supervised paradigm for learning a clas-
sification task from a small set of labeled data
and a large set of unlabeled data, using sepa-
rate, but redundant, views of the data. While
previous research (summarized in Section 2) has
investigated the theoretical basis of co-training,
this study is motivated by practical concerns.
We seek to apply the co-training paradigm to
problems in natural language learning, with
the goal of reducing the amount of human-
annotated data required for developing natural
language processing components. In particular,
many natural language learning tasks contrast
sharply with the classification tasks previously
studied in conjunction with co-training in that
they require hundreds of thousands, rather than
hundreds, of training examples. Consequently,
our focus on natural language learning raises
the question of how co-training scales when a
large number of training examples are required
to achieve usable performance levels.
This case study of co-training for natural lan-
guage learning addresses the scalability ques-
tion using the task of base noun phrase iden-
tification. For this task, co-training reduces by
36% the difference in error between classifiers
trained on 500 labeled examples and classifiers
trained on 211,000 labeled examples. While
this result is satisfying, further investigation re-
veals that deterioration in the quality of the la-
beled data accumulated by co-training hinders
further improvement. We address this prob-
lem with a moderately supervised variant, cor-
rected co-training, that employs a human anno-
tator to correct the errors made during boot-
strapping. Corrected co-training proves to be
quite successful, bridging the remaining gap in
accuracy. Analysis of corrected co-training il-
luminates an interesting tension within weakly
supervised learning, between the need to boot-
strap accurate labeled data, and the need to
cover the desired task. We evaluate one ap-
proach, using corrected co-training, to resolving
this tension; and as another approach, we sug-
gest combining weakly supervised learning with
active learning (Cohn et al., 1994).
The next section of this paper introduces is-
sues and concerns surrounding co-training. Sec-
tions 3 and 4 describe the base noun phrase
bracketing task, and the application of co-
training to the task, respectively. Section 5 con-
tains an evaluation of co-training for base noun
identification.
</bodyText>
<sectionHeader confidence="0.5381005" genericHeader="introduction">
2 Theoretical and Practical
Considerations for Co-Training
</sectionHeader>
<bodyText confidence="0.999960658536585">
The co-training paradigm applies when accu-
rate classification hypotheses for a task can be
learned from either of two sets of features of the
data, each called a view. For example, Blum
and Mitchell (1998) describe a web page classi-
fication task, in which the goal is to determine
whether or not a given web page is a univer-
sity faculty member&apos;s home page. For this task,
they suggest the following two views: (1) the
words contained in the text of the page; for ex-
ample, research interests or publications; (2) the
words contained in links pointing to the page;
for example, my advisor.
The intuition behind Blum and Mitchell&apos;s co-
training algorithm CT&apos; (Figure 1) is that two
views of the data can be used to train two clas-
sifiers that can help each other. Each classifier
is trained using one view of the labeled data.
Then it predicts labels for instances of the unla-
beled data. By selecting its most confident pre-
dictions and adding the corresponding instances
with their predicted labels to the labeled data,
each classifier can add to the other&apos;s available
training data. Continuing the above example,
web pages pointed to by my advisor links can
be used to train the page classifier, while web
pages about research interests and publications
can be used to train the link classifier.
Initial studies of co-training focused on the
applicability of the co-training paradigm, and in
particular, on clarifying the assumptions needed
to ensure the effectiveness of the CT algo-
rithm. Blum and Mitchell (1998) presented a
PAC-style analysis of co-training, introducing
the concept of compatibility between the tar-
get function and the unlabeled data: that is,
the target function should assign the same label
to an instance regardless of which view it sees.
They made two additional important points:
first, that each view of the data should itself be
sufficient for learning the classification task; and
</bodyText>
<footnote confidence="0.9951175">
1-We refer to Blum and Mitchell&apos;s co-training algo-
rithm as CT, to distinguish it from alternative algo-
rithms that exploit the co-training paradigm, i.e. by us-
ing labeled and unlabeled data partitioned into distinct
views. CoBoost, mentioned below, is one such alterna-
tive algorithm.
</footnote>
<bodyText confidence="0.596187142857143">
repeat until done
train classifier h1 on view V1 of L
train classifier h2 on view V2 of L
allow h1 to posit labels for examples in U
allow h2 to posit labels for examples in U
add hi&apos;s most confidently labeled examples to L
add h2&apos;s most confidently labeled examples to L
</bodyText>
<figureCaption confidence="0.987471">
Figure 1: An abstract schema of Blum and
Mitchell&apos;s CT algorithm for co-training using
a small set of labeled data (L), a large set of
unlabeled data (U), and two views of the data
(Vi, V2).
</figureCaption>
<bodyText confidence="0.999757685714286">
second, that the views should be conditionally
independent of each other in order to be use-
ful. They proved that under these assumptions,
a task that is learnable with random classifica-
tion noise is learnable with co-training. In ex-
periments with the CT algorithm, they noticed
that it is important to preserve the distribution
of class labels in the growing body of labeled
data. Finally, they demonstrated the effective-
ness of co-training on a web page classification
task similar to that described above.
Collins and Singer (1999) were concerned that
the CT algorithm does not strongly enforce the
requirement that hypothesis functions should be
compatible with the unlabeled data. They in-
troduced an algorithm, CoBoost, that directly
minimizes mismatch between views of the un-
labeled data, using a combination of ideas from
co-training and AdaBoost (Freund and Shapire,
1997).
Nigam and Ghani (2000) performed the most
thorough empirical investigation of the desider-
atum of conditional independence of views un-
derlying co-training. Their experiments sug-
gested that view independence does indeed af-
fect the performance of co-training; but that
CT, when compared to other algorithms that
use labeled and unlabeled data, such as EM
(Dempster et al., 1977; Nigam et al., 2000), may
still prove effective even when an explicit feature
split is unknown, provided that there is enough
implicit redundancy in the data.
In contrast to previous investigations of the
theoretical basis of co-training, this study is mo-
tivated by practical concerns about the applica-
</bodyText>
<listItem confidence="0.791637">
(a) In [happier news], [South Korea], in establishing [diplomatic ties] with [Poland] [yesterday],
announced [$450 million] in [loans] to [the financially strapped Warsaw government].
(b) Ino [happier, newsi] ,0 [South, Koreai] ,0 ino establishing° [diplomatic, tiesi] with° [Polandi]
[yesterdays] ,0 announced° [$, 450, millioni] ino [loansi] too [the, financially, strapped&apos;
Warsaw, governmenti] .0
</listItem>
<table confidence="0.75231875">
Left Context Focus Word Right Context Label
/ In/IN happier/JJR news/NN / I
/ in/IN establishing/VBG diplomatic/JJ ties/NNS 0
with/IN Poland/NNP yesterday/NN / announced/VBD B
</table>
<figureCaption confidence="0.9831205">
Figure 2: The base NPs in (a) are indicated by square brackets, and in (b) by JOB tags. The
training instances in (c) consist of the focus word with part-of-speech tag, its context words with
part-of-speech tags, and its JOB label. The part-of-speech tags are from the Penn Treebank tag
set. Asterisks indicate missing features at the beginning or end of the sentence.
</figureCaption>
<bodyText confidence="0.999821595238095">
tion of weakly supervised learning to problems
in natural language learning (NLL). Many NLL
tasks contrast in two ways with the web page
classification task studied in previous work on
co-training. First, the web page task factors
naturally into page and link views, while other
NLL tasks may not have such natural views.
Second, many NLL problems require hundreds
of thousands of training examples, while the
web page task can be learned using hundreds
of examples.
Consequently, our focus on natural language
learning introduces new questions about the
scalability of the co-training paradigm. First,
can co-training be applied to learning prob-
lems without natural factorizations into views?
Nigam and Ghani&apos;s study suggests a qualified
affirmative answer to this question, for a text
classification task designed to contain redun-
dant information; however, it is desirable to
continue investigation of the issue for large-scale
NLL tasks. Second, how does co-training scale
when a large number of training examples are
required to achieve usable performance levels?
It is plausible to expect that the CT algorithm
will not scale well, due to mistakes made by
the view classifiers. To elaborate, the view clas-
sifiers may occasionally add incorrectly labeled
instances to the labeled data. If many iterations
of CT are required for learning the task, degra-
dation in the quality of the labeled data may
become a problem, in turn affecting the quality
of subsequent view classifiers. For large-scale
learning tasks, the effectiveness of co-training
may be dulled over time.
Finally, we note that the accuracy of auto-
matically accumulated training data is an im-
portant issue for many bootstrapping learning
methods (e.g. Yarowsky (1995), Riloff and Jones
(1999)), suggesting that the rewards of under-
standing and dealing with this issue may be sig-
nificant.
</bodyText>
<sectionHeader confidence="0.931887" genericHeader="method">
3 Base Noun Phrase Identification
</sectionHeader>
<bodyText confidence="0.999976529411765">
Base noun phrases (base NPs) are tradition-
ally defined as nonrecursive noun phrases, i.e.
NPs that do not contain NPs. (Figure 2a illus-
trates base NPs with a short example.) Base
noun phrase identification is the task of lo-
cating the base NPs in a sentence from the
words of the sentence and their part-of-speech
tags. Base noun phrase identification is a cru-
cial component of systems that employ par-
tial syntactic analysis, including information re-
trieval (e.g. Mitra et al. (1997)) and question
answering (e.g. Cardie et al. (2000)) systems.
Many corpus-based methods have been applied
to the task, including statistical methods (e.g.
Church (1988)), transformation-based learning
(e.g. Ramshaw and Marcus (1998)), rote se-
quence learning (e.g. Cardie and Pierce (1998)),
memory-based sequence learning (e.g. Argamon
et al. (1999)), and memory-based learning (e.g.
Sang and Veenstra (1999)), among others.
Our case study employs a well-known bracket
representation, introduced by Ramshaw and
Marcus, wherein each word of a sentence is
tagged with one of the following tags: I, mean-
ing the word is within a bracket (inside); 0,
meaning the word is not within a bracket (out-
side); or B, meaning the word is within a
bracket, but not the same bracket as the pre-
ceding word, i.e. the word begins a new bracket.
Thus, the bracketing task is transformed into a
word tagging task. Figure 2b repeats the exam-
ple sentence, showing the JOB tag representa-
tion. Training examples for JOB tagging have
the form
</bodyText>
<equation confidence="0.90055">
W—k/t—k, • • • , WOO, • • • , Wk/tk : 1)
</equation>
<bodyText confidence="0.999984733333333">
where wo is the focus word (i.e. the word whose
tag is to be learned) and to is its syntactic cat-
egory (i.e. part-of-speech) tag. Words to the
left and right of the focus word are included for
context. Finally, / is the JOB tag of wo. Fig-
ure 2c illustrates a few instances taken from the
example sentence.
We chose naive bayes classifiers for the study,
first, because they are convenient to use and,
indeed, have been used in previous co-training
studies; and second, because they are particu-
larly well-suited to co-training by virtue of cal-
culating probabilities for each prediction. For
an instance x, the classifier determines the max-
imum a posteriori label as follows.
</bodyText>
<equation confidence="0.974545">
arg max P(11x)
1c {I,O,B}
arg max P(/)P(4)
arg max P(1) H
i= —k
</equation>
<bodyText confidence="0.999900555555556">
In experiments with these naive bayes JOB clas-
sifiers, we found that very little accuracy was
sacrificed when the word information (i.e. wi)
was ignored by the classifier.2 We therefore sub-
stitute the simpler term P(ti 1/) for P(wiltill)
above.
The probabilities P(ti 1/) are estimated from
the training data by determining the fraction
of the instances labeled 1 that have syntactic
</bodyText>
<footnote confidence="0.984617">
2This contrasts with other results, such as Ramshaw
and Marcus&apos; (1998), indicating that word information
is important for base NP identification. We speculate
that the naive bayes classifiers used here are simply not
sophisticated enough to take advantage of word informa-
tion.
</footnote>
<equation confidence="0.86446">
category ti (on word wi), with m-estimation.
=
N(/)+ 45
</equation>
<bodyText confidence="0.9999716">
Here N(x) denotes the frequency of event x in
the training data. This estimate smoothes the
training probability by including virtual (un-
seen) samples for each part-of-speech tag (of
which there are 45).
</bodyText>
<sectionHeader confidence="0.997842" genericHeader="method">
4 Co-Training for JOB Classifiers
</sectionHeader>
<bodyText confidence="0.9375318">
To apply co-training, the base NP classification
task must first be factored into views. For the
JOB instances (vectors of part-of-speech tags in-
dexed from —k to k) a view corresponds to a
subset of the set of indices {—k, , k} . The
most natural views are perhaps {—k, , Of and
{0, , k}, indicating that one classifier looks
at the focus tag and the tags to its left, while
the other looks at the focus tag and the tags
to its right. Note that these views certainly vi-
olate the desideratum of conditional indepen-
dence between view features since both include
the focus tag. Other views, such as left/right
views omitting the focus tag, for example, may
be more theoretically attractive, but we found
that the left/right views including focus proved
most effectual in practice.
The JOB tagging task requires some minor
modifications to the CT algorithm. First, it is
impractical for the co-training classifiers to pre-
dict labels for each instance from the enormous
set of unlabeled data. Instead, a smaller data
pool is maintained, fed with randomly selected
instances from the larger set.3 Second, the JOB
tagging task is a ternary, rather than a binary,
classification. Furthermore, the distribution of
labels in the training data is more unbalanced
than the distribution of positive and negative
examples in the web page task: namely, 53.9%
of examples are labeled I, 44.0% 0, and 2.1%
B. Since it is impractical to add, say, 27 I, 22
0, and 1 B, to the labeled data at each step of
co-training, instead, instances are selected by
first choosing a label 1 at random according to
the label distribution, then adding the instance
3This standard modification was introduced by Blum
and Mitchell (1998) in an effort to cover the underlying
distribution of unlabeled instances; however, Nigam and
Ghani (2000) found it to be unnecessary in their exper-
iments.
</bodyText>
<equation confidence="0.775531">
i&apos;map
N(ti, 1) + 1
repeat until done
</equation>
<bodyText confidence="0.828479375">
train classifier h1 on view V1 of L
train classifier h2 on view V2 of L
transfer randomly selected examples
from U to U&apos; until = u
for he h2}
allow h to posit labels for all examples in U&apos;
repeat g times
select label 1 at random according to DL
</bodyText>
<figure confidence="0.857637">
transfer most confidently labeled
1 example from U&apos; to L
</figure>
<figureCaption confidence="0.607288833333333">
Figure 3: The modified co-training algorithm
maintains a data pool U&apos; of size u, and labels g
instances per iteration selected according to the
distribution of labels DL. As in the original al-
gorithm, L is the labeled data, U the unlabeled
data, and V1, V2 the views.
</figureCaption>
<bodyText confidence="0.9986598">
most confidently labeled 1 to the labeled data.
This procedure preserves the distribution of la-
bels in the labeled data as instances are labeled
and added. The modified CT algorithm is pre-
sented in Figure 3.
</bodyText>
<sectionHeader confidence="0.985303" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.99825556">
We evaluate co-training for JOB classifica-
tion using a standard data set assembled by
Ramshaw and Marcus from sections 15 18
(training data, 211727 instances) and 20 (test
data, 47377 instances) of the Penn Treebank
Wall Street Journal corpus (Marcus et al.,
1993). Training instances consist of part-of-
speech tag and JOB label for a focus word, along
with contexts of two part-of-speech tags to the
left and right of the focus. Our goal accuracy
of 95.17% is the performance of a supervised
JOB classifier trained on the correctly labeled
version of the full training data. (In our experi-
ments the goal classifier uses the left view of the
data, which actually outperforms the combined
left/right view.) For initial labeled data, the
first L instances of the training data are given
their correct labels. We determined the best set-
ting for the parameters of the CT algorithm by
testing multiple values: L (initial amount of la-
beled data) varied from 10 to 5000, then u (pool
size) from 200 to 5000, then g (growth size) from
1 to 50. The best setting, in terms of effective-
ness of co-training in improving the accuracy of
the classifier, was L = 500,u = 1000,g = 5.
These values are used throughout the evalua-
tion unless noted otherwise.
Co-Training. We observe the progress of the
co-training process by determining, at each it-
eration, the accuracy of the co-training classi-
fiers over the test data. We also record the
accuracy of the growing body of labeled data.
These measurements can be plotted to depict
a learning curve, indicating the progress of co-
training as the classifier accuracy changes. Fig-
ure 4 presents two representative curves, one
for the left context classifier and one for the
labeled data. (The right context classifier be-
haves similarly to the left, but its performance
is slightly worse.) As shown, co-training results
in improvement in test accuracy over the ini-
tial classifier after about 160 iterations, reduc-
ing by 36% the difference in error between the
co-training classifier and the goal classifier.
Unfortunately, the improvement in test accu-
racy does not continue as co-training progresses;
rather, performance peaks, then declines some-
what before stabilizing at around 92.5%. We
hypothesize that this decline is due to degra-
dation in the quality of the labeled data. This
hypothesis is supported by Figure 4b, indicating
that labeled data accuracy decreases steadily
before stabilizing at around 94%. Note that the
accuracy of the classifier stabilizes at a point
a bit lower than the stable accuracy of the la-
beled data, as would be expected if labeled data
quality hinders further improvement from co-
training.
Furthermore, co-training for base NP identi-
fication seems to be quite sensitive to the CT
parameter settings. For example, with L = 200
the co-training classifiers appear not to be ac-
curate enough to sustain co-training, while with
L = 1000, they are too accurate, in the sense
that co-training contributes very little accuracy
before the labeled data deteriorates (Figure 5).
In the next sections, we address the problems
of data degradation and parameter sensitivity
for co-training.
Corrected Co-Training. As shown above,
the degradation of the labeled data introduces a
scalability problem for co-training because suc-
cessive view classifiers use successively poorer
quality data for training. A straightforward so-
lution to this problem is to have a human an-
</bodyText>
<figure confidence="0.996229385542169">
Accuracy of Labeled Data
Iterations of Co−Training
0.96
0.955
0.95
0.945
Accuracy
0.94
0.935
0.93
0.925
Iterations of Co−Training
1
0.99
0.98
0.97
0.96
0.95
0.94
0 100 200 300 400 500 600 700 800 900 1000
0.92
0
100 200 300 400 500 600 700 800 900 1000
Accuracy
Accuracy of Left Classifier
Accuracy of Left Classifier (L = 200)
Accuracy of Left Classifier (L = 1000)
0.96
0.95
0.94
0.93
0.92
0.91
0.9
0.89
0.88
0.87
0.86
0 100 200 300 400 500 600 700 800 900 1000
0.92
0.91
0.9
0.89
0.88
0.87
0.86
0 100 200 300 400 500 600 700 800 900 1000
Iterations of Co−Training
Accuracy
Accuracy
0.96
0.95
0.94
0.93
Iterations of Co−Training
Accuracy of Left Classifier
Accuracy of Left Classifier
0.96
0.96
0.955
0.955
0.95
0.95
0.945
0.945
Accuracy
0.94
0.94
Accuracy
0.935
0.935
0.93
0.93
0.925
0.925
0.92
0
100 200 300 400 500 600 700 800 900 1000
100 200 300 400 500 600 700 800 900 1000
0.92
0
Iterations of Co−Training
Iterations of Co−Training
</figure>
<bodyText confidence="0.999921133333333">
ized, as co-training achieves 95.03% accuracy,
just 0.14% away from the goal, after 600 itera-
tions (and reaches 95.12% after 800 iterations).
Additionally, the human annotator reviews 6000
examples and corrects only 358. Thus, by lim-
iting the number of unlabeled examples under
consideration with the hope of forcing broader
task coverage we achieve essentially the goal
accuracy in fewer iterations and with fewer cor-
rections! Surprisingly, the error rate of the view
classifiers per iteration remains essentially un-
changed despite the reduction of the pool of un-
labeled examples to choose from.
We believe the preceding experiment illumi-
nates a fundamental tension in weakly super-
vised learning, between automatically obtain-
ing reliable training data (usually requiring fa-
miliar examples), and adequately covering the
learning task (usually requiring unfamiliar ex-
amples). This tension suggests that combining
weakly supervised learning methods with active
learning methods might be a fruitful endeavor.
On one hand, the goal of weakly supervised
learning is to bootstrap a classifier from small
amounts of labeled data and large amounts of
unlabeled data, often by automatically labeling
some of the unlabeled data. On the other hand,
the goal of active learning is to process (unla-
beled) training examples in the order
in which
they are most useful or informative to the classi-
fier (Cohn et al., 1994). Usefulness is commonly
quantified as the learner&apos;s uncertainty about the
class of an example (Lewis and Catlett, 1994).
This neatly dovetails with the criterion for se-
lecting instances to label in CT. We envision
a learner that would alternate between select-
ing its most certain unlabeled examples to la-
bel and present to the human for acknowledg-
ment, and selecting its most uncertain exam-
ples to present to the human for annotation.
Ideally, efficient automatic bootstrapping would
be complemented by good coverage of the task.
We leave evaluation of this possibility to future
work.
</bodyText>
<sectionHeader confidence="0.998508" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999913689655172">
This case study explored issues involved
with applying co-training to the natural lan-
guage processing task of identifying base noun
phrases, particularly, the scalability of co-
training for large-scale problems. Our exper-
iments indicate that co-training is an effec-
tive method for learning bracketers from small
amounts of labeled data. Naturally, the re-
sulting classifier does not perform as well as a
fully supervised classifier trained on hundreds
of times as much labeled data, but if the dif-
ference in accuracy is less important than the
effort required to produce the labeled training
data, co-training is especially attractive.
Furthermore, our experiments support the
hypothesis that labeled data quality is a crucial
issue for co-training. Our moderately super-
vised variant, corrected co-training, maintains
labeled data quality without unduly increasing
the burden on the human annotator. Corrected
co-training bridges the gap in accuracy between
weak initial classifiers and fully supervised clas-
sifiers.
Finally, as an approach to resolving the ten-
sion in weakly supervised learning between ac-
cumulating accurate training data and covering
the desired task, we suggest combining weakly
supervised methods such as co-training or self-
training with active learning.
</bodyText>
<sectionHeader confidence="0.998586" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999848">
Thanks to three anonymous reviewers for their
comments and suggestions. This work was
supported in part by DARPA TIDES contract
N66001-00-C-8009, and NSF Grants 9454149,
0081334, and 0074896.
</bodyText>
<sectionHeader confidence="0.998956" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998969934065934">
S. Argamon, I. Dagan, and Y. Krymolowski.
1999. A memory-based approach to learning
shallow natural language patterns. Journal of
Experimental and Theoretical Artificial Intel-
ligence, 11(3). Available as cmp-lg/9806011.
A. Blum and T. Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In
Proceedings of the 11th Annual Conference on
Computational Learning Theory (COLT-98).
C. Cardie and D. Pierce. 1998. Error-driven
pruning of treebank grammars for base
noun phrase identification. In Proceedings of
the 36th Annual Meeting of the ACL and
COLING-98, pages 218 224. Available as
cmp-lg/9808015.
C. Cardie, V. Ng, D. Pierce, and C. Buck-
ley. 2000. Examining the role of statisti-
cal and linguistic knowledge sources in a
general-knowledge question answering sys-
tem. In Proceedings of the Sixth Ap-
plied Natural Language Processing Confer-
ence (ANLP-NAACL 2000), pages 180 187.
K. Church. 1988. A stochastic parts programs
and noun phrase parser for unrestricted text.
In Proceedings of the Second Conference on
Applied Natural Language Processing, pages
136 143.
D. Cohn, L. Atlas, and R. Ladner. 1994. Im-
proving generalization with active learning.
Machine Learning, 15(2):201 221.
M. Collins and Y. Singer. 1999. Unsuper-
vised models for named entity classification.
In Proceedings of the 1999 Joint SIGDAT
Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora
(EMNLP/VLC-99).
A. Dempster, N. Laird, and D. Rubin. 1977.
Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal
Statistical Society, Series B, 39(1):1 38.
Y. Freund and R. Shapire. 1997. A decision-
theoretic generalization of on-line learning
and an application to boosting. Journal of
Computer and System Sciences, 55(1):119
139.
D. Lewis and J. Catlett. 1994. Heterogeneous
uncertainty sampling for supervised learning.
In Proceedings of the Eleventh International
Conference on Machine Learning, pages 148
156.
M. Marcus, M. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of
English: The Penn Treebank. Computational
Linguistics, 19(2):313 330.
M. Mitra, C. Buckley, A. Singhal, and
C. Cardie. 1997. An analysis of statistical
and syntactic phrases. In 5TH RIAO Confer-
ence, Computer-Assisted Information Search-
ing On the Internet, pages 200 214.
I. Muslea, S. Minton, and C. Knoblock. 2000.
Selective sampling with redundant views. In
Proceedings of the Seventeenth National Con-
ference on Artificial Intelligence, pages 621
626.
K. Nigam and R. Ghani. 2000. Analyzing the
effectiveness and applicability of co-training.
In Ninth International Conference on Infor-
mation and Knowledge Management (CIKM-
2000).
K. Nigam, A. McCallum, S. Thrun, and
T. Mitchell. 2000. Text classification from
labeled and unlabeled documents using EM.
Machine Learning, 39(2/3):103 134.
L. Ramshaw and M. Marcus. 1998. Text
chunking using transformation-based learn-
ing. In Natural Language Processing Using
Very Large Corpora. Kluwer. Originally ap-
peared in WVLC95.
E. Riloff and R. Jones. 1999. Learning dictio-
naries for information extraction by multi-
level bootstrapping. In Proceedings of the
Sixteenth National Conference on Artificial
Intelligence, pages 474 479.
E. Tjong Kim Sang and J. Veenstra. 1999.
Representing text chunks. In Proceedings of
EACL&apos;99. Available as cs.CL/9907006.
D. Yarowsky. 1995. Unsupervised word sense
disambiguation rivaling supervised methods.
In Proceedings of the 33rd Annual Meeting of
the Association for Computational Linguis-
tics, pages 189 196.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.448268">
<title confidence="0.998709666666667">of the on Empirical Methods in Natural Language Processing Limitations of Co-Training for Natural Language from Large Datasets</title>
<author confidence="0.999054">David Pierce</author>
<author confidence="0.999054">Claire</author>
<affiliation confidence="0.9679845">Department of Computer Cornell</affiliation>
<address confidence="0.515239">Ithaca NY</address>
<email confidence="0.997313">pierceKtcs.cornell.edu</email>
<email confidence="0.997313">cardieKtcs.cornell.edu</email>
<abstract confidence="0.998793807692308">Co-Training is a weakly supervised learning paradigm in which the redundancy of the learning task is captured by training two classifiers using separate views of the same data. This enables bootstrapping from a small set of labeled training data via a large set of unlabeled data. This study examines the learning behavior of co-training on natural language processing tasks that typically require large numbers of training instances to achieve usable performance levels. Using base noun phrase bracketing as a case study, we find that co-training reduces by 36% the difference in error between classifiers and supervised clastrained on a labeled version all available data. However, degradation in the quality of the bootstrapped data arises as an obstacle to further improvement. To address this, we propose a moderately supervised variant of cotraining in which a human corrects the mistakes made during automatic labeling. Our analysis suggests that corrected co-training and similar moderately supervised methods may help cotraining scale to large natural language learning tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Argamon</author>
<author>I Dagan</author>
<author>Y Krymolowski</author>
</authors>
<title>A memory-based approach to learning shallow natural language patterns.</title>
<date>1999</date>
<journal>Journal of Experimental and Theoretical Artificial Intelligence,</journal>
<volume>11</volume>
<issue>3</issue>
<note>Available as cmp-lg/9806011.</note>
<contexts>
<context position="11812" citStr="Argamon et al. (1999)" startWordPosition="1858" endWordPosition="1861">he task of locating the base NPs in a sentence from the words of the sentence and their part-of-speech tags. Base noun phrase identification is a crucial component of systems that employ partial syntactic analysis, including information retrieval (e.g. Mitra et al. (1997)) and question answering (e.g. Cardie et al. (2000)) systems. Many corpus-based methods have been applied to the task, including statistical methods (e.g. Church (1988)), transformation-based learning (e.g. Ramshaw and Marcus (1998)), rote sequence learning (e.g. Cardie and Pierce (1998)), memory-based sequence learning (e.g. Argamon et al. (1999)), and memory-based learning (e.g. Sang and Veenstra (1999)), among others. Our case study employs a well-known bracket representation, introduced by Ramshaw and Marcus, wherein each word of a sentence is tagged with one of the following tags: I, meaning the word is within a bracket (inside); 0, meaning the word is not within a bracket (outside); or B, meaning the word is within a bracket, but not the same bracket as the preceding word, i.e. the word begins a new bracket. Thus, the bracketing task is transformed into a word tagging task. Figure 2b repeats the example sentence, showing the JOB </context>
</contexts>
<marker>Argamon, Dagan, Krymolowski, 1999</marker>
<rawString>S. Argamon, I. Dagan, and Y. Krymolowski. 1999. A memory-based approach to learning shallow natural language patterns. Journal of Experimental and Theoretical Artificial Intelligence, 11(3). Available as cmp-lg/9806011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the 11th Annual Conference on Computational Learning Theory (COLT-98).</booktitle>
<contexts>
<context position="1472" citStr="Blum and Mitchell, 1998" startWordPosition="220" endWordPosition="223">ining reduces by 36% the difference in error between co-trained classifiers and fully supervised classifiers trained on a labeled version of all available data. However, degradation in the quality of the bootstrapped data arises as an obstacle to further improvement. To address this, we propose a moderately supervised variant of cotraining in which a human corrects the mistakes made during automatic labeling. Our analysis suggests that corrected co-training and similar moderately supervised methods may help cotraining scale to large natural language learning tasks. 1 Introduction Co-Training (Blum and Mitchell, 1998) is a weakly supervised paradigm for learning a classification task from a small set of labeled data and a large set of unlabeled data, using separate, but redundant, views of the data. While previous research (summarized in Section 2) has investigated the theoretical basis of co-training, this study is motivated by practical concerns. We seek to apply the co-training paradigm to problems in natural language learning, with the goal of reducing the amount of humanannotated data required for developing natural language processing components. In particular, many natural language learning tasks co</context>
<context position="4144" citStr="Blum and Mitchell (1998)" startWordPosition="629" endWordPosition="632"> weakly supervised learning with active learning (Cohn et al., 1994). The next section of this paper introduces issues and concerns surrounding co-training. Sections 3 and 4 describe the base noun phrase bracketing task, and the application of cotraining to the task, respectively. Section 5 contains an evaluation of co-training for base noun identification. 2 Theoretical and Practical Considerations for Co-Training The co-training paradigm applies when accurate classification hypotheses for a task can be learned from either of two sets of features of the data, each called a view. For example, Blum and Mitchell (1998) describe a web page classification task, in which the goal is to determine whether or not a given web page is a university faculty member&apos;s home page. For this task, they suggest the following two views: (1) the words contained in the text of the page; for example, research interests or publications; (2) the words contained in links pointing to the page; for example, my advisor. The intuition behind Blum and Mitchell&apos;s cotraining algorithm CT&apos; (Figure 1) is that two views of the data can be used to train two classifiers that can help each other. Each classifier is trained using one view of th</context>
<context position="5448" citStr="Blum and Mitchell (1998)" startWordPosition="851" endWordPosition="854">selecting its most confident predictions and adding the corresponding instances with their predicted labels to the labeled data, each classifier can add to the other&apos;s available training data. Continuing the above example, web pages pointed to by my advisor links can be used to train the page classifier, while web pages about research interests and publications can be used to train the link classifier. Initial studies of co-training focused on the applicability of the co-training paradigm, and in particular, on clarifying the assumptions needed to ensure the effectiveness of the CT algorithm. Blum and Mitchell (1998) presented a PAC-style analysis of co-training, introducing the concept of compatibility between the target function and the unlabeled data: that is, the target function should assign the same label to an instance regardless of which view it sees. They made two additional important points: first, that each view of the data should itself be sufficient for learning the classification task; and 1-We refer to Blum and Mitchell&apos;s co-training algorithm as CT, to distinguish it from alternative algorithms that exploit the co-training paradigm, i.e. by using labeled and unlabeled data partitioned into</context>
<context position="15915" citStr="Blum and Mitchell (1998)" startWordPosition="2562" endWordPosition="2565"> larger set.3 Second, the JOB tagging task is a ternary, rather than a binary, classification. Furthermore, the distribution of labels in the training data is more unbalanced than the distribution of positive and negative examples in the web page task: namely, 53.9% of examples are labeled I, 44.0% 0, and 2.1% B. Since it is impractical to add, say, 27 I, 22 0, and 1 B, to the labeled data at each step of co-training, instead, instances are selected by first choosing a label 1 at random according to the label distribution, then adding the instance 3This standard modification was introduced by Blum and Mitchell (1998) in an effort to cover the underlying distribution of unlabeled instances; however, Nigam and Ghani (2000) found it to be unnecessary in their experiments. i&apos;map N(ti, 1) + 1 repeat until done train classifier h1 on view V1 of L train classifier h2 on view V2 of L transfer randomly selected examples from U to U&apos; until = u for he h2} allow h to posit labels for all examples in U&apos; repeat g times select label 1 at random according to DL transfer most confidently labeled 1 example from U&apos; to L Figure 3: The modified co-training algorithm maintains a data pool U&apos; of size u, and labels g instances p</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>A. Blum and T. Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the 11th Annual Conference on Computational Learning Theory (COLT-98).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cardie</author>
<author>D Pierce</author>
</authors>
<title>Error-driven pruning of treebank grammars for base noun phrase identification.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the ACL and COLING-98,</booktitle>
<pages>218--224</pages>
<note>Available as cmp-lg/9808015.</note>
<contexts>
<context position="11751" citStr="Cardie and Pierce (1998)" startWordPosition="1850" endWordPosition="1853"> NPs with a short example.) Base noun phrase identification is the task of locating the base NPs in a sentence from the words of the sentence and their part-of-speech tags. Base noun phrase identification is a crucial component of systems that employ partial syntactic analysis, including information retrieval (e.g. Mitra et al. (1997)) and question answering (e.g. Cardie et al. (2000)) systems. Many corpus-based methods have been applied to the task, including statistical methods (e.g. Church (1988)), transformation-based learning (e.g. Ramshaw and Marcus (1998)), rote sequence learning (e.g. Cardie and Pierce (1998)), memory-based sequence learning (e.g. Argamon et al. (1999)), and memory-based learning (e.g. Sang and Veenstra (1999)), among others. Our case study employs a well-known bracket representation, introduced by Ramshaw and Marcus, wherein each word of a sentence is tagged with one of the following tags: I, meaning the word is within a bracket (inside); 0, meaning the word is not within a bracket (outside); or B, meaning the word is within a bracket, but not the same bracket as the preceding word, i.e. the word begins a new bracket. Thus, the bracketing task is transformed into a word tagging t</context>
</contexts>
<marker>Cardie, Pierce, 1998</marker>
<rawString>C. Cardie and D. Pierce. 1998. Error-driven pruning of treebank grammars for base noun phrase identification. In Proceedings of the 36th Annual Meeting of the ACL and COLING-98, pages 218 224. Available as cmp-lg/9808015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cardie</author>
<author>V Ng</author>
<author>D Pierce</author>
<author>C Buckley</author>
</authors>
<title>Examining the role of statistical and linguistic knowledge sources in a general-knowledge question answering system.</title>
<date>2000</date>
<booktitle>In Proceedings of the Sixth Applied Natural Language Processing Conference (ANLP-NAACL</booktitle>
<pages>180--187</pages>
<contexts>
<context position="11514" citStr="Cardie et al. (2000)" startWordPosition="1818" endWordPosition="1821">ding and dealing with this issue may be significant. 3 Base Noun Phrase Identification Base noun phrases (base NPs) are traditionally defined as nonrecursive noun phrases, i.e. NPs that do not contain NPs. (Figure 2a illustrates base NPs with a short example.) Base noun phrase identification is the task of locating the base NPs in a sentence from the words of the sentence and their part-of-speech tags. Base noun phrase identification is a crucial component of systems that employ partial syntactic analysis, including information retrieval (e.g. Mitra et al. (1997)) and question answering (e.g. Cardie et al. (2000)) systems. Many corpus-based methods have been applied to the task, including statistical methods (e.g. Church (1988)), transformation-based learning (e.g. Ramshaw and Marcus (1998)), rote sequence learning (e.g. Cardie and Pierce (1998)), memory-based sequence learning (e.g. Argamon et al. (1999)), and memory-based learning (e.g. Sang and Veenstra (1999)), among others. Our case study employs a well-known bracket representation, introduced by Ramshaw and Marcus, wherein each word of a sentence is tagged with one of the following tags: I, meaning the word is within a bracket (inside); 0, meani</context>
</contexts>
<marker>Cardie, Ng, Pierce, Buckley, 2000</marker>
<rawString>C. Cardie, V. Ng, D. Pierce, and C. Buckley. 2000. Examining the role of statistical and linguistic knowledge sources in a general-knowledge question answering system. In Proceedings of the Sixth Applied Natural Language Processing Conference (ANLP-NAACL 2000), pages 180 187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
</authors>
<title>A stochastic parts programs and noun phrase parser for unrestricted text.</title>
<date>1988</date>
<booktitle>In Proceedings of the Second Conference on Applied Natural Language Processing,</booktitle>
<pages>136--143</pages>
<contexts>
<context position="11631" citStr="Church (1988)" startWordPosition="1836" endWordPosition="1837">itionally defined as nonrecursive noun phrases, i.e. NPs that do not contain NPs. (Figure 2a illustrates base NPs with a short example.) Base noun phrase identification is the task of locating the base NPs in a sentence from the words of the sentence and their part-of-speech tags. Base noun phrase identification is a crucial component of systems that employ partial syntactic analysis, including information retrieval (e.g. Mitra et al. (1997)) and question answering (e.g. Cardie et al. (2000)) systems. Many corpus-based methods have been applied to the task, including statistical methods (e.g. Church (1988)), transformation-based learning (e.g. Ramshaw and Marcus (1998)), rote sequence learning (e.g. Cardie and Pierce (1998)), memory-based sequence learning (e.g. Argamon et al. (1999)), and memory-based learning (e.g. Sang and Veenstra (1999)), among others. Our case study employs a well-known bracket representation, introduced by Ramshaw and Marcus, wherein each word of a sentence is tagged with one of the following tags: I, meaning the word is within a bracket (inside); 0, meaning the word is not within a bracket (outside); or B, meaning the word is within a bracket, but not the same bracket a</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>K. Church. 1988. A stochastic parts programs and noun phrase parser for unrestricted text. In Proceedings of the Second Conference on Applied Natural Language Processing, pages 136 143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cohn</author>
<author>L Atlas</author>
<author>R Ladner</author>
</authors>
<title>Improving generalization with active learning.</title>
<date>1994</date>
<booktitle>Machine Learning,</booktitle>
<volume>15</volume>
<issue>2</issue>
<pages>221</pages>
<contexts>
<context position="3588" citStr="Cohn et al., 1994" startWordPosition="540" endWordPosition="543">oderately supervised variant, corrected co-training, that employs a human annotator to correct the errors made during bootstrapping. Corrected co-training proves to be quite successful, bridging the remaining gap in accuracy. Analysis of corrected co-training illuminates an interesting tension within weakly supervised learning, between the need to bootstrap accurate labeled data, and the need to cover the desired task. We evaluate one approach, using corrected co-training, to resolving this tension; and as another approach, we suggest combining weakly supervised learning with active learning (Cohn et al., 1994). The next section of this paper introduces issues and concerns surrounding co-training. Sections 3 and 4 describe the base noun phrase bracketing task, and the application of cotraining to the task, respectively. Section 5 contains an evaluation of co-training for base noun identification. 2 Theoretical and Practical Considerations for Co-Training The co-training paradigm applies when accurate classification hypotheses for a task can be learned from either of two sets of features of the data, each called a view. For example, Blum and Mitchell (1998) describe a web page classification task, in</context>
<context position="22654" citStr="Cohn et al., 1994" startWordPosition="3697" endWordPosition="3700">es), and adequately covering the learning task (usually requiring unfamiliar examples). This tension suggests that combining weakly supervised learning methods with active learning methods might be a fruitful endeavor. On one hand, the goal of weakly supervised learning is to bootstrap a classifier from small amounts of labeled data and large amounts of unlabeled data, often by automatically labeling some of the unlabeled data. On the other hand, the goal of active learning is to process (unlabeled) training examples in the order in which they are most useful or informative to the classifier (Cohn et al., 1994). Usefulness is commonly quantified as the learner&apos;s uncertainty about the class of an example (Lewis and Catlett, 1994). This neatly dovetails with the criterion for selecting instances to label in CT. We envision a learner that would alternate between selecting its most certain unlabeled examples to label and present to the human for acknowledgment, and selecting its most uncertain examples to present to the human for annotation. Ideally, efficient automatic bootstrapping would be complemented by good coverage of the task. We leave evaluation of this possibility to future work. 6 Conclusions</context>
</contexts>
<marker>Cohn, Atlas, Ladner, 1994</marker>
<rawString>D. Cohn, L. Atlas, and R. Ladner. 1994. Improving generalization with active learning. Machine Learning, 15(2):201 221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>Y Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-99).</booktitle>
<contexts>
<context position="7122" citStr="Collins and Singer (1999)" startWordPosition="1133" endWordPosition="1136">of labeled data (L), a large set of unlabeled data (U), and two views of the data (Vi, V2). second, that the views should be conditionally independent of each other in order to be useful. They proved that under these assumptions, a task that is learnable with random classification noise is learnable with co-training. In experiments with the CT algorithm, they noticed that it is important to preserve the distribution of class labels in the growing body of labeled data. Finally, they demonstrated the effectiveness of co-training on a web page classification task similar to that described above. Collins and Singer (1999) were concerned that the CT algorithm does not strongly enforce the requirement that hypothesis functions should be compatible with the unlabeled data. They introduced an algorithm, CoBoost, that directly minimizes mismatch between views of the unlabeled data, using a combination of ideas from co-training and AdaBoost (Freund and Shapire, 1997). Nigam and Ghani (2000) performed the most thorough empirical investigation of the desideratum of conditional independence of views underlying co-training. Their experiments suggested that view independence does indeed affect the performance of co-train</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>M. Collins and Y. Singer. 1999. Unsupervised models for named entity classification. In Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-99).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dempster</author>
<author>N Laird</author>
<author>D Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<volume>39</volume>
<issue>1</issue>
<pages>38</pages>
<contexts>
<context position="7844" citStr="Dempster et al., 1977" startWordPosition="1243" endWordPosition="1246">ions should be compatible with the unlabeled data. They introduced an algorithm, CoBoost, that directly minimizes mismatch between views of the unlabeled data, using a combination of ideas from co-training and AdaBoost (Freund and Shapire, 1997). Nigam and Ghani (2000) performed the most thorough empirical investigation of the desideratum of conditional independence of views underlying co-training. Their experiments suggested that view independence does indeed affect the performance of co-training; but that CT, when compared to other algorithms that use labeled and unlabeled data, such as EM (Dempster et al., 1977; Nigam et al., 2000), may still prove effective even when an explicit feature split is unknown, provided that there is enough implicit redundancy in the data. In contrast to previous investigations of the theoretical basis of co-training, this study is motivated by practical concerns about the applica(a) In [happier news], [South Korea], in establishing [diplomatic ties] with [Poland] [yesterday], announced [$450 million] in [loans] to [the financially strapped Warsaw government]. (b) Ino [happier, newsi] ,0 [South, Koreai] ,0 ino establishing° [diplomatic, tiesi] with° [Polandi] [yesterdays]</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. Dempster, N. Laird, and D. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1 38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R Shapire</author>
</authors>
<title>A decisiontheoretic generalization of on-line learning and an application to boosting.</title>
<date>1997</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>55</volume>
<issue>1</issue>
<pages>139</pages>
<contexts>
<context position="7468" citStr="Freund and Shapire, 1997" startWordPosition="1185" endWordPosition="1188">algorithm, they noticed that it is important to preserve the distribution of class labels in the growing body of labeled data. Finally, they demonstrated the effectiveness of co-training on a web page classification task similar to that described above. Collins and Singer (1999) were concerned that the CT algorithm does not strongly enforce the requirement that hypothesis functions should be compatible with the unlabeled data. They introduced an algorithm, CoBoost, that directly minimizes mismatch between views of the unlabeled data, using a combination of ideas from co-training and AdaBoost (Freund and Shapire, 1997). Nigam and Ghani (2000) performed the most thorough empirical investigation of the desideratum of conditional independence of views underlying co-training. Their experiments suggested that view independence does indeed affect the performance of co-training; but that CT, when compared to other algorithms that use labeled and unlabeled data, such as EM (Dempster et al., 1977; Nigam et al., 2000), may still prove effective even when an explicit feature split is unknown, provided that there is enough implicit redundancy in the data. In contrast to previous investigations of the theoretical basis </context>
</contexts>
<marker>Freund, Shapire, 1997</marker>
<rawString>Y. Freund and R. Shapire. 1997. A decisiontheoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119 139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lewis</author>
<author>J Catlett</author>
</authors>
<title>Heterogeneous uncertainty sampling for supervised learning.</title>
<date>1994</date>
<booktitle>In Proceedings of the Eleventh International Conference on Machine Learning,</booktitle>
<pages>148--156</pages>
<contexts>
<context position="22774" citStr="Lewis and Catlett, 1994" startWordPosition="3715" endWordPosition="3718"> combining weakly supervised learning methods with active learning methods might be a fruitful endeavor. On one hand, the goal of weakly supervised learning is to bootstrap a classifier from small amounts of labeled data and large amounts of unlabeled data, often by automatically labeling some of the unlabeled data. On the other hand, the goal of active learning is to process (unlabeled) training examples in the order in which they are most useful or informative to the classifier (Cohn et al., 1994). Usefulness is commonly quantified as the learner&apos;s uncertainty about the class of an example (Lewis and Catlett, 1994). This neatly dovetails with the criterion for selecting instances to label in CT. We envision a learner that would alternate between selecting its most certain unlabeled examples to label and present to the human for acknowledgment, and selecting its most uncertain examples to present to the human for annotation. Ideally, efficient automatic bootstrapping would be complemented by good coverage of the task. We leave evaluation of this possibility to future work. 6 Conclusions This case study explored issues involved with applying co-training to the natural language processing task of identifyi</context>
</contexts>
<marker>Lewis, Catlett, 1994</marker>
<rawString>D. Lewis and J. Catlett. 1994. Heterogeneous uncertainty sampling for supervised learning. In Proceedings of the Eleventh International Conference on Machine Learning, pages 148 156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>M Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>330</pages>
<contexts>
<context position="17163" citStr="Marcus et al., 1993" startWordPosition="2785" endWordPosition="2788">ing to the distribution of labels DL. As in the original algorithm, L is the labeled data, U the unlabeled data, and V1, V2 the views. most confidently labeled 1 to the labeled data. This procedure preserves the distribution of labels in the labeled data as instances are labeled and added. The modified CT algorithm is presented in Figure 3. 5 Evaluation We evaluate co-training for JOB classification using a standard data set assembled by Ramshaw and Marcus from sections 15 18 (training data, 211727 instances) and 20 (test data, 47377 instances) of the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993). Training instances consist of part-ofspeech tag and JOB label for a focus word, along with contexts of two part-of-speech tags to the left and right of the focus. Our goal accuracy of 95.17% is the performance of a supervised JOB classifier trained on the correctly labeled version of the full training data. (In our experiments the goal classifier uses the left view of the data, which actually outperforms the combined left/right view.) For initial labeled data, the first L instances of the training data are given their correct labels. We determined the best setting for the parameters of the C</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M. Marcus, M. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313 330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mitra</author>
<author>C Buckley</author>
<author>A Singhal</author>
<author>C Cardie</author>
</authors>
<title>An analysis of statistical and syntactic phrases.</title>
<date>1997</date>
<booktitle>In 5TH RIAO Conference, Computer-Assisted Information Searching On the Internet,</booktitle>
<pages>200--214</pages>
<contexts>
<context position="11463" citStr="Mitra et al. (1997)" startWordPosition="1810" endWordPosition="1813"> (1999)), suggesting that the rewards of understanding and dealing with this issue may be significant. 3 Base Noun Phrase Identification Base noun phrases (base NPs) are traditionally defined as nonrecursive noun phrases, i.e. NPs that do not contain NPs. (Figure 2a illustrates base NPs with a short example.) Base noun phrase identification is the task of locating the base NPs in a sentence from the words of the sentence and their part-of-speech tags. Base noun phrase identification is a crucial component of systems that employ partial syntactic analysis, including information retrieval (e.g. Mitra et al. (1997)) and question answering (e.g. Cardie et al. (2000)) systems. Many corpus-based methods have been applied to the task, including statistical methods (e.g. Church (1988)), transformation-based learning (e.g. Ramshaw and Marcus (1998)), rote sequence learning (e.g. Cardie and Pierce (1998)), memory-based sequence learning (e.g. Argamon et al. (1999)), and memory-based learning (e.g. Sang and Veenstra (1999)), among others. Our case study employs a well-known bracket representation, introduced by Ramshaw and Marcus, wherein each word of a sentence is tagged with one of the following tags: I, mean</context>
</contexts>
<marker>Mitra, Buckley, Singhal, Cardie, 1997</marker>
<rawString>M. Mitra, C. Buckley, A. Singhal, and C. Cardie. 1997. An analysis of statistical and syntactic phrases. In 5TH RIAO Conference, Computer-Assisted Information Searching On the Internet, pages 200 214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Muslea</author>
<author>S Minton</author>
<author>C Knoblock</author>
</authors>
<title>Selective sampling with redundant views.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth National Conference on Artificial Intelligence,</booktitle>
<pages>621--626</pages>
<marker>Muslea, Minton, Knoblock, 2000</marker>
<rawString>I. Muslea, S. Minton, and C. Knoblock. 2000. Selective sampling with redundant views. In Proceedings of the Seventeenth National Conference on Artificial Intelligence, pages 621 626.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Nigam</author>
<author>R Ghani</author>
</authors>
<title>Analyzing the effectiveness and applicability of co-training.</title>
<date>2000</date>
<booktitle>In Ninth International Conference on Information and Knowledge Management (CIKM2000).</booktitle>
<contexts>
<context position="7492" citStr="Nigam and Ghani (2000)" startWordPosition="1189" endWordPosition="1192">t it is important to preserve the distribution of class labels in the growing body of labeled data. Finally, they demonstrated the effectiveness of co-training on a web page classification task similar to that described above. Collins and Singer (1999) were concerned that the CT algorithm does not strongly enforce the requirement that hypothesis functions should be compatible with the unlabeled data. They introduced an algorithm, CoBoost, that directly minimizes mismatch between views of the unlabeled data, using a combination of ideas from co-training and AdaBoost (Freund and Shapire, 1997). Nigam and Ghani (2000) performed the most thorough empirical investigation of the desideratum of conditional independence of views underlying co-training. Their experiments suggested that view independence does indeed affect the performance of co-training; but that CT, when compared to other algorithms that use labeled and unlabeled data, such as EM (Dempster et al., 1977; Nigam et al., 2000), may still prove effective even when an explicit feature split is unknown, provided that there is enough implicit redundancy in the data. In contrast to previous investigations of the theoretical basis of co-training, this stu</context>
<context position="16021" citStr="Nigam and Ghani (2000)" startWordPosition="2578" endWordPosition="2581">the distribution of labels in the training data is more unbalanced than the distribution of positive and negative examples in the web page task: namely, 53.9% of examples are labeled I, 44.0% 0, and 2.1% B. Since it is impractical to add, say, 27 I, 22 0, and 1 B, to the labeled data at each step of co-training, instead, instances are selected by first choosing a label 1 at random according to the label distribution, then adding the instance 3This standard modification was introduced by Blum and Mitchell (1998) in an effort to cover the underlying distribution of unlabeled instances; however, Nigam and Ghani (2000) found it to be unnecessary in their experiments. i&apos;map N(ti, 1) + 1 repeat until done train classifier h1 on view V1 of L train classifier h2 on view V2 of L transfer randomly selected examples from U to U&apos; until = u for he h2} allow h to posit labels for all examples in U&apos; repeat g times select label 1 at random according to DL transfer most confidently labeled 1 example from U&apos; to L Figure 3: The modified co-training algorithm maintains a data pool U&apos; of size u, and labels g instances per iteration selected according to the distribution of labels DL. As in the original algorithm, L is the l</context>
</contexts>
<marker>Nigam, Ghani, 2000</marker>
<rawString>K. Nigam and R. Ghani. 2000. Analyzing the effectiveness and applicability of co-training. In Ninth International Conference on Information and Knowledge Management (CIKM2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Nigam</author>
<author>A McCallum</author>
<author>S Thrun</author>
<author>T Mitchell</author>
</authors>
<title>Text classification from labeled and unlabeled documents using EM.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="7865" citStr="Nigam et al., 2000" startWordPosition="1247" endWordPosition="1250">le with the unlabeled data. They introduced an algorithm, CoBoost, that directly minimizes mismatch between views of the unlabeled data, using a combination of ideas from co-training and AdaBoost (Freund and Shapire, 1997). Nigam and Ghani (2000) performed the most thorough empirical investigation of the desideratum of conditional independence of views underlying co-training. Their experiments suggested that view independence does indeed affect the performance of co-training; but that CT, when compared to other algorithms that use labeled and unlabeled data, such as EM (Dempster et al., 1977; Nigam et al., 2000), may still prove effective even when an explicit feature split is unknown, provided that there is enough implicit redundancy in the data. In contrast to previous investigations of the theoretical basis of co-training, this study is motivated by practical concerns about the applica(a) In [happier news], [South Korea], in establishing [diplomatic ties] with [Poland] [yesterday], announced [$450 million] in [loans] to [the financially strapped Warsaw government]. (b) Ino [happier, newsi] ,0 [South, Koreai] ,0 ino establishing° [diplomatic, tiesi] with° [Polandi] [yesterdays] ,0 announced° [$, 45</context>
</contexts>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. 2000. Text classification from labeled and unlabeled documents using EM. Machine Learning, 39(2/3):103 134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ramshaw</author>
<author>M Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1998</date>
<booktitle>In Natural Language Processing Using Very Large Corpora. Kluwer. Originally</booktitle>
<note>appeared in WVLC95.</note>
<contexts>
<context position="11695" citStr="Ramshaw and Marcus (1998)" startWordPosition="1841" endWordPosition="1844"> NPs that do not contain NPs. (Figure 2a illustrates base NPs with a short example.) Base noun phrase identification is the task of locating the base NPs in a sentence from the words of the sentence and their part-of-speech tags. Base noun phrase identification is a crucial component of systems that employ partial syntactic analysis, including information retrieval (e.g. Mitra et al. (1997)) and question answering (e.g. Cardie et al. (2000)) systems. Many corpus-based methods have been applied to the task, including statistical methods (e.g. Church (1988)), transformation-based learning (e.g. Ramshaw and Marcus (1998)), rote sequence learning (e.g. Cardie and Pierce (1998)), memory-based sequence learning (e.g. Argamon et al. (1999)), and memory-based learning (e.g. Sang and Veenstra (1999)), among others. Our case study employs a well-known bracket representation, introduced by Ramshaw and Marcus, wherein each word of a sentence is tagged with one of the following tags: I, meaning the word is within a bracket (inside); 0, meaning the word is not within a bracket (outside); or B, meaning the word is within a bracket, but not the same bracket as the preceding word, i.e. the word begins a new bracket. Thus, </context>
</contexts>
<marker>Ramshaw, Marcus, 1998</marker>
<rawString>L. Ramshaw and M. Marcus. 1998. Text chunking using transformation-based learning. In Natural Language Processing Using Very Large Corpora. Kluwer. Originally appeared in WVLC95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>R Jones</author>
</authors>
<title>Learning dictionaries for information extraction by multilevel bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixteenth National Conference on Artificial Intelligence,</booktitle>
<pages>474--479</pages>
<contexts>
<context position="10851" citStr="Riloff and Jones (1999)" startWordPosition="1707" endWordPosition="1710">, due to mistakes made by the view classifiers. To elaborate, the view classifiers may occasionally add incorrectly labeled instances to the labeled data. If many iterations of CT are required for learning the task, degradation in the quality of the labeled data may become a problem, in turn affecting the quality of subsequent view classifiers. For large-scale learning tasks, the effectiveness of co-training may be dulled over time. Finally, we note that the accuracy of automatically accumulated training data is an important issue for many bootstrapping learning methods (e.g. Yarowsky (1995), Riloff and Jones (1999)), suggesting that the rewards of understanding and dealing with this issue may be significant. 3 Base Noun Phrase Identification Base noun phrases (base NPs) are traditionally defined as nonrecursive noun phrases, i.e. NPs that do not contain NPs. (Figure 2a illustrates base NPs with a short example.) Base noun phrase identification is the task of locating the base NPs in a sentence from the words of the sentence and their part-of-speech tags. Base noun phrase identification is a crucial component of systems that employ partial syntactic analysis, including information retrieval (e.g. Mitra e</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>E. Riloff and R. Jones. 1999. Learning dictionaries for information extraction by multilevel bootstrapping. In Proceedings of the Sixteenth National Conference on Artificial Intelligence, pages 474 479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Tjong Kim Sang</author>
<author>J Veenstra</author>
</authors>
<title>Representing text chunks.</title>
<date>1999</date>
<booktitle>In Proceedings of EACL&apos;99. Available as cs.CL/9907006.</booktitle>
<contexts>
<context position="11871" citStr="Sang and Veenstra (1999)" startWordPosition="1866" endWordPosition="1869">words of the sentence and their part-of-speech tags. Base noun phrase identification is a crucial component of systems that employ partial syntactic analysis, including information retrieval (e.g. Mitra et al. (1997)) and question answering (e.g. Cardie et al. (2000)) systems. Many corpus-based methods have been applied to the task, including statistical methods (e.g. Church (1988)), transformation-based learning (e.g. Ramshaw and Marcus (1998)), rote sequence learning (e.g. Cardie and Pierce (1998)), memory-based sequence learning (e.g. Argamon et al. (1999)), and memory-based learning (e.g. Sang and Veenstra (1999)), among others. Our case study employs a well-known bracket representation, introduced by Ramshaw and Marcus, wherein each word of a sentence is tagged with one of the following tags: I, meaning the word is within a bracket (inside); 0, meaning the word is not within a bracket (outside); or B, meaning the word is within a bracket, but not the same bracket as the preceding word, i.e. the word begins a new bracket. Thus, the bracketing task is transformed into a word tagging task. Figure 2b repeats the example sentence, showing the JOB tag representation. Training examples for JOB tagging have </context>
</contexts>
<marker>Sang, Veenstra, 1999</marker>
<rawString>E. Tjong Kim Sang and J. Veenstra. 1999. Representing text chunks. In Proceedings of EACL&apos;99. Available as cs.CL/9907006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="10826" citStr="Yarowsky (1995)" startWordPosition="1705" endWordPosition="1706">ll not scale well, due to mistakes made by the view classifiers. To elaborate, the view classifiers may occasionally add incorrectly labeled instances to the labeled data. If many iterations of CT are required for learning the task, degradation in the quality of the labeled data may become a problem, in turn affecting the quality of subsequent view classifiers. For large-scale learning tasks, the effectiveness of co-training may be dulled over time. Finally, we note that the accuracy of automatically accumulated training data is an important issue for many bootstrapping learning methods (e.g. Yarowsky (1995), Riloff and Jones (1999)), suggesting that the rewards of understanding and dealing with this issue may be significant. 3 Base Noun Phrase Identification Base noun phrases (base NPs) are traditionally defined as nonrecursive noun phrases, i.e. NPs that do not contain NPs. (Figure 2a illustrates base NPs with a short example.) Base noun phrase identification is the task of locating the base NPs in a sentence from the words of the sentence and their part-of-speech tags. Base noun phrase identification is a crucial component of systems that employ partial syntactic analysis, including informatio</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 189 196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>