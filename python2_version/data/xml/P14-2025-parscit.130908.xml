<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003733">
<title confidence="0.994861">
Simple extensions for a reparameterised IBM Model 2
</title>
<author confidence="0.996244">
Douwe Gelling Trevor Cohn
</author>
<affiliation confidence="0.9944795">
Department of Computer Science Computing and Information Systems
The University of Sheffield The University of Melbourne
</affiliation>
<email confidence="0.992962">
d.gelling@shef.ac.uk t.cohn@unimelb.edu.au
</email>
<sectionHeader confidence="0.993737" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997894">
A modification of a reparameterisation of
IBM Model 2 is presented, which makes
the model more flexible, and able to model
a preference for aligning to words to either
the right or left, and take into account POS
tags on the target side of the corpus. We
show that this extension has a very small
impact on training times, while obtain-
ing better alignments in terms of BLEU
scores.
</bodyText>
<sectionHeader confidence="0.998793" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999206964285714">
Word alignment is at the basis of most statistical
machine translation. The models that are gener-
ally used are often slow to train, and have a large
number of parameters. Dyer et al. (2013) present
a simple reparameterization of IBM Model 2 that
is very fast to train, and achieves results similar to
IBM Model 4.
While this model is very effective, it also has
a very low number of parameters, and as such
doesn’t have a large amount of expressive power.
For one thing, it forces the model to consider
alignments on both sides of the diagonal equally
likely. However, it isn’t clear that this is the case,
as for some languages an alignment to earlier or
later in the sentence (above or below the diagonal)
could be common, due to word order differences.
For example, when aligning to Dutch, it may be
common for one verb to be aligned near the end of
the sentence that would be at the beginning in En-
glish. This would mean most of the other words in
the sentence would also align slightly away from
the diagonal in one direction. Figure 1 shows an
example sentence in which this happens. Here, a
circle denotes an alignment, and darker squares
are more likely under the alignment model. In
this case the modified Model 2 would simply make
both directions equally likely, where we would re-
ally like for only one direction to be more likely.
</bodyText>
<figureCaption confidence="0.802352">
Figure 1: Visualization of aligned sentence pair in
</figureCaption>
<bodyText confidence="0.993316">
Dutch and English, darker shaded squares have a
higher alignment probability under the model, a
circle indicates a correct alignment. The English
sentence runs from bottom to top, the Dutch sen-
tence left to Right.
In some cases it could be that the prior probability
for a word alignment should be off the diagonal.
Furthermore, it is common in word alignment to
take word classes into account. This is commonly
implemented for the HMM alignment model as
well as Models 4 and 5. Och and Ney (2003) show
that for larger corpora, using word classes leads
to lower Alignment Error Rate (AER). This is not
implemented for Model 2, as it already has an
alignment model that is dependent on both source
and target length, and the position in both sen-
tences, and adding a dependency to word classes
would make the the Model even more prone to
overfitting than it already is. However, using the
reparameterization in (Dyer et al., 2013) would
leave the model simple enough even with a rela-
tively large amount of word classes.
Figure 2 shows an example of how the model
extensions could benefit word alignment. In the
example, all the Dutch words have a different
</bodyText>
<figure confidence="0.6912496">
man
the
seen
had
He
</figure>
<page confidence="0.944409">
150
</page>
<bodyText confidence="0.609102">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 150–154,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</bodyText>
<equation confidence="0.88228575">
�m
i=1
n
j=0
</equation>
<figureCaption confidence="0.935129">
Figure 2: Visualization of aligned sentence pair in
</figureCaption>
<bodyText confidence="0.9182058">
Dutch and English, darker shaded squares have a
higher alignment probability under the model, a
circle indicates a correct alignment. The English
sentence runs from bottom to top, the Dutch sen-
tence left to Right.
</bodyText>
<equation confidence="0.827886">
S(ai|i, m, n) · B(ei|fai)
</equation>
<bodyText confidence="0.995783142857143">
here, m is the length of the target sentence e, n
the same for source sentence f, S is the alignment
model and B is the translation model. In this pa-
per we are mainly concerned with the alignment
model S. In the original formulation (with a minor
tweak to ensure symmetry through the center), this
function is defined as:
</bodyText>
<equation confidence="0.9955558">
S(ai = j|i, m, n) =
{ p0 j = 0
gh(i,m,n)
(1 − p0) j &apos; Z(i,m,n) 0 &lt; j ≤ n
0 otherwise
</equation>
<bodyText confidence="0.982085464285714">
man
the
seen
had
He
where, h(·) is defined as
word class, and so can have different gradients for
alignment probability over the english words. If
the model has learned that prepositions and nouns
are more likely to align to words later in the sen-
tence, it could have a lower lambda for both word
classes, resulting in a less steep slope. If we also
split lambda into two variables, we can get align-
ment probabilities as shown above for the Dutch
word ’de’, where aligning to one side of the diag-
onal is made more likely for some word classes.
Finally, instead of just having one side of the di-
agonal less steep than the other, it may be useful
to instead move the peak of the alignment prob-
ability function off the diagonal, while keeping it
equally likely. In Figure 2, this is done for the past
participle ’gezien’.
We will present a simple model for adding the
above extensions to achieve the above (splitting
the parameter, adding an offset and conditioning
the parameters on the POS tag of the target word)
in section 2, results on a set of experiments in sec-
tion 3 and present our conclusions in section 4.
</bodyText>
<sectionHeader confidence="0.990407" genericHeader="introduction">
2 Methods
</sectionHeader>
<bodyText confidence="0.999587666666667">
We make use of a modified version of Model 2,
from Dyer et al. (2013), which has an alignment
model that is parameterised in its original form
solely on the variable A. Specifically, the proba-
bility of a sentence e given a sentence f is given
as:
</bodyText>
<equation confidence="0.44043">
h(i, j, m, n) = −A
</equation>
<bodyText confidence="0.9953485">
and Zλ(i, m, n) is �nj&apos;=1 eλh(i,j&apos;,m,n), i.e. a
normalising function. Like the original Model 2
(Brown et al., 1993), this model is trained us-
ing Expectation-Maximisation. However, it is not
possible to directly update the A parameter during
training, as it cannot be computed analytically. In-
stead, a gradient-based approach is used during the
M-step.
Two different optimisations are employed, the
first of which is used for calculating Zλ. This
function forms a geometric series away from the
diagonal (for each target word), which can be
computed efficiently for each of the directions
from the diagonal. The second is used during the
M-step when computing the derivative, and is very
similar, but instead of using a geometric series, an
arithmetico-geometric series is used.
In order to allow the model to have a different
parameter above and below the diagonal, the only
change needed is to redefine h(·) to use a different
parameter for A above and below the diagonal. We
denote these parameters as A and γ for below and
above the diagonal respectively. Further, the offset
is denoted as w.
we change the definition of h(·) to the following
instead:
</bodyText>
<equation confidence="0.9959349375">
i i j i
n + 1
m + 1
151
h(i, j,m, n) =
i
∇ωL =
n
k=1
j&lt;= ji
{
A
m + 1
X9↓
l=1
−
i
otherwise
����
����
&apos;y
i
m + 1
����
p(ai = k|ei, f, m, n)h&apos;(i, k, m, n)
δ(l|i, m, n)h&apos;(i,l, m, n)
j
+ w
n + 1
j
+ w
n + 1
</equation>
<bodyText confidence="0.6304245">
ji is the point closest to or on the diagonal here,
calculated as:
</bodyText>
<equation confidence="0.9794155">
max(min(bi · (n + 1)
m + 1 + w · (n + 1)c, n), 0)
</equation>
<bodyText confidence="0.999856222222222">
Here, w can range from −1 to 1, and thus the
calculation for the diagonal ji is clamped to be in
a valid range for alignments.
As the partition function (Z(·)) used in (Dyer et
al., 2013) consists of 2 calculations for each tar-
get position i, one for above and one for below the
diagonal, we can simply substitute &apos;y for the geo-
metric series calculations in order to use different
parameters for each:
</bodyText>
<equation confidence="0.893028">
si(eAh(i,9↓,m,n), r) + sn−T(eγh(i,9T,m,n), r)
</equation>
<bodyText confidence="0.991153">
where jT is ji + 1.
</bodyText>
<subsectionHeader confidence="0.993624">
2.1 Optimizing the Parameters
</subsectionHeader>
<bodyText confidence="0.984158432432433">
As in the original formulation, we need to use
gradient-based optimisation in order to find good
values for A, &apos;y and w. Unfortunately, optimizing
w would require taking the derivative of h(·), and
thus the derivative of the absolute value. This is
unfortunately undefined when the argument is 0,
however we work around this by choosing a sub-
gradient of 0 at that point. This means the steps we
take do not always improve the objective function,
but in practice the method works well.
The first derivative of L with respect to A at a
single target word becomes:
p(ai = k|ei, f, m, n)h(i, k, m, n)
δ(l|i, m, n)h(i,l, m, n)
And similar for finding the first derivative with
respect to &apos;y, but summing from jT to n instead.
The first derivative with respect to w then, is:
Where h&apos;(·) is the first derivative of h(·) with
respect to w. For obtaining this derivative, the
arithmetico-geometric series (Fernandez et al.,
2006) was originally used as an optimization, and
for the gradient with respect to omega a geometric
series should suffice, as an optimization, as there
is no conditioning on the source words. This is
not done in the current work however, so timing
results will not be directly comparable to those
found in (Dyer et al., 2013).
Conditioning on the POS of the target words
then becomes as simple as using a different A, &apos;y,
and w for each POS tag in the input, and calculat-
ing a separate derivative for each of them, using
only the derivatives at those target words that use
the POS tag. A minor detail is to keep a count of
alignment positions used for finding the derivative
for each different parameter, and normalizing the
resulting derivatives with those counts, so the step
size can be kept constant across POS tags.
</bodyText>
<sectionHeader confidence="0.995393" genericHeader="method">
3 Empirical results
</sectionHeader>
<bodyText confidence="0.999857181818182">
The above described model is evaluated with ex-
periments on a set of 3 language pairs, on which
AER scores and BLEU scores are computed. We
use similar corpora as used in (Dyer et al., 2013):
a French-English corpus made up of Europarl ver-
sion 7 and news-commentary corpora, the Arabic-
English parallel data consisting of the non-UN
portions of the NIST training corpora, and the
FBIS Chinese-English corpora.
The models that are compared are the original
reparameterization of Model 2, a version where A
is split around the diagonal (split), one where pos
tags are used, but A is not split around the diagonal
(pos), one where an offset is used, but parameters
aren’t split about the diagonal (offset), one that’s
split about the diagonal and uses pos tags (pos &amp;
split) and finally one with all three (pos &amp; split &amp;
offset). All are trained for 5 iterations, with uni-
form initialisation, where the first iteration only
the translation probabilities are updated, and the
other parameters are updated as well in the sub-
sequent iterations. The same hyperparameters are
</bodyText>
<equation confidence="0.991317666666667">
X9↓
k=1
∇AL =
−
X9↓
l=1
</equation>
<page confidence="0.995627">
152
</page>
<table confidence="0.9991286">
Model Fr-En Ar-En Zh-En
Tokens 111M 46M 17.3M
(after) 110M 29.0M 10.4M
average 1.64 0.76 0.27
Model 4 15.5 6.3 2.2
</table>
<tableCaption confidence="0.90382925">
Table 1: Token counts and average amount of time
to train models (and separately training time for
Model 4) on original corpora in one direction in
hours, by corpus.
</tableCaption>
<bodyText confidence="0.998275394736842">
used as in (Dyer et al., 2013), with stepsize for up-
dates to A and -y during gradient ascent is 1000,
and that for w is 0.03, decaying after every gradi-
ent descent step by 0.9, using 8 steps every iter-
ation. Both A and -y are initialised to 6, and w is
initialised to 0. For these experiments the pos and
pos &amp; split use POS tags generated using the Stan-
ford POS tagger (Toutanova and Manning, 2000),
using the supplied models for all of the languages
used in the experiments. For comparison, Model
4 is trained for 5 iterations using 5 iterations each
of Model 1 and Model 3 as initialization, using
GIZA++ (Och and Ney, 2003).
For the comparisons in AER, the corpora are
used as-is, but for the BLEU comparisons, sen-
tences longer than 50 words are filtered out. In
Table 2 the sizes of the corpora before filtering are
listed, as well as the time taken in hours to align
the corpora for AER. As the training times for
the different versions barely differ, only the aver-
age is displayed for the models here described and
Model 4 training times are given for comparison.
Note that the times for the models optimizing only
A and -y, and the model only optimizing w still cal-
culate the derivatives for the other parameters, and
so could be made to be faster than here displayed.
For both the BLEU and AER results, the align-
ments are generated in both directions, and sym-
metrised using the grow-diag-final-and heuristic,
which in preliminary tests had shown to do best in
terms of AER.
The results are given in Table 2. These scores
were computed using the WMT2012 data as gold
standard. The different extensions to the model
make no difference to the AER scores for Chinese-
English, and actually do slightly worse for French-
English. In both cases, Model 4 does better than
the models introduced here.
</bodyText>
<table confidence="0.9980085">
Model Fr-En Zh-En
Original 16.3 42.5
Split 16.8 42.5
Pos 16.6 42.5
Offset 16.8 42.5
Pos &amp; Split 16.8 42.5
Pos &amp; Split &amp; Offset 16.7 42.5
Model 4 11.2 40.5
</table>
<tableCaption confidence="0.8514775">
Table 2: AER results on Chinese-English and
French-English data sets
</tableCaption>
<table confidence="0.999817125">
Model Fr-En Ar-En Zh-En
Original 25.9 43.8 32.8
Split 25.9 43.2 32.8
Pos 25.9 43.9 32.9
Offset 26.0 43.9 32.8
Pos &amp; Split 26.0 44.1 33.2
Pos &amp; Split &amp; Offset 26.0 44.2 33.3
Model 4 26.8 43.9 32.4
</table>
<tableCaption confidence="0.9596435">
Table 3: BLEU results on Chinese-English and
French-English data sets
</tableCaption>
<bodyText confidence="0.95783192">
For the comparisons of translation quality, the
models are trained up using a phrase-based trans-
lation system (Koehn et al., 2007) that used the
above listed models to align the data. Language
models were augmented with data outside of the
corpora for Chinese-English (200M words total)
and Arabic-English (100M words total). Test sets
for Chinese are MT02, MT03, MT06 and MT08,
for Arabic they were MT05, MT06 and MT08, and
for French they were the newssyscomb2009 data
and the newstest 2009-2012 data.
The results are listed in Table 31. BLEU scores
for Arabic-English and Chinese-English are com-
puted with multiple references, while those for
French-English are against a single reference. Al-
though the different models made little difference
in AER, there is quite a bit of variation in the
BLEU scores between the different models. In
all cases, the models conditioned on POS tags
did better than the original model, by as much
as 0.5 BLEU points. For Arabic-English as well
as Chinese-English, the full model outperformed
1The difference in these results compared to those re-
ported in Dyer et al. (2013) is due to differences in corpus
size, and the fact that a different translation model is used.
</bodyText>
<page confidence="0.997747">
153
</page>
<bodyText confidence="0.999777285714286">
Model 4, in the case of Chinese-English by 0.9
BLEU points.
The low impact of the split and offset models
are most likely due to the need to model all align-
ments in the corpus. The distributions can’t skew
too far to aligning to one direction, as that would
lower the probability of a large amount of align-
ments. This is reflected in the resulting parame-
ters A, -y and w that are estimated, as the first two
do not differ much from the parameters estimated
when both are kept the same, and the second tends
to be very small.
As for the Pos model, it seems that only vary-
ing the symmetrical slope for the different POS
tags doesn’t capture the differences between dis-
tributions for POS tags. For example, the A and
-y parameters can differ quite a lot in the Pos &amp;
Split model when compared to the Pos model, with
one side having a much smaller parameter and the
other a much larger parameter for a given POS tag
in the first model, and the single parameter being
closer to the model average for the same POS tag
in the second model.
The low variation in results between the differ-
ent models for French-English might be explained
by less word movement when translating between
these languages, which could mean the original
model is sufficient to capture this behaviour.
</bodyText>
<sectionHeader confidence="0.999554" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999953454545454">
We have shown some extensions to a reparame-
terized IBM Model 2, allowing it to model word
reordering better. Although these models don’t
improve on the baseline in terms of AER, they
do better than the original in all three languages
tested, and outperform M4 in two of these lan-
guages, at little cost in terms of training time. Fu-
ture directions for this work include allowing for
more expressivity of the alignment model by using
a Beta distribution instead of the current exponen-
tial model.
</bodyText>
<sectionHeader confidence="0.999679" genericHeader="acknowledgments">
5 Acknowledgments
</sectionHeader>
<bodyText confidence="0.662831333333333">
Dr Cohn is the recipient of an Australian Re-
search Council Future Fellowship (project number
FT130101105).
</bodyText>
<sectionHeader confidence="0.966355" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999886970588235">
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert. L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19:263–311.
Chris Dyer, Victor Chahuneau, and Noah A Smith.
2013. A simple, fast, and effective reparameteri-
zation of ibm model 2. In Proceedings of NAACL-
HLT, pages 644–648.
P. A. Fernandez, T. Foregger, and J. Pahikkala. 2006.
Arithmetico-geometric series.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ’07, pages 177–180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29:19–51, March.
Kristina Toutanova and Christopher D. Manning.
2000. Enriching the knowledge sources used in a
maximum entropy part-of-speech tagger. In Pro-
ceedings of the 2000 Joint SIGDAT Conference on
Empirical Methods in Natural Language Process-
ing and Very Large Corpora: Held in Conjunction
with the 38th Annual Meeting of the Association
for Computational Linguistics - Volume 13, EMNLP
’00, pages 63–70, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
</reference>
<page confidence="0.999772">
154
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.818893">
<title confidence="0.888124">Simple extensions for a reparameterised IBM Model 2</title>
<author confidence="0.994103">Douwe Gelling Trevor Cohn</author>
<affiliation confidence="0.9995755">Department of Computer Science Computing and Information Systems The University of Sheffield The University of Melbourne</affiliation>
<email confidence="0.976648">d.gelling@shef.ac.ukt.cohn@unimelb.edu.au</email>
<abstract confidence="0.995098727272727">A modification of a reparameterisation of IBM Model 2 is presented, which makes the model more flexible, and able to model a preference for aligning to words to either the right or left, and take into account POS tags on the target side of the corpus. We show that this extension has a very small impact on training times, while obtaining better alignments in terms of BLEU scores.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Mercer</author>
</authors>
<date>1993</date>
<marker>Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert. L. Mercer. 1993.</rawString>
</citation>
<citation valid="false">
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<journal>Computational Linguistics,</journal>
<pages>19--263</pages>
<marker></marker>
<rawString>The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19:263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Victor Chahuneau</author>
<author>Noah A Smith</author>
</authors>
<title>A simple, fast, and effective reparameterization of ibm model 2.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACLHLT,</booktitle>
<pages>644--648</pages>
<contexts>
<context position="837" citStr="Dyer et al. (2013)" startWordPosition="132" endWordPosition="135"> t.cohn@unimelb.edu.au Abstract A modification of a reparameterisation of IBM Model 2 is presented, which makes the model more flexible, and able to model a preference for aligning to words to either the right or left, and take into account POS tags on the target side of the corpus. We show that this extension has a very small impact on training times, while obtaining better alignments in terms of BLEU scores. 1 Introduction Word alignment is at the basis of most statistical machine translation. The models that are generally used are often slow to train, and have a large number of parameters. Dyer et al. (2013) present a simple reparameterization of IBM Model 2 that is very fast to train, and achieves results similar to IBM Model 4. While this model is very effective, it also has a very low number of parameters, and as such doesn’t have a large amount of expressive power. For one thing, it forces the model to consider alignments on both sides of the diagonal equally likely. However, it isn’t clear that this is the case, as for some languages an alignment to earlier or later in the sentence (above or below the diagonal) could be common, due to word order differences. For example, when aligning to Dut</context>
<context position="2970" citStr="Dyer et al., 2013" startWordPosition="510" endWordPosition="513">onal. Furthermore, it is common in word alignment to take word classes into account. This is commonly implemented for the HMM alignment model as well as Models 4 and 5. Och and Ney (2003) show that for larger corpora, using word classes leads to lower Alignment Error Rate (AER). This is not implemented for Model 2, as it already has an alignment model that is dependent on both source and target length, and the position in both sentences, and adding a dependency to word classes would make the the Model even more prone to overfitting than it already is. However, using the reparameterization in (Dyer et al., 2013) would leave the model simple enough even with a relatively large amount of word classes. Figure 2 shows an example of how the model extensions could benefit word alignment. In the example, all the Dutch words have a different man the seen had He 150 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 150–154, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics �m i=1 n j=0 Figure 2: Visualization of aligned sentence pair in Dutch and English, darker shaded squares have a higher alignment probabil</context>
<context position="5337" citStr="Dyer et al. (2013)" startWordPosition="941" endWordPosition="944"> one side of the diagonal less steep than the other, it may be useful to instead move the peak of the alignment probability function off the diagonal, while keeping it equally likely. In Figure 2, this is done for the past participle ’gezien’. We will present a simple model for adding the above extensions to achieve the above (splitting the parameter, adding an offset and conditioning the parameters on the POS tag of the target word) in section 2, results on a set of experiments in section 3 and present our conclusions in section 4. 2 Methods We make use of a modified version of Model 2, from Dyer et al. (2013), which has an alignment model that is parameterised in its original form solely on the variable A. Specifically, the probability of a sentence e given a sentence f is given as: h(i, j, m, n) = −A and Zλ(i, m, n) is �nj&apos;=1 eλh(i,j&apos;,m,n), i.e. a normalising function. Like the original Model 2 (Brown et al., 1993), this model is trained using Expectation-Maximisation. However, it is not possible to directly update the A parameter during training, as it cannot be computed analytically. Instead, a gradient-based approach is used during the M-step. Two different optimisations are employed, the firs</context>
<context position="7198" citStr="Dyer et al., 2013" startWordPosition="1303" endWordPosition="1306">respectively. Further, the offset is denoted as w. we change the definition of h(·) to the following instead: i i j i n + 1 m + 1 151 h(i, j,m, n) = i ∇ωL = n k=1 j&lt;= ji { A m + 1 X9↓ l=1 − i otherwise ���� ���� &apos;y i m + 1 ���� p(ai = k|ei, f, m, n)h&apos;(i, k, m, n) δ(l|i, m, n)h&apos;(i,l, m, n) j + w n + 1 j + w n + 1 ji is the point closest to or on the diagonal here, calculated as: max(min(bi · (n + 1) m + 1 + w · (n + 1)c, n), 0) Here, w can range from −1 to 1, and thus the calculation for the diagonal ji is clamped to be in a valid range for alignments. As the partition function (Z(·)) used in (Dyer et al., 2013) consists of 2 calculations for each target position i, one for above and one for below the diagonal, we can simply substitute &apos;y for the geometric series calculations in order to use different parameters for each: si(eAh(i,9↓,m,n), r) + sn−T(eγh(i,9T,m,n), r) where jT is ji + 1. 2.1 Optimizing the Parameters As in the original formulation, we need to use gradient-based optimisation in order to find good values for A, &apos;y and w. Unfortunately, optimizing w would require taking the derivative of h(·), and thus the derivative of the absolute value. This is unfortunately undefined when the argumen</context>
<context position="8751" citStr="Dyer et al., 2013" startWordPosition="1575" endWordPosition="1578">d similar for finding the first derivative with respect to &apos;y, but summing from jT to n instead. The first derivative with respect to w then, is: Where h&apos;(·) is the first derivative of h(·) with respect to w. For obtaining this derivative, the arithmetico-geometric series (Fernandez et al., 2006) was originally used as an optimization, and for the gradient with respect to omega a geometric series should suffice, as an optimization, as there is no conditioning on the source words. This is not done in the current work however, so timing results will not be directly comparable to those found in (Dyer et al., 2013). Conditioning on the POS of the target words then becomes as simple as using a different A, &apos;y, and w for each POS tag in the input, and calculating a separate derivative for each of them, using only the derivatives at those target words that use the POS tag. A minor detail is to keep a count of alignment positions used for finding the derivative for each different parameter, and normalizing the resulting derivatives with those counts, so the step size can be kept constant across POS tags. 3 Empirical results The above described model is evaluated with experiments on a set of 3 language pairs</context>
<context position="10672" citStr="Dyer et al., 2013" startWordPosition="1913" endWordPosition="1916">ith all three (pos &amp; split &amp; offset). All are trained for 5 iterations, with uniform initialisation, where the first iteration only the translation probabilities are updated, and the other parameters are updated as well in the subsequent iterations. The same hyperparameters are X9↓ k=1 ∇AL = − X9↓ l=1 152 Model Fr-En Ar-En Zh-En Tokens 111M 46M 17.3M (after) 110M 29.0M 10.4M average 1.64 0.76 0.27 Model 4 15.5 6.3 2.2 Table 1: Token counts and average amount of time to train models (and separately training time for Model 4) on original corpora in one direction in hours, by corpus. used as in (Dyer et al., 2013), with stepsize for updates to A and -y during gradient ascent is 1000, and that for w is 0.03, decaying after every gradient descent step by 0.9, using 8 steps every iteration. Both A and -y are initialised to 6, and w is initialised to 0. For these experiments the pos and pos &amp; split use POS tags generated using the Stanford POS tagger (Toutanova and Manning, 2000), using the supplied models for all of the languages used in the experiments. For comparison, Model 4 is trained for 5 iterations using 5 iterations each of Model 1 and Model 3 as initialization, using GIZA++ (Och and Ney, 2003). F</context>
<context position="14039" citStr="Dyer et al. (2013)" startWordPosition="2504" endWordPosition="2507">9-2012 data. The results are listed in Table 31. BLEU scores for Arabic-English and Chinese-English are computed with multiple references, while those for French-English are against a single reference. Although the different models made little difference in AER, there is quite a bit of variation in the BLEU scores between the different models. In all cases, the models conditioned on POS tags did better than the original model, by as much as 0.5 BLEU points. For Arabic-English as well as Chinese-English, the full model outperformed 1The difference in these results compared to those reported in Dyer et al. (2013) is due to differences in corpus size, and the fact that a different translation model is used. 153 Model 4, in the case of Chinese-English by 0.9 BLEU points. The low impact of the split and offset models are most likely due to the need to model all alignments in the corpus. The distributions can’t skew too far to aligning to one direction, as that would lower the probability of a large amount of alignments. This is reflected in the resulting parameters A, -y and w that are estimated, as the first two do not differ much from the parameters estimated when both are kept the same, and the second</context>
</contexts>
<marker>Dyer, Chahuneau, Smith, 2013</marker>
<rawString>Chris Dyer, Victor Chahuneau, and Noah A Smith. 2013. A simple, fast, and effective reparameterization of ibm model 2. In Proceedings of NAACLHLT, pages 644–648.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P A Fernandez</author>
<author>T Foregger</author>
<author>J Pahikkala</author>
</authors>
<title>Arithmetico-geometric series.</title>
<date>2006</date>
<contexts>
<context position="8430" citStr="Fernandez et al., 2006" startWordPosition="1519" endWordPosition="1522">owever we work around this by choosing a subgradient of 0 at that point. This means the steps we take do not always improve the objective function, but in practice the method works well. The first derivative of L with respect to A at a single target word becomes: p(ai = k|ei, f, m, n)h(i, k, m, n) δ(l|i, m, n)h(i,l, m, n) And similar for finding the first derivative with respect to &apos;y, but summing from jT to n instead. The first derivative with respect to w then, is: Where h&apos;(·) is the first derivative of h(·) with respect to w. For obtaining this derivative, the arithmetico-geometric series (Fernandez et al., 2006) was originally used as an optimization, and for the gradient with respect to omega a geometric series should suffice, as an optimization, as there is no conditioning on the source words. This is not done in the current work however, so timing results will not be directly comparable to those found in (Dyer et al., 2013). Conditioning on the POS of the target words then becomes as simple as using a different A, &apos;y, and w for each POS tag in the input, and calculating a separate derivative for each of them, using only the derivatives at those target words that use the POS tag. A minor detail is </context>
</contexts>
<marker>Fernandez, Foregger, Pahikkala, 2006</marker>
<rawString>P. A. Fernandez, T. Foregger, and J. Pahikkala. 2006. Arithmetico-geometric series.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondˇrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="13060" citStr="Koehn et al., 2007" startWordPosition="2343" endWordPosition="2346">odel Fr-En Zh-En Original 16.3 42.5 Split 16.8 42.5 Pos 16.6 42.5 Offset 16.8 42.5 Pos &amp; Split 16.8 42.5 Pos &amp; Split &amp; Offset 16.7 42.5 Model 4 11.2 40.5 Table 2: AER results on Chinese-English and French-English data sets Model Fr-En Ar-En Zh-En Original 25.9 43.8 32.8 Split 25.9 43.2 32.8 Pos 25.9 43.9 32.9 Offset 26.0 43.9 32.8 Pos &amp; Split 26.0 44.1 33.2 Pos &amp; Split &amp; Offset 26.0 44.2 33.3 Model 4 26.8 43.9 32.4 Table 3: BLEU results on Chinese-English and French-English data sets For the comparisons of translation quality, the models are trained up using a phrase-based translation system (Koehn et al., 2007) that used the above listed models to align the data. Language models were augmented with data outside of the corpora for Chinese-English (200M words total) and Arabic-English (100M words total). Test sets for Chinese are MT02, MT03, MT06 and MT08, for Arabic they were MT05, MT06 and MT08, and for French they were the newssyscomb2009 data and the newstest 2009-2012 data. The results are listed in Table 31. BLEU scores for Arabic-English and Chinese-English are computed with multiple references, while those for French-English are against a single reference. Although the different models made li</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07, pages 177–180, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Comput. Linguist.,</journal>
<pages>29--19</pages>
<contexts>
<context position="2539" citStr="Och and Ney (2003)" startWordPosition="435" endWordPosition="438">where we would really like for only one direction to be more likely. Figure 1: Visualization of aligned sentence pair in Dutch and English, darker shaded squares have a higher alignment probability under the model, a circle indicates a correct alignment. The English sentence runs from bottom to top, the Dutch sentence left to Right. In some cases it could be that the prior probability for a word alignment should be off the diagonal. Furthermore, it is common in word alignment to take word classes into account. This is commonly implemented for the HMM alignment model as well as Models 4 and 5. Och and Ney (2003) show that for larger corpora, using word classes leads to lower Alignment Error Rate (AER). This is not implemented for Model 2, as it already has an alignment model that is dependent on both source and target length, and the position in both sentences, and adding a dependency to word classes would make the the Model even more prone to overfitting than it already is. However, using the reparameterization in (Dyer et al., 2013) would leave the model simple enough even with a relatively large amount of word classes. Figure 2 shows an example of how the model extensions could benefit word alignm</context>
<context position="11269" citStr="Och and Ney, 2003" startWordPosition="2025" endWordPosition="2028">n (Dyer et al., 2013), with stepsize for updates to A and -y during gradient ascent is 1000, and that for w is 0.03, decaying after every gradient descent step by 0.9, using 8 steps every iteration. Both A and -y are initialised to 6, and w is initialised to 0. For these experiments the pos and pos &amp; split use POS tags generated using the Stanford POS tagger (Toutanova and Manning, 2000), using the supplied models for all of the languages used in the experiments. For comparison, Model 4 is trained for 5 iterations using 5 iterations each of Model 1 and Model 3 as initialization, using GIZA++ (Och and Ney, 2003). For the comparisons in AER, the corpora are used as-is, but for the BLEU comparisons, sentences longer than 50 words are filtered out. In Table 2 the sizes of the corpora before filtering are listed, as well as the time taken in hours to align the corpora for AER. As the training times for the different versions barely differ, only the average is displayed for the models here described and Model 4 training times are given for comparison. Note that the times for the models optimizing only A and -y, and the model only optimizing w still calculate the derivatives for the other parameters, and s</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Comput. Linguist., 29:19–51, March.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
</authors>
<title>Enriching the knowledge sources used in a maximum entropy part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora: Held in Conjunction with the 38th Annual Meeting of the Association for Computational Linguistics - Volume 13, EMNLP ’00,</booktitle>
<pages>63--70</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="11041" citStr="Toutanova and Manning, 2000" startWordPosition="1985" endWordPosition="1988">) 110M 29.0M 10.4M average 1.64 0.76 0.27 Model 4 15.5 6.3 2.2 Table 1: Token counts and average amount of time to train models (and separately training time for Model 4) on original corpora in one direction in hours, by corpus. used as in (Dyer et al., 2013), with stepsize for updates to A and -y during gradient ascent is 1000, and that for w is 0.03, decaying after every gradient descent step by 0.9, using 8 steps every iteration. Both A and -y are initialised to 6, and w is initialised to 0. For these experiments the pos and pos &amp; split use POS tags generated using the Stanford POS tagger (Toutanova and Manning, 2000), using the supplied models for all of the languages used in the experiments. For comparison, Model 4 is trained for 5 iterations using 5 iterations each of Model 1 and Model 3 as initialization, using GIZA++ (Och and Ney, 2003). For the comparisons in AER, the corpora are used as-is, but for the BLEU comparisons, sentences longer than 50 words are filtered out. In Table 2 the sizes of the corpora before filtering are listed, as well as the time taken in hours to align the corpora for AER. As the training times for the different versions barely differ, only the average is displayed for the mod</context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>Kristina Toutanova and Christopher D. Manning. 2000. Enriching the knowledge sources used in a maximum entropy part-of-speech tagger. In Proceedings of the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora: Held in Conjunction with the 38th Annual Meeting of the Association for Computational Linguistics - Volume 13, EMNLP ’00, pages 63–70, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>