<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.043195">
<title confidence="0.990091">
A Wearable Headset Speech-to-Speech Translation System
</title>
<author confidence="0.92395">
Kriste Krstovski, Michael Decerbo, Rohit Prasad, David Stallard, Shirin Saleem,
</author>
<address confidence="0.67862925">
Premkumar Natarajan
Speech and Language Processing Department
BBN Technologies
10 Moulton Street, Cambridge, MA, 02138
</address>
<email confidence="0.972104">
{krstovski, mdecerbo, rprasad, stallard, ssaleem, prem}@bbn.com
</email>
<sectionHeader confidence="0.998986" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.969513133333333">
In this paper we present a wearable, headset
integrated eyes- and hands-free speech-to-
speech (S2S) translation system. The S2S sys-
tem described here is configured for translin-
gual communication between English and
colloquial Iraqi Arabic. It employs an n-gram
speech recognition engine, a rudimentary
phrase-based translator for translating recog-
nized Iraqi text, and a rudimentary text-to-
speech (TTS) synthesis engine for playing
back the English translation. This paper de-
scribes the system architecture, the functional-
ity of its components, and the configurations
of the speech recognition and machine transla-
tion engines.
</bodyText>
<sectionHeader confidence="0.994768" genericHeader="keywords">
1 Background
</sectionHeader>
<bodyText confidence="0.9993959375">
Humanitarian personnel, military personnel, and
visitors in foreign countries often need to commu-
nicate with residents of a host country. Human in-
terpreters are inevitably in short supply, and
training personnel to speak a new language is diffi-
cult. Under the DARPA TRANSTAC and Babylon
programs, various teams have developed systems
that enable two-way communication over a lan-
guage barrier (Waibel et al., 2003; Zhou et al.,
2004; Stallard et al., 2006). The two-way speech-
to-speech (S2S) translation systems seek, in prin-
ciple, to translate any utterance, by using general
statistical models trained on large amounts of
speech and text data.
The performance and usability of such two-way
speech-to-speech (S2S) translation systems is
</bodyText>
<page confidence="0.968571">
10
</page>
<bodyText confidence="0.984757591836735">
heavily dependent on the computational resources,
such as processing power and memory, of the plat-
form they are running on. To enable open-ended
conversation these S2S systems employ powerful
but highly memory- and computation-intensive
statistical speech recognition and machine transla-
tion models. Thus, at the very minimum they re-
quire the processing and memory configuration of
common-of-the-shelf (COTS) laptops.
Unfortunately, most laptops do not have a form
factor that is suitable for mobile users. The size,
weight, and shape of laptops render them unsuit-
able for handheld use. Moreover, simply carrying
the laptop can be infeasible for users, such as mili-
tary personnel, who are already overburdened with
other equipment. Embedded platforms, on the
other hand, offer a more suitable form factor in
terms of size and weight, but lack the computa-
tional resources required to run more open-ended
2-way S2S systems.
In previous work, Prasad et al. (2007) reported
on the development of a S2S system for Windows
Mobile based handheld computers. To overcome
the challenges posed by the limited resources of
that platform, the PDA version of the S2S system
was designed to be more constrained in terms of
the ASR and MT vocabulary. As described in de-
tail in (Prasad et al., 2007), the PDA based S2S
system configured for English/Iraqi S2S translation
delivers fairly accurate translation at faster than
real-time.
In this paper, we present ongoing development
work on an S2S system that runs on an even more
constrained hardware platform; namely, a proces-
sor embedded in a wearable headset with just 32
MB of memory. Compared to the PDA based sys-
Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing, pages 10–12,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
tem described in (Prasad et al., 2007), the wearable
system is designed for both eyes- and hands-free
operation. The headset-integrated translation de-
vice described in this paper is configured for two-
way conversation in English/Iraqi. The target do-
main is the force protection, which includes sce-
narios of checkpoints, house searches, civil affairs,
medical, etc.
In what follows, we discuss the hardware and
software details of the headset-integrated transla-
tion device.
</bodyText>
<sectionHeader confidence="0.984843" genericHeader="introduction">
2 Hardware Platform
</sectionHeader>
<bodyText confidence="0.9999745">
The wearable S2S system described in this paper
runs on a headset-integrated computational plat-
form developed by Integrated Wave Technologies,
Inc. (IWT). The headset-integrated platform em-
ploys a 200 MHz StrongARM integer processor
with a total of just 32MB RAM available for both
the operating system and the translation software.
The operating system currently running on the
platform is Embedded Linux.
There are two audio cards on the headset plat-
form for two-way communication through separate
audio input and output channels. The default sound
card uses the headset integrated close-talking mi-
crophone as an audio input and the second audio
card can be used with an ambient microphone
mounted on the device or an external microphone.
In addition, each headset earpiece contains inner
and outer set of speakers. The inner earpiece
speakers are for the English speaking user who
wears the headset, whereas the outer speakers are
for the foreign language speaker who is not re-
quired to wear the headset.
</bodyText>
<sectionHeader confidence="0.987258" genericHeader="method">
3 Software Architecture
</sectionHeader>
<bodyText confidence="0.999716583333334">
Depicted in Figure 1 is the software system archi-
tecture for the headset-integrated wearable S2S
system. We are currently using a fixed-phrase Eng-
lish-to-Iraqi speech translation module from IWT
for translating from English to Iraqi. In the Iraqi-
to-English (I2E) direction, we use an n-gram ASR
engine to recognize Iraqi speech, a custom, phrase-
based “micro translator” for translating Iraqi text to
English text, and finally a TTS module for convert-
ing the English text into speech. The rest of this
paper focuses on the components of the Iraqi-to-
English translation module.
</bodyText>
<figureCaption confidence="0.995607">
Figure 1. Software architecture of the S2S system.
</figureCaption>
<bodyText confidence="0.993316405405405">
Fixed point ASR Engine: The ASR engine uses
phonetic hidden Markov models (HMM) with one
or more forms of the following parameter tying:
Phonetic-Tied Mixture (PTM), State-Tied Mixture
(STM), and State-Clustered-Tied Mixture (SCTM)
models.
For the headset-integrated platform, we use a
fixed-point ASR engine described in (Prasad et al.,
2007). As in (Prasad et al., 2007) for real-time per-
formance we use the compact PTM models in both
recognition passes of our two-pass ASR decoder.
Phrase-based Micro Translator: Phrase-based
statistical machine translation (SMT) has been
widely adopted as the translation engine in S2S
systems. Such SMT engines require only a large
corpus of bilingual sentence pairs to deliver robust
performance on the domain of that corpus. How-
ever, phrase-based SMT engines require significant
amount of memory, even when configured for me-
dium vocabulary tasks. Given the limited memory
on the headset platform, we chose to develop in-
stead a phrase-based “micro translator” module,
which acts like a bottom-up parser. The micro-
translator uses translation rules derived from our
phrase-based SMT engine. Rules are created auto-
matically by running the SMT engine on a small
training corpus and recording the phrase pairs it
used in decoding it. These phrase pairs then be-
come rules which are treated just as though they
had been written by hand. The micro translator
currently makes no use of probabilities. Instead, as
shown in Figure 2, for any given Arabic utterance,
the translator greedily chooses the longest match-
ing source phrase that does not overlap a source
phrase already chosen. The target phrases for these
source phrases are then output as the translation.
These target phrases come out in source-language
</bodyText>
<page confidence="0.998187">
11
</page>
<bodyText confidence="0.999790714285714">
order, as no language model is currently used for
reordering.
The micro translator currently consists of 1300
rules and 2000 words. Its memory footprint is just
32KB. This small memory footprint is achieved by
representing the rules in binary format rather than
text format.
</bodyText>
<figureCaption confidence="0.995494">
Figure 2. Decoding in micro translator.
</figureCaption>
<bodyText confidence="0.999831736842105">
English Playback using TTS: To play the Eng-
lish translation to the headset user we developed a
rudimentary TTS module. The TTS module parses
the output of the I2E translator to extract each
translated word. It then uses the list of extracted
words to read the appropriate pre-recorded (or syn-
thesized) audio. Once the word pronunciations au-
dio files are read we splice the beginning and the
end of the audio files to reduce the amount of si-
lence and concatenate them into a single file which
is then played to the user on the inner earphone
speakers.
The total memory footprint of our current Iraqi
to English translation module running on the head-
set-integrated platform is just 9MB. The current
configuration of the translation module’s Iraqi
ASR engine yields word error rate (WER) of 20%
on test-set utterances without out-of-vocabulary
(OOV) words.
</bodyText>
<sectionHeader confidence="0.99893" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.986068833333334">
In this paper we have presented the initial setup of
a speech-to-speech translation system configured
for the headset platform. Our current work is fo-
cused on expanding the vocabulary of the Iraqi-to-
English translation module by exploiting the rich
morphology of Iraqi Arabic. In particular, we are
investigating the use of morphemes (prefix, stems,
and suffixes) for expanding the effective vocabu-
lary of the headset translator. We are also develop-
ing use cases for performing a formal evaluation of
both the usability and performance of the headset
translator.
</bodyText>
<sectionHeader confidence="0.980592" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997667939393939">
Alex Waibel, Ahmed Badran, Alan W Black, Robert
Frederking, Donna Gates ,Alon Lavie, Lori Levin,
Kevin Lenzo, Laura Mayfield Tomokiyo, J¨urgen
Reichert, Tanja Schultz, Dorcas Wallace, Monika
Woszczyna and Jing Zhang. 2003. “Speechalator:
Two-way Speech-to-Speech Translation on a Con-
sumer PDA,” Proc. 8th European Conference on
Speech Communication and Technology
(EUROSPEECH 2003), Geneva, Switzerland.
Bowen Zhou, Daniel D´echelotte and Yuqing Gao.
2004. “Two-way Speech-to-Speech Translation on
Handheld Devices,” Proc. 8th International Confer-
ence on Spoken Language Processing, Jeju Island,
Korea.
David Stallard, Frederick Choi, Kriste Krstovski, Prem
Natarajan and Shirin Saleem. 2006. “A Hybrid
Phrase-based/Statistical Speech Translation System,”
Proc. The 9th International Conference on Spoken
Language Processing (Interspeech 2006 - ICSLP),
Pittsburg, PA.
David Stallard, John Makhoul, Frederick Choi, Ehry
Macrostie, Premkumar Natarajan, Richard Schwartz
and Bushra Zawaydeh. 2003. “Design and Evaluation
of a Limited two-way Speech Translator,” Proc. 8th
European Conference on Speech Communication and
Technology (EUROSPEECH 2003), Geneva, Swit-
zerland.
Rohit Prasad, Kriste Krstovski, Frederick Choi, Shirin
Saleem, Prem Natarajan, Michael Decerbo and David
Stallard. 2007. “Real-Time Speech-to-Speech Trans-
lation for PDAs,” Proc. IEEE International Confer-
ence on Portable Information Devices (IEEE Portable
2007), Orlando, FL.
</reference>
<page confidence="0.998462">
12
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000494">
<title confidence="0.999488">A Wearable Headset Speech-to-Speech Translation System</title>
<author confidence="0.7255365">Kriste Krstovski</author>
<author confidence="0.7255365">Michael Decerbo</author>
<author confidence="0.7255365">Rohit Prasad</author>
<author confidence="0.7255365">David Stallard</author>
<author confidence="0.7255365">Shirin Premkumar Natarajan</author>
<affiliation confidence="0.7344885">Speech and Language Processing Department BBN Technologies</affiliation>
<address confidence="0.999713">10 Moulton Street, Cambridge, MA, 02138</address>
<email confidence="0.999558">krstovski@bbn.com</email>
<email confidence="0.999558">mdecerbo@bbn.com</email>
<email confidence="0.999558">rprasad@bbn.com</email>
<email confidence="0.999558">stallard@bbn.com</email>
<email confidence="0.999558">ssaleem@bbn.com</email>
<email confidence="0.999558">prem@bbn.com</email>
<abstract confidence="0.998733808080809">In this paper we present a wearable, headset integrated eyesand hands-free speech-tospeech (S2S) translation system. The S2S system described here is configured for translingual communication between English and colloquial Iraqi Arabic. It employs an n-gram speech recognition engine, a rudimentary phrase-based translator for translating recognized Iraqi text, and a rudimentary text-tospeech (TTS) synthesis engine for playing back the English translation. This paper describes the system architecture, the functionality of its components, and the configurations of the speech recognition and machine translation engines. 1 Background Humanitarian personnel, military personnel, and visitors in foreign countries often need to communicate with residents of a host country. Human interpreters are inevitably in short supply, and training personnel to speak a new language is difficult. Under the DARPA TRANSTAC and Babylon programs, various teams have developed systems that enable two-way communication over a language barrier (Waibel et al., 2003; Zhou et al., 2004; Stallard et al., 2006). The two-way speechto-speech (S2S) translation systems seek, in principle, to translate any utterance, by using general statistical models trained on large amounts of speech and text data. The performance and usability of such two-way speech-to-speech (S2S) translation systems is 10 heavily dependent on the computational resources, such as processing power and memory, of the platform they are running on. To enable open-ended conversation these S2S systems employ powerful but highly memoryand computation-intensive statistical speech recognition and machine translation models. Thus, at the very minimum they require the processing and memory configuration of common-of-the-shelf (COTS) laptops. Unfortunately, most laptops do not have a form factor that is suitable for mobile users. The size, weight, and shape of laptops render them unsuitable for handheld use. Moreover, simply carrying the laptop can be infeasible for users, such as military personnel, who are already overburdened with other equipment. Embedded platforms, on the other hand, offer a more suitable form factor in terms of size and weight, but lack the computational resources required to run more open-ended 2-way S2S systems. In previous work, Prasad et al. (2007) reported on the development of a S2S system for Windows Mobile based handheld computers. To overcome the challenges posed by the limited resources of that platform, the PDA version of the S2S system was designed to be more constrained in terms of the ASR and MT vocabulary. As described in detail in (Prasad et al., 2007), the PDA based S2S system configured for English/Iraqi S2S translation delivers fairly accurate translation at faster than real-time. In this paper, we present ongoing development work on an S2S system that runs on an even more constrained hardware platform; namely, a processor embedded in a wearable headset with just 32 of memory. Compared to the PDA based sysof the ACL-08: HLT Workshop on Mobile Language pages 10–12, Ohio, USA, June 2008. Association for Computational Linguistics tem described in (Prasad et al., 2007), the wearable system is designed for both eyesand hands-free operation. The headset-integrated translation device described in this paper is configured for twoway conversation in English/Iraqi. The target domain is the force protection, which includes scenarios of checkpoints, house searches, civil affairs, medical, etc. In what follows, we discuss the hardware and software details of the headset-integrated translation device. 2 Hardware Platform The wearable S2S system described in this paper runs on a headset-integrated computational platform developed by Integrated Wave Technologies, Inc. (IWT). The headset-integrated platform employs a 200 MHz StrongARM integer processor with a total of just 32MB RAM available for both the operating system and the translation software. The operating system currently running on the platform is Embedded Linux. There are two audio cards on the headset platform for two-way communication through separate audio input and output channels. The default sound card uses the headset integrated close-talking microphone as an audio input and the second audio card can be used with an ambient microphone mounted on the device or an external microphone. In addition, each headset earpiece contains inner and outer set of speakers. The inner earpiece speakers are for the English speaking user who wears the headset, whereas the outer speakers are for the foreign language speaker who is not required to wear the headset. 3 Software Architecture Depicted in Figure 1 is the software system architecture for the headset-integrated wearable S2S system. We are currently using a fixed-phrase English-to-Iraqi speech translation module from IWT for translating from English to Iraqi. In the Iraqito-English (I2E) direction, we use an n-gram ASR engine to recognize Iraqi speech, a custom, phrasebased “micro translator” for translating Iraqi text to English text, and finally a TTS module for converting the English text into speech. The rest of this paper focuses on the components of the Iraqi-to- English translation module. Figure 1. Software architecture of the S2S system. point ASR Engine: ASR engine uses phonetic hidden Markov models (HMM) with one or more forms of the following parameter tying: Phonetic-Tied Mixture (PTM), State-Tied Mixture (STM), and State-Clustered-Tied Mixture (SCTM) models. For the headset-integrated platform, we use a fixed-point ASR engine described in (Prasad et al., 2007). As in (Prasad et al., 2007) for real-time performance we use the compact PTM models in both recognition passes of our two-pass ASR decoder. Micro Phrase-based statistical machine translation (SMT) has been widely adopted as the translation engine in S2S systems. Such SMT engines require only a large corpus of bilingual sentence pairs to deliver robust performance on the domain of that corpus. However, phrase-based SMT engines require significant amount of memory, even when configured for medium vocabulary tasks. Given the limited memory on the headset platform, we chose to develop instead a phrase-based “micro translator” module, which acts like a bottom-up parser. The microtranslator uses translation rules derived from our phrase-based SMT engine. Rules are created automatically by running the SMT engine on a small training corpus and recording the phrase pairs it used in decoding it. These phrase pairs then become rules which are treated just as though they had been written by hand. The micro translator currently makes no use of probabilities. Instead, as shown in Figure 2, for any given Arabic utterance, the translator greedily chooses the longest matching source phrase that does not overlap a source phrase already chosen. The target phrases for these source phrases are then output as the translation. These target phrases come out in source-language 11 order, as no language model is currently used for reordering. The micro translator currently consists of 1300 rules and 2000 words. Its memory footprint is just 32KB. This small memory footprint is achieved by representing the rules in binary format rather than text format. Figure 2. Decoding in micro translator. Playback using TTS: play the English translation to the headset user we developed a rudimentary TTS module. The TTS module parses the output of the I2E translator to extract each translated word. It then uses the list of extracted words to read the appropriate pre-recorded (or synthesized) audio. Once the word pronunciations audio files are read we splice the beginning and the end of the audio files to reduce the amount of silence and concatenate them into a single file which is then played to the user on the inner earphone speakers. The total memory footprint of our current Iraqi to English translation module running on the headset-integrated platform is just 9MB. The current configuration of the translation module’s Iraqi ASR engine yields word error rate (WER) of 20% on test-set utterances without out-of-vocabulary (OOV) words. 4 Conclusions and Future Work In this paper we have presented the initial setup of a speech-to-speech translation system configured for the headset platform. Our current work is focused on expanding the vocabulary of the Iraqi-to- English translation module by exploiting the rich morphology of Iraqi Arabic. In particular, we are investigating the use of morphemes (prefix, stems, and suffixes) for expanding the effective vocabulary of the headset translator. We are also developing use cases for performing a formal evaluation of both the usability and performance of the headset translator.</abstract>
<title confidence="0.862926">References</title>
<author confidence="0.98406">Alex Waibel</author>
<author confidence="0.98406">Ahmed Badran</author>
<author confidence="0.98406">Alan W Black</author>
<author confidence="0.98406">Robert</author>
<affiliation confidence="0.905699">Frederking, Donna Gates ,Alon Lavie, Lori Levin,</affiliation>
<address confidence="0.460863">Kevin Lenzo, Laura Mayfield Tomokiyo, J¨urgen</address>
<note confidence="0.740212">Reichert, Tanja Schultz, Dorcas Wallace, Monika Woszczyna and Jing Zhang. 2003. “Speechalator: Two-way Speech-to-Speech Translation on a Con- PDA,” Proc. European Conference on Speech Communication and Technology (EUROSPEECH 2003), Geneva, Switzerland. Bowen Zhou, Daniel D´echelotte and Yuqing Gao. 2004. “Two-way Speech-to-Speech Translation on Devices,” Proc. International Conference on Spoken Language Processing, Jeju Island, Korea. David Stallard, Frederick Choi, Kriste Krstovski, Prem</note>
<title confidence="0.403659">Natarajan and Shirin Saleem. 2006. “A Hybrid</title>
<author confidence="0.282953">Phrase-basedStatistical Speech Translation System</author>
<affiliation confidence="0.765607">The International Conference on Spoken Language Processing (Interspeech 2006 - ICSLP),</affiliation>
<address confidence="0.935988">Pittsburg, PA.</address>
<author confidence="0.9537185">David Stallard</author>
<author confidence="0.9537185">John Makhoul</author>
<author confidence="0.9537185">Frederick Choi</author>
<author confidence="0.9537185">Ehry Macrostie</author>
<author confidence="0.9537185">Premkumar Natarajan</author>
<author confidence="0.9537185">Richard Schwartz</author>
<affiliation confidence="0.768671">and Bushra Zawaydeh. 2003. “Design and Evaluation a Limited two-way Speech Translator,” Proc. European Conference on Speech Communication and</affiliation>
<note confidence="0.564977222222222">Technology (EUROSPEECH 2003), Geneva, Switzerland. Rohit Prasad, Kriste Krstovski, Frederick Choi, Shirin Saleem, Prem Natarajan, Michael Decerbo and David Stallard. 2007. “Real-Time Speech-to-Speech Translation for PDAs,” Proc. IEEE International Conference on Portable Information Devices (IEEE Portable 2007), Orlando, FL. 12</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Alex Waibel</author>
<author>Ahmed Badran</author>
<author>Alan W Black</author>
</authors>
<title>Robert Frederking, Donna Gates ,Alon Lavie,</title>
<date>2003</date>
<booktitle>Proc. 8th European Conference on Speech Communication and Technology (EUROSPEECH</booktitle>
<location>Lori Levin, Kevin Lenzo, Laura Mayfield Tomokiyo, J¨urgen Reichert, Tanja Schultz, Dorcas Wallace, Monika Woszczyna</location>
<contexts>
<context position="1378" citStr="Waibel et al., 2003" startWordPosition="191" endWordPosition="194">ack the English translation. This paper describes the system architecture, the functionality of its components, and the configurations of the speech recognition and machine translation engines. 1 Background Humanitarian personnel, military personnel, and visitors in foreign countries often need to communicate with residents of a host country. Human interpreters are inevitably in short supply, and training personnel to speak a new language is difficult. Under the DARPA TRANSTAC and Babylon programs, various teams have developed systems that enable two-way communication over a language barrier (Waibel et al., 2003; Zhou et al., 2004; Stallard et al., 2006). The two-way speechto-speech (S2S) translation systems seek, in principle, to translate any utterance, by using general statistical models trained on large amounts of speech and text data. The performance and usability of such two-way speech-to-speech (S2S) translation systems is 10 heavily dependent on the computational resources, such as processing power and memory, of the platform they are running on. To enable open-ended conversation these S2S systems employ powerful but highly memory- and computation-intensive statistical speech recognition and </context>
</contexts>
<marker>Waibel, Badran, Black, 2003</marker>
<rawString>Alex Waibel, Ahmed Badran, Alan W Black, Robert Frederking, Donna Gates ,Alon Lavie, Lori Levin, Kevin Lenzo, Laura Mayfield Tomokiyo, J¨urgen Reichert, Tanja Schultz, Dorcas Wallace, Monika Woszczyna and Jing Zhang. 2003. “Speechalator: Two-way Speech-to-Speech Translation on a Consumer PDA,” Proc. 8th European Conference on Speech Communication and Technology (EUROSPEECH 2003), Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bowen Zhou</author>
<author>Daniel D´echelotte</author>
<author>Yuqing Gao</author>
</authors>
<title>Two-way Speech-to-Speech Translation on Handheld Devices,”</title>
<date>2004</date>
<booktitle>Proc. 8th International Conference on Spoken Language Processing,</booktitle>
<location>Jeju Island,</location>
<marker>Zhou, D´echelotte, Gao, 2004</marker>
<rawString>Bowen Zhou, Daniel D´echelotte and Yuqing Gao. 2004. “Two-way Speech-to-Speech Translation on Handheld Devices,” Proc. 8th International Conference on Spoken Language Processing, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Stallard</author>
<author>Frederick Choi</author>
<author>Kriste Krstovski</author>
<author>Prem Natarajan</author>
<author>Shirin Saleem</author>
</authors>
<title>A Hybrid Phrase-based/Statistical Speech Translation System,”</title>
<date>2006</date>
<booktitle>Proc. The 9th International Conference on Spoken Language Processing (Interspeech</booktitle>
<location>Pittsburg, PA.</location>
<contexts>
<context position="1421" citStr="Stallard et al., 2006" startWordPosition="199" endWordPosition="202">describes the system architecture, the functionality of its components, and the configurations of the speech recognition and machine translation engines. 1 Background Humanitarian personnel, military personnel, and visitors in foreign countries often need to communicate with residents of a host country. Human interpreters are inevitably in short supply, and training personnel to speak a new language is difficult. Under the DARPA TRANSTAC and Babylon programs, various teams have developed systems that enable two-way communication over a language barrier (Waibel et al., 2003; Zhou et al., 2004; Stallard et al., 2006). The two-way speechto-speech (S2S) translation systems seek, in principle, to translate any utterance, by using general statistical models trained on large amounts of speech and text data. The performance and usability of such two-way speech-to-speech (S2S) translation systems is 10 heavily dependent on the computational resources, such as processing power and memory, of the platform they are running on. To enable open-ended conversation these S2S systems employ powerful but highly memory- and computation-intensive statistical speech recognition and machine translation models. Thus, at the ve</context>
</contexts>
<marker>Stallard, Choi, Krstovski, Natarajan, Saleem, 2006</marker>
<rawString>David Stallard, Frederick Choi, Kriste Krstovski, Prem Natarajan and Shirin Saleem. 2006. “A Hybrid Phrase-based/Statistical Speech Translation System,” Proc. The 9th International Conference on Spoken Language Processing (Interspeech 2006 - ICSLP), Pittsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Stallard</author>
<author>John Makhoul</author>
<author>Frederick Choi</author>
<author>Ehry Macrostie</author>
<author>Premkumar Natarajan</author>
<author>Richard Schwartz</author>
<author>Bushra Zawaydeh</author>
</authors>
<title>Design and Evaluation of a Limited two-way Speech Translator,”</title>
<date>2003</date>
<booktitle>Proc. 8th European Conference on Speech Communication and Technology (EUROSPEECH</booktitle>
<location>Geneva, Switzerland.</location>
<marker>Stallard, Makhoul, Choi, Macrostie, Natarajan, Schwartz, Zawaydeh, 2003</marker>
<rawString>David Stallard, John Makhoul, Frederick Choi, Ehry Macrostie, Premkumar Natarajan, Richard Schwartz and Bushra Zawaydeh. 2003. “Design and Evaluation of a Limited two-way Speech Translator,” Proc. 8th European Conference on Speech Communication and Technology (EUROSPEECH 2003), Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit Prasad</author>
<author>Kriste Krstovski</author>
<author>Frederick Choi</author>
<author>Shirin Saleem</author>
<author>Prem Natarajan</author>
<author>Michael Decerbo</author>
<author>David Stallard</author>
</authors>
<title>Real-Time Speech-to-Speech Translation for PDAs,”</title>
<date>2007</date>
<booktitle>Proc. IEEE International Conference on Portable Information Devices (IEEE Portable</booktitle>
<location>Orlando, FL.</location>
<contexts>
<context position="2666" citStr="Prasad et al. (2007)" startWordPosition="390" endWordPosition="393">he processing and memory configuration of common-of-the-shelf (COTS) laptops. Unfortunately, most laptops do not have a form factor that is suitable for mobile users. The size, weight, and shape of laptops render them unsuitable for handheld use. Moreover, simply carrying the laptop can be infeasible for users, such as military personnel, who are already overburdened with other equipment. Embedded platforms, on the other hand, offer a more suitable form factor in terms of size and weight, but lack the computational resources required to run more open-ended 2-way S2S systems. In previous work, Prasad et al. (2007) reported on the development of a S2S system for Windows Mobile based handheld computers. To overcome the challenges posed by the limited resources of that platform, the PDA version of the S2S system was designed to be more constrained in terms of the ASR and MT vocabulary. As described in detail in (Prasad et al., 2007), the PDA based S2S system configured for English/Iraqi S2S translation delivers fairly accurate translation at faster than real-time. In this paper, we present ongoing development work on an S2S system that runs on an even more constrained hardware platform; namely, a processo</context>
<context position="6014" citStr="Prasad et al., 2007" startWordPosition="917" endWordPosition="920"> “micro translator” for translating Iraqi text to English text, and finally a TTS module for converting the English text into speech. The rest of this paper focuses on the components of the Iraqi-toEnglish translation module. Figure 1. Software architecture of the S2S system. Fixed point ASR Engine: The ASR engine uses phonetic hidden Markov models (HMM) with one or more forms of the following parameter tying: Phonetic-Tied Mixture (PTM), State-Tied Mixture (STM), and State-Clustered-Tied Mixture (SCTM) models. For the headset-integrated platform, we use a fixed-point ASR engine described in (Prasad et al., 2007). As in (Prasad et al., 2007) for real-time performance we use the compact PTM models in both recognition passes of our two-pass ASR decoder. Phrase-based Micro Translator: Phrase-based statistical machine translation (SMT) has been widely adopted as the translation engine in S2S systems. Such SMT engines require only a large corpus of bilingual sentence pairs to deliver robust performance on the domain of that corpus. However, phrase-based SMT engines require significant amount of memory, even when configured for medium vocabulary tasks. Given the limited memory on the headset platform, we ch</context>
</contexts>
<marker>Prasad, Krstovski, Choi, Saleem, Natarajan, Decerbo, Stallard, 2007</marker>
<rawString>Rohit Prasad, Kriste Krstovski, Frederick Choi, Shirin Saleem, Prem Natarajan, Michael Decerbo and David Stallard. 2007. “Real-Time Speech-to-Speech Translation for PDAs,” Proc. IEEE International Conference on Portable Information Devices (IEEE Portable 2007), Orlando, FL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>