<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.990165">
Building Comparable Corpora Based on Bilingual LDA Model
</title>
<author confidence="0.997002">
Zede Zhu
</author>
<affiliation confidence="0.84832225">
University of Science and Technology
of China, Institute of Intelligent Ma-
chines Chinese Academy of Sciences
Hefei, China
</affiliation>
<email confidence="0.997575">
zhuzede@mail.ustc.edu.cn
</email>
<author confidence="0.982882">
Miao Li, Lei Chen, Zhenxin Yang
</author>
<affiliation confidence="0.854923">
Institute of Intelligent Machines Chinese
Academy of Sciences
Hefei, China
</affiliation>
<email confidence="0.9745925">
mli@iim.ac.cn,alan.cl@163.com,
xinzyang@mail.ustc.edu.cn
</email>
<sectionHeader confidence="0.993697" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999605785714286">
Comparable corpora are important basic re-
sources in cross-language information pro-
cessing. However, the existing methods of
building comparable corpora, which use inter-
translate words and relative features, cannot
evaluate the topical relation between document
pairs. This paper adopts the bilingual LDA
model to predict the topical structures of the
documents and proposes three algorithms of
document similarity in different languages.
Experiments show that the novel method can
obtain similar documents with consistent top-
ics own better adaptability and stability per-
formance.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999986235294118">
Comparable corpora can be mined fine-grained
translation equivalents, such as bilingual termi-
nologies, named entities and parallel sentences,
to support the bilingual lexicography, statistical
machine translation and cross-language infor-
mation retrieval (AbduI-Rauf et al., 2009). Com-
parable corpora are defined as pairs of monolin-
gual corpora selected according to the criteria of
content similarity but non-direct translation in
different languages, which reduces limitation of
matching source language and target language
documents. Thus comparable corpora have the
advantage over parallel corpora in which they are
more up-to-date, abundant and accessible (Ji,
2009).
Many works, which focused on the exploita-
tion of building comparable corpora, were pro-
posed in the past years. Tao et al. (2005) ac-
quired comparable corpora based on the truth
that terms are inter-translation in different lan-
guages if they have similar frequency correlation
at the same time periods. Talvensaari et al. (2007)
extracted appropriate keywords from the source
language documents and translated them into the
target language, which were regarded as the que-
ry words to retrieve similar target documents.
Thuy et al. (2009) analyzed document similarity
based on the publication dates, linguistic inde-
pendent units, bilingual dictionaries and word
frequency distributions. Otero et al. (2010) took
advantage of the translation equivalents inserted
in Wikipedia by means of interlanguage links to
extract similar articles. Bo et al. (2010) proposed
a comparability measure based on the expecta-
tion of finding the translation for each word.
The above studies rely on the high coverage of
the original bilingual knowledge and a specific
data source together with the translation vocabu-
laries, co-occurrence information and language
links. However, the severest problem is that they
cannot understand semantic information. The
new studies seek to match similar documents on
topic level to solve the traditional problems. Pre-
iss (2012) transformed the source language topi-
cal model to the target language and classified
probability distribution of topics in the same lan-
guage, whose shortcoming is that the effect of
model translation seriously hampers the compa-
rable corpora quality. Ni et al. (2009) adapted
monolingual topic model to bilingual topic mod-
el in which the documents of a concept unit in
different languages were assumed to share iden-
tical topic distribution. Bilingual topic model is
widely adopted to mine translation equivalents
from multi-language documents (Mimno et al.,
2009; Ivan et al., 2011).
Based on the bilingual topic model, this paper
predicts the topical structure of documents in
different languages and calculates the similarity
of topics over documents to build comparable
corpora. The paper concretely includes: 1) Intro-
duce the Bilingual LDA (Latent Dirichlet Alloca-
tion) model which builds comparable corpora
and improves the efficiency of matching similar
documents; 2) Design a novel method of TFIDF
(Topic Frequency-Inverse Document Frequency)
to enhance the distinguishing ability of topics
from different documents; 3) Propose a tailored
</bodyText>
<page confidence="0.956795">
278
</page>
<bodyText confidence="0.792880714285714">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 278–282,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
method of conditional probability to calculate
document similarity; 4) Address a language-
independent study which isn’t limited to a par-
ticular data source in any language.
token as word v from a topic k. For new collec-
</bodyText>
<equation confidence="0.93528380952381">

tion of documents M , keeping k, v , the distri-
bution l k of sampling a topic k from document
m,
m can be obtained as follows:
k 1
(
k  |ml )
n ( )
k  
m  l k
  
m k
 l , K

( )
k
( n  )
m  l k
P
, (1)
</equation>
<sectionHeader confidence="0.983457" genericHeader="method">
2 Bilingual LDA Model
</sectionHeader>
<subsectionHeader confidence="0.990114">
2.1 Standard LDA
</subsectionHeader>
<bodyText confidence="0.998629">
LDA model (Blei et al., 2003) represents the la-
tent topic of the document distribution by Di-
richlet distribution with a K-dimensional implicit
random variable, which is transformed into a
complete generative model when  is exerted to
Dirichlet distribution (Griffiths et al., 2004)
(Shown in Fig. 1),
</bodyText>
<figureCaption confidence="0.998959">
Figure 1: Standard LDA model
</figureCaption>
<bodyText confidence="0.99848825">
where  and  denote the parameters distribut-
ed by Dirichlet; K denotes the topic numbers; k
denotes the vocabulary probability distribution in
the topic k; M denotes the document number; m
denotes the topic probability distribution in the
document m; Nm denotes the length of m; m,n
and ωm,n denote the topic and the word in m re-
spectively.
</bodyText>
<subsectionHeader confidence="0.998579">
2.2 Bilingual LDA
</subsectionHeader>
<bodyText confidence="0.999599142857143">
Bilingual LDA is a bilingual extension of a
standard LDA model. It takes advantage of the
document alignment which shares the same topic
distribution m and uses different word distribu-
tions for each topic (Shown in Fig. 2), where S
and T denote source language and target lan-
guage respectively.
</bodyText>
<figureCaption confidence="0.967916">
Figure 2: Bilingual LDA model
</figureCaption>
<equation confidence="0.760815285714286">
For each language l ( l  {S, T } ), ,
m n and
l m n are drawn using , (  |)
l m n  P n m and
l l
,  m n P(
n  |m,n ,  ) . l l l ,
</equation>
<bodyText confidence="0.969512">
Giving the comparable corpora M, the distri-
bution k, v can be obtained by sampling a new
generate the bilingual topic model
v from the
given bilingual corpora, predict the topic distri-
bution
similarity of documents and select the largest
similar document pairs. The key step is that the
document similarity is calculated to align the
source language document
with relevant
target language document
.
As one general way of expressing similarity,
the Kullback-Leibler (KL) Divergence is adopted
to measure the document similari
</bodyText>
<equation confidence="0.709728">
k,
,
</equation>
<bodyText confidence="0.7087">
 of the new documents, calculate the
</bodyText>
<equation confidence="0.9902854">
m k
l
S
m
T
m
ty by topic dis-
dT as follows:
m ,k
( , ) [ (  |), (  |)]
m m KL P m P m
S T S
  T
   

</equation>
<bodyText confidence="0.82500275">
(2)
.
The remainder section focuses on other two
methods of calculating document similarity.
</bodyText>
<subsectionHeader confidence="0.828469">
Cosine Similarity
</subsectionHeader>
<bodyText confidence="0.999129857142857">
The similarity between
and
can be meas-
ured by Topic Frequency-Inverse Document
Frequency. It gives high weights to the topic
which appears frequently in a specific document
an
</bodyText>
<equation confidence="0.9241842">
3.1
S
m
T
m
</equation>
<bodyText confidence="0.9943465">
d rarely appears in other documents. Then the
relation between
</bodyText>
<subsectionHeader confidence="0.5790745">
Frequency (Manning et
Topic Fre-
</subsectionHeader>
<bodyText confidence="0.468711">
quency (TF) denoting frequency of topic
</bodyText>
<figure confidence="0.892955230769231">
for
the document
is denoted by P(
). Given
a constant value
Inverse
al.,1999),

l
m
|ml
 ,
Document Frequency
</figure>
<figureCaption confidence="0.233104">
(IDF) is defined as the total number of docu-
</figureCaption>
<bodyText confidence="0.936776333333333">

where n(
) denotes the total number of times

m
that the document m is assigned to the topic k.
3 Building comparable corpora
Based on the bilingual LDA model, building
comparable corpora includes several steps to
measured by Cosine Similarity (CS).
Similar to Term Frequency-Inverse Document
divided by the number of documents
</bodyText>
<equation confidence="0.685685692307692">
 m m, n
n [1,Nm]
m [1,M]
 m, n
 k
k  [1, K ]


m
m,n
m,n
m,n
m,n
n  Nm
S [1, ]
S
n  N m
T [1, ]
T
m[1,M]
k [1, K]
S
k
k T


S
T
mS,k
an
tributions  
SimKL
K 
 
     S log S T
,   ,
  , 
m k m k m k
k  1
</equation>
<figure confidence="0.873421743243243">
TFIDF
S
an


d
m,
TFIDF
is
T
m
,
ments M
279

1ml :P( |ml)
1
k
P ()P(mT I k).
1
k
1
k
,
F
. (11)
ml : P(  |ml)   containing a particular topic,
and then taking the logarithm, which is calculat-
ed as follows:
1ml : P( |m)
l
. (3)

�
M
log
IDF
The TFIDF is calculated as follows:
TFIDF TF IDF
 *
�
M
P(  |ml) log
Thus, the TFIDF score of the topic k over
document l
m� is given by:
The similarity between S
m� and T
m� is given by:
Sim CS (Fn S, Fn T ) Cos (TFIDF S ,TFIDF T )
K
TFIDF
m k
� l,
�
M


�
M

log
m k
� l,
1  m_l:
mlk ,
1ml :P( k|ml)
P(k  |ml) log
. (5)
. (6)
3.2 Conditional Probability
The similarity between S
m� and T
m� is defined as
</figure>
<figureCaption confidence="0.2466665">
the Conditional Probability (CP) of documents
P(mT  |m�s) that mT will be generated as a re-
</figureCaption>
<bodyText confidence="0.928015714285714">
sponse to the cue S
m� .
P() as prior topic distribution is assumed a
uniform distribution and satisfied the condition
P(k)  P() . According to the total probabil-
ity formula, the document T
m� is given as:
</bodyText>
<equation confidence="0.999024">
K
P(mT)  P(mT  |k)P(k)
K
</equation>
<bodyText confidence="0.999559666666667">
Based on the Bayesian formula, the probabil-
ity that a given topic  is assigned to a particu-
lar target language document T
</bodyText>
<equation confidence="0.982745">
m� is expressed:
(AT
)  [P(  |mT )P(mT )] P()
K
=P(Z  |mT ) P(mT  |k )
P

P
(m-  |T
k)
</equation>
<note confidence="0.413009">
The sum of all probabilities
</note>
<figure confidence="0.993218923076923">
1
k
that all topics  are assigned to a particular doc-
ument T
m� is a constant , thus equation (8) is
converted as follows:
P(mT  |)  P(  |m- T) . (9)
According to the total probability formula, the
similarity between S
m� and T
m� is given by:
(mS,mT)  P(mT  |mS)
  [P(m T  |k)P(k  |mS )]
k
1

CP
K
Sim
K
K
[P( k|mT )P(k  |mS)]
[-S-T ].
m ,k m ,k

4 Experiments and analysis
</figure>
<subsectionHeader confidence="0.56549">
4.1 Datasets and Evaluation
</subsectionHeader>
<bodyText confidence="0.995330291666667">
The experiments are conducted on two sets of
Chinese-English comparable corpora. The first
dataset is news corpora with 3254 comparable
document pairs, from which 200 pairs are ran-
domly selected as the test dataset News-Test and
the remainder is the training dataset News-Train.
The second dataset contains 8317 bilingual Wik-
ipedia entry pairs, from which 200 pairs are ran-
domly selected as the test dataset Wiki-Test and
the remainder is the training dataset Wiki-Train.
Then News-Train and Wiki-Train are merged
into the training dataset NW-Train. And the
hand-labeled gold standard namely NW-Test is
composed of News-Test and Wiki-Test.
Braschler et al. (1998) used five levels of rele-
vance to assess the alignments as follows: Same
Story, Related Story, Shared Aspect, Common
Terminology and Unrelated. The paper selects
the documents with Same Story and Related Sto-
ry as comparable corpora. Let Cp be the compa-
rable corpora in the building result and Cl be the
comparable corpora in the labeled result. The
Precision (P), Recall (R) and F-measure (F) are
defined as:
</bodyText>
<figure confidence="0.876501083333333">
P=CpI ICl R CpI ICl
Cp
Cl
. (4)
(10)
k
1

TFIDF S kTFIDF T m ,k
k
1
K K
TFIDF m kTFIDFm ,k
k 1 k1

  
1
k
K

 
2PR
P R

</figure>
<subsectionHeader confidence="0.342514">
4.2 Results and analysis
</subsectionHeader>
<bodyText confidence="0.5320045">
Two groups of validation experiments are set
with sampling frequency of 1000, parameter 
</bodyText>
<page confidence="0.978446">
280
</page>
<table confidence="0.9833956">
Train Test KL CS CP
P F P F P F
Iews Iews 0.62 0.52 0.73 0.59 0.69 0.56
Iews Wiki 0.60 0.47 0.68 0.56 0.66 0.52
Wiki Iews 0.61 0.48 0.71 0.58 0.68 0.55
Wiki Wiki 0.63 0.50 0.75 0.60 0.71 0.59
IW IW 0.66 0.55 0.76 0.62 0.73 0.60
of 50/K, parameter  of 0.01 and topic number
K of 600.
Group 1: Different data source
</table>
<bodyText confidence="0.999044333333333">
We learn bilingual LDA models by taking differ-
ent training datasets. The performance of three
approaches (KL, CS and CP) is examined on dif-
ferent test datasets. Tab. 1 demonstrates these
results with the winners for each algorithm in
bold.
</bodyText>
<tableCaption confidence="0.993833">
Table 1: Sensitivity of Data Source
</tableCaption>
<bodyText confidence="0.999974172413793">
The results indicate the robustness and effec-
tiveness of these algorithms. The performance of
algorithms on Wiki-Train is much better than
News-Train. The main reason is that Wiki-Train
is an extensive snapshot of human knowledge
which can cover most topics talked in News-
Train. The probability of vocabularies among the
test dataset which have not appeared in the train-
ing data is very low. And then the document top-
ic can effectively concentrate all the vocabular-
ies’ expressions. The topic model slightly faces
with the problem of knowledge migration issue,
so the performance of the topic model trained by
Wiki-Train shows a slight decline in the experi-
ments on News-Test.
CS shows the strongest performance among
the three algorithms to recognize the document
pairs with similar topics. CP has almost equiva-
lent performance with CS. Comparing the equa-
tion (5) and (6) with (10), we can find out that
CP is similar to a simplified CS. CP can improve
the operating efficiency and decrease the perfor-
mance. The performance achieved by KL is the
weakest and there is a large gap between KL and
others. In addition, the shortage of KL is that
when the exchange between the source language
and the target language documents takes place,
different evaluations will occur in the same doc-
ument pairs.
</bodyText>
<subsectionHeader confidence="0.608314">
Group 2: Existing Methods Comparison
</subsectionHeader>
<bodyText confidence="0.999939428571429">
We adopt the NW-Train and NW-Test as training
set and test set respectively, and utilize the CS
algorithm to calculate the document similarity to
verify the excellence of methods in the study.
Then we compare its performance with the exist-
ing representative approaches proposed by Thuy
et al. (2009) and Preiss (2012) (Shown in Tab. 2).
</bodyText>
<table confidence="0.9996485">
Algorithm P R F
Thuy 0.45 0.32 0.37
Preiss 0.67 0.44 0.53
CS 0.76 0.53 0.62
</table>
<tableCaption confidence="0.997855">
Table 2: Existing Methods Comparison
</tableCaption>
<bodyText confidence="0.999981066666667">
The table shows CS outperforms other algo-
rithms, which indicates that bilingual LDA is
valid to construct comparable corpora. Thuy et al.
(2009) matches similar documents in the view of
inter-translated vocabulary and co-occurrence
information features, which cannot understand
the content effectively. Preiss (2012) uses mono-
lingual training dataset to generate topic model
and translates source language topic model into
target language topic model respectively. Yet the
translation accuracy constrains the matching ef-
fectiveness of similar documents, and the cosine
similarity is directly used to calculate document-
topic similarity failing to highlight the topic con-
tributions of different documents.
</bodyText>
<sectionHeader confidence="0.998835" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999970619047619">
This study proposes a new method of using bi-
lingual topic to match similar documents. When
CS is used to match the documents, TFIDF is
proposed to enhance the topic discrepancies
among different documents. The method of CP is
also addressed to measure document similarity.
Experimental results show that the matching
algorithm is superior to the existing algorithms.
It can utilize comprehensively large scales of
document information in training set to avoid the
information deficiency of the document itself and
over-reliance on bilingual knowledge. The algo-
rithm makes the document match on the basis of
understanding the document. This study does not
calculate similar contents existed in the monolin-
gual documents. However, a large number of
documents in the same language describe the
same event. We intend to incorporate monolin-
gual document similarity into bilingual topics
analysis to match multi-documents in different
languages perfectly.
</bodyText>
<sectionHeader confidence="0.999468" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<reference confidence="0.586925">
The work is supported by the National Natural
Science Foundation of China under No.
61070099 and the project of MSR-CNIC Win-
dows Azure Theme.
</reference>
<page confidence="0.997997">
281
</page>
<note confidence="0.97171">
References national conference on World wide web. ACM,
</note>
<reference confidence="0.994138365079365">
2009: 1155-1156.
AbduI-Rauf S, Schwenk H. On the use of comparable
corpora to improve SMT perfor-
mance[C]//Proceedings of the 12th Conference of
the European Chapter of the Association for Com-
putational Linguistics. Association for Computa-
tional Linguistics, 2009: 16-23.
Ji H. Mining name translations from comparable cor-
pora by creating bilingual information networks[C]
// Proceedings of BUCC 2009. Suntec, Singapore,
2009: 34-37.
Braschler M, Schauble P. Multilingual Information
Retrieval based on document alignment tech-
niques[C] // Proceedings of the Second European
Conference on Research and Advanced Technolo-
gy for Digital Libraries. Heraklion, Greece. 1998:
183-197.
Tao Tao, Chengxiang Zhai. Mining comparable bilin-
gual text corpora for cross-language information
integration[C] // Proceedings of ACM SIGKDD,
Chicago, Illinois, USA. 2005:691-696.
Talvensaari T, Laurikkala J, Jarvelin K, et al. Creating
and Exploiting a Comparable Corpus in Cross-
Language Information Retrieval[J]. ACM Transac-
tions on Information Systems. 2007, 25(1): 322-
334.
Thuy Vu, Ai Ti Aw, Min Zhang. Feature-based meth-
od for document alignment in comparable news
corpora[C] // Proceedings of the 12th Conference
of the European Chapter of the ACL, Athens,
Greece. 2009: 843-851.
Otero P G, L’opez I G. Wikipedia as Multilingual
Source of Comparable Corpora[C] // Proceedings
of the 3rd Workshop on BUCC, LREC2010. Malta.
2010: 21-25.
Li B, Gaussier E. Improving corpus comparability for
bilingual lexicon extraction from comparable cor-
pora[C]//Proceedings of the 23rd International
Conference on Computational Linguistics. Associ-
ation for Computational Linguistics, 2010: 644-652.
Judita Preiss. Identifying Comparable Corpora Using
LDA[C]//2012 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies. Mon-
tre´al, Canada, June 3-8, 2012: 558-562.
Mimno D, Wallach H, Naradowsky J et al. Polylin-
gual topic models[C]//Proceedings of the EMNLP.
Singapore, 2009: 880-889.
Vulic I, De Smet W, Moens M F, et al. Identifying
word translations from comparable corpora using
latent topic models[C]//Proceedings of ACL. 2011:
479-484.
Ni X, Sun J T, Hu J, et al. Mining multilingual topics
from wikipedia[C]//Proceedings of the 18th inter-
Blei D M, Ng A Y, Jordan M I. Latent dirichlet allo-
cation[J]. the Journal of machine Learning research,
2003, 3: 993-1022.
Griffiths T L, Steyvers M. Finding scientific topics[J].
Proceedings of the National academy of Sciences
of the United States of America, 2004, 101: 5228-
5235.
Manning C D, Schütze H. Foundations of statistical
natural language processing[M]. MIT press, 1999.
</reference>
<page confidence="0.99744">
282
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.238880">
<title confidence="0.997477">Building Comparable Corpora Based on Bilingual LDA Model</title>
<author confidence="0.863594">Zede</author>
<affiliation confidence="0.962307333333333">University of Science and China, Institute of Intelligent chines Chinese Academy of</affiliation>
<address confidence="0.944449">Hefei, China</address>
<email confidence="0.967126">zhuzede@mail.ustc.edu.cn</email>
<author confidence="0.997033">Miao Li</author>
<author confidence="0.997033">Lei Chen</author>
<author confidence="0.997033">Zhenxin</author>
<affiliation confidence="0.858604333333333">Institute of Intelligent Machines Academy of Hefei, China</affiliation>
<email confidence="0.8749085">mli@iim.ac.cn,alan.cl@163.com,xinzyang@mail.ustc.edu.cn</email>
<abstract confidence="0.986864333333333">Comparable corpora are important basic resources in cross-language information processing. However, the existing methods of building comparable corpora, which use intertranslate words and relative features, cannot evaluate the topical relation between document pairs. This paper adopts the bilingual LDA model to predict the topical structures of the documents and proposes three algorithms of document similarity in different languages. show that the can obtain similar documents with consistent topics own better adaptability and stability performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>The work is supported by the National Natural Science Foundation of China under No. 61070099 and the project of MSR-CNIC Windows Azure Theme.</title>
<date>2009</date>
<pages>1155--1156</pages>
<contexts>
<context position="2187" citStr="(2009)" startWordPosition="308" endWordPosition="308"> in which they are more up-to-date, abundant and accessible (Ji, 2009). Many works, which focused on the exploitation of building comparable corpora, were proposed in the past years. Tao et al. (2005) acquired comparable corpora based on the truth that terms are inter-translation in different languages if they have similar frequency correlation at the same time periods. Talvensaari et al. (2007) extracted appropriate keywords from the source language documents and translated them into the target language, which were regarded as the query words to retrieve similar target documents. Thuy et al. (2009) analyzed document similarity based on the publication dates, linguistic independent units, bilingual dictionaries and word frequency distributions. Otero et al. (2010) took advantage of the translation equivalents inserted in Wikipedia by means of interlanguage links to extract similar articles. Bo et al. (2010) proposed a comparability measure based on the expectation of finding the translation for each word. The above studies rely on the high coverage of the original bilingual knowledge and a specific data source together with the translation vocabularies, co-occurrence information and lang</context>
<context position="13031" citStr="(2009)" startWordPosition="2324" endWordPosition="2324">hieved by KL is the weakest and there is a large gap between KL and others. In addition, the shortage of KL is that when the exchange between the source language and the target language documents takes place, different evaluations will occur in the same document pairs. Group 2: Existing Methods Comparison We adopt the NW-Train and NW-Test as training set and test set respectively, and utilize the CS algorithm to calculate the document similarity to verify the excellence of methods in the study. Then we compare its performance with the existing representative approaches proposed by Thuy et al. (2009) and Preiss (2012) (Shown in Tab. 2). Algorithm P R F Thuy 0.45 0.32 0.37 Preiss 0.67 0.44 0.53 CS 0.76 0.53 0.62 Table 2: Existing Methods Comparison The table shows CS outperforms other algorithms, which indicates that bilingual LDA is valid to construct comparable corpora. Thuy et al. (2009) matches similar documents in the view of inter-translated vocabulary and co-occurrence information features, which cannot understand the content effectively. Preiss (2012) uses monolingual training dataset to generate topic model and translates source language topic model into target language topic mode</context>
</contexts>
<marker>2009</marker>
<rawString>The work is supported by the National Natural Science Foundation of China under No. 61070099 and the project of MSR-CNIC Windows Azure Theme. 2009: 1155-1156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S AbduI-Rauf</author>
<author>H Schwenk</author>
</authors>
<title>On the use of comparable corpora to improve</title>
<date>2009</date>
<booktitle>SMT performance[C]//Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics,</booktitle>
<pages>16--23</pages>
<marker>AbduI-Rauf, Schwenk, 2009</marker>
<rawString>AbduI-Rauf S, Schwenk H. On the use of comparable corpora to improve SMT performance[C]//Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, 2009: 16-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ji</author>
</authors>
<title>Mining name translations from comparable corpora by creating bilingual information networks[C]</title>
<date>2009</date>
<booktitle>Proceedings of BUCC 2009. Suntec,</booktitle>
<pages>34--37</pages>
<contexts>
<context position="1651" citStr="Ji, 2009" startWordPosition="221" endWordPosition="222">valents, such as bilingual terminologies, named entities and parallel sentences, to support the bilingual lexicography, statistical machine translation and cross-language information retrieval (AbduI-Rauf et al., 2009). Comparable corpora are defined as pairs of monolingual corpora selected according to the criteria of content similarity but non-direct translation in different languages, which reduces limitation of matching source language and target language documents. Thus comparable corpora have the advantage over parallel corpora in which they are more up-to-date, abundant and accessible (Ji, 2009). Many works, which focused on the exploitation of building comparable corpora, were proposed in the past years. Tao et al. (2005) acquired comparable corpora based on the truth that terms are inter-translation in different languages if they have similar frequency correlation at the same time periods. Talvensaari et al. (2007) extracted appropriate keywords from the source language documents and translated them into the target language, which were regarded as the query words to retrieve similar target documents. Thuy et al. (2009) analyzed document similarity based on the publication dates, li</context>
</contexts>
<marker>Ji, 2009</marker>
<rawString>Ji H. Mining name translations from comparable corpora by creating bilingual information networks[C] // Proceedings of BUCC 2009. Suntec, Singapore, 2009: 34-37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Braschler</author>
<author>P Schauble</author>
</authors>
<title>Multilingual Information Retrieval based on document alignment techniques[C]</title>
<date>1998</date>
<booktitle>Proceedings of the Second European Conference on Research and Advanced Technology for Digital Libraries. Heraklion,</booktitle>
<pages>183--197</pages>
<marker>Braschler, Schauble, 1998</marker>
<rawString>Braschler M, Schauble P. Multilingual Information Retrieval based on document alignment techniques[C] // Proceedings of the Second European Conference on Research and Advanced Technology for Digital Libraries. Heraklion, Greece. 1998: 183-197.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Tao Tao</author>
</authors>
<title>Chengxiang Zhai. Mining comparable bilingual text corpora for cross-language information integration[C]</title>
<booktitle>Proceedings of ACM SIGKDD,</booktitle>
<pages>2005--691</pages>
<location>Chicago, Illinois, USA.</location>
<marker>Tao, </marker>
<rawString>Tao Tao, Chengxiang Zhai. Mining comparable bilingual text corpora for cross-language information integration[C] // Proceedings of ACM SIGKDD, Chicago, Illinois, USA. 2005:691-696.</rawString>
</citation>
<citation valid="false">
<authors>
<author>T Talvensaari</author>
<author>J Laurikkala</author>
<author>K Jarvelin</author>
</authors>
<title>Creating and Exploiting a Comparable Corpus in CrossLanguage Information Retrieval[J].</title>
<journal>ACM Transactions on Information Systems.</journal>
<volume>25</volume>
<issue>1</issue>
<pages>322--334</pages>
<marker>Talvensaari, Laurikkala, Jarvelin, </marker>
<rawString>Talvensaari T, Laurikkala J, Jarvelin K, et al. Creating and Exploiting a Comparable Corpus in CrossLanguage Information Retrieval[J]. ACM Transactions on Information Systems. 2007, 25(1): 322-334.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thuy Vu</author>
</authors>
<title>Ai Ti Aw, Min Zhang. Feature-based method for document alignment in comparable news corpora[C]</title>
<date>2009</date>
<booktitle>Proceedings of the 12th Conference of the European Chapter of the ACL,</booktitle>
<pages>843--851</pages>
<location>Athens,</location>
<marker>Vu, 2009</marker>
<rawString>Thuy Vu, Ai Ti Aw, Min Zhang. Feature-based method for document alignment in comparable news corpora[C] // Proceedings of the 12th Conference of the European Chapter of the ACL, Athens, Greece. 2009: 843-851.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P G Otero</author>
<author>I G L’opez</author>
</authors>
<title>Wikipedia as Multilingual Source of Comparable Corpora[C]</title>
<date>2010</date>
<booktitle>Proceedings of the 3rd Workshop on BUCC, LREC2010.</booktitle>
<pages>21--25</pages>
<marker>Otero, L’opez, 2010</marker>
<rawString>Otero P G, L’opez I G. Wikipedia as Multilingual Source of Comparable Corpora[C] // Proceedings of the 3rd Workshop on BUCC, LREC2010. Malta. 2010: 21-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Li</author>
<author>E Gaussier</author>
</authors>
<title>Improving corpus comparability for bilingual lexicon extraction from</title>
<date>2010</date>
<booktitle>comparable corpora[C]//Proceedings of the 23rd International Conference on Computational Linguistics. Association for Computational Linguistics,</booktitle>
<pages>644--652</pages>
<marker>Li, Gaussier, 2010</marker>
<rawString>Li B, Gaussier E. Improving corpus comparability for bilingual lexicon extraction from comparable corpora[C]//Proceedings of the 23rd International Conference on Computational Linguistics. Association for Computational Linguistics, 2010: 644-652.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judita Preiss</author>
</authors>
<title>Identifying Comparable Corpora Using LDA[C]//2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</title>
<date>2012</date>
<pages>558--562</pages>
<location>Montre´al, Canada,</location>
<contexts>
<context position="2993" citStr="Preiss (2012)" startWordPosition="425" endWordPosition="427">anslation equivalents inserted in Wikipedia by means of interlanguage links to extract similar articles. Bo et al. (2010) proposed a comparability measure based on the expectation of finding the translation for each word. The above studies rely on the high coverage of the original bilingual knowledge and a specific data source together with the translation vocabularies, co-occurrence information and language links. However, the severest problem is that they cannot understand semantic information. The new studies seek to match similar documents on topic level to solve the traditional problems. Preiss (2012) transformed the source language topical model to the target language and classified probability distribution of topics in the same language, whose shortcoming is that the effect of model translation seriously hampers the comparable corpora quality. Ni et al. (2009) adapted monolingual topic model to bilingual topic model in which the documents of a concept unit in different languages were assumed to share identical topic distribution. Bilingual topic model is widely adopted to mine translation equivalents from multi-language documents (Mimno et al., 2009; Ivan et al., 2011). Based on the bili</context>
<context position="13049" citStr="Preiss (2012)" startWordPosition="2326" endWordPosition="2327">L is the weakest and there is a large gap between KL and others. In addition, the shortage of KL is that when the exchange between the source language and the target language documents takes place, different evaluations will occur in the same document pairs. Group 2: Existing Methods Comparison We adopt the NW-Train and NW-Test as training set and test set respectively, and utilize the CS algorithm to calculate the document similarity to verify the excellence of methods in the study. Then we compare its performance with the existing representative approaches proposed by Thuy et al. (2009) and Preiss (2012) (Shown in Tab. 2). Algorithm P R F Thuy 0.45 0.32 0.37 Preiss 0.67 0.44 0.53 CS 0.76 0.53 0.62 Table 2: Existing Methods Comparison The table shows CS outperforms other algorithms, which indicates that bilingual LDA is valid to construct comparable corpora. Thuy et al. (2009) matches similar documents in the view of inter-translated vocabulary and co-occurrence information features, which cannot understand the content effectively. Preiss (2012) uses monolingual training dataset to generate topic model and translates source language topic model into target language topic model respectively. Ye</context>
</contexts>
<marker>Preiss, 2012</marker>
<rawString>Judita Preiss. Identifying Comparable Corpora Using LDA[C]//2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Montre´al, Canada, June 3-8, 2012: 558-562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mimno</author>
<author>H Wallach</author>
<author>J Naradowsky</author>
</authors>
<date>2009</date>
<booktitle>Polylingual topic models[C]//Proceedings of the EMNLP.</booktitle>
<pages>880--889</pages>
<contexts>
<context position="3554" citStr="Mimno et al., 2009" startWordPosition="511" endWordPosition="514">ic level to solve the traditional problems. Preiss (2012) transformed the source language topical model to the target language and classified probability distribution of topics in the same language, whose shortcoming is that the effect of model translation seriously hampers the comparable corpora quality. Ni et al. (2009) adapted monolingual topic model to bilingual topic model in which the documents of a concept unit in different languages were assumed to share identical topic distribution. Bilingual topic model is widely adopted to mine translation equivalents from multi-language documents (Mimno et al., 2009; Ivan et al., 2011). Based on the bilingual topic model, this paper predicts the topical structure of documents in different languages and calculates the similarity of topics over documents to build comparable corpora. The paper concretely includes: 1) Introduce the Bilingual LDA (Latent Dirichlet Allocation) model which builds comparable corpora and improves the efficiency of matching similar documents; 2) Design a novel method of TFIDF (Topic Frequency-Inverse Document Frequency) to enhance the distinguishing ability of topics from different documents; 3) Propose a tailored 278 Proceedings </context>
</contexts>
<marker>Mimno, Wallach, Naradowsky, 2009</marker>
<rawString>Mimno D, Wallach H, Naradowsky J et al. Polylingual topic models[C]//Proceedings of the EMNLP. Singapore, 2009: 880-889.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Vulic</author>
<author>W De Smet</author>
<author>M F Moens</author>
</authors>
<title>Identifying word translations from comparable corpora using latent topic models[C]//Proceedings of ACL.</title>
<date>2011</date>
<pages>479--484</pages>
<marker>Vulic, De Smet, Moens, 2011</marker>
<rawString>Vulic I, De Smet W, Moens M F, et al. Identifying word translations from comparable corpora using latent topic models[C]//Proceedings of ACL. 2011: 479-484.</rawString>
</citation>
<citation valid="false">
<authors>
<author>X Ni</author>
<author>J T Sun</author>
<author>J Hu</author>
</authors>
<booktitle>Mining multilingual topics from wikipedia[C]//Proceedings of the 18th inter-</booktitle>
<marker>Ni, Sun, Hu, </marker>
<rawString>Ni X, Sun J T, Hu J, et al. Mining multilingual topics from wikipedia[C]//Proceedings of the 18th inter-</rawString>
</citation>
<citation valid="false">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<title>Latent dirichlet allocation[J].</title>
<journal>the Journal of machine Learning research,</journal>
<volume>2003</volume>
<pages>993--1022</pages>
<marker>Blei, Ng, Jordan, </marker>
<rawString>Blei D M, Ng A Y, Jordan M I. Latent dirichlet allocation[J]. the Journal of machine Learning research, 2003, 3: 993-1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics[J].</title>
<date>2004</date>
<booktitle>Proceedings of the National academy of Sciences of the United States of America,</booktitle>
<volume>101</volume>
<pages>5228--5235</pages>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Griffiths T L, Steyvers M. Finding scientific topics[J]. Proceedings of the National academy of Sciences of the United States of America, 2004, 101: 5228-5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Schütze</author>
</authors>
<title>Foundations of statistical natural language processing[M].</title>
<date>1999</date>
<publisher>MIT press,</publisher>
<marker>Manning, Schütze, 1999</marker>
<rawString>Manning C D, Schütze H. Foundations of statistical natural language processing[M]. MIT press, 1999.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>