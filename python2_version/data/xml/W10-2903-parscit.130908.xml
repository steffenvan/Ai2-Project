<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000025">
<title confidence="0.992175">
Driving Semantic Parsing from the World’s Response
</title>
<author confidence="0.996907">
James Clarke Dan Goldwasser Ming-Wei Chang Dan Roth
</author>
<affiliation confidence="0.99889">
Department of Computer Science
University of Illinois
</affiliation>
<address confidence="0.597826">
Urbana, IL 61820
</address>
<email confidence="0.998277">
{clarkeje,goldwas1,mchang21,danr}@illinois.edu
</email>
<sectionHeader confidence="0.993872" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999919">
Current approaches to semantic parsing,
the task of converting text to a formal
meaning representation, rely on annotated
training data mapping sentences to logi-
cal forms. Providing this supervision is
a major bottleneck in scaling semantic
parsers. This paper presents a new learn-
ing paradigm aimed at alleviating the su-
pervision burden. We develop two novel
learning algorithms capable of predicting
complex structures which only rely on a
binary feedback signal based on the con-
text of an external world. In addition we
reformulate the semantic parsing problem
to reduce the dependency of the model on
syntactic patterns, thus allowing our parser
to scale better using less supervision. Our
results surprisingly show that without us-
ing any annotated meaning representations
learning with a weak feedback signal is ca-
pable of producing a parser that is compet-
itive with fully supervised parsers.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.978746125">
Semantic Parsing, the process of converting text
into a formal meaning representation (MR), is one
of the key challenges in natural language process-
ing. Unlike shallow approaches for semantic in-
terpretation (e.g., semantic role labeling and in-
formation extraction) which often result in an in-
complete or ambiguous interpretation of the natu-
ral language (NL) input, the output of a semantic
parser is a complete meaning representation that
can be executed directly by a computer program.
Semantic parsing has mainly been studied in the
context of providing natural language interfaces
to computer systems. In these settings the target
meaning representation is defined by the seman-
tics of the underlying task. For example, provid-
ing access to databases: a question posed in nat-
ural language is converted into a formal database
query that can be executed to retrieve information.
Example 1 shows a NL input query and its corre-
sponding meaning representation.
Example 1 Geoquery input text and output MR
“What is the largest state that borders Texas?”
largest(state(next to(const(texas))))
Previous works (Zelle and Mooney, 1996; Tang
and Mooney, 2001; Zettlemoyer and Collins,
2005; Ge and Mooney, 2005; Zettlemoyer and
Collins, 2007; Wong and Mooney, 2007) employ
machine learning techniques to construct a seman-
tic parser. The learning algorithm is given a set of
input sentences and their corresponding meaning
representations, and learns a statistical semantic
parser — a set of rules mapping lexical items and
syntactic patterns to their meaning representation
and a score associated with each rule. Given a sen-
tence, these rules are applied recursively to derive
the most probable meaning representation. Since
semantic interpretation is limited to syntactic pat-
terns identified in the training data, the learning
algorithm requires considerable amounts of anno-
tated data to account for the syntactic variations
associated with the meaning representation. An-
notating sentences with their MR is a difficult,
time consuming task; minimizing the supervision
effort required for learning is a major challenge in
scaling semantic parsers.
This paper proposes a new model and learning
paradigm for semantic parsing aimed to alleviate
the supervision bottleneck. Following the obser-
vation that the target meaning representation is to
be executed by a computer program which in turn
provides a response or outcome; we propose a re-
sponse driven learning framework capable of ex-
ploiting feedback based on the response. The feed-
back can be viewed as a teacher judging whether
the execution of the meaning representation pro-
duced the desired response for the input sentence.
</bodyText>
<page confidence="0.989076">
18
</page>
<note confidence="0.95541">
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 18–27,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999102">
This type of supervision is very natural in many
situations and requires no expertise, thus can be
supplied by any user.
Continuing with Example 1, the response gen-
erated by executing a database query would be
used to provide feedback. The feedback would be
whether the generated response is the correct an-
swer for the input question or not, in this case New
Mexico is the desired response.
In response driven semantic parsing, the learner
is provided with a set of natural language sen-
tences and a feedback function that encapsulates
the teacher. The feedback function informs the
learner whether its interpretation of the input sen-
tence produces the desired response. We consider
scenarios where the feedback is provided as a bi-
nary signal, correct +1 or incorrect −1.
This weaker form of supervision poses a chal-
lenge to conventional learning methods: semantic
parsing is in essence a structured prediction prob-
lem requiring supervision for a set of interdepen-
dent decisions, while the provided supervision is
binary, indicating the correctness of a generated
meaning representation. To bridge this difference
we propose two novel learning algorithms suited
to the response driven setting.
Furthermore, to account for the many syntac-
tic variations associated with the MR, we propose
a new model for semantic parsing that allows us
to learn effectively and generalize better. Cur-
rent semantic parsing approaches extract parsing
rules mapping NL to their MR, restricting pos-
sible interpretations to previously seen syntactic
patterns. We replace the rigid inference process
induced by the learned parsing rules with a flex-
ible framework. We model semantic interpreta-
tion as a sequence of interdependent decisions,
mapping text spans to predicates and use syntac-
tic information to determine how the meaning of
these logical fragments should be composed. We
frame this process as an Integer Linear Program-
ming (ILP) problem, a powerful and flexible in-
ference framework that allows us to inject rele-
vant domain knowledge into the inference process,
such as specific domain semantics that restrict the
space of possible interpretations.
We evaluate our learning approach and model
on the well studied Geoquery domain (Zelle and
Mooney, 1996; Tang and Mooney, 2001), a
database consisting of U.S. geographical informa-
tion, and natural language questions. Our experi-
mental results show that our model with response
driven learning can outperform existing models
trained with annotated logical forms.
The key contributions of this paper are:
Response driven learning for semantic parsing
We propose a new learning paradigm for learn-
ing semantic parsers without any annotated mean-
ing representations. The supervision for learning
comes from a binary feedback signal based a re-
sponse generated by executing a meaning repre-
sentation. This type of supervision signal is nat-
ural to produce and can be acquired from non-
expert users.
Novel training algorithms Two novel train-
ing algorithms are developed within the response
driven learning paradigm. The training algorithms
are applicable beyond semantic parsing and can be
used in situations where it is possible to obtain bi-
nary feedback for a structured learning problem.
Flexible semantic interpretation process We
propose a novel flexible semantic parsing model
that can handle previously unseen syntactic varia-
tions of the meaning representation.
</bodyText>
<sectionHeader confidence="0.949045" genericHeader="introduction">
2 Semantic Parsing
</sectionHeader>
<bodyText confidence="0.999747428571429">
The goal of semantic parsing is to produce a func-
tion F : X → Z that maps from the space natural
language input sentences, X, to the space of mean-
ing representations, Z. This type of task is usu-
ally cast as a structured output prediction problem,
where the goal is to obtain a model that assigns the
highest score to the correct meaning representa-
tion given an input sentence. However, in the task
of semantic parsing, this decision relies on identi-
fying a hidden intermediate representation (or an
alignment) that captures the way in which frag-
ments of the text correspond to the meaning repre-
sentation. Therefore, we formulate the prediction
function as follows:
</bodyText>
<equation confidence="0.992777">
z� = FK,(x) = arg max wTb(x, y, z) (1)
yEY,zEZ
</equation>
<bodyText confidence="0.997670125">
Where b is a feature function that describes the
relationships between an input sentence x, align-
ment y and meaning representation z. w is the
weight vector which contains the parameters of the
model. We refer to the arg max above as the in-
ference problem. The feature function combined
with the nature of the inference problem defines
the semantic parsing model. The key to producing
</bodyText>
<page confidence="0.994667">
19
</page>
<figure confidence="0.998501823529412">
Algorithm 1 Direct Approach (Binary Learning)
Input: Sentences {xl}N l=1,
Feedback : X × Z → {+1, 1},
initial weight vector w
1: Bl ← {} for all l = 1, ... , N
2: repeat
3: for l = 1,... ,N do
4: ˆy, zˆ = arg maxy,z wTΦ(xl, y, z)
5: f = Feedback (xl, ˆz)
6: add (Φ(xl,ˆy,ˆz)/|xl|,f) to Bl
7: end for
8: w ← BinaryLearn(B) where B = ∪lBl
9: until no Bl has new unique examples
10: return w
What is the (argest state that borders Texas?
largest( state( next—t*(c*nst(texas))))
New Mexico
</figure>
<figureCaption confidence="0.715232666666667">
Figure 1: Example input sentence, meaning repre-
sentation, alignment and answer for the Geoquery
domain
</figureCaption>
<bodyText confidence="0.998056315789474">
a semantic parser involves defining a model and a
learning algorithm to obtain w.
In order to exemplify these concepts we con-
sider the Geoquery domain. Geoquery contains a
query language for a database of U.S. geograph-
ical facts. Figure 1 illustrates concrete examples
of the terminology introduce. The input sentences
x are natural language queries about U.S. geog-
raphy. The meaning representations z are logical
forms which can be executed on the database to
obtain a response which we denote with r. The
alignment y captures the associations between x
and z.
Building a semantic parser involves defining the
model (feature function Φ and inference problem)
and a learning strategy to obtain weights (w) as-
sociated with the model. We defer discussion of
our model until Section 4 and first focus on our
learning strategy.
</bodyText>
<sectionHeader confidence="0.9685255" genericHeader="method">
3 Structured Learning with Binary
Feedback
</sectionHeader>
<bodyText confidence="0.99976596">
Previous approaches to semantic parsing have
assumed a fully supervised setting where
a training set is available consisting of ei-
ther: input sentences and logical forms
{(xl, zl)}Nl=1 (e.g., (Zettlemoyer and Collins,
2005)) or input sentences, logical forms
and a mapping between their constituents
{(xl, yl, zl)}Nl=1 (e.g., (Ge and Mooney, 2005)).
Given such training examples a weight vector w
can be learned using structured learning methods.
Obtaining, through annotation or other means, this
form of training data is an expensive and difficult
process which presents a major bottleneck for
semantic parsing.
To reduce the burden of annotation we focus
on a new learning paradigm which uses feedback
from a teacher. The feedback signal is binary
(+1, −1) and informs the learner whether a pre-
dicted logical form zˆ when executed on the target
domain produces the desired response or outcome.
This is a very natural method for providing super-
vision in many situations and requires no exper-
tise. For example, a user can observe the response
and provide a judgement. The general form of
the teacher’s feedback is provided by a function
</bodyText>
<equation confidence="0.628695">
Feedback : X × Z → {+1, −1}.
</equation>
<bodyText confidence="0.99698475">
For the Geoquery domain this amounts to
whether the logical form produces the correct re-
sponse r for the input sentence. Geoquery has the
added benefit that the teacher can be automated
if we have a dataset consisting of input sentences
and response pairs {(xl, rl)}Nl=1. Feedback eval-
uates whether a logical form produces a response
matching r:
</bodyText>
<equation confidence="0.985216">
� +1 if execute(z) = rl
Feedback(xl,z) =
−1 otherwise
</equation>
<bodyText confidence="0.9999832">
We are now ready to present our learning
with feedback algorithms that operate in situations
where input sentences, {xl}N l=1, and a teacher
feedback mechanism, Feedback, are available. We
do not assume the availability of logical forms.
</bodyText>
<subsectionHeader confidence="0.999017">
3.1 Direct Approach (Binary Learning)
</subsectionHeader>
<bodyText confidence="0.957090133333333">
In general, a weight vector can be considered
good if when used in the inference problem (Equa-
tion (1)) it scores the correct logical form and
alignment (which may be hidden) higher than all
other logical forms and alignments for a given in-
put sentence. The intuition behind the direct ap-
proach is that the feedback function can be used to
subsample the space of possible structures (align-
ments and logical forms (Y × Z)) for a given in-
put x. The feedback mechanism indicates whether
the structure is good (+1) or bad (−1). Using this
x:
Y:
z:
r:
</bodyText>
<page confidence="0.785511">
20
</page>
<bodyText confidence="0.99996665">
intuition we can cast the problem of learning a
weight vector for Equation (1) as a binary classifi-
cation problem where we directly consider struc-
tures the feedback assigns +1 as positive examples
and those assigned −1 as negative.
We represent the input to the binary classifier
as the feature vector 4b(x, y, z) normalized by the
size of the input sentence1 |x|, and the label as the
result from Feedback(x, z).
Algorithm 1 outlines the approach in detail. The
first stage of the algorithm iterates over all the
training input sentences and computes the best
logical form z� and alignment y� by solving the in-
ference problem (line 4). The feedback function
is queried (line 5) and a training example for the
binary predictor created using the normalized fea-
ture vector from the triple containing the sentence,
alignment and logical form as input and the feed-
back as the label. This training example is added
to the working set of training examples for this in-
put sentence (line 6). All the feedback training ex-
amples are used to train a binary classifier whose
weight vector is used in the next iteration (line 8).
The algorithm repeats until no new unique training
examples are added to any of the working sets for
any input sentence. Although the number of possi-
ble training examples is very large, in practice the
algorithm is efficient and converges quickly. Note
that this approach is capable of using a wide va-
riety of linear classifiers as the base learner (line
8).
A policy is required to specify the nature of
the working set of training examples (Bl) used for
training the base classifier. This is pertinent in line
6 of the algorithm. Possible policies include: al-
lowing duplicates in the working set (i.e., Bl is
a multiset), disallowing duplicates (Bl is a set),
or only allowing one example per input sentence
(kBlk = 1). We adopt the first approach in this
paper.2
</bodyText>
<subsectionHeader confidence="0.9906085">
3.2 Aggressive Approach (Structured
Learning)
</subsectionHeader>
<bodyText confidence="0.9999755">
There is important implicit information which
the direct approach ignores. It is implicit that
when the teacher indicates an input paired with
an alignment and logical form is good (+1 feed-
</bodyText>
<footnote confidence="0.7170728">
1Normalization is required to ensure that each sentence
contributes equally to the binary learning problem regardless
of the sentence’s length.
2The working set B, for each sentence may contain multi-
ple positive examples with the same and differing alignments.
</footnote>
<construct confidence="0.244541">
Algorithm 2 Aggressive Approach (Structured
</construct>
<equation confidence="0.646897333333333">
Learning)
Input: Sentences {xl}Nl�1,
Feedback : X × Z → {+1, 1},
</equation>
<bodyText confidence="0.611007">
initial weight vector w
</bodyText>
<listItem confidence="0.995079333333333">
1: Sl ← ∅ for all l = 1,... ,N
2: repeat
3: for l = 1,... , N do
4: y, z� = arg mazy,z wT�(xl, y, z)
5: f = Feedback (xl, i)
6: if f is +1 then
7: Sl ← {(xl, �y, �z)}
8: end if
9: end for
10: w ← StructLearn(S, 4b) where S = ∪lSl
11: until no Sl has changed
12: return w
</listItem>
<bodyText confidence="0.994139939393939">
back) that in order to repeat this behavior all other
competing structures should be made suboptimal
(or bad). To leverage this implicit information
we adopt a structured learning strategy in which
we consider the prediction as the optimal structure
and all others as suboptimal. This is in contrast to
the direct approach where only structures that have
explicitly received negative feedback are consid-
ered subopitmal.
When a structure is found with positive feed-
back it is added to the training pool for a struc-
tured learner. We consider this approach aggres-
sive as the structured learner implicitly considers
all other structures as being suboptimal. Negative
feedback indicates that the structure should not be
added to the training pool as it will introduce noise
into the learning process.
Algorithm 2 outlines the learning in more detail.
As before, y� and z� are predicted using the cur-
rent weight vector and feedback received (lines 4
and 5). When positive feedback is received a new
training instance for a structured learner is created
from the input sentence and prediction (line 7) this
training instance replaces any previous instance
for the input sentence. When negative feedback
is received the training pool Sl is not updated. A
weight vector is learned using a structured learner
where the training data S contains at most one ex-
ample per input sentence. In the first iteration of
the outer loop the training data S will contain very
few examples. In each subsequent iteration the
newly learned weight vector allows the algorithm
to acquire new examples. This is repeated until no
</bodyText>
<page confidence="0.995876">
21
</page>
<bodyText confidence="0.9954746">
new examples are added or changed in S.
Like the direct approach, this learning frame-
work is makes very few assumptions about the
type of structured learner used as a base learner
(line 10).3
</bodyText>
<sectionHeader confidence="0.995377" genericHeader="method">
4 Model
</sectionHeader>
<bodyText confidence="0.98967675">
Semantic parsing is the process of converting a
natural language input into a formal logic repre-
sentation. This process is performed by associat-
ing lexical items and syntactic patterns with logi-
cal fragments and composing them into a complete
formula. Existing approaches rely on extracting
a set of parsing rules, mapping text constituents
to a logical representation, from annotated train-
ing data and applying them recursively to obtain
the meaning representation. Adapting to new data
is a major limitation of these approaches as they
cannot handle inputs containing syntactic patterns
which were not observed in the training data. For
example, assume the training data produced the
following set of parsing rules:
Example 2 Typical parsing rules
</bodyText>
<listItem confidence="0.9920986">
(1) NP [Ax.capital(x)] --+ capital
(2) PP [ const(texas)] of Texas
(3) NNP [ const(texas)] --+ Texas
(4) NP [capital(const(texas))] --+
NP[Ax.capital(x)] PP [ const(texas)]
</listItem>
<bodyText confidence="0.999276714285714">
At test time the parser is given the sentences in
Example 3. Despite the lexical similarity in these
examples, the semantic parser will correctly parse
the first sentence but fail to parse the second be-
cause the lexical items belong to different a syn-
tactic category (i.e., the word Texas is not part of a
preposition phrase in the second sentence).
</bodyText>
<equation confidence="0.352556">
Example 3 Syntactic variations of the same MR
Target logical form: capital(const(texas))
Sentence 1: “What is the capital of Texas?”
Sentence 2: “What is Texas’ capital?”
</equation>
<bodyText confidence="0.999897">
The ability to adapt to unseen inputs is one
of the key challenges in semantic parsing. Sev-
eral works (Zettlemoyer and Collins, 2007; Kate,
2008) have addressed this issue explicitly by man-
ually defining syntactic transformation rules that
can help the learned parser generalize better. Un-
fortunately these are only partial solutions as a
</bodyText>
<footnote confidence="0.921755">
3Mistake driven algorithms that do not enforce margin
constraints may not be able to generalize using this proto-
col since they will repeat the same prediction at training time
and therefore will not update the model.
</footnote>
<bodyText confidence="0.999245763157895">
manually constructed rule set cannot cover the
many syntactic variations.
Given the previous example, we observe
that it is enough to identify that the function
capital(·) and the constant const(texas)
appear in the target MR, since there is only a single
way to compose these entities into a single formula
— capital(const(texas)).
Motivated by this observation we define our
meaning derivation process over the rules of the
MR language and use syntactic information as a
way to bias the MR construction process. That
is, our inference process considers the entire space
of meaning representations irrespective of the pat-
terns observed in the training data. This is possi-
ble as the MRs are defined by a formal language
and formal grammar.4 The syntactic information
present in the natural language is used as soft ev-
idence (features) which guides the inference pro-
cess to good meaning representations.
This formulation is a major shift from existing
approaches that rely on extracting parsing rules
from the training data. In existing approaches
the space of possible meaning representations is
constrained by the patterns in the training data
and syntactic structure of the natural language in-
put. Our formulation considers the entire space of
meaning representations and allows the model to
adapt to previously unseen data and always pro-
duce a semantic interpretation by using the pat-
terns observed in the input.
We frame our semantic interpretation process
as a constrained optimization process, maximiz-
ing the objective function defined by Equation 1
which relies on extracting lexical and syntactic
features instead of parsing rules. In the remain-
der of this section we explain the components of
our inference model.
</bodyText>
<subsectionHeader confidence="0.987003">
4.1 Target Meaning Representation
</subsectionHeader>
<bodyText confidence="0.999876888888889">
Following previous work, we capture the se-
mantics of the Geoquery domain using a sub-
set of first-order logic consisting of typed con-
stants and functions. There are two types: en-
tities E in the domain and numeric values N.
Functions describe a functional relationship over
types (e.g., population : E → N). A com-
plete logical form is constructed through func-
tional composition; in our formalism this is per-
</bodyText>
<footnote confidence="0.965809">
4This is true for all meaning representations designed to
be executed by a computer system.
</footnote>
<page confidence="0.998909">
22
</page>
<bodyText confidence="0.999833666666667">
formed by the substitution operator. For ex-
ample, given the function next to(x) and
the expression const(texas), substitution re-
places the occurrence of the free variable x, with
the expression, resulting in a new logical form:
next to(const(texas)). Due to space lim-
itations we refer the reader to (Zelle and Mooney,
1996) for a detailed description of the Geoquery
domain.
</bodyText>
<subsectionHeader confidence="0.9434835">
4.2 Semantic Parsing as Constrained
Optimization
</subsectionHeader>
<bodyText confidence="0.999802">
Recall that the goal of semantic parsing is to pro-
duce the following function (Equation (1)):
</bodyText>
<equation confidence="0.9274105">
FK,(x) = arg max wT b(x, y, z)
y,z
</equation>
<bodyText confidence="0.99964146875">
However, given that y and z are complex struc-
tures it is necessary to decompose the structure
into a set of smaller decisions to facilitate efficient
inference.
In order to define our decomposition we intro-
duce additional notation: c is a constituent (or
word span) in the input sentence x and D is the
set of all function and constant symbols in the do-
main. The alignment y is defined as a set of map-
pings between constituents and symbols in the do-
main y = {(c, s)} where s E D.
We decompose the construction of an alignment
and logical form into two types of decisions:
First-order decisions. A mapping between con-
stituents and logical symbols (functions and con-
stants).
Second-order decisions. Expressing how logi-
cal symbols are composed into a complete logical
interpretation. For example, whether next to
and state forms next to(state(·)) or
state(next to(·)).
Note that for all possible logical forms and
alignments there exists a one-to-one mapping to
these decisions.
We frame the inference problem as an Integer
Linear Programming (ILP) problem (Equation (2))
in which the first-order decisions are governed by
αcs, a binary decision variable indicating that con-
stituent c is aligned with logical symbol s. And
Qcs,dt capture the second-order decisions indicat-
ing the symbol t (associated with constituent d)
is an argument to function s (associated with con-
</bodyText>
<equation confidence="0.9129638">
stituent c).
1: 1:
F�(x) = arg max αcs · wT b1(x, c, s)
a,a cEx sED
Qcs,dt · wTb2(x, c, s, d, t) (2)
</equation>
<bodyText confidence="0.999326818181818">
It is clear that there are dependencies between
the α-variables and Q-variables. For example,
given that Qcs,dt is active, the corresponding α-
variables αcs and αdt must also be active. In order
to ensure a consistent solution we introduce a set
of constraints on Equation (2). In addition we add
constraints which leverage the typing information
inherent in the domain to eliminate logical forms
that are invalid in the Geoquery domain. For ex-
ample, the function length only accepts river
types as input. The set of constraints are:
</bodyText>
<listItem confidence="0.997907375">
• A given constituent can be associated with
exactly one logical symbol.
• Qcs,dt is active if and only if αcs and αdt are
active.
• If Qcs,dt is active, s must be a function and
the types of s and t should be consistent.
• Functional composition is directional and
acyclic.
</listItem>
<bodyText confidence="0.99969525">
The flexibility of ILP has previously been advan-
tageous in natural language processing tasks (Roth
and Yih, 2007) as it allows us to easily incorporate
such constraints.
</bodyText>
<subsectionHeader confidence="0.950951">
4.3 Features
</subsectionHeader>
<bodyText confidence="0.999915615384615">
The inference problem defined in Equation (2)
uses two feature functions: b1 and b2.
First-order decision features b1 Determining
if a logical symbol is aligned with a specific con-
stituent depends mostly on lexical information.
Following previous work (e.g., (Zettlemoyer and
Collins, 2005)) we create a small lexicon, mapping
logical symbols to surface forms.5 This lexicon is
small and only used as a starting point. Existing
approaches rely on annotated logical forms to ex-
tend the lexicon. However, in our setting we do
not have access to annotated logical forms, instead
we rely on external knowledge to supply further
</bodyText>
<footnote confidence="0.918768">
5The lexicon contains on average 1.42 words per func-
tion and 1.07 words per constant. For example the function
next to has the lexical entries: borders, next, adjacent and
the constant illinois the lexical item illinois.
</footnote>
<figure confidence="0.949223">
1:
+
c,dEx
1:
s,tED
</figure>
<page confidence="0.996205">
23
</page>
<bodyText confidence="0.99999075">
information. We add features which measure the
lexical similarity between a constituent and a logi-
cal symbol’s surface forms (as defined by the lexi-
con). Two metrics are used: stemmed word match
and a similarity metric based on WordNet (Miller
et al., 1990) which allows our model to account
for words not in the lexicon. The WordNet met-
ric measures similarity based on synonymy, hy-
ponymy and meronymy (Do et al., 2010). In the
case where the constituent is a preposition, which
are notorious for being ambiguous, we add a fea-
ture that considers the current lexical context (one
word to the left and right) in addition to word sim-
ilarity.
Second-order decision features 4b2 Determin-
ing how to compose two logical symbols relies on
syntactic information, in our model we use the de-
pendency tree (Klein and Manning, 2003) of the
input sentence. Given a second-order decision
Q1-s,dt, the dependency feature takes the normal-
ized distance between the head words in the con-
stituents c and d. A set of features also indicate
which logical symbols are usually composed to-
gether, without considering their alignment to text.
</bodyText>
<sectionHeader confidence="0.998503" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999147333333333">
In this section we describe our experimental setup,
which includes the details of the domain, re-
sources and parameters.
</bodyText>
<subsectionHeader confidence="0.994422">
5.1 Domain and Corpus
</subsectionHeader>
<bodyText confidence="0.940372548387097">
We evaluate our system on the Geoquery domain
as described previously. The domain consists of
a database and Prolog query language for U.S.
geographical facts. The corpus contains of 880
natural language queries paired with Prolog log-
ical form queries ((x, z) pairs). We follow previ-
ous approaches and transform these queries into a
functional representation. We randomly select 250
sentences for training and 250 sentences for test-
ing.6 We refer to the training set as Response 250
(R250) indicating that each example x in this data
set has a corresponding desired database response
r. We refer the testing set as Query 250 (Q250)
where the examples only contain the natural lan-
guage queries.
6Our inference problem is less constrained than previous
approaches thus we limit the training data to 250 examples
due to scalability issues. We also prune the search space by
limiting the number of logical symbol candidates per word
(on average 13 logical symbols per word).
Precision and recall are typically used as eval-
uation metrics in semantic parsing. However, as
our model inherently has the ability to map any
input sentence into the space of meaning repre-
sentations the trade off between precision and re-
call does not exist. Thus, we report accuracy: the
percentage of meaning representations which pro-
duce the correct response. This is equivalent to
recall in previous work (Wong and Mooney, 2007;
Zettlemoyer and Collins, 2005; Zettlemoyer and
Collins, 2007).
</bodyText>
<subsectionHeader confidence="0.999325">
5.2 Resources and Parameters
</subsectionHeader>
<bodyText confidence="0.999944451612903">
Feedback Recall that our learning framework
does not require meaning representation annota-
tions. However, we do require a Feedback func-
tion that informs the learner whether a predicted
meaning representation when executed produces
the desired response for a given input sentence.
We automatically generate a set of natural lan-
guage queries and response pairs {(x, r)} by exe-
cuting the annotated logical forms on the database.
Using this data we construct an automatic feed-
back function as described in Section 3.
Domain knowledge Our learning approaches
require an initial weight vector as input. In or-
der to provide an initial starting point, we initialize
the weight vector using a similar procedure to the
one used in (Zettlemoyer and Collins, 2007) to set
weights for three features and a bias term. The
weights were developed on the training set using
the feedback function to guide our choices.
Underlying Learning Algorithms In the direct
approach the base linear classifier we use is a lin-
ear kernel Support Vector Machine with squared-
hinge loss. In the aggressive approach we de-
fine our base structured learner to be a structural
Support Vector Machine with squared-hinge loss
and use hamming distance as the distance func-
tion. We use a custom implementation to op-
timize the objective function using the Cutting-
Plane method, this allows us to parrallelize the
learning process by solving the inference problem
for multiple training examples simultaneously.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.948825">
Our experiments are designed to answer three
questions:
</bodyText>
<footnote confidence="0.5350175">
1. Is it possible to learn a semantic parser with-
out annotated logical forms?
</footnote>
<page confidence="0.996896">
24
</page>
<table confidence="0.9992662">
Algorithm R250 Q250
NOLEARN 22.2 —
DIRECT 75.2 69.2
AGGRESSIVE 82.4 73.2
SUPERVISED 87.6 80.4
</table>
<tableCaption confidence="0.995026">
Table 1: Accuracy of learned models on R250 data and
Q250 (testing) data. NOLEARN: using initialized weight
vector, DIRECT: using feedback with the direct approach,
AGGRESSIVE: using feedback with the aggressive approach,
SUPERVISED: using gold 250 logical forms for training.
Note that none of the approaches use any annotated logical
forms besides the SUPERVISED approach.
</tableCaption>
<table confidence="0.999920625">
Algorithm # LF Accuracy
AGGRESSIVE — 73.2
SUPERVISED 250 80.4
W&amp;M 2006 — 310 — 60.0
W&amp;M 2007 — 310 — 75.0
Z&amp;C 2005 600 79.29
Z&amp;C 2007 600 86.07
W&amp;M 2007 800 86.59
</table>
<tableCaption confidence="0.9444176">
Table 2: Comparison against previously published results.
Results show that with a similar number of logical forms
(# LF) for training our SUPERVISED approach outperforms
existing systems, while the AGGRESSIVE approach remains
competitive without using any logical forms.
</tableCaption>
<listItem confidence="0.9524364">
2. How much performance do we sacrifice by
not restricting our model to parsing rules?
3. What, if any, are the differences in behaviour
between the two learning with feedback ap-
proaches?
</listItem>
<bodyText confidence="0.999977970588235">
We first compare how well our model performs
under four different learning regimes. NOLEARN
uses a manually initialized weight vector. DIRECT
and AGGRESSIVE use the two response driven
learning approaches, where a feedback function
but no logical forms are provided. As an up-
per bound we train the model using a fully SU-
PERVISED approach where the input sentences are
paired with hand annotated logical forms.
Table 1 shows the accuracy of each setup. The
model without learning (NOLEARN) gives a start-
ing point with an accuracy of 22.2%. The re-
sponse driven learning methods perform substan-
tially better than the starting point. The DIRECT
approach which uses a binary learner reaches an
accuracy of 75.2% on the R250 data and 69.2% on
the Q250 (testing) data. While the AGGRESSIVE
approach which uses a structured learner sees a
bigger improvement, reaching 82.4% and 73.2%
respectively. This is only 7% below the fully SU-
PERVISED upper bound of the model.
To answer the second question, we compare a
supervised version of our model to existing se-
mantic parsers. The results are in Table 2. Al-
though the numbers are not directly comparable
due to different splits in the data7, we can see that
with a similar number of logical forms for train-
ing our SUPERVISED approach outperforms ex-
isting systems (Wong and Mooney, 2006; Wong
and Mooney, 2007), while the AGGRESSIVE ap-
proach remains competitive without using any log-
ical forms. Our SUPERVISED model is still very
competitive with other approaches (Zettlemoyer
and Collins, 2007; Wong and Mooney, 2007),
which used considerably more annotated logical
forms in the training phase.
In order to answer the third question, we turn
our attention to the differences between the two
response driven learning approaches. The DIRECT
and AGGRESSIVE approaches use binary feedback
to learn, however they utilize the signal differently.
DIRECT uses the signal directly to learn a bi-
nary classifier capable of replicating the feedback,
whereas AGGRESSIVE learns a structured predic-
tor that can repeatedly obtain the logical forms
for which positive feedback was received. Thus,
although the AGGRESSIVE outperforms the DI-
RECT approach the concepts each approach learns
may be different. Analysis over the training data
shows that in 66.8% examples both approaches
predict a logical form that gives the correct an-
swer. While AGGRESSIVE correctly answers an
additional 16% which DIRECT gets incorrect. In
the opposite direction, DIRECT correctly answers
8.8% that AGGRESSIVE does not. Leaving only
8.4% of the examples that both approaches pre-
dict incorrect logical forms. This suggests that an
approach which combines DIRECT and AGGRES-
SIVE may be able to improve even further.
Figure 2 shows the accuracy on the entire train-
ing data (R250) at each iteration of learning. We
see that the AGGRESSIVE approach learns to cover
more of the training data and at a faster rate than
DIRECT. Note that the performance of the DI-
RECT approach drops at the first iteration. We hy-
pothesize this is due to imbalances in the binary
feedback dataset (too many negative examples) in
the first iteration.
</bodyText>
<footnote confidence="0.987578">
7It is relatively difficult to compare different approaches
in the Geoquery domain given that many existing papers do
not use the same data split.
</footnote>
<page confidence="0.995108">
25
</page>
<figure confidence="0.85302">
Learning Iterations
</figure>
<figureCaption confidence="0.997979">
Figure 2: Accuracy on training set as number of learning
iterations increases.
</figureCaption>
<sectionHeader confidence="0.999855" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999937886363637">
Learning to map sentences to a meaning repre-
sentation has been studied extensively in the NLP
community. Early works (Zelle and Mooney,
1996; Tang and Mooney, 2000) employed induc-
tive logic programming approaches to learn a se-
mantic parser. More recent works apply statisti-
cal learning methods to the problem. In (Ge and
Mooney, 2005; Nguyen et al., 2006), the input to
the learner consists of complete syntactic deriva-
tions for the input sentences annotated with logi-
cal expressions. Other works (Wong and Mooney,
2006; Kate and Mooney, 2006; Zettlemoyer and
Collins, 2005; Zettlemoyer and Collins, 2007;
Zettlemoyer and Collins, 2009) try to alleviate the
annotation effort by only taking sentence and log-
ical form pairs to train the models. Learning is
then defined over hidden patterns in the training
data that associate logical symbols with lexical
and syntactic elements.
In this work we take an additional step to-
wards alleviating the difficulty of training seman-
tic parsers and present a world response based
training protocol. Several recent works (Chen and
Mooney, 2008; Liang et al., 2009; Branavan et
al., 2009) explore using an external world context
as a supervision signal for semantic interpretation.
These works operate in settings different to ours as
they rely on an external world state that is directly
referenced by the input text. Although our frame-
work can also be applied in these settings we do
not assume that the text can be grounded in a world
state. In our experiments the input text consists of
generalized statements which describe some infor-
mation need that does not correspond directly to a
grounded world state.
Our learning framework closely follows recent
work on learning from indirect supervision. The
direct approach resembles learning a binary clas-
sifier over a latent structure (Chang et al., 2010a);
while the aggressive approach has similarities with
work that uses labeled structures and a binary
signal indicating the existence of good structures
to improve structured prediction (Chang et al.,
2010b).
</bodyText>
<sectionHeader confidence="0.998815" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999194230769231">
In this paper we tackle one of the key bottlenecks
in semantic parsing — providing sufficient super-
vision to train a semantic parser. Our solution is
two fold, first we present a new training paradigm
for semantic parsing that relies on natural, hu-
man level supervision. Second, we suggest a new
model for semantic interpretation that does not
rely on NL syntactic parsing rules, but rather uses
the syntactic information to bias the interpretation
process. This approach allows the model to gener-
alize better and reduce the required amount of su-
pervision. We demonstrate the effectiveness of our
training paradigm and interpretation model over
the Geoquery domain, and show that our model
can outperform fully supervised systems.
Acknowledgements We are grateful to Rohit Kate and
Raymond Mooney for their help with the Geoquery dataset.
Thanks to Yee Seng Chan, Nick Rizzolo, Shankar Vembu
and the three anonymous reviewers for their insightful com-
ments. This material is based upon work supported by the
Air Force Research Laboratory (AFRL) under prime contract
no. FA8750-09-C-0181 and by DARPA under the Bootstrap
Learning Program. Any opinions, findings, and conclusion or
recommendations expressed in this material are those of the
authors and do not necessarily reflect the views of the AFRL
or DARPA.
</bodyText>
<sectionHeader confidence="0.999265" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.989543">
S.R.K. Branavan, H. Chen, L. Zettlemoyer, and
R. Barzilay. 2009. Reinforcement learning for map-
ping instructions to actions. In Proc. of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010a. Discriminative learning over constrained la-
tent representations. In Proc. of the Annual Meeting
ofthe North American Association of Computational
Linguistics (NAACL).
</reference>
<figure confidence="0.9847050625">
Accuracy on Response 250
90
80
70
60
50
40
30
20
10
0
0 1 2 3 4 5 6
Direct Approach
Aggressive Approach
Initialization
7
</figure>
<page confidence="0.973051">
26
</page>
<reference confidence="0.999785536585366">
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010b. Structured output learning with indirect su-
pervision. In Proc. of the International Conference
on Machine Learning (ICML).
D. Chen and R. Mooney. 2008. Learning to sportscast:
a test of grounded language acquisition. In Proc. of
the International Conference on Machine Learning
(ICML).
Q. Do, D. Roth, M. Sammons, Y. Tu, and V.G. Vydis-
waran. 2010. Robust, Light-weight Approaches to
compute Lexical Similarity. Computer Science Re-
search and Technical Reports, University of Illinois.
http://hdl.handle.net/2142/15462.
R. Ge and R. Mooney. 2005. A statistical semantic
parser that integrates syntax and semantics. In Proc.
of the Annual Conference on Computational Natural
Language Learning (CoNLL).
R. Kate and R. Mooney. 2006. Using string-kernels
for learning semantic parsers. In Proc. of the An-
nual Meeting of the Association for Computational
Linguistics (ACL).
R. Kate. 2008. Transforming meaning representation
grammars to improve semantic parsing. In Proc. of
the Annual Conference on Computational Natural
Language Learning (CoNLL).
D. Klein and C. D. Manning. 2003. Fast exact in-
ference with a factored model for natural language
parsing. In Proc. of the Conference on Advances in
Neural Information Processing Systems (NIPS).
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
Proc. of the Annual Meeting of the Association for
Computational Linguistics (ACL).
G. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K.J. Miller. 1990. Wordnet: An on-line lexical
database. International Journal ofLexicography.
L. Nguyen, A. Shimazu, and X. Phan. 2006. Semantic
parsing with structured svm ensemble classification
models. In Proc. of the Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL).
D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning.
L. Tang and R. Mooney. 2000. Automated construc-
tion of database interfaces: integrating statistical and
relational learning for semantic parsing. In Proc. of
the Conference on Empirical Methods for Natural
Language Processing (EMNLP).
L. R. Tang and R. J. Mooney. 2001. Using multiple
clause constructors in inductive logic programming
for semantic parsing. In Proc. of the European Con-
ference on Machine Learning (ECML).
Y.-W. Wong and R. Mooney. 2006. Learning for
semantic parsing with statistical machine transla-
tion. In Proc. of the Annual Meeting of the North
American Association of Computational Linguistics
(NAACL).
Y.-W. Wong and R. Mooney. 2007. Learning
synchronous grammars for semantic parsing with
lambda calculus. In Proc. of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
J. M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic proramming.
In Proc. of the National Conference on Artificial In-
telligence (AAAI).
L. Zettlemoyer and M. Collins. 2005. Learning to map
sentences to logical form: Structured classification
with probabilistic categorial grammars. In Proc. of
the Annual Conference in Uncertainty in Artificial
Intelligence (UAI).
L. Zettlemoyer and M. Collins. 2007. Online learn-
ing of relaxed CCG grammars for parsing to logical
form. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Pro-
cessing and on Computational Natural Language
Learning (EMNLP-CoNLL).
L. Zettlemoyer and M. Collins. 2009. Learning
context-dependent mappings from sentences to log-
ical form. In Proc. of the Annual Meeting of the
Association for Computational Linguistics (ACL).
</reference>
<page confidence="0.9988">
27
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.912170">
<title confidence="0.999737">Driving Semantic Parsing from the World’s Response</title>
<author confidence="0.999975">James Clarke Dan Goldwasser Ming-Wei Chang Dan Roth</author>
<affiliation confidence="0.999832">Department of Computer University of</affiliation>
<address confidence="0.947797">Urbana, IL</address>
<abstract confidence="0.998329608695652">Current approaches to semantic parsing, the task of converting text to a formal meaning representation, rely on annotated training data mapping sentences to logical forms. Providing this supervision is a major bottleneck in scaling semantic parsers. This paper presents a new learning paradigm aimed at alleviating the supervision burden. We develop two novel learning algorithms capable of predicting complex structures which only rely on a binary feedback signal based on the context of an external world. In addition we reformulate the semantic parsing problem to reduce the dependency of the model on syntactic patterns, thus allowing our parser to scale better using less supervision. Our results surprisingly show that without using any annotated meaning representations learning with a weak feedback signal is capable of producing a parser that is competitive with fully supervised parsers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>H Chen</author>
<author>L Zettlemoyer</author>
<author>R Barzilay</author>
</authors>
<title>Reinforcement learning for mapping instructions to actions.</title>
<date>2009</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="35064" citStr="Branavan et al., 2009" startWordPosition="5731" endWordPosition="5734"> and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models. Learning is then defined over hidden patterns in the training data that associate logical symbols with lexical and syntactic elements. In this work we take an additional step towards alleviating the difficulty of training semantic parsers and present a world response based training protocol. Several recent works (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009) explore using an external world context as a supervision signal for semantic interpretation. These works operate in settings different to ours as they rely on an external world state that is directly referenced by the input text. Although our framework can also be applied in these settings we do not assume that the text can be grounded in a world state. In our experiments the input text consists of generalized statements which describe some information need that does not correspond directly to a grounded world state. Our learning framework closely follows recent work on learning from indirect</context>
</contexts>
<marker>Branavan, Chen, Zettlemoyer, Barzilay, 2009</marker>
<rawString>S.R.K. Branavan, H. Chen, L. Zettlemoyer, and R. Barzilay. 2009. Reinforcement learning for mapping instructions to actions. In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chang</author>
<author>D Goldwasser</author>
<author>D Roth</author>
<author>V Srikumar</author>
</authors>
<title>Discriminative learning over constrained latent representations.</title>
<date>2010</date>
<booktitle>In Proc. of the Annual Meeting ofthe North American Association of Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="35780" citStr="Chang et al., 2010" startWordPosition="5848" endWordPosition="5851">e works operate in settings different to ours as they rely on an external world state that is directly referenced by the input text. Although our framework can also be applied in these settings we do not assume that the text can be grounded in a world state. In our experiments the input text consists of generalized statements which describe some information need that does not correspond directly to a grounded world state. Our learning framework closely follows recent work on learning from indirect supervision. The direct approach resembles learning a binary classifier over a latent structure (Chang et al., 2010a); while the aggressive approach has similarities with work that uses labeled structures and a binary signal indicating the existence of good structures to improve structured prediction (Chang et al., 2010b). 8 Conclusions In this paper we tackle one of the key bottlenecks in semantic parsing — providing sufficient supervision to train a semantic parser. Our solution is two fold, first we present a new training paradigm for semantic parsing that relies on natural, human level supervision. Second, we suggest a new model for semantic interpretation that does not rely on NL syntactic parsing rul</context>
</contexts>
<marker>Chang, Goldwasser, Roth, Srikumar, 2010</marker>
<rawString>M. Chang, D. Goldwasser, D. Roth, and V. Srikumar. 2010a. Discriminative learning over constrained latent representations. In Proc. of the Annual Meeting ofthe North American Association of Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chang</author>
<author>D Goldwasser</author>
<author>D Roth</author>
<author>V Srikumar</author>
</authors>
<title>Structured output learning with indirect supervision.</title>
<date>2010</date>
<booktitle>In Proc. of the International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="35780" citStr="Chang et al., 2010" startWordPosition="5848" endWordPosition="5851">e works operate in settings different to ours as they rely on an external world state that is directly referenced by the input text. Although our framework can also be applied in these settings we do not assume that the text can be grounded in a world state. In our experiments the input text consists of generalized statements which describe some information need that does not correspond directly to a grounded world state. Our learning framework closely follows recent work on learning from indirect supervision. The direct approach resembles learning a binary classifier over a latent structure (Chang et al., 2010a); while the aggressive approach has similarities with work that uses labeled structures and a binary signal indicating the existence of good structures to improve structured prediction (Chang et al., 2010b). 8 Conclusions In this paper we tackle one of the key bottlenecks in semantic parsing — providing sufficient supervision to train a semantic parser. Our solution is two fold, first we present a new training paradigm for semantic parsing that relies on natural, human level supervision. Second, we suggest a new model for semantic interpretation that does not rely on NL syntactic parsing rul</context>
</contexts>
<marker>Chang, Goldwasser, Roth, Srikumar, 2010</marker>
<rawString>M. Chang, D. Goldwasser, D. Roth, and V. Srikumar. 2010b. Structured output learning with indirect supervision. In Proc. of the International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chen</author>
<author>R Mooney</author>
</authors>
<title>Learning to sportscast: a test of grounded language acquisition.</title>
<date>2008</date>
<booktitle>In Proc. of the International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="35020" citStr="Chen and Mooney, 2008" startWordPosition="5723" endWordPosition="5726">with logical expressions. Other works (Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models. Learning is then defined over hidden patterns in the training data that associate logical symbols with lexical and syntactic elements. In this work we take an additional step towards alleviating the difficulty of training semantic parsers and present a world response based training protocol. Several recent works (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009) explore using an external world context as a supervision signal for semantic interpretation. These works operate in settings different to ours as they rely on an external world state that is directly referenced by the input text. Although our framework can also be applied in these settings we do not assume that the text can be grounded in a world state. In our experiments the input text consists of generalized statements which describe some information need that does not correspond directly to a grounded world state. Our learning framework closely f</context>
</contexts>
<marker>Chen, Mooney, 2008</marker>
<rawString>D. Chen and R. Mooney. 2008. Learning to sportscast: a test of grounded language acquisition. In Proc. of the International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Do</author>
<author>D Roth</author>
<author>M Sammons</author>
<author>Y Tu</author>
<author>V G Vydiswaran</author>
</authors>
<title>Robust, Light-weight Approaches to compute Lexical Similarity. Computer Science Research and Technical Reports,</title>
<date>2010</date>
<institution>University of Illinois.</institution>
<note>http://hdl.handle.net/2142/15462.</note>
<contexts>
<context position="25496" citStr="Do et al., 2010" startWordPosition="4182" endWordPosition="4185"> function and 1.07 words per constant. For example the function next to has the lexical entries: borders, next, adjacent and the constant illinois the lexical item illinois. 1: + c,dEx 1: s,tED 23 information. We add features which measure the lexical similarity between a constituent and a logical symbol’s surface forms (as defined by the lexicon). Two metrics are used: stemmed word match and a similarity metric based on WordNet (Miller et al., 1990) which allows our model to account for words not in the lexicon. The WordNet metric measures similarity based on synonymy, hyponymy and meronymy (Do et al., 2010). In the case where the constituent is a preposition, which are notorious for being ambiguous, we add a feature that considers the current lexical context (one word to the left and right) in addition to word similarity. Second-order decision features 4b2 Determining how to compose two logical symbols relies on syntactic information, in our model we use the dependency tree (Klein and Manning, 2003) of the input sentence. Given a second-order decision Q1-s,dt, the dependency feature takes the normalized distance between the head words in the constituents c and d. A set of features also indicate </context>
</contexts>
<marker>Do, Roth, Sammons, Tu, Vydiswaran, 2010</marker>
<rawString>Q. Do, D. Roth, M. Sammons, Y. Tu, and V.G. Vydiswaran. 2010. Robust, Light-weight Approaches to compute Lexical Similarity. Computer Science Research and Technical Reports, University of Illinois. http://hdl.handle.net/2142/15462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ge</author>
<author>R Mooney</author>
</authors>
<title>A statistical semantic parser that integrates syntax and semantics.</title>
<date>2005</date>
<booktitle>In Proc. of the Annual Conference on Computational Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="2343" citStr="Ge and Mooney, 2005" startWordPosition="353" endWordPosition="356">es to computer systems. In these settings the target meaning representation is defined by the semantics of the underlying task. For example, providing access to databases: a question posed in natural language is converted into a formal database query that can be executed to retrieve information. Example 1 shows a NL input query and its corresponding meaning representation. Example 1 Geoquery input text and output MR “What is the largest state that borders Texas?” largest(state(next to(const(texas)))) Previous works (Zelle and Mooney, 1996; Tang and Mooney, 2001; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007) employ machine learning techniques to construct a semantic parser. The learning algorithm is given a set of input sentences and their corresponding meaning representations, and learns a statistical semantic parser — a set of rules mapping lexical items and syntactic patterns to their meaning representation and a score associated with each rule. Given a sentence, these rules are applied recursively to derive the most probable meaning representation. Since semantic interpretation is limited to syntactic patterns identified in the training d</context>
<context position="10306" citStr="Ge and Mooney, 2005" startWordPosition="1633" endWordPosition="1636">ves defining the model (feature function Φ and inference problem) and a learning strategy to obtain weights (w) associated with the model. We defer discussion of our model until Section 4 and first focus on our learning strategy. 3 Structured Learning with Binary Feedback Previous approaches to semantic parsing have assumed a fully supervised setting where a training set is available consisting of either: input sentences and logical forms {(xl, zl)}Nl=1 (e.g., (Zettlemoyer and Collins, 2005)) or input sentences, logical forms and a mapping between their constituents {(xl, yl, zl)}Nl=1 (e.g., (Ge and Mooney, 2005)). Given such training examples a weight vector w can be learned using structured learning methods. Obtaining, through annotation or other means, this form of training data is an expensive and difficult process which presents a major bottleneck for semantic parsing. To reduce the burden of annotation we focus on a new learning paradigm which uses feedback from a teacher. The feedback signal is binary (+1, −1) and informs the learner whether a predicted logical form zˆ when executed on the target domain produces the desired response or outcome. This is a very natural method for providing superv</context>
<context position="34273" citStr="Ge and Mooney, 2005" startWordPosition="5605" endWordPosition="5608"> the first iteration. 7It is relatively difficult to compare different approaches in the Geoquery domain given that many existing papers do not use the same data split. 25 Learning Iterations Figure 2: Accuracy on training set as number of learning iterations increases. 7 Related Work Learning to map sentences to a meaning representation has been studied extensively in the NLP community. Early works (Zelle and Mooney, 1996; Tang and Mooney, 2000) employed inductive logic programming approaches to learn a semantic parser. More recent works apply statistical learning methods to the problem. In (Ge and Mooney, 2005; Nguyen et al., 2006), the input to the learner consists of complete syntactic derivations for the input sentences annotated with logical expressions. Other works (Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models. Learning is then defined over hidden patterns in the training data that associate logical symbols with lexical and syntactic elements. In this work we take an additional step towards allevi</context>
</contexts>
<marker>Ge, Mooney, 2005</marker>
<rawString>R. Ge and R. Mooney. 2005. A statistical semantic parser that integrates syntax and semantics. In Proc. of the Annual Conference on Computational Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kate</author>
<author>R Mooney</author>
</authors>
<title>Using string-kernels for learning semantic parsers.</title>
<date>2006</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="34482" citStr="Kate and Mooney, 2006" startWordPosition="5639" endWordPosition="5642">acy on training set as number of learning iterations increases. 7 Related Work Learning to map sentences to a meaning representation has been studied extensively in the NLP community. Early works (Zelle and Mooney, 1996; Tang and Mooney, 2000) employed inductive logic programming approaches to learn a semantic parser. More recent works apply statistical learning methods to the problem. In (Ge and Mooney, 2005; Nguyen et al., 2006), the input to the learner consists of complete syntactic derivations for the input sentences annotated with logical expressions. Other works (Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models. Learning is then defined over hidden patterns in the training data that associate logical symbols with lexical and syntactic elements. In this work we take an additional step towards alleviating the difficulty of training semantic parsers and present a world response based training protocol. Several recent works (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009) explore using an </context>
</contexts>
<marker>Kate, Mooney, 2006</marker>
<rawString>R. Kate and R. Mooney. 2006. Using string-kernels for learning semantic parsers. In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kate</author>
</authors>
<title>Transforming meaning representation grammars to improve semantic parsing.</title>
<date>2008</date>
<booktitle>In Proc. of the Annual Conference on Computational Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="18527" citStr="Kate, 2008" startWordPosition="3031" endWordPosition="3032">le 3. Despite the lexical similarity in these examples, the semantic parser will correctly parse the first sentence but fail to parse the second because the lexical items belong to different a syntactic category (i.e., the word Texas is not part of a preposition phrase in the second sentence). Example 3 Syntactic variations of the same MR Target logical form: capital(const(texas)) Sentence 1: “What is the capital of Texas?” Sentence 2: “What is Texas’ capital?” The ability to adapt to unseen inputs is one of the key challenges in semantic parsing. Several works (Zettlemoyer and Collins, 2007; Kate, 2008) have addressed this issue explicitly by manually defining syntactic transformation rules that can help the learned parser generalize better. Unfortunately these are only partial solutions as a 3Mistake driven algorithms that do not enforce margin constraints may not be able to generalize using this protocol since they will repeat the same prediction at training time and therefore will not update the model. manually constructed rule set cannot cover the many syntactic variations. Given the previous example, we observe that it is enough to identify that the function capital(·) and the constant </context>
</contexts>
<marker>Kate, 2008</marker>
<rawString>R. Kate. 2008. Transforming meaning representation grammars to improve semantic parsing. In Proc. of the Annual Conference on Computational Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>In Proc. of the Conference on Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="25896" citStr="Klein and Manning, 2003" startWordPosition="4250" endWordPosition="4253">and a similarity metric based on WordNet (Miller et al., 1990) which allows our model to account for words not in the lexicon. The WordNet metric measures similarity based on synonymy, hyponymy and meronymy (Do et al., 2010). In the case where the constituent is a preposition, which are notorious for being ambiguous, we add a feature that considers the current lexical context (one word to the left and right) in addition to word similarity. Second-order decision features 4b2 Determining how to compose two logical symbols relies on syntactic information, in our model we use the dependency tree (Klein and Manning, 2003) of the input sentence. Given a second-order decision Q1-s,dt, the dependency feature takes the normalized distance between the head words in the constituents c and d. A set of features also indicate which logical symbols are usually composed together, without considering their alignment to text. 5 Experiments In this section we describe our experimental setup, which includes the details of the domain, resources and parameters. 5.1 Domain and Corpus We evaluate our system on the Geoquery domain as described previously. The domain consists of a database and Prolog query language for U.S. geogra</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. D. Manning. 2003. Fast exact inference with a factored model for natural language parsing. In Proc. of the Conference on Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning semantic correspondences with less supervision.</title>
<date>2009</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="35040" citStr="Liang et al., 2009" startWordPosition="5727" endWordPosition="5730">s. Other works (Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models. Learning is then defined over hidden patterns in the training data that associate logical symbols with lexical and syntactic elements. In this work we take an additional step towards alleviating the difficulty of training semantic parsers and present a world response based training protocol. Several recent works (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009) explore using an external world context as a supervision signal for semantic interpretation. These works operate in settings different to ours as they rely on an external world state that is directly referenced by the input text. Although our framework can also be applied in these settings we do not assume that the text can be grounded in a world state. In our experiments the input text consists of generalized statements which describe some information need that does not correspond directly to a grounded world state. Our learning framework closely follows recent work o</context>
</contexts>
<marker>Liang, Jordan, Klein, 2009</marker>
<rawString>P. Liang, M. I. Jordan, and D. Klein. 2009. Learning semantic correspondences with less supervision. In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>R Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K J Miller</author>
</authors>
<title>Wordnet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal ofLexicography.</journal>
<contexts>
<context position="25334" citStr="Miller et al., 1990" startWordPosition="4153" endWordPosition="4156"> our setting we do not have access to annotated logical forms, instead we rely on external knowledge to supply further 5The lexicon contains on average 1.42 words per function and 1.07 words per constant. For example the function next to has the lexical entries: borders, next, adjacent and the constant illinois the lexical item illinois. 1: + c,dEx 1: s,tED 23 information. We add features which measure the lexical similarity between a constituent and a logical symbol’s surface forms (as defined by the lexicon). Two metrics are used: stemmed word match and a similarity metric based on WordNet (Miller et al., 1990) which allows our model to account for words not in the lexicon. The WordNet metric measures similarity based on synonymy, hyponymy and meronymy (Do et al., 2010). In the case where the constituent is a preposition, which are notorious for being ambiguous, we add a feature that considers the current lexical context (one word to the left and right) in addition to word similarity. Second-order decision features 4b2 Determining how to compose two logical symbols relies on syntactic information, in our model we use the dependency tree (Klein and Manning, 2003) of the input sentence. Given a second</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>G. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K.J. Miller. 1990. Wordnet: An on-line lexical database. International Journal ofLexicography.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Nguyen</author>
<author>A Shimazu</author>
<author>X Phan</author>
</authors>
<title>Semantic parsing with structured svm ensemble classification models.</title>
<date>2006</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="34295" citStr="Nguyen et al., 2006" startWordPosition="5609" endWordPosition="5612"> 7It is relatively difficult to compare different approaches in the Geoquery domain given that many existing papers do not use the same data split. 25 Learning Iterations Figure 2: Accuracy on training set as number of learning iterations increases. 7 Related Work Learning to map sentences to a meaning representation has been studied extensively in the NLP community. Early works (Zelle and Mooney, 1996; Tang and Mooney, 2000) employed inductive logic programming approaches to learn a semantic parser. More recent works apply statistical learning methods to the problem. In (Ge and Mooney, 2005; Nguyen et al., 2006), the input to the learner consists of complete syntactic derivations for the input sentences annotated with logical expressions. Other works (Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models. Learning is then defined over hidden patterns in the training data that associate logical symbols with lexical and syntactic elements. In this work we take an additional step towards alleviating the difficulty o</context>
</contexts>
<marker>Nguyen, Shimazu, Phan, 2006</marker>
<rawString>L. Nguyen, A. Shimazu, and X. Phan. 2006. Semantic parsing with structured svm ensemble classification models. In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>Global inference for entity and relation identification via a linear programming formulation.</title>
<date>2007</date>
<booktitle>In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning.</booktitle>
<contexts>
<context position="24139" citStr="Roth and Yih, 2007" startWordPosition="3958" endWordPosition="3961">aints which leverage the typing information inherent in the domain to eliminate logical forms that are invalid in the Geoquery domain. For example, the function length only accepts river types as input. The set of constraints are: • A given constituent can be associated with exactly one logical symbol. • Qcs,dt is active if and only if αcs and αdt are active. • If Qcs,dt is active, s must be a function and the types of s and t should be consistent. • Functional composition is directional and acyclic. The flexibility of ILP has previously been advantageous in natural language processing tasks (Roth and Yih, 2007) as it allows us to easily incorporate such constraints. 4.3 Features The inference problem defined in Equation (2) uses two feature functions: b1 and b2. First-order decision features b1 Determining if a logical symbol is aligned with a specific constituent depends mostly on lexical information. Following previous work (e.g., (Zettlemoyer and Collins, 2005)) we create a small lexicon, mapping logical symbols to surface forms.5 This lexicon is small and only used as a starting point. Existing approaches rely on annotated logical forms to extend the lexicon. However, in our setting we do not ha</context>
</contexts>
<marker>Roth, Yih, 2007</marker>
<rawString>D. Roth and W. Yih. 2007. Global inference for entity and relation identification via a linear programming formulation. In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Tang</author>
<author>R Mooney</author>
</authors>
<title>Automated construction of database interfaces: integrating statistical and relational learning for semantic parsing.</title>
<date>2000</date>
<booktitle>In Proc. of the Conference on Empirical Methods for Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="34104" citStr="Tang and Mooney, 2000" startWordPosition="5577" endWordPosition="5580">the performance of the DIRECT approach drops at the first iteration. We hypothesize this is due to imbalances in the binary feedback dataset (too many negative examples) in the first iteration. 7It is relatively difficult to compare different approaches in the Geoquery domain given that many existing papers do not use the same data split. 25 Learning Iterations Figure 2: Accuracy on training set as number of learning iterations increases. 7 Related Work Learning to map sentences to a meaning representation has been studied extensively in the NLP community. Early works (Zelle and Mooney, 1996; Tang and Mooney, 2000) employed inductive logic programming approaches to learn a semantic parser. More recent works apply statistical learning methods to the problem. In (Ge and Mooney, 2005; Nguyen et al., 2006), the input to the learner consists of complete syntactic derivations for the input sentences annotated with logical expressions. Other works (Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models. Learning is then def</context>
</contexts>
<marker>Tang, Mooney, 2000</marker>
<rawString>L. Tang and R. Mooney. 2000. Automated construction of database interfaces: integrating statistical and relational learning for semantic parsing. In Proc. of the Conference on Empirical Methods for Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Tang</author>
<author>R J Mooney</author>
</authors>
<title>Using multiple clause constructors in inductive logic programming for semantic parsing.</title>
<date>2001</date>
<booktitle>In Proc. of the European Conference on Machine Learning (ECML).</booktitle>
<contexts>
<context position="2291" citStr="Tang and Mooney, 2001" startWordPosition="345" endWordPosition="348"> in the context of providing natural language interfaces to computer systems. In these settings the target meaning representation is defined by the semantics of the underlying task. For example, providing access to databases: a question posed in natural language is converted into a formal database query that can be executed to retrieve information. Example 1 shows a NL input query and its corresponding meaning representation. Example 1 Geoquery input text and output MR “What is the largest state that borders Texas?” largest(state(next to(const(texas)))) Previous works (Zelle and Mooney, 1996; Tang and Mooney, 2001; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007) employ machine learning techniques to construct a semantic parser. The learning algorithm is given a set of input sentences and their corresponding meaning representations, and learns a statistical semantic parser — a set of rules mapping lexical items and syntactic patterns to their meaning representation and a score associated with each rule. Given a sentence, these rules are applied recursively to derive the most probable meaning representation. Since semantic interpretation is limite</context>
<context position="6255" citStr="Tang and Mooney, 2001" startWordPosition="959" endWordPosition="962">We model semantic interpretation as a sequence of interdependent decisions, mapping text spans to predicates and use syntactic information to determine how the meaning of these logical fragments should be composed. We frame this process as an Integer Linear Programming (ILP) problem, a powerful and flexible inference framework that allows us to inject relevant domain knowledge into the inference process, such as specific domain semantics that restrict the space of possible interpretations. We evaluate our learning approach and model on the well studied Geoquery domain (Zelle and Mooney, 1996; Tang and Mooney, 2001), a database consisting of U.S. geographical information, and natural language questions. Our experimental results show that our model with response driven learning can outperform existing models trained with annotated logical forms. The key contributions of this paper are: Response driven learning for semantic parsing We propose a new learning paradigm for learning semantic parsers without any annotated meaning representations. The supervision for learning comes from a binary feedback signal based a response generated by executing a meaning representation. This type of supervision signal is n</context>
</contexts>
<marker>Tang, Mooney, 2001</marker>
<rawString>L. R. Tang and R. J. Mooney. 2001. Using multiple clause constructors in inductive logic programming for semantic parsing. In Proc. of the European Conference on Machine Learning (ECML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-W Wong</author>
<author>R Mooney</author>
</authors>
<title>Learning for semantic parsing with statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of the Annual Meeting of the North American Association of Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="31874" citStr="Wong and Mooney, 2006" startWordPosition="5222" endWordPosition="5225">5.2% on the R250 data and 69.2% on the Q250 (testing) data. While the AGGRESSIVE approach which uses a structured learner sees a bigger improvement, reaching 82.4% and 73.2% respectively. This is only 7% below the fully SUPERVISED upper bound of the model. To answer the second question, we compare a supervised version of our model to existing semantic parsers. The results are in Table 2. Although the numbers are not directly comparable due to different splits in the data7, we can see that with a similar number of logical forms for training our SUPERVISED approach outperforms existing systems (Wong and Mooney, 2006; Wong and Mooney, 2007), while the AGGRESSIVE approach remains competitive without using any logical forms. Our SUPERVISED model is still very competitive with other approaches (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007), which used considerably more annotated logical forms in the training phase. In order to answer the third question, we turn our attention to the differences between the two response driven learning approaches. The DIRECT and AGGRESSIVE approaches use binary feedback to learn, however they utilize the signal differently. DIRECT uses the signal directly to learn a bi</context>
<context position="34459" citStr="Wong and Mooney, 2006" startWordPosition="5635" endWordPosition="5638">rations Figure 2: Accuracy on training set as number of learning iterations increases. 7 Related Work Learning to map sentences to a meaning representation has been studied extensively in the NLP community. Early works (Zelle and Mooney, 1996; Tang and Mooney, 2000) employed inductive logic programming approaches to learn a semantic parser. More recent works apply statistical learning methods to the problem. In (Ge and Mooney, 2005; Nguyen et al., 2006), the input to the learner consists of complete syntactic derivations for the input sentences annotated with logical expressions. Other works (Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models. Learning is then defined over hidden patterns in the training data that associate logical symbols with lexical and syntactic elements. In this work we take an additional step towards alleviating the difficulty of training semantic parsers and present a world response based training protocol. Several recent works (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., </context>
</contexts>
<marker>Wong, Mooney, 2006</marker>
<rawString>Y.-W. Wong and R. Mooney. 2006. Learning for semantic parsing with statistical machine translation. In Proc. of the Annual Meeting of the North American Association of Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-W Wong</author>
<author>R Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="2398" citStr="Wong and Mooney, 2007" startWordPosition="361" endWordPosition="364"> meaning representation is defined by the semantics of the underlying task. For example, providing access to databases: a question posed in natural language is converted into a formal database query that can be executed to retrieve information. Example 1 shows a NL input query and its corresponding meaning representation. Example 1 Geoquery input text and output MR “What is the largest state that borders Texas?” largest(state(next to(const(texas)))) Previous works (Zelle and Mooney, 1996; Tang and Mooney, 2001; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007) employ machine learning techniques to construct a semantic parser. The learning algorithm is given a set of input sentences and their corresponding meaning representations, and learns a statistical semantic parser — a set of rules mapping lexical items and syntactic patterns to their meaning representation and a score associated with each rule. Given a sentence, these rules are applied recursively to derive the most probable meaning representation. Since semantic interpretation is limited to syntactic patterns identified in the training data, the learning algorithm requires considerable amoun</context>
<context position="27752" citStr="Wong and Mooney, 2007" startWordPosition="4553" endWordPosition="4556">raining data to 250 examples due to scalability issues. We also prune the search space by limiting the number of logical symbol candidates per word (on average 13 logical symbols per word). Precision and recall are typically used as evaluation metrics in semantic parsing. However, as our model inherently has the ability to map any input sentence into the space of meaning representations the trade off between precision and recall does not exist. Thus, we report accuracy: the percentage of meaning representations which produce the correct response. This is equivalent to recall in previous work (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). 5.2 Resources and Parameters Feedback Recall that our learning framework does not require meaning representation annotations. However, we do require a Feedback function that informs the learner whether a predicted meaning representation when executed produces the desired response for a given input sentence. We automatically generate a set of natural language queries and response pairs {(x, r)} by executing the annotated logical forms on the database. Using this data we construct an automatic feedback function as described in Sect</context>
<context position="31898" citStr="Wong and Mooney, 2007" startWordPosition="5226" endWordPosition="5229">nd 69.2% on the Q250 (testing) data. While the AGGRESSIVE approach which uses a structured learner sees a bigger improvement, reaching 82.4% and 73.2% respectively. This is only 7% below the fully SUPERVISED upper bound of the model. To answer the second question, we compare a supervised version of our model to existing semantic parsers. The results are in Table 2. Although the numbers are not directly comparable due to different splits in the data7, we can see that with a similar number of logical forms for training our SUPERVISED approach outperforms existing systems (Wong and Mooney, 2006; Wong and Mooney, 2007), while the AGGRESSIVE approach remains competitive without using any logical forms. Our SUPERVISED model is still very competitive with other approaches (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007), which used considerably more annotated logical forms in the training phase. In order to answer the third question, we turn our attention to the differences between the two response driven learning approaches. The DIRECT and AGGRESSIVE approaches use binary feedback to learn, however they utilize the signal differently. DIRECT uses the signal directly to learn a binary classifier capable </context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Y.-W. Wong and R. Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Zelle</author>
<author>R J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic proramming.</title>
<date>1996</date>
<booktitle>In Proc. of the National Conference on Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="2268" citStr="Zelle and Mooney, 1996" startWordPosition="341" endWordPosition="344"> has mainly been studied in the context of providing natural language interfaces to computer systems. In these settings the target meaning representation is defined by the semantics of the underlying task. For example, providing access to databases: a question posed in natural language is converted into a formal database query that can be executed to retrieve information. Example 1 shows a NL input query and its corresponding meaning representation. Example 1 Geoquery input text and output MR “What is the largest state that borders Texas?” largest(state(next to(const(texas)))) Previous works (Zelle and Mooney, 1996; Tang and Mooney, 2001; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007) employ machine learning techniques to construct a semantic parser. The learning algorithm is given a set of input sentences and their corresponding meaning representations, and learns a statistical semantic parser — a set of rules mapping lexical items and syntactic patterns to their meaning representation and a score associated with each rule. Given a sentence, these rules are applied recursively to derive the most probable meaning representation. Since semantic i</context>
<context position="6231" citStr="Zelle and Mooney, 1996" startWordPosition="955" endWordPosition="958">h a flexible framework. We model semantic interpretation as a sequence of interdependent decisions, mapping text spans to predicates and use syntactic information to determine how the meaning of these logical fragments should be composed. We frame this process as an Integer Linear Programming (ILP) problem, a powerful and flexible inference framework that allows us to inject relevant domain knowledge into the inference process, such as specific domain semantics that restrict the space of possible interpretations. We evaluate our learning approach and model on the well studied Geoquery domain (Zelle and Mooney, 1996; Tang and Mooney, 2001), a database consisting of U.S. geographical information, and natural language questions. Our experimental results show that our model with response driven learning can outperform existing models trained with annotated logical forms. The key contributions of this paper are: Response driven learning for semantic parsing We propose a new learning paradigm for learning semantic parsers without any annotated meaning representations. The supervision for learning comes from a binary feedback signal based a response generated by executing a meaning representation. This type of</context>
<context position="21518" citStr="Zelle and Mooney, 1996" startWordPosition="3509" endWordPosition="3512">and numeric values N. Functions describe a functional relationship over types (e.g., population : E → N). A complete logical form is constructed through functional composition; in our formalism this is per4This is true for all meaning representations designed to be executed by a computer system. 22 formed by the substitution operator. For example, given the function next to(x) and the expression const(texas), substitution replaces the occurrence of the free variable x, with the expression, resulting in a new logical form: next to(const(texas)). Due to space limitations we refer the reader to (Zelle and Mooney, 1996) for a detailed description of the Geoquery domain. 4.2 Semantic Parsing as Constrained Optimization Recall that the goal of semantic parsing is to produce the following function (Equation (1)): FK,(x) = arg max wT b(x, y, z) y,z However, given that y and z are complex structures it is necessary to decompose the structure into a set of smaller decisions to facilitate efficient inference. In order to define our decomposition we introduce additional notation: c is a constituent (or word span) in the input sentence x and D is the set of all function and constant symbols in the domain. The alignme</context>
<context position="34080" citStr="Zelle and Mooney, 1996" startWordPosition="5573" endWordPosition="5576"> than DIRECT. Note that the performance of the DIRECT approach drops at the first iteration. We hypothesize this is due to imbalances in the binary feedback dataset (too many negative examples) in the first iteration. 7It is relatively difficult to compare different approaches in the Geoquery domain given that many existing papers do not use the same data split. 25 Learning Iterations Figure 2: Accuracy on training set as number of learning iterations increases. 7 Related Work Learning to map sentences to a meaning representation has been studied extensively in the NLP community. Early works (Zelle and Mooney, 1996; Tang and Mooney, 2000) employed inductive logic programming approaches to learn a semantic parser. More recent works apply statistical learning methods to the problem. In (Ge and Mooney, 2005; Nguyen et al., 2006), the input to the learner consists of complete syntactic derivations for the input sentences annotated with logical expressions. Other works (Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the mode</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>J. M. Zelle and R. J. Mooney. 1996. Learning to parse database queries using inductive logic proramming. In Proc. of the National Conference on Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Proc. of the Annual Conference in Uncertainty in Artificial Intelligence (UAI).</booktitle>
<contexts>
<context position="2322" citStr="Zettlemoyer and Collins, 2005" startWordPosition="349" endWordPosition="352">iding natural language interfaces to computer systems. In these settings the target meaning representation is defined by the semantics of the underlying task. For example, providing access to databases: a question posed in natural language is converted into a formal database query that can be executed to retrieve information. Example 1 shows a NL input query and its corresponding meaning representation. Example 1 Geoquery input text and output MR “What is the largest state that borders Texas?” largest(state(next to(const(texas)))) Previous works (Zelle and Mooney, 1996; Tang and Mooney, 2001; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007) employ machine learning techniques to construct a semantic parser. The learning algorithm is given a set of input sentences and their corresponding meaning representations, and learns a statistical semantic parser — a set of rules mapping lexical items and syntactic patterns to their meaning representation and a score associated with each rule. Given a sentence, these rules are applied recursively to derive the most probable meaning representation. Since semantic interpretation is limited to syntactic patterns identif</context>
<context position="10182" citStr="Zettlemoyer and Collins, 2005" startWordPosition="1614" endWordPosition="1617"> obtain a response which we denote with r. The alignment y captures the associations between x and z. Building a semantic parser involves defining the model (feature function Φ and inference problem) and a learning strategy to obtain weights (w) associated with the model. We defer discussion of our model until Section 4 and first focus on our learning strategy. 3 Structured Learning with Binary Feedback Previous approaches to semantic parsing have assumed a fully supervised setting where a training set is available consisting of either: input sentences and logical forms {(xl, zl)}Nl=1 (e.g., (Zettlemoyer and Collins, 2005)) or input sentences, logical forms and a mapping between their constituents {(xl, yl, zl)}Nl=1 (e.g., (Ge and Mooney, 2005)). Given such training examples a weight vector w can be learned using structured learning methods. Obtaining, through annotation or other means, this form of training data is an expensive and difficult process which presents a major bottleneck for semantic parsing. To reduce the burden of annotation we focus on a new learning paradigm which uses feedback from a teacher. The feedback signal is binary (+1, −1) and informs the learner whether a predicted logical form zˆ whe</context>
<context position="24499" citStr="Zettlemoyer and Collins, 2005" startWordPosition="4012" endWordPosition="4015"> are active. • If Qcs,dt is active, s must be a function and the types of s and t should be consistent. • Functional composition is directional and acyclic. The flexibility of ILP has previously been advantageous in natural language processing tasks (Roth and Yih, 2007) as it allows us to easily incorporate such constraints. 4.3 Features The inference problem defined in Equation (2) uses two feature functions: b1 and b2. First-order decision features b1 Determining if a logical symbol is aligned with a specific constituent depends mostly on lexical information. Following previous work (e.g., (Zettlemoyer and Collins, 2005)) we create a small lexicon, mapping logical symbols to surface forms.5 This lexicon is small and only used as a starting point. Existing approaches rely on annotated logical forms to extend the lexicon. However, in our setting we do not have access to annotated logical forms, instead we rely on external knowledge to supply further 5The lexicon contains on average 1.42 words per function and 1.07 words per constant. For example the function next to has the lexical entries: borders, next, adjacent and the constant illinois the lexical item illinois. 1: + c,dEx 1: s,tED 23 information. We add fe</context>
<context position="27783" citStr="Zettlemoyer and Collins, 2005" startWordPosition="4557" endWordPosition="4560">mples due to scalability issues. We also prune the search space by limiting the number of logical symbol candidates per word (on average 13 logical symbols per word). Precision and recall are typically used as evaluation metrics in semantic parsing. However, as our model inherently has the ability to map any input sentence into the space of meaning representations the trade off between precision and recall does not exist. Thus, we report accuracy: the percentage of meaning representations which produce the correct response. This is equivalent to recall in previous work (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). 5.2 Resources and Parameters Feedback Recall that our learning framework does not require meaning representation annotations. However, we do require a Feedback function that informs the learner whether a predicted meaning representation when executed produces the desired response for a given input sentence. We automatically generate a set of natural language queries and response pairs {(x, r)} by executing the annotated logical forms on the database. Using this data we construct an automatic feedback function as described in Section 3. Domain knowledge Our lea</context>
<context position="34513" citStr="Zettlemoyer and Collins, 2005" startWordPosition="5643" endWordPosition="5646">number of learning iterations increases. 7 Related Work Learning to map sentences to a meaning representation has been studied extensively in the NLP community. Early works (Zelle and Mooney, 1996; Tang and Mooney, 2000) employed inductive logic programming approaches to learn a semantic parser. More recent works apply statistical learning methods to the problem. In (Ge and Mooney, 2005; Nguyen et al., 2006), the input to the learner consists of complete syntactic derivations for the input sentences annotated with logical expressions. Other works (Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models. Learning is then defined over hidden patterns in the training data that associate logical symbols with lexical and syntactic elements. In this work we take an additional step towards alleviating the difficulty of training semantic parsers and present a world response based training protocol. Several recent works (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009) explore using an external world context as a sup</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>L. Zettlemoyer and M. Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proc. of the Annual Conference in Uncertainty in Artificial Intelligence (UAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Online learning of relaxed CCG grammars for parsing to logical form.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and on Computational Natural Language Learning (EMNLP-CoNLL).</booktitle>
<contexts>
<context position="2374" citStr="Zettlemoyer and Collins, 2007" startWordPosition="357" endWordPosition="360">s. In these settings the target meaning representation is defined by the semantics of the underlying task. For example, providing access to databases: a question posed in natural language is converted into a formal database query that can be executed to retrieve information. Example 1 shows a NL input query and its corresponding meaning representation. Example 1 Geoquery input text and output MR “What is the largest state that borders Texas?” largest(state(next to(const(texas)))) Previous works (Zelle and Mooney, 1996; Tang and Mooney, 2001; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007) employ machine learning techniques to construct a semantic parser. The learning algorithm is given a set of input sentences and their corresponding meaning representations, and learns a statistical semantic parser — a set of rules mapping lexical items and syntactic patterns to their meaning representation and a score associated with each rule. Given a sentence, these rules are applied recursively to derive the most probable meaning representation. Since semantic interpretation is limited to syntactic patterns identified in the training data, the learning algorithm req</context>
<context position="18514" citStr="Zettlemoyer and Collins, 2007" startWordPosition="3027" endWordPosition="3030">is given the sentences in Example 3. Despite the lexical similarity in these examples, the semantic parser will correctly parse the first sentence but fail to parse the second because the lexical items belong to different a syntactic category (i.e., the word Texas is not part of a preposition phrase in the second sentence). Example 3 Syntactic variations of the same MR Target logical form: capital(const(texas)) Sentence 1: “What is the capital of Texas?” Sentence 2: “What is Texas’ capital?” The ability to adapt to unseen inputs is one of the key challenges in semantic parsing. Several works (Zettlemoyer and Collins, 2007; Kate, 2008) have addressed this issue explicitly by manually defining syntactic transformation rules that can help the learned parser generalize better. Unfortunately these are only partial solutions as a 3Mistake driven algorithms that do not enforce margin constraints may not be able to generalize using this protocol since they will repeat the same prediction at training time and therefore will not update the model. manually constructed rule set cannot cover the many syntactic variations. Given the previous example, we observe that it is enough to identify that the function capital(·) and </context>
<context position="27815" citStr="Zettlemoyer and Collins, 2007" startWordPosition="4561" endWordPosition="4564">. We also prune the search space by limiting the number of logical symbol candidates per word (on average 13 logical symbols per word). Precision and recall are typically used as evaluation metrics in semantic parsing. However, as our model inherently has the ability to map any input sentence into the space of meaning representations the trade off between precision and recall does not exist. Thus, we report accuracy: the percentage of meaning representations which produce the correct response. This is equivalent to recall in previous work (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). 5.2 Resources and Parameters Feedback Recall that our learning framework does not require meaning representation annotations. However, we do require a Feedback function that informs the learner whether a predicted meaning representation when executed produces the desired response for a given input sentence. We automatically generate a set of natural language queries and response pairs {(x, r)} by executing the annotated logical forms on the database. Using this data we construct an automatic feedback function as described in Section 3. Domain knowledge Our learning approaches require an init</context>
<context position="32082" citStr="Zettlemoyer and Collins, 2007" startWordPosition="5253" endWordPosition="5256"> 7% below the fully SUPERVISED upper bound of the model. To answer the second question, we compare a supervised version of our model to existing semantic parsers. The results are in Table 2. Although the numbers are not directly comparable due to different splits in the data7, we can see that with a similar number of logical forms for training our SUPERVISED approach outperforms existing systems (Wong and Mooney, 2006; Wong and Mooney, 2007), while the AGGRESSIVE approach remains competitive without using any logical forms. Our SUPERVISED model is still very competitive with other approaches (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007), which used considerably more annotated logical forms in the training phase. In order to answer the third question, we turn our attention to the differences between the two response driven learning approaches. The DIRECT and AGGRESSIVE approaches use binary feedback to learn, however they utilize the signal differently. DIRECT uses the signal directly to learn a binary classifier capable of replicating the feedback, whereas AGGRESSIVE learns a structured predictor that can repeatedly obtain the logical forms for which positive feedback was received. Thus, although the </context>
<context position="34544" citStr="Zettlemoyer and Collins, 2007" startWordPosition="5647" endWordPosition="5650">ncreases. 7 Related Work Learning to map sentences to a meaning representation has been studied extensively in the NLP community. Early works (Zelle and Mooney, 1996; Tang and Mooney, 2000) employed inductive logic programming approaches to learn a semantic parser. More recent works apply statistical learning methods to the problem. In (Ge and Mooney, 2005; Nguyen et al., 2006), the input to the learner consists of complete syntactic derivations for the input sentences annotated with logical expressions. Other works (Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models. Learning is then defined over hidden patterns in the training data that associate logical symbols with lexical and syntactic elements. In this work we take an additional step towards alleviating the difficulty of training semantic parsers and present a world response based training protocol. Several recent works (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009) explore using an external world context as a supervision signal for semantic in</context>
</contexts>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>L. Zettlemoyer and M. Collins. 2007. Online learning of relaxed CCG grammars for parsing to logical form. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and on Computational Natural Language Learning (EMNLP-CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Learning context-dependent mappings from sentences to logical form.</title>
<date>2009</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="34576" citStr="Zettlemoyer and Collins, 2009" startWordPosition="5651" endWordPosition="5654">ng to map sentences to a meaning representation has been studied extensively in the NLP community. Early works (Zelle and Mooney, 1996; Tang and Mooney, 2000) employed inductive logic programming approaches to learn a semantic parser. More recent works apply statistical learning methods to the problem. In (Ge and Mooney, 2005; Nguyen et al., 2006), the input to the learner consists of complete syntactic derivations for the input sentences annotated with logical expressions. Other works (Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models. Learning is then defined over hidden patterns in the training data that associate logical symbols with lexical and syntactic elements. In this work we take an additional step towards alleviating the difficulty of training semantic parsers and present a world response based training protocol. Several recent works (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009) explore using an external world context as a supervision signal for semantic interpretation. These works operat</context>
</contexts>
<marker>Zettlemoyer, Collins, 2009</marker>
<rawString>L. Zettlemoyer and M. Collins. 2009. Learning context-dependent mappings from sentences to logical form. In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>