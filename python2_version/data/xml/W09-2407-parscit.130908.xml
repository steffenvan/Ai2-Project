<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008645">
<title confidence="0.989333">
Making Semantic Topicality Robust Through Term Abstraction∗
</title>
<author confidence="0.998143">
Paul M. Heider
</author>
<affiliation confidence="0.990125">
Department of Linguistics
University at Buffalo
The State University of New York
</affiliation>
<address confidence="0.843822">
Buffalo, NY 14260, USA
</address>
<email confidence="0.999175">
pmheider@buffalo.edu
</email>
<author confidence="0.559955">
Rohini K. Srihari
</author>
<affiliation confidence="0.520322">
Janya, Inc.
</affiliation>
<address confidence="0.942604">
1408 Sweet Home Road
Suite 1
Amherst, NY 14228
</address>
<email confidence="0.999509">
rohini@cedar.buffalo.edu
</email>
<sectionHeader confidence="0.995716" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996937">
Despite early intuitions, semantic similarity
has not proven to be robust for splitting multi-
party interactions into separate conversations.
We discuss some initial successes with using
thesaural headwords to abstract the seman-
tics of an utterance. This simple profiling
technique showed improvements over base-
line conversation threading models.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.982140565217391">
Topic segmentation is the problem of dividing a
document into smaller coherent units. The seg-
ments can be hierarchical or linear; the topics can
be localized or distributed; the documents can be
newswire or chat logs. Of course, each of these
variables is best analyzed as continuous rather than
discrete. Newswire, for instance, is a more formal,
monologue-style genre while a chat log tends to-
wards the informal register with different conversa-
tions interwoven.
We present a topic segmenter which uses seman-
tics to define coherent conversations within a larger,
multi-party document. Using a word’s thesaurus en-
try as a proxy for its underlying semantics provides
a domain-neutral metric for distinguishing conver-
sations. Also, our classifier does not rely on met-
alinguistic properties that may not be robust across
genres.
∗ The first author was partially funded through a fel-
lowship from the SUNY at Buffalo Department of Linguis-
tics and partially through a research assistantship at Janya,
Inc. (http://www.janyainc.com, Air Force Grant No.s
FA8750-07-C-0077 and FA8750-07-D-0019, Task Order 0004)
</bodyText>
<sectionHeader confidence="0.987406" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999865366666667">
Most work on lexical cohesion extends from Halli-
day and Hasan (1976). They formalize a text as any
semantic unit realized through sentences. Linguistic
features found to justify binding sentences together
into Halliday and Hasan’s notion of a text include
pronouns (Hobbs, 1979; Kehler, 2000), lexical over-
lap (Hearst, 1997; Kozima, 1993; Morris and Hirst,
1991), cue phrases (Manning, 1998), and discourse
markers (Power et al., 2003; Reynar, 1999; Beefer-
man et al., 1999), among others. Of course, most
of this earlier work assumes the sentences constitut-
ing any text are contiguous. Thus, a document is
comprised of a series of semantic units that progress
from one to the next with no returns to old topics.
Multi-party interactions1 abide by a different set
of assumptions. Namely, a multi-party interaction
can include multiple floors (Aoki et al., 2006). Much
like at a cocktail party, we can expect more than a
single conversation at every given time. These dif-
ferent conversational floors are the major semantic
units a topic segmentation algorithm must recog-
nize. Spoken chat models (Aoki et al., 2006; Aoki
et al., 2003) can make a simplifying assumption that
speakers tend to only participate in one conversation
at a time. However, in text chat models, Elsner and
Charniak (2008) show that speakers seem to partici-
pate in more conversations roughly as a function of
how talkative they are (cf. Camtepe et al., 2005).
In both modalities, speaker tendency to stay on the
same topics is a robust cue for conversational coher-
</bodyText>
<footnote confidence="0.9925175">
1See O’Neill and Martin (2003) for an analysis of differ-
ences between two- and multi-party interactions.
</footnote>
<page confidence="0.991204">
46
</page>
<note confidence="0.8210035">
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 46–51,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.997294976190476">
ence (Elsner and Charniak, 2008; Acar et al., 2005).
Despite the initial intuitions of Halliday and
Hasan (1976), semantic similarity has not proven
to be a robust cue for multi-party topic segmen-
tation. For instance, Acar et al. (2005) and Gal-
ley et al. (2003) used word repetition in their defi-
nition of coherence but found that words common
to too many conversations hurt modeling perfor-
mance. Elsner and Charniak (2008) used frequency
binning based on the entire document to reduce the
noise introduced by high-frequency words. Un-
fortunately, binning requires a priori knowledge of
the relative frequencies of words.2 Additionally,
those authors used an on-topic/off-topic word list
to bifurcate technical and non-technical utterances.
Again, this technique assumes prior knowledge of
the strongest on-topic cue words.
Since semantic repetition is clearly useful but
simple word repetition is not a reliable measure, we
investigated other measures of semantic relatedness.
Elsner and Charniak (2008) conceded that context-
based measures like LSA (Deerwester et al., 1990)
require a clear notion of document boundary to func-
tion well. Dictionary-based models (Kozima and
Furugori, 1993) are a step in the right direction be-
cause they leverage word co-occurrence within defi-
nitions to measure relatedness. The richer set of con-
nections available in WordNet models should pro-
vide an even better measure of relatedness (Sussna,
1993; Resnik, 1995). Unfortunately, these mea-
sures have unequal distribution by part-of-speech
and uneven density of lemmas by semantic domain.3
Thesaurus-based models (Morris and Hirst, 1991)
provide many of the same advantages as dictionary-
and WordNet-based models.4 In addition to the hier-
archical relations encoded by the thesaurus, we can
treat each thesaural category as one dimension of
a topicality domain similar to the way Elsner and
Charniak leveraged their list of technical terms. In
sum, our model focuses on the abstraction of lem-
mas that is inherent to a thesaurus while limiting
the domain-specific and a priori knowledge required
</bodyText>
<footnote confidence="0.946736">
2One could use frequencies from a general corpus but that
should only perform as well as a graded stop-word list.
3As one reviewer noted, some parts-of-speech may con-
tribute more to a topic profile than others. Unfortunately, this
empirical question must wait to be tested.
4Budanitsky and Hirst (2006) review the advantages.
</footnote>
<bodyText confidence="0.8739545">
by a classifier to divide multi-party interactions into
separate conversational floors.
</bodyText>
<sectionHeader confidence="0.989156" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999976357142857">
At a high level, our chat topic segmenter works like
most other classifiers: each input line is tokenized5,
passed to a feature analyzer, and clustered with re-
lated lines. Unlike traditional topic segmentation
models, each input represents a new utterance in the
chat log. These utterances can range from single
words to multiple sentences. Another aspect of our
model (although not unique to it) is the on-line clas-
sification of text. We aim to model topic segmenta-
tion as if our classifier were sitting in a chat room,
and not as a post-process.
While our feature analyzer focuses on semantic
markers, interlocutor names are also recorded. Two
intuitions were implemented with respect to individ-
uals’ names: continued affiliation with old conver-
sations and naming interlocutors to focus attention.
All else being equal, one would assume a speaker
will continue in the conversations she has already
participated in. Moreover, she will most likely con-
tinue with the last conversation she was part of. As
the total number of conversations increases, the like-
lihood of sticking to the last conversation will de-
crease.
The second intuition derives from the observa-
tion in O’Neill and Martin (2003) that speakers ac-
commodate for cocktail-style conversations by using
direct mentions of interlocutors’ names. We only
model backward referencing names. That is, if a
speaker uses the name of another user, we assume
that the speaker is overtly affiliating with a conver-
sation of the other user. Forward referencing is dis-
cussed under future work (see Section 6).
Following Budanitsky and Hirst (2006), we base
our notion of semantic topicality on thesaural rela-
tions. Broadly speaking, two utterances are highly
related if their tokenized words (hereafter, lemmas)
co-occur in more of the same thesaural categories
than not. We will defer further explanation of these
features until we have explained our reference the-
sauri in Subsection 3.1. Unfortunately, many desir-
able and robust features are missing from our classi-
fier. See Section 6 for a discussion of future work.
</bodyText>
<footnote confidence="0.932584">
5We used SemantexTM (Srihari et al., 2008).
</footnote>
<page confidence="0.998818">
47
</page>
<bodyText confidence="0.99997325">
In the final stage of our processing pipeline, we
use a panel of experts to generate a simple weighted
classification. Each feature described above con-
tributes a roughly equal vote towards the final sort-
ing decision. Barring a single strong preference or a
cohort of weak preferences for one conversation, the
model assumes the incoming utterance introduces a
new conversational floor.
</bodyText>
<subsectionHeader confidence="0.997441">
3.1 Thesauri
</subsectionHeader>
<bodyText confidence="0.999983411764706">
We chose two machine-readable and public-domain
thesauri for our model: Roget’s Thesaurus (1911)
and Moby Thesaurus II (2002). Both are avail-
able from Project Gutenberg (gutenberg.org). In the
compilation notes for Roget’s Thesaurus, the editor
mentions a supplement of 1,000+ words to the orig-
inal work. A rough count shows 1,000 headwords
(the basic grouping level) and 55,000 synonyms (any
word listed under a headword). The second edi-
tion of Moby Thesaurus contains some 30,000 head-
words and 2.5 million synonyms. Moby Thesaurus
includes many newer terms than Roget’s Thesaurus.
Structurally, Roget’s Thesaurus has a distinct advan-
tage over Moby Thesaurus. The former includes a
six-tiered category structure with cross-indexing be-
tween headwords. The latter is only organized into
headword lists.
</bodyText>
<subsectionHeader confidence="0.996039">
3.2 Metrics
</subsectionHeader>
<bodyText confidence="0.999924411764706">
As we mentioned above, our model uses three pri-
mary metrics in classifying a new utterance: con-
versation affiliations of the current speaker, conver-
sation affiliations of any explicitly mentioned inter-
locutors, and semantic similarity. In the end, all the
conversation affiliation votes are summed with the
one conversation preferred by each of the three the-
saural measures.6 The input line is then merged with
the conversation that received the most votes. De-
tails for deriving the votes follow.
Every conversation a speaker has participated in
receives a vote. Moreover, his last conversation gets
additional votes as a function of his total number
of conversations (see Equation 1). Likewise, every
conversation a named interlocutor has participated
in receives a vote with extra votes given to her last
conversation as a function of how gregarious she is.
</bodyText>
<footnote confidence="0.9747095">
6In the long run, a list of conversations ranked by similarity
score would be better than a winner-takes-all return value.
</footnote>
<table confidence="0.96564025">
Headword Type Weight Change
Direct Match 1
Co-hyponymous Headword 0.25
Cross-indexed Headword 0.75
</table>
<tableCaption confidence="0.99684">
Table 1: Spreading Activation Weights.
</tableCaption>
<equation confidence="0.966144">
3
V ote = ()
ln(|Conversationsspeaker|) + 1 1
</equation>
<bodyText confidence="0.999611931034483">
Each utterance is then profiled in terms of the-
saural headwords. Every lemma in an utterance
matching to some headword increments the activa-
tion of that headword by one.7 A conversation’s se-
mantic profile is a summation of the profiles of its
constituent sentences. In order to simulate the drift
of topic in a conversation, the conversation’s seman-
tic profile decays with every utterance. Thus, more
recent headwords will be more activated than head-
words activated near the beginning of a conversa-
tion. Decay is modeled by halving the activation of
a headword in every cycle that it was not topical.
Moreover, a third profile was kept to simulate
spreading activation within the thesaurus. For this
profile, each topical headword is activated. Every
cross-indexed headword listed within this category
is also augmented by a fixed degree. Finally, every
headword that occupies the same thesaurus section
is augmented. An overview of the weight changes
is listed in Table 1. The specific weights fit the au-
thors’ intuitions as good baselines. These weights
can easily be trained to generate a better model.
The similarity between a new line (the test) and
a conversation (the base) is computed as the sum of
match bonuses and mismatch penalties in Table 2.8
Table 3 scores an input line (TEST) against two con-
versations (BASET and BASE2) with respect to four
headwords (A, B, C, and D). In order to control for
text size, we also computed the average headword
</bodyText>
<footnote confidence="0.881661375">
7Most other models include an explicit stop-word list to re-
duce the effect of function words. Our model implicitly relies
on the thesaurus look-up to filter out function words. One ad-
vantage to our approach is the ability to preferentially weight
different headwords or lemma to headword relations.
8Like with Table 1, these numbers reflect the authors’ intu-
itions and can be improved through standard machine learning
methods.
</footnote>
<page confidence="0.984691">
48
</page>
<table confidence="0.99971">
Headword Test
Present
Yes No
Avg? Above Below
Base Yes Above +1 +0.5 -0.1
Below +0.5 +1 -0.05
-1 -0.5 +0.0001
No
</table>
<tableCaption confidence="0.8867">
Table 2: Similarity Score Calculations.
</tableCaption>
<table confidence="0.999761333333333">
A B C D Score
TEST high high low 0 –
BASE1 high low low high 2.4
+1 +.5 +1 -.1
BASE2 0 high 0 0 -.4999
-1 +1 -.5 +.0001
</table>
<tableCaption confidence="0.9919045">
Table 3: A Example Similarity Scoring for Two Conver-
sations. ‘High’ and ‘low’ refers to headword activation.
</tableCaption>
<bodyText confidence="0.999922857142857">
activation in a conversation. Intuitively, we consider
it best when a headword is activated for both the
base and test condition. Moreover, a headword with
equally above average or equally below average acti-
vation is better than a headword with above average
activation in the base but below average activation
in the test. In the second best case, neither condition
shows any activation in a headword. The penulti-
mately bad condition occurs when the base contains
a headword that the test does not. We do not want
to penalize the test (which is usually smaller) for not
containing everything that the base does. Finally, if
the test condition contains a headword but the base
does not, we want to penalize the conversation most.
</bodyText>
<sectionHeader confidence="0.99904" genericHeader="method">
4 Dataset
</sectionHeader>
<bodyText confidence="0.999992">
Our primary dataset was distributed by Elsner and
Charniak (2008). They collected conversations from
the IRC (Internet Relay Chat) channel ##LINUX,
a very popular room on freenode.net with widely
ranging topics. University students then annotated
these chat logs into conversations. We take the col-
lection of these annotations to be our gold standard
for topic segmentation with respect to the chat logs.
</bodyText>
<subsectionHeader confidence="0.967229">
4.1 Metrics
</subsectionHeader>
<bodyText confidence="0.993167">
Elsner and Charniak (2008) use three major mea-
sures to compare annotations: a 1-to-1 comparison,
</bodyText>
<table confidence="0.86215225">
E&amp;C Annotators Our Model
Mean Max Min
Conversations 81.33 128 50 153
Avg. Length 10.6 16.0 6.2 5.2
</table>
<tableCaption confidence="0.9804815">
Table 4: General statistics for our model as compared
with Elsner and Charniak’s human annotators. Some
numbers are taken from Table 1 (Elsner and Charniak,
2008).
</tableCaption>
<table confidence="0.995612666666667">
Mean Max Min
Inter-annotator 86.70 94.13 75.50
Our Model 65.17 74.50 53.38
</table>
<tableCaption confidence="0.994814">
Table 5: Comparative many-to-1 measures for evaluating
differences in annotation granularity. Some numbers are
taken from Table 1 (Elsner and Charniak, 2008).
</tableCaption>
<bodyText confidence="0.999720230769231">
a loci comparison, and a many-to-1 comparison.
The 1-to-1 metric tries to maximize the global con-
versation overlap in two annotations. The loci scale
is better at measuring local agreement. This score
calculates accuracy between two annotations for
each window of three utterances. Slight differences
in a conversation’s start and end are minimized. Fi-
nally, the many-to-1 score measures the entropy dif-
ference between annotations. In other words, sim-
plifying a fine-grained analysis to a coarse-grained
analysis will yield good results because of shared
major boundaries. Disagreeing about the major con-
versation boundaries will yield a low score.
</bodyText>
<sectionHeader confidence="0.977582" genericHeader="method">
5 Analysis
</sectionHeader>
<bodyText confidence="0.9999514">
Compared with the gold standard, our model has a
strong preference to split conversations into smaller
units. As is evident from Table 4, our model has
more conversations than the maximally splitting hu-
man annotator. These results are unsurprising given
that our classifier posits a new conversation in the
absence of contrary evidence. Despite a low 1-to-1
score, our many-to-1 score is relatively high (see Ta-
ble 5). We can interpret these results to mean that
our model is splitting gold standard conversations
into smaller sets rather than creating conversations
across gold standard boundaries.
A similar interaction of annotation granularity
shows up in Table 6. Our 1-to-1 measures are just
barely above the baseline, on average. On the other
</bodyText>
<page confidence="0.998186">
49
</page>
<table confidence="0.9889824">
As % of ... Misclassified All
Error Type Mean Max Min Mean
Mismatch 25.84 23.78 28.13 11.93
Split Error 62.96 63.11 63.89 29.08
Lump Error 11.20 13.11 7.99 5.17
</table>
<tableCaption confidence="0.97722">
Table 7: Source of misclassified utterances as a percent-
age of misclassified utterances and all utterances.
</tableCaption>
<bodyText confidence="0.999907517241379">
hand, our loci measure jumps much closer to the
human annotators. In other words, the maximum an-
notation overlap of our model and any given human
is poor9 while the local coherence of our annotation
with respect to any human annotation is high. This
pattern is symptomatic of over-splitting, which is ex-
cessively penalized by the 1-to-1 metric.10
We also analyzed the types of errors our model
made while holding the conversation history con-
stant. We simulated a consistent conversation his-
tory by treating the gold standard’s choice as an un-
beatable vote and tabulating the number of times our
model voted with and against the winning conversa-
tion. There were five numbers tabulated: matching
new conversation votes, matching old conversation
votes, mismatching old conversation votes, incorrect
split vote, and incorrect lump vote. The mismatch-
ing old conversation votes occurred when our model
voted for an old conversation but guessed the wrong
conversation. The incorrect split vote occurred when
our model wanted to create a new conversation but
the gold standard voted with an old conversation.
Finally, the incorrect lump vote occurred when our
model matched the utterance with a old conversation
when the gold standard created a new conversation.
Across all six gold standard annotations, nearly
two-thirds of the errors arose from incorrect splitting
votes (see Table 7). In fact, nearly one-third of all
utterances fell into this category.
</bodyText>
<sectionHeader confidence="0.998999" genericHeader="discussions">
6 Future Work
</sectionHeader>
<bodyText confidence="0.925420032258064">
The high granularity for what our model considers a
conversation had a huge impact on our performance
9Elsner and Charniak (2008) found their annotators also
tended to disagree on the exact point when a new conversation
begins.
10Aoki et al. (2006) present a thorough analysis of conver-
sational features associated with schisming, the splitting off of
new conversations from old conversations.
scores. The high many-to-1 scores imply that more
human-like chunks will improve performance. The
granularity may be very task dependent and so we
will need to be careful not to overfit our model to this
data set and these annotators. New features should
be tested with several chat corpora to better under-
stand the cue trading effects of genre.
At present, our model uses only a minimal set of
features. Discourse cues and temporal cues are two
simple measures that can be added. Our current fea-
tures can also use refinement. For instance, even par-
tially disambiguating the particular sense of the lem-
mas should reduce the noise in our similarity mea-
sures. Ranking the semantic similarity, in contrast
with the current winner-takes-all approach, should
improve our results. Accounting for forward refer-
encing, when a speaker invokes another’s name to
draw them into a conversation, is also important.
Finally, understanding the different voting pat-
terns of each feature system will help us to better
understand the reliability of the different cues. To-
wards this end, we need to monitor and act upon the
strength and type of disagreement among voters.
</bodyText>
<sectionHeader confidence="0.997477" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9985595">
Harish Srinivasan was great help in tokenizing the
data. The NLP Group at Janya, Inc., Jordana Heller,
Jean-Pierre Koenig, Michael Prentice, and three
anonymous reviewers provided useful feedback.
</bodyText>
<sectionHeader confidence="0.998495" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995014666666667">
Evrim Acar, Seyit Ahmet Camtepe, Mukkai S. Kr-
ishnamoorthy, and Blent Yener. 2005. Model-
ing and multiway analysis of chatroom tensors. In
Paul B. Kantor, Gheorghe Muresan, Fred Roberts,
Daniel Dajun Zeng, Fei-Yue Wang, Hsinchun Chen,
and Ralph C. Merkle, editors, ISI, volume 3495 of
Lecture Notes in Computer Science, pages 256–268.
Springer.
Paul M. Aoki, Matthew Romaine, Margaret H. Szyman-
ski, James D. Thornton, Daniel Wilson, and Allison
Woodruff. 2003. The Mad Hatter’s cocktail party: A
social mobile audio space supporting multiple simul-
taneous conversations. In CHI 03: Proceedings of the
SIGCHI Conference on Human Factors in Computing
Systems, pages 425–432, New York, NY, USA. ACM
Press.
Paul M. Aoki, Margaret H. Szymanski, Luke Plurkowski,
James D. Thornton, Allison Woodruff, and Weilie Yi.
</reference>
<page confidence="0.951114">
50
</page>
<table confidence="0.999292428571429">
Other Annotators E&amp;C Model Our Model E&amp;C Best Baseline
Mean 1-to-1 52.98 40.62 35.77 34.73
Max 1-to-1 63.50 51.12 49.88 56.00
Min 1-to-1 35.63 33.63 28.25 28.62
Mean loci 81.09 72.75 68.73 62.16
Max loci 86.53 75.16 72.77 69.05
Min loci 74.75 70.47 64.45 54.37
</table>
<tableCaption confidence="0.8027185">
Table 6: Metric values for our model as compared with Elsner and Charniak’s human annotators and classifier. Some
numbers are taken from Table 3 (Elsner and Charniak, 2008).
2006. Where’s the “party” in “multi-party”? Ana-
lyzing the structure of small-group sociable talk. In
</tableCaption>
<reference confidence="0.9990009375">
ACM Conference on Computer Supported Coopera-
tive Work, pages 393–402, Banff, Alberta, Canada,
November. ACM Press.
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical models for text segmentation. In Ma-
chine Learning, pages 177–210.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating WordNet-based measures of lexical semantic re-
latedness. Computational Linguistics, 32(1):13–47.
Seyit Ahmet Camtepe, Mark K. Goldberg, Malik
Magdon-Ismail, and Mukkai Krishnamoorty. 2005.
Detecting conversing groups of chatters: A model, al-
gorithms, and tests. In IADIS AC, pages 89–96.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-
dauer, and R. Harshman. 1990. Indexing by Latent
Semantic Analysis. Journal of the American Society
of Information Science, 41:391–407.
Micha Elsner and Eugene Charniak. 2008. You talk-
ing to me? A corpus and algorithm for conversa-
tion disentanglement. In The Association for Compu-
tational Linguistics: Human Language Technologies
(ACL-HLT 2008), Columbus, Ohio.
Michel Galley, Kathleen McKeown, Eric Fosler-Lussier,
and Hongyan Jing. 2003. Discourse segmentation of
multi-party conversation. In Proceedings of the ACL,
pages 562–569.
Michael Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. Longman Group, New York.
Marti A. Hearst. 1997. TextTiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33–64.
J. R. Hobbs. 1979. Coherence and coreference. Cogni-
tive Science, 3:67–90.
A. Kehler. 2000. Coherence, Reference, and the Theory
of Grammar. CSLI Publications.
Hideki Kozima and Teiji Furugori. 1993. Similarity
between words computed by spreading activation on
an English dictionary. In Proceedings of 6th Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL–93), pages 232–
239, Utrecht.
Hideki Kozima. 1993. Text segmentation based on sim-
ilarity between words. In Proceedings of ACL’93,
pages 286–288, Ohio.
C. D. Manning. 1998. Rethinking text segmentation
models: An information extraction case study. Tech-
nical Report SULTRY–98–07–01, University of Syd-
ney.
Jane Morris and Graeme Hirst. 1991. Lexical cohe-
sion computed by thesaural relations as an indicator
of the structure of text. Computational Linguistics,
17(1):21–48.
Jacki O’Neill and David Martin. 2003. Text chat in ac-
tion. In GROUP ‘03: Proceedings of the 2003 Inter-
national ACM SIGGROUP Conference on Supporting
Group Work, pages 40–49, New York, NY, USA. ACM
Press.
R. Power, D. Scott, and N. Bouayad-Agha. 2003.
Document structure. Computational Linguistics,
29(2):211–260.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceedings
of the 14th International Joint Conference on Artificial
Intelligence, pages 448–453.
Jeffrey C. Reynar. 1999. Statistical models for topic seg-
mentation. In Proceedings of the 37th Annual Meeting
of the ACL, pages 357–364, Maryland, USA, June.
Peter Mark Roget, editor. 1911. Roget’s Thesaurus.
Project Gutenberg.
R. K. Srihari, W. Li, C. Niu, and T. Cornell. 2008. Infox-
tract: A customizable intermediate level information
extraction engine. Journal of Natural Language Engi-
neering, 14(1):33–69.
Michael Sussna. 1993. Word sense disambiguation
for free-text indexing using a massive semantic net-
work. In Proceedings of the Second International
Conference on Information and Knowledge Manage-
ment (CIKMA–93), pages 67–74, Arlington, VA.
Grady Ward, editor. 2002. Moby Thesaurus List. Project
Gutenberg.
</reference>
<page confidence="0.99912">
51
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.349330">
<title confidence="0.999611">Semantic Topicality Robust Through Term</title>
<author confidence="0.999949">M Paul</author>
<affiliation confidence="0.997853333333333">Department of University at The State University of New</affiliation>
<address confidence="0.971608">Buffalo, NY 14260,</address>
<email confidence="0.999571">pmheider@buffalo.edu</email>
<author confidence="0.967054">K Rohini</author>
<affiliation confidence="0.910985">Janya,</affiliation>
<address confidence="0.774613333333333">1408 Sweet Home Suite Amherst, NY</address>
<email confidence="0.999631">rohini@cedar.buffalo.edu</email>
<abstract confidence="0.996810444444444">Despite early intuitions, semantic similarity has not proven to be robust for splitting multiparty interactions into separate conversations. We discuss some initial successes with using thesaural headwords to abstract the semantics of an utterance. This simple profiling technique showed improvements over baseline conversation threading models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Evrim Acar</author>
<author>Seyit Ahmet Camtepe</author>
<author>Mukkai S Krishnamoorthy</author>
<author>Blent Yener</author>
</authors>
<title>Modeling and multiway analysis of chatroom tensors.</title>
<date>2005</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>3495</volume>
<pages>256--268</pages>
<editor>In Paul B. Kantor, Gheorghe Muresan, Fred Roberts, Daniel Dajun Zeng, Fei-Yue Wang, Hsinchun Chen, and Ralph C. Merkle, editors, ISI,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="3679" citStr="Acar et al., 2005" startWordPosition="563" endWordPosition="566">and Charniak (2008) show that speakers seem to participate in more conversations roughly as a function of how talkative they are (cf. Camtepe et al., 2005). In both modalities, speaker tendency to stay on the same topics is a robust cue for conversational coher1See O’Neill and Martin (2003) for an analysis of differences between two- and multi-party interactions. 46 Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 46–51, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ence (Elsner and Charniak, 2008; Acar et al., 2005). Despite the initial intuitions of Halliday and Hasan (1976), semantic similarity has not proven to be a robust cue for multi-party topic segmentation. For instance, Acar et al. (2005) and Galley et al. (2003) used word repetition in their definition of coherence but found that words common to too many conversations hurt modeling performance. Elsner and Charniak (2008) used frequency binning based on the entire document to reduce the noise introduced by high-frequency words. Unfortunately, binning requires a priori knowledge of the relative frequencies of words.2 Additionally, those authors u</context>
</contexts>
<marker>Acar, Camtepe, Krishnamoorthy, Yener, 2005</marker>
<rawString>Evrim Acar, Seyit Ahmet Camtepe, Mukkai S. Krishnamoorthy, and Blent Yener. 2005. Modeling and multiway analysis of chatroom tensors. In Paul B. Kantor, Gheorghe Muresan, Fred Roberts, Daniel Dajun Zeng, Fei-Yue Wang, Hsinchun Chen, and Ralph C. Merkle, editors, ISI, volume 3495 of Lecture Notes in Computer Science, pages 256–268. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul M Aoki</author>
<author>Matthew Romaine</author>
<author>Margaret H Szymanski</author>
<author>James D Thornton</author>
<author>Daniel Wilson</author>
<author>Allison Woodruff</author>
</authors>
<title>The Mad Hatter’s cocktail party: A social mobile audio space supporting multiple simultaneous conversations.</title>
<date>2003</date>
<booktitle>In CHI 03: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,</booktitle>
<pages>425--432</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2919" citStr="Aoki et al., 2003" startWordPosition="444" endWordPosition="447">earlier work assumes the sentences constituting any text are contiguous. Thus, a document is comprised of a series of semantic units that progress from one to the next with no returns to old topics. Multi-party interactions1 abide by a different set of assumptions. Namely, a multi-party interaction can include multiple floors (Aoki et al., 2006). Much like at a cocktail party, we can expect more than a single conversation at every given time. These different conversational floors are the major semantic units a topic segmentation algorithm must recognize. Spoken chat models (Aoki et al., 2006; Aoki et al., 2003) can make a simplifying assumption that speakers tend to only participate in one conversation at a time. However, in text chat models, Elsner and Charniak (2008) show that speakers seem to participate in more conversations roughly as a function of how talkative they are (cf. Camtepe et al., 2005). In both modalities, speaker tendency to stay on the same topics is a robust cue for conversational coher1See O’Neill and Martin (2003) for an analysis of differences between two- and multi-party interactions. 46 Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Fu</context>
</contexts>
<marker>Aoki, Romaine, Szymanski, Thornton, Wilson, Woodruff, 2003</marker>
<rawString>Paul M. Aoki, Matthew Romaine, Margaret H. Szymanski, James D. Thornton, Daniel Wilson, and Allison Woodruff. 2003. The Mad Hatter’s cocktail party: A social mobile audio space supporting multiple simultaneous conversations. In CHI 03: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 425–432, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul M Aoki</author>
<author>Margaret H Szymanski</author>
<author>Luke Plurkowski</author>
<author>James D Thornton</author>
<author>Allison Woodruff</author>
<author>Weilie Yi</author>
</authors>
<date></date>
<booktitle>ACM Conference on Computer Supported Cooperative Work,</booktitle>
<pages>393--402</pages>
<publisher>ACM Press.</publisher>
<location>Banff, Alberta, Canada,</location>
<marker>Aoki, Szymanski, Plurkowski, Thornton, Woodruff, Yi, </marker>
<rawString>Paul M. Aoki, Margaret H. Szymanski, Luke Plurkowski, James D. Thornton, Allison Woodruff, and Weilie Yi. ACM Conference on Computer Supported Cooperative Work, pages 393–402, Banff, Alberta, Canada, November. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Beeferman</author>
<author>Adam Berger</author>
<author>John Lafferty</author>
</authors>
<title>Statistical models for text segmentation.</title>
<date>1999</date>
<booktitle>In Machine Learning,</booktitle>
<pages>177--210</pages>
<contexts>
<context position="2261" citStr="Beeferman et al., 1999" startWordPosition="334" endWordPosition="338">arch assistantship at Janya, Inc. (http://www.janyainc.com, Air Force Grant No.s FA8750-07-C-0077 and FA8750-07-D-0019, Task Order 0004) 2 Background Most work on lexical cohesion extends from Halliday and Hasan (1976). They formalize a text as any semantic unit realized through sentences. Linguistic features found to justify binding sentences together into Halliday and Hasan’s notion of a text include pronouns (Hobbs, 1979; Kehler, 2000), lexical overlap (Hearst, 1997; Kozima, 1993; Morris and Hirst, 1991), cue phrases (Manning, 1998), and discourse markers (Power et al., 2003; Reynar, 1999; Beeferman et al., 1999), among others. Of course, most of this earlier work assumes the sentences constituting any text are contiguous. Thus, a document is comprised of a series of semantic units that progress from one to the next with no returns to old topics. Multi-party interactions1 abide by a different set of assumptions. Namely, a multi-party interaction can include multiple floors (Aoki et al., 2006). Much like at a cocktail party, we can expect more than a single conversation at every given time. These different conversational floors are the major semantic units a topic segmentation algorithm must recognize.</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1999</marker>
<rawString>Doug Beeferman, Adam Berger, and John Lafferty. 1999. Statistical models for text segmentation. In Machine Learning, pages 177–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating WordNet-based measures of lexical semantic relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="6005" citStr="Budanitsky and Hirst (2006)" startWordPosition="922" endWordPosition="925">he thesaurus, we can treat each thesaural category as one dimension of a topicality domain similar to the way Elsner and Charniak leveraged their list of technical terms. In sum, our model focuses on the abstraction of lemmas that is inherent to a thesaurus while limiting the domain-specific and a priori knowledge required 2One could use frequencies from a general corpus but that should only perform as well as a graded stop-word list. 3As one reviewer noted, some parts-of-speech may contribute more to a topic profile than others. Unfortunately, this empirical question must wait to be tested. 4Budanitsky and Hirst (2006) review the advantages. by a classifier to divide multi-party interactions into separate conversational floors. 3 Model At a high level, our chat topic segmenter works like most other classifiers: each input line is tokenized5, passed to a feature analyzer, and clustered with related lines. Unlike traditional topic segmentation models, each input represents a new utterance in the chat log. These utterances can range from single words to multiple sentences. Another aspect of our model (although not unique to it) is the on-line classification of text. We aim to model topic segmentation as if our</context>
<context position="7718" citStr="Budanitsky and Hirst (2006)" startWordPosition="1195" endWordPosition="1198">last conversation she was part of. As the total number of conversations increases, the likelihood of sticking to the last conversation will decrease. The second intuition derives from the observation in O’Neill and Martin (2003) that speakers accommodate for cocktail-style conversations by using direct mentions of interlocutors’ names. We only model backward referencing names. That is, if a speaker uses the name of another user, we assume that the speaker is overtly affiliating with a conversation of the other user. Forward referencing is discussed under future work (see Section 6). Following Budanitsky and Hirst (2006), we base our notion of semantic topicality on thesaural relations. Broadly speaking, two utterances are highly related if their tokenized words (hereafter, lemmas) co-occur in more of the same thesaural categories than not. We will defer further explanation of these features until we have explained our reference thesauri in Subsection 3.1. Unfortunately, many desirable and robust features are missing from our classifier. See Section 6 for a discussion of future work. 5We used SemantexTM (Srihari et al., 2008). 47 In the final stage of our processing pipeline, we use a panel of experts to gene</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Alexander Budanitsky and Graeme Hirst. 2006. Evaluating WordNet-based measures of lexical semantic relatedness. Computational Linguistics, 32(1):13–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seyit Ahmet Camtepe</author>
<author>Mark K Goldberg</author>
<author>Malik Magdon-Ismail</author>
<author>Mukkai Krishnamoorty</author>
</authors>
<title>Detecting conversing groups of chatters: A model, algorithms, and tests.</title>
<date>2005</date>
<booktitle>In IADIS AC,</booktitle>
<pages>89--96</pages>
<contexts>
<context position="3216" citStr="Camtepe et al., 2005" startWordPosition="494" endWordPosition="497">ction can include multiple floors (Aoki et al., 2006). Much like at a cocktail party, we can expect more than a single conversation at every given time. These different conversational floors are the major semantic units a topic segmentation algorithm must recognize. Spoken chat models (Aoki et al., 2006; Aoki et al., 2003) can make a simplifying assumption that speakers tend to only participate in one conversation at a time. However, in text chat models, Elsner and Charniak (2008) show that speakers seem to participate in more conversations roughly as a function of how talkative they are (cf. Camtepe et al., 2005). In both modalities, speaker tendency to stay on the same topics is a robust cue for conversational coher1See O’Neill and Martin (2003) for an analysis of differences between two- and multi-party interactions. 46 Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 46–51, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ence (Elsner and Charniak, 2008; Acar et al., 2005). Despite the initial intuitions of Halliday and Hasan (1976), semantic similarity has not proven to be a robust cue for multi-party topi</context>
</contexts>
<marker>Camtepe, Goldberg, Magdon-Ismail, Krishnamoorty, 2005</marker>
<rawString>Seyit Ahmet Camtepe, Mark K. Goldberg, Malik Magdon-Ismail, and Mukkai Krishnamoorty. 2005. Detecting conversing groups of chatters: A model, algorithms, and tests. In IADIS AC, pages 89–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1990</date>
<journal>Journal of the American Society of Information Science,</journal>
<pages>41--391</pages>
<contexts>
<context position="4702" citStr="Deerwester et al., 1990" startWordPosition="717" endWordPosition="720">ed on the entire document to reduce the noise introduced by high-frequency words. Unfortunately, binning requires a priori knowledge of the relative frequencies of words.2 Additionally, those authors used an on-topic/off-topic word list to bifurcate technical and non-technical utterances. Again, this technique assumes prior knowledge of the strongest on-topic cue words. Since semantic repetition is clearly useful but simple word repetition is not a reliable measure, we investigated other measures of semantic relatedness. Elsner and Charniak (2008) conceded that contextbased measures like LSA (Deerwester et al., 1990) require a clear notion of document boundary to function well. Dictionary-based models (Kozima and Furugori, 1993) are a step in the right direction because they leverage word co-occurrence within definitions to measure relatedness. The richer set of connections available in WordNet models should provide an even better measure of relatedness (Sussna, 1993; Resnik, 1995). Unfortunately, these measures have unequal distribution by part-of-speech and uneven density of lemmas by semantic domain.3 Thesaurus-based models (Morris and Hirst, 1991) provide many of the same advantages as dictionaryand W</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman. 1990. Indexing by Latent Semantic Analysis. Journal of the American Society of Information Science, 41:391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Eugene Charniak</author>
</authors>
<title>You talking to me? A corpus and algorithm for conversation disentanglement.</title>
<date>2008</date>
<booktitle>In The Association for Computational Linguistics: Human Language Technologies (ACL-HLT</booktitle>
<location>Columbus, Ohio.</location>
<contexts>
<context position="3080" citStr="Elsner and Charniak (2008)" startWordPosition="470" endWordPosition="473">ne to the next with no returns to old topics. Multi-party interactions1 abide by a different set of assumptions. Namely, a multi-party interaction can include multiple floors (Aoki et al., 2006). Much like at a cocktail party, we can expect more than a single conversation at every given time. These different conversational floors are the major semantic units a topic segmentation algorithm must recognize. Spoken chat models (Aoki et al., 2006; Aoki et al., 2003) can make a simplifying assumption that speakers tend to only participate in one conversation at a time. However, in text chat models, Elsner and Charniak (2008) show that speakers seem to participate in more conversations roughly as a function of how talkative they are (cf. Camtepe et al., 2005). In both modalities, speaker tendency to stay on the same topics is a robust cue for conversational coher1See O’Neill and Martin (2003) for an analysis of differences between two- and multi-party interactions. 46 Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 46–51, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ence (Elsner and Charniak, 2008; Acar et al., 2005).</context>
<context position="4631" citStr="Elsner and Charniak (2008)" startWordPosition="706" endWordPosition="709">deling performance. Elsner and Charniak (2008) used frequency binning based on the entire document to reduce the noise introduced by high-frequency words. Unfortunately, binning requires a priori knowledge of the relative frequencies of words.2 Additionally, those authors used an on-topic/off-topic word list to bifurcate technical and non-technical utterances. Again, this technique assumes prior knowledge of the strongest on-topic cue words. Since semantic repetition is clearly useful but simple word repetition is not a reliable measure, we investigated other measures of semantic relatedness. Elsner and Charniak (2008) conceded that contextbased measures like LSA (Deerwester et al., 1990) require a clear notion of document boundary to function well. Dictionary-based models (Kozima and Furugori, 1993) are a step in the right direction because they leverage word co-occurrence within definitions to measure relatedness. The richer set of connections available in WordNet models should provide an even better measure of relatedness (Sussna, 1993; Resnik, 1995). Unfortunately, these measures have unequal distribution by part-of-speech and uneven density of lemmas by semantic domain.3 Thesaurus-based models (Morris </context>
<context position="13695" citStr="Elsner and Charniak (2008)" startWordPosition="2170" endWordPosition="2173">r equally below average activation is better than a headword with above average activation in the base but below average activation in the test. In the second best case, neither condition shows any activation in a headword. The penultimately bad condition occurs when the base contains a headword that the test does not. We do not want to penalize the test (which is usually smaller) for not containing everything that the base does. Finally, if the test condition contains a headword but the base does not, we want to penalize the conversation most. 4 Dataset Our primary dataset was distributed by Elsner and Charniak (2008). They collected conversations from the IRC (Internet Relay Chat) channel ##LINUX, a very popular room on freenode.net with widely ranging topics. University students then annotated these chat logs into conversations. We take the collection of these annotations to be our gold standard for topic segmentation with respect to the chat logs. 4.1 Metrics Elsner and Charniak (2008) use three major measures to compare annotations: a 1-to-1 comparison, E&amp;C Annotators Our Model Mean Max Min Conversations 81.33 128 50 153 Avg. Length 10.6 16.0 6.2 5.2 Table 4: General statistics for our model as compare</context>
<context position="17917" citStr="Elsner and Charniak (2008)" startWordPosition="2836" endWordPosition="2839">ncorrect split vote occurred when our model wanted to create a new conversation but the gold standard voted with an old conversation. Finally, the incorrect lump vote occurred when our model matched the utterance with a old conversation when the gold standard created a new conversation. Across all six gold standard annotations, nearly two-thirds of the errors arose from incorrect splitting votes (see Table 7). In fact, nearly one-third of all utterances fell into this category. 6 Future Work The high granularity for what our model considers a conversation had a huge impact on our performance 9Elsner and Charniak (2008) found their annotators also tended to disagree on the exact point when a new conversation begins. 10Aoki et al. (2006) present a thorough analysis of conversational features associated with schisming, the splitting off of new conversations from old conversations. scores. The high many-to-1 scores imply that more human-like chunks will improve performance. The granularity may be very task dependent and so we will need to be careful not to overfit our model to this data set and these annotators. New features should be tested with several chat corpora to better understand the cue trading effects</context>
</contexts>
<marker>Elsner, Charniak, 2008</marker>
<rawString>Micha Elsner and Eugene Charniak. 2008. You talking to me? A corpus and algorithm for conversation disentanglement. In The Association for Computational Linguistics: Human Language Technologies (ACL-HLT 2008), Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
<author>Eric Fosler-Lussier</author>
<author>Hongyan Jing</author>
</authors>
<title>Discourse segmentation of multi-party conversation.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>562--569</pages>
<contexts>
<context position="3889" citStr="Galley et al. (2003)" startWordPosition="598" endWordPosition="602"> same topics is a robust cue for conversational coher1See O’Neill and Martin (2003) for an analysis of differences between two- and multi-party interactions. 46 Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 46–51, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ence (Elsner and Charniak, 2008; Acar et al., 2005). Despite the initial intuitions of Halliday and Hasan (1976), semantic similarity has not proven to be a robust cue for multi-party topic segmentation. For instance, Acar et al. (2005) and Galley et al. (2003) used word repetition in their definition of coherence but found that words common to too many conversations hurt modeling performance. Elsner and Charniak (2008) used frequency binning based on the entire document to reduce the noise introduced by high-frequency words. Unfortunately, binning requires a priori knowledge of the relative frequencies of words.2 Additionally, those authors used an on-topic/off-topic word list to bifurcate technical and non-technical utterances. Again, this technique assumes prior knowledge of the strongest on-topic cue words. Since semantic repetition is clearly u</context>
</contexts>
<marker>Galley, McKeown, Fosler-Lussier, Jing, 2003</marker>
<rawString>Michel Galley, Kathleen McKeown, Eric Fosler-Lussier, and Hongyan Jing. 2003. Discourse segmentation of multi-party conversation. In Proceedings of the ACL, pages 562–569.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Halliday</author>
<author>Ruqaiya Hasan</author>
</authors>
<title>Cohesion in English.</title>
<date>1976</date>
<publisher>Longman Group,</publisher>
<location>New York.</location>
<contexts>
<context position="1856" citStr="Halliday and Hasan (1976)" startWordPosition="272" endWordPosition="276">ger, multi-party document. Using a word’s thesaurus entry as a proxy for its underlying semantics provides a domain-neutral metric for distinguishing conversations. Also, our classifier does not rely on metalinguistic properties that may not be robust across genres. ∗ The first author was partially funded through a fellowship from the SUNY at Buffalo Department of Linguistics and partially through a research assistantship at Janya, Inc. (http://www.janyainc.com, Air Force Grant No.s FA8750-07-C-0077 and FA8750-07-D-0019, Task Order 0004) 2 Background Most work on lexical cohesion extends from Halliday and Hasan (1976). They formalize a text as any semantic unit realized through sentences. Linguistic features found to justify binding sentences together into Halliday and Hasan’s notion of a text include pronouns (Hobbs, 1979; Kehler, 2000), lexical overlap (Hearst, 1997; Kozima, 1993; Morris and Hirst, 1991), cue phrases (Manning, 1998), and discourse markers (Power et al., 2003; Reynar, 1999; Beeferman et al., 1999), among others. Of course, most of this earlier work assumes the sentences constituting any text are contiguous. Thus, a document is comprised of a series of semantic units that progress from one</context>
<context position="3740" citStr="Halliday and Hasan (1976)" startWordPosition="572" endWordPosition="575">pate in more conversations roughly as a function of how talkative they are (cf. Camtepe et al., 2005). In both modalities, speaker tendency to stay on the same topics is a robust cue for conversational coher1See O’Neill and Martin (2003) for an analysis of differences between two- and multi-party interactions. 46 Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 46–51, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ence (Elsner and Charniak, 2008; Acar et al., 2005). Despite the initial intuitions of Halliday and Hasan (1976), semantic similarity has not proven to be a robust cue for multi-party topic segmentation. For instance, Acar et al. (2005) and Galley et al. (2003) used word repetition in their definition of coherence but found that words common to too many conversations hurt modeling performance. Elsner and Charniak (2008) used frequency binning based on the entire document to reduce the noise introduced by high-frequency words. Unfortunately, binning requires a priori knowledge of the relative frequencies of words.2 Additionally, those authors used an on-topic/off-topic word list to bifurcate technical an</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>Michael Halliday and Ruqaiya Hasan. 1976. Cohesion in English. Longman Group, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>TextTiling: Segmenting text into multi-paragraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="2111" citStr="Hearst, 1997" startWordPosition="313" endWordPosition="314"> The first author was partially funded through a fellowship from the SUNY at Buffalo Department of Linguistics and partially through a research assistantship at Janya, Inc. (http://www.janyainc.com, Air Force Grant No.s FA8750-07-C-0077 and FA8750-07-D-0019, Task Order 0004) 2 Background Most work on lexical cohesion extends from Halliday and Hasan (1976). They formalize a text as any semantic unit realized through sentences. Linguistic features found to justify binding sentences together into Halliday and Hasan’s notion of a text include pronouns (Hobbs, 1979; Kehler, 2000), lexical overlap (Hearst, 1997; Kozima, 1993; Morris and Hirst, 1991), cue phrases (Manning, 1998), and discourse markers (Power et al., 2003; Reynar, 1999; Beeferman et al., 1999), among others. Of course, most of this earlier work assumes the sentences constituting any text are contiguous. Thus, a document is comprised of a series of semantic units that progress from one to the next with no returns to old topics. Multi-party interactions1 abide by a different set of assumptions. Namely, a multi-party interaction can include multiple floors (Aoki et al., 2006). Much like at a cocktail party, we can expect more than a sing</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Marti A. Hearst. 1997. TextTiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics, 23(1):33–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
</authors>
<title>Coherence and coreference.</title>
<date>1979</date>
<journal>Cognitive Science,</journal>
<pages>3--67</pages>
<contexts>
<context position="2065" citStr="Hobbs, 1979" startWordPosition="306" endWordPosition="307">rties that may not be robust across genres. ∗ The first author was partially funded through a fellowship from the SUNY at Buffalo Department of Linguistics and partially through a research assistantship at Janya, Inc. (http://www.janyainc.com, Air Force Grant No.s FA8750-07-C-0077 and FA8750-07-D-0019, Task Order 0004) 2 Background Most work on lexical cohesion extends from Halliday and Hasan (1976). They formalize a text as any semantic unit realized through sentences. Linguistic features found to justify binding sentences together into Halliday and Hasan’s notion of a text include pronouns (Hobbs, 1979; Kehler, 2000), lexical overlap (Hearst, 1997; Kozima, 1993; Morris and Hirst, 1991), cue phrases (Manning, 1998), and discourse markers (Power et al., 2003; Reynar, 1999; Beeferman et al., 1999), among others. Of course, most of this earlier work assumes the sentences constituting any text are contiguous. Thus, a document is comprised of a series of semantic units that progress from one to the next with no returns to old topics. Multi-party interactions1 abide by a different set of assumptions. Namely, a multi-party interaction can include multiple floors (Aoki et al., 2006). Much like at a </context>
</contexts>
<marker>Hobbs, 1979</marker>
<rawString>J. R. Hobbs. 1979. Coherence and coreference. Cognitive Science, 3:67–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kehler</author>
</authors>
<title>Coherence, Reference, and the Theory of Grammar.</title>
<date>2000</date>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="2080" citStr="Kehler, 2000" startWordPosition="308" endWordPosition="309">y not be robust across genres. ∗ The first author was partially funded through a fellowship from the SUNY at Buffalo Department of Linguistics and partially through a research assistantship at Janya, Inc. (http://www.janyainc.com, Air Force Grant No.s FA8750-07-C-0077 and FA8750-07-D-0019, Task Order 0004) 2 Background Most work on lexical cohesion extends from Halliday and Hasan (1976). They formalize a text as any semantic unit realized through sentences. Linguistic features found to justify binding sentences together into Halliday and Hasan’s notion of a text include pronouns (Hobbs, 1979; Kehler, 2000), lexical overlap (Hearst, 1997; Kozima, 1993; Morris and Hirst, 1991), cue phrases (Manning, 1998), and discourse markers (Power et al., 2003; Reynar, 1999; Beeferman et al., 1999), among others. Of course, most of this earlier work assumes the sentences constituting any text are contiguous. Thus, a document is comprised of a series of semantic units that progress from one to the next with no returns to old topics. Multi-party interactions1 abide by a different set of assumptions. Namely, a multi-party interaction can include multiple floors (Aoki et al., 2006). Much like at a cocktail party,</context>
</contexts>
<marker>Kehler, 2000</marker>
<rawString>A. Kehler. 2000. Coherence, Reference, and the Theory of Grammar. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Kozima</author>
<author>Teiji Furugori</author>
</authors>
<title>Similarity between words computed by spreading activation on an English dictionary.</title>
<date>1993</date>
<booktitle>In Proceedings of 6th Conference of the European Chapter of the Association for Computational Linguistics (EACL–93),</booktitle>
<pages>232--239</pages>
<location>Utrecht.</location>
<contexts>
<context position="4816" citStr="Kozima and Furugori, 1993" startWordPosition="734" endWordPosition="737">s a priori knowledge of the relative frequencies of words.2 Additionally, those authors used an on-topic/off-topic word list to bifurcate technical and non-technical utterances. Again, this technique assumes prior knowledge of the strongest on-topic cue words. Since semantic repetition is clearly useful but simple word repetition is not a reliable measure, we investigated other measures of semantic relatedness. Elsner and Charniak (2008) conceded that contextbased measures like LSA (Deerwester et al., 1990) require a clear notion of document boundary to function well. Dictionary-based models (Kozima and Furugori, 1993) are a step in the right direction because they leverage word co-occurrence within definitions to measure relatedness. The richer set of connections available in WordNet models should provide an even better measure of relatedness (Sussna, 1993; Resnik, 1995). Unfortunately, these measures have unequal distribution by part-of-speech and uneven density of lemmas by semantic domain.3 Thesaurus-based models (Morris and Hirst, 1991) provide many of the same advantages as dictionaryand WordNet-based models.4 In addition to the hierarchical relations encoded by the thesaurus, we can treat each thesau</context>
</contexts>
<marker>Kozima, Furugori, 1993</marker>
<rawString>Hideki Kozima and Teiji Furugori. 1993. Similarity between words computed by spreading activation on an English dictionary. In Proceedings of 6th Conference of the European Chapter of the Association for Computational Linguistics (EACL–93), pages 232– 239, Utrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Kozima</author>
</authors>
<title>Text segmentation based on similarity between words.</title>
<date>1993</date>
<booktitle>In Proceedings of ACL’93,</booktitle>
<pages>286--288</pages>
<location>Ohio.</location>
<contexts>
<context position="2125" citStr="Kozima, 1993" startWordPosition="315" endWordPosition="316">hor was partially funded through a fellowship from the SUNY at Buffalo Department of Linguistics and partially through a research assistantship at Janya, Inc. (http://www.janyainc.com, Air Force Grant No.s FA8750-07-C-0077 and FA8750-07-D-0019, Task Order 0004) 2 Background Most work on lexical cohesion extends from Halliday and Hasan (1976). They formalize a text as any semantic unit realized through sentences. Linguistic features found to justify binding sentences together into Halliday and Hasan’s notion of a text include pronouns (Hobbs, 1979; Kehler, 2000), lexical overlap (Hearst, 1997; Kozima, 1993; Morris and Hirst, 1991), cue phrases (Manning, 1998), and discourse markers (Power et al., 2003; Reynar, 1999; Beeferman et al., 1999), among others. Of course, most of this earlier work assumes the sentences constituting any text are contiguous. Thus, a document is comprised of a series of semantic units that progress from one to the next with no returns to old topics. Multi-party interactions1 abide by a different set of assumptions. Namely, a multi-party interaction can include multiple floors (Aoki et al., 2006). Much like at a cocktail party, we can expect more than a single conversatio</context>
</contexts>
<marker>Kozima, 1993</marker>
<rawString>Hideki Kozima. 1993. Text segmentation based on similarity between words. In Proceedings of ACL’93, pages 286–288, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
</authors>
<title>Rethinking text segmentation models: An information extraction case study.</title>
<date>1998</date>
<tech>Technical Report SULTRY–98–07–01,</tech>
<institution>University of Sydney.</institution>
<contexts>
<context position="2179" citStr="Manning, 1998" startWordPosition="323" endWordPosition="324">he SUNY at Buffalo Department of Linguistics and partially through a research assistantship at Janya, Inc. (http://www.janyainc.com, Air Force Grant No.s FA8750-07-C-0077 and FA8750-07-D-0019, Task Order 0004) 2 Background Most work on lexical cohesion extends from Halliday and Hasan (1976). They formalize a text as any semantic unit realized through sentences. Linguistic features found to justify binding sentences together into Halliday and Hasan’s notion of a text include pronouns (Hobbs, 1979; Kehler, 2000), lexical overlap (Hearst, 1997; Kozima, 1993; Morris and Hirst, 1991), cue phrases (Manning, 1998), and discourse markers (Power et al., 2003; Reynar, 1999; Beeferman et al., 1999), among others. Of course, most of this earlier work assumes the sentences constituting any text are contiguous. Thus, a document is comprised of a series of semantic units that progress from one to the next with no returns to old topics. Multi-party interactions1 abide by a different set of assumptions. Namely, a multi-party interaction can include multiple floors (Aoki et al., 2006). Much like at a cocktail party, we can expect more than a single conversation at every given time. These different conversational </context>
</contexts>
<marker>Manning, 1998</marker>
<rawString>C. D. Manning. 1998. Rethinking text segmentation models: An information extraction case study. Technical Report SULTRY–98–07–01, University of Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
<author>Graeme Hirst</author>
</authors>
<title>Lexical cohesion computed by thesaural relations as an indicator of the structure of text.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="2150" citStr="Morris and Hirst, 1991" startWordPosition="317" endWordPosition="320">lly funded through a fellowship from the SUNY at Buffalo Department of Linguistics and partially through a research assistantship at Janya, Inc. (http://www.janyainc.com, Air Force Grant No.s FA8750-07-C-0077 and FA8750-07-D-0019, Task Order 0004) 2 Background Most work on lexical cohesion extends from Halliday and Hasan (1976). They formalize a text as any semantic unit realized through sentences. Linguistic features found to justify binding sentences together into Halliday and Hasan’s notion of a text include pronouns (Hobbs, 1979; Kehler, 2000), lexical overlap (Hearst, 1997; Kozima, 1993; Morris and Hirst, 1991), cue phrases (Manning, 1998), and discourse markers (Power et al., 2003; Reynar, 1999; Beeferman et al., 1999), among others. Of course, most of this earlier work assumes the sentences constituting any text are contiguous. Thus, a document is comprised of a series of semantic units that progress from one to the next with no returns to old topics. Multi-party interactions1 abide by a different set of assumptions. Namely, a multi-party interaction can include multiple floors (Aoki et al., 2006). Much like at a cocktail party, we can expect more than a single conversation at every given time. Th</context>
<context position="5247" citStr="Morris and Hirst, 1991" startWordPosition="799" endWordPosition="802"> (2008) conceded that contextbased measures like LSA (Deerwester et al., 1990) require a clear notion of document boundary to function well. Dictionary-based models (Kozima and Furugori, 1993) are a step in the right direction because they leverage word co-occurrence within definitions to measure relatedness. The richer set of connections available in WordNet models should provide an even better measure of relatedness (Sussna, 1993; Resnik, 1995). Unfortunately, these measures have unequal distribution by part-of-speech and uneven density of lemmas by semantic domain.3 Thesaurus-based models (Morris and Hirst, 1991) provide many of the same advantages as dictionaryand WordNet-based models.4 In addition to the hierarchical relations encoded by the thesaurus, we can treat each thesaural category as one dimension of a topicality domain similar to the way Elsner and Charniak leveraged their list of technical terms. In sum, our model focuses on the abstraction of lemmas that is inherent to a thesaurus while limiting the domain-specific and a priori knowledge required 2One could use frequencies from a general corpus but that should only perform as well as a graded stop-word list. 3As one reviewer noted, some p</context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>Jane Morris and Graeme Hirst. 1991. Lexical cohesion computed by thesaural relations as an indicator of the structure of text. Computational Linguistics, 17(1):21–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacki O’Neill</author>
<author>David Martin</author>
</authors>
<title>Text chat in action.</title>
<date>2003</date>
<booktitle>In GROUP ‘03: Proceedings of the 2003 International ACM SIGGROUP Conference on Supporting Group Work,</booktitle>
<pages>40--49</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<marker>O’Neill, Martin, 2003</marker>
<rawString>Jacki O’Neill and David Martin. 2003. Text chat in action. In GROUP ‘03: Proceedings of the 2003 International ACM SIGGROUP Conference on Supporting Group Work, pages 40–49, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Power</author>
<author>D Scott</author>
<author>N Bouayad-Agha</author>
</authors>
<date>2003</date>
<booktitle>Document structure. Computational Linguistics,</booktitle>
<pages>29--2</pages>
<contexts>
<context position="2222" citStr="Power et al., 2003" startWordPosition="328" endWordPosition="331">stics and partially through a research assistantship at Janya, Inc. (http://www.janyainc.com, Air Force Grant No.s FA8750-07-C-0077 and FA8750-07-D-0019, Task Order 0004) 2 Background Most work on lexical cohesion extends from Halliday and Hasan (1976). They formalize a text as any semantic unit realized through sentences. Linguistic features found to justify binding sentences together into Halliday and Hasan’s notion of a text include pronouns (Hobbs, 1979; Kehler, 2000), lexical overlap (Hearst, 1997; Kozima, 1993; Morris and Hirst, 1991), cue phrases (Manning, 1998), and discourse markers (Power et al., 2003; Reynar, 1999; Beeferman et al., 1999), among others. Of course, most of this earlier work assumes the sentences constituting any text are contiguous. Thus, a document is comprised of a series of semantic units that progress from one to the next with no returns to old topics. Multi-party interactions1 abide by a different set of assumptions. Namely, a multi-party interaction can include multiple floors (Aoki et al., 2006). Much like at a cocktail party, we can expect more than a single conversation at every given time. These different conversational floors are the major semantic units a topic</context>
</contexts>
<marker>Power, Scott, Bouayad-Agha, 2003</marker>
<rawString>R. Power, D. Scott, and N. Bouayad-Agha. 2003. Document structure. Computational Linguistics, 29(2):211–260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>448--453</pages>
<contexts>
<context position="5074" citStr="Resnik, 1995" startWordPosition="778" endWordPosition="779">ntic repetition is clearly useful but simple word repetition is not a reliable measure, we investigated other measures of semantic relatedness. Elsner and Charniak (2008) conceded that contextbased measures like LSA (Deerwester et al., 1990) require a clear notion of document boundary to function well. Dictionary-based models (Kozima and Furugori, 1993) are a step in the right direction because they leverage word co-occurrence within definitions to measure relatedness. The richer set of connections available in WordNet models should provide an even better measure of relatedness (Sussna, 1993; Resnik, 1995). Unfortunately, these measures have unequal distribution by part-of-speech and uneven density of lemmas by semantic domain.3 Thesaurus-based models (Morris and Hirst, 1991) provide many of the same advantages as dictionaryand WordNet-based models.4 In addition to the hierarchical relations encoded by the thesaurus, we can treat each thesaural category as one dimension of a topicality domain similar to the way Elsner and Charniak leveraged their list of technical terms. In sum, our model focuses on the abstraction of lemmas that is inherent to a thesaurus while limiting the domain-specific and</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of the 14th International Joint Conference on Artificial Intelligence, pages 448–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
</authors>
<title>Statistical models for topic segmentation.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the ACL,</booktitle>
<pages>357--364</pages>
<location>Maryland, USA,</location>
<contexts>
<context position="2236" citStr="Reynar, 1999" startWordPosition="332" endWordPosition="333">through a research assistantship at Janya, Inc. (http://www.janyainc.com, Air Force Grant No.s FA8750-07-C-0077 and FA8750-07-D-0019, Task Order 0004) 2 Background Most work on lexical cohesion extends from Halliday and Hasan (1976). They formalize a text as any semantic unit realized through sentences. Linguistic features found to justify binding sentences together into Halliday and Hasan’s notion of a text include pronouns (Hobbs, 1979; Kehler, 2000), lexical overlap (Hearst, 1997; Kozima, 1993; Morris and Hirst, 1991), cue phrases (Manning, 1998), and discourse markers (Power et al., 2003; Reynar, 1999; Beeferman et al., 1999), among others. Of course, most of this earlier work assumes the sentences constituting any text are contiguous. Thus, a document is comprised of a series of semantic units that progress from one to the next with no returns to old topics. Multi-party interactions1 abide by a different set of assumptions. Namely, a multi-party interaction can include multiple floors (Aoki et al., 2006). Much like at a cocktail party, we can expect more than a single conversation at every given time. These different conversational floors are the major semantic units a topic segmentation </context>
</contexts>
<marker>Reynar, 1999</marker>
<rawString>Jeffrey C. Reynar. 1999. Statistical models for topic segmentation. In Proceedings of the 37th Annual Meeting of the ACL, pages 357–364, Maryland, USA, June.</rawString>
</citation>
<citation valid="true">
<title>Roget’s Thesaurus. Project Gutenberg.</title>
<date>1911</date>
<editor>Peter Mark Roget, editor.</editor>
<contexts>
<context position="8733" citStr="(1911)" startWordPosition="1357" endWordPosition="1357"> our classifier. See Section 6 for a discussion of future work. 5We used SemantexTM (Srihari et al., 2008). 47 In the final stage of our processing pipeline, we use a panel of experts to generate a simple weighted classification. Each feature described above contributes a roughly equal vote towards the final sorting decision. Barring a single strong preference or a cohort of weak preferences for one conversation, the model assumes the incoming utterance introduces a new conversational floor. 3.1 Thesauri We chose two machine-readable and public-domain thesauri for our model: Roget’s Thesaurus (1911) and Moby Thesaurus II (2002). Both are available from Project Gutenberg (gutenberg.org). In the compilation notes for Roget’s Thesaurus, the editor mentions a supplement of 1,000+ words to the original work. A rough count shows 1,000 headwords (the basic grouping level) and 55,000 synonyms (any word listed under a headword). The second edition of Moby Thesaurus contains some 30,000 headwords and 2.5 million synonyms. Moby Thesaurus includes many newer terms than Roget’s Thesaurus. Structurally, Roget’s Thesaurus has a distinct advantage over Moby Thesaurus. The former includes a six-tiered ca</context>
</contexts>
<marker>1911</marker>
<rawString>Peter Mark Roget, editor. 1911. Roget’s Thesaurus. Project Gutenberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R K Srihari</author>
<author>W Li</author>
<author>C Niu</author>
<author>T Cornell</author>
</authors>
<title>Infoxtract: A customizable intermediate level information extraction engine.</title>
<date>2008</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>14</volume>
<issue>1</issue>
<contexts>
<context position="8233" citStr="Srihari et al., 2008" startWordPosition="1277" endWordPosition="1280"> Forward referencing is discussed under future work (see Section 6). Following Budanitsky and Hirst (2006), we base our notion of semantic topicality on thesaural relations. Broadly speaking, two utterances are highly related if their tokenized words (hereafter, lemmas) co-occur in more of the same thesaural categories than not. We will defer further explanation of these features until we have explained our reference thesauri in Subsection 3.1. Unfortunately, many desirable and robust features are missing from our classifier. See Section 6 for a discussion of future work. 5We used SemantexTM (Srihari et al., 2008). 47 In the final stage of our processing pipeline, we use a panel of experts to generate a simple weighted classification. Each feature described above contributes a roughly equal vote towards the final sorting decision. Barring a single strong preference or a cohort of weak preferences for one conversation, the model assumes the incoming utterance introduces a new conversational floor. 3.1 Thesauri We chose two machine-readable and public-domain thesauri for our model: Roget’s Thesaurus (1911) and Moby Thesaurus II (2002). Both are available from Project Gutenberg (gutenberg.org). In the com</context>
</contexts>
<marker>Srihari, Li, Niu, Cornell, 2008</marker>
<rawString>R. K. Srihari, W. Li, C. Niu, and T. Cornell. 2008. Infoxtract: A customizable intermediate level information extraction engine. Journal of Natural Language Engineering, 14(1):33–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Sussna</author>
</authors>
<title>Word sense disambiguation for free-text indexing using a massive semantic network.</title>
<date>1993</date>
<booktitle>In Proceedings of the Second International Conference on Information and Knowledge Management (CIKMA–93),</booktitle>
<pages>67--74</pages>
<location>Arlington, VA.</location>
<contexts>
<context position="5059" citStr="Sussna, 1993" startWordPosition="776" endWordPosition="777">ds. Since semantic repetition is clearly useful but simple word repetition is not a reliable measure, we investigated other measures of semantic relatedness. Elsner and Charniak (2008) conceded that contextbased measures like LSA (Deerwester et al., 1990) require a clear notion of document boundary to function well. Dictionary-based models (Kozima and Furugori, 1993) are a step in the right direction because they leverage word co-occurrence within definitions to measure relatedness. The richer set of connections available in WordNet models should provide an even better measure of relatedness (Sussna, 1993; Resnik, 1995). Unfortunately, these measures have unequal distribution by part-of-speech and uneven density of lemmas by semantic domain.3 Thesaurus-based models (Morris and Hirst, 1991) provide many of the same advantages as dictionaryand WordNet-based models.4 In addition to the hierarchical relations encoded by the thesaurus, we can treat each thesaural category as one dimension of a topicality domain similar to the way Elsner and Charniak leveraged their list of technical terms. In sum, our model focuses on the abstraction of lemmas that is inherent to a thesaurus while limiting the doma</context>
</contexts>
<marker>Sussna, 1993</marker>
<rawString>Michael Sussna. 1993. Word sense disambiguation for free-text indexing using a massive semantic network. In Proceedings of the Second International Conference on Information and Knowledge Management (CIKMA–93), pages 67–74, Arlington, VA.</rawString>
</citation>
<citation valid="true">
<date>2002</date>
<booktitle>Moby Thesaurus List. Project Gutenberg.</booktitle>
<editor>Grady Ward, editor.</editor>
<contexts>
<context position="8762" citStr="(2002)" startWordPosition="1362" endWordPosition="1362">6 for a discussion of future work. 5We used SemantexTM (Srihari et al., 2008). 47 In the final stage of our processing pipeline, we use a panel of experts to generate a simple weighted classification. Each feature described above contributes a roughly equal vote towards the final sorting decision. Barring a single strong preference or a cohort of weak preferences for one conversation, the model assumes the incoming utterance introduces a new conversational floor. 3.1 Thesauri We chose two machine-readable and public-domain thesauri for our model: Roget’s Thesaurus (1911) and Moby Thesaurus II (2002). Both are available from Project Gutenberg (gutenberg.org). In the compilation notes for Roget’s Thesaurus, the editor mentions a supplement of 1,000+ words to the original work. A rough count shows 1,000 headwords (the basic grouping level) and 55,000 synonyms (any word listed under a headword). The second edition of Moby Thesaurus contains some 30,000 headwords and 2.5 million synonyms. Moby Thesaurus includes many newer terms than Roget’s Thesaurus. Structurally, Roget’s Thesaurus has a distinct advantage over Moby Thesaurus. The former includes a six-tiered category structure with cross-i</context>
</contexts>
<marker>2002</marker>
<rawString>Grady Ward, editor. 2002. Moby Thesaurus List. Project Gutenberg.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>