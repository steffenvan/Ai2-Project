<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.9561195">
Automatic extraction of subcorpora based on subcategorization
frames from a part-of-speech tagged corpus
</title>
<note confidence="0.550706">
Susanne GAHL
UC Berkeley, Department of Linguistics
ICSI
1947 Center St, Suite 600
</note>
<address confidence="0.721984">
Berkeley, CA 94704-1105
</address>
<email confidence="0.979819">
gahl@icsi.berkeley.edu
</email>
<sectionHeader confidence="0.997152" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999815">
This paper presents a method for extracting
subcorpora documenting different subcate-
gorization frames for verbs, nouns, and
adjectives in the 100 mio. word British
National Corpus. The extraction tool consists
of a set of batch files for use with the Corpus
Query Processor (CQP), which is part of the
IMS corpus workbench (cf. Christ 1994a,b).
A macroprocessor has been developed that
allows the user to specify in a simple input file
which subcorpora are to be created for a given
lemma.
The resulting subcorpora can be used (1) to
provide evidence for the subcategorization
properties of a given lemma, and to facilitate
the selection of corpus lines for lexicographic
research, and (2) to determine the frequencies
of different syntactic contexts of each lemma.
</bodyText>
<sectionHeader confidence="0.978992" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.970227347826087">
A number of resources are available for
obtaining subcategorization information, i.e.
information on the types of syntactic
complements associated with valence-bearing
predicators (which include verbs, nouns, and
adjectives). This information, also referred to
as valence information is available both in
machine-readable form, as in the COMLEX
database (Macleod et al. 1995), and in human-
readable dictionaries (e.g. Hornby 1989,
Procter 1978, Sinclair 1987). Increasingly,
tools are also becoming available for acquiring
subcategorization information from corpora,
i.e. for inferring the subcategorization frames
of a given lemma (e.g. Manning 1993).
None of these resources provide immediate
access to corpus evidence, nor do they provide
information on the relative frequency of the
patterns that are listed for a given lemma.
There is a need for a tool that can (1) find
evidence for subcategorization patterns and
(2) determine their frequencies in large
corpora:
</bodyText>
<listItem confidence="0.8833734">
1. Statistical approaches to NLP rely on
information not just on the range of
combinatory possibilities of words, but
also the relative frequencies of the
expected patterns.
2. Dictionaries that list subcategorization
frames often list expected patterns, rather
than actual ones. Lexicographers and
lexicologist need access to the evidence
for this information.
3. Frequency information has come to be
the focus of much psycholinguistic
research on sentence processing (see for
example MacDonald 1997). While
information on word frequency is readily
available (e.g. Francis and Kucera
(1982)), there is as yet no easy way of
obtaining information from large corpora
on the relative frequency of complemen-
tation patterns.
</listItem>
<bodyText confidence="0.999236923076923">
None of these points argue against the use-
fulness of the available resources, but they
show that there is a gap in the available in-
formation.
To address this need, we have developed a tool
for extracting evidence for subcategorization
patterns from the 100 mio. word British
National Corpus (BNC). The tool is used as
part of the lexicon-building process in the
FrameNet project, an NSF-funded project
aimed at creating a lexical database based on
the principles of Frame Semantics (Fillmore
1982).
</bodyText>
<page confidence="0.999212">
428
</page>
<sectionHeader confidence="0.996858" genericHeader="method">
1 Infrastructure
</sectionHeader>
<subsectionHeader confidence="0.993247">
1.1 Tools
</subsectionHeader>
<bodyText confidence="0.948309">
We are using the 100 mio. word British
National Corpus, with the following corpus
query tools:
</bodyText>
<listItem confidence="0.907895636363636">
. CQP (Corpus Query Processor, Christ
(1994)), a general corpus query processor
for complex queries with any number and
combination of annotated information
types, including part-of-speech tags,
morphosyntactic tags, lemmas and
sentence boundarie S.
• A macroprocessor for use with CQP that
allows the user to specify which
subcorpora are to be created for a given
lemma.
</listItem>
<bodyText confidence="0.999831428571428">
The corpus queries are written in the CQP
corpus query language, which uses regular
expressions over part-of-speech tags, lemmas,
morphosyntactic tags, and sentence
boundaries. For details, see Christ (1994a).
The queries essentially simulate a chunk
parser, using a regular grammar.
</bodyText>
<subsectionHeader confidence="0.984989">
1.2 Coverage
</subsectionHeader>
<bodyText confidence="0.999755277777778">
A list of the verb frames that are currently
searchable is given in figure 1 below, along
with an example of each pattern. The
categories we are using are roughly based on
those used in the COMLEX syntactic
dictionary (Macleod et al. 1995).
In our queries for nouns and adjectives as
targets, we are able to extract prepositional,
clausal, infinitival, and gerundial complements.
In addition, the tool accomodates searches for
compounds and for possessor phrases Om
nei.hbor&apos;s addiction to cake,tax milk allergy).
ENI though these categories are not tied to
the syntactic subcategorization frames of the
target lemmas, they often instantiate semantic
arguments, or, more specifically, Frame
elements (Fillmore 1982, Baker et al.
forthcoming).
</bodyText>
<subsectionHeader confidence="0.8526885">
1.3 Method
1.3.1 Overview
</subsectionHeader>
<bodyText confidence="0.892261414634146">
We start by creating a subcorpus containing all
concordance lines for a given lemma. We call
this subcorpus a lemma-subcorpus. The
extraction of smaller subcorpora from the
lemma subcorpus then proceeds in two stages.
During the first stage, syntactic patterns
involving &apos;displaced&apos; arguments (i.e. &apos;left
isolation&apos; or &apos;movement&apos; phenomena) are
extracted, such as passives, tough movement
and constructions involving WH-extraction.
The result of this procedure is a set of
subcorpora that are homogeneous with respect
to major constituent order. Following this, the
remainder of the lemma-subcorpus is
partitioned into subcorpora based on the
subcategorization properties of the lemma in
question.
intransitive
np
np_np
np_pp
np_Pvping
np_pwh
np_vpto
np_vping
np_sfin
np_wh
np_ap
np_sbrst
ap
&apos;worms wiggle&apos;
&apos;kiss me&apos;
&apos;brought her flowers&apos;
&apos;replaced it with a new one&apos;
&apos;prevented him from leaving&apos;
&apos;asked her about what it all
meant&apos;
&apos;advised her to go&apos;
&apos;kept them laughing&apos;
&apos;told them (that) he was back&apos;
&apos;asked him where the money
</bodyText>
<figure confidence="0.803674027027027">
was&apos;
&apos;considered him foolish&apos;
&apos;had him clean up&apos; ,
&apos;turned blue&apos;
PP
PP—PP
Pvping
Pwh
intrans. part.
np_particle
particle_pp:
particle_wh:
vping
sfin
sbrst
vpto
directquote
adverb
&apos;look at the picture&apos;
&apos;turned from a frog into a
prince&apos;
&apos;responded by nodding her
head&apos;
&apos;wonder about how it
happened&apos;
&apos;touch down&apos;, &apos;turn over&apos;
&apos;put the dishes away&apos;,
&apos;put away the dishes&apos;
&apos;run off with it&apos;
&apos;figured out how to get there&apos;
&apos;needs fixing&apos;
&apos;claimed (that) it was over&apos;
&apos;demanded (that) he leave&apos;
&apos;agreed to do it over&apos;
&apos;no, said he&apos;, &amp;quot;no&amp;quot;, &apos;he said&apos;,
&apos;he said: &amp;quot;no&amp;quot;
&apos;behave badly&apos;
</figure>
<figureCaption confidence="0.729551">
figure 1: searchable complement types for verbs
</figureCaption>
<page confidence="0.992792">
429
</page>
<subsubsectionHeader confidence="0.501924">
1.3.2 Search strategies: positive and negative
queries
</subsubsectionHeader>
<bodyText confidence="0.984991666666667">
For the extraction of certain subcategorization
patterns, it is not necessary to simulate a parse
of all of the constituents. Where an explicit
context cue exists, a partial parse suffices. For
example, the query given in figure 2 below is
used to find [_ NP VPing] patterns (e.g. kept
them laughing). Note that the query does not
positively identify a noun phrase in the
osition following the target verb.
</bodyText>
<table confidence="0.972116454545454">
encoding description example
[Ssearch_by] target lemma kept
[pos!=&amp;quot;V.*ICJC them
ICJSICJTIPRFIP coming
RPIPUN1 { 1,5 }
[pos =&amp;quot;VVG gerund
IVBGIVDGIVH
G&amp;quot;]
within s; within a
sentence
igure 2: A query for [_NP VPing
</table>
<subsubsectionHeader confidence="0.860701">
1.3.3 Searches driven by subcategorization
frames
</subsubsectionHeader>
<bodyText confidence="0.999840904761905">
Applying queries like the one for [NP VPing]
&amp;quot;blindly&amp;quot;, i.e. in the absence of any
information on the target lemma, would
produce many false hits, since the query also
matches gerunds that are not subcategorized.
However, the information that the target verb
subcategorizes for a gerund dramatically
reduces the number of such errors.
The same mechanism is used for addressing
the problems associated with prepositional
phrase attachment. The general principle is
that prepositional phrases in certain contexts
are considered to be embedded in a preceding
noun phrase , unless the user specifies that a
given preposition is subcategorized for by the
target lemma. For example, the of-phrase in a
sequence Verb - NP - of - NP is interpreted as
part of the first NP (as in met the president of
the company), unless we are dealing with a
verb that has a [_NP PPoi] subcategorization
frame, e.g. cured the president of his asthma.
</bodyText>
<subsubsectionHeader confidence="0.849859">
1.3.4 Cascading queries
</subsubsectionHeader>
<bodyText confidence="0.999499777777778">
The result of each query is subtracted from the
lemma subcorpus and the remainder submitted
to the next set of queries. As a result, earlier
queries pre-empt later queries. For example,
concordance lines matching the queries for
passives, e.g. he was cured of his asthma are
filtered out early on in the process, so as to
avoid getting matched by the queries dealing
with (active intransitive) verb + prepositional
phrase complements, such as he boasted of his
achievements.
Another example of this type of preemption
concerns the interaction of the query for
ditransitive frames (brought her flowers) with
later queries for NP complements. A proper
name immediately followed by another
proper name (e.g. Henry James) is interpreted
as a single noun phrase except when the target
lemma subcategorizes for a ditransitive frame&apos;.
An analogous strategy is used for identifying
noun compounds. For ditransitives, strings that
represent two consecutive noun phrases are
queried for first. Note that this method
crucially relies on the fact that the
subcategorization properties of the target
lemma are given as the input to the query
process.
</bodyText>
<sectionHeader confidence="0.926508" genericHeader="method">
2 Examples
2.1 NPs
</sectionHeader>
<bodyText confidence="0.8135335">
An example of a complex query expression of
the kind we are using is given in figure 3. The
expression matches noun phrases like &amp;quot;the
three kittens&amp;quot;, &amp;quot;poor Mr. Smith&amp;quot;, &amp;quot;all three&amp;quot;,
&amp;quot;blue flowers&amp;quot;, &amp;quot;an unusually large hat&amp;quot;, etc.
([pos = &amp;quot;ATOICRDIDPSIDTOIORDICJT-
DTOICRD-PNI&amp;quot;]* [pos = &amp;quot;AVOIAJO-
AVO&amp;quot;]* [pos = &amp;quot;AJOIAJC I AJS IAJO-
AVOI AJO-NN1 IAJO-VVG&amp;quot;] * [pos=&amp;quot;NNO
INNIINN2IAJO-NN1INN1-NPOINN1-
VVBINN1VVGINN2-VVZ&amp;quot;])
I([pos = &amp;quot;ATOICRDIDPSIDTOIORDICJT-
DTOICRD-PNI&amp;quot;]+ [pos = &amp;quot;AVOIAJO-
AVO&amp;quot;]* [pos = &amp;quot;AJOIAJCIAJSIAJO-
AVOIAJO-NN1IAJO-VVG&amp;quot;]+)1 ([pos =
&amp;quot;ATOICRDIDPSIDTOIORDICJT-DT01
CRD-PNIT [pos = &amp;quot;AVOIAJO-AVO&amp;quot;]
[pos = &amp;quot;AJOI AJCIAJS IAJO-AVOIAJO-
NN1I AJO-VVGT [pos = &amp;quot;NPOINN1-
NPO&amp;quot;]+)1([pos = &amp;quot;AJOIAJCIAJST [pos
= &amp;quot;PNIIPNPIPNXICRD-PNI&amp;quot;])
figure 3. A regular expression matching NPs
</bodyText>
<subsectionHeader confidence="0.996096">
2.2 Coordinated passives
</subsectionHeader>
<bodyText confidence="0.947742125">
As an example of a query matching a
&apos;movement&apos; structure, consider the query for
coordinated passives, given in figure 3 below.
The leftmost column gives the query
expression itself, while the other columns show
Inevitably, this strategy fails in some cases, such as
&amp;quot;I&apos;m reading Henry James now&amp;quot; (vs. &amp;quot;I read Henry
stories.&amp;quot;
</bodyText>
<page confidence="0.995671">
430
</page>
<table confidence="0.937643941176471">
concordance lines found by this query. The target lemma is the verb cure:
[(lemma = &amp;quot;belbeinglgee) [(clas1= &amp;quot;c&apos;)1(class=&amp;quot;c&amp;quot;&amp; [Inonl=&amp;quot;orivon:1=&apos;hridlw [lemma = &amp;quot;cure&amp;quot; &amp;
&amp;(v■oid 1= &amp;quot;s&apos;) &amp; (pos 1= pos = &amp;quot;PUQ&amp;quot;) I (wird = ord=&apos;butUord=&amp;quot;lw)rd= po&amp;quot;VVBIVVDIVVGIV
&apos;1•1111INN2&apos;)] &amp;quot;:A{04) &amp;quot;rather thanUord=1&amp;quot;] V11VVNIVVZIAJO-
[po&amp;quot;VVNIVVDIVVD- [(pos&amp;quot;VVNIVVDIVBE31 VVN1AJOIVVDI MO-
VVNIAJO-VVNIADJO- V13DIVBGIVBEVBNIVB VVGINN1-VVB IN111-
VVD1 [po&amp;quot;AVP&apos;1? ZWDBVDDIVDGIVDD VVGINN2-VV ZIVVD-
[(((pos= &amp;quot;PUQ&apos;) I (vord = VDM1VDOVHBIVHDIV VVN&amp;quot; &amp; pos = &amp;quot;VVN&amp;quot; &amp;
&apos;7)) &amp; (class = &amp;quot;c&apos;)) I (clas HGIVI-111VHN1VITAVM pos &amp;quot;AJO&apos;l[pos
&amp;quot;c&apos;)]{W} 01VVBIVVCilVVINVOA &amp;quot;AJOIAJCIAJS1ATOICRDI
TOIDPSIIYIDIDTQIPNEP DPS1DT011YTQINNOINN
NEIPNQ&apos;)I (pos = &apos;F&apos;NQ&amp;quot; 1INN21NPOIORDIPNIIPN
&amp; ward = ?ewe)] {W} F1PNQPNX1VVGI VVD1
been ameliorated but not cured
be prevented or largely cured
be managed and often cured
be treated for it and cured
</table>
<tableCaption confidence="0.440135">
gure 4. A query fäf passives in coordination structures
</tableCaption>
<sectionHeader confidence="0.97883" genericHeader="method">
3 The macroprocessor
</sectionHeader>
<bodyText confidence="0.896543583333333">
A macroprocessor has been developed&apos;
that allows the user to specify in a simple
input file which subcorpora are to be
created for a given lemma.
The macroprocessor reads the the number
of matches for each subcategorization
pattern into an output file. A sample input
file for the lemma insist is given in figure 5
below.
4 Output format figure 5 Input form for macroporcessor
sorted, usually by the head of the first
complement following the target lemma.
</bodyText>
<figure confidence="0.945451121951219">
lemma: insist
CQP Search Definition
search_by: lemma
POS: verb
np: (yin) n
np_np: (yin) n
np_ap: (yin) n
np_pp: (list_ prepositions) none
np_pmg: (list_ prepositions) none
np_pwh: (_list_ prepositions) none
np_vpto: (yin) n
np_vping: (yin) n
np_sfin: (yin) n
np_wh: (yin) n
np_sbrst: (yin) n
save_text:
save_binary:
pp:
ping:
pwh:
particle:
np_particle:
particle_pp:
particle_wh:
ap:
directquote:
sfin
sbrst:
no
yes
(_list_ prepositions) on
(_list_ prepositions) on
(_list_ prepositions) on
(yin) n
(yin) n
{y/n) n
(y&apos;n) n
(yin) n
(yin) y
(yin) y
(yin) y
</figure>
<bodyText confidence="0.83761625">
The subcorpora can be saved as binary files
for further processing in CQP or XKwIC,
an interactive corpus query tool (Christ
1994) and as text files. The text files are
</bodyText>
<sectionHeader confidence="0.936948" genericHeader="method">
5 Limitations of the approach
</sectionHeader>
<bodyText confidence="0.99985025">
Our tool relies on subcategorization informa-
tion as its input. Hence it is not capable of
automatically learning subcategorization
frames, e.g. ones that are missing in diction-
</bodyText>
<footnote confidence="0.421929">
2 Our macroprocessor was developed by Collin Baker (ICSI-Berkeley) and Douglas Roland (U of Colorado, Boulder).
</footnote>
<page confidence="0.997869">
431
</page>
<bodyText confidence="0.999737869565217">
aries or omitted in the input file. The tool
facilitates the (manual) discovery of evidence
for new subcategorization frames, however, as
potential complement patterns are saved in
separate subcorpora. Indeed, this is one of the
ways in which the tool is being used in the
context of the FrameNet project.
Some of the technical limitations of the exist-
ing tools result from the fact that we are
working with an unparsed corpus. Thus, many
types of &apos;null&apos; or &apos;empty&apos; constituents&apos; are not
recognized by the queries. Ambiguities in
prepositional phrase attachment are another
major source of errors. For instance, of the
concordance lines supposedly instantiating a
[_NP PPwith] frame for the verb heal, several
in fact contained embedded PPs (e.g. LNP], as
in heal [children with asthma], rather than
[_NP PPwith], as in healing [arthritis] [with a
crystal ball]),
Finally, the search results can only be as accu-
rate as the part-of-speech tags and other an-
notations in the corpus.
</bodyText>
<sectionHeader confidence="0.996412" genericHeader="method">
7 Future directions
</sectionHeader>
<bodyText confidence="0.999849833333334">
Future versions of the tool will include
searches for predicative (vs. attributive) uses
for adjectives and nouns. For verbs, the
searches will be expanded to cover the entire
set of complementation patterns described in
the COMLEX syntactic dictionary.
</bodyText>
<sectionHeader confidence="0.972253" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.999750375">
We have presented an overview of a set of tools
for extracting corpus lines illustrating subcate-
gorization patterns of nouns, verbs, and adjec-
tives, and for determining the frequency of
these patterns. The tools are currently used as
part of the FrameNet project. An overview of
the whole project can be found at:
http://www.icsi.berkeley.edu/—framenet.
</bodyText>
<sectionHeader confidence="0.998111" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.910136625">
This work grew out of an extremely enjoyable
collaborative effort with Dr. Ulrich Heid of
IMS Stuttgart and Dan Jurafsky of the
University of Boulder, Colorado. I would like
to thank Doug Roland and especially the
untiring Collin Baker for their work on the
macroprocessor. I would also like to thank the
3 Our system is able to identify passive structures,
tough-movement, and a number of common left
isolation constructions, i.e. constructions involving
&apos;traces&apos; or movement sites.
members of the FrameNet project for their
comments and suggestions. I thank Judith
Eckle-Kohler of IMS-Stuttgart, JB Lowe of
ICSI-Berkeley and Dan Jurafsky for com-
ments on an earlier draft of this paper.
</bodyText>
<sectionHeader confidence="0.996492" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999756121212121">
Baker, C. F., Fillmore, C. J. and Lowe, J. B
(forthcoming). The Berkeley FrameNet project.
Proceedings of the 1998 ACL-COLING conference.
Christ, 0. (1994a) The IMS Corpus Workbench
Technical Manual. Institut fur maschinelle
Sprachverarbeitung, Universitat Stuttgart.
Christ, 0. (1994b) The XKwic User Manual. Institut
für maschinelle Sprachverarbeitung, Universitat
Stuttgart.
Fillmore, C. J. (1982) Frame Semantics. In
&amp;quot;Linguistics in the morning calm&amp;quot;, Hanshin
Publishing Co., Seoul, South Korea, 111-137.
Francis, W. N. and Kucera, H. (1982) Frequency
Analysis of English Usage: Lexicon and Grammar.
Houghton Mifflin, Boston, MA.
Hornby, A. S. (1989) Oxford Advanced Learner&apos;s
Dictionary of Current English. 4th edition. Oxford
University Press, Oxford, England.
MacDonald, M. C. (ed.) (1997) Lexical Representa-
tions and Sentence Processing. Erlbaum Taylor &amp;
Francis.
Macleod, C. and Grishman, R. (1995) COMLEX
Syntax Reference Manual. Linguistic Data
Consortium, U. of Pennsylvania.
Manning, Christopher D. (1993). Automatic Acquisi-
tion of a large subcategorization dictionary from
corpora. Proceedings of the 31st ACL, pp. 235-
242.
Procter, P. (ed.). (1989) Longman Dictionary of
Contemporary English. Longman, Burnt Mill,
Harlow, Essex, England.
Sinclair, J. M. (1987) Collins Cobuild English
Language Dictionary. Collins, London, England.
</reference>
<page confidence="0.998558">
432
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.697607">
<title confidence="0.877202">Automatic extraction of subcorpora based on subcategorization frames from a part-of-speech tagged corpus</title>
<author confidence="0.972715">Susanne GAHL</author>
<affiliation confidence="0.9746445">UC Berkeley, Department of Linguistics ICSI</affiliation>
<address confidence="0.9981065">1947 Center St, Suite 600 Berkeley, CA 94704-1105</address>
<email confidence="0.999791">gahl@icsi.berkeley.edu</email>
<abstract confidence="0.996681263157895">This paper presents a method for extracting subcorpora documenting different subcategorization frames for verbs, nouns, and adjectives in the 100 mio. word British National Corpus. The extraction tool consists of a set of batch files for use with the Corpus Query Processor (CQP), which is part of the IMS corpus workbench (cf. Christ 1994a,b). A macroprocessor has been developed that allows the user to specify in a simple input file which subcorpora are to be created for a given lemma. The resulting subcorpora can be used (1) to provide evidence for the subcategorization properties of a given lemma, and to facilitate the selection of corpus lines for lexicographic research, and (2) to determine the frequencies of different syntactic contexts of each lemma.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>C F Baker</author>
<author>C J Fillmore</author>
<author>J Lowe</author>
</authors>
<title>B (forthcoming). The Berkeley FrameNet project.</title>
<booktitle>Proceedings of the 1998 ACL-COLING conference.</booktitle>
<marker>Baker, Fillmore, Lowe, </marker>
<rawString>Baker, C. F., Fillmore, C. J. and Lowe, J. B (forthcoming). The Berkeley FrameNet project. Proceedings of the 1998 ACL-COLING conference.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Christ</author>
</authors>
<title>(1994a) The IMS Corpus Workbench Technical Manual. Institut fur maschinelle Sprachverarbeitung,</title>
<location>Universitat Stuttgart.</location>
<marker>Christ, </marker>
<rawString>Christ, 0. (1994a) The IMS Corpus Workbench Technical Manual. Institut fur maschinelle Sprachverarbeitung, Universitat Stuttgart.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Christ</author>
</authors>
<title>(1994b) The XKwic User Manual. Institut für maschinelle Sprachverarbeitung,</title>
<location>Universitat Stuttgart.</location>
<marker>Christ, </marker>
<rawString>Christ, 0. (1994b) The XKwic User Manual. Institut für maschinelle Sprachverarbeitung, Universitat Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Fillmore</author>
</authors>
<title>Frame Semantics. In &amp;quot;Linguistics in the morning calm&amp;quot;,</title>
<date>1982</date>
<journal>Hanshin Publishing Co., Seoul, South Korea,</journal>
<pages>111--137</pages>
<contexts>
<context position="3210" citStr="Fillmore 1982" startWordPosition="482" endWordPosition="483">s yet no easy way of obtaining information from large corpora on the relative frequency of complementation patterns. None of these points argue against the usefulness of the available resources, but they show that there is a gap in the available information. To address this need, we have developed a tool for extracting evidence for subcategorization patterns from the 100 mio. word British National Corpus (BNC). The tool is used as part of the lexicon-building process in the FrameNet project, an NSF-funded project aimed at creating a lexical database based on the principles of Frame Semantics (Fillmore 1982). 428 1 Infrastructure 1.1 Tools We are using the 100 mio. word British National Corpus, with the following corpus query tools: . CQP (Corpus Query Processor, Christ (1994)), a general corpus query processor for complex queries with any number and combination of annotated information types, including part-of-speech tags, morphosyntactic tags, lemmas and sentence boundarie S. • A macroprocessor for use with CQP that allows the user to specify which subcorpora are to be created for a given lemma. The corpus queries are written in the CQP corpus query language, which uses regular expressions over</context>
<context position="4721" citStr="Fillmore 1982" startWordPosition="712" endWordPosition="713">mple of each pattern. The categories we are using are roughly based on those used in the COMLEX syntactic dictionary (Macleod et al. 1995). In our queries for nouns and adjectives as targets, we are able to extract prepositional, clausal, infinitival, and gerundial complements. In addition, the tool accomodates searches for compounds and for possessor phrases Om nei.hbor&apos;s addiction to cake,tax milk allergy). ENI though these categories are not tied to the syntactic subcategorization frames of the target lemmas, they often instantiate semantic arguments, or, more specifically, Frame elements (Fillmore 1982, Baker et al. forthcoming). 1.3 Method 1.3.1 Overview We start by creating a subcorpus containing all concordance lines for a given lemma. We call this subcorpus a lemma-subcorpus. The extraction of smaller subcorpora from the lemma subcorpus then proceeds in two stages. During the first stage, syntactic patterns involving &apos;displaced&apos; arguments (i.e. &apos;left isolation&apos; or &apos;movement&apos; phenomena) are extracted, such as passives, tough movement and constructions involving WH-extraction. The result of this procedure is a set of subcorpora that are homogeneous with respect to major constituent order.</context>
</contexts>
<marker>Fillmore, 1982</marker>
<rawString>Fillmore, C. J. (1982) Frame Semantics. In &amp;quot;Linguistics in the morning calm&amp;quot;, Hanshin Publishing Co., Seoul, South Korea, 111-137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W N Francis</author>
<author>H Kucera</author>
</authors>
<title>Frequency Analysis of English Usage: Lexicon and Grammar.</title>
<date>1982</date>
<location>Houghton Mifflin, Boston, MA.</location>
<contexts>
<context position="2583" citStr="Francis and Kucera (1982)" startWordPosition="377" endWordPosition="380">eir frequencies in large corpora: 1. Statistical approaches to NLP rely on information not just on the range of combinatory possibilities of words, but also the relative frequencies of the expected patterns. 2. Dictionaries that list subcategorization frames often list expected patterns, rather than actual ones. Lexicographers and lexicologist need access to the evidence for this information. 3. Frequency information has come to be the focus of much psycholinguistic research on sentence processing (see for example MacDonald 1997). While information on word frequency is readily available (e.g. Francis and Kucera (1982)), there is as yet no easy way of obtaining information from large corpora on the relative frequency of complementation patterns. None of these points argue against the usefulness of the available resources, but they show that there is a gap in the available information. To address this need, we have developed a tool for extracting evidence for subcategorization patterns from the 100 mio. word British National Corpus (BNC). The tool is used as part of the lexicon-building process in the FrameNet project, an NSF-funded project aimed at creating a lexical database based on the principles of Fram</context>
</contexts>
<marker>Francis, Kucera, 1982</marker>
<rawString>Francis, W. N. and Kucera, H. (1982) Frequency Analysis of English Usage: Lexicon and Grammar. Houghton Mifflin, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A S Hornby</author>
</authors>
<title>Oxford Advanced Learner&apos;s Dictionary of Current English. 4th edition.</title>
<date>1989</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford, England.</location>
<contexts>
<context position="1451" citStr="Hornby 1989" startWordPosition="212" endWordPosition="213">en lemma, and to facilitate the selection of corpus lines for lexicographic research, and (2) to determine the frequencies of different syntactic contexts of each lemma. Introduction A number of resources are available for obtaining subcategorization information, i.e. information on the types of syntactic complements associated with valence-bearing predicators (which include verbs, nouns, and adjectives). This information, also referred to as valence information is available both in machine-readable form, as in the COMLEX database (Macleod et al. 1995), and in humanreadable dictionaries (e.g. Hornby 1989, Procter 1978, Sinclair 1987). Increasingly, tools are also becoming available for acquiring subcategorization information from corpora, i.e. for inferring the subcategorization frames of a given lemma (e.g. Manning 1993). None of these resources provide immediate access to corpus evidence, nor do they provide information on the relative frequency of the patterns that are listed for a given lemma. There is a need for a tool that can (1) find evidence for subcategorization patterns and (2) determine their frequencies in large corpora: 1. Statistical approaches to NLP rely on information not ju</context>
</contexts>
<marker>Hornby, 1989</marker>
<rawString>Hornby, A. S. (1989) Oxford Advanced Learner&apos;s Dictionary of Current English. 4th edition. Oxford University Press, Oxford, England.</rawString>
</citation>
<citation valid="true">
<date>1997</date>
<booktitle>Lexical Representations and Sentence Processing.</booktitle>
<editor>MacDonald, M. C. (ed.)</editor>
<publisher>Erlbaum Taylor &amp; Francis.</publisher>
<marker>1997</marker>
<rawString>MacDonald, M. C. (ed.) (1997) Lexical Representations and Sentence Processing. Erlbaum Taylor &amp; Francis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Macleod</author>
<author>R Grishman</author>
</authors>
<title>COMLEX Syntax Reference Manual. Linguistic Data Consortium, U. of Pennsylvania.</title>
<date>1995</date>
<marker>Macleod, Grishman, 1995</marker>
<rawString>Macleod, C. and Grishman, R. (1995) COMLEX Syntax Reference Manual. Linguistic Data Consortium, U. of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
</authors>
<title>Automatic Acquisition of a large subcategorization dictionary from corpora.</title>
<date>1993</date>
<booktitle>Proceedings of the 31st ACL,</booktitle>
<pages>235--242</pages>
<contexts>
<context position="1673" citStr="Manning 1993" startWordPosition="241" endWordPosition="242">or obtaining subcategorization information, i.e. information on the types of syntactic complements associated with valence-bearing predicators (which include verbs, nouns, and adjectives). This information, also referred to as valence information is available both in machine-readable form, as in the COMLEX database (Macleod et al. 1995), and in humanreadable dictionaries (e.g. Hornby 1989, Procter 1978, Sinclair 1987). Increasingly, tools are also becoming available for acquiring subcategorization information from corpora, i.e. for inferring the subcategorization frames of a given lemma (e.g. Manning 1993). None of these resources provide immediate access to corpus evidence, nor do they provide information on the relative frequency of the patterns that are listed for a given lemma. There is a need for a tool that can (1) find evidence for subcategorization patterns and (2) determine their frequencies in large corpora: 1. Statistical approaches to NLP rely on information not just on the range of combinatory possibilities of words, but also the relative frequencies of the expected patterns. 2. Dictionaries that list subcategorization frames often list expected patterns, rather than actual ones. L</context>
</contexts>
<marker>Manning, 1993</marker>
<rawString>Manning, Christopher D. (1993). Automatic Acquisition of a large subcategorization dictionary from corpora. Proceedings of the 31st ACL, pp. 235-242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Procter</author>
</authors>
<title>Longman Dictionary of Contemporary</title>
<date>1989</date>
<editor>English. Longman, Burnt Mill,</editor>
<location>Harlow, Essex, England.</location>
<marker>Procter, 1989</marker>
<rawString>Procter, P. (ed.). (1989) Longman Dictionary of Contemporary English. Longman, Burnt Mill, Harlow, Essex, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Sinclair</author>
</authors>
<title>Collins Cobuild English Language Dictionary.</title>
<date>1987</date>
<location>Collins, London, England.</location>
<contexts>
<context position="1481" citStr="Sinclair 1987" startWordPosition="216" endWordPosition="217"> the selection of corpus lines for lexicographic research, and (2) to determine the frequencies of different syntactic contexts of each lemma. Introduction A number of resources are available for obtaining subcategorization information, i.e. information on the types of syntactic complements associated with valence-bearing predicators (which include verbs, nouns, and adjectives). This information, also referred to as valence information is available both in machine-readable form, as in the COMLEX database (Macleod et al. 1995), and in humanreadable dictionaries (e.g. Hornby 1989, Procter 1978, Sinclair 1987). Increasingly, tools are also becoming available for acquiring subcategorization information from corpora, i.e. for inferring the subcategorization frames of a given lemma (e.g. Manning 1993). None of these resources provide immediate access to corpus evidence, nor do they provide information on the relative frequency of the patterns that are listed for a given lemma. There is a need for a tool that can (1) find evidence for subcategorization patterns and (2) determine their frequencies in large corpora: 1. Statistical approaches to NLP rely on information not just on the range of combinatory</context>
</contexts>
<marker>Sinclair, 1987</marker>
<rawString>Sinclair, J. M. (1987) Collins Cobuild English Language Dictionary. Collins, London, England.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>