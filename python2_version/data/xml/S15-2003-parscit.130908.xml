<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006504">
<title confidence="0.999375">
CICBUAPnlp: Graph-Based Approach for Answer Selection
in Community Question Answering Task
</title>
<author confidence="0.953768">
Helena G´omez-Adorno, Grigori Sidorov Darnes Vilari˜no, David Pinto
</author>
<affiliation confidence="0.7770035">
Center for Computing Research Faculty of Computer Science
Instituto Polit´ecnico Nacional Benem´erita Universidad Aut´onoma de Puebla
</affiliation>
<address confidence="0.6061345">
Av. Juan de Dios B´atiz Av. San Claudio y 14 sur
C.P. 07738, Mexico City, Mexico C.P. 72570, Puebla, Mexico
</address>
<email confidence="0.9662975">
helena.adorno@gmail.com darnes@cs.buap.mx
sidorov@cic.ipn.mx dpinto@cs.buap.mx
</email>
<sectionHeader confidence="0.995348" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997705">
This paper describes our approach for the
Community Question Answering Task, which
was presented at the SemEval 2015. The sys-
tem should read a given question and identify
good, potentially relevant, and bad answers
for that question. Our approach transforms the
answers of the training set into a graph based
representation for each answer class, which
contains lexical, morphological, and syntactic
features. The answers in the test set are also
transformed into the graph based representa-
tion individually. After this, different paths are
traversed in the training and test sets in order
to find relevant features of the graphs. As a
result of this procedure, the system constructs
several vectors of features: one for each tra-
versed graph. Finally, a cosine similarity is
calculated between the vectors in order to find
the class that best matches a given answer.
Our system was developed for the English lan-
guage only, and it obtained an accuracy of
53.74 for subtask A and 44.0 for subtask B.
</bodyText>
<sectionHeader confidence="0.999139" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999615318181818">
In this paper we present the experiments carried out
as part of our participation in the SemEval-2015
Task 3 (Answer Selection in Community Question
Answering). The Answer Selection in Commu-
nity Question Answering task is proposed for the
first time this year in the International Workshop on
Semantic Evaluation (SemEval-2015). The task is
based on an application scenario, which is related to
textual entailment, semantic similarity and NL infer-
ence.
Community question answering (CQA) websites
enable people to post questions and answers in var-
ious domains. In this way, users can obtain specific
answers to their questions, instead of searching in
the large volume of information available in the web.
However, it takes effort to go through all possible an-
swers and select which one is the most accurate one
for a specific question. The task proposes to auto-
mate this process by predicting the quality of exist-
ing answers with respect to a question.
There are few works in the literature on evaluat-
ing the quality of answers provided in CQA sites.
Most of such works employ non-textual and tem-
poral features in order to built classification models
for predicting the best answer for a given question.
In (Jeon et al., 2006), the authors extract 13 non-
textual features from the Naver data set and build
a maximum entropy classification model to predict
the quality (three classes: Bad, Medium and Good)
of a given answer. A similar approach is used in
(Shah and Pomerantz, 2010), but extracting 21 fea-
tures (mainly non-textual) from Yahoo! Answers;
the authors employ a logistic regression and classi-
fication model to predict the best answer. Besides,
a set of temporal features is proposed in (Cai and
Chakravarthy, 2011) in order to predict the best an-
swer for a given question. In this work the authors
argue that the traditional classification approaches
are not well suited for this problem because of the
highly imbalanced ratio of the best answer and the
non-best answers in their data set, so they propose
to use learning to rank approaches.
Unlike these approaches, we use only textual in-
formation for predicting the quality of the answers.
</bodyText>
<page confidence="0.99327">
18
</page>
<bodyText confidence="0.915775714285714">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 18–22,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
Our approach is based on our previous research
(Pinto et al., 2014) and (Sidorov et al., 2014), where
we propose the graph-based representation model
(Integrated Syntactic Graph) and the soft similarity
measure (soft cosine measure). Our experimental re-
sults are promising, they overcome the baseline sys-
tem for this challenge.
The rest of the paper is organized as follows. Sec-
tion 2 describes our approach. Section 3 presents the
configuration of the submitted runs and the evalua-
tion results. Finally, Section 4 presents the conclu-
sions and outlines some directions of future work.
</bodyText>
<sectionHeader confidence="0.980806" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.976836045454545">
For many problems in natural language processing,
graph structure is an intuitive, natural and direct way
to represent data. There exist several research works
that have employed graphs for text representation in
order to solve some particular problem (Mihalcea
and Radev, 2011). We propose an approach based on
a graph methodology, which was described in detail
in (Pinto et al., 2014), for building the correspond-
ing system of the two subtasks. These subtasks are
described as follows:
Subtask A Given a question (short title + extended
description) and a list of community answers,
classify each of the answers as: Good, Potential
or Bad (bad, dialog, non-English, other).
Subtask B Given a YES/NO question (short title +
extended description) and a list of community
answers, decide whether the global answer to
the question should be yes, no or unsure, based
on the individual good answers.
The proposed system consists of the following sub-
modules: document preprocessing, graph genera-
tion, and answer quality classification.
</bodyText>
<subsectionHeader confidence="0.99769">
2.1 Document Preprocessing
</subsectionHeader>
<bodyText confidence="0.9999495">
An XML parser receives as input a structured cor-
pus in XML format. This XML file contains all the
questions, along with their respective answers. An
XML interpreter extracts the questions and associ-
ated answers.
Thereafter, we process the answers for both sub-
tasks separately. All the answers belonging to the
same class are grouped together, and the result is
passed to the next module. This means that at the
end of this module, we will have all the good an-
swers in one document, the bad ones in another doc-
ument and so on for all classes. In the same way, for
the task B, the yes/no answers are grouped together
in different documents.
</bodyText>
<subsectionHeader confidence="0.999531">
2.2 Graph Generation
</subsectionHeader>
<bodyText confidence="0.997154">
In the graph generation module, all sentences of
each class are parsed to produce what we call their
Integrated Syntactic Graph (ISG) representation (see
(Pinto et al., 2014)). For the graph representation
we took into account various linguistic levels (lexi-
cal, syntactic, morphological, and semantic) in order
to capture the majority of the features present in the
text.
The process of the graph generation is performed
by the following submodules:
The Syntactic Parser is the base of the graph struc-
ture. We use the Stanford Dependency Parser1
for producing the parsed tree for each sentence
of the documents. In this type of parsing, we
detect grammatical relation.
The Morphological Tagger obtains PoS tags
of words. For this purpose we used the Stanford
Log linear Part-Of-Speech Tagger2 for English.
The Lancaster stemmer algorithm was used in
order to obtain word stems.
As a result of this process, each class is repre-
sented as a graph rooted in a ROOT − 0 node. The
vertices to sub-trees represent all sentences in the
class document. The nodes of the trees represent
words or lemmas of the sentences along with their
part-of-speech tags. The vertices between nodes
represent the dependency tags between these con-
nected nodes along with a frequency label, for exam-
ple: nsubj-S, that shows the number of occurrences
of the pair (initial node, final node) in the graph plus
the frequency of the dependency tag of the same pair
of nodes. In the same way, the answers to be clas-
sified in one of the quality classes are represented in
an ISG with the same characteristics.
In order to fully understand the process of con-
struction of the ISG and the collapse of nodes in the
</bodyText>
<footnote confidence="0.9999045">
1http://nlp.stanford.edu/software/lex-parser.shtml
2http://nlp.stanford.edu/software/tagger.shtml
</footnote>
<page confidence="0.993458">
19
</page>
<figure confidence="0.999868686746987">
cc
And_CC
I_PRP
ROOT-0 root
aux
going_VBG
’m_VBP
aux
nsubj
xcomp
to_TO
how_WRB
pobj
with_IN
you_PRP
share_VB
advmod
prep
dobj
dep
have_VBP
story_NN
the_DT
aux
det
prep
(b ) And this is the name of my campaign, SING Campaign
campaigner_NN
become_VBN
pcomp
as_IN
cop
det
nn
an_DT
HIV/AIDS_NN
nsubj
this_DT
ROOT-0 root
name_NN
is_VBZ
cop
det
prep
the_DT
of_IN
campaign_NN
appos
poss my_PRP$
pobj
Campaign_NNP
nn
SING_NNP
pobj
In_IN
prep
Nelson_NNP
nn
November_NNP
Mandela_NNP
nsubjpass I_PRP
of_IN
pobj
poss
prep
’s_POS
was_VBD
aux
Foundation_NNP
nn
xcomp
det
46664_CD
the_DT
launch_NN
dobj part_NN
pobj
take_VB
prep
in_IN
ROOT-0 root invited_VBN auxpass to_TO
prep pobj 2003_CD
(c ) In November of 2003 I was invited to take part in the launch of Nelson Mandela&apos;s 46664 Foundation
</figure>
<figureCaption confidence="0.9890865">
Figure 1: Dependency trees of three sentences of a target text using word POS combination for the nodes and depen-
dency labels for the edges
</figureCaption>
<figure confidence="0.999439540229885">
appos-2
nn-6
was_VBD
auxpass-2
pobj-7
prep-8
invited_VBN
pobj-7
poss-4
my_PRP$
pobj-7
2003_CD
In_IN
prep-8
pobj-7
poss-4 Mandela_NNP
’s_POS
possessive-2
Foundation_NNP
prep-8
nsubjpass-2
campaign_NN
nn-6 Nelson_NNP
November_NNP prep-8 of_IN
xcomp-3
46664_CD
nn-6
cc-2 And_CC
pobj-7
in_IN
launch_NN
root-5
det-5
take_VB
this_DT
part_NN
nsubj-5
det-5
name_NN
the_DT
cop-4
prep-8
dobj-3
root-5
det-5
is_VBZ
ROOT-0
aux-5
cop-4
root-5
his_PRP$
foundation_NN
poss-4
root-5
nsubj-5
That_DT
nn-6
nsubj-5
I_PRP
going_VBG
aux-5
’m_VBP
HIV/AIDS_NN
xcomp-3
to_TO
nn-6
prep-8
share_VB
become_VBN
pobj-7
with_IN
you_PRP
cop-4
prep-8
dobj-3 story_NN as_IN
dep-2
pcomp-2
advmod-2
how_WRB
aux-5
campaigner_NN
an_DT
det-5
aux-5
have_VBP
Campaign_NNP
SING_NNP
</figure>
<figureCaption confidence="0.999238">
Figure 2: The Integrated Syntactic Graph for the three sentences considered as example
</figureCaption>
<page confidence="0.980154">
20
</page>
<bodyText confidence="0.9986644">
graph, in Figure 1, we show the dependency trees
of three sentences; each node of the graph is aug-
mented with other annotations, such as the combi-
nation of lemma (or word) and POS tags: (lemma
POS).
The collapsed graph of the three sentences is
shown in Figure 2. Each edge of this graph contains
the dependency tag together with a number that in-
dicates the frequency of the dependency tag plus the
frequency of the pair of nodes, both calculated using
the occurrences of the dependency trees associated
to each sentence.
The feature extraction process starts by fixing the
root node of the answer graph as the initial node,
whereas the selected final nodes correspond to the
remaining nodes of the answer graph. We use the
Dijkstra′s Algorithm (Dijkstra, 1959) for find-
ing the shortest paths between the initial and each
final node. After this, we count the occurrences of
all the multi-level linguistic features considered in
the text representation such as POS tags and depen-
dency tags found in the path. The same procedure
is performed with the class document graph, using
the pair of nodes identified in the answer graph as
the initial and final node. As a result of this proce-
dure, we obtain two feature vectors: one for the an-
swer and another one for the class document. This
module was implemented in Python, using the Net-
workX3 package for creation and manipulation of
graphs.
</bodyText>
<subsectionHeader confidence="0.991312">
2.3 Classification based on Quality of Answers
</subsectionHeader>
<bodyText confidence="0.7496146">
This module receives several feature vectors (−→ft,i)
for each class document. Thus, the class docu-
ment d is now →represented by m features (d∗ _
{−→
fd,1, fd,2,..., fd,m}), as well as the different an-
</bodyText>
<equation confidence="0.87894">
swers a, (a∗
_ {−→
fa,1, −→
fa,2,..., −−→
fa,m}),
</equation>
<bodyText confidence="0.9999776">
number of different paths that can be traversed in
both graphs.
We use the cosine similarity measure from the
equation below for calculating the degree of simi-
larity among each traversed path.
</bodyText>
<equation confidence="0.9715336">
Similarity(a∗, d∗) _
3https://networkx.github.io/
fa,i · −→
−→fd,i
||−→fa,i ||· ||−→fd,i||
</equation>
<bodyText confidence="0.9993974">
After obtaining all similarity scores between the
answers with each of the class documents, the class
(to which the document belongs) achieving the high-
est score is selected as the correct class for each an-
swer.
</bodyText>
<sectionHeader confidence="0.999974" genericHeader="background">
3 Results
</sectionHeader>
<bodyText confidence="0.999884846153846">
The acronym of our system is CICBUAPnlp. Tables
1 and 2 show the scores for the English subtasks A
and B on the test data, respectively. Although, our
results did not overcome the general average, it is
worth noting that our methodology is quite simple
and straightforward. We only used syntactic and
morphological features, thus comparing the struc-
tures of the answers against the structure of the la-
beled sets. Instead of training a classifier, we built
a Syntactic Integrated Graph for each class and then
try to match the answers in the test set against them,
calculating in this way the similarity between the
graphs.
</bodyText>
<tableCaption confidence="0.999669">
Table 1: Results of the subtask A, English
</tableCaption>
<table confidence="0.999552384615385">
TeamId Macro F1 Accuracy Rank
JAIS 57.19 72.52 1
HITSZ-ICRC 56.41 68.67 2
QCRI 53.74 70.50 3
ECNU 53.47 70.55 4
ICRC-HIT 49.60 67.68 5
VectorSlu 49.10 66.45 5
Shiraz 47.34 56.83 7
FBK-HLT 47.32 69.13 8
Voltron 46.07 62.35 9
CICBUAPnlp 40.40 53.74 10
Yamraj 37.65 45.50 11
CoMiC 30.63 54.20 12
</table>
<sectionHeader confidence="0.996429" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999980714285714">
We described the approach and the system devel-
oped as a part of our participation in the Answer
Selection in Community Question Answering task.
The approach uses a graph structure for represent-
ing the classes and the answers. It extracts lin-
guistic features from both graphs—classes and an-
swers—by traversing shortest paths. The features
</bodyText>
<figure confidence="0.562995142857143">
being m the
→ →
Cosine(fa,i, fd,i)
m
i=1
m
i=1
</figure>
<page confidence="0.998526">
21
</page>
<tableCaption confidence="0.999507">
Table 2: Results of the subtask B, English
</tableCaption>
<table confidence="0.999351555555556">
TeamId Macro F1 Accuracy Rank
VectorSlu 63.7 72.0 1
ECNU 55.8 68.0 2
QCRI 53.6 64.0 3=4
HITSZ-ICRC 53.6 64.0 3=4
CICBUAPnlp 38.8 44.0 5
ICRC-HIT 30.9 52.0 6
Yamraj 29.8 28.0 7
FBK-HLT 27.8 40.0 8
</table>
<bodyText confidence="0.993642666666667">
are further used for computing the similarity be-
tween the classes and the answers.
We sent two runs (primary and contrastive) for
each English subtask to the evaluation forum. The
best run in both cases was the primary run.
In future work, we are planning to use the soft
cosine measure to compare the similarity between
the answers and the quality classes, thus evaluating
the feasibility of this kind of structures for this task.
</bodyText>
<sectionHeader confidence="0.998237" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.975443166666667">
This work was done under partial support of the
Mexican Government (CONACYT, SNI, COFAA-
IPN, SIP-IPN 20144534, 20144274) and FP7
-PEOPLE-2010-IRSES: “Web Information Quality
Evaluation Initiative (WIQ-EI)” European Commis-
sion project 269180.
</bodyText>
<sectionHeader confidence="0.997227" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999346117647059">
Yuanzhe Cai and Sharma Chakravarthy. 2011. Predict-
ing answer quality in q/a social networks: Using tem-
poral features. Technical report, University of Texas
at Arlington.
Edsger. W. Dijkstra. 1959. A note on two problems
in connexion with graphs. Numerische Mathematik,
1:269–271.
Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee, and Soyeon
Park. 2006. A framework to predict the quality of
answers with non-textual features. In Proceedings
of the 29th Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, SIGIR ’06, pages 228–235, New York, NY,
USA. ACM.
Rada Mihalcea and Dragomir Radev. 2011. Graph-
based natural language processing and information
retrieval. Cambridge; New York.
David Pinto, Helena G´omez-Adorno, Darnes Vilari˜no,
and Vivek Kumar Singh. 2014. A graph-based
multi-level linguistic representation for document un-
derstanding. Pattern Recognition Letters, 41(0):93
– 102. Supervised and Unsupervised Classification
Techniques and their Applications.
Chirag Shah and Jefferey Pomerantz. 2010. Evaluat-
ing and predicting answer quality in community qa.
In Proceedings of the 33rd International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, SIGIR ’10, pages 411–418, New York,
NY, USA. ACM.
Grigori Sidorov, Alexander F. Gelbukh, Helena Gmez-
Adorno, and David Pinto. 2014. Soft similarity and
soft cosine measure: Similarity of features in vector
space model. Computaci´on y Sistemas, 18(3):491 –
504.
</reference>
<page confidence="0.999024">
22
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.377087">
<title confidence="0.9949495">CICBUAPnlp: Graph-Based Approach for Answer in Community Question Answering Task</title>
<author confidence="0.992792">Grigori Sidorov Darnes David Pinto G´omez-Adorno</author>
<affiliation confidence="0.975378">Center for Computing Research Faculty of Computer Science Instituto Polit´ecnico Nacional Benem´erita Universidad Aut´onoma de Puebla</affiliation>
<abstract confidence="0.960058370370371">Av. Juan de Dios B´atiz Av. San Claudio y 14 sur C.P. 07738, Mexico City, Mexico C.P. 72570, Puebla, Mexico helena.adorno@gmail.com darnes@cs.buap.mx sidorov@cic.ipn.mx dpinto@cs.buap.mx Abstract This paper describes our approach for the Community Question Answering Task, which was presented at the SemEval 2015. The system should read a given question and identify good, potentially relevant, and bad answers for that question. Our approach transforms the answers of the training set into a graph based representation for each answer class, which contains lexical, morphological, and syntactic features. The answers in the test set are also transformed into the graph based representation individually. After this, different paths are traversed in the training and test sets in order to find relevant features of the graphs. As a result of this procedure, the system constructs several vectors of features: one for each traversed graph. Finally, a cosine similarity is calculated between the vectors in order to find the class that best matches a given answer. Our system was developed for the English language only, and it obtained an accuracy of 53.74 for subtask A and 44.0 for subtask B.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yuanzhe Cai</author>
<author>Sharma Chakravarthy</author>
</authors>
<title>Predicting answer quality in q/a social networks: Using temporal features.</title>
<date>2011</date>
<tech>Technical report,</tech>
<institution>University of Texas at Arlington.</institution>
<contexts>
<context position="3227" citStr="Cai and Chakravarthy, 2011" startWordPosition="506" endWordPosition="509">oral features in order to built classification models for predicting the best answer for a given question. In (Jeon et al., 2006), the authors extract 13 nontextual features from the Naver data set and build a maximum entropy classification model to predict the quality (three classes: Bad, Medium and Good) of a given answer. A similar approach is used in (Shah and Pomerantz, 2010), but extracting 21 features (mainly non-textual) from Yahoo! Answers; the authors employ a logistic regression and classification model to predict the best answer. Besides, a set of temporal features is proposed in (Cai and Chakravarthy, 2011) in order to predict the best answer for a given question. In this work the authors argue that the traditional classification approaches are not well suited for this problem because of the highly imbalanced ratio of the best answer and the non-best answers in their data set, so they propose to use learning to rank approaches. Unlike these approaches, we use only textual information for predicting the quality of the answers. 18 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 18–22, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational </context>
</contexts>
<marker>Cai, Chakravarthy, 2011</marker>
<rawString>Yuanzhe Cai and Sharma Chakravarthy. 2011. Predicting answer quality in q/a social networks: Using temporal features. Technical report, University of Texas at Arlington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Dijkstra</author>
</authors>
<title>A note on two problems in connexion with graphs.</title>
<date>1959</date>
<journal>Numerische Mathematik,</journal>
<pages>1--269</pages>
<contexts>
<context position="10444" citStr="Dijkstra, 1959" startWordPosition="1659" endWordPosition="1660">nation of lemma (or word) and POS tags: (lemma POS). The collapsed graph of the three sentences is shown in Figure 2. Each edge of this graph contains the dependency tag together with a number that indicates the frequency of the dependency tag plus the frequency of the pair of nodes, both calculated using the occurrences of the dependency trees associated to each sentence. The feature extraction process starts by fixing the root node of the answer graph as the initial node, whereas the selected final nodes correspond to the remaining nodes of the answer graph. We use the Dijkstra′s Algorithm (Dijkstra, 1959) for finding the shortest paths between the initial and each final node. After this, we count the occurrences of all the multi-level linguistic features considered in the text representation such as POS tags and dependency tags found in the path. The same procedure is performed with the class document graph, using the pair of nodes identified in the answer graph as the initial and final node. As a result of this procedure, we obtain two feature vectors: one for the answer and another one for the class document. This module was implemented in Python, using the NetworkX3 package for creation and</context>
</contexts>
<marker>Dijkstra, 1959</marker>
<rawString>Edsger. W. Dijkstra. 1959. A note on two problems in connexion with graphs. Numerische Mathematik, 1:269–271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwoon Jeon</author>
<author>W Bruce Croft</author>
<author>Joon Ho Lee</author>
<author>Soyeon Park</author>
</authors>
<title>A framework to predict the quality of answers with non-textual features.</title>
<date>2006</date>
<booktitle>In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’06,</booktitle>
<pages>228--235</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2729" citStr="Jeon et al., 2006" startWordPosition="425" endWordPosition="428">o their questions, instead of searching in the large volume of information available in the web. However, it takes effort to go through all possible answers and select which one is the most accurate one for a specific question. The task proposes to automate this process by predicting the quality of existing answers with respect to a question. There are few works in the literature on evaluating the quality of answers provided in CQA sites. Most of such works employ non-textual and temporal features in order to built classification models for predicting the best answer for a given question. In (Jeon et al., 2006), the authors extract 13 nontextual features from the Naver data set and build a maximum entropy classification model to predict the quality (three classes: Bad, Medium and Good) of a given answer. A similar approach is used in (Shah and Pomerantz, 2010), but extracting 21 features (mainly non-textual) from Yahoo! Answers; the authors employ a logistic regression and classification model to predict the best answer. Besides, a set of temporal features is proposed in (Cai and Chakravarthy, 2011) in order to predict the best answer for a given question. In this work the authors argue that the tra</context>
</contexts>
<marker>Jeon, Croft, Lee, Park, 2006</marker>
<rawString>Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee, and Soyeon Park. 2006. A framework to predict the quality of answers with non-textual features. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’06, pages 228–235, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Dragomir Radev</author>
</authors>
<title>Graphbased natural language processing and information retrieval.</title>
<date>2011</date>
<location>Cambridge; New York.</location>
<contexts>
<context position="4713" citStr="Mihalcea and Radev, 2011" startWordPosition="740" endWordPosition="743">l results are promising, they overcome the baseline system for this challenge. The rest of the paper is organized as follows. Section 2 describes our approach. Section 3 presents the configuration of the submitted runs and the evaluation results. Finally, Section 4 presents the conclusions and outlines some directions of future work. 2 Approach For many problems in natural language processing, graph structure is an intuitive, natural and direct way to represent data. There exist several research works that have employed graphs for text representation in order to solve some particular problem (Mihalcea and Radev, 2011). We propose an approach based on a graph methodology, which was described in detail in (Pinto et al., 2014), for building the corresponding system of the two subtasks. These subtasks are described as follows: Subtask A Given a question (short title + extended description) and a list of community answers, classify each of the answers as: Good, Potential or Bad (bad, dialog, non-English, other). Subtask B Given a YES/NO question (short title + extended description) and a list of community answers, decide whether the global answer to the question should be yes, no or unsure, based on the individ</context>
</contexts>
<marker>Mihalcea, Radev, 2011</marker>
<rawString>Rada Mihalcea and Dragomir Radev. 2011. Graphbased natural language processing and information retrieval. Cambridge; New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Pinto</author>
<author>Helena G´omez-Adorno</author>
<author>Darnes Vilari˜no</author>
<author>Vivek Kumar Singh</author>
</authors>
<title>A graph-based multi-level linguistic representation for document understanding.</title>
<date>2014</date>
<journal>Pattern Recognition Letters,</journal>
<volume>41</volume>
<issue>0</issue>
<marker>Pinto, G´omez-Adorno, Vilari˜no, Singh, 2014</marker>
<rawString>David Pinto, Helena G´omez-Adorno, Darnes Vilari˜no, and Vivek Kumar Singh. 2014. A graph-based multi-level linguistic representation for document understanding. Pattern Recognition Letters, 41(0):93 – 102. Supervised and Unsupervised Classification Techniques and their Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chirag Shah</author>
<author>Jefferey Pomerantz</author>
</authors>
<title>Evaluating and predicting answer quality in community qa.</title>
<date>2010</date>
<booktitle>In Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’10,</booktitle>
<pages>411--418</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2983" citStr="Shah and Pomerantz, 2010" startWordPosition="468" endWordPosition="471">o automate this process by predicting the quality of existing answers with respect to a question. There are few works in the literature on evaluating the quality of answers provided in CQA sites. Most of such works employ non-textual and temporal features in order to built classification models for predicting the best answer for a given question. In (Jeon et al., 2006), the authors extract 13 nontextual features from the Naver data set and build a maximum entropy classification model to predict the quality (three classes: Bad, Medium and Good) of a given answer. A similar approach is used in (Shah and Pomerantz, 2010), but extracting 21 features (mainly non-textual) from Yahoo! Answers; the authors employ a logistic regression and classification model to predict the best answer. Besides, a set of temporal features is proposed in (Cai and Chakravarthy, 2011) in order to predict the best answer for a given question. In this work the authors argue that the traditional classification approaches are not well suited for this problem because of the highly imbalanced ratio of the best answer and the non-best answers in their data set, so they propose to use learning to rank approaches. Unlike these approaches, we </context>
</contexts>
<marker>Shah, Pomerantz, 2010</marker>
<rawString>Chirag Shah and Jefferey Pomerantz. 2010. Evaluating and predicting answer quality in community qa. In Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’10, pages 411–418, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grigori Sidorov</author>
<author>Alexander F Gelbukh</author>
<author>Helena GmezAdorno</author>
<author>David Pinto</author>
</authors>
<title>Soft similarity and soft cosine measure: Similarity of features in vector space model.</title>
<date>2014</date>
<journal>Computaci´on y Sistemas,</journal>
<volume>18</volume>
<issue>3</issue>
<pages>504</pages>
<contexts>
<context position="3933" citStr="Sidorov et al., 2014" startWordPosition="620" endWordPosition="623">rgue that the traditional classification approaches are not well suited for this problem because of the highly imbalanced ratio of the best answer and the non-best answers in their data set, so they propose to use learning to rank approaches. Unlike these approaches, we use only textual information for predicting the quality of the answers. 18 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 18–22, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics Our approach is based on our previous research (Pinto et al., 2014) and (Sidorov et al., 2014), where we propose the graph-based representation model (Integrated Syntactic Graph) and the soft similarity measure (soft cosine measure). Our experimental results are promising, they overcome the baseline system for this challenge. The rest of the paper is organized as follows. Section 2 describes our approach. Section 3 presents the configuration of the submitted runs and the evaluation results. Finally, Section 4 presents the conclusions and outlines some directions of future work. 2 Approach For many problems in natural language processing, graph structure is an intuitive, natural and dir</context>
</contexts>
<marker>Sidorov, Gelbukh, GmezAdorno, Pinto, 2014</marker>
<rawString>Grigori Sidorov, Alexander F. Gelbukh, Helena GmezAdorno, and David Pinto. 2014. Soft similarity and soft cosine measure: Similarity of features in vector space model. Computaci´on y Sistemas, 18(3):491 – 504.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>