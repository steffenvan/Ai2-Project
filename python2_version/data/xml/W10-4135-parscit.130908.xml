<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011146">
<title confidence="0.979399">
High OOV-Recall Chinese Word Segmenter
</title>
<author confidence="0.998996">
Xiaoming Xu, Muhua Zhu, Xiaoxu Fei, and Jingbo Zhu
</author>
<affiliation confidence="0.995911333333333">
School of
Information Science and Engineering
Northeastern University
</affiliation>
<email confidence="0.9839115">
{xuxm, zhumh, feixx}@ics.neu.edu.cn
zhujingbo@mail.neu.edu.cn
</email>
<sectionHeader confidence="0.995535" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999761">
For the competition of Chinese word seg-
mentation held in the first CIPS-SIGHNA
joint conference. We applied a subword-
based word segmenter using CRFs and ex-
tended the segmenter with OOV words
recognized by Accessor Variety. More-
over, we proposed several post-processing
rules to improve the performance. Our
system achieved promising OOV recall
among all the participants.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999789652173913">
Chinese word segmentation is deemed to be a pre-
requisite for Chinese language processing. The
competition in the first CIPS-SIGHAN joint con-
ference put the task of Chinese word segmenta-
tion in a more challengeable setting, where train-
ing and test data are obtained from different do-
mains. This setting is widely known as domain
adaptation.
For domain adaptation, either a large-scale un-
labeled target domain data or a small size of la-
beled target domain data is required to adapt a
system built on source domain data to the tar-
get domain. In this word segmentation competi-
tion, unfortunately, only a small size of unlabeled
target domain data is available. Thus we focus
on handling out-of-vocabulary (OOV) words. For
this purpose, our system is based on a combina-
tion of subword-based tagging method (Zhang et
al., 2006) and accessor variety-based new word
recognition method (Feng et al., 2004). In more
detail, we adopted and extended subword-based
method. Subword list is augmented with new-
word list recognized by accessor variety method.
</bodyText>
<tableCaption confidence="0.97124">
Table 1: Basic Features for CRF-based Segmenter
</tableCaption>
<bodyText confidence="0.9998162">
We participated in the close track of the word
segmentation competition, on all the four test
datasets, in two of which our system is ranked at
the 1st position with respect to the metric of OOV
recall.
</bodyText>
<sectionHeader confidence="0.995625" genericHeader="introduction">
2 System Description
</sectionHeader>
<subsectionHeader confidence="0.997677">
2.1 Subword-based Tagging with CRFs
</subsectionHeader>
<bodyText confidence="0.998240941176471">
The backbone of our system is a character-based
segmenter with the application of Conditional
Random Fields (CRFs) (Zhao and Kit, 2008). In
detail, we apply a six-tag tagging scheme, as in
(Zhao et al., 2006). That is , each Chinese char-
acter can be assigned to one of the tags in {B,
B2, B3, M, E, S }. Refer to (Zhao et al., 2006)
for detailed meaning of the tags. Table 1 shows
basic feature templates used in our system, where
feature templates a, b, d, e are also used in (Zhu et
al., 2006) for SVM-based word segmentation.
In order to extend basic CRF-based segmenter,
we first collect 2k most frequent words from train-
ing data. Hereafter, the list of such words is
referred to as subword list. Moreover, single-
character words 1, if they are not contained in
the subword list, are also added. Such proce-
</bodyText>
<footnote confidence="0.9518995">
1By single-character word, we refer to words that consist
solely of a Chinese character.
</footnote>
<equation confidence="0.928014857142857">
Feature Template
Description
a) cn(−2, −1, 0, 1, 2)
b) cncn+1(−2, −1, 0, 1)
c) cn−1cncn+1(−1, 0, 1)
d) Pu(C0)
e) T (C−1)T (C0)T (C+1)
</equation>
<bodyText confidence="0.991212423076923">
unigram of characters
bigram of characters
trigram of characters
whether punctuation
type of characters
Table 2: Subword Features for CRF-based Seg-
menter
dure for constructing a subword list is similar to
the one used in (Zhang et al., 2006). To en-
hance the effect of subwords, we go one step
further to build a list, named confident-word list
here and below, which contains words that are
not a portion of other words and are never seg-
mented in the training data. In the competition,
400 most frequent words in the confident-word list
are used. With subword list and confident-word
list, both training and test data are segmented
with forward maximum match method by using
the union of subword list and confident-word list.
Each segmentation unit (single-character or multi-
character unit) in the segmentation results are re-
garded as “pseudo character” and thus can be rep-
resented with the basic features in Table 1 and
two additional features as shown in Table 2. See
the details of subword-based Chinese word seg-
mentation in (Zhang et al., 2006)
</bodyText>
<subsectionHeader confidence="0.968294">
2.2 OOV Recognition with Accessor Variety
</subsectionHeader>
<bodyText confidence="0.997736833333333">
Accessor variety (AV) (Feng et al., 2004) is a sim-
ple and effective unsupervised method for extrac-
tion of new Chinese words. Given a unsegmented
text, each substring (candidate word) in the text
can be assigned a value according to the follow-
ing equation:
</bodyText>
<equation confidence="0.979533">
AV (s) = min{Lav(s),Rav(s)} (1)
</equation>
<bodyText confidence="0.999293736842105">
where the left and right AV values, Lav(s) and
Rav(s) are defined to be the number of distinct
character types appearing on the left and right,
respectively. Candidate words are sorted in the
descending order of AV values and most highly
ranked ones can be chosen as new words. In
practical applications, heuristic filtering rules are
generally needed (Feng et al., 2004). We re-
implemented the AV method and filtering rules,
as in (Feng et al., 2004). Moreover, we filter out
candidate words that have AV values less than 3.
Unfortunately, candidate word list generated this
way still contains many noisy words (substrings
that are not words). One possible reason is that
unlabeled data (test data) used in the competition
is extremely small in size. In order to refine the
results derived from the AV method, we make use
of the training data to filter the results from two
different perspectives.
</bodyText>
<listItem confidence="0.973766357142857">
• Segment test data with the CRF-based seg-
menter described above. Then we collect
(candidate) words that are in the CRF-based
segmentation results, but not appear in the
training data. Such words are called CRF-
OOV words hereafter. We retain the intersec-
tion of CRF-OOV words and AV-based re-
sults as the set of candidate words to be pro-
cessed by the following step.
• Any candidate word in the intersection of
CRF-based and AV-based results will be fil-
tered out if they satisfy one of the following
conditions: 1) the candidate word is a part of
some word in the training data; 2) the candi-
</listItem>
<bodyText confidence="0.972377333333333">
date word is formed by connection of consec-
utive words in the training data; 3) the candi-
date word contains position words, such as
上 (up), 下 (down), 左 (left), 右 (right), etc.
Moreover, we take all English words in test data
as OOV words. A simple heuristic rule is defined
for the purpose of English word recognition: an
English word is a consecutive sequence of English
characters and punctuations between two English
characters (including these two characters).
We finally add all the OOV words into subword
list and confident-word list.
</bodyText>
<sectionHeader confidence="0.993883" genericHeader="method">
3 Post-Processing Rules
</sectionHeader>
<bodyText confidence="0.999949166666667">
In the results of subword-based word segmenta-
tion with CRFs, we found some errors could be
corrected with heuristic rules. For this purpose,
we propose following post-processing rules, for
handling OOV and in-vocabulary (IV) words, re-
spectively.
</bodyText>
<sectionHeader confidence="0.743111" genericHeader="method">
3.1 OOV Rules
3.1.1 Annotation-Standard Independent
Rules
</sectionHeader>
<bodyText confidence="0.824742666666667">
We assume the phenomena discussed in the fol-
lowing are general across all kinds of annotation
Feature Template
</bodyText>
<equation confidence="0.869078666666667">
Description
f) in(str, subword-list)
is str in subword list
g) in(str, confident-word-list)
is str in confident-word
list
</equation>
<bodyText confidence="0.973836">
standards. Thus corresponding rules can be ap-
plied without considering annotation standards of
training data.
</bodyText>
<listItem confidence="0.881611">
• A punctuation tends to be a single-character
</listItem>
<bodyText confidence="0.6486636">
word. If a punctation’s previous character
and next character are both Chinese charac-
ters, i.e. not punctuation, digit, or English
character, we always regard the punctuation
as a word.
</bodyText>
<listItem confidence="0.942989045454546">
• Consecutive and identical punctuations tend
to be joined together as a word. For exam-
ple, “—” represents a Chinese hyphen which
consists of three “-”, and “!!!” is used to
show emphasizing. Inspired by this obser-
vations, we would like to unite consecutive
and identical punctuations as a single word.
• When the character “·” appears in the train-
ing data, it is generally used as a connec-
tions symbol in a foreign person name, such
as “圣·约翰 (Saint John)”. Taking this ob-
servation into consideration, we always unite
the character “·” and its previous and next
segment units into a single word. A similar
rule is designed to unite consecutive digits on
the sides of the symbol “.”, ex. “1.11”.
• We notice that four consecutive characters
which are in the pattern of AABB generally
form a single word in Chinese, for example
”平平淡淡 (dull)”. Taking this observation
into account, we always unite consecutive
characters in the AABB into a single word.
</listItem>
<subsectionHeader confidence="0.939057">
3.1.2 Templates with Generalized Digits
</subsectionHeader>
<bodyText confidence="0.999983470588235">
Words containing digits generally belong to a
open class, for example, the word “2012* (AD
2012)” means a date. Thus CRF-based seg-
menter has difficulties in recognizing such words
since they are frequently OOV words. To attack
this challenge, we first generalize digits in the
training data. In detail, we replaced consecutive
digits with ”*”. For example, the word “2012*”
will be transformed into “**”. Second, we col-
lect word templates which consist of three con-
secutive words on condition that at least one of
the words in a template contains the character “*”
and that the template appears in the training data
more than 4 times. For example, we can get a
template like “*月(month) *日(day) 电(publish)”.
With such templates, we are able to correct errors,
say “10月 17日电” into “10月 17日 电”.
</bodyText>
<subsectionHeader confidence="0.99781">
3.2 IV Rules
</subsectionHeader>
<bodyText confidence="0.999970131578947">
We notice that long words have less ambiguity
than short words in the sense of being words.
For example, characters in “.-A, t a a (full
of talents)” always form a word in the training
data, whereas “.-A,t” have two plausible split-
ting forms, as “.-A,t (talent)” or “.-A, (people) t
(only)”. In our system, we collect words that have
at least four characters and filter out words which
belong to one of following cases: 1) the word is
a part of other words; 2) the word consists solely
of punctation and/or digit. For example, “唯*
3.-V- (materialism)” and “—百=十 (120)” are
discarded, since the former is a substring of the
word “唯*3.-V-者 (materialist)” and the latter is
a word of digits. Finally we get a list containing
about 6k words. If a character sequence in the test
data is a member in the list, it is retained as a word
in the final segmentation results.
Another group of IV rules concern character
sequences that have unique splitting in the train-
ing data. For example, “女.-A,417 (women)” is al-
ways split as “女.-A, (woman) 417 (s)”. Hereafter,
we refer to such character sequences as unique-
split-sequence (USS). In our system, we are con-
cerned with UUSs which are composed of less
than 5 words. In order to apply UUSs for post-
processing, we first collect word sequence of vari-
able length (word number) from training data. In
detail, we collect word sequences of two words,
three words, and four words. Second, word se-
quences that have more than one splitting cases
in the training data are filtered out. Third, spaces
between words are removed to form USSs. For
example, the words “女.-A, (woman) 417 (s)” will
form the USS “女.-A,417 ”. Finally, we search the
test data for each USS. If the searching succeeds,
the USS will be replaced with the corresponding
word sequence.
</bodyText>
<sectionHeader confidence="0.998133" genericHeader="evaluation">
4 Evaluation Results
</sectionHeader>
<bodyText confidence="0.992472">
We evaluated our Chinese word segmenter in the
close track, in four domain: literature (Lit), com-
</bodyText>
<table confidence="0.9998915">
Domain Basic +OOV +OOV+IV
ROV RzV F ROV RzV F ROV RzV F
Lit .643 .946 .927 .652 .947 .929 .648 .952 .934
Com .839 .961 .938 .850 .961 .941 .852 .965 .947
Med .725 .938 .912 .754 .939 .917 .756 .944 .923
Fin .761 .956 .932 .854 .958 .950 .871 .961 .955
</table>
<tableCaption confidence="0.999612">
Table 3: Effectiveness of post-processing rules
</tableCaption>
<bodyText confidence="0.9985338">
puter (Com), medicine (Med) and finance (Fin).
The results are depicted in Table 4, where R,
P and F refer to Recall, Precision, F measure
respectively, and ROOV and RIV refer to recall
of OOV and IV words respectively. Since OOV
words are the obstacle for practical Chinese word
segmenters to achieve high accuracy, we have spe-
cial interest in the metric of OOV recall. We
found that our system achieved high OOV recall
2. Actually, OOV recall of our system in the do-
mains of computer and finance are both ranked at
the 1st position among all the participants. Com-
pared with the systems ranked second in these
two domains, our system achieved OOV recall
.853 vs. .827 and .871 vs. .857 respectively.
We also examined the effectiveness of post-
processing rules, as shown in Table 3, where
Basic represents the performance achieved be-
fore post-processing, +OOV represents the results
achieved after applying OOV post-processing
rules, and +OOV+IV denotes the results achieved
after using all the post-processing rules, including
both OOV and IV rules. As the table shows, de-
signed post-processing rules can improve both IV
and OOV recall significantly.
</bodyText>
<table confidence="0.9982296">
Domain R P F ROOV RzV
Lit .931 .936 .934 .648 .952
Com .948 .945 .947 .853 .965
Med .924 .922 .923 .756 .944
Fin .953 .956 .955 .871 .961
</table>
<tableCaption confidence="0.9255505">
Table 4: Performance of our system in the compe-
tition
</tableCaption>
<footnote confidence="0.826644">
2For the test data from the domain of literature, we actu-
ally use combination of our system and forward maximum
match, so we will omit the results on this test dataset in our
discussion.
</footnote>
<sectionHeader confidence="0.996701" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999964777777778">
We proposed an approach to refine new words rec-
ognized with the accessor variety method, and in-
corporated such words into a subword-based word
segmenter. We found that such method could
achieve high OOV recall. Moreover, we designed
effective post-processing rules to further enhance
the performance of our systems. Our system fi-
nally achieved satisfactory results in the competi-
tion.
</bodyText>
<sectionHeader confidence="0.998221" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9763995">
This work was supported in part by the National
Science Foundation of China (60873091).
</bodyText>
<sectionHeader confidence="0.998663" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999691260869565">
Feng, Haodi, Kang Chen, Xiaotie Deng, and Weimin
zhang. 2004. Accessor Variety Criteriafor Chinese
Word Extraction. Computational Linguistics 2004,
30(1), pages 75-93.
Zhang, Ruiqiang, Genichiro Kikui, and Eiichiro
Sumita. 2006. Subword-based Tagging by Condi-
tional Random Fileds for Chinese Word Segmenta-
tion. In Proceedings of HLT-NAACL 2006, pages
193-196.
Zhao, Hai, Chang-Ning Huang, and Mu Li. 2006.
Improved Chinese Word Segmentation System with
Conditional Random Field. In Proceedings of
SIGHAN-5 2006, pages 162-165.
Zhao, Hai and Chunyu Kit. 2008. Unsupervised Seg-
mentation Helps Supervised Learning of Character
Tagging for Word Segmentation and Named Entity
Recognition. In Proceedings of SIGHAN-6 2008,
pages 106-111.
Zhu, Muhua, Yiling Wang, Zhenxing Wang, Huizhen
Wang, and Jingbo Zhu. 2006. Designing Spe-
cial Post-Processing Rules for SVM-based Chinese
Word Segmentation. In Proceedigns of SIGHAN-5
2006, pages 217-220.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.155773">
<title confidence="0.999858">High OOV-Recall Chinese Word Segmenter</title>
<author confidence="0.771339">Xiaoming Xu</author>
<author confidence="0.771339">Muhua Zhu</author>
<author confidence="0.771339">Xiaoxu Fei</author>
<author confidence="0.771339">Jingbo</author>
<affiliation confidence="0.680221333333333">School Information Science and Northeastern</affiliation>
<email confidence="0.7033195">zhumh,zhujingbo@mail.neu.edu.cn</email>
<abstract confidence="0.999252727272727">For the competition of Chinese word segmentation held in the first CIPS-SIGHNA joint conference. We applied a subwordbased word segmenter using CRFs and extended the segmenter with OOV words recognized by Accessor Variety. Moreover, we proposed several post-processing rules to improve the performance. Our system achieved promising OOV recall among all the participants.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Haodi Feng</author>
<author>Kang Chen</author>
<author>Xiaotie Deng</author>
<author>Weimin zhang</author>
</authors>
<title>Accessor Variety Criteriafor Chinese Word Extraction. Computational Linguistics</title>
<date>2004</date>
<volume>30</volume>
<issue>1</issue>
<pages>75--93</pages>
<contexts>
<context position="1514" citStr="Feng et al., 2004" startWordPosition="231" endWordPosition="234">t domains. This setting is widely known as domain adaptation. For domain adaptation, either a large-scale unlabeled target domain data or a small size of labeled target domain data is required to adapt a system built on source domain data to the target domain. In this word segmentation competition, unfortunately, only a small size of unlabeled target domain data is available. Thus we focus on handling out-of-vocabulary (OOV) words. For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004). In more detail, we adopted and extended subword-based method. Subword list is augmented with newword list recognized by accessor variety method. Table 1: Basic Features for CRF-based Segmenter We participated in the close track of the word segmentation competition, on all the four test datasets, in two of which our system is ranked at the 1st position with respect to the metric of OOV recall. 2 System Description 2.1 Subword-based Tagging with CRFs The backbone of our system is a character-based segmenter with the application of Conditional Random Fields (CRFs) (Zhao and Kit, 2008). In detai</context>
<context position="4132" citStr="Feng et al., 2004" startWordPosition="681" endWordPosition="684">he confident-word list are used. With subword list and confident-word list, both training and test data are segmented with forward maximum match method by using the union of subword list and confident-word list. Each segmentation unit (single-character or multicharacter unit) in the segmentation results are regarded as “pseudo character” and thus can be represented with the basic features in Table 1 and two additional features as shown in Table 2. See the details of subword-based Chinese word segmentation in (Zhang et al., 2006) 2.2 OOV Recognition with Accessor Variety Accessor variety (AV) (Feng et al., 2004) is a simple and effective unsupervised method for extraction of new Chinese words. Given a unsegmented text, each substring (candidate word) in the text can be assigned a value according to the following equation: AV (s) = min{Lav(s),Rav(s)} (1) where the left and right AV values, Lav(s) and Rav(s) are defined to be the number of distinct character types appearing on the left and right, respectively. Candidate words are sorted in the descending order of AV values and most highly ranked ones can be chosen as new words. In practical applications, heuristic filtering rules are generally needed (</context>
</contexts>
<marker>Feng, Chen, Deng, zhang, 2004</marker>
<rawString>Feng, Haodi, Kang Chen, Xiaotie Deng, and Weimin zhang. 2004. Accessor Variety Criteriafor Chinese Word Extraction. Computational Linguistics 2004, 30(1), pages 75-93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruiqiang Zhang</author>
<author>Genichiro Kikui</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Subword-based Tagging by Conditional Random Fileds for Chinese Word Segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>193--196</pages>
<contexts>
<context position="1439" citStr="Zhang et al., 2006" startWordPosition="220" endWordPosition="223">allengeable setting, where training and test data are obtained from different domains. This setting is widely known as domain adaptation. For domain adaptation, either a large-scale unlabeled target domain data or a small size of labeled target domain data is required to adapt a system built on source domain data to the target domain. In this word segmentation competition, unfortunately, only a small size of unlabeled target domain data is available. Thus we focus on handling out-of-vocabulary (OOV) words. For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004). In more detail, we adopted and extended subword-based method. Subword list is augmented with newword list recognized by accessor variety method. Table 1: Basic Features for CRF-based Segmenter We participated in the close track of the word segmentation competition, on all the four test datasets, in two of which our system is ranked at the 1st position with respect to the metric of OOV recall. 2 System Description 2.1 Subword-based Tagging with CRFs The backbone of our system is a character-based segmenter with the app</context>
<context position="3242" citStr="Zhang et al., 2006" startWordPosition="531" endWordPosition="534"> of such words is referred to as subword list. Moreover, singlecharacter words 1, if they are not contained in the subword list, are also added. Such proce1By single-character word, we refer to words that consist solely of a Chinese character. Feature Template Description a) cn(−2, −1, 0, 1, 2) b) cncn+1(−2, −1, 0, 1) c) cn−1cncn+1(−1, 0, 1) d) Pu(C0) e) T (C−1)T (C0)T (C+1) unigram of characters bigram of characters trigram of characters whether punctuation type of characters Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006). To enhance the effect of subwords, we go one step further to build a list, named confident-word list here and below, which contains words that are not a portion of other words and are never segmented in the training data. In the competition, 400 most frequent words in the confident-word list are used. With subword list and confident-word list, both training and test data are segmented with forward maximum match method by using the union of subword list and confident-word list. Each segmentation unit (single-character or multicharacter unit) in the segmentation results are regarded as “pseudo</context>
</contexts>
<marker>Zhang, Kikui, Sumita, 2006</marker>
<rawString>Zhang, Ruiqiang, Genichiro Kikui, and Eiichiro Sumita. 2006. Subword-based Tagging by Conditional Random Fileds for Chinese Word Segmentation. In Proceedings of HLT-NAACL 2006, pages 193-196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chang-Ning Huang</author>
<author>Mu Li</author>
</authors>
<title>Improved Chinese Word Segmentation System with Conditional Random Field.</title>
<date>2006</date>
<booktitle>In Proceedings of SIGHAN-5</booktitle>
<pages>162--165</pages>
<contexts>
<context position="2177" citStr="Zhao et al., 2006" startWordPosition="340" endWordPosition="343">word-based method. Subword list is augmented with newword list recognized by accessor variety method. Table 1: Basic Features for CRF-based Segmenter We participated in the close track of the word segmentation competition, on all the four test datasets, in two of which our system is ranked at the 1st position with respect to the metric of OOV recall. 2 System Description 2.1 Subword-based Tagging with CRFs The backbone of our system is a character-based segmenter with the application of Conditional Random Fields (CRFs) (Zhao and Kit, 2008). In detail, we apply a six-tag tagging scheme, as in (Zhao et al., 2006). That is , each Chinese character can be assigned to one of the tags in {B, B2, B3, M, E, S }. Refer to (Zhao et al., 2006) for detailed meaning of the tags. Table 1 shows basic feature templates used in our system, where feature templates a, b, d, e are also used in (Zhu et al., 2006) for SVM-based word segmentation. In order to extend basic CRF-based segmenter, we first collect 2k most frequent words from training data. Hereafter, the list of such words is referred to as subword list. Moreover, singlecharacter words 1, if they are not contained in the subword list, are also added. Such proc</context>
</contexts>
<marker>Zhao, Huang, Li, 2006</marker>
<rawString>Zhao, Hai, Chang-Ning Huang, and Mu Li. 2006. Improved Chinese Word Segmentation System with Conditional Random Field. In Proceedings of SIGHAN-5 2006, pages 162-165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Unsupervised Segmentation Helps Supervised Learning of Character Tagging for Word Segmentation and Named Entity Recognition.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGHAN-6</booktitle>
<pages>106--111</pages>
<contexts>
<context position="2104" citStr="Zhao and Kit, 2008" startWordPosition="326" endWordPosition="329">on method (Feng et al., 2004). In more detail, we adopted and extended subword-based method. Subword list is augmented with newword list recognized by accessor variety method. Table 1: Basic Features for CRF-based Segmenter We participated in the close track of the word segmentation competition, on all the four test datasets, in two of which our system is ranked at the 1st position with respect to the metric of OOV recall. 2 System Description 2.1 Subword-based Tagging with CRFs The backbone of our system is a character-based segmenter with the application of Conditional Random Fields (CRFs) (Zhao and Kit, 2008). In detail, we apply a six-tag tagging scheme, as in (Zhao et al., 2006). That is , each Chinese character can be assigned to one of the tags in {B, B2, B3, M, E, S }. Refer to (Zhao et al., 2006) for detailed meaning of the tags. Table 1 shows basic feature templates used in our system, where feature templates a, b, d, e are also used in (Zhu et al., 2006) for SVM-based word segmentation. In order to extend basic CRF-based segmenter, we first collect 2k most frequent words from training data. Hereafter, the list of such words is referred to as subword list. Moreover, singlecharacter words 1,</context>
</contexts>
<marker>Zhao, Kit, 2008</marker>
<rawString>Zhao, Hai and Chunyu Kit. 2008. Unsupervised Segmentation Helps Supervised Learning of Character Tagging for Word Segmentation and Named Entity Recognition. In Proceedings of SIGHAN-6 2008, pages 106-111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Muhua Zhu</author>
<author>Yiling Wang</author>
<author>Zhenxing Wang</author>
<author>Huizhen Wang</author>
<author>Jingbo Zhu</author>
</authors>
<title>Designing Special Post-Processing Rules for SVM-based Chinese Word Segmentation.</title>
<date>2006</date>
<booktitle>In Proceedigns of SIGHAN-5</booktitle>
<pages>217--220</pages>
<contexts>
<context position="2464" citStr="Zhu et al., 2006" startWordPosition="400" endWordPosition="403">ed at the 1st position with respect to the metric of OOV recall. 2 System Description 2.1 Subword-based Tagging with CRFs The backbone of our system is a character-based segmenter with the application of Conditional Random Fields (CRFs) (Zhao and Kit, 2008). In detail, we apply a six-tag tagging scheme, as in (Zhao et al., 2006). That is , each Chinese character can be assigned to one of the tags in {B, B2, B3, M, E, S }. Refer to (Zhao et al., 2006) for detailed meaning of the tags. Table 1 shows basic feature templates used in our system, where feature templates a, b, d, e are also used in (Zhu et al., 2006) for SVM-based word segmentation. In order to extend basic CRF-based segmenter, we first collect 2k most frequent words from training data. Hereafter, the list of such words is referred to as subword list. Moreover, singlecharacter words 1, if they are not contained in the subword list, are also added. Such proce1By single-character word, we refer to words that consist solely of a Chinese character. Feature Template Description a) cn(−2, −1, 0, 1, 2) b) cncn+1(−2, −1, 0, 1) c) cn−1cncn+1(−1, 0, 1) d) Pu(C0) e) T (C−1)T (C0)T (C+1) unigram of characters bigram of characters trigram of character</context>
</contexts>
<marker>Zhu, Wang, Wang, Wang, Zhu, 2006</marker>
<rawString>Zhu, Muhua, Yiling Wang, Zhenxing Wang, Huizhen Wang, and Jingbo Zhu. 2006. Designing Special Post-Processing Rules for SVM-based Chinese Word Segmentation. In Proceedigns of SIGHAN-5 2006, pages 217-220.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>