<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.997916666666667">
A quantitative evaluation of naturalistic models of language acquisition; the
efficiency of the Triggering Learning Algorithm compared to a Categorial
Grammar Learner
</title>
<author confidence="0.842922">
Paula Buttery
</author>
<affiliation confidence="0.763304">
Natural Language and Information Processing Group,
Computer Laboratory, Cambridge University,
</affiliation>
<address confidence="0.936775">
15 JJ Thomson Avenue, Cambridge, CB3 0FD, UK
</address>
<email confidence="0.997603">
paula.buttery@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.997369" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999912826086957">
Naturalistic theories of language acquisition assume
learners to be endowed with some innate language
knowledge. The purpose of this innate knowledge
is to facilitate language acquisition by constrain-
ing a learner’s hypothesis space. This paper dis-
cusses a naturalistic learning system (a Categorial
Grammar Learner (CGL)) that differs from previous
learners (such as the Triggering Learning Algorithm
(TLA) (Gibson and Wexler, 1994)) by employing a
dynamic definition of the hypothesis-space which
is driven by the Bayesian Incremental Parameter
Setting algorithm (Briscoe, 1999). We compare
the efficiency of the TLA with the CGL when ac-
quiring an independently and identically distributed
English-like language in noiseless conditions. We
show that when convergence to the target gram-
mar occurs (which is not guaranteed), the expected
number of steps to convergence for the TLA is
shorter than that for the CGL initialized with uni-
form priors. However, the CGL converges more
reliably than the TLA. We discuss the trade-off of
efficiency against more reliable convergence to the
target grammar.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.988196203389831">
A normal child acquires the language of her envi-
ronment without any specific training. Chomsky
(1965) claims that, given the “relatively slight ex-
posure” to examples and “remarkable complexity”
of language, it would be “an extraordinary intellec-
tual achievement” for a child to acquire a language
if not specifically designed to do so. His Argument
from the Poverty of the Stimulus suggests that if we
know X, and X is undetermined by learning expe-
rience then X must be innate. For an example con-
sider structure dependency in language syntax:
A question in English can be formed by invert-
ing the auxiliary verb and subject noun-phrase: (1a)
“Dinah was drinking a saucer of milk”; (1b) “was
Dinah drinking a saucer of milk?”
Upon exposure to this example, a child could hy-
pothesize infinitely many question-formation rules,
such as: (i) swap the first and second words in the
sentence; (ii) front the first auxiliary verb; (iii) front
words beginning with w.
The first two of these rules are refuted if the child
encounters the following: (2a) “the cat who was
grinning at Alice was disappearing”; (2b) “was the
cat who was grinning at Alice disappearing?”
If a child is to converge upon the correct hypoth-
esis unaided she must be exposed to sufficient ex-
amples so that all false hypotheses are refuted. Un-
fortunately such examples are not readily available
in child-directed speech; even the constructions in
examples (2a) and (2b) are rare (Legate, 1999). To
compensate for this lack of data Chomsky suggests
that some principles of language are already avail-
able in the child’s mind. For example, if the child
had innately “known” that all grammar rules are
structurally-dependent upon syntax she would never
have hypothesized rules (i) and (iii). Thus, Chom-
sky theorizes that a human mind contains a Univer-
sal Grammar which defines a hypothesis-space of
“legal” grammars.&apos; This hypothesis-space must be
both large enough to contain grammar’s for all of
the world’s languages and small enough to ensure
successful acquisition given the sparsity of data.
Language acquisition is the process of searching the
hypothesis-space for the grammar that most closely
describes the language of the environment. With
estimates of the number of living languages being
around 6800 (Ethnologue, 2004) it is not sensible to
model the hypothesis-space of grammars explicitly,
rather it must be modeled parametrically. Language
acquisition is then the process of setting these pa-
rameters. Chomsky (1981) suggested that param-
eters should represent points of variation between
languages, however the only requirement for pa-
rameters is that they define the current hypothesis-
space.
&apos;Discussion of structural dependence as evidence of the Ar-
gument from the Poverty of Stimulus is illustrative, the sig-
nificance being that innate knowledge in any form will place
constraints on the hypothesis-space
</bodyText>
<page confidence="0.968177">
1
</page>
<bodyText confidence="0.99648675">
The properties of the parameters used by this
learner (the CGL) are as follows: (1) Parameters are
lexical; (2) Parameters are inheritance based; (3) Pa-
rameter setting is statistical.
</bodyText>
<subsectionHeader confidence="0.440907">
1 - Lexical Parameters
</subsectionHeader>
<bodyText confidence="0.99891275">
The CGL employs parameter setting as a means
to acquire a lexicon; differing from other paramet-
ric learners, (such as the Triggering Learning Al-
gorithm (TLA) (Gibson and Wexler, 1994) and the
Structural Triggers Learner (STL) (Fodor, 1998b),
(Sakas and Fodor, 2001)) which acquire general
syntactic information rather than the syntactic prop-
erties associated with individual words.2
In particular, a categorial grammar is acquired.
The syntactic properties of a word are contained in
its lexical entry in the form of a syntactic category.
A word that may be used in multiple syntactic situ-
ations (or sub-categorization frames) will have mul-
tiple entries in the lexicon.
Syntactic categories are constructed from a finite
set of primitive categories combined with two op-
erators (/ and \) and are defined by their members
ability to combine with other constituents; thus con-
stituents may be thought of as either functions or
arguments.
The arguments of a functional constituent are
shown to the right of the operators and the result
to the left. The forward slash operator (/) indicates
that the argument must appear to the right of the
function and a backward slash (\) indicates that it
must appear on the left. Consider the following
CFG structure which describes the properties of a
transitive verb:
</bodyText>
<subsectionHeader confidence="0.3643495">
s —&gt; np vp
vp —&gt; tv np
</subsectionHeader>
<bodyText confidence="0.9565935">
tv —&gt; gets, finds, ...
Assume that there is a set of primitive categories
{s, np}. A vp must be in the category of func-
tional constituents that takes a np from the left and
returns an s. This can be written s\np. Likewise
a tv takes an np from the right and returns a vp
(whose type we already know). A tv may be writ-
ten (s\np)/np.
</bodyText>
<subsectionHeader confidence="0.664119">
Rules may be used to combine categories. We
</subsectionHeader>
<bodyText confidence="0.99859275">
assume that our learner is innately endowed with the
rules of function application, function composition
and generalized weak permutation (Briscoe, 1999)
(see figures 1 and 2).
</bodyText>
<listItem confidence="0.9950365">
• Forward Application (&gt;)
X/Y Y —&gt; X
</listItem>
<footnote confidence="0.57067">
2The concept of lexical parameters and the lexical-linking
of parameters is to be attributed to Borer (1984).
</footnote>
<listItem confidence="0.96524425">
• Backward Application (&lt;)
Y X\Y —&gt; X
• Forward Composition (&gt; B)
X/Y Y/Z —&gt; X/Z
• Backward Composition (&lt; B)
Y \X Z\Y —&gt; X\Z
• Generalized Weak Permutation (P)
((X  |Yl)...  |Yn) —&gt; ((X  |Yn)...  |Yl)
</listItem>
<bodyText confidence="0.565182">
where  |is a variable over \ and /.
</bodyText>
<equation confidence="0.64378">
may eat
(s\np)/(s\np) (s\np)/np
&gt;
s\np
&lt;
s
</equation>
<figureCaption confidence="0.9965145">
Figure 1: Illustration of forward/backward applica-
tion (&gt;, &lt;) and forward composition (&gt; B)
</figureCaption>
<figure confidence="0.9988619">
saw
she (s\np)/np P
np (s/np)\np
&lt;
rabbit (n\n)/(s/np)
n\n
&lt;
n
&gt;
np
</figure>
<figureCaption confidence="0.9784475">
Figure 2: Illustration of generalized weak permuta-
tion (P)
</figureCaption>
<bodyText confidence="0.997707214285714">
The lexicon for a language will contain a finite
subset of all possible syntactic categories, the size of
which depends on the language. Steedman (2000)
suggests that for English the lexical functional cate-
gories never need more than five arguments and that
these are needed only in a limited number of cases
such as for the verb bet in the sentence I bet you five
pounds for England to win.
The categorial grammar parameters of the CGL
are concerned with defining the set of syntactic
categories present in the language of the environ-
ment. Converging on the correct set aids acquisition
by constraining the learner’s hypothesized syntactic
categories for an unknown word. A parameter (with
</bodyText>
<figure confidence="0.9988355">
Alice (s\np)/np
the cake
···
np
&gt; B
np
that
(s/np)
&gt;
the
n
np/n
</figure>
<page confidence="0.995763">
2
</page>
<bodyText confidence="0.9999382">
value of either ACTIVE or INACTIVE) is associ-
ated with every possible syntactic category to indi-
cate whether the learner considers the category to be
part of the target grammar.
Some previous parametric learners (TLA and
STL) have been primarily concerned with overall
syntactic phenomena rather than the syntactic prop-
erties of individual words. Movement parameters
(such as the V 2 parameter of the TLA) may be cap-
tured by the CGL using innate rules or multiple lex-
ical entries. For instance, Dutch and German word
order is captured by assuming that verbs in these
languages systematically have two categories, one
determining main clause order and the other subor-
dinate clause orders.
</bodyText>
<sectionHeader confidence="0.882456" genericHeader="method">
2 - Inheritance Based Parameters
</sectionHeader>
<bodyText confidence="0.999905466666667">
The complex syntactic categories of a categorial
grammar are a sub-categorization of simpler cate-
gories; consequently categories may be arranged in
a hierarchy with more complex categories inheriting
from simpler ones. Figure 3 shows a fragment of a
possible hierarchy. This hierarchical organization of
parameters provides the learner with several bene-
fits: (1) The hierarchy can enforce an order on learn-
ing; constraints may be imposed such that a parent
parameter must be acquired before a child parame-
ter (for example, in Figure 3, the learner must ac-
quire intransitive verbs before transitive verbs may
be hypothesized). (2) Parameter values may be in-
herited as a method of acquisition. (3) The parame-
ters are stored efficiently.
</bodyText>
<equation confidence="0.9500418">
s - ACTIVE
������ ������
s/s s\np - ACTIVE
����� �����
[s\np]/np -ACTIVE [s\np]/[s\np]
</equation>
<figureCaption confidence="0.739711666666667">
Figure 3: Partial hierarchy of syntactic categories.
Each category is associated with a parameter indi-
cating either ACTIVE or INACTIVE status.
</figureCaption>
<sectionHeader confidence="0.878172" genericHeader="method">
3 - Statistical Parameter Setting
</sectionHeader>
<bodyText confidence="0.999968857142857">
The learner uses a statistical method to track rela-
tive frequencies of parameter-setting-utterances in
the input.3 We use the Bayesian Incremental Pa-
rameter Setting (BIPS) algorithm (Briscoe, 1999)
to set the categorial parameters. Such an approach
sets the parameters to the values that are most likely
given all the accumulated evidence. This represents
</bodyText>
<subsectionHeader confidence="0.9541065">
3Other statistical parameter setting models include Yang’s
Variational model (2002) and the Guessing STL (Fodor, 1998a)
</subsectionHeader>
<bodyText confidence="0.978103285714286">
a compromise between two extremes: implementa-
tions of the TLA are memoryless allowing a param-
eter values to oscillate; some implementations of the
STL set a parameter once, for all time.
Using the BIPS algorithm, evidence from an in-
put utterance will either strengthen the current pa-
rameter settings or weaken them. Either way, there
is re-estimation of the probabilities associated with
possible parameter values. Values are only assigned
when sufficient evidence has been accumulated, i.e.
once the associated probability reaches a threshold
value. By employing this method, it becomes un-
likely for parameters to switch between settings as
the consequence of an erroneous utterance.
Another advantage of using a Bayesian approach
is that we may set default parameter values by as-
signing Bayesian priors; if a parameter’s default
value is strongly biased against the accumulated ev-
idence then it will be difficult to switch. Also, we no
longer need to worry about ambiguity in parameter-
setting-utterances (Clark, 1992) (Fodor, 1998b): the
Bayesian approach allows us to solve this problem
“for free” since indeterminacy just becomes another
case of error due to misclassification of input data
(Buttery and Briscoe, 2004).
2 Overview of the Categorial Grammar
Learner
The learning system is composed of a three mod-
ules: a semantics learning module, syntax learning
module and memory module. For each utterance
heard the learner receives an input stream of word
tokens paired with possible semantic hypotheses.
For example, on hearing the utterance “Dinah drinks
milk” the learner may receive the pairing: ({dinah,
drinks, milk}, drinks(dinah, milk)).
</bodyText>
<subsectionHeader confidence="0.990219">
2.1 The Semantic Module
</subsectionHeader>
<bodyText confidence="0.9999544375">
The semantic module attempts to learn the mapping
between word tokens and semantic symbols, build-
ing a lexicon containing the meaning associated
with each word sense. This is achieved by analyz-
ing each input utterance and its associated semantic
hypotheses using cross-situational techniques (fol-
lowing Siskind (1996)).
For a trivial example consider the utterances “Al-
ice laughs” and “Alice eats cookies”; they might
have word tokens paired with semantic expressions
as follows: ({alice, laughs}, laugh(alice)), ({alice,
eats, cookies}, eat(alice, cookies)).
From these two utterances it is possible to ascer-
tain that the meaning associated with the word token
alice must be alice since it is the only semantic ele-
ment that is common to both utterances.
</bodyText>
<page confidence="0.979464">
3
</page>
<subsectionHeader confidence="0.994308">
2.2 The Syntactic Module
</subsectionHeader>
<bodyText confidence="0.999971458333333">
The learning system links the semantic module and
syntactic module by using a typing assumption: the
semantic arity of a word is usually the same as its
number of syntactic arguments. For example, if it is
known that likes maps to like(x, y), then the typ-
ing assumption suggests that its syntactic category
will be in one of the following forms: a\b\c, a/b\c,
a\b/c, a/b/c or more concisely a  |b  |c (where a, b
and c may be basic or complex syntactic categories
themselves).
By employing the typing assumption the number
of arguments in a word’s syntactic category can be
hypothesized. Thus, the objective of the syntactic
module is to discover the arguments’ category types
and locations.
The module attempts to create valid parse trees
starting from the syntactic information already as-
sumed by the typing assumption (following But-
tery (2003)). A valid parse is one that adheres
to the rules of the categorial grammar as well as
the constraints imposed by the current settings of
the parameters. If a valid parse can not be found
the learner assumes the typing assumption to have
failed and backtracks to allow type raising.
</bodyText>
<subsectionHeader confidence="0.998218">
2.3 Memory Module
</subsectionHeader>
<bodyText confidence="0.98868846875">
The memory module records the current state of
the hypothesis-space. The syntactic module refers
to this information to place constraints upon which
syntactic categories may be hypothesized. The
module consists of two hierarchies of parameters
which may be set using the BIPS algorithm:
Categorial Parameters determine whether a cat-
egory is in use within the learner’s current model
of the input language. An inheritance hierarchy of
all possible syntactic categories (for up to five argu-
ments) is defined and a parameter associated with
each one (Villavicencio, 2002). Every parameter
(except those associated with primitive categories
such as S) is originally set to INACTIVE, i.e. no
categories (except primitives) are known upon the
commencement of learning. A categorial parameter
may only be set to ACTIVE if its parent category
is already active and there has been satisfactory ev-
idence that the associated category is present in the
language of the environment.
Word Order Parameters determine the underly-
ing order in which constituents occur. They may be
set to either FORWARD or BACKWARD depend-
ing on whether the constituents involved are gen-
erally located to the right or left. An example is
the parameter that specifies the direction of the sub-
ject of a verb: if the language of the environment
is English this parameter would be set to BACK-
WARD since subjects generally appear to the left of
the verb. Evidence for the setting of word order pa-
rameters is collected from word order statistics of
the input language.
</bodyText>
<sectionHeader confidence="0.89077" genericHeader="method">
3 The acquisition of an English-type
language
</sectionHeader>
<bodyText confidence="0.999897285714286">
The English-like language of the three-parameter
system studied by Gibson and Wexler has the
parameter settings and associated unembedded
surface-strings as shown in Figure 4. For this task
we assume that the surface-strings of the English-
like language are independent and identically dis-
tributed in the input to the learner.
</bodyText>
<subsectionHeader confidence="0.483772">
Specifier Complement V2
</subsectionHeader>
<listItem confidence="0.970872153846154">
0 (Left) 1 (Right) 0 (off)
1. Subj Verb
2. Subj Verb Obj
3. Subj Verb Obj Obj
4. Subj Aux Verb
5. Subj Aux Verb Obj
6. Subj Aux Verb Obj Obj
7. Adv Subj Verb
8. Adv Subj Verb Obj
9. Adv Subj Verb Obj Obj
10. Adv Subj Aux Verb
11. Adv Subj Aux Verb Obj
12. Adv Subj Aux Verb Obj Obj
</listItem>
<figureCaption confidence="0.9374725">
Figure 4: Parameter settings and surface-strings of
Gibson and Wexler’s English-like Language.
</figureCaption>
<subsectionHeader confidence="0.999792">
3.1 Efficiency of Trigger Learning Algorithm
</subsectionHeader>
<bodyText confidence="0.999936625">
For the TLA to be successful it must converge to
the correct parameter settings of the English-like
language. Berwick and Niyogi (1996) modeled the
TLA as a Markov process (see Figure 5).
Using this model it is possible to calculate the
probability of converging to the target from each
starting grammar and the expected number of steps
before convergence.
</bodyText>
<subsectionHeader confidence="0.864943">
Probability of Convergence:
</subsectionHeader>
<bodyText confidence="0.999855428571429">
Consider starting from Grammar 3, after the process
finishes looping it has a 3/5 probability of mov-
ing to Grammar 4 (from which it will never con-
verge) and a 2/5 probability of moving to Grammar
7 (from which it will definitely converge), therefore
there is a 40% probability of converging to the target
grammar when starting at Grammar 3.
</bodyText>
<page confidence="0.995408">
4
</page>
<note confidence="0.638908">
Expected number of Steps to Convergence:
</note>
<bodyText confidence="0.885865666666667">
Let S,,, be the expected number of steps from state
n to the target state. For starting grammars 6, 7 and
8, which definitely converge, we know:
</bodyText>
<table confidence="0.895812777777778">
S6 = 5 8
S7 = 1 + 6S6 + 9S8
S8 = 2 1
1 S7 S8
+ + 18
3
1 1
1
+ 12S6 + 36S7
</table>
<bodyText confidence="0.870537">
and for the times when we do converge from gram-
mars 3 and 1 we can expect:
</bodyText>
<equation confidence="0.60798425">
S1 = 1 3
S3 = 1 + 5S1
31
+ 33S3
</equation>
<bodyText confidence="0.9977045625">
Figure 6 shows the probability of convergence and
expected number of steps to convergence for each
of the starting grammars. The expected number of
steps to convergence ranges from infinity (for start-
ing grammars 2 and 4) down to 2.5 for Grammar
1. If the distribution over the starting grammars is
uniform then the overall probability of converging
is the sum of the probabilities of converging from
each state divided by the total number of states:
= 0.63
and the expected number of steps given that you
converge is the weighted average of the number of
steps from each possibly converging state:
The criteria for success for the CGL when acquir-
ing Gibson and Wexler’s English-like language is a
lexicon containing the following:4
</bodyText>
<table confidence="0.967318">
Adv S/S Aux [S\SB]/[S\SB]
Obj OB Verb S\SB
Subj SB [S\SB]/OB
[[S\SB]/OB]/OB
</table>
<bodyText confidence="0.99947825">
where S (sentence), SB (subject) and OB (ob-
ject) are primitive categories which are innate to the
learner with SB and OB assumed to be derivable
from the semantic module.
During the learning process the CGL will have
constructed a category hierarchy by setting appro-
priate categorial parameters to true (see Figure 7).
The learner will have also constructed a word-order
hierarchy (Figure 8), setting parameters to FOR-
WARD or BACKWARD. These hierarchies are used
during the learning process to constrain hypothe-
sized syntactic categories. For this task the set-
ting of the word-order parameters becomes trivial
and their role in constraining hypotheses negligible;
consequently, the rest of our argument will relate to
categorial parameters only. For the purpose of this
</bodyText>
<equation confidence="0.9664965">
gendir = /
� ���
� �
subjdir = \ vargdir = /
</equation>
<figureCaption confidence="0.9740285">
Figure 8: Word-order parameter settings required to
parse Gibson and Wexler’s English-like language.
</figureCaption>
<bodyText confidence="0.992293095238095">
analysis parameters are initialized with uniform pri-
ors and are originally set INACTIVE. Since the in-
put is noiseless, the switching threshold is set such
that parameters may be set ACTIVE upon the evi-
dence from one surface-string.
It is a requirement of the parameter setting de-
vice that the parent-types of hypothesized syntax
categories are ACTIVE before new parameters are
set. Thus, the learner is not allowed to hypoth-
esize the syntactic category for a transitive verb
[[S\SB]/OB] before it has learnt the category for
an intransitive verb [S\SB]; this behaviour con-
strains over-generation. Additionally, it is usually
not possible to derive a word’s full syntactic cate-
gory (i.e. without any remaining unknowns) unless
it is the only new word in the clause.
As a consequence of these issues, the order in
which the surface-strings appear to the learner af-
¢Note that the lexicon would usually contain orthographic
entries for the words in the language rather than word type en-
tries.
</bodyText>
<figure confidence="0.979412666666667">
5.47 + 14.87 + 6 + 21.98 x 0.4 + 2.5 x 0.66 = 7.26
1.00 + 1.00 + 1.00 + 1.00 + 0.40 + 0.66
(7)
</figure>
<subsectionHeader confidence="0.975209">
3.2 Efficiency of Categorial Grammar Learner
</subsectionHeader>
<bodyText confidence="0.9978604375">
The input data to the CGL would usually be an ut-
terance annotated with a logical form; the only data
available here however, is surface-strings consist-
ing of word types. Hence, for the purpose of com-
parison with the TLA the semantic module of our
learner is by-passed; we assume that mappings to
semantic forms have previously been acquired and
that the subject and objects of surface-strings are
known. For example, given surface-string 1 (Subj
Verb) we assume the mapping Verb &gt; verb(x),
which provides Verb with a syntactic category of the
form a|b by the typing assumption (where a, b are
unknown syntactic categories and  |is an operator
over \ and /); we also assume Subj to map to a prim-
itive syntactic category SB, since it is the subject of
Verb.
</bodyText>
<figure confidence="0.9247505">
1.00 + 1.00 + 1.00 + 1.00 + 0.40 + 0.66
8
</figure>
<page confidence="0.944168">
5
</page>
<bodyText confidence="0.980767095238095">
fects the speed of acquisition. For instance, the
learner prefers to see the surface-string Subj Verb
before Subj Verb Obj so that it can acquire the
maximum information without wasting any strings.
For the English-type language described by Gib-
son and Wexler the learner can optimally acquire
the whole lexicon after seeing only 5 surface-strings
(one string needed for each new complex syntactic
category to be learnt). However, the strings appear
to the learner in a random order so it is necessary to
calculate the expected number of strings (or steps)
before convergence.
The learner must necessarily see the string Subj
Verb before it can learn any other information. With
12 surface-strings the probability of seeing Subj
Verb is 1/12 and the expected number of strings be-
fore it is seen is 12. The learner can now learn from
3 surface-strings: Subj Verb Obj, Subj Aux Verb and
Adv Subj Verb. Figure 9 shows a Markov structure
of the process. From the model we can calculate the
expected number of steps to converge to be 24.53.
</bodyText>
<sectionHeader confidence="0.999592" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.99865165625">
The TLA and CGL were compared for efficiency
(expected number of steps to convergence) when
acquiring the English-type grammar of the three-
parameter system studied by Gibson and Wexler.
The expected number of steps for the TLA was
found to be 7.26 but the algorithm only converged
63% of the time. The expected number of steps for
the CGL is 24.53 but the learner converges more re-
liably; a trade off between efficiency and success.
With noiseless input the CGL can only fail if there
is insufficient input strings or if Bayesian priors are
heavily biased against the target. Furthermore, the
CGL can be made robust to noise by increasing the
probability threshold at which a parameter may be
set ACTIVE; the TLA has no mechanism for coping
with noisy data.
The CGL learns incrementally; the hypothesis-
space from which it can select possible syntactic
categories expands dynamically and, as a conse-
quence of the hierarchical structure of parameters,
the speed of acquisition increases over time. For
instance, in the starting state there is only a 1/12
probability of learning from surface-strings whereas
in state k (when all but one category has been ac-
quired) there is a 1/2 probability. It is likely that
with a more complex learning task the benefits of
this incremental approach will outweigh the slow
starting costs. Related work on the effects of incre-
mental learning on STL performance (Sakas, 2000)
draws similar conclusions. Future work hopes to
compare the CGL with other parametric learners
(such as the STL) in larger domains.
</bodyText>
<sectionHeader confidence="0.984683" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99979158">
R Berwick and P Niyogi. 1996. Learning from trig-
gers. Linguistic Inquiry, 27(4):605–622.
H Borer. 1984. Parametric Syntax: Case Studies
in Semitic and Romance Languages. Foris, Dor-
drecht.
E Briscoe. 1999. The acquisition of grammar in an
evolving population of language agents. Machine
Intelligence, 16.
P Buttery and T Briscoe. 2004. The significance of
errors to parametric models of language acquisi-
tion. Technical Report SS-04-05, American As-
sociation of Artificial Intelligence, March.
P Buttery. 2003. A computational model for first
language acquisition. In CLUK-6, Edinburgh.
N Chomsky. 1965. Aspects of the Theory of Syntax.
MIT Press.
N Chomsky. 1981. Lectures on Government and
Binding. Foris Publications.
R Clark. 1992. The selection of syntactic knowl-
edge. Language Acquisition, 2(2):83–149.
Ethnologue. 2004. Languages of the
world, 14th edition. SIL International.
http://www.ethnologue.com/.
J Fodor. 1998a. Parsing to learn. Journal of Psy-
cholinguistic Research, 27(3):339–374.
J Fodor. 1998b. Unambiguous triggers. Linguistic
Inquiry, 29(1):1–36.
E Gibson and K Wexler. 1994. Triggers. Linguistic
Inquiry, 25(3):407–454.
J Legate. 1999. Was the argument that was made
empirical? Ms, Massachusetts Institute of Tech-
nology.
W Sakas and J Fodor. 2001. The structural triggers
learner. In S Bertolo, editor, Language Acquisi-
tion and Learnability, chapter 5. Cambridge Uni-
versity Press, Cambridge, UK.
W Sakas. 2000. Ambiguity and the Computational
Feasibility of Syntax Acquisition. Ph.D. thesis,
City University of New York.
J Siskind. 1996. A computational study of
cross situational techniques for learning word-to-
meaning mappings. Cognition, 61(1-2):39–91,
Nov/Oct.
M Steedman. 2000. The Syntactic Process. MIT
Press/Bradford Books.
A Villavicencio. 2002. The acquisition of a
unification-based generalised categorial gram-
mar. Ph.D. thesis, University of Cambridge.
C Yang. 2002. Knowledge and Learning in Natural
Language. Oxford University Press.
</reference>
<page confidence="0.998142">
6
</page>
<figureCaption confidence="0.876504">
Figure 5: Gibson and Wexler’s TLA as a Markov structure. Circles represent possible grammars (a config-
</figureCaption>
<bodyText confidence="0.980131125">
uration of parameter settings). The target grammar lies at the centre of the structure. Arrows represent the
possible transitions between grammars. Note that the TLA is constrained to only allow movement between
grammars that differ by one parameter value. The probability of moving between Grammar GZ and Gram-
mar Gj is a measure of the number of target surface-strings that are in Gj but not GZ normalized by the total
number of target surface-strings as well as the number of alternate grammars the learner can move to. For
example the probability of moving from Grammar 3 to Grammar 7 is 2/12 * 1/3 = 1/18 since there are 2
target surface-strings allowed by Grammar 7 that are not allowed by Grammar 3 out of a possible of 12 and
three grammars that differ from Grammar 3 by one parameter value.
</bodyText>
<table confidence="0.998536111111111">
Initial Language Initial Grammar Prob. of Converging Expected no. of Steps
VOS -V2 110 0.66 2.50
VOS +V2 111 0.00 n/a
OVS -V2 100 0.40 21.98
OVS +V2 101 0.00 n/a
SVO -V2 010 1.00 0.00
SVO +V2 011 1.00 6.00
SOV -V2 000 1.00 5.47
SOV +V2 001 1.00 14.87
</table>
<figureCaption confidence="0.9768195">
Figure 6: Probability and expected number of steps to convergence from each starting grammar to an
English-like grammar (SVO -V2) when using the TLA.
</figureCaption>
<figure confidence="0.994774666666666">
top
SB OB ������S
S/S S\SB
����� �����
[S\SB]/OB [S\SB]/[S\SB]
[[S\SB]/OB]/OB
</figure>
<figureCaption confidence="0.957634333333333">
Figure 7: Category hierarchy required to parse Gibson and Wexler’s English-like language.
Figure 9: The CGL as a Markov structure. The states represent the set of known syntactic cate-
gories: state S - {}, state a - {S\SB}, state b - {S\SB, S/S}, state c - {S\SB, [S\SB]/OB},
</figureCaption>
<construct confidence="0.8548302">
state d - {S\SB, [S\SB]/[S\SB]}, state e - {S\SB, S/S, [S\SB]/OB}, state f - {S\SB,
[S\SB]/OB, [[S\SB]/OB]/OB}, state g - {S\SB, [S\SB]/[S\SB], S/S} state h - {S\SB,
[S\SB]/[S\SB], [S\SB]/OB}, state i - {S\SB, S/S, [S\SB]/OB, [S\SB]/[S\SB]}, state j -
{S\SB, S/S, [S\SB]/OB, [[S\SB]/OB]/OB}, state k - {S\SB, [S\SB]/OB, [[S\SB]/OB]/OB,
[S\SB]/[S\SB]}, state l - {S\SB, [S\SB]/OB, [[S\SB]/OB]/OB, [S\SB]/[S\SB], S/S}.
</construct>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.543208">
<title confidence="0.979352">A quantitative evaluation of naturalistic models of language acquisition; efficiency of the Triggering Learning Algorithm compared to a Grammar Learner</title>
<author confidence="0.903755">Paula</author>
<affiliation confidence="0.873594">Natural Language and Information Processing Computer Laboratory, Cambridge</affiliation>
<address confidence="0.711583">15 JJ Thomson Avenue, Cambridge, CB3 0FD,</address>
<email confidence="0.998353">paula.buttery@cl.cam.ac.uk</email>
<abstract confidence="0.998866333333333">Naturalistic theories of language acquisition assume learners to be endowed with some innate language knowledge. The purpose of this innate knowledge is to facilitate language acquisition by constraining a learner’s hypothesis space. This paper discusses a naturalistic learning system (a Categorial Grammar Learner (CGL)) that differs from previous learners (such as the Triggering Learning Algorithm (TLA) (Gibson and Wexler, 1994)) by employing a dynamic definition of the hypothesis-space which is driven by the Bayesian Incremental Parameter Setting algorithm (Briscoe, 1999). We compare the efficiency of the TLA with the CGL when acquiring an independently and identically distributed English-like language in noiseless conditions. We show that when convergence to the target grammar occurs (which is not guaranteed), the expected number of steps to convergence for the TLA is shorter than that for the CGL initialized with uniform priors. However, the CGL converges more reliably than the TLA. We discuss the trade-off of efficiency against more reliable convergence to the target grammar.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Berwick</author>
<author>P Niyogi</author>
</authors>
<title>Learning from triggers.</title>
<date>1996</date>
<booktitle>Linguistic Inquiry,</booktitle>
<pages>27--4</pages>
<contexts>
<context position="16114" citStr="Berwick and Niyogi (1996)" startWordPosition="2602" endWordPosition="2605">ly distributed in the input to the learner. Specifier Complement V2 0 (Left) 1 (Right) 0 (off) 1. Subj Verb 2. Subj Verb Obj 3. Subj Verb Obj Obj 4. Subj Aux Verb 5. Subj Aux Verb Obj 6. Subj Aux Verb Obj Obj 7. Adv Subj Verb 8. Adv Subj Verb Obj 9. Adv Subj Verb Obj Obj 10. Adv Subj Aux Verb 11. Adv Subj Aux Verb Obj 12. Adv Subj Aux Verb Obj Obj Figure 4: Parameter settings and surface-strings of Gibson and Wexler’s English-like Language. 3.1 Efficiency of Trigger Learning Algorithm For the TLA to be successful it must converge to the correct parameter settings of the English-like language. Berwick and Niyogi (1996) modeled the TLA as a Markov process (see Figure 5). Using this model it is possible to calculate the probability of converging to the target from each starting grammar and the expected number of steps before convergence. Probability of Convergence: Consider starting from Grammar 3, after the process finishes looping it has a 3/5 probability of moving to Grammar 4 (from which it will never converge) and a 2/5 probability of moving to Grammar 7 (from which it will definitely converge), therefore there is a 40% probability of converging to the target grammar when starting at Grammar 3. 4 Expecte</context>
</contexts>
<marker>Berwick, Niyogi, 1996</marker>
<rawString>R Berwick and P Niyogi. 1996. Learning from triggers. Linguistic Inquiry, 27(4):605–622.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Borer</author>
</authors>
<title>Parametric Syntax: Case Studies in Semitic and Romance Languages.</title>
<date>1984</date>
<location>Foris, Dordrecht.</location>
<contexts>
<context position="6580" citStr="Borer (1984)" startWordPosition="1051" endWordPosition="1052">p must be in the category of functional constituents that takes a np from the left and returns an s. This can be written s\np. Likewise a tv takes an np from the right and returns a vp (whose type we already know). A tv may be written (s\np)/np. Rules may be used to combine categories. We assume that our learner is innately endowed with the rules of function application, function composition and generalized weak permutation (Briscoe, 1999) (see figures 1 and 2). • Forward Application (&gt;) X/Y Y —&gt; X 2The concept of lexical parameters and the lexical-linking of parameters is to be attributed to Borer (1984). • Backward Application (&lt;) Y X\Y —&gt; X • Forward Composition (&gt; B) X/Y Y/Z —&gt; X/Z • Backward Composition (&lt; B) Y \X Z\Y —&gt; X\Z • Generalized Weak Permutation (P) ((X |Yl)... |Yn) —&gt; ((X |Yn)... |Yl) where |is a variable over \ and /. may eat (s\np)/(s\np) (s\np)/np &gt; s\np &lt; s Figure 1: Illustration of forward/backward application (&gt;, &lt;) and forward composition (&gt; B) saw she (s\np)/np P np (s/np)\np &lt; rabbit (n\n)/(s/np) n\n &lt; n &gt; np Figure 2: Illustration of generalized weak permutation (P) The lexicon for a language will contain a finite subset of all possible syntactic categories, the size </context>
</contexts>
<marker>Borer, 1984</marker>
<rawString>H Borer. 1984. Parametric Syntax: Case Studies in Semitic and Romance Languages. Foris, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Briscoe</author>
</authors>
<title>The acquisition of grammar in an evolving population of language agents.</title>
<date>1999</date>
<journal>Machine Intelligence,</journal>
<volume>16</volume>
<contexts>
<context position="936" citStr="Briscoe, 1999" startWordPosition="124" endWordPosition="125">ula.buttery@cl.cam.ac.uk Abstract Naturalistic theories of language acquisition assume learners to be endowed with some innate language knowledge. The purpose of this innate knowledge is to facilitate language acquisition by constraining a learner’s hypothesis space. This paper discusses a naturalistic learning system (a Categorial Grammar Learner (CGL)) that differs from previous learners (such as the Triggering Learning Algorithm (TLA) (Gibson and Wexler, 1994)) by employing a dynamic definition of the hypothesis-space which is driven by the Bayesian Incremental Parameter Setting algorithm (Briscoe, 1999). We compare the efficiency of the TLA with the CGL when acquiring an independently and identically distributed English-like language in noiseless conditions. We show that when convergence to the target grammar occurs (which is not guaranteed), the expected number of steps to convergence for the TLA is shorter than that for the CGL initialized with uniform priors. However, the CGL converges more reliably than the TLA. We discuss the trade-off of efficiency against more reliable convergence to the target grammar. 1 Introduction A normal child acquires the language of her environment without any</context>
<context position="6411" citStr="Briscoe, 1999" startWordPosition="1021" endWordPosition="1022">tructure which describes the properties of a transitive verb: s —&gt; np vp vp —&gt; tv np tv —&gt; gets, finds, ... Assume that there is a set of primitive categories {s, np}. A vp must be in the category of functional constituents that takes a np from the left and returns an s. This can be written s\np. Likewise a tv takes an np from the right and returns a vp (whose type we already know). A tv may be written (s\np)/np. Rules may be used to combine categories. We assume that our learner is innately endowed with the rules of function application, function composition and generalized weak permutation (Briscoe, 1999) (see figures 1 and 2). • Forward Application (&gt;) X/Y Y —&gt; X 2The concept of lexical parameters and the lexical-linking of parameters is to be attributed to Borer (1984). • Backward Application (&lt;) Y X\Y —&gt; X • Forward Composition (&gt; B) X/Y Y/Z —&gt; X/Z • Backward Composition (&lt; B) Y \X Z\Y —&gt; X\Z • Generalized Weak Permutation (P) ((X |Yl)... |Yn) —&gt; ((X |Yn)... |Yl) where |is a variable over \ and /. may eat (s\np)/(s\np) (s\np)/np &gt; s\np &lt; s Figure 1: Illustration of forward/backward application (&gt;, &lt;) and forward composition (&gt; B) saw she (s\np)/np P np (s/np)\np &lt; rabbit (n\n)/(s/np) n\n &lt; </context>
<context position="9751" citStr="Briscoe, 1999" startWordPosition="1575" endWordPosition="1576">fore transitive verbs may be hypothesized). (2) Parameter values may be inherited as a method of acquisition. (3) The parameters are stored efficiently. s - ACTIVE ������ ������ s/s s\np - ACTIVE ����� ����� [s\np]/np -ACTIVE [s\np]/[s\np] Figure 3: Partial hierarchy of syntactic categories. Each category is associated with a parameter indicating either ACTIVE or INACTIVE status. 3 - Statistical Parameter Setting The learner uses a statistical method to track relative frequencies of parameter-setting-utterances in the input.3 We use the Bayesian Incremental Parameter Setting (BIPS) algorithm (Briscoe, 1999) to set the categorial parameters. Such an approach sets the parameters to the values that are most likely given all the accumulated evidence. This represents 3Other statistical parameter setting models include Yang’s Variational model (2002) and the Guessing STL (Fodor, 1998a) a compromise between two extremes: implementations of the TLA are memoryless allowing a parameter values to oscillate; some implementations of the STL set a parameter once, for all time. Using the BIPS algorithm, evidence from an input utterance will either strengthen the current parameter settings or weaken them. Eithe</context>
</contexts>
<marker>Briscoe, 1999</marker>
<rawString>E Briscoe. 1999. The acquisition of grammar in an evolving population of language agents. Machine Intelligence, 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Buttery</author>
<author>T Briscoe</author>
</authors>
<title>The significance of errors to parametric models of language acquisition.</title>
<date>2004</date>
<journal>American Association of Artificial Intelligence, March.</journal>
<tech>Technical Report SS-04-05,</tech>
<contexts>
<context position="11252" citStr="Buttery and Briscoe, 2004" startWordPosition="1804" endWordPosition="1807"> for parameters to switch between settings as the consequence of an erroneous utterance. Another advantage of using a Bayesian approach is that we may set default parameter values by assigning Bayesian priors; if a parameter’s default value is strongly biased against the accumulated evidence then it will be difficult to switch. Also, we no longer need to worry about ambiguity in parametersetting-utterances (Clark, 1992) (Fodor, 1998b): the Bayesian approach allows us to solve this problem “for free” since indeterminacy just becomes another case of error due to misclassification of input data (Buttery and Briscoe, 2004). 2 Overview of the Categorial Grammar Learner The learning system is composed of a three modules: a semantics learning module, syntax learning module and memory module. For each utterance heard the learner receives an input stream of word tokens paired with possible semantic hypotheses. For example, on hearing the utterance “Dinah drinks milk” the learner may receive the pairing: ({dinah, drinks, milk}, drinks(dinah, milk)). 2.1 The Semantic Module The semantic module attempts to learn the mapping between word tokens and semantic symbols, building a lexicon containing the meaning associated w</context>
</contexts>
<marker>Buttery, Briscoe, 2004</marker>
<rawString>P Buttery and T Briscoe. 2004. The significance of errors to parametric models of language acquisition. Technical Report SS-04-05, American Association of Artificial Intelligence, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Buttery</author>
</authors>
<title>A computational model for first language acquisition.</title>
<date>2003</date>
<booktitle>In CLUK-6,</booktitle>
<location>Edinburgh.</location>
<contexts>
<context position="13332" citStr="Buttery (2003)" startWordPosition="2136" endWordPosition="2138"> like(x, y), then the typing assumption suggests that its syntactic category will be in one of the following forms: a\b\c, a/b\c, a\b/c, a/b/c or more concisely a |b |c (where a, b and c may be basic or complex syntactic categories themselves). By employing the typing assumption the number of arguments in a word’s syntactic category can be hypothesized. Thus, the objective of the syntactic module is to discover the arguments’ category types and locations. The module attempts to create valid parse trees starting from the syntactic information already assumed by the typing assumption (following Buttery (2003)). A valid parse is one that adheres to the rules of the categorial grammar as well as the constraints imposed by the current settings of the parameters. If a valid parse can not be found the learner assumes the typing assumption to have failed and backtracks to allow type raising. 2.3 Memory Module The memory module records the current state of the hypothesis-space. The syntactic module refers to this information to place constraints upon which syntactic categories may be hypothesized. The module consists of two hierarchies of parameters which may be set using the BIPS algorithm: Categorial P</context>
</contexts>
<marker>Buttery, 2003</marker>
<rawString>P Buttery. 2003. A computational model for first language acquisition. In CLUK-6, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Aspects of the Theory of Syntax.</title>
<date>1965</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1570" citStr="Chomsky (1965)" startWordPosition="225" endWordPosition="226">iciency of the TLA with the CGL when acquiring an independently and identically distributed English-like language in noiseless conditions. We show that when convergence to the target grammar occurs (which is not guaranteed), the expected number of steps to convergence for the TLA is shorter than that for the CGL initialized with uniform priors. However, the CGL converges more reliably than the TLA. We discuss the trade-off of efficiency against more reliable convergence to the target grammar. 1 Introduction A normal child acquires the language of her environment without any specific training. Chomsky (1965) claims that, given the “relatively slight exposure” to examples and “remarkable complexity” of language, it would be “an extraordinary intellectual achievement” for a child to acquire a language if not specifically designed to do so. His Argument from the Poverty of the Stimulus suggests that if we know X, and X is undetermined by learning experience then X must be innate. For an example consider structure dependency in language syntax: A question in English can be formed by inverting the auxiliary verb and subject noun-phrase: (1a) “Dinah was drinking a saucer of milk”; (1b) “was Dinah drink</context>
</contexts>
<marker>Chomsky, 1965</marker>
<rawString>N Chomsky. 1965. Aspects of the Theory of Syntax. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Lectures on Government and Binding.</title>
<date>1981</date>
<publisher>Foris Publications.</publisher>
<contexts>
<context position="3957" citStr="Chomsky (1981)" startWordPosition="612" endWordPosition="613">&apos; This hypothesis-space must be both large enough to contain grammar’s for all of the world’s languages and small enough to ensure successful acquisition given the sparsity of data. Language acquisition is the process of searching the hypothesis-space for the grammar that most closely describes the language of the environment. With estimates of the number of living languages being around 6800 (Ethnologue, 2004) it is not sensible to model the hypothesis-space of grammars explicitly, rather it must be modeled parametrically. Language acquisition is then the process of setting these parameters. Chomsky (1981) suggested that parameters should represent points of variation between languages, however the only requirement for parameters is that they define the current hypothesisspace. &apos;Discussion of structural dependence as evidence of the Argument from the Poverty of Stimulus is illustrative, the significance being that innate knowledge in any form will place constraints on the hypothesis-space 1 The properties of the parameters used by this learner (the CGL) are as follows: (1) Parameters are lexical; (2) Parameters are inheritance based; (3) Parameter setting is statistical. 1 - Lexical Parameters </context>
</contexts>
<marker>Chomsky, 1981</marker>
<rawString>N Chomsky. 1981. Lectures on Government and Binding. Foris Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Clark</author>
</authors>
<title>The selection of syntactic knowledge.</title>
<date>1992</date>
<journal>Language Acquisition,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="11049" citStr="Clark, 1992" startWordPosition="1775" endWordPosition="1776">values. Values are only assigned when sufficient evidence has been accumulated, i.e. once the associated probability reaches a threshold value. By employing this method, it becomes unlikely for parameters to switch between settings as the consequence of an erroneous utterance. Another advantage of using a Bayesian approach is that we may set default parameter values by assigning Bayesian priors; if a parameter’s default value is strongly biased against the accumulated evidence then it will be difficult to switch. Also, we no longer need to worry about ambiguity in parametersetting-utterances (Clark, 1992) (Fodor, 1998b): the Bayesian approach allows us to solve this problem “for free” since indeterminacy just becomes another case of error due to misclassification of input data (Buttery and Briscoe, 2004). 2 Overview of the Categorial Grammar Learner The learning system is composed of a three modules: a semantics learning module, syntax learning module and memory module. For each utterance heard the learner receives an input stream of word tokens paired with possible semantic hypotheses. For example, on hearing the utterance “Dinah drinks milk” the learner may receive the pairing: ({dinah, drin</context>
</contexts>
<marker>Clark, 1992</marker>
<rawString>R Clark. 1992. The selection of syntactic knowledge. Language Acquisition, 2(2):83–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ethnologue</author>
</authors>
<title>Languages of the world, 14th edition.</title>
<date>2004</date>
<note>SIL International. http://www.ethnologue.com/.</note>
<contexts>
<context position="3757" citStr="Ethnologue, 2004" startWordPosition="582" endWordPosition="583">y-dependent upon syntax she would never have hypothesized rules (i) and (iii). Thus, Chomsky theorizes that a human mind contains a Universal Grammar which defines a hypothesis-space of “legal” grammars.&apos; This hypothesis-space must be both large enough to contain grammar’s for all of the world’s languages and small enough to ensure successful acquisition given the sparsity of data. Language acquisition is the process of searching the hypothesis-space for the grammar that most closely describes the language of the environment. With estimates of the number of living languages being around 6800 (Ethnologue, 2004) it is not sensible to model the hypothesis-space of grammars explicitly, rather it must be modeled parametrically. Language acquisition is then the process of setting these parameters. Chomsky (1981) suggested that parameters should represent points of variation between languages, however the only requirement for parameters is that they define the current hypothesisspace. &apos;Discussion of structural dependence as evidence of the Argument from the Poverty of Stimulus is illustrative, the significance being that innate knowledge in any form will place constraints on the hypothesis-space 1 The pro</context>
</contexts>
<marker>Ethnologue, 2004</marker>
<rawString>Ethnologue. 2004. Languages of the world, 14th edition. SIL International. http://www.ethnologue.com/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Fodor</author>
</authors>
<title>Parsing to learn.</title>
<date>1998</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="4795" citStr="Fodor, 1998" startWordPosition="743" endWordPosition="744"> the Argument from the Poverty of Stimulus is illustrative, the significance being that innate knowledge in any form will place constraints on the hypothesis-space 1 The properties of the parameters used by this learner (the CGL) are as follows: (1) Parameters are lexical; (2) Parameters are inheritance based; (3) Parameter setting is statistical. 1 - Lexical Parameters The CGL employs parameter setting as a means to acquire a lexicon; differing from other parametric learners, (such as the Triggering Learning Algorithm (TLA) (Gibson and Wexler, 1994) and the Structural Triggers Learner (STL) (Fodor, 1998b), (Sakas and Fodor, 2001)) which acquire general syntactic information rather than the syntactic properties associated with individual words.2 In particular, a categorial grammar is acquired. The syntactic properties of a word are contained in its lexical entry in the form of a syntactic category. A word that may be used in multiple syntactic situations (or sub-categorization frames) will have multiple entries in the lexicon. Syntactic categories are constructed from a finite set of primitive categories combined with two operators (/ and \) and are defined by their members ability to combine</context>
<context position="10027" citStr="Fodor, 1998" startWordPosition="1616" endWordPosition="1617">c categories. Each category is associated with a parameter indicating either ACTIVE or INACTIVE status. 3 - Statistical Parameter Setting The learner uses a statistical method to track relative frequencies of parameter-setting-utterances in the input.3 We use the Bayesian Incremental Parameter Setting (BIPS) algorithm (Briscoe, 1999) to set the categorial parameters. Such an approach sets the parameters to the values that are most likely given all the accumulated evidence. This represents 3Other statistical parameter setting models include Yang’s Variational model (2002) and the Guessing STL (Fodor, 1998a) a compromise between two extremes: implementations of the TLA are memoryless allowing a parameter values to oscillate; some implementations of the STL set a parameter once, for all time. Using the BIPS algorithm, evidence from an input utterance will either strengthen the current parameter settings or weaken them. Either way, there is re-estimation of the probabilities associated with possible parameter values. Values are only assigned when sufficient evidence has been accumulated, i.e. once the associated probability reaches a threshold value. By employing this method, it becomes unlikely </context>
</contexts>
<marker>Fodor, 1998</marker>
<rawString>J Fodor. 1998a. Parsing to learn. Journal of Psycholinguistic Research, 27(3):339–374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Fodor</author>
</authors>
<title>Unambiguous triggers.</title>
<date>1998</date>
<journal>Linguistic Inquiry,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="4795" citStr="Fodor, 1998" startWordPosition="743" endWordPosition="744"> the Argument from the Poverty of Stimulus is illustrative, the significance being that innate knowledge in any form will place constraints on the hypothesis-space 1 The properties of the parameters used by this learner (the CGL) are as follows: (1) Parameters are lexical; (2) Parameters are inheritance based; (3) Parameter setting is statistical. 1 - Lexical Parameters The CGL employs parameter setting as a means to acquire a lexicon; differing from other parametric learners, (such as the Triggering Learning Algorithm (TLA) (Gibson and Wexler, 1994) and the Structural Triggers Learner (STL) (Fodor, 1998b), (Sakas and Fodor, 2001)) which acquire general syntactic information rather than the syntactic properties associated with individual words.2 In particular, a categorial grammar is acquired. The syntactic properties of a word are contained in its lexical entry in the form of a syntactic category. A word that may be used in multiple syntactic situations (or sub-categorization frames) will have multiple entries in the lexicon. Syntactic categories are constructed from a finite set of primitive categories combined with two operators (/ and \) and are defined by their members ability to combine</context>
<context position="10027" citStr="Fodor, 1998" startWordPosition="1616" endWordPosition="1617">c categories. Each category is associated with a parameter indicating either ACTIVE or INACTIVE status. 3 - Statistical Parameter Setting The learner uses a statistical method to track relative frequencies of parameter-setting-utterances in the input.3 We use the Bayesian Incremental Parameter Setting (BIPS) algorithm (Briscoe, 1999) to set the categorial parameters. Such an approach sets the parameters to the values that are most likely given all the accumulated evidence. This represents 3Other statistical parameter setting models include Yang’s Variational model (2002) and the Guessing STL (Fodor, 1998a) a compromise between two extremes: implementations of the TLA are memoryless allowing a parameter values to oscillate; some implementations of the STL set a parameter once, for all time. Using the BIPS algorithm, evidence from an input utterance will either strengthen the current parameter settings or weaken them. Either way, there is re-estimation of the probabilities associated with possible parameter values. Values are only assigned when sufficient evidence has been accumulated, i.e. once the associated probability reaches a threshold value. By employing this method, it becomes unlikely </context>
</contexts>
<marker>Fodor, 1998</marker>
<rawString>J Fodor. 1998b. Unambiguous triggers. Linguistic Inquiry, 29(1):1–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gibson</author>
<author>K Wexler</author>
</authors>
<date>1994</date>
<journal>Triggers. Linguistic Inquiry,</journal>
<volume>25</volume>
<issue>3</issue>
<contexts>
<context position="789" citStr="Gibson and Wexler, 1994" startWordPosition="102" endWordPosition="105">r Paula Buttery Natural Language and Information Processing Group, Computer Laboratory, Cambridge University, 15 JJ Thomson Avenue, Cambridge, CB3 0FD, UK paula.buttery@cl.cam.ac.uk Abstract Naturalistic theories of language acquisition assume learners to be endowed with some innate language knowledge. The purpose of this innate knowledge is to facilitate language acquisition by constraining a learner’s hypothesis space. This paper discusses a naturalistic learning system (a Categorial Grammar Learner (CGL)) that differs from previous learners (such as the Triggering Learning Algorithm (TLA) (Gibson and Wexler, 1994)) by employing a dynamic definition of the hypothesis-space which is driven by the Bayesian Incremental Parameter Setting algorithm (Briscoe, 1999). We compare the efficiency of the TLA with the CGL when acquiring an independently and identically distributed English-like language in noiseless conditions. We show that when convergence to the target grammar occurs (which is not guaranteed), the expected number of steps to convergence for the TLA is shorter than that for the CGL initialized with uniform priors. However, the CGL converges more reliably than the TLA. We discuss the trade-off of eff</context>
<context position="4740" citStr="Gibson and Wexler, 1994" startWordPosition="733" endWordPosition="736">hypothesisspace. &apos;Discussion of structural dependence as evidence of the Argument from the Poverty of Stimulus is illustrative, the significance being that innate knowledge in any form will place constraints on the hypothesis-space 1 The properties of the parameters used by this learner (the CGL) are as follows: (1) Parameters are lexical; (2) Parameters are inheritance based; (3) Parameter setting is statistical. 1 - Lexical Parameters The CGL employs parameter setting as a means to acquire a lexicon; differing from other parametric learners, (such as the Triggering Learning Algorithm (TLA) (Gibson and Wexler, 1994) and the Structural Triggers Learner (STL) (Fodor, 1998b), (Sakas and Fodor, 2001)) which acquire general syntactic information rather than the syntactic properties associated with individual words.2 In particular, a categorial grammar is acquired. The syntactic properties of a word are contained in its lexical entry in the form of a syntactic category. A word that may be used in multiple syntactic situations (or sub-categorization frames) will have multiple entries in the lexicon. Syntactic categories are constructed from a finite set of primitive categories combined with two operators (/ and</context>
</contexts>
<marker>Gibson, Wexler, 1994</marker>
<rawString>E Gibson and K Wexler. 1994. Triggers. Linguistic Inquiry, 25(3):407–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Legate</author>
</authors>
<title>Was the argument that was made empirical? Ms,</title>
<date>1999</date>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="2924" citStr="Legate, 1999" startWordPosition="453" endWordPosition="454">he first and second words in the sentence; (ii) front the first auxiliary verb; (iii) front words beginning with w. The first two of these rules are refuted if the child encounters the following: (2a) “the cat who was grinning at Alice was disappearing”; (2b) “was the cat who was grinning at Alice disappearing?” If a child is to converge upon the correct hypothesis unaided she must be exposed to sufficient examples so that all false hypotheses are refuted. Unfortunately such examples are not readily available in child-directed speech; even the constructions in examples (2a) and (2b) are rare (Legate, 1999). To compensate for this lack of data Chomsky suggests that some principles of language are already available in the child’s mind. For example, if the child had innately “known” that all grammar rules are structurally-dependent upon syntax she would never have hypothesized rules (i) and (iii). Thus, Chomsky theorizes that a human mind contains a Universal Grammar which defines a hypothesis-space of “legal” grammars.&apos; This hypothesis-space must be both large enough to contain grammar’s for all of the world’s languages and small enough to ensure successful acquisition given the sparsity of data.</context>
</contexts>
<marker>Legate, 1999</marker>
<rawString>J Legate. 1999. Was the argument that was made empirical? Ms, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Sakas</author>
<author>J Fodor</author>
</authors>
<title>The structural triggers learner.</title>
<date>2001</date>
<booktitle>Language Acquisition and Learnability, chapter 5.</booktitle>
<editor>In S Bertolo, editor,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="4822" citStr="Sakas and Fodor, 2001" startWordPosition="745" endWordPosition="748">om the Poverty of Stimulus is illustrative, the significance being that innate knowledge in any form will place constraints on the hypothesis-space 1 The properties of the parameters used by this learner (the CGL) are as follows: (1) Parameters are lexical; (2) Parameters are inheritance based; (3) Parameter setting is statistical. 1 - Lexical Parameters The CGL employs parameter setting as a means to acquire a lexicon; differing from other parametric learners, (such as the Triggering Learning Algorithm (TLA) (Gibson and Wexler, 1994) and the Structural Triggers Learner (STL) (Fodor, 1998b), (Sakas and Fodor, 2001)) which acquire general syntactic information rather than the syntactic properties associated with individual words.2 In particular, a categorial grammar is acquired. The syntactic properties of a word are contained in its lexical entry in the form of a syntactic category. A word that may be used in multiple syntactic situations (or sub-categorization frames) will have multiple entries in the lexicon. Syntactic categories are constructed from a finite set of primitive categories combined with two operators (/ and \) and are defined by their members ability to combine with other constituents; t</context>
</contexts>
<marker>Sakas, Fodor, 2001</marker>
<rawString>W Sakas and J Fodor. 2001. The structural triggers learner. In S Bertolo, editor, Language Acquisition and Learnability, chapter 5. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Sakas</author>
</authors>
<title>Ambiguity and the Computational Feasibility of Syntax Acquisition.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>City University of New York.</institution>
<marker>Sakas, 2000</marker>
<rawString>W Sakas. 2000. Ambiguity and the Computational Feasibility of Syntax Acquisition. Ph.D. thesis, City University of New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Siskind</author>
</authors>
<title>A computational study of cross situational techniques for learning word-tomeaning mappings.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--1</pages>
<contexts>
<context position="12023" citStr="Siskind (1996)" startWordPosition="1922" endWordPosition="1923">mory module. For each utterance heard the learner receives an input stream of word tokens paired with possible semantic hypotheses. For example, on hearing the utterance “Dinah drinks milk” the learner may receive the pairing: ({dinah, drinks, milk}, drinks(dinah, milk)). 2.1 The Semantic Module The semantic module attempts to learn the mapping between word tokens and semantic symbols, building a lexicon containing the meaning associated with each word sense. This is achieved by analyzing each input utterance and its associated semantic hypotheses using cross-situational techniques (following Siskind (1996)). For a trivial example consider the utterances “Alice laughs” and “Alice eats cookies”; they might have word tokens paired with semantic expressions as follows: ({alice, laughs}, laugh(alice)), ({alice, eats, cookies}, eat(alice, cookies)). From these two utterances it is possible to ascertain that the meaning associated with the word token alice must be alice since it is the only semantic element that is common to both utterances. 3 2.2 The Syntactic Module The learning system links the semantic module and syntactic module by using a typing assumption: the semantic arity of a word is usuall</context>
</contexts>
<marker>Siskind, 1996</marker>
<rawString>J Siskind. 1996. A computational study of cross situational techniques for learning word-tomeaning mappings. Cognition, 61(1-2):39–91, Nov/Oct.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT Press/Bradford Books.</publisher>
<contexts>
<context position="7229" citStr="Steedman (2000)" startWordPosition="1168" endWordPosition="1169"> —&gt; X • Forward Composition (&gt; B) X/Y Y/Z —&gt; X/Z • Backward Composition (&lt; B) Y \X Z\Y —&gt; X\Z • Generalized Weak Permutation (P) ((X |Yl)... |Yn) —&gt; ((X |Yn)... |Yl) where |is a variable over \ and /. may eat (s\np)/(s\np) (s\np)/np &gt; s\np &lt; s Figure 1: Illustration of forward/backward application (&gt;, &lt;) and forward composition (&gt; B) saw she (s\np)/np P np (s/np)\np &lt; rabbit (n\n)/(s/np) n\n &lt; n &gt; np Figure 2: Illustration of generalized weak permutation (P) The lexicon for a language will contain a finite subset of all possible syntactic categories, the size of which depends on the language. Steedman (2000) suggests that for English the lexical functional categories never need more than five arguments and that these are needed only in a limited number of cases such as for the verb bet in the sentence I bet you five pounds for England to win. The categorial grammar parameters of the CGL are concerned with defining the set of syntactic categories present in the language of the environment. Converging on the correct set aids acquisition by constraining the learner’s hypothesized syntactic categories for an unknown word. A parameter (with Alice (s\np)/np the cake ··· np &gt; B np that (s/np) &gt; the n np</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>M Steedman. 2000. The Syntactic Process. MIT Press/Bradford Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Villavicencio</author>
</authors>
<title>The acquisition of a unification-based generalised categorial grammar.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Cambridge.</institution>
<contexts>
<context position="14201" citStr="Villavicencio, 2002" startWordPosition="2277" endWordPosition="2278">backtracks to allow type raising. 2.3 Memory Module The memory module records the current state of the hypothesis-space. The syntactic module refers to this information to place constraints upon which syntactic categories may be hypothesized. The module consists of two hierarchies of parameters which may be set using the BIPS algorithm: Categorial Parameters determine whether a category is in use within the learner’s current model of the input language. An inheritance hierarchy of all possible syntactic categories (for up to five arguments) is defined and a parameter associated with each one (Villavicencio, 2002). Every parameter (except those associated with primitive categories such as S) is originally set to INACTIVE, i.e. no categories (except primitives) are known upon the commencement of learning. A categorial parameter may only be set to ACTIVE if its parent category is already active and there has been satisfactory evidence that the associated category is present in the language of the environment. Word Order Parameters determine the underlying order in which constituents occur. They may be set to either FORWARD or BACKWARD depending on whether the constituents involved are generally located t</context>
</contexts>
<marker>Villavicencio, 2002</marker>
<rawString>A Villavicencio. 2002. The acquisition of a unification-based generalised categorial grammar. Ph.D. thesis, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Yang</author>
</authors>
<title>Knowledge and Learning in Natural Language.</title>
<date>2002</date>
<publisher>Oxford University Press.</publisher>
<marker>Yang, 2002</marker>
<rawString>C Yang. 2002. Knowledge and Learning in Natural Language. Oxford University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>