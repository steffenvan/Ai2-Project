<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000183">
<title confidence="0.9940785">
Learning Case-based Knowledge for Disambiguating
Chinese Word Segmentation: A preliminary study
</title>
<author confidence="0.953712">
Chunyu Kitt Haihua Pant
</author>
<affiliation confidence="0.9983315">
Dept. of Chinese, Translation &amp; Linguisticst
City University of Hong Kong
</affiliation>
<email confidence="0.987688">
{ctckit, cthpanj@cityu.edu.hk
</email>
<author confidence="0.980459">
Hongbiao Chentt
</author>
<affiliation confidence="0.99236">
Dept. of Foreign Trade &amp; Economict
Cooperation of Guangdong Province, China
</affiliation>
<email confidence="0.982035">
drhbchen@21cn.com
</email>
<sectionHeader confidence="0.968761" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998649">
Just like other NLP applications, a serious problem
with Chinese word segmentation lies in the ambigu-
ities involved. Disambiguation methods fall into dif-
ferent categories, e.g., rule-based, statistical-based
and example-based approaches, each of which may
involve a variety of machine learning techniques. In
this paper we report our current progress within
the example-based approach, including its frame-
work, example representation and collection, exam-
ple matching and application. Experimental results
show that this effective approach resolves more than
90% of ambiguities found. Hence, if it is integrated
effectively with a segmentation method of the preci-
sion P &gt; 95%, the resulting segmentation accuracy
can reach, theoretically, beyond 99.5%.
</bodyText>
<sectionHeader confidence="0.992563" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.944334666666667">
It has been nearly two decades since the early
work of Chinese word segmentation (Liang,
1984; Liang and Liu, 1985; Liu and Liang, 1986;
Liang, 1986) . Tokenization has been recog-
nized as a widespread problem, rather than be-
ing unique to Chinese and other oriental lan-
guages. It is an initial or prerequisite phase
of NLP for all languages, although the obscu-
rity of the problem varies from language to
language (Webster and Kit, 1992a; Palmer,
2000). Recent work on tokenization for Eu-
ropean languages such as English is reported
in (Grefenstette and Tapanainen, 1994; Grefen-
stette, 1999; Grefenstette et al., 2000), adopt-
ing a finite-state approach. However, identifica-
tion of multi-word units such as proper names
and technical terms in these languages is highly
comparable to that of multi-character Chinese
words: there are no delimiters available.
So far, a great variety of segmentation strate-
gies for Chinese with various linguistic resources
have been explored, yielding a large volume
of literature on both linguistic and compu-
tational sides, as listed in (Liu et al., 1994;
Guo, 1997), among many others. In general,
these strategies can be divided into two camps,
namely, dictionary-based and statistical-based
approaches. Nevertheless, the former can be
understood as a restricted instance of the lat-
ter, with an equi-probability for each word in a
given dictionaryl.
Most, if not all, dictionary-based strategies
are built upon a few basic &amp;quot;mechanical&amp;quot; seg-
mentation methods based on string matching
(Kit et al., 1989), among which the most ap-
plicable, thus widely used since the very begin-
ning, are the two maximum matching methods
(MMs), one scanning forward (FMM) and the
other backward (BMM). Interestingly, their per-
formance, frequently used as the baseline for
evaluation, is never too far away from the state-
of-the-art approaches in terms of segmentation
accuracy. Although performing little statistical
computation, the MMs comply, in general, with
the essential principle of the statistical-based
approaches: select a segmentation as probable
as possible among all choices. This ad hoc way
of choosing the segmentation with fewest words
usually leads to, by coincidence, a more proba-
ble output than most other choices with more
words2.
&apos;A dictionary is actually a restricted form of language
model, in this sense.
2The coincidence of fewer words with a greater prob-
ability can be illustrated as follows: given a string s, the
probability of its most probable segmentation seg(s) in
terms of a given language model is
</bodyText>
<equation confidence="0.758293">
prob(seg(s)) =
</equation>
<bodyText confidence="0.790623">
where prob(wil•) is some conditional probability in the
model. Since all prob(wil•) &lt; 1.0, this probability be-
comes smaller for a greater n. Clearly, it looks more
straightforward in an equi-probability setting.
</bodyText>
<equation confidence="0.782991">
prob(wiI•)
max
8=WlW2 ...W�
</equation>
<bodyText confidence="0.983321481481481">
Statistical approaches involve language mod-
els, mostly finite-state ones, trained on some
large-scale corpora, as showed in Fan and Tsai
(1988), Chang et al. (1991), Chiang et al.
(1992), Sproat et al. (1996), Pont and Croft
(1996) and Ng and Lua (forthcoming). These
approaches do not provide any explicit strat-
egy for disambiguation, but they get more am-
biguous chunks correctly segmented than MMs
by virtue of probability. Other linguistic re-
sources or computational processes can also be
integrated for further improvement, e.g., Lai et
al. (1991) attempts to integrate POS tagging
with word segmentation for the enhancement of
accuracy and Gan et al. (1997) integrates word
boundary disambiguation into sentence pro-
cessing within a probabilistic emergent model.
There are also other approaches that incorpo-
rate various techniques of statistical NLP and
machine learning, e.g., transformation-based
error-driven learning (Palmer, 1997; Hocken-
maier and Brew, 1998) and compression-based
algorithm (Teahan et al., 2000).
Recent research shifts its focus onto the fol-
lowing aspects, resorting to a variety of re-
sources and techniques, in particular, machine
learning techniques:
</bodyText>
<listItem confidence="0.998777307692308">
1. Lexical resource acquisition, including
compilation and automatic detection of
high-tech terms and unknown words like
names, to complement a never-big-enough
dictionary (Chang et al., 1995; Pont and
Croft, 1996; Chang and Su, 1997);
2. Investigation into the nature and statistics
of ambiguities (Sun and Zhou, 1998);
3. Unsupervised learning of words ( Ge et al.,
1999; Peng and Schuurmans, 2001)3;
4. Disambiguation with different approaches
(Liang, 1989; Jin, 1994; Sun and T&apos;sou,
1995)
</listItem>
<bodyText confidence="0.985385">
The work reported in this paper belongs to the
last category, taking an instance-based learning
</bodyText>
<footnote confidence="0.976831428571429">
3Recent research in this direction appears to be
closely related to the studies on computational lexical
acquisition of other languages such as English (de Mar-
cken, 1996; Brent, 1999; Kit and Wilks, 1999; Kit, 2000;
Venkataraman, 2001) and to language modeling technol-
ogy (Jelinek, 1997), typically involving a version of the
EM algorithm (Dempster et al., 1977).
</footnote>
<bodyText confidence="0.999516538461539">
approach, aimed to examine its prospects of dis-
ambiguation.
The rest of the paper is organized as follows.
Section 2 briefly introduces the ambiguity prob-
lem and existing ambiguity detection strategies.
Section 3 defines the notion and representation
of examples, and formulates a similarity mea-
sure between an ambiguous input and an exam-
ple. We present our disambiguation algorithm
in Section 4 and experimental results and eval-
uation in Section 5, together with some discus-
sion on the remaining errors, before concluding
the paper in Section 6.
</bodyText>
<sectionHeader confidence="0.92446" genericHeader="introduction">
2 Ambiguity
</sectionHeader>
<bodyText confidence="0.986194333333333">
Conceptually there are two essential types of
ambiguity in Chinese word segmentation, which
are conventionally termed as overlapping and
combinational ambiguities. They can be for-
mally defined as follows, given a dictionary D:
Overlapping ambiguity A given string a, y
involves an overlapping ambiguity, if the set
of sub-strings {a, , , yICD.
Combinational ambiguity A given string
a, involves a combinational ambiguity, if
the set of sub-strings {a, , , a, ICD.
In practice the first type commonly co-occurs
with the second, because almost all Chinese
characters can be mono-character words. For
the same reason, almost every multi-character
word involves a combinational ambiguity. For-
tunately, however, most of them are &amp;quot;resolved&amp;quot;,
characteristically, in a sense, by a MM strategy.
Therefore, the focus of disambiguation is unsur-
prisingly put on the unresolved ones as well as
the overlapping ambiguities.
</bodyText>
<subsectionHeader confidence="0.990442">
2.1 Ambiguity detection
</subsectionHeader>
<bodyText confidence="0.997274071428571">
Conventionally, a straightforward strategy is ex-
ploited to detect ambiguities with the aid of
FMM and BMM: the discrepancies of their out-
puts signal ambiguous strings. It appears ade-
quately efficient, because only a forward and a
backward scanning of the input will do.
However, its reliability remains a question,
although it has been taken for granted for a
long time that there would be few ambiguities
left out, which is at odds with our observation
that there are ambiguous strings for which both
MMs output an identical segmentation. E.g.,
given a string abcde with {a, ab, bcd, c, de, e}E
D, it is conceivable that both MMs output
</bodyText>
<listItem confidence="0.896642">
• • • ab c de • • • , and consequently the embed-
ded ambiguity is unseen. So far we haven&apos;t seen
any report on the incompleteness of ambiguity
detection via this strategy.
</listItem>
<bodyText confidence="0.999598538461538">
A more comprehensive strategy would be that
we first locate the boundaries of all possible
words in terms of a given dictionary are first lo-
cated, and then, the common sub-strings among
these words are detected: any common sub-
string indicates an ambiguity.
Since our current work is intended to examine
the effectiveness of an example-based learning
approach to resolve found ambiguities, its mer-
its do not rely on the completeness of ambiguity
detection. The conventional strategy would suf-
fice for the purpose of identifying an adequate
number of ambiguities for our experiments.
</bodyText>
<sectionHeader confidence="0.791116" genericHeader="method">
3 Examples and similarity measure
</sectionHeader>
<bodyText confidence="0.999870666666667">
We intend to disambiguate Chinese word seg-
mentation ambiguities within the framework
of case-based learning. This supervised learn-
ing approach, also labeled as memory-based,
instance-based or example-based learning, has
been popular for various NLP applications in
recent years, e.g., the TiMBL learner (Daelemans
et al., 2001). TiMBL is developed as a gen-
eral memory-based learning environment to in-
tegrate a set of learning algorithms. It has been
widely applied to disambiguating a variety of
NLP tasks, including PP attachment (Zavrel et
al., 1998), shallow parsing (Daelemans et al.,
1999) and WSD (Veenstra et al., 2000; Steven-
son and Wilks, 2001). In this paper, the general
principle of case-based learning is followed but
the formulation below is nevertheless specific to
our problem.
An example here is defined as a quadruple
&lt; Cl, e, Cr, S &gt;, where the strings Cl and Cr
are the left and right contexts within which the
ambiguous string e appears, and S is the correct
segmentation of e within the particular context.
If denoting the quadruple as E, we also refer to
S as seg(E) or seg(e), interchangeably.
The distance, or similarity, between an exam-
ple E and a given triple A =&lt;Q1 �� a Ca &gt; with
</bodyText>
<equation confidence="0.6810682">
4Notice that ambiguities are dictionary-dependent.
the ambiguous string a is defined as
{t,r}
A(A, E) = Æ(a, e)(1 + Æ&apos;(~~~, C&apos;)) (1)
z
</equation>
<bodyText confidence="0.981411">
where Æ(•, •) indicates the identity of two am-
biguous strings, defined as
</bodyText>
<equation confidence="0.988186">
1, if a=e
Æ(a e) =(2)
0~ otherwise
</equation>
<bodyText confidence="0.999762">
and Æ&apos;(•, •) (for i E {l, r}) is the similarity of the
corresponding contexts, measured in terms of
the length of their common prefix (for the right
contexts) or suffix (for the left contexts) in num-
ber of words5. For two given strings if we denote
their common suffix (i.e., affix from the right)
and prefix (i.e., affix from the lefty) respectively
ash fr(•, •) and fl(•, •), we have Æ2(•,•) = If
Thus, we can rewrite (1) into (3) below.
</bodyText>
<equation confidence="0.916236">
{t,r}
A(A, E) = Æ(a, e)(1 + If &apos;(Ca, C&apos;)I) (3)
z
</equation>
<bodyText confidence="0.999683307692308">
Actually, the idea behind this equation is
more straightforward than it looks. Basically,
we measure the similarity of a given triple (i.e.,
an ambiguous string and its contexts) and an
example in terms of the similarity of their con-
texts. However, this similarity is meaningful if
and only if the strings in question are identical.
This is why we define Æ(a, e).
Given a triple A =&lt; Cil a Ca &gt; and a col-
lection E of examples, known as example base
(EB), the strategy we undertake to determine a
segmentation for the ambiguous string a can be
formulated as follows, for A(A, E) &gt; 1:
</bodyText>
<equation confidence="0.9866815">
seg(a E A) = segCarg max A(A, E))
E E F
</equation>
<bodyText confidence="0.9960572">
where seg(•) denotes the segmentation of a given
string or example. Straightforwardly, Equation
(4) can be read off as the following: segment a
in the same way as its most similar example in
the example base.
</bodyText>
<footnote confidence="0.998083333333333">
5Obviously, measuring the length in number of char-
acters is an alternative to explore in our future research.
6For example,
</footnote>
<equation confidence="0.7322135">
fl(abc, dbc) = null, f&apos;(abc, dbc) = bc
fl(abc, abd) = ab, f&apos;(abc, abd) = null
~(•~ •)I.
(4)
</equation>
<sectionHeader confidence="0.583636" genericHeader="method">
4 Algorithms
</sectionHeader>
<bodyText confidence="0.999842375">
In order to test the effectiveness of the disam-
biguation strategy formulated above, we need to
collect examples from a large-scale unrestricted
corpus via a sound ambiguity detection pro-
gram, and apply the examples to ambiguous
strings in a test corpus via an example appli-
cation program. In this section we present the
algorithms for these purposes.
</bodyText>
<subsectionHeader confidence="0.990327">
4.1 Ambiguity detection
</subsectionHeader>
<bodyText confidence="0.94178">
We take a conventional approach to ambiguity
detection, by detecting the discrepancies of the
outputs from the FMM and BMM segmenta-
tions. Given an input corpus C, it can be real-
ized, plainly, by the following algorithm:
Ambiguity detection algorithm: ambd(C)
</bodyText>
<listItem confidence="0.9937015">
1. 97= FMM(C) and B = BMM(C)
2. Return diff(g7, 8)
</listItem>
<bodyText confidence="0.9999284">
where FMM(•) and FMM(•) return the FMM and
BMM segmentations of C, and diff(•, •) returns
the discrepancies of the two segmentations.
The dictionary used to support the MMs is a
merger of the word lists from Liu et al. (1994)
and Yu (1998), consisting of 53K entries. It
is a medium-sized dictionary. With regards to
the dictionary size and the weakness of the am-
biguity detection algorithm, we keep ourselves
alert of the fact that there are a certain num-
ber of ambiguities that are not detected by our
program. And the resolutions for the ambigu-
ous strings so detected are manually prepared,
by selecting an answer from the outputs of the
MMs in use.
</bodyText>
<subsectionHeader confidence="0.935765">
4.2 Disambiguation
</subsectionHeader>
<bodyText confidence="0.999983333333333">
Given an example base E and a text corpus C
as testing data, the disambiguation algorithm
works along the following steps:
</bodyText>
<listItem confidence="0.779074125">
Disambiguation algorithm: disamb(C, E)
1. Ambiguity detection: ,A = ambd(C)
2. For every aa, E C such that a E ,A,
let A =&lt;a, a, , &gt;
2.1 Search for E = arg max A(A, e)
eEE
2.2 If A(A, E) &gt; 1, seg(a) = seg(E)
2.3 Else seg(a) = arg max q(s)
</listItem>
<bodyText confidence="0.985237">
sE{FMM(a), BMM(a)I
where q(•) gives a probability-like score for a
segmentation, by which we hope to get a bet-
ter result than a random or brute-force choice
between the FMM and BMM outputs (that we
could have made). We refer to q(•) as a solid-
ness function that is defined as the following,
mainly for the simplicity of implementation:
</bodyText>
<equation confidence="0.987857">
n
q(wIw2... wn) = H p.(wi) (5)
</equation>
<bodyText confidence="0.9923555">
where p.(•) is the probability of a given string
being a word. It is defined as
</bodyText>
<equation confidence="0.9904665">
pw(wi) = fw(wi)
f(wi)
</equation>
<bodyText confidence="0.9999634">
where f.(•) and f(•) are, respectively, the fre-
quencies of a given item occurring as a word
and as a string in the training corpus. Since
it is an approximation, we can count the word
frequencies based on the FMM output.
</bodyText>
<sectionHeader confidence="0.639273" genericHeader="method">
5 Experiment and evaluation
</sectionHeader>
<bodyText confidence="0.999971166666667">
A number of experiments were conducted on un-
restricted texts for the purpose of testing the
effectiveness of the above disambiguation ap-
proach. In this section we present the data for
training (i.e., example collection) and testing,
experimental results and evaluation.
</bodyText>
<subsectionHeader confidence="0.940336">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999984692307692">
The data we used for the experiments are news
texts collected from mainland China, Hong
Kong and Taiwan. The corpus size is of 778K
words and 1.5M characters in total, in 1534 text
files. About 3/4 of the data, of 1.16M characters
in 1.1K files, are used for training and the re-
maining 1/4, of 360K characters in about 0.4K
files, for testing. The statistics about the am-
biguous strings found in the training and testing
data is given in Table 1. From the ambiguity-
word (EW) ratio, we can see that the ambiguity
distribution among the two data sets is approx-
imately even.
</bodyText>
<subsectionHeader confidence="0.817483">
5.2 Results and evaluation
</subsectionHeader>
<bodyText confidence="0.999142">
Theoretically, disambiguation accuracy on the
training data should be 100%, because all found
ambiguities are manually resolved. In contrast,
the accuracy on the test set is more indicative of
the effectiveness of the disambiguation strategy.
</bodyText>
<table confidence="0.9998615">
Training Data EW Ratio
Number Total 5401 0.91%
of cases
Unique 3018 0.51%
Testing Data EW Ratio
Number Total 1648 0.90%
of cases
Unique 995 0.54%
</table>
<tableCaption confidence="0.999975">
Table 1: Ambiguities in training &amp; testing data
</tableCaption>
<bodyText confidence="0.999990285714286">
Our experimental results show that among
1648 ambiguities found in the test set, 1488 are
properly resolved, in terms of our manual check-
ing of the disambiguation outputs. Accordingly,
the disambiguation accuracy is 90.29%.
We do not report the overall segmentation ac-
curacy here for a number of reasons. Firstly, al-
most every paper in recent years reports a seg-
mentation accuracy that nearly reaches the ceil-
ing. This fact suggests that such figures seem
to have carried less and less academic signifi-
cance, in the sense that they do not measure
any significant advance in tackling the major
remaining problems in Chinese word segmenta-
tion, such as unknown words and segmentation
ambiguities. Instead, all these figures seem to
indicate a similar performance, which is, more
interestingly, even similar to the performance
reported a decade ago. Secondly, we have not
had much ground to compare different systems&apos;
performance, not only because they were tested
with different sets of data but also because the
ways of calculating the segmentation accuracy
are observed to be different from one another.
On the contrary, the disambiguation accuracy
is more specific, revealing exactly the capacity
of a disambiguation strategy to resolve partic-
ular ambiguities found. It is reasonable to as-
sume that everyone can get the unambiguous
part correct in word segmentation, so we need
not bother taking this part into account for the
evaluation of disambiguation performance. In-
stead, we choose to concentrate on the prob-
lematic part, reporting only the disambiguation
accuracy for the purpose of evaluation.
</bodyText>
<sectionHeader confidence="0.963062" genericHeader="method">
5. 3 Discussions
</sectionHeader>
<bodyText confidence="0.999929454545455">
As pointed out before, the conventional strategy
for ambiguity detection that we have adopted is
known to be incomplete. Many remaining am-
biguities in the data are still to be brought to
light. It is certainly a research direction that
deserves more effort. Discovering more such
missing cases can no doubt enlarge the example
base significantly, and consequently enhance the
strength of this case-based learning approach to
disambiguation.
This problem is also related to the intrinsic
disambiguation ability of the rudimental MMs:
they segment many ambiguous strings correctly
because of their own characteristics rather than
by chance. Thus, it is worth digging out these
uncovered ambiguities as examples so that they
can be correctly handled when they show up
elsewhere that would puzzle the MMs.
A more detailed analysis of experimental re-
sults is also expected, e.g., how many cases are
resolved by existing examples and how many
others by chance, i.e., by the q(•) function,
which was designed to alleviate, rather than re-
solve, the problem. Also, a careful analysis of
unseen cases in the testing data is also critical
for a more thorough evaluation of the merits of
the case-based learning approach. It will reveal
the coverage of the EB and severity of the sparse
data problem. A conceivable solution for the
moment is that we construct all possible ambi-
guities based on a given dictionary and assign
to them proper resolutions, so as to produce an
EB with greater coverage.
</bodyText>
<sectionHeader confidence="0.994554" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.984606545454545">
In this paper we have presented a case-based
learning approach to resolving Chinese word
segmentation ambiguities. We adopted a simple
representation for the examples, each consisting
of an ambiguous string and its contexts, and
also formulated a similarity measure for match-
ing an ambiguity and an example from the ex-
ample base. The effectiveness of this learning
approach was tested on a set of unrestricted
news texts of 1.5M characters, and a disam-
biguation accuracy of 90% was achieved
With this promising result, what we can ex-
pect is that if this approach could be effectively
integrated with a segmentation algorithm that
has a segmentation performance of the accuracy
P, the overall segmentation accuracy one can ex-
pect would be
P&apos; = P + (1 — P)90% = (90 + 10P)%
From this formula, we can see that if P &gt; 90%,
then P&apos; &gt;99%, and if P &gt;95%, then P&apos; &gt;99.5%.
Therefore, a bright future seems to be promised,
because most Chinese word segmenters were re-
ported to have achieved an accuracy over 95%,
according to the literature.
However, the problems we still have with this
case-based learning approach include, mainly,
the incompleteness of ambiguity detection and
the unknown coverage of the example base col-
lected from unrestricted texts. All these re-
maining problems, that we will tackle in our
future research, would have certain effect on the
effectiveness of integrating it into any Chinese
word segmenter.
</bodyText>
<sectionHeader confidence="0.988258" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999138927884615">
M. R. Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word dis-
covery. Machine Learning, 34:71-106.
Jing-Shin Chang and Keh-Yih Su. 1997. An unsu-
pervised iterative method for Chinese new lexi-
con extraction. International Journal of Compu-
tational Linguistics &amp; Chinese Language Process-
ing, 1(1):101-157.
Jyun-Sheng Chang, C.-D. Chen, and S.-D. Chen.
1991. Chinese word segmentation through con-
straint satisfaction and statistical optimization.
In ROCLING-IV, pages 147-165, National Chiao-
Tung University, Hsinchu, Taiwan.
Jing-Shin Chang, Yi-Chung Lin, and Keh-Yih Su.
1995. Automatic construction of a Chinese elec-
tronic dictionary. In David Yarovsky and Ken-
neth Church, editors, WVLC-3, pages 107-120,
Somerset, New Jersey, June.
Keh-Jiann Chen and Shing-Huan Liu. 1992. Word
identification for mandarin Chinese sentences. In
COLING&apos;92, volume I, pages 101-107, Nantes,
France, Jul y 23-28.
Tung-Hui Chiang, Ming-Yu Lin, and Keh-Yih Su.
1992. Statistical models for word segmentation
and unknown word resolution. In ROCLING-V,
pages 121-146, Taiwan.
W. Daelemans, S. Buchholz, and J. Veenstra. 1999.
Memory-based shallow parsing. In CoNLL-99,
pages 53-60, Bergen, Norway.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot,
and Antal van den Bosch. 2001. Timbl: Tilburg
memor y based learner, version 4.0, reference
guide. Technical Report ILK Technical Report
01-04, Induction of Linguistic Knowledge, Tilburg
University, The Netherlands.
C. de Marcken. 1996. Unsupervised Language Ac-
quisition. Ph.D. thesis, MIT, Cambridge, Mass.y.
A. P. Dempster, N. M. Laird, and D. B.Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. Journal of the Royal Statistical So-
ciety, Series B, 34:1-38.
Charng-Kang Fan and Wen-Hsiang Tsai. 1988. Au-
tomatic word identification in Chinese sentences
by the relaxation technique. Computer Processing
of Chinese and Oriental Languages, 4(1):33-56.
Kok-Wee Gan, Martha Palmer, and Kim-Teng Lua.
1997. A statisticall y emergent approach for lan-
guage processing: Application to modeling effects
in ambiguous Chinese word boundar y perception.
Computational Linguistics, 22(4):531-553.
Xianping Ge, Wanda Pratt, and Padhraic Smyth.
1999. Discovering Chinese words from unseg-
mented text (poster abstract). In SIGIR&apos;99,
pages 271-272, Berkeley, August.
Gregor y Grefenstette and P. Tapanainen. 1994.
What is a word, what is a sentence? Problems
of tokenization. In 3rd Conference on Compu-
tational Lexicography and Text Research, COM-
PLEX&apos;94, Budapest, Jul y 7-10.
Gregor y Grefenstette, Anne Schiller, and Salah Ait-
Mokhtar. 2000. Recognizing lexical patterns in
text. In F. van Eynde, D. Gibbon, and I. Schu-
urman, editors, Lexicon Development for Speech
and Language Processing, pages 141-168. Kluwer,
Dordrecht.
Gregor y Grefenstette. 1999. Tokenization. In Hans
van Halteren, editor, Syntactic Wordclass Tag-
ging, pages 117-133. Kluwer, Dordrecht.
Yingchun Guan and Bei Qin. 1986. The design and
implementation of a Chinese word statistical sys-
tem. Journal of Chinese Information Processing,
1(1):26-32. (In Chinese).
Jin Guo. 1997. Critical tokenization and its proper-
ties. Computational Linguistics, 23(4):569-596.
J. Hockenmaier and C. Brew. 1998. Error-driven
learning of Chinese word segmentation. In
PACLIC-12, pages 218-229, Singapore. Chinese
and Oriental Languages Processing Society.
F. Jelinek. 1997. Statistical Methods for Speech Pro-
cessing. MIT Press, Cambridge, MA.
Wanying Jin. 1992. A case study: Chinese segmen-
tation and its disambiguation. Technical Report
MCCS-92-227, Computing Research Laboratory,
New Mexico State University, Las Cruces.
Wanying Jin. 1994. Chinese segmentation disam-
biguation. In COLING-94, pages 1245-1249.
Chunyu Kit and Yorick Wilks. 1999. Unsupervised
learning of word boundar y with description length
gain. In M. Osborne and E. T. K. Sang, editors,
CoNLL-99, pages 1-6, Bergen, June.
Chunyu Kit, Yuan Liu, and Nanyuan Liang. 1989.
On methods of Chinese automatic word segmenta-
tion. Journal of Chinese Information Processing,
3(1):1-32. (In Chinese).
Chunyu Kit. 2000. Unsupervised Lexical Learning
as Inductive Inference. Ph.D. thesis, University
of Sheffield, UK.
Tom B. Y. Lai, Sun C. Lin, Chaofen Sun, and
Maosong Sun. 1991. A maximal matching auto-
matic Chinese word segmentation algorithm us-
ing corpus tagging for ambiguity resolution. In
ROCLING-IV, pages 17-23.
Nanyuan Liang and Yuan Liu. 1985. The OM
method of automatic word segmentation. Chinese
Information, 1(2). (In Chinese).
Nanyuan Liang. 1984. Automatic word segmenta-
tion for written Chinese and the segmentation
system CDWS. Journal of Beijing University of
Aeronautics and Astronautics, ?(4). (In Chinese).
Nanyuan Liang. 1986. CDWS - an automatic word
segmentation system for written Chinese. Jour-
nal of Chinese Information Processing, 1(2):44-
52. (In Chinese).
Nanyuan Liang. 1989. Knowledge for Chinese word
segmentation. Journal of Chinese Information
Processing, 4(2):29-33. (In Chinese).
Yuan Liu and Nanyuan Liang. 1986. Basic engi-
neering for Chinese processing - Modern Chinese
word frequency counting. Journal of Chinese In-
formation Processing, 1(1):17-23. (In Chinese).
Yuan Liu, Qiang Tan, and Xukun Shen. 1994. Con-
temporary Chinese Word Segmentation Standard
Used for Information Processing, and Automatic
Word Segmentation Methods. Tsinghua Univer-
sity Press, BeJing. (In Chinese).
Hong I Ng and Kim Teng Lua. (forthcoming). A
word finding automation for Chinese sentence to-
kenization. Submitted to ACM Transaction of
Asian Languages Processing.
David Palmer and J. Burger. 1997. Chinese word
segmentation and information retrieval. In AAAI
Spring Symposium on Cross-Language Text and
Speech Retrieval.
David Palmer. 1997. A trainable rule-based algo-
rithm for word segmentation. In ACL-9&apos;1, pages
321-328, Madrid.
David D. Palmer. 2000. Tokenization and sen-
tence segmentation. In R. Dale, H. Moisl, and
H. Somers, editors, Handbook of Natural Lan-
guage Processing, pages 11-35. Marcel Dekker,
New York.
Fuchun Peng and Dale Schuurmans. 2001. Self-
supervised Chinese word segmentation. In 4th In-
ternational Symposium of Intelligent Data Anal-
ysis, pages 238-247.
Jay M. Pont and W. Bruce Croft. 1996. USeg: A
retargetable word segmentation procedure for in-
formation retrieval. In Symposium on Document
Analysis and Information Retrieval &apos; 96 (SDAIR).
UMass Technical Report TR96-2, Univ. of Mass.,
Amherst, MA.
R. Sproat, C. Shih, W. Gale, and N. Chang. 1996.
A stochastic finite-state word-segmentation al-
gorithm for Chinese. Computational Linguistics,
22(3):377-404.
Mark Stevenson and Yorick A. Wilks. 2001.
The interaction of knowledge sources in word
sense disambiguation. Computational Linguistics,
27(3):321-349.
Maosong Sun and BenJamin K. T&apos;sou. 1995. Am-
biguity resolution in Chinese word segmentation.
In BenJamin K. T&apos;sou and Tom B. Y. Lai, editors,
PACLIC-10, Hong Kong, December 27-28.
Maosong Sun and Zhengping Zhou. 1998. Word seg-
mentation ambiguityin Chinese texts. In BenJi-
amin K. T&apos;sou, Tom B. Y. Lai, Samuel W. K.
Chan, and Williams S-Y. Wang, editors, Quanti-
tative and Computational Studies on the Chinese
Language, pages 323-338. Language Information
Sciences Research Centre, City University of Hong
Kong.
W. J. Teahan, Yingying Wen, Rodger J. McNab,
and Ian H. Witten. 2000. A compression-based
algorithm for Chinese word segmentation. Com-
putational Linguistics, 26(3):375-393.
J. Veenstra, A. Van den Bosch, S. Buchholz,
W. Daelemans, and J. Zavrel. 2000. Memory-
based word sense disambiguation. Computing and
the Humanities, special issue on SENSEVAL,
34(1-2 y).
Anand Venkataraman. 2001. A statistical model for
word discoveryin transcribed speech. Computa-
tional Linguistics, 27(3):353-372.
Jonathan J. Webster and Chunyu Kit. 1992a. To-
kenization as the initial phase in NLP. In COL-
ING&apos;92, pages 1106-1110, Nantes, France, July
23-28.
Jonathan J. Webster and Chunyu Kit. 1992b. To-
kenization for machine translation: What can be
learned from Chinese word identification. In Proc.
of 3rd International Conference on Chinese Infor-
mation Processing,, BeiJing.
Zimin Wu and Gwyneth Tseng. 1993. Chinese text
segmentation for text retrieval: achievements and
problems. JASIS, 44(9):532-542.
Shiwen Yu. 1998. Knowledge Base of Grammati-
cal Information for Contemporary Chinese. Ts-
inghua University Press, BeJing. (In Chinese).
Jakub Zavrel, Walter Daelemans, and Jorn Veen-
stra. 1998. Resolving PP attachment ambiguities
with memory-based learning. In T. Mark Ellison,
editor, CoNLL9&apos;1. Computational Natural Lan-
guage Learning, pages 136-144, Somerset, New
Jersey.
Guodong Zhou and Kim Teng Lua. (forthcoming).
A hybrid approach toward ambiguity resolution
in segmenting Chinese sentences. Submitted to
Computer Processing of Oriental Languages.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.804049">
<title confidence="0.999744">Learning Case-based Knowledge for</title>
<author confidence="0.937072">Chinese Word Segmentation A preliminary study</author>
<affiliation confidence="0.9543015">of Chinese, Translation &amp; City University of Hong of Foreign Trade &amp; Cooperation of Guangdong Province,</affiliation>
<email confidence="0.995825">drhbchen@21cn.com</email>
<abstract confidence="0.9960138125">Just like other NLP applications, a serious problem with Chinese word segmentation lies in the ambiguities involved. Disambiguation methods fall into different categories, e.g., rule-based, statistical-based and example-based approaches, each of which may involve a variety of machine learning techniques. In this paper we report our current progress within the example-based approach, including its framework, example representation and collection, example matching and application. Experimental results show that this effective approach resolves more than 90% of ambiguities found. Hence, if it is integrated effectively with a segmentation method of the preci- &gt; the resulting segmentation accuracy can reach, theoretically, beyond 99.5%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M R Brent</author>
</authors>
<title>An efficient, probabilistically sound algorithm for segmentation and word discovery.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--71</pages>
<contexts>
<context position="5855" citStr="Brent, 1999" startWordPosition="886" endWordPosition="887"> dictionary (Chang et al., 1995; Pont and Croft, 1996; Chang and Su, 1997); 2. Investigation into the nature and statistics of ambiguities (Sun and Zhou, 1998); 3. Unsupervised learning of words ( Ge et al., 1999; Peng and Schuurmans, 2001)3; 4. Disambiguation with different approaches (Liang, 1989; Jin, 1994; Sun and T&apos;sou, 1995) The work reported in this paper belongs to the last category, taking an instance-based learning 3Recent research in this direction appears to be closely related to the studies on computational lexical acquisition of other languages such as English (de Marcken, 1996; Brent, 1999; Kit and Wilks, 1999; Kit, 2000; Venkataraman, 2001) and to language modeling technology (Jelinek, 1997), typically involving a version of the EM algorithm (Dempster et al., 1977). approach, aimed to examine its prospects of disambiguation. The rest of the paper is organized as follows. Section 2 briefly introduces the ambiguity problem and existing ambiguity detection strategies. Section 3 defines the notion and representation of examples, and formulates a similarity measure between an ambiguous input and an example. We present our disambiguation algorithm in Section 4 and experimental resul</context>
</contexts>
<marker>Brent, 1999</marker>
<rawString>M. R. Brent. 1999. An efficient, probabilistically sound algorithm for segmentation and word discovery. Machine Learning, 34:71-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing-Shin Chang</author>
<author>Keh-Yih Su</author>
</authors>
<title>An unsupervised iterative method for Chinese new lexicon extraction.</title>
<date>1997</date>
<journal>International Journal of Computational Linguistics &amp; Chinese Language Processing,</journal>
<pages>1--1</pages>
<contexts>
<context position="5318" citStr="Chang and Su, 1997" startWordPosition="800" endWordPosition="803">hes that incorporate various techniques of statistical NLP and machine learning, e.g., transformation-based error-driven learning (Palmer, 1997; Hockenmaier and Brew, 1998) and compression-based algorithm (Teahan et al., 2000). Recent research shifts its focus onto the following aspects, resorting to a variety of resources and techniques, in particular, machine learning techniques: 1. Lexical resource acquisition, including compilation and automatic detection of high-tech terms and unknown words like names, to complement a never-big-enough dictionary (Chang et al., 1995; Pont and Croft, 1996; Chang and Su, 1997); 2. Investigation into the nature and statistics of ambiguities (Sun and Zhou, 1998); 3. Unsupervised learning of words ( Ge et al., 1999; Peng and Schuurmans, 2001)3; 4. Disambiguation with different approaches (Liang, 1989; Jin, 1994; Sun and T&apos;sou, 1995) The work reported in this paper belongs to the last category, taking an instance-based learning 3Recent research in this direction appears to be closely related to the studies on computational lexical acquisition of other languages such as English (de Marcken, 1996; Brent, 1999; Kit and Wilks, 1999; Kit, 2000; Venkataraman, 2001) and to la</context>
</contexts>
<marker>Chang, Su, 1997</marker>
<rawString>Jing-Shin Chang and Keh-Yih Su. 1997. An unsupervised iterative method for Chinese new lexicon extraction. International Journal of Computational Linguistics &amp; Chinese Language Processing, 1(1):101-157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jyun-Sheng Chang</author>
<author>C-D Chen</author>
<author>S-D Chen</author>
</authors>
<title>Chinese word segmentation through constraint satisfaction and statistical optimization.</title>
<date>1991</date>
<booktitle>In ROCLING-IV,</booktitle>
<pages>147--165</pages>
<institution>National ChiaoTung University,</institution>
<location>Hsinchu, Taiwan.</location>
<contexts>
<context position="4068" citStr="Chang et al. (1991)" startWordPosition="615" endWordPosition="618">e coincidence of fewer words with a greater probability can be illustrated as follows: given a string s, the probability of its most probable segmentation seg(s) in terms of a given language model is prob(seg(s)) = where prob(wil•) is some conditional probability in the model. Since all prob(wil•) &lt; 1.0, this probability becomes smaller for a greater n. Clearly, it looks more straightforward in an equi-probability setting. prob(wiI•) max 8=WlW2 ...W� Statistical approaches involve language models, mostly finite-state ones, trained on some large-scale corpora, as showed in Fan and Tsai (1988), Chang et al. (1991), Chiang et al. (1992), Sproat et al. (1996), Pont and Croft (1996) and Ng and Lua (forthcoming). These approaches do not provide any explicit strategy for disambiguation, but they get more ambiguous chunks correctly segmented than MMs by virtue of probability. Other linguistic resources or computational processes can also be integrated for further improvement, e.g., Lai et al. (1991) attempts to integrate POS tagging with word segmentation for the enhancement of accuracy and Gan et al. (1997) integrates word boundary disambiguation into sentence processing within a probabilistic emergent mode</context>
</contexts>
<marker>Chang, Chen, Chen, 1991</marker>
<rawString>Jyun-Sheng Chang, C.-D. Chen, and S.-D. Chen. 1991. Chinese word segmentation through constraint satisfaction and statistical optimization. In ROCLING-IV, pages 147-165, National ChiaoTung University, Hsinchu, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing-Shin Chang</author>
<author>Yi-Chung Lin</author>
<author>Keh-Yih Su</author>
</authors>
<title>Automatic construction of a Chinese electronic dictionary.</title>
<date>1995</date>
<booktitle>In David Yarovsky and Kenneth Church, editors, WVLC-3,</booktitle>
<pages>107--120</pages>
<location>Somerset, New Jersey,</location>
<contexts>
<context position="5275" citStr="Chang et al., 1995" startWordPosition="792" endWordPosition="795">ergent model. There are also other approaches that incorporate various techniques of statistical NLP and machine learning, e.g., transformation-based error-driven learning (Palmer, 1997; Hockenmaier and Brew, 1998) and compression-based algorithm (Teahan et al., 2000). Recent research shifts its focus onto the following aspects, resorting to a variety of resources and techniques, in particular, machine learning techniques: 1. Lexical resource acquisition, including compilation and automatic detection of high-tech terms and unknown words like names, to complement a never-big-enough dictionary (Chang et al., 1995; Pont and Croft, 1996; Chang and Su, 1997); 2. Investigation into the nature and statistics of ambiguities (Sun and Zhou, 1998); 3. Unsupervised learning of words ( Ge et al., 1999; Peng and Schuurmans, 2001)3; 4. Disambiguation with different approaches (Liang, 1989; Jin, 1994; Sun and T&apos;sou, 1995) The work reported in this paper belongs to the last category, taking an instance-based learning 3Recent research in this direction appears to be closely related to the studies on computational lexical acquisition of other languages such as English (de Marcken, 1996; Brent, 1999; Kit and Wilks, 199</context>
</contexts>
<marker>Chang, Lin, Su, 1995</marker>
<rawString>Jing-Shin Chang, Yi-Chung Lin, and Keh-Yih Su. 1995. Automatic construction of a Chinese electronic dictionary. In David Yarovsky and Kenneth Church, editors, WVLC-3, pages 107-120, Somerset, New Jersey, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Jiann Chen</author>
<author>Shing-Huan Liu</author>
</authors>
<title>Word identification for mandarin Chinese sentences.</title>
<date>1992</date>
<booktitle>In COLING&apos;92, volume I,</booktitle>
<volume>y</volume>
<pages>101--107</pages>
<location>Nantes, France,</location>
<marker>Chen, Liu, 1992</marker>
<rawString>Keh-Jiann Chen and Shing-Huan Liu. 1992. Word identification for mandarin Chinese sentences. In COLING&apos;92, volume I, pages 101-107, Nantes, France, Jul y 23-28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tung-Hui Chiang</author>
<author>Ming-Yu Lin</author>
<author>Keh-Yih Su</author>
</authors>
<title>Statistical models for word segmentation and unknown word resolution.</title>
<date>1992</date>
<booktitle>In ROCLING-V,</booktitle>
<pages>121--146</pages>
<contexts>
<context position="4090" citStr="Chiang et al. (1992)" startWordPosition="619" endWordPosition="622">r words with a greater probability can be illustrated as follows: given a string s, the probability of its most probable segmentation seg(s) in terms of a given language model is prob(seg(s)) = where prob(wil•) is some conditional probability in the model. Since all prob(wil•) &lt; 1.0, this probability becomes smaller for a greater n. Clearly, it looks more straightforward in an equi-probability setting. prob(wiI•) max 8=WlW2 ...W� Statistical approaches involve language models, mostly finite-state ones, trained on some large-scale corpora, as showed in Fan and Tsai (1988), Chang et al. (1991), Chiang et al. (1992), Sproat et al. (1996), Pont and Croft (1996) and Ng and Lua (forthcoming). These approaches do not provide any explicit strategy for disambiguation, but they get more ambiguous chunks correctly segmented than MMs by virtue of probability. Other linguistic resources or computational processes can also be integrated for further improvement, e.g., Lai et al. (1991) attempts to integrate POS tagging with word segmentation for the enhancement of accuracy and Gan et al. (1997) integrates word boundary disambiguation into sentence processing within a probabilistic emergent model. There are also othe</context>
</contexts>
<marker>Chiang, Lin, Su, 1992</marker>
<rawString>Tung-Hui Chiang, Ming-Yu Lin, and Keh-Yih Su. 1992. Statistical models for word segmentation and unknown word resolution. In ROCLING-V, pages 121-146, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>S Buchholz</author>
<author>J Veenstra</author>
</authors>
<title>Memory-based shallow parsing.</title>
<date>1999</date>
<booktitle>In CoNLL-99,</booktitle>
<pages>53--60</pages>
<location>Bergen,</location>
<contexts>
<context position="9549" citStr="Daelemans et al., 1999" startWordPosition="1473" endWordPosition="1476">mples and similarity measure We intend to disambiguate Chinese word segmentation ambiguities within the framework of case-based learning. This supervised learning approach, also labeled as memory-based, instance-based or example-based learning, has been popular for various NLP applications in recent years, e.g., the TiMBL learner (Daelemans et al., 2001). TiMBL is developed as a general memory-based learning environment to integrate a set of learning algorithms. It has been widely applied to disambiguating a variety of NLP tasks, including PP attachment (Zavrel et al., 1998), shallow parsing (Daelemans et al., 1999) and WSD (Veenstra et al., 2000; Stevenson and Wilks, 2001). In this paper, the general principle of case-based learning is followed but the formulation below is nevertheless specific to our problem. An example here is defined as a quadruple &lt; Cl, e, Cr, S &gt;, where the strings Cl and Cr are the left and right contexts within which the ambiguous string e appears, and S is the correct segmentation of e within the particular context. If denoting the quadruple as E, we also refer to S as seg(E) or seg(e), interchangeably. The distance, or similarity, between an example E and a given triple A =&lt;Q1 </context>
</contexts>
<marker>Daelemans, Buchholz, Veenstra, 1999</marker>
<rawString>W. Daelemans, S. Buchholz, and J. Veenstra. 1999. Memory-based shallow parsing. In CoNLL-99, pages 53-60, Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Ko van der Sloot</author>
<author>Antal van den Bosch</author>
</authors>
<title>Timbl: Tilburg memor y based learner, version 4.0, reference guide.</title>
<date>2001</date>
<tech>Technical Report ILK Technical Report 01-04,</tech>
<institution>Induction of Linguistic Knowledge, Tilburg University, The Netherlands.</institution>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 2001</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch. 2001. Timbl: Tilburg memor y based learner, version 4.0, reference guide. Technical Report ILK Technical Report 01-04, Induction of Linguistic Knowledge, Tilburg University, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C de Marcken</author>
</authors>
<title>Unsupervised Language Acquisition.</title>
<date>1996</date>
<booktitle>Ph.D. thesis, MIT,</booktitle>
<location>Cambridge, Mass.y.</location>
<marker>de Marcken, 1996</marker>
<rawString>C. de Marcken. 1996. Unsupervised Language Acquisition. Ph.D. thesis, MIT, Cambridge, Mass.y.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the em algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<pages>34--1</pages>
<contexts>
<context position="6035" citStr="Dempster et al., 1977" startWordPosition="912" endWordPosition="915">ervised learning of words ( Ge et al., 1999; Peng and Schuurmans, 2001)3; 4. Disambiguation with different approaches (Liang, 1989; Jin, 1994; Sun and T&apos;sou, 1995) The work reported in this paper belongs to the last category, taking an instance-based learning 3Recent research in this direction appears to be closely related to the studies on computational lexical acquisition of other languages such as English (de Marcken, 1996; Brent, 1999; Kit and Wilks, 1999; Kit, 2000; Venkataraman, 2001) and to language modeling technology (Jelinek, 1997), typically involving a version of the EM algorithm (Dempster et al., 1977). approach, aimed to examine its prospects of disambiguation. The rest of the paper is organized as follows. Section 2 briefly introduces the ambiguity problem and existing ambiguity detection strategies. Section 3 defines the notion and representation of examples, and formulates a similarity measure between an ambiguous input and an example. We present our disambiguation algorithm in Section 4 and experimental results and evaluation in Section 5, together with some discussion on the remaining errors, before concluding the paper in Section 6. 2 Ambiguity Conceptually there are two essential ty</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B.Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society, Series B, 34:1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charng-Kang Fan</author>
<author>Wen-Hsiang Tsai</author>
</authors>
<title>Automatic word identification in Chinese sentences by the relaxation technique.</title>
<date>1988</date>
<booktitle>Computer Processing of Chinese and Oriental Languages,</booktitle>
<pages>4--1</pages>
<contexts>
<context position="4047" citStr="Fan and Tsai (1988)" startWordPosition="611" endWordPosition="614">l, in this sense. 2The coincidence of fewer words with a greater probability can be illustrated as follows: given a string s, the probability of its most probable segmentation seg(s) in terms of a given language model is prob(seg(s)) = where prob(wil•) is some conditional probability in the model. Since all prob(wil•) &lt; 1.0, this probability becomes smaller for a greater n. Clearly, it looks more straightforward in an equi-probability setting. prob(wiI•) max 8=WlW2 ...W� Statistical approaches involve language models, mostly finite-state ones, trained on some large-scale corpora, as showed in Fan and Tsai (1988), Chang et al. (1991), Chiang et al. (1992), Sproat et al. (1996), Pont and Croft (1996) and Ng and Lua (forthcoming). These approaches do not provide any explicit strategy for disambiguation, but they get more ambiguous chunks correctly segmented than MMs by virtue of probability. Other linguistic resources or computational processes can also be integrated for further improvement, e.g., Lai et al. (1991) attempts to integrate POS tagging with word segmentation for the enhancement of accuracy and Gan et al. (1997) integrates word boundary disambiguation into sentence processing within a probab</context>
</contexts>
<marker>Fan, Tsai, 1988</marker>
<rawString>Charng-Kang Fan and Wen-Hsiang Tsai. 1988. Automatic word identification in Chinese sentences by the relaxation technique. Computer Processing of Chinese and Oriental Languages, 4(1):33-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kok-Wee Gan</author>
<author>Martha Palmer</author>
<author>Kim-Teng Lua</author>
</authors>
<title>A statisticall y emergent approach for language processing: Application to modeling effects in ambiguous Chinese word boundar y perception.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>22--4</pages>
<contexts>
<context position="4566" citStr="Gan et al. (1997)" startWordPosition="695" endWordPosition="698">s, mostly finite-state ones, trained on some large-scale corpora, as showed in Fan and Tsai (1988), Chang et al. (1991), Chiang et al. (1992), Sproat et al. (1996), Pont and Croft (1996) and Ng and Lua (forthcoming). These approaches do not provide any explicit strategy for disambiguation, but they get more ambiguous chunks correctly segmented than MMs by virtue of probability. Other linguistic resources or computational processes can also be integrated for further improvement, e.g., Lai et al. (1991) attempts to integrate POS tagging with word segmentation for the enhancement of accuracy and Gan et al. (1997) integrates word boundary disambiguation into sentence processing within a probabilistic emergent model. There are also other approaches that incorporate various techniques of statistical NLP and machine learning, e.g., transformation-based error-driven learning (Palmer, 1997; Hockenmaier and Brew, 1998) and compression-based algorithm (Teahan et al., 2000). Recent research shifts its focus onto the following aspects, resorting to a variety of resources and techniques, in particular, machine learning techniques: 1. Lexical resource acquisition, including compilation and automatic detection of </context>
</contexts>
<marker>Gan, Palmer, Lua, 1997</marker>
<rawString>Kok-Wee Gan, Martha Palmer, and Kim-Teng Lua. 1997. A statisticall y emergent approach for language processing: Application to modeling effects in ambiguous Chinese word boundar y perception. Computational Linguistics, 22(4):531-553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianping Ge</author>
<author>Wanda Pratt</author>
<author>Padhraic Smyth</author>
</authors>
<title>Discovering Chinese words from unsegmented text (poster abstract).</title>
<date>1999</date>
<booktitle>In SIGIR&apos;99,</booktitle>
<pages>271--272</pages>
<location>Berkeley,</location>
<contexts>
<context position="5456" citStr="Ge et al., 1999" startWordPosition="823" endWordPosition="826">7; Hockenmaier and Brew, 1998) and compression-based algorithm (Teahan et al., 2000). Recent research shifts its focus onto the following aspects, resorting to a variety of resources and techniques, in particular, machine learning techniques: 1. Lexical resource acquisition, including compilation and automatic detection of high-tech terms and unknown words like names, to complement a never-big-enough dictionary (Chang et al., 1995; Pont and Croft, 1996; Chang and Su, 1997); 2. Investigation into the nature and statistics of ambiguities (Sun and Zhou, 1998); 3. Unsupervised learning of words ( Ge et al., 1999; Peng and Schuurmans, 2001)3; 4. Disambiguation with different approaches (Liang, 1989; Jin, 1994; Sun and T&apos;sou, 1995) The work reported in this paper belongs to the last category, taking an instance-based learning 3Recent research in this direction appears to be closely related to the studies on computational lexical acquisition of other languages such as English (de Marcken, 1996; Brent, 1999; Kit and Wilks, 1999; Kit, 2000; Venkataraman, 2001) and to language modeling technology (Jelinek, 1997), typically involving a version of the EM algorithm (Dempster et al., 1977). approach, aimed to </context>
</contexts>
<marker>Ge, Pratt, Smyth, 1999</marker>
<rawString>Xianping Ge, Wanda Pratt, and Padhraic Smyth. 1999. Discovering Chinese words from unsegmented text (poster abstract). In SIGIR&apos;99, pages 271-272, Berkeley, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor y Grefenstette</author>
<author>P Tapanainen</author>
</authors>
<title>What is a word, what is a sentence? Problems of tokenization.</title>
<date>1994</date>
<booktitle>In 3rd Conference on Computational Lexicography and Text Research, COMPLEX&apos;94,</booktitle>
<volume>y</volume>
<pages>7--10</pages>
<location>Budapest,</location>
<contexts>
<context position="1678" citStr="Grefenstette and Tapanainen, 1994" startWordPosition="244" endWordPosition="247"> accuracy can reach, theoretically, beyond 99.5%. 1 Introduction It has been nearly two decades since the early work of Chinese word segmentation (Liang, 1984; Liang and Liu, 1985; Liu and Liang, 1986; Liang, 1986) . Tokenization has been recognized as a widespread problem, rather than being unique to Chinese and other oriental languages. It is an initial or prerequisite phase of NLP for all languages, although the obscurity of the problem varies from language to language (Webster and Kit, 1992a; Palmer, 2000). Recent work on tokenization for European languages such as English is reported in (Grefenstette and Tapanainen, 1994; Grefenstette, 1999; Grefenstette et al., 2000), adopting a finite-state approach. However, identification of multi-word units such as proper names and technical terms in these languages is highly comparable to that of multi-character Chinese words: there are no delimiters available. So far, a great variety of segmentation strategies for Chinese with various linguistic resources have been explored, yielding a large volume of literature on both linguistic and computational sides, as listed in (Liu et al., 1994; Guo, 1997), among many others. In general, these strategies can be divided into two</context>
</contexts>
<marker>Grefenstette, Tapanainen, 1994</marker>
<rawString>Gregor y Grefenstette and P. Tapanainen. 1994. What is a word, what is a sentence? Problems of tokenization. In 3rd Conference on Computational Lexicography and Text Research, COMPLEX&apos;94, Budapest, Jul y 7-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor y Grefenstette</author>
<author>Anne Schiller</author>
<author>Salah AitMokhtar</author>
</authors>
<title>Recognizing lexical patterns in text.</title>
<date>2000</date>
<booktitle>Lexicon Development for Speech and Language Processing,</booktitle>
<pages>141--168</pages>
<editor>In F. van Eynde, D. Gibbon, and I. Schuurman, editors,</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="1726" citStr="Grefenstette et al., 2000" startWordPosition="251" endWordPosition="254">roduction It has been nearly two decades since the early work of Chinese word segmentation (Liang, 1984; Liang and Liu, 1985; Liu and Liang, 1986; Liang, 1986) . Tokenization has been recognized as a widespread problem, rather than being unique to Chinese and other oriental languages. It is an initial or prerequisite phase of NLP for all languages, although the obscurity of the problem varies from language to language (Webster and Kit, 1992a; Palmer, 2000). Recent work on tokenization for European languages such as English is reported in (Grefenstette and Tapanainen, 1994; Grefenstette, 1999; Grefenstette et al., 2000), adopting a finite-state approach. However, identification of multi-word units such as proper names and technical terms in these languages is highly comparable to that of multi-character Chinese words: there are no delimiters available. So far, a great variety of segmentation strategies for Chinese with various linguistic resources have been explored, yielding a large volume of literature on both linguistic and computational sides, as listed in (Liu et al., 1994; Guo, 1997), among many others. In general, these strategies can be divided into two camps, namely, dictionary-based and statistical</context>
</contexts>
<marker>Grefenstette, Schiller, AitMokhtar, 2000</marker>
<rawString>Gregor y Grefenstette, Anne Schiller, and Salah AitMokhtar. 2000. Recognizing lexical patterns in text. In F. van Eynde, D. Gibbon, and I. Schuurman, editors, Lexicon Development for Speech and Language Processing, pages 141-168. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor y Grefenstette</author>
</authors>
<date>1999</date>
<booktitle>Syntactic Wordclass Tagging,</booktitle>
<pages>117--133</pages>
<editor>Tokenization. In Hans van Halteren, editor,</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="1698" citStr="Grefenstette, 1999" startWordPosition="248" endWordPosition="250"> beyond 99.5%. 1 Introduction It has been nearly two decades since the early work of Chinese word segmentation (Liang, 1984; Liang and Liu, 1985; Liu and Liang, 1986; Liang, 1986) . Tokenization has been recognized as a widespread problem, rather than being unique to Chinese and other oriental languages. It is an initial or prerequisite phase of NLP for all languages, although the obscurity of the problem varies from language to language (Webster and Kit, 1992a; Palmer, 2000). Recent work on tokenization for European languages such as English is reported in (Grefenstette and Tapanainen, 1994; Grefenstette, 1999; Grefenstette et al., 2000), adopting a finite-state approach. However, identification of multi-word units such as proper names and technical terms in these languages is highly comparable to that of multi-character Chinese words: there are no delimiters available. So far, a great variety of segmentation strategies for Chinese with various linguistic resources have been explored, yielding a large volume of literature on both linguistic and computational sides, as listed in (Liu et al., 1994; Guo, 1997), among many others. In general, these strategies can be divided into two camps, namely, dict</context>
</contexts>
<marker>Grefenstette, 1999</marker>
<rawString>Gregor y Grefenstette. 1999. Tokenization. In Hans van Halteren, editor, Syntactic Wordclass Tagging, pages 117-133. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yingchun Guan</author>
<author>Bei Qin</author>
</authors>
<title>The design and implementation of a Chinese word statistical system.</title>
<date>1986</date>
<journal>Journal of Chinese Information Processing,</journal>
<pages>1--1</pages>
<note>(In Chinese).</note>
<marker>Guan, Qin, 1986</marker>
<rawString>Yingchun Guan and Bei Qin. 1986. The design and implementation of a Chinese word statistical system. Journal of Chinese Information Processing, 1(1):26-32. (In Chinese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Guo</author>
</authors>
<title>Critical tokenization and its properties.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--4</pages>
<contexts>
<context position="2205" citStr="Guo, 1997" startWordPosition="328" endWordPosition="329">uropean languages such as English is reported in (Grefenstette and Tapanainen, 1994; Grefenstette, 1999; Grefenstette et al., 2000), adopting a finite-state approach. However, identification of multi-word units such as proper names and technical terms in these languages is highly comparable to that of multi-character Chinese words: there are no delimiters available. So far, a great variety of segmentation strategies for Chinese with various linguistic resources have been explored, yielding a large volume of literature on both linguistic and computational sides, as listed in (Liu et al., 1994; Guo, 1997), among many others. In general, these strategies can be divided into two camps, namely, dictionary-based and statistical-based approaches. Nevertheless, the former can be understood as a restricted instance of the latter, with an equi-probability for each word in a given dictionaryl. Most, if not all, dictionary-based strategies are built upon a few basic &amp;quot;mechanical&amp;quot; segmentation methods based on string matching (Kit et al., 1989), among which the most applicable, thus widely used since the very beginning, are the two maximum matching methods (MMs), one scanning forward (FMM) and the other b</context>
</contexts>
<marker>Guo, 1997</marker>
<rawString>Jin Guo. 1997. Critical tokenization and its properties. Computational Linguistics, 23(4):569-596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hockenmaier</author>
<author>C Brew</author>
</authors>
<title>Error-driven learning of Chinese word segmentation.</title>
<date>1998</date>
<booktitle>In PACLIC-12,</booktitle>
<pages>218--229</pages>
<publisher>Society.</publisher>
<contexts>
<context position="4871" citStr="Hockenmaier and Brew, 1998" startWordPosition="734" endWordPosition="738">they get more ambiguous chunks correctly segmented than MMs by virtue of probability. Other linguistic resources or computational processes can also be integrated for further improvement, e.g., Lai et al. (1991) attempts to integrate POS tagging with word segmentation for the enhancement of accuracy and Gan et al. (1997) integrates word boundary disambiguation into sentence processing within a probabilistic emergent model. There are also other approaches that incorporate various techniques of statistical NLP and machine learning, e.g., transformation-based error-driven learning (Palmer, 1997; Hockenmaier and Brew, 1998) and compression-based algorithm (Teahan et al., 2000). Recent research shifts its focus onto the following aspects, resorting to a variety of resources and techniques, in particular, machine learning techniques: 1. Lexical resource acquisition, including compilation and automatic detection of high-tech terms and unknown words like names, to complement a never-big-enough dictionary (Chang et al., 1995; Pont and Croft, 1996; Chang and Su, 1997); 2. Investigation into the nature and statistics of ambiguities (Sun and Zhou, 1998); 3. Unsupervised learning of words ( Ge et al., 1999; Peng and Schu</context>
</contexts>
<marker>Hockenmaier, Brew, 1998</marker>
<rawString>J. Hockenmaier and C. Brew. 1998. Error-driven learning of Chinese word segmentation. In PACLIC-12, pages 218-229, Singapore. Chinese and Oriental Languages Processing Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Statistical Methods for Speech Processing.</title>
<date>1997</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5960" citStr="Jelinek, 1997" startWordPosition="902" endWordPosition="903">nature and statistics of ambiguities (Sun and Zhou, 1998); 3. Unsupervised learning of words ( Ge et al., 1999; Peng and Schuurmans, 2001)3; 4. Disambiguation with different approaches (Liang, 1989; Jin, 1994; Sun and T&apos;sou, 1995) The work reported in this paper belongs to the last category, taking an instance-based learning 3Recent research in this direction appears to be closely related to the studies on computational lexical acquisition of other languages such as English (de Marcken, 1996; Brent, 1999; Kit and Wilks, 1999; Kit, 2000; Venkataraman, 2001) and to language modeling technology (Jelinek, 1997), typically involving a version of the EM algorithm (Dempster et al., 1977). approach, aimed to examine its prospects of disambiguation. The rest of the paper is organized as follows. Section 2 briefly introduces the ambiguity problem and existing ambiguity detection strategies. Section 3 defines the notion and representation of examples, and formulates a similarity measure between an ambiguous input and an example. We present our disambiguation algorithm in Section 4 and experimental results and evaluation in Section 5, together with some discussion on the remaining errors, before concluding </context>
</contexts>
<marker>Jelinek, 1997</marker>
<rawString>F. Jelinek. 1997. Statistical Methods for Speech Processing. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanying Jin</author>
</authors>
<title>A case study: Chinese segmentation and its disambiguation.</title>
<date>1992</date>
<tech>Technical Report MCCS-92-227,</tech>
<institution>Computing Research Laboratory, New Mexico State University, Las Cruces.</institution>
<marker>Jin, 1992</marker>
<rawString>Wanying Jin. 1992. A case study: Chinese segmentation and its disambiguation. Technical Report MCCS-92-227, Computing Research Laboratory, New Mexico State University, Las Cruces.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanying Jin</author>
</authors>
<title>Chinese segmentation disambiguation.</title>
<date>1994</date>
<booktitle>In COLING-94,</booktitle>
<pages>1245--1249</pages>
<contexts>
<context position="5554" citStr="Jin, 1994" startWordPosition="838" endWordPosition="839">ifts its focus onto the following aspects, resorting to a variety of resources and techniques, in particular, machine learning techniques: 1. Lexical resource acquisition, including compilation and automatic detection of high-tech terms and unknown words like names, to complement a never-big-enough dictionary (Chang et al., 1995; Pont and Croft, 1996; Chang and Su, 1997); 2. Investigation into the nature and statistics of ambiguities (Sun and Zhou, 1998); 3. Unsupervised learning of words ( Ge et al., 1999; Peng and Schuurmans, 2001)3; 4. Disambiguation with different approaches (Liang, 1989; Jin, 1994; Sun and T&apos;sou, 1995) The work reported in this paper belongs to the last category, taking an instance-based learning 3Recent research in this direction appears to be closely related to the studies on computational lexical acquisition of other languages such as English (de Marcken, 1996; Brent, 1999; Kit and Wilks, 1999; Kit, 2000; Venkataraman, 2001) and to language modeling technology (Jelinek, 1997), typically involving a version of the EM algorithm (Dempster et al., 1977). approach, aimed to examine its prospects of disambiguation. The rest of the paper is organized as follows. Section 2 </context>
</contexts>
<marker>Jin, 1994</marker>
<rawString>Wanying Jin. 1994. Chinese segmentation disambiguation. In COLING-94, pages 1245-1249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chunyu Kit</author>
<author>Yorick Wilks</author>
</authors>
<title>Unsupervised learning of word boundar y with description length gain.</title>
<date>1999</date>
<booktitle>CoNLL-99,</booktitle>
<pages>1--6</pages>
<editor>In M. Osborne and E. T. K. Sang, editors,</editor>
<location>Bergen,</location>
<contexts>
<context position="5876" citStr="Kit and Wilks, 1999" startWordPosition="888" endWordPosition="891">Chang et al., 1995; Pont and Croft, 1996; Chang and Su, 1997); 2. Investigation into the nature and statistics of ambiguities (Sun and Zhou, 1998); 3. Unsupervised learning of words ( Ge et al., 1999; Peng and Schuurmans, 2001)3; 4. Disambiguation with different approaches (Liang, 1989; Jin, 1994; Sun and T&apos;sou, 1995) The work reported in this paper belongs to the last category, taking an instance-based learning 3Recent research in this direction appears to be closely related to the studies on computational lexical acquisition of other languages such as English (de Marcken, 1996; Brent, 1999; Kit and Wilks, 1999; Kit, 2000; Venkataraman, 2001) and to language modeling technology (Jelinek, 1997), typically involving a version of the EM algorithm (Dempster et al., 1977). approach, aimed to examine its prospects of disambiguation. The rest of the paper is organized as follows. Section 2 briefly introduces the ambiguity problem and existing ambiguity detection strategies. Section 3 defines the notion and representation of examples, and formulates a similarity measure between an ambiguous input and an example. We present our disambiguation algorithm in Section 4 and experimental results and evaluation in </context>
</contexts>
<marker>Kit, Wilks, 1999</marker>
<rawString>Chunyu Kit and Yorick Wilks. 1999. Unsupervised learning of word boundar y with description length gain. In M. Osborne and E. T. K. Sang, editors, CoNLL-99, pages 1-6, Bergen, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chunyu Kit</author>
<author>Yuan Liu</author>
<author>Nanyuan Liang</author>
</authors>
<title>On methods of Chinese automatic word segmentation.</title>
<date>1989</date>
<journal>Journal of Chinese Information Processing,</journal>
<pages>3--1</pages>
<note>(In Chinese).</note>
<contexts>
<context position="2641" citStr="Kit et al., 1989" startWordPosition="392" endWordPosition="395">se with various linguistic resources have been explored, yielding a large volume of literature on both linguistic and computational sides, as listed in (Liu et al., 1994; Guo, 1997), among many others. In general, these strategies can be divided into two camps, namely, dictionary-based and statistical-based approaches. Nevertheless, the former can be understood as a restricted instance of the latter, with an equi-probability for each word in a given dictionaryl. Most, if not all, dictionary-based strategies are built upon a few basic &amp;quot;mechanical&amp;quot; segmentation methods based on string matching (Kit et al., 1989), among which the most applicable, thus widely used since the very beginning, are the two maximum matching methods (MMs), one scanning forward (FMM) and the other backward (BMM). Interestingly, their performance, frequently used as the baseline for evaluation, is never too far away from the stateof-the-art approaches in terms of segmentation accuracy. Although performing little statistical computation, the MMs comply, in general, with the essential principle of the statistical-based approaches: select a segmentation as probable as possible among all choices. This ad hoc way of choosing the seg</context>
</contexts>
<marker>Kit, Liu, Liang, 1989</marker>
<rawString>Chunyu Kit, Yuan Liu, and Nanyuan Liang. 1989. On methods of Chinese automatic word segmentation. Journal of Chinese Information Processing, 3(1):1-32. (In Chinese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chunyu Kit</author>
</authors>
<title>Unsupervised Lexical Learning as Inductive Inference.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Sheffield, UK.</institution>
<contexts>
<context position="5887" citStr="Kit, 2000" startWordPosition="892" endWordPosition="893">ont and Croft, 1996; Chang and Su, 1997); 2. Investigation into the nature and statistics of ambiguities (Sun and Zhou, 1998); 3. Unsupervised learning of words ( Ge et al., 1999; Peng and Schuurmans, 2001)3; 4. Disambiguation with different approaches (Liang, 1989; Jin, 1994; Sun and T&apos;sou, 1995) The work reported in this paper belongs to the last category, taking an instance-based learning 3Recent research in this direction appears to be closely related to the studies on computational lexical acquisition of other languages such as English (de Marcken, 1996; Brent, 1999; Kit and Wilks, 1999; Kit, 2000; Venkataraman, 2001) and to language modeling technology (Jelinek, 1997), typically involving a version of the EM algorithm (Dempster et al., 1977). approach, aimed to examine its prospects of disambiguation. The rest of the paper is organized as follows. Section 2 briefly introduces the ambiguity problem and existing ambiguity detection strategies. Section 3 defines the notion and representation of examples, and formulates a similarity measure between an ambiguous input and an example. We present our disambiguation algorithm in Section 4 and experimental results and evaluation in Section 5, </context>
</contexts>
<marker>Kit, 2000</marker>
<rawString>Chunyu Kit. 2000. Unsupervised Lexical Learning as Inductive Inference. Ph.D. thesis, University of Sheffield, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom B Y Lai</author>
<author>Sun C Lin</author>
<author>Chaofen Sun</author>
<author>Maosong Sun</author>
</authors>
<title>A maximal matching automatic Chinese word segmentation algorithm using corpus tagging for ambiguity resolution. In</title>
<date>1991</date>
<booktitle>ROCLING-IV,</booktitle>
<pages>17--23</pages>
<contexts>
<context position="4455" citStr="Lai et al. (1991)" startWordPosition="677" endWordPosition="680">rward in an equi-probability setting. prob(wiI•) max 8=WlW2 ...W� Statistical approaches involve language models, mostly finite-state ones, trained on some large-scale corpora, as showed in Fan and Tsai (1988), Chang et al. (1991), Chiang et al. (1992), Sproat et al. (1996), Pont and Croft (1996) and Ng and Lua (forthcoming). These approaches do not provide any explicit strategy for disambiguation, but they get more ambiguous chunks correctly segmented than MMs by virtue of probability. Other linguistic resources or computational processes can also be integrated for further improvement, e.g., Lai et al. (1991) attempts to integrate POS tagging with word segmentation for the enhancement of accuracy and Gan et al. (1997) integrates word boundary disambiguation into sentence processing within a probabilistic emergent model. There are also other approaches that incorporate various techniques of statistical NLP and machine learning, e.g., transformation-based error-driven learning (Palmer, 1997; Hockenmaier and Brew, 1998) and compression-based algorithm (Teahan et al., 2000). Recent research shifts its focus onto the following aspects, resorting to a variety of resources and techniques, in particular, </context>
</contexts>
<marker>Lai, Lin, Sun, Sun, 1991</marker>
<rawString>Tom B. Y. Lai, Sun C. Lin, Chaofen Sun, and Maosong Sun. 1991. A maximal matching automatic Chinese word segmentation algorithm using corpus tagging for ambiguity resolution. In ROCLING-IV, pages 17-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nanyuan Liang</author>
<author>Yuan Liu</author>
</authors>
<title>The OM method of automatic word segmentation.</title>
<date>1985</date>
<journal>Chinese Information,</journal>
<volume>1</volume>
<issue>2</issue>
<note>(In Chinese).</note>
<contexts>
<context position="1224" citStr="Liang and Liu, 1985" startWordPosition="168" endWordPosition="171">e a variety of machine learning techniques. In this paper we report our current progress within the example-based approach, including its framework, example representation and collection, example matching and application. Experimental results show that this effective approach resolves more than 90% of ambiguities found. Hence, if it is integrated effectively with a segmentation method of the precision P &gt; 95%, the resulting segmentation accuracy can reach, theoretically, beyond 99.5%. 1 Introduction It has been nearly two decades since the early work of Chinese word segmentation (Liang, 1984; Liang and Liu, 1985; Liu and Liang, 1986; Liang, 1986) . Tokenization has been recognized as a widespread problem, rather than being unique to Chinese and other oriental languages. It is an initial or prerequisite phase of NLP for all languages, although the obscurity of the problem varies from language to language (Webster and Kit, 1992a; Palmer, 2000). Recent work on tokenization for European languages such as English is reported in (Grefenstette and Tapanainen, 1994; Grefenstette, 1999; Grefenstette et al., 2000), adopting a finite-state approach. However, identification of multi-word units such as proper nam</context>
</contexts>
<marker>Liang, Liu, 1985</marker>
<rawString>Nanyuan Liang and Yuan Liu. 1985. The OM method of automatic word segmentation. Chinese Information, 1(2). (In Chinese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nanyuan Liang</author>
</authors>
<title>Automatic word segmentation for written Chinese and the segmentation system CDWS.</title>
<date>1984</date>
<journal>Journal of Beijing University of Aeronautics and Astronautics,</journal>
<volume>4</volume>
<note>(In Chinese).</note>
<contexts>
<context position="1203" citStr="Liang, 1984" startWordPosition="166" endWordPosition="167">ch may involve a variety of machine learning techniques. In this paper we report our current progress within the example-based approach, including its framework, example representation and collection, example matching and application. Experimental results show that this effective approach resolves more than 90% of ambiguities found. Hence, if it is integrated effectively with a segmentation method of the precision P &gt; 95%, the resulting segmentation accuracy can reach, theoretically, beyond 99.5%. 1 Introduction It has been nearly two decades since the early work of Chinese word segmentation (Liang, 1984; Liang and Liu, 1985; Liu and Liang, 1986; Liang, 1986) . Tokenization has been recognized as a widespread problem, rather than being unique to Chinese and other oriental languages. It is an initial or prerequisite phase of NLP for all languages, although the obscurity of the problem varies from language to language (Webster and Kit, 1992a; Palmer, 2000). Recent work on tokenization for European languages such as English is reported in (Grefenstette and Tapanainen, 1994; Grefenstette, 1999; Grefenstette et al., 2000), adopting a finite-state approach. However, identification of multi-word uni</context>
</contexts>
<marker>Liang, 1984</marker>
<rawString>Nanyuan Liang. 1984. Automatic word segmentation for written Chinese and the segmentation system CDWS. Journal of Beijing University of Aeronautics and Astronautics, ?(4). (In Chinese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nanyuan Liang</author>
</authors>
<title>CDWS - an automatic word segmentation system for written Chinese.</title>
<date>1986</date>
<journal>Journal of Chinese Information Processing,</journal>
<pages>1--2</pages>
<note>(In Chinese).</note>
<contexts>
<context position="1245" citStr="Liang, 1986" startWordPosition="174" endWordPosition="175">ng techniques. In this paper we report our current progress within the example-based approach, including its framework, example representation and collection, example matching and application. Experimental results show that this effective approach resolves more than 90% of ambiguities found. Hence, if it is integrated effectively with a segmentation method of the precision P &gt; 95%, the resulting segmentation accuracy can reach, theoretically, beyond 99.5%. 1 Introduction It has been nearly two decades since the early work of Chinese word segmentation (Liang, 1984; Liang and Liu, 1985; Liu and Liang, 1986; Liang, 1986) . Tokenization has been recognized as a widespread problem, rather than being unique to Chinese and other oriental languages. It is an initial or prerequisite phase of NLP for all languages, although the obscurity of the problem varies from language to language (Webster and Kit, 1992a; Palmer, 2000). Recent work on tokenization for European languages such as English is reported in (Grefenstette and Tapanainen, 1994; Grefenstette, 1999; Grefenstette et al., 2000), adopting a finite-state approach. However, identification of multi-word units such as proper names and technical term</context>
</contexts>
<marker>Liang, 1986</marker>
<rawString>Nanyuan Liang. 1986. CDWS - an automatic word segmentation system for written Chinese. Journal of Chinese Information Processing, 1(2):44-52. (In Chinese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nanyuan Liang</author>
</authors>
<title>Knowledge for Chinese word segmentation.</title>
<date>1989</date>
<journal>Journal of Chinese Information Processing,</journal>
<pages>4--2</pages>
<note>(In Chinese).</note>
<contexts>
<context position="5543" citStr="Liang, 1989" startWordPosition="836" endWordPosition="837">t research shifts its focus onto the following aspects, resorting to a variety of resources and techniques, in particular, machine learning techniques: 1. Lexical resource acquisition, including compilation and automatic detection of high-tech terms and unknown words like names, to complement a never-big-enough dictionary (Chang et al., 1995; Pont and Croft, 1996; Chang and Su, 1997); 2. Investigation into the nature and statistics of ambiguities (Sun and Zhou, 1998); 3. Unsupervised learning of words ( Ge et al., 1999; Peng and Schuurmans, 2001)3; 4. Disambiguation with different approaches (Liang, 1989; Jin, 1994; Sun and T&apos;sou, 1995) The work reported in this paper belongs to the last category, taking an instance-based learning 3Recent research in this direction appears to be closely related to the studies on computational lexical acquisition of other languages such as English (de Marcken, 1996; Brent, 1999; Kit and Wilks, 1999; Kit, 2000; Venkataraman, 2001) and to language modeling technology (Jelinek, 1997), typically involving a version of the EM algorithm (Dempster et al., 1977). approach, aimed to examine its prospects of disambiguation. The rest of the paper is organized as follows.</context>
</contexts>
<marker>Liang, 1989</marker>
<rawString>Nanyuan Liang. 1989. Knowledge for Chinese word segmentation. Journal of Chinese Information Processing, 4(2):29-33. (In Chinese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Liu</author>
<author>Nanyuan Liang</author>
</authors>
<title>Basic engineering for Chinese processing - Modern Chinese word frequency counting.</title>
<date>1986</date>
<journal>Journal of Chinese Information Processing,</journal>
<pages>1--1</pages>
<note>(In Chinese).</note>
<contexts>
<context position="1245" citStr="Liu and Liang, 1986" startWordPosition="172" endWordPosition="175">e learning techniques. In this paper we report our current progress within the example-based approach, including its framework, example representation and collection, example matching and application. Experimental results show that this effective approach resolves more than 90% of ambiguities found. Hence, if it is integrated effectively with a segmentation method of the precision P &gt; 95%, the resulting segmentation accuracy can reach, theoretically, beyond 99.5%. 1 Introduction It has been nearly two decades since the early work of Chinese word segmentation (Liang, 1984; Liang and Liu, 1985; Liu and Liang, 1986; Liang, 1986) . Tokenization has been recognized as a widespread problem, rather than being unique to Chinese and other oriental languages. It is an initial or prerequisite phase of NLP for all languages, although the obscurity of the problem varies from language to language (Webster and Kit, 1992a; Palmer, 2000). Recent work on tokenization for European languages such as English is reported in (Grefenstette and Tapanainen, 1994; Grefenstette, 1999; Grefenstette et al., 2000), adopting a finite-state approach. However, identification of multi-word units such as proper names and technical term</context>
</contexts>
<marker>Liu, Liang, 1986</marker>
<rawString>Yuan Liu and Nanyuan Liang. 1986. Basic engineering for Chinese processing - Modern Chinese word frequency counting. Journal of Chinese Information Processing, 1(1):17-23. (In Chinese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Liu</author>
<author>Qiang Tan</author>
<author>Xukun Shen</author>
</authors>
<title>Contemporary Chinese Word Segmentation Standard Used for Information Processing, and Automatic Word Segmentation Methods.</title>
<date>1994</date>
<publisher>Tsinghua University Press,</publisher>
<location>BeJing. (In Chinese).</location>
<contexts>
<context position="2193" citStr="Liu et al., 1994" startWordPosition="324" endWordPosition="327">tokenization for European languages such as English is reported in (Grefenstette and Tapanainen, 1994; Grefenstette, 1999; Grefenstette et al., 2000), adopting a finite-state approach. However, identification of multi-word units such as proper names and technical terms in these languages is highly comparable to that of multi-character Chinese words: there are no delimiters available. So far, a great variety of segmentation strategies for Chinese with various linguistic resources have been explored, yielding a large volume of literature on both linguistic and computational sides, as listed in (Liu et al., 1994; Guo, 1997), among many others. In general, these strategies can be divided into two camps, namely, dictionary-based and statistical-based approaches. Nevertheless, the former can be understood as a restricted instance of the latter, with an equi-probability for each word in a given dictionaryl. Most, if not all, dictionary-based strategies are built upon a few basic &amp;quot;mechanical&amp;quot; segmentation methods based on string matching (Kit et al., 1989), among which the most applicable, thus widely used since the very beginning, are the two maximum matching methods (MMs), one scanning forward (FMM) and</context>
<context position="12868" citStr="Liu et al. (1994)" startWordPosition="2071" endWordPosition="2074">his section we present the algorithms for these purposes. 4.1 Ambiguity detection We take a conventional approach to ambiguity detection, by detecting the discrepancies of the outputs from the FMM and BMM segmentations. Given an input corpus C, it can be realized, plainly, by the following algorithm: Ambiguity detection algorithm: ambd(C) 1. 97= FMM(C) and B = BMM(C) 2. Return diff(g7, 8) where FMM(•) and FMM(•) return the FMM and BMM segmentations of C, and diff(•, •) returns the discrepancies of the two segmentations. The dictionary used to support the MMs is a merger of the word lists from Liu et al. (1994) and Yu (1998), consisting of 53K entries. It is a medium-sized dictionary. With regards to the dictionary size and the weakness of the ambiguity detection algorithm, we keep ourselves alert of the fact that there are a certain number of ambiguities that are not detected by our program. And the resolutions for the ambiguous strings so detected are manually prepared, by selecting an answer from the outputs of the MMs in use. 4.2 Disambiguation Given an example base E and a text corpus C as testing data, the disambiguation algorithm works along the following steps: Disambiguation algorithm: disa</context>
</contexts>
<marker>Liu, Tan, Shen, 1994</marker>
<rawString>Yuan Liu, Qiang Tan, and Xukun Shen. 1994. Contemporary Chinese Word Segmentation Standard Used for Information Processing, and Automatic Word Segmentation Methods. Tsinghua University Press, BeJing. (In Chinese).</rawString>
</citation>
<citation valid="false">
<title>A word finding automation for Chinese sentence tokenization.</title>
<journal>ACM Transaction of Asian Languages Processing.</journal>
<note>Submitted to</note>
<marker></marker>
<rawString>Hong I Ng and Kim Teng Lua. (forthcoming). A word finding automation for Chinese sentence tokenization. Submitted to ACM Transaction of Asian Languages Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Palmer</author>
<author>J Burger</author>
</authors>
<title>Chinese word segmentation and information retrieval.</title>
<date>1997</date>
<booktitle>In AAAI Spring Symposium on Cross-Language Text and Speech Retrieval.</booktitle>
<marker>Palmer, Burger, 1997</marker>
<rawString>David Palmer and J. Burger. 1997. Chinese word segmentation and information retrieval. In AAAI Spring Symposium on Cross-Language Text and Speech Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Palmer</author>
</authors>
<title>A trainable rule-based algorithm for word segmentation. In</title>
<date>1997</date>
<booktitle>ACL-9&apos;1,</booktitle>
<pages>321--328</pages>
<location>Madrid.</location>
<contexts>
<context position="4842" citStr="Palmer, 1997" startWordPosition="732" endWordPosition="733">iguation, but they get more ambiguous chunks correctly segmented than MMs by virtue of probability. Other linguistic resources or computational processes can also be integrated for further improvement, e.g., Lai et al. (1991) attempts to integrate POS tagging with word segmentation for the enhancement of accuracy and Gan et al. (1997) integrates word boundary disambiguation into sentence processing within a probabilistic emergent model. There are also other approaches that incorporate various techniques of statistical NLP and machine learning, e.g., transformation-based error-driven learning (Palmer, 1997; Hockenmaier and Brew, 1998) and compression-based algorithm (Teahan et al., 2000). Recent research shifts its focus onto the following aspects, resorting to a variety of resources and techniques, in particular, machine learning techniques: 1. Lexical resource acquisition, including compilation and automatic detection of high-tech terms and unknown words like names, to complement a never-big-enough dictionary (Chang et al., 1995; Pont and Croft, 1996; Chang and Su, 1997); 2. Investigation into the nature and statistics of ambiguities (Sun and Zhou, 1998); 3. Unsupervised learning of words ( G</context>
</contexts>
<marker>Palmer, 1997</marker>
<rawString>David Palmer. 1997. A trainable rule-based algorithm for word segmentation. In ACL-9&apos;1, pages 321-328, Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Palmer</author>
</authors>
<title>Tokenization and sentence segmentation.</title>
<date>2000</date>
<booktitle>Handbook of Natural Language Processing,</booktitle>
<pages>11--35</pages>
<editor>In R. Dale, H. Moisl, and H. Somers, editors,</editor>
<publisher>Marcel Dekker,</publisher>
<location>New York.</location>
<contexts>
<context position="1560" citStr="Palmer, 2000" startWordPosition="228" endWordPosition="229">grated effectively with a segmentation method of the precision P &gt; 95%, the resulting segmentation accuracy can reach, theoretically, beyond 99.5%. 1 Introduction It has been nearly two decades since the early work of Chinese word segmentation (Liang, 1984; Liang and Liu, 1985; Liu and Liang, 1986; Liang, 1986) . Tokenization has been recognized as a widespread problem, rather than being unique to Chinese and other oriental languages. It is an initial or prerequisite phase of NLP for all languages, although the obscurity of the problem varies from language to language (Webster and Kit, 1992a; Palmer, 2000). Recent work on tokenization for European languages such as English is reported in (Grefenstette and Tapanainen, 1994; Grefenstette, 1999; Grefenstette et al., 2000), adopting a finite-state approach. However, identification of multi-word units such as proper names and technical terms in these languages is highly comparable to that of multi-character Chinese words: there are no delimiters available. So far, a great variety of segmentation strategies for Chinese with various linguistic resources have been explored, yielding a large volume of literature on both linguistic and computational side</context>
</contexts>
<marker>Palmer, 2000</marker>
<rawString>David D. Palmer. 2000. Tokenization and sentence segmentation. In R. Dale, H. Moisl, and H. Somers, editors, Handbook of Natural Language Processing, pages 11-35. Marcel Dekker, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Dale Schuurmans</author>
</authors>
<title>Selfsupervised Chinese word segmentation.</title>
<date>2001</date>
<booktitle>In 4th International Symposium of Intelligent Data Analysis,</booktitle>
<pages>238--247</pages>
<contexts>
<context position="5484" citStr="Peng and Schuurmans, 2001" startWordPosition="827" endWordPosition="830">d Brew, 1998) and compression-based algorithm (Teahan et al., 2000). Recent research shifts its focus onto the following aspects, resorting to a variety of resources and techniques, in particular, machine learning techniques: 1. Lexical resource acquisition, including compilation and automatic detection of high-tech terms and unknown words like names, to complement a never-big-enough dictionary (Chang et al., 1995; Pont and Croft, 1996; Chang and Su, 1997); 2. Investigation into the nature and statistics of ambiguities (Sun and Zhou, 1998); 3. Unsupervised learning of words ( Ge et al., 1999; Peng and Schuurmans, 2001)3; 4. Disambiguation with different approaches (Liang, 1989; Jin, 1994; Sun and T&apos;sou, 1995) The work reported in this paper belongs to the last category, taking an instance-based learning 3Recent research in this direction appears to be closely related to the studies on computational lexical acquisition of other languages such as English (de Marcken, 1996; Brent, 1999; Kit and Wilks, 1999; Kit, 2000; Venkataraman, 2001) and to language modeling technology (Jelinek, 1997), typically involving a version of the EM algorithm (Dempster et al., 1977). approach, aimed to examine its prospects of dis</context>
</contexts>
<marker>Peng, Schuurmans, 2001</marker>
<rawString>Fuchun Peng and Dale Schuurmans. 2001. Selfsupervised Chinese word segmentation. In 4th International Symposium of Intelligent Data Analysis, pages 238-247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay M Pont</author>
<author>W Bruce Croft</author>
</authors>
<title>USeg: A retargetable word segmentation procedure for information retrieval. In</title>
<date>1996</date>
<booktitle>Symposium on Document Analysis and Information Retrieval &apos; 96 (SDAIR). UMass</booktitle>
<tech>Technical Report TR96-2,</tech>
<institution>Univ. of Mass.,</institution>
<location>Amherst, MA.</location>
<contexts>
<context position="4135" citStr="Pont and Croft (1996)" startWordPosition="627" endWordPosition="630">lustrated as follows: given a string s, the probability of its most probable segmentation seg(s) in terms of a given language model is prob(seg(s)) = where prob(wil•) is some conditional probability in the model. Since all prob(wil•) &lt; 1.0, this probability becomes smaller for a greater n. Clearly, it looks more straightforward in an equi-probability setting. prob(wiI•) max 8=WlW2 ...W� Statistical approaches involve language models, mostly finite-state ones, trained on some large-scale corpora, as showed in Fan and Tsai (1988), Chang et al. (1991), Chiang et al. (1992), Sproat et al. (1996), Pont and Croft (1996) and Ng and Lua (forthcoming). These approaches do not provide any explicit strategy for disambiguation, but they get more ambiguous chunks correctly segmented than MMs by virtue of probability. Other linguistic resources or computational processes can also be integrated for further improvement, e.g., Lai et al. (1991) attempts to integrate POS tagging with word segmentation for the enhancement of accuracy and Gan et al. (1997) integrates word boundary disambiguation into sentence processing within a probabilistic emergent model. There are also other approaches that incorporate various techniq</context>
</contexts>
<marker>Pont, Croft, 1996</marker>
<rawString>Jay M. Pont and W. Bruce Croft. 1996. USeg: A retargetable word segmentation procedure for information retrieval. In Symposium on Document Analysis and Information Retrieval &apos; 96 (SDAIR). UMass Technical Report TR96-2, Univ. of Mass., Amherst, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sproat</author>
<author>C Shih</author>
<author>W Gale</author>
<author>N Chang</author>
</authors>
<title>A stochastic finite-state word-segmentation algorithm for Chinese. Computational Linguistics,</title>
<date>1996</date>
<pages>22--3</pages>
<contexts>
<context position="4112" citStr="Sproat et al. (1996)" startWordPosition="623" endWordPosition="626"> probability can be illustrated as follows: given a string s, the probability of its most probable segmentation seg(s) in terms of a given language model is prob(seg(s)) = where prob(wil•) is some conditional probability in the model. Since all prob(wil•) &lt; 1.0, this probability becomes smaller for a greater n. Clearly, it looks more straightforward in an equi-probability setting. prob(wiI•) max 8=WlW2 ...W� Statistical approaches involve language models, mostly finite-state ones, trained on some large-scale corpora, as showed in Fan and Tsai (1988), Chang et al. (1991), Chiang et al. (1992), Sproat et al. (1996), Pont and Croft (1996) and Ng and Lua (forthcoming). These approaches do not provide any explicit strategy for disambiguation, but they get more ambiguous chunks correctly segmented than MMs by virtue of probability. Other linguistic resources or computational processes can also be integrated for further improvement, e.g., Lai et al. (1991) attempts to integrate POS tagging with word segmentation for the enhancement of accuracy and Gan et al. (1997) integrates word boundary disambiguation into sentence processing within a probabilistic emergent model. There are also other approaches that inco</context>
</contexts>
<marker>Sproat, Shih, Gale, Chang, 1996</marker>
<rawString>R. Sproat, C. Shih, W. Gale, and N. Chang. 1996. A stochastic finite-state word-segmentation algorithm for Chinese. Computational Linguistics, 22(3):377-404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Stevenson</author>
<author>Yorick A Wilks</author>
</authors>
<title>The interaction of knowledge sources in word sense disambiguation.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<pages>27--3</pages>
<contexts>
<context position="9608" citStr="Stevenson and Wilks, 2001" startWordPosition="1483" endWordPosition="1487">hinese word segmentation ambiguities within the framework of case-based learning. This supervised learning approach, also labeled as memory-based, instance-based or example-based learning, has been popular for various NLP applications in recent years, e.g., the TiMBL learner (Daelemans et al., 2001). TiMBL is developed as a general memory-based learning environment to integrate a set of learning algorithms. It has been widely applied to disambiguating a variety of NLP tasks, including PP attachment (Zavrel et al., 1998), shallow parsing (Daelemans et al., 1999) and WSD (Veenstra et al., 2000; Stevenson and Wilks, 2001). In this paper, the general principle of case-based learning is followed but the formulation below is nevertheless specific to our problem. An example here is defined as a quadruple &lt; Cl, e, Cr, S &gt;, where the strings Cl and Cr are the left and right contexts within which the ambiguous string e appears, and S is the correct segmentation of e within the particular context. If denoting the quadruple as E, we also refer to S as seg(E) or seg(e), interchangeably. The distance, or similarity, between an example E and a given triple A =&lt;Q1 �� a Ca &gt; with 4Notice that ambiguities are dictionary-depe</context>
</contexts>
<marker>Stevenson, Wilks, 2001</marker>
<rawString>Mark Stevenson and Yorick A. Wilks. 2001. The interaction of knowledge sources in word sense disambiguation. Computational Linguistics, 27(3):321-349.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maosong Sun</author>
<author>BenJamin K T&apos;sou</author>
</authors>
<title>Ambiguity resolution in Chinese word segmentation.</title>
<date>1995</date>
<pages>27--28</pages>
<editor>In BenJamin K. T&apos;sou and Tom B. Y. Lai, editors, PACLIC-10, Hong Kong,</editor>
<contexts>
<context position="5576" citStr="Sun and T&apos;sou, 1995" startWordPosition="840" endWordPosition="843">cus onto the following aspects, resorting to a variety of resources and techniques, in particular, machine learning techniques: 1. Lexical resource acquisition, including compilation and automatic detection of high-tech terms and unknown words like names, to complement a never-big-enough dictionary (Chang et al., 1995; Pont and Croft, 1996; Chang and Su, 1997); 2. Investigation into the nature and statistics of ambiguities (Sun and Zhou, 1998); 3. Unsupervised learning of words ( Ge et al., 1999; Peng and Schuurmans, 2001)3; 4. Disambiguation with different approaches (Liang, 1989; Jin, 1994; Sun and T&apos;sou, 1995) The work reported in this paper belongs to the last category, taking an instance-based learning 3Recent research in this direction appears to be closely related to the studies on computational lexical acquisition of other languages such as English (de Marcken, 1996; Brent, 1999; Kit and Wilks, 1999; Kit, 2000; Venkataraman, 2001) and to language modeling technology (Jelinek, 1997), typically involving a version of the EM algorithm (Dempster et al., 1977). approach, aimed to examine its prospects of disambiguation. The rest of the paper is organized as follows. Section 2 briefly introduces the</context>
</contexts>
<marker>Sun, T&apos;sou, 1995</marker>
<rawString>Maosong Sun and BenJamin K. T&apos;sou. 1995. Ambiguity resolution in Chinese word segmentation. In BenJamin K. T&apos;sou and Tom B. Y. Lai, editors, PACLIC-10, Hong Kong, December 27-28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maosong Sun</author>
<author>Zhengping Zhou</author>
</authors>
<title>Word segmentation ambiguityin Chinese texts. In BenJiamin</title>
<date>1998</date>
<booktitle>Quantitative and Computational Studies on the Chinese Language,</booktitle>
<pages>323--338</pages>
<editor>K. T&apos;sou, Tom B. Y. Lai, Samuel W. K. Chan, and Williams S-Y. Wang, editors,</editor>
<institution>Language Information Sciences Research Centre, City University of Hong Kong.</institution>
<contexts>
<context position="5403" citStr="Sun and Zhou, 1998" startWordPosition="813" endWordPosition="816">, transformation-based error-driven learning (Palmer, 1997; Hockenmaier and Brew, 1998) and compression-based algorithm (Teahan et al., 2000). Recent research shifts its focus onto the following aspects, resorting to a variety of resources and techniques, in particular, machine learning techniques: 1. Lexical resource acquisition, including compilation and automatic detection of high-tech terms and unknown words like names, to complement a never-big-enough dictionary (Chang et al., 1995; Pont and Croft, 1996; Chang and Su, 1997); 2. Investigation into the nature and statistics of ambiguities (Sun and Zhou, 1998); 3. Unsupervised learning of words ( Ge et al., 1999; Peng and Schuurmans, 2001)3; 4. Disambiguation with different approaches (Liang, 1989; Jin, 1994; Sun and T&apos;sou, 1995) The work reported in this paper belongs to the last category, taking an instance-based learning 3Recent research in this direction appears to be closely related to the studies on computational lexical acquisition of other languages such as English (de Marcken, 1996; Brent, 1999; Kit and Wilks, 1999; Kit, 2000; Venkataraman, 2001) and to language modeling technology (Jelinek, 1997), typically involving a version of the EM a</context>
</contexts>
<marker>Sun, Zhou, 1998</marker>
<rawString>Maosong Sun and Zhengping Zhou. 1998. Word segmentation ambiguityin Chinese texts. In BenJiamin K. T&apos;sou, Tom B. Y. Lai, Samuel W. K. Chan, and Williams S-Y. Wang, editors, Quantitative and Computational Studies on the Chinese Language, pages 323-338. Language Information Sciences Research Centre, City University of Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J Teahan</author>
<author>Yingying Wen</author>
<author>Rodger J McNab</author>
<author>Ian H Witten</author>
</authors>
<title>A compression-based algorithm for Chinese word segmentation.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<pages>26--3</pages>
<contexts>
<context position="4925" citStr="Teahan et al., 2000" startWordPosition="742" endWordPosition="745">y virtue of probability. Other linguistic resources or computational processes can also be integrated for further improvement, e.g., Lai et al. (1991) attempts to integrate POS tagging with word segmentation for the enhancement of accuracy and Gan et al. (1997) integrates word boundary disambiguation into sentence processing within a probabilistic emergent model. There are also other approaches that incorporate various techniques of statistical NLP and machine learning, e.g., transformation-based error-driven learning (Palmer, 1997; Hockenmaier and Brew, 1998) and compression-based algorithm (Teahan et al., 2000). Recent research shifts its focus onto the following aspects, resorting to a variety of resources and techniques, in particular, machine learning techniques: 1. Lexical resource acquisition, including compilation and automatic detection of high-tech terms and unknown words like names, to complement a never-big-enough dictionary (Chang et al., 1995; Pont and Croft, 1996; Chang and Su, 1997); 2. Investigation into the nature and statistics of ambiguities (Sun and Zhou, 1998); 3. Unsupervised learning of words ( Ge et al., 1999; Peng and Schuurmans, 2001)3; 4. Disambiguation with different appro</context>
</contexts>
<marker>Teahan, Wen, McNab, Witten, 2000</marker>
<rawString>W. J. Teahan, Yingying Wen, Rodger J. McNab, and Ian H. Witten. 2000. A compression-based algorithm for Chinese word segmentation. Computational Linguistics, 26(3):375-393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Veenstra</author>
<author>A Van den Bosch</author>
<author>S Buchholz</author>
<author>W Daelemans</author>
<author>J Zavrel</author>
</authors>
<title>Memorybased word sense disambiguation. Computing and the Humanities, special issue on SENSEVAL,</title>
<date>2000</date>
<pages>34--1</pages>
<marker>Veenstra, Van den Bosch, Buchholz, Daelemans, Zavrel, 2000</marker>
<rawString>J. Veenstra, A. Van den Bosch, S. Buchholz, W. Daelemans, and J. Zavrel. 2000. Memorybased word sense disambiguation. Computing and the Humanities, special issue on SENSEVAL, 34(1-2 y).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anand Venkataraman</author>
</authors>
<title>A statistical model for word discoveryin transcribed speech.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<pages>27--3</pages>
<contexts>
<context position="5908" citStr="Venkataraman, 2001" startWordPosition="894" endWordPosition="895">ft, 1996; Chang and Su, 1997); 2. Investigation into the nature and statistics of ambiguities (Sun and Zhou, 1998); 3. Unsupervised learning of words ( Ge et al., 1999; Peng and Schuurmans, 2001)3; 4. Disambiguation with different approaches (Liang, 1989; Jin, 1994; Sun and T&apos;sou, 1995) The work reported in this paper belongs to the last category, taking an instance-based learning 3Recent research in this direction appears to be closely related to the studies on computational lexical acquisition of other languages such as English (de Marcken, 1996; Brent, 1999; Kit and Wilks, 1999; Kit, 2000; Venkataraman, 2001) and to language modeling technology (Jelinek, 1997), typically involving a version of the EM algorithm (Dempster et al., 1977). approach, aimed to examine its prospects of disambiguation. The rest of the paper is organized as follows. Section 2 briefly introduces the ambiguity problem and existing ambiguity detection strategies. Section 3 defines the notion and representation of examples, and formulates a similarity measure between an ambiguous input and an example. We present our disambiguation algorithm in Section 4 and experimental results and evaluation in Section 5, together with some di</context>
</contexts>
<marker>Venkataraman, 2001</marker>
<rawString>Anand Venkataraman. 2001. A statistical model for word discoveryin transcribed speech. Computational Linguistics, 27(3):353-372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan J Webster</author>
<author>Chunyu Kit</author>
</authors>
<title>Tokenization as the initial phase in NLP.</title>
<date>1992</date>
<booktitle>In COLING&apos;92,</booktitle>
<pages>1106--1110</pages>
<location>Nantes, France,</location>
<contexts>
<context position="1544" citStr="Webster and Kit, 1992" startWordPosition="224" endWordPosition="227">nd. Hence, if it is integrated effectively with a segmentation method of the precision P &gt; 95%, the resulting segmentation accuracy can reach, theoretically, beyond 99.5%. 1 Introduction It has been nearly two decades since the early work of Chinese word segmentation (Liang, 1984; Liang and Liu, 1985; Liu and Liang, 1986; Liang, 1986) . Tokenization has been recognized as a widespread problem, rather than being unique to Chinese and other oriental languages. It is an initial or prerequisite phase of NLP for all languages, although the obscurity of the problem varies from language to language (Webster and Kit, 1992a; Palmer, 2000). Recent work on tokenization for European languages such as English is reported in (Grefenstette and Tapanainen, 1994; Grefenstette, 1999; Grefenstette et al., 2000), adopting a finite-state approach. However, identification of multi-word units such as proper names and technical terms in these languages is highly comparable to that of multi-character Chinese words: there are no delimiters available. So far, a great variety of segmentation strategies for Chinese with various linguistic resources have been explored, yielding a large volume of literature on both linguistic and co</context>
</contexts>
<marker>Webster, Kit, 1992</marker>
<rawString>Jonathan J. Webster and Chunyu Kit. 1992a. Tokenization as the initial phase in NLP. In COLING&apos;92, pages 1106-1110, Nantes, France, July 23-28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan J Webster</author>
<author>Chunyu Kit</author>
</authors>
<title>Tokenization for machine translation: What can be learned from Chinese word identification.</title>
<date>1992</date>
<booktitle>In Proc. of 3rd International Conference on Chinese Information Processing,,</booktitle>
<location>BeiJing.</location>
<contexts>
<context position="1544" citStr="Webster and Kit, 1992" startWordPosition="224" endWordPosition="227">nd. Hence, if it is integrated effectively with a segmentation method of the precision P &gt; 95%, the resulting segmentation accuracy can reach, theoretically, beyond 99.5%. 1 Introduction It has been nearly two decades since the early work of Chinese word segmentation (Liang, 1984; Liang and Liu, 1985; Liu and Liang, 1986; Liang, 1986) . Tokenization has been recognized as a widespread problem, rather than being unique to Chinese and other oriental languages. It is an initial or prerequisite phase of NLP for all languages, although the obscurity of the problem varies from language to language (Webster and Kit, 1992a; Palmer, 2000). Recent work on tokenization for European languages such as English is reported in (Grefenstette and Tapanainen, 1994; Grefenstette, 1999; Grefenstette et al., 2000), adopting a finite-state approach. However, identification of multi-word units such as proper names and technical terms in these languages is highly comparable to that of multi-character Chinese words: there are no delimiters available. So far, a great variety of segmentation strategies for Chinese with various linguistic resources have been explored, yielding a large volume of literature on both linguistic and co</context>
</contexts>
<marker>Webster, Kit, 1992</marker>
<rawString>Jonathan J. Webster and Chunyu Kit. 1992b. Tokenization for machine translation: What can be learned from Chinese word identification. In Proc. of 3rd International Conference on Chinese Information Processing,, BeiJing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zimin Wu</author>
<author>Gwyneth Tseng</author>
</authors>
<title>Chinese text segmentation for text retrieval: achievements and problems.</title>
<date>1993</date>
<journal>JASIS,</journal>
<pages>44--9</pages>
<marker>Wu, Tseng, 1993</marker>
<rawString>Zimin Wu and Gwyneth Tseng. 1993. Chinese text segmentation for text retrieval: achievements and problems. JASIS, 44(9):532-542.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiwen Yu</author>
</authors>
<title>Knowledge Base of Grammatical Information for Contemporary Chinese.</title>
<date>1998</date>
<publisher>Tsinghua University Press,</publisher>
<location>BeJing. (In Chinese).</location>
<contexts>
<context position="12882" citStr="Yu (1998)" startWordPosition="2076" endWordPosition="2077"> the algorithms for these purposes. 4.1 Ambiguity detection We take a conventional approach to ambiguity detection, by detecting the discrepancies of the outputs from the FMM and BMM segmentations. Given an input corpus C, it can be realized, plainly, by the following algorithm: Ambiguity detection algorithm: ambd(C) 1. 97= FMM(C) and B = BMM(C) 2. Return diff(g7, 8) where FMM(•) and FMM(•) return the FMM and BMM segmentations of C, and diff(•, •) returns the discrepancies of the two segmentations. The dictionary used to support the MMs is a merger of the word lists from Liu et al. (1994) and Yu (1998), consisting of 53K entries. It is a medium-sized dictionary. With regards to the dictionary size and the weakness of the ambiguity detection algorithm, we keep ourselves alert of the fact that there are a certain number of ambiguities that are not detected by our program. And the resolutions for the ambiguous strings so detected are manually prepared, by selecting an answer from the outputs of the MMs in use. 4.2 Disambiguation Given an example base E and a text corpus C as testing data, the disambiguation algorithm works along the following steps: Disambiguation algorithm: disamb(C, E) 1. Am</context>
</contexts>
<marker>Yu, 1998</marker>
<rawString>Shiwen Yu. 1998. Knowledge Base of Grammatical Information for Contemporary Chinese. Tsinghua University Press, BeJing. (In Chinese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakub Zavrel</author>
<author>Walter Daelemans</author>
<author>Jorn Veenstra</author>
</authors>
<title>Resolving PP attachment ambiguities with memory-based learning.</title>
<date>1998</date>
<booktitle>CoNLL9&apos;1. Computational Natural Language Learning,</booktitle>
<pages>136--144</pages>
<editor>In T. Mark Ellison, editor,</editor>
<location>Somerset, New Jersey.</location>
<contexts>
<context position="9507" citStr="Zavrel et al., 1998" startWordPosition="1467" endWordPosition="1470"> ambiguities for our experiments. 3 Examples and similarity measure We intend to disambiguate Chinese word segmentation ambiguities within the framework of case-based learning. This supervised learning approach, also labeled as memory-based, instance-based or example-based learning, has been popular for various NLP applications in recent years, e.g., the TiMBL learner (Daelemans et al., 2001). TiMBL is developed as a general memory-based learning environment to integrate a set of learning algorithms. It has been widely applied to disambiguating a variety of NLP tasks, including PP attachment (Zavrel et al., 1998), shallow parsing (Daelemans et al., 1999) and WSD (Veenstra et al., 2000; Stevenson and Wilks, 2001). In this paper, the general principle of case-based learning is followed but the formulation below is nevertheless specific to our problem. An example here is defined as a quadruple &lt; Cl, e, Cr, S &gt;, where the strings Cl and Cr are the left and right contexts within which the ambiguous string e appears, and S is the correct segmentation of e within the particular context. If denoting the quadruple as E, we also refer to S as seg(E) or seg(e), interchangeably. The distance, or similarity, betwe</context>
</contexts>
<marker>Zavrel, Daelemans, Veenstra, 1998</marker>
<rawString>Jakub Zavrel, Walter Daelemans, and Jorn Veenstra. 1998. Resolving PP attachment ambiguities with memory-based learning. In T. Mark Ellison, editor, CoNLL9&apos;1. Computational Natural Language Learning, pages 136-144, Somerset, New Jersey.</rawString>
</citation>
<citation valid="false">
<title>A hybrid approach toward ambiguity resolution in segmenting Chinese sentences. Submitted to Computer Processing of Oriental Languages.</title>
<marker></marker>
<rawString>Guodong Zhou and Kim Teng Lua. (forthcoming). A hybrid approach toward ambiguity resolution in segmenting Chinese sentences. Submitted to Computer Processing of Oriental Languages.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>