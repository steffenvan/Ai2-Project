<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.979466">
Generating Code-switched Text for Lexical Learning
</title>
<author confidence="0.996369">
Igor Labutov Hod Lipson
</author>
<affiliation confidence="0.999475">
Cornell University Cornell University
</affiliation>
<email confidence="0.992942">
iil4@cornell.edu hod.lipson@cornell.edu
</email>
<sectionHeader confidence="0.994633" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999900611111111">
A vast majority of L1 vocabulary acqui-
sition occurs through incidental learning
during reading (Nation, 2001; Schmitt et
al., 2001). We propose a probabilistic ap-
proach to generating code-mixed text as
an L2 technique for increasing retention
in adult lexical learning through reading.
Our model that takes as input a bilingual
dictionary and an English text, and gener-
ates a code-switched text that optimizes a
defined “learnability” metric by construct-
ing a factor graph over lexical mentions.
Using an artificial language vocabulary,
we evaluate a set of algorithms for gener-
ating code-switched text automatically by
presenting it to Mechanical Turk subjects
and measuring recall in a sentence com-
pletion task.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999705">
Today, an adult trying to learn a new language is
likely to embrace an age-old and widely accepted
practice of learning vocabulary through curated
word lists and rote memorization. Yet, it is not
uncommon to find yourself surrounded by speak-
ers of a foreign language and instinctively pick up
words and phrases without ever seeing the defini-
tion in your native tongue. Hearing “pass le sale
please” at the dinner table from your in-laws vis-
iting from abroad, is unlikely to make you think
twice about passing the salt. Humans are extraor-
dinarily good at inferring meaning from context,
whether this context is your physical surround-
ing, or the surrounding text in the paragraph of the
word that you don’t yet understand.
Recently, a novel method of L2 language teach-
ing had been shown effective in improving adult
lexical acquisition rate and retention 1. This tech-
</bodyText>
<footnote confidence="0.467448">
1authors’ unpublished work
</footnote>
<bodyText confidence="0.999935536585366">
nique relies on a phenomenon that elicits a nat-
ural simulation of L1-like vocabulary learning in
adults — significantly closer to L1 learning for L2
learners than any model studied previously. By in-
fusing foreign words into text in the learner’s na-
tive tongue into low-surprisal contexts, the lexi-
cal acquisition process is facilitated naturally and
non-obtrusively. Incidentally, this phenomenon
occurs “in the wild” and is termed code-switching
or code-mixing, and refers to the linguistic pattern
of bilingual speakers swapping words and phrases
between two languages during speech. While this
phenomenon had received significant attention
from both a socio-linguistic (Milroy and Muysken,
1995) and theoretical linguistic perspectives (Be-
lazi et al., 1994; Bhatt, 1997) (including some
computational studies), only recently has it been
hypothesizes that “code-switching” is a marking
of bilingual proficiency, rather than deficiency
(Genesee, 2001).
Until recently it was widely believed that inci-
dental lexical acquisition through reading can only
occur for words that occur at sufficient density
in a single text, so as to elicit the “noticing” ef-
fect needed for lexical acquisition to occur (Cobb,
2007). Recent neurophysiological findings, how-
ever, indicate that even a single incidental expo-
sure to a novel word in a sufficiently constrained
context is sufficient to trigger an early integra-
tion of the word in the brain’s semantic network
(Borovsky et al., 2012).
An approach explored in this paper, and moti-
vated by the above findings, exploits “constrain-
ing” contexts in text to introduce novel words. A
state-of-the-art approach for generating such text
is based on an expert annotator whose job is to
decide which words to “switch out” with novel
foreign words (from hereon we will refer to the
“switched out” word as the source word and to the
“switched in” word as the target word). Conse-
quently the process is labor-intensive and leads to
</bodyText>
<page confidence="0.950823">
562
</page>
<note confidence="0.831481">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 562–571,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999497090909091">
a “one size fits all solution” that is insensitive to
the learner’s skill level or vocabulary proficiency.
This limitation is also cited in literature as a sig-
nificant roadblock to the widespread adaptation
of graded reading series (Hill, 2008). A reading-
based tool that follows the same principle, i.e. by
systematic exposure of a learner to an incremen-
tally more challenging text, will result in more ef-
fective learning (Lantolf and Appel, 1994).
To address the above limitation, we develop an
approach for automatically generating such “code-
switched” text with an explicit goal of maximizing
the lexical acquisition rate in adults. Our method
is based on a global optimization approach that
incorporates a “knowledge model” of a user with
the content of the text, to generate a sequence of
lexical “switches”. To facilitate the selection of
“switch points”, we learn a discriminative model
for predicting switch point locations on a corpus
that we collect for this purpose (and release to the
community). Below is a high-level outline of this
paper.
</bodyText>
<listItem confidence="0.997006416666667">
• We formalize our approach within a prob-
abilistic graphical model framework, infer-
ence in which yields “code-switched” text
that maximizes a surrogate to the acquisition
rate objective.
• We compare this global method to sev-
eral baseline techniques, including the strong
“high-frequency” baseline.
• We analyze the operating range in which
our model is effective and motivate the near-
future extension of this approach with the
proposed improvements.
</listItem>
<sectionHeader confidence="0.99902" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999946625">
Our proposed approach to the computational gen-
eration of code-switched text, for the purpose of
L2 pedagogy, is influenced by a number of fields
that studied aspects of this phenomenon from dis-
tinct perspectives. In this section, we briefly de-
scribe a motivation from the areas of socio- and
psycho- linguistics and language pedagogy re-
search that indicate the promise of this approach.
</bodyText>
<subsectionHeader confidence="0.999704">
2.1 Code-switching as a natural phenomenon
</subsectionHeader>
<bodyText confidence="0.999364590909091">
Code-switching (or code-mixing) is a widely stud-
ied phenomenon that received significant attention
over the course of the last three decades, across
the disciplines of sociolinguistics, theoretical and
psycholinguistics and even literary and cultural
studies (predominantly in the domain of Spanish-
English code-switching) (Lipski, 2005).
Code-switching that occurs naturally in bilin-
gual populations, and especially in children, has
for a long time been considered a marking of
incompetency in the second language. A more
recent view on this phenomenon, however, sug-
gests that due to the underlying syntactic com-
plexity of code-switching, code-switching is ac-
tually a marking of bilingual fluency (Genesee,
2001). More recently, the idea of employing
code-switching in the classroom, in a form of
conversation-based exercises, has attracted the
attention of multiple researchers and educators
(Moodley, 2010; Macaro, 2005), yielding promis-
ing results in an elementary school study in South-
Africa.
</bodyText>
<subsectionHeader confidence="0.9990695">
2.2 Computational Approaches to
Code-switching
</subsectionHeader>
<bodyText confidence="0.999961862068966">
Additionally, there has been a limited number
of studies of the computational approaches to
code-switching, and in particular code-switched
text generation. Solorio and Liu (2008), record
and transcribe a corpus of Spanish-English code-
mixed conversation to train a generative model
(Naive Bayes) for the task of predicting code-
switch points in conversation. Additionally they
test their trained model in its ability to generate
code-switched text with convincing results. Build-
ing on their work, (Adel et al., 2012) employ ad-
ditional features and a recurrent network language
model for modeling code-switching in conversa-
tional speech. Adel and collegues (2011) propose
a statistical machine translation-based approach
for generating code-switched text. We note, how-
ever, that the primary goal of these methods is in
the faithful modeling of the natural phenomenon
of code-switching in bilingual populations, and
not as a tool for language teaching. While useful
in generating coherent, syntactically constrained
code-switched texts in its own right, none of these
methods explicitly consider code-switching as a
vehicle for teaching language, and thus do not
take on an optimization-based view with an ob-
jective of improving lexical acquisition through
the reading of the generated text. More recently,
and concurrently with our work, Google’s Lan-
guage Immersion app employs the principle of
</bodyText>
<page confidence="0.997603">
563
</page>
<bodyText confidence="0.9999575">
code-switching for language pedagogy, by gener-
ating code-switched web content, and allowing its
users to tune it to their skill level. It does not, how-
ever, seem to model the user explicitly, nor is it
clear if it performs any optimization in generating
the text, as no studies have been published to date.
</bodyText>
<subsectionHeader confidence="0.9928855">
2.3 Computational Approaches to Sentence
Simplification
</subsectionHeader>
<bodyText confidence="0.999975684210526">
Although not explicitly for teaching language,
computational approaches that facilitate accessi-
bility to texts that might otherwise be too difficult
for its readers, either due to physical or learning
disabilities, or language barriers, are relevant. In
the recent work of (Kauchak, 2013), for example
demonstrates an approach to increasing readability
of texts by learning from unsimplified texts. Ap-
proaches in this area span methods for simplify-
ing lexis (Yatskar et al., 2010; Biran et al., 2011),
syntax (Siddharthan, 2006; Siddharthan et al.,
2004), discourse properties (Hutchinson, 2005),
and making technical terminology more accessible
to non-experts (Elhadad and Sutaria, 2007). While
the resulting texts are of great potential aid to lan-
guage learners and may implicitly improve upon a
reader’s language proficiency, they do not explic-
itly attempt to promote learning as an objective in
generating the simplified text.
</bodyText>
<subsectionHeader confidence="0.997704">
2.4 Recent Neurophysiological findings
</subsectionHeader>
<bodyText confidence="0.999959739130435">
Evidence for the potential effectiveness of code-
switching for language acquisition, stem from the
recent findings of (Borovsky et al., 2012), who
have shown that even a single exposure to a novel
word in a constrained context, results in the inte-
gration of the word within your existing semantic
base, as indicated by a change in the N400 elec-
trophysiological response recorded from the sub-
jects’ scalps. N400 ERP marker has been found
to correlate with the semantic “expectedness” of a
word (Kutas and Hillyard, 1984), and is believed
to be an early indicator of word learning. Further-
more, recent work of (Frank et al., 2013), show
that word surprisal predicts N400, providing con-
crete motivation for artificial manipulation of text
to explicitly elicit word learning through natural
reading, directly motivating our approach. Prior to
the above findings, it was widely believed that for
evoking “incidental” word learning through read-
ing alone, the word must appear with sufficiently
high frequency within the text, such as to elicit the
“noticing” effect — a prerequisite to lexical acqui-
sition (Schmidt and Schmidt, 1995; Cobb, 2007).
</bodyText>
<sectionHeader confidence="0.976836" genericHeader="method">
3 Model
</sectionHeader>
<subsectionHeader confidence="0.988989">
3.1 Overview
</subsectionHeader>
<bodyText confidence="0.997983">
The formulation of our model is primarily moti-
vated by two hypotheses that have been validated
experimentally in the cognitive science literature.
We re-state these hypotheses in the language of
“surprisal”:
</bodyText>
<listItem confidence="0.892473166666667">
1. Inserting a target word into a low surprisal
context increases the rate of that word’s inte-
gration into a learner’s lexicon.
2. Multiple exposures to the word in low sur-
prisal contexts increases rate of that word’s
integration.
</listItem>
<bodyText confidence="0.999431588235294">
Hypothesis 1 is supported by evidence from
(Borovsky et al., 2012; Frank et al., 2013), and hy-
pothesis 2 is supported by evidence from (Schmidt
and Schmidt, 1995). We adopt the term “low-
surprisal” context to identify contexts (e.g. n-
grams) that are highly predictive of the target word
(e.g. trailing word in the n-gram). The motiva-
tion stems from the recent evidence (Frank et al.,
2013) that low-surprisal contexts affect the N400
response and thus correlate with word acquisi-
tion. To realize a “code-switched” mixture that
adheres maximally to the above postulates, it is
self-evident that a non-trivial optimization prob-
lem must be solved. For example, naively select-
ing a few words that appear in low-surprisal con-
texts may facilitate their acquisition, but at the ex-
pense of other words within the same context that
may appear in a larger number of low-surprisal
contexts further in the text.
To address this problem, we approach it with
a formulation of a factor graph that takes global
structure of the text into account. Factor graph for-
malism allows us to capture local features of indi-
vidual contexts, such as lexical and syntactic sur-
prisal, while inducing dependencies between con-
sequent “switching decisions” in the text. Max-
imizing likelihood of the joint probability under
the factorization of this graph yields an optimal
sequence of these “switching decisions” in the en-
tirety of the text. Maximizing joint likelihood, as
we will show in the next section, is a surrogate to
maximizing the probability of the learner acquir-
ing novel words through the process of reading the
generated text.
</bodyText>
<page confidence="0.996905">
564
</page>
<figureCaption confidence="0.97448875">
Figure 1: Overview of the approach. Probabilistic learner model (PLM) provides the current value of the
belief in the learner’s knowledge of any given word. Local contextual model provides the value of the
belief in learning the word from the context alone. Upon exposure of the learner to the word in the given
context, PLM is updated with the posterior belief in the user’s knowledge of the word.
</figureCaption>
<figure confidence="0.999269815789474">
KNOW
DON’T
KNOW
KNOW
DON’T
KNOW
Mixed-Language
Content
User’s lexical
knowledge model
...
w1 w2 w3 w4 w5 w|V |
infused word
Meaning of malhela?
The door malhela to the beach
wi
0i
k
zi
k
Existing
knowledge
of word
Contextual
Interpretation
of word
Updated
Knowldge
Model
Updated
knowledge
belief
wi
Known + Constrained
Unknown + Constrained
Unknown + Unconstrained
Known + Unconstrained
LEGEND
</figure>
<subsectionHeader confidence="0.999697">
3.2 Language Learner Model
</subsectionHeader>
<bodyText confidence="0.999977076923077">
A simplified model of the learner, that we shall
term a Probabilistic Learner Model (PLM) serves
as a basis for our approach. PLM is a model of
a learner’s lexical knowledge at any given time.
PLM models the learner as a vector of indepen-
dent Bernoulli distributions, where each compo-
nent represents a probability of the learner know-
ing the corresponding word. We motivate a proba-
bilistic approach by taking the perspective of mea-
suring our belief in the learner’s knowledge of any
given word, rather than the learner’s uncertainty in
own knowledge. Formally, we can fully specify
this model for learner i as follows:
</bodyText>
<equation confidence="0.958683">
Ui = (gyri0, gyri1, ... , gyri |) (1)
</equation>
<bodyText confidence="0.999424333333333">
where V is the vocabulary set — identical
across all users, and gyrij is our degree of belief in
the learner i’s knowledge of a target word wj ∈ V .
Statistical estimation techniques exist for estimat-
ing an individual’s vocabulary size, such as (Bhat
and Sproat, 2009; Beglar, 2010), and can be di-
</bodyText>
<page confidence="0.992254">
565
</page>
<bodyText confidence="0.9999546">
rectly employed for estimating the parameters of
this model as our prior belief about user i’s knowl-
edge.
The primary motivation behind a probabilistic
user model, is to provide a mechanism for up-
dating these probabilities as the user progresses
through her reading. Maximizing the parameters
of the PLM under a given finite span of code-
switched text, thus, provides a handle for generat-
ing optimal code-switched content. Additionally,
a probabilistic approach allows for a natural inte-
gration of the user model with the uncertainty in
other components of the system, such as uncer-
tainty in determining the degree of constraint im-
posed by the context, and in bitext alignment.
</bodyText>
<subsectionHeader confidence="0.999325">
3.3 Model overview
</subsectionHeader>
<bodyText confidence="0.999995818181818">
At the high level, as illustrated in Figure 1, our ap-
proach integrates the model of the learner (PLM)
with the local contextual features to update the
PLM parameters incrementally as the learner pro-
gresses through the text. The fundamental as-
sumption behind our approach is that the learner’s
knowledge of a given word after observing it in
a sentence is a function of 1) the learner’s previ-
ous knowledge of the word, prior to observing it
in a given sentence and 2) a degree of constraint
that a given context imposes on the meaning of the
novel word, and is directly related to the surprisal
of novel word in that context. Broadly, as the
learner progresses from one sentence to the next,
exposing herself to more novel words, the updated
parameters of the language model in turn guide
the selection of new “switch-points” for replac-
ing source words with the target foreign words. In
practice, however, this process is carried out im-
plicitly and off-line by optimizing the estimated
progress of the learner’s PLM, without dynamic
feedback. Next, we describe the model in detail.
</bodyText>
<subsectionHeader confidence="0.996698">
3.4 Switching Factor Graph Model
</subsectionHeader>
<bodyText confidence="0.9996224">
To aid in the specification of the factor graph struc-
ture, we introduce new terminology. Because the
PLM is updated progressively, we will refer to the
parameters of the PLM for a given word wi after
observing its kth appearance (instance) in the text,
as the learner’s state of knowledge of that word,
and denote it as a binary random variable zik.
Without explicit testing of the user, this variable
is hidden. We can view the prior learning model
as the parameters of the vector of random variables
</bodyText>
<equation confidence="0.8820325">
(z0 0, z1 0, ... z|V |
0 ).
</equation>
<bodyText confidence="0.999988071428572">
The key to our approach is in how the param-
eters of these hidden variables are updated from
repeated exposures to words in various contexts.
Intuitively, an update to the parameter of zik from
zik_1 occurs after the learner observes word wi in
a context (this may be an n-gram, an entire sen-
tence or paragraph containing wi, but we will re-
strict our attention to fixed-length n-grams). In-
tuitively an update to the parameter of zik_1 will
depend on how “constrained” the meaning of wi
is in the given context. We will refer to it as the
“learnability”, denoted by Lki , of word wi on its
kth appearance, given its context. Formally, we
will define “learnability” as follows:
</bodyText>
<equation confidence="0.9914636">
P(Lik = 1|wi, w\i, z\i
k ) =
�P(constrained(wi) = 1|w)
i7�j
(2)
</equation>
<bodyText confidence="0.999998259259259">
where w\i represents the set of words that com-
prise the context window of wi, not including wi,
and z\ik are the states corresponding to each of the
words in w\i. P(constrained(wi) = 1|w) is a real
value (scaled between 0 and 1) that represents the
degree of constraint imposed on the meaning of
word wi by its context. This value comes from
a binary prediction model trained to predict the
“predictability” of a word in its context, and is
based on the dataset that we collected (described
later in the paper). Generally, this value may
come directly from the surprisal quantity given by
a language model, or may incorporate additional
features that are found informative in predicting
the constraint on the word. Finally, the quantity
is weighted by the parameters of the state vari-
ables corresponding to the words other than wi
contained in the context. This encodes an intu-
ition that a degree of predictability of a given word
given its context is related to the learner’s knowl-
edge of the other words in that context. If, for ex-
ample, in the sentence “pass me the salt and pep-
per, please”, both “salt” and “pepper” are substi-
tuted with their foreign translations that the learner
is unlikely to know, it’s equally unlikely that she
will learn them after being exposed to this con-
text, as the context itself will not offer sufficient
</bodyText>
<equation confidence="0.94720875">
P(zik = 1) = ⎧ Probability that
⎨ word wi E V
⎩ is understood on kthexposure
P(zjk = 1)
</equation>
<page confidence="0.98557">
566
</page>
<bodyText confidence="0.9992195">
information for both words to be inferred simulta-
neously. On the other hand, substituting “salt” and
“pepper” individually, is likely to make it much
easier to infer the meaning of the other.
</bodyText>
<equation confidence="0.76375875">
i
zk-1
zi
k
</equation>
<figureCaption confidence="0.963031666666667">
Figure 2: A noisy-OR combination of the learner’s
previous state of knowledge of the word zik−1 and
the word’s “learnability” in the observed context
</figureCaption>
<equation confidence="0.6745285">
Li
k
</equation>
<bodyText confidence="0.968522333333333">
The updated parameter of zik is obtained from a
noisy-OR combination of the parameters of zik−1
and Lik:
</bodyText>
<equation confidence="0.99948">
P(zik = 1|zik−1,Lik) =
1 − [1 − P(Lik = 1)][1 − P(zk−1 = 1)]
</equation>
<bodyText confidence="0.999925925925926">
A noisy-OR-based CPD provides a convenient
and tractable approximation in capturing the in-
tended intuition: updated state of knowledge of a
given word will increase if the word is observed in
a “good” context, or if the learner already knows
the word.
Combining Equation 2 for each word in the con-
text using the noisy-OR, the updated state for word
wi will now be conditioned on zik−1, z\ik , wk. Be-
cause of the dependence of each z in the context
on all other hidden variables in that context, we
can capture the dependence using a single factor
per context, with all of the z variables taking part
in a clique, whose dimension is the size of the con-
text.
We will now introduce a dual interpretation of
the z variables: as “switching” variables that de-
cide whether a given word will be replaced with its
translation in the foreign language. If, for exam-
ple, all of the words have high probability of be-
ing known by a learner, than maximizing the joint
likelihood of the model will result in most of the
words “switched-out” — a desired result. For an
arbitrary prior PLM and the input text, maximiz-
ing joint likelihood will result in the selection of
“switched-out” words that have the highest final
probability of being “known” by the learner.
</bodyText>
<sectionHeader confidence="0.764193" genericHeader="method">
3.5 Inference
</sectionHeader>
<bodyText confidence="0.999935454545454">
The problem of selecting “switch-points” reduces
to the problem of inference in the resulting factor
graph. Unfortunately, without a fairly strong con-
straint on the collocation of switched words, the
resulting graph will contain loops, requiring tech-
niques of approximate inference. To find the opti-
mal settings of the z variables, we apply the loopy
max-sum algorithm. While variants of loopy be-
lief propagation, in general, are not guaranteed to
converge, we found that the convergence does in-
deed occur in our experiments.
</bodyText>
<subsectionHeader confidence="0.99508">
3.6 Predicting “predictable” words
</subsectionHeader>
<bodyText confidence="0.999959772727273">
We carried out experiments to determine which
words are likely to be inferred from their context.
The collected data-set is then used to train a logis-
tic regression classifier to predict which words are
likely to be easily inferred from their context. We
believe that this dataset may also be useful to re-
searchers in studying related phenomena, and thus
make it publicly available.
For this task, we focus only on the following
context features for predicting the “predictability”
of words: n-gram probability, vector-space simi-
larity score, coreferring mentions. N-gram prob-
ability and vector-space similarity 2 score are all
computed within a fixed-size window of the word
(trigrams using Microsoft N-gram service). Coref-
erence feature is a binary feature which indicates
whether the word has a co-referring mention in a
3-sentence window preceding a given context (ob-
tained using Stanford’s CoreNLP package). We
train L2-regularized logistic regression to predict
a binary label L E {Constrained, Unconstrained}
using a crowd-sourced corpus described below.
</bodyText>
<subsectionHeader confidence="0.988496">
3.7 Corpus Construction
</subsectionHeader>
<bodyText confidence="0.999854">
For collecting data about which words are likely
to be “predicted” given their content, we devel-
oped an Amazon Mechanical Turk task that pre-
sented turkers with excerpts of a short story (En-
glish translation of “The Man who Repented” by
</bodyText>
<footnote confidence="0.9230495">
2we employ C&amp;W word embeddings from http://
metaoptimize.com/projects/wordreprs/
</footnote>
<figure confidence="0.954833441176471">
Li
k
567
Original Text Factor Graph
wj
wi
wi
wj
wi
S4
wi
S2
wj
S5
S3
wk
S1
S6
Mapping
zj zi
0 3
zi
1
f4
f2
f5
zi2
zj
1
f3
zi0
f1
j k
z2 z0
</figure>
<figureCaption confidence="0.994426333333333">
Figure 3: Sequence of sentences in the text (left) is mapped into a factor graph, whose nodes correspond
to specific occurences of individual words, connected in a clique corresponding to a context in which the
word occurs.
</figureCaption>
<bodyText confidence="0.999900166666667">
Ana Maria Matute), with some sentences contain-
ing a blank in place of a word. Only content words
were considered for the task. Turkers were re-
quired to type in their best guess, and the num-
ber of semantically similar guesses were counted
by an average number of 6 other turkers. A ra-
tio of the median of semantically similar guesses
to the total number of guesses was then taken as
the score representing “predictability” of the word
being guessed in the given context. All words cor-
responding to blanks whose scores were equal to
and above 0.6 were than taken as a positive la-
bel (Constrained) and scores below 0.6 were taken
as a negative label (Unconstrained). Turkers that
judged the semantic similarity of the guesses of
other turkers achieved an average Cohen’s kappa
agreement of 0.44, indicating fair to poor agree-
ment.
</bodyText>
<sectionHeader confidence="0.999084" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.964947">
We carried out experiments on the effectiveness
of our approach using the Amazon Mechanical
Turk platform. Our experimental procedure was
as follows: 162 turkers were partitioned into four
groups, each corresponding to a treatment con-
dition: OPT (N=34), HF (N=41), RANDOM
(N=43), MAN (N=44). Each condition corre-
</bodyText>
<figureCaption confidence="0.896706">
Figure 4: Visualization of the most “predictable”
</figureCaption>
<bodyText confidence="0.9413750625">
words in an excerpt from the “The Man who Re-
pented” by Ana Maria Matute (English transla-
tion). Font-size correlates with the score given by
judge turkers in evaluating guesses of other turk-
ers that were presented with the same text, but the
word replaced with a blank. Snippet of the dataset
that we release publicly.
sponded to a model used to generate the presented
code-switched text. For all experiments, the text
used was a short story “Lottery” by Shirley Jack-
son, and a total number of replaced words was
controlled (34). Target vocabulary consisted of
words from an artificial language, generated stat-
ically by a mix of words from several languages.
Below we describe the individual treatment condi-
tions:
</bodyText>
<footnote confidence="0.373551">
RANDOM (Baseline): words for switching are
</footnote>
<page confidence="0.994916">
568
</page>
<bodyText confidence="0.981620222222222">
selected at random from content only words.
HF (High Frequency) Baseline: words for
switching are selected at random from a ranked
list of words that occur most frequently in the pre-
sented text.
MAN (Manual) Baseline: words for switch-
ing are selected manually by the author, based on
the intuition of which words are most likely to be
guessed in context.
OPT (Optimization-based): factor graph-based
model proposed in this paper is used for generat-
ing code-switched content. The total number of
switched words generated by this method is used
as a constant for all baselines.
Turkers were solicited to participate in a study
that involved “reading a short story with a twist”
(title of HIT). Not the title, nor the description
gave away the purpose of the study, nor that it
would be followed by a quiz. Time was not con-
trolled for this study, but on average turkers took
27 minutes to complete the reading. Upon com-
pleting the reading portion of the task, turkers
were presented with novel sentences that featured
the words observed during reading, where only
one of the sentences used the word in a semanti-
cally correct way. Turkers were asked to select the
sentence that “made the most sense”. An example
of the sentences presented during the test:
Example 1
✓ My edzino loves to go shopping every
weekend.
The edzino was too big to explore on our
own, so went with a group.
English word: wife
Example 2
✓ His unpreadvers were utterly confus-
ing and useless.
The unpreadvers was so strong, that he
had to go to a hospital.
English word: directions
A “recall” metric was computed for each turker,
defined as the ratio of correctly selected sentences
to the total number of sentences presented. The
“grand-average recall” across all turkers was then
computed and reported here.
</bodyText>
<sectionHeader confidence="0.999583" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.999848631578947">
We perform a one-way ANOVA across the four
groups listed above, with the resulting F = 11.38
and p = 9.7e−7. Consequently, multiple pairwise
comparison of the models was performed with the
Bonferroni-corrected pairwise t-test, yielding the
only significantly different recall means between
HF − MAN (p = 0.00018), RANDOM −
MAN (p = 2.8e − 6), RANDOM − OPT
(p = 0.00587). The results indicate that, while
none of the automated methods (RANDOM,
HF, OPT) outperform manually generated code-
switched text, OPT outperforms the RANDOM
baseline (no decisive conclusion can be drawn
with respect to the HF − RANDOM pair).
Additionally, we note, that for words with fre-
quency less than 4, OPT produces recall that is
on average higher than the HF baseline (p=0.043,
Welch’s t-test), but at the expense of higher fre-
quency words.
</bodyText>
<figure confidence="0.444604">
HF MAN OPT RANDOM
Condition
</figure>
<figureCaption confidence="0.629338">
Figure 5: Results presented for 4 groups, sub-
</figureCaption>
<bodyText confidence="0.957621">
jected to 4 treatment conditions: RANDOM,
HF, MAN, OPT. Recall performance for
each group corresponds to the average ratio of
selected sentences that correctly utilize code-
switched words in novel contexts, across all turk-
ers.
</bodyText>
<sectionHeader confidence="0.999788" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.998986">
We observe from our experiments that the
optimization-based approach does not in general
outperform the HF baseline. The strength of the
</bodyText>
<figure confidence="0.981136263157895">
Recall
0.75
0.50
0.25
0.00
●
●
●
●
Condition
HF
MAN
OPT
RANDOM
●
●
569
HF OPT
Condition
</figure>
<figureCaption confidence="0.676793">
Figure 6: Subset of the results for 2 of the 4 treat-
ment conditions: HF and OPT that correspond
to recall only for words with item frequency in the
presented text below 4.
</figureCaption>
<bodyText confidence="0.992111394736842">
frequency-based baseline is attributed to a well-
known phenomenon that item frequency promotes
the “noticing” effect during reading, critical for
triggering incidental lexical acquisition. Gener-
ating code-switched text by replacing high fre-
quency content words, thus, in general is a sim-
ple and viable approach for generating effective
reading-based L2 curriculum aids. However, this
method is fundamentally less flexible than the
optimization-based method proposed in this paper,
for several reasons:
• The optimization-based method explicitly
models the learner and thus generates code-
switched text progressively more fit for a
given individual, even across a sequence of
multiple texts. A frequency-based baseline
alone would generate content at approxi-
mately the same level of difficulty consis-
tently, with the pattern that words that tend to
have high frequency in the natural language
in general to be the ones that are “switched-
out” most often.
• An optimization-based approach is able to
elicit higher recall in low frequency words,
as the mechanism for their selection is driven
by the context in which these words appear,
rather than frequency alone, favoring those
that are learned more readily through context.
Moreover, the proposed method in this pa-
per is extensible to more sophisticated learner
models, with a potential to surpass the results
presented here. Another worthwhile applica-
tion of this method is as a nested component
within a larger optimization-based tool, that
in addition to generating code-switched text
as demonstrated here, aids in selecting con-
tent (such as popular books) as units in the
code-switched curriculum.
</bodyText>
<sectionHeader confidence="0.9994" genericHeader="discussions">
7 Future Work
</sectionHeader>
<bodyText confidence="0.999989272727273">
In this work we demonstrated a pilot implemen-
tation of a model-based, optimization-based ap-
proach to content generation for assisting in the
reading-based L2 language acquisition. Our ap-
proach is based on static optimization, and while
it would, in theory progress in difficulty with more
reading, its open-loop nature precludes it from
maintaining an accurate model of the learner in
the long-term. For generating effecting L2 con-
tent, it is important that the user be kept in a “zone
of proximal development” — a tight region where
the level of the taught content is at just the right
difficulty. Maintaining an accurate internal model
of the learner is the single most important require-
ment for achieving this functionality. Closed-loop
learning, with active user feedback is, thus, going
to be functionally critical component of any sys-
tem of this type that is designed to function in the
long-term.
Additionally, our approach is currently a proof-
of-concept of an automated method for generat-
ing content for assisted L2 acquisition, and is lim-
ited to artificial language and only isolated lexi-
cal items. The next step would be to integrate
bitext alignment across texts in two natural lan-
guages, inevitably introducing another stochas-
tic component into the pipeline. Extending this
method to larger units, like chunks and simple
grammar is another important avenue along which
we are taking this work. Early results from concur-
rent research indicate that “code-switched based”
method proposed here is also effective in eliciting
acquisition of multi-word chunks.
</bodyText>
<sectionHeader confidence="0.984791" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.3709685">
Heike Adel, Ngoc Thang Vu, Franziska Kraus, Tim
Schlippe, Haizhou Li, and Tanja Schultz. 2012. Re-
</bodyText>
<figure confidence="0.9962926">
●
●
Condition
HF
OPT
Recall 0.8
0.6
0.4
0.2
0.0
</figure>
<page confidence="0.946077">
570
</page>
<reference confidence="0.999593072727273">
current neural network language modeling for code
switching conversational speech. ICASSP.
David Beglar. 2010. A rasch-based validation of the
vocabulary size test. Language Testing, 27(1):101–
118.
Hedi M Belazi, Edward J Rubin, and Almeida Jacque-
line Toribio. 1994. Code switching and x-bar the-
ory: The functional head constraint. Linguistic in-
quiry, pages 221–237.
Suma Bhat and Richard Sproat. 2009. Knowing
the unseen: estimating vocabulary size over unseen
samples. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 1-Volume
1, pages 109–117. Association for Computational
Linguistics.
Rakesh Mohan Bhatt. 1997. Code-switching,
constraints, and optimal grammars. Lingua,
102(4):223–251.
Or Biran, Samuel Brody, and Noemie Elhadad. 2011.
Putting it simply: a context-aware approach to lexi-
cal simplification.
Fabian Blaicher. 2011. SMT-based Text Generation
for Code-Switching Language Models. Ph.D. thesis,
Nanyang Technological University, Singapore.
Arielle Borovsky, Jeffrey L Elman, and Marta Kutas.
2012. Once is enough: N400 indexes semantic inte-
gration of novel word meanings from a single expo-
sure in context. Language Learning and Develop-
ment, 8(3):278–302.
Tom Cobb. 2007. Computing the vocabulary demands
of l2 reading. Language Learning &amp; Technology,
11(3):38–63.
Noemie Elhadad and Komal Sutaria. 2007. Min-
ing a lexicon of technical terms and lay equivalents.
In Proceedings of the Workshop on BioNLP 2007:
Biological, Translational, and Clinical Language
Processing, pages 49–56. Association for Compu-
tational Linguistics.
Stefan L Frank, Leun J Otten, Giulia Galli, and
Gabriella Vigliocco. 2013. Word surprisal predicts
n400 amplitude during reading. In Proceedings of
the 51st annual meeting of the Association for Com-
putational Linguistics, pages 878–883.
Fred Genesee. 2001. Bilingual first language acqui-
sition: Exploring the limits of the language faculty.
Annual Review ofApplied Linguistics, 21:153–168.
David R Hill. 2008. Graded readers in english. ELT
journal, 62(2):184–204.
Ben Hutchinson. 2005. Modelling the substitutabil-
ity of discourse connectives. In Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 149–156. Association for
Computational Linguistics.
David Kauchak. 2013. Improving text simplification
language modeling using unsimplified text data. In
Proceedings ofACL.
Marta Kutas and Steven A Hillyard. 1984. Brain po-
tentials during reading reflect word expectancy and
semantic association. Nature.
James P Lantolf and Gabriela Appel. 1994. Vy-
gotskian approaches to second language research.
Greenwood Publishing Group.
John M Lipski. 2005. Code-switching or borrowing?
no s´e so no puedo decir, you know. In Selected Pro-
ceedings of the Second Workshop on Spanish Soci-
olinguistics, pages 1–15.
Ernesto Macaro. 2005. Codeswitching in the l2
classroom: A communication and learning strat-
egy. In Non-native language teachers, pages 63–84.
Springer.
Lesley Milroy and Pieter Muysken. 1995. One
speaker, two languages: Cross-disciplinary per-
spectives on code-switching. Cambridge University
Press.
Visvaganthie Moodley. 2010. Code-switching and
communicative competence in the language class-
room. Journal for Language Teaching, 44(1):7–22.
Ian SP Nation. 2001. Learning vocabulary in another
language. Ernst Klett Sprachen.
Richard C Schmidt and Richard W Schmidt. 1995. At-
tention and awareness in foreign language learning,
volume 9. Natl Foreign Lg Resource Ctr.
Norbert Schmitt, Diane Schmitt, and Caroline
Clapham. 2001. Developing and exploring the be-
haviour of two new versions of the vocabulary levels
test. Language testing, 18(1):55–88.
Advaith Siddharthan, Ani Nenkova, and Kathleen
McKeown. 2004. Syntactic simplification for im-
proving content selection in multi-document sum-
marization. In Proceedings of the 20th international
conference on Computational Linguistics, page 896.
Association for Computational Linguistics.
Advaith Siddharthan. 2006. Syntactic simplification
and text cohesion. Research on Language and Com-
putation, 4(1):77–109.
Thamar Solorio and Yang Liu. 2008. Learning to pre-
dict code-switching points. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 973–981. Association for
Computational Linguistics.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of sim-
plicity: Unsupervised extraction of lexical simplifi-
cations from wikipedia. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 365–368. Association for
Computational Linguistics.
</reference>
<page confidence="0.99786">
571
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.930661">
<title confidence="0.999928">Generating Code-switched Text for Lexical Learning</title>
<author confidence="0.973822">Igor Labutov Hod Lipson</author>
<affiliation confidence="0.999981">Cornell University Cornell University</affiliation>
<email confidence="0.995361">iil4@cornell.eduhod.lipson@cornell.edu</email>
<abstract confidence="0.997879210526316">A vast majority of L1 vocabulary acquisition occurs through incidental learning during reading (Nation, 2001; Schmitt et al., 2001). We propose a probabilistic approach to generating code-mixed text as an L2 technique for increasing retention in adult lexical learning through reading. Our model that takes as input a bilingual dictionary and an English text, and generates a code-switched text that optimizes a defined “learnability” metric by constructing a factor graph over lexical mentions. Using an artificial language vocabulary, we evaluate a set of algorithms for generating code-switched text automatically by it to Turk and measuring recall in a sentence completion task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>current neural network language modeling for code switching conversational speech.</title>
<publisher>ICASSP.</publisher>
<marker></marker>
<rawString>current neural network language modeling for code switching conversational speech. ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Beglar</author>
</authors>
<title>A rasch-based validation of the vocabulary size test.</title>
<date>2010</date>
<journal>Language Testing,</journal>
<volume>27</volume>
<issue>1</issue>
<pages>118</pages>
<contexts>
<context position="14631" citStr="Beglar, 2010" startWordPosition="2299" endWordPosition="2300">ner knowing the corresponding word. We motivate a probabilistic approach by taking the perspective of measuring our belief in the learner’s knowledge of any given word, rather than the learner’s uncertainty in own knowledge. Formally, we can fully specify this model for learner i as follows: Ui = (gyri0, gyri1, ... , gyri |) (1) where V is the vocabulary set — identical across all users, and gyrij is our degree of belief in the learner i’s knowledge of a target word wj ∈ V . Statistical estimation techniques exist for estimating an individual’s vocabulary size, such as (Bhat and Sproat, 2009; Beglar, 2010), and can be di565 rectly employed for estimating the parameters of this model as our prior belief about user i’s knowledge. The primary motivation behind a probabilistic user model, is to provide a mechanism for updating these probabilities as the user progresses through her reading. Maximizing the parameters of the PLM under a given finite span of codeswitched text, thus, provides a handle for generating optimal code-switched content. Additionally, a probabilistic approach allows for a natural integration of the user model with the uncertainty in other components of the system, such as uncer</context>
</contexts>
<marker>Beglar, 2010</marker>
<rawString>David Beglar. 2010. A rasch-based validation of the vocabulary size test. Language Testing, 27(1):101– 118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hedi M Belazi</author>
<author>Edward J Rubin</author>
<author>Almeida Jacqueline Toribio</author>
</authors>
<title>Code switching and x-bar theory: The functional head constraint. Linguistic inquiry,</title>
<date>1994</date>
<pages>221--237</pages>
<contexts>
<context position="2542" citStr="Belazi et al., 1994" startWordPosition="385" endWordPosition="389">g for L2 learners than any model studied previously. By infusing foreign words into text in the learner’s native tongue into low-surprisal contexts, the lexical acquisition process is facilitated naturally and non-obtrusively. Incidentally, this phenomenon occurs “in the wild” and is termed code-switching or code-mixing, and refers to the linguistic pattern of bilingual speakers swapping words and phrases between two languages during speech. While this phenomenon had received significant attention from both a socio-linguistic (Milroy and Muysken, 1995) and theoretical linguistic perspectives (Belazi et al., 1994; Bhatt, 1997) (including some computational studies), only recently has it been hypothesizes that “code-switching” is a marking of bilingual proficiency, rather than deficiency (Genesee, 2001). Until recently it was widely believed that incidental lexical acquisition through reading can only occur for words that occur at sufficient density in a single text, so as to elicit the “noticing” effect needed for lexical acquisition to occur (Cobb, 2007). Recent neurophysiological findings, however, indicate that even a single incidental exposure to a novel word in a sufficiently constrained context </context>
</contexts>
<marker>Belazi, Rubin, Toribio, 1994</marker>
<rawString>Hedi M Belazi, Edward J Rubin, and Almeida Jacqueline Toribio. 1994. Code switching and x-bar theory: The functional head constraint. Linguistic inquiry, pages 221–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suma Bhat</author>
<author>Richard Sproat</author>
</authors>
<title>Knowing the unseen: estimating vocabulary size over unseen samples.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>1</volume>
<pages>109--117</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14616" citStr="Bhat and Sproat, 2009" startWordPosition="2295" endWordPosition="2298">probability of the learner knowing the corresponding word. We motivate a probabilistic approach by taking the perspective of measuring our belief in the learner’s knowledge of any given word, rather than the learner’s uncertainty in own knowledge. Formally, we can fully specify this model for learner i as follows: Ui = (gyri0, gyri1, ... , gyri |) (1) where V is the vocabulary set — identical across all users, and gyrij is our degree of belief in the learner i’s knowledge of a target word wj ∈ V . Statistical estimation techniques exist for estimating an individual’s vocabulary size, such as (Bhat and Sproat, 2009; Beglar, 2010), and can be di565 rectly employed for estimating the parameters of this model as our prior belief about user i’s knowledge. The primary motivation behind a probabilistic user model, is to provide a mechanism for updating these probabilities as the user progresses through her reading. Maximizing the parameters of the PLM under a given finite span of codeswitched text, thus, provides a handle for generating optimal code-switched content. Additionally, a probabilistic approach allows for a natural integration of the user model with the uncertainty in other components of the system</context>
</contexts>
<marker>Bhat, Sproat, 2009</marker>
<rawString>Suma Bhat and Richard Sproat. 2009. Knowing the unseen: estimating vocabulary size over unseen samples. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pages 109–117. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rakesh Mohan Bhatt</author>
</authors>
<title>Code-switching, constraints, and optimal grammars.</title>
<date>1997</date>
<journal>Lingua,</journal>
<volume>102</volume>
<issue>4</issue>
<contexts>
<context position="2556" citStr="Bhatt, 1997" startWordPosition="390" endWordPosition="391">n any model studied previously. By infusing foreign words into text in the learner’s native tongue into low-surprisal contexts, the lexical acquisition process is facilitated naturally and non-obtrusively. Incidentally, this phenomenon occurs “in the wild” and is termed code-switching or code-mixing, and refers to the linguistic pattern of bilingual speakers swapping words and phrases between two languages during speech. While this phenomenon had received significant attention from both a socio-linguistic (Milroy and Muysken, 1995) and theoretical linguistic perspectives (Belazi et al., 1994; Bhatt, 1997) (including some computational studies), only recently has it been hypothesizes that “code-switching” is a marking of bilingual proficiency, rather than deficiency (Genesee, 2001). Until recently it was widely believed that incidental lexical acquisition through reading can only occur for words that occur at sufficient density in a single text, so as to elicit the “noticing” effect needed for lexical acquisition to occur (Cobb, 2007). Recent neurophysiological findings, however, indicate that even a single incidental exposure to a novel word in a sufficiently constrained context is sufficient </context>
</contexts>
<marker>Bhatt, 1997</marker>
<rawString>Rakesh Mohan Bhatt. 1997. Code-switching, constraints, and optimal grammars. Lingua, 102(4):223–251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Or Biran</author>
<author>Samuel Brody</author>
<author>Noemie Elhadad</author>
</authors>
<title>Putting it simply: a context-aware approach to lexical simplification.</title>
<date>2011</date>
<contexts>
<context position="9192" citStr="Biran et al., 2011" startWordPosition="1408" endWordPosition="1411">enerating the text, as no studies have been published to date. 2.3 Computational Approaches to Sentence Simplification Although not explicitly for teaching language, computational approaches that facilitate accessibility to texts that might otherwise be too difficult for its readers, either due to physical or learning disabilities, or language barriers, are relevant. In the recent work of (Kauchak, 2013), for example demonstrates an approach to increasing readability of texts by learning from unsimplified texts. Approaches in this area span methods for simplifying lexis (Yatskar et al., 2010; Biran et al., 2011), syntax (Siddharthan, 2006; Siddharthan et al., 2004), discourse properties (Hutchinson, 2005), and making technical terminology more accessible to non-experts (Elhadad and Sutaria, 2007). While the resulting texts are of great potential aid to language learners and may implicitly improve upon a reader’s language proficiency, they do not explicitly attempt to promote learning as an objective in generating the simplified text. 2.4 Recent Neurophysiological findings Evidence for the potential effectiveness of codeswitching for language acquisition, stem from the recent findings of (Borovsky et </context>
</contexts>
<marker>Biran, Brody, Elhadad, 2011</marker>
<rawString>Or Biran, Samuel Brody, and Noemie Elhadad. 2011. Putting it simply: a context-aware approach to lexical simplification.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Blaicher</author>
</authors>
<title>SMT-based Text Generation for Code-Switching Language Models.</title>
<date>2011</date>
<tech>Ph.D. thesis,</tech>
<institution>Nanyang Technological University, Singapore.</institution>
<marker>Blaicher, 2011</marker>
<rawString>Fabian Blaicher. 2011. SMT-based Text Generation for Code-Switching Language Models. Ph.D. thesis, Nanyang Technological University, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arielle Borovsky</author>
<author>Jeffrey L Elman</author>
<author>Marta Kutas</author>
</authors>
<title>Once is enough: N400 indexes semantic integration of novel word meanings from a single exposure in context.</title>
<date>2012</date>
<journal>Language Learning and Development,</journal>
<volume>8</volume>
<issue>3</issue>
<contexts>
<context position="3255" citStr="Borovsky et al., 2012" startWordPosition="496" endWordPosition="499">es that “code-switching” is a marking of bilingual proficiency, rather than deficiency (Genesee, 2001). Until recently it was widely believed that incidental lexical acquisition through reading can only occur for words that occur at sufficient density in a single text, so as to elicit the “noticing” effect needed for lexical acquisition to occur (Cobb, 2007). Recent neurophysiological findings, however, indicate that even a single incidental exposure to a novel word in a sufficiently constrained context is sufficient to trigger an early integration of the word in the brain’s semantic network (Borovsky et al., 2012). An approach explored in this paper, and motivated by the above findings, exploits “constraining” contexts in text to introduce novel words. A state-of-the-art approach for generating such text is based on an expert annotator whose job is to decide which words to “switch out” with novel foreign words (from hereon we will refer to the “switched out” word as the source word and to the “switched in” word as the target word). Consequently the process is labor-intensive and leads to 562 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 562–571, Baltimor</context>
<context position="9802" citStr="Borovsky et al., 2012" startWordPosition="1495" endWordPosition="1498">t al., 2011), syntax (Siddharthan, 2006; Siddharthan et al., 2004), discourse properties (Hutchinson, 2005), and making technical terminology more accessible to non-experts (Elhadad and Sutaria, 2007). While the resulting texts are of great potential aid to language learners and may implicitly improve upon a reader’s language proficiency, they do not explicitly attempt to promote learning as an objective in generating the simplified text. 2.4 Recent Neurophysiological findings Evidence for the potential effectiveness of codeswitching for language acquisition, stem from the recent findings of (Borovsky et al., 2012), who have shown that even a single exposure to a novel word in a constrained context, results in the integration of the word within your existing semantic base, as indicated by a change in the N400 electrophysiological response recorded from the subjects’ scalps. N400 ERP marker has been found to correlate with the semantic “expectedness” of a word (Kutas and Hillyard, 1984), and is believed to be an early indicator of word learning. Furthermore, recent work of (Frank et al., 2013), show that word surprisal predicts N400, providing concrete motivation for artificial manipulation of text to ex</context>
<context position="11327" citStr="Borovsky et al., 2012" startWordPosition="1742" endWordPosition="1745">ticing” effect — a prerequisite to lexical acquisition (Schmidt and Schmidt, 1995; Cobb, 2007). 3 Model 3.1 Overview The formulation of our model is primarily motivated by two hypotheses that have been validated experimentally in the cognitive science literature. We re-state these hypotheses in the language of “surprisal”: 1. Inserting a target word into a low surprisal context increases the rate of that word’s integration into a learner’s lexicon. 2. Multiple exposures to the word in low surprisal contexts increases rate of that word’s integration. Hypothesis 1 is supported by evidence from (Borovsky et al., 2012; Frank et al., 2013), and hypothesis 2 is supported by evidence from (Schmidt and Schmidt, 1995). We adopt the term “lowsurprisal” context to identify contexts (e.g. ngrams) that are highly predictive of the target word (e.g. trailing word in the n-gram). The motivation stems from the recent evidence (Frank et al., 2013) that low-surprisal contexts affect the N400 response and thus correlate with word acquisition. To realize a “code-switched” mixture that adheres maximally to the above postulates, it is self-evident that a non-trivial optimization problem must be solved. For example, naively </context>
</contexts>
<marker>Borovsky, Elman, Kutas, 2012</marker>
<rawString>Arielle Borovsky, Jeffrey L Elman, and Marta Kutas. 2012. Once is enough: N400 indexes semantic integration of novel word meanings from a single exposure in context. Language Learning and Development, 8(3):278–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Cobb</author>
</authors>
<title>Computing the vocabulary demands of l2 reading.</title>
<date>2007</date>
<journal>Language Learning &amp; Technology,</journal>
<volume>11</volume>
<issue>3</issue>
<contexts>
<context position="2993" citStr="Cobb, 2007" startWordPosition="456" endWordPosition="457">enomenon had received significant attention from both a socio-linguistic (Milroy and Muysken, 1995) and theoretical linguistic perspectives (Belazi et al., 1994; Bhatt, 1997) (including some computational studies), only recently has it been hypothesizes that “code-switching” is a marking of bilingual proficiency, rather than deficiency (Genesee, 2001). Until recently it was widely believed that incidental lexical acquisition through reading can only occur for words that occur at sufficient density in a single text, so as to elicit the “noticing” effect needed for lexical acquisition to occur (Cobb, 2007). Recent neurophysiological findings, however, indicate that even a single incidental exposure to a novel word in a sufficiently constrained context is sufficient to trigger an early integration of the word in the brain’s semantic network (Borovsky et al., 2012). An approach explored in this paper, and motivated by the above findings, exploits “constraining” contexts in text to introduce novel words. A state-of-the-art approach for generating such text is based on an expert annotator whose job is to decide which words to “switch out” with novel foreign words (from hereon we will refer to the “</context>
<context position="10800" citStr="Cobb, 2007" startWordPosition="1659" endWordPosition="1660">believed to be an early indicator of word learning. Furthermore, recent work of (Frank et al., 2013), show that word surprisal predicts N400, providing concrete motivation for artificial manipulation of text to explicitly elicit word learning through natural reading, directly motivating our approach. Prior to the above findings, it was widely believed that for evoking “incidental” word learning through reading alone, the word must appear with sufficiently high frequency within the text, such as to elicit the “noticing” effect — a prerequisite to lexical acquisition (Schmidt and Schmidt, 1995; Cobb, 2007). 3 Model 3.1 Overview The formulation of our model is primarily motivated by two hypotheses that have been validated experimentally in the cognitive science literature. We re-state these hypotheses in the language of “surprisal”: 1. Inserting a target word into a low surprisal context increases the rate of that word’s integration into a learner’s lexicon. 2. Multiple exposures to the word in low surprisal contexts increases rate of that word’s integration. Hypothesis 1 is supported by evidence from (Borovsky et al., 2012; Frank et al., 2013), and hypothesis 2 is supported by evidence from (Sc</context>
</contexts>
<marker>Cobb, 2007</marker>
<rawString>Tom Cobb. 2007. Computing the vocabulary demands of l2 reading. Language Learning &amp; Technology, 11(3):38–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noemie Elhadad</author>
<author>Komal Sutaria</author>
</authors>
<title>Mining a lexicon of technical terms and lay equivalents.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on BioNLP 2007: Biological, Translational, and Clinical Language Processing,</booktitle>
<pages>49--56</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9380" citStr="Elhadad and Sutaria, 2007" startWordPosition="1431" endWordPosition="1434">pproaches that facilitate accessibility to texts that might otherwise be too difficult for its readers, either due to physical or learning disabilities, or language barriers, are relevant. In the recent work of (Kauchak, 2013), for example demonstrates an approach to increasing readability of texts by learning from unsimplified texts. Approaches in this area span methods for simplifying lexis (Yatskar et al., 2010; Biran et al., 2011), syntax (Siddharthan, 2006; Siddharthan et al., 2004), discourse properties (Hutchinson, 2005), and making technical terminology more accessible to non-experts (Elhadad and Sutaria, 2007). While the resulting texts are of great potential aid to language learners and may implicitly improve upon a reader’s language proficiency, they do not explicitly attempt to promote learning as an objective in generating the simplified text. 2.4 Recent Neurophysiological findings Evidence for the potential effectiveness of codeswitching for language acquisition, stem from the recent findings of (Borovsky et al., 2012), who have shown that even a single exposure to a novel word in a constrained context, results in the integration of the word within your existing semantic base, as indicated by </context>
</contexts>
<marker>Elhadad, Sutaria, 2007</marker>
<rawString>Noemie Elhadad and Komal Sutaria. 2007. Mining a lexicon of technical terms and lay equivalents. In Proceedings of the Workshop on BioNLP 2007: Biological, Translational, and Clinical Language Processing, pages 49–56. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan L Frank</author>
<author>Leun J Otten</author>
<author>Giulia Galli</author>
<author>Gabriella Vigliocco</author>
</authors>
<title>Word surprisal predicts n400 amplitude during reading.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>878--883</pages>
<contexts>
<context position="10289" citStr="Frank et al., 2013" startWordPosition="1580" endWordPosition="1583"> for the potential effectiveness of codeswitching for language acquisition, stem from the recent findings of (Borovsky et al., 2012), who have shown that even a single exposure to a novel word in a constrained context, results in the integration of the word within your existing semantic base, as indicated by a change in the N400 electrophysiological response recorded from the subjects’ scalps. N400 ERP marker has been found to correlate with the semantic “expectedness” of a word (Kutas and Hillyard, 1984), and is believed to be an early indicator of word learning. Furthermore, recent work of (Frank et al., 2013), show that word surprisal predicts N400, providing concrete motivation for artificial manipulation of text to explicitly elicit word learning through natural reading, directly motivating our approach. Prior to the above findings, it was widely believed that for evoking “incidental” word learning through reading alone, the word must appear with sufficiently high frequency within the text, such as to elicit the “noticing” effect — a prerequisite to lexical acquisition (Schmidt and Schmidt, 1995; Cobb, 2007). 3 Model 3.1 Overview The formulation of our model is primarily motivated by two hypothe</context>
<context position="11650" citStr="Frank et al., 2013" startWordPosition="1798" endWordPosition="1801">1. Inserting a target word into a low surprisal context increases the rate of that word’s integration into a learner’s lexicon. 2. Multiple exposures to the word in low surprisal contexts increases rate of that word’s integration. Hypothesis 1 is supported by evidence from (Borovsky et al., 2012; Frank et al., 2013), and hypothesis 2 is supported by evidence from (Schmidt and Schmidt, 1995). We adopt the term “lowsurprisal” context to identify contexts (e.g. ngrams) that are highly predictive of the target word (e.g. trailing word in the n-gram). The motivation stems from the recent evidence (Frank et al., 2013) that low-surprisal contexts affect the N400 response and thus correlate with word acquisition. To realize a “code-switched” mixture that adheres maximally to the above postulates, it is self-evident that a non-trivial optimization problem must be solved. For example, naively selecting a few words that appear in low-surprisal contexts may facilitate their acquisition, but at the expense of other words within the same context that may appear in a larger number of low-surprisal contexts further in the text. To address this problem, we approach it with a formulation of a factor graph that takes g</context>
</contexts>
<marker>Frank, Otten, Galli, Vigliocco, 2013</marker>
<rawString>Stefan L Frank, Leun J Otten, Giulia Galli, and Gabriella Vigliocco. 2013. Word surprisal predicts n400 amplitude during reading. In Proceedings of the 51st annual meeting of the Association for Computational Linguistics, pages 878–883.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred Genesee</author>
</authors>
<title>Bilingual first language acquisition: Exploring the limits of the language faculty. Annual Review ofApplied Linguistics,</title>
<date>2001</date>
<pages>21--153</pages>
<contexts>
<context position="2735" citStr="Genesee, 2001" startWordPosition="413" endWordPosition="414">turally and non-obtrusively. Incidentally, this phenomenon occurs “in the wild” and is termed code-switching or code-mixing, and refers to the linguistic pattern of bilingual speakers swapping words and phrases between two languages during speech. While this phenomenon had received significant attention from both a socio-linguistic (Milroy and Muysken, 1995) and theoretical linguistic perspectives (Belazi et al., 1994; Bhatt, 1997) (including some computational studies), only recently has it been hypothesizes that “code-switching” is a marking of bilingual proficiency, rather than deficiency (Genesee, 2001). Until recently it was widely believed that incidental lexical acquisition through reading can only occur for words that occur at sufficient density in a single text, so as to elicit the “noticing” effect needed for lexical acquisition to occur (Cobb, 2007). Recent neurophysiological findings, however, indicate that even a single incidental exposure to a novel word in a sufficiently constrained context is sufficient to trigger an early integration of the word in the brain’s semantic network (Borovsky et al., 2012). An approach explored in this paper, and motivated by the above findings, explo</context>
<context position="6600" citStr="Genesee, 2001" startWordPosition="1021" endWordPosition="1022"> over the course of the last three decades, across the disciplines of sociolinguistics, theoretical and psycholinguistics and even literary and cultural studies (predominantly in the domain of SpanishEnglish code-switching) (Lipski, 2005). Code-switching that occurs naturally in bilingual populations, and especially in children, has for a long time been considered a marking of incompetency in the second language. A more recent view on this phenomenon, however, suggests that due to the underlying syntactic complexity of code-switching, code-switching is actually a marking of bilingual fluency (Genesee, 2001). More recently, the idea of employing code-switching in the classroom, in a form of conversation-based exercises, has attracted the attention of multiple researchers and educators (Moodley, 2010; Macaro, 2005), yielding promising results in an elementary school study in SouthAfrica. 2.2 Computational Approaches to Code-switching Additionally, there has been a limited number of studies of the computational approaches to code-switching, and in particular code-switched text generation. Solorio and Liu (2008), record and transcribe a corpus of Spanish-English codemixed conversation to train a gen</context>
</contexts>
<marker>Genesee, 2001</marker>
<rawString>Fred Genesee. 2001. Bilingual first language acquisition: Exploring the limits of the language faculty. Annual Review ofApplied Linguistics, 21:153–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Hill</author>
</authors>
<title>Graded readers in english.</title>
<date>2008</date>
<journal>ELT journal,</journal>
<pages>62--2</pages>
<contexts>
<context position="4183" citStr="Hill, 2008" startWordPosition="646" endWordPosition="647">e will refer to the “switched out” word as the source word and to the “switched in” word as the target word). Consequently the process is labor-intensive and leads to 562 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 562–571, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics a “one size fits all solution” that is insensitive to the learner’s skill level or vocabulary proficiency. This limitation is also cited in literature as a significant roadblock to the widespread adaptation of graded reading series (Hill, 2008). A readingbased tool that follows the same principle, i.e. by systematic exposure of a learner to an incrementally more challenging text, will result in more effective learning (Lantolf and Appel, 1994). To address the above limitation, we develop an approach for automatically generating such “codeswitched” text with an explicit goal of maximizing the lexical acquisition rate in adults. Our method is based on a global optimization approach that incorporates a “knowledge model” of a user with the content of the text, to generate a sequence of lexical “switches”. To facilitate the selection of </context>
</contexts>
<marker>Hill, 2008</marker>
<rawString>David R Hill. 2008. Graded readers in english. ELT journal, 62(2):184–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Hutchinson</author>
</authors>
<title>Modelling the substitutability of discourse connectives.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>149--156</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9287" citStr="Hutchinson, 2005" startWordPosition="1421" endWordPosition="1422">ntence Simplification Although not explicitly for teaching language, computational approaches that facilitate accessibility to texts that might otherwise be too difficult for its readers, either due to physical or learning disabilities, or language barriers, are relevant. In the recent work of (Kauchak, 2013), for example demonstrates an approach to increasing readability of texts by learning from unsimplified texts. Approaches in this area span methods for simplifying lexis (Yatskar et al., 2010; Biran et al., 2011), syntax (Siddharthan, 2006; Siddharthan et al., 2004), discourse properties (Hutchinson, 2005), and making technical terminology more accessible to non-experts (Elhadad and Sutaria, 2007). While the resulting texts are of great potential aid to language learners and may implicitly improve upon a reader’s language proficiency, they do not explicitly attempt to promote learning as an objective in generating the simplified text. 2.4 Recent Neurophysiological findings Evidence for the potential effectiveness of codeswitching for language acquisition, stem from the recent findings of (Borovsky et al., 2012), who have shown that even a single exposure to a novel word in a constrained context</context>
</contexts>
<marker>Hutchinson, 2005</marker>
<rawString>Ben Hutchinson. 2005. Modelling the substitutability of discourse connectives. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 149–156. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kauchak</author>
</authors>
<title>Improving text simplification language modeling using unsimplified text data.</title>
<date>2013</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="8980" citStr="Kauchak, 2013" startWordPosition="1376" endWordPosition="1377"> by generating code-switched web content, and allowing its users to tune it to their skill level. It does not, however, seem to model the user explicitly, nor is it clear if it performs any optimization in generating the text, as no studies have been published to date. 2.3 Computational Approaches to Sentence Simplification Although not explicitly for teaching language, computational approaches that facilitate accessibility to texts that might otherwise be too difficult for its readers, either due to physical or learning disabilities, or language barriers, are relevant. In the recent work of (Kauchak, 2013), for example demonstrates an approach to increasing readability of texts by learning from unsimplified texts. Approaches in this area span methods for simplifying lexis (Yatskar et al., 2010; Biran et al., 2011), syntax (Siddharthan, 2006; Siddharthan et al., 2004), discourse properties (Hutchinson, 2005), and making technical terminology more accessible to non-experts (Elhadad and Sutaria, 2007). While the resulting texts are of great potential aid to language learners and may implicitly improve upon a reader’s language proficiency, they do not explicitly attempt to promote learning as an ob</context>
</contexts>
<marker>Kauchak, 2013</marker>
<rawString>David Kauchak. 2013. Improving text simplification language modeling using unsimplified text data. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Kutas</author>
<author>Steven A Hillyard</author>
</authors>
<title>Brain potentials during reading reflect word expectancy and semantic association.</title>
<date>1984</date>
<journal>Nature.</journal>
<contexts>
<context position="10180" citStr="Kutas and Hillyard, 1984" startWordPosition="1560" endWordPosition="1563">promote learning as an objective in generating the simplified text. 2.4 Recent Neurophysiological findings Evidence for the potential effectiveness of codeswitching for language acquisition, stem from the recent findings of (Borovsky et al., 2012), who have shown that even a single exposure to a novel word in a constrained context, results in the integration of the word within your existing semantic base, as indicated by a change in the N400 electrophysiological response recorded from the subjects’ scalps. N400 ERP marker has been found to correlate with the semantic “expectedness” of a word (Kutas and Hillyard, 1984), and is believed to be an early indicator of word learning. Furthermore, recent work of (Frank et al., 2013), show that word surprisal predicts N400, providing concrete motivation for artificial manipulation of text to explicitly elicit word learning through natural reading, directly motivating our approach. Prior to the above findings, it was widely believed that for evoking “incidental” word learning through reading alone, the word must appear with sufficiently high frequency within the text, such as to elicit the “noticing” effect — a prerequisite to lexical acquisition (Schmidt and Schmid</context>
</contexts>
<marker>Kutas, Hillyard, 1984</marker>
<rawString>Marta Kutas and Steven A Hillyard. 1984. Brain potentials during reading reflect word expectancy and semantic association. Nature.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James P Lantolf</author>
<author>Gabriela Appel</author>
</authors>
<title>Vygotskian approaches to second language research.</title>
<date>1994</date>
<publisher>Greenwood Publishing Group.</publisher>
<contexts>
<context position="4386" citStr="Lantolf and Appel, 1994" startWordPosition="678" endWordPosition="681">52nd Annual Meeting of the Association for Computational Linguistics, pages 562–571, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics a “one size fits all solution” that is insensitive to the learner’s skill level or vocabulary proficiency. This limitation is also cited in literature as a significant roadblock to the widespread adaptation of graded reading series (Hill, 2008). A readingbased tool that follows the same principle, i.e. by systematic exposure of a learner to an incrementally more challenging text, will result in more effective learning (Lantolf and Appel, 1994). To address the above limitation, we develop an approach for automatically generating such “codeswitched” text with an explicit goal of maximizing the lexical acquisition rate in adults. Our method is based on a global optimization approach that incorporates a “knowledge model” of a user with the content of the text, to generate a sequence of lexical “switches”. To facilitate the selection of “switch points”, we learn a discriminative model for predicting switch point locations on a corpus that we collect for this purpose (and release to the community). Below is a high-level outline of this p</context>
</contexts>
<marker>Lantolf, Appel, 1994</marker>
<rawString>James P Lantolf and Gabriela Appel. 1994. Vygotskian approaches to second language research. Greenwood Publishing Group.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Lipski</author>
</authors>
<title>Code-switching or borrowing? no s´e so no puedo decir, you know.</title>
<date>2005</date>
<booktitle>In Selected Proceedings of the Second Workshop on Spanish Sociolinguistics,</booktitle>
<pages>1--15</pages>
<contexts>
<context position="6224" citStr="Lipski, 2005" startWordPosition="963" endWordPosition="964">died aspects of this phenomenon from distinct perspectives. In this section, we briefly describe a motivation from the areas of socio- and psycho- linguistics and language pedagogy research that indicate the promise of this approach. 2.1 Code-switching as a natural phenomenon Code-switching (or code-mixing) is a widely studied phenomenon that received significant attention over the course of the last three decades, across the disciplines of sociolinguistics, theoretical and psycholinguistics and even literary and cultural studies (predominantly in the domain of SpanishEnglish code-switching) (Lipski, 2005). Code-switching that occurs naturally in bilingual populations, and especially in children, has for a long time been considered a marking of incompetency in the second language. A more recent view on this phenomenon, however, suggests that due to the underlying syntactic complexity of code-switching, code-switching is actually a marking of bilingual fluency (Genesee, 2001). More recently, the idea of employing code-switching in the classroom, in a form of conversation-based exercises, has attracted the attention of multiple researchers and educators (Moodley, 2010; Macaro, 2005), yielding pro</context>
</contexts>
<marker>Lipski, 2005</marker>
<rawString>John M Lipski. 2005. Code-switching or borrowing? no s´e so no puedo decir, you know. In Selected Proceedings of the Second Workshop on Spanish Sociolinguistics, pages 1–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ernesto Macaro</author>
</authors>
<title>Codeswitching in the l2 classroom: A communication and learning strategy. In Non-native language teachers,</title>
<date>2005</date>
<pages>63--84</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="6810" citStr="Macaro, 2005" startWordPosition="1050" endWordPosition="1051">e-switching) (Lipski, 2005). Code-switching that occurs naturally in bilingual populations, and especially in children, has for a long time been considered a marking of incompetency in the second language. A more recent view on this phenomenon, however, suggests that due to the underlying syntactic complexity of code-switching, code-switching is actually a marking of bilingual fluency (Genesee, 2001). More recently, the idea of employing code-switching in the classroom, in a form of conversation-based exercises, has attracted the attention of multiple researchers and educators (Moodley, 2010; Macaro, 2005), yielding promising results in an elementary school study in SouthAfrica. 2.2 Computational Approaches to Code-switching Additionally, there has been a limited number of studies of the computational approaches to code-switching, and in particular code-switched text generation. Solorio and Liu (2008), record and transcribe a corpus of Spanish-English codemixed conversation to train a generative model (Naive Bayes) for the task of predicting codeswitch points in conversation. Additionally they test their trained model in its ability to generate code-switched text with convincing results. Buildi</context>
</contexts>
<marker>Macaro, 2005</marker>
<rawString>Ernesto Macaro. 2005. Codeswitching in the l2 classroom: A communication and learning strategy. In Non-native language teachers, pages 63–84. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lesley Milroy</author>
<author>Pieter Muysken</author>
</authors>
<title>One speaker, two languages: Cross-disciplinary perspectives on code-switching.</title>
<date>1995</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2481" citStr="Milroy and Muysken, 1995" startWordPosition="377" endWordPosition="380"> vocabulary learning in adults — significantly closer to L1 learning for L2 learners than any model studied previously. By infusing foreign words into text in the learner’s native tongue into low-surprisal contexts, the lexical acquisition process is facilitated naturally and non-obtrusively. Incidentally, this phenomenon occurs “in the wild” and is termed code-switching or code-mixing, and refers to the linguistic pattern of bilingual speakers swapping words and phrases between two languages during speech. While this phenomenon had received significant attention from both a socio-linguistic (Milroy and Muysken, 1995) and theoretical linguistic perspectives (Belazi et al., 1994; Bhatt, 1997) (including some computational studies), only recently has it been hypothesizes that “code-switching” is a marking of bilingual proficiency, rather than deficiency (Genesee, 2001). Until recently it was widely believed that incidental lexical acquisition through reading can only occur for words that occur at sufficient density in a single text, so as to elicit the “noticing” effect needed for lexical acquisition to occur (Cobb, 2007). Recent neurophysiological findings, however, indicate that even a single incidental ex</context>
</contexts>
<marker>Milroy, Muysken, 1995</marker>
<rawString>Lesley Milroy and Pieter Muysken. 1995. One speaker, two languages: Cross-disciplinary perspectives on code-switching. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Visvaganthie Moodley</author>
</authors>
<title>Code-switching and communicative competence in the language classroom.</title>
<date>2010</date>
<journal>Journal for Language Teaching,</journal>
<volume>44</volume>
<issue>1</issue>
<contexts>
<context position="6795" citStr="Moodley, 2010" startWordPosition="1048" endWordPosition="1049">nishEnglish code-switching) (Lipski, 2005). Code-switching that occurs naturally in bilingual populations, and especially in children, has for a long time been considered a marking of incompetency in the second language. A more recent view on this phenomenon, however, suggests that due to the underlying syntactic complexity of code-switching, code-switching is actually a marking of bilingual fluency (Genesee, 2001). More recently, the idea of employing code-switching in the classroom, in a form of conversation-based exercises, has attracted the attention of multiple researchers and educators (Moodley, 2010; Macaro, 2005), yielding promising results in an elementary school study in SouthAfrica. 2.2 Computational Approaches to Code-switching Additionally, there has been a limited number of studies of the computational approaches to code-switching, and in particular code-switched text generation. Solorio and Liu (2008), record and transcribe a corpus of Spanish-English codemixed conversation to train a generative model (Naive Bayes) for the task of predicting codeswitch points in conversation. Additionally they test their trained model in its ability to generate code-switched text with convincing </context>
</contexts>
<marker>Moodley, 2010</marker>
<rawString>Visvaganthie Moodley. 2010. Code-switching and communicative competence in the language classroom. Journal for Language Teaching, 44(1):7–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian SP Nation</author>
</authors>
<title>Learning vocabulary in another language. Ernst Klett Sprachen.</title>
<date>2001</date>
<marker>Nation, 2001</marker>
<rawString>Ian SP Nation. 2001. Learning vocabulary in another language. Ernst Klett Sprachen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard C Schmidt</author>
<author>Richard W Schmidt</author>
</authors>
<title>Attention and awareness in foreign language learning, volume 9. Natl Foreign Lg Resource Ctr.</title>
<date>1995</date>
<contexts>
<context position="10787" citStr="Schmidt and Schmidt, 1995" startWordPosition="1655" endWordPosition="1658">nd Hillyard, 1984), and is believed to be an early indicator of word learning. Furthermore, recent work of (Frank et al., 2013), show that word surprisal predicts N400, providing concrete motivation for artificial manipulation of text to explicitly elicit word learning through natural reading, directly motivating our approach. Prior to the above findings, it was widely believed that for evoking “incidental” word learning through reading alone, the word must appear with sufficiently high frequency within the text, such as to elicit the “noticing” effect — a prerequisite to lexical acquisition (Schmidt and Schmidt, 1995; Cobb, 2007). 3 Model 3.1 Overview The formulation of our model is primarily motivated by two hypotheses that have been validated experimentally in the cognitive science literature. We re-state these hypotheses in the language of “surprisal”: 1. Inserting a target word into a low surprisal context increases the rate of that word’s integration into a learner’s lexicon. 2. Multiple exposures to the word in low surprisal contexts increases rate of that word’s integration. Hypothesis 1 is supported by evidence from (Borovsky et al., 2012; Frank et al., 2013), and hypothesis 2 is supported by evid</context>
</contexts>
<marker>Schmidt, Schmidt, 1995</marker>
<rawString>Richard C Schmidt and Richard W Schmidt. 1995. Attention and awareness in foreign language learning, volume 9. Natl Foreign Lg Resource Ctr.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norbert Schmitt</author>
<author>Diane Schmitt</author>
<author>Caroline Clapham</author>
</authors>
<title>Developing and exploring the behaviour of two new versions of the vocabulary levels test. Language testing,</title>
<date>2001</date>
<pages>18--1</pages>
<marker>Schmitt, Schmitt, Clapham, 2001</marker>
<rawString>Norbert Schmitt, Diane Schmitt, and Caroline Clapham. 2001. Developing and exploring the behaviour of two new versions of the vocabulary levels test. Language testing, 18(1):55–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
<author>Ani Nenkova</author>
<author>Kathleen McKeown</author>
</authors>
<title>Syntactic simplification for improving content selection in multi-document summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>896</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9246" citStr="Siddharthan et al., 2004" startWordPosition="1415" endWordPosition="1418">ished to date. 2.3 Computational Approaches to Sentence Simplification Although not explicitly for teaching language, computational approaches that facilitate accessibility to texts that might otherwise be too difficult for its readers, either due to physical or learning disabilities, or language barriers, are relevant. In the recent work of (Kauchak, 2013), for example demonstrates an approach to increasing readability of texts by learning from unsimplified texts. Approaches in this area span methods for simplifying lexis (Yatskar et al., 2010; Biran et al., 2011), syntax (Siddharthan, 2006; Siddharthan et al., 2004), discourse properties (Hutchinson, 2005), and making technical terminology more accessible to non-experts (Elhadad and Sutaria, 2007). While the resulting texts are of great potential aid to language learners and may implicitly improve upon a reader’s language proficiency, they do not explicitly attempt to promote learning as an objective in generating the simplified text. 2.4 Recent Neurophysiological findings Evidence for the potential effectiveness of codeswitching for language acquisition, stem from the recent findings of (Borovsky et al., 2012), who have shown that even a single exposure</context>
</contexts>
<marker>Siddharthan, Nenkova, McKeown, 2004</marker>
<rawString>Advaith Siddharthan, Ani Nenkova, and Kathleen McKeown. 2004. Syntactic simplification for improving content selection in multi-document summarization. In Proceedings of the 20th international conference on Computational Linguistics, page 896. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
</authors>
<title>Syntactic simplification and text cohesion.</title>
<date>2006</date>
<journal>Research on Language and Computation,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="9219" citStr="Siddharthan, 2006" startWordPosition="1413" endWordPosition="1414">dies have been published to date. 2.3 Computational Approaches to Sentence Simplification Although not explicitly for teaching language, computational approaches that facilitate accessibility to texts that might otherwise be too difficult for its readers, either due to physical or learning disabilities, or language barriers, are relevant. In the recent work of (Kauchak, 2013), for example demonstrates an approach to increasing readability of texts by learning from unsimplified texts. Approaches in this area span methods for simplifying lexis (Yatskar et al., 2010; Biran et al., 2011), syntax (Siddharthan, 2006; Siddharthan et al., 2004), discourse properties (Hutchinson, 2005), and making technical terminology more accessible to non-experts (Elhadad and Sutaria, 2007). While the resulting texts are of great potential aid to language learners and may implicitly improve upon a reader’s language proficiency, they do not explicitly attempt to promote learning as an objective in generating the simplified text. 2.4 Recent Neurophysiological findings Evidence for the potential effectiveness of codeswitching for language acquisition, stem from the recent findings of (Borovsky et al., 2012), who have shown </context>
</contexts>
<marker>Siddharthan, 2006</marker>
<rawString>Advaith Siddharthan. 2006. Syntactic simplification and text cohesion. Research on Language and Computation, 4(1):77–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thamar Solorio</author>
<author>Yang Liu</author>
</authors>
<title>Learning to predict code-switching points.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>973--981</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7111" citStr="Solorio and Liu (2008)" startWordPosition="1090" endWordPosition="1093">ntactic complexity of code-switching, code-switching is actually a marking of bilingual fluency (Genesee, 2001). More recently, the idea of employing code-switching in the classroom, in a form of conversation-based exercises, has attracted the attention of multiple researchers and educators (Moodley, 2010; Macaro, 2005), yielding promising results in an elementary school study in SouthAfrica. 2.2 Computational Approaches to Code-switching Additionally, there has been a limited number of studies of the computational approaches to code-switching, and in particular code-switched text generation. Solorio and Liu (2008), record and transcribe a corpus of Spanish-English codemixed conversation to train a generative model (Naive Bayes) for the task of predicting codeswitch points in conversation. Additionally they test their trained model in its ability to generate code-switched text with convincing results. Building on their work, (Adel et al., 2012) employ additional features and a recurrent network language model for modeling code-switching in conversational speech. Adel and collegues (2011) propose a statistical machine translation-based approach for generating code-switched text. We note, however, that th</context>
</contexts>
<marker>Solorio, Liu, 2008</marker>
<rawString>Thamar Solorio and Yang Liu. 2008. Learning to predict code-switching points. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 973–981. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Yatskar</author>
<author>Bo Pang</author>
<author>Cristian Danescu-NiculescuMizil</author>
<author>Lillian Lee</author>
</authors>
<title>For the sake of simplicity: Unsupervised extraction of lexical simplifications from wikipedia.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>365--368</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9171" citStr="Yatskar et al., 2010" startWordPosition="1404" endWordPosition="1407"> any optimization in generating the text, as no studies have been published to date. 2.3 Computational Approaches to Sentence Simplification Although not explicitly for teaching language, computational approaches that facilitate accessibility to texts that might otherwise be too difficult for its readers, either due to physical or learning disabilities, or language barriers, are relevant. In the recent work of (Kauchak, 2013), for example demonstrates an approach to increasing readability of texts by learning from unsimplified texts. Approaches in this area span methods for simplifying lexis (Yatskar et al., 2010; Biran et al., 2011), syntax (Siddharthan, 2006; Siddharthan et al., 2004), discourse properties (Hutchinson, 2005), and making technical terminology more accessible to non-experts (Elhadad and Sutaria, 2007). While the resulting texts are of great potential aid to language learners and may implicitly improve upon a reader’s language proficiency, they do not explicitly attempt to promote learning as an objective in generating the simplified text. 2.4 Recent Neurophysiological findings Evidence for the potential effectiveness of codeswitching for language acquisition, stem from the recent find</context>
</contexts>
<marker>Yatskar, Pang, Danescu-NiculescuMizil, Lee, 2010</marker>
<rawString>Mark Yatskar, Bo Pang, Cristian Danescu-NiculescuMizil, and Lillian Lee. 2010. For the sake of simplicity: Unsupervised extraction of lexical simplifications from wikipedia. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 365–368. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>