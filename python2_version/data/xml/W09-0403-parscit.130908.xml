<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.030343">
<title confidence="0.992884">
A Simple Automatic MT Evaluation Metric
</title>
<author confidence="0.978905">
Petr Homola
</author>
<affiliation confidence="0.8415215">
Charles University
Prague, Czech Republic
</affiliation>
<author confidence="0.812466">
Vladislav Kuboˇn
</author>
<affiliation confidence="0.7265015">
Charles University
Prague, Czech Republic
</affiliation>
<author confidence="0.979799">
Pavel Pecina
</author>
<affiliation confidence="0.777058">
Charles University
Prague, Czech Republic
</affiliation>
<email confidence="0.976507">
{homola |vk|pecina}@ufal.mff .cuni.cz
</email>
<sectionHeader confidence="0.99339" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999989571428571">
This paper describes a simple evaluation
metric for MT which attempts to overcome
the well-known deficits of the standard
BLEU metric from a slightly different an-
gle. It employes Levenshtein’s edit dis-
tance for establishing alignment between
the MT output and the reference transla-
tion in order to reflect the morphological
properties of highly inflected languages. It
also incorporates a very simple measure
expressing the differences in the word or-
der. The paper also includes evaluation on
the data from the previous SMT workshop
for several language pairs.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.979519242424242">
The problem of finding a reliable machine trans-
lation metrics corresponding with a human judg-
ment has recently returned to the centre of atten-
tion. After a brief period following the introduc-
tion of generally accepted and widely used met-
rics, BLEU (Papineni et al., 2002) and NIST (Dod-
dington, 2002), when it seemed that this persistent
problem has finally been solved, the researchers
active in the field of machine translation (MT)
started to express their worries that although these
metrics are simple, fast and able to provide con-
sistent results for a particular system during its de-
velopment, they are not sufficiently reliable for the
comparison of different systems or different lan-
guage pairs.
The results of the NIST evaluation in 2005
(Le and Przybocki, 2005) have also strengthened
the suspicion that the correlation between human
judgment and the BLEU and NIST measures is not
as strong as it was widely believed. Both mea-
sures seem to favor the MT output created by sys-
tems based on n-gram architecture, they are un-
able to take into account certain factors which are
very important for the human judges of translation
quality.
The article (Callison-Burch et al., 2006) thor-
oughly discusses the deficits of the BLEU and
similar metrics. The authors claim that the existing
automatic metrics, including some of the new and
seemingly more reliable ones as e.g. Meteor (cf.
(Banerjee and Lavie, 2005)) ”...they are all quite
rough measures of translation similarity, and have
inexact models of allowable variation in transla-
tion.” This claim is supported by a construction of
translation variations which have identical BLEU
score, but which are very different for a human
judge. The authors identify three prominent fac-
tors which contribute to the inadequacy of BLEU –
the failure to deal with synonyms and paraphrases,
no penalties for missing content, and the crudeness
of the brevity penalty.
Let us add some more factors based on our ex-
periments with languages typologically different
than English, Arabic or Chinese, which are prob-
ably the languages most frequently used in recent
shared-task MT evaluations. The highly inflected
languages and languages with a higher degree of
word-order freedom may provide additional ex-
amples of sentences in which relatively small al-
terations of correct word forms may have a dire
effect on the BLEU score while the sentence still
remains understandable and acceptable for human
evaluators.
The effect of rich inflection has been observed
for example in (T´ynovsk´y, 2007), where the au-
thor mentions the fact that the BLEU score used
for measuring the improvements in his experimen-
tal Czech-German EBMT system penalized heav-
ily all subtle errors in Czech morphology arising
from an out-of-context combined partial transla-
tions taken from different examples.
The problem of the insensitivity of BLEU to the
variations of the order of n-grams identified in ref-
erence translations has already been mentioned in
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 33–36,
Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics
</bodyText>
<page confidence="0.999133">
33
</page>
<bodyText confidence="0.997398821782179">
the paper (Callison-Burch et al., 2006). The au-
thors showed examples where changing a good
word order into an unacceptable one did not af-
fect the BLEU score. We may add a different ex-
ample documenting the phenomenon that a pair
of syntactically correct Czech sentences with the
same word forms, differing only in the word order
whose n-gram score for n = 2, 3, and 4 differs
greatly. Let us take one of the sentences from the
2008 SMT workshop and its reference translation:
When Caligula appointed his horse to the Sen-
ate, the horse at least did not have blood on its
hoofs. — Kdyˇz Caligula zvolil do sen´atu sv´eho
konˇe, nemˇel jeho k˚uˇn aspoˇn na kopytech krev.
If we modify the Czech reference sentence into
Kdyˇz sv´eho konˇe do sen´atu zvolil Caligula, jeho
k˚uˇn aspoˇn nemˇel na kopytech krev., we destroy 8
out of 15 bigrams, 11 out of 14 trigrams and 12
out of 13 quadrigrams while we still have sentence
with almost identical meaning and probably very
similar human evaluation. The BLEU score of the
modified sentence is, however, lower than it would
be for the identical copy of the reference transla-
tion.
2 The description of the proposed metric
There is one aspect of the problem of a MT
quality metric which tends to be overlooked but
which is very important from the practical point
of view. This aspect concerns the expected diffi-
culties when post-editing the MT output. It is very
important for everybody who really wants to use
the MT output and who faces the decision whether
it is better to post-edit the MT output or whether a
new translation made by human translators would
be faster and more efficient way towards the de-
sired quality. It is no wonder that such a met-
ric is mentioned only in connection with systems
which really aim at practical exploitation, not with
a majority of experimental MT system which will
hardly ever reach the stage of industrial exploita-
tion.
We have described one example of such practi-
cally oriented metric in (Hajiˇc et al., 2003). The
metric exploits the matching algorithm of Trados
Translator’s Workbench for obtaining the percent-
age of differences between the MT output and the
reference translation (created by post-editing the
MT output). The advantage of this measure is its
close connection to the real world of human trans-
lating by means of translation memory, the disad-
vantage concerns the use of a proprietary match-
ing algorithm which has not been made public and
which requires the actual use of the Trados soft-
ware.
Nevertheless, the matching algorithm of Trados
gives results which to a great extent correspond
to a much simpler traditional metric, to the Lev-
enshtein’s edit distance. The use of this metric
may help to refine a very strict treatment of word-
form differences by BLEU. A similar approach at
the level of unigram matching has been used by
the well-known METEOR metric (Agarwal and
Lavie, 2008), which proved its qualities during the
previous MT evaluation task in 2008 (Callison-
Burch et al., 2008). Meteor uses Porter stemmer
as one step in the word alignment algorithm. It
also relies on synonymy relations in WordNet.
When designing our metric, we have decided to
follow two general strategies – to use as simple
means as possible and to avoid using any language
dependent tools or resources. Levenshtein metric
(or its modification for word-level edit distance)
therefore seemed to be the best candidate for sev-
eral aspects of the proposed measure.
The first aspect we have decided to include was
the inflection. The edit distance has one advan-
tage over the language independent stemmer – it
can uniformly handle the differences regardless of
their position in the string. The stemmer will prob-
ably face certain problems with changes inside the
stem as e.g. in the Czech equivalent of the word
house in different cases d˚um (nom.sg) — domu
(gen., dat. or loc. sg.) or German Mann in differ-
ent numbers der Mann (sg.) — die M¨anner (pl.),
while the edit distance will treat them uniformly
with the variation of prefixes, suffixes and infixes.
As mentioned above, we have also intended to
aim at the treatment of the free word order in our
metric. However this seems to be one of the ma-
jor flaws of the BLEU score, it turned out that the
word order is extremely difficult if we stick to the
use of simple and language independent means. If
we take Czech as an example of a language with
relatively high degree of word-order freedom, we
can still find certain restrictions (e.g. the sentence-
second position of clitics, their mutual order, the
adjectives typically, but not always preceding the
nouns they depend upon etc.) which will defi-
nitely influence the human judgment of the accept-
ability of a particular sentence. These restrictions
are language dependent (for example Polish, the
</bodyText>
<page confidence="0.995509">
34
</page>
<bodyText confidence="0.999967533333333">
language very closely related to Czech, has dif-
ferent rules for congruent attributes, the adjectives
stand much more often to the right of the govern-
ing noun) and they are also very difficult to capture
algorithmically. If the MT output is compared to
a single reference translation only, there is, in fact,
no way how the metric could account for the pos-
sible correct variations of the word order without
exploiting very deep language dependent informa-
tion. If there are more reference translations, it is
possible that they will provide the natural varia-
tions of the word order, but it, in fact, means that
if we want to stick to the above mentioned require-
ments, we have to give up the hope that our metric
will capture this important phenomenon.
</bodyText>
<subsectionHeader confidence="0.982939">
2.1 Word alignment algorithm
</subsectionHeader>
<bodyText confidence="0.999995392857143">
In order to capture the word form variations
caused by the inflection, we have decided to em-
ploy the following alignment algorithm at the level
of individual word forms. Let us use the follow-
ing notation: Let the reference translation R be a
sequence of words ri, where i E&lt; 1, ... , n &gt;.
Let the MT output T be a sequence of words tj,
where j E&lt; 1, ... , m &gt;. Let us also set a thresh-
old of similarity s E&lt; 0,1 &gt;. (s roughly ex-
presses how different the forms of a lemma may
be. The idea behind this criterion is that a mistake
in one morphological category (reflected mostly
by a different ending of the corresponding word
form) is not as serious as a completely different
lexeme. This holds especially for morphologically
rich languages that can have tens or even hun-
dreds of distinct word forms for a single lemma.)
Starting from t1, let us find for each tj the best
ri for i E&lt; 1, ... , n &gt; such that the edit dis-
tance dj from tj to ri normalized by the length
of tj is minimal and at the same time dj &lt; s.
If the ri is already aligned to some tk, k &lt; j
and the edit distance dk &gt; dj, then align tj to
ri and re-calculate the alignment for tk to its sec-
ond best candidate, otherwise take the second best
candidate rl conforming with the above mentioned
conditions and align it to tj. As a result of this
process, we get the alignment score ATR from T
</bodyText>
<equation confidence="0.932678">
�(1�di)
to R. ATR = m (for i E&lt; 1, ... , n &gt;)
</equation>
<bodyText confidence="0.9999538">
where di = 1 for those word forms ti which are
not aligned to any of the word forms rj from R.
Then we calculate the alignment score ART using
the same algorithm and aligning the words from R
to T. The similarity score S equals the minimum
from ATR and ART. The way how the similar-
ity score S is constructed ensures that the score
takes into account a difference in length between
T and R, therefore it is not necessary to include
any brevity penalty into the metric.
</bodyText>
<subsectionHeader confidence="0.999065">
2.2 A structural metric
</subsectionHeader>
<bodyText confidence="0.999902538461538">
In order to express word-order difference between
the MT output and the reference translation we
have designed a structural part of the metric. It
is based on an algorithm similar to one of the stan-
dard sorting methods, an insert sort. The refer-
ence translation R represents the desired word or-
der and the algorithm counts the number of op-
erations necessary for obtaining the correct word
order from the word order of the MT output T by
inserting the words ti to their desired positions rj
(ti is aligned to rj). If a particular word ti is not
aligned to any rj, a penalty of 1 is added to the
number of operations.
</bodyText>
<subsectionHeader confidence="0.997597">
2.3 A combination of both metrics
</subsectionHeader>
<bodyText confidence="0.988630642857143">
The overall score is computed as a weighted aver-
age of both metrics mentioned above. Let L be the
lexical similarity score and M the structural score
based on a word mapping. Then then overall score
5 can be obtained as follows:
5=aL+bM
The coefficients a and b must sum up to one.
They allow to capture the difference in the degree
of word-order freedom among target languages.
The coefficient b should be set lower for the tar-
get languages with more free word-order. Because
both then partial measures L and M have values in
the interval &lt; 0,1 &gt;, the value of 5 will also fall
into this interval.
</bodyText>
<sectionHeader confidence="0.972492" genericHeader="introduction">
3 The experiment
</sectionHeader>
<bodyText confidence="0.998832333333333">
We have performed a test of the proposed met-
ric using the data from the last year’s SMT work-
shop.1 The parameters a, b, and s have been set to
the same value for all evaluated language pairs, no
language dependent alterations were tested in this
experiment:
</bodyText>
<figure confidence="0.926608">
Parameter Value
0.15
0.9
b 0.1
1The data are available at http://www.statmt.org/wmt08.
s
a
</figure>
<page confidence="0.996037">
35
</page>
<bodyText confidence="0.999959714285714">
The values for the parameters have been set up
empirically with special attention being paid to
Czech, the only language with really rich inflec-
tion among the languages being tested.
We have performed sentence-level and system-
level evaluation using the Spearman’s rank corre-
lation coefficient which is defined as follows:
</bodyText>
<equation confidence="0.992255333333333">
6Ed2
i
p = 1 − n(n2 − 1)
</equation>
<bodyText confidence="0.99952875">
where di = xi −yi is the difference between the
ranks of corresponding values Xi and Y and n is
the number of values in each data set.
The following scores express the correlation of
our automatic metric and the human judgements
for the language pairs English-Czech and English-
German. The sentence-level correlation psent is
the average of Spearman’s p across all sentences.
</bodyText>
<table confidence="0.9769944">
Language pair Metric psent psys
English-Czech proposed 0.20 0.50
English-Czech BLEU 0.21 0.50
English-German proposed 0.91 0.37
English-German BLEU 0.90 0.20
</table>
<subsectionHeader confidence="0.974014">
3.1 Conclusions
</subsectionHeader>
<bodyText confidence="0.999983391304348">
The metric presented in this paper attempts to
combine some of the important factors which
seem to be neglected by some generally accepted
MT evaluation metrics. Inspired by the fact that
human judges tend to accept incorrect word-forms
of corectly translated lemmas, it employs a simi-
larity measure relaxing the requirements on iden-
tity (or similarity) of matching word forms in the
MT output and the reference translation. At the
same time, it also incorporates a penalty for dif-
ferent length of the MT output and the reference
translation. The second component of the metric
tackles the problem of incorrect word-order. The
constants used in the metric allow to set the weight
of its two components with regard to the target lan-
guage properties.
The experiments performed on the data from
the previous shared evaluation task are promising.
They indicate that the first component of the met-
ric succesfully replaces the strict unigram mea-
sure used in BLEU while the second component
may require certain alteration in order to achieve a
higher correlation with human judgement.
</bodyText>
<sectionHeader confidence="0.995491" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99976025">
The presented research has been supported by the
grant No. 1ET100300517 of the GAAV ˇCR and
by Ministry of Education of the Czech Republic,
project MSM 0021620838.
</bodyText>
<sectionHeader confidence="0.999152" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999763093023256">
Abhaya Agarwal and Alon Lavie. 2008. Meteor,
M-BLEU and M-TER: Evaluation metrics for high
correlation with human rankings of machine trans-
lation output. In Proceedings of the Third Work-
shop on Statistical Machine Translation, pages 115-
118. Columbus, Ohio, Association for Computa-
tional Linguistics.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for MT evaluation with improved
correlation with human judgments.. In Workshop
on Intrinsic and Extrinsic Evaluation Measures for
MT and/or Summarization, Ann Arbor, Michigan.
Chris Callison-Burch, Miles Osborne, Philipp Koehn.
2006. Re-evaluating the Role of BLEU in Ma-
chine Translation Research.. In Proceedings of the
EACL’06, Trento, Italy.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, Josh Schroeder. 2008.
Further Meta-Evaluation of Machine Translation..
In Proceedings of the Third Workshop on Statisti-
cal Machine Translation, pages 70-106, Columbus,
Ohio. Association for Computational Linguistics.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second
international conference on Human Language Tech-
nology Research, San Diego, California,USA
Jan Hajiˇc, Petr Homola, Vladislav Kuboˇn. 2003. A
Simple Multilingual Machine Translation System..
In Proceedings of the MT Summit IX, New Orleans,
USA.
Kishore Papineni, Salim Roukos, ToddWard, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation.. In Proceedings of
ACL 2002.
Audrey Le and Mark Przybocki. 2005. NIST 2005
machine translation evaluation official results.. Of-
ficial release of automatic evaluation scores for all
submissions.
Miroslav T´ynovsk´y. 2007. Exploitation of Linguis-
tic Information in EBMT.. Master thesis at Charles
University in Prague, Faculty of Mathematics and
Physics.
</reference>
<page confidence="0.99892">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.088391">
<title confidence="0.997043">A Simple Automatic MT Evaluation Metric</title>
<degree confidence="0.554636875">Petr Charles Prague, Czech Republic Vladislav Charles Prague, Czech Republic Pavel Charles</degree>
<address confidence="0.608422">Prague, Czech Republic</address>
<email confidence="0.981386">.cuni.cz</email>
<abstract confidence="0.998092133333333">This paper describes a simple evaluation metric for MT which attempts to overcome the well-known deficits of the standard BLEU metric from a slightly different angle. It employes Levenshtein’s edit distance for establishing alignment between the MT output and the reference translation in order to reflect the morphological properties of highly inflected languages. It also incorporates a very simple measure expressing the differences in the word order. The paper also includes evaluation on the data from the previous SMT workshop for several language pairs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Abhaya Agarwal</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor, M-BLEU and M-TER: Evaluation metrics for high correlation with human rankings of machine translation output.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>115--118</pages>
<institution>Association for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="6851" citStr="Agarwal and Lavie, 2008" startWordPosition="1120" endWordPosition="1123">n to the real world of human translating by means of translation memory, the disadvantage concerns the use of a proprietary matching algorithm which has not been made public and which requires the actual use of the Trados software. Nevertheless, the matching algorithm of Trados gives results which to a great extent correspond to a much simpler traditional metric, to the Levenshtein’s edit distance. The use of this metric may help to refine a very strict treatment of wordform differences by BLEU. A similar approach at the level of unigram matching has been used by the well-known METEOR metric (Agarwal and Lavie, 2008), which proved its qualities during the previous MT evaluation task in 2008 (CallisonBurch et al., 2008). Meteor uses Porter stemmer as one step in the word alignment algorithm. It also relies on synonymy relations in WordNet. When designing our metric, we have decided to follow two general strategies – to use as simple means as possible and to avoid using any language dependent tools or resources. Levenshtein metric (or its modification for word-level edit distance) therefore seemed to be the best candidate for several aspects of the proposed measure. The first aspect we have decided to inclu</context>
</contexts>
<marker>Agarwal, Lavie, 2008</marker>
<rawString>Abhaya Agarwal and Alon Lavie. 2008. Meteor, M-BLEU and M-TER: Evaluation metrics for high correlation with human rankings of machine translation output. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 115-118. Columbus, Ohio, Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor: An automatic metric for MT evaluation with improved correlation with human judgments..</title>
<date>2005</date>
<booktitle>In Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization,</booktitle>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="2241" citStr="Banerjee and Lavie, 2005" startWordPosition="351" endWordPosition="354">ngthened the suspicion that the correlation between human judgment and the BLEU and NIST measures is not as strong as it was widely believed. Both measures seem to favor the MT output created by systems based on n-gram architecture, they are unable to take into account certain factors which are very important for the human judges of translation quality. The article (Callison-Burch et al., 2006) thoroughly discusses the deficits of the BLEU and similar metrics. The authors claim that the existing automatic metrics, including some of the new and seemingly more reliable ones as e.g. Meteor (cf. (Banerjee and Lavie, 2005)) ”...they are all quite rough measures of translation similarity, and have inexact models of allowable variation in translation.” This claim is supported by a construction of translation variations which have identical BLEU score, but which are very different for a human judge. The authors identify three prominent factors which contribute to the inadequacy of BLEU – the failure to deal with synonyms and paraphrases, no penalties for missing content, and the crudeness of the brevity penalty. Let us add some more factors based on our experiments with languages typologically different than Engli</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for MT evaluation with improved correlation with human judgments.. In Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Re-evaluating the Role of BLEU in Machine Translation Research..</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL’06,</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="2013" citStr="Callison-Burch et al., 2006" startWordPosition="314" endWordPosition="317"> a particular system during its development, they are not sufficiently reliable for the comparison of different systems or different language pairs. The results of the NIST evaluation in 2005 (Le and Przybocki, 2005) have also strengthened the suspicion that the correlation between human judgment and the BLEU and NIST measures is not as strong as it was widely believed. Both measures seem to favor the MT output created by systems based on n-gram architecture, they are unable to take into account certain factors which are very important for the human judges of translation quality. The article (Callison-Burch et al., 2006) thoroughly discusses the deficits of the BLEU and similar metrics. The authors claim that the existing automatic metrics, including some of the new and seemingly more reliable ones as e.g. Meteor (cf. (Banerjee and Lavie, 2005)) ”...they are all quite rough measures of translation similarity, and have inexact models of allowable variation in translation.” This claim is supported by a construction of translation variations which have identical BLEU score, but which are very different for a human judge. The authors identify three prominent factors which contribute to the inadequacy of BLEU – th</context>
<context position="4010" citStr="Callison-Burch et al., 2006" startWordPosition="627" endWordPosition="630">s the fact that the BLEU score used for measuring the improvements in his experimental Czech-German EBMT system penalized heavily all subtle errors in Czech morphology arising from an out-of-context combined partial translations taken from different examples. The problem of the insensitivity of BLEU to the variations of the order of n-grams identified in reference translations has already been mentioned in Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 33–36, Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics 33 the paper (Callison-Burch et al., 2006). The authors showed examples where changing a good word order into an unacceptable one did not affect the BLEU score. We may add a different example documenting the phenomenon that a pair of syntactically correct Czech sentences with the same word forms, differing only in the word order whose n-gram score for n = 2, 3, and 4 differs greatly. Let us take one of the sentences from the 2008 SMT workshop and its reference translation: When Caligula appointed his horse to the Senate, the horse at least did not have blood on its hoofs. — Kdyˇz Caligula zvolil do sen´atu sv´eho konˇe, nemˇel jeho k˚</context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>Chris Callison-Burch, Miles Osborne, Philipp Koehn. 2006. Re-evaluating the Role of BLEU in Machine Translation Research.. In Proceedings of the EACL’06, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>Further Meta-Evaluation of Machine Translation..</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>70--106</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio.</location>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2008</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, Josh Schroeder. 2008. Further Meta-Evaluation of Machine Translation.. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 70-106, Columbus, Ohio. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram cooccurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the second international conference on Human Language Technology Research,</booktitle>
<location>San Diego, California,USA</location>
<contexts>
<context position="1130" citStr="Doddington, 2002" startWordPosition="169" endWordPosition="171">d the reference translation in order to reflect the morphological properties of highly inflected languages. It also incorporates a very simple measure expressing the differences in the word order. The paper also includes evaluation on the data from the previous SMT workshop for several language pairs. 1 Introduction The problem of finding a reliable machine translation metrics corresponding with a human judgment has recently returned to the centre of attention. After a brief period following the introduction of generally accepted and widely used metrics, BLEU (Papineni et al., 2002) and NIST (Doddington, 2002), when it seemed that this persistent problem has finally been solved, the researchers active in the field of machine translation (MT) started to express their worries that although these metrics are simple, fast and able to provide consistent results for a particular system during its development, they are not sufficiently reliable for the comparison of different systems or different language pairs. The results of the NIST evaluation in 2005 (Le and Przybocki, 2005) have also strengthened the suspicion that the correlation between human judgment and the BLEU and NIST measures is not as strong</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. In Proceedings of the second international conference on Human Language Technology Research, San Diego, California,USA</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Petr Homola</author>
<author>Vladislav Kuboˇn</author>
</authors>
<title>A Simple Multilingual Machine Translation System..</title>
<date>2003</date>
<booktitle>In Proceedings of the MT Summit IX,</booktitle>
<location>New Orleans, USA.</location>
<marker>Hajiˇc, Homola, Kuboˇn, 2003</marker>
<rawString>Jan Hajiˇc, Petr Homola, Vladislav Kuboˇn. 2003. A Simple Multilingual Machine Translation System.. In Proceedings of the MT Summit IX, New Orleans, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>ToddWard</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation..</title>
<date>2002</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="1102" citStr="Papineni et al., 2002" startWordPosition="163" endWordPosition="166">lignment between the MT output and the reference translation in order to reflect the morphological properties of highly inflected languages. It also incorporates a very simple measure expressing the differences in the word order. The paper also includes evaluation on the data from the previous SMT workshop for several language pairs. 1 Introduction The problem of finding a reliable machine translation metrics corresponding with a human judgment has recently returned to the centre of attention. After a brief period following the introduction of generally accepted and widely used metrics, BLEU (Papineni et al., 2002) and NIST (Doddington, 2002), when it seemed that this persistent problem has finally been solved, the researchers active in the field of machine translation (MT) started to express their worries that although these metrics are simple, fast and able to provide consistent results for a particular system during its development, they are not sufficiently reliable for the comparison of different systems or different language pairs. The results of the NIST evaluation in 2005 (Le and Przybocki, 2005) have also strengthened the suspicion that the correlation between human judgment and the BLEU and NI</context>
</contexts>
<marker>Papineni, Roukos, ToddWard, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, ToddWard, and WeiJing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation.. In Proceedings of ACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Audrey Le and Mark Przybocki</author>
</authors>
<title>machine translation evaluation official results.. Official release of automatic evaluation scores for all submissions.</title>
<date>2005</date>
<publisher>NIST</publisher>
<contexts>
<context position="1601" citStr="Przybocki, 2005" startWordPosition="246" endWordPosition="247">a brief period following the introduction of generally accepted and widely used metrics, BLEU (Papineni et al., 2002) and NIST (Doddington, 2002), when it seemed that this persistent problem has finally been solved, the researchers active in the field of machine translation (MT) started to express their worries that although these metrics are simple, fast and able to provide consistent results for a particular system during its development, they are not sufficiently reliable for the comparison of different systems or different language pairs. The results of the NIST evaluation in 2005 (Le and Przybocki, 2005) have also strengthened the suspicion that the correlation between human judgment and the BLEU and NIST measures is not as strong as it was widely believed. Both measures seem to favor the MT output created by systems based on n-gram architecture, they are unable to take into account certain factors which are very important for the human judges of translation quality. The article (Callison-Burch et al., 2006) thoroughly discusses the deficits of the BLEU and similar metrics. The authors claim that the existing automatic metrics, including some of the new and seemingly more reliable ones as e.g</context>
</contexts>
<marker>Przybocki, 2005</marker>
<rawString>Audrey Le and Mark Przybocki. 2005. NIST 2005 machine translation evaluation official results.. Official release of automatic evaluation scores for all submissions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miroslav T´ynovsk´y</author>
</authors>
<title>Exploitation of Linguistic Information in EBMT.. Master thesis at</title>
<date>2007</date>
<institution>Charles University in Prague, Faculty of Mathematics and Physics.</institution>
<marker>T´ynovsk´y, 2007</marker>
<rawString>Miroslav T´ynovsk´y. 2007. Exploitation of Linguistic Information in EBMT.. Master thesis at Charles University in Prague, Faculty of Mathematics and Physics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>