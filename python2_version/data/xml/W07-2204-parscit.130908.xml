<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.259522">
<title confidence="0.997911">
Adapting WSJ-Trained Parsers to the British National Corpus Using
In-Domain Self-Training
</title>
<author confidence="0.999558">
Jennifer Foster, Joachim Wagner, Djam´e Seddah and Josef van Genabith
</author>
<affiliation confidence="0.9981985">
National Centre for Language Technology
School of Computing, Dublin City University, Dublin 9, Ireland
</affiliation>
<email confidence="0.993336">
{jfoster, jwagner, josef}@computing.dcu.ie, dseddah@paris4.sorbonne.fr*
</email>
<sectionHeader confidence="0.993781" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999956818181818">
We introduce a set of 1,000 gold standard
parse trees for the British National Corpus
(BNC) and perform a series of self-training
experiments with Charniak and Johnson’s
reranking parser and BNC sentences. We
show that retraining this parser with a com-
bination of one million BNC parse trees
(produced by the same parser) and the orig-
inal WSJ training data yields improvements
of 0.4% on WSJ Section 23 and 1.7% on the
new BNC gold standard set.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99317235">
Given the success of statistical parsing models on
the Wall Street Journal (WSJ) section of the Penn
Treebank (PTB) (Charniak, 2000; Collins, 2003, for
example), there has been a change in focus in recent
years towards the problem of replicating this success
on genres other than American financial news sto-
ries. The main challenge in solving the parser adap-
tation problem are the resources required to con-
struct reliable annotated training examples.
A breakthrough has come in the form of research
by McClosky et al. (2006a; 2006b) who show that
self-training can be used to improve parser perfor-
mance when combined with a two-stage reranking
parser model (Charniak and Johnson, 2005). Self-
training is the process of training a parser on its own
output, and earlier self-training experiments using
generative statistical parsers did not yield encour-
aging results (Steedman et al., 2003). McClosky et
al. (2006a; 2006b) proceed as follows: sentences
Now affiliated to Lalic, Universit´e Paris 4 La Sorbonne.
</bodyText>
<page confidence="0.986831">
33
</page>
<bodyText confidence="0.999920647058824">
from the LA Times newspaper are parsed by a first-
stage generative statistical parser trained on some
seed training data (WSJ Sections 2-21) and the n-
best parse trees produced by this parser are reranked
by a discriminative reranker. The highest ranked
parse trees are added to the training set of the parser
and the parser is retrained. This self-training method
gives improved performance, not only on Section
23 of the WSJ (an absolute f-score improvement of
0.8%), but also on test sentences from the Brown
corpus (Francis and Kuˇcera, 1979) (an absolute f-
score improvement of 2.6%).
In the experiments of McClosky et al. (2006a;
2006b), the parse trees used for self-training come
from the same domain (American newspaper text)
as the parser’s original seed training material. Bac-
chiani et al. (2006) find that self-training is ef-
fective when the parse trees used for self-training
(WSJ parse trees) come from a different domain to
the seed training data and from the same domain as
the test data (WSJ sentences). They report a per-
formance boost of 4.2% on WSJ Section 23 for a
generative statistical parser trained on Brown seed
data when it is self-trained using 200,000 WSJ parse
trees. However, McCloskey et al. (2006b) report a
drop in performance for their reranking parser when
the experiment is repeated in the opposite direction,
i.e. with Brown data for self-training and testing,
and WSJ data for seed training. In contrast, we re-
port successful in-domain1 self-training experiments
with the BNC data as self-training and test material,
and with the WSJ-trained reranking parser used by
McCloskey et al. (2006a; 2006b).
We parse the BNC (Burnard, 2000) in its entirety
</bodyText>
<footnote confidence="0.8045235">
1We refer to data as being in-domain if it comes from the
same domain as the test data and out-of-domain if it does not.
</footnote>
<note confidence="0.487876">
Proceedings of the 10th Conference on Parsing Technologies, pages 33–35,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999962055555556">
using the reranking parser of Charniak and Johnson
(2005). 1,000 BNC sentences are manually anno-
tated for constituent structure, resulting in the first
gold standard set for this corpus. The gold standard
set is split into a development set of 500 parse trees
and a test set of 500 parse trees and used in a series
of self-training experiments: Charniak and John-
son’s parser is retrained on combinations of WSJ
treebank data and its own parses of BNC sentences.
These combinations are tested on the BNC devel-
opment set and Section 00 of the WSJ. An optimal
combination is chosen which achieves a Parseval la-
belled bracketing f-score of 91.7% on Section 23
and 85.6% on the BNC gold standard test set. For
Section 23 this is an absolute improvement of 0.4%
on the baseline results of this parser, and for the
BNC data this is a statistically significant improve-
ment of 1.7%.
</bodyText>
<sectionHeader confidence="0.896507" genericHeader="method">
2 The BNC Data
</sectionHeader>
<bodyText confidence="0.999970078947369">
The BNC is a 100-million-word balanced part-of-
speech-tagged corpus of written and transcribed
spoken English. Written text comprises 90% of the
BNC: 75% non-fictional and 25% fictional. To fa-
cilitate parsing with a WSJ-trained parser, some re-
versible transformations were applied to the BNC
data, e.g. British English spellings were converted
to American English and neutral quotes disam-
biguated. The reranking parser of Charniak and
Johnson (2005) was used to parse the BNC. 99.8%
of the 6 million BNC sentences obtained a parse,
with an average parsing speed of 1.4s per sentence.
A gold standard set of 1,000 BNC sentences was
constructed by one annotator by correcting the out-
put of the first stage of Charniak and Johnson’s
reranking parser. The sentences included in the gold
standard were chosen at random from the BNC, sub-
ject to the condition that they contain a verb which
does not occur in the training sections of the WSJ
section of the PTB (Marcus et al., 1993). A deci-
sion was made to select sentences for the gold stan-
dard set which differ from the sentences in the WSJ
training sections, and one way of finding different
sentences is to focus on verbs which are not attested
in the WSJ Sections 2-21. It is expected that these
gold standard parse trees can be used as training
data although they are used only as test and develop-
ment data in this work. Because they contain verbs
which do not occur in the parser’s training set, they
are likely to represent a hard test for WSJ-trained
parsers. The PTB bracketing guidelines (Bies et al.,
1995) and the PTB itself were used as references by
the BNC annotator. Functional tags and traces were
not annotated. The annotator noticed that the PTB
parse trees sometimes violate the PTB bracketing
guidelines, and in these cases, the annotator chose
the analysis set out in the guidelines. It took approx-
imately 60 hours to build the gold standard set.
</bodyText>
<sectionHeader confidence="0.99412" genericHeader="method">
3 Self-Training Experiments
</sectionHeader>
<bodyText confidence="0.999901448275862">
Charniak and Johnson’s reranking parser (June 2006
version) is evaluated against the BNC gold stan-
dard development set. Labelled precision (LP), re-
call (LR) and f-score measures2 for this parser are
shown in the first row of Table 1. The f-score of
83.7% is lower than the f-score of 85.2% reported
by McClosky et al. (2006b) for the same parser on
Brown corpus data. This difference is reasonable
since there is greater domain variation between the
WSJ and the BNC than between the WSJ and the
Brown corpus, and all BNC gold standard sentences
contain verbs not attested in WSJ Sections 2-21.
We retrain the first-stage generative statistical
parser of Charniak and Johnson using combinations
of BNC trees (parsed using the reranking parser)
and WSJ treebank trees. We test the combinations
on the BNC gold standard development set and on
WSJ Section 00. Table 1 shows that parser accu-
racy increases with the size of the in-domain self-
training material.3 The figures confirm the claim of
McClosky et al. (2006a) that self-training with a
reranking parsing model is effective for improving
parser accuracy in general, and the claim of Gildea
(2001) that training on in-domain data is effective
for parser adaption. They confirm that self-training
on in-domain data is effective for parser adaptation.
The WSJ Section 00 results suggest that, in order
to maintain performance on the seed training do-
main, it is necessary to combine BNC parse trees
</bodyText>
<footnote confidence="0.973940142857143">
2All scores are for the second stage of the parsing process,
i.e. the evaluation takes place after the reranking. All evalua-
tion is carried out using the Parseval labelled bracketing metrics,
with evalb and parameter file new.prm.
3The notation bnc500K+5wsj refers to a set of 500,000
parser output parse trees of sentences taken randomly from the
BNC concatenated with five copies of WSJ Sections 2-21.
</footnote>
<page confidence="0.992648">
34
</page>
<table confidence="0.999985">
BNC Development WSJ Section 00
Self-Training LP LR LF LP LR LF
- 83.6 83.7 83.7 91.6 90.5 91.0
bnc50k 83.7 83.7 83.7 90.0 88.0 89.0
bnc50k+1wsj 84.4 84.4 84.4 91.6 90.3 91.0
bnc250k 84.7 84.5 84.6 91.1 89.3 90.2
bnc250k+5wsj 85.0 84.9 85.0 91.8 90.5 91.2
bnc500k+5wsj 85.2 85.1 85.2 91.9 90.4 91.2
bnc500k+10wsj 85.1 85.1 85.1 91.9 90.6 91.2
bnc1000k+5wsj 86.5 86.2 86.3 91.7 90.3 91.0
bnc1000k+10wsj 86.1 85.9 86.0 92.0 90.5 91.3
bnc1000k+40wsj 85.5 85.5 85.5 91.9 90.6 91.3
BNC Test WSJ Section 23
- 84.0 83.7 83.9 91.8 90.9 91.3
bnc1000k+10wsj 85.7 85.4 85.6 92.3 91.1 91.7
</table>
<tableCaption confidence="0.999546">
Table 1: In-domain Self-Training Results
</tableCaption>
<bodyText confidence="0.999944384615385">
training data, we predict that the novel verbs in the
BNC gold standard set add to the variety of train-
ing material, and will further help parser adaptation
from the WSJ domain – a matter for further research.
Acknowledgments We thank the IRCSET Em-
bark Initiative (basic research grant SC/02/298
and postdoctoral fellowship P/04/232), Science
Foundation Ireland (Principal Investigator grant
04/IN.3/I527) and the Irish Centre for High End
Computing for supporting this research.
with the original seed training material during the
self-training phase.
Of the self-training combinations with above-
baseline improvements for both development sets,
the combination of 1,000K BNC parse trees and
Section 2-21 of the WSJ (multiplied by ten) yields
the highest improvement for the BNC data, and we
present final results with this combination for the
BNC gold standard test set and WSJ Section 23.
There is an absolute improvement on the original
reranking parser of 1.7% on the BNC gold standard
test set and 0.4% on WSJ Section 23. The improve-
ment on BNC data is statistically significant for both
precision and recall (p &lt; 0.0002, p &lt; 0.0002). The
improvement on WSJ Section 23 is statistically sig-
nificant for precision only (p &lt; 0.003).
</bodyText>
<sectionHeader confidence="0.995607" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999935823529412">
We have introduced a set of 1,000 gold standard
parse trees for the BNC. We have performed self-
training experiments with Charniak and Johnson’s
reranking parser and sentences from the BNC. We
have shown that retraining this parser with a com-
bination of one million BNC parse trees (produced
by the same parser) and the original WSJ train-
ing data yields improvements of 0.4% on WSJ Sec-
tion 23 and 1.7% on the BNC gold standard sen-
tences. These results indicate that self-training on
in-domain data can be used for parser adaptation.
Our BNC gold standard set consists of sentences
containing verbs which are not in the WSJ train-
ing sections. We suspect that this makes the gold
standard set a hard test for WSJ-trained parsers, and
our results are likely to represent a lower bound for
WSJ-trained parsers on BNC data. When used as
</bodyText>
<sectionHeader confidence="0.99894" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99977075">
Michiel Bacchiani, Michael Riley, Brian Roark, and Richard
Sproat. 2006. Map adaptation of stochastic grammars.
Computer Speech and Language, 20(1):41–68.
Ann Bies, Mark Ferguson, Karen Katz, and Robert MacIntyre.
1995. Bracketing guidelines for treebank II style, Penn Tree-
bank project. Technical Report MS-CIS-95-06, University
of Pennsylvania.
Lou Burnard. 2000. User reference guide for the British Na-
tional Corpus. Technical report, Oxford University.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best-parsing and maxent discriminative reranking. In Pro-
ceedings ofACL-05, pages 173–180, Barcelona.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings ofNAACL-00, pages 132–139, Seattle.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguistics,
29(4):499–637.
W. Nelson Francis and Henry Kuˇcera. 1979. Brown Corpus
Manual. Technical report, Brown University, Providence.
Daniel Gildea. 2001. Corpus variation and parser performance.
In Proceedings ofEMNLP-01, pages 167–202, Barcelona.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: the Penn Treebank. Computational Linguistics,
19(2):313–330.
David McClosky, Eugene Charniak, and Mark Johnson. 2006a.
Effective self-training for parsing. In Proceedings of HLT-
NAACL-06, pages 152–159, New York.
David McClosky, Eugene Charniak, and Mark Johnson. 2006b.
Reranking and self-training for parser adaptation. In Pro-
ceedings of COLING-ACL-06, pages 337–344, Sydney.
Mark Steedman, Miles Osbourne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Boot-strapping
statistical parsers from small datasets. In Proceedings of
EACL-03, Budapest.
</reference>
<page confidence="0.999324">
35
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.828970">
<title confidence="0.9917525">Adapting WSJ-Trained Parsers to the British National Corpus In-Domain Self-Training</title>
<author confidence="0.999753">Jennifer Foster</author>
<author confidence="0.999753">Joachim Wagner</author>
<author confidence="0.999753">Djam´e Seddah</author>
<author confidence="0.999753">Josef van</author>
<affiliation confidence="0.9500315">National Centre for Language School of Computing, Dublin City University, Dublin 9,</affiliation>
<email confidence="0.960949">jwagner,</email>
<abstract confidence="0.996434416666666">We introduce a set of 1,000 gold standard parse trees for the British National Corpus (BNC) and perform a series of self-training experiments with Charniak and Johnson’s reranking parser and BNC sentences. We show that retraining this parser with a combination of one million BNC parse trees (produced by the same parser) and the original WSJ training data yields improvements of 0.4% on WSJ Section 23 and 1.7% on the new BNC gold standard set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michiel Bacchiani</author>
<author>Michael Riley</author>
<author>Brian Roark</author>
<author>Richard Sproat</author>
</authors>
<title>Map adaptation of stochastic grammars.</title>
<date>2006</date>
<journal>Computer Speech and Language,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="2620" citStr="Bacchiani et al. (2006)" startWordPosition="408" endWordPosition="412">s parser are reranked by a discriminative reranker. The highest ranked parse trees are added to the training set of the parser and the parser is retrained. This self-training method gives improved performance, not only on Section 23 of the WSJ (an absolute f-score improvement of 0.8%), but also on test sentences from the Brown corpus (Francis and Kuˇcera, 1979) (an absolute fscore improvement of 2.6%). In the experiments of McClosky et al. (2006a; 2006b), the parse trees used for self-training come from the same domain (American newspaper text) as the parser’s original seed training material. Bacchiani et al. (2006) find that self-training is effective when the parse trees used for self-training (WSJ parse trees) come from a different domain to the seed training data and from the same domain as the test data (WSJ sentences). They report a performance boost of 4.2% on WSJ Section 23 for a generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees. However, McCloskey et al. (2006b) report a drop in performance for their reranking parser when the experiment is repeated in the opposite direction, i.e. with Brown data for self-training and testing, and WSJ d</context>
</contexts>
<marker>Bacchiani, Riley, Roark, Sproat, 2006</marker>
<rawString>Michiel Bacchiani, Michael Riley, Brian Roark, and Richard Sproat. 2006. Map adaptation of stochastic grammars. Computer Speech and Language, 20(1):41–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Robert MacIntyre</author>
</authors>
<title>Bracketing guidelines for treebank II style, Penn Treebank project.</title>
<date>1995</date>
<tech>Technical Report MS-CIS-95-06,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="6227" citStr="Bies et al., 1995" startWordPosition="1024" endWordPosition="1027">of the PTB (Marcus et al., 1993). A decision was made to select sentences for the gold standard set which differ from the sentences in the WSJ training sections, and one way of finding different sentences is to focus on verbs which are not attested in the WSJ Sections 2-21. It is expected that these gold standard parse trees can be used as training data although they are used only as test and development data in this work. Because they contain verbs which do not occur in the parser’s training set, they are likely to represent a hard test for WSJ-trained parsers. The PTB bracketing guidelines (Bies et al., 1995) and the PTB itself were used as references by the BNC annotator. Functional tags and traces were not annotated. The annotator noticed that the PTB parse trees sometimes violate the PTB bracketing guidelines, and in these cases, the annotator chose the analysis set out in the guidelines. It took approximately 60 hours to build the gold standard set. 3 Self-Training Experiments Charniak and Johnson’s reranking parser (June 2006 version) is evaluated against the BNC gold standard development set. Labelled precision (LP), recall (LR) and f-score measures2 for this parser are shown in the first ro</context>
</contexts>
<marker>Bies, Ferguson, Katz, MacIntyre, 1995</marker>
<rawString>Ann Bies, Mark Ferguson, Karen Katz, and Robert MacIntyre. 1995. Bracketing guidelines for treebank II style, Penn Treebank project. Technical Report MS-CIS-95-06, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lou Burnard</author>
</authors>
<title>User reference guide for the British National Corpus.</title>
<date>2000</date>
<tech>Technical report,</tech>
<institution>Oxford University.</institution>
<contexts>
<context position="3483" citStr="Burnard, 2000" startWordPosition="553" endWordPosition="554">4.2% on WSJ Section 23 for a generative statistical parser trained on Brown seed data when it is self-trained using 200,000 WSJ parse trees. However, McCloskey et al. (2006b) report a drop in performance for their reranking parser when the experiment is repeated in the opposite direction, i.e. with Brown data for self-training and testing, and WSJ data for seed training. In contrast, we report successful in-domain1 self-training experiments with the BNC data as self-training and test material, and with the WSJ-trained reranking parser used by McCloskey et al. (2006a; 2006b). We parse the BNC (Burnard, 2000) in its entirety 1We refer to data as being in-domain if it comes from the same domain as the test data and out-of-domain if it does not. Proceedings of the 10th Conference on Parsing Technologies, pages 33–35, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics using the reranking parser of Charniak and Johnson (2005). 1,000 BNC sentences are manually annotated for constituent structure, resulting in the first gold standard set for this corpus. The gold standard set is split into a development set of 500 parse trees and a test set of 500 parse trees and used in</context>
</contexts>
<marker>Burnard, 2000</marker>
<rawString>Lou Burnard. 2000. User reference guide for the British National Corpus. Technical report, Oxford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-to-fine nbest-parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL-05,</booktitle>
<pages>173--180</pages>
<location>Barcelona.</location>
<contexts>
<context position="1490" citStr="Charniak and Johnson, 2005" startWordPosition="226" endWordPosition="229">l Street Journal (WSJ) section of the Penn Treebank (PTB) (Charniak, 2000; Collins, 2003, for example), there has been a change in focus in recent years towards the problem of replicating this success on genres other than American financial news stories. The main challenge in solving the parser adaptation problem are the resources required to construct reliable annotated training examples. A breakthrough has come in the form of research by McClosky et al. (2006a; 2006b) who show that self-training can be used to improve parser performance when combined with a two-stage reranking parser model (Charniak and Johnson, 2005). Selftraining is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al., 2003). McClosky et al. (2006a; 2006b) proceed as follows: sentences Now affiliated to Lalic, Universit´e Paris 4 La Sorbonne. 33 from the LA Times newspaper are parsed by a firststage generative statistical parser trained on some seed training data (WSJ Sections 2-21) and the nbest parse trees produced by this parser are reranked by a discriminative reranker. The highest ranked parse trees are added </context>
<context position="3835" citStr="Charniak and Johnson (2005)" startWordPosition="607" endWordPosition="610">g, and WSJ data for seed training. In contrast, we report successful in-domain1 self-training experiments with the BNC data as self-training and test material, and with the WSJ-trained reranking parser used by McCloskey et al. (2006a; 2006b). We parse the BNC (Burnard, 2000) in its entirety 1We refer to data as being in-domain if it comes from the same domain as the test data and out-of-domain if it does not. Proceedings of the 10th Conference on Parsing Technologies, pages 33–35, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics using the reranking parser of Charniak and Johnson (2005). 1,000 BNC sentences are manually annotated for constituent structure, resulting in the first gold standard set for this corpus. The gold standard set is split into a development set of 500 parse trees and a test set of 500 parse trees and used in a series of self-training experiments: Charniak and Johnson’s parser is retrained on combinations of WSJ treebank data and its own parses of BNC sentences. These combinations are tested on the BNC development set and Section 00 of the WSJ. An optimal combination is chosen which achieves a Parseval labelled bracketing f-score of 91.7% on Section 23 a</context>
<context position="5115" citStr="Charniak and Johnson (2005)" startWordPosition="822" endWordPosition="825">on 23 this is an absolute improvement of 0.4% on the baseline results of this parser, and for the BNC data this is a statistically significant improvement of 1.7%. 2 The BNC Data The BNC is a 100-million-word balanced part-ofspeech-tagged corpus of written and transcribed spoken English. Written text comprises 90% of the BNC: 75% non-fictional and 25% fictional. To facilitate parsing with a WSJ-trained parser, some reversible transformations were applied to the BNC data, e.g. British English spellings were converted to American English and neutral quotes disambiguated. The reranking parser of Charniak and Johnson (2005) was used to parse the BNC. 99.8% of the 6 million BNC sentences obtained a parse, with an average parsing speed of 1.4s per sentence. A gold standard set of 1,000 BNC sentences was constructed by one annotator by correcting the output of the first stage of Charniak and Johnson’s reranking parser. The sentences included in the gold standard were chosen at random from the BNC, subject to the condition that they contain a verb which does not occur in the training sections of the WSJ section of the PTB (Marcus et al., 1993). A decision was made to select sentences for the gold standard set which </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine nbest-parsing and maxent discriminative reranking. In Proceedings ofACL-05, pages 173–180, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings ofNAACL-00,</booktitle>
<pages>132--139</pages>
<location>Seattle.</location>
<contexts>
<context position="936" citStr="Charniak, 2000" startWordPosition="138" endWordPosition="139">sorbonne.fr* Abstract We introduce a set of 1,000 gold standard parse trees for the British National Corpus (BNC) and perform a series of self-training experiments with Charniak and Johnson’s reranking parser and BNC sentences. We show that retraining this parser with a combination of one million BNC parse trees (produced by the same parser) and the original WSJ training data yields improvements of 0.4% on WSJ Section 23 and 1.7% on the new BNC gold standard set. 1 Introduction Given the success of statistical parsing models on the Wall Street Journal (WSJ) section of the Penn Treebank (PTB) (Charniak, 2000; Collins, 2003, for example), there has been a change in focus in recent years towards the problem of replicating this success on genres other than American financial news stories. The main challenge in solving the parser adaptation problem are the resources required to construct reliable annotated training examples. A breakthrough has come in the form of research by McClosky et al. (2006a; 2006b) who show that self-training can be used to improve parser performance when combined with a two-stage reranking parser model (Charniak and Johnson, 2005). Selftraining is the process of training a pa</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings ofNAACL-00, pages 132–139, Seattle.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="951" citStr="Collins, 2003" startWordPosition="140" endWordPosition="141">tract We introduce a set of 1,000 gold standard parse trees for the British National Corpus (BNC) and perform a series of self-training experiments with Charniak and Johnson’s reranking parser and BNC sentences. We show that retraining this parser with a combination of one million BNC parse trees (produced by the same parser) and the original WSJ training data yields improvements of 0.4% on WSJ Section 23 and 1.7% on the new BNC gold standard set. 1 Introduction Given the success of statistical parsing models on the Wall Street Journal (WSJ) section of the Penn Treebank (PTB) (Charniak, 2000; Collins, 2003, for example), there has been a change in focus in recent years towards the problem of replicating this success on genres other than American financial news stories. The main challenge in solving the parser adaptation problem are the resources required to construct reliable annotated training examples. A breakthrough has come in the form of research by McClosky et al. (2006a; 2006b) who show that self-training can be used to improve parser performance when combined with a two-stage reranking parser model (Charniak and Johnson, 2005). Selftraining is the process of training a parser on its own</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-driven statistical models for natural language parsing. Computational Linguistics, 29(4):499–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Nelson Francis</author>
<author>Henry Kuˇcera</author>
</authors>
<date>1979</date>
<tech>Technical report,</tech>
<institution>Brown Corpus Manual.</institution>
<location>Providence.</location>
<marker>Francis, Kuˇcera, 1979</marker>
<rawString>W. Nelson Francis and Henry Kuˇcera. 1979. Brown Corpus Manual. Technical report, Brown University, Providence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>In Proceedings ofEMNLP-01,</booktitle>
<pages>167--202</pages>
<location>Barcelona.</location>
<contexts>
<context position="7755" citStr="Gildea (2001)" startWordPosition="1279" endWordPosition="1280">sentences contain verbs not attested in WSJ Sections 2-21. We retrain the first-stage generative statistical parser of Charniak and Johnson using combinations of BNC trees (parsed using the reranking parser) and WSJ treebank trees. We test the combinations on the BNC gold standard development set and on WSJ Section 00. Table 1 shows that parser accuracy increases with the size of the in-domain selftraining material.3 The figures confirm the claim of McClosky et al. (2006a) that self-training with a reranking parsing model is effective for improving parser accuracy in general, and the claim of Gildea (2001) that training on in-domain data is effective for parser adaption. They confirm that self-training on in-domain data is effective for parser adaptation. The WSJ Section 00 results suggest that, in order to maintain performance on the seed training domain, it is necessary to combine BNC parse trees 2All scores are for the second stage of the parsing process, i.e. the evaluation takes place after the reranking. All evaluation is carried out using the Parseval labelled bracketing metrics, with evalb and parameter file new.prm. 3The notation bnc500K+5wsj refers to a set of 500,000 parser output pa</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Daniel Gildea. 2001. Corpus variation and parser performance. In Proceedings ofEMNLP-01, pages 167–202, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="5641" citStr="Marcus et al., 1993" startWordPosition="918" endWordPosition="921"> English and neutral quotes disambiguated. The reranking parser of Charniak and Johnson (2005) was used to parse the BNC. 99.8% of the 6 million BNC sentences obtained a parse, with an average parsing speed of 1.4s per sentence. A gold standard set of 1,000 BNC sentences was constructed by one annotator by correcting the output of the first stage of Charniak and Johnson’s reranking parser. The sentences included in the gold standard were chosen at random from the BNC, subject to the condition that they contain a verb which does not occur in the training sections of the WSJ section of the PTB (Marcus et al., 1993). A decision was made to select sentences for the gold standard set which differ from the sentences in the WSJ training sections, and one way of finding different sentences is to focus on verbs which are not attested in the WSJ Sections 2-21. It is expected that these gold standard parse trees can be used as training data although they are used only as test and development data in this work. Because they contain verbs which do not occur in the parser’s training set, they are likely to represent a hard test for WSJ-trained parsers. The PTB bracketing guidelines (Bies et al., 1995) and the PTB i</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of HLTNAACL-06,</booktitle>
<pages>152--159</pages>
<location>New York.</location>
<contexts>
<context position="1328" citStr="McClosky et al. (2006" startWordPosition="201" endWordPosition="204"> improvements of 0.4% on WSJ Section 23 and 1.7% on the new BNC gold standard set. 1 Introduction Given the success of statistical parsing models on the Wall Street Journal (WSJ) section of the Penn Treebank (PTB) (Charniak, 2000; Collins, 2003, for example), there has been a change in focus in recent years towards the problem of replicating this success on genres other than American financial news stories. The main challenge in solving the parser adaptation problem are the resources required to construct reliable annotated training examples. A breakthrough has come in the form of research by McClosky et al. (2006a; 2006b) who show that self-training can be used to improve parser performance when combined with a two-stage reranking parser model (Charniak and Johnson, 2005). Selftraining is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al., 2003). McClosky et al. (2006a; 2006b) proceed as follows: sentences Now affiliated to Lalic, Universit´e Paris 4 La Sorbonne. 33 from the LA Times newspaper are parsed by a firststage generative statistical parser trained on some seed traini</context>
<context position="6930" citStr="McClosky et al. (2006" startWordPosition="1142" endWordPosition="1145">and traces were not annotated. The annotator noticed that the PTB parse trees sometimes violate the PTB bracketing guidelines, and in these cases, the annotator chose the analysis set out in the guidelines. It took approximately 60 hours to build the gold standard set. 3 Self-Training Experiments Charniak and Johnson’s reranking parser (June 2006 version) is evaluated against the BNC gold standard development set. Labelled precision (LP), recall (LR) and f-score measures2 for this parser are shown in the first row of Table 1. The f-score of 83.7% is lower than the f-score of 85.2% reported by McClosky et al. (2006b) for the same parser on Brown corpus data. This difference is reasonable since there is greater domain variation between the WSJ and the BNC than between the WSJ and the Brown corpus, and all BNC gold standard sentences contain verbs not attested in WSJ Sections 2-21. We retrain the first-stage generative statistical parser of Charniak and Johnson using combinations of BNC trees (parsed using the reranking parser) and WSJ treebank trees. We test the combinations on the BNC gold standard development set and on WSJ Section 00. Table 1 shows that parser accuracy increases with the size of the i</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006a. Effective self-training for parsing. In Proceedings of HLTNAACL-06, pages 152–159, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Reranking and self-training for parser adaptation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL-06,</booktitle>
<pages>337--344</pages>
<location>Sydney.</location>
<contexts>
<context position="1328" citStr="McClosky et al. (2006" startWordPosition="201" endWordPosition="204"> improvements of 0.4% on WSJ Section 23 and 1.7% on the new BNC gold standard set. 1 Introduction Given the success of statistical parsing models on the Wall Street Journal (WSJ) section of the Penn Treebank (PTB) (Charniak, 2000; Collins, 2003, for example), there has been a change in focus in recent years towards the problem of replicating this success on genres other than American financial news stories. The main challenge in solving the parser adaptation problem are the resources required to construct reliable annotated training examples. A breakthrough has come in the form of research by McClosky et al. (2006a; 2006b) who show that self-training can be used to improve parser performance when combined with a two-stage reranking parser model (Charniak and Johnson, 2005). Selftraining is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al., 2003). McClosky et al. (2006a; 2006b) proceed as follows: sentences Now affiliated to Lalic, Universit´e Paris 4 La Sorbonne. 33 from the LA Times newspaper are parsed by a firststage generative statistical parser trained on some seed traini</context>
<context position="6930" citStr="McClosky et al. (2006" startWordPosition="1142" endWordPosition="1145">and traces were not annotated. The annotator noticed that the PTB parse trees sometimes violate the PTB bracketing guidelines, and in these cases, the annotator chose the analysis set out in the guidelines. It took approximately 60 hours to build the gold standard set. 3 Self-Training Experiments Charniak and Johnson’s reranking parser (June 2006 version) is evaluated against the BNC gold standard development set. Labelled precision (LP), recall (LR) and f-score measures2 for this parser are shown in the first row of Table 1. The f-score of 83.7% is lower than the f-score of 85.2% reported by McClosky et al. (2006b) for the same parser on Brown corpus data. This difference is reasonable since there is greater domain variation between the WSJ and the BNC than between the WSJ and the Brown corpus, and all BNC gold standard sentences contain verbs not attested in WSJ Sections 2-21. We retrain the first-stage generative statistical parser of Charniak and Johnson using combinations of BNC trees (parsed using the reranking parser) and WSJ treebank trees. We test the combinations on the BNC gold standard development set and on WSJ Section 00. Table 1 shows that parser accuracy increases with the size of the i</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006b. Reranking and self-training for parser adaptation. In Proceedings of COLING-ACL-06, pages 337–344, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
<author>Miles Osbourne</author>
<author>Anoop Sarkar</author>
<author>Stephen Clark</author>
<author>Rebecca Hwa</author>
<author>Julia Hockenmaier</author>
<author>Paul Ruhlen</author>
<author>Steven Baker</author>
<author>Jeremiah Crim</author>
</authors>
<title>Boot-strapping statistical parsers from small datasets.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL-03,</booktitle>
<location>Budapest.</location>
<contexts>
<context position="1692" citStr="Steedman et al., 2003" startWordPosition="257" endWordPosition="260">res other than American financial news stories. The main challenge in solving the parser adaptation problem are the resources required to construct reliable annotated training examples. A breakthrough has come in the form of research by McClosky et al. (2006a; 2006b) who show that self-training can be used to improve parser performance when combined with a two-stage reranking parser model (Charniak and Johnson, 2005). Selftraining is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al., 2003). McClosky et al. (2006a; 2006b) proceed as follows: sentences Now affiliated to Lalic, Universit´e Paris 4 La Sorbonne. 33 from the LA Times newspaper are parsed by a firststage generative statistical parser trained on some seed training data (WSJ Sections 2-21) and the nbest parse trees produced by this parser are reranked by a discriminative reranker. The highest ranked parse trees are added to the training set of the parser and the parser is retrained. This self-training method gives improved performance, not only on Section 23 of the WSJ (an absolute f-score improvement of 0.8%), but also</context>
</contexts>
<marker>Steedman, Osbourne, Sarkar, Clark, Hwa, Hockenmaier, Ruhlen, Baker, Crim, 2003</marker>
<rawString>Mark Steedman, Miles Osbourne, Anoop Sarkar, Stephen Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen, Steven Baker, and Jeremiah Crim. 2003. Boot-strapping statistical parsers from small datasets. In Proceedings of EACL-03, Budapest.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>