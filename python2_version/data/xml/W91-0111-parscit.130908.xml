<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<sectionHeader confidence="0.761579" genericHeader="abstract">
COMMON HEURISTICS FOR
</sectionHeader>
<author confidence="0.193674">
PARSING, GENERATION, AND WHATEVER ...
</author>
<affiliation confidence="0.377436">
HASIDA. KOiti
Institute for New Generation Computer Technology (ICOT)
</affiliation>
<address confidence="0.4630215">
Mita Kokusai Bldg. 21F. 1-4-28 Mita. Minato-ku. Tokyo 108 JAPAN
Tel: +81-3-3456-3069. E-mail: hasidaÂ©icot.or.jp
</address>
<email confidence="0.63805">
ABSTRACT
</email>
<bodyText confidence="0.999613">
This paper discusses general heuristics to control
computation on symbolic constraints represented in
terms of first-order logic programs. These heuristics
are totally independent, of specific domains and tasks.
Efficient computation for sentence parsing and genera-
tion naturally emerge from these heuristics, capturing
the essence of standard. parsing procedures and seman-
tic head-driven generation. Thus, the same representa-
tion of knowledge, including grammar and lexicon, can
be exploited in a multi-directional manner in various
aspects of language use.
</bodyText>
<sectionHeader confidence="0.99939" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996029547619048">
One lesson to learn from the repeated failure to design
large Al systems in general is that the information flow
in the cognitive systems is too complex and diverse to
stipulate in the design of these Al systems. To capture
this diversity of information flow. therefore. Al systems
must be designed at a More abstract level where direc-
tion of information flow is not explicit.
This is where constraint paradigm comes in. Since
constraints do not stipulate the direction of informa-
tion flow or processing order, constraint-based systems
could be tailored to have tractable complexity, unlike
procedural systems, which stipulate information flow
and thus quickly become too complex for human de-
signers to extend or maintain.
Naturally, the key issue in the constraint-based ap-
proach is how to control information flow. A very gen-
eral control schema independent of any specific domain
or task is vitally necessary for the success of this ap-
proach.
The present paper introduces a system of constraint
in a form of logic program. and a set of very general
heuristics to control symbolic operation on the con-
straints. The symbolic operations here are regarded
as transforming logic programs. They are quite per-
missive operations as a whole, allowing very diverse
information processing involving top-down, bottom-up
and other directions of information flow. The heuristics
control this computatiOn so that only relevant infor-
mation should be exploited and the resulting represen-
tation should be compact. Parsing and generation of
sentences are shown to be efficiently done under these
heuristics, and a standard parsing algorithm and the
semantic head-driven generation [8] naturally emerge
thereof.
The rest of the paper is organized as follows. Sec-
tion 2 describes the syntax of our system of constraint.
Section 3 defines the symbolic computation on these
constraints, and proposes a set of general heuristics to
control computation. Section 4 and Section 5 show
how sentence parsing and generation are executed effi-
ciently by those heuristics. Finally, Section 6 concludes
the paper.
</bodyText>
<sectionHeader confidence="0.99825" genericHeader="method">
2 Constraint Network
</sectionHeader>
<bodyText confidence="0.998420590909091">
A program is a set of clauses. A clause is a set of lit-
erals. A literal is an atomic constraint with a sign in
front of it. The sign is a +&apos;, `-&apos;, or nil. A literal with
a sign &apos;+&apos; or nil is called a positive literal and one with
a sign &apos;-&apos; is a negative literal. An atomic constraint is
an atomic formula such as p(X,Y,Z), a binding such as
X=f(Y), a feature specification such as a(X,Y), or an
equality such as X=Y. Names beginning with capital let-
ters represent variables, and the other names predicates
and functions. A feature specification may be regarded
as an atomic formula with a special binary predicate
called a feature. A feature is a partial function from the
first argument to the second argument: that is, if a is
a feature and both a(X,Y) and a(X,Z) hold, then Y=Z
must also hold. The other atomic constraints may be
understood in the standard fashion. The atomic con-
straints other than equalities are called proper atomic
constraints.
A clause is written as a sequence of literals it contains
followed by a semicolon. The order among literals is
not significant. So (1) is a clause, which may also be
written as (2).
</bodyText>
<listItem confidence="0.9027595">
(1) -p(U,Y) +q(Z) -U=f(X) -XZ;
(2) +q(Z) -p(f(Z),Y);
</listItem>
<bodyText confidence="0.998861571428571">
.A clause containing a literal with null sign is a, defini-
tion clause of the predicate of that literal. A predicate
having definition clauses are called defined predicate,
and its meaning is defined in terms of completion based
on the definition clauses. For instance, if the definition
clauses of predicate p are those in (3), the declarative
meaning of p is given by (4).
</bodyText>
<equation confidence="0.986298666666667">
(3) p(X) -q(X,a); p(f(X)) -r(X);
VA{p(A) {3Y(q(A,Y) A Y = a)V
aX(A = f(X) A r(X))))
</equation>
<page confidence="0.991346">
81
</page>
<bodyText confidence="0.999602375">
A predicate which is not a defined predicate is called a
free predicate. There is a special 0-ary defined predicate
true. Its definition clauses are called top clauses. A
top clause corresponds to the query clause of Prolog.
although the latter has false instead of true.
Programs are regarded as constraint networks. For
instance, the following program is a network as in Fig-
ure 1.
</bodyText>
<listItem confidence="0.926893333333333">
(i) true -member(a,X);
(ii) member (A ,[AI S] ) ;
(iii) member(A, [BIS]) -member(A,S);
</listItem>
<figureCaption confidence="0.999026">
Figure 1: Constraint Network
</figureCaption>
<bodyText confidence="0.996080393617021">
In graphical representations like Figure 1, a `.. often
represents an argument of an atomic constraint. There
are two types of nodes: arguments. and proper atomic
constraints. An argument is involved in at most one
proper atomic constraint, but in any number of equal-
ities. An argument bound to a constant is identified
with that constant. That is, the first argument of a
binding v=a. for instance, is represented simply by a.
A link, represented as a curve, connects two nodes. For
any two (possibly the same) nodes, there is at most
one link connecting them. A link connecting two argu-
ments is an equality between them. A link connecting
two proper atomic constraints is called an inference
link. No link connects an argument and an atomic
constraint. Although often not explicitly shown, an
inference link accompanies equalities between the cor-
responding arguments of the two proper atomic con-
straints. A clausal domain of clause 4) is the part of the
constraint network consisting of the atomic constraints
referred to as literals in 4 except equalities concerning
constants. A clausal domain is depicted by a closed
curve enclosing the included atomic constraints. The
short thick arrows indicate the references to the atomic
constraints as positive literals in clauses. A predicate
domain of predicate 7r consists of all the proper atomic
constraints with r (binding X=f (Y) is regarded as hav-
ing binary free predicate =f, for instance), inference
links among them, and equalities accompanying these
inference links.
The instantiation possibilities of the constraint net-
work is defined by regarding nodes and links as sets.
Those sets are disjoint of each other. An instance of
an argument corresponds to an individual in the do-
main of interpretation, and an instance of an atomic
constraint corresponds to an atomic proposition. Con-
stants (bindings to constants) and 0-ary atomic formu-
las are singleton sets. A link b between nodes a and
stands for a symmetric relation. That is, b = RU R&apos;
for some relation RCax 3. We call ix E al3Y xt5y1 the
a-domain of b. Every link in a clausal domain or the
predicate domain of a defined predicate is of the form
R U for some bijection R. Let A be the transitive
closure of the union of all the links. xAy means that
x and y correspond to the same object in the domain
of interpretation if x and y belong to arguments, and
that they correspond to the same atomic proposition if
they belong to proper atomic constraints. We say that
node a subsumes node ,t3 when a/A D 3/A; that is, for
every y E 3 there exists x E a such that xAy. For each
pair of a proper atomic constraint a and an argument
3 of a, there is a bijection p from a to 3, such that x py
holds if y E is an argument of x E a. p is called a
role assignment.
Consider a part P of the constraint network and
the minimum equivalence relation including the links
and the role assingments in P. A layer of P is an
equivalence class with respect to this relation. A split-
ting domain is a part S of the network in which every
link is of the form R U R&amp;quot; where R is the union of
(a n .C) x n E) over all the layers E of S and a
and )3 are the two endnodes of that link. Thus, if a
link in a splitting domain splits into two links sharing
an endnode a and having disjoint a-domains, then the
entire splitting domain splits into two separate splitting
domains each containing one of these two links. The
clausal domains and predicate domains are assumed to
be splitting domains.
A joint is a part of a node which connects the node
with a link or more. Figure 2 shows some joints. The
figures below illustrate the instantiation possibilities of
the networks shown above by depicting each node as an
ellipse enclosing its instances, and each link as a bundle
of curves representing the pairs belonging to the link.
A joint J of node a is depicted as an arc convex towards
a crossing the links involved in J. A joint involving just
one link, as in (a) and (b), is called a unitary joint, and
one containing several links, as in (c) and (d), is called
a multiple joint. Distinct links involved in the same
multiple joint on node a have disjoint a-domains. A
joint is partial if it stretches out of the involved links,
as in (b) and (d), and total otherwise, as in (a) and
(c). The union of a-domains of the links involved in
the same joint on node a is equal to a. A total unitary
joint as in (a) is not explicitly shown as an arc. Partial
joints on node a are complementary when the union
of the a-domains of the links involved in them is a.
Complementary joints are indicated by a dashed arc
crossing these links. So the union of the a-domains of
the three links is a in (e). When node a and are
connected by link 6 and the joint of involving b is
total and unitary, a and b are said to dominate 0.
The initial structures of predicate domains are shown
in Figure 3. Such structures, as well as the other
structures, will change as computation proceeds.
</bodyText>
<sectionHeader confidence="0.997385" genericHeader="method">
3 Computation
</sectionHeader>
<bodyText confidence="0.99858125">
Here we introduce a method of symbolic computation
together with some general control heuristics for con-
trolling computation. There are two types of symbolic
operation: subsumption and deletion. Here we chiefly
</bodyText>
<figure confidence="0.83386175">
member(
(Iii) Ito
member( ,4
82
a a
(a) (b) (c) (d) (e)
Figure 2: Joints between Nodes and Links
Bound Predicate Free Predicate
</figure>
<figureCaption confidence="0.999846">
Figure 3: Predicate Domains
</figureCaption>
<bodyText confidence="0.7566">
concern ourselves with Subsumption.
</bodyText>
<subsectionHeader confidence="0.999341">
3.1 Subsumption
</subsectionHeader>
<bodyText confidence="0.999990833333333">
â¢Subsumption means two things: subsuniption rela-
tion. which we defined above, and subsumption opera-
tion. which we discuss below.
The purpose of a subsumption is to let information
flow from a node. A node a may have probes. a is
called the origin of these probes. Each probe is placed
on a link and directed towards an endnode. The origin
of a probe subsumes the:node behind the probe. Probes
transmit information of their origins across the network
via subsumptions. The origin of probes has its scope.
The scope of node a is the part. S of the constraint.
network satisfying the following conditions.
</bodyText>
<listItem confidence="0.99982675">
â¢ S is a connected graph containing a.
â¢ A node B is behind a probe 7r on link 6 and with
origin a, if 3 is in S but 6 is not.
â¢ a subsumes every node in S.
</listItem>
<bodyText confidence="0.998985">
So the scope of a may be illustrated as in Figure 4,
where arrows are probes, which just cover the boundary
</bodyText>
<figureCaption confidence="0.99802">
Figure 4: Scope of Node
</figureCaption>
<bodyText confidence="0.999010894736842">
of the scope.
Every node a can just once create probes on all the
links connected to a so that a is behind these probes.
Subsumption extends the scope of a node by advancing
probes, while preserving the instantiation possibilities
of the network described above. We consider a sub-
sumption from node iota to node along link b. t, C,
and 6 are called the input node, the target node, and the
axis, respectively, of this subsumption. The joint J of
C involving b is called the target joint. This subsump-
tion extends the scopes of the origins of the probes on
6 directed towards C. It proceeds as follows.
First, the set II of the probes on b towards is de-
tached from 6, and 6 is shifted from J to another joint
as illustrated in Figure 5. .P is a copy of J and
is on a node C&apos; which is a copy of e. J&apos; and may be
created here and now, but may also have been made in
a previous subsumption, as mentioned below. Below
we proceed so as to make co = ci u e&apos; A Cifl =
</bodyText>
<equation confidence="0.863225666666667">
rE),
P(...) p(...)
p(...) p(...)
</equation>
<page confidence="0.92802">
83
</page>
<figure confidence="0.9969704">
G1
G2
8
--
J&apos;
</figure>
<figureCaption confidence="0.999987">
Figure 5: Shifting of Link and Augmentation of Foldability
</figureCaption>
<bodyText confidence="0.999384375">
true, where eo and el stand for e before and after this
subsumption, respectively.
A joint may be foldable to another joint by a set of
origins of probes. Each joint involved here, called a
foldable joint, is one obtained by copying zero or more
times a multiple joint in the initial state of computa-
tion. Typically, a foldable joint is one involving links in
the predicate domain of a defined predicate. No joint,
just created is foldable to any joint. For any joint G
and set 0 of nodes, there is at most one joint H such
that G is foldable to H by 0.
Let, E be the set of origins of the probes in II. If J is
foldable, then for each joint G the foldability relation
extends in the following way, as illustrated in Figure
5, where the foldability relation is depicted by dashed
arrows.
</bodyText>
<listItem confidence="0.983258">
â¢ .1 is foldable to J&apos; by E.
â¢ If G is foldable to J by 0. then G is foldable to
.1&apos; by 0 U E.
â¢ If .1 is foldable to G by 0 such that 0 D .11 then
J&apos; is foldable to G by 0 â E.
</listItem>
<bodyText confidence="0.992835518518518">
If there has already been a joint to which J is fold-
able by E. then J&apos; is that joint, e&apos; is the node on J&apos;,
.1&apos; becomes a total multiple joint, and the foldability
relation remains unchanged. Otherwise, J&apos; and e&apos; are
newly created, b dominates e&apos;, and the foldability re-
lation is augmented. We call the former case folding.
and the latter unfolding.
If a is a proper atomic constraint or an argument of
a proper atomic constraint, then &amp; stands for the set
whose elements are this proper atomic constraint and
its arguments; otherwise &amp; = {u}.
In the case of unfolding, each node v in is copied to
v&apos;, and each link a (a 6) connecting v and n is copied
to a&apos; connecting v&apos; and some node n&apos;. is the copy of
n if n E and n&apos; ri otherwise. Relevant Joints are
copied accordingly so as to preserve the instantiation
possibilities of the network.
There are two cases, splitting and non-splitting,
about how to create a&apos;. In the former, it is guaran-
teed that no layer of the splitting domain including a
before the copy overlaps with both v and v&apos; after the
copy. Such a guarantee is obtained if a = R U R-1 for
some bijection R or (inclusive) 6 and a belong to the
same splitting domain. There is no such guarantee in
the non-splitting case.
In the splitting case, as is illustrated in Figure 6, the
n-domains of a and a&apos; are disjoint when n&apos; =
</bodyText>
<figure confidence="0.837068">
a,
11
</figure>
<figureCaption confidence="0.999852">
Figure 6: Copy of Links (Splitting)
</figureCaption>
<bodyText confidence="0.9525945">
In the non-splitting case, as is illustrated in Figure
7, if a was a loop, v and v&apos; is connected by an addi-
</bodyText>
<figure confidence="0.914635">
Ez&gt;
11
</figure>
<figureCaption confidence="0.999962">
Figure 7: Copy of Links (Non-Splitting)
</figureCaption>
<bodyText confidence="0.9995155">
tional link representing a relation pertaining to the lay-
ers overlapping both v and v&apos;. Further if a was involved
in a multiple joint of 71, then a subsumption along a to
77 must be done before creating a&apos;; otherwise the right
instantiation possibilities cannot be represented.
In both splitting and non-splitting cases, the probes
that v had, if any, are deleted, and v and /I are licensed
to generate new probes. Then every remaining probe
on a is copied to a probe on a&apos;, towards v&apos;, and the same
origin. Further, each probe in II is advanced through
e&apos; onto every link r ( b) connected with e&apos; so that e&apos;
should be behind the probe. If there is another probe
on r towards e&apos; and with the same origin, then both
probes are deleted.
Finally, in both folding and unfolding, if 6 dominated
e before this subsumption, e is deleted because it has
become the empty set now. This deletion propagates
across links and nodes until possibly non-empty sets
</bodyText>
<page confidence="0.9918">
84
</page>
<bodyText confidence="0.999903391304348">
are encountered: that is. until you come across par-
tial or multiple joints of remaining nodes.&apos; Now the
subsumption is done.
To properly split splitting domains, we must aug-
ment this subsumption procedure so that a probe may
carry. instead of origin.. some information about which
layers of the relevant splitting domain are involved in
the node behind the probe. Such probes are transmit-
ted from proper atomic constraints to their arguments
and vice versa. A link is deleted if it contains two
probes with opposite directions and associated with
disjoint sets of layers. Further details are omitted due
to the space limitation.,
So far we have discussed subsumption in general.
Below we describe the particularities of subsump-
tions along equalities and subsumptions along inference
links.
A subsumption along an equality is triggered by a
dependency between arguments. We say that there is a
dependency between two arguments, when they com-
pete with each other and are connected by a dependency
path. Nodes a and 3 compete with each other when
they are the first arguments of
</bodyText>
<listItem confidence="0.998855666666667">
â¢ two bindings (as in e=f(*) and ti=g(â¢)),
â¢ a binding and a feature specification, or
â¢ two feature specifications with the same feature.
</listItem>
<bodyText confidence="0.999775294117647">
A dependency path connecting a and 3 is a sequence
6162 â¢ â¢ â¢ 6, of strong egublities such that the endpoints
of bi are at-, and ai (1 &lt; i &lt; n), bi and bi+i are
involved in different joints of ai one of which is total
(1 &lt; i &lt;n). ao = a and aâ = 3. An equality is strong
when it belongs to a clause or the predicate domain of
a defined predicate, or When a subsumption has taken
place along that equality.
A probe it on an equality b might trigger a subsump-
tion to advance it. when there is a dependency between
the origin a of 7r and another node 3 and b is included
in a dependency path connecting a and 3.
Suppose the scope of a includes another node /..3 com-
peting with a. If the prOper atomic constraints A and
B. each involving a and 3 as the first argument. re-
spectively, are connected by an inference link b. then b
absorbs B. as shown in Figure 8. That is, the joint
</bodyText>
<figureCaption confidence="0.993712">
Figure 8: Absorption by Link
</figureCaption>
<bodyText confidence="0.997920071428571">
of B involving 6 is modified so that 3 dominates 3,
because A has turned out to subsume B. Any other
inference link involved in this joint is deleted, because
&apos;This combination of copy and deletion is vacuous and thus
may be omitted in actual implementation for the unfolding cases.
The deletion of probes in the splitting case may also be avoided
in such a situation.
it has turned out to be the empty set. Of course each
equality accompanying b must absorb its endnode in B
at the same time. If there is no inference link between A
and B. then B is deleted. Deletions of links and nodes
propagate so long as the empty set is encountered, as
said before.
A subsumption along an inference link may be trig-
gered by cost assigned to the input node. Each literal
in a clause may be assigned a cost. Similarly to the as-
sumability cost of Hobbs et al. [5], the cost of a literal
corresponds to the difficulty to abductively assume its
negation. For instance, if you want to assume atomic
constraint a by using a clause backwards whereas the
cost of the literal in this clause is not zero, then you
are to do something in order to cancel the cost. In this
sense, an atomic constraint with a cost is regarded as a
goal to achieve, and the abductive usage of the clause
which gives rise to the goal is regarded as the motiva-
tion to set up that goal. A cost may be canceled by
making the atomic constraint subsume another which
is more readily believable. That is, a goal is fulfilled
when it is established by some reason other than its
motivation.
The input node of a subsumption along an inference
link is the goal atomic constraint in the rest of the
paper.2 Such a subsumption eliminates the cost if the
target node has been derived from the top clause with-
out recourse to that very subsumption. Otherwise the
cost is inherited into the clause which contains the out-
put node. In a Horn clause normally used with all the
atomic constraints therein being true, the head literal
inherits the cost from a body atomic constraint, and
the body atomic constraints inherit the cost from the
head literal. We neglect the cost inheritance among
body atomic constraints.
</bodyText>
<subsectionHeader confidence="0.99823">
3.2 Heuristics
</subsectionHeader>
<bodyText confidence="0.999600909090909">
Subsumptions along equalities and those along infer-
ence links both encompass top-down and bottom-up
information flow. Some heuristics are necessary to con-
trol such an otherwise promiscuous system of computa-
tion so that more relevant pieces of information should
be exploited with greater preferences.
Each heuristic for a subsumption along an equality
is that one of the following conditions raises the pref-
erence of such a subsumption.
(HI) The origin of a probe on the axis is close to (typ-
ically included in) the top clause or is a constant.
(II2) A dependency path involving the axis and con-
necting an argument with the origin of a probe
on the axis is short.
Both these conditions are regarded as indicating that
the transmitted information (about the origin) is highly
relevant to the destination of this transmission. In this
connection, a subsumption along an equality is unlikely
to happen if the axis belongs to the predicate domain
of a free predicate and the target joint is partial, since
the conveyed information would not be very relevant
to the target node.
</bodyText>
<tableCaption confidence="0.3148405">
2Subsumptions for checking consistency need not be triggered
by cost.
</tableCaption>
<figure confidence="0.42514825">
a=f(.) a=f(.)
13=f(*)
13=f(*)
8
</figure>
<page confidence="0.998038">
85
</page>
<bodyText confidence="0.9951104375">
As for subsuinptions along inference links, the fol-
lowing conditions each raise the preference.
(H3) Corresponding arguments of the input node and
the target node are connected via short depen-
dency paths with the same node. (That is, those
arguments are shared.&apos;)
(H4) The target node has already been derived from
the top clause.
(1-13) raises the possibility for instances of the two ar-
guments to coincide in the domain of interpretation.
(H3) amounts to a generalization (or relaxation) of the
condition on which an inference link absorbs one of its
endnodes. (114) guarantees that. the subsumption in
question will lead to an immediate elimination of the
cost of the input node. Probably (114) could be relaxed
to he a graded condition.
</bodyText>
<sectionHeader confidence="0.995378" genericHeader="method">
4 Parsing
</sectionHeader>
<bodyText confidence="0.997111">
Let us consider a simple case of context-free parsing
based on the following grammar.
</bodyText>
<equation confidence="0.647548">
P a
P PP
</equation>
<bodyText confidence="0.6442365">
A parsing based on this grammar is formulated by the
program as follows.
</bodyText>
<figure confidence="0.983137">
(5) true -p(Ao,B) -A0=[alA1] -A1=[a1A2] â¢ â¢;
(4)) p(CalX],X);
(41) p(X,Z) -p(X,Y) -p(Y,Z);
Depicted in Figure 9 are the four types of clauses cre-
(a) (b)
(c) (d)
</figure>
<figureCaption confidence="0.999957">
Figure 9: Clauses Produced through Parsing
</figureCaption>
<bodyText confidence="0.98364424">
ated by this parsing process. A â -= Ca l .] is a shorthand
representation for a â¢=[.1.] plus an equality between
the second argument and (the argument bound by) a.
(a) is a copy of clause 4) in (5), and the other clauses are
copies of W. A label i of a link means that the relevant
part of the network is in the scope of argument A,. The
reason why only these types of clauses are generated
is that in this case every dependency arises between a
.= [a I .] in the top clause and another .= [a I.] some-
where else and the first argument of the former is the
origin of the subsumptions to resolve that dependency.
A strict proof will be obtained by mathematical induc-
tion. Since the number of these clauses is 0(n3) due to
(d) and each of them may be generated in a constant
time, the time complexity of the entire parsing is 0(773),
where n is the sentence length. Each clause is guaran-
teed to be generated in a constant time, because each
foldability test can be performed in a constant time, as
discussed later. By employing a general optimization
technique, we can eliminate the clauses of type (d), so
that the space complexity is reduced to 0(n2). Thus,
our general control scheme naturally gives rise to stan-
dard parsing procedures such as Earley&apos;s algorithm and
chart parsing.
(5) is graphically represented as Figure 10. We
</bodyText>
<figureCaption confidence="0.914449">
Figure 10: Parsing (1)
</figureCaption>
<bodyText confidence="0.999926181818182">
omit the links involved in the predicate domain of a
free predicate, until they are modified as in Figure 8.
Thus no links among â¢=[ale]s are shown in Figure
10. Here is a dependency between the first â¢=Ea I.] in
the top clause and the &apos;&lt;a1.] in 4), as indicated by
the dependency paths, which consist of thick links. To
let information flow from the top clause following the
above heuristic (H1), we are to do the two subsump-
tions indicated by the two thin arrows.
Those subsumptions copy 4) to 4)1 and 41 to 411, re-
sulting in Figure 11. For expository convenience, we
</bodyText>
<figureCaption confidence="0.975114">
Figure 11: Parsing (2)
</figureCaption>
<page confidence="0.989764">
86
</page>
<bodyText confidence="0.9932458">
assume here without loss of generality that copying of a
clause produces a separate clause rather than one shar-
ing atomic constraints with the original clause. Note
that the first argument of the â¢=[a I.] in (1)1 is sub-
sumed by Ao.
Computation goes on into the same direction, and
the two subsumptions are to happen as shown in Fig-
ure 11. Folding takes place this time, and the result is
to shift the two inference links upwards, as in Figure
12. Now the first .= [a I.] in the top clause dominates
</bodyText>
<figureCaption confidence="0.970608">
Figure 12: Parsing (3)
</figureCaption>
<bodyText confidence="0.9998679375">
the â¢=[a I.] in 4:01 as indicated by the inference link
between them. because, as indicated by number 0 in
(Di, the first argument of the former is within the scope
of the first argument Of the latter. Now the equality
in the right-hand side of (1)1 is within the scope of A1,
as indicated in the figure. This subsumption also en-
genders a new set of dependencies between the first
argument of the second .= [a I .] in the top clause and
that of .= [a I .] in 4), as indicated again by thick links
in Figure 12. By executing the indicated subsumption
following (H1). W1 is copied to t1/2, so that we obtain
Figure 13. Further advancing subsumptions as shown
there, we get Figure 14. Computation goes on in the
similar way.
As mentioned above, we are able to assume that each
foldability test is performed in a constant time. This
assumption is justified by, for instance, sorting the fold-
ability information from each joint in the chronical or-
der of the first subsuffiption which advanced probes
with the relevant origin. In the present parsing exam-
ple. this order happens to be the increasing order of
the suffix of A.
It is straightforward to integrate such a phrase-
structure parsing with computation on internal struc-
tures of grammatical categories represented in terms of
feature bundles, for instance. See [2, 3] for further de-
tails in this regard. Note that the above derivation of
the parsing process is more general than the parsing-
as-deduction approaches [6, 7], because it is free from
stipulation of the left-to-right and to-down processing
direction and also from task-dependency with regard
to parsing or context-free grammar.
</bodyText>
<figureCaption confidence="0.9999255">
Figure 13: Parsing (4)
Figure 14: Parsing (5)
</figureCaption>
<page confidence="0.998443">
87
</page>
<sectionHeader confidence="0.983877" genericHeader="method">
5 Generation
</sectionHeader>
<bodyText confidence="0.968291066666667">
Here we consider how to verbalize the following seman-
tic content in English.
S((laughed, kim))
This means that Kim laughed, based on Situation
Theory [1]. That is. in some situation S there is an
event which is of the sort laughed and whose agent
is kim. So a sentence we might want to generate is
&apos;Kim laughed.&apos; S may be interpreted as, for instance,
the speaker&apos;s model of the hearer&apos;s model of the world.
A state of affairs ((laughed, kim)) will be regarded as
variable Li constrained by two feature specifications
rel(L1,1aughed) and agt (L1 ,kim)
The initial state of computation could be formulated
in terms of a program including the following clauses,
among much more others.
</bodyText>
<equation confidence="0.57265675">
(A) true -s(SEM,WO,W1) -SHSEM -say(WO)
-rel(L1,1aughed)$
-agt (1..1,kim)s â¢ â¢ â¢ ;
(B) s(SEM,X,Z) -np(SBJSEM,X,Y)
vp(SEM,S8JSEM,Y,Z) ;
(C) np(kim,X,Y) -X= [&apos;kiln &apos; Ms;
(D) vp(L,AGT,X,Y) -X= [&apos;laughed&apos; 1&apos;W
-rel(L,laughed) -agt(L,AGT) ;
</equation>
<bodyText confidence="0.999375117647059">
say(WO) means that the utterance beginning at HO
should be actually uttered. SHSEM and SHI1 seper-
a.tely exist in (A), because the next utterance need
not directly refer to Ll. For instance, one can mean
that Kim laughed by saying &apos;Do you know that Kim
laughed?&apos; instead of just &apos;Kim laughed,&apos; or by doing
something other than utterance. One might even just
give up the goal and say something quite different.
A attached to an atomic constraint represents a
cost, so that the atomic constraint is a goal. The three
goals in (A) altogether amount to a macroscopic goal
to make a state of affairs ((laughed, kiln)) hold in sit-
uation S.
What we would like to demonstrate below is again
that the control heuristics described in Section 3 tend
to trigger the right operations depending upon the com-
putational context, provided that the current goal is to
be reached by some linguistic means; that is, by eventu-
ally uttering some sentence. Below we pay attention to
only one maximal consistent structure of the sentence
at a time just for the sake of simplicity, but the actual
generation process may involve OR-parallel computa-
tion similar to that in parsing of the previous section.
Figure 15 graphically represents clauses (A) and
(C). A proper atomic constraint with a binary pred-
icate. possibly together with equalities involving the
two arguments, is represented here as an arrow from
(an argument equalized with) the first argument to (an
argument equalized with) the second argument. Links
in predicate domains are selectively displayed for ex-
pository simplicity.
The most. probable operations to take place here
are subsumptions involving one of these three goals.
There should be innumerable combinations for such
</bodyText>
<sectionHeader confidence="0.456214" genericHeader="method">
&apos;laughed&apos;
</sectionHeader>
<figureCaption confidence="0.995591">
Figure 15: Generation (1)
</figureCaption>
<bodyText confidence="0.996928222222222">
subsumptions, because the speaker&apos;s lexicon must in-
clude a large number of atomic constraints of the form
â¢ â¢, rel (0 ,â¢) and agt (. ,â¢), even though subsump-
tions with extralinguistic parts of the constraints are
excluded due to the above provision that the current
goal is to be fulfilled by linguistic means.
However, two of such subsumptions are preferred to
the others. One is the subsumption concerning the
two â¢Hos in (A), and the other is from the rel (0 ,â¢)
in (A) to that in (D). In both cases, the two atomic
constraints share the same argument for the same ar-
gument place. which raises the preference due to (113).
Let us tentatively choose just the latter subsumption
in this particular presentation. No big difference would
follow in the long run, even if the former subsumption
or both were chosen instead.
By the subsumption concerning the two rel(â¢,â¢)s,
we obtain the structure shown in Figure 16. We have
</bodyText>
<figureCaption confidence="0.998319">
Figure 16: Generation (2)
</figureCaption>
<figure confidence="0.926022333333333">
kim
...... ---subsumption
absorption
kim
laughed..
subsumption-----
\,1)( )$(D&apos;)
â411$
&apos;laughed&apos;
</figure>
<page confidence="0.997348">
88
</page>
<bodyText confidence="0.998761818181818">
copied clause (D) to (D), because the re].(.,.) in (A)
is a goal. Now in Figure 16. vp(â¢,â¢,â¢,â¢) in (D&apos;) is
a goal. by inheriting the cost from rel(.,.) of (A).
The cost of â¢=[. I.] in (D&apos;) is inherent, as indicated
in (D). Now the most probable next computation is
the sequence of subsumptions along the thick link(s)
constituting a dependency path. Following the heuris-
tic (H1). those subsumptions propagate from the top
clause. After that. the inference link between the two
agt (â¢,â¢)s absorbs the one in (B).
This gives us Figure 17. (D&apos;) has not been dupli-
</bodyText>
<figure confidence="0.498015">
&apos;laughed&apos;
</figure>
<figureCaption confidence="0.987576">
Figure 17: Generation (3)
</figureCaption>
<bodyText confidence="0.998871833333333">
cated here, because the above subsumptions did not,
actually duplicate any clause. In this context, the sub-
sumption concerning the two vp (â¢ ,â¢ , â¢ , â¢)s is possible.
since the one in (D&apos;) is a goal. Due to (H3), this sub-
sumption is more preferable than the others concerning
two vp(â¢,â¢,â¢,â¢)s, because their first arguments are
both connected to kim (that is, the first argument of
â¢=kim) via short dependency paths. As a result, (B)
is copied to (B&apos;) and the vp(â¢ ,â¢,â¢,â¢) in (B&apos;) is domi-
nated by that in (D&apos;), as in Figure 18.
Now that s (â¢ , â¢ , â¢) in. (B&apos;) is anew goal, it is caused
to subsume another s(., â¢ ,â¢) in (A). According to
(H4), this subsumption is particularly preferable be-
cause (A) is the top clause. On the other band. the
subsumption from the first argument. of np(â¢ ,â¢,.) in
(B&apos;) to the first argument of np(â¢,â¢,â¢) in (C) could
take place here, to resolve the cyclic dependency about
kim referred to from (A) and (C). This subsumption
is the most probable operation concerning this depen-
dency in this context, because it is along the shortest
relevant dependency path. We assume that the direc-
tion of this subsumption, is downwards, as indicated in
Figure 18. It will be the same in the long run if it were
in the opposite direction.
The mentioned computation in Figure 18 takes us
to Figure 19. We have a new top clause (Al. which
shares most part. of itself with (A). except the copy
of s(â¢ , â¢ ,â¢). Some of the previous goals have disap-
peared due to the subsumption concerning s(â¢ , â¢
Now the remaining goals are â¢= [. â¢]s in (C&apos;) and (D&apos;).
</bodyText>
<figure confidence="0.99034975">
kim
substnfiption
subsumption
subsumption
</figure>
<figureCaption confidence="0.962468">
Figure 18: Generation (4)
</figureCaption>
<figure confidence="0.638756">
subsumption
</figure>
<figureCaption confidence="0.99752">
Figure 19: Generation (5)
</figureCaption>
<page confidence="0.999004">
89
</page>
<bodyText confidence="0.981363733333333">
and the â¢â¢ in the intersection of (A) and (C&apos;). We
might do a. subsumption concerning the two be-
cause they share both the arguments. This subsump-
tion could have happened earlier, of course, particu-
larly ever since both arguments came to be shared in
Figure 16 via (B) and (D&apos;). As mentioned before, how-
ever, it would have caused no essential difference even-
tually. At the same time we could execute the proce-
dure say(o) to realize the goal .=[.1.] in (C&apos;). It is
reasonable to assume that this computation is triggered
by the fact that the argument of say(.) subsumes the
first argument of this .= [.1.]. This heuristic for fir-
ing procedures looks generally applicable not only to
utterance but also to every other output procedure.
Thus we move to a new computational context in
</bodyText>
<figureCaption confidence="0.9461935">
Figure 20. The execution of say(.) has created a new
Figure 20: Generation (6)
</figureCaption>
<bodyText confidence="0.9704309">
â¢=E4, I â¢], so that &apos;Kim&apos; has been spoken aloud. This
.[.l.] dominates the .= [. I .] in (C&apos;), as indicated by
the thick link. Generation of &apos;Kim laughed&apos; completes
if say(.) is executed one step further.
Note that this generation process captures the
bottom-up feature of semantic head-driven generation
[8], especially when we move from Figure 15 through
Figure 18. The subsumption concerning the arguments
of np(â¢ ,â¢ ,â¢)s happening between Figure 18 and Figure
19 captures the top-down aspect as well.
</bodyText>
<sectionHeader confidence="0.995502" genericHeader="conclusions">
6 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999990117647059">
We have introduced a set of general heuristics for con-
trolling symbolic computation on logic constraints, and
demonstrated that sentence parsing and generation are
attributed to these heuristics. In the above presenta-
tion, parsing is for the most part based on truth main-
tenance (resolution of dependencies among arguments)
controlled by heuristics (H1) and (H2), whereas gener-
ation is more dependent on goal satisfaction controlled
by (113) and (114). In more realistic cases, however,
both processes would involve both kinds of computa-
tion in a more intertwined way.
A related nice feature of our framework is that, in
principle, all the types of constraints â syntactic, se-
mantic, pragmatic and extralinguistic â are treated
uniformly and integrated naturally, though a really ef-
ficient implementation of such an integrated system re-
quires further research. In this connection, we have un-
dertaken to study how to implement the above heuris-
tics in a more principled and flexible way, based on a
notion of potential energy [4], but the present paper
lacks the space for discussing the details.
In this paper we have discussed only task-
independent aspects of control heuristics. Our conjec-
ture is that we will be able to dispense with domain-
dependent and task-dependent control heuristics alto-
gether. The domain/task-dependent characteristics of
information processing will be captured in terms of the
assignment of energy functions to the relevant con-
straints. The resulting system will still be free from
stipulation of the directions of information flow, allow-
ing multi-directional information processing, since nei-
ther the symbolic component nor the analog compo-
nent (that is, energy) of the constraint refers explicitly
to information flow.
</bodyText>
<sectionHeader confidence="0.999457" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.989523333333333">
[1] Barwise, J. (1990) The Situation in Logic, CSLI
Lecture Notes No. 17.
[2] Hasida, K. (1990) &apos;Sentence Processing as Con-
straint Transformation,&apos; Proceedings of ECA I &apos;90.
[3] Hasida, K. and Tsuda, 11. (1991) &apos;Parsing without
Parser,&apos; International Workshop on Parsing Tech-
nologies, pp. 1-10, Cancun.
[4] Hasida, K. (in preparation) Potential Energy of
Combinatorial Constraints.
[5] Hobbs, J., Stickel, M., Martin, P., and Edwards,
D. (1988) &apos;Interpretation as Abduction,&apos; Proceed-
ings of the 26th Annual Meeting of ACL, pp. 95-
103.
[6] Pereira, F. C. N. and Warren, D.H.D. (1983)
&apos;Parsing as Deduction,&apos; Proceedings of the 21st
Annual Meeting of ACL, pp. 137-144.
[7] Shieber, S. M. (1988) &apos;A Uniform Architecture for
Parsing and Generation,&apos; Proceedings of the 12th
COLING, pp. 614-619.
[8] Shieber, S. M., van Noord, G., and Moore, R. C.
(1989) &apos;A Semantic-Head-Driven Generation Al-
gorithm for Unification-Based Formalisms,&apos; Pro-
ceedings of the 27th Annual Meeting of ACL, pp. 7-
17.
</reference>
<figure confidence="0.9971865625">
execution ..........
say(
true\
rel
s(
(B&apos;)
vp(
np(
laughed
rel
16.\/1:)(
(D&apos;)
&apos;laughed&apos;
&apos;kim&apos;
I I
kim
</figure>
<page confidence="0.943818">
90
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.257610">
<affiliation confidence="0.59042125">COMMON HEURISTICS FOR PARSING, GENERATION, AND WHATEVER ... HASIDA. Institute for New Generation Computer Technology</affiliation>
<address confidence="0.995852">Mita Kokusai Bldg. 21F. 1-4-28 Mita. Minato-ku. Tokyo 108</address>
<phone confidence="0.92459">Tel: +81-3-3456-3069. E-mail: hasidaÂ©icot.or.jp</phone>
<abstract confidence="0.99952175">This paper discusses general heuristics to control computation on symbolic constraints represented in terms of first-order logic programs. These heuristics are totally independent, of specific domains and tasks. Efficient computation for sentence parsing and generation naturally emerge from these heuristics, capturing essence of parsing procedures and semantic head-driven generation. Thus, the same representation of knowledge, including grammar and lexicon, can be exploited in a multi-directional manner in various aspects of language use.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Barwise</author>
</authors>
<title>The Situation in Logic,</title>
<date>1990</date>
<journal>CSLI Lecture Notes No.</journal>
<volume>17</volume>
<contexts>
<context position="26792" citStr="[1]" startWordPosition="4787" endWordPosition="4787">represented in terms of feature bundles, for instance. See [2, 3] for further details in this regard. Note that the above derivation of the parsing process is more general than the parsingas-deduction approaches [6, 7], because it is free from stipulation of the left-to-right and to-down processing direction and also from task-dependency with regard to parsing or context-free grammar. Figure 13: Parsing (4) Figure 14: Parsing (5) 87 5 Generation Here we consider how to verbalize the following semantic content in English. S((laughed, kim)) This means that Kim laughed, based on Situation Theory [1]. That is. in some situation S there is an event which is of the sort laughed and whose agent is kim. So a sentence we might want to generate is &apos;Kim laughed.&apos; S may be interpreted as, for instance, the speaker&apos;s model of the hearer&apos;s model of the world. A state of affairs ((laughed, kim)) will be regarded as variable Li constrained by two feature specifications rel(L1,1aughed) and agt (L1 ,kim) The initial state of computation could be formulated in terms of a program including the following clauses, among much more others. (A) true -s(SEM,WO,W1) -SHSEM -say(WO) -rel(L1,1aughed)$ -agt (1..1,k</context>
</contexts>
<marker>[1]</marker>
<rawString>Barwise, J. (1990) The Situation in Logic, CSLI Lecture Notes No. 17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hasida</author>
</authors>
<title>Sentence Processing as Constraint Transformation,&apos;</title>
<date>1990</date>
<booktitle>Proceedings of ECA I &apos;90.</booktitle>
<contexts>
<context position="26254" citStr="[2, 3]" startWordPosition="4700" endWordPosition="4701">imilar way. As mentioned above, we are able to assume that each foldability test is performed in a constant time. This assumption is justified by, for instance, sorting the foldability information from each joint in the chronical order of the first subsuffiption which advanced probes with the relevant origin. In the present parsing example. this order happens to be the increasing order of the suffix of A. It is straightforward to integrate such a phrasestructure parsing with computation on internal structures of grammatical categories represented in terms of feature bundles, for instance. See [2, 3] for further details in this regard. Note that the above derivation of the parsing process is more general than the parsingas-deduction approaches [6, 7], because it is free from stipulation of the left-to-right and to-down processing direction and also from task-dependency with regard to parsing or context-free grammar. Figure 13: Parsing (4) Figure 14: Parsing (5) 87 5 Generation Here we consider how to verbalize the following semantic content in English. S((laughed, kim)) This means that Kim laughed, based on Situation Theory [1]. That is. in some situation S there is an event which is of t</context>
</contexts>
<marker>[2]</marker>
<rawString>Hasida, K. (1990) &apos;Sentence Processing as Constraint Transformation,&apos; Proceedings of ECA I &apos;90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hasida</author>
<author>Tsuda</author>
</authors>
<title>Parsing without Parser,&apos;</title>
<date>1991</date>
<booktitle>International Workshop on Parsing Technologies,</booktitle>
<pages>1--10</pages>
<location>Cancun.</location>
<contexts>
<context position="26254" citStr="[2, 3]" startWordPosition="4700" endWordPosition="4701">imilar way. As mentioned above, we are able to assume that each foldability test is performed in a constant time. This assumption is justified by, for instance, sorting the foldability information from each joint in the chronical order of the first subsuffiption which advanced probes with the relevant origin. In the present parsing example. this order happens to be the increasing order of the suffix of A. It is straightforward to integrate such a phrasestructure parsing with computation on internal structures of grammatical categories represented in terms of feature bundles, for instance. See [2, 3] for further details in this regard. Note that the above derivation of the parsing process is more general than the parsingas-deduction approaches [6, 7], because it is free from stipulation of the left-to-right and to-down processing direction and also from task-dependency with regard to parsing or context-free grammar. Figure 13: Parsing (4) Figure 14: Parsing (5) 87 5 Generation Here we consider how to verbalize the following semantic content in English. S((laughed, kim)) This means that Kim laughed, based on Situation Theory [1]. That is. in some situation S there is an event which is of t</context>
</contexts>
<marker>[3]</marker>
<rawString>Hasida, K. and Tsuda, 11. (1991) &apos;Parsing without Parser,&apos; International Workshop on Parsing Technologies, pp. 1-10, Cancun.</rawString>
</citation>
<citation valid="false">
<authors>
<author>K Hasida</author>
</authors>
<title>(in preparation)</title>
<journal>Potential Energy of Combinatorial Constraints.</journal>
<contexts>
<context position="35040" citStr="[4]" startWordPosition="6192" endWordPosition="6192">ntrolled by (113) and (114). In more realistic cases, however, both processes would involve both kinds of computation in a more intertwined way. A related nice feature of our framework is that, in principle, all the types of constraints â syntactic, semantic, pragmatic and extralinguistic â are treated uniformly and integrated naturally, though a really efficient implementation of such an integrated system requires further research. In this connection, we have undertaken to study how to implement the above heuristics in a more principled and flexible way, based on a notion of potential energy [4], but the present paper lacks the space for discussing the details. In this paper we have discussed only taskindependent aspects of control heuristics. Our conjecture is that we will be able to dispense with domaindependent and task-dependent control heuristics altogether. The domain/task-dependent characteristics of information processing will be captured in terms of the assignment of energy functions to the relevant constraints. The resulting system will still be free from stipulation of the directions of information flow, allowing multi-directional information processing, since neither the </context>
</contexts>
<marker>[4]</marker>
<rawString>Hasida, K. (in preparation) Potential Energy of Combinatorial Constraints.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hobbs</author>
<author>M Stickel</author>
<author>P Martin</author>
<author>D Edwards</author>
</authors>
<title>Interpretation as Abduction,&apos;</title>
<date>1988</date>
<booktitle>Proceedings of the 26th Annual Meeting of ACL,</booktitle>
<pages>95--103</pages>
<contexts>
<context position="18844" citStr="[5]" startWordPosition="3393" endWordPosition="3393">ementation for the unfolding cases. The deletion of probes in the splitting case may also be avoided in such a situation. it has turned out to be the empty set. Of course each equality accompanying b must absorb its endnode in B at the same time. If there is no inference link between A and B. then B is deleted. Deletions of links and nodes propagate so long as the empty set is encountered, as said before. A subsumption along an inference link may be triggered by cost assigned to the input node. Each literal in a clause may be assigned a cost. Similarly to the assumability cost of Hobbs et al. [5], the cost of a literal corresponds to the difficulty to abductively assume its negation. For instance, if you want to assume atomic constraint a by using a clause backwards whereas the cost of the literal in this clause is not zero, then you are to do something in order to cancel the cost. In this sense, an atomic constraint with a cost is regarded as a goal to achieve, and the abductive usage of the clause which gives rise to the goal is regarded as the motivation to set up that goal. A cost may be canceled by making the atomic constraint subsume another which is more readily believable. Tha</context>
</contexts>
<marker>[5]</marker>
<rawString>Hobbs, J., Stickel, M., Martin, P., and Edwards, D. (1988) &apos;Interpretation as Abduction,&apos; Proceedings of the 26th Annual Meeting of ACL, pp. 95-103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F C N Pereira</author>
<author>D H D Warren</author>
</authors>
<title>Parsing as Deduction,&apos;</title>
<date>1983</date>
<booktitle>Proceedings of the 21st Annual Meeting of ACL,</booktitle>
<pages>137--144</pages>
<contexts>
<context position="26407" citStr="[6, 7]" startWordPosition="4726" endWordPosition="4727">stance, sorting the foldability information from each joint in the chronical order of the first subsuffiption which advanced probes with the relevant origin. In the present parsing example. this order happens to be the increasing order of the suffix of A. It is straightforward to integrate such a phrasestructure parsing with computation on internal structures of grammatical categories represented in terms of feature bundles, for instance. See [2, 3] for further details in this regard. Note that the above derivation of the parsing process is more general than the parsingas-deduction approaches [6, 7], because it is free from stipulation of the left-to-right and to-down processing direction and also from task-dependency with regard to parsing or context-free grammar. Figure 13: Parsing (4) Figure 14: Parsing (5) 87 5 Generation Here we consider how to verbalize the following semantic content in English. S((laughed, kim)) This means that Kim laughed, based on Situation Theory [1]. That is. in some situation S there is an event which is of the sort laughed and whose agent is kim. So a sentence we might want to generate is &apos;Kim laughed.&apos; S may be interpreted as, for instance, the speaker&apos;s mo</context>
</contexts>
<marker>[6]</marker>
<rawString>Pereira, F. C. N. and Warren, D.H.D. (1983) &apos;Parsing as Deduction,&apos; Proceedings of the 21st Annual Meeting of ACL, pp. 137-144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Shieber</author>
</authors>
<title>A Uniform Architecture for Parsing and Generation,&apos;</title>
<date>1988</date>
<booktitle>Proceedings of the 12th COLING,</booktitle>
<pages>614--619</pages>
<contexts>
<context position="26407" citStr="[6, 7]" startWordPosition="4726" endWordPosition="4727">stance, sorting the foldability information from each joint in the chronical order of the first subsuffiption which advanced probes with the relevant origin. In the present parsing example. this order happens to be the increasing order of the suffix of A. It is straightforward to integrate such a phrasestructure parsing with computation on internal structures of grammatical categories represented in terms of feature bundles, for instance. See [2, 3] for further details in this regard. Note that the above derivation of the parsing process is more general than the parsingas-deduction approaches [6, 7], because it is free from stipulation of the left-to-right and to-down processing direction and also from task-dependency with regard to parsing or context-free grammar. Figure 13: Parsing (4) Figure 14: Parsing (5) 87 5 Generation Here we consider how to verbalize the following semantic content in English. S((laughed, kim)) This means that Kim laughed, based on Situation Theory [1]. That is. in some situation S there is an event which is of the sort laughed and whose agent is kim. So a sentence we might want to generate is &apos;Kim laughed.&apos; S may be interpreted as, for instance, the speaker&apos;s mo</context>
</contexts>
<marker>[7]</marker>
<rawString>Shieber, S. M. (1988) &apos;A Uniform Architecture for Parsing and Generation,&apos; Proceedings of the 12th COLING, pp. 614-619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Shieber</author>
<author>G van Noord</author>
<author>R C Moore</author>
</authors>
<title>A Semantic-Head-Driven Generation Algorithm for Unification-Based Formalisms,&apos;</title>
<date>1989</date>
<booktitle>Proceedings of the 27th Annual Meeting of ACL,</booktitle>
<pages>7--17</pages>
<contexts>
<context position="2492" citStr="[8]" startWordPosition="373" endWordPosition="373"> control symbolic operation on the constraints. The symbolic operations here are regarded as transforming logic programs. They are quite permissive operations as a whole, allowing very diverse information processing involving top-down, bottom-up and other directions of information flow. The heuristics control this computatiOn so that only relevant information should be exploited and the resulting representation should be compact. Parsing and generation of sentences are shown to be efficiently done under these heuristics, and a standard parsing algorithm and the semantic head-driven generation [8] naturally emerge thereof. The rest of the paper is organized as follows. Section 2 describes the syntax of our system of constraint. Section 3 defines the symbolic computation on these constraints, and proposes a set of general heuristics to control computation. Section 4 and Section 5 show how sentence parsing and generation are executed efficiently by those heuristics. Finally, Section 6 concludes the paper. 2 Constraint Network A program is a set of clauses. A clause is a set of literals. A literal is an atomic constraint with a sign in front of it. The sign is a +&apos;, `-&apos;, or nil. A literal</context>
<context position="33795" citStr="[8]" startWordPosition="5993" endWordPosition="5993">mes the first argument of this .= [.1.]. This heuristic for firing procedures looks generally applicable not only to utterance but also to every other output procedure. Thus we move to a new computational context in Figure 20. The execution of say(.) has created a new Figure 20: Generation (6) â¢=E4, I â¢], so that &apos;Kim&apos; has been spoken aloud. This .[.l.] dominates the .= [. I .] in (C&apos;), as indicated by the thick link. Generation of &apos;Kim laughed&apos; completes if say(.) is executed one step further. Note that this generation process captures the bottom-up feature of semantic head-driven generation [8], especially when we move from Figure 15 through Figure 18. The subsumption concerning the arguments of np(â¢ ,â¢ ,â¢)s happening between Figure 18 and Figure 19 captures the top-down aspect as well. 6 Concluding Remarks We have introduced a set of general heuristics for controlling symbolic computation on logic constraints, and demonstrated that sentence parsing and generation are attributed to these heuristics. In the above presentation, parsing is for the most part based on truth maintenance (resolution of dependencies among arguments) controlled by heuristics (H1) and (H2), whereas generation</context>
</contexts>
<marker>[8]</marker>
<rawString>Shieber, S. M., van Noord, G., and Moore, R. C. (1989) &apos;A Semantic-Head-Driven Generation Algorithm for Unification-Based Formalisms,&apos; Proceedings of the 27th Annual Meeting of ACL, pp. 7-17.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>