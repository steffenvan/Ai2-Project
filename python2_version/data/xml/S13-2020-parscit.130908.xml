<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.023214">
<title confidence="0.998024">
UNAL: Discriminating between Literal and Figurative
Phrasal Usage Using Distributional Statistics and POS tags
</title>
<author confidence="0.97377">
Sergio Jimenez, Claudia Becerra
</author>
<affiliation confidence="0.831158">
Universidad Nacional de Colombia
</affiliation>
<address confidence="0.761597666666667">
Ciudad Universitaria,
edificio 453, oficina 114
Bogotá, Colombia
</address>
<email confidence="0.9864465">
sgjimenezv@unal.edu.co
cjbecerrac@unal.edu.co
</email>
<sectionHeader confidence="0.995647" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994586733333333">
In this paper we describe the system used to
participate in the sub task 5b in the Phrasal Se-
mantics challenge (task 5) in SemEval 2013.
This sub task consists in discriminating lit-
eral and figurative usage of phrases with
compositional and non-compositional mean-
ings in context. The proposed approach is
based on part-of-speech tags, stylistic features
and distributional statistics gathered from the
same development-training-test text collec-
tion. The system obtained a relative improve-
ment in accuracy against the most-frequent-
class baseline of 49.8% in the “unseen con-
texts” (LexSample) setting and 8.5% in “un-
seen phrases” (AllWords).
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99943975">
The Phrasal Semantics task-5b in SemEval 2013
consisted in the discrimination of literal of figura-
tive usage of phrases in context (Korkontzelos et al.,
2013). For instance, the occurrence in a text of the
phrase “a piece of cake” can be used whether to re-
fer to something that is pretty easy or to an actual
piece of cake. The motivation for this task is that
such discrimination could improve the quality and
performance of other tasks like machine translation
and information retrieval.
This problem has been studied in the past. Lin
(1999) observed that the distributional characteris-
tics of the literal and figurative usage are different.
Katz and Giesbrecht (2006) showed that the similar-
ities among contexts are correlated with their literal
or figurative usage. Birke and Sarkar (2006) clus-
</bodyText>
<note confidence="0.515842666666667">
Alexander Gelbukh
CIC-IPN
Av. Juan Dios Bátiz, Av. Mendizábal,
</note>
<address confidence="0.3544905">
Col. Nueva Industrial Vallejo
CP 07738, DF, México
</address>
<email confidence="0.786771">
gelbukh@gelbukh.com
</email>
<bodyText confidence="0.999902911764706">
tered literal and figurative contexts using a word-
sense-disambiguation approach. Fazly et al. (2009)
showed that literal and figurative usages are related
to particular syntactical forms. Sporleder and Li
(2009) showed that for a particular phrase the con-
texts of its literal usages are more cohesive than
those of its figurative usages. Inspired by these
works and in a new observation, we proposed a set or
features based on cohesiveness, syntax and stylom-
etry (Section 2), which are used to train a machine
learning classifier.
The cohesiveness between a phrase an its context
can be measured aggregating the relatedness of the
context words against the target phrase. This cohe-
siveness should be high for phrases used literally.
Conversely, figurative usages can occur in a large
variety of contexts implying low cohesiveness. For
instance, the cohesiveness of the phrase “a piece of
cake” against context words such as “coffee”, “birth-
day” and “bakery” should be high. The distribu-
tional measures used to obtain the needed related-
ness scores and the proposed measures of cohesive-
ness are presented in subsection 2.1.
Moreover, we observed a stylistic trend in the
training data set. That is, figurative usage tends to
occur later in the document in comparison with the
literal usage. Consequently, a small set of features
that exploits this particular observation is proposed
in subsection 2.2.
Fazly et al. (2009) showed that idiomatic phrases
composed of a verb and a noun (e.g. “break a leg”)
differ from their literal usages in the use of some
syntactic structures. For instance, idiomatic phrases
are less flexible in the use of determiners, pluraliza-
</bodyText>
<page confidence="0.977936">
114
</page>
<bodyText confidence="0.986074636363636">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 114–117, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
tion and passivization. In order to capture that no-
tion in a simple way, a set of features form a part-
of-speech tagger was included in the feature set (see
subsection 2.3).
In Section, additional details of the proposed sys-
tem are provided jointly with the obtained official
results. Finally, in sections 4 and 5 a brief discus-
sion of the results and some concluding remarks are
presented.
</bodyText>
<sectionHeader confidence="0.99424" genericHeader="introduction">
2 Features
</sectionHeader>
<bodyText confidence="0.999934857142857">
Each instance of the training and test sets consist of a
short document d where one or more occurrences of
its target phase pd are annotated. For each particular
phrase p, several instances are provided correspond-
ing to literal or figurative usages. In this section, the
set of features that was extracted from each instance
to provide a vectorial representation is presented.
</bodyText>
<subsectionHeader confidence="0.994931">
2.1 Cohesiveness Features
</subsectionHeader>
<bodyText confidence="0.979914615384615">
Let’s start with some definitions borrowed from the
information retrieval field: D is a collection of doc-
uments, df(w) is the number of documents in D
where the word w occurs (document frequency),
df(w ∧ pd) is the number of documents where w
and a target phrase pd co-occur, tf(w, d) is the num-
ber of occurrences of w in a document d ∈ D (term
frequency), and idf (w) = 1o92 d� (w) is the inverse
D|
document frequency of w (Jones, 2004).
A simple distributional measure of relatedness be-
tween w and p can be obtained with the following
ratio:
</bodyText>
<equation confidence="0.996658333333333">
df(w ∧ pd)
R(w,p) = (1)
df(w)
</equation>
<bodyText confidence="0.9992404">
Pointwise mutual information (PMI) (Church and
Hanks, 1990) is another distributional measure that
can be used for measuring the relatedness of w and
p. The probabilities needed for its calculation can be
obtained by maximum likelihood estimation (MLE):
</bodyText>
<equation confidence="0.99909575">
P(w) ≈ df(w)
|D |, P(pd) ≈ df(pd)
|D |and P(w ∧ pd) ≈
df(wnpd)
|D |.
Thus, PMI is given by this expression:
PMI(w, pd) = 1o92P(w ∧ pd) l (2)
G(w) · P(pd) /
</equation>
<table confidence="0.9945739">
EwEd&apos; R(w, pd)
EwEd&apos; tf(w, d)
E wEd&apos; idf(w)
E wEd&apos; PMI(w, pd)
E wEd&apos; NPMI(w, pd)
E wEd&apos; (tf(w, d) · R(w, pd))
E wEd&apos; (idf(w) · R(w, pd))
F8 E wEd&apos; (R(w, pd) · PMI(w,pd))
E wEd&apos; (R(w,pd) · NPMI(w,pd))
E wEd&apos; (tf(w,d) · idf(w))
E wEd&apos; (tf(w, pd) · PMI(w, pd))
E wEd&apos; (tf(w, pd) · NPMI(w, pd))
E wEd&apos; (idf(w) · PMI(w,pd))
E wEd&apos; (idf(w) · NPMI(w, pd))
EwEd&apos; (PMI(w, pd) · NPMI(w, pd))
EwEd&apos; (tf(w, d) · idf(w) · R(w, pd))
EwEd&apos; (tf(w, d) · R(w, pd) · PMI(w, pd))
EwEd&apos; (tf(w, d) · R(w, pd) · NPMI(w, pd))
EwEd&apos; (tf(w, d) · idf(w) · PMI(w, pd))
EwEd&apos; (tf(w, d) · idf(w) · NPMI(w, pd))
</table>
<tableCaption confidence="0.999657">
Table 1: Cohesiveness features
</tableCaption>
<bodyText confidence="0.987335666666667">
Furthermore, the scores obtained through eq. 2
can be normalized in the interval [+2,0] with the fol-
lowing expression:
</bodyText>
<equation confidence="0.993956666666667">
PMI(w, pd)
NPMI(w, pd) = − + 1 (3)
1o92(P(w ∧ pd))
</equation>
<bodyText confidence="0.999987083333333">
A measure of the cohesiveness between a docu-
ment d against its target phrase pd, can be obtained
by aggregating the pairwise relatedness scores be-
tween all the words in d and pd. For instance, us-
ing eq. 1 that measure is EwEd&apos; R(w, pd), where d&apos;
is the set of different words in d. The equations 1,
2 and 3 can be used as weights associated to each
word, which can also be combined among them and
with if and idf weights. Such weight combinations
produce measures that can be used as cohesiveness
features for a document. The set of 20 features ob-
tained using this approach is shown in Table 1.
</bodyText>
<subsectionHeader confidence="0.997552">
2.2 Stylistic Features
</subsectionHeader>
<bodyText confidence="0.999655">
The set of stylistic features related to the document
length, vocabulary size and relative position of the
occurrence of the target phrase in a document is
shown in Table 2.
</bodyText>
<page confidence="0.990032">
115
</page>
<table confidence="0.97603375">
Relative position of pd in d
Document length in characters
Document length in tokens
Number of different words
</table>
<tableCaption confidence="0.989948">
Table 2: Stylistic features
</tableCaption>
<subsectionHeader confidence="0.998093">
2.3 Syntactic Features
</subsectionHeader>
<bodyText confidence="0.9990216">
The features F25 to F67 correspond to the set of 43
part-of-speech tags of the NLTK English POS tag-
ger (Loper and Bird, 2002). Each feature contains
the frequency of occurrence of each POS-tag in a
document d.
</bodyText>
<sectionHeader confidence="0.992675" genericHeader="method">
3 Experimental Setup and Results
</sectionHeader>
<bodyText confidence="0.99992875">
The data provided for this task consists of two data
sets LexSample and AllWords, which are divided
into development, training and test sets. Neverthe-
less, we considered a single training set aggregat-
ing the development and training parts from both
data sets for a total of 3,230 instances. Each train-
ing instance has a class label whether “literally” or
“figuratively” depending on the usage or the tar-
get phrase. Similarly, the aggregated test set con-
tains 1,112 instances, but with unknown values in
the class attribute.
Firstly, the syntactic features for each text were
obtained using the POS tagger included in the NLTK
v.2.0.4 (Loper and Bird, 2002). Secondly, all texts
were preprocessed by tokenizing, lowecasing, stop-
word removing, punctuation removing and stem-
ming using the Porter’s algorithm (1980). This pre-
processed version of the texts was used to obtain the
remaining cohesiveness and stylistic features. The
resulting vectorial data set was used to produce the
predictions labeled “UNAL.RUN1” through a Lo-
gistic classifier (Cessie and Houwelingen, 1992).
The implementation used for this classifier was the
included in WEKA v.3.6.9 (Hall et al., 2009). The
accuracies obtained by the different feature groups
in the training set using 10-fold cross validation are
shown in Table 3. The last column shows the per-
centage of relative improvement of different feature
sets combinations from the most frequent class base-
line to our best system using all features.
The predictions labeled “UNAL.RUN2” were ob-
tained with the same vectorial data set but adding
</bodyText>
<table confidence="0.999827142857143">
Features Accuracy % improv.
All features 0.7272 100.0%
Cohesiveness+Syntactic 0.7034 87.1%
Cohesiveness 0.6833 76.2%
Syntactic 0.6229 43.5%
Stylistic 0.5492 3.5%
Baseline MFC 0.5427 0.0%
</table>
<tableCaption confidence="0.9677925">
Table 3: Results by group of features in the training set
using 10-fold cross validation
</tableCaption>
<table confidence="0.999894333333333">
System LexSample AllWords Both
UNAL.RUN1 0.7222 0.6680 0.6970
UNAL.RUN2 0.7542 0.6448 0.7032
Baseline MFC 0.5034 0.6158 0.5558
Best SemEval’13 0.7795 0.6680 0.7276
# test instances 594 518 1,112
</table>
<tableCaption confidence="0.999489">
Table 4: Official results in the test set (accuracy)
</tableCaption>
<bodyText confidence="0.9999539">
as a nominal feature the target phrase of each in-
stance. The official results obtained by both sub-
mitted runs are shown in Table 4. Note that official
results in the test set are reported separately for the
data sets LexSample and AllWords. The LexSample
test set contains instances whose target phrases were
seen in the training set (i.e. unseen contexts). Un-
like LexSample, AllWords contains instances whose
target phrases were unseen in the training set (i.e.
unseen phrases).
</bodyText>
<sectionHeader confidence="0.998908" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999922666666667">
As it was expected, the results obtained in the “un-
seen context” setting were consistently better than
in “unseen phrases”. This result suggests that the
discrimination of literal and figurative usage heavily
depends on particular idiomatic phrases. This can
also be confirmed by the best accuracy obtained by
RUN2 compared with RUN1 in LexSample. Clearly,
the classifier used in RUN2 exploited the identifica-
tion of the phrase to leverage a priori information
about the phrase such as the most frequent usage.
Another factor that could undermine the results in
the “unseen phrases” setting is the low number of in-
stances per phrase in the AllWords test set, roughly a
third in comparison with LexSample. Given that the
effectiveness of the cohesiveness features depends
</bodyText>
<page confidence="0.997724">
116
</page>
<bodyText confidence="0.999955083333333">
on the number of documents where the idiomatic
phrase occurs, the predictions for this test set relied
mainly on the less effective features, namely syn-
tactic and stylistic features (see Table 3). However,
this problem could be alleviated obtaining the distri-
butional statistics from a large corpus with enough
occurrences of the unseen phrases.
Besides it is important to note, that in spite of the
low individual contribution of the stylistic features
to the overall accuracy (3.5%), when these are com-
bined with the remaining features they provide an
improvement of 12.9% (see Table 3).
</bodyText>
<sectionHeader confidence="0.999407" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999927625">
We participated in the Phrasal Semantics sub task 5b
in SemEval 2013. Our system proved the effective-
ness of the use of cohesiveness, stylistic and syn-
tactic features for discriminating literal from figura-
tive usage of idiomatic phrases. The most-frequent-
class baseline was overcame by 49.8% in the “un-
seen contexts” setting (LexSample) and 8.5% in “un-
seen phrases” (AllWords).
</bodyText>
<sectionHeader confidence="0.997475" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99992">
This research was funded in part by the Systems
and Industrial Engineering Department, the Office
of Student Welfare of the National University of
Colombia, Bogotá, and through a grant from the
Colombian Department for Science, Technology
and Innovation, Colciencias, proj. 1101-521-28465
with funding from “El Patrimonio Autónomo Fondo
Nacional de Financiamiento para la Ciencia, la Tec-
nología y la Innovación, Francisco José de Caldas.”
The third author recognizes the support from Mexi-
can Government (SNI, COFAA-IPN, SIP 20131702,
CONACYT 50206-H) and CONACYT–DST India
(proj. 122030 “Answer Validation through Textual
Entailment”).
</bodyText>
<sectionHeader confidence="0.998543" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998310759259259">
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for nearly unsupervised recognition of nonlit-
eral language. In Proceedings of the 11th Conference
of the European Chapter of the Association for Com-
putational Linguistics, Trento, Italy.
S. Le Cessie and J. C. Van Houwelingen. 1992. Ridge
estimators in logistic regression. Applied Statistics,
41(1):191.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Comput. Linguist., 16(1):22–29, March.
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identifica-
tion of idiomatic expressions. Comput. Linguist.,
35(1):61–103, March.
Mark Hall, Frank Eibe, Geoffrey Holmes, and Bernhard
Pfahringer. 2009. The WEKA data mining software:
An update. SIGKDD Explorations, 11(1):10–18.
Karen Spärck Jones. 2004. A statistical interpretation of
term specificity and its application in retrieval. Jour-
nal of Documentation, 60(5):493–502, October.
Graham Katz and Eugenie Giesbrecht. 2006. Automatic
identification of non-compositional multi-word ex-
pressions using latent semantic analysis. In Proceed-
ings of the Workshop on Multiword Expressions: Iden-
tifying and Exploiting Underlying Properties, MWE
’06, pages 12–19, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ioannis Korkontzelos, Torsten Zesch, Fabio Massimo
Zanzotto, and Chris Biemann. 2013. SemEval-2013
task 5: Evaluating phrasal semantics. In Proceedings
of the 7th International Workshop on Semantic Evalu-
ation (SemEval 2013).
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of the 37th
annual meeting of the Association for Computational
Linguistics on Computational Linguistics, ACL ’99,
page 317–324, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Edward Loper and Steven Bird. 2002. NLTK: the natu-
ral language toolkit. In Proceedings of the ACL Work-
shop on Effective Tools and Methodologies for Teach-
ing Natural Language Processing and Computational
Linguistics. Philadelphia. Association for Computa-
tional Linguistics.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 3(14):130–137, October.
Caroline Sporleder and Linlin Li. 2009. Unsupervised
recognition of literal and non-literal use of idiomatic
expressions. In Proceedings of the 12th Conference of
the European Chapter of the Association for Computa-
tional Linguistics, EACL ’09, page 754–762, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.998135">
117
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.344475">
<title confidence="0.995571">UNAL: Discriminating between Literal and Phrasal Usage Using Distributional Statistics and POS tags</title>
<author confidence="0.978608">Sergio Jimenez</author>
<author confidence="0.978608">Claudia</author>
<affiliation confidence="0.710793">Universidad Nacional de Ciudad</affiliation>
<address confidence="0.7641735">edificio 453, oficina Bogotá,</address>
<email confidence="0.987436">cjbecerrac@unal.edu.co</email>
<abstract confidence="0.997731733333333">In this paper we describe the system used to participate in the sub task 5b in the Phrasal Semantics challenge (task 5) in SemEval 2013. This sub task consists in discriminating literal and figurative usage of phrases with compositional and non-compositional meanings in context. The proposed approach is based on part-of-speech tags, stylistic features and distributional statistics gathered from the same development-training-test text collection. The system obtained a relative improvement in accuracy against the most-frequentclass baseline of 49.8% in the “unseen consetting and 8.5% in “un-</abstract>
<intro confidence="0.719468">phrases”</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Julia Birke</author>
<author>Anoop Sarkar</author>
</authors>
<title>A clustering approach for nearly unsupervised recognition of nonliteral language.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="1744" citStr="Birke and Sarkar (2006)" startWordPosition="259" endWordPosition="262">ce, the occurrence in a text of the phrase “a piece of cake” can be used whether to refer to something that is pretty easy or to an actual piece of cake. The motivation for this task is that such discrimination could improve the quality and performance of other tasks like machine translation and information retrieval. This problem has been studied in the past. Lin (1999) observed that the distributional characteristics of the literal and figurative usage are different. Katz and Giesbrecht (2006) showed that the similarities among contexts are correlated with their literal or figurative usage. Birke and Sarkar (2006) clusAlexander Gelbukh CIC-IPN Av. Juan Dios Bátiz, Av. Mendizábal, Col. Nueva Industrial Vallejo CP 07738, DF, México gelbukh@gelbukh.com tered literal and figurative contexts using a wordsense-disambiguation approach. Fazly et al. (2009) showed that literal and figurative usages are related to particular syntactical forms. Sporleder and Li (2009) showed that for a particular phrase the contexts of its literal usages are more cohesive than those of its figurative usages. Inspired by these works and in a new observation, we proposed a set or features based on cohesiveness, syntax and stylometr</context>
</contexts>
<marker>Birke, Sarkar, 2006</marker>
<rawString>Julia Birke and Anoop Sarkar. 2006. A clustering approach for nearly unsupervised recognition of nonliteral language. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Le Cessie</author>
<author>J C Van Houwelingen</author>
</authors>
<title>Ridge estimators in logistic regression.</title>
<date>1992</date>
<journal>Applied Statistics,</journal>
<volume>41</volume>
<issue>1</issue>
<marker>Le Cessie, Van Houwelingen, 1992</marker>
<rawString>S. Le Cessie and J. C. Van Houwelingen. 1992. Ridge estimators in logistic regression. Applied Statistics, 41(1):191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Comput. Linguist.,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="5237" citStr="Church and Hanks, 1990" startWordPosition="829" endWordPosition="832"> with some definitions borrowed from the information retrieval field: D is a collection of documents, df(w) is the number of documents in D where the word w occurs (document frequency), df(w ∧ pd) is the number of documents where w and a target phrase pd co-occur, tf(w, d) is the number of occurrences of w in a document d ∈ D (term frequency), and idf (w) = 1o92 d� (w) is the inverse D| document frequency of w (Jones, 2004). A simple distributional measure of relatedness between w and p can be obtained with the following ratio: df(w ∧ pd) R(w,p) = (1) df(w) Pointwise mutual information (PMI) (Church and Hanks, 1990) is another distributional measure that can be used for measuring the relatedness of w and p. The probabilities needed for its calculation can be obtained by maximum likelihood estimation (MLE): P(w) ≈ df(w) |D |, P(pd) ≈ df(pd) |D |and P(w ∧ pd) ≈ df(wnpd) |D |. Thus, PMI is given by this expression: PMI(w, pd) = 1o92P(w ∧ pd) l (2) G(w) · P(pd) / EwEd&apos; R(w, pd) EwEd&apos; tf(w, d) E wEd&apos; idf(w) E wEd&apos; PMI(w, pd) E wEd&apos; NPMI(w, pd) E wEd&apos; (tf(w, d) · R(w, pd)) E wEd&apos; (idf(w) · R(w, pd)) F8 E wEd&apos; (R(w, pd) · PMI(w,pd)) E wEd&apos; (R(w,pd) · NPMI(w,pd)) E wEd&apos; (tf(w,d) · idf(w)) E wEd&apos; (tf(w, pd) · PMI</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Comput. Linguist., 16(1):22–29, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Afsaneh Fazly</author>
<author>Paul Cook</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Unsupervised type and token identification of idiomatic expressions.</title>
<date>2009</date>
<journal>Comput. Linguist.,</journal>
<volume>35</volume>
<issue>1</issue>
<contexts>
<context position="1983" citStr="Fazly et al. (2009)" startWordPosition="292" endWordPosition="295">rformance of other tasks like machine translation and information retrieval. This problem has been studied in the past. Lin (1999) observed that the distributional characteristics of the literal and figurative usage are different. Katz and Giesbrecht (2006) showed that the similarities among contexts are correlated with their literal or figurative usage. Birke and Sarkar (2006) clusAlexander Gelbukh CIC-IPN Av. Juan Dios Bátiz, Av. Mendizábal, Col. Nueva Industrial Vallejo CP 07738, DF, México gelbukh@gelbukh.com tered literal and figurative contexts using a wordsense-disambiguation approach. Fazly et al. (2009) showed that literal and figurative usages are related to particular syntactical forms. Sporleder and Li (2009) showed that for a particular phrase the contexts of its literal usages are more cohesive than those of its figurative usages. Inspired by these works and in a new observation, we proposed a set or features based on cohesiveness, syntax and stylometry (Section 2), which are used to train a machine learning classifier. The cohesiveness between a phrase an its context can be measured aggregating the relatedness of the context words against the target phrase. This cohesiveness should be </context>
<context position="3302" citStr="Fazly et al. (2009)" startWordPosition="505" endWordPosition="508">xts implying low cohesiveness. For instance, the cohesiveness of the phrase “a piece of cake” against context words such as “coffee”, “birthday” and “bakery” should be high. The distributional measures used to obtain the needed relatedness scores and the proposed measures of cohesiveness are presented in subsection 2.1. Moreover, we observed a stylistic trend in the training data set. That is, figurative usage tends to occur later in the document in comparison with the literal usage. Consequently, a small set of features that exploits this particular observation is proposed in subsection 2.2. Fazly et al. (2009) showed that idiomatic phrases composed of a verb and a noun (e.g. “break a leg”) differ from their literal usages in the use of some syntactic structures. For instance, idiomatic phrases are less flexible in the use of determiners, pluraliza114 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 114–117, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics tion and passivization. In order to capture that notion in a simple way, a set of features form a parto</context>
</contexts>
<marker>Fazly, Cook, Stevenson, 2009</marker>
<rawString>Afsaneh Fazly, Paul Cook, and Suzanne Stevenson. 2009. Unsupervised type and token identification of idiomatic expressions. Comput. Linguist., 35(1):61–103, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Frank Eibe</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
</authors>
<title>The WEKA data mining software: An update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="8740" citStr="Hall et al., 2009" startWordPosition="1449" endWordPosition="1452">ures for each text were obtained using the POS tagger included in the NLTK v.2.0.4 (Loper and Bird, 2002). Secondly, all texts were preprocessed by tokenizing, lowecasing, stopword removing, punctuation removing and stemming using the Porter’s algorithm (1980). This preprocessed version of the texts was used to obtain the remaining cohesiveness and stylistic features. The resulting vectorial data set was used to produce the predictions labeled “UNAL.RUN1” through a Logistic classifier (Cessie and Houwelingen, 1992). The implementation used for this classifier was the included in WEKA v.3.6.9 (Hall et al., 2009). The accuracies obtained by the different feature groups in the training set using 10-fold cross validation are shown in Table 3. The last column shows the percentage of relative improvement of different feature sets combinations from the most frequent class baseline to our best system using all features. The predictions labeled “UNAL.RUN2” were obtained with the same vectorial data set but adding Features Accuracy % improv. All features 0.7272 100.0% Cohesiveness+Syntactic 0.7034 87.1% Cohesiveness 0.6833 76.2% Syntactic 0.6229 43.5% Stylistic 0.5492 3.5% Baseline MFC 0.5427 0.0% Table 3: Re</context>
</contexts>
<marker>Hall, Eibe, Holmes, Pfahringer, 2009</marker>
<rawString>Mark Hall, Frank Eibe, Geoffrey Holmes, and Bernhard Pfahringer. 2009. The WEKA data mining software: An update. SIGKDD Explorations, 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Spärck Jones</author>
</authors>
<title>A statistical interpretation of term specificity and its application in retrieval.</title>
<date>2004</date>
<journal>Journal of Documentation,</journal>
<volume>60</volume>
<issue>5</issue>
<contexts>
<context position="5041" citStr="Jones, 2004" startWordPosition="798" endWordPosition="799">r figurative usages. In this section, the set of features that was extracted from each instance to provide a vectorial representation is presented. 2.1 Cohesiveness Features Let’s start with some definitions borrowed from the information retrieval field: D is a collection of documents, df(w) is the number of documents in D where the word w occurs (document frequency), df(w ∧ pd) is the number of documents where w and a target phrase pd co-occur, tf(w, d) is the number of occurrences of w in a document d ∈ D (term frequency), and idf (w) = 1o92 d� (w) is the inverse D| document frequency of w (Jones, 2004). A simple distributional measure of relatedness between w and p can be obtained with the following ratio: df(w ∧ pd) R(w,p) = (1) df(w) Pointwise mutual information (PMI) (Church and Hanks, 1990) is another distributional measure that can be used for measuring the relatedness of w and p. The probabilities needed for its calculation can be obtained by maximum likelihood estimation (MLE): P(w) ≈ df(w) |D |, P(pd) ≈ df(pd) |D |and P(w ∧ pd) ≈ df(wnpd) |D |. Thus, PMI is given by this expression: PMI(w, pd) = 1o92P(w ∧ pd) l (2) G(w) · P(pd) / EwEd&apos; R(w, pd) EwEd&apos; tf(w, d) E wEd&apos; idf(w) E wEd&apos; PM</context>
</contexts>
<marker>Jones, 2004</marker>
<rawString>Karen Spärck Jones. 2004. A statistical interpretation of term specificity and its application in retrieval. Journal of Documentation, 60(5):493–502, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Katz</author>
<author>Eugenie Giesbrecht</author>
</authors>
<title>Automatic identification of non-compositional multi-word expressions using latent semantic analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, MWE ’06,</booktitle>
<pages>12--19</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1621" citStr="Katz and Giesbrecht (2006)" startWordPosition="240" endWordPosition="243">3 consisted in the discrimination of literal of figurative usage of phrases in context (Korkontzelos et al., 2013). For instance, the occurrence in a text of the phrase “a piece of cake” can be used whether to refer to something that is pretty easy or to an actual piece of cake. The motivation for this task is that such discrimination could improve the quality and performance of other tasks like machine translation and information retrieval. This problem has been studied in the past. Lin (1999) observed that the distributional characteristics of the literal and figurative usage are different. Katz and Giesbrecht (2006) showed that the similarities among contexts are correlated with their literal or figurative usage. Birke and Sarkar (2006) clusAlexander Gelbukh CIC-IPN Av. Juan Dios Bátiz, Av. Mendizábal, Col. Nueva Industrial Vallejo CP 07738, DF, México gelbukh@gelbukh.com tered literal and figurative contexts using a wordsense-disambiguation approach. Fazly et al. (2009) showed that literal and figurative usages are related to particular syntactical forms. Sporleder and Li (2009) showed that for a particular phrase the contexts of its literal usages are more cohesive than those of its figurative usages. </context>
</contexts>
<marker>Katz, Giesbrecht, 2006</marker>
<rawString>Graham Katz and Eugenie Giesbrecht. 2006. Automatic identification of non-compositional multi-word expressions using latent semantic analysis. In Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, MWE ’06, pages 12–19, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Korkontzelos</author>
<author>Torsten Zesch</author>
<author>Fabio Massimo Zanzotto</author>
<author>Chris Biemann</author>
</authors>
<title>SemEval-2013 task 5: Evaluating phrasal semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<contexts>
<context position="1109" citStr="Korkontzelos et al., 2013" startWordPosition="154" endWordPosition="157">nd figurative usage of phrases with compositional and non-compositional meanings in context. The proposed approach is based on part-of-speech tags, stylistic features and distributional statistics gathered from the same development-training-test text collection. The system obtained a relative improvement in accuracy against the most-frequentclass baseline of 49.8% in the “unseen contexts” (LexSample) setting and 8.5% in “unseen phrases” (AllWords). 1 Introduction The Phrasal Semantics task-5b in SemEval 2013 consisted in the discrimination of literal of figurative usage of phrases in context (Korkontzelos et al., 2013). For instance, the occurrence in a text of the phrase “a piece of cake” can be used whether to refer to something that is pretty easy or to an actual piece of cake. The motivation for this task is that such discrimination could improve the quality and performance of other tasks like machine translation and information retrieval. This problem has been studied in the past. Lin (1999) observed that the distributional characteristics of the literal and figurative usage are different. Katz and Giesbrecht (2006) showed that the similarities among contexts are correlated with their literal or figura</context>
</contexts>
<marker>Korkontzelos, Zesch, Zanzotto, Biemann, 2013</marker>
<rawString>Ioannis Korkontzelos, Torsten Zesch, Fabio Massimo Zanzotto, and Chris Biemann. 2013. SemEval-2013 task 5: Evaluating phrasal semantics. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic identification of noncompositional phrases.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, ACL ’99,</booktitle>
<pages>317--324</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1494" citStr="Lin (1999)" startWordPosition="224" endWordPosition="225">e) setting and 8.5% in “unseen phrases” (AllWords). 1 Introduction The Phrasal Semantics task-5b in SemEval 2013 consisted in the discrimination of literal of figurative usage of phrases in context (Korkontzelos et al., 2013). For instance, the occurrence in a text of the phrase “a piece of cake” can be used whether to refer to something that is pretty easy or to an actual piece of cake. The motivation for this task is that such discrimination could improve the quality and performance of other tasks like machine translation and information retrieval. This problem has been studied in the past. Lin (1999) observed that the distributional characteristics of the literal and figurative usage are different. Katz and Giesbrecht (2006) showed that the similarities among contexts are correlated with their literal or figurative usage. Birke and Sarkar (2006) clusAlexander Gelbukh CIC-IPN Av. Juan Dios Bátiz, Av. Mendizábal, Col. Nueva Industrial Vallejo CP 07738, DF, México gelbukh@gelbukh.com tered literal and figurative contexts using a wordsense-disambiguation approach. Fazly et al. (2009) showed that literal and figurative usages are related to particular syntactical forms. Sporleder and Li (2009)</context>
</contexts>
<marker>Lin, 1999</marker>
<rawString>Dekang Lin. 1999. Automatic identification of noncompositional phrases. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, ACL ’99, page 317–324, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Loper</author>
<author>Steven Bird</author>
</authors>
<title>NLTK: the natural language toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics. Philadelphia. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7453" citStr="Loper and Bird, 2002" startWordPosition="1246" endWordPosition="1249">s that can be used as cohesiveness features for a document. The set of 20 features obtained using this approach is shown in Table 1. 2.2 Stylistic Features The set of stylistic features related to the document length, vocabulary size and relative position of the occurrence of the target phrase in a document is shown in Table 2. 115 Relative position of pd in d Document length in characters Document length in tokens Number of different words Table 2: Stylistic features 2.3 Syntactic Features The features F25 to F67 correspond to the set of 43 part-of-speech tags of the NLTK English POS tagger (Loper and Bird, 2002). Each feature contains the frequency of occurrence of each POS-tag in a document d. 3 Experimental Setup and Results The data provided for this task consists of two data sets LexSample and AllWords, which are divided into development, training and test sets. Nevertheless, we considered a single training set aggregating the development and training parts from both data sets for a total of 3,230 instances. Each training instance has a class label whether “literally” or “figuratively” depending on the usage or the target phrase. Similarly, the aggregated test set contains 1,112 instances, but wi</context>
</contexts>
<marker>Loper, Bird, 2002</marker>
<rawString>Edward Loper and Steven Bird. 2002. NLTK: the natural language toolkit. In Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics. Philadelphia. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>3</volume>
<issue>14</issue>
<marker>Porter, 1980</marker>
<rawString>Martin Porter. 1980. An algorithm for suffix stripping. Program, 3(14):130–137, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Sporleder</author>
<author>Linlin Li</author>
</authors>
<title>Unsupervised recognition of literal and non-literal use of idiomatic expressions.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’09,</booktitle>
<pages>754--762</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2094" citStr="Sporleder and Li (2009)" startWordPosition="308" endWordPosition="311">in the past. Lin (1999) observed that the distributional characteristics of the literal and figurative usage are different. Katz and Giesbrecht (2006) showed that the similarities among contexts are correlated with their literal or figurative usage. Birke and Sarkar (2006) clusAlexander Gelbukh CIC-IPN Av. Juan Dios Bátiz, Av. Mendizábal, Col. Nueva Industrial Vallejo CP 07738, DF, México gelbukh@gelbukh.com tered literal and figurative contexts using a wordsense-disambiguation approach. Fazly et al. (2009) showed that literal and figurative usages are related to particular syntactical forms. Sporleder and Li (2009) showed that for a particular phrase the contexts of its literal usages are more cohesive than those of its figurative usages. Inspired by these works and in a new observation, we proposed a set or features based on cohesiveness, syntax and stylometry (Section 2), which are used to train a machine learning classifier. The cohesiveness between a phrase an its context can be measured aggregating the relatedness of the context words against the target phrase. This cohesiveness should be high for phrases used literally. Conversely, figurative usages can occur in a large variety of contexts implyin</context>
</contexts>
<marker>Sporleder, Li, 2009</marker>
<rawString>Caroline Sporleder and Linlin Li. 2009. Unsupervised recognition of literal and non-literal use of idiomatic expressions. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’09, page 754–762, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>