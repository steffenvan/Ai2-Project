<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018751">
<title confidence="0.9832365">
HulTech: A General Purpose System for Cross-Level Semantic Similarity
based on Anchor Web Counts
</title>
<author confidence="0.901951">
Jose G. Moreno Rumen Moraliyski Asma Berrezoug Ga¨el Dias
</author>
<affiliation confidence="0.891067">
Normandie University
</affiliation>
<address confidence="0.446921">
UNICAEN, GREYC CNRS
F-14032 Caen, France
</address>
<email confidence="0.997885">
firstname.lastname@unicaen.fr
</email>
<sectionHeader confidence="0.993872" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999989133333333">
This paper describes the HULTECH team par-
ticipation in Task 3 of SemEval-2014. Four
different subtasks are provided to the partici-
pants, who are asked to determine the semantic
similarity of cross-level test pairs: paragraph-
to-sentence, sentence-to-phrase, phrase-to-
word and word-to-sense. Our system adopts
a unified strategy (general purpose system) to
calculate similarity across all subtasks based
on word Web frequencies. For that purpose,
we define ClueWeb InfoSimba, a cross-level
similarity corpus-based metric. Results show
that our strategy overcomes the proposed base-
lines and achieves adequate to moderate re-
sults when compared to other systems.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.987935521739131">
Similarity between text documents is considered a
challenging task. Recently, many works concentrate on
the study of semantic similarity for multi-level text doc-
uments (Pilehvar et al., 2013), but skipping the cross-
level similarity task. In the later, the underlying idea is
that text similarity can be considered between pairs of
text documents at different granularities levels: para-
graph, sentence, phrase or word. One obvious partic-
ularity of this task is that text pairs may not share the
same characteristics of size, context or structure, i.e.,
the granularity level.
In task 3 of SemEval-2014, two different strategies
have been proposed to solve this issue. On the one
hand, participants may propose a combination of indi-
vidual systems, each one solving a particular subtask.
On the other hand, a general purpose system may be
proposed, which deals with all the subtasks following
the exact same strategy.
In this paper, we describe a language-independent
corpus-based general purpose system, which relies on
a huge freely available Web collection called Anchor-
ClueWeb12 (Hiemstra and Hauff, 2010). In particular,
we calculate ClueWeb InfoSimba1 a cross-level seman-
</bodyText>
<footnote confidence="0.992631">
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
1It is a Web version of InfoSimba (Dias et al., 2007).
</footnote>
<bodyText confidence="0.999196785714286">
tic similarity based on word-word frequencies. Indeed,
these frequencies are captured by the use of a colloca-
tion metric called SCP2 (Silva et al., 1999), which has
similar properties as the well studied PMI-IR (Turney,
2001) but does not over-evaluate rare events.
Our system outputs a normalized (between 0 and 1)
similarity value between two pieces of texts. However,
the subtasks proposed in task 3 of SemEval-2014 in-
clude a different scoring scale between 0 and 4. To
solve this issue, we applied linear, polynomial and ex-
ponential regressions as three different runs. Results
show that our strategy overcomes the proposed base-
lines and achieves adequate to moderate results when
compared to other systems.
</bodyText>
<sectionHeader confidence="0.975314" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.99947075">
Our system is based on a reduced version of the
ClueWeb12 dataset called Anchor ClueWeb12 and an
informative attributional similarity measure called In-
foSimba (Dias et al., 2007) adapted to this dataset.
</bodyText>
<subsectionHeader confidence="0.990819">
2.1 Anchor ClueWeb12 Dataset
</subsectionHeader>
<bodyText confidence="0.999936">
The Anchor ClueWeb12 dataset contains 0.5 billion
Web pages, which cover about 64% of the total num-
ber of Web pages in ClueWeb12. The particularity of
Anchor ClueWeb12 is that each Web page is repre-
sented by the anchor texts of the links pointing to it
in ClueWeb12. Web pages are indexed not on their
content but on their references. As such, the size of
the index is drastically reduced and the overall results
are consistent with full text indexing as discussed in
(Hiemstra and Hauff, 2010).
For development purposes, this dataset was indexed
in Solr 4.4 on a desktop computer using a batch in-
dexing script. Particularly, each compressed part file
of the Anchor ClueWeb12 was uncompressed, prepro-
cessed and indexed in a sequential way using the fea-
tures of incremental indexing offered by Solr (Smiley
and Pugh, 2009).
</bodyText>
<subsectionHeader confidence="0.940805">
2.2 InfoSimba
</subsectionHeader>
<bodyText confidence="0.999109333333333">
In (Dias et al., 2007), the authors proposed the hypothe-
sis that two texts are similar if they share related (even-
tually different) constituents. So, their concept of simi-
</bodyText>
<footnote confidence="0.626419">
2Symmetric Conditional Probability.
</footnote>
<page confidence="0.990274">
305
</page>
<note confidence="0.672777">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 305–308,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.9995516">
larity is not any more based on the exact match of con-
stituents but relies on related constituents (e.g. words).
For example, it is clear that the following text pieces
extracted from the sentence-to-phrase subtask are re-
lated3 although they do not share any word.
</bodyText>
<listItem confidence="0.9892255">
1. he is a nose-picker
2. an uncouth young man
</listItem>
<bodyText confidence="0.999016875">
The InfoSimba similarity measure models this phe-
nomenon evaluating individual similarities between all
possible words pairs. Indeed, each piece of text is rep-
resented by the vector of its words. So, given two
pieces of texts Xi and Xj, their similarity is defined
in Equation 1 where SCP(.,.) is the Symmetric Con-
ditional Probability association measure proposed in
(Silva et al., 1999) and defined in Equation 2.
</bodyText>
<equation confidence="0.998617">
SCP(wik,wjl). (1)
P(wik, wjl)2
SCP(wik, wjl) = P(wik) X P(wjl). (2)
</equation>
<bodyText confidence="0.9976765">
Following the previous example, the In-
foSimba value between the two vectors
</bodyText>
<equation confidence="0.956214">
X1 = {“he&amp;quot;, “is&amp;quot;, “a&amp;quot;, “nose-picker”} and
X2 = {“an&amp;quot;, “uncouth”, “young”, “man&amp;quot;} is
</equation>
<bodyText confidence="0.9995981">
an average weight formed by all possible words pairs
associations as illustrated in Figure 1. Note that each
vertex is a word of a Xl vector and each edge is
weighted by the SCP(.,.) value of the connected
words. In particular, each wij corresponds to the
word at the jth position in vector Xi, P(.,.) is the
joint probability of two words appearing in the same
document, P(.) is the marginal probability of any
word appearing in a document and p (resp. q) is the
size of the vector Xi (resp. Xj).
</bodyText>
<figureCaption confidence="0.938969">
Figure 1: Pairs of words evaluated when InfoSimba is
calculated.
</figureCaption>
<bodyText confidence="0.9571725">
In the case of task 3 of SemEval-2014, each text
pair is represented by two word vectors for which a
modified version of InfoSimba, ClueWeb InfoSimba,
is computed.
</bodyText>
<footnote confidence="0.9638695">
3The score of this pair (#85) in the training set is the max-
imum value 4.
</footnote>
<subsectionHeader confidence="0.985423">
2.3 ClueWeb InfoSimba
</subsectionHeader>
<bodyText confidence="0.999932666666667">
The final similarity metric, called ClueWeb InfoSimba
(CWIS), between two pieces of texts is defined in
Equation 3, where hits(w) returns the number of doc-
uments retrieved by Solr over Anchor ClueWeb12 for
the query w and hits(wa n wb) is the number of doc-
uments retrieved when both words are present simul-
taneously. In this case, SCP is modified into SCP-IR
similarly as PMI is to PMI-IR, i.e., using hits counts
instead of probability values (see Equation 4).
</bodyText>
<equation confidence="0.979509666666667">
1
CWIS(Xi,Xj) =
pq
SCP — IR(wik, wjl) = hits(wik n wjl)2w (4)
hits
hits (wik). ( jl)
</equation>
<subsectionHeader confidence="0.964276">
2.4 System Input
</subsectionHeader>
<bodyText confidence="0.999985875">
The task 3 of SemEval-2014 consists of (1) paragraph-
to-sentence, (2) sentence-to-phrase, (3) phrase-to-word
and (4) word-to-sense subtasks. Before submitting the
pieces of texts to our system, we first performed simple
stop-words removal with the NLTK toolkit (Bird et al.,
2009). Note that in the case of the word-to-sense sub-
task, the similarity is performed over the word itself
and the gloss of the corresponding sense4.
</bodyText>
<subsectionHeader confidence="0.952317">
2.5 Output Values Transformations
</subsectionHeader>
<bodyText confidence="0.9996485">
The CWIS(., .) similarity metric returns a value be-
tween 0 and 1. However, the subtasks suppose that
each pair must be attributed a score between 0 and 4.
As such, an adequate scale transformation must be per-
formed. For that purpose, we proposed linear, polyno-
mial and exponential regressions and submitted three
different runs, one for each regressions. Note that the
regressions have been tuned on the training dataset us-
ing the respective R regression functions with default
parameters:
</bodyText>
<listItem confidence="0.999931666666667">
• lm(y — x),
• lm(y — x + I(x2) + I(x3)),
• lm(log(y + e) — x),
</listItem>
<bodyText confidence="0.787015666666667">
where c6 is a small value included to avoid undefined
log values. The regression results on the test datasets
are presented in Figure 2.
</bodyText>
<footnote confidence="0.993746">
4Glosses are obtained from WordNet using the sense id
provided for the task by the organizers.
5In the case of linear and exponential, these are mono-
thetic functions therefore ranking-based evaluation metrics
give the same score before and after this step.
6In our experiments, this value was set to 0.001.
</footnote>
<equation confidence="0.945385857142857">
p
E
pq k=1
E q
l=1
1
IS(Xi,Xj) =
SCP — IR(wik, wjl).
(3)
p
E
k=1
E q
l=1
</equation>
<page confidence="0.992223">
306
</page>
<bodyText confidence="0.989600285714286">
tial transformations obviously show exact same values
(See Table 2).
Figure 2: Linear, polynomial and exponential predic-
tions for the test dataset of the paragraph-to-sentence
subtask (colored dots). Black dots correspond to the
obtained ClueWeb InfoSimba value versus the manu-
ally assigned score in the training dataset.
</bodyText>
<sectionHeader confidence="0.980854" genericHeader="evaluation">
3 Evaluation and Results
</sectionHeader>
<bodyText confidence="0.999520464285715">
For evaluation purposes, two metrics have been se-
lected by the organizers: Pearson correlation (Pearson,
1895) and Spearman’s rank correlation (Hollander and
Wolfe, 1973). Detailed information about the evalu-
ation setup can be found in the task discussion paper
(Jurgens et al., 2014).
All results are given in Tables 1 and 2 for each
run. Note that the baseline metric is calculated for the
longest common string (LCS) and that each regression
has been tuned on the training dataset for each one of
the four tasks.
First, in almost all cases, the results outperform the
baseline. Second, performances show that with a cer-
tain amount of information (longer pieces of texts), in-
teresting results can be obtained. However, when the
size decreases, the performance diminishes and extra
information is certainly needed to better capture the se-
mantics between two pieces of text. Third, the poly-
nomial regression provides better results for the Pear-
son correlation evaluation, while for the Rho test, linear
and polynomial regressions get the lead. Note that this
situation depends on the data distribution and cannot
be seen as a conclusive remark. However, it is cer-
tainly an important subject of study for our unsuper-
vised methodology.
Another key point is that training examples were
used only for evaluation purposes7. In the case of
Spearman’s rank correlation, the linear and exponen-
</bodyText>
<footnote confidence="0.943474">
7For Pearson correlation, valid interval was fixed to [0,4].
</footnote>
<sectionHeader confidence="0.997428" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999971363636364">
In this paper, we proposed a general purpose system
to deal with cross-level text similarity. The aim of
our research was to push as far as possible the lim-
its of language-independent corpus-based solutions in
a general context of text similarity. We were also con-
cerned with reproducibility and as such we exclusively
used publicly available datasets and tools$. The results
clearly show the limits of a simple solution based on
word statistics. Nevertheless, the framework can easily
be empowered with the straightforward introduction of
more competitive resources.
</bodyText>
<sectionHeader confidence="0.978696" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999836333333333">
The authors would like to thank the University of
Mostaganem (Algeria) for providing an internship to
Asma Berrezoug at the Normandie University.
</bodyText>
<sectionHeader confidence="0.998693" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9919293">
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O’Reilly Media, Inc., 1st edition.
Ga¨el Dias, Elsa Alves, and Jos´e Gabriel Pereira Lopes.
2007. Topic segmentation algorithms for text sum-
marization and passage retrieval: An exhaustive
evaluation. In Proceedings of AAAI, pages 1334–
1339.
Djoerd Hiemstra and Claudia Hauff. 2010. Mirex:
Mapreduce information retrieval experiments. In
CTIT Technical Report TR-CTIT-10-15, Centre for
Telematics and Information Technology, University
of Twente, pages 1–8.
Myles Hollander and Douglas A. Wolfe. 1973. Non-
parametric Statistical Methods. John Wiley and
Sons, New York.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. Task 3: Cross-level seman-
tic similarity. In Proceedings of SemEval-2014.
Karl Pearson. 1895. Note on regression and inheri-
tance in the case of two parents. Proceedings of the
Royal Society of London, 58(347-352):240–242.
Mohammad Taher Pilehvar, David Jurgens, and
Roberto Navigli. 2013. Align, disambiguate and
walk: A unified approach for measuring semantic
similarity. In Proceedings of ACL, pages 1341–
1351.
Joaquim Ferreira da Silva, Ga¨el Dias, Sylvie Guillor´e,
and Jos´e Gabriel Pereira Lopes. 1999. Using local-
maxs algorithm for the extraction of contiguous and
</reference>
<footnote confidence="0.8329115">
8Scripts to Index the Anchor ClueWeb12 Dataset are
available under request.
</footnote>
<page confidence="0.9938">
307
</page>
<table confidence="0.9998802">
Method Paragraph2Sentence Sentence2Phrase Phrase2Word Word2Sense
Linear (run 3) 0.669 0.671 0.232 0.137
Polynomial (run 1) 0.693 0.665 0.254 0.150
Exponential (run 2) 0.667 0.633 0.180 0.169
Baseline (LCS) 0.527 0.562 0.165 0.109
</table>
<tableCaption confidence="0.977745">
Table 1: Overall results for the Pearson correlation.
</tableCaption>
<table confidence="0.9999508">
Method Paragraph2Sentence Sentence2Phrase Phrase2Word Word2Sense
Linear (run 3) 0.688 0.633 0.260 0.124
Polynomial (run 1) 0.666 0.633 0.260 0.126
Exponential (run 2) 0.688 0.633 0.260 0.124
Baseline (LCS) 0.613 0.626 0.162 0.130
</table>
<tableCaption confidence="0.990667">
Table 2: Overall results for the Spearman’s rank correlation.
</tableCaption>
<reference confidence="0.993072714285714">
non-contiguous multiword lexical units. In Proceed-
ings of EPIA, pages 113–132.
David Smiley and Eric Pugh. 2009. Solr 1.4 Enter-
prise Search Server. Packt Publishing.
Peter Turney. 2001. Mining the web for synonyms:
Pmi-ir versus lsa on toefl. In Proceedings of ECML,
pages 491–502.
</reference>
<page confidence="0.998562">
308
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.751045">
<title confidence="0.9942455">HulTech: A General Purpose System for Cross-Level Semantic Similarity based on Anchor Web Counts</title>
<author confidence="0.998334">Jose G Moreno Rumen Moraliyski Asma Berrezoug Ga¨el_Dias</author>
<affiliation confidence="0.8936715">Normandie UNICAEN, GREYC</affiliation>
<address confidence="0.963807">F-14032 Caen,</address>
<email confidence="0.999413">firstname.lastname@unicaen.fr</email>
<abstract confidence="0.999381375">This paper describes the HULTECH team participation in Task 3 of SemEval-2014. Four different subtasks are provided to the participants, who are asked to determine the semantic similarity of cross-level test pairs: paragraphto-sentence, sentence-to-phrase, phrase-toword and word-to-sense. Our system adopts a unified strategy (general purpose system) to calculate similarity across all subtasks based on word Web frequencies. For that purpose, we define ClueWeb InfoSimba, a cross-level similarity corpus-based metric. Results show that our strategy overcomes the proposed baselines and achieves adequate to moderate results when compared to other systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
</authors>
<date>2009</date>
<booktitle>Natural Language Processing with Python. O’Reilly Media, Inc., 1st edition.</booktitle>
<contexts>
<context position="7141" citStr="Bird et al., 2009" startWordPosition="1140" endWordPosition="1143">s(wa n wb) is the number of documents retrieved when both words are present simultaneously. In this case, SCP is modified into SCP-IR similarly as PMI is to PMI-IR, i.e., using hits counts instead of probability values (see Equation 4). 1 CWIS(Xi,Xj) = pq SCP — IR(wik, wjl) = hits(wik n wjl)2w (4) hits hits (wik). ( jl) 2.4 System Input The task 3 of SemEval-2014 consists of (1) paragraphto-sentence, (2) sentence-to-phrase, (3) phrase-to-word and (4) word-to-sense subtasks. Before submitting the pieces of texts to our system, we first performed simple stop-words removal with the NLTK toolkit (Bird et al., 2009). Note that in the case of the word-to-sense subtask, the similarity is performed over the word itself and the gloss of the corresponding sense4. 2.5 Output Values Transformations The CWIS(., .) similarity metric returns a value between 0 and 1. However, the subtasks suppose that each pair must be attributed a score between 0 and 4. As such, an adequate scale transformation must be performed. For that purpose, we proposed linear, polynomial and exponential regressions and submitted three different runs, one for each regressions. Note that the regressions have been tuned on the training dataset</context>
</contexts>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python. O’Reilly Media, Inc., 1st edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elsa Alves Ga¨el Dias</author>
<author>Jos´e Gabriel Pereira Lopes</author>
</authors>
<title>Topic segmentation algorithms for text summarization and passage retrieval: An exhaustive evaluation.</title>
<date>2007</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>1334--1339</pages>
<marker>Ga¨el Dias, Lopes, 2007</marker>
<rawString>Ga¨el Dias, Elsa Alves, and Jos´e Gabriel Pereira Lopes. 2007. Topic segmentation algorithms for text summarization and passage retrieval: An exhaustive evaluation. In Proceedings of AAAI, pages 1334– 1339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Djoerd Hiemstra</author>
<author>Claudia Hauff</author>
</authors>
<title>Mirex: Mapreduce information retrieval experiments. In</title>
<date>2010</date>
<tech>CTIT Technical Report TR-CTIT-10-15,</tech>
<pages>1--8</pages>
<institution>Centre for Telematics and Information Technology, University of Twente,</institution>
<contexts>
<context position="2035" citStr="Hiemstra and Hauff, 2010" startWordPosition="298" endWordPosition="301">ot share the same characteristics of size, context or structure, i.e., the granularity level. In task 3 of SemEval-2014, two different strategies have been proposed to solve this issue. On the one hand, participants may propose a combination of individual systems, each one solving a particular subtask. On the other hand, a general purpose system may be proposed, which deals with all the subtasks following the exact same strategy. In this paper, we describe a language-independent corpus-based general purpose system, which relies on a huge freely available Web collection called AnchorClueWeb12 (Hiemstra and Hauff, 2010). In particular, we calculate ClueWeb InfoSimba1 a cross-level semanThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1It is a Web version of InfoSimba (Dias et al., 2007). tic similarity based on word-word frequencies. Indeed, these frequencies are captured by the use of a collocation metric called SCP2 (Silva et al., 1999), which has similar properties as the well studied PMI-IR (Turney, 2001) but does not over-evaluate rare events.</context>
<context position="3830" citStr="Hiemstra and Hauff, 2010" startWordPosition="585" endWordPosition="588">mative attributional similarity measure called InfoSimba (Dias et al., 2007) adapted to this dataset. 2.1 Anchor ClueWeb12 Dataset The Anchor ClueWeb12 dataset contains 0.5 billion Web pages, which cover about 64% of the total number of Web pages in ClueWeb12. The particularity of Anchor ClueWeb12 is that each Web page is represented by the anchor texts of the links pointing to it in ClueWeb12. Web pages are indexed not on their content but on their references. As such, the size of the index is drastically reduced and the overall results are consistent with full text indexing as discussed in (Hiemstra and Hauff, 2010). For development purposes, this dataset was indexed in Solr 4.4 on a desktop computer using a batch indexing script. Particularly, each compressed part file of the Anchor ClueWeb12 was uncompressed, preprocessed and indexed in a sequential way using the features of incremental indexing offered by Solr (Smiley and Pugh, 2009). 2.2 InfoSimba In (Dias et al., 2007), the authors proposed the hypothesis that two texts are similar if they share related (eventually different) constituents. So, their concept of simi2Symmetric Conditional Probability. 305 Proceedings of the 8th International Workshop </context>
</contexts>
<marker>Hiemstra, Hauff, 2010</marker>
<rawString>Djoerd Hiemstra and Claudia Hauff. 2010. Mirex: Mapreduce information retrieval experiments. In CTIT Technical Report TR-CTIT-10-15, Centre for Telematics and Information Technology, University of Twente, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myles Hollander</author>
<author>Douglas A Wolfe</author>
</authors>
<title>Nonparametric Statistical Methods.</title>
<date>1973</date>
<publisher>John Wiley and Sons,</publisher>
<location>New York.</location>
<contexts>
<context position="8913" citStr="Hollander and Wolfe, 1973" startWordPosition="1438" endWordPosition="1441">s, this value was set to 0.001. p E pq k=1 E q l=1 1 IS(Xi,Xj) = SCP — IR(wik, wjl). (3) p E k=1 E q l=1 306 tial transformations obviously show exact same values (See Table 2). Figure 2: Linear, polynomial and exponential predictions for the test dataset of the paragraph-to-sentence subtask (colored dots). Black dots correspond to the obtained ClueWeb InfoSimba value versus the manually assigned score in the training dataset. 3 Evaluation and Results For evaluation purposes, two metrics have been selected by the organizers: Pearson correlation (Pearson, 1895) and Spearman’s rank correlation (Hollander and Wolfe, 1973). Detailed information about the evaluation setup can be found in the task discussion paper (Jurgens et al., 2014). All results are given in Tables 1 and 2 for each run. Note that the baseline metric is calculated for the longest common string (LCS) and that each regression has been tuned on the training dataset for each one of the four tasks. First, in almost all cases, the results outperform the baseline. Second, performances show that with a certain amount of information (longer pieces of texts), interesting results can be obtained. However, when the size decreases, the performance diminish</context>
</contexts>
<marker>Hollander, Wolfe, 1973</marker>
<rawString>Myles Hollander and Douglas A. Wolfe. 1973. Nonparametric Statistical Methods. John Wiley and Sons, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Mohammad Taher Pilehvar</author>
<author>Roberto Navigli</author>
</authors>
<title>Task 3: Cross-level semantic similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of SemEval-2014.</booktitle>
<contexts>
<context position="9027" citStr="Jurgens et al., 2014" startWordPosition="1457" endWordPosition="1460">formations obviously show exact same values (See Table 2). Figure 2: Linear, polynomial and exponential predictions for the test dataset of the paragraph-to-sentence subtask (colored dots). Black dots correspond to the obtained ClueWeb InfoSimba value versus the manually assigned score in the training dataset. 3 Evaluation and Results For evaluation purposes, two metrics have been selected by the organizers: Pearson correlation (Pearson, 1895) and Spearman’s rank correlation (Hollander and Wolfe, 1973). Detailed information about the evaluation setup can be found in the task discussion paper (Jurgens et al., 2014). All results are given in Tables 1 and 2 for each run. Note that the baseline metric is calculated for the longest common string (LCS) and that each regression has been tuned on the training dataset for each one of the four tasks. First, in almost all cases, the results outperform the baseline. Second, performances show that with a certain amount of information (longer pieces of texts), interesting results can be obtained. However, when the size decreases, the performance diminishes and extra information is certainly needed to better capture the semantics between two pieces of text. Third, th</context>
</contexts>
<marker>Jurgens, Pilehvar, Navigli, 2014</marker>
<rawString>David Jurgens, Mohammad Taher Pilehvar, and Roberto Navigli. 2014. Task 3: Cross-level semantic similarity. In Proceedings of SemEval-2014.</rawString>
</citation>
<citation valid="false">
<title>Note on regression and inheritance in the case of two parents.</title>
<journal>Proceedings of the Royal Society of London,</journal>
<pages>58--347</pages>
<marker></marker>
<rawString>Karl Pearson. 1895. Note on regression and inheritance in the case of two parents. Proceedings of the Royal Society of London, 58(347-352):240–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Taher Pilehvar</author>
<author>David Jurgens</author>
<author>Roberto Navigli</author>
</authors>
<title>Align, disambiguate and walk: A unified approach for measuring semantic similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1341--1351</pages>
<contexts>
<context position="1120" citStr="Pilehvar et al., 2013" startWordPosition="154" endWordPosition="157">nce-to-phrase, phrase-toword and word-to-sense. Our system adopts a unified strategy (general purpose system) to calculate similarity across all subtasks based on word Web frequencies. For that purpose, we define ClueWeb InfoSimba, a cross-level similarity corpus-based metric. Results show that our strategy overcomes the proposed baselines and achieves adequate to moderate results when compared to other systems. 1 Introduction Similarity between text documents is considered a challenging task. Recently, many works concentrate on the study of semantic similarity for multi-level text documents (Pilehvar et al., 2013), but skipping the crosslevel similarity task. In the later, the underlying idea is that text similarity can be considered between pairs of text documents at different granularities levels: paragraph, sentence, phrase or word. One obvious particularity of this task is that text pairs may not share the same characteristics of size, context or structure, i.e., the granularity level. In task 3 of SemEval-2014, two different strategies have been proposed to solve this issue. On the one hand, participants may propose a combination of individual systems, each one solving a particular subtask. On the</context>
</contexts>
<marker>Pilehvar, Jurgens, Navigli, 2013</marker>
<rawString>Mohammad Taher Pilehvar, David Jurgens, and Roberto Navigli. 2013. Align, disambiguate and walk: A unified approach for measuring semantic similarity. In Proceedings of ACL, pages 1341– 1351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joaquim Ferreira da Silva</author>
<author>Sylvie Guillor´e Ga¨el Dias</author>
<author>Jos´e Gabriel Pereira Lopes</author>
</authors>
<title>Using localmaxs algorithm for the extraction of contiguous and non-contiguous multiword lexical units.</title>
<date>1999</date>
<booktitle>In Proceedings of EPIA,</booktitle>
<pages>113--132</pages>
<marker>Silva, Ga¨el Dias, Lopes, 1999</marker>
<rawString>Joaquim Ferreira da Silva, Ga¨el Dias, Sylvie Guillor´e, and Jos´e Gabriel Pereira Lopes. 1999. Using localmaxs algorithm for the extraction of contiguous and non-contiguous multiword lexical units. In Proceedings of EPIA, pages 113–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Smiley</author>
<author>Eric Pugh</author>
</authors>
<title>Solr 1.4 Enterprise Search Server.</title>
<date>2009</date>
<publisher>Packt Publishing.</publisher>
<contexts>
<context position="4157" citStr="Smiley and Pugh, 2009" startWordPosition="638" endWordPosition="641">ented by the anchor texts of the links pointing to it in ClueWeb12. Web pages are indexed not on their content but on their references. As such, the size of the index is drastically reduced and the overall results are consistent with full text indexing as discussed in (Hiemstra and Hauff, 2010). For development purposes, this dataset was indexed in Solr 4.4 on a desktop computer using a batch indexing script. Particularly, each compressed part file of the Anchor ClueWeb12 was uncompressed, preprocessed and indexed in a sequential way using the features of incremental indexing offered by Solr (Smiley and Pugh, 2009). 2.2 InfoSimba In (Dias et al., 2007), the authors proposed the hypothesis that two texts are similar if they share related (eventually different) constituents. So, their concept of simi2Symmetric Conditional Probability. 305 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 305–308, Dublin, Ireland, August 23-24, 2014. larity is not any more based on the exact match of constituents but relies on related constituents (e.g. words). For example, it is clear that the following text pieces extracted from the sentence-to-phrase subtask are related3 although</context>
</contexts>
<marker>Smiley, Pugh, 2009</marker>
<rawString>David Smiley and Eric Pugh. 2009. Solr 1.4 Enterprise Search Server. Packt Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Mining the web for synonyms: Pmi-ir versus lsa on toefl.</title>
<date>2001</date>
<booktitle>In Proceedings of ECML,</booktitle>
<pages>491--502</pages>
<contexts>
<context position="2595" citStr="Turney, 2001" startWordPosition="383" endWordPosition="384">ion called AnchorClueWeb12 (Hiemstra and Hauff, 2010). In particular, we calculate ClueWeb InfoSimba1 a cross-level semanThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1It is a Web version of InfoSimba (Dias et al., 2007). tic similarity based on word-word frequencies. Indeed, these frequencies are captured by the use of a collocation metric called SCP2 (Silva et al., 1999), which has similar properties as the well studied PMI-IR (Turney, 2001) but does not over-evaluate rare events. Our system outputs a normalized (between 0 and 1) similarity value between two pieces of texts. However, the subtasks proposed in task 3 of SemEval-2014 include a different scoring scale between 0 and 4. To solve this issue, we applied linear, polynomial and exponential regressions as three different runs. Results show that our strategy overcomes the proposed baselines and achieves adequate to moderate results when compared to other systems. 2 System Description Our system is based on a reduced version of the ClueWeb12 dataset called Anchor ClueWeb12 an</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter Turney. 2001. Mining the web for synonyms: Pmi-ir versus lsa on toefl. In Proceedings of ECML, pages 491–502.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>