<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.098251">
<title confidence="0.969186">
The Interaction of Syntactic Theory and Computational Psycholinguistics
</title>
<author confidence="0.998158">
Frank Keller
</author>
<affiliation confidence="0.999918">
School of Informatics, University of Edinburgh
</affiliation>
<address confidence="0.985218">
10 Crichton Street, Edinburgh EH8 9AB, UK
</address>
<email confidence="0.995474">
keller@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.999556" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999946026315789">
Typically, current research in psycholinguistics
does not rely heavily on results from theoretical
linguistics. In particular, most experimental work
studying human sentence processing makes very
straightforward assumptions about sentence struc-
ture; essentially only a simple context-free gram-
mar is assumed. The main text book in psycholin-
guistics, for instance, mentions Minimalism in its
chapter on linguistic description (Harley, 2001,
ch. 2), but does not provide any details, and all the
examples in this chapter, as well as in the chapters
on sentence processing and language production
(Harley, 2001, chs. 9, 12), only use context-free
syntactic structures with uncontroversial phrase
markers (S, VP, NP, etc.). The one exception is
traces, which the textbook discusses in the context
of syntactic ambiguity resolution.
Harley’s (2001) textbook is typical of experi-
mental psycholinguistics, a field in which most
of the work is representationally agnostic, i.e., as-
sumptions about syntactic structure left implicit,
or limited to uncontroversial ones. However, the
situation is different in computational psycholin-
guistics, where researchers built computationally
implemented models of human language process-
ing. This typically involves making one’s theoret-
ical assumptions explicit, a prerequisite for being
able to implement them. For example, Crocker’s
(1996) model explicitly implements assumptions
from the Principles and Parameters framework,
while Hale (2006) uses probabilistic Minimalist
grammars, or Mazzei et al. (2007) tree-adjoining
grammars.
Here, we will investigate how evidence regard-
ing human sentence processing can inform our as-
sumptions about syntactic structure, at least in so
far as this structure is used in computational mod-
els of human parsing.
</bodyText>
<sectionHeader confidence="0.79396" genericHeader="categories and subject descriptors">
2 Prediction and Incrementality
</sectionHeader>
<bodyText confidence="0.999962871794872">
Evidence from psycholinguistic research suggests
that language comprehension is largely incremen-
tal, i.e., that comprehenders build an interpretation
of a sentence on a word-by-word basis. Evidence
for incrementality comes from speech shadow-
ing, self-paced reading, and eye-tracking studies
(Marslen-Wilson, 1973; Konieczny, 2000; Tanen-
haus et al., 1995): as soon as a reader or listener
perceives a word in a sentence, they integrate it as
fully as possible into a representation of the sen-
tence thus far. They experience differential pro-
cessing difficulty during this integration process,
depending on the properties of the word and its re-
lationship to the preceding context.
There is also evidence for full connectivity in
human language processing (Sturt and Lombardo,
2005). Full connectivity means that all words
are connected by a single syntactic structure; the
parser builds no unconnected tree fragments, even
for the incomplete sentences (sentence prefixes)
that arise during incremental processing.
Furthermore, there is evidence that readers or
listeners make predictions about upcoming mate-
rial on the basis of sentence prefixes. Listeners
can predict an upcoming post-verbal element, de-
pending on the semantics of the preceding verb
(Kamide et al., 2003). Prediction effects can also
be observed in reading. Staub and Clifton (2006)
showed that following the word either readers pre-
dict the conjunction or and the complement that
follows it; processing was facilitated compared to
structures that include or without either. In an
ERP study, van Berkum et al. (1999) found that
listeners use contextual information to predict spe-
cific lexical items and experience processing diffi-
culty if the input is incompatible with the predic-
tion.
The concepts of incrementality, connectedness,
and prediction are closely related: in order to guar-
</bodyText>
<note confidence="0.3499185">
Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics, pages 43–46,
Athens, Greece, 30 March, 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.999437">
43
</page>
<bodyText confidence="0.999982263157895">
antee that the syntactic structure of a sentence pre-
fix is fully connected, it may be necessary to build
phrases whose lexical anchors (the words that they
relate to) have not been encountered yet. Full con-
nectedness is required to ensure that a fully inter-
pretable structure is available at any point during
incremental sentence processing.
Here, we explore how these key psycholinguis-
tic concepts (incrementality, connectedness, and
prediction) can be realized within a new version of
tree-adjoining grammar, which we call Psycholin-
guistically Motivated TAG (PLTAG). We propose
a formalization of PLTAG and a linking theory that
derives predictions of processing difficulty from it.
We then present an implementation of this model
and evaluate it against key experimental data re-
lating to incrementality and prediction. This ap-
proach is described in more detail in Demberg and
Keller (2008) and Demberg and Keller (2009).
</bodyText>
<sectionHeader confidence="0.953035" genericHeader="method">
3 Modeling Explicit Prediction
</sectionHeader>
<bodyText confidence="0.999988105263158">
We propose a theory of sentence processing
guided by the principles of incrementality, con-
nectedness, and prediction. The core assumption
of our proposal is that a sentence processor that
maintains explicit predictions about the upcoming
structure has to validate these predictions against
the input it encounters. Using this assumption, we
can naturally combine the forward-looking aspect
of Hale’s (2001) surprisal (sentence structures are
computed incrementally and unexpected continu-
ations cause difficulty) with the backward-looking
integration view of Gibson’s (1998) dependency
locality theory (previously predicted structures are
verified against new evidence, leading to process-
ing difficulty as predictions decay with time).
In order to build a model that implements this
theory, we require an incremental parser that is ca-
pable of building fully connected structures and
generating explicit predictions from which we can
then derive a measure of processing difficulty. Ex-
isting parsers and grammar formalism do not meet
this specification. While there is substantial previ-
ous work on incremental parsing, none of the ex-
isting model observes full connectivity. One likely
reason for this is that full connectivity cannot be
achieved using canonical linguistic structures as
assumed in standard grammar formalisms such
as CFG, CCG, TAG, LFG, or HPSG. Instead, a
stack has to be used to store partial structures
and retrieve them later when it has become clear
(through additional input) how to combine them.
We therefore propose a new variant of the tree-
adjoining grammar (TAG) formalism which real-
izes full connectedness. The key idea is that in
cases where new input cannot be combined im-
mediately with the existing structure, we need to
predict additional syntactic material, which needs
to be verified against future input later on.
</bodyText>
<sectionHeader confidence="0.998637" genericHeader="method">
4 Incremental Processing with PLTAG
</sectionHeader>
<bodyText confidence="0.999974857142857">
PLTAG extends normal LTAG in that it specifies
not only the canonical lexicon containing lexical-
ized initial and auxiliary trees, but also a predictive
lexicon which contains potentially unlexicalized
trees, which we will call prediction trees. Each
node in a prediction tree is annotated with indices
of the form sj
sj, where inner nodes have two iden-
tical indices, root nodes only have a lower index
and foot and substitution nodes only have an up-
per index. The reason for only having half of the
indices is that these nodes (root, foot, and substitu-
tion nodes) still need to combine with another tree
in order to build a full node. If an initial tree substi-
tutes into a substitution node, the node where they
are integrated becomes a full node, with the upper
half contributed by the substitution node and the
lower half contributed by the root node.
Prediction trees have the same shape as trees
from the normal lexicon, with the difference that
they do not contain substitution nodes to the right
of their spine (the spine is the path from the root
node to the anchor), and that their spine does not
have to end with a lexical item. The reason for
the missing right side of the spine and the miss-
ing lexical item are considerations regarding the
granularity of prediction. This way, for example,
we avoid predicting verbs with specific subcate-
gorization frames (or even a specific verb). In gen-
eral, we only predict upcoming structure as far
as we need it, i.e., as required by connectivity or
subcategorization. (However, this is a preliminary
assumption, the optimal prediction grain size re-
mains an open research question.)
PLTAG allows the same basic operations (sub-
stitution and adjunction) as normal LTAG, the only
difference is that these operations can also be ap-
plied to prediction trees. In addition, we assume
a verification operation, which is needed to val-
idate previously integrated prediction trees. The
tree against which verification happens has to al-
ways match the predicted tree in shape (i.e., the
</bodyText>
<page confidence="0.998125">
44
</page>
<bodyText confidence="0.9997428">
verification tree must contain all the nodes with
a unique, identical index that were present in the
prediction tree, and in the same order; any addi-
tional nodes present in the verification tree must
be below the prediction tree anchor or to the right
of its spine). This means that the verification op-
eration does not introduce any tree configurations
that would not be allowed by normal LTAG. Note
that substitution or adjunction with a predictive
tree and the verification of that tree always occur
pairwise, since each predicted node has to be veri-
fied. A valid parse for a sentence must not contain
any nodes that are still annotated as being predic-
tive – all of them have to be validated through ver-
ification by the end of the sentence.
</bodyText>
<sectionHeader confidence="0.99387" genericHeader="method">
5 Modeling Processing Difficulty
</sectionHeader>
<bodyText confidence="0.999971925">
Our variant of TAG is designed to implement a
specific set of assumptions about human language
processing (strong incrementality with full con-
nectedness, prediction, ranked parallel process-
ing). The formalism forms the basis for the pro-
cessing theory, which uses the parser states to de-
rive estimates of processing difficulty. In addition,
we need a linking theory that specifies the mathe-
matical relationship between parser states and pro-
cessing difficulty in our model.
During processing, the elementary tree of each
new word is integrated with any previous struc-
ture, and a set of syntactic expectations is gener-
ated (these expectations can be easily read off the
generated tree in the form of predicted trees). Each
of these predicted trees has a time-stamp that en-
codes when it was first predicted, or last activated
(i.e., accessed). Based on the timestamp, a tree’s
decay at verification time is calculated, under the
assumption that recently-accessed structures are
easier to integrate.
In our model, processing difficulty is thus in-
curred during the construction of the syntactic
analyses, as calculated from the probabilities of
the elementary trees (this directly corresponds to
Haleian surprisal calculated over PLTAG struc-
tures instead of over CFG structures). In addition
to this, processing difficulty has a second com-
ponent, the cost of verifying earlier predictions,
which is subject to decay.
The verification cost component bears similari-
ties to DLT integration costs, but we do not calcu-
late distance in terms of number of discourse refer-
ents intervening between a dependent and its head.
Rather verification cost is determined by the num-
ber of words intervening between a prediction and
its verification, subject to decay. This captures the
intuition that a prediction becomes less and less
useful the longer ago it was made, as it decays
from memory with increasing distance.
</bodyText>
<sectionHeader confidence="0.994238" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999964035714286">
In Demberg and Keller (2009), we present an im-
plementation of the PLTAG model, including a
lexicon induction procedure, a parsing algorithm,
and a probability model. We show that the result-
ing framework can capture experimental results
from the literature, and can explain both locality
and prediction effects, which standard models of
sentence processing like DLT and surprisal are un-
able to account for simultaneously.
Our model therefore constitutes a step towards
a unified theory of human parsing that potentially
captures a broad range of experimental findings.
This work also demonstrates that (computational)
psycholinguistics cannot afford to be representa-
tionally agnostic – a comprehensive, computation-
ally realistic theory of human sentence process-
ing needs to make explicit assumptions about syn-
tactic structure. Here, we showed how the fact
that human parsing is incremental and predic-
tive necessitates certain assumptions about syntac-
tic structure (such as full connectedness), which
can be implemented by augmenting an existing
grammar formalism, viz., tree-adjoining grammar.
Note, however, that it is difficult to show that this
approach is the only one that is able to realize the
required representational assumptions; other solu-
tions using different grammar formalisms are pre-
sumably possible.
</bodyText>
<sectionHeader confidence="0.998742" genericHeader="conclusions">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9950478">
This abstract reports joint work with Vera Dem-
berg, described in more detail in Demberg and
Keller (2008) and Demberg and Keller (2009).
The research was supported by EPSRC grant
EP/C546830/1 Prediction in Human Parsing.
</bodyText>
<sectionHeader confidence="0.992362" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.5919902">
Crocker, Matthew W. 1996. Computational Psy-
cholinguistics: An Interdisciplinary Approach
to the Study of Language. Kluwer, Dordrecht.
Demberg, Vera and Frank Keller. 2008. A psy-
cholinguistically motivated version of TAG. In
</reference>
<page confidence="0.99624">
45
</page>
<reference confidence="0.997001857142857">
Proceedings of the 9th International Workshop
on Tree Adjoining Grammars and Related For-
malisms. T¨ubingen.
Demberg, Vera and Frank Keller. 2009. A com-
putational model of prediction in human pars-
ing: Unifying locality and surprisal effects.
Manuscript under review.
Gibson, Edward. 1998. Linguistic complexity:
locality of syntactic dependencies. Cognition
68:1–76.
Hale, John. 2001. A probabilistic Earley parser as
a psycholinguistic model. In Proceedings of the
2nd Conference of the North American Chapter
of the Association for Computational Linguis-
tics. Association for Computational Linguistics,
Pittsburgh, PA, volume 2, pages 159–166.
Hale, John. 2006. Uncertainty about the rest of the
sentence. Cognitive Science 30(4):609–642.
Harley, Trevor. 2001. The Psychology of Lan-
guage: From Data to Theory. Psychology
Press,, Hove, 2 edition.
Kamide, Yuki, Christoph Scheepers, and Gerry
T. M. Altmann. 2003. Integration of syntactic
and semantic information in predictive process-
ing: Cross-linguistic evidence from german and
english. Journal of Psycholinguistic Research
32:37–55.
Konieczny, Lars. 2000. Locality and parsing com-
plexity. Journal of Psycholinguistic Research
29(6):627–645.
Marslen-Wilson, William D. 1973. Linguistic
structure and speech shadowing at very short la-
tencies. Nature 244:522–523.
Mazzei, Alessandro, Vicenzo Lombardo, and
Patrick Sturt. 2007. Dynamic TAG and lexi-
cal dependencies. Research on Language and
Computation 5(3):309–332.
Staub, Adrian and Charles Clifton. 2006. Syntac-
tic prediction in language comprehension: Evi-
dence from either ... or. Journal of Experimen-
tal Psychology: Learning, Memory, and Cogni-
tion 32:425–436.
Sturt, Patrick and Vincenzo Lombardo. 2005.
Processing coordinated structures: Incremen-
tality and connectedness. Cognitive Science
29(2):291–305.
Tanenhaus, Michael K., Michael J. Spivey-
Knowlton, Kathleen M. Eberhard, and Julie C.
Sedivy. 1995. Integration of visual and linguis-
tic information in spoken language comprehen-
sion. Science 268:1632–1634.
van Berkum, J. J. A., C. M. Brown, and Peter Ha-
goort. 1999. Early referential context effects
in sentence processing: Evidence from event-
related brain potentials. Journal of Memory and
Language 41:147–182.
</reference>
<page confidence="0.999609">
46
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.814413">
<title confidence="0.997978">The Interaction of Syntactic Theory and Computational Psycholinguistics</title>
<author confidence="0.998894">Frank</author>
<affiliation confidence="0.9965">School of Informatics, University of</affiliation>
<address confidence="0.919816">10 Crichton Street, Edinburgh EH8 9AB,</address>
<email confidence="0.87712">keller@inf.ed.ac.uk</email>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Matthew W Crocker</author>
</authors>
<title>Computational Psycholinguistics: An Interdisciplinary Approach to the Study of Language.</title>
<date>1996</date>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<marker>Crocker, 1996</marker>
<rawString>Crocker, Matthew W. 1996. Computational Psycholinguistics: An Interdisciplinary Approach to the Study of Language. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vera Demberg</author>
<author>Frank Keller</author>
</authors>
<title>A psycholinguistically motivated version of TAG.</title>
<date>2008</date>
<booktitle>In Proceedings of the 9th International Workshop on Tree Adjoining Grammars and Related Formalisms. T¨ubingen.</booktitle>
<contexts>
<context position="4967" citStr="Demberg and Keller (2008)" startWordPosition="724" endWordPosition="727">tructure is available at any point during incremental sentence processing. Here, we explore how these key psycholinguistic concepts (incrementality, connectedness, and prediction) can be realized within a new version of tree-adjoining grammar, which we call Psycholinguistically Motivated TAG (PLTAG). We propose a formalization of PLTAG and a linking theory that derives predictions of processing difficulty from it. We then present an implementation of this model and evaluate it against key experimental data relating to incrementality and prediction. This approach is described in more detail in Demberg and Keller (2008) and Demberg and Keller (2009). 3 Modeling Explicit Prediction We propose a theory of sentence processing guided by the principles of incrementality, connectedness, and prediction. The core assumption of our proposal is that a sentence processor that maintains explicit predictions about the upcoming structure has to validate these predictions against the input it encounters. Using this assumption, we can naturally combine the forward-looking aspect of Hale’s (2001) surprisal (sentence structures are computed incrementally and unexpected continuations cause difficulty) with the backward-looking</context>
</contexts>
<marker>Demberg, Keller, 2008</marker>
<rawString>Demberg, Vera and Frank Keller. 2008. A psycholinguistically motivated version of TAG. In Proceedings of the 9th International Workshop on Tree Adjoining Grammars and Related Formalisms. T¨ubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vera Demberg</author>
<author>Frank Keller</author>
</authors>
<title>A computational model of prediction in human parsing: Unifying locality and surprisal effects. Manuscript under review.</title>
<date>2009</date>
<contexts>
<context position="4997" citStr="Demberg and Keller (2009)" startWordPosition="729" endWordPosition="732">oint during incremental sentence processing. Here, we explore how these key psycholinguistic concepts (incrementality, connectedness, and prediction) can be realized within a new version of tree-adjoining grammar, which we call Psycholinguistically Motivated TAG (PLTAG). We propose a formalization of PLTAG and a linking theory that derives predictions of processing difficulty from it. We then present an implementation of this model and evaluate it against key experimental data relating to incrementality and prediction. This approach is described in more detail in Demberg and Keller (2008) and Demberg and Keller (2009). 3 Modeling Explicit Prediction We propose a theory of sentence processing guided by the principles of incrementality, connectedness, and prediction. The core assumption of our proposal is that a sentence processor that maintains explicit predictions about the upcoming structure has to validate these predictions against the input it encounters. Using this assumption, we can naturally combine the forward-looking aspect of Hale’s (2001) surprisal (sentence structures are computed incrementally and unexpected continuations cause difficulty) with the backward-looking integration view of Gibson’s </context>
<context position="11653" citStr="Demberg and Keller (2009)" startWordPosition="1804" endWordPosition="1807">ty has a second component, the cost of verifying earlier predictions, which is subject to decay. The verification cost component bears similarities to DLT integration costs, but we do not calculate distance in terms of number of discourse referents intervening between a dependent and its head. Rather verification cost is determined by the number of words intervening between a prediction and its verification, subject to decay. This captures the intuition that a prediction becomes less and less useful the longer ago it was made, as it decays from memory with increasing distance. 6 Evaluation In Demberg and Keller (2009), we present an implementation of the PLTAG model, including a lexicon induction procedure, a parsing algorithm, and a probability model. We show that the resulting framework can capture experimental results from the literature, and can explain both locality and prediction effects, which standard models of sentence processing like DLT and surprisal are unable to account for simultaneously. Our model therefore constitutes a step towards a unified theory of human parsing that potentially captures a broad range of experimental findings. This work also demonstrates that (computational) psycholingu</context>
</contexts>
<marker>Demberg, Keller, 2009</marker>
<rawString>Demberg, Vera and Frank Keller. 2009. A computational model of prediction in human parsing: Unifying locality and surprisal effects. Manuscript under review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Gibson</author>
</authors>
<title>Linguistic complexity: locality of syntactic dependencies.</title>
<date>1998</date>
<journal>Cognition</journal>
<pages>68--1</pages>
<marker>Gibson, 1998</marker>
<rawString>Gibson, Edward. 1998. Linguistic complexity: locality of syntactic dependencies. Cognition 68:1–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hale</author>
</authors>
<title>A probabilistic Earley parser as a psycholinguistic model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics,</booktitle>
<volume>2</volume>
<pages>159--166</pages>
<location>Pittsburgh, PA,</location>
<marker>Hale, 2001</marker>
<rawString>Hale, John. 2001. A probabilistic Earley parser as a psycholinguistic model. In Proceedings of the 2nd Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, Pittsburgh, PA, volume 2, pages 159–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hale</author>
</authors>
<title>Uncertainty about the rest of the sentence.</title>
<date>2006</date>
<journal>Cognitive Science</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="1683" citStr="Hale (2006)" startWordPosition="231" endWordPosition="232"> of experimental psycholinguistics, a field in which most of the work is representationally agnostic, i.e., assumptions about syntactic structure left implicit, or limited to uncontroversial ones. However, the situation is different in computational psycholinguistics, where researchers built computationally implemented models of human language processing. This typically involves making one’s theoretical assumptions explicit, a prerequisite for being able to implement them. For example, Crocker’s (1996) model explicitly implements assumptions from the Principles and Parameters framework, while Hale (2006) uses probabilistic Minimalist grammars, or Mazzei et al. (2007) tree-adjoining grammars. Here, we will investigate how evidence regarding human sentence processing can inform our assumptions about syntactic structure, at least in so far as this structure is used in computational models of human parsing. 2 Prediction and Incrementality Evidence from psycholinguistic research suggests that language comprehension is largely incremental, i.e., that comprehenders build an interpretation of a sentence on a word-by-word basis. Evidence for incrementality comes from speech shadowing, self-paced readi</context>
</contexts>
<marker>Hale, 2006</marker>
<rawString>Hale, John. 2006. Uncertainty about the rest of the sentence. Cognitive Science 30(4):609–642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Harley</author>
</authors>
<title>The Psychology of Language: From Data to Theory.</title>
<date>2001</date>
<volume>2</volume>
<pages>edition.</pages>
<publisher>Psychology Press,,</publisher>
<location>Hove,</location>
<contexts>
<context position="647" citStr="Harley, 2001" startWordPosition="83" endWordPosition="84"> and Computational Psycholinguistics Frank Keller School of Informatics, University of Edinburgh 10 Crichton Street, Edinburgh EH8 9AB, UK keller@inf.ed.ac.uk 1 Introduction Typically, current research in psycholinguistics does not rely heavily on results from theoretical linguistics. In particular, most experimental work studying human sentence processing makes very straightforward assumptions about sentence structure; essentially only a simple context-free grammar is assumed. The main text book in psycholinguistics, for instance, mentions Minimalism in its chapter on linguistic description (Harley, 2001, ch. 2), but does not provide any details, and all the examples in this chapter, as well as in the chapters on sentence processing and language production (Harley, 2001, chs. 9, 12), only use context-free syntactic structures with uncontroversial phrase markers (S, VP, NP, etc.). The one exception is traces, which the textbook discusses in the context of syntactic ambiguity resolution. Harley’s (2001) textbook is typical of experimental psycholinguistics, a field in which most of the work is representationally agnostic, i.e., assumptions about syntactic structure left implicit, or limited to </context>
</contexts>
<marker>Harley, 2001</marker>
<rawString>Harley, Trevor. 2001. The Psychology of Language: From Data to Theory. Psychology Press,, Hove, 2 edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuki Kamide</author>
<author>Christoph Scheepers</author>
<author>Gerry T M Altmann</author>
</authors>
<title>Integration of syntactic and semantic information in predictive processing: Cross-linguistic evidence from german and english.</title>
<date>2003</date>
<journal>Journal of Psycholinguistic Research</journal>
<pages>32--37</pages>
<contexts>
<context position="3290" citStr="Kamide et al., 2003" startWordPosition="468" endWordPosition="471">ip to the preceding context. There is also evidence for full connectivity in human language processing (Sturt and Lombardo, 2005). Full connectivity means that all words are connected by a single syntactic structure; the parser builds no unconnected tree fragments, even for the incomplete sentences (sentence prefixes) that arise during incremental processing. Furthermore, there is evidence that readers or listeners make predictions about upcoming material on the basis of sentence prefixes. Listeners can predict an upcoming post-verbal element, depending on the semantics of the preceding verb (Kamide et al., 2003). Prediction effects can also be observed in reading. Staub and Clifton (2006) showed that following the word either readers predict the conjunction or and the complement that follows it; processing was facilitated compared to structures that include or without either. In an ERP study, van Berkum et al. (1999) found that listeners use contextual information to predict specific lexical items and experience processing difficulty if the input is incompatible with the prediction. The concepts of incrementality, connectedness, and prediction are closely related: in order to guarProceedings of the E</context>
</contexts>
<marker>Kamide, Scheepers, Altmann, 2003</marker>
<rawString>Kamide, Yuki, Christoph Scheepers, and Gerry T. M. Altmann. 2003. Integration of syntactic and semantic information in predictive processing: Cross-linguistic evidence from german and english. Journal of Psycholinguistic Research 32:37–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lars Konieczny</author>
</authors>
<title>Locality and parsing complexity.</title>
<date>2000</date>
<journal>Journal of Psycholinguistic Research</journal>
<volume>29</volume>
<issue>6</issue>
<contexts>
<context position="2350" citStr="Konieczny, 2000" startWordPosition="324" endWordPosition="325">t al. (2007) tree-adjoining grammars. Here, we will investigate how evidence regarding human sentence processing can inform our assumptions about syntactic structure, at least in so far as this structure is used in computational models of human parsing. 2 Prediction and Incrementality Evidence from psycholinguistic research suggests that language comprehension is largely incremental, i.e., that comprehenders build an interpretation of a sentence on a word-by-word basis. Evidence for incrementality comes from speech shadowing, self-paced reading, and eye-tracking studies (Marslen-Wilson, 1973; Konieczny, 2000; Tanenhaus et al., 1995): as soon as a reader or listener perceives a word in a sentence, they integrate it as fully as possible into a representation of the sentence thus far. They experience differential processing difficulty during this integration process, depending on the properties of the word and its relationship to the preceding context. There is also evidence for full connectivity in human language processing (Sturt and Lombardo, 2005). Full connectivity means that all words are connected by a single syntactic structure; the parser builds no unconnected tree fragments, even for the i</context>
</contexts>
<marker>Konieczny, 2000</marker>
<rawString>Konieczny, Lars. 2000. Locality and parsing complexity. Journal of Psycholinguistic Research 29(6):627–645.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William D Marslen-Wilson</author>
</authors>
<title>Linguistic structure and speech shadowing at very short latencies.</title>
<date>1973</date>
<journal>Nature</journal>
<pages>244--522</pages>
<contexts>
<context position="2333" citStr="Marslen-Wilson, 1973" startWordPosition="322" endWordPosition="323"> grammars, or Mazzei et al. (2007) tree-adjoining grammars. Here, we will investigate how evidence regarding human sentence processing can inform our assumptions about syntactic structure, at least in so far as this structure is used in computational models of human parsing. 2 Prediction and Incrementality Evidence from psycholinguistic research suggests that language comprehension is largely incremental, i.e., that comprehenders build an interpretation of a sentence on a word-by-word basis. Evidence for incrementality comes from speech shadowing, self-paced reading, and eye-tracking studies (Marslen-Wilson, 1973; Konieczny, 2000; Tanenhaus et al., 1995): as soon as a reader or listener perceives a word in a sentence, they integrate it as fully as possible into a representation of the sentence thus far. They experience differential processing difficulty during this integration process, depending on the properties of the word and its relationship to the preceding context. There is also evidence for full connectivity in human language processing (Sturt and Lombardo, 2005). Full connectivity means that all words are connected by a single syntactic structure; the parser builds no unconnected tree fragment</context>
</contexts>
<marker>Marslen-Wilson, 1973</marker>
<rawString>Marslen-Wilson, William D. 1973. Linguistic structure and speech shadowing at very short latencies. Nature 244:522–523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Mazzei</author>
<author>Vicenzo Lombardo</author>
<author>Patrick Sturt</author>
</authors>
<title>Dynamic TAG and lexical dependencies.</title>
<date>2007</date>
<journal>Research on Language and Computation</journal>
<volume>5</volume>
<issue>3</issue>
<contexts>
<context position="1747" citStr="Mazzei et al. (2007)" startWordPosition="238" endWordPosition="241">st of the work is representationally agnostic, i.e., assumptions about syntactic structure left implicit, or limited to uncontroversial ones. However, the situation is different in computational psycholinguistics, where researchers built computationally implemented models of human language processing. This typically involves making one’s theoretical assumptions explicit, a prerequisite for being able to implement them. For example, Crocker’s (1996) model explicitly implements assumptions from the Principles and Parameters framework, while Hale (2006) uses probabilistic Minimalist grammars, or Mazzei et al. (2007) tree-adjoining grammars. Here, we will investigate how evidence regarding human sentence processing can inform our assumptions about syntactic structure, at least in so far as this structure is used in computational models of human parsing. 2 Prediction and Incrementality Evidence from psycholinguistic research suggests that language comprehension is largely incremental, i.e., that comprehenders build an interpretation of a sentence on a word-by-word basis. Evidence for incrementality comes from speech shadowing, self-paced reading, and eye-tracking studies (Marslen-Wilson, 1973; Konieczny, 2</context>
</contexts>
<marker>Mazzei, Lombardo, Sturt, 2007</marker>
<rawString>Mazzei, Alessandro, Vicenzo Lombardo, and Patrick Sturt. 2007. Dynamic TAG and lexical dependencies. Research on Language and Computation 5(3):309–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adrian Staub</author>
<author>Charles Clifton</author>
</authors>
<title>Syntactic prediction in language comprehension: Evidence from either ...</title>
<date>2006</date>
<journal>or. Journal of Experimental Psychology: Learning, Memory, and Cognition</journal>
<pages>32--425</pages>
<contexts>
<context position="3368" citStr="Staub and Clifton (2006)" startWordPosition="480" endWordPosition="483">in human language processing (Sturt and Lombardo, 2005). Full connectivity means that all words are connected by a single syntactic structure; the parser builds no unconnected tree fragments, even for the incomplete sentences (sentence prefixes) that arise during incremental processing. Furthermore, there is evidence that readers or listeners make predictions about upcoming material on the basis of sentence prefixes. Listeners can predict an upcoming post-verbal element, depending on the semantics of the preceding verb (Kamide et al., 2003). Prediction effects can also be observed in reading. Staub and Clifton (2006) showed that following the word either readers predict the conjunction or and the complement that follows it; processing was facilitated compared to structures that include or without either. In an ERP study, van Berkum et al. (1999) found that listeners use contextual information to predict specific lexical items and experience processing difficulty if the input is incompatible with the prediction. The concepts of incrementality, connectedness, and prediction are closely related: in order to guarProceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Lin</context>
</contexts>
<marker>Staub, Clifton, 2006</marker>
<rawString>Staub, Adrian and Charles Clifton. 2006. Syntactic prediction in language comprehension: Evidence from either ... or. Journal of Experimental Psychology: Learning, Memory, and Cognition 32:425–436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Sturt</author>
<author>Vincenzo Lombardo</author>
</authors>
<title>Processing coordinated structures: Incrementality and connectedness.</title>
<date>2005</date>
<journal>Cognitive Science</journal>
<volume>29</volume>
<issue>2</issue>
<contexts>
<context position="2799" citStr="Sturt and Lombardo, 2005" startWordPosition="396" endWordPosition="399">a sentence on a word-by-word basis. Evidence for incrementality comes from speech shadowing, self-paced reading, and eye-tracking studies (Marslen-Wilson, 1973; Konieczny, 2000; Tanenhaus et al., 1995): as soon as a reader or listener perceives a word in a sentence, they integrate it as fully as possible into a representation of the sentence thus far. They experience differential processing difficulty during this integration process, depending on the properties of the word and its relationship to the preceding context. There is also evidence for full connectivity in human language processing (Sturt and Lombardo, 2005). Full connectivity means that all words are connected by a single syntactic structure; the parser builds no unconnected tree fragments, even for the incomplete sentences (sentence prefixes) that arise during incremental processing. Furthermore, there is evidence that readers or listeners make predictions about upcoming material on the basis of sentence prefixes. Listeners can predict an upcoming post-verbal element, depending on the semantics of the preceding verb (Kamide et al., 2003). Prediction effects can also be observed in reading. Staub and Clifton (2006) showed that following the word</context>
</contexts>
<marker>Sturt, Lombardo, 2005</marker>
<rawString>Sturt, Patrick and Vincenzo Lombardo. 2005. Processing coordinated structures: Incrementality and connectedness. Cognitive Science 29(2):291–305.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Michael K Tanenhaus</author>
<author>Michael J SpiveyKnowlton</author>
<author>Kathleen M Eberhard</author>
<author>C Julie</author>
</authors>
<marker>Tanenhaus, SpiveyKnowlton, Eberhard, Julie, </marker>
<rawString>Tanenhaus, Michael K., Michael J. SpiveyKnowlton, Kathleen M. Eberhard, and Julie C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sedivy</author>
</authors>
<title>Integration of visual and linguistic information in spoken language comprehension.</title>
<date>1995</date>
<journal>Science</journal>
<pages>268--1632</pages>
<marker>Sedivy, 1995</marker>
<rawString>Sedivy. 1995. Integration of visual and linguistic information in spoken language comprehension. Science 268:1632–1634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J A van Berkum</author>
<author>C M Brown</author>
<author>Peter Hagoort</author>
</authors>
<title>Early referential context effects in sentence processing: Evidence from eventrelated brain potentials.</title>
<date>1999</date>
<journal>Journal of Memory and Language</journal>
<pages>41--147</pages>
<marker>van Berkum, Brown, Hagoort, 1999</marker>
<rawString>van Berkum, J. J. A., C. M. Brown, and Peter Hagoort. 1999. Early referential context effects in sentence processing: Evidence from eventrelated brain potentials. Journal of Memory and Language 41:147–182.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>