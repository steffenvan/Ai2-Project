<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000155">
<title confidence="0.869877666666667">
HYENA-live: Fine-Grained Online Entity Type Classification from
Natural-language Text
Mohamed Amir Yosefi Sandro Bauer&apos; Johannes Hoffarti
</title>
<author confidence="0.6750585">
Marc Spanioli Gerhard Weikumi
(1) Max-Planck-Institut f¨ur Informatik, Saarbr¨ucken, Germany
</author>
<affiliation confidence="0.798939">
(2) Computer Laboratory, University of Cambridge, UK
</affiliation>
<email confidence="0.6908745">
{mamir|jhoffart|mspaniol|weikum}@mpi-inf.mpg.de
sandro.bauer@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.997293" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999805">
Recent research has shown progress in
achieving high-quality, very fine-grained
type classification in hierarchical tax-
onomies. Within such a multi-level type
hierarchy with several hundreds of types at
different levels, many entities naturally be-
long to multiple types. In order to achieve
high-precision in type classification, cur-
rent approaches are either limited to certain
domains or require time consuming multi-
stage computations. As a consequence, ex-
isting systems are incapable of performing
ad-hoc type classification on arbitrary input
texts. In this demo, we present a novel Web-
based tool that is able to perform domain
independent entity type classification under
real time conditions. Thanks to its efficient
implementation and compacted feature rep-
resentation, the system is able to process
text inputs on-the-fly while still achieving
equally high precision as leading state-of-
the-art implementations. Our system offers
an online interface where natural-language
text can be inserted, which returns seman-
tic type labels for entity mentions. Further
more, the user interface allows users to ex-
plore the assigned types by visualizing and
navigating along the type-hierarchy.
</bodyText>
<sectionHeader confidence="0.999626" genericHeader="keywords">
1 Introduction
</sectionHeader>
<subsectionHeader confidence="0.663108">
Motivation
</subsectionHeader>
<bodyText confidence="0.999260777777778">
Web contents such as news, blogs and other so-
cial media are full of named entities. Each en-
tity belongs to one or more semantic types as-
sociated with it. For instance, an entity such as
Bob Dylan should be assigned the types Singer,
Musician, Poet, etc., and also the correspond-
ing supertype(s) (hypernyms) in a type hierarchy,
in this case Person. Such fine-grained typing of
entities in texts can be a great asset for various
NLP tasks including semantic role labeling, sense
disambiguation and named entity disambiguation
(NED). For instance, noun phrases such as “song-
writer Dylan”, “Google founder Page”, or “rock
legend Page” can be easily mapped to the entities
Bob Dylan, Larry Page, and Jimmy Page if their re-
spective types Singer, BusinessPerson, and
Guitarist are available (cf. Figure 1 for an il-
lustrative example).
</bodyText>
<figureCaption confidence="0.998904">
Figure 1: Fine-grained entity type classification
</figureCaption>
<subsectionHeader confidence="0.885872">
Problem Statement
</subsectionHeader>
<bodyText confidence="0.99941075">
Type classification is not only be based on hier-
archical sub-type relationships (e.g. Musician
isA Person), but also has to do on multi-labeling.
Within a very fine-grained type hierarchy, many en-
tities naturally belong to multiple types. For exam-
ple, a guitarist is also a musician and a person, but
may also be a singer, an actor, or even a politician.
Consequently, entities should not only be assigned
the most (fine-grained) label associated to them,
but with all labels relevant to them. So we face
a hierarchical multi-label classification problem
(Tsoumakas et al., 2012).
</bodyText>
<sectionHeader confidence="0.511179" genericHeader="introduction">
Contribution
</sectionHeader>
<bodyText confidence="0.9997428">
This paper introduces HYENA-live, which allows
an on-the-fly computation of semantic types for en-
tity mentions, based on a multi-level type hierarchy.
Our approach uses a suite of features for a given
entity mention, such as neighboring words and bi-
</bodyText>
<figure confidence="0.9769515">
&amp;quot;Funded with 100,000$, Google was founded by Brin and Page&amp;quot;
Business_people
Entrepreneur
Entertainer
Musician
&amp;quot;Pageplayed on his first guitar in 1952 &amp;quot;
</figure>
<page confidence="0.981483">
133
</page>
<bodyText confidence="0.82466775">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 133–138,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
grams, part-of-speech tags, and also phrases from a
large gazetteer derived from state-of-the-art knowl-
edge bases. In order to perform “live” entity type
classification based on ad-hoc text inputs, several
performance optimizations have been undertaken
to operate under real-time conditions.
</bodyText>
<sectionHeader confidence="0.959876" genericHeader="method">
2 Entity Type Classification Systems
</sectionHeader>
<bodyText confidence="0.999896541666667">
State-of-the-art tools for named entity recognition
such as the Stanford NER Tagger (Finkel et al.,
2005) compute semantic tags only for a small set of
coarse-grained types: Person, Location, and
Organization (plus tags for non-entity phrases
of type time, money, percent, and date). However,
we are not aware of any online tool that performs
fine-grained typing of entity mentions. The most
common workaround to perform entity classifica-
tion is a two-stage process: in first applying an on-
line tool for Named-Entity Disambiguation (NED),
such as DBpedia Spotlight (Mendes et al., 2011)
or AIDA (Yosef et al., 2011; Hoffart et al., 2011),
in order to map the mentions onto canonical enti-
ties and subsequently query the knowledge base for
their types. In fact, (Ling and Weld, 2012) followed
this approach when comparing their entity classi-
fication system results against those obtained by
an adoption of the Illinois’ Named-Entity Linking
system (NEL) (Ratinov et al., 2011) and reached
the conclusion that while NEL performed decently
for prominent entities, it could not scale to cover
long tail ones. Specifically, entity typing via NED
has three major drawbacks:
</bodyText>
<listItem confidence="0.9914822">
1. NED is an inherently hard problem, especially
with highly ambiguous mentions. As a conse-
quence, accurate NED systems come at a high
computation costs.
2. NED only works for those mentions that cor-
respond to a canonical entity within a knowl-
edge base. However, this fails for all out-of-
knowledge-base entities like unregistered per-
sons, start-up companies, etc.
3. NED heavily depends on the quality of the un-
derlying knowledge base. Yet, only very few
knowledge bases have comprehensive class
labeling of entities. Even more, in the best
case, coverage drops sharply for relatively un-
common entities.
</listItem>
<bodyText confidence="0.999432823529412">
We decided to adopt one of the existing ap-
proaches to make it suitable for online querying.
We considered five systems. In the rest of this
section we will briefly describe each of them.
(Fleischman and Hovy, 2002) is one of the earli-
est approaches to perform entity classification into
subtypes of PERSON. They developed a decision-
tree classifier based on contextual features that can
be automatically extracted from the text. In order
to account for scarcity of labeled training data, they
tapped on WordNet synonyms to achieve higher
coverage. While their approach is fundamentally
suitable, their type system is very restricted. In or-
der to account for more fine-grained classes, more
features need to be added to their feature set.
(Ekbal et al., 2010) considered 141 subtypes of
WordNet class PERSON and developed a maximum
entropy classifier exploiting the words surrounding
the mentions together with their POS tags and other
contextual features. Their type hierarchy is fine-
grained, but still limited to sub classes of PERSON.
In addition, their experimental results have been
flagged as non-reproducible in the ACL Anthology.
(Altaf ur Rahman and Ng, 2010) considered a
two-level type hierarchy consisting of 29 top-level
classes and a total of 92 sub-classes. These include
many non-entity types such as date, time, percent,
money, quantity, ordinal, cardinal, etc. They in-
corporated a hierarchical classifier using a rich fea-
ture set and made use of WordNet sense tagging.
However, the latter requires human interception,
which is not suitable for ad-hoc processing of out-
of-domain texts.
(Ling and Weld, 2012) developed FIGER,
which classifies entity mentions onto a two-level
taxonomy based on the Freebase knowledge base
(Bollacker et al., 2008). This results in a two-level
hierarchy with top-level topics and 112 types. They
trained a CRF for the joint task of recognizing en-
tity mentions and inferring type tags. Although
they handle multi-label assignment, their test data
is sparse. Many classes are absent and plenty of
instances come with only a single label (e.g. 216
of the 562 entities were of type PERSON without
subtypes). Further, their results are instance based,
which does not guarantee that the quality of their
system will be reproducible for all the 112 types in
their taxonomy.
(Yosef et al., 2012) is the most recent work in
multi-label type classification. The HYENA sys-
tem incorporates a large hierarchy of 505 classes
</bodyText>
<page confidence="0.993397">
134
</page>
<bodyText confidence="0.999920523809524">
organized under 5 top level classes, with 100 de-
scendant classes under each of them. The hierarchy
reaches a depth of up to 9 levels in some parts.
The system is based on an SVM classifier using a
comprehensive set of features and provides results
for all classes of a large data set. In their exper-
iments the superiority of the system in terms of
precision and recall has been shown. However, the
main drawback of HYENA comes from its large
hierarchy and the extensive set of features extracted
from the fairly large training corpus it requires. As
a result, on-the-fly type classification with HYENA
is impossible in its current implementation.
We decided to build on top of HYENA sys-
tem by spotting the bottlenecks in the architec-
ture and modifying it accordingly to be suitable
for online querying. In Section 3 we explain in
details HYENA’s type taxonomy and their feature
portfolio. Later on, we explain the engineering
undertaken in order to develop the on-the-fly type
classification system HYENA-live (cf. Section 4).
</bodyText>
<sectionHeader confidence="0.87933" genericHeader="method">
3 Type Hierarchy and Feature Set
</sectionHeader>
<subsectionHeader confidence="0.973381">
3.1 Fine-grained Taxonomy
</subsectionHeader>
<bodyText confidence="0.999953272727273">
The type system is an automatically gathered fine-
grained taxonomy of 505 classes. The classes are
organized under 5 top level classes, with 100 de-
scendant classes under each. The YAGO knowl-
edge base (Hoffart et al., 2013) is selected to de-
rive the taxonomy from because of its highly pre-
cise classification of entities into WordNet classes,
which is a result of the accurate mapping YAGO
has from Wikipedia Categories to WordNet synsets.
We start with five top classes namely PERSON,
LOCATION, ORGANIZATION, EVENT and
ARTIFACT. Under each top class, the most 100
prominent descendant classes are picked. Promi-
nence is estimated by the number of YAGO entities
tagged with this class. This results in a very-fine
grained taxonomy of 505 types, represented as a
directed acyclic graph with 9 levels in its deepest
parts. While the classes are picked from the YAGO
type system, the approach is generic and can be
applied to derive type taxonomies from other
knowledge bases such as Freebase or DBpedia
(Auer et al., 2007) as in (Ling and Weld, 2012).
</bodyText>
<subsectionHeader confidence="0.998175">
3.2 Feature Set
</subsectionHeader>
<bodyText confidence="0.999474833333333">
For the sake of generality and applicability to ar-
bitrary text, we opted for features that can be au-
tomatically extracted from the input text without
any human interaction, or manual annotation. The
extracted features fall under five categories, which
we briefly explain in the rest of this section.
</bodyText>
<subsectionHeader confidence="0.93884">
Mention String
</subsectionHeader>
<bodyText confidence="0.9998722">
We derive four features from the entity mention
string. The mention string itself, a noun phrase
consisting of one or more consecutive words. The
other three features are unigrams, bigrams, and
trigrams that overlap with the mention string.
</bodyText>
<subsectionHeader confidence="0.892457">
Sentence Surrounding Mention
</subsectionHeader>
<bodyText confidence="0.999997916666667">
We also exploit a bounded-size window around the
mention to extract four features: all unigrams, bi-
grams, and trigrams. Two versions of those features
are extracted, one to account for the occurrence of
those tokens around the mention, and another to ac-
count for the position at which they occurred with
respect to the mention (before or after). In addition,
unigrams are also included with their absolute dis-
tance ignoring whether before of after the mention.
Our demo is using a conservative threshold for the
size of the window which is three tokens on each
side of the mention.
</bodyText>
<subsectionHeader confidence="0.882829">
Mention Paragraph
</subsectionHeader>
<bodyText confidence="0.999981444444444">
We also leverage the entire paragraph of the men-
tion. This gives additional topical cues about the
mention type (e.g., if the paragraph is about a mu-
sic concert, this is a cue for mapping people names
to musician types). We create three features here:
unigrams, bigrams, and trigrams without including
any distance information. In our demo, we extract
those features from a bounded window of size 2000
characters before and after the mention.
</bodyText>
<subsectionHeader confidence="0.700669">
Grammatical Features
</subsectionHeader>
<bodyText confidence="0.9999801">
We exploit the semantics of the text by extracting
four features. First, we use part-of-speech tags of
the tokens in a size-bounded window around the
mention in distance and absolute distance versions.
Second and third, we create a feature for the first
occurrence of a “he” or “she” pronoun in the same
sentence and in the subsequent sentence following
the mention, along with the distance to the mention.
Finally, we use the closest verb-preposition pair
preceding the mention as another feature.
</bodyText>
<sectionHeader confidence="0.50757" genericHeader="method">
Gazetteer Features
</sectionHeader>
<bodyText confidence="0.9996635">
We leverage YAGO2 knowledge base even further
by building a type-specific gazetteer of words oc-
</bodyText>
<page confidence="0.996761">
135
</page>
<table confidence="0.997852363636364">
# of articles 50,000
# of instances (all types) 1,613,340
# of location instances 489,003 (30%)
# of person instances 426,467 (26.4%)
# of organization instances 219,716 (13.6%)
# of artifact instances 204,802 (12.7%)
# of event instances 176,549 (10.9%)
# instances in 1 top-level class 1,131,994 (70.2%)
# instances in 2 top-level classes 182,508 (11.3%)
# instances in more than 2 top-level classes 6,492 (0.4%)
# instances not in any class 292,346 (18.1%)
</table>
<tableCaption confidence="0.999889">
Table 1: Properties of the labeled data used for training HYENA-live
</tableCaption>
<bodyText confidence="0.999806285714286">
curring in the names of the entities of that type.
YAGO2 knowledge base comes with an exten-
sive dictionary of name-entity pairs extracted from
Wikipedia redirects and link-anchor texts. We con-
struct, for each type, a binary feature that indicates
if the mention contains a word occurring in this
type’s gazetteer. Note that this is a fully automated
feature construction, and it does by no means de-
termine the mention type(s) already, as most words
occur in the gazetteers of many different types. For
example, “Alice” occurs in virtually every subclass
of Person but also in city names like “Alice Springs”
and other locations, as well as in songs, movies,
and other products or organizations.
</bodyText>
<sectionHeader confidence="0.996886" genericHeader="method">
4 System Implementation
</sectionHeader>
<subsectionHeader confidence="0.892669">
4.1 Overview
</subsectionHeader>
<bodyText confidence="0.999995037037037">
As described in Section 3, HYENA classifies men-
tions of named entities onto a hierarchy of 505
types using large set of features. A random sub-
set of the English Wikipedia has been used for
training HYENA. By exploiting Wikipedia anchor
links, mentions of named entities are automati-
cally disambiguated to their correct entities. Each
Wikipedia named entity has a corresponding YAGO
entity labeled with an accurate set of types, and
hence we effortlessly obtain a huge training data
set (cf. data properties in Table 1).
We build type-specific classifiers using the SVM
software LIBLINEAR (cf. http://liblinear.
bwaldvogel.de/). Each model comes with a com-
prehensive feature set. While larger models (with
more features) improve the accuracy, they signifi-
cantly affect the applicability of the system. A sin-
gle model file occupies around 150MB disk space
leading to a total of 84.7GB for all models. As
a consequence, there is a substantial setup time
to load all models in memory and a high-memory
server (48 cores with 512GB of RAM) is required
for computation. An analysis showed that each sin-
gle feature contributes to the overall performance
of HYENA, but only a tiny subset of all features is
relevant for a single classifier. Therefore, most of
the models are extremely sparse.
</bodyText>
<subsectionHeader confidence="0.992146">
4.2 Sparse Models Representation
</subsectionHeader>
<bodyText confidence="0.999925818181818">
There are several workarounds applicable to batch
mode operations, e.g. by performing classifications
per level only. However, this is not an option for
on-the-fly computations. For that reason we opted
for a sparse-model representation.
LIBLINEAR model files are normalized textual
files: a header (data about the model and the to-
tal number of features), followed by listing the
weights assigned to each feature (line number in-
dicates the feature ID). Each model file has been
post-processed to produce 2 files:
</bodyText>
<listItem confidence="0.9862582">
• A compacted model file containing only fea-
tures of non-zero weights. Its header reflects
the reduced number of features.
• A meta-data file. It maps the new features IDs
to the original feature IDs.
</listItem>
<bodyText confidence="0.9997668">
Due to the observed sparsity in the model files,
particularly at deeper levels, there is a significant
decrease in disk space consumption for the com-
pacted model files and hence in the memory re-
quirements.
</bodyText>
<subsectionHeader confidence="0.998535">
4.3 Sparse Models Classification
</subsectionHeader>
<bodyText confidence="0.99993425">
By switching to the sparse model representation the
architecture of the whole system is affected. In par-
ticular, modified versions of feature vectors need
to be generated for each classifier; this is because
</bodyText>
<page confidence="0.998129">
136
</page>
<figureCaption confidence="0.997769">
Figure 2: Modified system architecture designed for handling sparse models
</figureCaption>
<figure confidence="0.997841176470588">
Input
Text
Feature
Extractor
Classification
Models
Sparse Models
Meta-Data
Sparse Model
Representation
Model-Specific
Feature Vector
Feature Vector
Post
Processing
Classifier
Decision
</figure>
<bodyText confidence="0.9968274">
a lot of features have been omitted from specific
classifiers (those with zero weights). Consequently,
the feature IDs need to be mapped to the new fea-
ture space of each classifier. The conceptual design
of the new architecture is illustrated in Figure 4.2.
</bodyText>
<sectionHeader confidence="0.998843" genericHeader="method">
5 Demo Presentation
</sectionHeader>
<bodyText confidence="0.998427103448276">
HYENA-live has been fully implemented as a Web
application. Figure 5 shows the user interface of
HYENA-live in a Web browser:
1) On top, there is a panel where a user can input
any text, e.g. by copy-and-paste from news ar-
ticles. We employ the Stanford NER Tagger to
identify noun phrases as candidates of entity
mentions. Alternatively, users can flag entity
mentions by double brackets (e.g. “Harry is
the opponent of [[you know who]]”). For the
sake of simplicity, detected entity mentions by
HYENA-live are highlighted in yellow. Each
mention is clickable to study its type classifi-
cation results.
2) The output of type classification is shown in-
side a tabbed widget. Each tab corresponds
to a detected mention by the system and tabs
are sorted by the order of occurrence in the
input text. To open a tab, the tab header or the
corresponding mention in the input area needs
to be clicked.
3) The type classification of a mention is shown
as a color-coded interactive tree. While the
original type hierarchy is a directed acyclic
graph, for the ease of navigation the classifi-
cation output has been converted into a tree.
In order to do so, nodes that belong to more
than a parent have been duplicated. There are
three different types of nodes:
</bodyText>
<listItem confidence="0.787728">
• Green Nodes: referring to a class that has
been accepted by the classifier. These
nodes can be further expanded in order
to check which sub-classes have been
accepted or rejected by HYENA-live.
• Red Nodes: corresponding to a class that
was rejected by the classifier, and hence
HYENA-live did not traverse deeper to
test its sub-classes.
• White Nodes: matching classes that have
not been tested. These nodes are either
known upfront (e.g. ENTITY) or their
super class was rejected by the system.
</listItem>
<bodyText confidence="0.999842">
It is worth noting that HYENA-live automati-
cally adjusts the layouting so that as much as
possible of the hierarchy is shown to the user.
For the sake of explorability, this is being dy-
namically adjusted once the user decides to
navigate along a certain (child-)node.
The system is available online at:
d5gate.ag5.mpi-sb.mpg.de/webhyena/.
The data transfer between the client and the server
is done via JSON objects. Hence, we also provide
HYENA-live as a JSON compliant entity classi-
fication Web-service. As a result, the back-end
becomes easily interchangeable (e.g. by a different
classification technique or a different type taxon-
omy) with minimum modifications required on the
user interface side.
</bodyText>
<sectionHeader confidence="0.999076" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.4946175">
This work is supported by the 7th Framework IST programme
of the European Union through the focused research project
(STREP) on Longitudinal Analytics of Web Archive data
(LAWA) under contract no. 258105.
</bodyText>
<page confidence="0.996215">
137
</page>
<figureCaption confidence="0.995194">
Figure 3: Interactively exploring the types of the “Battle of Waterloo” in the HYENA-live interface
</figureCaption>
<sectionHeader confidence="0.998255" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999912964285714">
Md. Altaf ur Rahman and Vincent Ng. 2010. Inducing
fine-grained semantic classes via hierarchical and
collective classification. In COLING, pages 931–
939.
S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, and Zachary Ives. 2007. Dbpedia: A nu-
cleus for a web of open data. In ISWC, pages 11–15.
Springer.
Kurt D. Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In SIGMOD, pages 1247–1250.
Asif Ekbal, Eva Sourjikova, Anette Frank, and Si-
mone P. Ponzetto. 2010. Assessing the challenge of
fine-grained named entity recognition and classifica-
tion. In Named Entities Workshop, pages 93–101.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In ACL, pages 363–370.
Michael Fleischman and Eduard Hovy. 2002. Fine
grained classification of named entities. In COLING,
pages 1–7.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen F¨urstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust disambiguation of named
entities in text. In EMNLP, pages 782–792.
Johannes Hoffart, Fabian M. Suchanek, Klaus
Berberich, and Gerhard Weikum. 2013. YAGO2: A
spatially and temporally enhanced knowledge base
from wikipedia. Artificial Intelligence, 194(0):28 –
61.
Xiao Ling and Daniel S. Weld. 2012. Fine-grained
entity recognition. In AAAI, pages 94–100.
Pablo N. Mendes, Max Jakob, Andr´es Garc´ıa-Silva,
and Christian Bizer. 2011. Dbpedia spotlight:
shedding light on the web of documents. In I-
SEMANTICS, pages 1–8.
Lev-Arie Ratinov, Dan Roth, Doug Downey, and Mike
Anderson. 2011. Local and global algorithms for
disambiguation to wikipedia. In ACL, pages 1375–
1384.
Grigorios Tsoumakas, Min-Ling Zhang, and Zhi-Hua
Zhou. 2012. Introduction to the special issue on
learning from multi-label data. Machine Learning,
88(1-2):1–4.
Mohamed Amir Yosef, Johannes Hoffart, Ilaria Bor-
dino, Marc Spaniol, and Gerhard Weikum. 2011.
AIDA: An online tool for accurate disambiguation
of named entities in text and tables. PVLDB,
4(12):1450–1453.
Mohamed Amir Yosef, Sandro Bauer, Johannes Hof-
fart, Marc Spaniol, and Gerhard Weikum. 2012.
HYENA: Hierarchical Type Classification for Entity
Names. In COLING, pages 1361–1370.
</reference>
<page confidence="0.997339">
138
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.757781">
<title confidence="0.9893665">HYENA-live: Fine-Grained Online Entity Type Classification Natural-language Text</title>
<author confidence="0.916334">Amir</author>
<affiliation confidence="0.908702">(1) Max-Planck-Institut f¨ur Informatik, Saarbr¨ucken, (2) Computer Laboratory, University of Cambridge,</affiliation>
<email confidence="0.999346">sandro.bauer@cl.cam.ac.uk</email>
<abstract confidence="0.998721275862069">Recent research has shown progress in achieving high-quality, very fine-grained type classification in hierarchical taxonomies. Within such a multi-level type hierarchy with several hundreds of types at different levels, many entities naturally belong to multiple types. In order to achieve high-precision in type classification, current approaches are either limited to certain domains or require time consuming multistage computations. As a consequence, existing systems are incapable of performing ad-hoc type classification on arbitrary input texts. In this demo, we present a novel Webbased tool that is able to perform domain independent entity type classification under real time conditions. Thanks to its efficient implementation and compacted feature representation, the system is able to process text inputs on-the-fly while still achieving equally high precision as leading state-ofthe-art implementations. Our system offers an online interface where natural-language text can be inserted, which returns semantic type labels for entity mentions. Further more, the user interface allows users to explore the assigned types by visualizing and navigating along the type-hierarchy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Altaf ur Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Inducing fine-grained semantic classes via hierarchical and collective classification.</title>
<date>2010</date>
<booktitle>In COLING,</booktitle>
<pages>931--939</pages>
<contexts>
<context position="6931" citStr="Rahman and Ng, 2010" startWordPosition="1051" endWordPosition="1054"> While their approach is fundamentally suitable, their type system is very restricted. In order to account for more fine-grained classes, more features need to be added to their feature set. (Ekbal et al., 2010) considered 141 subtypes of WordNet class PERSON and developed a maximum entropy classifier exploiting the words surrounding the mentions together with their POS tags and other contextual features. Their type hierarchy is finegrained, but still limited to sub classes of PERSON. In addition, their experimental results have been flagged as non-reproducible in the ACL Anthology. (Altaf ur Rahman and Ng, 2010) considered a two-level type hierarchy consisting of 29 top-level classes and a total of 92 sub-classes. These include many non-entity types such as date, time, percent, money, quantity, ordinal, cardinal, etc. They incorporated a hierarchical classifier using a rich feature set and made use of WordNet sense tagging. However, the latter requires human interception, which is not suitable for ad-hoc processing of outof-domain texts. (Ling and Weld, 2012) developed FIGER, which classifies entity mentions onto a two-level taxonomy based on the Freebase knowledge base (Bollacker et al., 2008). This</context>
</contexts>
<marker>Rahman, Ng, 2010</marker>
<rawString>Md. Altaf ur Rahman and Vincent Ng. 2010. Inducing fine-grained semantic classes via hierarchical and collective classification. In COLING, pages 931– 939.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S¨oren Auer</author>
<author>Christian Bizer</author>
<author>Georgi Kobilarov</author>
<author>Jens Lehmann</author>
<author>Zachary Ives</author>
</authors>
<title>Dbpedia: A nucleus for a web of open data. In</title>
<date>2007</date>
<booktitle>ISWC,</booktitle>
<pages>11--15</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="10329" citStr="Auer et al., 2007" startWordPosition="1612" endWordPosition="1615">kipedia Categories to WordNet synsets. We start with five top classes namely PERSON, LOCATION, ORGANIZATION, EVENT and ARTIFACT. Under each top class, the most 100 prominent descendant classes are picked. Prominence is estimated by the number of YAGO entities tagged with this class. This results in a very-fine grained taxonomy of 505 types, represented as a directed acyclic graph with 9 levels in its deepest parts. While the classes are picked from the YAGO type system, the approach is generic and can be applied to derive type taxonomies from other knowledge bases such as Freebase or DBpedia (Auer et al., 2007) as in (Ling and Weld, 2012). 3.2 Feature Set For the sake of generality and applicability to arbitrary text, we opted for features that can be automatically extracted from the input text without any human interaction, or manual annotation. The extracted features fall under five categories, which we briefly explain in the rest of this section. Mention String We derive four features from the entity mention string. The mention string itself, a noun phrase consisting of one or more consecutive words. The other three features are unigrams, bigrams, and trigrams that overlap with the mention string</context>
</contexts>
<marker>Auer, Bizer, Kobilarov, Lehmann, Ives, 2007</marker>
<rawString>S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, and Zachary Ives. 2007. Dbpedia: A nucleus for a web of open data. In ISWC, pages 11–15. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt D Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: a collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In SIGMOD,</booktitle>
<pages>1247--1250</pages>
<contexts>
<context position="7525" citStr="Bollacker et al., 2008" startWordPosition="1142" endWordPosition="1145">Altaf ur Rahman and Ng, 2010) considered a two-level type hierarchy consisting of 29 top-level classes and a total of 92 sub-classes. These include many non-entity types such as date, time, percent, money, quantity, ordinal, cardinal, etc. They incorporated a hierarchical classifier using a rich feature set and made use of WordNet sense tagging. However, the latter requires human interception, which is not suitable for ad-hoc processing of outof-domain texts. (Ling and Weld, 2012) developed FIGER, which classifies entity mentions onto a two-level taxonomy based on the Freebase knowledge base (Bollacker et al., 2008). This results in a two-level hierarchy with top-level topics and 112 types. They trained a CRF for the joint task of recognizing entity mentions and inferring type tags. Although they handle multi-label assignment, their test data is sparse. Many classes are absent and plenty of instances come with only a single label (e.g. 216 of the 562 entities were of type PERSON without subtypes). Further, their results are instance based, which does not guarantee that the quality of their system will be reproducible for all the 112 types in their taxonomy. (Yosef et al., 2012) is the most recent work in</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt D. Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In SIGMOD, pages 1247–1250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asif Ekbal</author>
<author>Eva Sourjikova</author>
<author>Anette Frank</author>
<author>Simone P Ponzetto</author>
</authors>
<title>Assessing the challenge of fine-grained named entity recognition and classification.</title>
<date>2010</date>
<booktitle>In Named Entities Workshop,</booktitle>
<pages>93--101</pages>
<contexts>
<context position="6522" citStr="Ekbal et al., 2010" startWordPosition="989" endWordPosition="992">s section we will briefly describe each of them. (Fleischman and Hovy, 2002) is one of the earliest approaches to perform entity classification into subtypes of PERSON. They developed a decisiontree classifier based on contextual features that can be automatically extracted from the text. In order to account for scarcity of labeled training data, they tapped on WordNet synonyms to achieve higher coverage. While their approach is fundamentally suitable, their type system is very restricted. In order to account for more fine-grained classes, more features need to be added to their feature set. (Ekbal et al., 2010) considered 141 subtypes of WordNet class PERSON and developed a maximum entropy classifier exploiting the words surrounding the mentions together with their POS tags and other contextual features. Their type hierarchy is finegrained, but still limited to sub classes of PERSON. In addition, their experimental results have been flagged as non-reproducible in the ACL Anthology. (Altaf ur Rahman and Ng, 2010) considered a two-level type hierarchy consisting of 29 top-level classes and a total of 92 sub-classes. These include many non-entity types such as date, time, percent, money, quantity, ordi</context>
</contexts>
<marker>Ekbal, Sourjikova, Frank, Ponzetto, 2010</marker>
<rawString>Asif Ekbal, Eva Sourjikova, Anette Frank, and Simone P. Ponzetto. 2010. Assessing the challenge of fine-grained named entity recognition and classification. In Named Entities Workshop, pages 93–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In ACL,</booktitle>
<pages>363--370</pages>
<contexts>
<context position="4099" citStr="Finkel et al., 2005" startWordPosition="598" endWordPosition="601">roceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 133–138, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics grams, part-of-speech tags, and also phrases from a large gazetteer derived from state-of-the-art knowledge bases. In order to perform “live” entity type classification based on ad-hoc text inputs, several performance optimizations have been undertaken to operate under real-time conditions. 2 Entity Type Classification Systems State-of-the-art tools for named entity recognition such as the Stanford NER Tagger (Finkel et al., 2005) compute semantic tags only for a small set of coarse-grained types: Person, Location, and Organization (plus tags for non-entity phrases of type time, money, percent, and date). However, we are not aware of any online tool that performs fine-grained typing of entity mentions. The most common workaround to perform entity classification is a two-stage process: in first applying an online tool for Named-Entity Disambiguation (NED), such as DBpedia Spotlight (Mendes et al., 2011) or AIDA (Yosef et al., 2011; Hoffart et al., 2011), in order to map the mentions onto canonical entities and subsequen</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In ACL, pages 363–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Fleischman</author>
<author>Eduard Hovy</author>
</authors>
<title>Fine grained classification of named entities.</title>
<date>2002</date>
<booktitle>In COLING,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="5979" citStr="Fleischman and Hovy, 2002" startWordPosition="902" endWordPosition="905">hat correspond to a canonical entity within a knowledge base. However, this fails for all out-ofknowledge-base entities like unregistered persons, start-up companies, etc. 3. NED heavily depends on the quality of the underlying knowledge base. Yet, only very few knowledge bases have comprehensive class labeling of entities. Even more, in the best case, coverage drops sharply for relatively uncommon entities. We decided to adopt one of the existing approaches to make it suitable for online querying. We considered five systems. In the rest of this section we will briefly describe each of them. (Fleischman and Hovy, 2002) is one of the earliest approaches to perform entity classification into subtypes of PERSON. They developed a decisiontree classifier based on contextual features that can be automatically extracted from the text. In order to account for scarcity of labeled training data, they tapped on WordNet synonyms to achieve higher coverage. While their approach is fundamentally suitable, their type system is very restricted. In order to account for more fine-grained classes, more features need to be added to their feature set. (Ekbal et al., 2010) considered 141 subtypes of WordNet class PERSON and deve</context>
</contexts>
<marker>Fleischman, Hovy, 2002</marker>
<rawString>Michael Fleischman and Eduard Hovy. 2002. Fine grained classification of named entities. In COLING, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Hoffart</author>
<author>Mohamed Amir Yosef</author>
<author>Ilaria Bordino</author>
<author>Hagen F¨urstenau</author>
<author>Manfred Pinkal</author>
<author>Marc Spaniol</author>
<author>Bilyana Taneva</author>
<author>Stefan Thater</author>
<author>Gerhard Weikum</author>
</authors>
<title>Robust disambiguation of named entities in text.</title>
<date>2011</date>
<booktitle>In EMNLP,</booktitle>
<pages>782--792</pages>
<marker>Hoffart, Yosef, Bordino, F¨urstenau, Pinkal, Spaniol, Taneva, Thater, Weikum, 2011</marker>
<rawString>Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen F¨urstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum. 2011. Robust disambiguation of named entities in text. In EMNLP, pages 782–792.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Hoffart</author>
<author>Fabian M Suchanek</author>
<author>Klaus Berberich</author>
<author>Gerhard Weikum</author>
</authors>
<title>YAGO2: A spatially and temporally enhanced knowledge base from wikipedia.</title>
<date>2013</date>
<journal>Artificial Intelligence,</journal>
<volume>194</volume>
<issue>0</issue>
<contexts>
<context position="9533" citStr="Hoffart et al., 2013" startWordPosition="1479" endWordPosition="1482">tem by spotting the bottlenecks in the architecture and modifying it accordingly to be suitable for online querying. In Section 3 we explain in details HYENA’s type taxonomy and their feature portfolio. Later on, we explain the engineering undertaken in order to develop the on-the-fly type classification system HYENA-live (cf. Section 4). 3 Type Hierarchy and Feature Set 3.1 Fine-grained Taxonomy The type system is an automatically gathered finegrained taxonomy of 505 classes. The classes are organized under 5 top level classes, with 100 descendant classes under each. The YAGO knowledge base (Hoffart et al., 2013) is selected to derive the taxonomy from because of its highly precise classification of entities into WordNet classes, which is a result of the accurate mapping YAGO has from Wikipedia Categories to WordNet synsets. We start with five top classes namely PERSON, LOCATION, ORGANIZATION, EVENT and ARTIFACT. Under each top class, the most 100 prominent descendant classes are picked. Prominence is estimated by the number of YAGO entities tagged with this class. This results in a very-fine grained taxonomy of 505 types, represented as a directed acyclic graph with 9 levels in its deepest parts. Whi</context>
</contexts>
<marker>Hoffart, Suchanek, Berberich, Weikum, 2013</marker>
<rawString>Johannes Hoffart, Fabian M. Suchanek, Klaus Berberich, and Gerhard Weikum. 2013. YAGO2: A spatially and temporally enhanced knowledge base from wikipedia. Artificial Intelligence, 194(0):28 – 61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Ling</author>
<author>Daniel S Weld</author>
</authors>
<title>Fine-grained entity recognition.</title>
<date>2012</date>
<booktitle>In AAAI,</booktitle>
<pages>94--100</pages>
<contexts>
<context position="4775" citStr="Ling and Weld, 2012" startWordPosition="708" endWordPosition="711">ained types: Person, Location, and Organization (plus tags for non-entity phrases of type time, money, percent, and date). However, we are not aware of any online tool that performs fine-grained typing of entity mentions. The most common workaround to perform entity classification is a two-stage process: in first applying an online tool for Named-Entity Disambiguation (NED), such as DBpedia Spotlight (Mendes et al., 2011) or AIDA (Yosef et al., 2011; Hoffart et al., 2011), in order to map the mentions onto canonical entities and subsequently query the knowledge base for their types. In fact, (Ling and Weld, 2012) followed this approach when comparing their entity classification system results against those obtained by an adoption of the Illinois’ Named-Entity Linking system (NEL) (Ratinov et al., 2011) and reached the conclusion that while NEL performed decently for prominent entities, it could not scale to cover long tail ones. Specifically, entity typing via NED has three major drawbacks: 1. NED is an inherently hard problem, especially with highly ambiguous mentions. As a consequence, accurate NED systems come at a high computation costs. 2. NED only works for those mentions that correspond to a ca</context>
<context position="7387" citStr="Ling and Weld, 2012" startWordPosition="1122" endWordPosition="1125">limited to sub classes of PERSON. In addition, their experimental results have been flagged as non-reproducible in the ACL Anthology. (Altaf ur Rahman and Ng, 2010) considered a two-level type hierarchy consisting of 29 top-level classes and a total of 92 sub-classes. These include many non-entity types such as date, time, percent, money, quantity, ordinal, cardinal, etc. They incorporated a hierarchical classifier using a rich feature set and made use of WordNet sense tagging. However, the latter requires human interception, which is not suitable for ad-hoc processing of outof-domain texts. (Ling and Weld, 2012) developed FIGER, which classifies entity mentions onto a two-level taxonomy based on the Freebase knowledge base (Bollacker et al., 2008). This results in a two-level hierarchy with top-level topics and 112 types. They trained a CRF for the joint task of recognizing entity mentions and inferring type tags. Although they handle multi-label assignment, their test data is sparse. Many classes are absent and plenty of instances come with only a single label (e.g. 216 of the 562 entities were of type PERSON without subtypes). Further, their results are instance based, which does not guarantee that</context>
<context position="10357" citStr="Ling and Weld, 2012" startWordPosition="1618" endWordPosition="1621">Net synsets. We start with five top classes namely PERSON, LOCATION, ORGANIZATION, EVENT and ARTIFACT. Under each top class, the most 100 prominent descendant classes are picked. Prominence is estimated by the number of YAGO entities tagged with this class. This results in a very-fine grained taxonomy of 505 types, represented as a directed acyclic graph with 9 levels in its deepest parts. While the classes are picked from the YAGO type system, the approach is generic and can be applied to derive type taxonomies from other knowledge bases such as Freebase or DBpedia (Auer et al., 2007) as in (Ling and Weld, 2012). 3.2 Feature Set For the sake of generality and applicability to arbitrary text, we opted for features that can be automatically extracted from the input text without any human interaction, or manual annotation. The extracted features fall under five categories, which we briefly explain in the rest of this section. Mention String We derive four features from the entity mention string. The mention string itself, a noun phrase consisting of one or more consecutive words. The other three features are unigrams, bigrams, and trigrams that overlap with the mention string. Sentence Surrounding Menti</context>
</contexts>
<marker>Ling, Weld, 2012</marker>
<rawString>Xiao Ling and Daniel S. Weld. 2012. Fine-grained entity recognition. In AAAI, pages 94–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo N Mendes</author>
<author>Max Jakob</author>
<author>Andr´es Garc´ıa-Silva</author>
<author>Christian Bizer</author>
</authors>
<title>Dbpedia spotlight: shedding light on the web of documents.</title>
<date>2011</date>
<booktitle>In ISEMANTICS,</booktitle>
<pages>1--8</pages>
<marker>Mendes, Jakob, Garc´ıa-Silva, Bizer, 2011</marker>
<rawString>Pablo N. Mendes, Max Jakob, Andr´es Garc´ıa-Silva, and Christian Bizer. 2011. Dbpedia spotlight: shedding light on the web of documents. In ISEMANTICS, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev-Arie Ratinov</author>
<author>Dan Roth</author>
<author>Doug Downey</author>
<author>Mike Anderson</author>
</authors>
<title>Local and global algorithms for disambiguation to wikipedia. In</title>
<date>2011</date>
<booktitle>ACL,</booktitle>
<pages>1375--1384</pages>
<contexts>
<context position="4968" citStr="Ratinov et al., 2011" startWordPosition="736" endWordPosition="739">ed typing of entity mentions. The most common workaround to perform entity classification is a two-stage process: in first applying an online tool for Named-Entity Disambiguation (NED), such as DBpedia Spotlight (Mendes et al., 2011) or AIDA (Yosef et al., 2011; Hoffart et al., 2011), in order to map the mentions onto canonical entities and subsequently query the knowledge base for their types. In fact, (Ling and Weld, 2012) followed this approach when comparing their entity classification system results against those obtained by an adoption of the Illinois’ Named-Entity Linking system (NEL) (Ratinov et al., 2011) and reached the conclusion that while NEL performed decently for prominent entities, it could not scale to cover long tail ones. Specifically, entity typing via NED has three major drawbacks: 1. NED is an inherently hard problem, especially with highly ambiguous mentions. As a consequence, accurate NED systems come at a high computation costs. 2. NED only works for those mentions that correspond to a canonical entity within a knowledge base. However, this fails for all out-ofknowledge-base entities like unregistered persons, start-up companies, etc. 3. NED heavily depends on the quality of th</context>
</contexts>
<marker>Ratinov, Roth, Downey, Anderson, 2011</marker>
<rawString>Lev-Arie Ratinov, Dan Roth, Doug Downey, and Mike Anderson. 2011. Local and global algorithms for disambiguation to wikipedia. In ACL, pages 1375– 1384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grigorios Tsoumakas</author>
<author>Min-Ling Zhang</author>
<author>Zhi-Hua Zhou</author>
</authors>
<title>Introduction to the special issue on learning from multi-label data.</title>
<date>2012</date>
<booktitle>Machine Learning,</booktitle>
<pages>88--1</pages>
<contexts>
<context position="3058" citStr="Tsoumakas et al., 2012" startWordPosition="449" endWordPosition="452">ntity type classification Problem Statement Type classification is not only be based on hierarchical sub-type relationships (e.g. Musician isA Person), but also has to do on multi-labeling. Within a very fine-grained type hierarchy, many entities naturally belong to multiple types. For example, a guitarist is also a musician and a person, but may also be a singer, an actor, or even a politician. Consequently, entities should not only be assigned the most (fine-grained) label associated to them, but with all labels relevant to them. So we face a hierarchical multi-label classification problem (Tsoumakas et al., 2012). Contribution This paper introduces HYENA-live, which allows an on-the-fly computation of semantic types for entity mentions, based on a multi-level type hierarchy. Our approach uses a suite of features for a given entity mention, such as neighboring words and bi&amp;quot;Funded with 100,000$, Google was founded by Brin and Page&amp;quot; Business_people Entrepreneur Entertainer Musician &amp;quot;Pageplayed on his first guitar in 1952 &amp;quot; 133 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 133–138, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Lingu</context>
</contexts>
<marker>Tsoumakas, Zhang, Zhou, 2012</marker>
<rawString>Grigorios Tsoumakas, Min-Ling Zhang, and Zhi-Hua Zhou. 2012. Introduction to the special issue on learning from multi-label data. Machine Learning, 88(1-2):1–4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Amir Yosef</author>
<author>Johannes Hoffart</author>
<author>Ilaria Bordino</author>
<author>Marc Spaniol</author>
<author>Gerhard Weikum</author>
</authors>
<title>AIDA: An online tool for accurate disambiguation of named entities in text and tables.</title>
<date>2011</date>
<journal>PVLDB,</journal>
<volume>4</volume>
<issue>12</issue>
<contexts>
<context position="4608" citStr="Yosef et al., 2011" startWordPosition="679" endWordPosition="682">ems State-of-the-art tools for named entity recognition such as the Stanford NER Tagger (Finkel et al., 2005) compute semantic tags only for a small set of coarse-grained types: Person, Location, and Organization (plus tags for non-entity phrases of type time, money, percent, and date). However, we are not aware of any online tool that performs fine-grained typing of entity mentions. The most common workaround to perform entity classification is a two-stage process: in first applying an online tool for Named-Entity Disambiguation (NED), such as DBpedia Spotlight (Mendes et al., 2011) or AIDA (Yosef et al., 2011; Hoffart et al., 2011), in order to map the mentions onto canonical entities and subsequently query the knowledge base for their types. In fact, (Ling and Weld, 2012) followed this approach when comparing their entity classification system results against those obtained by an adoption of the Illinois’ Named-Entity Linking system (NEL) (Ratinov et al., 2011) and reached the conclusion that while NEL performed decently for prominent entities, it could not scale to cover long tail ones. Specifically, entity typing via NED has three major drawbacks: 1. NED is an inherently hard problem, especiall</context>
</contexts>
<marker>Yosef, Hoffart, Bordino, Spaniol, Weikum, 2011</marker>
<rawString>Mohamed Amir Yosef, Johannes Hoffart, Ilaria Bordino, Marc Spaniol, and Gerhard Weikum. 2011. AIDA: An online tool for accurate disambiguation of named entities in text and tables. PVLDB, 4(12):1450–1453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Amir Yosef</author>
<author>Sandro Bauer</author>
<author>Johannes Hoffart</author>
<author>Marc Spaniol</author>
<author>Gerhard Weikum</author>
</authors>
<title>HYENA: Hierarchical Type Classification for Entity Names. In</title>
<date>2012</date>
<booktitle>COLING,</booktitle>
<pages>1361--1370</pages>
<contexts>
<context position="8098" citStr="Yosef et al., 2012" startWordPosition="1238" endWordPosition="1241">eebase knowledge base (Bollacker et al., 2008). This results in a two-level hierarchy with top-level topics and 112 types. They trained a CRF for the joint task of recognizing entity mentions and inferring type tags. Although they handle multi-label assignment, their test data is sparse. Many classes are absent and plenty of instances come with only a single label (e.g. 216 of the 562 entities were of type PERSON without subtypes). Further, their results are instance based, which does not guarantee that the quality of their system will be reproducible for all the 112 types in their taxonomy. (Yosef et al., 2012) is the most recent work in multi-label type classification. The HYENA system incorporates a large hierarchy of 505 classes 134 organized under 5 top level classes, with 100 descendant classes under each of them. The hierarchy reaches a depth of up to 9 levels in some parts. The system is based on an SVM classifier using a comprehensive set of features and provides results for all classes of a large data set. In their experiments the superiority of the system in terms of precision and recall has been shown. However, the main drawback of HYENA comes from its large hierarchy and the extensive se</context>
</contexts>
<marker>Yosef, Bauer, Hoffart, Spaniol, Weikum, 2012</marker>
<rawString>Mohamed Amir Yosef, Sandro Bauer, Johannes Hoffart, Marc Spaniol, and Gerhard Weikum. 2012. HYENA: Hierarchical Type Classification for Entity Names. In COLING, pages 1361–1370.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>