<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.093331">
<title confidence="0.9951125">
A Pipeline Approach to Chinese Personal Name
Disambiguation
</title>
<author confidence="0.99782">
Yang Song, Zhengyan He, Chen Chen, Houfeng Wang
</author>
<affiliation confidence="0.9932845">
Key Laboratory of Computational Linguistics (Peking University)
Ministry of Education,China
</affiliation>
<email confidence="0.980603">
{ysong, hezhengyan, chenchen, wanghf}@pku.edu.cn
</email>
<sectionHeader confidence="0.978621" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999960555555556">
In this paper, we describe our sys-
tem for Chinese personal name dis-
ambiguation task in the first CIPS-
SIGHAN joint conference on Chinese
Language Processing(CLP2010). We
use a pipeline approach, in which pre-
processing, unrelated documents dis-
carding, Chinese personal name exten-
sion and document clustering are per-
formed separately. Chinese personal
name extension is the most important
part of the system. It uses two addi-
tional dictionaries to extract full per-
sonal names in Chinese text. And then
document clustering is performed un-
der different personal names. Exper-
imental results show that our system
can achieve good performances.
</bodyText>
<sectionHeader confidence="0.997335" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999887571428571">
Personal name search is one of the most im-
portant tasks for search engines. When a per-
sonal name query is given to a search engine,
a list of related documents will be shown. But
not all of the returned documents refer to the
same person whom users want to find. For ex-
ample, the query name “jordan” is submitted
to a search engine, we can get a lot of doc-
uments containing “jordan”. Some of them
may refer to the computer scientist, others
perhaps refer to the basketball player. For
English, there have been three Web People
Search (WePSI) evaluation campaigns on per-
sonal name disambiguation. But for Chinese,
</bodyText>
<footnote confidence="0.928449">
1http://nlp.uned.es/weps/
</footnote>
<bodyText confidence="0.999772194444444">
this is the first time. It encounters more chal-
lenge for Chinese personal name disambigua-
tion. There are no word boundary in Chinese
text, so it becomes difficult to recognize the
full personal names from Chinese text. For ex-
ample, a query name “高明” is given, but the
full personal name from some documents may
be an extension of “高明”, like “高明光” or
“高明珍”, and so on. Meanwhile, “高明” can
also be a common Chinese word. So we need
to discard those documents which are not ref-
ered to any person related to the given query
name.
To solve the above-mentioned problem, we
explore a pipeline approach to Chinese per-
sonal name disambiguation. The overview of
our system is illustrated in Figure 1. We split
this task into four parts: preprocessing, unre-
lated documents discarding, Chinese personal
name extension and document clustering. In
preprocessing and unrelated documents dis-
carding, we use word segmentation and part-
of-speech tagging tools to process the given
dataset and documents are discarded when
the given query name is not tagged as a per-
sonal name or part of a personal name. After
that we perform personal name extension in
the documents for a given query name. When
the query name has only two characters. We
extend it to the left or right for one character.
For example, we can extend “林鹏” to “金林
鹏” or “林鹏飞”. The purpose of extending
the query name is to obtain the full personal
name. In this way, we can get a lot of full per-
sonal names for a given query name from the
documents. And then document clustering
</bodyText>
<figureCaption confidence="0.986395">
Figure 1: Overview of the System
</figureCaption>
<bodyText confidence="0.886628130434783">
is performed under different personal names.
HAC (Hierarchical Agglomerative Clustering)
is selected here. We represent documents with
bag of words and solve the problem in vector
space model, nouns, verbs, bigrams of nouns
or verbs and named entities are selected as
features. The feature weight value takes 0 or
1. In HAC, we use group-average link method
as the distance measure and consine similar-
ity as the similarity computing measure. The
stopping criteria is dependent on a threshold
which is obtained from training data. Our sys-
tem produces pretty good results in the final
evaluation.
The remainder of this paper is organized as
follows. Section 2 introduces related work.
Section 3 gives a detailed description about
our pipeline approach. It includes preprocess-
ing, unrelated documents discarding, Chinese
personal name extension and document clus-
tering. Section 4 presents the experimental
results. The conclusions are given in Section
5.
</bodyText>
<sectionHeader confidence="0.998973" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999946509803922">
Several important studies have tried to
solve the task introduced in the previous sec-
tion. Most of them treated it as an cluster-
ing problem. Bagga &amp; Baldwin (1998) first
selected tokens from local context as features
to perform intra-document coreference resolu-
tion. Mann &amp; Yarowsky (2003) extracted lo-
cal biographical information as features. Niu
et al. (2004) used relation extraction results
in addition to local context features and get a
perfect results. Al-Kamha and Embley (2004)
clustered search results with feature set in-
cluding attributes, links and page similarities.
In recent years, this problem has attracted
a great deal of attention from many research
institutes. Ying Chen et al. (2009) used a
Web 1T 5-gram corpus released by Google
to extract additional features for clustering.
Masaki Ikeda et al. (2009) proposed a two-
stage clustering algorithm to improve the low
recall values. In the first stage, some reliable
features (like named entities) are used to con-
nect documents about the same person. Af-
ter that, the connected documents (document
cluster) are used as a source from which new
features (compound keyword features) are ex-
tracted. These new features are used in the
second stage to make additional connections
between documents. Their approach is to im-
prove clusters step by step, where each step
refines clusters conservatively. Han &amp; Zhao
(2009) presented a system named CASIANED
to disambiguate personal names based on pro-
fessional categorization. They first catego-
rize different personal name appearances into
a real world professional taxonomy, and then
the personal name appearances are clustered
into a single cluster. Chen Chen et al. (2009)
explored a novel feature weight computing
method in clustering. It is based on the point-
wise mutual information between the ambigu-
ous name and features. In their paper, they
also develop a trade-off point based cluster
stopping criteria which find the trade-off point
between intra-cluster compactness and inter-
cluster separation.
Our approach is based on Chinese per-
sonal name extension. We recognize the full
personal names in Chinese text and perform
document clustering under different personal
names.
</bodyText>
<sectionHeader confidence="0.995091" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.99990725">
In this section, we will explain preprocess-
ing, unrelated documents discarding, Chinese
personal name extension and document clus-
tering in order.
</bodyText>
<subsectionHeader confidence="0.989665">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999998315789474">
We use ltp-service2 to process the given Chi-
nese personal name disambiguation dataset
(a detailed introduction to it will be given
in section 4). Training data in the dataset
contains 32 query names. There are 100-300
documents under every query name. All the
documents are collected from Xinhua News
Agency. They contain the exact same string
as query names. Ltp-service is a web ser-
vice interface for LTP3(Language Technology
Platform). LTP has integrated many Chinese
processing modules, including word segmen-
tation, part-of-speech tagging, named entity
recognition, word sense disambiguation, and
so on. Jun Lang et al. (2006) give a detailed
introduction to LTP. Here we only use LTP
to generate word segmentation, part-of-speech
tagging and named entity recognition results
for the given dataset.
</bodyText>
<subsectionHeader confidence="0.975063">
3.2 Unrelated documents discarding
</subsectionHeader>
<bodyText confidence="0.999868333333333">
Under every query name, there are 100-300
documents. But not all of them are really re-
lated. For example, “高军” is a query name in
training data. In corresponding documents,
some are refered to real personal names like
“高军” or “高军田”. But others may be a sub-
string of an expression such as “最高军事 法
院”. These documents are needed to be fil-
tered out. We use the preprocessing tool LTP
to slove this problem. LTP can do word seg-
mentation and part-of-speech tagging for us.
For each document under a given query name,
if the query name in the document is tagged as
a personal name or part of some extended per-
sonal name, the document will be marked as
undiscarded, otherwise the document will be
discarded. Generally speaking, for the query
name containing three characters, we don’t
need to discard any of the corresponding doc-
uments. But in practice, we find that for some
query names, LTP always gives the invariable
</bodyText>
<footnote confidence="0.999742">
2http://code.google.com/p/ltp-service/
3http://ir.hit.edu.cn/ltp/
</footnote>
<bodyText confidence="0.999843">
part-of-speech. For example, no matter what
the context of “黄海” is, it is always tagged
as a geographic name. So we use another pre-
processing tool ICTCLAS4. Only when both
of them mark one document as discarded, we
discard the corresponding document.
</bodyText>
<subsectionHeader confidence="0.991036">
3.3 Chinese personal name extension
</subsectionHeader>
<bodyText confidence="0.999950257142857">
After discarding unrelated documents, we
need to recognize the full Chinese personal
names. We hypothesize that the full Chinese
personal name has not more than three char-
acters (We don’t consider the compound sur-
names here). So the query names containing
only two Chinese characters are considered to
extend. In our approach, we use two Chinese
personal names dictionaries. One is a sur-
name dictionary containing 423 one-character
entries. We use it to do left extend for the
query name. For example, the query name
is “高明” and its left character in a docu-
ment is “刘”, we will extend it to full per-
sonal name “刘高明”. The other is a non-
ending Chinese character dictionary contain-
ing 64 characters which could not occur at the
end of personal names. It is constructed by a
personal title dictionary. We use every title’s
first character and some other special charac-
ters (such as numbers or punctuations) to con-
stuct the dictionary. Some manual work has
also been done to filter a few incorrect charac-
ters. Several examples of the two dictionaries
are shown in Table 1.
Through the analysis of Xinhua News arti-
cles, we also find that nearly half of the docu-
ments under given query name actually refer
to the reporters. And they often appear in
the first or last brackets in the body of cor-
responding document. For example, “(通讯
员刘国党、黄海生)” is a sentence containing
query name “黄海”. We use some simple but
efficient rules to get full personal names for
this case.
</bodyText>
<subsectionHeader confidence="0.935865">
3.4 Document clustering
</subsectionHeader>
<bodyText confidence="0.9748805">
For every query name, we can get a list of
full peronal names. For example, when the
</bodyText>
<footnote confidence="0.833047">
4http://ictclas.org/
</footnote>
<tableCaption confidence="0.991839">
Table 1: Several Examples of the two Dictionaries
</tableCaption>
<table confidence="0.880978666666667">
Dictionaries Examples
Surnames T-, t, 4i, . , M, �, �, �, �, �, �, �, �, �...
Non-ending Chinese characters !A, M, -9, F , AK, R, �, �, 4E, 03, �T...
</table>
<bodyText confidence="0.99882525">
query name is “#395”, we can get the per-
sonal names like “#395F�”, “#395�”, “#395
A”, “#395�”. And then document clustering
is performed under different personal names.
</bodyText>
<sectionHeader confidence="0.676826" genericHeader="method">
3.4.1 Features
</sectionHeader>
<bodyText confidence="0.999976153846154">
We use bag of words to represent docu-
ments. Some representative words need to be
chosen as features. LTP can give us POS tag-
ging and NER results. We select all the nouns,
verbs and named entities which appear in the
same paragraph with given query name as fea-
tures. Meanwhile, the bigrams of nouns or
verbs are also selected. We take 0 or 1 for
feature weight value. 0 represents that the
feature doesn’t appear in corresponding para-
graphs, and 1 represents just the opposite. We
find that this weighting scheme is more effec-
tive than TFIDF.
</bodyText>
<subsectionHeader confidence="0.845333">
3.4.2 Clustering
</subsectionHeader>
<bodyText confidence="0.99997447826087">
All features are represented in vector space
model. Every document is modeled as a ver-
tex in the vector space. So every document
can be seen as a feature vector. Before cluster-
ing, the similarity between documents is com-
puted by cosine value of the angle between
feature vectors. We use HAC to do document
clustering. It is a bottom-up algorithm which
treats each document as a singleton cluster at
the outset and then successively merges (or
agglomerates) pairs of clusters until all clus-
ters have been merged into a single cluster
that contains all documents. From our ex-
perience, single link and group-average link
method seem to work better than complete
link one. We use group-average link method
in the final submission. The stopping criteria
is a difficult problem for clustering. Here we
use a threshold for terminating condition. So
it is not necessary to determine the number
of clusters beforehand. We select a threshold
which produces the best performance in train-
ing data.
</bodyText>
<sectionHeader confidence="0.994334" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999553666666667">
The dataset for Chinese personal name dis-
ambiguation task contains training data and
testing data. The training data contains
32 query names. Every query name folder
contains 100-300 news articles. Given the
query name, all the documents are retrived
by character-based matching from a collection
of Xinhua news documents in a time span of
fourteen years. The testing data contains 25
query names. Two threshold values as termi-
nating conditions are obtained from training
data. They are 0.4 and 0.5. For evaluation,
we use P-IP score and B-cubed score (Bagga
and Baldwin, 1998). Table 2 &amp; Table 3 show
the official evaluation results.
</bodyText>
<tableCaption confidence="0.989382">
Table 2: Official Results for P-IP score
</tableCaption>
<table confidence="0.99939275">
Threshold P-IP
P IP F score
0.4 88.32 94.9 91.15
0.5 91.3 91.77 91.18
</table>
<tableCaption confidence="0.977624">
Table 3: Official Results for B-Cubed score
</tableCaption>
<table confidence="0.99202725">
Threshold B-Cubed
Precision Recall F score
0.4 83.68 92.23 86.94
0.5 87.87 87.49 86.84
</table>
<bodyText confidence="0.9993345">
Besides the formal evaluation, the organizer
also provide a diagnosis test designed to ex-
plore the relationship between Chinese word
segmentation and personal name disambigua-
tion. That means the query names in the
documents are segmented correctly by manual
work. Table 4 &amp; Table 5 show the diagnosis
results.
</bodyText>
<table confidence="0.99523425">
Threshold B-Cubed
Precision Recall F score
0.4 84.53 93.42 87.96
0.5 88.59 88.59 87.8
</table>
<tableCaption confidence="0.898037">
Table 4: Diagnosis Results for P-IP score
</tableCaption>
<table confidence="0.99945325">
Threshold P-IP
P IP F score
0.4 89.01 95.83 91.96
0.5 91.85 92.68 91.96
</table>
<tableCaption confidence="0.998204">
Table 5: Diagnosis Results for B-Cubed score
</tableCaption>
<bodyText confidence="0.999867222222222">
The official results show that our method
performs pretty good. The diagnosis results
show that correct word segmentation can im-
prove the evaluation results. But the improve-
ment is rather limited. That is mainly because
Chinese personal name extension is done well
in our approach. So the diagnosis results don’t
gain much profit from query names’ correct
segmentation.
</bodyText>
<sectionHeader confidence="0.999584" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999778125">
We describe our framework in this paper.
First, we use LTP to do preprocessing for orig-
inal dataset which comes from Xinhua news
articles. LTP can produce good results for
Chinese text processing. And then we use
two additional dictionaries(one is Chinese sur-
name dictionary, the other is Non-ending Chi-
nese character dictionary) to do Chinese per-
sonal name extension. After that we perform
document clustering under different personal
names. Official evaluation results show that
our method can achieve good performances.
In the future, we will attempt to use other
features to represent corresponding persons in
the documents. We will also investigate auto-
matic terminating condition.
</bodyText>
<sectionHeader confidence="0.920701" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<footnote confidence="0.7326032">
This research is supported by National
Natural Science Foundation of Chinese
(No.60973053) and Research Fund for the
Doctoral Program of Higher Education of
China (No.20090001110047).
</footnote>
<sectionHeader confidence="0.899265" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99972892">
J. Artiles, J. Gonzalo, and S. Sekine. 2009. WePS
2 evaluation campaign: overview of the web peo-
ple search clustering task. In 2nd Web People
Search Evaluation Workshop(WePS 2009), 18th
WWW Conference.
Bagga and B. Baldwin. 1998. Entity-based
cross-document coreferencing using the vector
space model. In Proceedings of 17th Interna-
tional Conference on Computational Linguis-
tics, 79–85.
Mann G. and D. Yarowsky. 2003. Unsupervised
personal name disambiguation. In Proceedings
of CoNLL-2003, 33–40, Edmonton, Canada.
C. Niu, W. Li, and R. K. Srihari. 2004. Weakly
Supervised Learning for Cross-document Person
Name Disambiguation Supported by Informa-
tion Extraction. In Proceedings of ACL 2004.
Al-Kamha. R. and D. W. Embley. 2004. Group-
ing search-engine returned citations for person-
name queries. In Proceedings of WIDM 2004,
96-103, Washington, DC, USA.
Ying Chen, Sophia Yat Mei Lee, and Chu-Ren
Huang. 2009. PolyUHK:A Robust Information
Extraction System for Web Personal Names.
In 2nd Web People Search Evaluation Work-
shop(WePS 2009), 18th WWW Conference.
Masaki Ikeda, Shingo Ono, Issei Sato, Minoru
Yoshida, and Hiroshi Nakagawa. 2009. Person
Name Disambiguation on the Web by Two-Stage
Clustering. In 2nd Web People Search Evalua-
tion Workshop(WePS 2009), 18th WWW Con-
ference.
Xianpei Han and Jun Zhao. 2009. CASIANED:
Web Personal Name Disambiguation Based on
Professional Categorization. In 2nd Web People
Search Evaluation Workshop(WePS 2009), 18th
WWW Conference.
Chen Chen, Junfeng Hu, and Houfeng Wang.
2009. Clustering technique in multi-document
personal name disambiguation. In Proceed-
ings of the ACL-IJCNLP 2009 Student Research
Workshop, pages 88–95.
Jun Lang, Ting Liu, Huipeng Zhang and Sheng Li.
2006. LTP: Language Technology Platform. In
Proceedings of SWCL 2006.
Bagga, Amit and B. Baldwin. 1998. Algorithms
for scoring co-reference chains. In Proceedings
of the First International Conference on Lan-
guage Resources and Evaluation Workshop on
Linguistic co-reference.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.253405">
<title confidence="0.998057">A Pipeline Approach to Chinese Personal Disambiguation</title>
<author confidence="0.968578">Yang Song</author>
<author confidence="0.968578">Zhengyan He</author>
<author confidence="0.968578">Chen Chen</author>
<author confidence="0.968578">Houfeng</author>
<affiliation confidence="0.467853">Key Laboratory of Computational Linguistics (Peking Ministry of</affiliation>
<email confidence="0.67594">hezhengyan,chenchen,</email>
<abstract confidence="0.999237263157895">In this paper, we describe our system for Chinese personal name disambiguation task in the first CIPS- SIGHAN joint conference on Chinese Language Processing(CLP2010). We use a pipeline approach, in which preprocessing, unrelated documents discarding, Chinese personal name extension and document clustering are performed separately. Chinese personal name extension is the most important part of the system. It uses two additional dictionaries to extract full personal names in Chinese text. And then document clustering is performed under different personal names. Experimental results show that our system can achieve good performances.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Artiles</author>
<author>J Gonzalo</author>
<author>S Sekine</author>
</authors>
<title>evaluation campaign: overview of the web people search clustering task.</title>
<date>2009</date>
<journal>WePS</journal>
<booktitle>In 2nd Web People Search Evaluation Workshop(WePS 2009), 18th WWW Conference.</booktitle>
<volume>2</volume>
<marker>Artiles, Gonzalo, Sekine, 2009</marker>
<rawString>J. Artiles, J. Gonzalo, and S. Sekine. 2009. WePS 2 evaluation campaign: overview of the web people search clustering task. In 2nd Web People Search Evaluation Workshop(WePS 2009), 18th WWW Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bagga</author>
<author>B Baldwin</author>
</authors>
<title>Entity-based cross-document coreferencing using the vector space model.</title>
<date>1998</date>
<booktitle>In Proceedings of 17th International Conference on Computational Linguistics,</booktitle>
<pages>79--85</pages>
<contexts>
<context position="12662" citStr="Bagga and Baldwin, 1998" startWordPosition="2080" endWordPosition="2083">mance in training data. 4 Experimental Results The dataset for Chinese personal name disambiguation task contains training data and testing data. The training data contains 32 query names. Every query name folder contains 100-300 news articles. Given the query name, all the documents are retrived by character-based matching from a collection of Xinhua news documents in a time span of fourteen years. The testing data contains 25 query names. Two threshold values as terminating conditions are obtained from training data. They are 0.4 and 0.5. For evaluation, we use P-IP score and B-cubed score (Bagga and Baldwin, 1998). Table 2 &amp; Table 3 show the official evaluation results. Table 2: Official Results for P-IP score Threshold P-IP P IP F score 0.4 88.32 94.9 91.15 0.5 91.3 91.77 91.18 Table 3: Official Results for B-Cubed score Threshold B-Cubed Precision Recall F score 0.4 83.68 92.23 86.94 0.5 87.87 87.49 86.84 Besides the formal evaluation, the organizer also provide a diagnosis test designed to explore the relationship between Chinese word segmentation and personal name disambiguation. That means the query names in the documents are segmented correctly by manual work. Table 4 &amp; Table 5 show the diagnosis</context>
<context position="4236" citStr="Bagga &amp; Baldwin (1998)" startWordPosition="695" endWordPosition="698">d from training data. Our system produces pretty good results in the final evaluation. The remainder of this paper is organized as follows. Section 2 introduces related work. Section 3 gives a detailed description about our pipeline approach. It includes preprocessing, unrelated documents discarding, Chinese personal name extension and document clustering. Section 4 presents the experimental results. The conclusions are given in Section 5. 2 Related Work Several important studies have tried to solve the task introduced in the previous section. Most of them treated it as an clustering problem. Bagga &amp; Baldwin (1998) first selected tokens from local context as features to perform intra-document coreference resolution. Mann &amp; Yarowsky (2003) extracted local biographical information as features. Niu et al. (2004) used relation extraction results in addition to local context features and get a perfect results. Al-Kamha and Embley (2004) clustered search results with feature set including attributes, links and page similarities. In recent years, this problem has attracted a great deal of attention from many research institutes. Ying Chen et al. (2009) used a Web 1T 5-gram corpus released by Google to extract </context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Bagga and B. Baldwin. 1998. Entity-based cross-document coreferencing using the vector space model. In Proceedings of 17th International Conference on Computational Linguistics, 79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Mann</author>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised personal name disambiguation.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL-2003,</booktitle>
<location>33–40, Edmonton, Canada.</location>
<contexts>
<context position="4362" citStr="Mann &amp; Yarowsky (2003)" startWordPosition="713" endWordPosition="716">d as follows. Section 2 introduces related work. Section 3 gives a detailed description about our pipeline approach. It includes preprocessing, unrelated documents discarding, Chinese personal name extension and document clustering. Section 4 presents the experimental results. The conclusions are given in Section 5. 2 Related Work Several important studies have tried to solve the task introduced in the previous section. Most of them treated it as an clustering problem. Bagga &amp; Baldwin (1998) first selected tokens from local context as features to perform intra-document coreference resolution. Mann &amp; Yarowsky (2003) extracted local biographical information as features. Niu et al. (2004) used relation extraction results in addition to local context features and get a perfect results. Al-Kamha and Embley (2004) clustered search results with feature set including attributes, links and page similarities. In recent years, this problem has attracted a great deal of attention from many research institutes. Ying Chen et al. (2009) used a Web 1T 5-gram corpus released by Google to extract additional features for clustering. Masaki Ikeda et al. (2009) proposed a twostage clustering algorithm to improve the low rec</context>
</contexts>
<marker>Mann, Yarowsky, 2003</marker>
<rawString>Mann G. and D. Yarowsky. 2003. Unsupervised personal name disambiguation. In Proceedings of CoNLL-2003, 33–40, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Niu</author>
<author>W Li</author>
<author>R K Srihari</author>
</authors>
<title>Weakly Supervised Learning for Cross-document Person Name Disambiguation Supported by Information Extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="4434" citStr="Niu et al. (2004)" startWordPosition="724" endWordPosition="727">escription about our pipeline approach. It includes preprocessing, unrelated documents discarding, Chinese personal name extension and document clustering. Section 4 presents the experimental results. The conclusions are given in Section 5. 2 Related Work Several important studies have tried to solve the task introduced in the previous section. Most of them treated it as an clustering problem. Bagga &amp; Baldwin (1998) first selected tokens from local context as features to perform intra-document coreference resolution. Mann &amp; Yarowsky (2003) extracted local biographical information as features. Niu et al. (2004) used relation extraction results in addition to local context features and get a perfect results. Al-Kamha and Embley (2004) clustered search results with feature set including attributes, links and page similarities. In recent years, this problem has attracted a great deal of attention from many research institutes. Ying Chen et al. (2009) used a Web 1T 5-gram corpus released by Google to extract additional features for clustering. Masaki Ikeda et al. (2009) proposed a twostage clustering algorithm to improve the low recall values. In the first stage, some reliable features (like named entit</context>
</contexts>
<marker>Niu, Li, Srihari, 2004</marker>
<rawString>C. Niu, W. Li, and R. K. Srihari. 2004. Weakly Supervised Learning for Cross-document Person Name Disambiguation Supported by Information Extraction. In Proceedings of ACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R</author>
<author>D W Embley</author>
</authors>
<title>Grouping search-engine returned citations for personname queries.</title>
<date>2004</date>
<booktitle>In Proceedings of WIDM</booktitle>
<pages>96--103</pages>
<location>Washington, DC, USA.</location>
<marker>R, Embley, 2004</marker>
<rawString>Al-Kamha. R. and D. W. Embley. 2004. Grouping search-engine returned citations for personname queries. In Proceedings of WIDM 2004, 96-103, Washington, DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Chen</author>
<author>Sophia Yat Mei Lee</author>
<author>Chu-Ren Huang</author>
</authors>
<title>PolyUHK:A Robust Information Extraction System for Web Personal Names.</title>
<date>2009</date>
<booktitle>In 2nd Web People Search Evaluation Workshop(WePS 2009), 18th WWW Conference.</booktitle>
<contexts>
<context position="4777" citStr="Chen et al. (2009)" startWordPosition="777" endWordPosition="780">ion. Most of them treated it as an clustering problem. Bagga &amp; Baldwin (1998) first selected tokens from local context as features to perform intra-document coreference resolution. Mann &amp; Yarowsky (2003) extracted local biographical information as features. Niu et al. (2004) used relation extraction results in addition to local context features and get a perfect results. Al-Kamha and Embley (2004) clustered search results with feature set including attributes, links and page similarities. In recent years, this problem has attracted a great deal of attention from many research institutes. Ying Chen et al. (2009) used a Web 1T 5-gram corpus released by Google to extract additional features for clustering. Masaki Ikeda et al. (2009) proposed a twostage clustering algorithm to improve the low recall values. In the first stage, some reliable features (like named entities) are used to connect documents about the same person. After that, the connected documents (document cluster) are used as a source from which new features (compound keyword features) are extracted. These new features are used in the second stage to make additional connections between documents. Their approach is to improve clusters step b</context>
</contexts>
<marker>Chen, Lee, Huang, 2009</marker>
<rawString>Ying Chen, Sophia Yat Mei Lee, and Chu-Ren Huang. 2009. PolyUHK:A Robust Information Extraction System for Web Personal Names. In 2nd Web People Search Evaluation Workshop(WePS 2009), 18th WWW Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaki Ikeda</author>
<author>Shingo Ono</author>
<author>Issei Sato</author>
<author>Minoru Yoshida</author>
<author>Hiroshi Nakagawa</author>
</authors>
<title>Person Name Disambiguation on the Web by Two-Stage Clustering.</title>
<date>2009</date>
<booktitle>In 2nd Web People Search Evaluation Workshop(WePS 2009), 18th WWW Conference.</booktitle>
<contexts>
<context position="4898" citStr="Ikeda et al. (2009)" startWordPosition="797" endWordPosition="800">s features to perform intra-document coreference resolution. Mann &amp; Yarowsky (2003) extracted local biographical information as features. Niu et al. (2004) used relation extraction results in addition to local context features and get a perfect results. Al-Kamha and Embley (2004) clustered search results with feature set including attributes, links and page similarities. In recent years, this problem has attracted a great deal of attention from many research institutes. Ying Chen et al. (2009) used a Web 1T 5-gram corpus released by Google to extract additional features for clustering. Masaki Ikeda et al. (2009) proposed a twostage clustering algorithm to improve the low recall values. In the first stage, some reliable features (like named entities) are used to connect documents about the same person. After that, the connected documents (document cluster) are used as a source from which new features (compound keyword features) are extracted. These new features are used in the second stage to make additional connections between documents. Their approach is to improve clusters step by step, where each step refines clusters conservatively. Han &amp; Zhao (2009) presented a system named CASIANED to disambigu</context>
</contexts>
<marker>Ikeda, Ono, Sato, Yoshida, Nakagawa, 2009</marker>
<rawString>Masaki Ikeda, Shingo Ono, Issei Sato, Minoru Yoshida, and Hiroshi Nakagawa. 2009. Person Name Disambiguation on the Web by Two-Stage Clustering. In 2nd Web People Search Evaluation Workshop(WePS 2009), 18th WWW Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianpei Han</author>
<author>Jun Zhao</author>
</authors>
<title>CASIANED: Web Personal Name Disambiguation Based on Professional Categorization.</title>
<date>2009</date>
<booktitle>In 2nd Web People Search Evaluation Workshop(WePS 2009), 18th WWW Conference.</booktitle>
<contexts>
<context position="5451" citStr="Han &amp; Zhao (2009)" startWordPosition="887" endWordPosition="890">t additional features for clustering. Masaki Ikeda et al. (2009) proposed a twostage clustering algorithm to improve the low recall values. In the first stage, some reliable features (like named entities) are used to connect documents about the same person. After that, the connected documents (document cluster) are used as a source from which new features (compound keyword features) are extracted. These new features are used in the second stage to make additional connections between documents. Their approach is to improve clusters step by step, where each step refines clusters conservatively. Han &amp; Zhao (2009) presented a system named CASIANED to disambiguate personal names based on professional categorization. They first categorize different personal name appearances into a real world professional taxonomy, and then the personal name appearances are clustered into a single cluster. Chen Chen et al. (2009) explored a novel feature weight computing method in clustering. It is based on the pointwise mutual information between the ambiguous name and features. In their paper, they also develop a trade-off point based cluster stopping criteria which find the trade-off point between intra-cluster compact</context>
</contexts>
<marker>Han, Zhao, 2009</marker>
<rawString>Xianpei Han and Jun Zhao. 2009. CASIANED: Web Personal Name Disambiguation Based on Professional Categorization. In 2nd Web People Search Evaluation Workshop(WePS 2009), 18th WWW Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Chen</author>
<author>Junfeng Hu</author>
<author>Houfeng Wang</author>
</authors>
<title>Clustering technique in multi-document personal name disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Student Research Workshop,</booktitle>
<pages>88--95</pages>
<contexts>
<context position="4777" citStr="Chen et al. (2009)" startWordPosition="777" endWordPosition="780">ion. Most of them treated it as an clustering problem. Bagga &amp; Baldwin (1998) first selected tokens from local context as features to perform intra-document coreference resolution. Mann &amp; Yarowsky (2003) extracted local biographical information as features. Niu et al. (2004) used relation extraction results in addition to local context features and get a perfect results. Al-Kamha and Embley (2004) clustered search results with feature set including attributes, links and page similarities. In recent years, this problem has attracted a great deal of attention from many research institutes. Ying Chen et al. (2009) used a Web 1T 5-gram corpus released by Google to extract additional features for clustering. Masaki Ikeda et al. (2009) proposed a twostage clustering algorithm to improve the low recall values. In the first stage, some reliable features (like named entities) are used to connect documents about the same person. After that, the connected documents (document cluster) are used as a source from which new features (compound keyword features) are extracted. These new features are used in the second stage to make additional connections between documents. Their approach is to improve clusters step b</context>
</contexts>
<marker>Chen, Hu, Wang, 2009</marker>
<rawString>Chen Chen, Junfeng Hu, and Houfeng Wang. 2009. Clustering technique in multi-document personal name disambiguation. In Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 88–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Lang</author>
</authors>
<title>Ting Liu, Huipeng Zhang and Sheng Li.</title>
<date>2006</date>
<booktitle>In Proceedings of SWCL</booktitle>
<marker>Lang, 2006</marker>
<rawString>Jun Lang, Ting Liu, Huipeng Zhang and Sheng Li. 2006. LTP: Language Technology Platform. In Proceedings of SWCL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>B Baldwin</author>
</authors>
<title>Algorithms for scoring co-reference chains.</title>
<date>1998</date>
<booktitle>In Proceedings of the First International Conference on Language Resources and Evaluation Workshop on Linguistic co-reference.</booktitle>
<contexts>
<context position="12662" citStr="Bagga and Baldwin, 1998" startWordPosition="2080" endWordPosition="2083">mance in training data. 4 Experimental Results The dataset for Chinese personal name disambiguation task contains training data and testing data. The training data contains 32 query names. Every query name folder contains 100-300 news articles. Given the query name, all the documents are retrived by character-based matching from a collection of Xinhua news documents in a time span of fourteen years. The testing data contains 25 query names. Two threshold values as terminating conditions are obtained from training data. They are 0.4 and 0.5. For evaluation, we use P-IP score and B-cubed score (Bagga and Baldwin, 1998). Table 2 &amp; Table 3 show the official evaluation results. Table 2: Official Results for P-IP score Threshold P-IP P IP F score 0.4 88.32 94.9 91.15 0.5 91.3 91.77 91.18 Table 3: Official Results for B-Cubed score Threshold B-Cubed Precision Recall F score 0.4 83.68 92.23 86.94 0.5 87.87 87.49 86.84 Besides the formal evaluation, the organizer also provide a diagnosis test designed to explore the relationship between Chinese word segmentation and personal name disambiguation. That means the query names in the documents are segmented correctly by manual work. Table 4 &amp; Table 5 show the diagnosis</context>
<context position="4236" citStr="Bagga &amp; Baldwin (1998)" startWordPosition="695" endWordPosition="698">d from training data. Our system produces pretty good results in the final evaluation. The remainder of this paper is organized as follows. Section 2 introduces related work. Section 3 gives a detailed description about our pipeline approach. It includes preprocessing, unrelated documents discarding, Chinese personal name extension and document clustering. Section 4 presents the experimental results. The conclusions are given in Section 5. 2 Related Work Several important studies have tried to solve the task introduced in the previous section. Most of them treated it as an clustering problem. Bagga &amp; Baldwin (1998) first selected tokens from local context as features to perform intra-document coreference resolution. Mann &amp; Yarowsky (2003) extracted local biographical information as features. Niu et al. (2004) used relation extraction results in addition to local context features and get a perfect results. Al-Kamha and Embley (2004) clustered search results with feature set including attributes, links and page similarities. In recent years, this problem has attracted a great deal of attention from many research institutes. Ying Chen et al. (2009) used a Web 1T 5-gram corpus released by Google to extract </context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Bagga, Amit and B. Baldwin. 1998. Algorithms for scoring co-reference chains. In Proceedings of the First International Conference on Language Resources and Evaluation Workshop on Linguistic co-reference.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>