<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003012">
<title confidence="0.97967">
Different Sense Granularities for Different Applications
</title>
<author confidence="0.942327">
Martha Palmer, Olga Babko-Malaya, Hoa Trang Dang
</author>
<affiliation confidence="0.992826">
University of Pennsylvania
</affiliation>
<email confidence="0.998847">
{mpalmer/malayao/htd}@linc.cis.upenn.edu
</email>
<sectionHeader confidence="0.998599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999986909090909">
This paper describes an hierarchical approach
to WordNet sense distinctions that provides
different types of automatic Word Sense Dis-
ambiguation (WSD) systems, which perform
at varying levels of accuracy. For tasks where
fine-grained sense distinctions may not be es-
sential, an accurate coarse-grained WSD sys-
tem may be sufficient. The paper discusses the
criteria behind the three different levels of
sense granularity, as well as the machine learn-
ing approach used by the WSD system.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998415410526316">
The difficulty of finding consistent criteria for making
sense distinctions has been thoroughly attested to in the
literature (Kilgarriff, ‘97, Hanks, ’00). Difficulties have
been found with truth-theoretical criteria, linguistic crite-
ria and definitional criteria (Sparck-Jones, ‘86, Geer-
aerts, ‘93). In spite of the proliferation of dictionaries,
there is no methodology by which two lexicographers
working independently are guaranteed to derive the same
set of distinctions for a given word, with objects and
events vying for which is the most difficult to character-
ize (Cruse, ‘86, Apresjan, ‘74, Pustejovsky, ’91, ‘95).
On the other hand, accurate Word Sense Disambiguation
(WSD) could significantly improve the precision of In-
formation Retrieval by ensuring that the senses of verbs
in the retrieved documents match the sense of the verb in
the query. For example, the two queries What do you
call a successful movie? and Whom do you call for a
successful movie? submitted to AskJeeves both retrieve
the same set of documents, even though they are asking
quite different questions, referencing very different
senses of call. The documents retrieved are also not very
relevant, again because they do not distinguish which
matches contain relevant senses and which do not.
Tips on Being a Successful Movie Vampire ... I shall
call the police.
Successful Casting Call &amp; Shoot for ̏Clash of Em-
pires&amp;quot; ... thank everyone for their participation in the
making of yesterday&apos;s movie.
Demme&apos;s casting is also highly entertaining, although I
wouldn&apos;t go so far as to call it successful. This movie&apos;s
resemblance to its predecessor is pretty vague...
VHS Movies: Successful Cold Call Selling: Over 100
New Ideas, Scripts, and Examples from the Nation&apos;s
Foremost Sales Trainer.
The two senses of call in the two queries can be easily
distinguished by their differing predicate-argument
structures. They are also separate senses in WordNet,
but WordNet has an additional 26 senses for call, and the
current best performance of an automatic Word Sense
Disambiguation system this type of polysemous verb is
only 60.2% (Dang and Palmer, 2002). Is it possible that
sense distinctions that are less fine-grained than Word-
Net’s distinctions could be made more reliably, and
could still benefit this type of NLP application?
The idea of underspecification as a solution to WSD has
been proposed in Buitelaar 2000 (among others), who
pointed out that for some applications, such as document
categorization, information retrieval, and information
extraction it may be sufficient to know if a given word
belongs to a certain class of WordNet senses or under-
specified sense. On the other hand, there is evidence that
machine translation of languages as diverse as Chinese
and English will require all of the fine-grained sense
distinctions that WordNet is capable of providing, and
even more (Ng, et al 2003, Palmer, et. al., to appear).
An hierarchical approach to verb senses, of the type dis-
cussed in this paper, presents obvious advantages for the
problem of word sense disambiguation. The human an-
notation task is simplified, since there are fewer choices
at each level and clearer distinctions between them. The
automated systems can combine training data from
closely related senses to overcome the sparse data prob-
lem, and both humans and systems can back off to a
more coarse-grained choice when fine-grained choices
prove too difficult.
The approach to verb senses presented in this paper as-
sumes three different levels of sense distinctions: Prop-
Bank Framesets, WordNet groupings, and WordNet
senses. In a project for the semantic annotation of predi-
cate-argument structure, PropBank, we have made
coarse-grained sense distinctions for the 700 most
polysemous verbs in the Penn TreeBank (Kingsbury and
Palmer, ’02). These distinctions are based primarily on
different subcategorization frames that require different
argument label annotations. In a separate project, as dis-
cussed in Palmer et al 2004, we have grouped
SENSEVAL-2 verb senses (which came from WordNet
1.7). These manual groupings were shown to reconcile a
substantial portion of the manual and automatic tagging
disagreements, showing that many of these disagree-
ments are fairly subtle (Palmer, et.al., ’04).
The tree levels of sense distinctions form a continuum of
granularity. Our criterion for the Framesets, being pri-
marily syntactic, is also the most clear cut. These distinc-
tions are based primarily on usages of a verb that have
different numbers of predicate-arguments, however they
also separate verb senses on semantic grounds, if these
senses are not closely related. Sense groupings provide
an intermediate level of hierarchy, where groups are
distinguished by more fine-grained criteria. Both
Frameset and grouping distinctions can be made consis-
tently by humans and systems (over 90% accuracy for
Framesets and 82% for groupings) and are surprisingly
compatible; 95% of our groups map directly onto a sin-
gle PropBank sense.
</bodyText>
<sectionHeader confidence="0.981326" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.963788">
2.1 Propbank
</subsectionHeader>
<bodyText confidence="0.99561002173913">
PropBank [Kingsbury &amp; Palmer, 2002] is an annotation
of the Wall Street Journal portion of the Penn Treebank
II [Marcus, 1994] with dependency structures (or `predi-
cate-argument&apos; structures), using sense tags for highly
polysemous words and semantic role labels for each de-
pendency. An important goal is to provide consistent
semantic role labels across different syntactic realiza-
tions of the same verb, as in the window in [ARG0 John]
broke [ARG, the window] and [ARG, The window] broke.
PropBank can provide frequency counts for (statistical)
analysis or generation components in a machine transla-
tion system, but provides only a shallow semantic analy-
sis in that the annotation is close to the syntactic
structure and each verb is its own predicate.
In addition to the annotated corpus, PropBank provides a
lexicon that lists, for each broad meaning of each anno-
tated verb, its Frameset, i.e., the possible arguments in
the predicate and their labels and all possible syntactic
realizations. The notion of ̏meaning&amp;quot; used is fairly
coarse-grained, and it is typically motivated from differ-
ing syntactic behavior. The Frameset also includes a
̏descriptor&amp;quot; field for each role which is intended for use
during annotation and as documentation, but which does
not have any theoretical standing. The collection of
Frameset entries for a verb is referred to as the verb&apos;s
frame. As an example of a PropBank entry, we give the
frame for the verb leave below. Currently, there are
frames for over 3,000 verbs, with a total of just over
4,300 Framesets described. Of these 3,000 verb frames,
only a small percentage 21.8 % (700) have more than
one Frameset, with less than 100 verbs with 4 or more.
The process of sense-tagging the PropBank corpus with
the Frameset tags has just been completed.
The criteria used for the Framesets are primarily syntac-
tic and clear cut. The guiding principle is that two verb
meanings are distinguished as different framesets if they
have distinct subcategorization frames. For example, the
verb ‘leave’ has 2 framesets with the following frames,
illustrated by the examples in (1) and (2):
Frameset 1: move away from
Arg0:entity leaving
Arg1:place left
Frameset 2: give
Arg0:giver / leaver
Arg1:thing given
Arg2:benefactive / given-to
</bodyText>
<listItem confidence="0.999146">
(1) John left the room.
(2) Mary left her daughter-in-law her pearls in her will
</listItem>
<subsectionHeader confidence="0.999269">
2.2 WordNet Sense Groupings
</subsectionHeader>
<bodyText confidence="0.999938356164384">
In a separate project, as part of Senseval tagging exer-
cises, we have developed a lexicon with another level of
coarse-grained distinctions, as described below.
The Senseval-1 workshop (Kilgarriff and Palmer, 2000)
provided convincing evidence that supervised automatic
systems can perform word sense disambiguation (WSD)
satisfactorily, given clear, consistent sense distinctions
and suitable training data. However, the Hector lexicon
that was used as the sense inventory was very small and
under proprietary constraints, and the question remained
whether it was possible to have a publicly available,
broad-coverage lexical resource for English and other
languages, with the requisite clear, consistent sense dis-
tinctions.
Subsequently, the Senseval-2 (Edmonds and Cotton,
2001) exercise was run, which included WSD tasks for
10 languages. A concerted effort was made to use exist-
ing WordNets as sense inventories because of their
widespread popularity and availability. Each language
had a choice between the lexical sample task and the all-
words task. The most polysemous words in the English
Lexical Sample task are the 29 verbs, with an average
polysemy of 16.28 senses using the pre-release version
of WordNet 1.7. Double blind annotation by two lin-
guistically trained annotators was performed on corpus
instances, with a third linguist adjudicating between in-
ter-annotator differences to create the “Gold Standard.”
The average inter-annotator agreement rate was only
71%, which is comparable to the 73% agreement for all
words in SemCor, with a much lower average polysemy.
However, a comparison of system performance on words
of similar polysemy in Senseval-1 and Senseval-2
showed very little difference in accuracy (Palmer et al.,
submitted). In spite of the lower inter-annotator agree-
ment figures for Senseval-2, the double blind annotation
and adjudication provided a reliable enough filter to en-
sure consistently tagged data with WordNet senses.
Even so, the high polysemy of the WordNet 1.7 entries
on average poses a challenge for automatic word sense
disambiguation. In addition, WordNet only gives a flat
listing of alternative senses, unlike most standard dic-
tionaries which are more structured and often provide
hierarchical entries. To address this lack, the verbs were
grouped by two or more people, with differences being
reconciled, and the sense groups were used for coarse-
grained scoring of the systems.
The criteria used for groupings included syntactic and
semantic ones. Syntactic structure performed two dis-
tinct functions in our groupings. Recognizable alterna-
tions with similar corresponding predicate-argument
structures were often a factor in choosing to group
senses together, as in the Levin classes and PropBank,
whereas distinct subcategorization frames were also of-
ten a factor in putting senses in separate groups. Fur-
thermore, senses were grouped together if they were
more specialized versions of a general sense. The se-
mantic criteria for grouping senses separately included
differences in semantic classes of arguments (abstract
versus concrete, animal versus human, animacy versus
inanimacy, different instrument types...), differences in
the number and type of arguments (often reflected in the
subcategorization frame as discussed above), differences
in entailments (whether an argument refers to a created
entity or a resultant state), differences in the type of
event (abstract, concrete, mental, emotional...), whether
there is a specialized subject domain, etc.
Senseval-2 verb inter-annotator disagreements were re-
duced by more than a third when evaluated against the
groups, from 29% to 18%, and by over half in a separate
study, from 28% to 12%. A similar number of random
groups provided almost no benefit to the inter-annotator
agreement figures (74% instead of 71%), confirming the
greater coherence of the manual groupings.
</bodyText>
<sectionHeader confidence="0.658303" genericHeader="method">
3 Mapping of Sense Groups to Framesets
</sectionHeader>
<bodyText confidence="0.999944055555556">
Groupings of senses for Senseval-2, as discussed above,
use both syntactic and semantic criteria. Propbank, on
the other hand, uses mostly syntactic cues to divide verb
senses into framesets. As a result, framesets are more
general than sense-groups and usually incorporate sev-
eral sense groups. We have been investigating whether
or not the groups developed for SENSEVAL-2 can provide
an intermediate level of hierarchy in between the Prop-
Bank Framesets and the WN 1.7 senses, and our initial
results are promising. Based on our existing WN 1.7
tags and frameset tags of the Senseval2 verbs in the Penn
TreeBank, 95% of the verb instances map directly from
sense groups to framesets, with each frameset typically
corresponding to two or more sense groups, as illustrated
by the tables 1-4 for the verbs ‘serve’, ‘leave’, ‘pull’, and
‘see’1 below.
As the tables 1-4 illustrate, the criteria used to split the
Framesets into groups are as follows:
</bodyText>
<listItem confidence="0.608576">
1) Syntactic Frames. Most verb senses which allow syn-
tactic alternations (such as transitive/inchoative, unspeci-
fied object deletion, etc) are analyzed as one sense
group. However, in some cases, as illustrated by the verb
leave, intransitive and transitive uses are distinguished as
different sense groups:
</listItem>
<figureCaption confidence="0.624833">
Group 1: DEPART (Ship leaves at midnight)
Group 2: LEAVE BEHIND (She left a mess.)
</figureCaption>
<bodyText confidence="0.9993518">
The DEPART sense of the verb can be used transitively if
the object specifies the place of departure. The LEAVE
BEHIND sense is more general and allows syntactic varia-
tion as well as different semantic types of NPs. In Prop-
Bank, these groups are unified as one frameset (Frameset
</bodyText>
<footnote confidence="0.85902575">
1 MOVE AWAY FROM).
1 All these verbs have one or more additional framesets, which
correspond to one group or sense, and therefore are not in-
cluded here
</footnote>
<table confidence="0.999913125">
Frameset Senseval-2 Groupings Examples from WordNet
serve 01: Act, work GROUP 1: His freedom served him well
Roles: WN1 (function) The scandal served to increase his popularity
Arg0:worker WN3(contribute to) Nothing else will serve
Arg1:job, project WN12 (answer)
Arg2:employer
GROUP 2: She served in Congress
WN2 (do duty) She served in Vietnam
WN13 (do military service)
GROUP 5: She served the art of music
WN7 (devote one’s efforts) May I serve you?
WN10 (attend to)
GROUP 3: The garage served to shelter horses
WN4 (be used by) Art serves commerce
WN8 (serve well) Male animals serve the females for breeding
WN14 (service) purposes
</table>
<tableCaption confidence="0.998617">
Table 1. Frameset serve 01.
</tableCaption>
<table confidence="0.999369105263158">
Frameset Senseval-2 Groupings Examples from WordNet
leave 02: Move away GROUP 2: She left a mess
from WN2 (leave behind) He left six children I left my keys
Roles: WN12 (be survived by)
Arg0:entity leaving WN14 (forget)
Arg1:thing left
Arg2 :attribute / sec-
ondary predication
GROUP 1: The ship leaves at midnight
WN1 (go away) Leave the room
WN5 (exit, go out) The teenager left home
WN8 (depart)
GROUP 3: The inflation left them penniless
WN3 (to act) Her blood left a stain on the napkin
WN7 (result in)
SINGLETON Leave it as is
WN4 (leave behind)
SINGLETON Leave lots of time for the trip
WN6 (allow for, provide)
</table>
<tableCaption confidence="0.999309">
Table 2. Frameset leave 02.
</tableCaption>
<listItem confidence="0.720643">
2. Optional Arguments. In PropBank verbs of manner
</listItem>
<bodyText confidence="0.996471">
of motion and verbs of directed motion are usually
grouped into one frameset. For example, one of the
framesets of the verb pull (TRY) TO CAUSE MOTION
unifies the following two group senses:
</bodyText>
<figureCaption confidence="0.56159">
Group 1: MOVE ALONG (pull a sled)
Group 2: MOVE INTO A CERTAIN DIRECTION (The van
pulled up)
</figureCaption>
<bodyText confidence="0.966080333333333">
Although the frame for the frameset 1 of the verb pull
has a ‘direction’ argument, this argument does not
have to be present (or implied), and verbs with this
frame can also be understood as verbs of manner of
motion in PropBank.
3) Syntactic variation of arguments. Syntactic varia-
tion in objects can also be used to distinguish sense
groups, but are not taken into consideration for distin-
guishing framesets. Here both noun phrases and sen-
</bodyText>
<table confidence="0.999602">
Frameset Senseval-2 Groupings Examples from WordNet
pull.01: try to cause motion GROUP 1: Pull a sled
Roles: WN1 (draw) Pull the rope
Arg0:puller WN4(apply force) A declining dollar pulled down the export figures
Arg1:thing pulled WN9 (cause to move) Pull the oars
Arg2: direction or predication WN10 (operate) Pull the ball
Arg3:extent, distance moved WN13 (hit)
GROUP 2: The ad pulled in many potential customers
WN2 (attract) Pull the cooked chicken into strips
WN12 (rip)
GROUP 3: The car pulls to the right
WN3 (move) Pull the car over
WN7 (steer)
GROUP 4: The mugger pulled a knife on his victim
WN6 (pull out) Pull weeds
WN15 (extract) Pull the old soup cans from the shelf
WN17(take away)
</table>
<tableCaption confidence="0.973678">
Table 3. Frameset pull 01.
</tableCaption>
<table confidence="0.999885052631579">
Frameset Senseval-2 Groupings Examples from WordNet
see.01: view GROUP 1: Can you the bird?
Roles: WN1 (perceive by sight) See a movie
Arg0:viewer WN7 (watch) The camera saw the burglary
Arg1:thing viewed WN19 (observe as if with an eye) I must see your passport
Arg2:secondary attribute WN20 (examine)
GROUP 3: I want to see the results
WN3 (witness) I see that you have been promoted
WN6 (learn)
GROUP 4: I don’t see the situation quite as negatively
WN5 (consider) What message do you see in this letter?
WN24 (interpret)
GROUP 5: See whether it works
WN8 (determine) See that the curtains are closed
WN10 (check) Could you see about lunch?
WN14 (attend)
GROUP 6: You should see a lawyer
WN11 (see a professional) The doctor will see you now
WN15 (receive as a guest)
</table>
<tableCaption confidence="0.99976">
Table 4. Frameset see 01.
</tableCaption>
<bodyText confidence="0.9957994">
tential complements are contained in the same frame-
set. These could also be distinguished by the type of
event, a physical perception vs. an abstract or mental
perception, but these would also not distinguished by
PropBank.
</bodyText>
<figureCaption confidence="0.53854">
Group 1: PERCEIVE BY SIGHT (Can you see the bird?)
Group 5: DETERMINE, CHECK (See whether it works)
</figureCaption>
<bodyText confidence="0.880233285714286">
4) Semantic classes of arguments. Differences in se-
mantic classes of arguments, such as ANIMACY versus
INANIMACY, are also not considered for distinguishing
framesets. The verb serve, for example, has the follow-
ing group senses, the second of which requires an
ANIMATE agent, which are unified as one frameset in
PropBank:
</bodyText>
<figureCaption confidence="0.7834205">
Group 1: FUNCTION (His freedom served him well)
Group 2: WORK (He served in Congress)
</figureCaption>
<bodyText confidence="0.999483363636363">
Most of the criteria which are used to split Framesets
into groupings, as the tables above illustrate, are se-
mantic. These distinctions, although more fine-grained
than Framesets, are still more easily distinguished than
WordNet senses.
Mismatches between Framesets and groupings usually
occur for the following two reasons. First, some senses
can be missing in the PropBank, if they do not occur in
the corpus. Second, given that PropBank is an annota-
tion of the Wall Street Journal, it often distinguishes
obscure financial senses of the verb as separate senses.
</bodyText>
<sectionHeader confidence="0.970057" genericHeader="method">
4 Experiments with Automatic WSD
</sectionHeader>
<bodyText confidence="0.980561">
We have also been investigating the suitability of these
distinctions for training automatic Word Sense
Disambiguation systems. The system that we used to
tag verbs with their frameset is the same maximum
entropy system as that of Dang and Palmer (2002),
including both topical and local features. Topical
features looked for the presence of keywords occurring
anywhere in the sentence and any surrounding
sentences provided as context (usually one or two
sentences). The set of keywords is specific to each
lemma to be disambiguated, and is determined
automatically from training data so as to minimize the
entropy of the probability of the senses conditioned on
the keyword.
The local features for a verb w in a particular sentence
tend to look only within the smallest clause containing
w. They include collocational features requiring no
linguistic prepro essing beyond part-of-speech tagging
(1), syntactic features that capture relations
between the verb and its complements (2-4), and se-
mantic features that incorporate information about
noun classes for objects (5-6):
1) the word w, the part of speech of w, and
words at positions -2, -1, +1, +2, relative to w
</bodyText>
<listItem confidence="0.670597875">
2) whether or not the sentence is passive
3) whether there is a subject, direct object, indi-
rect object, or clausal complement (a comple-
ment whose node label is S in the parse tree)
4) the words (if any) in the positions of subject,
direct object, indirect object, particle, preposi-
tional complement (and its object)
5) a Named Entity tag (PERSON,
</listItem>
<bodyText confidence="0.996354285714286">
ORGANIZATION, LOCATION) for proper
nouns appearing in (4).
The system performed well on the English verbs in
Senseval-2, achieving an accuracy of 60.2% when tag-
ging verbs with their fine-grained WordNet senses, and
70.2% when tagging with the more coarse-grained
sense groups.
</bodyText>
<table confidence="0.996211047619048">
Verb Framesets Instances Accuracy
call 11 522 0.835
carry 4 195 0.933
develop 2 240 0.938
draw 3 94 0.926
dress 3 15 0.800
drive 2 99 0.808
keep 5 136 0.919
leave 3 147 0.762
live 4 125 0.888
play 5 98 0.806
pull 6 88 0.784
see 2 187 0.995
serve 2 150 0.967
strike 10 59 0.610
train 2 17 0.941
treat 2 51 0.863
turn 14 141 0.638
use 2 820 0.988
wash 2 8 0.875
work 7 398 0.955
</table>
<tableCaption confidence="0.998681">
Table 5. Frameset tagging results
</tableCaption>
<bodyText confidence="0.993522315789474">
6) all possible WordNet synsets and hypernyms
for the nouns appearing in (4).
For frameset tagging, we collected a total of 3590 in-
stances of 20 verbs in the PropBank corpus that had
been annotated with their framesets. The verbs all had
more than one possible frameset and were a subset of
the ones used for the English lexical sample task of
Senseval-2. Local features for frameset taging were
extracted using the gold-standard part-of-speech tags
and bracketing of the Penn Treebank. Table 5 shows
the number of framesets, the number of instances, and
the system accuracy for each verb using 10-fold cross-
validation. The overall accuracy of our automatic
frameset tagging was 90.0%, compared to a baseline
accuracy of 73.5% if verbs are tagged with their most
frequent frameset. While the data is only a subset of
that used in Senseval-2, it is clear that framesets can be
much more reliably tagged than fine-grained WordNet
senses and even sense groups.
</bodyText>
<sectionHeader confidence="0.97232" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.999984888888889">
This paper described an hierarchical approach to
WordNet sense distinctions that provided different
types of automatic Word Sense Disambiguation (WSD)
systems, which perform at varying levels of accuracy.
We have described three different levels of sense
granularity, with PropBank Framesets being the most
syntactic, the most coarse-grained, and most easily
reproduced. A set of manual groupings devised for
Senseval2 provides a middle level of granularity that
mediates between Framesets and WordNet. For tasks
where fine-grained sense distinctions may not be essen-
tial such as an AskJeeves information retrieval task, an
accurate coarse-grained WSD system such as our
Frameset tagger may be sufficient. There is evidence,
however, that machine translation of languages as di-
verse as Chinese and English might require all of the
fine-grained sense distinctions of WordNet, and even
more (Ng, et al 2003, Palmer, et. al., to appear).
</bodyText>
<sectionHeader confidence="0.999233" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999738035714286">
Apresjan, J. D. .(1974) Regular polysemy, Linguistics,
142:5—32.
Atkins, S. (1993) Tools for computer-aided corpus
lexicography: The Hector Project. Actu Linguis-
tica Hunguricu, 41:5-72.
Buitelaar, P.P (2000). Reducing Lexical Semantic
Complexity with Systematic Polysemous Classes
and Underspecification. In Poceedings of the
ANLP Workshop on Syntactic and Semantic Com-
plexity in NLP Systems. Seattle, WA.
Cruse, D. A., (1986), Lexical Semantics, Cambridge
University Press, Cambridge, UK, 1986.
Dang, H. T. and Palmer, M., (2002). Combining Con-
textual Features for Word Sense Disambiguation.
In Proceedings of the Workshop on Word Sense
Disambiguation: Recent Successes and Future Di-
rections, Philadelphia, Pa.
Edmonds, P. and Cotton, S. (2001). SENSEVAL-2:
Overview. In Proceedings of SENSEVAL-2: Sec-
ond International Workshop on Evaluating Word
Sense Disambiguation Systems, ACL-SIGLEX,
Toulouse, France.
Hanks, P., (2000), Do word meanings exist? Com-
puters and the Humanities, Special Issue on
SENSEVAL, 34(1-2).
Geeraerts, D., (1993), Vagueness&apos;s puzzles, polysemy&apos;s
vagaries, Cognitive Linguistics, 4.
Kilgarriff, A., (1997), I don&apos;t believe in word senses,
Computers and the Humanities, 31(2).
Kilgarriff, A. and Palmer, M., (2000), Introduction to
the special issue on Senseval, Computers and the
Humanities, 34(1-2):1-13.
Kingsbury, P., and Palmer, M, (2002), From TreeBank
to PropBank, Third International Conference on
Language Resources and Evaluation, LREC-02,
Las Palmas, Canary Islands, Spain, May 28- June
3.
Marcus, M, (1994), The Penn TreeBank: A revised
corpus design for extracting predicate argument
structure, In Proceedings of the ARPA Human
Language Technology Workshop, Princeton, NJ.
Ng, H. T., &amp; Wang, B., &amp; Chan, Y. S. (2003).
Exploiting Parallel Texts for Word Sense Disam-
biguation: An Empirical Study. In the Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL-03). Sapporo,
Japan, July.
Palmer, M., Dang, H. T., and Fellbaum, C., (to appear,
2004), Making fine-grained and coarse-grained
sense distinctions, both manually and automati-
cally, under revision for Natural Language Engi-
neering.
Pustejovsky, J. (1991) The Generative Lexicon, in
Computational Linguistics 17(4).
Pustejovsky, J. (1995) The Generative Lexicon, Cam-
bridge, MIT Press, Mass.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.946710">
<title confidence="0.997647">Different Sense Granularities for Different Applications</title>
<author confidence="0.999684">Martha Palmer</author>
<author confidence="0.999684">Olga Babko-Malaya</author>
<author confidence="0.999684">Hoa Trang</author>
<affiliation confidence="0.999781">University of Pennsylvania</affiliation>
<email confidence="0.999802">{mpalmer/malayao/htd}@linc.cis.upenn.edu</email>
<abstract confidence="0.995790833333333">This paper describes an hierarchical approach to WordNet sense distinctions that provides different types of automatic Word Sense Disambiguation (WSD) systems, which perform at varying levels of accuracy. For tasks where fine-grained sense distinctions may not be essential, an accurate coarse-grained WSD system may be sufficient. The paper discusses the criteria behind the three different levels of sense granularity, as well as the machine learning approach used by the WSD system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J D Apresjan</author>
</authors>
<title>Regular polysemy, Linguistics,</title>
<date>1974</date>
<pages>142--5</pages>
<marker>Apresjan, 1974</marker>
<rawString>Apresjan, J. D. .(1974) Regular polysemy, Linguistics, 142:5—32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Atkins</author>
</authors>
<title>Tools for computer-aided corpus lexicography: The Hector Project. Actu Linguistica Hunguricu,</title>
<date>1993</date>
<pages>41--5</pages>
<marker>Atkins, 1993</marker>
<rawString>Atkins, S. (1993) Tools for computer-aided corpus lexicography: The Hector Project. Actu Linguistica Hunguricu, 41:5-72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P P Buitelaar</author>
</authors>
<title>Reducing Lexical Semantic Complexity with Systematic Polysemous Classes and Underspecification.</title>
<date>2000</date>
<booktitle>In Poceedings of the ANLP Workshop on Syntactic and Semantic Complexity in NLP Systems.</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="3081" citStr="Buitelaar 2000" startWordPosition="471" endWordPosition="472">es of call in the two queries can be easily distinguished by their differing predicate-argument structures. They are also separate senses in WordNet, but WordNet has an additional 26 senses for call, and the current best performance of an automatic Word Sense Disambiguation system this type of polysemous verb is only 60.2% (Dang and Palmer, 2002). Is it possible that sense distinctions that are less fine-grained than WordNet’s distinctions could be made more reliably, and could still benefit this type of NLP application? The idea of underspecification as a solution to WSD has been proposed in Buitelaar 2000 (among others), who pointed out that for some applications, such as document categorization, information retrieval, and information extraction it may be sufficient to know if a given word belongs to a certain class of WordNet senses or underspecified sense. On the other hand, there is evidence that machine translation of languages as diverse as Chinese and English will require all of the fine-grained sense distinctions that WordNet is capable of providing, and even more (Ng, et al 2003, Palmer, et. al., to appear). An hierarchical approach to verb senses, of the type discussed in this paper, </context>
</contexts>
<marker>Buitelaar, 2000</marker>
<rawString>Buitelaar, P.P (2000). Reducing Lexical Semantic Complexity with Systematic Polysemous Classes and Underspecification. In Poceedings of the ANLP Workshop on Syntactic and Semantic Complexity in NLP Systems. Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Cruse</author>
</authors>
<title>Lexical Semantics,</title>
<date>1986</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK,</location>
<marker>Cruse, 1986</marker>
<rawString>Cruse, D. A., (1986), Lexical Semantics, Cambridge University Press, Cambridge, UK, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Dang</author>
<author>M Palmer</author>
</authors>
<title>Combining Contextual Features for Word Sense Disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop on Word Sense Disambiguation: Recent Successes and Future Directions,</booktitle>
<location>Philadelphia, Pa.</location>
<contexts>
<context position="2815" citStr="Dang and Palmer, 2002" startWordPosition="426" endWordPosition="429">ly entertaining, although I wouldn&apos;t go so far as to call it successful. This movie&apos;s resemblance to its predecessor is pretty vague... VHS Movies: Successful Cold Call Selling: Over 100 New Ideas, Scripts, and Examples from the Nation&apos;s Foremost Sales Trainer. The two senses of call in the two queries can be easily distinguished by their differing predicate-argument structures. They are also separate senses in WordNet, but WordNet has an additional 26 senses for call, and the current best performance of an automatic Word Sense Disambiguation system this type of polysemous verb is only 60.2% (Dang and Palmer, 2002). Is it possible that sense distinctions that are less fine-grained than WordNet’s distinctions could be made more reliably, and could still benefit this type of NLP application? The idea of underspecification as a solution to WSD has been proposed in Buitelaar 2000 (among others), who pointed out that for some applications, such as document categorization, information retrieval, and information extraction it may be sufficient to know if a given word belongs to a certain class of WordNet senses or underspecified sense. On the other hand, there is evidence that machine translation of languages </context>
<context position="18964" citStr="Dang and Palmer (2002)" startWordPosition="3014" endWordPosition="3017">senses. Mismatches between Framesets and groupings usually occur for the following two reasons. First, some senses can be missing in the PropBank, if they do not occur in the corpus. Second, given that PropBank is an annotation of the Wall Street Journal, it often distinguishes obscure financial senses of the verb as separate senses. 4 Experiments with Automatic WSD We have also been investigating the suitability of these distinctions for training automatic Word Sense Disambiguation systems. The system that we used to tag verbs with their frameset is the same maximum entropy system as that of Dang and Palmer (2002), including both topical and local features. Topical features looked for the presence of keywords occurring anywhere in the sentence and any surrounding sentences provided as context (usually one or two sentences). The set of keywords is specific to each lemma to be disambiguated, and is determined automatically from training data so as to minimize the entropy of the probability of the senses conditioned on the keyword. The local features for a verb w in a particular sentence tend to look only within the smallest clause containing w. They include collocational features requiring no linguistic </context>
</contexts>
<marker>Dang, Palmer, 2002</marker>
<rawString>Dang, H. T. and Palmer, M., (2002). Combining Contextual Features for Word Sense Disambiguation. In Proceedings of the Workshop on Word Sense Disambiguation: Recent Successes and Future Directions, Philadelphia, Pa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Edmonds</author>
<author>S Cotton</author>
</authors>
<title>SENSEVAL-2: Overview.</title>
<date>2001</date>
<booktitle>In Proceedings of SENSEVAL-2: Second International Workshop on Evaluating Word Sense Disambiguation Systems, ACL-SIGLEX,</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="8860" citStr="Edmonds and Cotton, 2001" startWordPosition="1370" endWordPosition="1373">low. The Senseval-1 workshop (Kilgarriff and Palmer, 2000) provided convincing evidence that supervised automatic systems can perform word sense disambiguation (WSD) satisfactorily, given clear, consistent sense distinctions and suitable training data. However, the Hector lexicon that was used as the sense inventory was very small and under proprietary constraints, and the question remained whether it was possible to have a publicly available, broad-coverage lexical resource for English and other languages, with the requisite clear, consistent sense distinctions. Subsequently, the Senseval-2 (Edmonds and Cotton, 2001) exercise was run, which included WSD tasks for 10 languages. A concerted effort was made to use existing WordNets as sense inventories because of their widespread popularity and availability. Each language had a choice between the lexical sample task and the allwords task. The most polysemous words in the English Lexical Sample task are the 29 verbs, with an average polysemy of 16.28 senses using the pre-release version of WordNet 1.7. Double blind annotation by two linguistically trained annotators was performed on corpus instances, with a third linguist adjudicating between inter-annotator </context>
</contexts>
<marker>Edmonds, Cotton, 2001</marker>
<rawString>Edmonds, P. and Cotton, S. (2001). SENSEVAL-2: Overview. In Proceedings of SENSEVAL-2: Second International Workshop on Evaluating Word Sense Disambiguation Systems, ACL-SIGLEX, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hanks</author>
</authors>
<title>Do word meanings exist?</title>
<date>2000</date>
<journal>Computers and the Humanities, Special Issue on SENSEVAL,</journal>
<pages>34--1</pages>
<marker>Hanks, 2000</marker>
<rawString>Hanks, P., (2000), Do word meanings exist? Computers and the Humanities, Special Issue on SENSEVAL, 34(1-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Geeraerts</author>
</authors>
<title>Vagueness&apos;s puzzles, polysemy&apos;s vagaries,</title>
<date>1993</date>
<journal>Cognitive Linguistics,</journal>
<volume>4</volume>
<marker>Geeraerts, 1993</marker>
<rawString>Geeraerts, D., (1993), Vagueness&apos;s puzzles, polysemy&apos;s vagaries, Cognitive Linguistics, 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
</authors>
<title>I don&apos;t believe in word senses,</title>
<date>1997</date>
<journal>Computers and the Humanities,</journal>
<volume>31</volume>
<issue>2</issue>
<marker>Kilgarriff, 1997</marker>
<rawString>Kilgarriff, A., (1997), I don&apos;t believe in word senses, Computers and the Humanities, 31(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
<author>M Palmer</author>
</authors>
<title>Introduction to the special issue on</title>
<date>2000</date>
<booktitle>Senseval, Computers and the Humanities,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="8293" citStr="Kilgarriff and Palmer, 2000" startWordPosition="1293" endWordPosition="1296">ey have distinct subcategorization frames. For example, the verb ‘leave’ has 2 framesets with the following frames, illustrated by the examples in (1) and (2): Frameset 1: move away from Arg0:entity leaving Arg1:place left Frameset 2: give Arg0:giver / leaver Arg1:thing given Arg2:benefactive / given-to (1) John left the room. (2) Mary left her daughter-in-law her pearls in her will 2.2 WordNet Sense Groupings In a separate project, as part of Senseval tagging exercises, we have developed a lexicon with another level of coarse-grained distinctions, as described below. The Senseval-1 workshop (Kilgarriff and Palmer, 2000) provided convincing evidence that supervised automatic systems can perform word sense disambiguation (WSD) satisfactorily, given clear, consistent sense distinctions and suitable training data. However, the Hector lexicon that was used as the sense inventory was very small and under proprietary constraints, and the question remained whether it was possible to have a publicly available, broad-coverage lexical resource for English and other languages, with the requisite clear, consistent sense distinctions. Subsequently, the Senseval-2 (Edmonds and Cotton, 2001) exercise was run, which included</context>
</contexts>
<marker>Kilgarriff, Palmer, 2000</marker>
<rawString>Kilgarriff, A. and Palmer, M., (2000), Introduction to the special issue on Senseval, Computers and the Humanities, 34(1-2):1-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kingsbury</author>
<author>M Palmer</author>
</authors>
<title>From TreeBank to PropBank,</title>
<date>2002</date>
<booktitle>Third International Conference on Language Resources and Evaluation, LREC-02,</booktitle>
<location>Las Palmas, Canary Islands, Spain,</location>
<contexts>
<context position="5740" citStr="Kingsbury &amp; Palmer, 2002" startWordPosition="882" endWordPosition="885">istinctions are based primarily on usages of a verb that have different numbers of predicate-arguments, however they also separate verb senses on semantic grounds, if these senses are not closely related. Sense groupings provide an intermediate level of hierarchy, where groups are distinguished by more fine-grained criteria. Both Frameset and grouping distinctions can be made consistently by humans and systems (over 90% accuracy for Framesets and 82% for groupings) and are surprisingly compatible; 95% of our groups map directly onto a single PropBank sense. 2 Background 2.1 Propbank PropBank [Kingsbury &amp; Palmer, 2002] is an annotation of the Wall Street Journal portion of the Penn Treebank II [Marcus, 1994] with dependency structures (or `predicate-argument&apos; structures), using sense tags for highly polysemous words and semantic role labels for each dependency. An important goal is to provide consistent semantic role labels across different syntactic realizations of the same verb, as in the window in [ARG0 John] broke [ARG, the window] and [ARG, The window] broke. PropBank can provide frequency counts for (statistical) analysis or generation components in a machine translation system, but provides only a s</context>
</contexts>
<marker>Kingsbury, Palmer, 2002</marker>
<rawString>Kingsbury, P., and Palmer, M, (2002), From TreeBank to PropBank, Third International Conference on Language Resources and Evaluation, LREC-02, Las Palmas, Canary Islands, Spain, May 28- June 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
</authors>
<title>The Penn TreeBank: A revised corpus design for extracting predicate argument structure,</title>
<date>1994</date>
<booktitle>In Proceedings of the ARPA Human Language Technology Workshop,</booktitle>
<location>Princeton, NJ.</location>
<contexts>
<context position="5831" citStr="Marcus, 1994" startWordPosition="900" endWordPosition="901"> however they also separate verb senses on semantic grounds, if these senses are not closely related. Sense groupings provide an intermediate level of hierarchy, where groups are distinguished by more fine-grained criteria. Both Frameset and grouping distinctions can be made consistently by humans and systems (over 90% accuracy for Framesets and 82% for groupings) and are surprisingly compatible; 95% of our groups map directly onto a single PropBank sense. 2 Background 2.1 Propbank PropBank [Kingsbury &amp; Palmer, 2002] is an annotation of the Wall Street Journal portion of the Penn Treebank II [Marcus, 1994] with dependency structures (or `predicate-argument&apos; structures), using sense tags for highly polysemous words and semantic role labels for each dependency. An important goal is to provide consistent semantic role labels across different syntactic realizations of the same verb, as in the window in [ARG0 John] broke [ARG, the window] and [ARG, The window] broke. PropBank can provide frequency counts for (statistical) analysis or generation components in a machine translation system, but provides only a shallow semantic analysis in that the annotation is close to the syntactic structure and eac</context>
</contexts>
<marker>Marcus, 1994</marker>
<rawString>Marcus, M, (1994), The Penn TreeBank: A revised corpus design for extracting predicate argument structure, In Proceedings of the ARPA Human Language Technology Workshop, Princeton, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>B Wang</author>
<author>Y S Chan</author>
</authors>
<title>Exploiting Parallel Texts for Word Sense Disambiguation: An Empirical Study.</title>
<date>2003</date>
<booktitle>In the Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03).</booktitle>
<location>Sapporo, Japan,</location>
<contexts>
<context position="3572" citStr="Ng, et al 2003" startWordPosition="548" endWordPosition="551">nefit this type of NLP application? The idea of underspecification as a solution to WSD has been proposed in Buitelaar 2000 (among others), who pointed out that for some applications, such as document categorization, information retrieval, and information extraction it may be sufficient to know if a given word belongs to a certain class of WordNet senses or underspecified sense. On the other hand, there is evidence that machine translation of languages as diverse as Chinese and English will require all of the fine-grained sense distinctions that WordNet is capable of providing, and even more (Ng, et al 2003, Palmer, et. al., to appear). An hierarchical approach to verb senses, of the type discussed in this paper, presents obvious advantages for the problem of word sense disambiguation. The human annotation task is simplified, since there are fewer choices at each level and clearer distinctions between them. The automated systems can combine training data from closely related senses to overcome the sparse data problem, and both humans and systems can back off to a more coarse-grained choice when fine-grained choices prove too difficult. The approach to verb senses presented in this paper assumes </context>
</contexts>
<marker>Ng, Wang, Chan, 2003</marker>
<rawString>Ng, H. T., &amp; Wang, B., &amp; Chan, Y. S. (2003). Exploiting Parallel Texts for Word Sense Disambiguation: An Empirical Study. In the Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03). Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>H T Dang</author>
<author>C Fellbaum</author>
</authors>
<title>Making fine-grained and coarse-grained sense distinctions, both manually and automatically, under revision for Natural Language Engineering.</title>
<date>2004</date>
<contexts>
<context position="4676" citStr="Palmer et al 2004" startWordPosition="718" endWordPosition="721">hoice when fine-grained choices prove too difficult. The approach to verb senses presented in this paper assumes three different levels of sense distinctions: PropBank Framesets, WordNet groupings, and WordNet senses. In a project for the semantic annotation of predicate-argument structure, PropBank, we have made coarse-grained sense distinctions for the 700 most polysemous verbs in the Penn TreeBank (Kingsbury and Palmer, ’02). These distinctions are based primarily on different subcategorization frames that require different argument label annotations. In a separate project, as discussed in Palmer et al 2004, we have grouped SENSEVAL-2 verb senses (which came from WordNet 1.7). These manual groupings were shown to reconcile a substantial portion of the manual and automatic tagging disagreements, showing that many of these disagreements are fairly subtle (Palmer, et.al., ’04). The tree levels of sense distinctions form a continuum of granularity. Our criterion for the Framesets, being primarily syntactic, is also the most clear cut. These distinctions are based primarily on usages of a verb that have different numbers of predicate-arguments, however they also separate verb senses on semantic groun</context>
</contexts>
<marker>Palmer, Dang, Fellbaum, 2004</marker>
<rawString>Palmer, M., Dang, H. T., and Fellbaum, C., (to appear, 2004), Making fine-grained and coarse-grained sense distinctions, both manually and automatically, under revision for Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
</authors>
<title>The Generative Lexicon,</title>
<date>1991</date>
<booktitle>in Computational Linguistics 17(4).</booktitle>
<marker>Pustejovsky, 1991</marker>
<rawString>Pustejovsky, J. (1991) The Generative Lexicon, in Computational Linguistics 17(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
</authors>
<title>The Generative Lexicon,</title>
<date>1995</date>
<publisher>MIT Press,</publisher>
<location>Cambridge,</location>
<marker>Pustejovsky, 1995</marker>
<rawString>Pustejovsky, J. (1995) The Generative Lexicon, Cambridge, MIT Press, Mass.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>