<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003012">
<title confidence="0.851503">
Committed Belief Annotation and Tagging
</title>
<author confidence="0.655205">
Mona T. Diab Lori Levin
</author>
<affiliation confidence="0.354492">
CCLS LTI
</affiliation>
<address confidence="0.41748">
Columbia U. CMU
</address>
<email confidence="0.987377">
mdiab@cs.columbia.edu lsl@cs.cmu.edu
</email>
<author confidence="0.942577">
Teruko Mitamura Owen Rambow
</author>
<sectionHeader confidence="0.5629395" genericHeader="abstract">
LTI CCLS
CMU Columbia U.
</sectionHeader>
<email confidence="0.994783">
teruko+@cs.cmu.edu rambow@ccls.columbia.edu
</email>
<author confidence="0.915984">
Vinodkumar Prabhakaran Weiwei Guo
</author>
<affiliation confidence="0.680405">
CS CS
Columbia U. Columbia U.
</affiliation>
<sectionHeader confidence="0.971954" genericHeader="introduction">
Abstract
</sectionHeader>
<bodyText confidence="0.999977678571429">
We present a preliminary pilot study of
belief annotation and automatic tagging.
Our objective is to explore semantic mean-
ing beyond surface propositions. We aim
to model people’s cognitive states, namely
their beliefs as expressed through linguis-
tic means. We model the strength of their
beliefs and their (the human) degree of
commitment to their utterance. We ex-
plore only the perspective of the author of
a text. We classify predicates into one of
three possibilities: committed belief, non
committed belief, or not applicable. We
proceed to manually annotate data to that
end, then we build a supervised frame-
work to test the feasibility of automati-
cally predicting these belief states. Even
though the data is relatively small, we
show that automatic prediction of a belief
class is a feasible task. Using syntactic
features, we are able to obtain significant
improvements over a simple baseline of
23% F-measure absolute points. The best
performing automatic tagging condition is
where we use POS tag, word type fea-
ture AlphaNumeric, and shallow syntac-
tic chunk information CHUNK. Our best
overall performance is 53.97% F-measure.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="method">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997336625">
As access to large amounts of textual informa-
tion increases, there is a strong realization that
searches and processing purely based on surface
words is highly limiting. Researchers in infor-
mation retrieval and natural language processing
(NLP) have long used morphological and (in a
more limited way) syntactic analysis to improve
access and processing of text; recently, interest has
grown in relating text to more abstract representa-
tions of its propositional meaning, as witnessed by
work on semantic role labeling, word sense disam-
biguation, and textual entailment. However, there
are more levels to “meaning” than just proposi-
tional content. Consider the following examples,
and suppose we find these sentences in the New
York Times:1
</bodyText>
<listItem confidence="0.992950333333333">
(1) a. GM will lay off workers.
b. A spokesman for GM said GM will lay off
workers.
c. GM may lay off workers.
d. The politician claimed that GM will lay
off workers.
e. Some wish GM would lay of workers.
f. Will GM lay off workers?
g. Many wonder if GM will lay off workers.
</listItem>
<bodyText confidence="0.8927905">
If we are searching text to find out whether GM
will lay off workers, all of the sentences in (1) con-
</bodyText>
<footnote confidence="0.927757666666667">
1In this paper, we concentrate on written communication,
and we use the terms reader and writer. However, nothing in
the approach precludes applying it to spoken communication.
</footnote>
<bodyText confidence="0.999830764705883">
tain the proposition LAYOFF(GM,WORKERS).
However, the six sentences clearly allow us very
different inferences about whether GM will lay off
workers or not. Supposing we consider the Times
a trustworthy news source, we would be fairly cer-
tain with (1a) and (1b). (1c) suggests the Times is
not certain about the layoffs, but considers them
possible. When reading (1d), we know that some-
one else thinks that GM will lay off workers, but
that the Times does not necessarily share this be-
lief. (1e), (1f), and (1g) do not tell us anything
about whether anyone believes whether GM will
lay off workers.
In order to tease apart what is happening, we
need to refine a simple IR-ish view of text as a
repository of propositions about the world. We use
two theories to aid us. The first theory is that in ad-
dition to facts about the world (GM will or will not
lay off workers), we have facts about people’s cog-
nitive states, and these cognitive states relate their
bearer to the facts in the world. (Though perhaps
there are only cognitive states, and no facts about
the world.) Following the literature in Artificial
Intelligence (Cohen and Levesque, 1990), we can
model cognitive state as beliefs, desires, and inten-
tions. In this paper, we are only interested in be-
liefs (and in distinguishing them from desires and
intentions). The second theory is that communi-
cation is intention-driven, and understanding text
actually means understanding the communicative
intention of the writer. Furthermore, communica-
tive intentions are intentions to affect the reader’s
cognitive state – his or her beliefs, desires, and/or
intentions. This view has been worked out in the
text generation and dialog community more than
in the text understanding community (Mann and
Thompson, 1987; Hovy, 1993; Moore, 1994).
In this paper we are interested in exploring the
following: we would like to recognize what the
text wants to make us believe about various peo-
ple’s cognitive states, including the speaker’s. As
mentioned, we are only interested in people’s be-
lief. In this view, the result of text processing is
not a list of facts about the world, but a list of facts
about different people’s cognitive states.
This paper is part of an on-going research effort.
The goals of this paper are to summarize a pilot
annotation effort, and to present the results of ini-
tial experiments in automatically extracting facts
about people’s beliefs from open domain running
text.
</bodyText>
<sectionHeader confidence="0.957743" genericHeader="method">
2 Belief Annotation
</sectionHeader>
<bodyText confidence="0.9705959">
We have developed a manual for annotating be-
lief, which we summarize here. For more de-
tailed information, we refer to the cited works. In
general, we are interested in the writer’s intention
as to making us believe that various people have
certain beliefs, desires, and intentions. We sim-
plify the annotation in two ways: we are only in-
teretsed in beliefs, and we are only interested in
the writer’s beliefs. This is not because we think
this is the only interesting information in text, but
we do this in order to obtain a manageable anno-
tation in our pilot study. Specifically, we annotate
whether the writer intends the reader to interpret
a stated proposition as the writer’s strongly held
belief, as a proposition which the writer does not
believe strongly (but could), or as a proposition
towards which the writer has an entirely differ-
ent cognitive attitude, such as desire or intention.
We do not annotate subjectivity (Janyce Wiebe and
Martin, 2004; Wilson and Wiebe, 2005), nor opin-
ion (for example: (Somasundaran et al., 2008)):
the nature of the proposition (opinion and type of
opinion, statement about interior world, external
world) is not of interest. Thus, this work is or-
thogonal to the extensive literature on opinion de-
tection. And we do not annotate truth: real-world
(encyclopedic) truth is not relevant.
We have three categories:
• Committed belief (CB): the writer indicates
in this utterance that he or she believes the
proposition. For example, GM has laid off
workers, or, even stronger, We know that GM
has laid off workers.
A subcase of committed belief concerns
propositions about the future, such as GM
will lay off workers. People can have equally
strong beliefs about the future as about the
past, though in practice probably we have
stronger beliefs about the past than about the
future.
</bodyText>
<listItem confidence="0.989764142857143">
• Non-committed belief (NCB): the writer
identifies the propositon as something which
he or she could believe, but he or she hap-
pens not to have a strong belief in. There are
two subcases. First, there are cases in which
the writer makes clear that the belief is not
strong, for example by using a modal auxil-
iary:2 GM may lay off workers. Second, in
reported speech, the writer is not signaling to
us what he or she believes about the reported
speech: The politician claimed that GM will
lay off workers. However, sometimes, we can
use the speech act verb to infer the writer’s
attitude,3 and we can use our own knowledge
</listItem>
<footnote confidence="0.794035">
2The annotators must distinguish epistemic and deontic
uses of modals.
3Some languages may also use grammatical devices; for
</footnote>
<bodyText confidence="0.99866025">
to infer the writer’s beliefs; for example, in
A GM spokesman said that GM will lay off
workers, we can assume that the writer be-
lieves that GM intends to lay off workers, not
just the spokesman. However, this is not part
of the annotation, and all reported speech is
annotated as NCB. Again, the issue of tense
is orthogonal.
</bodyText>
<listItem confidence="0.881124083333333">
• Not applicable (NA): for the writer, the
proposition is not of the type in which he or
she is expressing a belief, or could express a
belief. Usually, this is because the proposi-
tion does not have a truth value in this world
(be it in the past or in the future). This covers
expressions of desire (Some wish GM would
lay of workers), questions (Will GM lay off
workers? or Many wonder if GM will lay
off workers, and expressions of requirements
(GM is required to lay off workers or Lay off
workers!).
</listItem>
<bodyText confidence="0.998084333333333">
This sort of annotation is part of an annotation
of all “modalities” that a text may express. We
only annotate belief. A further complication is
that these modalities can be nested: one can ex-
press a belief about someone else’s belief, and one
may be strong and the other weak (I believe John
may believe that GM will lay off workers). At this
phase, we only annotate from the perspective of
the writer, i.e. what the writer of the text that is
being annotated believes.
The annotation units (annotatables) are, con-
ceptually, propositions as defined by PropBank
(Kingsbury et al., 2002). In practice, annotators
are asked to identify full lexical verbs (whether
in main or embedded clauses, whether finite or
non-finite). In predicative constructions (John is a
doctor/in the kitchen/drunk), we ask them to iden-
tify the nominal, prepositional, or adjectival head
rather than the form of to be, in order to also han-
dle small clauses (I think [John an idiot]).
The interest of the annotation is clear: we want
to be able to determine automatically from a given
text what beliefs we can ascribe to the writer,
and with what strengths he or she holds them.
Across languages, many different linguistic means
are used to denote this attitude towards an uttered
proposition, including syntax, lexicon, and mor-
phology. To our knowledge, no systematic empir-
ical study exists for English, and this annotation is
a step towards that goal.
example, in German, the choice between indicative mood and
subjunctive mood in reported speech can signal the writer’s
attitude.
</bodyText>
<sectionHeader confidence="0.999872" genericHeader="method">
3 Related Work
</sectionHeader>
<bodyText confidence="0.9999725">
The work of Roser et al. (2006) is, in many re-
spects, very similar to ours. In particular, they are
concerned with extracting information about peo-
ple’s beliefs and the strength of these beliefs from
text. However, their annotation is very different
from ours. They extend the TimeML annotation
scheme to include annotation of markers of belief
and strength of belief. For example, in the sen-
tence The Human Rights Committee regretted that
discrimination against women persisted in prac-
tice, TimeML identifies the events associated with
the verbs regret and persist, and then the extension
to the annotation adds the mark that there is a ”fac-
tive” link between the regret event and the persist
event, i.e., if we regret something, then we assume
the truth of that something. In contrast, in our
annotation, we directly annotate events with their
level of belief. In this example, we would annotate
persist as being a committed belief of the Human
Rights Committee (though in this paper we only
report on beliefs attributed to the writer). This dif-
ference is important, as in the annotation of Roser
et al. (2006), the annotator must analyze the situ-
ation and find evidence for the level of belief at-
tributed to an event. As a result, we cannot use
the annotation to discover how natural language
expresses level of belief. Our annotation is more
primitively semantic: we ask the annotators sim-
ply to annotate meaning (does X believe the event
takes place), as opposed to annotating the linguis-
tic structures which express meaning. As a conse-
quence of the difference in annotation, we cannot
compare our automatic prediction results to theirs.
Other related works explored belief systems in
an inference scenario as opposed to an intentional-
ity scenario. In work by (Ralf Krestel and Bergler,
2007; Krestel et al., 2008), the authors explore
belief in the context of news media exploring re-
ported speech where they track newspaper text
looking for elements indicating evidentiality. The
notion of belief is more akin to finding statements
that support or negate specific events with differ-
ent degrees of support. This is different from our
notion of committed belief in this work, since we
seek to make explicit the intention of the author or
the speaker.
</bodyText>
<sectionHeader confidence="0.992373" genericHeader="method">
4 Our Approach
</sectionHeader>
<subsectionHeader confidence="0.869404">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.9998934">
We create a relatively small corpus of English
manually annotated for the three categories: CB,
NCB, NA. The data covers different domains and
genres from newswire, to blog data, to email cor-
respondence, to letter correspondence, to tran-
scribed dialogue data. The data comprises 10K
words of running text. 70% of the data was dou-
bly annotated comprising 6188 potentially anno-
tatable tokens. Hence we had a 4 way manual clas-
sification in essence between NONE, CB, NCB,
and NA. Most of the confusions between NONE
and CB from both annotators, for 103 tokens.
The next point of disagreement was on NCB and
NONE for 48 tokens.They disagreed on NCB and
CB for 32 of the tokens. In general the interanno-
tator agreements were high as they agreed 95.8%
of the time on the annotatable and the exact belief
classification.4 Here is an example of a disagree-
ment between the two annotators, The Iraqi gov-
ernment has agreed to let Rep Tony Hall visit the
country next week to assess a humanitarian cri-
sis that has festered since the Gulf War of 1991
Hall’s office said Monday. One annotator deemed
“agreed” a CB while the other considered it an
NCB.
</bodyText>
<subsectionHeader confidence="0.997783">
4.2 Automatic approach
</subsectionHeader>
<bodyText confidence="0.999984677419355">
Once we had the data manually annotated and re-
vised, we wanted to explore the feasibility of au-
tomatically predicting belief states based on lin-
guistic features. We apply a supervised learning
framework to the problem of both identifying and
classifying a belief annotatable token in context.
This is a three way classification task where an
annotatable token is tagged as one of our three
classes: Committed Belief (CB), Non Committed
Belief (NCB), and Not Applicable (NA). We adopt
a chunking approach to the problem using an In-
side Outside Beginning (IOB) tagging framework
for performing the identification and classification
of belief tokens in context. For chunk tagging,
we use YamCha sequence labeling system.5 Yam-
Cha is based on SVM technology. We use the de-
fault parameter settings most importantly the ker-
nels are polynomial degree 2 with a c value of 0.5.
We label each sentence with standard IOB tags.
Since this is a ternary classification task, we have
7 different tags: B-CB (Beginning of a commit-
ted belief chunk), I-CB (Inside of a committed be-
lief chunk), B-NCB (Beginning of non commit-
ted belief chunk), I-NCB (Inside of a non com-
mitted belief chunk), B-NA (Beginning of a not
applicable chunk), I-NA (Inside a not applicable
chunk), and O (Outside a chunk) for the cases
that are not annotatable tokens. As an example
of the annotation, a sentence such as Hall said
he wanted to investigate reports from relief agen-
cies that a quarter of Iraqi children may be suffer-
</bodyText>
<footnote confidence="0.993501">
4This interannotator agreement number includes the
NONE category.
5http://www.tado-chasen.com/yamcha
</footnote>
<bodyText confidence="0.999677511627907">
ing from chronic malnutrition. will be annotated
as follows: {Hall O said B-CB he O wanted B-
NCB to B-NA investigate I-NA reports O from O
relief O agencies O that O a O quarter O of O
Iraqi O children O may O be O suffering B-NCB
from O chronic O malnutrition O.}
We experiment with some basic features and
some more linguistically motivated ones.
CXT: Since we adopt a sequence labeling
paradigm, we experiment with different window
sizes for context ranging from −/+2 tokens after
and before the token of interest to −/+5.
NGRAM: This is a character n-gram feature,
explicity representing the first and last character
ngrams of a word. In this case we experiment with
up to −/+4 characters of a token. This feature
allows us to capture implicitly the word inflection
morphology.
POS: An important feature is the Part-of-Speech
(POS) tag of the words. Most of the annotatables
are predicates but not all predicates in the text are
annotatables. We obtain the POS tags from the
TreeTagger POS tagger tool which is trained on
the Penn Treebank.6
ALPHANUM: This feature indicates whether
the word has a digit in it or not or if it is a non
alphanumeric token.
VerbType: We classify the verbs as to whether
they are modals (eg. may, might, shall, will,
should, can, etc.), auxilliaries (eg. do, be, have),7
or regular verbs. Many of our annotatables occur
in the vicinity of modals and auxilliaries. The list
of modals and auxilliaries is deterministic.
Syntactic Chunk (CHUNK): This feature ex-
plicitly models the syntactic phrases in which our
tokens occur. The possible phrases are shallow
syntactic representations that we obtain from the
TreeTagger chunker:8 ADJC (Adjective Chunk),
ADVC (Adverbial Chunk), CONJC (Conjunc-
tional Chunk), INTJ (Interjunctional Chunk), LST
(numbers 1, 2,3 etc), NC (Noun Chunk), PC
(Prepositional Chunk), PRT (off,out,up etc), VC
(Verb Chunk).
</bodyText>
<sectionHeader confidence="0.993202" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.969533">
5.1 Conditions
</subsectionHeader>
<bodyText confidence="0.9990465">
Since the data is very small, we tested our au-
tomatic annotation using 5 fold cross validation
</bodyText>
<footnote confidence="0.999537">
6http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
7We realize in some of the grammar books auxilliaries
include modal verbs.
8http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
</footnote>
<bodyText confidence="0.999977705882353">
where 10% of the data is set aside as development
data, then 70% is used for training and 20% for
testing. The reported results are averaged over the
5 folds for the Test data for each of our experimen-
tal conditions.
Our baseline condition is using the tokenized
words only with no other features (TOK). We em-
pirically establish that a context size of −/+3
yields the best results in the baseline condition as
evaluated on the development data set. Hence all
the results are yielded from a CXT of size 3.
The next conditions present the impact of
adding a single feature at a time and then combin-
ing them. It is worth noting that the results reflect
the ability of the classifier to identify a token that
could be annotatable and also classify it correctly
as one of the possible classes.
</bodyText>
<subsectionHeader confidence="0.997602">
5.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999687142857143">
We use Fβ=1 (F-measure) as the harmonic mean
between (P)recision and (R)ecall. All the pre-
sented results are the F-measure. We report the
results separately for the three classes CB, NCB,
and NA as well as the overall global F measure for
any one condition averaged over the 5 folds of the
TEST data set.
</bodyText>
<subsectionHeader confidence="0.885147">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.999988211267606">
In Table 1 we present the results yielded per con-
dition including the baseline TOK and presented
for the three different classes as well as the overall
F-measure.
All the results yielded by our experiments
outperform the baseline TOK. We highlight
the highest performing conditions in Ta-
ble 1: TOK+AlphaNum+POS +CHUNK,
TOK+AN+POS and TOK+POS. Even though
all the features independently outperform the
baseline TOK in isolation, POS is the single most
contributing feature. The least contributing factor
independently is the AlphaNumeric feature AN.
However combining AN with character Ngram
NG yields better results than using each of them
independently. We note that adding NG to any
other feature combination is not helpful, in fact
it seems to add noise rather than signal to the
learning process in the presence of more sophis-
ticated features such as POS or syntactic chunk
information. Adding the verbtype VT explicitly
as a feature is not helpful for all categories, it
seems most effective with CB. As mentioned
earlier we deterministically considered all modal
verbs to be modal. This might not be the case
for all modal auxilliaries since some of them
are used epistemically while others deontically,
hence our feature could be introducing an element
of noise. Adding syntactic chunk information
helps boost the results by a small margin from
53.5 to 53.97 F-measure. All the results seem to
suggest the domination of the POS feature and it’s
importance for such a tagging problem. In general
our performance on CB is the highest, followed
by NA then we note that NCB is the hardest
category to predict. Examining the data, NCB
has the lowest number of occurrence instances
in this data set across the board in the whole
data set and accordingly in the training data,
which might explain the very low performance.
Also in our annotation effort, it was the hardest
category to annotate since the annotation takes
more than the sentential context into account.
Hence a typical CB verb such as “believe” in the
scope of a reporting predicate such as “say” as
in the following example Mary said he believed
the suspect with no qualms. The verb believed
should be tagged NCB however in most cases it
is tagged as a CB. Our syntactic feature CHUNK
helps a little but it does not capture the overall
dependencies in the structure. We believe that
representing deeper syntactic structure should
help tremendously as it will model these relatively
longer dependencies.
We also calculated a confusion matrix for the
different classes. The majority of the errors are
identification errors where an annotatable is con-
sidered an O class as opposed to one of the 3 rel-
evant classes. This suggests that identifying the
annotatable words is a harder task than classifica-
tion into one of the three classes, which is consis-
tent with our observation from the interannotator
disagreements where most of their disagreements
were on the annotatable tokens, though a small
overall number of tokens, 103 tokens out of 6188,
it was the most significant disagreement category.
We find that for the TOK+POS condition, CBs are
mistagged as un-annotatable O 55% of the time.
We find most of the confusions between NA and
CB, and NCB and CB, both cases favoring a CB
tag.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999494090909091">
We presented a preliminary pilot study of belief
annotation and automatic tagging. Even though
the data is relatively tiny, we show that automatic
prediction of a belief class is a feasible task. Us-
ing syntactic features, we are able to obtain signif-
icant improvements over a simple baseline of 23%
F-measure absolute points. The best performing
automatic tagging condition is where we use POS
tag, word type feature AlphaNumeric, and shallow
syntactic chunk information CHUNK. Our best
overall performance is 53.97% F-measure.
</bodyText>
<table confidence="0.997643461538462">
CB NA NCB Overall F
TOK 25.12 41.18 13.64 30.3
TOK+NG 33.18 42.29 5 34.25
TOK+AN 30.43 44.57 12.24 33.92
TOK+AN+NG 37.17 42.46 9.3 36.61
TOK+POS 54.8 59.23 13.95 53.5
TOK+NG+POS 43.15 50.5 22.73 44.35
TOK+AN+POS 54.79 58.97 22.64 53.54
TOK+NG+AN+POS 43.09 54.98 18.18 45.91
TOK+POS+CHUNK 55.45 57.5 15.38 52.77
TOK+POS+VT+CHUNK 53.74 57.14 14.29 51.43
TOK+AN+POS+CHUNK 55.89 59.59 22.58 53.97
TOK+AN+POS+VT+CHUNK 56.27 58.87 12.9 52.89
</table>
<tableCaption confidence="0.748701">
Table 1: Final results averaged over 5 folds of test data using different features and their combinations:
NG is NGRAM, AN is AlphaNumeric, VT is verbtype
</tableCaption>
<bodyText confidence="0.999907333333333">
In the future we are looking at ways of adding
more sophisticated deep syntactic and semantic
features using lexical chains from discourse struc-
ture. We will also be exploring belief annotation in
Arabic and Urdu on a parallel data collection since
these languages express evidentiality in ways that
differ linguistically from English. Finally we will
explore ways of automatically augmenting the la-
beled data pool using active learning.
</bodyText>
<sectionHeader confidence="0.979918" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999972666666667">
This work was supported by grants from the Hu-
man Language Technology Center of Excellence.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the sponsor.
</bodyText>
<sectionHeader confidence="0.999197" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999440918367347">
Philip R. Cohen and Hector J. Levesque. 1990. Ratio-
nal interaction as the basis for communication. In
Jerry Morgan Philip Cohen and James Allen, edi-
tors, Intentions in Communication. MIT Press.
Eduard H. Hovy. 1993. Automated discourse gener-
ation using discourse structure relations. Artificial
Intelligence, 63:341–385.
Rebecca Bruce Matthew Bell Janyce Wiebe,
Theresa Wilson and Melanie Martin. 2004.
Learning subjective language. In Computational
Linguistics, Volume 30 (3).
Paul Kingsbury, Martha Palmer, and Mitch Marcus.
2002. Adding semantic annotation to the Penn Tree-
Bank. In Proceedings of the Human Language Tech-
nology Conference, San Diego, CA.
Ralf Krestel, Sabine Bergler, and Ren´e Witte. 2008.
Minding the Source: Automatic Tagging of Re-
ported Speech in Newspaper Articles. In European
Language Resources Association (ELRA), editor,
Proceedings of the Sixth International Language Re-
sources and Evaluation (LREC 2008), Marrakech,
Morocco, May 28–30.
William C. Mann and Sandra A. Thompson. 1987.
Rhetorical Structure Theory: A theory of text orga-
nization. Technical Report ISI/RS-87-190, ISI.
Johanna Moore. 1994. Participating in Explanatory
Dialogues. MIT Press.
Ren´e Witte Ralf Krestel and Sabine Bergler. 2007.
Processing of Beliefs extracted from Reported
Speech in Newspaper Articles. In International
Conference on Recent Advances in Natural Lan-
guage Processing (RANLP 2007), Borovets, Bul-
garia, September 27–29.
Saur´ı Roser, Marc Verhagen, and James Pustejovsky.
2006. Annotating and Recognizing Event Modality
in Text. In FLAIRS 2006, editor, In Proceedings
of the 19th International FLAIRS Conference, Mel-
bourne Beach, Florida, May 11-13.
Swapna Somasundaran, Janyce Wiebe, and Josef Rup-
penhofer. 2008. Discourse level opinion interpre-
tation. In Proceedings of the 22nd International
Conference on Computational Linguistics (Coling
2008), pages 801–808, Manchester, UK, August.
Coling 2008 Organizing Committee.
Theresa Wilson and Janyce Wiebe. 2005. Annotat-
ing attributions and private states. In Proceedings of
the Workshop on Frontiers in Corpus Annotations II:
Pie in the Sky, pages 53–60, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.620338">
<title confidence="0.998353">Committed Belief Annotation and Tagging</title>
<author confidence="0.999954">Mona T Diab Lori Levin</author>
<affiliation confidence="0.9527945">CCLS LTI Columbia U. CMU</affiliation>
<email confidence="0.996357">mdiab@cs.columbia.edulsl@cs.cmu.edu</email>
<author confidence="0.997698">Teruko Mitamura Owen Rambow</author>
<affiliation confidence="0.9446675">LTI CCLS CMU Columbia U.</affiliation>
<email confidence="0.998147">teruko+@cs.cmu.edurambow@ccls.columbia.edu</email>
<author confidence="0.99793">Vinodkumar Prabhakaran Weiwei Guo</author>
<affiliation confidence="0.909904">CS CS Columbia U. Columbia U.</affiliation>
<abstract confidence="0.996950137931035">We present a preliminary pilot study of belief annotation and automatic tagging. Our objective is to explore semantic meaning beyond surface propositions. We aim to model people’s cognitive states, namely their beliefs as expressed through linguistic means. We model the strength of their beliefs and their (the human) degree of commitment to their utterance. We explore only the perspective of the author of a text. We classify predicates into one of three possibilities: committed belief, non committed belief, or not applicable. We proceed to manually annotate data to that end, then we build a supervised framework to test the feasibility of automatically predicting these belief states. Even though the data is relatively small, we show that automatic prediction of a belief class is a feasible task. Using syntactic features, we are able to obtain significant improvements over a simple baseline of 23% F-measure absolute points. The best performing automatic tagging condition is where we use POS tag, word type feature AlphaNumeric, and shallow syntactic chunk information CHUNK. Our best overall performance is 53.97% F-measure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Philip R Cohen</author>
<author>Hector J Levesque</author>
</authors>
<title>Rational interaction as the basis for communication.</title>
<date>1990</date>
<booktitle>Intentions in Communication.</booktitle>
<editor>In Jerry Morgan Philip Cohen and James Allen, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3893" citStr="Cohen and Levesque, 1990" startWordPosition="637" endWordPosition="640">us anything about whether anyone believes whether GM will lay off workers. In order to tease apart what is happening, we need to refine a simple IR-ish view of text as a repository of propositions about the world. We use two theories to aid us. The first theory is that in addition to facts about the world (GM will or will not lay off workers), we have facts about people’s cognitive states, and these cognitive states relate their bearer to the facts in the world. (Though perhaps there are only cognitive states, and no facts about the world.) Following the literature in Artificial Intelligence (Cohen and Levesque, 1990), we can model cognitive state as beliefs, desires, and intentions. In this paper, we are only interested in beliefs (and in distinguishing them from desires and intentions). The second theory is that communication is intention-driven, and understanding text actually means understanding the communicative intention of the writer. Furthermore, communicative intentions are intentions to affect the reader’s cognitive state – his or her beliefs, desires, and/or intentions. This view has been worked out in the text generation and dialog community more than in the text understanding community (Mann a</context>
</contexts>
<marker>Cohen, Levesque, 1990</marker>
<rawString>Philip R. Cohen and Hector J. Levesque. 1990. Rational interaction as the basis for communication. In Jerry Morgan Philip Cohen and James Allen, editors, Intentions in Communication. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard H Hovy</author>
</authors>
<title>Automated discourse generation using discourse structure relations.</title>
<date>1993</date>
<journal>Artificial Intelligence,</journal>
<pages>63--341</pages>
<contexts>
<context position="4522" citStr="Hovy, 1993" startWordPosition="735" endWordPosition="736">itive state as beliefs, desires, and intentions. In this paper, we are only interested in beliefs (and in distinguishing them from desires and intentions). The second theory is that communication is intention-driven, and understanding text actually means understanding the communicative intention of the writer. Furthermore, communicative intentions are intentions to affect the reader’s cognitive state – his or her beliefs, desires, and/or intentions. This view has been worked out in the text generation and dialog community more than in the text understanding community (Mann and Thompson, 1987; Hovy, 1993; Moore, 1994). In this paper we are interested in exploring the following: we would like to recognize what the text wants to make us believe about various people’s cognitive states, including the speaker’s. As mentioned, we are only interested in people’s belief. In this view, the result of text processing is not a list of facts about the world, but a list of facts about different people’s cognitive states. This paper is part of an on-going research effort. The goals of this paper are to summarize a pilot annotation effort, and to present the results of initial experiments in automatically ex</context>
</contexts>
<marker>Hovy, 1993</marker>
<rawString>Eduard H. Hovy. 1993. Automated discourse generation using discourse structure relations. Artificial Intelligence, 63:341–385.</rawString>
</citation>
<citation valid="false">
<institution>Rebecca Bruce Matthew Bell Janyce Wiebe,</institution>
<marker></marker>
<rawString>Rebecca Bruce Matthew Bell Janyce Wiebe,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Melanie Martin</author>
</authors>
<title>Learning subjective language.</title>
<date>2004</date>
<journal>In Computational Linguistics, Volume</journal>
<volume>30</volume>
<issue>3</issue>
<marker>Wilson, Martin, 2004</marker>
<rawString>Theresa Wilson and Melanie Martin. 2004. Learning subjective language. In Computational Linguistics, Volume 30 (3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kingsbury</author>
<author>Martha Palmer</author>
<author>Mitch Marcus</author>
</authors>
<title>Adding semantic annotation to the Penn TreeBank.</title>
<date>2002</date>
<booktitle>In Proceedings of the Human Language Technology Conference,</booktitle>
<location>San Diego, CA.</location>
<contexts>
<context position="9198" citStr="Kingsbury et al., 2002" startWordPosition="1542" endWordPosition="1545">ay off workers or Lay off workers!). This sort of annotation is part of an annotation of all “modalities” that a text may express. We only annotate belief. A further complication is that these modalities can be nested: one can express a belief about someone else’s belief, and one may be strong and the other weak (I believe John may believe that GM will lay off workers). At this phase, we only annotate from the perspective of the writer, i.e. what the writer of the text that is being annotated believes. The annotation units (annotatables) are, conceptually, propositions as defined by PropBank (Kingsbury et al., 2002). In practice, annotators are asked to identify full lexical verbs (whether in main or embedded clauses, whether finite or non-finite). In predicative constructions (John is a doctor/in the kitchen/drunk), we ask them to identify the nominal, prepositional, or adjectival head rather than the form of to be, in order to also handle small clauses (I think [John an idiot]). The interest of the annotation is clear: we want to be able to determine automatically from a given text what beliefs we can ascribe to the writer, and with what strengths he or she holds them. Across languages, many different </context>
</contexts>
<marker>Kingsbury, Palmer, Marcus, 2002</marker>
<rawString>Paul Kingsbury, Martha Palmer, and Mitch Marcus. 2002. Adding semantic annotation to the Penn TreeBank. In Proceedings of the Human Language Technology Conference, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Krestel</author>
<author>Sabine Bergler</author>
<author>Ren´e Witte</author>
</authors>
<title>Minding the Source: Automatic Tagging of Reported Speech in Newspaper Articles.</title>
<date>2008</date>
<booktitle>In European Language Resources Association (ELRA), editor, Proceedings of the Sixth International Language Resources and Evaluation (LREC 2008),</booktitle>
<pages>28--30</pages>
<location>Marrakech, Morocco,</location>
<contexts>
<context position="12003" citStr="Krestel et al., 2008" startWordPosition="2007" endWordPosition="2010">ttributed to an event. As a result, we cannot use the annotation to discover how natural language expresses level of belief. Our annotation is more primitively semantic: we ask the annotators simply to annotate meaning (does X believe the event takes place), as opposed to annotating the linguistic structures which express meaning. As a consequence of the difference in annotation, we cannot compare our automatic prediction results to theirs. Other related works explored belief systems in an inference scenario as opposed to an intentionality scenario. In work by (Ralf Krestel and Bergler, 2007; Krestel et al., 2008), the authors explore belief in the context of news media exploring reported speech where they track newspaper text looking for elements indicating evidentiality. The notion of belief is more akin to finding statements that support or negate specific events with different degrees of support. This is different from our notion of committed belief in this work, since we seek to make explicit the intention of the author or the speaker. 4 Our Approach 4.1 Data We create a relatively small corpus of English manually annotated for the three categories: CB, NCB, NA. The data covers different domains a</context>
</contexts>
<marker>Krestel, Bergler, Witte, 2008</marker>
<rawString>Ralf Krestel, Sabine Bergler, and Ren´e Witte. 2008. Minding the Source: Automatic Tagging of Reported Speech in Newspaper Articles. In European Language Resources Association (ELRA), editor, Proceedings of the Sixth International Language Resources and Evaluation (LREC 2008), Marrakech, Morocco, May 28–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical Structure Theory: A theory of text organization.</title>
<date>1987</date>
<tech>Technical Report ISI/RS-87-190, ISI.</tech>
<contexts>
<context position="4510" citStr="Mann and Thompson, 1987" startWordPosition="731" endWordPosition="734"> 1990), we can model cognitive state as beliefs, desires, and intentions. In this paper, we are only interested in beliefs (and in distinguishing them from desires and intentions). The second theory is that communication is intention-driven, and understanding text actually means understanding the communicative intention of the writer. Furthermore, communicative intentions are intentions to affect the reader’s cognitive state – his or her beliefs, desires, and/or intentions. This view has been worked out in the text generation and dialog community more than in the text understanding community (Mann and Thompson, 1987; Hovy, 1993; Moore, 1994). In this paper we are interested in exploring the following: we would like to recognize what the text wants to make us believe about various people’s cognitive states, including the speaker’s. As mentioned, we are only interested in people’s belief. In this view, the result of text processing is not a list of facts about the world, but a list of facts about different people’s cognitive states. This paper is part of an on-going research effort. The goals of this paper are to summarize a pilot annotation effort, and to present the results of initial experiments in auto</context>
</contexts>
<marker>Mann, Thompson, 1987</marker>
<rawString>William C. Mann and Sandra A. Thompson. 1987. Rhetorical Structure Theory: A theory of text organization. Technical Report ISI/RS-87-190, ISI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johanna Moore</author>
</authors>
<title>Participating in Explanatory Dialogues.</title>
<date>1994</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4536" citStr="Moore, 1994" startWordPosition="737" endWordPosition="738">as beliefs, desires, and intentions. In this paper, we are only interested in beliefs (and in distinguishing them from desires and intentions). The second theory is that communication is intention-driven, and understanding text actually means understanding the communicative intention of the writer. Furthermore, communicative intentions are intentions to affect the reader’s cognitive state – his or her beliefs, desires, and/or intentions. This view has been worked out in the text generation and dialog community more than in the text understanding community (Mann and Thompson, 1987; Hovy, 1993; Moore, 1994). In this paper we are interested in exploring the following: we would like to recognize what the text wants to make us believe about various people’s cognitive states, including the speaker’s. As mentioned, we are only interested in people’s belief. In this view, the result of text processing is not a list of facts about the world, but a list of facts about different people’s cognitive states. This paper is part of an on-going research effort. The goals of this paper are to summarize a pilot annotation effort, and to present the results of initial experiments in automatically extracting facts</context>
</contexts>
<marker>Moore, 1994</marker>
<rawString>Johanna Moore. 1994. Participating in Explanatory Dialogues. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ren´e Witte Ralf Krestel</author>
<author>Sabine Bergler</author>
</authors>
<title>Processing of Beliefs extracted from Reported Speech in Newspaper Articles.</title>
<date>2007</date>
<booktitle>In International Conference on Recent Advances in Natural Language Processing (RANLP</booktitle>
<location>Borovets, Bulgaria,</location>
<contexts>
<context position="11980" citStr="Krestel and Bergler, 2007" startWordPosition="2003" endWordPosition="2006">e for the level of belief attributed to an event. As a result, we cannot use the annotation to discover how natural language expresses level of belief. Our annotation is more primitively semantic: we ask the annotators simply to annotate meaning (does X believe the event takes place), as opposed to annotating the linguistic structures which express meaning. As a consequence of the difference in annotation, we cannot compare our automatic prediction results to theirs. Other related works explored belief systems in an inference scenario as opposed to an intentionality scenario. In work by (Ralf Krestel and Bergler, 2007; Krestel et al., 2008), the authors explore belief in the context of news media exploring reported speech where they track newspaper text looking for elements indicating evidentiality. The notion of belief is more akin to finding statements that support or negate specific events with different degrees of support. This is different from our notion of committed belief in this work, since we seek to make explicit the intention of the author or the speaker. 4 Our Approach 4.1 Data We create a relatively small corpus of English manually annotated for the three categories: CB, NCB, NA. The data cov</context>
</contexts>
<marker>Krestel, Bergler, 2007</marker>
<rawString>Ren´e Witte Ralf Krestel and Sabine Bergler. 2007. Processing of Beliefs extracted from Reported Speech in Newspaper Articles. In International Conference on Recent Advances in Natural Language Processing (RANLP 2007), Borovets, Bulgaria, September 27–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saur´ı Roser</author>
<author>Marc Verhagen</author>
<author>James Pustejovsky</author>
</authors>
<title>Annotating and Recognizing Event Modality in Text.</title>
<date>2006</date>
<booktitle>In FLAIRS 2006, editor, In Proceedings of the 19th International FLAIRS Conference,</booktitle>
<pages>11--13</pages>
<location>Melbourne Beach, Florida,</location>
<contexts>
<context position="10215" citStr="Roser et al. (2006)" startWordPosition="1710" endWordPosition="1713">ation is clear: we want to be able to determine automatically from a given text what beliefs we can ascribe to the writer, and with what strengths he or she holds them. Across languages, many different linguistic means are used to denote this attitude towards an uttered proposition, including syntax, lexicon, and morphology. To our knowledge, no systematic empirical study exists for English, and this annotation is a step towards that goal. example, in German, the choice between indicative mood and subjunctive mood in reported speech can signal the writer’s attitude. 3 Related Work The work of Roser et al. (2006) is, in many respects, very similar to ours. In particular, they are concerned with extracting information about people’s beliefs and the strength of these beliefs from text. However, their annotation is very different from ours. They extend the TimeML annotation scheme to include annotation of markers of belief and strength of belief. For example, in the sentence The Human Rights Committee regretted that discrimination against women persisted in practice, TimeML identifies the events associated with the verbs regret and persist, and then the extension to the annotation adds the mark that ther</context>
</contexts>
<marker>Roser, Verhagen, Pustejovsky, 2006</marker>
<rawString>Saur´ı Roser, Marc Verhagen, and James Pustejovsky. 2006. Annotating and Recognizing Event Modality in Text. In FLAIRS 2006, editor, In Proceedings of the 19th International FLAIRS Conference, Melbourne Beach, Florida, May 11-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Janyce Wiebe</author>
<author>Josef Ruppenhofer</author>
</authors>
<title>Discourse level opinion interpretation.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>801--808</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="6248" citStr="Somasundaran et al., 2008" startWordPosition="1025" endWordPosition="1028">is is not because we think this is the only interesting information in text, but we do this in order to obtain a manageable annotation in our pilot study. Specifically, we annotate whether the writer intends the reader to interpret a stated proposition as the writer’s strongly held belief, as a proposition which the writer does not believe strongly (but could), or as a proposition towards which the writer has an entirely different cognitive attitude, such as desire or intention. We do not annotate subjectivity (Janyce Wiebe and Martin, 2004; Wilson and Wiebe, 2005), nor opinion (for example: (Somasundaran et al., 2008)): the nature of the proposition (opinion and type of opinion, statement about interior world, external world) is not of interest. Thus, this work is orthogonal to the extensive literature on opinion detection. And we do not annotate truth: real-world (encyclopedic) truth is not relevant. We have three categories: • Committed belief (CB): the writer indicates in this utterance that he or she believes the proposition. For example, GM has laid off workers, or, even stronger, We know that GM has laid off workers. A subcase of committed belief concerns propositions about the future, such as GM wil</context>
</contexts>
<marker>Somasundaran, Wiebe, Ruppenhofer, 2008</marker>
<rawString>Swapna Somasundaran, Janyce Wiebe, and Josef Ruppenhofer. 2008. Discourse level opinion interpretation. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 801–808, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
</authors>
<title>Annotating attributions and private states.</title>
<date>2005</date>
<booktitle>In Proceedings of the Workshop on Frontiers in Corpus Annotations II: Pie in the Sky,</booktitle>
<pages>53--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="6193" citStr="Wilson and Wiebe, 2005" startWordPosition="1016" endWordPosition="1019">d we are only interested in the writer’s beliefs. This is not because we think this is the only interesting information in text, but we do this in order to obtain a manageable annotation in our pilot study. Specifically, we annotate whether the writer intends the reader to interpret a stated proposition as the writer’s strongly held belief, as a proposition which the writer does not believe strongly (but could), or as a proposition towards which the writer has an entirely different cognitive attitude, such as desire or intention. We do not annotate subjectivity (Janyce Wiebe and Martin, 2004; Wilson and Wiebe, 2005), nor opinion (for example: (Somasundaran et al., 2008)): the nature of the proposition (opinion and type of opinion, statement about interior world, external world) is not of interest. Thus, this work is orthogonal to the extensive literature on opinion detection. And we do not annotate truth: real-world (encyclopedic) truth is not relevant. We have three categories: • Committed belief (CB): the writer indicates in this utterance that he or she believes the proposition. For example, GM has laid off workers, or, even stronger, We know that GM has laid off workers. A subcase of committed belief</context>
</contexts>
<marker>Wilson, Wiebe, 2005</marker>
<rawString>Theresa Wilson and Janyce Wiebe. 2005. Annotating attributions and private states. In Proceedings of the Workshop on Frontiers in Corpus Annotations II: Pie in the Sky, pages 53–60, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>