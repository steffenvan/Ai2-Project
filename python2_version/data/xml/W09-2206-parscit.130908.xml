<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004490">
<title confidence="0.990508">
Latent Dirichlet Allocation with Topic-in-Set Knowledge∗
</title>
<author confidence="0.991822">
David Andrzejewski
</author>
<affiliation confidence="0.837256333333333">
Computer Sciences Department
University of Wisconsin-Madison
Madison, WI 53706, USA
</affiliation>
<email confidence="0.997795">
andrzeje@cs.wisc.edu
</email>
<author confidence="0.989427">
Xiaojin Zhu
</author>
<affiliation confidence="0.836737">
Computer Sciences Department
University of Wisconsin-Madison
Madison, WI 53706, USA
</affiliation>
<email confidence="0.998897">
jerryzhu@cs.wisc.edu
</email>
<sectionHeader confidence="0.995554" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998121583333333">
Latent Dirichlet Allocation is an unsupervised
graphical model which can discover latent top-
ics in unlabeled data. We propose a mech-
anism for adding partial supervision, called
topic-in-set knowledge, to latent topic mod-
eling. This type of supervision can be used
to encourage the recovery of topics which are
more relevant to user modeling goals than the
topics which would be recovered otherwise.
Preliminary experiments on text datasets are
presented to demonstrate the potential effec-
tiveness of this method.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.987819684210526">
Latent topic models such as Latent Dirichlet Alloca-
tion (LDA) (Blei et al., 2003) have emerged as a use-
ful family of graphical models with many interesting
applications in natural language processing. One of
the key virtues of LDA is its status as a fully genera-
tive probabilistic model, allowing principled exten-
sions and variations capable of expressing rich prob-
lem domain structure (Newman et al., 2007; Rosen-
Zvi et al., 2004; Boyd-Graber et al., 2007; Griffiths
et al., 2005).
LDA is an unsupervised learning model. This
work aims to add supervised information in the form
of latent topic assignments to LDA. Traditionally,
topic assignments have been denoted by the variable
z in LDA, and we will call such supervised informa-
tion “z-labels.” In particular, a z-label is the knowl-
∗ We would like to acknowledge the assistance of Brandi
Gancarz with the biological annotations. This work is supported
in part by the Wisconsin Alumni Research Foundation.
</bodyText>
<page confidence="0.996838">
43
</page>
<bodyText confidence="0.998393714285714">
edge that the topic assignment for a given word po-
sition is within a subset of topics. As such, this work
is a combination of unsupervised model and super-
vised knowledge, and falls into the category simi-
lar to constrained clustering (Basu et al., 2008) and
semi-supervised dimensionality reduction (Yang et
al., 2006).
</bodyText>
<sectionHeader confidence="0.72001" genericHeader="related work">
1.1 Related Work
</sectionHeader>
<bodyText confidence="0.99996692">
A similar but simpler type of topic labeling infor-
mation has been applied to computer vision tasks.
Topic modeling approaches have been applied to
scene modeling (Sudderth et al., 2005), segmen-
tation, and classification or detection (Wang and
Grimson, 2008). In some of these vision applica-
tions, the latent topics themselves are assumed to
correspond to object labels. If labeled data is avail-
able, either all (Wang and Mori, 2009) or some (Cao
and Fei-Fei, 2007) of the z values can be treated as
observed, rather than latent, variables. Our model
extends z-labels from single values to subsets, thus
offer additional model expressiveness.
If the topic-based representations of documents
are to be used for document clustering or classi-
fication, providing z-labels for words can be seen
as similar to semi-supervised learning with labeled
features (Druck et al., 2008). Here the words are
features, and z-label guidance acts as a feature la-
bel. This differs from other supervised LDA vari-
ants (Blei and McAuliffe, 2008; Lacoste-Julien et
al., 2008) which use document label information.
The ΔLDA model for statistical software debug-
ging (Andrzejewski et al., 2007) partitions the topics
into 2 sets: “usage” topics which can appear in all
</bodyText>
<note confidence="0.6697005">
Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 43–48,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999916375">
documents, and “bug” topics which can only appear
in a special subset of documents. This effect was
achieved by using different α hyperparameters for
the 2 subsets of documents. z-labels can achieve the
same effect by restricting the z’s in documents out-
side the special subset, so that the z’s cannot assume
the “bug” topic values. Therefore, the present ap-
proach can be viewed as a generalization of OLDA.
Another perspective is that our z-labels may
guide the topic model towards the discovery of sec-
ondary or non-dominant statistical patterns in the
data (Chechik and Tishby, 2002). These topics may
be more interesting or relevant to the goals of the
user, but standard LDA would ignore them in favor
of more prominent (and perhaps orthogonal) struc-
ture.
</bodyText>
<sectionHeader confidence="0.999444" genericHeader="method">
2 Our Model
</sectionHeader>
<subsectionHeader confidence="0.999591">
2.1 Review of Latent Dirichlet Allocation
</subsectionHeader>
<bodyText confidence="0.999987">
We briefly review LDA, following the notation
of (Griffiths and Steyvers, 2004) 1. Let there be
T topics. Let w = w1 ... wn represent a cor-
pus of D documents, with a total of n words. We
use di to denote the document of word wi, and zi
the hidden topic from which wi is generated. Let
</bodyText>
<equation confidence="0.890324875">
φ(w)
j = p(w|z = j), and θ(d) j= p(z = j) for
document d. LDA involves the following generative
model:
θ ∼ Dirichlet(α)
zi|θ(di) ∼ Multinomial(θ(di))
φ ∼ Dirichlet(β)
wi|zi, φ ∼ Multinomial(φzi),
</equation>
<bodyText confidence="0.999183545454546">
where α and β are hyperparameters for the
document-topic and topic-word Dirichlet distribu-
tions, respectively. Even though they can be vector
valued, for simplicity we assume α and β are scalars,
resulting in symmetric Dirichlet priors.
Given our observed words w, the key task is in-
ference of the hidden topics z. Unfortunately, this
posterior is intractable and we resort to a Markov
Chain Monte Carlo (MCMC) sampling scheme,
specifically Collapsed Gibbs Sampling (Griffiths
and Steyvers, 2004). The full conditional equation
</bodyText>
<footnote confidence="0.625337">
1We enclose superscripts in parentheses in this paper.
</footnote>
<bodyText confidence="0.985201">
used for sampling individual zi values from the pos-
terior is given by
</bodyText>
<equation confidence="0.999700333333333">
P(zi = v|z−i, w, α, β) ∝
! !
n(d) n(wi)
−i,v + α −i,v + β
Pu (n(d)
−i,u + α) Pw (β + n(w) −i,v)
</equation>
<bodyText confidence="0.9769876">
where n(d) −i,v is the number of times topic v is used in
document d, and n(wi)
−i,v is the number of times word
wi is generated by topic v. The −i notation signifies
that the counts are taken omitting the value of zi.
</bodyText>
<subsectionHeader confidence="0.968763">
2.2 Topic-in-Set Knowledge: z-labels
</subsectionHeader>
<bodyText confidence="0.843677">
Let
</bodyText>
<equation confidence="0.998398333333333">
!nd� v + α n(wi) + β
Pu(n−i),u + α) PW (β +w n(w�)
−i,v)
</equation>
<bodyText confidence="0.9999698">
We now define our z-labels. Let C(i) be the set of
possible z-labels for latent topic zi. We set a hard
constraint by modifying the Gibbs sampling equa-
tion with an indicator function δ(v ∈ C(i)), which
takes on value 1 if v ∈ C(i) and is 0 otherwise:
</bodyText>
<equation confidence="0.995666">
P(zi = v|z−i, w, α, β) ∝ qivδ(v ∈ C(i)) (6)
</equation>
<bodyText confidence="0.99990795">
If we wish to restrict zi to a single value (e.g., zi =
5), this can now be accomplished by setting C(i) =
{5}. Likewise, we can restrict zi to a subset of val-
ues {1, 2,3} by setting C(i) = {1, 2, 3}. Finally, for
unconstrained zi we simply set C(i) = {1, 2,..., T},
in which case our modified sampling (6) reduces to
the standard Gibbs sampling (5).
This formulation gives us a flexible method for in-
serting prior domain knowledge into the inference of
latent topics. We can set C(i) independently for ev-
ery single word wi in the corpus. This allows us, for
example, to force two occurrences of the same word
(e.g., “Apple pie” and “Apple iPod”) to be explained
by different topics. This effect would be impossible
to achieve by using topic-specific asymmetric β vec-
tors and setting some entries to zero.
This hard constraint model can be relaxed. Let
0 ≤ η ≤ 1 be the strength of our constraint, where
η = 1 recovers the hard constraint (6) and η = 0
recovers unconstrained sampling (5):
</bodyText>
<equation confidence="0.989854333333333">
� �
P(zi = v|z−i, w, α, β) ∝ qiv ηδ(v ∈ C(i)) + 1 − η .
qiv =
</equation>
<page confidence="0.982508">
44
</page>
<bodyText confidence="0.999707">
While we present the z-label constraints as a me-
chanical modification to the Gibbs sampling equa-
tions, it can be derived from an undirected extension
of LDA (omitted here) which encodes z-labels. The
soft constraint Gibbs sampling equation arises nat-
urally from this formulation, which is the basis for
the First-Order Logic constraints described later in
the future work section.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="evaluation">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999936">
We now present preliminary experimental results to
demonstrate some interesting applications for topic-
in-set knowledge. Unless otherwise specified, sym-
metric hyperparameters α = .5 and Q = .1 were
used and all MCMC chains were run for 2000 sam-
ples before estimating φ and θ from the final sample,
as in (Griffiths and Steyvers, 2004).
</bodyText>
<subsectionHeader confidence="0.992292">
3.1 Concept Expansion
</subsectionHeader>
<bodyText confidence="0.999911513513513">
We explore the use of topic-in-set for identifying
words related to a target concept, given a set of
seed words associated with that concept. For ex-
ample, a biological expert may be interested in the
concept “translation”. The expert would then pro-
vide a set of seed words which are strongly related
to this concept, here we assume the seed word set
{translation,trna,anticodon,ribosome}. We add the
hard constraint that zz = 0 for all occurrences of
these four words in our corpus of approximately
9,000 yeast-related abstracts.
We ran LDA with the number of topics T = 100,
both with and without the z-label knowledge on the
seed words. Table 1 shows the most probable words
in selected topics from both runs. Table 1a shows
Topic 0 from the constrained run, while Table 1b
shows the topics which contained seed words among
the top 50 most probable words from the uncon-
strained run.
In order to better understand the results, these
top words were annotated for relevance to the tar-
get concept (translation) by an outside biological ex-
pert. The words in Table 1 were then colored blue
if they were one of the original seed words, red if
they were judged as relevant, and left black other-
wise. From a quick glance, we can see that Topic
0 from the constrained run contains more relevant
terms than Topic 43 from the standard LDA run.
Topic 31 has a similar number of relevant terms, but
taken together we can see that the emphasis of Topic
31 is slightly off-target, more focused on “mRNA
turnover” than “translation”. Likewise, Topic 73
seems more focused on the ribosome itself than the
process of translation. Overall, these results demon-
strate the potential effectiveness of z-label informa-
tion for guiding topic models towards a user-seeded
concept.
</bodyText>
<subsectionHeader confidence="0.999168">
3.2 Concept Exploration
</subsectionHeader>
<bodyText confidence="0.999977513513514">
Suppose that a user has chosen a set of terms and
wishes to discover different topics related to these
terms. By constraining these terms to only appear
in a restricted set of topics, these terms will be con-
centrated in the set of topics. The split within those
set of topics may be different from what a standard
LDA will produce, thus revealing new information
within the data.
To make this concrete, say we are interested in
the location “United Kingdom”. We seed this con-
cept with the following LOCATION-tagged terms
{britain, british, england, uk, u.k., wales, scotland,
london}. These terms are then restricted to ap-
pear only in the first 3 topics. Our corpus is an
entity-tagged Reuters newswire corpus used for the
CoNLL-2003 shared task (Tjong Kim Sang and
De Meulder, 2003). In order to focus on our tar-
get location, we also restrict all other LOCATION-
tagged tokens to not appear in the first 3 topics. For
this experiment we set T = 12, arrived at by trial-
and-error in the baseline (standard LDA) case.
The 50 most probable words for each topic are
shown in Figure 2, and tagged entities are prefixed
with their tags for easy identification. Table 2a
shows the top words for the first 3 topics of our z-
label run. These three topics are all related to the
target LOCATION United Kingdom, but they also
split nicely into business, cricket, and soccer. Words
which are highly relevant to each of these 3 concepts
are colored blue, red, and green, respectively.
In contrast, in Table 2b we show topics from stan-
dard LDA which contain any of the “United King-
dom” LOCATION terms (which are underlined)
among the 50 most probable words for that topic.
We make several observations about these topics.
First, standard LDA Topic 0 is mostly concerned
with political unrest in Russia, which is not particu-
</bodyText>
<page confidence="0.986541">
45
</page>
<bodyText confidence="0.670143529411765">
Topic 0 translation, ribosomal, trna, rrna, initiation, ribosome, protein, ribosomes, is, factor, processing, translational
nucleolar, pre-rrna, synthesis, small, 60s, eukaryotic, biogenesis, subunit, trnas, subunits, large, nucleolus
factors, 40, synthetase, free, modification, rna, depletion, eif-2, initiator, 40s, ef-3, anticodon, maturation
18s, eif2, mature, eif4e, associated, synthetases, aminoacylation, snornas, assembly, eif4g, elongation
(a) Topic 0 with z-label
Topic 31 mrna, translation, initiation, mrnas, rna, transcripts, 3, transcript, polya, factor, 5, translational, decay, codon
decapping, factors, degradation, end, termination, eukaryotic, polyadenylation, cap, required, efficiency
synthesis, show, codons, abundance, rnas, aug, nmd, messenger, turnover, rna-binding, processing, eif2, eif4e
eif4g, cf, occurs, pab1p, cleavage, eif5, cerevisiae, major, primary, rapid, tail, efficient, upf1p, eif-2
Topic 43 type, is, wild, yeast, trna, synthetase, both, methionine, synthetases, class, trnas, enzyme, whereas, cytoplasmic
because, direct, efficiency, presence, modification, aminoacylation, anticodon, either, eukaryotic, between
different, specific, discussed, results, similar, some, met, compared, aminoacyl-trna, able, initiator, sam
not, free, however, recognition, several, arc1p, fully, same, forms, leads, identical, responsible, found, only, well
Topic 73 ribosomal, rrna, protein, is, processing, ribosome, ribosomes, rna, nucleolar, pre-rrna, rnase, small, biogenesis
depletion, subunits, 60s, subunit, large, synthesis, maturation, nucleolus, associated, essential, assembly
components, translation, involved, rnas, found, component, mature, rp, 40s, accumulation, 18s, 40, particles
snornas, factors, precursor, during, primary, rrnas, 35s, has, 21s, specifically, results, ribonucleoprotein, early
</bodyText>
<figure confidence="0.979204">
(b) Standard LDA Topics
</figure>
<figureCaption confidence="0.994712">
Figure 1: Concept seed words are colored blue, other words judged relevant to the target concept are colored
red.
</figureCaption>
<bodyText confidence="0.999984647058824">
larly related to the target location. Second, Topic 2
is similar to our previous business topic, but with
a more US-oriented slant. Note that “dollar” ap-
pears with high probability in standard LDA Topic
2, but not in our z-label LDA Topic 0. Standard
LDA Topic 8 appears to be a mix of both soccer and
cricket words. Therefore, it seems that our topic-in-
set knowledge helps in distilling topics related to the
seed words.
Given this promising result, we attempted to
repeat this experiment with some other nations
(United States, Germany, China), but without much
success. When we tried to restrict these LOCATION
words to the first few topics, these topics tended to
be used to explain other concepts unrelated to the
target location (often other sports). We are investi-
gating the possible causes of this problem.
</bodyText>
<sectionHeader confidence="0.994788" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99996925">
We have defined Topic-in-Set knowledge and
demonstrated its use within LDA. As shown in the
experiments, the partial supervision provided by z-
labels can encourage LDA to recover topics rele-
vant to user interests. This approach combines the
pattern-discovery power of LDA with user-provided
guidance, which we believe will be very attractive to
practical users of topic modeling.
Future work will deal with at least two impor-
tant issues. First, when will this form of partial
supervision be most effective or appropriate? Our
experimental results suggest that this approach will
struggle if the user’s target concepts are simply not
prevalent in the text. Second, can we modify this
approach to express richer forms of partial super-
vision? More sophisticated forms of knowledge
may allow users to specify their preferences or prior
knowledge more effectively. Towards this end, we
are investigating the use of First-Order Logic in
specifying prior knowledge. Note that the set z-
labels presented here can be expressed as simple log-
ical formulas. Extending our model to general log-
ical formulas would allow the expression of more
powerful relational preferences.
</bodyText>
<sectionHeader confidence="0.989165" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.944249">
David Andrzejewski, Anne Mulhern, Ben Liblit, and Xi-
aojin Zhu. 2007. Statistical debugging using latent
topic models. In Stan Matwin and Dunja Mladenic,
editors,18th European Conference on Machine Learn-
ing, Warsaw, Poland.
</reference>
<page confidence="0.998188">
46
</page>
<table confidence="0.970877764705882">
Topic 0 million, company, ’s, year, shares, net, profit, half, group, [I-ORG]corp, market, sales, share, percent
expected, business, loss, stock, results, forecast, companies, deal, earnings, statement, price, [I-LOC]london
billion, [I-ORG]newsroom, industry, newsroom, pay, pct, analysts, issue, services, analyst, profits, sale
added, firm, [I-ORG]london, chief, quarter, investors, contract, note, tax, financial, months, costs
Topic 1 [I-LOC]england, [I-LOC]london, [I-LOC]britain, cricket, [I-PER]m., overs, test, wickets, scores, [I-PER]ahmed
[I-PER]paul, [I-PER]wasim, innings, [I-PER]a., [I-PER]akram, [I-PER]mushtaq, day, one-day, [I-PER]mark, final
[I-LOC]scotland, [I-PER]waqar, [I-MISC]series, [I-PER]croft, [I-PER]david, [I-PER]younis, match, [I-PER]ian
total, [I-MISC]english, [I-PER]khan, [I-PER]mullally, bat, declared, fall, [I-PER]d., [I-PER]g., [I-PER]j.
bowling, [I-PER]r., [I-PER]robert, [I-PER]s., [I-PER]steve, [I-PER]c. captain, golf, tour, [I-PER]sohail, extras
[I-ORG]surrey
Topic 2 soccer, division, results, played, standings, league, matches, halftime, goals, attendance, points, won, [I-ORG]st
drawn, saturday, [I-MISC]english, lost, premier, [I-MISC]french, result, scorers, [I-MISC]dutch, [I-ORG]united
[I-MISC]scottish, sunday, match, [I-LOC]london, [I-ORG]psv, tabulate, [I-ORG]hapoel, [I-ORG]sydney, friday
summary, [I-ORG]ajax, [I-ORG]manchester, tabulated, [I-MISC]german, [I-ORG]munich, [I-ORG]city
[I-MISC]european, [I-ORG]rangers, summaries, weekend, [I-ORG]fc, [I-ORG]sheffield, wednesday, [I-ORG]borussia
[I-ORG]fortuna, [I-ORG]paris, tuesday
(a) Topics with set z-labels
</table>
<bodyText confidence="0.92203176">
Topic 0 police, ’s, people, killed, [I-MISC]russian, friday, spokesman, [I-LOC]moscow, told, rebels, group, officials
[I-PER]yeltsin, arrested, found, miles, km, [I-PER]lebed, capital, thursday, tuesday, [I-LOC]chechnya, news
saturday, town, authorities, airport, man, government, state, agency, plane, reported, security, forces
city, monday, air, quoted, students, region, area, local, [I-LOC]russia, [I-ORG]reuters, military, [I-LOC]london
held, southern, died
Topic 2 percent, ’s, market, thursday, july, tonnes, week, year, lower, [I-LOC]u.s., rate, prices, billion, cents, dollar
friday, trade, bank, closed, trading, higher, close, oil, bond, fell, markets, index, points, rose
demand, june, rates, september, traders, [I-ORG]newsroom, day, bonds, million, price, shares, budget, government
growth, interest, monday, [I-LOC]london, economic, august, expected, rise
Topic 5 ’s, match, team, win, play, season, [I-MISC]french, lead, home, year, players, [I-MISC]cup, back, minutes
champion, victory, time, n’t, game, saturday, title, side, set, made, wednesday, [I-LOC]england
league, run, club, top, good, final, scored, coach, shot, world, left, [I-MISC]american, captain
[I-MISC]world, goal, start, won, champions, round, winner, end, years, defeat, lost
Topic 8 division, [I-LOC]england, soccer, results, [I-LOC]london, [I-LOC]pakistan, [I-MISC]english, matches, played
standings, league, points, [I-ORG]st, cricket, saturday, [I-PER]ahmed, won, [I-ORG]united, goals
[I-PER]wasim, [I-PER]akram, [I-PER]m., [I-MISC]scottish, [I-PER]mushtaq, drawn, innings, premier, lost
[I-PER]waqar, test, [I-PER]croft, [I-PER]a., [I-PER]younis, declared, wickets, [I-ORG]hapoel, [I-PER]mullally
[I-ORG]sydney, day, [I-ORG]manchester, [I-PER]khan, final, scores, [I-PER]d., [I-MISC]german, [I-ORG]munich
[I-PER]sohail, friday, total, [I-LOC]oval
Topic 10 [I-LOC]germany, ’s, [I-LOC]italy, [I-LOC]u.s., metres, seconds, [I-LOC]france, [I-LOC]britain, [I-LOC]russia
world, race, leading, [I-LOC]sweden, [I-LOC]australia, [I-LOC]spain, women, [I-MISC]world, [I-LOC]belgium
[I-LOC]netherlands, [I-PER]paul, [I-LOC]japan, [I-MISC]olympic, [I-LOC]austria, [I-LOC]kenya, men, time
results, [I-LOC]brussels, [I-MISC]cup, [I-LOC]canada, final, minutes, record, [I-PER]michael, meeting, round
[I-LOC]norway, friday, scores, [I-PER]mark, [I-PER]van, [I-LOC]ireland, [I-PER]peter, [I-MISC]grand
[I-MISC]prix, points, saturday, [I-LOC]finland, cycling, [I-ORG]honda
</bodyText>
<figure confidence="0.938301">
(b) Standard LDA Topics
</figure>
<figureCaption confidence="0.9215215">
Figure 2: Topics containing “United Kingdom” location words. Words related to business are colored blue,
cricket red, and soccer green.
</figureCaption>
<reference confidence="0.99426075">
Sugato Basu, Ian Davidson, and Kiri Wagstaff, edi-
tors. 2008. Constrained Clustering: Advances in
Algorithms, Theory, and Applications. Chapman &amp;
Hall/CRC Press.
David Blei and Jon McAuliffe. 2008. Supervised topic
models. In J.C. Platt, D. Koller, Y. Singer, and
S. Roweis, editors, Advances in Neural Information
Processing Systems 20, pages 121–128. MIT Press,
</reference>
<page confidence="0.994842">
47
</page>
<reference confidence="0.99915985483871">
Cambridge, MA.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal ofMachine
Learning Research, 3:993–1022.
Jordan Boyd-Graber, David Blei, and Xiaojin Zhu. 2007.
A topic model for word sense disambiguation. In
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 1024–1033.
Liangliang Cao and Li Fei-Fei. 2007. Spatially coher-
ent latent topic model for concurrent segmentation and
classification of objects and scenes. In ICCV, pages 1–
8.
Gal Chechik and Naftali Tishby. 2002. Extracting rel-
evant structures with side information. In NIPS 15,
pages 857–864. MIT press.
Gregory Druck, Gideon Mann, and Andrew McCallum.
2008. Learning from labeled features using general-
ized expectation criteria. In SIGIR 2008, pages 595–
602, New York, NY, USA. ACM.
Thomas Griffiths and Mark Steyvers. 2004. Finding sci-
entific topics. Proceedings of the National Academy of
Sciences, 101(suppl. 1):5228–5235.
Thomas L. Griffiths, Mark Steyvers, David M. Blei, and
Joshua B. Tenenbaum. 2005. Integrating topics and
syntax. In NIPS 17.
S. Lacoste-Julien, F. Sha, and M. Jordan. 2008. Disclda:
Discriminative learning for dimensionality reduction
and classification. In Advances in Neural Information
Processing Systems 21 (NIPS08).
David Newman, Kat Hagedorn, Chaitanya
Chemudugunta, and Padhraic Smyth. 2007. Subject
metadata enrichment using statistical topic models.
In JCDL ’07: Proceedings of the 7th ACM/IEEE-CS
joint conference on Digital libraries, pages 366–375,
New York, NY, USA. ACM.
Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, and
Padhraic Smyth. 2004. The author-topic model for au-
thors and documents. In Proceedings of the 20th con-
ference on Uncertainty in artificial intelligence (UAI),
pages 487–494, Arlington, Virginia, United States.
AUAI Press.
Erik B. Sudderth, Antonio B. Torralba, William T. Free-
man, and Alan S. Willsky. 2005. Learning hierar-
chical models of scenes, objects, and parts. In ICCV,
pages 1331–1338.
Erik Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the conll-2003 shared task: Language-
independent named entity recognition. In Proceedings
of CoNLL-2003, pages 142–147, Edmonton, Canada.
Xiaogang Wang and Eric Grimson. 2008. Spatial latent
dirichlet allocation. In J.C. Platt, D. Koller, Y. Singer,
and S. Roweis, editors, NIPS 20, pages 1577–1584.
MIT Press, Cambridge, MA.
Yang Wang and Greg Mori. 2009. Human action recog-
nition by semi-latent topic models. In IEEE Transac-
tions on Pattern Analysis and Machine Intelligence.
Xin Yang, Haoying Fu, Hongyuan Zha, and Jesse Barlow.
2006. Semi-supervised nonlinear dimensionality re-
duction. In ICML-06, 23nd International Conference
on Machine Learning.
</reference>
<page confidence="0.999352">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.414770">
<title confidence="0.999692">Dirichlet Allocation with Topic-in-Set</title>
<author confidence="0.983069">David</author>
<affiliation confidence="0.9995745">Computer Sciences University of</affiliation>
<address confidence="0.996692">Madison, WI 53706,</address>
<email confidence="0.999335">andrzeje@cs.wisc.edu</email>
<author confidence="0.444334">Xiaojin</author>
<affiliation confidence="0.9987455">Computer Sciences University of</affiliation>
<address confidence="0.998173">Madison, WI 53706,</address>
<email confidence="0.999895">jerryzhu@cs.wisc.edu</email>
<abstract confidence="0.995247538461539">Latent Dirichlet Allocation is an unsupervised graphical model which can discover latent topics in unlabeled data. We propose a mechanism for adding partial supervision, called topic-in-set knowledge, to latent topic modeling. This type of supervision can be used to encourage the recovery of topics which are more relevant to user modeling goals than the topics which would be recovered otherwise. Preliminary experiments on text datasets are presented to demonstrate the potential effectiveness of this method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Andrzejewski</author>
<author>Anne Mulhern</author>
<author>Ben Liblit</author>
<author>Xiaojin Zhu</author>
</authors>
<title>Statistical debugging using latent topic models.</title>
<date>2007</date>
<booktitle>In Stan Matwin and Dunja Mladenic, editors,18th European Conference on Machine Learning,</booktitle>
<location>Warsaw, Poland.</location>
<contexts>
<context position="3298" citStr="Andrzejewski et al., 2007" startWordPosition="509" endWordPosition="512">Our model extends z-labels from single values to subsets, thus offer additional model expressiveness. If the topic-based representations of documents are to be used for document clustering or classification, providing z-labels for words can be seen as similar to semi-supervised learning with labeled features (Druck et al., 2008). Here the words are features, and z-label guidance acts as a feature label. This differs from other supervised LDA variants (Blei and McAuliffe, 2008; Lacoste-Julien et al., 2008) which use document label information. The ΔLDA model for statistical software debugging (Andrzejewski et al., 2007) partitions the topics into 2 sets: “usage” topics which can appear in all Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 43–48, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics documents, and “bug” topics which can only appear in a special subset of documents. This effect was achieved by using different α hyperparameters for the 2 subsets of documents. z-labels can achieve the same effect by restricting the z’s in documents outside the special subset, so that the z’s cannot assume the “bug” topic values. T</context>
</contexts>
<marker>Andrzejewski, Mulhern, Liblit, Zhu, 2007</marker>
<rawString>David Andrzejewski, Anne Mulhern, Ben Liblit, and Xiaojin Zhu. 2007. Statistical debugging using latent topic models. In Stan Matwin and Dunja Mladenic, editors,18th European Conference on Machine Learning, Warsaw, Poland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sugato Basu</author>
<author>Ian Davidson</author>
<author>Kiri Wagstaff</author>
<author>editors</author>
</authors>
<date>2008</date>
<booktitle>Constrained Clustering: Advances in Algorithms, Theory, and Applications.</booktitle>
<publisher>Chapman &amp; Hall/CRC Press.</publisher>
<contexts>
<context position="2048" citStr="Basu et al., 2008" startWordPosition="313" endWordPosition="316">ssignments to LDA. Traditionally, topic assignments have been denoted by the variable z in LDA, and we will call such supervised information “z-labels.” In particular, a z-label is the knowl∗ We would like to acknowledge the assistance of Brandi Gancarz with the biological annotations. This work is supported in part by the Wisconsin Alumni Research Foundation. 43 edge that the topic assignment for a given word position is within a subset of topics. As such, this work is a combination of unsupervised model and supervised knowledge, and falls into the category similar to constrained clustering (Basu et al., 2008) and semi-supervised dimensionality reduction (Yang et al., 2006). 1.1 Related Work A similar but simpler type of topic labeling information has been applied to computer vision tasks. Topic modeling approaches have been applied to scene modeling (Sudderth et al., 2005), segmentation, and classification or detection (Wang and Grimson, 2008). In some of these vision applications, the latent topics themselves are assumed to correspond to object labels. If labeled data is available, either all (Wang and Mori, 2009) or some (Cao and Fei-Fei, 2007) of the z values can be treated as observed, rather </context>
</contexts>
<marker>Basu, Davidson, Wagstaff, editors, 2008</marker>
<rawString>Sugato Basu, Ian Davidson, and Kiri Wagstaff, editors. 2008. Constrained Clustering: Advances in Algorithms, Theory, and Applications. Chapman &amp; Hall/CRC Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Jon McAuliffe</author>
</authors>
<title>Supervised topic models.</title>
<date>2008</date>
<booktitle>Advances in Neural Information Processing Systems 20,</booktitle>
<pages>121--128</pages>
<editor>In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3152" citStr="Blei and McAuliffe, 2008" startWordPosition="488" endWordPosition="491">le, either all (Wang and Mori, 2009) or some (Cao and Fei-Fei, 2007) of the z values can be treated as observed, rather than latent, variables. Our model extends z-labels from single values to subsets, thus offer additional model expressiveness. If the topic-based representations of documents are to be used for document clustering or classification, providing z-labels for words can be seen as similar to semi-supervised learning with labeled features (Druck et al., 2008). Here the words are features, and z-label guidance acts as a feature label. This differs from other supervised LDA variants (Blei and McAuliffe, 2008; Lacoste-Julien et al., 2008) which use document label information. The ΔLDA model for statistical software debugging (Andrzejewski et al., 2007) partitions the topics into 2 sets: “usage” topics which can appear in all Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 43–48, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics documents, and “bug” topics which can only appear in a special subset of documents. This effect was achieved by using different α hyperparameters for the 2 subsets of documents. z-labels c</context>
</contexts>
<marker>Blei, McAuliffe, 2008</marker>
<rawString>David Blei and Jon McAuliffe. 2008. Supervised topic models. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 121–128. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal ofMachine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="916" citStr="Blei et al., 2003" startWordPosition="124" endWordPosition="127">Abstract Latent Dirichlet Allocation is an unsupervised graphical model which can discover latent topics in unlabeled data. We propose a mechanism for adding partial supervision, called topic-in-set knowledge, to latent topic modeling. This type of supervision can be used to encourage the recovery of topics which are more relevant to user modeling goals than the topics which would be recovered otherwise. Preliminary experiments on text datasets are presented to demonstrate the potential effectiveness of this method. 1 Introduction Latent topic models such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) have emerged as a useful family of graphical models with many interesting applications in natural language processing. One of the key virtues of LDA is its status as a fully generative probabilistic model, allowing principled extensions and variations capable of expressing rich problem domain structure (Newman et al., 2007; RosenZvi et al., 2004; Boyd-Graber et al., 2007; Griffiths et al., 2005). LDA is an unsupervised learning model. This work aims to add supervised information in the form of latent topic assignments to LDA. Traditionally, topic assignments have been denoted by the variable </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal ofMachine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>David Blei</author>
<author>Xiaojin Zhu</author>
</authors>
<title>A topic model for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>1024--1033</pages>
<contexts>
<context position="1290" citStr="Boyd-Graber et al., 2007" startWordPosition="186" endWordPosition="189">ics which would be recovered otherwise. Preliminary experiments on text datasets are presented to demonstrate the potential effectiveness of this method. 1 Introduction Latent topic models such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) have emerged as a useful family of graphical models with many interesting applications in natural language processing. One of the key virtues of LDA is its status as a fully generative probabilistic model, allowing principled extensions and variations capable of expressing rich problem domain structure (Newman et al., 2007; RosenZvi et al., 2004; Boyd-Graber et al., 2007; Griffiths et al., 2005). LDA is an unsupervised learning model. This work aims to add supervised information in the form of latent topic assignments to LDA. Traditionally, topic assignments have been denoted by the variable z in LDA, and we will call such supervised information “z-labels.” In particular, a z-label is the knowl∗ We would like to acknowledge the assistance of Brandi Gancarz with the biological annotations. This work is supported in part by the Wisconsin Alumni Research Foundation. 43 edge that the topic assignment for a given word position is within a subset of topics. As such</context>
</contexts>
<marker>Boyd-Graber, Blei, Zhu, 2007</marker>
<rawString>Jordan Boyd-Graber, David Blei, and Xiaojin Zhu. 2007. A topic model for word sense disambiguation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 1024–1033.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liangliang Cao</author>
<author>Li Fei-Fei</author>
</authors>
<title>Spatially coherent latent topic model for concurrent segmentation and classification of objects and scenes.</title>
<date>2007</date>
<booktitle>In ICCV,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="2596" citStr="Cao and Fei-Fei, 2007" startWordPosition="400" endWordPosition="403">ls into the category similar to constrained clustering (Basu et al., 2008) and semi-supervised dimensionality reduction (Yang et al., 2006). 1.1 Related Work A similar but simpler type of topic labeling information has been applied to computer vision tasks. Topic modeling approaches have been applied to scene modeling (Sudderth et al., 2005), segmentation, and classification or detection (Wang and Grimson, 2008). In some of these vision applications, the latent topics themselves are assumed to correspond to object labels. If labeled data is available, either all (Wang and Mori, 2009) or some (Cao and Fei-Fei, 2007) of the z values can be treated as observed, rather than latent, variables. Our model extends z-labels from single values to subsets, thus offer additional model expressiveness. If the topic-based representations of documents are to be used for document clustering or classification, providing z-labels for words can be seen as similar to semi-supervised learning with labeled features (Druck et al., 2008). Here the words are features, and z-label guidance acts as a feature label. This differs from other supervised LDA variants (Blei and McAuliffe, 2008; Lacoste-Julien et al., 2008) which use doc</context>
</contexts>
<marker>Cao, Fei-Fei, 2007</marker>
<rawString>Liangliang Cao and Li Fei-Fei. 2007. Spatially coherent latent topic model for concurrent segmentation and classification of objects and scenes. In ICCV, pages 1– 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gal Chechik</author>
<author>Naftali Tishby</author>
</authors>
<title>Extracting relevant structures with side information.</title>
<date>2002</date>
<booktitle>In NIPS 15,</booktitle>
<pages>857--864</pages>
<publisher>MIT press.</publisher>
<contexts>
<context position="4149" citStr="Chechik and Tishby, 2002" startWordPosition="643" endWordPosition="646">sociation for Computational Linguistics documents, and “bug” topics which can only appear in a special subset of documents. This effect was achieved by using different α hyperparameters for the 2 subsets of documents. z-labels can achieve the same effect by restricting the z’s in documents outside the special subset, so that the z’s cannot assume the “bug” topic values. Therefore, the present approach can be viewed as a generalization of OLDA. Another perspective is that our z-labels may guide the topic model towards the discovery of secondary or non-dominant statistical patterns in the data (Chechik and Tishby, 2002). These topics may be more interesting or relevant to the goals of the user, but standard LDA would ignore them in favor of more prominent (and perhaps orthogonal) structure. 2 Our Model 2.1 Review of Latent Dirichlet Allocation We briefly review LDA, following the notation of (Griffiths and Steyvers, 2004) 1. Let there be T topics. Let w = w1 ... wn represent a corpus of D documents, with a total of n words. We use di to denote the document of word wi, and zi the hidden topic from which wi is generated. Let φ(w) j = p(w|z = j), and θ(d) j= p(z = j) for document d. LDA involves the following g</context>
</contexts>
<marker>Chechik, Tishby, 2002</marker>
<rawString>Gal Chechik and Naftali Tishby. 2002. Extracting relevant structures with side information. In NIPS 15, pages 857–864. MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Druck</author>
<author>Gideon Mann</author>
<author>Andrew McCallum</author>
</authors>
<title>Learning from labeled features using generalized expectation criteria.</title>
<date>2008</date>
<booktitle>In SIGIR</booktitle>
<pages>595--602</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3002" citStr="Druck et al., 2008" startWordPosition="462" endWordPosition="465"> 2008). In some of these vision applications, the latent topics themselves are assumed to correspond to object labels. If labeled data is available, either all (Wang and Mori, 2009) or some (Cao and Fei-Fei, 2007) of the z values can be treated as observed, rather than latent, variables. Our model extends z-labels from single values to subsets, thus offer additional model expressiveness. If the topic-based representations of documents are to be used for document clustering or classification, providing z-labels for words can be seen as similar to semi-supervised learning with labeled features (Druck et al., 2008). Here the words are features, and z-label guidance acts as a feature label. This differs from other supervised LDA variants (Blei and McAuliffe, 2008; Lacoste-Julien et al., 2008) which use document label information. The ΔLDA model for statistical software debugging (Andrzejewski et al., 2007) partitions the topics into 2 sets: “usage” topics which can appear in all Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 43–48, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics documents, and “bug” topics which can </context>
</contexts>
<marker>Druck, Mann, McCallum, 2008</marker>
<rawString>Gregory Druck, Gideon Mann, and Andrew McCallum. 2008. Learning from labeled features using generalized expectation criteria. In SIGIR 2008, pages 595– 602, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<volume>101</volume>
<pages>1--5228</pages>
<contexts>
<context position="4457" citStr="Griffiths and Steyvers, 2004" startWordPosition="694" endWordPosition="697">pecial subset, so that the z’s cannot assume the “bug” topic values. Therefore, the present approach can be viewed as a generalization of OLDA. Another perspective is that our z-labels may guide the topic model towards the discovery of secondary or non-dominant statistical patterns in the data (Chechik and Tishby, 2002). These topics may be more interesting or relevant to the goals of the user, but standard LDA would ignore them in favor of more prominent (and perhaps orthogonal) structure. 2 Our Model 2.1 Review of Latent Dirichlet Allocation We briefly review LDA, following the notation of (Griffiths and Steyvers, 2004) 1. Let there be T topics. Let w = w1 ... wn represent a corpus of D documents, with a total of n words. We use di to denote the document of word wi, and zi the hidden topic from which wi is generated. Let φ(w) j = p(w|z = j), and θ(d) j= p(z = j) for document d. LDA involves the following generative model: θ ∼ Dirichlet(α) zi|θ(di) ∼ Multinomial(θ(di)) φ ∼ Dirichlet(β) wi|zi, φ ∼ Multinomial(φzi), where α and β are hyperparameters for the document-topic and topic-word Dirichlet distributions, respectively. Even though they can be vector valued, for simplicity we assume α and β are scalars, re</context>
<context position="8003" citStr="Griffiths and Steyvers, 2004" startWordPosition="1349" endWordPosition="1352">ns, it can be derived from an undirected extension of LDA (omitted here) which encodes z-labels. The soft constraint Gibbs sampling equation arises naturally from this formulation, which is the basis for the First-Order Logic constraints described later in the future work section. 3 Experiments We now present preliminary experimental results to demonstrate some interesting applications for topicin-set knowledge. Unless otherwise specified, symmetric hyperparameters α = .5 and Q = .1 were used and all MCMC chains were run for 2000 samples before estimating φ and θ from the final sample, as in (Griffiths and Steyvers, 2004). 3.1 Concept Expansion We explore the use of topic-in-set for identifying words related to a target concept, given a set of seed words associated with that concept. For example, a biological expert may be interested in the concept “translation”. The expert would then provide a set of seed words which are strongly related to this concept, here we assume the seed word set {translation,trna,anticodon,ribosome}. We add the hard constraint that zz = 0 for all occurrences of these four words in our corpus of approximately 9,000 yeast-related abstracts. We ran LDA with the number of topics T = 100, </context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, 101(suppl. 1):5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>David M Blei</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Integrating topics and syntax.</title>
<date>2005</date>
<booktitle>In NIPS 17.</booktitle>
<contexts>
<context position="1315" citStr="Griffiths et al., 2005" startWordPosition="190" endWordPosition="193">ed otherwise. Preliminary experiments on text datasets are presented to demonstrate the potential effectiveness of this method. 1 Introduction Latent topic models such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) have emerged as a useful family of graphical models with many interesting applications in natural language processing. One of the key virtues of LDA is its status as a fully generative probabilistic model, allowing principled extensions and variations capable of expressing rich problem domain structure (Newman et al., 2007; RosenZvi et al., 2004; Boyd-Graber et al., 2007; Griffiths et al., 2005). LDA is an unsupervised learning model. This work aims to add supervised information in the form of latent topic assignments to LDA. Traditionally, topic assignments have been denoted by the variable z in LDA, and we will call such supervised information “z-labels.” In particular, a z-label is the knowl∗ We would like to acknowledge the assistance of Brandi Gancarz with the biological annotations. This work is supported in part by the Wisconsin Alumni Research Foundation. 43 edge that the topic assignment for a given word position is within a subset of topics. As such, this work is a combinat</context>
</contexts>
<marker>Griffiths, Steyvers, Blei, Tenenbaum, 2005</marker>
<rawString>Thomas L. Griffiths, Mark Steyvers, David M. Blei, and Joshua B. Tenenbaum. 2005. Integrating topics and syntax. In NIPS 17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lacoste-Julien</author>
<author>F Sha</author>
<author>M Jordan</author>
</authors>
<title>Disclda: Discriminative learning for dimensionality reduction and classification.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems</booktitle>
<volume>21</volume>
<pages>08</pages>
<contexts>
<context position="3182" citStr="Lacoste-Julien et al., 2008" startWordPosition="492" endWordPosition="495">ori, 2009) or some (Cao and Fei-Fei, 2007) of the z values can be treated as observed, rather than latent, variables. Our model extends z-labels from single values to subsets, thus offer additional model expressiveness. If the topic-based representations of documents are to be used for document clustering or classification, providing z-labels for words can be seen as similar to semi-supervised learning with labeled features (Druck et al., 2008). Here the words are features, and z-label guidance acts as a feature label. This differs from other supervised LDA variants (Blei and McAuliffe, 2008; Lacoste-Julien et al., 2008) which use document label information. The ΔLDA model for statistical software debugging (Andrzejewski et al., 2007) partitions the topics into 2 sets: “usage” topics which can appear in all Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 43–48, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics documents, and “bug” topics which can only appear in a special subset of documents. This effect was achieved by using different α hyperparameters for the 2 subsets of documents. z-labels can achieve the same effect by </context>
</contexts>
<marker>Lacoste-Julien, Sha, Jordan, 2008</marker>
<rawString>S. Lacoste-Julien, F. Sha, and M. Jordan. 2008. Disclda: Discriminative learning for dimensionality reduction and classification. In Advances in Neural Information Processing Systems 21 (NIPS08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Kat Hagedorn</author>
<author>Chaitanya Chemudugunta</author>
<author>Padhraic Smyth</author>
</authors>
<title>Subject metadata enrichment using statistical topic models.</title>
<date>2007</date>
<booktitle>In JCDL ’07: Proceedings of the 7th ACM/IEEE-CS joint conference on Digital libraries,</booktitle>
<pages>366--375</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1241" citStr="Newman et al., 2007" startWordPosition="177" endWordPosition="180">relevant to user modeling goals than the topics which would be recovered otherwise. Preliminary experiments on text datasets are presented to demonstrate the potential effectiveness of this method. 1 Introduction Latent topic models such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) have emerged as a useful family of graphical models with many interesting applications in natural language processing. One of the key virtues of LDA is its status as a fully generative probabilistic model, allowing principled extensions and variations capable of expressing rich problem domain structure (Newman et al., 2007; RosenZvi et al., 2004; Boyd-Graber et al., 2007; Griffiths et al., 2005). LDA is an unsupervised learning model. This work aims to add supervised information in the form of latent topic assignments to LDA. Traditionally, topic assignments have been denoted by the variable z in LDA, and we will call such supervised information “z-labels.” In particular, a z-label is the knowl∗ We would like to acknowledge the assistance of Brandi Gancarz with the biological annotations. This work is supported in part by the Wisconsin Alumni Research Foundation. 43 edge that the topic assignment for a given wo</context>
</contexts>
<marker>Newman, Hagedorn, Chemudugunta, Smyth, 2007</marker>
<rawString>David Newman, Kat Hagedorn, Chaitanya Chemudugunta, and Padhraic Smyth. 2007. Subject metadata enrichment using statistical topic models. In JCDL ’07: Proceedings of the 7th ACM/IEEE-CS joint conference on Digital libraries, pages 366–375, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michal Rosen-Zvi</author>
<author>Thomas Griffiths</author>
<author>Mark Steyvers</author>
<author>Padhraic Smyth</author>
</authors>
<title>The author-topic model for authors and documents.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th conference on Uncertainty in artificial intelligence (UAI),</booktitle>
<pages>487--494</pages>
<publisher>AUAI Press.</publisher>
<location>Arlington, Virginia, United States.</location>
<marker>Rosen-Zvi, Griffiths, Steyvers, Smyth, 2004</marker>
<rawString>Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, and Padhraic Smyth. 2004. The author-topic model for authors and documents. In Proceedings of the 20th conference on Uncertainty in artificial intelligence (UAI), pages 487–494, Arlington, Virginia, United States. AUAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik B Sudderth</author>
<author>Antonio B Torralba</author>
<author>William T Freeman</author>
<author>Alan S Willsky</author>
</authors>
<title>Learning hierarchical models of scenes, objects, and parts.</title>
<date>2005</date>
<booktitle>In ICCV,</booktitle>
<pages>1331--1338</pages>
<contexts>
<context position="2317" citStr="Sudderth et al., 2005" startWordPosition="354" endWordPosition="357">ological annotations. This work is supported in part by the Wisconsin Alumni Research Foundation. 43 edge that the topic assignment for a given word position is within a subset of topics. As such, this work is a combination of unsupervised model and supervised knowledge, and falls into the category similar to constrained clustering (Basu et al., 2008) and semi-supervised dimensionality reduction (Yang et al., 2006). 1.1 Related Work A similar but simpler type of topic labeling information has been applied to computer vision tasks. Topic modeling approaches have been applied to scene modeling (Sudderth et al., 2005), segmentation, and classification or detection (Wang and Grimson, 2008). In some of these vision applications, the latent topics themselves are assumed to correspond to object labels. If labeled data is available, either all (Wang and Mori, 2009) or some (Cao and Fei-Fei, 2007) of the z values can be treated as observed, rather than latent, variables. Our model extends z-labels from single values to subsets, thus offer additional model expressiveness. If the topic-based representations of documents are to be used for document clustering or classification, providing z-labels for words can be s</context>
</contexts>
<marker>Sudderth, Torralba, Freeman, Willsky, 2005</marker>
<rawString>Erik B. Sudderth, Antonio B. Torralba, William T. Freeman, and Alan S. Willsky. 2005. Learning hierarchical models of scenes, objects, and parts. In ICCV, pages 1331–1338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Tjong Kim Sang</author>
<author>Fien De Meulder</author>
</authors>
<title>Introduction to the conll-2003 shared task: Languageindependent named entity recognition.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL-2003,</booktitle>
<pages>142--147</pages>
<location>Edmonton, Canada.</location>
<marker>Sang, De Meulder, 2003</marker>
<rawString>Erik Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Languageindependent named entity recognition. In Proceedings of CoNLL-2003, pages 142–147, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaogang Wang</author>
<author>Eric Grimson</author>
</authors>
<title>Spatial latent dirichlet allocation.</title>
<date>2008</date>
<journal>NIPS</journal>
<volume>20</volume>
<pages>1577--1584</pages>
<editor>In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2389" citStr="Wang and Grimson, 2008" startWordPosition="364" endWordPosition="367">lumni Research Foundation. 43 edge that the topic assignment for a given word position is within a subset of topics. As such, this work is a combination of unsupervised model and supervised knowledge, and falls into the category similar to constrained clustering (Basu et al., 2008) and semi-supervised dimensionality reduction (Yang et al., 2006). 1.1 Related Work A similar but simpler type of topic labeling information has been applied to computer vision tasks. Topic modeling approaches have been applied to scene modeling (Sudderth et al., 2005), segmentation, and classification or detection (Wang and Grimson, 2008). In some of these vision applications, the latent topics themselves are assumed to correspond to object labels. If labeled data is available, either all (Wang and Mori, 2009) or some (Cao and Fei-Fei, 2007) of the z values can be treated as observed, rather than latent, variables. Our model extends z-labels from single values to subsets, thus offer additional model expressiveness. If the topic-based representations of documents are to be used for document clustering or classification, providing z-labels for words can be seen as similar to semi-supervised learning with labeled features (Druck </context>
</contexts>
<marker>Wang, Grimson, 2008</marker>
<rawString>Xiaogang Wang and Eric Grimson. 2008. Spatial latent dirichlet allocation. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, NIPS 20, pages 1577–1584. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Wang</author>
<author>Greg Mori</author>
</authors>
<title>Human action recognition by semi-latent topic models.</title>
<date>2009</date>
<booktitle>In IEEE Transactions on Pattern Analysis and Machine Intelligence.</booktitle>
<contexts>
<context position="2564" citStr="Wang and Mori, 2009" startWordPosition="394" endWordPosition="397"> supervised knowledge, and falls into the category similar to constrained clustering (Basu et al., 2008) and semi-supervised dimensionality reduction (Yang et al., 2006). 1.1 Related Work A similar but simpler type of topic labeling information has been applied to computer vision tasks. Topic modeling approaches have been applied to scene modeling (Sudderth et al., 2005), segmentation, and classification or detection (Wang and Grimson, 2008). In some of these vision applications, the latent topics themselves are assumed to correspond to object labels. If labeled data is available, either all (Wang and Mori, 2009) or some (Cao and Fei-Fei, 2007) of the z values can be treated as observed, rather than latent, variables. Our model extends z-labels from single values to subsets, thus offer additional model expressiveness. If the topic-based representations of documents are to be used for document clustering or classification, providing z-labels for words can be seen as similar to semi-supervised learning with labeled features (Druck et al., 2008). Here the words are features, and z-label guidance acts as a feature label. This differs from other supervised LDA variants (Blei and McAuliffe, 2008; Lacoste-Ju</context>
</contexts>
<marker>Wang, Mori, 2009</marker>
<rawString>Yang Wang and Greg Mori. 2009. Human action recognition by semi-latent topic models. In IEEE Transactions on Pattern Analysis and Machine Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Yang</author>
<author>Haoying Fu</author>
<author>Hongyuan Zha</author>
<author>Jesse Barlow</author>
</authors>
<title>Semi-supervised nonlinear dimensionality reduction.</title>
<date>2006</date>
<booktitle>In ICML-06, 23nd International Conference on Machine Learning.</booktitle>
<contexts>
<context position="2113" citStr="Yang et al., 2006" startWordPosition="321" endWordPosition="324">oted by the variable z in LDA, and we will call such supervised information “z-labels.” In particular, a z-label is the knowl∗ We would like to acknowledge the assistance of Brandi Gancarz with the biological annotations. This work is supported in part by the Wisconsin Alumni Research Foundation. 43 edge that the topic assignment for a given word position is within a subset of topics. As such, this work is a combination of unsupervised model and supervised knowledge, and falls into the category similar to constrained clustering (Basu et al., 2008) and semi-supervised dimensionality reduction (Yang et al., 2006). 1.1 Related Work A similar but simpler type of topic labeling information has been applied to computer vision tasks. Topic modeling approaches have been applied to scene modeling (Sudderth et al., 2005), segmentation, and classification or detection (Wang and Grimson, 2008). In some of these vision applications, the latent topics themselves are assumed to correspond to object labels. If labeled data is available, either all (Wang and Mori, 2009) or some (Cao and Fei-Fei, 2007) of the z values can be treated as observed, rather than latent, variables. Our model extends z-labels from single va</context>
</contexts>
<marker>Yang, Fu, Zha, Barlow, 2006</marker>
<rawString>Xin Yang, Haoying Fu, Hongyuan Zha, and Jesse Barlow. 2006. Semi-supervised nonlinear dimensionality reduction. In ICML-06, 23nd International Conference on Machine Learning.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>