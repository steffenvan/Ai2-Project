<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000015">
<note confidence="0.770332">
THE COMPUTATIONAL DIFFICULTY OF ID/LP
PARSING
G. Edward Barton, Jr.
M.I.T. Artificial Intelligence Laboratory
545 Technology Square
Cambridge, MA 02139
</note>
<sectionHeader confidence="0.907532" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999452058823529">
Modern linguistic theory attributes surface complexity
to interacting subsystems of constraints. For instance, the
ID. LP grammar formalism separates constraints
on immediate dominance front those on linear order.
Shieber&apos;s (1983) ID/LP parsing algorithm shows how to
use ID and LP constraints directly in language process-
ing, without expanding them into an intermediate &amp;quot;object
grammar.&amp;quot; However, Shieber&apos;s purported O(GI&apos;&amp;quot; • n3) run-
time bound underestimates t he difficulty of ID/LP parsing.
ID/ LP parsing is actually NP-complete, and the worst-case
runtime of Shieber&apos;s algorithm is actually exponential in
grammar size. The growth of parser data structures causes
the difficulty. Some computational and linguistic implica-
tions follow; in particular, it is important to note that
despite its potential for combinatorial explosion, Shieber&apos;s
algorithm remains better than the alternative of parsing
an expanded object grammar.
</bodyText>
<sectionHeader confidence="0.998998" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.994761041666667">
Recent linguistic theories derive surface complexity
from modular subsystems of constraints; Chomsky (1981:5)
proposes separate theories of bounding, government,
0-marking, and so forth. while Gazdar and Pullum&apos;s GPSG
formalism (Shieber. 1983.21f) uses immediate-dominance
( ID) rules, linear-precedence (LP) constraints, and
inetarules. When modular constraints are involved, rule
sys.tems that multiply out their surface effects are large
and clumsy (see Barton. 1984a). The expanded context-
free &amp;quot;object grammar&amp;quot; that multiplies out the constraints
in a typical GPSG system would contain trillions of rules
(Shieber, 1983:1).
Shicber (1983) thus leads in a welcome direction by
,howing how 1D, LP grammars can he parsed &amp;quot;directly,&amp;quot;
wit !tout the conibinatorially explosive step of multiplying
out the effects of the ID and Ll&apos; constraints. Shieber&apos;s
algorithm applies ID and LP constraints one step at a
time.is needed. However, some doubts about computa-
tional complexity remain. Shieber (1983:15) argues that
his algorithm is identical to Earley&apos;s in time complexity,
but this result seems almost too much to hope for. An
ID4.1) grammar C can be much smaller than an equiva-
lent context-free grammar C&apos;; for example, if G1 contains
only the rule S —913 abcde, the corresponding C&apos;1 contains
</bodyText>
<listItem confidence="0.53788">
5! 120 rules. If Shieber&apos;s algorithm has the same time
</listItem>
<bodyText confidence="0.958980470588235">
complexity as Earley&apos;s, this brevity of expression comes
free (up to a constant). Shieber says little to allay possible
doubts:
We will not pre,ent a rigorous demonstration of time
complexity. but it should ht chin from the close relation
hempen the presented algorit lull and Earley:s that the
complexity is that of Earley:s algoritlim. In the worst
ri,P. where the LI&apos; rules always specify a unique order-
fir t!a• right-hand sue&apos;ft ycry ID roie. the presented
;;I”:orithen rhice: F,arley:: algorulim Sittet• given
the erammar. checking :ie. LP rules takes constant time.
Ow time CrMildt•Xlly of the presented al•zorit fun is iden-
tical to finley ... That is. it is GI G 2 rt . :G:
tIi in of the .gratumar (number of ID rules) and n
is the length of the input. (:1-1f)
Among other questions, it is unclear why a situation of
maximal constraint should represent the worst case. Mini-
ma/ constraint may mean that there are more possibilities
to consider.
Shieber&apos;s algorithm does have a time advantage over
the use of Earley&apos;s algorithm on the expanded CFC, but
it blows up in the worst case; the claim of 0(:C&apos;&apos;2 • n3)
time complexity is mistaken. A reduction of the vertex-
cover problem shows that ID /LP parsing is actually NP-
complete; hence this blowup arises from the inherent diffi-
culty of ID &apos;LP parsing rather titan a defect in Shieber&apos;s al-
gorithm (unless P = .VP). The following sections explain
and discuss this result. LP constraints are neglected be-
cause it is the ID riles that make parsing difficult.
Attention focuses on unordered context-free grammars
(1&apos;,CFGs; essentially, ID/LP grammars sans LP). A L;CFG
rule :s like a standard CIT. rule except that when used in a
derivation it may have the symbols tti its expansion writ-
ten in any order.
</bodyText>
<sectionHeader confidence="0.969119" genericHeader="method">
SHIEBER&apos;S ALGORITHM
</sectionHeader>
<bodyText confidence="0.882554857142857">
Shieber generalizes Earley&apos;s algorithm by generalizing
the dotted-rule representation that Earley uses to track
progress through rule expansions. A UCFG rule differs
front a CFG rule only in that its right-hand side is un-
ordered; hence successive accumulation of set elements re-
places linear advancement through a sequence. Obvious
interpretations follow for the operations that the Earley
</bodyText>
<footnote confidence="0.405139">
parser performs on dotted rules: X (HA, B, C} i3 a
</footnote>
<page confidence="0.627413">
76
</page>
<equation confidence="0.9578556">
typical initial state for a dotted LICFG rule;
X — {A, B, C}.{} is a tjTical completed state;
Z {W}.{a, X, Y} predicts terminal a and nontermi-
nals X, Y; and X {A}.{B,C, CI should be advanced
to X (A, CHB, CI after the predicted C is located.&apos;
</equation>
<bodyText confidence="0.995923772727273">
Except for these changes, Shieber&apos;s algorithm is identical
to Earley&apos;s.
As Shieber hoped, direct parsing is better than using
Earley&apos;s algorithm on an expanded grammar. If Shieber&apos;s
parser is used to parse abcde according to Ci, the state
sets of the parser remain small. The first state set con-
tains only S {} b, c, d, el, 0, the second state set
contains only {a}.{b, c, d, el , 01, and so forth. The
state sets grow much larger if the Earley parser is used to
parse the string according to G&amp;quot;, with its 120 rules. After
the first terminal a has been processed, the second state set
of the Earley parser contains .1! -- 2.1 states spelling out all
possible orders in which the remaining symbols {b,e,d,e}
may appear: :5 a.bcde, 0 5 --• li.redb,0■, and so on.
Shieber&apos;s parser should be faster, since both parsers work
by successively processing all of the states in the state sets.
Similar examples show that the Shieber parser can have
an arbitrarily large advantage over the use of the Earley
parser on the object grammar.
Shieber&apos;s parser does not always enjoy such a large ad-
vantage; in fact it can blow tip in the presence of ambiguity.
Derive G2 by modifying CI in two ways. First, introduce
dummy categories A. 13,C, D, E so that A a and so
forth, with S ABCDE. Second, !et. z be ambiguously
in any of the categories A, B, C, D, E so that the rule for
A becomes A — a ; x and so on. What happens when
the string xxxxa is parsed according to C27 After the first
three occurrences of x, the state set of the parser will reflect
the possibility that any three of the phrases A, 13,C, D, E
might have been seen and any two of them might remain to
be parsed. There will be (53) = 10 states reflecting progress
through the rule expanding S; — {A, B,C}.{D, E}, 01
will be in the state set, as will (5 — (A, C, E} .{B, D},0!,
etc. There will also be 15 states reflecting the completion
and prediction of phrases. In cases like this, Shieber&apos;s al-
gorithm enumerates all of the combinations of k elements
taken i at a time, where k is the rule length and i is the
number of elements already processed. Thus it can be
combinatorially explosive. Note, however, that Shieber&apos;s
algorithm is still better than parsing the object grammar.
With the Earley parser, the state set would reflect the same
possibilities, but encoded in a less concise representation.
In place of the state involving S {A, B, CI .{D, E),
for instance, there would be 3! • 2! = 12 states involving
</bodyText>
<subsectionHeader confidence="0.686836">
S ALIC.DE, S BC A.ED, and so forth.&apos; Instead
</subsectionHeader>
<bodyText confidence="0.93683735483871">
For more details see Barton (1984bi anti Shieber (1983). Shieher&apos;s
representation differs in sonic ways from the representation de-
scribed here. which tots developed independently by the author.
The differences are generally inessential. hut see tune 2.
2 In contrast to the represent:it .illitstrateil here. Shieber&apos;s rep-
resentation actually suffers Li.st extent from the saute prob-
of a total of 25 states, the Earley state set would contain
135 = 12 • 10 + 15 states.
With G2, the parser could not be sure of the categorial
identities of the phrases parsed, but at least it was certain
of the number and extent of the phrases. The situation gets
worse if there is uncertainty in those areas as well. Derive
G3 by replacing every x in G2 with the empty string E so
that an A, for instance, can be either a or nothing. Before
any input has been read, state set So in Shieber&apos;s parser
must reflect the possibility that the correct parse may in-
clude any of the 25 = 32 possible subsets of A, B, C, D, E
as empty initial constituents. For example, So must in-
clude IS — {A, B,C, D, El. {}, 01 because the input might
turn out to be the null string. Similarly, 5,, must include
,5 (A, C, El .{13, D}, Ot because the input might be bd
or db. Counting all possible subsets in addition to other
states having to do with predictions, completions, and the
parser start symbol that smile implementations introduce,
there will be 44 states in So. (There are 338 states in the
corresponding state when the object grammar is used.)
How can Shieber&apos;s algorithm be exponential in gram-
mar size despite its similarity to Earley&apos;s algorithm,
which is polynomial in grammar size? The answer is that
Shieber&apos;s :tlgorithm involves a notch larger bound on the
number of states in a state set. Since the Earley parser
successively processes all of the states in each state set
(Earley, 1970:97), an explosion in the size of the state sets
kills any small runtime bound.
Consider the Earley parser. Resulting from each rule
X Ak in a grammar G„ there are only k – 1 pos-
sible dotted rules. The number of possible dotted rules
is thus bounded by the number of symbols that it takes
to write C„ down, i.e. by :C„,. Since an Earley state
just pairs a dotted rule with an interword position ranging
front 0 to the length n of the input string, there are only
• n) possible states; hence no state set may contain
more than 0(C0; • n) (distinct) states. By an argument
due to Earley, this limit allows an OCC„.&amp;quot; bound to
he placed on Earley-parser runtime. In contrast, the state
sets of Shieher&apos;s parser may grow much larger relative to
grammar size. A rule X Ai...Ak in a CCFG C yields
not k 1 ordinary dotted rules, but but 2&apos; possible dot-
ted UCFC rules tracking accumulation of set elements. In
the worst case the grammar contains only one rule and k
is on the order of hence a bound on the number of
possible dotted UCFG rules is not given by 0( but
by 0(2 &amp;quot;^ ). (Recall the exponential blowup illustrated for
grammar Ci.) The parser sometimes blows up because
there are exponentially more possible ways to to progress
through an unordered rule expansion than an through an
ordered one. In ID/LP parsing, the easiest case occurs
Shieber (1983:10) uses an ordered sequence instead of A mini.
tism before the 1101: consequently. in place of the state involving
S (A. /3.(7).{D. Shieber would have the 3! = 6 states in-
volving S ut. {D. E}, where a ranges over the six permutations of
ABC.
</bodyText>
<page confidence="0.996119">
77
</page>
<table confidence="0.9634376">
START IIIII21I3H4UUDDDD
U aaaa bbbb cccc dddd
H2 61 c D
H3-4Cld
H4 –461d
</table>
<figureCaption confidence="0.999383333333333">
Figure 1: This graph illustrates a trivial instance of the
vertex cover problem. The set {c,d} is a vertex cover of
size 2.
</figureCaption>
<bodyText confidence="0.9991806">
when the LP constraints force a unique ordering for ev-
ery rule expansion. Given sufficiently strong constraints,
Shieber&apos;s parser reduces to Earley&apos;s as Shieber thought,
but strong constraint represents the best case computa-
tionally rather than the worst case.
</bodyText>
<sectionHeader confidence="0.714702" genericHeader="method">
NP-COMPLETENESS
</sectionHeader>
<bodyText confidence="0.963608658536585">
The worst-case time complexity of Shieber&apos;s algorithm
is exponential in grammar size rather than quadratic as
Shiebcr (1983:15) believed. Did Shieber choose a poor al-
gorithm, or is ID/LP parsing inherently difficult? In fact,
the simpler problem of recognuang sentences according to a
UCFG is NP-complete. Thus, unless P = NP, no ID/LP
parsing algorithm can always run in time polynomial in
the combined size of grammar and input. The proof is a
reduction of the vertex cover problem (Carey and John-
son, 1979:46), which involves finding a small set of vertices
in a graph such that every edge of the graph has an end-
point in the set. Figure I gives a trivial example.
To make the parser decide whether the graph in Fig-
ure 1 has a vertex cover of size 2, take the vertex names a,
b, c, and d as the alphabet. Take HI through H4 as special
symbols, one per edge; also take U and D as dummy sym-
bols. Next, encode the edges of the graph: for instance,
edge el runs from a to c, so include the rules HI –* a and
c. Rules for the dummy symbols are also needed.
Dummy symbol D will be used to soak up excess input
symbols, so D a through D d should be rules.
Dummy symbol U will also soak up excess input symbols,
but U will be allowed to match only when there are four
occurrences in a row of the same symbol (one occurrence
for each edge). Take U aaaa, U bbbb, and U cccc,
and U dddci as the rules expanding U.
Now, what does it take for the graph to have a vertex
cover of size k = 2? One way to get a vertex cover is to go
through the list of edges and underline one endpoint of each
edge. If the vertex cover is to be of size 2, the underlining
must be done in such a way that only two distinct vertices
are ever touched in the process. Alternatively, since there
are 4 vertices in all, the vertex cover will be of size 2 if there
are 4 – 2 = 2 vertices left untouched in the underlining.
This method. of finding a vertex cover can be translated
Figure 2: For k = 2, the construction described in the text
transforms the vertex-cover problem of Figure 1 into this
UCFG. A parse exists for the string aaaabbbbccccdddd if
the graph in the previous figure has a vertex cover of size
&lt;2.
into an initial rule for the UCFG, as follows:
</bodyText>
<sectionHeader confidence="0.299952" genericHeader="method">
START 11111211.3114UUDDDD
</sectionHeader>
<bodyText confidence="0.9851618">
Each 11-symbol will match one of the endpoints of the
corresponding edge, each (i-symbol will correspond to a
vertex that was left untouched by the If-matching, and
the D-symbols are just for bookkeeping. (Note that this is
the only rule in the construction that makes essential use
of the unordered nature of rule right-hand sides.) Figure 2
shows the complete grammar that encodes the vertex-cover
problem of Figure 1.
To make all of this work properly, take
a = aaaabbbbccccdddd
as the input string to be parsed. (For every vertex name x,
include in a a contiguous run of occurrences of x, one for
each edge in the graph.) The grammar encodes the under-
lining procedure by requiring each /f-symbol to match one
of its endpoints in a. Since the expansion of the START
rule is unordered, an If-symbol can match anywhere in a,
hence can match any vertex name (subject to interference
from previously matched rules). Furthermore, since there
is one occurrence of each vertex name for every edge, it&apos;s
impossible to run out of vertex-name occurrences. The
grammar will allow either endpoint of an edge to be &amp;quot;un-
derlined&apos;. that is, included in the vertex cover — so the
parser must figure out which vertex cover to select. How-
ever, the grlunmar also requires two occurrences of U to
match. U can only match four contiguous identical input
symbols that have not been matched in any other way;
thus if the parser chooses too large a vertex cover, the U-
symbols will not match and the parse will fail. The proper
number of D-symbols equals the length of the input string,
minus the number of edges in the graph (to account for the
11,-matches), minus k times the number of edges (to ac-
count for the U-matches): in this case, 16 – 4 – (2 • 4) = 4,
as illustrated in the START rule.
The result of this construction is that in order to decide
whether a is in the language generated by the UCFG, the
</bodyText>
<page confidence="0.989342">
78
</page>
<figure confidence="0.97266675">
START
U H1H2H3D DDD
AAIIII
aaaabbbb c c C C d ddd
</figure>
<figureCaption confidence="0.9844554">
Figure 3: The grammar of Figure 2, which encodes the
vertex-cover problem of Figure 1, generates the string
a = aaaabbbbccectIddr1 according to this parse tree. The
vertex cover {c, d} can be read off from the parse- tree as
the set of elements dominated by if -symbols.
</figureCaption>
<bodyText confidence="0.992867166666667">
parser must search for a vertex cover of size 2 or less.&apos; If
a parse exists, an appropriate vertex cover can be read off
from beneath the H-symbols in the parse tree; conversely,
if an appropriate vertex cover exists, it shows how to con-
struct a parse. Figure 3 shows the parse tree that encodes a
solution to the vertex-cover problem of Figure 1. The con-
struction thus reduces Vertex Cover to UCFG recognition,
and since the construction can be carried out in polyno-
mial time, it follows that UCFG recognition and the more
general task of ID/LP parsing must be computationally
difficult. For a more detailed treatment of the reduction,
see Barton (1984b).
</bodyText>
<sectionHeader confidence="0.960775" genericHeader="conclusions">
IMPLICATIONS
</sectionHeader>
<figureCaption confidence="0.802863411764706">
The reduction of Vertex Cover shows that the 1D/LP
parsing problem is NP-complete; unless P = NP, its time
complexity is not bounded by any polynomial in the size of
the grammar and input. Hence complexity analysis must
be done carefully: despite similarity to Earley&apos;s algorithm,
Shieber&apos;s algnfithin does not have complexity 0(1C12. n3),
but can sometimes undergo exponential growth of its in-
ternal structures. Other computational and linguistic con-
sequences also follow.
Although Shieber&apos;s parser sometimes blows up, it re-
mains better than the alternative of parsing an expanded
&amp;quot;object grammar.&amp;quot; The NP-completeness result shows that
the general case of 11)/LP parsing is inherently difficult;
hence it is not surprising that Shieber&apos;s ID/LP parser some-
times suffers from combinatorial explosion. It is more im-
portant to note that parsing with the expanded CFG blows
up in easy cases. It should not be hard to parse the Ian-
</figureCaption>
<bodyText confidence="0.997629052631579">
TI the vertex cover is smaller than expected, the D-symbolswiU
soak up the extra contignotw runs that could have been matched by
more U-symbols.
guage that consists of all permutations of the string abcde,
but in so doing, the Earley parser can use 24 states or more
to encode what the Shieber parser encodes in only one (re-
call GI). The significant fact is not that the Shieber parser
can blow up; it is that the use of the object grammar blows
up unnecessarily.
The construction that reduces the Vertex Cover prob-
lem to ID/LP Parsing involves a grammar and input string
that both depend on the problem instance; hence it leaves
it open that a clever programmer might concentrate most
of the computational difficulty of ID/LP parsing into an
offline grammar-precompilation stage independent of the
input — under optimistic hopes, perhaps reducing the time
required for parsing an input (after precompilation) to a
polynomial function of grammar size and input length.
Shieber&apos;s algorithm has no precompilation step,&apos; so the
present complexity results apply with full force; any pos-
sible precompilation phase remains hypothetical. More-
over, it is not clear that a clever precompilation step is
even possible. For example, if n enters into the true com-
plexity of ID/LP parsing as a factor multiplying an expo-
nential, an input-independent precompilation phase can-
not help enough to make the parsing phase always run in
polynomial time. On a related note, suppo,e the precom-
pilation step is conversion to CR; form and the runtime
algorithm is the Earley parser. Although the precompila-
tion step does a potentially exponential amount of work in
producing G&apos; from G, another exponential factor shows up
at runtime because C&apos; in the complexity bound G&apos; 2 R3
is exponentially larger than the original C.
The NP-completeness result would be strengthened if
the reduction used the same ,grantinar for all vertex-cover
problems, for it would follow that precompilation could
not bring runtime down to polynomial time. However,
unless P = i P, there can be no such reduction. Since
grammar size would not count as a parameter of a fixed-
grammar ID/LP parsing problem, the use of the Earley
parser on the object grammar would already constitute a
polynomial-time algorithm for solving it. (See the next
section for discussion.)
The Vertex Cover reduction also helps pin down the
computational power of UCFGs. As GI and C, illus-
trated, a UCFG (or an ID/LP grammar) is sometimes
much smaller than an equivalent CFO. The NP-complete-
ness result illuminates this property in three ways. First,
the reduction shows that enough brevity is gained so that
an instance of any problem in .4 P can be stated in a UCFG
that is only polynomially larger than the original problem
instance. In contrast, the current polynomial-time reduc-
tion could not be carried out with a CFG instead of a
UCFG, since the necessity of spelling out all the orders in
which symbols !night appear could make the CFG expo-
nentially larger than the instance. Second, the reduction
shows that this brevity of expression is not free. CFG
</bodyText>
<footnote confidence="0.9871905">
4Shieber (198315 n. 6) mentions a possible precompilation step, but
it is concerned with the LI&apos; relation rather than the Ill nays.
</footnote>
<page confidence="0.998724">
79
</page>
<bodyText confidence="0.999756577319588">
recognition can be solved in cubic time or less, but unless
P =.WP, general UCFG recognition cannot be solved in
polynomial time. Third, the reduction shows that only
one essential use of the power to permute rule expansions
is necessary to make the parsing problem NP-complete,
though the rule in question may need to be arbitrarily
long.
Finally, the ID/LP parsing problem illustrates how
weakness of constraint can make a problem computation-
ally difficult. One might perhaps think that weak
constraints would make a problem easier since weak con-
straints sound easy to verify, but it often takes strong con-
straints to reduce the number of possibilities that an algo-
rithm must consider. In the present case, the removal of
constraints on constituent order causes the dependence of
the runtime bound on grammar size to grow from IG12 to
The key factors that cause difficulty in ID/LP parsing
are familiar to linguistic theory. GB-theory and GPSG
both permit the existence of constituents that are empty
on the surface, and thus in principle they both allow the
kind of pathology illustrated by C3, subject to ameliora-
tion by additional constraints. Similarly, every current
theory acknowledges lexical ambiguity, a key ingredient of
the vertex-cover reduction. Though the reduction illumi-
nates the power of certain mechanisms and formal devices,
the direct implications of the NP-completeness result for
grammatical theory are few.
The reduction does expose the weakness of attempts
to link context-free generative power directly to efficient
parsability. Consider, for instance, Gazdar&apos;s (1981:155)
claim that the use of a formalism with only context-free
power can help explain the rapidity of human sentence
processing:
Suppose ... that the permitted class of genera-
tive grammars constituted a subset of those phrase
St ructure grammars capable only of generating con-
text-free languages. Such a move would have two
iii port ant tnet atheoretical consequences, one hav-
ing to do with learnability, the other with procnss-
ability ... We would have the beginnings of an ex-
planatiett for the obvious, but largely ignored, fact
that humans process the utterances they hear very
rapidly. :.entences of .L context-free language are
provably parsabh: ut A uine I hat is proportional to
the cube of the length of the sentence or less.
As previously remarked, the use of Earley&apos;s algorithm on
the expanded object grammar constitutes a parsing method
for the fixed-grammar 1D/LP parsing problem that is in-
deed no worse than cubic in sentence length. However, the
most important aspect of this possibility is that it is devoid
of practical significance. The object grammar could con-
tain trillions of rules in practical cases (Shieber, 1983:4).
If 1C2 • n3 complexity is too slow, then it remains too slow
when !GT is regarded as a constant. Thus it is impossi-
ble to sustain this particular argument for the advantages
of such formalisms as GPSG over other linguistic theo-
ries; instead, GPSG and other modern theories seem to
be (very roughly) in the same boat with respect to com-
plexity. In such a situation, the linguistic merits of various
theories are more important than complexity results. (See
Berwick (1982), Berwick and Weinberg (1984), and Ris-
tad (1985) for further discussion.)
The reduction does not rule out the use of formalisms
that decouple ID and LP constraints; note that Shieber&apos;s
direct parsing algorithm wins out over the use of the object
grammar. However, if we assume that natural languages
are efficiently parsable (EP), then computational difficul-
ties in parsing a formalism do indicate that the formalism
itself fails to capture whatever constraints are responsible
for making natural languages EP. If the linguistically rel-
evant ID/LP grammars are EP but the general ID/LP
grammars are not, there must be additional factors that
guarantee, say, a certain amount of constraint from the LP
relation.&apos; (Constraints beyond the bare ID, LP formalism
are required on linguistic grounds as well.) The subset
principle of language acquisition (cf. Berwick and Wein-
berg, 1984:233) would lead the language !carnet to initially
hypothesize strong order constraints, to be weakened only
in response to positive evidence.
However, there are other potential ways to guarantee
that languages will be EP. It is possible that the principles
of grammatical theory permit languages that are not EP
in the worst case, just as grazumatical theory allows sen-
tences that are deeply center-embedded (Miller and Chom-
sky, 1963). Difficult languages or sentences still would not
turn lip in general use, precisely because they would be dif-
ficult to process.° The factors making languages EP would
not be part of grammatical theory because they would
represent extragrammatical factors, i.e. the resource lim-
itations of the language-processing mechanisms. In the
same way, the limitations of language-acquisition mech-
anisms might make hard-to-parse languages inaccessible
to the language learner in spite of satisfying grammatical
constraints. However, these &amp;quot;easy explanations&amp;quot; are not
tenable without a detailed account of processing mecha-
nisms; correct uredictions are necessary about which con-
structions will be easy to parse.
</bodyText>
<sectionHeader confidence="0.999012" genericHeader="acknowledgments">
ACKNOWLEDGEMENTS
</sectionHeader>
<bodyText confidence="0.962793384615385">
This report describes research done at the Artificial
Intelligence Laboratory of the Massachusetts Institute of
Slo the (;B-friunework of Chomsky (1081). for instance, the syn-
tactic expression of unordered 0-grids at the X level is constrained
by the principles of Case theory. Endocentricity is another signifi-
cant constraint. See also Berwick&apos;s (1082) discussion of constraints
that could be placell on another grammatical formalism — lexical-
functional grammar - to avoid a similar intractability result.
&amp;quot;It is often anecdotally remarked that languages that allow relatively
free word order tend to make heavy use of inflections. A rich inflec-
tional system rim supply parsing constraints that make up for the
lack of ordering constraints: thus the situation we do not find is the
computationally difficult case of weak constraint.
</bodyText>
<page confidence="0.989351">
80
</page>
<bodyText confidence="0.880314444444444">
Technology. Support for the Laboratory&apos;s artificial intel-
ligence research has been provided in part by the Ad-
vanced Research Projects Agency of the Department of
Defense under Office of Naval Research contract N00014-
80-C-0505. During a portion of this research the author&apos;s
graduate studies were supported by the Fannie and John
Hertz Foundation. Useful guidance and commentary dur-
ing this research were provided by Bob Berwick, Michael
Sipser, and Joyce Friedman.
</bodyText>
<sectionHeader confidence="0.995022" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999922096774193">
Barton, E. (1984a). &amp;quot;Toward a Principle-Based Parser,&amp;quot;
A.I. Memo No. 788, M.I.T. Artificial Intelligence Lab-
oratory, Cambridge, Mass.
Barton, E. (1984b). &amp;quot;On the Complexity of ID/LP Pars-
ing,&amp;quot; A.I. Memo No. 812, M.I.T. Artificial Intelligence
Laboratory, Cambridge, Mass.
Berwick, R. (1982). &amp;quot;Computational Complexity and
Lexical-Functional Grammar,&amp;quot; American Journal of
Computational Linguistic3 8.3-4:97-109.
Berwick, R., and A. Weinberg (1984). The Grammatical
Basis of Linguistic Performance. Cambridge, Mass.:
M.I.T. Press.
Chonisky, N. (1981). Lectures on Government and Bind-
ing. Dordrecht, Holland: Foris Publications.
Earley, J. (1970). &amp;quot;An Efficient Context-Free Parsing Al-
gorithm,&amp;quot; Comm. ACM 13.2:94-102.
Garey, M., and D. Johnson (1979). Computers and In-
tractability. San Francisco: W. H. Freeman and Co.
Gazdar, Gerald (1981). &amp;quot;Unbounded Dependencies and
Coordinate Structure,&amp;quot; Linguistic Inquiry 12.2:155-184.
Miller, G., and N. Choinsky (1963). &amp;quot;Finitary Models of
Language Users.&amp;quot; in R. D. Luce, R. R. Bush, and E.
Galanter, eds., Handbook of Mathematical Psychology,
vol. II, 419-492. New York: John Wiley and Sons, Inc.
Ristad, E. (1985). &amp;quot;GPSG-Recognition is NP-Hard,&amp;quot; A.I.
Memo No. 837, M.I.T. Artificial Intelligence Labora-
tory, Cambridge, Mass., forthcoming.
Shieber, S. (1983). &amp;quot;Direct Parsing of 1D/LP Grammars.&amp;quot;
Technical Report 291R, SRI International, Menlo Park,
California. Also appears in Linguistics and Philosophy
7:2.
</reference>
<page confidence="0.999261">
81
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.792523">
<title confidence="0.998525">THE COMPUTATIONAL DIFFICULTY OF ID/LP PARSING</title>
<author confidence="0.997784">G Edward Barton</author>
<affiliation confidence="0.999799">M.I.T. Artificial Intelligence Laboratory</affiliation>
<address confidence="0.9977995">545 Technology Square Cambridge, MA 02139</address>
<abstract confidence="0.988820944444444">Modern linguistic theory attributes surface complexity to interacting subsystems of constraints. For instance, the ID. LP grammar formalism separates constraints on immediate dominance front those on linear order. Shieber&apos;s (1983) ID/LP parsing algorithm shows how to use ID and LP constraints directly in language processing, without expanding them into an intermediate &amp;quot;object However, Shieber&apos;s purported O(GI&apos;&amp;quot; • runtime bound underestimates t he difficulty of ID/LP parsing. ID/ LP parsing is actually NP-complete, and the worst-case runtime of Shieber&apos;s algorithm is actually exponential in grammar size. The growth of parser data structures causes the difficulty. Some computational and linguistic implications follow; in particular, it is important to note that despite its potential for combinatorial explosion, Shieber&apos;s algorithm remains better than the alternative of parsing expanded object</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Barton</author>
</authors>
<title>Toward a Principle-Based Parser,&amp;quot;</title>
<date>1984</date>
<booktitle>A.I. Memo No. 788, M.I.T. Artificial Intelligence Laboratory,</booktitle>
<location>Cambridge, Mass.</location>
<contexts>
<context position="7457" citStr="Barton (1984" startWordPosition="1249" endWordPosition="1250">s, Shieber&apos;s algorithm enumerates all of the combinations of k elements taken i at a time, where k is the rule length and i is the number of elements already processed. Thus it can be combinatorially explosive. Note, however, that Shieber&apos;s algorithm is still better than parsing the object grammar. With the Earley parser, the state set would reflect the same possibilities, but encoded in a less concise representation. In place of the state involving S {A, B, CI .{D, E), for instance, there would be 3! • 2! = 12 states involving S ALIC.DE, S BC A.ED, and so forth.&apos; Instead For more details see Barton (1984bi anti Shieber (1983). Shieher&apos;s representation differs in sonic ways from the representation described here. which tots developed independently by the author. The differences are generally inessential. hut see tune 2. 2 In contrast to the represent:it .illitstrateil here. Shieber&apos;s representation actually suffers Li.st extent from the saute probof a total of 25 states, the Earley state set would contain 135 = 12 • 10 + 15 states. With G2, the parser could not be sure of the categorial identities of the phrases parsed, but at least it was certain of the number and extent of the phrases. The s</context>
<context position="16479" citStr="Barton (1984" startWordPosition="2895" endWordPosition="2896">size 2 or less.&apos; If a parse exists, an appropriate vertex cover can be read off from beneath the H-symbols in the parse tree; conversely, if an appropriate vertex cover exists, it shows how to construct a parse. Figure 3 shows the parse tree that encodes a solution to the vertex-cover problem of Figure 1. The construction thus reduces Vertex Cover to UCFG recognition, and since the construction can be carried out in polynomial time, it follows that UCFG recognition and the more general task of ID/LP parsing must be computationally difficult. For a more detailed treatment of the reduction, see Barton (1984b). IMPLICATIONS The reduction of Vertex Cover shows that the 1D/LP parsing problem is NP-complete; unless P = NP, its time complexity is not bounded by any polynomial in the size of the grammar and input. Hence complexity analysis must be done carefully: despite similarity to Earley&apos;s algorithm, Shieber&apos;s algnfithin does not have complexity 0(1C12. n3), but can sometimes undergo exponential growth of its internal structures. Other computational and linguistic consequences also follow. Although Shieber&apos;s parser sometimes blows up, it remains better than the alternative of parsing an expanded &amp;quot;</context>
</contexts>
<marker>Barton, 1984</marker>
<rawString>Barton, E. (1984a). &amp;quot;Toward a Principle-Based Parser,&amp;quot; A.I. Memo No. 788, M.I.T. Artificial Intelligence Laboratory, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Barton</author>
</authors>
<title>On the Complexity of ID/LP Parsing,&amp;quot;</title>
<date>1984</date>
<booktitle>A.I. Memo No. 812, M.I.T. Artificial Intelligence Laboratory,</booktitle>
<location>Cambridge, Mass.</location>
<contexts>
<context position="7457" citStr="Barton (1984" startWordPosition="1249" endWordPosition="1250">s, Shieber&apos;s algorithm enumerates all of the combinations of k elements taken i at a time, where k is the rule length and i is the number of elements already processed. Thus it can be combinatorially explosive. Note, however, that Shieber&apos;s algorithm is still better than parsing the object grammar. With the Earley parser, the state set would reflect the same possibilities, but encoded in a less concise representation. In place of the state involving S {A, B, CI .{D, E), for instance, there would be 3! • 2! = 12 states involving S ALIC.DE, S BC A.ED, and so forth.&apos; Instead For more details see Barton (1984bi anti Shieber (1983). Shieher&apos;s representation differs in sonic ways from the representation described here. which tots developed independently by the author. The differences are generally inessential. hut see tune 2. 2 In contrast to the represent:it .illitstrateil here. Shieber&apos;s representation actually suffers Li.st extent from the saute probof a total of 25 states, the Earley state set would contain 135 = 12 • 10 + 15 states. With G2, the parser could not be sure of the categorial identities of the phrases parsed, but at least it was certain of the number and extent of the phrases. The s</context>
<context position="16479" citStr="Barton (1984" startWordPosition="2895" endWordPosition="2896">size 2 or less.&apos; If a parse exists, an appropriate vertex cover can be read off from beneath the H-symbols in the parse tree; conversely, if an appropriate vertex cover exists, it shows how to construct a parse. Figure 3 shows the parse tree that encodes a solution to the vertex-cover problem of Figure 1. The construction thus reduces Vertex Cover to UCFG recognition, and since the construction can be carried out in polynomial time, it follows that UCFG recognition and the more general task of ID/LP parsing must be computationally difficult. For a more detailed treatment of the reduction, see Barton (1984b). IMPLICATIONS The reduction of Vertex Cover shows that the 1D/LP parsing problem is NP-complete; unless P = NP, its time complexity is not bounded by any polynomial in the size of the grammar and input. Hence complexity analysis must be done carefully: despite similarity to Earley&apos;s algorithm, Shieber&apos;s algnfithin does not have complexity 0(1C12. n3), but can sometimes undergo exponential growth of its internal structures. Other computational and linguistic consequences also follow. Although Shieber&apos;s parser sometimes blows up, it remains better than the alternative of parsing an expanded &amp;quot;</context>
</contexts>
<marker>Barton, 1984</marker>
<rawString>Barton, E. (1984b). &amp;quot;On the Complexity of ID/LP Parsing,&amp;quot; A.I. Memo No. 812, M.I.T. Artificial Intelligence Laboratory, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Berwick</author>
</authors>
<title>Computational Complexity and Lexical-Functional Grammar,&amp;quot;</title>
<date>1982</date>
<journal>American Journal of Computational</journal>
<volume>3</volume>
<pages>8--3</pages>
<contexts>
<context position="23847" citStr="Berwick (1982)" startWordPosition="4101" endWordPosition="4102">ility is that it is devoid of practical significance. The object grammar could contain trillions of rules in practical cases (Shieber, 1983:4). If 1C2 • n3 complexity is too slow, then it remains too slow when !GT is regarded as a constant. Thus it is impossible to sustain this particular argument for the advantages of such formalisms as GPSG over other linguistic theories; instead, GPSG and other modern theories seem to be (very roughly) in the same boat with respect to complexity. In such a situation, the linguistic merits of various theories are more important than complexity results. (See Berwick (1982), Berwick and Weinberg (1984), and Ristad (1985) for further discussion.) The reduction does not rule out the use of formalisms that decouple ID and LP constraints; note that Shieber&apos;s direct parsing algorithm wins out over the use of the object grammar. However, if we assume that natural languages are efficiently parsable (EP), then computational difficulties in parsing a formalism do indicate that the formalism itself fails to capture whatever constraints are responsible for making natural languages EP. If the linguistically relevant ID/LP grammars are EP but the general ID/LP grammars are n</context>
</contexts>
<marker>Berwick, 1982</marker>
<rawString>Berwick, R. (1982). &amp;quot;Computational Complexity and Lexical-Functional Grammar,&amp;quot; American Journal of Computational Linguistic3 8.3-4:97-109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Berwick</author>
<author>A Weinberg</author>
</authors>
<title>The Grammatical Basis of Linguistic Performance.</title>
<date>1984</date>
<publisher>M.I.T. Press.</publisher>
<location>Cambridge, Mass.:</location>
<contexts>
<context position="23876" citStr="Berwick and Weinberg (1984)" startWordPosition="4103" endWordPosition="4106"> is devoid of practical significance. The object grammar could contain trillions of rules in practical cases (Shieber, 1983:4). If 1C2 • n3 complexity is too slow, then it remains too slow when !GT is regarded as a constant. Thus it is impossible to sustain this particular argument for the advantages of such formalisms as GPSG over other linguistic theories; instead, GPSG and other modern theories seem to be (very roughly) in the same boat with respect to complexity. In such a situation, the linguistic merits of various theories are more important than complexity results. (See Berwick (1982), Berwick and Weinberg (1984), and Ristad (1985) for further discussion.) The reduction does not rule out the use of formalisms that decouple ID and LP constraints; note that Shieber&apos;s direct parsing algorithm wins out over the use of the object grammar. However, if we assume that natural languages are efficiently parsable (EP), then computational difficulties in parsing a formalism do indicate that the formalism itself fails to capture whatever constraints are responsible for making natural languages EP. If the linguistically relevant ID/LP grammars are EP but the general ID/LP grammars are not, there must be additional </context>
</contexts>
<marker>Berwick, Weinberg, 1984</marker>
<rawString>Berwick, R., and A. Weinberg (1984). The Grammatical Basis of Linguistic Performance. Cambridge, Mass.: M.I.T. Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chonisky</author>
</authors>
<date>1981</date>
<booktitle>Lectures on Government and Binding.</booktitle>
<publisher>Foris Publications.</publisher>
<location>Dordrecht, Holland:</location>
<marker>Chonisky, 1981</marker>
<rawString>Chonisky, N. (1981). Lectures on Government and Binding. Dordrecht, Holland: Foris Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An Efficient Context-Free Parsing Algorithm,&amp;quot;</title>
<date>1970</date>
<journal>Comm. ACM</journal>
<pages>13--2</pages>
<contexts>
<context position="9289" citStr="Earley, 1970" startWordPosition="1575" endWordPosition="1576">ble subsets in addition to other states having to do with predictions, completions, and the parser start symbol that smile implementations introduce, there will be 44 states in So. (There are 338 states in the corresponding state when the object grammar is used.) How can Shieber&apos;s algorithm be exponential in grammar size despite its similarity to Earley&apos;s algorithm, which is polynomial in grammar size? The answer is that Shieber&apos;s :tlgorithm involves a notch larger bound on the number of states in a state set. Since the Earley parser successively processes all of the states in each state set (Earley, 1970:97), an explosion in the size of the state sets kills any small runtime bound. Consider the Earley parser. Resulting from each rule X Ak in a grammar G„ there are only k – 1 possible dotted rules. The number of possible dotted rules is thus bounded by the number of symbols that it takes to write C„ down, i.e. by :C„,. Since an Earley state just pairs a dotted rule with an interword position ranging front 0 to the length n of the input string, there are only • n) possible states; hence no state set may contain more than 0(C0; • n) (distinct) states. By an argument due to Earley, this limit all</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Earley, J. (1970). &amp;quot;An Efficient Context-Free Parsing Algorithm,&amp;quot; Comm. ACM 13.2:94-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Garey</author>
<author>D Johnson</author>
</authors>
<title>Computers and Intractability.</title>
<date>1979</date>
<location>San Francisco:</location>
<marker>Garey, Johnson, 1979</marker>
<rawString>Garey, M., and D. Johnson (1979). Computers and Intractability. San Francisco: W. H. Freeman and Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
</authors>
<title>Unbounded Dependencies and Coordinate Structure,&amp;quot; Linguistic Inquiry</title>
<date>1981</date>
<pages>12--2</pages>
<marker>Gazdar, 1981</marker>
<rawString>Gazdar, Gerald (1981). &amp;quot;Unbounded Dependencies and Coordinate Structure,&amp;quot; Linguistic Inquiry 12.2:155-184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>N Choinsky</author>
</authors>
<title>Finitary Models of Language Users.&amp;quot;</title>
<date>1963</date>
<booktitle>Handbook of Mathematical Psychology,</booktitle>
<volume>vol. II,</volume>
<pages>419--492</pages>
<editor>in R. D. Luce, R. R. Bush, and E. Galanter, eds.,</editor>
<publisher>John Wiley and Sons, Inc.</publisher>
<location>New York:</location>
<marker>Miller, Choinsky, 1963</marker>
<rawString>Miller, G., and N. Choinsky (1963). &amp;quot;Finitary Models of Language Users.&amp;quot; in R. D. Luce, R. R. Bush, and E. Galanter, eds., Handbook of Mathematical Psychology, vol. II, 419-492. New York: John Wiley and Sons, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Ristad</author>
</authors>
<title>GPSG-Recognition is NP-Hard,&amp;quot;</title>
<date>1985</date>
<booktitle>A.I. Memo No. 837, M.I.T. Artificial Intelligence Laboratory,</booktitle>
<location>Cambridge, Mass., forthcoming.</location>
<contexts>
<context position="23895" citStr="Ristad (1985)" startWordPosition="4108" endWordPosition="4110">nce. The object grammar could contain trillions of rules in practical cases (Shieber, 1983:4). If 1C2 • n3 complexity is too slow, then it remains too slow when !GT is regarded as a constant. Thus it is impossible to sustain this particular argument for the advantages of such formalisms as GPSG over other linguistic theories; instead, GPSG and other modern theories seem to be (very roughly) in the same boat with respect to complexity. In such a situation, the linguistic merits of various theories are more important than complexity results. (See Berwick (1982), Berwick and Weinberg (1984), and Ristad (1985) for further discussion.) The reduction does not rule out the use of formalisms that decouple ID and LP constraints; note that Shieber&apos;s direct parsing algorithm wins out over the use of the object grammar. However, if we assume that natural languages are efficiently parsable (EP), then computational difficulties in parsing a formalism do indicate that the formalism itself fails to capture whatever constraints are responsible for making natural languages EP. If the linguistically relevant ID/LP grammars are EP but the general ID/LP grammars are not, there must be additional factors that guaran</context>
</contexts>
<marker>Ristad, 1985</marker>
<rawString>Ristad, E. (1985). &amp;quot;GPSG-Recognition is NP-Hard,&amp;quot; A.I. Memo No. 837, M.I.T. Artificial Intelligence Laboratory, Cambridge, Mass., forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shieber</author>
</authors>
<title>Direct Parsing of 1D/LP Grammars.&amp;quot;</title>
<date>1983</date>
<tech>Technical Report 291R,</tech>
<institution>SRI International, Menlo Park, California.</institution>
<note>Also appears in Linguistics and Philosophy 7:2.</note>
<contexts>
<context position="1727" citStr="Shieber, 1983" startWordPosition="236" endWordPosition="237">guistic theories derive surface complexity from modular subsystems of constraints; Chomsky (1981:5) proposes separate theories of bounding, government, 0-marking, and so forth. while Gazdar and Pullum&apos;s GPSG formalism (Shieber. 1983.21f) uses immediate-dominance ( ID) rules, linear-precedence (LP) constraints, and inetarules. When modular constraints are involved, rule sys.tems that multiply out their surface effects are large and clumsy (see Barton. 1984a). The expanded contextfree &amp;quot;object grammar&amp;quot; that multiplies out the constraints in a typical GPSG system would contain trillions of rules (Shieber, 1983:1). Shicber (1983) thus leads in a welcome direction by ,howing how 1D, LP grammars can he parsed &amp;quot;directly,&amp;quot; wit !tout the conibinatorially explosive step of multiplying out the effects of the ID and Ll&apos; constraints. Shieber&apos;s algorithm applies ID and LP constraints one step at a time.is needed. However, some doubts about computational complexity remain. Shieber (1983:15) argues that his algorithm is identical to Earley&apos;s in time complexity, but this result seems almost too much to hope for. An ID4.1) grammar C can be much smaller than an equivalent context-free grammar C&apos;; for example, if G</context>
<context position="7479" citStr="Shieber (1983)" startWordPosition="1252" endWordPosition="1253">hm enumerates all of the combinations of k elements taken i at a time, where k is the rule length and i is the number of elements already processed. Thus it can be combinatorially explosive. Note, however, that Shieber&apos;s algorithm is still better than parsing the object grammar. With the Earley parser, the state set would reflect the same possibilities, but encoded in a less concise representation. In place of the state involving S {A, B, CI .{D, E), for instance, there would be 3! • 2! = 12 states involving S ALIC.DE, S BC A.ED, and so forth.&apos; Instead For more details see Barton (1984bi anti Shieber (1983). Shieher&apos;s representation differs in sonic ways from the representation described here. which tots developed independently by the author. The differences are generally inessential. hut see tune 2. 2 In contrast to the represent:it .illitstrateil here. Shieber&apos;s representation actually suffers Li.st extent from the saute probof a total of 25 states, the Earley state set would contain 135 = 12 • 10 + 15 states. With G2, the parser could not be sure of the categorial identities of the phrases parsed, but at least it was certain of the number and extent of the phrases. The situation gets worse if</context>
<context position="10638" citStr="Shieber (1983" startWordPosition="1824" endWordPosition="1825"> to grammar size. A rule X Ai...Ak in a CCFG C yields not k 1 ordinary dotted rules, but but 2&apos; possible dotted UCFC rules tracking accumulation of set elements. In the worst case the grammar contains only one rule and k is on the order of hence a bound on the number of possible dotted UCFG rules is not given by 0( but by 0(2 &amp;quot;^ ). (Recall the exponential blowup illustrated for grammar Ci.) The parser sometimes blows up because there are exponentially more possible ways to to progress through an unordered rule expansion than an through an ordered one. In ID/LP parsing, the easiest case occurs Shieber (1983:10) uses an ordered sequence instead of A mini. tism before the 1101: consequently. in place of the state involving S (A. /3.(7).{D. Shieber would have the 3! = 6 states involving S ut. {D. E}, where a ranges over the six permutations of ABC. 77 START IIIII21I3H4UUDDDD U aaaa bbbb cccc dddd H2 61 c D H3-4Cld H4 –461d Figure 1: This graph illustrates a trivial instance of the vertex cover problem. The set {c,d} is a vertex cover of size 2. when the LP constraints force a unique ordering for every rule expansion. Given sufficiently strong constraints, Shieber&apos;s parser reduces to Earley&apos;s as Shi</context>
<context position="20523" citStr="Shieber (1983" startWordPosition="3562" endWordPosition="3563">uivalent CFO. The NP-completeness result illuminates this property in three ways. First, the reduction shows that enough brevity is gained so that an instance of any problem in .4 P can be stated in a UCFG that is only polynomially larger than the original problem instance. In contrast, the current polynomial-time reduction could not be carried out with a CFG instead of a UCFG, since the necessity of spelling out all the orders in which symbols !night appear could make the CFG exponentially larger than the instance. Second, the reduction shows that this brevity of expression is not free. CFG 4Shieber (198315 n. 6) mentions a possible precompilation step, but it is concerned with the LI&apos; relation rather than the Ill nays. 79 recognition can be solved in cubic time or less, but unless P =.WP, general UCFG recognition cannot be solved in polynomial time. Third, the reduction shows that only one essential use of the power to permute rule expansions is necessary to make the parsing problem NP-complete, though the rule in question may need to be arbitrarily long. Finally, the ID/LP parsing problem illustrates how weakness of constraint can make a problem computationally difficult. One might perhaps t</context>
<context position="23372" citStr="Shieber, 1983" startWordPosition="4019" endWordPosition="4020">ed, fact that humans process the utterances they hear very rapidly. :.entences of .L context-free language are provably parsabh: ut A uine I hat is proportional to the cube of the length of the sentence or less. As previously remarked, the use of Earley&apos;s algorithm on the expanded object grammar constitutes a parsing method for the fixed-grammar 1D/LP parsing problem that is indeed no worse than cubic in sentence length. However, the most important aspect of this possibility is that it is devoid of practical significance. The object grammar could contain trillions of rules in practical cases (Shieber, 1983:4). If 1C2 • n3 complexity is too slow, then it remains too slow when !GT is regarded as a constant. Thus it is impossible to sustain this particular argument for the advantages of such formalisms as GPSG over other linguistic theories; instead, GPSG and other modern theories seem to be (very roughly) in the same boat with respect to complexity. In such a situation, the linguistic merits of various theories are more important than complexity results. (See Berwick (1982), Berwick and Weinberg (1984), and Ristad (1985) for further discussion.) The reduction does not rule out the use of formalis</context>
</contexts>
<marker>Shieber, 1983</marker>
<rawString>Shieber, S. (1983). &amp;quot;Direct Parsing of 1D/LP Grammars.&amp;quot; Technical Report 291R, SRI International, Menlo Park, California. Also appears in Linguistics and Philosophy 7:2.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>