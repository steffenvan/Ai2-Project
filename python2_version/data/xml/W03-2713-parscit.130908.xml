<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009839">
<title confidence="0.771614">
SesaME: A Framework for Personalised and Adaptive Speech Interfaces
</title>
<author confidence="0.568463">
Botond Pakucs
</author>
<affiliation confidence="0.48277">
CTT (Centre for Speech Technology)
KTH (Royal Institute of Technology)
</affiliation>
<address confidence="0.527252">
Drottning Kristin as vag 31
SE-100 44, Stockholm, Sweden
</address>
<email confidence="0.994929">
botte@speech.kth.se
</email>
<sectionHeader confidence="0.997336" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999905461538461">
This paper presents some motivations
for using highly personalised speech
interfaces. In particular, the focus
is on the requirements for adapta-
tion in mobile environments. Further-
more, SesaME, a framework for per-
sonalised and adaptive speech inter-
faces is described. SesaME supports
a multi-domain approach and event-
based, asynchronous dialogue manage-
ment. Finally, a description of how
SesaME is employed within the frame-
work of the PER demonstrator is given.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999858137931034">
In natural spoken human-to-human communica-
tion, speakers tend to adapt to each other and to
the communicative situation. Adaptation is there-
fore a desirable feature for making speech in-
terfaces more natural. Furthermore, it is com-
monly believed that spoken dialogue system per-
formance could be enhanced through adaptation.
In this paper, some challenging issues related
to adaptation in dialogue systems are discussed.
The focus is in particular on the requirements for
adaptation in mobile and ubiquitous environments
(Section 3). Based on these challenging issues,
motivations are presented for using highly per-
sonalised speech interfaces for adapting and fine-
tuning the interaction to individual users and their
actual situation (Section 4) .
The paper also introduces SesaME, a frame-
work for personalised and adaptive speech inter-
faces (Section 5). SesaME is designed to sup-
port a multitude of highly distributed applications
and to adapt to individual users and their envi-
ronment. SesaME features an event-based, asyn-
chronous dialogue management and supports a
dynamic plug-and-play solution which enables a
multi-domain approach. Furthermore, a solution
is proposed to achieve context-based adaptation
to an individual user. Finally, the use of SesaME
within the framework of the PER demonstrator is
described.
</bodyText>
<sectionHeader confidence="0.995488" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.99809647368421">
In the context of dialogue systems, adaptation
is the modification of the system&apos;s functionality
according to the changing circumstances and to
the variations in the system input. Based on the
source of the variations two major categories of
variation can be distinguished. Variations caused
by the changing situational circumstances are the
context-related variations. The user-related vari-
ations can be attributed to various differences in
user characteristics, behaviour and preferences.
It has to be emphasised that there is no clear
boundary between the variations attributed to the
user and the variations caused by the changing cir-
cumstances in the context. Variations in the con-
text may affect the user&apos;s behaviour and induce
different user related variations. For instance,
time pressure and increased cognitive load on the
user may cause variations in the user&apos;s speech
(Miiller et al., 2001).
</bodyText>
<page confidence="0.997582">
95
</page>
<bodyText confidence="0.999956272727273">
As the user characteristics and the parameters
of the situation may change during the same inter-
action, both the characteristics of the context and
the user&apos;s properties have to be considered simul-
taneously. The usability of a system can be ex-
pected to increase if both the user properties and
the user&apos;s situation are taken into account (Jame-
son, 2001). Thus, simultaneous adaptation to the
user and to the context is a major challenge for
providing more natural and conversational speech
interfaces.
</bodyText>
<subsectionHeader confidence="0.995299">
2.1 Adaptation to the User
</subsectionHeader>
<bodyText confidence="0.972672181818182">
From a practical point of view, not all kinds of
user related variations are equally interesting for
a dialogue system, see Figure 1. Only a subset
of all theoretically possible feature variations are
actually realised in practice. This is the set of
practically possible variations. For a realistic dia-
logue system, which encounters just a limited set
of users and handles a limited domain, only a sub-
set of these practically realised variations are usu-
ally relevant. These are the interpersonal varia-
tions.
</bodyText>
<figure confidence="0.52761">
Theoretically possible variations (infinite)
Practically possible variations
</figure>
<figureCaption confidence="0.997281">
Figure 1: User feature variations.
</figureCaption>
<bodyText confidence="0.9998904">
A robust dialogue system should be able to han-
dle all of the possible interpersonal feature vari-
ations relevant to the domain. No single user,
however, realises all these possible interpersonal
variations. The variations produced by a single
user are the intrapersonal variations. These intra-
personal variations carry information which may
be significant for the interaction with the specific
user.
While a speech interface should support in-
teraction with all potential users, the interaction
should also be adapted and fine-tuned to the cur-
rent individual user. During a single interaction a
dialogue system has to be able to identify and cap-
ture the intrapersonal variations and distinguish
them from the interpersonal variations. Conse-
quently, a key issue is how to model generic user
characteristics and specific individual user fea-
tures simultaneously (Jameson and Wittig, 2001).
User modelling has been successfully em-
ployed in several different dialogue systems (Zuk-
erman and Litman, 2001). However, how to
model and exploit data about users in general and
data about the current individual user still remains
a central challenging issue.
</bodyText>
<subsectionHeader confidence="0.999287">
2.2 Adaptation to the Context
</subsectionHeader>
<bodyText confidence="0.970705933333333">
Situational&apos; factors are used in human-to-human
communication to facilitate a smoother and more
natural interaction. Context-of-use is relevant
knowledge about the actual situation, conversa-
tional partners, domain, topic etc. The particu-
lar situation is, however, not entirely defined by
external variables, but depends also on how users
conceptualise the situation (Fischer, 2000). Thus,
context-of-use also includes the user&apos;s physical
and perceptual, social and emotional informa-
tional state (Bunt, 1994).
Most experimental and commercial dialogue
systems are single-domain dialogue systems.
Therefore, with regard to the physical and per-
ceptual context a more or less static situation is
usually assumed. However, the growing interest
for using speech interfaces in mobile and ubiqui-
tous computing environments makes it necessary
to develop better solutions for capturing, mod-
elling and adapting to the variations in the users&apos;
physical and perceptual context.
Speech-based interaction with ubiquitous ser-
vices in mobile environments differs from inter-
acting with desktop or telephony-based speech in-
terfaces. Being on the run, and with hands, eyes
and sometimes even the mind busy, the users&apos; re-
quirements on the speech interfaces can be ex-
&apos;Some researchers make distinction between the notions
of &amp;quot;context&amp;quot; and &amp;quot;situation&amp;quot;. In this paper, these terms are
used merely as synonyms.
</bodyText>
<page confidence="0.988682">
96
</page>
<bodyText confidence="0.999813428571429">
pected to increase. Accordingly, in mobile envi-
ronments it is even more important to fine-tune
the speech based communicative interaction to the
specific user and to the user&apos;s current situation.
Some of the requirements on speech interfaces in
mobile and ubiquitous environments are discussed
next.
</bodyText>
<sectionHeader confidence="0.992636" genericHeader="method">
3 Requirements in Mobile Environments
</sectionHeader>
<bodyText confidence="0.999990782608696">
In dynamically changing mobile environments,
the user&apos;s intentions and needs may change
rapidly. The user should be able to initiate a new
task while waiting for some other specific service
to be completed. The user should have the pos-
sibility to easily cancel a previously issued com-
mand or change the parameters of some previ-
ously initiated service. Furthermore, the system
itself should be able to interrupt an ongoing dia-
logue and direct the user&apos;s attention to higher pri-
ority events taking place in the user&apos;s immediate
environment (physical or virtual).
Thus, in mobile environments it is desirable
to support a wide range of domains within one
and the same dialogue. Thus, a multi-domain ap-
proach (Chung et al., 1999) is necessary to allow
the user to transparently and seamlessly switch
between several topic domains and services. Sim-
ilarly, asynchronous dialogue management (Boye
et al., 2000) should also be supported by speech
interfaces in mobile environments.
Employing speech interfaces in mobile envi-
ronments may even introduce some new user re-
quirements. For instance, means to coordinate
and control multiple concurrent speech interfaces
may become necessary in order to avoid the intro-
duction of new usability related problems. When
several services and appliances, embedded in the
same environment, are listening for user com-
mands, or even taking initiative pro-actively, it
is possible, due to errors or misrecognitions, that
several speech-based services may be triggered by
a single user utterance. To our knowledge, the ef-
fects of interacting with several concurrent speech
interfaces at the same time have never been stud-
ied.
In mobile environments, the users should be
able to concentrate upon the task to be performed
and not be forced to cope with interface issues.
The usability and user interface issues should be
considered for whole environments rather than for
isolated services and appliances. Consequently,
support for adaptation, situation awareness and
user modelling appears to play a central role
for providing natural, user-friendly conversational
speech interfaces in mobile environments.
</bodyText>
<sectionHeader confidence="0.961115" genericHeader="method">
4 Personalised Speech Interfaces
</sectionHeader>
<bodyText confidence="0.999755619047619">
Employing personalised speech interfaces appears
to be a promising solution for adapting and fine-
tuning the interaction to individual users and their
actual situation. Handling some of the above pre-
sented challenging issues could also be facilitated
by the use of personalised speech interfaces.
As the identity of the user is assumed to be
known, there is no need to detect the category or
the identity of the user. Only the user&apos;s context-of-
use has to be detected and modelled at runtime.
Consequently, user modelling and adaptation to
the user can be regarded as a long term adaptation
while adaptation to the context can be regarded as
short term adaptation. Thus, simultaneous adap-
tation to both the context and to the user is facili-
tated.
While using a personalised solution, only the
characteristics of a single individual user have to
be modelled. Thus, the simultaneous modelling
of generic user characteristics and specific user
features is also facilitated.
</bodyText>
<subsectionHeader confidence="0.984534">
4.1 The human-centered approach
</subsectionHeader>
<bodyText confidence="0.9999838125">
The human-centered approach is a feasible so-
lution, proposed for achieving personalisation of
speech interfaces which are intended to be used in
mobile environments (Pakucs, 2003).
According to this approach each user is ex-
pected to use an individual and highly person-
alised universal speech interface to access a mul-
titude of services and appliances. Access to the lo-
cally available services and appliances is handled
through the personalised speech interface, inte-
grated into some personal and wearable appliance
such as a mobile phone or a PDA. Application-
specific data, including dialogue management ca-
pabilities, domain knowledge etc., has to be en-
coded in service descriptions and stored locally
at the service provider side. Whenever the user
</bodyText>
<page confidence="0.997585">
97
</page>
<bodyText confidence="0.999967818181818">
enters a new environment, the available, dis-
tributed service descriptions, have to be dynam-
ically loaded into the personalised speech inter-
face.
The human-centered approach provides exten-
sive support for adapting and fine-tuning the in-
teraction to individual users. For instance, it is
even feasible to use speaker-dependent automatic
speech recognition which may reduce the amount
of the speech recognition errors. Due to the dis-
tributed functionality of the human-centred ap-
proach, all local services and appliances could be-
come available for all potential users in spite of
dialects or non-native accents.
Generally, the requirements on the amount of
data necessary for machine learning employed in
user modelling and adaptation are considerably
high. While using a highly personalised solution,
the same speech interface is used for accessing a
multitude of services. Thus, more data becomes
available for machine learning. Consequently,
user modelling and adaptation is facilitated.
A highly personalised universal speech inter-
face appears to be ideal for coordinating and con-
trolling multiple concurrent speech interfaces and
for supporting a multi-domain approach. By em-
ploying a personalised speech interface, the han-
dling of the potential security and integrity issues
are also facilitated. Furthermore, personalised
speech interfaces are expected to provide an un-
obtrusive, user-friendly interaction and seamless
access to services and appliances in mobile envi-
ronments (Pakucs, 2003).
</bodyText>
<sectionHeader confidence="0.995481" genericHeader="method">
5 The SesaME Framework
</sectionHeader>
<bodyText confidence="0.999881115384615">
SesaME, shown in Figure 2, is a generic, task-
oriented dialogue manager specially designed for
the human-centered approach as well as for mo-
bile environments.
SesaME features a blackboard and agent based
architecture. The central blackboard stores the
representation of the information state (Larsson
and Traum, 2000) of the dialogue. However, this
representation is not formalised; the information
state is merely a collection of all data available
to the dialogue system. The update of the infor-
mation state is event-based, where events can be
dialogue moves, internal events, or changes in the
user&apos;s external context. The event-based function-
ality enables an asynchronous information pro-
cessing (Blaylock et al., 2002). The SesaME ar-
chitecture and the theoretical considerations be-
hind it are in some aspects comparable to other
agent-based architectures such as the Jaspis (Tu-
runen and Hakulinen, 2000) and the TRIPS (Allen
et al., 2000) architectures.
SesaME relies on the ATLAS generic speech
technology platform (Melin, 2001). The ATLAS
platform provides high-level primitives for basic
speech I/0, but access to low-level data is also fa-
cilitated.
</bodyText>
<subsectionHeader confidence="0.991913">
5.1 A Multi-Domain Approach
</subsectionHeader>
<bodyText confidence="0.999774129032258">
One of the key-issues in the SesaME architecture
is to support a multi-domain approach. The lo-
cally available service descriptions, including dia-
logue descriptions, recognition grammars and do-
main knowledge has to be dynamically loaded and
activated on the fly. For handling these require-
ments, a dynamic plug-and-play functionality of
the dialogue management capabilities has been
developed (Pakucs, 2002).
In SesaME, most of the operations related to the
plug-and-play functionality are carried out by the
Dialogue Engine (DE). Synchronisation and com-
munication with the mobile service environment
is taken care by the Application Interface. When-
ever new changes occur in the service environ-
ment (e.g. new services become available or exist-
ing services disappear etc.), the Application Inter-
face dynamically updates the Dialogue Descrip-
tion Collection (DDC). All currently available di-
alogue descriptions are stored in the DDC. Beside
the different task- and domain-specific dialogue
descriptions, DDC also contains resident appli-
cation independent dialogue descriptions used for
error handling or for meta-dialogues necessary for
providing information on available services.
For interoperability reasons, the application-
specific data, including the dialogue descriptions
has to be described in some standardised way.
VoiceXML2 was chosen as the dialogue descrip-
tion formalism. The main reason is the fact
that VoiceXML is a markup language which
</bodyText>
<footnote confidence="0.9961225">
2Voice Extensible Markup Language. For more info see:
http://www.w3.org/TR/voicexml20/
</footnote>
<page confidence="0.989334">
98
</page>
<figure confidence="0.949035">
Mobile Service Environment
AI user
</figure>
<figureCaption confidence="0.925552">
Figure 2: SesaME system architecture with the Dialogue Engine.
</figureCaption>
<figure confidence="0.999110703703703">
ATLAS Speech Technology Platform
SesaME
Application Interface
Interaction Manager
roard
•
t_
Speech Technology API
Multi Media Manager
Domain
Knowledge
Manager
Dialogue
Engine
Dialogue
Description
Collection
1
Dialogue
Description
Loader
Dialogue
Interpreter
Context
Manager
User Model
Manager
</figure>
<bodyText confidence="0.996067666666667">
shields the developers from low-level implemen-
tation details, facilitating rapid application devel-
opment. VoiceXML also provides support for
simple grammars However, the VoiceXML for-
malism was slightly extended for allowing addi-
tional system prompts used for adaptations and
keywords used for topic detection.
Procedurally, the internal plug and play func-
tionality can be divided into three main parts:
</bodyText>
<listItem confidence="0.9994494">
• identification of the task &amp; topic and the as-
sociated dialogue description,
• activation of the identified dialogue descrip-
tion,
• the actual dialogue management.
</listItem>
<bodyText confidence="0.999818846153846">
At the current stage, the identification of the
correct dialogue description is based on topic vec-
tors and keywords extracted from the VoiceXML
documents. However, context models, user mod-
els or plan based mechanisms could also be used
for this purpose.
During the activation, the appropriate dialogue
description is translated into internal data struc-
tures appropriate for the dialogue management
in SesaME. This process is performed through
JAXB3 which provides a fast way to create a two-
way mapping between XML documents and Java
objects (Pakucs, 2002).
</bodyText>
<subsectionHeader confidence="0.996352">
5.2 Dialogue Management
</subsectionHeader>
<bodyText confidence="0.999926294117647">
During the dialogue management, the generated
internal data structures are used for frame-based
dialogue management. The actual dialogue man-
agement process does not follow the VoiceXML
specifications. Accordingly, VoiceXML is only
used as a domain description language.
During the dialogue activation process, two
additional parallel data structures are generated.
These data structures can be accessed by the Con-
text and the User Model Managers and by the Do-
main Knowledge Manager. These data structures
may be updated with suggested pieces of informa-
tion, which can be used to adapt the interaction to
the situation and to the user.
Generic, application independent features of
the dialogue management are handled by the In-
teraction Manager (IM). The IM also handles er-
</bodyText>
<footnote confidence="0.9975195">
3The Java Architecture for XML Binding, available at:
http://java.sun.com/xml/jaxb/
</footnote>
<page confidence="0.998936">
99
</page>
<bodyText confidence="0.999946375">
ror detection, planning, keeps track of dialogue
history, and coordinates the different system com-
ponents and knowledge sources. A central, shared
information storage, a blackboard, and a collec-
tion of autonomous software agents are the main
components of the IM. The detailed description
of the IM&apos;s functionality is, however, beyond the
scope of this paper.
</bodyText>
<subsectionHeader confidence="0.998884">
5.3 Adaptation to the Context
</subsectionHeader>
<bodyText confidence="0.96091376119403">
A major goal in SesaME is to make full use
of the potentials of the individual user models
and to achieve context-based adaptation. The
adopted solution is inspired by attempts to achieve
context-aware computing in the research field of
ubiquitous computing.
According to a proposed solution (Dey, 2001)
the different possible context types are categorised
in primary and secondary context types. The pri-
mary context types characterising the situation of
a particular entity consist of the location and the
identity of the entity, the time of the interaction,
and the activity being performed. The secondary
pieces of context share a major common charac-
teristic: they can be indexed by the primary con-
text because they are attributes of the entity with
primary context. For example, a person&apos;s phone
number is a secondary type of context and can be
obtained through the primary context.
In SesaME, after an interaction with the user
every utterance is represented as a feature vector
containing feature-value pairs of all relevant in-
formation (such as topic, start time of the utter-
ance, length of the utterance, user choices etc.).
The only common property of the features in the
feature vector is the co-occurrence. The feature
vectors are indexed and stored in the user model.
The user model is represented as an inverted
file, a common data structure used in information
retrieval applications. For manipulating the user
model, common information retrieval solutions
are used. Accordingly, the user model is domain
and task independent and is automatically built.
The user model is not formalised in some specific
knowledge-based structure. However, it is still
possible to apply machine learning solutions such
as memory-based learning or similarity-based rea-
soning.
The Context Manager keeps track of the current
context. During a new interaction, based on avail-
able contextual information, similar interactions
are retrieved from the user model. These retrieval
results can be used to predict specific features of
the ongoing interaction and to achieve adaptation
to the current context.
For example, based on earlier interactions with
a voice controlled elevator it may be possible to
detect that the user&apos;s most frequent choice of se-
mantic object was the &amp;quot;fifth floor&amp;quot; when answer-
ing to the standardised prompt: &amp;quot;Which floor
would you prefer?&amp;quot;. Thus, it is possible to pre-
dict that the user may want to take the elevator
to the fifth floor. By using the additional prompt
supported in SesaME, it is possible to ask the user
a more natural question: &amp;quot;Fifth,floon as usual?&amp;quot;
instead of the impersonal standardised prompt.
For enabling the use of alternative questions,
the additional system prompts are used:
&lt;prompt&gt;
Which floor would you prefer? &lt;/prompt&gt;
&lt;alt-prompt&gt;
&lt;value expr=&amp;quot;predicted-floor&amp;quot;/&gt; floor
as usual? &lt;/alt-prompt&gt;
If there are no similar interactions, or no ob-
vious patterns are present in the previous interac-
tions (such as a CD purchasing task), then the de-
fault standardised prompt is used.
</bodyText>
<sectionHeader confidence="0.989318" genericHeader="method">
6 Application
</sectionHeader>
<bodyText confidence="0.999742333333333">
Before evaluating SesaME as a generic, adap-
tive and personalised dialogue manager in mobile
and multi-domain environments, it is necessary to
evaluate it as a traditional domain-dependent dia-
logue manager. In the first application of SesaME,
the focus is on the evaluation of the plug-and-play
functionality. This evaluation is conducted within
the framework of the PER (Prototype Entrance
Receptionist) project (Pakucs and Melin, 2001).
</bodyText>
<subsectionHeader confidence="0.997261">
6.1 The PER demonstrator
</subsectionHeader>
<bodyText confidence="0.9994054">
PER, Figure 3, is an animated-agent based auto-
mated receptionist located at the entrance of our
department. Originally, the system was developed
to allow fast and robust access for employees. The
application&apos;s functionality is stream-lined for this
</bodyText>
<page confidence="0.974205">
100
</page>
<bodyText confidence="0.9980875">
purpose. PER features a multilingual speaker ver-
ification system for Swedish and English. An em-
ployee, when approaching the gate, is expected to
say his/her password, which consists of the em-
ployee&apos;s name and a random digit sequence dis-
played on the screen.
</bodyText>
<figureCaption confidence="0.996596">
Figure 3: A screen shot of the PER demonstrator.
</figureCaption>
<subsectionHeader confidence="0.998244">
6.2 Employing SesaME
</subsectionHeader>
<bodyText confidence="0.9999928">
The functionality of PER has been extended to al-
low handling of external visitors as well. The in-
teraction with the visitors relies on the SesaME
dialogue manager and makes use of VoiceXML-
based dialogue descriptions. PER features several
different dialogue descriptions associated with
different types of visitors and visitor goals such as
expected personal guests, seminar visitors, or stu-
dents attending lectures. The handling of these di-
alogue descriptions, the topic-based identification
of the correct dialogue description, the activation
of the identified dialogue description and the dia-
logue management, is performed according to the
description provided in the previous section.
The multi-domain approach allows the incre-
mental updating of the system with new tasks even
if the potential of dynamically updating the DDC
with new dialogue descriptions at run-time is not
fully employed. We plan, for instance, to add di-
alogue descriptions to help to guide the visitors
in the building. The plug and play functional-
ity allows extension of the application for support
of new languages. Adding support for English
speaking visitors is also planned in the near fu-
ture.
The domain-dependent data necessary for the
dialogues is stored in an external database and is
made available for manipulation through the web.
Thus, employees can easily add to the database
expected visitors, detailed information on semi-
nars or lectures, or they can change the parame-
ters of existing entries. In this way the data upon
which PER operates and upon which the dialogue
descriptions are generated is always kept up to
date.
</bodyText>
<sectionHeader confidence="0.999322" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.984033942857143">
This paper introduced SesaME, a framework
for personalised and adaptive speech interfaces.
SesaME features a blackboard and agent based
architecture which supports event-based, asyn-
chronous dialogue management. The employed
dynamic plug-and-play dialogue management so-
lution enables a multi-domain approach and the
run-time accessing of the locally available ser-
vices in mobile and ubiquitous environments.
Some parts of the SesaME architecture are still
under development. However, it has already been
successfully employed in the framework of the
PER demonstrator. The application demonstrates
that it is feasible to support a multi-domain ap-
proach through a dynamic plug-and-play solution
while still allowing a generic and flexible dialogue
management.
A major goal of the SesaME framework is to
support highly personalised speech interfaces and
to facilitate the adaptation and fine-tuning of the
interaction to individual users and their actual sit-
uation. A description of the solution employed
in SesaME for achieving context-based adaptation
to individual users&apos; current situation was also de-
scribed.
Using highly personalised speech interfaces ap-
pears particularly advantageous in mobile and
ubiquitous computing environments. The sug-
gested framework creates novel possibilities for
supporting personalisation, context awareness and
user modelling in dialogue management. The
adaptation and usability related advantages are in-
teresting enough to make the proposed framework
worthy of further development and evaluation.
Centrum for talteknologi
</bodyText>
<page confidence="0.99322">
101
</page>
<sectionHeader confidence="0.995672" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999928083333333">
This research was carried out at the CTT, Cen-
tre for Speech Technology, a competence centre
at KTH, supported by VINNOVA (The Swedish
Agency for Innovation Systems), KTH and par-
ticipating Swedish companies and organisations.
This work was also sponsored by GSLT, the
Swedish National Graduate School of Language
Technology and by the European Union&apos;s In-
formation Society Technologies Programme un-
der contract IST-2000-29452, DUMAS (Dynamic
Universal Mobility for Adaptive Speech Inter-
faces).
</bodyText>
<sectionHeader confidence="0.999312" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999312773333333">
James Allen, Donna Byron, Myroslava Dzikovska,
George Ferguson, Lucian Galescu, and Amanda
Stent. 2000. An architecture for a generic dialogue
shell. Natural Language Engineering, 6(3):1-16,
December.
Nate Blaylock, James Allen, and George Ferguson.
2002. Synchronization in an asynchronous agent-
based architecture for dialogue systems. In Pro-
ceedings of 3rd SIGdial Workshop on Discourse and
Dialogue, June.
Johan Boye, Beth Ann Hockey, and Manny Rayner.
2000. Asynchronous dialogue management: Two
case studies. In David Traum and Massimo Poesio,
editors, Proceedings of GOTALOG 2000, Gothen-
burg, Sweden, June.
Harry Bunt. 1994. Context and dialogue control.
THINK Quarterly, 3(1):19-31.
Grace Chung, Stephanie Seneff, and Lee Hetherington.
1999. Towards multi-domain speech understand-
ing using a two-stage recognizer. In Proceedings of
Eurospeech &apos;99, pages 2655-2658, Budapest, Hun-
gary, September.
Anind K. Dey. 2001. Understanding and using con-
text. Personal and Ubiquituos Computing, 5(1).
Special issue on Situated Interaction and Ubiquitous
Computing.
Kerstin Fischer. .2000. What is situation? In Pro-
ceedings of GOTALOG 2000, Gothenburg, Sweden,
June.
Anthony Jameson and Frank Wittig. 2001. Leveraging
data about users in general in the learning of individ-
ual user models. In B. Nebel, editor, Proceedings of
IJCAI 2001, pages 1185-1192, San Francisco, CA,
USA. Morgan Kaufmann.
Anthony Jameson. 2001. Modelling both the context
and the user. Personal and Ubiquitous Computing,
5:29-33.
Staffan Larsson and David R. Traum. 2000. Informa-
tion state and dialogue management in the TRINDI
dialogue move engine toolkit. Natural Language
Engineering, 6:323-340, September.
Rakan Melin. 2001. ATLAS: A generic software
platform for speech technology based applications.
TMH-QPRS,Quarterly Progress and Status Report,
42.
Christian Muller, Barbara GroBmann-Hutter, Anthony
Jameson, Ralf Rummer, and Frank Wittig. 2001.
Recognizing time pressure and cognitive load on
the basis of speech: An experimental study. In
UM2001, User Modeling: Proceedings of the
Eighth International Conference.
Botond Pakucs and Hakan Melin. 2001. PER: A
speech based automated entrance receptionist. Pre-
sented at the 13th Nordic Computational Linguistic
Conference, NoDaLiDa 2001, May. Available at:
http://www.speech.kth.se/ botte/.
Botond Pakucs. 2002. VoiceXML-based dynamic
plug and play dialogue management for mobile en-
vironments. In Proceedings of ISCA T&amp;R Workshop
on Multi-Modal Dialogue in Mobile Environments,
Kloster Irsee, Germany, June.
Botond Pakucs. 2003. A human-centered ap-
proach to speech interfaces in mobile and ubiquitous
computing environments. TMH-QPRS,Quarterly
Progress and Status Report, 45. Available at:
http://www.speech.kth.sel botte/.
Markku Turunen and Jaakko Hakulinen. 2000. Jaspis
- a framework for multilingual adaptive speech ap-
plications. In Proceedings of 6th International
Conference of Spoken Language Processing (ICSLP
2000), Peking, China.
Ingrid Zukerman and Diane Litman. 2001. Natural
language processing and user modeling: Synergies
and limitations. User Modeling and User-Adapted
Interaction, 11:129-158.
</reference>
<page confidence="0.998602">
102
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.337470">
<title confidence="0.7947658">SesaME: A Framework for Personalised and Adaptive Speech Interfaces Botond CTT (Centre for Speech KTH (Royal Institute of Drottning Kristin as vag</title>
<address confidence="0.773644">SE-100 44, Stockholm,</address>
<email confidence="0.981032">botte@speech.kth.se</email>
<abstract confidence="0.997933357142857">This paper presents some for using highly personalised speech interfaces. In particular, the is on the requirements for adaptation in mobile environments. Furthermore, SesaME, a framework for personalised and adaptive speech interfaces is described. SesaME supports a multi-domain approach and eventbased, asynchronous dialogue management. Finally, a description of how SesaME is employed within the framework of the PER demonstrator is given.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allen</author>
<author>Donna Byron</author>
<author>Myroslava Dzikovska</author>
<author>George Ferguson</author>
<author>Lucian Galescu</author>
<author>Amanda Stent</author>
</authors>
<title>An architecture for a generic dialogue shell.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<pages>6--3</pages>
<contexts>
<context position="13383" citStr="Allen et al., 2000" startWordPosition="2032" endWordPosition="2035">e dialogue. However, this representation is not formalised; the information state is merely a collection of all data available to the dialogue system. The update of the information state is event-based, where events can be dialogue moves, internal events, or changes in the user&apos;s external context. The event-based functionality enables an asynchronous information processing (Blaylock et al., 2002). The SesaME architecture and the theoretical considerations behind it are in some aspects comparable to other agent-based architectures such as the Jaspis (Turunen and Hakulinen, 2000) and the TRIPS (Allen et al., 2000) architectures. SesaME relies on the ATLAS generic speech technology platform (Melin, 2001). The ATLAS platform provides high-level primitives for basic speech I/0, but access to low-level data is also facilitated. 5.1 A Multi-Domain Approach One of the key-issues in the SesaME architecture is to support a multi-domain approach. The locally available service descriptions, including dialogue descriptions, recognition grammars and domain knowledge has to be dynamically loaded and activated on the fly. For handling these requirements, a dynamic plug-and-play functionality of the dialogue manageme</context>
</contexts>
<marker>Allen, Byron, Dzikovska, Ferguson, Galescu, Stent, 2000</marker>
<rawString>James Allen, Donna Byron, Myroslava Dzikovska, George Ferguson, Lucian Galescu, and Amanda Stent. 2000. An architecture for a generic dialogue shell. Natural Language Engineering, 6(3):1-16, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nate Blaylock</author>
<author>James Allen</author>
<author>George Ferguson</author>
</authors>
<title>Synchronization in an asynchronous agentbased architecture for dialogue systems.</title>
<date>2002</date>
<booktitle>In Proceedings of 3rd SIGdial Workshop on Discourse and Dialogue,</booktitle>
<contexts>
<context position="13163" citStr="Blaylock et al., 2002" startWordPosition="1996" endWordPosition="1999">uman-centered approach as well as for mobile environments. SesaME features a blackboard and agent based architecture. The central blackboard stores the representation of the information state (Larsson and Traum, 2000) of the dialogue. However, this representation is not formalised; the information state is merely a collection of all data available to the dialogue system. The update of the information state is event-based, where events can be dialogue moves, internal events, or changes in the user&apos;s external context. The event-based functionality enables an asynchronous information processing (Blaylock et al., 2002). The SesaME architecture and the theoretical considerations behind it are in some aspects comparable to other agent-based architectures such as the Jaspis (Turunen and Hakulinen, 2000) and the TRIPS (Allen et al., 2000) architectures. SesaME relies on the ATLAS generic speech technology platform (Melin, 2001). The ATLAS platform provides high-level primitives for basic speech I/0, but access to low-level data is also facilitated. 5.1 A Multi-Domain Approach One of the key-issues in the SesaME architecture is to support a multi-domain approach. The locally available service descriptions, inclu</context>
</contexts>
<marker>Blaylock, Allen, Ferguson, 2002</marker>
<rawString>Nate Blaylock, James Allen, and George Ferguson. 2002. Synchronization in an asynchronous agentbased architecture for dialogue systems. In Proceedings of 3rd SIGdial Workshop on Discourse and Dialogue, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Boye</author>
<author>Beth Ann Hockey</author>
<author>Manny Rayner</author>
</authors>
<title>Asynchronous dialogue management: Two case studies.</title>
<date>2000</date>
<booktitle>In David Traum and Massimo Poesio, editors, Proceedings of GOTALOG 2000,</booktitle>
<location>Gothenburg, Sweden,</location>
<contexts>
<context position="7909" citStr="Boye et al., 2000" startWordPosition="1199" endWordPosition="1202">and or change the parameters of some previously initiated service. Furthermore, the system itself should be able to interrupt an ongoing dialogue and direct the user&apos;s attention to higher priority events taking place in the user&apos;s immediate environment (physical or virtual). Thus, in mobile environments it is desirable to support a wide range of domains within one and the same dialogue. Thus, a multi-domain approach (Chung et al., 1999) is necessary to allow the user to transparently and seamlessly switch between several topic domains and services. Similarly, asynchronous dialogue management (Boye et al., 2000) should also be supported by speech interfaces in mobile environments. Employing speech interfaces in mobile environments may even introduce some new user requirements. For instance, means to coordinate and control multiple concurrent speech interfaces may become necessary in order to avoid the introduction of new usability related problems. When several services and appliances, embedded in the same environment, are listening for user commands, or even taking initiative pro-actively, it is possible, due to errors or misrecognitions, that several speech-based services may be triggered by a sing</context>
</contexts>
<marker>Boye, Hockey, Rayner, 2000</marker>
<rawString>Johan Boye, Beth Ann Hockey, and Manny Rayner. 2000. Asynchronous dialogue management: Two case studies. In David Traum and Massimo Poesio, editors, Proceedings of GOTALOG 2000, Gothenburg, Sweden, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harry Bunt</author>
</authors>
<title>Context and dialogue control.</title>
<date>1994</date>
<journal>THINK Quarterly,</journal>
<pages>3--1</pages>
<contexts>
<context position="5786" citStr="Bunt, 1994" startWordPosition="871" endWordPosition="872">ut the current individual user still remains a central challenging issue. 2.2 Adaptation to the Context Situational&apos; factors are used in human-to-human communication to facilitate a smoother and more natural interaction. Context-of-use is relevant knowledge about the actual situation, conversational partners, domain, topic etc. The particular situation is, however, not entirely defined by external variables, but depends also on how users conceptualise the situation (Fischer, 2000). Thus, context-of-use also includes the user&apos;s physical and perceptual, social and emotional informational state (Bunt, 1994). Most experimental and commercial dialogue systems are single-domain dialogue systems. Therefore, with regard to the physical and perceptual context a more or less static situation is usually assumed. However, the growing interest for using speech interfaces in mobile and ubiquitous computing environments makes it necessary to develop better solutions for capturing, modelling and adapting to the variations in the users&apos; physical and perceptual context. Speech-based interaction with ubiquitous services in mobile environments differs from interacting with desktop or telephony-based speech inter</context>
</contexts>
<marker>Bunt, 1994</marker>
<rawString>Harry Bunt. 1994. Context and dialogue control. THINK Quarterly, 3(1):19-31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grace Chung</author>
<author>Stephanie Seneff</author>
<author>Lee Hetherington</author>
</authors>
<title>Towards multi-domain speech understanding using a two-stage recognizer.</title>
<date>1999</date>
<booktitle>In Proceedings of Eurospeech &apos;99,</booktitle>
<pages>2655--2658</pages>
<location>Budapest, Hungary,</location>
<contexts>
<context position="7731" citStr="Chung et al., 1999" startWordPosition="1173" endWordPosition="1176">should be able to initiate a new task while waiting for some other specific service to be completed. The user should have the possibility to easily cancel a previously issued command or change the parameters of some previously initiated service. Furthermore, the system itself should be able to interrupt an ongoing dialogue and direct the user&apos;s attention to higher priority events taking place in the user&apos;s immediate environment (physical or virtual). Thus, in mobile environments it is desirable to support a wide range of domains within one and the same dialogue. Thus, a multi-domain approach (Chung et al., 1999) is necessary to allow the user to transparently and seamlessly switch between several topic domains and services. Similarly, asynchronous dialogue management (Boye et al., 2000) should also be supported by speech interfaces in mobile environments. Employing speech interfaces in mobile environments may even introduce some new user requirements. For instance, means to coordinate and control multiple concurrent speech interfaces may become necessary in order to avoid the introduction of new usability related problems. When several services and appliances, embedded in the same environment, are li</context>
</contexts>
<marker>Chung, Seneff, Hetherington, 1999</marker>
<rawString>Grace Chung, Stephanie Seneff, and Lee Hetherington. 1999. Towards multi-domain speech understanding using a two-stage recognizer. In Proceedings of Eurospeech &apos;99, pages 2655-2658, Budapest, Hungary, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anind K Dey</author>
</authors>
<title>Understanding and using context.</title>
<date>2001</date>
<booktitle>Personal and Ubiquituos Computing, 5(1). Special issue on Situated Interaction and Ubiquitous Computing.</booktitle>
<contexts>
<context position="18257" citStr="Dey, 2001" startWordPosition="2751" endWordPosition="2752">rent system components and knowledge sources. A central, shared information storage, a blackboard, and a collection of autonomous software agents are the main components of the IM. The detailed description of the IM&apos;s functionality is, however, beyond the scope of this paper. 5.3 Adaptation to the Context A major goal in SesaME is to make full use of the potentials of the individual user models and to achieve context-based adaptation. The adopted solution is inspired by attempts to achieve context-aware computing in the research field of ubiquitous computing. According to a proposed solution (Dey, 2001) the different possible context types are categorised in primary and secondary context types. The primary context types characterising the situation of a particular entity consist of the location and the identity of the entity, the time of the interaction, and the activity being performed. The secondary pieces of context share a major common characteristic: they can be indexed by the primary context because they are attributes of the entity with primary context. For example, a person&apos;s phone number is a secondary type of context and can be obtained through the primary context. In SesaME, after</context>
</contexts>
<marker>Dey, 2001</marker>
<rawString>Anind K. Dey. 2001. Understanding and using context. Personal and Ubiquituos Computing, 5(1). Special issue on Situated Interaction and Ubiquitous Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kerstin Fischer</author>
</authors>
<title>What is situation?</title>
<date>2000</date>
<booktitle>In Proceedings of GOTALOG</booktitle>
<location>Gothenburg, Sweden,</location>
<contexts>
<context position="5660" citStr="Fischer, 2000" startWordPosition="854" endWordPosition="855">ifferent dialogue systems (Zukerman and Litman, 2001). However, how to model and exploit data about users in general and data about the current individual user still remains a central challenging issue. 2.2 Adaptation to the Context Situational&apos; factors are used in human-to-human communication to facilitate a smoother and more natural interaction. Context-of-use is relevant knowledge about the actual situation, conversational partners, domain, topic etc. The particular situation is, however, not entirely defined by external variables, but depends also on how users conceptualise the situation (Fischer, 2000). Thus, context-of-use also includes the user&apos;s physical and perceptual, social and emotional informational state (Bunt, 1994). Most experimental and commercial dialogue systems are single-domain dialogue systems. Therefore, with regard to the physical and perceptual context a more or less static situation is usually assumed. However, the growing interest for using speech interfaces in mobile and ubiquitous computing environments makes it necessary to develop better solutions for capturing, modelling and adapting to the variations in the users&apos; physical and perceptual context. Speech-based int</context>
</contexts>
<marker>Fischer, 2000</marker>
<rawString>Kerstin Fischer. .2000. What is situation? In Proceedings of GOTALOG 2000, Gothenburg, Sweden, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Jameson</author>
<author>Frank Wittig</author>
</authors>
<title>Leveraging data about users in general in the learning of individual user models.</title>
<date>2001</date>
<booktitle>Proceedings of IJCAI 2001,</booktitle>
<pages>1185--1192</pages>
<editor>In B. Nebel, editor,</editor>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="4986" citStr="Jameson and Wittig, 2001" startWordPosition="754" endWordPosition="757">rapersonal variations. These intrapersonal variations carry information which may be significant for the interaction with the specific user. While a speech interface should support interaction with all potential users, the interaction should also be adapted and fine-tuned to the current individual user. During a single interaction a dialogue system has to be able to identify and capture the intrapersonal variations and distinguish them from the interpersonal variations. Consequently, a key issue is how to model generic user characteristics and specific individual user features simultaneously (Jameson and Wittig, 2001). User modelling has been successfully employed in several different dialogue systems (Zukerman and Litman, 2001). However, how to model and exploit data about users in general and data about the current individual user still remains a central challenging issue. 2.2 Adaptation to the Context Situational&apos; factors are used in human-to-human communication to facilitate a smoother and more natural interaction. Context-of-use is relevant knowledge about the actual situation, conversational partners, domain, topic etc. The particular situation is, however, not entirely defined by external variables,</context>
</contexts>
<marker>Jameson, Wittig, 2001</marker>
<rawString>Anthony Jameson and Frank Wittig. 2001. Leveraging data about users in general in the learning of individual user models. In B. Nebel, editor, Proceedings of IJCAI 2001, pages 1185-1192, San Francisco, CA, USA. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Jameson</author>
</authors>
<title>Modelling both the context and the user.</title>
<date>2001</date>
<booktitle>Personal and Ubiquitous Computing,</booktitle>
<pages>5--29</pages>
<contexts>
<context position="3306" citStr="Jameson, 2001" startWordPosition="498" endWordPosition="500">es in the context. Variations in the context may affect the user&apos;s behaviour and induce different user related variations. For instance, time pressure and increased cognitive load on the user may cause variations in the user&apos;s speech (Miiller et al., 2001). 95 As the user characteristics and the parameters of the situation may change during the same interaction, both the characteristics of the context and the user&apos;s properties have to be considered simultaneously. The usability of a system can be expected to increase if both the user properties and the user&apos;s situation are taken into account (Jameson, 2001). Thus, simultaneous adaptation to the user and to the context is a major challenge for providing more natural and conversational speech interfaces. 2.1 Adaptation to the User From a practical point of view, not all kinds of user related variations are equally interesting for a dialogue system, see Figure 1. Only a subset of all theoretically possible feature variations are actually realised in practice. This is the set of practically possible variations. For a realistic dialogue system, which encounters just a limited set of users and handles a limited domain, only a subset of these practical</context>
</contexts>
<marker>Jameson, 2001</marker>
<rawString>Anthony Jameson. 2001. Modelling both the context and the user. Personal and Ubiquitous Computing, 5:29-33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Staffan Larsson</author>
<author>David R Traum</author>
</authors>
<title>Information state and dialogue management in the TRINDI dialogue move engine toolkit.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<pages>6--323</pages>
<contexts>
<context position="12758" citStr="Larsson and Traum, 2000" startWordPosition="1934" endWordPosition="1937">rface, the handling of the potential security and integrity issues are also facilitated. Furthermore, personalised speech interfaces are expected to provide an unobtrusive, user-friendly interaction and seamless access to services and appliances in mobile environments (Pakucs, 2003). 5 The SesaME Framework SesaME, shown in Figure 2, is a generic, taskoriented dialogue manager specially designed for the human-centered approach as well as for mobile environments. SesaME features a blackboard and agent based architecture. The central blackboard stores the representation of the information state (Larsson and Traum, 2000) of the dialogue. However, this representation is not formalised; the information state is merely a collection of all data available to the dialogue system. The update of the information state is event-based, where events can be dialogue moves, internal events, or changes in the user&apos;s external context. The event-based functionality enables an asynchronous information processing (Blaylock et al., 2002). The SesaME architecture and the theoretical considerations behind it are in some aspects comparable to other agent-based architectures such as the Jaspis (Turunen and Hakulinen, 2000) and the T</context>
</contexts>
<marker>Larsson, Traum, 2000</marker>
<rawString>Staffan Larsson and David R. Traum. 2000. Information state and dialogue management in the TRINDI dialogue move engine toolkit. Natural Language Engineering, 6:323-340, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rakan Melin</author>
</authors>
<title>ATLAS: A generic software platform for speech technology based applications. TMH-QPRS,Quarterly Progress and Status</title>
<date>2001</date>
<tech>Report, 42.</tech>
<contexts>
<context position="13474" citStr="Melin, 2001" startWordPosition="2046" endWordPosition="2047">ection of all data available to the dialogue system. The update of the information state is event-based, where events can be dialogue moves, internal events, or changes in the user&apos;s external context. The event-based functionality enables an asynchronous information processing (Blaylock et al., 2002). The SesaME architecture and the theoretical considerations behind it are in some aspects comparable to other agent-based architectures such as the Jaspis (Turunen and Hakulinen, 2000) and the TRIPS (Allen et al., 2000) architectures. SesaME relies on the ATLAS generic speech technology platform (Melin, 2001). The ATLAS platform provides high-level primitives for basic speech I/0, but access to low-level data is also facilitated. 5.1 A Multi-Domain Approach One of the key-issues in the SesaME architecture is to support a multi-domain approach. The locally available service descriptions, including dialogue descriptions, recognition grammars and domain knowledge has to be dynamically loaded and activated on the fly. For handling these requirements, a dynamic plug-and-play functionality of the dialogue management capabilities has been developed (Pakucs, 2002). In SesaME, most of the operations relate</context>
<context position="21424" citStr="Melin, 2001" startWordPosition="3247" endWordPosition="3248">milar interactions, or no obvious patterns are present in the previous interactions (such as a CD purchasing task), then the default standardised prompt is used. 6 Application Before evaluating SesaME as a generic, adaptive and personalised dialogue manager in mobile and multi-domain environments, it is necessary to evaluate it as a traditional domain-dependent dialogue manager. In the first application of SesaME, the focus is on the evaluation of the plug-and-play functionality. This evaluation is conducted within the framework of the PER (Prototype Entrance Receptionist) project (Pakucs and Melin, 2001). 6.1 The PER demonstrator PER, Figure 3, is an animated-agent based automated receptionist located at the entrance of our department. Originally, the system was developed to allow fast and robust access for employees. The application&apos;s functionality is stream-lined for this 100 purpose. PER features a multilingual speaker verification system for Swedish and English. An employee, when approaching the gate, is expected to say his/her password, which consists of the employee&apos;s name and a random digit sequence displayed on the screen. Figure 3: A screen shot of the PER demonstrator. 6.2 Employing</context>
</contexts>
<marker>Melin, 2001</marker>
<rawString>Rakan Melin. 2001. ATLAS: A generic software platform for speech technology based applications. TMH-QPRS,Quarterly Progress and Status Report, 42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Muller</author>
<author>Barbara GroBmann-Hutter</author>
<author>Anthony Jameson</author>
<author>Ralf Rummer</author>
<author>Frank Wittig</author>
</authors>
<title>Recognizing time pressure and cognitive load on the basis of speech: An experimental study.</title>
<date>2001</date>
<booktitle>In UM2001, User Modeling: Proceedings of the Eighth International Conference.</booktitle>
<marker>Muller, GroBmann-Hutter, Jameson, Rummer, Wittig, 2001</marker>
<rawString>Christian Muller, Barbara GroBmann-Hutter, Anthony Jameson, Ralf Rummer, and Frank Wittig. 2001. Recognizing time pressure and cognitive load on the basis of speech: An experimental study. In UM2001, User Modeling: Proceedings of the Eighth International Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Botond Pakucs</author>
<author>Hakan Melin</author>
</authors>
<title>PER: A speech based automated entrance receptionist.</title>
<date>2001</date>
<booktitle>Presented at the 13th Nordic Computational Linguistic Conference, NoDaLiDa</booktitle>
<note>Available at: http://www.speech.kth.se/ botte/.</note>
<contexts>
<context position="21424" citStr="Pakucs and Melin, 2001" startWordPosition="3245" endWordPosition="3248">e are no similar interactions, or no obvious patterns are present in the previous interactions (such as a CD purchasing task), then the default standardised prompt is used. 6 Application Before evaluating SesaME as a generic, adaptive and personalised dialogue manager in mobile and multi-domain environments, it is necessary to evaluate it as a traditional domain-dependent dialogue manager. In the first application of SesaME, the focus is on the evaluation of the plug-and-play functionality. This evaluation is conducted within the framework of the PER (Prototype Entrance Receptionist) project (Pakucs and Melin, 2001). 6.1 The PER demonstrator PER, Figure 3, is an animated-agent based automated receptionist located at the entrance of our department. Originally, the system was developed to allow fast and robust access for employees. The application&apos;s functionality is stream-lined for this 100 purpose. PER features a multilingual speaker verification system for Swedish and English. An employee, when approaching the gate, is expected to say his/her password, which consists of the employee&apos;s name and a random digit sequence displayed on the screen. Figure 3: A screen shot of the PER demonstrator. 6.2 Employing</context>
</contexts>
<marker>Pakucs, Melin, 2001</marker>
<rawString>Botond Pakucs and Hakan Melin. 2001. PER: A speech based automated entrance receptionist. Presented at the 13th Nordic Computational Linguistic Conference, NoDaLiDa 2001, May. Available at: http://www.speech.kth.se/ botte/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Botond Pakucs</author>
</authors>
<title>VoiceXML-based dynamic plug and play dialogue management for mobile environments.</title>
<date>2002</date>
<booktitle>In Proceedings of ISCA T&amp;R Workshop on Multi-Modal Dialogue in Mobile Environments,</booktitle>
<location>Kloster Irsee, Germany,</location>
<contexts>
<context position="14032" citStr="Pakucs, 2002" startWordPosition="2128" endWordPosition="2129"> ATLAS generic speech technology platform (Melin, 2001). The ATLAS platform provides high-level primitives for basic speech I/0, but access to low-level data is also facilitated. 5.1 A Multi-Domain Approach One of the key-issues in the SesaME architecture is to support a multi-domain approach. The locally available service descriptions, including dialogue descriptions, recognition grammars and domain knowledge has to be dynamically loaded and activated on the fly. For handling these requirements, a dynamic plug-and-play functionality of the dialogue management capabilities has been developed (Pakucs, 2002). In SesaME, most of the operations related to the plug-and-play functionality are carried out by the Dialogue Engine (DE). Synchronisation and communication with the mobile service environment is taken care by the Application Interface. Whenever new changes occur in the service environment (e.g. new services become available or existing services disappear etc.), the Application Interface dynamically updates the Dialogue Description Collection (DDC). All currently available dialogue descriptions are stored in the DDC. Beside the different task- and domain-specific dialogue descriptions, DDC al</context>
<context position="16680" citStr="Pakucs, 2002" startWordPosition="2511" endWordPosition="2512">d dialogue description, • the actual dialogue management. At the current stage, the identification of the correct dialogue description is based on topic vectors and keywords extracted from the VoiceXML documents. However, context models, user models or plan based mechanisms could also be used for this purpose. During the activation, the appropriate dialogue description is translated into internal data structures appropriate for the dialogue management in SesaME. This process is performed through JAXB3 which provides a fast way to create a twoway mapping between XML documents and Java objects (Pakucs, 2002). 5.2 Dialogue Management During the dialogue management, the generated internal data structures are used for frame-based dialogue management. The actual dialogue management process does not follow the VoiceXML specifications. Accordingly, VoiceXML is only used as a domain description language. During the dialogue activation process, two additional parallel data structures are generated. These data structures can be accessed by the Context and the User Model Managers and by the Domain Knowledge Manager. These data structures may be updated with suggested pieces of information, which can be use</context>
</contexts>
<marker>Pakucs, 2002</marker>
<rawString>Botond Pakucs. 2002. VoiceXML-based dynamic plug and play dialogue management for mobile environments. In Proceedings of ISCA T&amp;R Workshop on Multi-Modal Dialogue in Mobile Environments, Kloster Irsee, Germany, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Botond Pakucs</author>
</authors>
<title>A human-centered approach to speech interfaces in mobile and ubiquitous computing environments. TMH-QPRS,Quarterly Progress and Status Report, 45. Available at: http://www.speech.kth.sel botte/.</title>
<date>2003</date>
<contexts>
<context position="10349" citStr="Pakucs, 2003" startWordPosition="1575" endWordPosition="1576"> long term adaptation while adaptation to the context can be regarded as short term adaptation. Thus, simultaneous adaptation to both the context and to the user is facilitated. While using a personalised solution, only the characteristics of a single individual user have to be modelled. Thus, the simultaneous modelling of generic user characteristics and specific user features is also facilitated. 4.1 The human-centered approach The human-centered approach is a feasible solution, proposed for achieving personalisation of speech interfaces which are intended to be used in mobile environments (Pakucs, 2003). According to this approach each user is expected to use an individual and highly personalised universal speech interface to access a multitude of services and appliances. Access to the locally available services and appliances is handled through the personalised speech interface, integrated into some personal and wearable appliance such as a mobile phone or a PDA. Applicationspecific data, including dialogue management capabilities, domain knowledge etc., has to be encoded in service descriptions and stored locally at the service provider side. Whenever the user 97 enters a new environment, </context>
<context position="12417" citStr="Pakucs, 2003" startWordPosition="1885" endWordPosition="1886">e data becomes available for machine learning. Consequently, user modelling and adaptation is facilitated. A highly personalised universal speech interface appears to be ideal for coordinating and controlling multiple concurrent speech interfaces and for supporting a multi-domain approach. By employing a personalised speech interface, the handling of the potential security and integrity issues are also facilitated. Furthermore, personalised speech interfaces are expected to provide an unobtrusive, user-friendly interaction and seamless access to services and appliances in mobile environments (Pakucs, 2003). 5 The SesaME Framework SesaME, shown in Figure 2, is a generic, taskoriented dialogue manager specially designed for the human-centered approach as well as for mobile environments. SesaME features a blackboard and agent based architecture. The central blackboard stores the representation of the information state (Larsson and Traum, 2000) of the dialogue. However, this representation is not formalised; the information state is merely a collection of all data available to the dialogue system. The update of the information state is event-based, where events can be dialogue moves, internal event</context>
</contexts>
<marker>Pakucs, 2003</marker>
<rawString>Botond Pakucs. 2003. A human-centered approach to speech interfaces in mobile and ubiquitous computing environments. TMH-QPRS,Quarterly Progress and Status Report, 45. Available at: http://www.speech.kth.sel botte/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markku Turunen</author>
<author>Jaakko Hakulinen</author>
</authors>
<title>Jaspis - a framework for multilingual adaptive speech applications.</title>
<date>2000</date>
<booktitle>In Proceedings of 6th International Conference of Spoken Language Processing (ICSLP</booktitle>
<location>Peking, China.</location>
<contexts>
<context position="13348" citStr="Turunen and Hakulinen, 2000" startWordPosition="2024" endWordPosition="2028">mation state (Larsson and Traum, 2000) of the dialogue. However, this representation is not formalised; the information state is merely a collection of all data available to the dialogue system. The update of the information state is event-based, where events can be dialogue moves, internal events, or changes in the user&apos;s external context. The event-based functionality enables an asynchronous information processing (Blaylock et al., 2002). The SesaME architecture and the theoretical considerations behind it are in some aspects comparable to other agent-based architectures such as the Jaspis (Turunen and Hakulinen, 2000) and the TRIPS (Allen et al., 2000) architectures. SesaME relies on the ATLAS generic speech technology platform (Melin, 2001). The ATLAS platform provides high-level primitives for basic speech I/0, but access to low-level data is also facilitated. 5.1 A Multi-Domain Approach One of the key-issues in the SesaME architecture is to support a multi-domain approach. The locally available service descriptions, including dialogue descriptions, recognition grammars and domain knowledge has to be dynamically loaded and activated on the fly. For handling these requirements, a dynamic plug-and-play fun</context>
</contexts>
<marker>Turunen, Hakulinen, 2000</marker>
<rawString>Markku Turunen and Jaakko Hakulinen. 2000. Jaspis - a framework for multilingual adaptive speech applications. In Proceedings of 6th International Conference of Spoken Language Processing (ICSLP 2000), Peking, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ingrid Zukerman</author>
<author>Diane Litman</author>
</authors>
<title>Natural language processing and user modeling: Synergies and limitations. User Modeling and User-Adapted Interaction,</title>
<date>2001</date>
<pages>11--129</pages>
<contexts>
<context position="5099" citStr="Zukerman and Litman, 2001" startWordPosition="770" endWordPosition="774">tion with the specific user. While a speech interface should support interaction with all potential users, the interaction should also be adapted and fine-tuned to the current individual user. During a single interaction a dialogue system has to be able to identify and capture the intrapersonal variations and distinguish them from the interpersonal variations. Consequently, a key issue is how to model generic user characteristics and specific individual user features simultaneously (Jameson and Wittig, 2001). User modelling has been successfully employed in several different dialogue systems (Zukerman and Litman, 2001). However, how to model and exploit data about users in general and data about the current individual user still remains a central challenging issue. 2.2 Adaptation to the Context Situational&apos; factors are used in human-to-human communication to facilitate a smoother and more natural interaction. Context-of-use is relevant knowledge about the actual situation, conversational partners, domain, topic etc. The particular situation is, however, not entirely defined by external variables, but depends also on how users conceptualise the situation (Fischer, 2000). Thus, context-of-use also includes th</context>
</contexts>
<marker>Zukerman, Litman, 2001</marker>
<rawString>Ingrid Zukerman and Diane Litman. 2001. Natural language processing and user modeling: Synergies and limitations. User Modeling and User-Adapted Interaction, 11:129-158.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>