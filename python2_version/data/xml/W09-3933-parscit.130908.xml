<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.976017">
Models for Multiparty Engagement in Open-World Dialog
</title>
<author confidence="0.98582">
Dan Bohus
</author>
<affiliation confidence="0.955034">
Microsoft Research
</affiliation>
<address confidence="0.949168">
One Microsoft Way
Redmond, WA, 98052
</address>
<email confidence="0.99951">
dbohus@microsoft.com
</email>
<author confidence="0.9852">
Eric Horvitz
</author>
<affiliation confidence="0.954474">
Microsoft Research
</affiliation>
<address confidence="0.9498695">
One Microsoft Way
Redmond, WA, 98052
</address>
<email confidence="0.999648">
horvitz@microsoft.com
</email>
<sectionHeader confidence="0.995655" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99997875">
We present computational models that allow
spoken dialog systems to handle multi-
participant engagement in open, dynamic envi-
ronments, where multiple people may enter and
leave conversations, and interact with the sys-
tem and with others in a natural manner. The
models for managing the engagement process
include components for (1) sensing the en-
gagement state, actions and intentions of mul-
tiple agents in the scene, (2) making engage-
ment decisions (i.e. whom to engage with, and
when) and (3) rendering these decisions in a set
of coordinated low-level behaviors in an embo-
died conversational agent. We review results
from a study of interactions &amp;quot;in the wild&amp;quot; with a
system that implements such a model.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999919794871795">
To date, nearly all spoken dialog systems research has
focused on the challenge of engaging single users on
tasks defined within a relatively narrow context. Efforts
in this realm have led to significant progress including
large-scale deployments that now make spoken dialog
systems common features in the daily lives of millions
of people. However, research on dialog systems has
largely overlooked important challenges with the initia-
tion, maintenance, and suspension of conversations that
are common in the course of natural communication and
collaborations among people. In (Bohus and Horvitz,
2009) we outlined a set of core challenges for extending
traditional closed-world dialog systems to systems that
have competency in open-world dialog. The work de-
scribed here is part of a larger research effort aimed at
addressing these challenges, and constructing computa-
tional models to support the core interaction skills re-
quired for open-world dialog. In particular, we focus our
attention in this paper on the challenges of managing
engagement – “the process by which two (or more) par-
ticipants establish, maintain and end their perceived
connection during interactions they jointly undertake”,
cf. Sidner et al. (2004) in open-world settings.
We begin by reviewing the challenges of managing
engagement in the open-world in the next section. In
Section 3, we survey the terrain of related efforts that
provides valuable context for the new work described in
this paper. In Section 4, we introduce a computational
model for multiparty situated engagement. The model
harnesses components for sensing the engagement state,
actions, and intentions of people in the scene for making
high-level engagement decisions (whom to engage with,
and when), and for rendering these decisions into a set
of low-level coordinated behaviors (e.g., gestures, eye
gaze, greetings, etc.). Then, we describe an initial ob-
servational study with the proposed model, and discuss
some of the lessons learned through this experiment.
Finally, in Section 6, we summarize this work and out-
line several directions for future research.
</bodyText>
<sectionHeader confidence="0.959157" genericHeader="introduction">
2 Engagement in Open-World Dialog
</sectionHeader>
<bodyText confidence="0.999871777777778">
In traditional, single-user systems the engagement prob-
lem can often be resolved in a relatively simple manner.
For instance, in telephony-based applications, it is typi-
cally safe to assume that a user is engaged with a dialog
system once a call has been received. Similarly, push-
to-talk buttons are often used in multimodal mobile ap-
plications. Although these solutions are sufficient and
even natural in closed, single-user contexts, they be-
come inappropriate for open-world systems that must
operate continuously in open, dynamic environments,
such as robots, interactive billboards, or embodied con-
versational agents.
Interaction in the open-world is characterized by two
aspects that capture key departures from assumptions
traditionally made in spoken dialog systems (Bohus and
Horvitz, 2009). The first one is the dynamic, multiparty
nature of the interaction, i.e., the world typically con-
tains not just one, but multiple agents that are relevant
</bodyText>
<note confidence="0.709873">
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 225–234,
</note>
<affiliation confidence="0.662781">
Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics
</affiliation>
<page confidence="0.999164">
225
</page>
<bodyText confidence="0.999941052631579">
to the interactive system. Engagements in open worlds
are often dynamic and asynchronous, i.e. relevant agents
may enter and leave the observable world at any time,
may interact with the system and with each other, and
their goals, needs, and intentions may change over time.
Managing the engagement process in this context re-
quires that a system explicitly represents, models, and
reasons about multiple agents and interaction contexts,
and maintains and leverages long-term memory of the
interactions to provide support and assistance.
A second important aspect that distinguishes open-
world from closed-world dialog is the situated nature of
the interaction, i.e., the fact that the surrounding physi-
cal environment provides rich, streaming context that is
relevant for conducting and organizing the interactions.
Situated interactions among people often hinge on
shared information about physical details and relation-
ships, including structures, geometric relationships and
pathways, objects, topologies, and communication af-
fordances. The often implicit, yet powerful physicality
of situated interaction, provides opportunities for mak-
ing inferences in open-world dialog systems, and chal-
lenges system designers to innovate across a spectrum
of complexity and sophistication. Physicality and em-
bodiment also provide important affordances that can be
used by a system to support the engagement process.
For instance, the use of a rendered or physically embo-
died avatar in a spoken dialog system provides a natural
point of visual engagement between the system and
people, and allows the system to employ natural signal-
ing about attention and engagement with head pose,
gaze, facial expressions, pointing and gesturing.
We present in this paper methods that move beyond
the realm of closed-world dialog with a situated multi-
party engagement model that can enable a computation-
al system to fluidly engage, disengage and re-engage
one or multiple people, and support natural interactions
in an open-world context.
</bodyText>
<sectionHeader confidence="0.999972" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999991020408163">
The process of engagement between people, and be-
tween people and computational systems has received a
fair amount of attention. Observational studies in the
sociolinguistics and conversational analysis communi-
ties have revealed that engagement is a complex, mixed-
initiative, highly-coordinated process that often involves
a variety of non-verbal cues and signals, (Goffman,
1963; Kendon, 1990), spatial trajectory and proximity
(Hall, 1966; Kendon, 1990b), gaze and mutual attention
(Argyle and Cook, 1976), head and hand gestures (Ken-
don, 1990), as well as verbal greetings.
A number of researchers have also investigated is-
sues of engagement in human-computer and human-
robot interaction contexts. Sidner and colleagues (2004)
define engagement as “the process by which two (or
more) participants establish, maintain and end their per-
ceived connection during interactions they jointly un-
dertake”, and focus on the process of maintaining en-
gagement. They show in a user study (Sidner et al.,
2004; 2005) that people directed their attention to a ro-
bot more often when the robot made engagement ges-
tures throughout the interaction (i.e. tracked the user’s
face, and pointed to relevant objects at appropriate times
in the conversation.) Peters (2005; 2005b) uses an alter-
native definition of engagement as “the value that a par-
ticipant in an interaction attributes to the goal of being
together with the other participant(s) and of continuing
the interaction,” and present the high-level schematics
for an algorithm for establishing and maintaining en-
gagement. The algorithm highlights the importance of
mutual attention and eye gaze and relies on a heuristi-
cally computed “interest level” to decide when to start a
conversation. Michalowski and colleagues (2006) pro-
pose and conduct experiments with a model of engage-
ment grounded in proxemics (Hall, 1966) which classi-
fies relevant agents in the scene in four different catego-
ries (present, attending, engaged and interacting) based
on their distance to the robot. The robot’s behaviors are
in turn conditioned on the four categories above.
In our work, we follow Sidner’s definition of en-
gagement as a process (Sidner et al., 2004) and describe
a computational model for situated multiparty engage-
ment. The proposed model draws on several ideas from
the existing body of work, but moves beyond it and
provides a more comprehensive framework for manag-
ing the engagement process in a dynamic, open-world
context, where multiple people with different and
changing goals may enter and leave, and communicate
and coordinate with each other and with the system.
</bodyText>
<sectionHeader confidence="0.998717" genericHeader="method">
4 Models for Multiparty Engagement
</sectionHeader>
<bodyText confidence="0.999826157894737">
The proposed framework for managing engagement is
centered on a reified notion of interaction, defined here
as a basic unit of sustained, interactive problem-solving.
Each interaction involves two or more participants, and
this number may vary in time; new participants may
join an existing interaction, or current participants may
leave an interaction at any point in time. The system is
actively engaged in at most one interaction at a time
(with one or multiple participants), but it can simulta-
neously keep track of additional, suspended interactions.
In this context, engagement is viewed as the process
subsuming the joint, coordinated activities by which
participants initiate, maintain, join, abandon, suspend,
resume, or terminate an interaction. Appendix A shows
by means of an example the various stages of an interac-
tion and the role played by the engagement process.
Successfully modeling the engagement process in a
situated, multi-participant context requires that the sys-
tem (1) senses and reasons about the engagement state,
</bodyText>
<page confidence="0.997102">
226
</page>
<bodyText confidence="0.974476555555556">
actions and intentions of multiple agents in the scene,
(2) makes high-level engagement control decisions (i.e.
about whom to engage or disengage with, and when)
and (3) executes and signals these decisions to the other
participants in an appropriate and expected manner (e.g.
renders them in a set of coordinated behaviors such as
gestures, greetings, etc.). The proposed model subsumes
these three components, which we discuss in more de-
tail in the following subsections.
</bodyText>
<subsectionHeader confidence="0.995425">
4.1 Engagement State, Actions, Intentions
</subsectionHeader>
<bodyText confidence="0.999193161290323">
As a prerequisite for making informed engagement de-
cisions, a system must be able to recognize various en-
gagement cues, and to reason about the engagement
actions and intentions of relevant agents in the scene. To
accomplish this, the sensing subcomponent of the pro-
posed engagement model tracks over time three related
engagement variables for each agent a and interaction i:
the engagement state ESai (t) , the engagement action
EAai (t) and the engagement intention EIai (t).
The engagement state, ESai (t), captures whether an
agent a is engaged in interaction i and is modeled as a
deterministic variable with two possible values: en-
gaged and not-engaged. The state is updated based on
the joint actions of the agent and the system (see Figures
3 and 4). Since engagement is a collaborative process,
the transitions to the engaged state require that both the
agent and the system take either an engage action (if the
agent was previously not engaged) or a maintain action
(if the agent was already engaged); we discuss these
actions in more detail shortly. On the other hand, disen-
gagement can be a unilateral act: an agent transitions to
the not-engaged state if either the agent or the system
take a disengage action or a no-action.
The second engagement variable, EAai (t), models the
actions that an agent takes to initiate, maintain or termi-
nate engagement. There are four engagement actions:
engage, no-action, maintain, disengage. The first two
are possible only from the not-engaged state, while the
last two are possible only from the engaged state. The
engagement actions are estimated based on a condition-
al probabilistic model of the form:
</bodyText>
<equation confidence="0.753502">
P (EAai (t)JESai (t), EAai (t − 1 ), SEAai (t− 1 ), T(t))
</equation>
<bodyText confidence="0.999676909090909">
The inference is conditioned on the current engage-
ment state, on the previous agent and system actions,
and on additional sensory evidence T(t). T (t) includes
the detection of explicit engagement cues such as: salu-
tations (e.g. “Hi!”, “Bye bye”); calling behaviors (e.g.
“Laura!”); the establishment or the breaking of an F-
formation (Kendon, 1990b), i.e. the agent approaches
and positions himself in front of the system and attends
to the system; an expected, opening dialog move (e.g.
“Come here!”). Note that each of these cues is explicit,
and marks a committed engagement action.
</bodyText>
<figureCaption confidence="0.8451265">
Figure 2. Engagement state transition diagram. EA is the
agent’s engagement action; SEA is the system’s action.
</figureCaption>
<bodyText confidence="0.999919708333333">
A third variable in the proposed model, EIai (t) ,
tracks the engagement intention of an agent with respect
to a conversation. Like the engagement state, the inten-
tion can either be engaged or not-engaged. Intentions
are tracked separately from actions since an agent might
intend to engage or disengage the system, but not yet
take an explicit engagement action. For instance, let us
consider the case in which the system is already en-
gaged in an interaction and another agent is waiting in
line to interact with the system. Although the waiting
agent does not take an explicit, committed engagement
action, she might still intend to engage in a new conver-
sation with the system once the opportunity arises. She
might also signal this engagement intention via various
cues (e.g. pacing around, glances that make brief but
clear eye contact with the system, etc.) More generally,
the engagement intention variable captures whether or
not an agent would respond positively should the system
initiate engagement. In that sense, it roughly corres-
ponds to Peters’ (2005; 2005b) “interest level”, i.e. to
the value the agent attaches to being engaged in a con-
versation with the system.
Like engagement actions, engagement intentions are
inferred based on a direct conditional model:
</bodyText>
<figureCaption confidence="0.99446">
Figure 3. Graphical model showing key variables and
dependencies in managing engagement.
</figureCaption>
<figure confidence="0.99104488">
EA=no-action  |EA=engage &amp; EA=maintain &amp;
SEA=no-action SEA=engage SEA=maintain
not-engaged engaged
EA=disengage |
SEA=disengage
additional
context
engagement
sensing
G
A
ES
EA
T
EI
r r
SEA
SEB
t t+1
ES
EA
G
A
T
EI
</figure>
<page confidence="0.943816">
227
</page>
<bodyText confidence="0.991253133333333">
𝑃(𝐸𝐼𝑎𝑖 (𝑡)J𝐸𝑆𝑎𝑖 𝑡 , 𝐸𝐴𝑎𝑖 𝑡 , 𝑆𝐸𝐴𝑎𝑖 𝑡 − 1 , 𝐸𝐼𝑎𝑖 t − 1 , Ψ(𝑡))
This model leverages information about the current
engagement state, the previous agent and system ac-
tions, the previous engagement intention, as well as ad-
ditional evidence Ψ(𝑡) capturing implicit engagement
cues. Such cues include the spatiotemporal trajectory of
the participant and the level of sustained mutual atten-
tion. The models for inferring engagement actions and
intentions are generally independent of the application.
They capture the typical behaviors and cues by which
people signal engagement, and, as such, should be reus-
able across different domains. In other work (Bohus and
Horvitz, 2009b), we describe these models in more de-
tail and show how they can be learned automatically
from interaction data.
</bodyText>
<subsectionHeader confidence="0.937811">
4.2 Engagement Control Policy
</subsectionHeader>
<bodyText confidence="0.993839842105264">
Based on the inferred state, actions and intentions of the
agents in the scene, as well as other additional evidence
to be discussed shortly, the proposed model outputs
high-level engagement actions, denoted by SEA deci-
sion node in Figure 3. The action-space on the system
side contains the same four actions previously dis-
cussed: engage, disengage, maintain and no-action.
Each action is parameterized with a set of agents {𝑎𝑘}
and an interaction 𝑖. Additional parameters that control
the lower level execution of these actions, such as spe-
cific greetings, waiting times, urgency, etc. may also be
specified. The actual execution mechanisms are dis-
cussed in more detail in the following subsection.
In making engagement decisions in an open-world
setting, a conversational system must balance the goals
and needs of multiple agents in the scene and resolve
various tradeoffs (for instance between continuing the
current interaction or interrupting it temporarily to ad-
dress another agent), all the while observing rules of
social etiquette in interaction. Apart from the detected
engagement state, actions and intentions of an agent
𝑬𝑎𝑖 = 𝐸𝑆𝑎𝑖, 𝐸𝐴𝑎𝑖, 𝐸𝐼𝑎𝑖 , the control policy can be en-
hanced through leveraging additional observational evi-
dence, including high-level information 𝑯𝑎 about the
various agents in the scene, such as their long-term
goals and activities, as well as other global context (𝚪),
including the multiple tasks at hand, the history of the
interactions, relationships between various agents in the
scene (e.g. which agents are in a group together), etc.
For instance, a system might decide to temporarily
refuse engagement even though an agent takes an en-
gage action, because it is currently involved in a higher
priority interaction. Or, a system might try to take the
initiative and engage an agent based on the current con-
text (e.g. the system has a message to deliver) and activ-
ity of the agent (e.g. the agent is passing by), even
though the agent has no intention to engage.
Engagement control policies have therefore the form,
𝜋𝑆𝐸𝐴 ({𝑬𝑎𝑖 }𝑎 ,𝑖, 𝑯𝑎 𝑎, 𝚪)
where we have omitted the time index for simplicity. In
contrast to the models for inferring engagement inten-
tions and action, the engagement control policy can of-
ten be application specific. Such policies can be au-
thored manually to capture the desired system behavior.
We will discuss a concrete example of this in Section
5.2. In certain contexts, a more principled solution can
be developed by casting the control of engagement as an
optimization problem for scheduling collaborations with
multiple parties under uncertainties about the estimated
goals and needs, the duration of the interactions, time
and frustration costs, social etiquette, etc. We are cur-
rently exploring such models, where the system also
uses information-gathering actions (e.g. “Are the two of
you together?” “Are you here for X?,” etc.), based on
value-of-information computations to optimize in the
nature and flow of attention and collaboration in multi-
party interactions.
</bodyText>
<subsectionHeader confidence="0.996182">
4.3 Behavioral Control Policy
</subsectionHeader>
<bodyText confidence="0.985235484848485">
At the lower level, the engagement decisions taken by
the system have to be executed and rendered in an ap-
propriate manner. With the use of a rendered or physical
embodied agent, these actions are translated into a set of
coordinated lower-level behaviors, such as head ges-
tures, making and breaking eye contact, facial expres-
sions, salutations, interjections, etc. The coordination of
these behaviors is governed by a behavioral control pol-
icy, conditioned on the estimated engagement state,
actions and intentions of the considered agents, as well
as other information extracted from the scene:
𝜋𝑆𝐸𝐵 (𝑆𝐸𝐴, {𝑬𝑎𝑖 }𝑎,𝑖, Ψ)
For example, in the current implementation, the en-
gage system action subsumes three sub-behaviors per-
formed in a sequence: EstablishAttention, Greeting, and
Monitor. First, the system attempts to establish sus-
tained mutual attention with the agent(s) to be engaged.
This is accomplished by directing the gaze towards the
agents, and if the agent’s focus of attention is not on the
system, triggering an interjection like “Excuse me!”
Once mutual attention is established, on optional Greet-
ing behavior is performed; a greeting can be specified as
an execution parameter of the engage action. Finally,
the system enters a Monitor behavior, in which it moni-
tors for the completion of engagement. The action com-
pletes successfully once the agent(s) are in an engaged
state. Alternatively if a certain period of time elapses
and the agent(s) have not yet transitioned to the engaged
state, the engage system action completes with failure
(which is signaled to the engagement control layer).
Like the high-level engagement control policies, the
behavioral control policies can either be authored ma-
nually, or learned from data, either in a supervised (e.g.
</bodyText>
<page confidence="0.990301">
228
</page>
<bodyText confidence="0.9999536">
from a human-human interaction corpus) or unsuper-
vised learning setting. Also, like the engagement sens-
ing component, the behavioral control component is
decoupled from the task at hand, and should be largely
reusable across multiple application domains.
</bodyText>
<sectionHeader confidence="0.988009" genericHeader="method">
5 Observational Study
</sectionHeader>
<bodyText confidence="0.999986333333333">
As an initial step towards evaluating the proposed si-
tuated multiparty engagement models, we conducted a
preliminary observational study with a spoken dialog
system that implements these models. The goals of this
study were (1) to investigate whether a system can use
the proposed engagement models to effectively create
and conduct multiparty interactions in an open-world
setting, (2) to study user behavior and responses in this
setting, and (3) to identify some of the key technical
challenges in supporting multiparty engagement and
dialog in open-world context. In this section, we de-
scribe this study and report on the lessons learned.
</bodyText>
<subsectionHeader confidence="0.972801">
5.1 Experimental platform
</subsectionHeader>
<bodyText confidence="0.999974275862069">
Studying multiparty engagement and more generally
open-world interaction poses significant challenges.
Controlled laboratory studies are by their very nature
closed-world. Furthermore, providing participants with
instructions, such as “Go interact with this system”, or
“Go join the existing interaction” can significantly
prime and alter the engagement behaviors they would
otherwise display upon encountering the system in an
unconstrained setting. This can in turn cast serious
doubts on the validity of the results. Open-world inte-
raction is best observed in the open-world.
To provide an ecologically valid basis for studying
situated, multiparty engagement we therefore developed
a conversational agent that implements the proposed
model, and deployed it in the real-world. The system,
illustrated in Figure 4, takes the form of an interactive
multi-modal kiosk that displays a realistically rendered
avatar head which can interact via natural language. The
avatar can engage with one or more participants and
plays a simple game, in which the users have to respond
to multiple-choice trivia questions.
The system’s hardware and software architecture is
illustrated in Figure 4. Data gathered from a wide-angle
camera, a 4-element linear microphone array, and a 19”
touch-screen is forwarded to a scene analysis module
that fuses the incoming streams and constructs in real-
time a coherent picture of the dynamics in the surround-
ing environment. The system detects and tracks the lo-
cation of multiple agents in the scene, tracks the head
pose for engaged agents, tracks the current speaker, and
infers the focus of attention, activities, and goals of each
agent, as well as the group relationships among different
agents. An in-depth description of the hardware and
scene analysis components falls beyond the scope of
this paper, but details are available in (Bohus and Hor-
vitz, 2009). The scene analysis results are forwarded to
the control level, which is structured in a two-layer reac-
tive-deliberative architecture. The reactive layer imple-
ments and coordinates various low-level behaviors, in-
cluding engagement, conversational floor management
and turn-taking, and coordinating spoken and gestural
outputs. The deliberative layer plans the system’s dialog
moves and high-level engagement actions.
Overall, the game task was purposefully designed to
minimize challenges in terms of speech recognition or
dialog management, and allow us to focus our attention
on the engagement processes. The avatar begins the
interactions by asking the engaged user if they would
like to play a trivia game. If the user agrees, the avatar
goes through four multiple-choice questions, one at a
time. After each question, the possible answers are dis-
played on the screen (Figure 4) and users can respond
by either speaking an answer or by touching it. When
the answer provided by the user is incorrect, the system
provides a short explanation regarding the correct an-
swer before moving on to the next question.
The system also supports multi-participant interac-
tions. The engagement policy used to attract and engage
</bodyText>
<figure confidence="0.97088325">
wide-angle camera
4-element linear microphone array
touch screen
speakers
Synthesis
Output Planning
Behavioral Control
Dialog Management
Avatar
Speech
Vision
Scene Analysis
</figure>
<figureCaption confidence="0.99958">
Figure 4. Trivia game dialog system: prototype, architectural overview, and runtime scene analysis
</figureCaption>
<page confidence="0.990293">
229
</page>
<bodyText confidence="0.9999093125">
multiple users in a game is the focus of this observa-
tional study, and is discussed in more detail in the next
subsection. Once the system is engaged with multiple
users, it uses a multi-participant turn taking model
which allows it to continuously track who the current
speaker is, and who has the conversational floor (Bohus
and Horvitz, 2009). At the behavioral level, the avatar
orients its head pose and gaze towards the current
speaker, or towards the addressee(s) of its own utter-
ances. During multiplayer games, the avatar alternates
between the users when asking questions. Also, after a
response is received from one of the users, the avatar
confirms the answer with the other user(s), e.g. “Do you
agree with that?” A full sample interaction with the sys-
tem is described in Appendix A, and the corresponding
video is available online (Situated Interaction, 2009).
</bodyText>
<subsectionHeader confidence="0.995457">
5.2 Multiparty Engagement Policy
</subsectionHeader>
<bodyText confidence="0.999974052631579">
The trivia game system implements the situated, multi-
party engagement model described in Section 4. The
sensing and behavioral control components are applica-
tion independent and were previously described. We
now describe the system’s engagement policy, which is
application specific.
As previously discussed, apart from using the in-
ferred engagement state, actions and intentions for the
agents in the scene, the proposed model also uses in-
formation about the high-level goals and activities of
these agents when making engagement decisions. Spe-
cifically, the system tracks the goal of each agent in the
scene, which can be play, watch, or other, and their cur-
rent activity, which can be passing-by, interacting, play-
ing, watching, or departing. The goal and activity rec-
ognition models are application specific, and in this case
are inferred based on probabilistic conditional models
that leverage information about the spatiotemporal tra-
jectory of each agent and their spoken utterances, as
well as global scene information (e.g. is the system en-
gaged in an active interaction, etc.).
Initially, when the system is idle, it uses a conserva-
tive engagement policy and waits for the user to initiate
engagement via an explicit action. Such actions include
the user approaching and entering in an F-formation
(Kendon, 1990b) with the system, i.e. standing right in
front of it, swiping their badge, or pushing the start but-
ton (in the idle state the GUI displays “swipe your
badge or press here to begin” below the avatar head).
While engaged in an interaction, the system attempts
to engage bystanders in an effort to create a collabora-
tive, multi-participant game. In this case, the engage-
ment policy is conditioned on the inferred activities of
the agents in the scene. Specifically, if a watching bys-
tander is detected, the system temporarily disengages
the current participant, and engages and attempts to
“convince” the watching bystander to join the existing
game. The prompts in this side interaction depend on
</bodyText>
<table confidence="0.46177235483871">
If a watching bystander B is detected during the first 3 questions
and the engaged participant E has not made any mistakes
A [S to E]: Hold on a second.
[S to B]: Hi. Excuse me, would you like to join in?
[B]: Okay.
[S to E&amp;B]: Great. So let’s see how well you can do together.
Next question [...]
and the engaged participant E has just made a mistake, and
the system did not previously engage the bystander B.
B [S to E]: Perhaps you can use some help. Hold on a
second.
[S to B]: Hi. Do you think you could help out?
[B]: No thanks
[S to B]: Okay. Well, I guess you can just watch for now
and if you wait for a bit we’ll be done and you can
start a new game yourself
[S to E]: Guess you’re still on your own. Next question [...]
and the engaged participant E has just made a mistake, and
the system did previously attempt to engage the bystander B.
C [S to E]: I think you could really use some help. Hold on a
second.
[S to B]: Are you sure you don’t want to help out? Come
on, this is fun.
[B]: Sure
[S to E&amp;B]: Great. So let’s see how well you can do together.
Next question [...]
If a watching bystander B is detected during the last question
D [S to E]: Excuse me for one moment.
[S to B]: We’re almost done here. If you wait for a bit we can start
a new game right after
[S to E]: Sorry about that [...]
</table>
<tableCaption confidence="0.998646">
Table 1. Multiparty engagement policy
</tableCaption>
<bodyText confidence="0.999948666666667">
the current game context, as shown in Table 1. If the
watching bystander agrees to join in, the system adds
him to the existing interaction, and continues a multi-
participant game (see Table 1.A.) Conversely, if the
bystander refuses, the system re-engages the previous
participant and resumes the single-user game (see Table
1.B.) Additional examples are available in Appendix A.
Finally, if the system is already engaged and a watch-
ing bystander is detected but only during the last ques-
tion, the system engages them temporarily to let them
know that the current game will end shortly and, if they
wait, they can also start a new game (see Table 1.D).
</bodyText>
<subsectionHeader confidence="0.947836">
5.3 Results and Lessons Learned
</subsectionHeader>
<bodyText confidence="0.999969571428571">
We deployed the system described above for 20 days
near one of the kitchenettes in our building. The system
attracted attention of passer-bys with the tracking mo-
tion of its virtual face that followed people as they
passed by. Most people that interacted with the system
did so for the first time; only a small number of people
interacted several times. No instructions were provided
for interacting with the system. We shall now review
results from analysis of the collected data.
Throughout the 20 days of deployment, the system
engaged in a total of 121 interactive events. Of these, in
54 cases (44%), a participant engaged the system but
did not play the game. Typically, the participant would
approach and enter in an F-formation with the system,
</bodyText>
<page confidence="0.983419">
230
</page>
<figure confidence="0.334969333333333">
Excuse me for one second ... Hi, would you like to join in? Perhaps you can use some [after non-understanding] We’re almost done here. If you
[1 2 cases] help... Do you think you could Sorry, did you want to join wait for a bit we can start a new
help out? [6 cases] in? [5 cases] game right after [6 cases]
</figure>
<figureCaption confidence="0.994182">
Figure 5. System multiparty engagement actions and responses from bystanders and already engaged participants.
</figureCaption>
<figure confidence="0.974658741935484">
For bystander responses, denotes a positive response; denotes a negative response; denotes no response. For responses
Y N
from previously engaged participant, denotes utterances addressed to the bystander, denotes side comments, denotes
B C S
responses directed to the system
B
B
C
S
S
S
S
B
S
S
B
response from
previously engaged
participant
C C C C
response from
solicited bystander
Y Y Y Y N N N N N N N
Y Y Y N N
N
N
N
T
Y
system
prompt
</figure>
<bodyText confidence="0.998738034090909">
but, once the system engaged and asked if they would
like to play the trivia game, they responded negatively
or left without responding. In 49 cases (40%), a single
participant engaged and played the game, but no bys-
tanders were observed during these interactions. In one
case, two participants approached and engaged simulta-
neously; the system played a multi-participant game, but
no other bystanders were observed. Finally, in the re-
maining 17 cases (14% of all engagements, 25% of ac-
tual interactions), at least one bystander was observed
and the system engaged in multiparty interaction. These
multiparty interactions are the focus of our observation-
al analysis, and we will discuss them in more detail.
In 2 of these 17 cases, bystanders appeared only late
in the interaction, after the system had already asked the
last question. In these cases, according to its engage-
ment policy, the system notified the bystander that they
would be attended to momentarily (see Table 1.D), and
then proceeded to finish the initial game. In 8 of the
remaining 15 cases (53%), the system successfully per-
suaded bystanders to join the current interaction and
carried on a multi-participant game. In the remaining 7
cases (47%), bystanders turned down the offer to join
the existing game. Although this corpus is still relatively
small, these statistics indicate that the system can suc-
cessfully engage bystanders and create and manage
multi-participant interactions in the open world.
Next, we analyzed more closely the responses and
reactions from bystanders and already engaged partici-
pants to the system’s multiparty engagement actions.
Throughout the 17 multiparty interactions, the system
planned and executed a total of 23 engagement actions
soliciting a bystander to enter the game, and 6 engage-
ment actions letting a bystander know that they will be
engaged momentarily. The system actions and res-
ponses from bystanders and engaged participants are
visually summarized in Figure 5, and are presented in
full in Appendix B. Overall, bystanders successfully
recognize that they are being engaged and solicited by
the system and respond (either positively or negatively)
in the large majority of cases (20 out of 23). In 2 of the
remaining 3 cases, the previously engaged participant
responded instead of the bystander; finally, in one case
the bystander did not respond and left the area.
While bystanders generally respond when engaged
by the system, the system’s engagement actions towards
bystanders also frequently elicits spoken responses from
the already engaged participants; this happened in 14
out of 23 cases (61%). The responses are sometimes
addressed to the system e.g. “Yes he does,” or towards
the bystander, e.g. “Say yes!”, or they reflect general
comments, e.g. “That’s crazy!” These results show that,
when creating the side interaction to solicit a bystander
to join the game, the system should engage both the
bystander and the existing user in this side interaction,
or at least allow the previous user to join this side inte-
raction (currently the system engages only the bystander
in this interaction; see example from Appendix A.)
Furthermore, we noticed that, in several cases, bys-
tanders provided responses to the system’s questions
even prior to the point the system engaged them in inte-
raction (sometimes directed toward the system, some-
times toward the engaged participant.) We employed a
system-initiative engagement policy towards bystanders
in the current experiment. The initiative being taken by
participants highlights the potential value of implement-
ing a mixed-initiative policy for engagement. If a rele-
vant response is detected from a bystander, this can be
interpreted as an engagement action (recall from subsec-
tion 4.1 that engagement actions subsume expected
opening dialog moves), and a mixed-initiative policy
can respond by engaging the bystander, e.g. “Did you
want to join in?” or “Please hang on, let’s let him finish.
We can play a new game right after that.” This policy
could be easily implemented under the proposed model.
We also noted side comments by both bystander and
the existing participant around the time of multiparty
engagement. These remarks typically indicate surprise
and excitement at the system’s multiparty capabilities.
Quotes include: “That’s awesome!”, “Isn’t that great!”,
“That’s funny!”, “Dude!”, “Oh my god that’s creepy!”,
“That’s cool!”, “It multitasks!”, “That is amazing!”,
“That’s pretty funny”, plus an abundance of laughter
and smiles. Although such surprise might be expected
today with a first-time exposure to an interactive system
that is aware of and can engage with multiple parties,
we believe that expectations will change in the future, as
these technologies become more commonplace.
</bodyText>
<page confidence="0.993584">
231
</page>
<bodyText confidence="0.999959666666667">
Overall, this preliminary study confirmed that the
system can effectively initiate engagement in multiparty
settings, and also highlighted several core challenges for
managing engagement and supporting multiparty inte-
ractions in the open world. A first important challenge
we have identified is developing robust models for
tracking the conversational dynamics in multiparty situ-
ations, i.e. identifying who is talking to whom at any
given point. Secondly, the experiment has highlighted
the opportunity for using more flexible, mixed-initiative
engagement policies. Such policies will rely heavily on
the ability to recognize engagement intentions; in (Bo-
hus and Horvitz, 2009b), we describe the automated
learning of engagement intentions from interaction data.
Finally, another lesson we learned from these initial
experiments is the importance of accurate face tracking
for supporting multiparty interaction. Out of the 17 mul-
tiparty interactions, 7 were affected by vision problems
(e.g. the system momentarily lost a face, or swapped the
identity of two faces); 4 of these were fatal errors that
eventually led to interaction breakdowns.
</bodyText>
<sectionHeader confidence="0.984152" genericHeader="conclusions">
6 Summary and Future Work
</sectionHeader>
<bodyText confidence="0.9999955">
We have described a computational model for managing
engagement decisions in open-world dialog. The model
harnesses components for sensing and reasoning about
the engagement state, actions, and intentions of multiple
participants in the scene, for making high-level en-
gagement control decisions about who and when to en-
gage, and for executing and rendering these actions in
an embodied agent. We reviewed an observational study
that showed that, when weaved together, these compo-
nents can provide support for effectively managing en-
gagement, and for creating and conducting multiparty
interactions in an open-world context.
We believe that the components and policies we have
presented provide a skeleton for engagement and inte-
raction in open-world settings. However, there are im-
portant challenges and opportunities ahead. Future re-
search includes developing methods for fine tuning and
optimizing each of these subcomponents and their inte-
ractions. Along these lines, there are opportunities to
employ machine learning to tune and adapt multiple
aspects of the operation of the system. In (Bohus and
Horvitz, 2009b) we introduce and evaluate an approach
to learning models for inferring engagement actions and
intentions online, through interaction. On another direc-
tion, we are investigating the use of decision-theoretic
approaches for optimizing mixed-initiative engagement
policies by taking into account the underlying uncertain-
ties, the costs and benefits of interruption versus contin-
uing collaboration, queue etiquette associated with ex-
pectations of fairness, etc. Another difficult challenge is
the creation of accurate low-level behavioral models,
including the fine-grained control of pose, gesture, and
facial expressions. Developing such methods will likely
have subtle, yet powerful influences on the effectiveness
of signaling and overall grounding in multiparty set-
tings. We believe that research on these and other prob-
lems of open-world dialog will provide essential and
necessary steps towards developing computational sys-
tems that can embed interaction deeply into the natural
flow of everyday tasks, activities, and collaborations.
</bodyText>
<sectionHeader confidence="0.983959" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999056666666667">
We thank George Chrysanthakopoulos, Zicheng Liu,
Tim Paek, Cha Zhang, and Qiang Wang for discussions
and feedback in the development of this work.
</bodyText>
<sectionHeader confidence="0.991077" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.95786956097561">
M. Argyle and M. Cook, 1976, Gaze and Mutual Gaze, Cam-
bridge University Press, New York
D. Bohus and E. Horvitz, 2009a, Open-World Dialog: Chal-
lenges, Directions and Prototype, to appear in KRPD’09,
Pasadena, CA
D. Bohus and E. Horvitz, 2009b, An Implicit-Learning Based
Model for Detecting Engagement Intentions, submitted to
SIGdial’09, London, UK
E. Goffman, 1963, Behaviour in public places: notes on the
social order of gatherings, The Free Press, New York
E.T. Hall, 1966, The Hidden Dimension: man’s use of space in
public and private, New York: Doubleday.
A. Kendon, 1990, A description of some human greetings,
Conducting Interaction: Patterns of behavior in focused en-
counters, Studies in International Sociolinguistics, Cam-
bridge University Press
A. Kendon, 1990b, Spatial organization in social encounters:
the F-formation system, Conducting Interaction: Patterns of
behavior in focused encounters, Studies in International
Sociolinguistics, Cambridge University Press
M.P. Michalowski, S. Sabanovic, and R. Simmons, A spatial
model of engagement for a social robot, in 9th IEEE Work-
shop on Advanced Motion Control, pp. 762-767
C. Peters, C. Pelachaud, E. Bevacqua, and M. Mancini, 2005,
A model of attention and interest using gaze behavior, Lec-
ture Notes in Computer Science, pp. 229-240.
C. Peters, 2005b, Direction of Attention Perception for Con-
versation Initiation in Virtual Environments, in Intelligent
Virtual Agents, 2005, pp. 215-228.
C.L. Sidner, C.D. Kidd, C. Lee, and N. Lesh, 2004, Where to
Look: A Study of Human-Robot Engagement, IUI’2004, pp.
78-84, Madeira, Portugal
C.L. Sidner, C. Lee, C.D. Kidd, N. Lesh, and C. Rich, 2005,
Explorations in engagement for humans and robots, Artifi-
cial Intelligence, 166 (1-2), pp. 140-164
Situated Interaction, 2009, Project page:
http://research.microsoft.com/~dbohus/research_situated_i
nteraction.html
R. Vertegaal, R. Slagter, G.C.v.d.Veer, and A. Nijholt, 2001,
Eye gaze patterns in conversations: there is more to con-
versational agents than meets the eyes, CHI’01
</reference>
<page confidence="0.973305">
232
</page>
<figure confidence="0.967612043103448">
Appendix A. Sample multiparty interaction with trivia game dialog system (not part of the experiment)
Engage((1),i1)
Disengage((1),i1)
Engage((2),i2)
Maintain((2),i2)
Diseng ((2),i2)
Engage((1),i1)
Maintain((1),i1)
Active
Disengage((1),i1)
Engage((2),i3)
Maintain((2),i3)
Diseng((2),i3)
Engage((1,2),i4)
Maintain((1,2),i4)
Disengage((1,2),i4)
Maintain((1),i1)
Active
Active
Active
Suspended
Suspended
Active
t0
[approaching]
[engaging]
Hi!
t1
1:
S:
t2
1
t3
t4
t5
t6
t7
t8
t10
t11
t12
t13
t14
S:
2:
S:
2:
S:
2:
S:
2:
S:
S:
S:
1:
S:
1:
S:
S:
S:
S:
S:
S:
S:
S:
Would you like to play a trivia game? If you answer three
out of four questions correctly I can enter you in a raffle
for a Zune player.
Sure.
Okay. First, could you please swipe your badge below
so I can email you the raffle ticket?
[swipes badge; in the meantime, bystander 2 appears]
Got it! Here is the first question.
The state with the most shark attacks each year is
California, Florida or Hawaii?
Florida.
Florida. Is that final?
Yup
That’s right
[disengaging]
Excuse me for one second.
[engaging]
Hi
Would you like to join in?
Uh. No thanks.
Sorry, did you want to join in?
No.
Well, you can just watch for now. And if you wait for a
bit, we’ll be done and then you can start a new game
yourself
[disengaging]
[engaging]
Sorry about that.
Next question.Which mammal lives the longest?
The human
You think it’s human, right?
Uh no, no no.
So what’s the correct answer?
Elephant.
Elephant. Is that final?
Yup.
Sorry but that’s actually incorrect. Current scientific
thought is that the longest possible human life span is
120 years, making humans the longest lived among the
mammals. Perhaps you could use some help. Hold on
one more second.
[disengaging]
[engaging]
Are you sure you don’t want to help out? Come on, this
is fun.
Okay.
Sorry, so did you wanna join in?
Yes.
Great
[disengaging]
[re-engaging with both 1 and 2]
</figure>
<bodyText confidence="0.735687166666667">
So let’s see if you can do better together. So far you
have one correct answer. Now on to the next question.
Is it true that Black Friday, the Friday after Thanksgiving
is the busiest shopping day in the year?
I heard that it wasn’t true actually... false.
Do you also think that’s false?
I guess.
You got it! Okay, so far you have 2 out of 3 correct
answers. If you get this last question right you can enter
the raffle to win a Zune. How many people are killed by
vending machines at the beach in a typical year?
It’s probably more than one... I’ll say thirteen.
Do you think that’s correct?
No, thirteen.
Is that final?
Yes.
Congratulations, you had 3 correct answers out of 4
questions. I will record an entry for you in the Zune
</bodyText>
<figure confidence="0.985066916666666">
raffle.
Great, thanks
first person engages -
around time t2
system
agent
bystander appears – prior to t3
system engages bystander — t5
participants play together — t14
active conversa-
tion
suspended conver-
sation
other conversation
S:
1:
S:
1:
S:
1:
S:
1:
S:
t9
S:
S:
S:
S:
2:
S:
t15 1:
S:
1:
S:
1:
S:
1
1
2
1
2
2
1
1
2
2
1
Bye-bye
</figure>
<page confidence="0.711839">
233
</page>
<table confidence="0.86324214">
Appendix B. User responses to multiparty engagement actions.
S denotes the system, E denotes the already engaged participant, B denotes a watching bystander.
Actions and Resposes Response from B Response from E Timing
[S to E]: Hold on one second. Yes B only
[S to B]: Excuse me, would you like to join in?
4 positive answers from B
7 negative answers from B
1 no answer from B (E answers)
Yes Say yes Overlap
Sure B only
Yes [to B]: Would you like to join E first
in?
No That’s crazy! B first
Oh, no. No + [moves away] That’s funny! B first
No thank you B only
No No? B first
Woah, no. That’s cool! B first
No + [moves away] + That’s [laughs looking at B] B first
pretty funny.
[laughs] [laughs] Yes. Oh yes. E only
[S to E]: Perhaps you could use some help. Yes. Yes. B first
Excuse me for one second.
[S to B]: Hi, do you think you could help out?
3 positive answers from B
2 negative answers from B
1 no-answer from B (moves away)
Yes Yes he does. Overlap
[laughs] + No. B only
[to E]: Isn’t that weird? [to B]: That is amazing! B first
[to S]: No.
[to E]: Isn’t that great?
[laughs] + [moves out] Quit E only
[laughs] + Sure Sure B first
If the initial response from B was not unders- No. Yes, I don’t know, help me! B first
tood by the system, system asks one more time Please.
[S to B]: Sorry, did you want to join in?
1 positive answer from B
3 negative answer from B
1 no-answer from B (E answers)
No. B only
No. B only
No. B only
No. E only
[S to B]: We’re almost done here. If you wait Great, thanks. B only
for a bit we can start a new game
right after.
1 answer from B
1 answer from E
4 no-answer from either B or E
That’s awesome E only
</table>
<page confidence="0.992291">
234
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.563833">
<title confidence="0.999973">Models for Multiparty Engagement in Open-World Dialog</title>
<author confidence="0.988935">Dan</author>
<affiliation confidence="0.97484">Microsoft</affiliation>
<address confidence="0.892865">One Microsoft Redmond, WA, 98052</address>
<email confidence="0.999623">dbohus@microsoft.com</email>
<author confidence="0.968987">Eric</author>
<affiliation confidence="0.962722">Microsoft</affiliation>
<address confidence="0.8917005">One Microsoft Redmond, WA, 98052</address>
<email confidence="0.999104">horvitz@microsoft.com</email>
<abstract confidence="0.999900882352941">We present computational models that allow spoken dialog systems to handle multiparticipant engagement in open, dynamic environments, where multiple people may enter and leave conversations, and interact with the system and with others in a natural manner. The models for managing the engagement process include components for (1) sensing the engagement state, actions and intentions of multiple agents in the scene, (2) making engagedecisions to engage with, and when) and (3) rendering these decisions in a set of coordinated low-level behaviors in an embodied conversational agent. We review results from a study of interactions &amp;quot;in the wild&amp;quot; with a system that implements such a model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Argyle</author>
<author>M Cook</author>
</authors>
<title>Gaze and Mutual Gaze,</title>
<date>1976</date>
<publisher>Cambridge University Press,</publisher>
<location>New York</location>
<contexts>
<context position="6824" citStr="Argyle and Cook, 1976" startWordPosition="1020" endWordPosition="1023">age and re-engage one or multiple people, and support natural interactions in an open-world context. 3 Related Work The process of engagement between people, and between people and computational systems has received a fair amount of attention. Observational studies in the sociolinguistics and conversational analysis communities have revealed that engagement is a complex, mixedinitiative, highly-coordinated process that often involves a variety of non-verbal cues and signals, (Goffman, 1963; Kendon, 1990), spatial trajectory and proximity (Hall, 1966; Kendon, 1990b), gaze and mutual attention (Argyle and Cook, 1976), head and hand gestures (Kendon, 1990), as well as verbal greetings. A number of researchers have also investigated issues of engagement in human-computer and humanrobot interaction contexts. Sidner and colleagues (2004) define engagement as “the process by which two (or more) participants establish, maintain and end their perceived connection during interactions they jointly undertake”, and focus on the process of maintaining engagement. They show in a user study (Sidner et al., 2004; 2005) that people directed their attention to a robot more often when the robot made engagement gestures thr</context>
</contexts>
<marker>Argyle, Cook, 1976</marker>
<rawString>M. Argyle and M. Cook, 1976, Gaze and Mutual Gaze, Cambridge University Press, New York</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bohus</author>
<author>E Horvitz</author>
</authors>
<title>Open-World Dialog: Challenges, Directions and Prototype, to appear in KRPD’09,</title>
<date>2009</date>
<location>Pasadena, CA</location>
<contexts>
<context position="1565" citStr="Bohus and Horvitz, 2009" startWordPosition="233" endWordPosition="236">odel. 1 Introduction To date, nearly all spoken dialog systems research has focused on the challenge of engaging single users on tasks defined within a relatively narrow context. Efforts in this realm have led to significant progress including large-scale deployments that now make spoken dialog systems common features in the daily lives of millions of people. However, research on dialog systems has largely overlooked important challenges with the initiation, maintenance, and suspension of conversations that are common in the course of natural communication and collaborations among people. In (Bohus and Horvitz, 2009) we outlined a set of core challenges for extending traditional closed-world dialog systems to systems that have competency in open-world dialog. The work described here is part of a larger research effort aimed at addressing these challenges, and constructing computational models to support the core interaction skills required for open-world dialog. In particular, we focus our attention in this paper on the challenges of managing engagement – “the process by which two (or more) participants establish, maintain and end their perceived connection during interactions they jointly undertake”, cf.</context>
<context position="3905" citStr="Bohus and Horvitz, 2009" startWordPosition="589" endWordPosition="592"> safe to assume that a user is engaged with a dialog system once a call has been received. Similarly, pushto-talk buttons are often used in multimodal mobile applications. Although these solutions are sufficient and even natural in closed, single-user contexts, they become inappropriate for open-world systems that must operate continuously in open, dynamic environments, such as robots, interactive billboards, or embodied conversational agents. Interaction in the open-world is characterized by two aspects that capture key departures from assumptions traditionally made in spoken dialog systems (Bohus and Horvitz, 2009). The first one is the dynamic, multiparty nature of the interaction, i.e., the world typically contains not just one, but multiple agents that are relevant Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 225–234, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 225 to the interactive system. Engagements in open worlds are often dynamic and asynchronous, i.e. relevant agents may enter and leave the observable world at any time, may interact with the system and with each other, a</context>
<context position="15190" citStr="Bohus and Horvitz, 2009" startWordPosition="2363" endWordPosition="2366"> t − 1 , Ψ(𝑡)) This model leverages information about the current engagement state, the previous agent and system actions, the previous engagement intention, as well as additional evidence Ψ(𝑡) capturing implicit engagement cues. Such cues include the spatiotemporal trajectory of the participant and the level of sustained mutual attention. The models for inferring engagement actions and intentions are generally independent of the application. They capture the typical behaviors and cues by which people signal engagement, and, as such, should be reusable across different domains. In other work (Bohus and Horvitz, 2009b), we describe these models in more detail and show how they can be learned automatically from interaction data. 4.2 Engagement Control Policy Based on the inferred state, actions and intentions of the agents in the scene, as well as other additional evidence to be discussed shortly, the proposed model outputs high-level engagement actions, denoted by SEA decision node in Figure 3. The action-space on the system side contains the same four actions previously discussed: engage, disengage, maintain and no-action. Each action is parameterized with a set of agents {𝑎𝑘} and an interaction 𝑖. Addit</context>
<context position="23015" citStr="Bohus and Horvitz, 2009" startWordPosition="3585" endWordPosition="3589"> a 19” touch-screen is forwarded to a scene analysis module that fuses the incoming streams and constructs in realtime a coherent picture of the dynamics in the surrounding environment. The system detects and tracks the location of multiple agents in the scene, tracks the head pose for engaged agents, tracks the current speaker, and infers the focus of attention, activities, and goals of each agent, as well as the group relationships among different agents. An in-depth description of the hardware and scene analysis components falls beyond the scope of this paper, but details are available in (Bohus and Horvitz, 2009). The scene analysis results are forwarded to the control level, which is structured in a two-layer reactive-deliberative architecture. The reactive layer implements and coordinates various low-level behaviors, including engagement, conversational floor management and turn-taking, and coordinating spoken and gestural outputs. The deliberative layer plans the system’s dialog moves and high-level engagement actions. Overall, the game task was purposefully designed to minimize challenges in terms of speech recognition or dialog management, and allow us to focus our attention on the engagement pro</context>
<context position="24855" citStr="Bohus and Horvitz, 2009" startWordPosition="3867" endWordPosition="3870">ge wide-angle camera 4-element linear microphone array touch screen speakers Synthesis Output Planning Behavioral Control Dialog Management Avatar Speech Vision Scene Analysis Figure 4. Trivia game dialog system: prototype, architectural overview, and runtime scene analysis 229 multiple users in a game is the focus of this observational study, and is discussed in more detail in the next subsection. Once the system is engaged with multiple users, it uses a multi-participant turn taking model which allows it to continuously track who the current speaker is, and who has the conversational floor (Bohus and Horvitz, 2009). At the behavioral level, the avatar orients its head pose and gaze towards the current speaker, or towards the addressee(s) of its own utterances. During multiplayer games, the avatar alternates between the users when asking questions. Also, after a response is received from one of the users, the avatar confirms the answer with the other user(s), e.g. “Do you agree with that?” A full sample interaction with the system is described in Appendix A, and the corresponding video is available online (Situated Interaction, 2009). 5.2 Multiparty Engagement Policy The trivia game system implements the</context>
<context position="36540" citStr="Bohus and Horvitz, 2009" startWordPosition="5804" endWordPosition="5808">tem can effectively initiate engagement in multiparty settings, and also highlighted several core challenges for managing engagement and supporting multiparty interactions in the open world. A first important challenge we have identified is developing robust models for tracking the conversational dynamics in multiparty situations, i.e. identifying who is talking to whom at any given point. Secondly, the experiment has highlighted the opportunity for using more flexible, mixed-initiative engagement policies. Such policies will rely heavily on the ability to recognize engagement intentions; in (Bohus and Horvitz, 2009b), we describe the automated learning of engagement intentions from interaction data. Finally, another lesson we learned from these initial experiments is the importance of accurate face tracking for supporting multiparty interaction. Out of the 17 multiparty interactions, 7 were affected by vision problems (e.g. the system momentarily lost a face, or swapped the identity of two faces); 4 of these were fatal errors that eventually led to interaction breakdowns. 6 Summary and Future Work We have described a computational model for managing engagement decisions in open-world dialog. The model h</context>
<context position="38149" citStr="Bohus and Horvitz, 2009" startWordPosition="6048" endWordPosition="6051">ide support for effectively managing engagement, and for creating and conducting multiparty interactions in an open-world context. We believe that the components and policies we have presented provide a skeleton for engagement and interaction in open-world settings. However, there are important challenges and opportunities ahead. Future research includes developing methods for fine tuning and optimizing each of these subcomponents and their interactions. Along these lines, there are opportunities to employ machine learning to tune and adapt multiple aspects of the operation of the system. In (Bohus and Horvitz, 2009b) we introduce and evaluate an approach to learning models for inferring engagement actions and intentions online, through interaction. On another direction, we are investigating the use of decision-theoretic approaches for optimizing mixed-initiative engagement policies by taking into account the underlying uncertainties, the costs and benefits of interruption versus continuing collaboration, queue etiquette associated with expectations of fairness, etc. Another difficult challenge is the creation of accurate low-level behavioral models, including the fine-grained control of pose, gesture, a</context>
</contexts>
<marker>Bohus, Horvitz, 2009</marker>
<rawString>D. Bohus and E. Horvitz, 2009a, Open-World Dialog: Challenges, Directions and Prototype, to appear in KRPD’09, Pasadena, CA</rawString>
</citation>
<citation valid="false">
<authors>
<author>D Bohus</author>
<author>E Horvitz</author>
</authors>
<title>2009b, An Implicit-Learning Based Model for Detecting Engagement Intentions, submitted to SIGdial’09,</title>
<location>London, UK</location>
<marker>Bohus, Horvitz, </marker>
<rawString>D. Bohus and E. Horvitz, 2009b, An Implicit-Learning Based Model for Detecting Engagement Intentions, submitted to SIGdial’09, London, UK</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Goffman</author>
</authors>
<title>Behaviour in public places: notes on the social order of gatherings,</title>
<date>1963</date>
<publisher>The Free Press,</publisher>
<location>New York</location>
<contexts>
<context position="6696" citStr="Goffman, 1963" startWordPosition="1004" endWordPosition="1005">rld dialog with a situated multiparty engagement model that can enable a computational system to fluidly engage, disengage and re-engage one or multiple people, and support natural interactions in an open-world context. 3 Related Work The process of engagement between people, and between people and computational systems has received a fair amount of attention. Observational studies in the sociolinguistics and conversational analysis communities have revealed that engagement is a complex, mixedinitiative, highly-coordinated process that often involves a variety of non-verbal cues and signals, (Goffman, 1963; Kendon, 1990), spatial trajectory and proximity (Hall, 1966; Kendon, 1990b), gaze and mutual attention (Argyle and Cook, 1976), head and hand gestures (Kendon, 1990), as well as verbal greetings. A number of researchers have also investigated issues of engagement in human-computer and humanrobot interaction contexts. Sidner and colleagues (2004) define engagement as “the process by which two (or more) participants establish, maintain and end their perceived connection during interactions they jointly undertake”, and focus on the process of maintaining engagement. They show in a user study (S</context>
</contexts>
<marker>Goffman, 1963</marker>
<rawString>E. Goffman, 1963, Behaviour in public places: notes on the social order of gatherings, The Free Press, New York</rawString>
</citation>
<citation valid="true">
<authors>
<author>E T Hall</author>
</authors>
<title>The Hidden Dimension: man’s use of space in public and private,</title>
<date>1966</date>
<publisher>Doubleday.</publisher>
<location>New York:</location>
<contexts>
<context position="6757" citStr="Hall, 1966" startWordPosition="1012" endWordPosition="1013">enable a computational system to fluidly engage, disengage and re-engage one or multiple people, and support natural interactions in an open-world context. 3 Related Work The process of engagement between people, and between people and computational systems has received a fair amount of attention. Observational studies in the sociolinguistics and conversational analysis communities have revealed that engagement is a complex, mixedinitiative, highly-coordinated process that often involves a variety of non-verbal cues and signals, (Goffman, 1963; Kendon, 1990), spatial trajectory and proximity (Hall, 1966; Kendon, 1990b), gaze and mutual attention (Argyle and Cook, 1976), head and hand gestures (Kendon, 1990), as well as verbal greetings. A number of researchers have also investigated issues of engagement in human-computer and humanrobot interaction contexts. Sidner and colleagues (2004) define engagement as “the process by which two (or more) participants establish, maintain and end their perceived connection during interactions they jointly undertake”, and focus on the process of maintaining engagement. They show in a user study (Sidner et al., 2004; 2005) that people directed their attentio</context>
<context position="8174" citStr="Hall, 1966" startWordPosition="1235" endWordPosition="1236">005; 2005b) uses an alternative definition of engagement as “the value that a participant in an interaction attributes to the goal of being together with the other participant(s) and of continuing the interaction,” and present the high-level schematics for an algorithm for establishing and maintaining engagement. The algorithm highlights the importance of mutual attention and eye gaze and relies on a heuristically computed “interest level” to decide when to start a conversation. Michalowski and colleagues (2006) propose and conduct experiments with a model of engagement grounded in proxemics (Hall, 1966) which classifies relevant agents in the scene in four different categories (present, attending, engaged and interacting) based on their distance to the robot. The robot’s behaviors are in turn conditioned on the four categories above. In our work, we follow Sidner’s definition of engagement as a process (Sidner et al., 2004) and describe a computational model for situated multiparty engagement. The proposed model draws on several ideas from the existing body of work, but moves beyond it and provides a more comprehensive framework for managing the engagement process in a dynamic, open-world co</context>
</contexts>
<marker>Hall, 1966</marker>
<rawString>E.T. Hall, 1966, The Hidden Dimension: man’s use of space in public and private, New York: Doubleday.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kendon</author>
</authors>
<title>A description of some human greetings, Conducting Interaction: Patterns of behavior in focused encounters,</title>
<date>1990</date>
<booktitle>Studies in International Sociolinguistics,</booktitle>
<publisher>Cambridge University Press</publisher>
<contexts>
<context position="6711" citStr="Kendon, 1990" startWordPosition="1006" endWordPosition="1007"> a situated multiparty engagement model that can enable a computational system to fluidly engage, disengage and re-engage one or multiple people, and support natural interactions in an open-world context. 3 Related Work The process of engagement between people, and between people and computational systems has received a fair amount of attention. Observational studies in the sociolinguistics and conversational analysis communities have revealed that engagement is a complex, mixedinitiative, highly-coordinated process that often involves a variety of non-verbal cues and signals, (Goffman, 1963; Kendon, 1990), spatial trajectory and proximity (Hall, 1966; Kendon, 1990b), gaze and mutual attention (Argyle and Cook, 1976), head and hand gestures (Kendon, 1990), as well as verbal greetings. A number of researchers have also investigated issues of engagement in human-computer and humanrobot interaction contexts. Sidner and colleagues (2004) define engagement as “the process by which two (or more) participants establish, maintain and end their perceived connection during interactions they jointly undertake”, and focus on the process of maintaining engagement. They show in a user study (Sidner et al., 2</context>
<context position="12585" citStr="Kendon, 1990" startWordPosition="1944" endWordPosition="1945">st two are possible only from the not-engaged state, while the last two are possible only from the engaged state. The engagement actions are estimated based on a conditional probabilistic model of the form: P (EAai (t)JESai (t), EAai (t − 1 ), SEAai (t− 1 ), T(t)) The inference is conditioned on the current engagement state, on the previous agent and system actions, and on additional sensory evidence T(t). T (t) includes the detection of explicit engagement cues such as: salutations (e.g. “Hi!”, “Bye bye”); calling behaviors (e.g. “Laura!”); the establishment or the breaking of an Fformation (Kendon, 1990b), i.e. the agent approaches and positions himself in front of the system and attends to the system; an expected, opening dialog move (e.g. “Come here!”). Note that each of these cues is explicit, and marks a committed engagement action. Figure 2. Engagement state transition diagram. EA is the agent’s engagement action; SEA is the system’s action. A third variable in the proposed model, EIai (t) , tracks the engagement intention of an agent with respect to a conversation. Like the engagement state, the intention can either be engaged or not-engaged. Intentions are tracked separately from acti</context>
<context position="26738" citStr="Kendon, 1990" startWordPosition="4165" endWordPosition="4166">ing, playing, watching, or departing. The goal and activity recognition models are application specific, and in this case are inferred based on probabilistic conditional models that leverage information about the spatiotemporal trajectory of each agent and their spoken utterances, as well as global scene information (e.g. is the system engaged in an active interaction, etc.). Initially, when the system is idle, it uses a conservative engagement policy and waits for the user to initiate engagement via an explicit action. Such actions include the user approaching and entering in an F-formation (Kendon, 1990b) with the system, i.e. standing right in front of it, swiping their badge, or pushing the start button (in the idle state the GUI displays “swipe your badge or press here to begin” below the avatar head). While engaged in an interaction, the system attempts to engage bystanders in an effort to create a collaborative, multi-participant game. In this case, the engagement policy is conditioned on the inferred activities of the agents in the scene. Specifically, if a watching bystander is detected, the system temporarily disengages the current participant, and engages and attempts to “convince” </context>
</contexts>
<marker>Kendon, 1990</marker>
<rawString>A. Kendon, 1990, A description of some human greetings, Conducting Interaction: Patterns of behavior in focused encounters, Studies in International Sociolinguistics, Cambridge University Press</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Kendon</author>
</authors>
<title>1990b, Spatial organization in social encounters: the F-formation system, Conducting Interaction: Patterns of behavior in focused encounters,</title>
<booktitle>Studies in International Sociolinguistics,</booktitle>
<publisher>Cambridge University Press</publisher>
<marker>Kendon, </marker>
<rawString>A. Kendon, 1990b, Spatial organization in social encounters: the F-formation system, Conducting Interaction: Patterns of behavior in focused encounters, Studies in International Sociolinguistics, Cambridge University Press</rawString>
</citation>
<citation valid="false">
<authors>
<author>M P Michalowski</author>
<author>S Sabanovic</author>
<author>R Simmons</author>
</authors>
<title>A spatial model of engagement for a social robot,</title>
<booktitle>in 9th IEEE Workshop on Advanced Motion Control,</booktitle>
<pages>762--767</pages>
<marker>Michalowski, Sabanovic, Simmons, </marker>
<rawString>M.P. Michalowski, S. Sabanovic, and R. Simmons, A spatial model of engagement for a social robot, in 9th IEEE Workshop on Advanced Motion Control, pp. 762-767</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Peters</author>
<author>C Pelachaud</author>
<author>E Bevacqua</author>
<author>M Mancini</author>
</authors>
<title>A model of attention and interest using gaze behavior,</title>
<date>2005</date>
<journal>Lecture Notes in Computer Science,</journal>
<pages>229--240</pages>
<marker>Peters, Pelachaud, Bevacqua, Mancini, 2005</marker>
<rawString>C. Peters, C. Pelachaud, E. Bevacqua, and M. Mancini, 2005, A model of attention and interest using gaze behavior, Lecture Notes in Computer Science, pp. 229-240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Peters</author>
</authors>
<title>2005b, Direction of Attention Perception for Conversation Initiation in Virtual Environments,</title>
<date>2005</date>
<booktitle>in Intelligent Virtual Agents,</booktitle>
<pages>215--228</pages>
<contexts>
<context position="7566" citStr="Peters (2005" startWordPosition="1140" endWordPosition="1141">agement in human-computer and humanrobot interaction contexts. Sidner and colleagues (2004) define engagement as “the process by which two (or more) participants establish, maintain and end their perceived connection during interactions they jointly undertake”, and focus on the process of maintaining engagement. They show in a user study (Sidner et al., 2004; 2005) that people directed their attention to a robot more often when the robot made engagement gestures throughout the interaction (i.e. tracked the user’s face, and pointed to relevant objects at appropriate times in the conversation.) Peters (2005; 2005b) uses an alternative definition of engagement as “the value that a participant in an interaction attributes to the goal of being together with the other participant(s) and of continuing the interaction,” and present the high-level schematics for an algorithm for establishing and maintaining engagement. The algorithm highlights the importance of mutual attention and eye gaze and relies on a heuristically computed “interest level” to decide when to start a conversation. Michalowski and colleagues (2006) propose and conduct experiments with a model of engagement grounded in proxemics (Hal</context>
</contexts>
<marker>Peters, 2005</marker>
<rawString>C. Peters, 2005b, Direction of Attention Perception for Conversation Initiation in Virtual Environments, in Intelligent Virtual Agents, 2005, pp. 215-228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Sidner</author>
<author>C D Kidd</author>
<author>C Lee</author>
<author>N Lesh</author>
</authors>
<title>Where to Look:</title>
<date>2004</date>
<journal>A Study of Human-Robot Engagement,</journal>
<volume>2004</volume>
<pages>78--84</pages>
<location>Madeira, Portugal</location>
<contexts>
<context position="2186" citStr="Sidner et al. (2004)" startWordPosition="329" endWordPosition="332">we outlined a set of core challenges for extending traditional closed-world dialog systems to systems that have competency in open-world dialog. The work described here is part of a larger research effort aimed at addressing these challenges, and constructing computational models to support the core interaction skills required for open-world dialog. In particular, we focus our attention in this paper on the challenges of managing engagement – “the process by which two (or more) participants establish, maintain and end their perceived connection during interactions they jointly undertake”, cf. Sidner et al. (2004) in open-world settings. We begin by reviewing the challenges of managing engagement in the open-world in the next section. In Section 3, we survey the terrain of related efforts that provides valuable context for the new work described in this paper. In Section 4, we introduce a computational model for multiparty situated engagement. The model harnesses components for sensing the engagement state, actions, and intentions of people in the scene for making high-level engagement decisions (whom to engage with, and when), and for rendering these decisions into a set of low-level coordinated behav</context>
<context position="7314" citStr="Sidner et al., 2004" startWordPosition="1098" endWordPosition="1101">3; Kendon, 1990), spatial trajectory and proximity (Hall, 1966; Kendon, 1990b), gaze and mutual attention (Argyle and Cook, 1976), head and hand gestures (Kendon, 1990), as well as verbal greetings. A number of researchers have also investigated issues of engagement in human-computer and humanrobot interaction contexts. Sidner and colleagues (2004) define engagement as “the process by which two (or more) participants establish, maintain and end their perceived connection during interactions they jointly undertake”, and focus on the process of maintaining engagement. They show in a user study (Sidner et al., 2004; 2005) that people directed their attention to a robot more often when the robot made engagement gestures throughout the interaction (i.e. tracked the user’s face, and pointed to relevant objects at appropriate times in the conversation.) Peters (2005; 2005b) uses an alternative definition of engagement as “the value that a participant in an interaction attributes to the goal of being together with the other participant(s) and of continuing the interaction,” and present the high-level schematics for an algorithm for establishing and maintaining engagement. The algorithm highlights the importa</context>
</contexts>
<marker>Sidner, Kidd, Lee, Lesh, 2004</marker>
<rawString>C.L. Sidner, C.D. Kidd, C. Lee, and N. Lesh, 2004, Where to Look: A Study of Human-Robot Engagement, IUI’2004, pp. 78-84, Madeira, Portugal</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Sidner</author>
<author>C Lee</author>
<author>C D Kidd</author>
<author>N Lesh</author>
<author>C Rich</author>
</authors>
<title>Explorations in engagement for humans and robots,</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>166</volume>
<pages>1--2</pages>
<marker>Sidner, Lee, Kidd, Lesh, Rich, 2005</marker>
<rawString>C.L. Sidner, C. Lee, C.D. Kidd, N. Lesh, and C. Rich, 2005, Explorations in engagement for humans and robots, Artificial Intelligence, 166 (1-2), pp. 140-164</rawString>
</citation>
<citation valid="true">
<authors>
<author>Situated Interaction</author>
</authors>
<date>2009</date>
<note>Project page: http://research.microsoft.com/~dbohus/research_situated_i nteraction.html</note>
<contexts>
<context position="25383" citStr="Interaction, 2009" startWordPosition="3955" endWordPosition="3956">who the current speaker is, and who has the conversational floor (Bohus and Horvitz, 2009). At the behavioral level, the avatar orients its head pose and gaze towards the current speaker, or towards the addressee(s) of its own utterances. During multiplayer games, the avatar alternates between the users when asking questions. Also, after a response is received from one of the users, the avatar confirms the answer with the other user(s), e.g. “Do you agree with that?” A full sample interaction with the system is described in Appendix A, and the corresponding video is available online (Situated Interaction, 2009). 5.2 Multiparty Engagement Policy The trivia game system implements the situated, multiparty engagement model described in Section 4. The sensing and behavioral control components are application independent and were previously described. We now describe the system’s engagement policy, which is application specific. As previously discussed, apart from using the inferred engagement state, actions and intentions for the agents in the scene, the proposed model also uses information about the high-level goals and activities of these agents when making engagement decisions. Specifically, the syste</context>
</contexts>
<marker>Interaction, 2009</marker>
<rawString>Situated Interaction, 2009, Project page: http://research.microsoft.com/~dbohus/research_situated_i nteraction.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Vertegaal</author>
<author>R Slagter</author>
<author>G C v d Veer</author>
<author>A Nijholt</author>
</authors>
<title>Eye gaze patterns in conversations: there is more to conversational agents than meets the eyes,</title>
<date>2001</date>
<pages>01</pages>
<marker>Vertegaal, Slagter, Veer, Nijholt, 2001</marker>
<rawString>R. Vertegaal, R. Slagter, G.C.v.d.Veer, and A. Nijholt, 2001, Eye gaze patterns in conversations: there is more to conversational agents than meets the eyes, CHI’01</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>