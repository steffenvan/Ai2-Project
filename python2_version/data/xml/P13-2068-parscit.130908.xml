<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001209">
<title confidence="0.9495815">
Bilingual Lexical Cohesion Trigger Model for Document-Level
Machine Translation
</title>
<author confidence="0.978211">
Guosheng Ben† Deyi Xiong$* Zhiyang Teng† Yajuan L¨u† Qun Liu§†
</author>
<affiliation confidence="0.976335">
†Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
</affiliation>
<email confidence="0.976429">
{benguosheng,tengzhiyang,lvyajuan,liuqun}@ict.ac.cn
</email>
<affiliation confidence="0.990263">
$School of Computer Science and Technology,Soochow University
</affiliation>
<email confidence="0.969976">
{dyxiong}@suda.edu.cn
</email>
<affiliation confidence="0.676565">
§Centre for Next Generation Localisation, Dublin City University
</affiliation>
<email confidence="0.99315">
{qliu}@computing.dcu.ie
</email>
<sectionHeader confidence="0.994622" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999728111111111">
In this paper, we propose a bilingual lexi-
cal cohesion trigger model to capture lex-
ical cohesion for document-level machine
translation. We integrate the model into
hierarchical phrase-based machine trans-
lation and achieve an absolute improve-
ment of 0.85 BLEU points on average over
the baseline on NIST Chinese-English test
sets.
</bodyText>
<sectionHeader confidence="0.998426" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.986516105263158">
Current statistical machine translation (SMT) sys-
tems are mostly sentence-based. The major draw-
back of such a sentence-based translation fash-
ion is the neglect of inter-sentential dependencies.
As a linguistic means to establish inter-sentential
links, lexical cohesion ties sentences together in-
to a meaningfully interwoven structure through
words with the same or related meanings (Wong
and Kit, 2012).
This paper studies lexical cohesion devices and
incorporate them into document-level machine
translation. We propose a bilingual lexical cohe-
sion trigger model to capture lexical cohesion for
document-level SMT. We consider a lexical co-
hesion item in the source language and its corre-
sponding counterpart in the target language as a
trigger pair, in which we treat the source language
lexical cohesion item as the trigger and its target
language counterpart as the triggered item. Then
we use mutual information to measure the strength
of the dependency between the trigger and trig-
gered item.
We integrate this model into a hierarchical
phrase-based SMT system. Experiment results
∗Corresponding author
show that it is able to achieve substantial improve-
ments over the baseline.
The remainder of this paper proceeds as fol-
lows: Section 2 introduces the related work and
highlights the differences between previous meth-
ods and our model. Section 3 elaborates the pro-
posed bilingual lexical cohesion trigger model, in-
cluding the details of identifying lexical cohesion
devices, measuring dependency strength of bilin-
gual lexical cohesion triggers and integrating the
model into SMT. Section 4 presents experiments
to validate the effectiveness of our model. Finally,
Section 5 concludes with future work.
</bodyText>
<sectionHeader confidence="0.999719" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999858260869565">
As a linguistic means to establish inter-sentential
links, cohesion has been explored in the literature
of both linguistics and computational linguistics.
Cohesion is defined as relations of meaning that
exist within the text and divided into grammatical
cohesion that refers to the syntactic links between
text items and lexical cohesion that is achieved
through word choices in a text by Halliday and
Hasan (1976). In order to improve the quality of
machine translation output, cohesion has served as
a high level quality criterion in post-editing (Vas-
concellos, 1989). As a part of COMTIS project,
grammatical cohesion is integrated into machine
translation models to capture inter-sentential links
(Cartoni et al., 2011). Wong and Kit (2012) in-
corporate lexical cohesion to machine translation
evaluation metrics to evaluate document-level ma-
chine translation quality. Xiong et al. (2013) inte-
grate various target-side lexical cohesion devices
into document-level machine translation. Lexical
cohesion is also partially explored in the cache-
based translation models of Gong et al. (2011) and
translation consistency constraints of Xiao et al.
</bodyText>
<page confidence="0.980327">
382
</page>
<bodyText confidence="0.905548866666667">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 382–386,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
(2011).
All previous methods on lexical cohesion for
document-level machine translation as mentioned
above have one thing in common, which is that
they do not use any source language information.
Our work is mostly related to the mutual infor-
mation trigger based lexical cohesion model pro-
posed by Xiong et al. (2013). However, we sig-
nificantly extend their model to a bilingual lexical
cohesion trigger model that captures both source
and target-side lexical cohesion items to improve
target word selection in document-level machine
translation.
</bodyText>
<sectionHeader confidence="0.9935465" genericHeader="method">
3 Bilingual Lexical Cohesion Trigger
Model
</sectionHeader>
<subsectionHeader confidence="0.999968">
3.1 Identification of Lexical Cohesion Devices
</subsectionHeader>
<bodyText confidence="0.9966920625">
Lexical cohesion can be divided into reiteration
and collocation (Wong and Kit, 2012). Reitera-
tion is a form of lexical cohesion which involves
the repetition of a lexical item. Collocation is a
pair of lexical items that have semantic relation-
s, such as synonym, near-synonym, superordinate,
subordinate, antonym, meronym and so on. In
the collocation, we focus on the synonym/near-
synonym and super-subordinate semantic relation-
s 1. We define lexical cohesion devices as content
words that have lexical cohesion relations, namely
the reiteration, synonym/near-synonym and super-
subordinate.
Reiteration is common in texts. Take the fol-
lowing two sentences extracted from a document
for example (Halliday and Hasan, 1976).
</bodyText>
<listItem confidence="0.9965135">
1. There is a boy climbing the old elm.
2. That elm is not very safe.
</listItem>
<bodyText confidence="0.986726714285714">
We see that word elm in the first sentence is re-
peated in the second sentence. Such reiteration de-
vices are easy to identify in texts. Synonym/near-
synonym is a semantic relationship set. We can
use WordNet (Fellbaum, 1998) to identify them.
WordNet is a lexical resource that clusters words
with the same sense into a semantic group called
synset. Synsets in WordNet are organized ac-
cording to their semantic relations. Let s(w) de-
note a function that defines all synonym words of
w grouped in the same synset in WordNet. We
can use the function to compute all synonyms and
near-synonyms for word w. In order to represen-
t conveniently, s0 denotes the set of synonyms in
</bodyText>
<footnote confidence="0.8473045">
1Other collocations are not used frequently, such as
antonyms. So we we do not consider them in our study.
</footnote>
<bodyText confidence="0.999623">
s(w). Near-synonym set s1 is defined as the union
of all synsets that are defined by the function s(w)
where wE s0. It can be formulated as follows.
</bodyText>
<equation confidence="0.995908222222222">
Us1 = s(w) (1)
wEs0
Us2 = s(w) (2)
wEs1
Us3 = s(w) (3)
wEs2
Similarly sm can be defined recursively as follows.
sm = U s(w) (4)
wEsm−1
</equation>
<bodyText confidence="0.999799727272727">
Obviously, We can find synonyms and near-
synonyms for word w according to formula (4).
Superordinate and subordinate are formed by
words with an is-a semantic relation in WordNet.
As the super-subordinate relation is also encoded
in WordNet, we can define a function that is simi-
lar to s(w) identify hypernyms and hyponyms.
We use rep, syn and hyp to represent the lex-
ical cohesion device reiteration, synonym/near-
synonym and super-subordinate respectively here-
after for convenience.
</bodyText>
<subsectionHeader confidence="0.984892">
3.2 Bilingual Lexical Cohesion Trigger
Model
</subsectionHeader>
<bodyText confidence="0.999883521739131">
In a bilingual text, lexical cohesion is present in
the source and target language in a synchronous
fashion. We use a trigger model capture such a
bilingual lexical cohesion relation. We define xRy
(RE{rep, syn, hyp}) as a trigger pair where x is
the trigger in the source language and y the trig-
gered item in the target language. In order to cap-
ture these synchronous relations between lexical
cohesion items in the source language and their
counterparts in the target language, we use word
alignments. First, we identify a monolingual lexi-
cal cohesion relation in the target language in the
form of tRy where t is the trigger, y the triggered
item that occurs in a sentence succeeding the sen-
tence of t, and RE{rep, syn, hyp}. Second, we
find word x in the source language that is aligned
to t in the target language. We may find multiple
words xk1 in the source language that are aligned
to t. We use all of them xiRt(1G_iG_k) to define
bilingual lexical cohesion relations. In this way,
we can create bilingual lexical cohesion relations
xRy (RE{rep, syn, hyp}): x being the trigger and
y the triggered item.
</bodyText>
<page confidence="0.996536">
383
</page>
<bodyText confidence="0.999607090909091">
The possibility that y will occur given x is equal
to the chance that x triggers y. Therefore we mea-
sure the strength of dependency between the trig-
ger and triggered item according to pointwise mu-
tual information (PMI) (Church and Hanks, 1990;
Xiong et al., 2011).
The PMI for the trigger pair xRy where x is the
trigger, y the triggered item that occurs in a target
sentence succeeding the target sentence that aligns
to the source sentence of x, and RE{rep, syn, hyp}
is calculated as follows.
</bodyText>
<equation confidence="0.995535333333333">
PMI(xRy) = log( p(x, y, R)
p(x, R)p(y, R)) (5)
The joint probability p(x, y, R) is:
C(x, y, R)
p(x, y, R) = (6)
Ex ,y C(x, y, R)
</equation>
<bodyText confidence="0.992330666666667">
where C(x, y, R) is the number of aligned bilin-
gual documents where both x and y occur
with the relation R in different sentences, and
</bodyText>
<equation confidence="0.896735">
E
x,y C(x, y, R) is the number of bilingual docu-
</equation>
<bodyText confidence="0.998955">
ments where this relation R occurs. The marginal
probabilities of p(x, R) and p(y, R) can be calcu-
lated as follows.
</bodyText>
<equation confidence="0.98773025">
p(x, R) = � C(x, y, R) (7)
y
p(y, R) = � C(x, y, R) (8)
x
</equation>
<bodyText confidence="0.944239">
Given a target sentence ym1 , our bilingual lexical
cohesion trigger model is defined as follows.
</bodyText>
<equation confidence="0.9993985">
MIR(ym 1 ) = H exp(PMI(·Ryi)) (9)
yi
</equation>
<bodyText confidence="0.9999394">
where yi are content words in the sentence ym1 and
PMI(·Ryi)is the maximum PMI value among all
trigger words xq1 from source sentences that have
been recently translated, where trigger words xq1
have an R relation with word yi.
</bodyText>
<equation confidence="0.867433">
PMI(·Ryi) = max1≤j≤qPMI(xjRyi) (10)
Three models MIrep(ym1 ), MIsyn(ym1 ),
</equation>
<bodyText confidence="0.9998476">
MIhyp(ym1 ) for the reiteration device, the
synonym/near-synonym device and the super-
subordinate device can be formulated as above.
They are integrated into the log-linear model of
SMT as three different features.
</bodyText>
<subsectionHeader confidence="0.997347">
3.3 Decoding
</subsectionHeader>
<bodyText confidence="0.999153666666667">
We incorporate our bilingual lexical cohesion trig-
ger model into a hierarchical phrase-based system
(Chiang, 2007). We add three features as follows.
</bodyText>
<listItem confidence="0.999319">
• MIrep(ym1 )
• MIsyn(ym1 )
• MIhyp(ym1 )
</listItem>
<bodyText confidence="0.999871428571429">
In order to quickly calculate the score of each fea-
ture, we calculate PMI for each trigger pair be-
fore decoding. We translate document one by one.
During translation, we maintain a cache to store
source language sentences of recently translated
target sentences and three sets Srep, Ssyn, Shyp
to store source language words that have the re-
lation of {rep, syn, hyp} with content words gen-
erated in target language. During decoding, we
update scores according to formula (9). When one
sentence is translated, we store the corresponding
source sentence into the cache. When the whole
document is translated, we clear the cache for the
next document.
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.956662">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.999072222222222">
Our experiments were conducted on the NIST
Chinese-English translation tasks with large-scale
training data. The bilingual training data contain-
s 3.8M sentence pairs with 96.9M Chinese word-
s and 109.5M English words from LDC2. The
monolingual data for training data English lan-
guage model includes the Xinhua portion of the
Gigaword corpus. The development set is the
NIST MT Evaluation test set of 2005 (MT05),
which contains 100 documents. We used the sets
of MT06 and MT08 as test sets. The numbers of
documents in MT06, MT08 are 79 and 109 respec-
tively. For the bilingual lexical cohesion trigger
model, we collected data with document bound-
aries explicitly provided. The corpora are select-
ed from our bilingual training data and the whole
Hong Kong parallel text corpus3, which contains
103,236 documents with 2.80M sentences.
</bodyText>
<footnote confidence="0.996651166666667">
2The corpora include LDC2002E18, LDC2003E07, LD-
C2003E14,LDC2004E12,LDC2004T07,LDC2004T08(Only
Hong Kong News), LDC2005T06 and LDC2005T10.
3They are LDC2003E14, LDC2004T07, LDC2005T06,
LDC2005T10 and LDC2004T08 (Hong Kong Hansard-
s/Laws/News).
</footnote>
<page confidence="0.99788">
384
</page>
<bodyText confidence="0.9459935">
We obtain the word alignments by running
GIZA++ (Och and Ney, 2003) in both direction-
s and applying “grow-diag-final-and” refinemen-
t (Koehn et al., 2003). We apply SRI Language
Modeling Toolkit (Stolcke, 2002) to train a 4-
gram language model with Kneser-Ney smooth-
ing. Case-insensitive NIST BLEU (Papineni et
al., 2002) was used to measure translation per-
formance. We used minimum error rate training
MERT (Och, 2003) for tuning the feature weights.
</bodyText>
<subsectionHeader confidence="0.98741">
4.2 Distribution of Lexical Cohesion Devices
</subsectionHeader>
<bodyText confidence="0.399398">
in the Target Language
</bodyText>
<table confidence="0.99530425">
Cohesion Device Percentage(%)
rep 30.85
syn 17.58
hyp 18.04
</table>
<tableCaption confidence="0.7824745">
Table 1: Distributions of lexical cohesion devices
in the target language.
</tableCaption>
<bodyText confidence="0.9997141875">
In this section we want to study how these
lexical cohesion devices distribute in the train-
ing data before conducting our experiments on
the bilingual lexical cohesion model. Here
we study the distribution of lexical cohesion in
the target language (English). Table 1 shows
the distribution of percentages that are counted
based on the content words in the training da-
ta. From Table 1, we can see that the reitera-
tion cohesion device is nearly a third of all con-
tent words (30.85%), synonym/near-synonym and
super-subordinate devices account for 17.58% and
18.04%. Obviously, lexical cohesion devices are
frequently used in real-world texts. Therefore cap-
turing lexical cohesion devices is very useful for
document-level machine translation.
</bodyText>
<subsectionHeader confidence="0.763079">
4.3 Results
</subsectionHeader>
<table confidence="0.817958">
System MT06 MT08 Avg
Base 30.43 23.32 26.88
rep 31.24 23.70 27.47
syn 30.92 23.71 27.32
hyp 30.97 23.48 27.23
rep+syn+hyp 31.47 23.98 27.73
</table>
<tableCaption confidence="0.836127">
Table 2: BLEU scores with various lexical co-
</tableCaption>
<bodyText confidence="0.990774266666667">
hesion devices on the test sets MT06 and MT08.
“Base” is the traditonal hierarchical system, “Avg”
is the average BLEU score on the two test sets.
Results are shown in Table 2. From the table,
we can see that integrating a single lexical cohe-
sion device into SMT, the model gains an improve-
ment of up to 0.81 BLEU points on the MT06 test
set. Combining all three features rep+syn+hyp to-
gether, the model gains an improvement of up to
1.04 BLEU points on MT06 test set, and an av-
erage improvement of 0.85 BLEU points on the
two test sets of MT06 and MT08. These stable
improvements strongly suggest that our bilingual
lexical cohesion trigger model is able to substan-
tially improve the translation quality.
</bodyText>
<sectionHeader confidence="0.9992" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999565857142857">
In this paper we have presented a bilingual lex-
ical cohesion trigger model to incorporate three
classes of lexical cohesion devices, namely the
reiteration, synonym/near-synonym and super-
subordinate devices into a hierarchical phrase-
based system. Our experimental results show
that our model achieves a substantial improvement
over the baseline. This displays the advantage of
exploiting bilingual lexical cohesion.
Grammatical and lexical cohesion have often
been studied together in discourse analysis. In
the future, we plan to extend our model to cap-
ture both grammatical and lexical cohesion in
document-level machine translation.
</bodyText>
<sectionHeader confidence="0.997561" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999928625">
This work was supported by 863 State Key Project
(No.2011AA01A207) and National Key Technol-
ogy R&amp;D Program(No.2012BAH39B03). Qun
Liu was also partially supported by Science Foun-
dation Ireland (Grant No.07/CE/I1142) as part of
the CNGL at Dublin City University. We would
like to thank the anonymous reviewers for their in-
sightful comments.
</bodyText>
<sectionHeader confidence="0.997907" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9969722">
Bruno Cartoni, Andrea Gesmundo, James Hender-
son, Cristina Grisot, Paola Merlo, Thomas Mey-
er, Jacques Moeschler, Sandrine Zufferey, Andrei
Popescu-Belis, et al. 2011. Improving mt coher-
ence through text-level processing of input texts:
the comtis project. http://webcast. in2p3. fr/videos-
the comtis project.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. computational linguistics, 33(2):201–228.
Kenneth Ward Church and Patrick Hanks. 1990. Word
</reference>
<page confidence="0.99439">
385
</page>
<reference confidence="0.999544130434783">
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22–29.
Christine Fellbaum. 1998. Wordnet: An electronic
lexical database.
Zhengxian Gong, Min Zhang, and Guodong Zhou.
2011. Cache-based document-level statistical ma-
chine translation. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 909–919, Edinburgh, Scotland,
UK., July. Association for Computational Linguis-
tics.
M.A.K Halliday and Ruqayia Hasan. 1976. Cohesion
in english. English language series, 9.
Philipp Koehn, Franz Josef Och, and Daniel Mar-
cu. 2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48–54. Association for Computa-
tional Linguistics.
Franz Josef Och and Hermann Ney. 2003. A systemat-
ic comparison of various statistical alignment mod-
els. Computational linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160–167, S-
apporo, Japan, July. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic e-
valuation of machine translation. In Proceedings of
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings of the internation-
al conference on spoken language processing, vol-
ume 2, pages 901–904.
Muriel Vasconcellos. 1989. Cohesion and coherence
in the presentation of machine translation products.
Georgetown University Round Table on Languages
and Linguistics, pages 89–105.
Billy T. M. Wong and Chunyu Kit. 2012. Extend-
ing machine translation evaluation metrics with lex-
ical cohesion to document level. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1060–1068, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.
2011. Document-level consistency verification in
machine translation. In Machine Translation Sum-
mit, volume 13, pages 131–138.
Deyi Xiong, Min Zhang, and Haizhou Li. 2011.
Enhancing language models in statistical machine
translation with backward n-grams and mutual in-
formation triggers. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
1288–1297, Portland, Oregon, USA, June. Associa-
tion for Computational Linguistics.
Deyi Xiong, Guosheng Ben, Min Zhang, Yajuan Lv,
and Qun Liu. 2013. Modeling lexical cohesion for
document-level machine translation. In Proceedings
of the Twenty-Third international joint conference
on Artificial Intelligence, Beijing,China.
</reference>
<page confidence="0.999015">
386
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.234812">
<title confidence="0.997314">Bilingual Lexical Cohesion Trigger Model for Machine Translation</title>
<author confidence="0.734075">Zhiyang</author>
<affiliation confidence="0.7360205">Laboratory of Intelligent Information Institute of Computing Technology, Chinese Academy of</affiliation>
<title confidence="0.761098">of Computer Science and Technology,Soochow</title>
<author confidence="0.415872">for Next Generation Localisation</author>
<author confidence="0.415872">Dublin City</author>
<abstract confidence="0.9967048">In this paper, we propose a bilingual lexical cohesion trigger model to capture lexical cohesion for document-level machine translation. We integrate the model into hierarchical phrase-based machine translation and achieve an absolute improvement of 0.85 BLEU points on average over the baseline on NIST Chinese-English test sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bruno Cartoni</author>
<author>Andrea Gesmundo</author>
<author>James Henderson</author>
<author>Cristina Grisot</author>
<author>Paola Merlo</author>
<author>Thomas Meyer</author>
<author>Jacques Moeschler</author>
<author>Sandrine Zufferey</author>
<author>Andrei Popescu-Belis</author>
</authors>
<title>Improving mt coherence through text-level processing of input texts: the comtis project. http://webcast. in2p3. fr/videosthe comtis project.</title>
<date>2011</date>
<contexts>
<context position="3288" citStr="Cartoni et al., 2011" startWordPosition="472" endWordPosition="475"> of both linguistics and computational linguistics. Cohesion is defined as relations of meaning that exist within the text and divided into grammatical cohesion that refers to the syntactic links between text items and lexical cohesion that is achieved through word choices in a text by Halliday and Hasan (1976). In order to improve the quality of machine translation output, cohesion has served as a high level quality criterion in post-editing (Vasconcellos, 1989). As a part of COMTIS project, grammatical cohesion is integrated into machine translation models to capture inter-sentential links (Cartoni et al., 2011). Wong and Kit (2012) incorporate lexical cohesion to machine translation evaluation metrics to evaluate document-level machine translation quality. Xiong et al. (2013) integrate various target-side lexical cohesion devices into document-level machine translation. Lexical cohesion is also partially explored in the cachebased translation models of Gong et al. (2011) and translation consistency constraints of Xiao et al. 382 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 382–386, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computationa</context>
</contexts>
<marker>Cartoni, Gesmundo, Henderson, Grisot, Merlo, Meyer, Moeschler, Zufferey, Popescu-Belis, 2011</marker>
<rawString>Bruno Cartoni, Andrea Gesmundo, James Henderson, Cristina Grisot, Paola Merlo, Thomas Meyer, Jacques Moeschler, Sandrine Zufferey, Andrei Popescu-Belis, et al. 2011. Improving mt coherence through text-level processing of input texts: the comtis project. http://webcast. in2p3. fr/videosthe comtis project.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation. computational linguistics,</title>
<date>2007</date>
<pages>33--2</pages>
<contexts>
<context position="9798" citStr="Chiang, 2007" startWordPosition="1570" endWordPosition="1571">he sentence ym1 and PMI(·Ryi)is the maximum PMI value among all trigger words xq1 from source sentences that have been recently translated, where trigger words xq1 have an R relation with word yi. PMI(·Ryi) = max1≤j≤qPMI(xjRyi) (10) Three models MIrep(ym1 ), MIsyn(ym1 ), MIhyp(ym1 ) for the reiteration device, the synonym/near-synonym device and the supersubordinate device can be formulated as above. They are integrated into the log-linear model of SMT as three different features. 3.3 Decoding We incorporate our bilingual lexical cohesion trigger model into a hierarchical phrase-based system (Chiang, 2007). We add three features as follows. • MIrep(ym1 ) • MIsyn(ym1 ) • MIhyp(ym1 ) In order to quickly calculate the score of each feature, we calculate PMI for each trigger pair before decoding. We translate document one by one. During translation, we maintain a cache to store source language sentences of recently translated target sentences and three sets Srep, Ssyn, Shyp to store source language words that have the relation of {rep, syn, hyp} with content words generated in target language. During decoding, we update scores according to formula (9). When one sentence is translated, we store the </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. computational linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<booktitle>Computational linguistics,</booktitle>
<pages>16--1</pages>
<contexts>
<context position="8279" citStr="Church and Hanks, 1990" startWordPosition="1293" endWordPosition="1296">we find word x in the source language that is aligned to t in the target language. We may find multiple words xk1 in the source language that are aligned to t. We use all of them xiRt(1G_iG_k) to define bilingual lexical cohesion relations. In this way, we can create bilingual lexical cohesion relations xRy (RE{rep, syn, hyp}): x being the trigger and y the triggered item. 383 The possibility that y will occur given x is equal to the chance that x triggers y. Therefore we measure the strength of dependency between the trigger and triggered item according to pointwise mutual information (PMI) (Church and Hanks, 1990; Xiong et al., 2011). The PMI for the trigger pair xRy where x is the trigger, y the triggered item that occurs in a target sentence succeeding the target sentence that aligns to the source sentence of x, and RE{rep, syn, hyp} is calculated as follows. PMI(xRy) = log( p(x, y, R) p(x, R)p(y, R)) (5) The joint probability p(x, y, R) is: C(x, y, R) p(x, y, R) = (6) Ex ,y C(x, y, R) where C(x, y, R) is the number of aligned bilingual documents where both x and y occur with the relation R in different sentences, and E x,y C(x, y, R) is the number of bilingual documents where this relation R occurs</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Fellbaum</author>
</authors>
<title>Wordnet: An electronic lexical database.</title>
<date>1998</date>
<contexts>
<context position="5553" citStr="Fellbaum, 1998" startWordPosition="818" endWordPosition="819">-subordinate semantic relations 1. We define lexical cohesion devices as content words that have lexical cohesion relations, namely the reiteration, synonym/near-synonym and supersubordinate. Reiteration is common in texts. Take the following two sentences extracted from a document for example (Halliday and Hasan, 1976). 1. There is a boy climbing the old elm. 2. That elm is not very safe. We see that word elm in the first sentence is repeated in the second sentence. Such reiteration devices are easy to identify in texts. Synonym/nearsynonym is a semantic relationship set. We can use WordNet (Fellbaum, 1998) to identify them. WordNet is a lexical resource that clusters words with the same sense into a semantic group called synset. Synsets in WordNet are organized according to their semantic relations. Let s(w) denote a function that defines all synonym words of w grouped in the same synset in WordNet. We can use the function to compute all synonyms and near-synonyms for word w. In order to represent conveniently, s0 denotes the set of synonyms in 1Other collocations are not used frequently, such as antonyms. So we we do not consider them in our study. s(w). Near-synonym set s1 is defined as the u</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christine Fellbaum. 1998. Wordnet: An electronic lexical database.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhengxian Gong</author>
<author>Min Zhang</author>
<author>Guodong Zhou</author>
</authors>
<title>Cache-based document-level statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>909--919</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="3655" citStr="Gong et al. (2011)" startWordPosition="524" endWordPosition="527">utput, cohesion has served as a high level quality criterion in post-editing (Vasconcellos, 1989). As a part of COMTIS project, grammatical cohesion is integrated into machine translation models to capture inter-sentential links (Cartoni et al., 2011). Wong and Kit (2012) incorporate lexical cohesion to machine translation evaluation metrics to evaluate document-level machine translation quality. Xiong et al. (2013) integrate various target-side lexical cohesion devices into document-level machine translation. Lexical cohesion is also partially explored in the cachebased translation models of Gong et al. (2011) and translation consistency constraints of Xiao et al. 382 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 382–386, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics (2011). All previous methods on lexical cohesion for document-level machine translation as mentioned above have one thing in common, which is that they do not use any source language information. Our work is mostly related to the mutual information trigger based lexical cohesion model proposed by Xiong et al. (2013). However, we significantly extend t</context>
</contexts>
<marker>Gong, Zhang, Zhou, 2011</marker>
<rawString>Zhengxian Gong, Min Zhang, and Guodong Zhou. 2011. Cache-based document-level statistical machine translation. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 909–919, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
<author>Ruqayia Hasan</author>
</authors>
<date>1976</date>
<note>Cohesion in english. English language series, 9.</note>
<contexts>
<context position="2979" citStr="Halliday and Hasan (1976)" startWordPosition="426" endWordPosition="429"> bilingual lexical cohesion triggers and integrating the model into SMT. Section 4 presents experiments to validate the effectiveness of our model. Finally, Section 5 concludes with future work. 2 Related Work As a linguistic means to establish inter-sentential links, cohesion has been explored in the literature of both linguistics and computational linguistics. Cohesion is defined as relations of meaning that exist within the text and divided into grammatical cohesion that refers to the syntactic links between text items and lexical cohesion that is achieved through word choices in a text by Halliday and Hasan (1976). In order to improve the quality of machine translation output, cohesion has served as a high level quality criterion in post-editing (Vasconcellos, 1989). As a part of COMTIS project, grammatical cohesion is integrated into machine translation models to capture inter-sentential links (Cartoni et al., 2011). Wong and Kit (2012) incorporate lexical cohesion to machine translation evaluation metrics to evaluate document-level machine translation quality. Xiong et al. (2013) integrate various target-side lexical cohesion devices into document-level machine translation. Lexical cohesion is also p</context>
<context position="5259" citStr="Halliday and Hasan, 1976" startWordPosition="761" endWordPosition="764">ion is a form of lexical cohesion which involves the repetition of a lexical item. Collocation is a pair of lexical items that have semantic relations, such as synonym, near-synonym, superordinate, subordinate, antonym, meronym and so on. In the collocation, we focus on the synonym/nearsynonym and super-subordinate semantic relations 1. We define lexical cohesion devices as content words that have lexical cohesion relations, namely the reiteration, synonym/near-synonym and supersubordinate. Reiteration is common in texts. Take the following two sentences extracted from a document for example (Halliday and Hasan, 1976). 1. There is a boy climbing the old elm. 2. That elm is not very safe. We see that word elm in the first sentence is repeated in the second sentence. Such reiteration devices are easy to identify in texts. Synonym/nearsynonym is a semantic relationship set. We can use WordNet (Fellbaum, 1998) to identify them. WordNet is a lexical resource that clusters words with the same sense into a semantic group called synset. Synsets in WordNet are organized according to their semantic relations. Let s(w) denote a function that defines all synonym words of w grouped in the same synset in WordNet. We can</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>M.A.K Halliday and Ruqayia Hasan. 1976. Cohesion in english. English language series, 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11780" citStr="Koehn et al., 2003" startWordPosition="1880" endWordPosition="1883">ollected data with document boundaries explicitly provided. The corpora are selected from our bilingual training data and the whole Hong Kong parallel text corpus3, which contains 103,236 documents with 2.80M sentences. 2The corpora include LDC2002E18, LDC2003E07, LDC2003E14,LDC2004E12,LDC2004T07,LDC2004T08(Only Hong Kong News), LDC2005T06 and LDC2005T10. 3They are LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News). 384 We obtain the word alignments by running GIZA++ (Och and Ney, 2003) in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We apply SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4- gram language model with Kneser-Ney smoothing. Case-insensitive NIST BLEU (Papineni et al., 2002) was used to measure translation performance. We used minimum error rate training MERT (Och, 2003) for tuning the feature weights. 4.2 Distribution of Lexical Cohesion Devices in the Target Language Cohesion Device Percentage(%) rep 30.85 syn 17.58 hyp 18.04 Table 1: Distributions of lexical cohesion devices in the target language. In this section we want to study how these lexical cohesion devices distribute in the training dat</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1, pages 48–54. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational linguistics,</journal>
<pages>29--1</pages>
<contexts>
<context position="11694" citStr="Och and Ney, 2003" startWordPosition="1867" endWordPosition="1870">8 are 79 and 109 respectively. For the bilingual lexical cohesion trigger model, we collected data with document boundaries explicitly provided. The corpora are selected from our bilingual training data and the whole Hong Kong parallel text corpus3, which contains 103,236 documents with 2.80M sentences. 2The corpora include LDC2002E18, LDC2003E07, LDC2003E14,LDC2004E12,LDC2004T07,LDC2004T08(Only Hong Kong News), LDC2005T06 and LDC2005T10. 3They are LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News). 384 We obtain the word alignments by running GIZA++ (Och and Ney, 2003) in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We apply SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4- gram language model with Kneser-Ney smoothing. Case-insensitive NIST BLEU (Papineni et al., 2002) was used to measure translation performance. We used minimum error rate training MERT (Och, 2003) for tuning the feature weights. 4.2 Distribution of Lexical Cohesion Devices in the Target Language Cohesion Device Percentage(%) rep 30.85 syn 17.58 hyp 18.04 Table 1: Distributions of lexical cohesion devices in the target language. In this sect</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<contexts>
<context position="12046" citStr="Och, 2003" startWordPosition="1924" endWordPosition="1925">,LDC2004E12,LDC2004T07,LDC2004T08(Only Hong Kong News), LDC2005T06 and LDC2005T10. 3They are LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News). 384 We obtain the word alignments by running GIZA++ (Och and Ney, 2003) in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We apply SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4- gram language model with Kneser-Ney smoothing. Case-insensitive NIST BLEU (Papineni et al., 2002) was used to measure translation performance. We used minimum error rate training MERT (Och, 2003) for tuning the feature weights. 4.2 Distribution of Lexical Cohesion Devices in the Target Language Cohesion Device Percentage(%) rep 30.85 syn 17.58 hyp 18.04 Table 1: Distributions of lexical cohesion devices in the target language. In this section we want to study how these lexical cohesion devices distribute in the training data before conducting our experiments on the bilingual lexical cohesion model. Here we study the distribution of lexical cohesion in the target language (English). Table 1 shows the distribution of percentages that are counted based on the content words in the trainin</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="11948" citStr="Papineni et al., 2002" startWordPosition="1906" endWordPosition="1909">which contains 103,236 documents with 2.80M sentences. 2The corpora include LDC2002E18, LDC2003E07, LDC2003E14,LDC2004E12,LDC2004T07,LDC2004T08(Only Hong Kong News), LDC2005T06 and LDC2005T10. 3They are LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News). 384 We obtain the word alignments by running GIZA++ (Och and Ney, 2003) in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We apply SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4- gram language model with Kneser-Ney smoothing. Case-insensitive NIST BLEU (Papineni et al., 2002) was used to measure translation performance. We used minimum error rate training MERT (Och, 2003) for tuning the feature weights. 4.2 Distribution of Lexical Cohesion Devices in the Target Language Cohesion Device Percentage(%) rep 30.85 syn 17.58 hyp 18.04 Table 1: Distributions of lexical cohesion devices in the target language. In this section we want to study how these lexical cohesion devices distribute in the training data before conducting our experiments on the bilingual lexical cohesion model. Here we study the distribution of lexical cohesion in the target language (English). Table </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the international conference on spoken language processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<contexts>
<context position="11836" citStr="Stolcke, 2002" startWordPosition="1890" endWordPosition="1891">he corpora are selected from our bilingual training data and the whole Hong Kong parallel text corpus3, which contains 103,236 documents with 2.80M sentences. 2The corpora include LDC2002E18, LDC2003E07, LDC2003E14,LDC2004E12,LDC2004T07,LDC2004T08(Only Hong Kong News), LDC2005T06 and LDC2005T10. 3They are LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News). 384 We obtain the word alignments by running GIZA++ (Och and Ney, 2003) in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We apply SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4- gram language model with Kneser-Ney smoothing. Case-insensitive NIST BLEU (Papineni et al., 2002) was used to measure translation performance. We used minimum error rate training MERT (Och, 2003) for tuning the feature weights. 4.2 Distribution of Lexical Cohesion Devices in the Target Language Cohesion Device Percentage(%) rep 30.85 syn 17.58 hyp 18.04 Table 1: Distributions of lexical cohesion devices in the target language. In this section we want to study how these lexical cohesion devices distribute in the training data before conducting our experiments on the bilingual lex</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm-an extensible language modeling toolkit. In Proceedings of the international conference on spoken language processing, volume 2, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Muriel Vasconcellos</author>
</authors>
<title>Cohesion and coherence in the presentation of machine translation products. Georgetown University Round Table on Languages and Linguistics,</title>
<date>1989</date>
<pages>89--105</pages>
<contexts>
<context position="3134" citStr="Vasconcellos, 1989" startWordPosition="451" endWordPosition="453">ion 5 concludes with future work. 2 Related Work As a linguistic means to establish inter-sentential links, cohesion has been explored in the literature of both linguistics and computational linguistics. Cohesion is defined as relations of meaning that exist within the text and divided into grammatical cohesion that refers to the syntactic links between text items and lexical cohesion that is achieved through word choices in a text by Halliday and Hasan (1976). In order to improve the quality of machine translation output, cohesion has served as a high level quality criterion in post-editing (Vasconcellos, 1989). As a part of COMTIS project, grammatical cohesion is integrated into machine translation models to capture inter-sentential links (Cartoni et al., 2011). Wong and Kit (2012) incorporate lexical cohesion to machine translation evaluation metrics to evaluate document-level machine translation quality. Xiong et al. (2013) integrate various target-side lexical cohesion devices into document-level machine translation. Lexical cohesion is also partially explored in the cachebased translation models of Gong et al. (2011) and translation consistency constraints of Xiao et al. 382 Proceedings of the </context>
</contexts>
<marker>Vasconcellos, 1989</marker>
<rawString>Muriel Vasconcellos. 1989. Cohesion and coherence in the presentation of machine translation products. Georgetown University Round Table on Languages and Linguistics, pages 89–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Billy T M Wong</author>
<author>Chunyu Kit</author>
</authors>
<title>Extending machine translation evaluation metrics with lexical cohesion to document level.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1060--1068</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="1243" citStr="Wong and Kit, 2012" startWordPosition="159" endWordPosition="162">ation. We integrate the model into hierarchical phrase-based machine translation and achieve an absolute improvement of 0.85 BLEU points on average over the baseline on NIST Chinese-English test sets. 1 Introduction Current statistical machine translation (SMT) systems are mostly sentence-based. The major drawback of such a sentence-based translation fashion is the neglect of inter-sentential dependencies. As a linguistic means to establish inter-sentential links, lexical cohesion ties sentences together into a meaningfully interwoven structure through words with the same or related meanings (Wong and Kit, 2012). This paper studies lexical cohesion devices and incorporate them into document-level machine translation. We propose a bilingual lexical cohesion trigger model to capture lexical cohesion for document-level SMT. We consider a lexical cohesion item in the source language and its corresponding counterpart in the target language as a trigger pair, in which we treat the source language lexical cohesion item as the trigger and its target language counterpart as the triggered item. Then we use mutual information to measure the strength of the dependency between the trigger and triggered item. We i</context>
<context position="3309" citStr="Wong and Kit (2012)" startWordPosition="476" endWordPosition="479">d computational linguistics. Cohesion is defined as relations of meaning that exist within the text and divided into grammatical cohesion that refers to the syntactic links between text items and lexical cohesion that is achieved through word choices in a text by Halliday and Hasan (1976). In order to improve the quality of machine translation output, cohesion has served as a high level quality criterion in post-editing (Vasconcellos, 1989). As a part of COMTIS project, grammatical cohesion is integrated into machine translation models to capture inter-sentential links (Cartoni et al., 2011). Wong and Kit (2012) incorporate lexical cohesion to machine translation evaluation metrics to evaluate document-level machine translation quality. Xiong et al. (2013) integrate various target-side lexical cohesion devices into document-level machine translation. Lexical cohesion is also partially explored in the cachebased translation models of Gong et al. (2011) and translation consistency constraints of Xiao et al. 382 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 382–386, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics (2011).</context>
<context position="4624" citStr="Wong and Kit, 2012" startWordPosition="666" endWordPosition="669">ove have one thing in common, which is that they do not use any source language information. Our work is mostly related to the mutual information trigger based lexical cohesion model proposed by Xiong et al. (2013). However, we significantly extend their model to a bilingual lexical cohesion trigger model that captures both source and target-side lexical cohesion items to improve target word selection in document-level machine translation. 3 Bilingual Lexical Cohesion Trigger Model 3.1 Identification of Lexical Cohesion Devices Lexical cohesion can be divided into reiteration and collocation (Wong and Kit, 2012). Reiteration is a form of lexical cohesion which involves the repetition of a lexical item. Collocation is a pair of lexical items that have semantic relations, such as synonym, near-synonym, superordinate, subordinate, antonym, meronym and so on. In the collocation, we focus on the synonym/nearsynonym and super-subordinate semantic relations 1. We define lexical cohesion devices as content words that have lexical cohesion relations, namely the reiteration, synonym/near-synonym and supersubordinate. Reiteration is common in texts. Take the following two sentences extracted from a document for</context>
</contexts>
<marker>Wong, Kit, 2012</marker>
<rawString>Billy T. M. Wong and Chunyu Kit. 2012. Extending machine translation evaluation metrics with lexical cohesion to document level. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1060–1068, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Xiao</author>
<author>Jingbo Zhu</author>
<author>Shujie Yao</author>
<author>Hao Zhang</author>
</authors>
<title>Document-level consistency verification in machine translation.</title>
<date>2011</date>
<booktitle>In Machine Translation Summit,</booktitle>
<volume>13</volume>
<pages>131--138</pages>
<marker>Xiao, Zhu, Yao, Zhang, 2011</marker>
<rawString>Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang. 2011. Document-level consistency verification in machine translation. In Machine Translation Summit, volume 13, pages 131–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
</authors>
<title>Enhancing language models in statistical machine translation with backward n-grams and mutual information triggers.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1288--1297</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="8300" citStr="Xiong et al., 2011" startWordPosition="1297" endWordPosition="1300">urce language that is aligned to t in the target language. We may find multiple words xk1 in the source language that are aligned to t. We use all of them xiRt(1G_iG_k) to define bilingual lexical cohesion relations. In this way, we can create bilingual lexical cohesion relations xRy (RE{rep, syn, hyp}): x being the trigger and y the triggered item. 383 The possibility that y will occur given x is equal to the chance that x triggers y. Therefore we measure the strength of dependency between the trigger and triggered item according to pointwise mutual information (PMI) (Church and Hanks, 1990; Xiong et al., 2011). The PMI for the trigger pair xRy where x is the trigger, y the triggered item that occurs in a target sentence succeeding the target sentence that aligns to the source sentence of x, and RE{rep, syn, hyp} is calculated as follows. PMI(xRy) = log( p(x, y, R) p(x, R)p(y, R)) (5) The joint probability p(x, y, R) is: C(x, y, R) p(x, y, R) = (6) Ex ,y C(x, y, R) where C(x, y, R) is the number of aligned bilingual documents where both x and y occur with the relation R in different sentences, and E x,y C(x, y, R) is the number of bilingual documents where this relation R occurs. The marginal probab</context>
</contexts>
<marker>Xiong, Zhang, Li, 2011</marker>
<rawString>Deyi Xiong, Min Zhang, and Haizhou Li. 2011. Enhancing language models in statistical machine translation with backward n-grams and mutual information triggers. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1288–1297, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Guosheng Ben</author>
<author>Min Zhang</author>
<author>Yajuan Lv</author>
<author>Qun Liu</author>
</authors>
<title>Modeling lexical cohesion for document-level machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence,</booktitle>
<location>Beijing,China.</location>
<contexts>
<context position="3456" citStr="Xiong et al. (2013)" startWordPosition="496" endWordPosition="499">rs to the syntactic links between text items and lexical cohesion that is achieved through word choices in a text by Halliday and Hasan (1976). In order to improve the quality of machine translation output, cohesion has served as a high level quality criterion in post-editing (Vasconcellos, 1989). As a part of COMTIS project, grammatical cohesion is integrated into machine translation models to capture inter-sentential links (Cartoni et al., 2011). Wong and Kit (2012) incorporate lexical cohesion to machine translation evaluation metrics to evaluate document-level machine translation quality. Xiong et al. (2013) integrate various target-side lexical cohesion devices into document-level machine translation. Lexical cohesion is also partially explored in the cachebased translation models of Gong et al. (2011) and translation consistency constraints of Xiao et al. 382 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 382–386, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics (2011). All previous methods on lexical cohesion for document-level machine translation as mentioned above have one thing in common, which is that they do</context>
</contexts>
<marker>Xiong, Ben, Zhang, Lv, Liu, 2013</marker>
<rawString>Deyi Xiong, Guosheng Ben, Min Zhang, Yajuan Lv, and Qun Liu. 2013. Modeling lexical cohesion for document-level machine translation. In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence, Beijing,China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>