<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000339">
<title confidence="0.9968135">
Cross-lingual Transfer of Named Entity Recognizers
without Parallel Corpora
</title>
<author confidence="0.977924">
Ayah Zirikly*
</author>
<affiliation confidence="0.987858">
Department of Computer Science
The George Washington University
</affiliation>
<address confidence="0.632614">
Washington DC, USA
</address>
<email confidence="0.99817">
ayaz@gwu.edu
</email>
<sectionHeader confidence="0.99738" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999843933333333">
We propose an approach to cross-lingual
named entity recognition model transfer
without the use of parallel corpora. In ad-
dition to global de-lexicalized features, we
introduce multilingual gazetteers that are
generated using graph propagation, and
cross-lingual word representation map-
pings without the use of parallel data. We
target the e-commerce domain, which is
challenging due to its unstructured and
noisy nature. The experiments have shown
that our approaches beat the strong MT
baseline, where the English model is trans-
ferred to two languages: Spanish and Chi-
nese.
</bodyText>
<sectionHeader confidence="0.999507" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.943778869565217">
Named Entity Recognition (NER) is usually
solved by a supervised learning approach, where
sequential labeling models are trained from a large
amount of manually annotated corpora. However,
such rich annotated data only exist for resource-
rich languages such as English, and building NER
systems for the majority of resource-poor lan-
guages, or specific domains in any languages, still
poses a great challenge.
Annotation projection through parallel text
(Yarowsky et al., 2001), (Das and Petrov, 2011),
(Wang and Manning, 2014) has been traditionally
used to overcome this issue, where the annotated
tags in the source (resource-rich) language are pro-
jected via word-aligned bilingual parallel text (bi-
text) and used to train sequential labeling mod-
els in the (resource-poor) target language. How-
ever, this could lead to two issues: firstly, word
Phis work has been performed while the authors were
at Rakuten Institute of Technology, New York. The authors
would like to thank Prof. Satoshi Sekine at New York Univer-
sity and other members of Rakuten Institute of Technology
for their support during the project.
</bodyText>
<note confidence="0.863222666666667">
Masato Hagiwara
Duolingo, Inc.
Pittsburgh PA, USA
</note>
<email confidence="0.980346">
masato@duolingo.com
</email>
<bodyText confidence="0.998540073170731">
alignment and projected tags are potentially noisy,
making the trained models sub-optimal. Instead of
projecting noisy labels explicitly, Wang and Man-
ning (2014) project posterior marginals expecta-
tions as soft constraints. Das and Petrov (2011)
projected POS tags from source language types
to target language trigarms using graph propaga-
tion and used the projected label distribution to
train robust POS taggers. Secondly, the availabil-
ity of such bitext is limited especially for resource-
poor languages and domains, where it is often the
case that available resources are moderately-sized
monolingual/comparable corpora and small bilin-
gual dictionaries.
Instead, we seek a direct transfer approach
(Figure 1) to cross-lingual NER (also classified
as transductive transfer learning (Pan and Yang,
2010) and closely related to domain adaptation).
Specifically, we only assume the availability of
comparable corpora and small-sized bilingual dic-
tionaries, and use the same sequential tagging
model trained on the source corpus for tagging
the target corpus. Direct transfer approaches are
extensively studied for cross-lingual dependency
parser transfer. For example, Zeman et al. (2008)
built a constituent parser using direct transfer be-
tween closely related languages, namely, Danish
and Swedish. McDonald et al. (2011) trained
de-lexicalized dependency parsers in English and
then “re-lexicalized” the parser. However, cross-
lingual transfer of named entity taggers have not
been studied enough, and this paper, to the best of
the authors’ knowledge, is the first to apply direct
transfer learning to NER.
Transfer of NER taggers poses a difficult chal-
lenge that is different from syntax transfer: most
of the past work deals with de-lexicalized parsers,
yet one of the most important clues for NER,
gazetteers, is inherently lexicalized. Also, various
features used for dependency parsing (Universal
POS tags, unsupervised clustering, etc.) are yet to
</bodyText>
<page confidence="0.863425">
390
</page>
<note confidence="0.367253333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 390–396,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999888">
Figure 1: System Framework
</figureCaption>
<bodyText confidence="0.9990475">
be proven useful for direct transfer of NER. There-
fore, the contributions of this paper is as follows:
</bodyText>
<listItem confidence="0.790821148148148">
1. We show that direct transfer approach for
multilingual NER actually works and per-
forms better than the strong MT baseline
(Shah et al., 2010), where the system’s out-
put in the source language is simply machine
translated into the target language.
2. We explore various non-lexical features,
namely, Universal POS tags and Brown clus-
ter mapping, which are deemed effective
for multilingual NER transfer. Although
brown cluster mapping (T¨ackstr¨om et al.,
2012), Universal POS Tagset (Petrov et al.,
2011), and re-lexicalization and self train-
ing (T¨ackstr¨om et al., 2013) are shown to
be effective for direct transfer of dependency
parsers, there have been no studies exploring
these features for NER transfer.
3. We show that gazetteers can actually be
generated only from the source language
gazetteers and a comparable corpus, through
a technique which we call gazetteer expan-
sion based on semi-supervised graph prop-
agation (Zhu et al., 2003). Gazetteer ex-
pansion has been used for various other pur-
poses, including POS tagging (Alexandrescu
and Kirchhoff, 2007) and dependency parsers
(Durrett et al., 2012).
</listItem>
<sectionHeader confidence="0.990228" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.998261272727273">
In this paper we propose a direct transfer learning
approach to train NER taggers in a multilingual
setting. Our goal is to identify named entities in a
target language LT, given solely annotated data in
the source language LS. Previous approaches rely
on parallel data to transfer the knowledge from one
language to another. However, parallel data is very
expensive to construct and not available for all lan-
guage pairs in all domains. Thus, our approach
loosens the constraint and only requires in-domain
comparable corpora.
</bodyText>
<subsectionHeader confidence="0.99588">
2.1 Monolingual NER in Source Language
</subsectionHeader>
<bodyText confidence="0.999322388888889">
Our framework is based on direct transfer ap-
proach, where we extract abstract, language-
independent and non lexical features FS and FT
in LS and LT. A subset of FT is generated us-
ing a mapping scheme discussed in Section 2.2,
then, directly apply LS NER model on LT using
FT. We adopt Conditional Random Field (CRF)
sequence labeling (Lafferty et al., 2001) to train
our system and generate the English model.
Monolingual Features 1) Token position: In-
stead of using token exact position, we use token
relative position in addition to position’s binary
features such as token is in: first, second, and last
third of the sentence. These features are based on
the observation that certain tokens, such as brand
names in title or description of a product, tend to
appear at the beginning of the sentence, while oth-
ers toward the end.
</bodyText>
<listItem confidence="0.969945219512195">
2) Word Shape: We use a list of binary fea-
tures: is-alphanumerical, is-number, is-alpha, is-
punctuation, the number length (if is-num is true),
pattern-based features (e.g. regular expressions to
capture certain patterns such as products model
numbers), latin-only features (first-is-capital, all-
391 pitals, all-small);
3) In-Title: A binary feature that specifies
whether the token is in the product’s title or de-
scription. For instance, brand names mostly ap-
pear in the beginning of titles, while this does not
hold in descriptions;
4) Preceding/Proceeding keywords within win-
dow: some NEs are often preceded by certain
keywords. For instance, often a product size is
preceded by certain keywords such as dimension,
height or word“size.” In our work we use a manu-
ally created list of keywords for two classes Color
and Size. Although the keyword list is domain de-
pendent, it is often short and can be easily updated.
5) Universal Part of Speech Tags: Part of
Speech (POS) tags have been widely used in many
NER systems. However, each language has its
own POS tagset that often has limited overlap with
other POS languages’ tagsets. Thus, we use a
coarse-grained layer of POS tags called Universal
POS, as proposed in (Petrov et al., 2011).
6) Token is a unit: A binary feature that is set to
true if it matches an entry in the units dictionary
(e.g., “cm.”)
7) Gazetteers: Building dictionaries for every
LT of interest is expensive; thus, we propose
a method, described in Section 3, to generate
gazetteers in LT given ones in LS.
8) Brown Clustering (BC): Word representa-
tions, especially Brown Clustering (Brown et al.,
1992), are used in many NLP tasks and are proven
to improve NER performance (Turian et al., 2010).
In this work, we use cluster IDs of variable pre-
fix lengths in order to retrieve word similarities on
different granularity levels.
</listItem>
<subsectionHeader confidence="0.998568">
2.2 Multilingual NER in Target Language
</subsectionHeader>
<bodyText confidence="0.99968475">
Our goal is to transfer each feature from LS to LT
space. The main challenge resides in transferring
features 7 and 8 without the use of external re-
sources and parallel data for every target language.
</bodyText>
<subsectionHeader confidence="0.746334">
2.2.1 Brown Clustering Mapping
</subsectionHeader>
<bodyText confidence="0.999709454545455">
Given i) Vocabulary in the source/target lan-
guages VS = {wS1 , wS2 , ..., wS NS} and VT =
{wT1 , wT2 , ..., vTNT }; ii) The output of brown clus-
tering on LS and LT: CS = {cS1 , ..., cSKS} and
CT = {cT1 , ..., cTKL}, we aim to find the best
mapping cS* that maximizes the cluster similarity
simC for each target cluster (Equation 1), and for
each metric discussed in the following. We cal-
culate the cluster similarity simC as the weighted
average of the word similarity simW of the mem-
bers of the two clusters (Equation 2).
</bodyText>
<equation confidence="0.762368">
simC(cS, cT) for each cT E CT (1)
simW (wS, wT )
</equation>
<listItem confidence="0.924668470588235">
(2)
Clusters Similarity Metrics The similarity
metrics used can be summarized in:
a) String Similarity (external resources indepen-
dent): This metric works only on languages that
share the same alphabet, as it is based on the in-
tuition that most NEs conserve the name’s shape
or present minor changes that can be identified us-
ing edit distance in closely related languages (we
use Levenshtein distance (Levenshtein, 1966)).
The two variations of string similarity metrics
used are: i) Exact match: simW (wi, wj) =
1 if wi = wj; ii) Edit distance: simW(wi, wj) =
1 if levenshtein-distance(wi, wj) &lt; θ.
b) Dictionary-based similarity: We present two
similarity metrics using BabelNet synsets (Nav-
igli and Ponzetto, 2012): i) Binary co-occurence:
</listItem>
<equation confidence="0.861701">
simbinary
W (wi, wj) = 1 if wj E synset(wi),
</equation>
<bodyText confidence="0.942997333333333">
where synset(wi) is the set of words in the
BabelNet synset of wi; ii) Frequency weighted:
Weighted version of the binary similarity that
is based on the observation that less frequent
words tend to be less reliable in brown clustering:
simweighted
</bodyText>
<equation confidence="0.968994">
W (wi, wj) = [log f(wi) + log f(wj)� x
simbinary
W (wi, wj) where f(w) is the frequency
</equation>
<bodyText confidence="0.999934">
of word w. Unlike String similarity metrics, this
metric is not limited to similar languages due to
the use of multilingual dictionaries i.e., BabelNet,
which covers 271 languages.
</bodyText>
<sectionHeader confidence="0.997945" genericHeader="method">
3 Gazetteer expansion
</sectionHeader>
<bodyText confidence="0.999825333333333">
In our approach, we use graph-based semi-
supervised learning to expand the gazetteers in
the source language to the target. Figure 2 il-
lustrates the motivation of our approach. Sup-
pose we have “New York” in the GPE gazetteer
in LS (English in this case), and we would like
to bootstrap the corresponding GPE gazetteer in
LT (Spanish). Although there is no direct link
between “New York” and “Nueva York,” you can
infer that “Puerto Rico” (in English) is similar to
“New York” based on some intra-language seman-
tic similarity model, then “Puerto Rico” is actually
</bodyText>
<page confidence="0.976595">
392
</page>
<figure confidence="0.995012666666667">
S*
c
= arg max
cSECS
1 ∑ simC(ct, cs) = |cS ||cT  |s
w EcS,wTEcT
</figure>
<figureCaption confidence="0.99977">
Figure 2: Gazeteer expansion
</figureCaption>
<bodyText confidence="0.9996263">
identical in both languages, then finally “Nueva
York” is similar to “Puerto Rico” (in Spanish)
again based on the Spanish intra-language similar-
ity model. This indirect inference of beliefs from
the source gazetteers to the target can be modeled
by semi-supervised graph propagation (Zhu et al.
2003), where graph nodes are VS U VT, positive
labels are entries in the LS gazetteer (e.g., GPE)
which we wish to expand to LT, and negative la-
bels are entries in other gazetteers (e.g., PERSON)
in LS. The edge weights between same-language
nodes wz and wj are given by exp(−u||wz −wj||)
where wz is the distributed vector representation
of word wz computed by word2vec (Mikolov et al.,
2013). The edge weights between node wz E VS
and vj E VT are defined 1 if the spelling of these
two words are identical and 0 otherwise. Note that
this spelling based similarity propagation is still
available for language pairs with different writing
systems such as English and Chinese, because ma-
jor NEs (e.g., brand names) are often written in
Roman alphabets even in Chinese products. Since
the analytical solution to this propagation involves
the computation of nxn (n is the number of unla-
beled nodes) matrix, we approximated it by run-
ning three propagation steps iteratively, namely,
LS —* LS, LS —* LT, and LT —* LT. After
the propagation, we used all the nodes with their
propagated values f(wz) &gt; 0 as entities in the new
gazetteer.
</bodyText>
<sectionHeader confidence="0.99973" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.913504">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.99998625">
The targeted dataset contains a list of products (ti-
tles and descriptions). The titles of products are
� 10 words long and poorly structured, adding
more difficulties to our task. On the other hand,
</bodyText>
<table confidence="0.99577725">
Color Brand Material Model Type Size
EN 358 814 733 203 1238 427
ES 207 425 301 172 606 126
ZH 416 60 381 24 690 306
</table>
<tableCaption confidence="0.999461">
Table 1: Language-Tags Numbers Stats
</tableCaption>
<bodyText confidence="0.9983261">
the length of product descriptions ranges from
12-130 words. The e-commerce genre poses the
need to introduce new NE tagset as opposed to
the conventional ones, thus we introduce 6 tag
types: 1) Color; 2) Brand names; 3) Size; 4) Type:
e.g. “camera,” “shirt”; 5) Material: e.g. “plas-
tic”, “cotton”; 6) Model: the model number of a
product: e.g., “A1533.”. For the rest of the ex-
periments, English (EN) is the source language,
whereas we experiment with Spanish (ES) and
</bodyText>
<listItem confidence="0.686689777777778">
Chinese (ZH) as target languages. The datasets
used are: i) Training data: 1800 annotated English
products from Rakuten.com shopping (Rakuten,
2013a); ii) Test data: 300 ES products from
Rakuten Spain (Rakuten, 2013b) and 500 prod-
ucts from Rakuten Taiwan (Rakuten, 2013c); iii)
Brown clustering: English: Rakuten shopping
2013 dump (19m unique products with 607m to-
kens); Spanish: Rakuten Spain 2013 dump (700K
</listItem>
<bodyText confidence="0.943046">
unique products that contains 41m tokens) in addi-
tion to Spanish Wikipedia dump (Al-Rfou’, 2013);
Chinese: Wikipedia Chinese 2014 dump (147m
tokens) plus 16k products crawled from Rakuten
Taiwan. Table 1 shows the numbers of tags per
category for each language.
</bodyText>
<subsectionHeader confidence="0.951116">
4.2 Baseline
</subsectionHeader>
<bodyText confidence="0.999996461538461">
To the best of our knowledge, there is no previ-
ous work that proposes transfer learning for NER
without the use of parallel data. Thus, we ought to
generate a strong baseline to compare our results
to. Given the language pair (LS, LT), we use Mi-
crosoft Bing Translate API to generate LT —* LS
translation. Then, we apply LS NER model on the
translated text and evaluate by mapping the tagged
tokens back to LT using the word alignments gen-
erated by Bing Translate. We choose Bing trans-
late as opposed to Google translate due to its free-
to-use API that provides word alignment informa-
tion on the character level.
</bodyText>
<subsectionHeader confidence="0.985503">
4.3 Results &amp; Discussion
</subsectionHeader>
<bodyText confidence="0.990650666666667">
For each studied language we use Stanford
CoreNLP (Manning et al., 2014) for EN and ZH,
and TreeTagger (Schmid, 1994) for ES to produce
</bodyText>
<page confidence="0.996641">
393
</page>
<table confidence="0.9988785">
Gazetteer in Lt
Color Brand Material Model Type Size Micro-Avg
EN-Mono 68.45 71.91 50.94 59.78 53.73 45.42 61.12
ES-Baseline 24.23 3.44 13.08 14.51 12.5 6.61 13.79
ES-TL 18.00 9.37 8.05 16.99 18.26 10.64 39.46
ES-GT 38.49 13.31 33.5 2.27 36.43 1.16 30.20
ZH-Baseline 19.16 2.79 11.96 None 9.35 6.34 12.58
ZH-TL 9.36 1.02 1.81 None 17.28 17.74 23.43
</table>
<tableCaption confidence="0.988937">
Table 2: F-score Results
</tableCaption>
<page confidence="0.998441">
394
</page>
<bodyText confidence="0.999725882352942">
the tokens and the POS tags. However, we ap-
ply extra processing steps to the tokenizer due to
the nature of the domain’s data (e.g., avoid tok-
enizing models instances), in addition to normaliz-
ing URLs, numbers, and elongation. We also map
POS tags for all the source and target languages to
the universal POS tagset as explained in 2.1.
Based on Table 2, we note that English mono-
lingual performance (80:20 train/test split and 5-
folds cross variation) is considerably lower than
state-of-the-art English NER systems, which is
due to the nature of our targeted domain, the newly
proposed NE tagset, and most importantly, the
considerably small training data (1280 products).
These factors also affects the baseline and our pro-
posed system performance.
Table 2 illustrates the results for the English
monolingual NER system (EN-Mono), baseline
for ES and ZH (ES-Baseline and ZH-Baseline,
respectively), our proposed transfer learning ap-
proach with the gazetteer expansion (ES-TL and
ZH-TL). Additionally, we added the results of our
proposed approach where the gazetteers used are
machine translated using Google translate from
the English gazetteers to Spanish (ES-MT), in or-
der to evaluate our gazetteer expansion approach
performance to the translated gazetteers.
We note that ES-Baseline and ZH-Baseline are
considerably low due to the poor word alignment
generated by Bing Translator, which results in in-
correct tag projection. The quality of mapping is
mainly due to the noisy nature of the domain’s
data, which can be very expensive to fix.
Although the performance of our proposed sys-
tem is low (39.46% for ES and 23.43% for ZH),
but it surpasses the baseline performance in most
of the tag classes and yields an overall improve-
ment on the micro-average F-score of Pt� 23% in
ES and 11% in ZH. We note that one of the rea-
sons behind ZH Brand low performance is that
universal-POS for brands in EN are mostly proper
noun as opposed to noun in ZH, additionally the
considerably low number of brands in ZH test
data (60). On the other hand, it is intuitive that
Model yields one of the best performance among
the tags, since it is the most language independent
tag (as depicted in ES-TL). However, this does not
hold true in ZH due to the very small number of
Model instances (24). Type produces the best per-
formance in ES and ZH, due to the high cover-
age of the new expanded gazetteer over Type in-
stances, in addition to the large number of train-
ing instances (1238), in comparison to the other
tags. After conducting leave-out experiments on
Brown clustering and gazetteers features in ES, we
note that both shows an improvement of Pt� 4% and
� 8% respectively.
Our system surpasses the MT-based gazetter ex-
pansion by Pt� 9%, when comparing ES-TL to ES-
MT. However, as expected the main improvement
is in Model and Size tags as opposed to other tags
(e.g. Brand and Color) where MT provides more
accurate gazetteers. In our system output, colors
that are included in LT expanded gazetteers (e.g.
“azul” in ES) and have a high similarity score in
our proposed BC mapping, are correctly tagged.
On the other hand OOV Brand have a very large
prediction error rate due to the small training data.
</bodyText>
<sectionHeader confidence="0.997705" genericHeader="conclusions">
5 Conclusion and Future Works
</sectionHeader>
<bodyText confidence="0.9679606">
In this paper, we propose a cross-lingual NER
transfer learning approach which does not depend
on parallel corpora. Our experiments showed the
ability to transfer NER model to latin (ES) and
non latin (ZH) languages. For the future work, we
would like to investigate the generality of our ap-
proach in broader languages and domains.
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217–
250.
</bodyText>
<note confidence="0.890965333333333">
References
Rami Al-Rfou’. 2013. Spanish wikipedia dump. url =
https://sites.google.com/site/rmyeid/projects/polyglot.
</note>
<reference confidence="0.998530463636363">
Andrei Alexandrescu and Katrin Kirchhoff. 2007.
Data-driven graph construction for semi-supervised
graph-based learning in NLP. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 204–211, Rochester, New York, April.
Association for Computational Linguistics.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467–479.
Dipanjan Das and Slav Petrov. 2011. Unsuper-
vised part-of-speech tagging with bilingual graph-
based projections. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
600–609, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syn-
tactic transfer using a bilingual lexicon. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1–11.
Association for Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data.
VI Levenshtein. 1966. Binary Codes Capable of Cor-
recting Deletions, Insertions and Reversals. Soviet
Physics Doklady, 10:707.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations, pages
55–60.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’11, pages 62–72, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. IEEE Trans. on Knowl. and Data
Eng., 22(10):1345–1359, October.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. arXiv preprint
arXiv:1104.2086.
Rakuten. 2013a. Rakuten shopping. url =
http://www.rakuten.com/.
Rakuten. 2013b. Rakuten spanish.
Rakuten. 2013c. Rakuten taiwanese.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees.
Rushin Shah, Bo Lin, Anatole Gershman, and Robert
Frederking. 2010. Synergy: A named entity recog-
nition system for resource-scarce languages such as
swahili using online machine translation. In Pro-
ceedings of the Second Workshop on African Lan-
guage Technology (AfLaT 2010), pages 21–26.
Oscar T¨ackstr¨om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 477–
487, Montr´eal, Canada, June. Association for Com-
putational Linguistics.
Oscar T¨ackstr¨om, Ryan McDonald, and Joakim Nivre.
2013. Target language adaptation of discrimina-
tive transfer parsers. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 1061–1071, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384–394. Association for
Computational Linguistics.
Mengqiu Wang and Christopher D Manning. 2014.
Cross-lingual projected expectation regularization
for weakly supervised learning.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analy-
sis tools via robust projection across aligned cor-
pora. In Proceedings of the First International Con-
ference on Human Language Technology Research,
HLT ’01, pages 1–8, Stroudsburg, PA, USA. Asso-
395 ciation for Computational Linguistics.
Daniel Zeman, Univerzita Karlova, and Philip Resnik.
2008. Cross-language parser adaptation between re-
lated languages. In In IJCNLP-08 Workshop on NLP
for Less Privileged Languages, pages 35–42.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using gaussian
fields and harmonic functions. In IN ICML, pages
912–919.
</reference>
<page confidence="0.999089">
396
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.156124">
<title confidence="0.977935">Cross-lingual Transfer of Named Entity</title>
<author confidence="0.752486">without Parallel Corpora</author>
<affiliation confidence="0.725827">Department of Computer The George Washington</affiliation>
<author confidence="0.776152">Washington DC</author>
<email confidence="0.999342">ayaz@gwu.edu</email>
<abstract confidence="0.96181425">We propose an approach to cross-lingual named entity recognition model transfer without the use of parallel corpora. In addition to global de-lexicalized features, we introduce multilingual gazetteers that are generated using graph propagation, and cross-lingual word representation mappings without the use of parallel data. We target the e-commerce domain, which is challenging due to its unstructured and noisy nature. The experiments have shown that our approaches beat the strong MT baseline, where the English model is transferred to two languages: Spanish and Chinese.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Andrei Alexandrescu</author>
<author>Katrin Kirchhoff</author>
</authors>
<title>Data-driven graph construction for semi-supervised graph-based learning in NLP.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>204--211</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="5401" citStr="Alexandrescu and Kirchhoff, 2007" startWordPosition="805" endWordPosition="808"> (T¨ackstr¨om et al., 2012), Universal POS Tagset (Petrov et al., 2011), and re-lexicalization and self training (T¨ackstr¨om et al., 2013) are shown to be effective for direct transfer of dependency parsers, there have been no studies exploring these features for NER transfer. 3. We show that gazetteers can actually be generated only from the source language gazetteers and a comparable corpus, through a technique which we call gazetteer expansion based on semi-supervised graph propagation (Zhu et al., 2003). Gazetteer expansion has been used for various other purposes, including POS tagging (Alexandrescu and Kirchhoff, 2007) and dependency parsers (Durrett et al., 2012). 2 Approach In this paper we propose a direct transfer learning approach to train NER taggers in a multilingual setting. Our goal is to identify named entities in a target language LT, given solely annotated data in the source language LS. Previous approaches rely on parallel data to transfer the knowledge from one language to another. However, parallel data is very expensive to construct and not available for all language pairs in all domains. Thus, our approach loosens the constraint and only requires in-domain comparable corpora. 2.1 Monolingua</context>
</contexts>
<marker>Alexandrescu, Kirchhoff, 2007</marker>
<rawString>Andrei Alexandrescu and Katrin Kirchhoff. 2007. Data-driven graph construction for semi-supervised graph-based learning in NLP. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 204–211, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V Desouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="8485" citStr="Brown et al., 1992" startWordPosition="1319" endWordPosition="1322">R systems. However, each language has its own POS tagset that often has limited overlap with other POS languages’ tagsets. Thus, we use a coarse-grained layer of POS tags called Universal POS, as proposed in (Petrov et al., 2011). 6) Token is a unit: A binary feature that is set to true if it matches an entry in the units dictionary (e.g., “cm.”) 7) Gazetteers: Building dictionaries for every LT of interest is expensive; thus, we propose a method, described in Section 3, to generate gazetteers in LT given ones in LS. 8) Brown Clustering (BC): Word representations, especially Brown Clustering (Brown et al., 1992), are used in many NLP tasks and are proven to improve NER performance (Turian et al., 2010). In this work, we use cluster IDs of variable prefix lengths in order to retrieve word similarities on different granularity levels. 2.2 Multilingual NER in Target Language Our goal is to transfer each feature from LS to LT space. The main challenge resides in transferring features 7 and 8 without the use of external resources and parallel data for every target language. 2.2.1 Brown Clustering Mapping Given i) Vocabulary in the source/target languages VS = {wS1 , wS2 , ..., wS NS} and VT = {wT1 , wT2 ,</context>
</contexts>
<marker>Brown, Desouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram models of natural language. Computational linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
</authors>
<title>Unsupervised part-of-speech tagging with bilingual graphbased projections.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>600--609</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="1285" citStr="Das and Petrov, 2011" startWordPosition="184" endWordPosition="187">beat the strong MT baseline, where the English model is transferred to two languages: Spanish and Chinese. 1 Introduction Named Entity Recognition (NER) is usually solved by a supervised learning approach, where sequential labeling models are trained from a large amount of manually annotated corpora. However, such rich annotated data only exist for resourcerich languages such as English, and building NER systems for the majority of resource-poor languages, or specific domains in any languages, still poses a great challenge. Annotation projection through parallel text (Yarowsky et al., 2001), (Das and Petrov, 2011), (Wang and Manning, 2014) has been traditionally used to overcome this issue, where the annotated tags in the source (resource-rich) language are projected via word-aligned bilingual parallel text (bitext) and used to train sequential labeling models in the (resource-poor) target language. However, this could lead to two issues: firstly, word Phis work has been performed while the authors were at Rakuten Institute of Technology, New York. The authors would like to thank Prof. Satoshi Sekine at New York University and other members of Rakuten Institute of Technology for their support during th</context>
</contexts>
<marker>Das, Petrov, 2011</marker>
<rawString>Dipanjan Das and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graphbased projections. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 600–609, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Syntactic transfer using a bilingual lexicon.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1--11</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5447" citStr="Durrett et al., 2012" startWordPosition="812" endWordPosition="815">et al., 2011), and re-lexicalization and self training (T¨ackstr¨om et al., 2013) are shown to be effective for direct transfer of dependency parsers, there have been no studies exploring these features for NER transfer. 3. We show that gazetteers can actually be generated only from the source language gazetteers and a comparable corpus, through a technique which we call gazetteer expansion based on semi-supervised graph propagation (Zhu et al., 2003). Gazetteer expansion has been used for various other purposes, including POS tagging (Alexandrescu and Kirchhoff, 2007) and dependency parsers (Durrett et al., 2012). 2 Approach In this paper we propose a direct transfer learning approach to train NER taggers in a multilingual setting. Our goal is to identify named entities in a target language LT, given solely annotated data in the source language LS. Previous approaches rely on parallel data to transfer the knowledge from one language to another. However, parallel data is very expensive to construct and not available for all language pairs in all domains. Thus, our approach loosens the constraint and only requires in-domain comparable corpora. 2.1 Monolingual NER in Source Language Our framework is base</context>
</contexts>
<marker>Durrett, Pauls, Klein, 2012</marker>
<rawString>Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syntactic transfer using a bilingual lexicon. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1–11. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando CN Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<contexts>
<context position="6382" citStr="Lafferty et al., 2001" startWordPosition="968" endWordPosition="971"> one language to another. However, parallel data is very expensive to construct and not available for all language pairs in all domains. Thus, our approach loosens the constraint and only requires in-domain comparable corpora. 2.1 Monolingual NER in Source Language Our framework is based on direct transfer approach, where we extract abstract, languageindependent and non lexical features FS and FT in LS and LT. A subset of FT is generated using a mapping scheme discussed in Section 2.2, then, directly apply LS NER model on LT using FT. We adopt Conditional Random Field (CRF) sequence labeling (Lafferty et al., 2001) to train our system and generate the English model. Monolingual Features 1) Token position: Instead of using token exact position, we use token relative position in addition to position’s binary features such as token is in: first, second, and last third of the sentence. These features are based on the observation that certain tokens, such as brand names in title or description of a product, tend to appear at the beginning of the sentence, while others toward the end. 2) Word Shape: We use a list of binary features: is-alphanumerical, is-number, is-alpha, ispunctuation, the number length (if </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Levenshtein</author>
</authors>
<title>Binary Codes Capable of Correcting Deletions, Insertions and Reversals. Soviet Physics Doklady,</title>
<date>1966</date>
<pages>10--707</pages>
<contexts>
<context position="9975" citStr="Levenshtein, 1966" startWordPosition="1590" endWordPosition="1591">owing. We calculate the cluster similarity simC as the weighted average of the word similarity simW of the members of the two clusters (Equation 2). simC(cS, cT) for each cT E CT (1) simW (wS, wT ) (2) Clusters Similarity Metrics The similarity metrics used can be summarized in: a) String Similarity (external resources independent): This metric works only on languages that share the same alphabet, as it is based on the intuition that most NEs conserve the name’s shape or present minor changes that can be identified using edit distance in closely related languages (we use Levenshtein distance (Levenshtein, 1966)). The two variations of string similarity metrics used are: i) Exact match: simW (wi, wj) = 1 if wi = wj; ii) Edit distance: simW(wi, wj) = 1 if levenshtein-distance(wi, wj) &lt; θ. b) Dictionary-based similarity: We present two similarity metrics using BabelNet synsets (Navigli and Ponzetto, 2012): i) Binary co-occurence: simbinary W (wi, wj) = 1 if wj E synset(wi), where synset(wi) is the set of words in the BabelNet synset of wi; ii) Frequency weighted: Weighted version of the binary similarity that is based on the observation that less frequent words tend to be less reliable in brown cluster</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>VI Levenshtein. 1966. Binary Codes Capable of Correcting Deletions, Insertions and Reversals. Soviet Physics Doklady, 10:707.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="15214" citStr="Manning et al., 2014" startWordPosition="2484" endWordPosition="2487">for NER without the use of parallel data. Thus, we ought to generate a strong baseline to compare our results to. Given the language pair (LS, LT), we use Microsoft Bing Translate API to generate LT —* LS translation. Then, we apply LS NER model on the translated text and evaluate by mapping the tagged tokens back to LT using the word alignments generated by Bing Translate. We choose Bing translate as opposed to Google translate due to its freeto-use API that provides word alignment information on the character level. 4.3 Results &amp; Discussion For each studied language we use Stanford CoreNLP (Manning et al., 2014) for EN and ZH, and TreeTagger (Schmid, 1994) for ES to produce 393 Gazetteer in Lt Color Brand Material Model Type Size Micro-Avg EN-Mono 68.45 71.91 50.94 59.78 53.73 45.42 61.12 ES-Baseline 24.23 3.44 13.08 14.51 12.5 6.61 13.79 ES-TL 18.00 9.37 8.05 16.99 18.26 10.64 39.46 ES-GT 38.49 13.31 33.5 2.27 36.43 1.16 30.20 ZH-Baseline 19.16 2.79 11.96 None 9.35 6.34 12.58 ZH-TL 9.36 1.02 1.81 None 17.28 17.74 23.43 Table 2: F-score Results 394 the tokens and the POS tags. However, we apply extra processing steps to the tokenizer due to the nature of the domain’s data (e.g., avoid tokenizing mode</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Slav Petrov</author>
<author>Keith Hall</author>
</authors>
<title>Multi-source transfer of delexicalized dependency parsers.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>62--72</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3288" citStr="McDonald et al. (2011)" startWordPosition="481" endWordPosition="484"> (Figure 1) to cross-lingual NER (also classified as transductive transfer learning (Pan and Yang, 2010) and closely related to domain adaptation). Specifically, we only assume the availability of comparable corpora and small-sized bilingual dictionaries, and use the same sequential tagging model trained on the source corpus for tagging the target corpus. Direct transfer approaches are extensively studied for cross-lingual dependency parser transfer. For example, Zeman et al. (2008) built a constituent parser using direct transfer between closely related languages, namely, Danish and Swedish. McDonald et al. (2011) trained de-lexicalized dependency parsers in English and then “re-lexicalized” the parser. However, crosslingual transfer of named entity taggers have not been studied enough, and this paper, to the best of the authors’ knowledge, is the first to apply direct transfer learning to NER. Transfer of NER taggers poses a difficult challenge that is different from syntax transfer: most of the past work deals with de-lexicalized parsers, yet one of the most important clues for NER, gazetteers, is inherently lexicalized. Also, various features used for dependency parsing (Universal POS tags, unsuperv</context>
</contexts>
<marker>McDonald, Petrov, Hall, 2011</marker>
<rawString>Ryan McDonald, Slav Petrov, and Keith Hall. 2011. Multi-source transfer of delexicalized dependency parsers. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 62–72, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="12231" citStr="Mikolov et al., 2013" startWordPosition="1973" endWordPosition="1976"> “Puerto Rico” (in Spanish) again based on the Spanish intra-language similarity model. This indirect inference of beliefs from the source gazetteers to the target can be modeled by semi-supervised graph propagation (Zhu et al. 2003), where graph nodes are VS U VT, positive labels are entries in the LS gazetteer (e.g., GPE) which we wish to expand to LT, and negative labels are entries in other gazetteers (e.g., PERSON) in LS. The edge weights between same-language nodes wz and wj are given by exp(−u||wz −wj||) where wz is the distributed vector representation of word wz computed by word2vec (Mikolov et al., 2013). The edge weights between node wz E VS and vj E VT are defined 1 if the spelling of these two words are identical and 0 otherwise. Note that this spelling based similarity propagation is still available for language pairs with different writing systems such as English and Chinese, because major NEs (e.g., brand names) are often written in Roman alphabets even in Chinese products. Since the analytical solution to this propagation involves the computation of nxn (n is the number of unlabeled nodes) matrix, we approximated it by running three propagation steps iteratively, namely, LS —* LS, LS —</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>BabelNet: The automatic construction, evaluation</title>
<date>2012</date>
<contexts>
<context position="10272" citStr="Navigli and Ponzetto, 2012" startWordPosition="1636" endWordPosition="1640">g Similarity (external resources independent): This metric works only on languages that share the same alphabet, as it is based on the intuition that most NEs conserve the name’s shape or present minor changes that can be identified using edit distance in closely related languages (we use Levenshtein distance (Levenshtein, 1966)). The two variations of string similarity metrics used are: i) Exact match: simW (wi, wj) = 1 if wi = wj; ii) Edit distance: simW(wi, wj) = 1 if levenshtein-distance(wi, wj) &lt; θ. b) Dictionary-based similarity: We present two similarity metrics using BabelNet synsets (Navigli and Ponzetto, 2012): i) Binary co-occurence: simbinary W (wi, wj) = 1 if wj E synset(wi), where synset(wi) is the set of words in the BabelNet synset of wi; ii) Frequency weighted: Weighted version of the binary similarity that is based on the observation that less frequent words tend to be less reliable in brown clustering: simweighted W (wi, wj) = [log f(wi) + log f(wj)� x simbinary W (wi, wj) where f(w) is the frequency of word w. Unlike String similarity metrics, this metric is not limited to similar languages due to the use of multilingual dictionaries i.e., BabelNet, which covers 271 languages. 3 Gazetteer</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelNet: The automatic construction, evaluation</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sinno Jialin Pan</author>
<author>Qiang Yang</author>
</authors>
<title>A survey on transfer learning.</title>
<date>2010</date>
<journal>IEEE Trans. on Knowl. and Data Eng.,</journal>
<volume>22</volume>
<issue>10</issue>
<contexts>
<context position="2770" citStr="Pan and Yang, 2010" startWordPosition="407" endWordPosition="410">nals expectations as soft constraints. Das and Petrov (2011) projected POS tags from source language types to target language trigarms using graph propagation and used the projected label distribution to train robust POS taggers. Secondly, the availability of such bitext is limited especially for resourcepoor languages and domains, where it is often the case that available resources are moderately-sized monolingual/comparable corpora and small bilingual dictionaries. Instead, we seek a direct transfer approach (Figure 1) to cross-lingual NER (also classified as transductive transfer learning (Pan and Yang, 2010) and closely related to domain adaptation). Specifically, we only assume the availability of comparable corpora and small-sized bilingual dictionaries, and use the same sequential tagging model trained on the source corpus for tagging the target corpus. Direct transfer approaches are extensively studied for cross-lingual dependency parser transfer. For example, Zeman et al. (2008) built a constituent parser using direct transfer between closely related languages, namely, Danish and Swedish. McDonald et al. (2011) trained de-lexicalized dependency parsers in English and then “re-lexicalized” th</context>
</contexts>
<marker>Pan, Yang, 2010</marker>
<rawString>Sinno Jialin Pan and Qiang Yang. 2010. A survey on transfer learning. IEEE Trans. on Knowl. and Data Eng., 22(10):1345–1359, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A universal part-of-speech tagset. arXiv preprint arXiv:1104.2086.</title>
<date>2011</date>
<contexts>
<context position="4839" citStr="Petrov et al., 2011" startWordPosition="717" endWordPosition="720">Framework be proven useful for direct transfer of NER. Therefore, the contributions of this paper is as follows: 1. We show that direct transfer approach for multilingual NER actually works and performs better than the strong MT baseline (Shah et al., 2010), where the system’s output in the source language is simply machine translated into the target language. 2. We explore various non-lexical features, namely, Universal POS tags and Brown cluster mapping, which are deemed effective for multilingual NER transfer. Although brown cluster mapping (T¨ackstr¨om et al., 2012), Universal POS Tagset (Petrov et al., 2011), and re-lexicalization and self training (T¨ackstr¨om et al., 2013) are shown to be effective for direct transfer of dependency parsers, there have been no studies exploring these features for NER transfer. 3. We show that gazetteers can actually be generated only from the source language gazetteers and a comparable corpus, through a technique which we call gazetteer expansion based on semi-supervised graph propagation (Zhu et al., 2003). Gazetteer expansion has been used for various other purposes, including POS tagging (Alexandrescu and Kirchhoff, 2007) and dependency parsers (Durrett et al</context>
<context position="8095" citStr="Petrov et al., 2011" startWordPosition="1252" endWordPosition="1255">n keywords. For instance, often a product size is preceded by certain keywords such as dimension, height or word“size.” In our work we use a manually created list of keywords for two classes Color and Size. Although the keyword list is domain dependent, it is often short and can be easily updated. 5) Universal Part of Speech Tags: Part of Speech (POS) tags have been widely used in many NER systems. However, each language has its own POS tagset that often has limited overlap with other POS languages’ tagsets. Thus, we use a coarse-grained layer of POS tags called Universal POS, as proposed in (Petrov et al., 2011). 6) Token is a unit: A binary feature that is set to true if it matches an entry in the units dictionary (e.g., “cm.”) 7) Gazetteers: Building dictionaries for every LT of interest is expensive; thus, we propose a method, described in Section 3, to generate gazetteers in LT given ones in LS. 8) Brown Clustering (BC): Word representations, especially Brown Clustering (Brown et al., 1992), are used in many NLP tasks and are proven to improve NER performance (Turian et al., 2010). In this work, we use cluster IDs of variable prefix lengths in order to retrieve word similarities on different gran</context>
</contexts>
<marker>Petrov, Das, McDonald, 2011</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011. A universal part-of-speech tagset. arXiv preprint arXiv:1104.2086.</rawString>
</citation>
<citation valid="false">
<authors>
<author>2013a</author>
</authors>
<title>Rakuten shopping.</title>
<note>url = http://www.rakuten.com/.</note>
<marker>2013a, </marker>
<rawString>Rakuten. 2013a. Rakuten shopping. url = http://www.rakuten.com/.</rawString>
</citation>
<citation valid="false">
<booktitle>Rakuten. 2013b. Rakuten spanish. Rakuten. 2013c. Rakuten taiwanese.</booktitle>
<marker></marker>
<rawString>Rakuten. 2013b. Rakuten spanish. Rakuten. 2013c. Rakuten taiwanese.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<contexts>
<context position="15259" citStr="Schmid, 1994" startWordPosition="2494" endWordPosition="2495">ght to generate a strong baseline to compare our results to. Given the language pair (LS, LT), we use Microsoft Bing Translate API to generate LT —* LS translation. Then, we apply LS NER model on the translated text and evaluate by mapping the tagged tokens back to LT using the word alignments generated by Bing Translate. We choose Bing translate as opposed to Google translate due to its freeto-use API that provides word alignment information on the character level. 4.3 Results &amp; Discussion For each studied language we use Stanford CoreNLP (Manning et al., 2014) for EN and ZH, and TreeTagger (Schmid, 1994) for ES to produce 393 Gazetteer in Lt Color Brand Material Model Type Size Micro-Avg EN-Mono 68.45 71.91 50.94 59.78 53.73 45.42 61.12 ES-Baseline 24.23 3.44 13.08 14.51 12.5 6.61 13.79 ES-TL 18.00 9.37 8.05 16.99 18.26 10.64 39.46 ES-GT 38.49 13.31 33.5 2.27 36.43 1.16 30.20 ZH-Baseline 19.16 2.79 11.96 None 9.35 6.34 12.58 ZH-TL 9.36 1.02 1.81 None 17.28 17.74 23.43 Table 2: F-score Results 394 the tokens and the POS tags. However, we apply extra processing steps to the tokenizer due to the nature of the domain’s data (e.g., avoid tokenizing models instances), in addition to normalizing URL</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rushin Shah</author>
<author>Bo Lin</author>
<author>Anatole Gershman</author>
<author>Robert Frederking</author>
</authors>
<title>Synergy: A named entity recognition system for resource-scarce languages such as swahili using online machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Second Workshop on African Language Technology (AfLaT</booktitle>
<pages>21--26</pages>
<contexts>
<context position="4476" citStr="Shah et al., 2010" startWordPosition="662" endWordPosition="665"> (Universal POS tags, unsupervised clustering, etc.) are yet to 390 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 390–396, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Figure 1: System Framework be proven useful for direct transfer of NER. Therefore, the contributions of this paper is as follows: 1. We show that direct transfer approach for multilingual NER actually works and performs better than the strong MT baseline (Shah et al., 2010), where the system’s output in the source language is simply machine translated into the target language. 2. We explore various non-lexical features, namely, Universal POS tags and Brown cluster mapping, which are deemed effective for multilingual NER transfer. Although brown cluster mapping (T¨ackstr¨om et al., 2012), Universal POS Tagset (Petrov et al., 2011), and re-lexicalization and self training (T¨ackstr¨om et al., 2013) are shown to be effective for direct transfer of dependency parsers, there have been no studies exploring these features for NER transfer. 3. We show that gazetteers ca</context>
</contexts>
<marker>Shah, Lin, Gershman, Frederking, 2010</marker>
<rawString>Rushin Shah, Bo Lin, Anatole Gershman, and Robert Frederking. 2010. Synergy: A named entity recognition system for resource-scarce languages such as swahili using online machine translation. In Proceedings of the Second Workshop on African Language Technology (AfLaT 2010), pages 21–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Ryan McDonald</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Cross-lingual word clusters for direct transfer of linguistic structure.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>477--487</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<marker>T¨ackstr¨om, McDonald, Uszkoreit, 2012</marker>
<rawString>Oscar T¨ackstr¨om, Ryan McDonald, and Jakob Uszkoreit. 2012. Cross-lingual word clusters for direct transfer of linguistic structure. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 477– 487, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Target language adaptation of discriminative transfer parsers.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1061--1071</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<marker>T¨ackstr¨om, McDonald, Nivre, 2013</marker>
<rawString>Oscar T¨ackstr¨om, Ryan McDonald, and Joakim Nivre. 2013. Target language adaptation of discriminative transfer parsers. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1061–1071, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8577" citStr="Turian et al., 2010" startWordPosition="1336" endWordPosition="1339">h other POS languages’ tagsets. Thus, we use a coarse-grained layer of POS tags called Universal POS, as proposed in (Petrov et al., 2011). 6) Token is a unit: A binary feature that is set to true if it matches an entry in the units dictionary (e.g., “cm.”) 7) Gazetteers: Building dictionaries for every LT of interest is expensive; thus, we propose a method, described in Section 3, to generate gazetteers in LT given ones in LS. 8) Brown Clustering (BC): Word representations, especially Brown Clustering (Brown et al., 1992), are used in many NLP tasks and are proven to improve NER performance (Turian et al., 2010). In this work, we use cluster IDs of variable prefix lengths in order to retrieve word similarities on different granularity levels. 2.2 Multilingual NER in Target Language Our goal is to transfer each feature from LS to LT space. The main challenge resides in transferring features 7 and 8 without the use of external resources and parallel data for every target language. 2.2.1 Brown Clustering Mapping Given i) Vocabulary in the source/target languages VS = {wS1 , wS2 , ..., wS NS} and VT = {wT1 , wT2 , ..., vTNT }; ii) The output of brown clustering on LS and LT: CS = {cS1 , ..., cSKS} and CT</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>Cross-lingual projected expectation regularization for weakly supervised learning.</title>
<date>2014</date>
<contexts>
<context position="1311" citStr="Wang and Manning, 2014" startWordPosition="188" endWordPosition="191">ine, where the English model is transferred to two languages: Spanish and Chinese. 1 Introduction Named Entity Recognition (NER) is usually solved by a supervised learning approach, where sequential labeling models are trained from a large amount of manually annotated corpora. However, such rich annotated data only exist for resourcerich languages such as English, and building NER systems for the majority of resource-poor languages, or specific domains in any languages, still poses a great challenge. Annotation projection through parallel text (Yarowsky et al., 2001), (Das and Petrov, 2011), (Wang and Manning, 2014) has been traditionally used to overcome this issue, where the annotated tags in the source (resource-rich) language are projected via word-aligned bilingual parallel text (bitext) and used to train sequential labeling models in the (resource-poor) target language. However, this could lead to two issues: firstly, word Phis work has been performed while the authors were at Rakuten Institute of Technology, New York. The authors would like to thank Prof. Satoshi Sekine at New York University and other members of Rakuten Institute of Technology for their support during the project. Masato Hagiwara</context>
</contexts>
<marker>Wang, Manning, 2014</marker>
<rawString>Mengqiu Wang and Christopher D Manning. 2014. Cross-lingual projected expectation regularization for weakly supervised learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
<author>Richard Wicentowski</author>
</authors>
<title>Inducing multilingual text analysis tools via robust projection across aligned corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of the First International Conference on Human Language Technology Research, HLT ’01,</booktitle>
<pages>1--8</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1261" citStr="Yarowsky et al., 2001" startWordPosition="180" endWordPosition="183">hown that our approaches beat the strong MT baseline, where the English model is transferred to two languages: Spanish and Chinese. 1 Introduction Named Entity Recognition (NER) is usually solved by a supervised learning approach, where sequential labeling models are trained from a large amount of manually annotated corpora. However, such rich annotated data only exist for resourcerich languages such as English, and building NER systems for the majority of resource-poor languages, or specific domains in any languages, still poses a great challenge. Annotation projection through parallel text (Yarowsky et al., 2001), (Das and Petrov, 2011), (Wang and Manning, 2014) has been traditionally used to overcome this issue, where the annotated tags in the source (resource-rich) language are projected via word-aligned bilingual parallel text (bitext) and used to train sequential labeling models in the (resource-poor) target language. However, this could lead to two issues: firstly, word Phis work has been performed while the authors were at Rakuten Institute of Technology, New York. The authors would like to thank Prof. Satoshi Sekine at New York University and other members of Rakuten Institute of Technology for</context>
</contexts>
<marker>Yarowsky, Ngai, Wicentowski, 2001</marker>
<rawString>David Yarowsky, Grace Ngai, and Richard Wicentowski. 2001. Inducing multilingual text analysis tools via robust projection across aligned corpora. In Proceedings of the First International Conference on Human Language Technology Research, HLT ’01, pages 1–8, Stroudsburg, PA, USA. Asso395 ciation for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Zeman</author>
<author>Univerzita Karlova</author>
<author>Philip Resnik</author>
</authors>
<title>Cross-language parser adaptation between related languages. In</title>
<date>2008</date>
<booktitle>In IJCNLP-08 Workshop on NLP for Less Privileged Languages,</booktitle>
<pages>35--42</pages>
<contexts>
<context position="3153" citStr="Zeman et al. (2008)" startWordPosition="461" endWordPosition="464">es are moderately-sized monolingual/comparable corpora and small bilingual dictionaries. Instead, we seek a direct transfer approach (Figure 1) to cross-lingual NER (also classified as transductive transfer learning (Pan and Yang, 2010) and closely related to domain adaptation). Specifically, we only assume the availability of comparable corpora and small-sized bilingual dictionaries, and use the same sequential tagging model trained on the source corpus for tagging the target corpus. Direct transfer approaches are extensively studied for cross-lingual dependency parser transfer. For example, Zeman et al. (2008) built a constituent parser using direct transfer between closely related languages, namely, Danish and Swedish. McDonald et al. (2011) trained de-lexicalized dependency parsers in English and then “re-lexicalized” the parser. However, crosslingual transfer of named entity taggers have not been studied enough, and this paper, to the best of the authors’ knowledge, is the first to apply direct transfer learning to NER. Transfer of NER taggers poses a difficult challenge that is different from syntax transfer: most of the past work deals with de-lexicalized parsers, yet one of the most important</context>
</contexts>
<marker>Zeman, Karlova, Resnik, 2008</marker>
<rawString>Daniel Zeman, Univerzita Karlova, and Philip Resnik. 2008. Cross-language parser adaptation between related languages. In In IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 35–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
<author>John Lafferty</author>
</authors>
<title>Semi-supervised learning using gaussian fields and harmonic functions.</title>
<date>2003</date>
<booktitle>In IN ICML,</booktitle>
<pages>912--919</pages>
<contexts>
<context position="5281" citStr="Zhu et al., 2003" startWordPosition="787" endWordPosition="790">luster mapping, which are deemed effective for multilingual NER transfer. Although brown cluster mapping (T¨ackstr¨om et al., 2012), Universal POS Tagset (Petrov et al., 2011), and re-lexicalization and self training (T¨ackstr¨om et al., 2013) are shown to be effective for direct transfer of dependency parsers, there have been no studies exploring these features for NER transfer. 3. We show that gazetteers can actually be generated only from the source language gazetteers and a comparable corpus, through a technique which we call gazetteer expansion based on semi-supervised graph propagation (Zhu et al., 2003). Gazetteer expansion has been used for various other purposes, including POS tagging (Alexandrescu and Kirchhoff, 2007) and dependency parsers (Durrett et al., 2012). 2 Approach In this paper we propose a direct transfer learning approach to train NER taggers in a multilingual setting. Our goal is to identify named entities in a target language LT, given solely annotated data in the source language LS. Previous approaches rely on parallel data to transfer the knowledge from one language to another. However, parallel data is very expensive to construct and not available for all language pairs </context>
<context position="11843" citStr="Zhu et al. 2003" startWordPosition="1905" endWordPosition="1908">here is no direct link between “New York” and “Nueva York,” you can infer that “Puerto Rico” (in English) is similar to “New York” based on some intra-language semantic similarity model, then “Puerto Rico” is actually 392 S* c = arg max cSECS 1 ∑ simC(ct, cs) = |cS ||cT |s w EcS,wTEcT Figure 2: Gazeteer expansion identical in both languages, then finally “Nueva York” is similar to “Puerto Rico” (in Spanish) again based on the Spanish intra-language similarity model. This indirect inference of beliefs from the source gazetteers to the target can be modeled by semi-supervised graph propagation (Zhu et al. 2003), where graph nodes are VS U VT, positive labels are entries in the LS gazetteer (e.g., GPE) which we wish to expand to LT, and negative labels are entries in other gazetteers (e.g., PERSON) in LS. The edge weights between same-language nodes wz and wj are given by exp(−u||wz −wj||) where wz is the distributed vector representation of word wz computed by word2vec (Mikolov et al., 2013). The edge weights between node wz E VS and vj E VT are defined 1 if the spelling of these two words are identical and 0 otherwise. Note that this spelling based similarity propagation is still available for lang</context>
</contexts>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. 2003. Semi-supervised learning using gaussian fields and harmonic functions. In IN ICML, pages 912–919.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>