<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004800">
<note confidence="0.539178333333333">
SENSEVAL-3: Third International Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, Barcelona, Spain, July 2004
Association for Computational Linguistics
</note>
<title confidence="0.998711">
A First Evaluation of Logic Form Identification Systems
</title>
<author confidence="0.996744">
Vasile Rus
</author>
<affiliation confidence="0.999158">
Department of Computer Science
Indiana University
</affiliation>
<address confidence="0.986811">
South Bend, IN 46634
</address>
<email confidence="0.999541">
vasile@cs.iusb.edu
</email>
<sectionHeader confidence="0.99565" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999826181818182">
This paper presents a first experience with evalu-
ating sytems that address the issue of Logic Form
Identification (LFi). A Gold Standard approach was
used in which experts provide solutions to test data.
The expert solutions, the gold standard, are then
compared against outputs from participanting sys-
tems and different metrics observed. We proposed
a few novel metrics, including precision and recall,
that are further used to provide comparative results.
The test data included 4155 arguments and 2398
predicates grouped in 300 sentences.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999641421052632">
The goal of a Logic Form Identification (LFi) task is
to evaluate the performance of different methods ad-
dressing the issue of LFi. The Logic Form (LF) that
we use is a flat, scope-free first order logic represen-
tation that embeds lexical and syntactic information.
Given a set of English sentences, participating sys-
tems were supposed to return the sentences in Logic
Form as in the example below.
Input: The Earth provides the food we eat every
day.
Output: Earth:n (x1) provide:v (e1,
x1, x2) food:n (x2) we(x3)
eat:v (e2, x3, x2; x4) day:n (x4)
The general approach adopted for evaluation was a
gold standard approach in which the test data is first
correctly mapped onto its corresponding LF by a
team of experts and then this correct LF is automat-
ically compared against outputs provided by partic-
ipating sytems.
</bodyText>
<sectionHeader confidence="0.992697" genericHeader="introduction">
2 General Guidelines
</sectionHeader>
<bodyText confidence="0.989512361111111">
The Logic Form of a sentence is the conjunction
of individual predicates, where the relationships
among them are expressed via shared arguments.
Predicates are generated for all content words such
as nouns, verb, adjectives and adverbs. Pronouns
are treated as independent nouns. Prepositions and
conjunctions are also mapped onto predicates that
capture the relation between the prepositional ob-
ject and the constituent to which it is attached and
the relation among the coordinated entities, respec-
tively.
There are two types of arguments: e - for events,
x - for entities. For the sentence presented above
we have two events - e1, e2 corresponding to each
verb/action in the sentence and four entities - x1, x2,
x3, x4 corresponding to the heads of the base noun
phrases (NP). Each verb predicate has the second ar-
gument set to the corresponding logical subject and
the third argument to its direct object. The remain-
ing slots for a verb predicate are filled with the ar-
guments of their indirect and prepositional objects.
In the example presented above the predicate eat
has arguments after ; (semicolon) which indicates
its adjuncts. The distinction between complements
and adjuncts is a novelty to the LF proposed by the
authors in this paper. For this first trial, we did
not make the distinction between the two and thus
the accepted representation would be eat:v (e2,
x3, x2, x4) - see below.
Output: Earth:n (x1) provide:v (e1,
x1, x2) food:n (x2) we(x3)
eat:v (e2, x3, x2, x4) day:n (x4)
Predicates are formed by the concatenation of the
base form of the word and its lexical category as
encoded in WordNet (since only nouns, verbs, ad-
jectives and adverbs are encoded in WordNet, only
predicates for those lexical categories have the cat-
egory attached to the predicate).
To ease the task, the notation was relaxed by
adopting few simplifications similar, to some ex-
tent, to the simplications in (Moldovan and Rus,
2001): determiners, plurals, negation, auxiliaries
and verb tenses, punctuation are ingnored. Colloca-
tions, such as New York, should be considered a sin-
gle predicate as well as verbs having particles (e.g.
give up). For cases when an argument is underspec-
ified, such as the logical subject in Jim was told to
say something, an artificial argument should be gen-
erated.
The advantages of the LF notation are mantfold:
it allows a simple syntax/semantics interface
it is user friendly
it has positional syntactic arguments that ease
other NLP tasks such as textual interpretationa
and textual inference
if predicates are disambiguated with respect to
a general ontology such as WordNet it leads to
concept predicates
it is easily customizable (for example to distin-
guish between arguments and adjuncts)
For details about the principles of Logic Forms
read Chapter 2 in (Rus, 2002), (Rus and Moldovan,
2002) and (Hobbs, 1986). The LF notation
proposed for the LFi competition is novel, and
different from the one described in the pre-
vious references since it distinguishes between
complements and adjuncts among other differ-
ences. A web page for the LFi task is available
at http://www.cs.iusb.edu/ vasile/logic/indexLF.html
and a discussion group, called logicform, was
opened at yahoo.groups.com which can also be con-
sulted.
</bodyText>
<sectionHeader confidence="0.834354" genericHeader="method">
3 Test Data
</sectionHeader>
<bodyText confidence="0.999987904761905">
The test data was compiled so that the impact of ex-
ternal tools that different sytems might use in the
LF identification process be minimal. For example,
it is well-known that the accuracy of automatic syn-
tactic parsing drops drastically for sentences larger
than 40 words and thus we kept the size of the col-
lected sentences below the 40 words threshold. The
average sentence size in the test data is 9.89 words.
Special attention was paid to covering linguis-
tic phenomena such as: coordination, compound
nouns, ditransitives, multiword expressions (give
up, as well as, etc.), relative clauses and others.
Different sources were used to look up such cases:
Treebank, WordNet and the web.
The size of the test set (4155 arguments, 2398
predicates, 300 sentences) allows a better evalu-
ation of the vertical scalability (coverage of as
many linguistics problems as possible) of sytems
rather than their horizontal scalability (handling
large data sets without significant deterioration of
performance displayed on small sets).
</bodyText>
<sectionHeader confidence="0.951497" genericHeader="method">
4 Annotation Guidelines
</sectionHeader>
<bodyText confidence="0.99989625">
The annotation part is the most critical part of any
evaluation exercise. For the Logic Form Identifica-
tion task the following steps were applied to obtain
the correct LF for the test data:
</bodyText>
<listItem confidence="0.806923368421053">
1. logic forms for the test data were automatically
obtained using an extended version of the LF
derivation engine developed in (Rus, 2002) for
LFi of WordNet glosses. As part of this step,
sentences were preprocessed: tokenized (sepa-
rating punctuation from words) using the Penn
Treebank guidelines, tagged with Brill’s tagger
(Brill, 1992) and then parsed with Collins’ sta-
tistical parser (Collins, 1996).
2. a first manual checking of the previously gen-
erated LF was done.
3. a second manual checking was done by another
annotator.
4. quality assurance of the previous steps was per-
formed by individual annotators by checking
specific cases (ditransitives, relative pronouns,
etc.) with much emphasis on consistency.
5. annotators agreement was done with a human
moderator solving conflicting cases.
</listItem>
<sectionHeader confidence="0.993201" genericHeader="method">
5 Metrics
</sectionHeader>
<bodyText confidence="0.998468032258064">
Two performance measures to evaluate Logic Form
Identification methods were developed by Rus in
(Rus and Moldovan, 2002) for the particular task of
LFi for WordNet glosses (the definitions of concepts
are shorter than regular sentences in terms of num-
ber of words, etc.). Each measure has advantages in
some context.
Predicate level performance is defined as the
number of predicates with correct arguments di-
vided by the total number of predicates. This mea-
sure focuses on the derivation method, though at
a coarse-grained level because it does not capture
the capability of a method to successfully identify a
specific argument, e.g. the subject of a verb.
Gloss level performance is the number of entire
glosses correctly transformed into logic forms di-
vided by the total number of glosses attempted. This
measure catches contextual capabilities of a method
in that it gives an idea of how well a method per-
forms at gloss level. It is a more appropriate mea-
sure when one tries to see the impact of using full
glosses in logic forms to applications such as plan-
ning. This measure is specific to the particular task
of LFi for concept definitions and thus is not suited
for general open text tasks.
Let us consider the following gloss from Word-
Net:
Abbey is a convent ruled by an abbess.
and let us suppose that some system, say Sys is able
to generate the following logic form (please note
that the subject of rule event is missing):
</bodyText>
<equation confidence="0.999485">
abbey(x1) &amp; be(e1, x1, x2) &amp;
convent(x2) &amp; rule(e2, , x2) &amp;
by(e2, x3) &amp; abbess(x3)
</equation>
<bodyText confidence="0.9999645">
Since one of the arguments is missing the predi-
cate level performance is 5/6 (there are 6 predicates
and for five of them the system generated all the ar-
guments correctly) and the gloss level performance
is 0/1 (this measure awards cases where all the pred-
icates in the statement have all their arguments cor-
rectly assigned).
None of the two measures can distinguish be-
tween two systems, where one misses the subject of
the rule event and the other misses both the subject
and object (both systems will miss one predicate).
We propose two new, finer metrics in the next sec-
tion, that are more suitable for a less restrictive LFi
task: precision and recall. Both precision and re-
call can be defined at argument and predicate level,
respectively.
</bodyText>
<subsectionHeader confidence="0.994032">
5.1 Argument Level
</subsectionHeader>
<bodyText confidence="0.9999945">
We define Precision at argument level as the num-
ber of correctly identified arguments divided by the
number of all identified arguments. Recall at argu-
ment level is the number of correctly identified ar-
guments divided by the number of arguments that
were supposed to be identified.
</bodyText>
<subsectionHeader confidence="0.998623">
5.2 Predicate Level
</subsectionHeader>
<bodyText confidence="0.9996661">
Precision at predicate level is the number of cor-
rectly and fully identified predicates (with ALL ar-
guments correctly identified) divided by the number
of all attempted predicates. Recall at predicate level
is the number of correctly and fully identified pred-
icates (with ALL arguments correctly identified) di-
vided by the number of all predicates that were sup-
posed to be identified.
Let us suppose that some system outputs the fol-
lowing logic form for the above example:
</bodyText>
<equation confidence="0.86032925">
Sample Output: Earth:n (x1)
provide:v (e1, x1, x2) food:n (x2)
we(x3) eat:v (e2, x3, x4)
day:n (x4)
Correct Output: Earth:n (x1)
provide:v (e1, x1, x2) food:n (x2)
we(x3) eat:v (e2, x3, x2, x4)
day:n (x4)
</equation>
<bodyText confidence="0.9834816">
where x4 is incorrectly indentified as the direct ob-
ject of eating event. In the correct output there are
11 slots to be filled and the predicate eat should have
4 arguments. The previously defined measures for
the sample output are given in Table 1:
</bodyText>
<table confidence="0.999054">
Metric / Level Argument Predicate
Precision 9/10 5/6
Recall 9/11 5/6
</table>
<tableCaption confidence="0.942107">
Table 1: Examples of Precision and Recall at argu-
ment and predicate level.
</tableCaption>
<bodyText confidence="0.998496">
In addition, we report a more global measure
called exact sentence which is defined as the num-
ber of sentences whose logic form was fully identi-
fied (all predicates and arguments correctly found)
divided by the number of sentences attempted. This
is similar to gloss level performance measure pre-
sented before. We proposed and computed several
variants for it which are described below.
</bodyText>
<listItem confidence="0.561112285714286">
Sentence-Argument (Sent-A): How many sen-
tences have ALL arguments correctly detected out
of all attempted sentences.
Sentence-Predicate (Sent-P): How many sen-
tences have ALL predicates correctly detected out
of all attempted sentences.
Sentence-Argument-Predicate Sent-AP: How
many sentences have ALL arguments correctly de-
tected out of sentences which have ALL predicates
correctly detected
Sentence-Argument-Predicate-Sentences Sent-
APSent: How many sentences have ALL arguments
and ALL predicates correctly detected out of all
attempted sentences.
</listItem>
<sectionHeader confidence="0.977648" genericHeader="method">
6 Extra Resources
</sectionHeader>
<bodyText confidence="0.9999445">
A package of trial data was provided to interested
participants. The trial package contains two data
files: (1) English sentences and (2) their corre-
sponding logic form. A software evaluator was
available for download on the web page of the
task. We compiled a dictionary of collocations from
WordNet which was also freely available for down-
load. It includes 62,611 collocations.
</bodyText>
<sectionHeader confidence="0.990027" genericHeader="method">
7 Submission Format
</sectionHeader>
<bodyText confidence="0.978010666666667">
Each team was supposed to submit a file containing
on each line the answer to a input sentence using the
following pattern:
</bodyText>
<footnote confidence="0.552862666666667">
InstitutionShortName Y000 Sentence# Score ::
Logic Form
Here is an example:
</footnote>
<table confidence="0.999425181818182">
Argument Level Predicate Level
Team / Metric Precision Recall Precision Recall
University of Amsterdam (ams) 0.729 0.691 0.819 0.783
Language Computer Corporation (lcc) 0.776 0.777 0.876 0.908
MITRE (mitre) 0.734 0.659 0.839 0.781
University of Sydney (syd) 0.763 0.655 0.839 0.849
Sent-A Sent-P Sent-AP Sent-APSent
University of Amsterdam (ams) 0.256 0.320 0.510 0.163
Language Computer Corporation (lcc) 0.236 0.516 0.419 0.216
MITRE (mitre) 0.266 0.213 0.406 0.086
University of Sydney (syd) 0.160 0.353 0.386 0.136
</table>
<tableCaption confidence="0.999335">
Table 2: Comparative view of valid submissions.
</tableCaption>
<equation confidence="0.4066235">
IUSB Y000 3 89.7 :: nn: (x1, x2, x3) logic:n
(x2) form:n (x3)
</equation>
<bodyText confidence="0.9955965">
The field Y000 was generated as is, for all lines.
It will be used in future trials.
</bodyText>
<sectionHeader confidence="0.999911" genericHeader="evaluation">
8 Results
</sectionHeader>
<bodyText confidence="0.999990571428571">
Initially, there were 27 teams registered to partic-
ipate in the Logic Form Identification task and 6
submissions were received by the deadline. One of
the submissions was discarded since the file con-
tained no valid data and another one was not in-
cluded in the comparative results shown in Table 2
since it used manual parsing (parsing is not a nec-
essary step in obtaining the LF). The part of speech
info attached to some predicates was ignored when
computing the scores. We plan to use it in further
trials.
If one looks at the results in the table one may no-
tice their consistency. At Argument level precision
and recall range from 0.729 to 0.776 and from 0.655
to 0.777, respectively. The same trend can be ob-
served at Predicate level (the results are slighlty bet-
ter). At a more coarse-grain level (Sentence level)
the results vary more but still one can distinguish a
certain degree of consistency: the Sent-A measure
ranges from 0.160 to 0.256 and the Sent-AP mea-
sure varies from 0.386 to 0.510.
</bodyText>
<sectionHeader confidence="0.998404" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999991631578948">
In the first attempt to systematically evaluate LFi
systems we managed to provide a gold standard,
a first software evaluator and proposed few perfor-
mance metrics that can be used by researchers in the
community to further study their approaches.
Among the drawbacks, it is worth mentioning the
lack of training data which we plan to offer in the
future.
The results reported by different systems consti-
tute a lower bound of their approaches since the test
data comprised raw sentences and thus the reported
performances include errors coming from tokeniza-
tion, part of speech tagging and parsing, wherever
parsing was used.
Due to a tight schedule it was not possible to an-
alyze the different approaches adopted by different
systems but we hope the ACL meeting will provide
the necessary background information and discus-
sions to foster the development of such a study.
</bodyText>
<sectionHeader confidence="0.999239" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99975296">
Eric Brill. 1992. A simple rule-based part of speech
tagger. In Proceedings of the Third Conference
on Applied Natural Language Processing, 152-
155.
Michael John Collins. 1996. A new statistical
parser based on bigram lexical dependencies. In
Arivind Joshi and Martha Palmer, editors, Pro-
ceedings of the Thirty-Fourth Annual Meeting of
the Association for Computational Linguistics,
pages 184–191, San Francisco. Morgan Kauf-
mann Publishers.
Jerry R. Hobbs. 1986. Overview of the TACITUS
project. Computational Linguistics, 12(3).
Dan I. Moldovan and Vasile Rus. 2001. Logic
Form transformation of wordNet and its Appli-
cability to question answering. In Proceedings of
ACL 2001, Toulouse, France, 6-11 July. Associa-
tion for Computational Linguistics.
Vasile Rus and Dan Moldovan. 2002. High preci-
sion logic form transformation. In International
Journal for Tools with Artificial Intelligence.
IEEE Computer Society, IEEE Press, September.
Vasile Rus. 2002. Logic Form for WordNet
Glosses and Applications. Phd thesis, Southern
Methodist University, May.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.411405">
<note confidence="0.6826465">SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July 2004</note>
<title confidence="0.943336">Association for Computational Linguistics A First Evaluation of Logic Form Identification Systems</title>
<author confidence="0.98527">Vasile</author>
<affiliation confidence="0.987655">Department of Computer Indiana</affiliation>
<address confidence="0.945763">South Bend, IN</address>
<email confidence="0.99908">vasile@cs.iusb.edu</email>
<abstract confidence="0.97847575">This paper presents a first experience with evaluating sytems that address the issue of Logic Form Identification (LFi). A Gold Standard approach was used in which experts provide solutions to test data. The expert solutions, the gold standard, are then compared against outputs from participanting systems and different metrics observed. We proposed a few novel metrics, including precision and recall, that are further used to provide comparative results. The test data included 4155 arguments and 2398 predicates grouped in 300 sentences.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>A simple rule-based part of speech tagger.</title>
<date>1992</date>
<booktitle>In Proceedings of the Third Conference on Applied Natural Language Processing,</booktitle>
<pages>152--155</pages>
<contexts>
<context position="6579" citStr="Brill, 1992" startWordPosition="1054" endWordPosition="1055">t deterioration of performance displayed on small sets). 4 Annotation Guidelines The annotation part is the most critical part of any evaluation exercise. For the Logic Form Identification task the following steps were applied to obtain the correct LF for the test data: 1. logic forms for the test data were automatically obtained using an extended version of the LF derivation engine developed in (Rus, 2002) for LFi of WordNet glosses. As part of this step, sentences were preprocessed: tokenized (separating punctuation from words) using the Penn Treebank guidelines, tagged with Brill’s tagger (Brill, 1992) and then parsed with Collins’ statistical parser (Collins, 1996). 2. a first manual checking of the previously generated LF was done. 3. a second manual checking was done by another annotator. 4. quality assurance of the previous steps was performed by individual annotators by checking specific cases (ditransitives, relative pronouns, etc.) with much emphasis on consistency. 5. annotators agreement was done with a human moderator solving conflicting cases. 5 Metrics Two performance measures to evaluate Logic Form Identification methods were developed by Rus in (Rus and Moldovan, 2002) for the</context>
</contexts>
<marker>Brill, 1992</marker>
<rawString>Eric Brill. 1992. A simple rule-based part of speech tagger. In Proceedings of the Third Conference on Applied Natural Language Processing, 152-155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael John Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Arivind Joshi and Martha Palmer, editors, Proceedings of the Thirty-Fourth Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>184--191</pages>
<publisher>Morgan Kaufmann Publishers.</publisher>
<location>San Francisco.</location>
<contexts>
<context position="6644" citStr="Collins, 1996" startWordPosition="1064" endWordPosition="1065">tation Guidelines The annotation part is the most critical part of any evaluation exercise. For the Logic Form Identification task the following steps were applied to obtain the correct LF for the test data: 1. logic forms for the test data were automatically obtained using an extended version of the LF derivation engine developed in (Rus, 2002) for LFi of WordNet glosses. As part of this step, sentences were preprocessed: tokenized (separating punctuation from words) using the Penn Treebank guidelines, tagged with Brill’s tagger (Brill, 1992) and then parsed with Collins’ statistical parser (Collins, 1996). 2. a first manual checking of the previously generated LF was done. 3. a second manual checking was done by another annotator. 4. quality assurance of the previous steps was performed by individual annotators by checking specific cases (ditransitives, relative pronouns, etc.) with much emphasis on consistency. 5. annotators agreement was done with a human moderator solving conflicting cases. 5 Metrics Two performance measures to evaluate Logic Form Identification methods were developed by Rus in (Rus and Moldovan, 2002) for the particular task of LFi for WordNet glosses (the definitions of c</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Michael John Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Arivind Joshi and Martha Palmer, editors, Proceedings of the Thirty-Fourth Annual Meeting of the Association for Computational Linguistics, pages 184–191, San Francisco. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<date>1986</date>
<journal>Overview of the TACITUS project. Computational Linguistics,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="4584" citStr="Hobbs, 1986" startWordPosition="739" endWordPosition="740">say something, an artificial argument should be generated. The advantages of the LF notation are mantfold: it allows a simple syntax/semantics interface it is user friendly it has positional syntactic arguments that ease other NLP tasks such as textual interpretationa and textual inference if predicates are disambiguated with respect to a general ontology such as WordNet it leads to concept predicates it is easily customizable (for example to distinguish between arguments and adjuncts) For details about the principles of Logic Forms read Chapter 2 in (Rus, 2002), (Rus and Moldovan, 2002) and (Hobbs, 1986). The LF notation proposed for the LFi competition is novel, and different from the one described in the previous references since it distinguishes between complements and adjuncts among other differences. A web page for the LFi task is available at http://www.cs.iusb.edu/ vasile/logic/indexLF.html and a discussion group, called logicform, was opened at yahoo.groups.com which can also be consulted. 3 Test Data The test data was compiled so that the impact of external tools that different sytems might use in the LF identification process be minimal. For example, it is well-known that the accura</context>
</contexts>
<marker>Hobbs, 1986</marker>
<rawString>Jerry R. Hobbs. 1986. Overview of the TACITUS project. Computational Linguistics, 12(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan I Moldovan</author>
<author>Vasile Rus</author>
</authors>
<title>Logic Form transformation of wordNet and its Applicability to question answering.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL 2001,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Toulouse,</location>
<contexts>
<context position="3668" citStr="Moldovan and Rus, 2001" startWordPosition="591" endWordPosition="594">ween the two and thus the accepted representation would be eat:v (e2, x3, x2, x4) - see below. Output: Earth:n (x1) provide:v (e1, x1, x2) food:n (x2) we(x3) eat:v (e2, x3, x2, x4) day:n (x4) Predicates are formed by the concatenation of the base form of the word and its lexical category as encoded in WordNet (since only nouns, verbs, adjectives and adverbs are encoded in WordNet, only predicates for those lexical categories have the category attached to the predicate). To ease the task, the notation was relaxed by adopting few simplifications similar, to some extent, to the simplications in (Moldovan and Rus, 2001): determiners, plurals, negation, auxiliaries and verb tenses, punctuation are ingnored. Collocations, such as New York, should be considered a single predicate as well as verbs having particles (e.g. give up). For cases when an argument is underspecified, such as the logical subject in Jim was told to say something, an artificial argument should be generated. The advantages of the LF notation are mantfold: it allows a simple syntax/semantics interface it is user friendly it has positional syntactic arguments that ease other NLP tasks such as textual interpretationa and textual inference if pr</context>
</contexts>
<marker>Moldovan, Rus, 2001</marker>
<rawString>Dan I. Moldovan and Vasile Rus. 2001. Logic Form transformation of wordNet and its Applicability to question answering. In Proceedings of ACL 2001, Toulouse, France, 6-11 July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasile Rus</author>
<author>Dan Moldovan</author>
</authors>
<title>High precision logic form transformation.</title>
<date>2002</date>
<booktitle>In International Journal for Tools with Artificial Intelligence. IEEE</booktitle>
<publisher>Computer Society, IEEE Press,</publisher>
<contexts>
<context position="4566" citStr="Rus and Moldovan, 2002" startWordPosition="734" endWordPosition="737">l subject in Jim was told to say something, an artificial argument should be generated. The advantages of the LF notation are mantfold: it allows a simple syntax/semantics interface it is user friendly it has positional syntactic arguments that ease other NLP tasks such as textual interpretationa and textual inference if predicates are disambiguated with respect to a general ontology such as WordNet it leads to concept predicates it is easily customizable (for example to distinguish between arguments and adjuncts) For details about the principles of Logic Forms read Chapter 2 in (Rus, 2002), (Rus and Moldovan, 2002) and (Hobbs, 1986). The LF notation proposed for the LFi competition is novel, and different from the one described in the previous references since it distinguishes between complements and adjuncts among other differences. A web page for the LFi task is available at http://www.cs.iusb.edu/ vasile/logic/indexLF.html and a discussion group, called logicform, was opened at yahoo.groups.com which can also be consulted. 3 Test Data The test data was compiled so that the impact of external tools that different sytems might use in the LF identification process be minimal. For example, it is well-kno</context>
<context position="7171" citStr="Rus and Moldovan, 2002" startWordPosition="1143" endWordPosition="1146">th Brill’s tagger (Brill, 1992) and then parsed with Collins’ statistical parser (Collins, 1996). 2. a first manual checking of the previously generated LF was done. 3. a second manual checking was done by another annotator. 4. quality assurance of the previous steps was performed by individual annotators by checking specific cases (ditransitives, relative pronouns, etc.) with much emphasis on consistency. 5. annotators agreement was done with a human moderator solving conflicting cases. 5 Metrics Two performance measures to evaluate Logic Form Identification methods were developed by Rus in (Rus and Moldovan, 2002) for the particular task of LFi for WordNet glosses (the definitions of concepts are shorter than regular sentences in terms of number of words, etc.). Each measure has advantages in some context. Predicate level performance is defined as the number of predicates with correct arguments divided by the total number of predicates. This measure focuses on the derivation method, though at a coarse-grained level because it does not capture the capability of a method to successfully identify a specific argument, e.g. the subject of a verb. Gloss level performance is the number of entire glosses corre</context>
</contexts>
<marker>Rus, Moldovan, 2002</marker>
<rawString>Vasile Rus and Dan Moldovan. 2002. High precision logic form transformation. In International Journal for Tools with Artificial Intelligence. IEEE Computer Society, IEEE Press, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasile Rus</author>
</authors>
<title>Logic Form for WordNet Glosses and Applications. Phd thesis,</title>
<date>2002</date>
<institution>Southern Methodist University,</institution>
<contexts>
<context position="4540" citStr="Rus, 2002" startWordPosition="732" endWordPosition="733">as the logical subject in Jim was told to say something, an artificial argument should be generated. The advantages of the LF notation are mantfold: it allows a simple syntax/semantics interface it is user friendly it has positional syntactic arguments that ease other NLP tasks such as textual interpretationa and textual inference if predicates are disambiguated with respect to a general ontology such as WordNet it leads to concept predicates it is easily customizable (for example to distinguish between arguments and adjuncts) For details about the principles of Logic Forms read Chapter 2 in (Rus, 2002), (Rus and Moldovan, 2002) and (Hobbs, 1986). The LF notation proposed for the LFi competition is novel, and different from the one described in the previous references since it distinguishes between complements and adjuncts among other differences. A web page for the LFi task is available at http://www.cs.iusb.edu/ vasile/logic/indexLF.html and a discussion group, called logicform, was opened at yahoo.groups.com which can also be consulted. 3 Test Data The test data was compiled so that the impact of external tools that different sytems might use in the LF identification process be minimal. F</context>
<context position="6377" citStr="Rus, 2002" startWordPosition="1024" endWordPosition="1025">s a better evaluation of the vertical scalability (coverage of as many linguistics problems as possible) of sytems rather than their horizontal scalability (handling large data sets without significant deterioration of performance displayed on small sets). 4 Annotation Guidelines The annotation part is the most critical part of any evaluation exercise. For the Logic Form Identification task the following steps were applied to obtain the correct LF for the test data: 1. logic forms for the test data were automatically obtained using an extended version of the LF derivation engine developed in (Rus, 2002) for LFi of WordNet glosses. As part of this step, sentences were preprocessed: tokenized (separating punctuation from words) using the Penn Treebank guidelines, tagged with Brill’s tagger (Brill, 1992) and then parsed with Collins’ statistical parser (Collins, 1996). 2. a first manual checking of the previously generated LF was done. 3. a second manual checking was done by another annotator. 4. quality assurance of the previous steps was performed by individual annotators by checking specific cases (ditransitives, relative pronouns, etc.) with much emphasis on consistency. 5. annotators agree</context>
</contexts>
<marker>Rus, 2002</marker>
<rawString>Vasile Rus. 2002. Logic Form for WordNet Glosses and Applications. Phd thesis, Southern Methodist University, May.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>