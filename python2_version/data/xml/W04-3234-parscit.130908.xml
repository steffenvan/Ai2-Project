<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017575">
<title confidence="0.98491">
Trained Named Entity Recognition Using Distributional Clusters
</title>
<author confidence="0.901864">
Dayne Freitag
</author>
<affiliation confidence="0.769548">
HNC Software, LLC
</affiliation>
<address confidence="0.9029475">
3661 Valley Centre Drive
San Diego, CA 92130
</address>
<email confidence="0.998553">
DayneFreitag@fairisaac.com
</email>
<sectionHeader confidence="0.993869" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999989461538461">
This work applies boosted wrapper induction
(BWI), a machine learning algorithm for informa-
tion extraction from semi-structured documents, to
the problem of named entity recognition. The de-
fault feature set of BWI is augmented with features
based on distributional term clusters induced from a
large unlabeled text corpus. Using no traditional lin-
guistic resources, such as syntactic tags or special-
purpose gazetteers, this approach yields results near
the state of the art in the MUC 6 named entity do-
main. Supervised learning using features derived
through unsupervised corpus analysis may be re-
garded as an alternative to bootstrapping methods.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999963384615385">
The problem of named entity recognition (NER) has
recently received increasing attention. Identifica-
tion of generic semantic categories in text—such
as mentions of people, organizations, locations, and
temporal and numeric expressions—is a necessary
first step in many applications of information ex-
traction, information retrieval, and question answer-
ing. To a large extent, knowledge-poor methods suf-
fice to yield good recognition performance. In par-
ticular, supervised learning can be used to produce
a system with performance at or near the state of the
art (Bikel et al., 1997).
In the supervised learning framework, a corpus
of (typically) a few hundred documents is annotated
by hand to identify the entities of interest. Features
of local context are then used to train a system to
distinguish instances from non-instances in novel
texts. Such features may include literal word tests,
patterns of orthography, parts of speech, seman-
tic categories, or membership in special-purpose
gazetteers.
While supervised training greatly facilitates the
development of a robust NER system, the re-
quirement of a substantial training corpus remains
an impediment to the rapid deployment of NER
in new domains or new languages. A number
</bodyText>
<figureCaption confidence="0.4918625">
bush peters reagan noriega...
john robert james david...
president chairman head owner...
japan california london chicago...
</figureCaption>
<tableCaption confidence="0.8661195">
Table 1: Sample members of four clusters from the
Wall Street Journal corpus.
</tableCaption>
<bodyText confidence="0.9998633">
of researchers have therefore sought to exploit
the availability of unlabeled documents, typically
by bootstrapping a classifier using automatic la-
bellings (Collins and Singer, 1999; Cucerzan and
Yarowsky, 1999; Thelen and Riloff, 2002).
Here, we investigate a different approach. Us-
ing a distributional clustering technique called co-
clustering, we produce clusters which, intuitively,
should be useful for NER. Table 1 shows exam-
ple terms from several sample clusters induced us-
ing a collection of documents from the Wall Street
Journal (WSJ). Several papers have shown that dis-
tributional clustering yields categories that have
high agreement with part of speech (Sch¨utze, 1995;
Clark, 2000). As the table illustrates, these clus-
ters also tend to have a useful semantic dimension.
Clustering on the WSJ portion of the North Ameri-
can News corpus yields two clusters that clearly cor-
respond to personal names, one for first names and
one for last names. As an experiment, we scanned
the MUC6 NER data set for token sequences con-
sisting of zero or more members of the first name
cluster (or an initial followed by a period), followed
by one or more members of the last name cluster.
This simple procedure identified 64% of personal
names with 77% precision.
In this paper, we attempt to improve on this
result by converting the clusters into features to
be exploited by a general-purpose machine learn-
ing algorithm for information extraction. In Sec-
tion 2, we provide a brief description of Boosted
Wrapper Induction (BWI), a pattern learner that has
yielded promising results on semi-structured infor-
mation extraction problems (Freitag and Kushmer-
ick, 2000). In Section 3, we describe our clustering
approach and its particular application. Section 4
presents the results of our experiments. Finally, in
Section 5, we assess the significance of our contri-
bution and attempt to identify promising future di-
rections.
</bodyText>
<sectionHeader confidence="0.991063" genericHeader="introduction">
2 BWI
</sectionHeader>
<bodyText confidence="0.999878464285714">
BWI decomposes the problem of recognizing field
instances into two Boolean classification problems:
recognizing field-initial and field-terminal tokens.
Given a target field, a separate classifier is learned
for each of these problems, and the distribution of
field lengths is modeled as a frequency histogram.
At application time, tokens that test positive for ini-
tial are paired with those testing positive for termi-
nal. If the length of a candidate instance, as defined
by such a pair, is determined to have non-zero like-
lihood using the length histogram, a prediction is
returned.
Each of the three parts of a full prediction—initial
boundary, terminal boundary, and length—is as-
signed a real-valued confidence. The confidence of
a boundary detection is its strength as determined by
AdaBoost, while that of the length assessment is the
empirical length probability, which is determined
using the length histogram. The confidence of the
full prediction is the product of these three individ-
ual confidence scores. In the event that overlapping
predictions are found in this way (a rare event, em-
pirically), the predictions with lower confidence are
discarded.
In this section, we sketch those aspects of BWI
relevant to the current application. More details
are available in the paper in which BWI was de-
fined (Freitag and Kushmerick, 2000).
</bodyText>
<subsectionHeader confidence="0.995588">
2.1 Boosting
</subsectionHeader>
<bodyText confidence="0.999528125">
BWI uses generalized AdaBoost to produce each
boundary classifier (Schapire and Singer, 1998).
Boosting is a procedure for improving the perfor-
mance of a “weak learner” by repeatedly applying
it to a training set, at each step modifying exam-
ple weights to emphasize those examples on which
the learner has done poorly in previous steps. The
output is a weighted collection of weak learner hy-
potheses. Classification involves having the individ-
ual hypotheses “vote,” with strengths proportional
to their weights, and summing overlapping votes.
Although this is the first application of BWI to
NER, boosting has previously been shown to work
well on this problem. Differing from BWI in the de-
tails of the application, two recent papers neverthe-
less demonstrate the effectiveness of the boosting
</bodyText>
<table confidence="0.990111555555556">
Cap Initial capital
AllCap All capitals
Uncap Initial lower case
Alpha Entirely alphabetic characters
ANum Entirely alpha-numeric characters
Punc Punctuation
Num Entirely numeric characters
Schar Single alphabetic character
Any Anything
</table>
<tableCaption confidence="0.933546">
Table 2: Default wildcards used in these experi-
ments.
</tableCaption>
<bodyText confidence="0.9487835">
paradigm for NER in several languages (Carreras
et al., 2002; Wu et al., 2002), one of them achiev-
ing the best overall performance in a comparison of
several systems (Sang, 2002).
</bodyText>
<subsectionHeader confidence="0.998906">
2.2 Boundary Detectors
</subsectionHeader>
<bodyText confidence="0.999870285714286">
The output of a single invocation of the weak learner
in BWI is always an individual pattern, called a
boundary detector. A detector has two parts, one
to match the text leading up to a boundary, the
other for trailing text. Each part is a list of zero
or more elements. In order for a boundary to
match a detector, the tokens preceding the bound-
ary (or following it) must match the corresponding
elements in sequence. For example, the detector
[ms .][jones] matches boundaries preceded
by the (case-normalized) two-token sequence “ms
.” and followed by the single token “jones”.
Detectors are grown iteratively, beginning with
an empty detector and repeatedly adding the ele-
ment that best increases the ability of the current
detector to discriminate true boundaries from false
ones, using a cost function sensitive to the exam-
ple weighting. A look-ahead parameter allows this
decision to be based on several additional context
tokens. The process terminates when no extensions
yield a higher score than the current detector.
</bodyText>
<subsectionHeader confidence="0.994404">
2.3 Wildcards
</subsectionHeader>
<bodyText confidence="0.999820875">
The elements of the detector [ms .][jones] are
literal elements, which match tokens using case-
normalized string comparison. More interesting el-
ements can be introduced by defining token wild-
cards. Each wildcard defines some Boolean func-
tion over the space of tokens.
Table 2 lists the baseline wildcards. Using wild-
cards from this list, the example detector can be gen-
eralized to match a much broader range of bound-
aries (e.g., [ms &lt;Any&gt;][&lt;Cap&gt;]). By defin-
ing new wildcards, we can inject useful domain
knowledge into the inference process, potentially
improving the performance of the resulting extrac-
tor. For example, we might define a wildcard called
“Honorific” that matches any of “ms”, “mr”,
“mrs”, and “dr”.
</bodyText>
<subsectionHeader confidence="0.99758">
2.4 Boundary Wildcards
</subsectionHeader>
<bodyText confidence="0.999568925925926">
In the original formulation of BWI, boundaries are
identified without reference to the location of the
opposing boundary. However, we might expect that
the end of a name, say, would be easier to identify if
we know where it begins. We can build detectors
that exploit this knowledge by introducing a spe-
cial wildcard (called Begin) that matches the be-
ginnings of names.
In these experiments, therefore, we modify
boundary detection in the following way. Instead
of two detector lists, we learn four—the two lists
as in the original formulation (call them and
), and two more lists ( and ). In
generating the latter two lists, we give the learner
access to these special wildcards (e.g., the wildcard
End in generating ).
At extraction time, and are first
used to detect boundaries, as before. These de-
tections are then used to determine which tokens
match the “special” wildcards used by and
. Then, instead of pairing predictions
with those of , they are paired with those
made by (and with ). In in-
formal experiments, we found that this procedure
tended to increase F1 performance by several points
on a range of tasks. We adopt it uniformly in the
experiments reported here.
</bodyText>
<sectionHeader confidence="0.871285" genericHeader="method">
3 Co-Clustering
</sectionHeader>
<bodyText confidence="0.999998125">
As in Brown, et al (1992), we seek a partition of
the vocabulary that maximizes the mutual infor-
mation between term categories and their contexts.
To achieve this, we use information theoretic co-
clustering (Dhillon et al., 2003), in which a space
of entities, on the one hand, and their contexts, on
the other, are alternately clustered to maximize mu-
tual information between the two spaces.
</bodyText>
<subsectionHeader confidence="0.996666">
3.1 Background
</subsectionHeader>
<bodyText confidence="0.998060041666667">
The input to our algorithm is two finite sets of sym-
bols, say (e.g., terms) and
(e.g., term contexts), to-
gether with a set of co-occurrence count data con-
sisting of a non-negative integer for every
pair of symbols from and . The out-
put is two partitions: and
, where each is a subset of
(a “cluster”), and each a subset of . The
co-clustering algorithm chooses the partitions
and to (locally) maximize the mutual informa-
tion between them, under a constraint limiting the
total number of clusters in each partition.
Recall that the entropy or Shannon information of
a discrete distribution is:
This quantifies average improvement in one’s
knowledge upon learning the specific value of an
event drawn from . It is large or small depending
on whether has many or few probable values.
The mutual information between random vari-
ables and can be written:
This quantifies the amount that one expects to learn
indirectly about upon learning the value of , or
vice versa.
</bodyText>
<subsectionHeader confidence="0.999301">
3.2 The Algorithm
</subsectionHeader>
<bodyText confidence="0.999369260869565">
Let be a random variable over vocabulary terms
as found in some text corpus. We define to
range over immediately adjacent tokens, encoding
co-occurrences in such a way as to distinguish left
from right occurrences.
Given co-occurrence matrices tabulated in this
way, we perform an approximate maximization of
using a simulated annealing procedure in
which each trial move takes a symbol or out
of the cluster to which it is tentatively assigned and
places it into another. Candidate moves are chosen
by selecting a non-empty cluster uniformly at ran-
dom, randomly selecting one of its members, then
randomly selecting a destination cluster other than
the source cluster. When temperature 0 is reached,
all possible moves are repeatedly attempted until no
further improvements are possible.
For efficiency and noise reduction, we first clus-
ter only the 5000 most frequent terms and context
terms. The remaining terms in the corpus vocabu-
lary are then added by assigning each term to the
cluster that maximizes the mutual information ob-
jective function.
</bodyText>
<sectionHeader confidence="0.998433" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.998872333333333">
We experimented with the MUC 6 named entity
data set, which consists of a training set of 318 doc-
uments, a validation set of 30 documents, and a test
set of 30 documents.
All documents are annotated to identify three
types of name (PERSON, ORGANIZATION,
</bodyText>
<table confidence="0.991832285714286">
DATE [][september]
[in &lt;Num&gt;][&lt;Punc&gt;]
TIME [][5 &lt;ANum&gt; . &lt;ANum&gt;]
[midnight][]
MONEY [][$]
[$ &lt;Any&gt; billion][]
PCT. [&lt;Alph&gt; &lt;Punc&gt;][&lt;Any&gt; %]
[&lt;Num&gt; percentage &lt;Alph&gt;][]
PERSON [mr &lt;Any&gt;][&lt;Cap&gt;]
[&lt;Cap&gt;][, vice]
ORG. [][nissan]
[inc &lt;Any&gt;][]
LOC. [in][&lt;Cap&gt; , &lt;Alph&gt; &lt;Punc&gt;]
[germany][]
</table>
<tableCaption confidence="0.995707">
Table 3: Sample boundary detectors for the seven
</tableCaption>
<bodyText confidence="0.975807333333333">
MUC 6 fields produced by BWI using the baseline
feature set. An initial and terminal detector is shown
for each field.
LOCATION), two types of temporal expression
(DATE, TIME), and two types of numeric expres-
sion (MONEY, PERCENT). It is common to re-
port performance in terms of precision, recall, and
their harmonic mean (F1), a convention to which we
adhere.
</bodyText>
<subsectionHeader confidence="0.908144">
4.1 Baseline
</subsectionHeader>
<bodyText confidence="0.999964428571429">
Using the wildcards listed in Table 2, we trained
BWI for 500 boosting iterations on each of the seven
entity fields. The output out each of these training
runs consists of boundary detectors.
Look-ahead was set to 3.
Table 3 shows a few of the boundary detectors
induced by this procedure. These detectors were
selected manually to illustrate the kinds of patterns
generated. Note how some of the detectors amount
to field-specific gazetteer entries. Others have more
interesting (and typically intuitive) structure. We
defer quantitative evaluation to the next section,
where a comparison with the cluster-enhanced ex-
tractors will be made.
</bodyText>
<subsectionHeader confidence="0.997814">
4.2 Adding Cluster Features
</subsectionHeader>
<bodyText confidence="0.997504888888889">
The MUC 6 dataset was produced using articles
from the Wall Street Journal. In order to pro-
duce maximally relevant clusters, we used docu-
ments from the WSJ portion of the North Ameri-
can News corpus as input to co-clustering—some
119,000 documents in total. Note that there is a tem-
poral disparity between the MUC 6 corpus and this
clustering corpus, which has an undetermined im-
pact on performance.
</bodyText>
<table confidence="0.998515333333333">
PERS. [&lt;C95&gt;][&lt;C73&gt;]
[&lt;C144&gt; &lt;Any&gt; &lt;C106&gt;][&lt;Uncap&gt;]
ORG. [][&lt;C178&gt; express]
[bank &lt;ANum&gt; &lt;C146&gt;][]
LOC. [][&lt;C72&gt; korea]
[&lt;C160&gt;][&lt;Punc&gt;]
</table>
<tableCaption confidence="0.980298">
Table 4: Sample boundary detectors for the seven
MUC 6 fields produced by BWI using the expanded
feature set.
</tableCaption>
<table confidence="0.999686">
72 general south north poor...
73 john robert james david...
95 says adds asks recalls...
106 clinton dole johnson gingrich...
144 mr ms dr sen ...
146 japan american china congress...
160 washington texas california...
178 american foreign local...
</table>
<tableCaption confidence="0.960363">
Table 5: Most frequent members of clusters refer-
enced by detectors in Table 4.
</tableCaption>
<bodyText confidence="0.999580740740741">
We used this data to produce 200 clusters, as de-
scribed in Section 3. Treating each of these clus-
ters as an unlabeled gazetteer, we then defined cor-
responding wildcards. For example, the value of
wildcard &lt;C35&gt; only matches a term belonging to
Cluster 35. In order to reduce the training time of a
given boundary learning problem, we tabulated the
frequency of wildcard occurrence within three to-
kens of any occurrences of the target boundary and
omitted from training wildcards testing true fewer
than ten times.1
Table 4, which lists sample detectors from these
runs, includes some that are clearly impossible to
express using the baseline feature set. An exam-
ple is the first row, which matches a third-person
present-tense verb used in quote attribution, fol-
lowed by a first name (see Table 5). At the same
time, some of the new wildcards are employed triv-
ially, such as the use of &lt;C178&gt; in the field-initial
detector for the ORGANIZATION field.
Table 6 shows performance of the two variants
on the individual MUC 6 fields, tested over the
“dryrun” and “formal” test sets combined. In this
table, we scored each field individually using our
own evaluation software. An entity instance was
judged to be correctly extracted if a prediction pre-
cisely identified its boundaries (ignoring “ALT” at-
</bodyText>
<footnote confidence="0.820839">
&apos;For the TIME field, which occurs a total of six times in the
training set, this cut-off was a single occurrence.
</footnote>
<table confidence="0.999893866666667">
Field F1 Prec Rec
DATE Base 0.766 0.765 0.768
Clust 0.782 0.776 0.789
TIME Base 0.667 1.000 0.500
Clust 0.667 1.000 0.500
MONEY Base 0.938 0.926 0.949
Clust 0.943 0.938 0.949
PERCENT Base 0.922 0.855 1.000
Clust 0.930 0.869 1.000
PERSON Base 0.827 0.810 0.844
Clust 0.892 0.859 0.927
ORG. Base 0.587 0.811 0.460
Clust 0.733 0.796 0.680
LOCATION Base 0.726 0.675 0.785
Clust 0.724 0.648 0.821
</table>
<tableCaption confidence="0.98949">
Table 6: Performance on the seven MUC 6 fields,
</tableCaption>
<bodyText confidence="0.992915866666668">
without (Base) and with (Clust) cluster-based fea-
tures. Significantly better precision or recall scores,
at the 95% confidence level, are in boldface.
tributes). Non-matching predictions and missed en-
tities were counted as false positives and false neg-
atives, respectively. We assessed the statistical sig-
nificance of precision and recall scores by comput-
ing beta confidence intervals at the 95% level. In the
table, the higher precision or recall is in boldface if
its separation from the lower score is significant.
Except for TIME and LOCATION, all fields ben-
efit from inclusion of the cluster features. TIME,
which is scarce in the training and test sets, is insen-
sitive to their inclusion. The effect on LOCATION
is more interesting. It shares in the general tendency
of cluster features to increase recall, but loses preci-
sion as a result.2 Although the increase in recall is
approximately the same as the loss in precision, the
F1 score, which is more heavily influenced by the
lower of precision and recall, drops slightly.
While the effect of the cluster features on pre-
cision is inconsistent, they typically benefit recall.
This effect is most dramatic in the case of ORGA-
NIZATION, where, at the expense of a small drop in
precision, recall increases by more than 20 points.
The somewhat counter-intuitive improvements in
precision on some fields (particularly the significant
improvement on PERSON) is attributable to our
learning framework. Boosting for a sufficient num-
ber of iterations forces a learner to account for all
boundary tokens through one or more detectors. To
the exent that the baseline’s features are unable to
2Note, however, that none of the differences observed for
LOCATION are significant at the 95% level.
account for as many of the boundary tokens, it is
forced to learn a larger number of over-specialized
detectors that rely on questionable patterns in the
data. Depending on the task, these detectors can
lead to a larger proportion of false positives.
The relatively weak result for DATE comes as
a surprise. Inspection of the data leads us to at-
tribute this to two factors. On the one hand, there
is considerable temporal drift between the training
and test sets. Many of the dates are specific to
contemporaneous events; patterns based on specific
years, therefore, generalize in only a limited way.
At the same time, the notion of date, as understood
in the MUC 6 corpus, is reasonably subtle. Mean-
ing roughly “non-TIME temporal expression,” it in-
cludes everything from shorthand date expressions
to more interesting phrases, such as, “the first six
months of fiscal 1994.”
In passing we note a few potentially relevant id-
iosyncrasies in these experiments. Most significant
is a representational choice we made in tokenizing
the cluster corpus. In tallying frequencies we treated
all numeric expressions as occurrences of a special
term, “*num*”. Consequently, the tokens “1989”
and “10,000” are treated as instances of the same
term, and clustering has no opportunity to distin-
guish, say, years from monetary amounts.
The (perhaps) disappointing performance on the
relatively simple fields, TIME and PERCENT,
somewhat under-reports the strength of the learner.
As noted above, TIME occurs only very infre-
quently. Consequently, little training data is avail-
able for this field and mistakes (BWI missed one of
the three instances in the test set) have a large effect
on the TIME-specific scores. In the case of PER-
CENT, we ignored MUC instructions not to attempt
to recognize instances in tabular regions. One of
the documents contains a significant number of un-
labeled percentages in such a table. BWI duly rec-
ognized these—to the detriment of the reported pre-
cision.
</bodyText>
<subsectionHeader confidence="0.995655">
4.3 MUC Evaluation
</subsectionHeader>
<bodyText confidence="0.998833">
For comparison with numbers reported in the lit-
erature, we used the learned extractors to produce
mark-up and evaluated the result using the MUC 6
scorer. The MUC 6 evaluation framework differs
from ours in two key ways. Most importantly, all
entity types are to be processed simultaneously. We
benefit from this framework, since spurious predic-
tions for one entity type may be superseded by cor-
rect predictions for a related type. The opportunity
is greatest for the three name types; in inspecting
the false positives, we observed a number of confu-
</bodyText>
<table confidence="0.999801823529412">
Field F1 Prec Rec
DATE Base 0.91 0.91 0.91
Clust 0.92 0.90 0.94
TIME Base 0 0 0
Clust 0 0 0
MONEY Base 0.95 0.94 0.96
Clust 0.95 0.95 0.96
PERCENT Base 0.97 0.94 1.0
Clust 1.0 1.0 1.0
PERSON Base 0.88 0.91 0.86
Clust 0.94 0.94 0.95
ORG. Base 0.62 0.78 0.52
Clust 0.79 0.84 0.74
LOCATION Base 0.86 0.86 0.87
Clust 0.86 0.80 0.92
ALL Base 0.79 0.85 0.73
Clust 0.87 0.88 0.86
</table>
<tableCaption confidence="0.816747">
Table 7: Performance on the markup task, as scored
by the MUC 6 scorer.
</tableCaption>
<bodyText confidence="0.999935785714286">
sions among these fields.3 The MUC scorer is also
more lenient than ours, awarding points for extrac-
tion of alternative strings and forgiving the inclusion
of certain functional tokens in the extracted text.
In moving to the multi-entity extraction setting,
the obvious approach is to collect predictions from
all extractors simultaneously. However, this re-
quires a strategy for dealing with overlapping pre-
dictions (e.g., a single text fragment labeled as both
a person and organization). We resolve such con-
flicts by preferring in each case the extraction with
the highest confidence. In order to render confi-
dence scores more comparable, we normalized the
weights of detectors making up each boundary clas-
sifier so they sum to one.
A comparison of Table 7 with Table 6 suggests
the extent to which BWI benefits from the multi-
field mark-up setting. Note that, here, we used only
the “formal” test set for evaluation, in contrast with
the numbers in Table 6, which combine the two test
sets. The lift we observe from cluster features is also
in evidence here, and is most evident as an increase
in recall, particularly of PERSON and ORGANI-
ZATION. There is now also an increase in global
precision, attributable in large part to the benefit of
extracting multiple fields simultaneously.
The F1 score produced by BWI is compara-
ble to the best machine-learning-based results re-
</bodyText>
<footnote confidence="0.99054">
3For example, companies are occasionally named after peo-
ple (e.g., Liz Claiborne).
</footnote>
<bodyText confidence="0.999878885714286">
ported elsewhere. Bikel, at al (1997), reports sum-
mary F1 of 0.93 on the same test set, but using
a model trained on 450,000 words. We count ap-
proximately 130,000 words in the experiments re-
ported here. The numbers reported by Bennett, et
al (1997), for PERSON, ORGANIZATION, and
LOCATION (F1 of 0.947, 0.815, and 0.925, respec-
tively), are slightly better than the numbers BWI
reaches on the same fields. Note, however, that the
features provided to their learner include syntactic
labels and carefully engineered semantic categories,
whereas we eschew knowledge- and labor-intensive
resources. This has important implications for the
portability of the approaches to new domains and
languages.
By taking a few post-processing steps, it is pos-
sible to realize further improvements. For example,
the learner occasionally identifies terms and phrases
which some simple rules can reliably reject. By sup-
pressing any prediction that consists entirely of a
stopword, we increase the precision of both ORGA-
NIZATION and LOCATION to 0.86 (from 0.84 and
0.80) and overall F1 to 0.88.
We can also exploit what Cucerzan and
Yarowsky (1999) call the one sense per discourse
phenomenon, the tendency of terms to have a fixed
meaning within a single document. By mark-
ing up unmarked strings that match extracted en-
tity instances in the same document, we can im-
prove the recall of some fields. We added this post-
processing step for the PERSON and ORGANI-
ZATION fields. This increased recall of PERSON
from 0.95 to 0.98 and of ORGANIZATION from
0.74 to 0.79 with minimal changes to precision and
a slight improvement in summary F1.
</bodyText>
<subsectionHeader confidence="0.999598">
4.4 Analysis and Related Work
</subsectionHeader>
<bodyText confidence="0.999929882352941">
The promise of this general method—supervised
learning on small training set using features derived
from a larger unlabeled set—lies in the support it
provides for rapid deployment in novel domains and
languages. Without relying on any linguistic re-
sources more advanced than a tokenizer and some
orthographic features, we can produce a NER mod-
ule using only a few annotated documents.
How few depends ultimately on the difficulty of
the domain. We might also expect the benefit of
distributional features to decrease with increasing
training set size. Figure 1 displays the F1 learning-
curve performance of BWI, both with and without
cluster features, on the two fields that benefit the
greatest from these features, PERSON and ORGA-
NIZATION. As expected, the difference appears to
be greatest on the low end of the horizontal axis (al-
</bodyText>
<figure confidence="0.972320666666667">
Formal test F1
0 50 100 150 200 250 300 350
Training documents
</figure>
<figureCaption confidence="0.8674595">
Figure 1: F1 as a function of training set size in
number of documents.
</figureCaption>
<bodyText confidence="0.999965250000001">
though overfitting complicates the comparison). At
the same time, the improvement is fairly consistent
at all training set sizes. Either the baseline feature
set is ultimately too impoverished for this task, or,
more likely, the complete MUC 6 training set (318
documents) is small for this class of learner.
Techniques to lessen the need for annotation for
NER have received a fair amount of attention re-
cently. The prevailing approach to this problem is
a bootstrapping technique, in which, starting with a
few hand-labeled examples, the system iteratively
adds automatic labels to a corpus, training itself,
as it were. Examples of this are Cucerzan and
Yarowsky (1999), Thelen and Riloff (2002), and
Collins and Singer (1999).
These techniques address the same problem as
this paper, but are otherwise quite different from the
work described here. The labeling method (seed-
ing) is an indirect form of corpus annotation. The
promise of all such approaches is that, by starting
with a small number of seeds, reasonable results can
be achieved at low expense. However, it is difficult
to tell how much labeling corresponds to a given
number of seeds, since this depends on the cover-
age of the seeds. Note, too, that any bootstrapping
approach must confront the problem of instability;
poor initial decisions by a bootstrapping algorithm
can lead to large eventual performance degrada-
tions. We might expect a lightly supervised learner
with access to features based on a full-corpus anal-
ysis to yield more consistently strong results.
Of the three approaches mentioned above, only
Cucerzan and Yarowsky do not presuppose a syn-
tactic analysis of the corpus, so their work is perhaps
most comparable to this one. Of course, compar-
isons must be strongly qualified, given the different
labeling methods and data sets. Nevertheless, per-
formance of cluster-enhanced BWI at the low end
of the horizontal axis compares favorably with the
English F1 performance of 0.543 they report using
190 seed words. And, arguably, annotating 10-20
documents is no more labor intensive than assem-
bling a list of 190 seed words.
Strong corroboration for the approach advocated
in this paper is provided by Miller, et al (2004),
in which cluster-based features are combined with
a sequential maximum entropy model proposed in
Collins (2002) to advance the state of the art. In ad-
dition, using active learning, the authors are able to
reduce human labeling effort by an order of magni-
tude.
Miller, et al, use a proprietary data set for train-
ing and testing, so it is difficult to make a close
comparison of outcomes. At roughly comparable
training set sizes, they appear to achieve a score
of about 0.89 (F1) with a “conventional” HMM,
versus 0.93 using the discriminative learner trained
with cluster features (compared with 0.86 reached
by BWI). Both the HMM and Collins model are
constrained to account for an entire sentence in tag-
ging it, making determinations for all fields simulta-
neously, in contrast to the individual, local boundary
detections made by BWI. This characteristic proba-
bly accounts for the accuracy advantage they appear
to enjoy.
An interesting distinguishing feature of Miller,
et al, is their use of hierarchical clustering. While
much is made of the ability of their approach to
accomodate different levels of granularity automat-
ically, no evidence is provided that the hierarchy
provides real benefit. At the same time, our work
shows that significant gains can be realized with a
single, sufficiently granular partition of terms. It is
known, moreover, that greedy agglomerative clus-
tering leads to partitions that are sub-optimal in
terms of a mutual information objective function
(see, for example, Brown, et al (1992)). Ultimately,
it is left to future research to determine how sensi-
tive, if at all, the NER gains are to the details of the
clustering.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.981440777777778">
There are several ways in which this work might be
extended and improved, both in its particular form
and in general:
BWI models initial and terminal boundaries,
but ignores characteristics of the extracted
phrase other than its length. We are explor-
ing mechanisms for modeling relevant phrasal
structure.
While global statistical approaches, such as se-
</bodyText>
<figure confidence="0.996700538461539">
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
PERSON (clust.)
PERSON (base)
ORG. (clust.)
ORG. (base)
</figure>
<bodyText confidence="0.996719083333333">
quential averaged perceptrons or CRFs (Mc-
Callum and Li, 2003), appear better suited to
the NER problem than local symbolic learners,
the two approaches search different hypothesis
spaces. Based on the surmise that, by combin-
ing them, we can realize improvements over ei-
ther in isolation, we are exploring mechanisms
for integration.
The distributional clusters we find are indepen-
dent of the problem to which we want to apply
them and may sometimes be inappropriate or
have the wrong granularity. We are exploring
ways to produce groupings that are sensitive to
the task at hand.
Our results clearly establish that an unsupervised
distributional analysis of a text corpus can produce
features that lead to enhanced precision and, espe-
cially, recall in information extraction. We have
successfully used these features in lieu of domain-
specific, labor-intensive resources, such as syntac-
tic analysis and special-purpose gazetteers. Distri-
butional analysis, combined with light supervision,
is an effective, stable alternative to bootstrapping
methods.
</bodyText>
<sectionHeader confidence="0.9982" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999824666666667">
This material is based on work funded in whole or
in part by the U.S. Government. Any opinions, find-
ings, conclusions, or recommendations expressed in
this material are those of the authors, and do not
necessarily reflect the views of the U.S. Govern-
ment.
</bodyText>
<sectionHeader confidence="0.99923" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999797857142857">
S.W. Bennett and C. Aone. 1997. Learning to tag
multilingual texts through observation. In Proc.
2nd Conference on Empirical Methods in Natural
Language Processing (EMNLP-2), August.
D.M. Bikel, S. Miller, R. Schwartz, and
R. Weischedel. 1997. Nymble: a high-
performance learning name-finder. In Proc.
5th Conference on Applied Natural Language
Processing (ANLP-97), April.
P.F. Brown, V.J. Della Pietra, P.V. deSouza, J.C.
Lai, and R.L. Mercer. 1992. Class-based n-gram
models of natural language. Computational Lin-
guistics, 18(4):467–479.
X. Carreras, L. M`arquez, and L. Padr´o. 2002.
Named entity extraction using AdaBoost. In Pro-
ceedings of CoNLL-2002, Taipei, Taiwan.
A. Clark. 2000. Inducing syntactic categories by
context distribution clustering. In CoNLL 2000,
September.
M. Collins and Y. Singer. 1999. Unsupervised
models for named entity classification. In Proc.
1999 Joint SIGDAT Conference on Empirical
Methods in NLP and Very Large Corpora.
M. Collins. 2002. Discriminative training methods
for hidden Markov models: Theory and exper-
iments with perceptron algorithms. In Proceed-
ings of EMNLP-2002.
S. Cucerzan and D. Yarowsky. 1999. Language
independent named entity recognition combin-
ing morphological and contextual evidence. In
Proc. 1999 Joint SIGDAT Conference on Empir-
ical Methods in NLP and Very Large Corpora,
pages 90–99.
I. S. Dhillon, S. Mallela, and D. S. Modha. 2003.
Information-theoretic co-clustering. Technical
Report TR-03-12, Dept. of Computer Science, U.
Texas at Austin.
D. Freitag and N. Kushmerick. 2000. Boosted
wrapper induction. In Proc. 17th National Con-
ference on Artificial Intelligence (AAAI-2000),
August.
A. McCallum and W. Li. 2003. Early results for
named entity recognition with conditional ran-
dom fields, feature induction and web-enhanced
lexicons. In Proc. 7th Conference on Natural
Language Learning (CoNLL-03).
S. Miller, J. Guinness, and A. Zamanian. 2004.
Name tagging with word clusters and discrimi-
native training. In Proceedings of HLT/NAACL
04.
E. F. Tjong Kim Sang. 2002. Introduction
to the CoNLL-2002 shared task: Language-
independent named entity recognition. In Pro-
ceedings of CoNLL-2002.
R.E. Schapire and Y. Singer. 1998. Improved
boosting algorithms using confidence-rated pre-
dictions. In Proc. 11th Annual Conference
on Computational Learning Theory (COLT-98),
pages 80–91, July.
H. Sch¨utze. 1995. Distributional part-of-speech
tagging. In Proc. 7th EACL Conference (EACL-
95), March.
M. Thelen and E. Riloff. 2002. A bootstrapping
method for learning semantic lexicons using ex-
traction pattern contexts. In Proc. 2002 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP 2002).
D. Wu, G. Ngai, M. Carpuat, J. Larsen, and Y. Yang.
2002. Boosting for named entity recognition. In
Proceedings of CoNLL-2002, Taipei, Taiwan.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.818391">
<title confidence="0.993041">Trained Named Entity Recognition Using Distributional Clusters</title>
<author confidence="0.94357">Dayne</author>
<affiliation confidence="0.999882">HNC Software,</affiliation>
<address confidence="0.967978">3661 Valley Centre San Diego, CA</address>
<email confidence="0.985634">DayneFreitag@fairisaac.com</email>
<abstract confidence="0.995567142857143">This work applies boosted wrapper induction (BWI), a machine learning algorithm for information extraction from semi-structured documents, to the problem of named entity recognition. The default feature set of BWI is augmented with features based on distributional term clusters induced from a large unlabeled text corpus. Using no traditional linguistic resources, such as syntactic tags or specialpurpose gazetteers, this approach yields results near the state of the art in the MUC 6 named entity domain. Supervised learning using features derived through unsupervised corpus analysis may be regarded as an alternative to bootstrapping methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S W Bennett</author>
<author>C Aone</author>
</authors>
<title>Learning to tag multilingual texts through observation.</title>
<date>1997</date>
<booktitle>In Proc. 2nd Conference on Empirical Methods in Natural Language Processing (EMNLP-2),</booktitle>
<marker>Bennett, Aone, 1997</marker>
<rawString>S.W. Bennett and C. Aone. 1997. Learning to tag multilingual texts through observation. In Proc. 2nd Conference on Empirical Methods in Natural Language Processing (EMNLP-2), August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Bikel</author>
<author>S Miller</author>
<author>R Schwartz</author>
<author>R Weischedel</author>
</authors>
<title>Nymble: a highperformance learning name-finder.</title>
<date>1997</date>
<booktitle>In Proc. 5th Conference on Applied Natural Language Processing (ANLP-97),</booktitle>
<contexts>
<context position="1420" citStr="Bikel et al., 1997" startWordPosition="206" endWordPosition="209">bootstrapping methods. 1 Introduction The problem of named entity recognition (NER) has recently received increasing attention. Identification of generic semantic categories in text—such as mentions of people, organizations, locations, and temporal and numeric expressions—is a necessary first step in many applications of information extraction, information retrieval, and question answering. To a large extent, knowledge-poor methods suffice to yield good recognition performance. In particular, supervised learning can be used to produce a system with performance at or near the state of the art (Bikel et al., 1997). In the supervised learning framework, a corpus of (typically) a few hundred documents is annotated by hand to identify the entities of interest. Features of local context are then used to train a system to distinguish instances from non-instances in novel texts. Such features may include literal word tests, patterns of orthography, parts of speech, semantic categories, or membership in special-purpose gazetteers. While supervised training greatly facilitates the development of a robust NER system, the requirement of a substantial training corpus remains an impediment to the rapid deployment </context>
</contexts>
<marker>Bikel, Miller, Schwartz, Weischedel, 1997</marker>
<rawString>D.M. Bikel, S. Miller, R. Schwartz, and R. Weischedel. 1997. Nymble: a highperformance learning name-finder. In Proc. 5th Conference on Applied Natural Language Processing (ANLP-97), April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J Della Pietra</author>
<author>P V deSouza</author>
<author>J C Lai</author>
<author>R L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="9837" citStr="Brown, et al (1992)" startWordPosition="1553" endWordPosition="1556">ing the latter two lists, we give the learner access to these special wildcards (e.g., the wildcard End in generating ). At extraction time, and are first used to detect boundaries, as before. These detections are then used to determine which tokens match the “special” wildcards used by and . Then, instead of pairing predictions with those of , they are paired with those made by (and with ). In informal experiments, we found that this procedure tended to increase F1 performance by several points on a range of tasks. We adopt it uniformly in the experiments reported here. 3 Co-Clustering As in Brown, et al (1992), we seek a partition of the vocabulary that maximizes the mutual information between term categories and their contexts. To achieve this, we use information theoretic coclustering (Dhillon et al., 2003), in which a space of entities, on the one hand, and their contexts, on the other, are alternately clustered to maximize mutual information between the two spaces. 3.1 Background The input to our algorithm is two finite sets of symbols, say (e.g., terms) and (e.g., term contexts), together with a set of co-occurrence count data consisting of a non-negative integer for every pair of symbols from</context>
<context position="29242" citStr="Brown, et al (1992)" startWordPosition="4745" endWordPosition="4748"> they appear to enjoy. An interesting distinguishing feature of Miller, et al, is their use of hierarchical clustering. While much is made of the ability of their approach to accomodate different levels of granularity automatically, no evidence is provided that the hierarchy provides real benefit. At the same time, our work shows that significant gains can be realized with a single, sufficiently granular partition of terms. It is known, moreover, that greedy agglomerative clustering leads to partitions that are sub-optimal in terms of a mutual information objective function (see, for example, Brown, et al (1992)). Ultimately, it is left to future research to determine how sensitive, if at all, the NER gains are to the details of the clustering. 5 Conclusion There are several ways in which this work might be extended and improved, both in its particular form and in general: BWI models initial and terminal boundaries, but ignores characteristics of the extracted phrase other than its length. We are exploring mechanisms for modeling relevant phrasal structure. While global statistical approaches, such as se0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 PERSON (clust.) PERSON (base) ORG. (clust.) ORG. (base) quenti</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>P.F. Brown, V.J. Della Pietra, P.V. deSouza, J.C. Lai, and R.L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L M`arquez</author>
<author>L Padr´o</author>
</authors>
<title>Named entity extraction using AdaBoost.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002,</booktitle>
<location>Taipei, Taiwan.</location>
<marker>Carreras, M`arquez, Padr´o, 2002</marker>
<rawString>X. Carreras, L. M`arquez, and L. Padr´o. 2002. Named entity extraction using AdaBoost. In Proceedings of CoNLL-2002, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Clark</author>
</authors>
<title>Inducing syntactic categories by context distribution clustering.</title>
<date>2000</date>
<booktitle>In CoNLL</booktitle>
<contexts>
<context position="2966" citStr="Clark, 2000" startWordPosition="440" endWordPosition="441">ments, typically by bootstrapping a classifier using automatic labellings (Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Thelen and Riloff, 2002). Here, we investigate a different approach. Using a distributional clustering technique called coclustering, we produce clusters which, intuitively, should be useful for NER. Table 1 shows example terms from several sample clusters induced using a collection of documents from the Wall Street Journal (WSJ). Several papers have shown that distributional clustering yields categories that have high agreement with part of speech (Sch¨utze, 1995; Clark, 2000). As the table illustrates, these clusters also tend to have a useful semantic dimension. Clustering on the WSJ portion of the North American News corpus yields two clusters that clearly correspond to personal names, one for first names and one for last names. As an experiment, we scanned the MUC6 NER data set for token sequences consisting of zero or more members of the first name cluster (or an initial followed by a period), followed by one or more members of the last name cluster. This simple procedure identified 64% of personal names with 77% precision. In this paper, we attempt to improve</context>
</contexts>
<marker>Clark, 2000</marker>
<rawString>A. Clark. 2000. Inducing syntactic categories by context distribution clustering. In CoNLL 2000, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>Y Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proc. 1999 Joint SIGDAT Conference on Empirical Methods in NLP and Very Large Corpora.</booktitle>
<contexts>
<context position="2453" citStr="Collins and Singer, 1999" startWordPosition="360" endWordPosition="363">zetteers. While supervised training greatly facilitates the development of a robust NER system, the requirement of a substantial training corpus remains an impediment to the rapid deployment of NER in new domains or new languages. A number bush peters reagan noriega... john robert james david... president chairman head owner... japan california london chicago... Table 1: Sample members of four clusters from the Wall Street Journal corpus. of researchers have therefore sought to exploit the availability of unlabeled documents, typically by bootstrapping a classifier using automatic labellings (Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Thelen and Riloff, 2002). Here, we investigate a different approach. Using a distributional clustering technique called coclustering, we produce clusters which, intuitively, should be useful for NER. Table 1 shows example terms from several sample clusters induced using a collection of documents from the Wall Street Journal (WSJ). Several papers have shown that distributional clustering yields categories that have high agreement with part of speech (Sch¨utze, 1995; Clark, 2000). As the table illustrates, these clusters also tend to have a useful semantic dimensio</context>
<context position="26245" citStr="Collins and Singer (1999)" startWordPosition="4258" endWordPosition="4261">t all training set sizes. Either the baseline feature set is ultimately too impoverished for this task, or, more likely, the complete MUC 6 training set (318 documents) is small for this class of learner. Techniques to lessen the need for annotation for NER have received a fair amount of attention recently. The prevailing approach to this problem is a bootstrapping technique, in which, starting with a few hand-labeled examples, the system iteratively adds automatic labels to a corpus, training itself, as it were. Examples of this are Cucerzan and Yarowsky (1999), Thelen and Riloff (2002), and Collins and Singer (1999). These techniques address the same problem as this paper, but are otherwise quite different from the work described here. The labeling method (seeding) is an indirect form of corpus annotation. The promise of all such approaches is that, by starting with a small number of seeds, reasonable results can be achieved at low expense. However, it is difficult to tell how much labeling corresponds to a given number of seeds, since this depends on the coverage of the seeds. Note, too, that any bootstrapping approach must confront the problem of instability; poor initial decisions by a bootstrapping a</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>M. Collins and Y. Singer. 1999. Unsupervised models for named entity classification. In Proc. 1999 Joint SIGDAT Conference on Empirical Methods in NLP and Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP-2002.</booktitle>
<contexts>
<context position="27833" citStr="Collins (2002)" startWordPosition="4517" endWordPosition="4518">is one. Of course, comparisons must be strongly qualified, given the different labeling methods and data sets. Nevertheless, performance of cluster-enhanced BWI at the low end of the horizontal axis compares favorably with the English F1 performance of 0.543 they report using 190 seed words. And, arguably, annotating 10-20 documents is no more labor intensive than assembling a list of 190 seed words. Strong corroboration for the approach advocated in this paper is provided by Miller, et al (2004), in which cluster-based features are combined with a sequential maximum entropy model proposed in Collins (2002) to advance the state of the art. In addition, using active learning, the authors are able to reduce human labeling effort by an order of magnitude. Miller, et al, use a proprietary data set for training and testing, so it is difficult to make a close comparison of outcomes. At roughly comparable training set sizes, they appear to achieve a score of about 0.89 (F1) with a “conventional” HMM, versus 0.93 using the discriminative learner trained with cluster features (compared with 0.86 reached by BWI). Both the HMM and Collins model are constrained to account for an entire sentence in tagging i</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
<author>D Yarowsky</author>
</authors>
<title>Language independent named entity recognition combining morphological and contextual evidence.</title>
<date>1999</date>
<booktitle>In Proc. 1999 Joint SIGDAT Conference on Empirical Methods in NLP and Very Large Corpora,</booktitle>
<pages>90--99</pages>
<contexts>
<context position="2482" citStr="Cucerzan and Yarowsky, 1999" startWordPosition="364" endWordPosition="367"> training greatly facilitates the development of a robust NER system, the requirement of a substantial training corpus remains an impediment to the rapid deployment of NER in new domains or new languages. A number bush peters reagan noriega... john robert james david... president chairman head owner... japan california london chicago... Table 1: Sample members of four clusters from the Wall Street Journal corpus. of researchers have therefore sought to exploit the availability of unlabeled documents, typically by bootstrapping a classifier using automatic labellings (Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Thelen and Riloff, 2002). Here, we investigate a different approach. Using a distributional clustering technique called coclustering, we produce clusters which, intuitively, should be useful for NER. Table 1 shows example terms from several sample clusters induced using a collection of documents from the Wall Street Journal (WSJ). Several papers have shown that distributional clustering yields categories that have high agreement with part of speech (Sch¨utze, 1995; Clark, 2000). As the table illustrates, these clusters also tend to have a useful semantic dimension. Clustering on the WSJ port</context>
<context position="24030" citStr="Cucerzan and Yarowsky (1999)" startWordPosition="3889" endWordPosition="3892">ly engineered semantic categories, whereas we eschew knowledge- and labor-intensive resources. This has important implications for the portability of the approaches to new domains and languages. By taking a few post-processing steps, it is possible to realize further improvements. For example, the learner occasionally identifies terms and phrases which some simple rules can reliably reject. By suppressing any prediction that consists entirely of a stopword, we increase the precision of both ORGANIZATION and LOCATION to 0.86 (from 0.84 and 0.80) and overall F1 to 0.88. We can also exploit what Cucerzan and Yarowsky (1999) call the one sense per discourse phenomenon, the tendency of terms to have a fixed meaning within a single document. By marking up unmarked strings that match extracted entity instances in the same document, we can improve the recall of some fields. We added this postprocessing step for the PERSON and ORGANIZATION fields. This increased recall of PERSON from 0.95 to 0.98 and of ORGANIZATION from 0.74 to 0.79 with minimal changes to precision and a slight improvement in summary F1. 4.4 Analysis and Related Work The promise of this general method—supervised learning on small training set using </context>
<context position="26188" citStr="Cucerzan and Yarowsky (1999)" startWordPosition="4249" endWordPosition="4252">n). At the same time, the improvement is fairly consistent at all training set sizes. Either the baseline feature set is ultimately too impoverished for this task, or, more likely, the complete MUC 6 training set (318 documents) is small for this class of learner. Techniques to lessen the need for annotation for NER have received a fair amount of attention recently. The prevailing approach to this problem is a bootstrapping technique, in which, starting with a few hand-labeled examples, the system iteratively adds automatic labels to a corpus, training itself, as it were. Examples of this are Cucerzan and Yarowsky (1999), Thelen and Riloff (2002), and Collins and Singer (1999). These techniques address the same problem as this paper, but are otherwise quite different from the work described here. The labeling method (seeding) is an indirect form of corpus annotation. The promise of all such approaches is that, by starting with a small number of seeds, reasonable results can be achieved at low expense. However, it is difficult to tell how much labeling corresponds to a given number of seeds, since this depends on the coverage of the seeds. Note, too, that any bootstrapping approach must confront the problem of</context>
</contexts>
<marker>Cucerzan, Yarowsky, 1999</marker>
<rawString>S. Cucerzan and D. Yarowsky. 1999. Language independent named entity recognition combining morphological and contextual evidence. In Proc. 1999 Joint SIGDAT Conference on Empirical Methods in NLP and Very Large Corpora, pages 90–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I S Dhillon</author>
<author>S Mallela</author>
<author>D S Modha</author>
</authors>
<title>Information-theoretic co-clustering.</title>
<date>2003</date>
<tech>Technical Report TR-03-12,</tech>
<institution>Dept. of Computer Science, U. Texas at Austin.</institution>
<contexts>
<context position="10040" citStr="Dhillon et al., 2003" startWordPosition="1585" endWordPosition="1588">tections are then used to determine which tokens match the “special” wildcards used by and . Then, instead of pairing predictions with those of , they are paired with those made by (and with ). In informal experiments, we found that this procedure tended to increase F1 performance by several points on a range of tasks. We adopt it uniformly in the experiments reported here. 3 Co-Clustering As in Brown, et al (1992), we seek a partition of the vocabulary that maximizes the mutual information between term categories and their contexts. To achieve this, we use information theoretic coclustering (Dhillon et al., 2003), in which a space of entities, on the one hand, and their contexts, on the other, are alternately clustered to maximize mutual information between the two spaces. 3.1 Background The input to our algorithm is two finite sets of symbols, say (e.g., terms) and (e.g., term contexts), together with a set of co-occurrence count data consisting of a non-negative integer for every pair of symbols from and . The output is two partitions: and , where each is a subset of (a “cluster”), and each a subset of . The co-clustering algorithm chooses the partitions and to (locally) maximize the mutual informat</context>
</contexts>
<marker>Dhillon, Mallela, Modha, 2003</marker>
<rawString>I. S. Dhillon, S. Mallela, and D. S. Modha. 2003. Information-theoretic co-clustering. Technical Report TR-03-12, Dept. of Computer Science, U. Texas at Austin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Freitag</author>
<author>N Kushmerick</author>
</authors>
<title>Boosted wrapper induction.</title>
<date>2000</date>
<booktitle>In Proc. 17th National Conference on Artificial Intelligence (AAAI-2000),</booktitle>
<contexts>
<context position="3930" citStr="Freitag and Kushmerick, 2000" startWordPosition="599" endWordPosition="603">sisting of zero or more members of the first name cluster (or an initial followed by a period), followed by one or more members of the last name cluster. This simple procedure identified 64% of personal names with 77% precision. In this paper, we attempt to improve on this result by converting the clusters into features to be exploited by a general-purpose machine learning algorithm for information extraction. In Section 2, we provide a brief description of Boosted Wrapper Induction (BWI), a pattern learner that has yielded promising results on semi-structured information extraction problems (Freitag and Kushmerick, 2000). In Section 3, we describe our clustering approach and its particular application. Section 4 presents the results of our experiments. Finally, in Section 5, we assess the significance of our contribution and attempt to identify promising future directions. 2 BWI BWI decomposes the problem of recognizing field instances into two Boolean classification problems: recognizing field-initial and field-terminal tokens. Given a target field, a separate classifier is learned for each of these problems, and the distribution of field lengths is modeled as a frequency histogram. At application time, toke</context>
<context position="5537" citStr="Freitag and Kushmerick, 2000" startWordPosition="853" endWordPosition="856">. The confidence of a boundary detection is its strength as determined by AdaBoost, while that of the length assessment is the empirical length probability, which is determined using the length histogram. The confidence of the full prediction is the product of these three individual confidence scores. In the event that overlapping predictions are found in this way (a rare event, empirically), the predictions with lower confidence are discarded. In this section, we sketch those aspects of BWI relevant to the current application. More details are available in the paper in which BWI was defined (Freitag and Kushmerick, 2000). 2.1 Boosting BWI uses generalized AdaBoost to produce each boundary classifier (Schapire and Singer, 1998). Boosting is a procedure for improving the performance of a “weak learner” by repeatedly applying it to a training set, at each step modifying example weights to emphasize those examples on which the learner has done poorly in previous steps. The output is a weighted collection of weak learner hypotheses. Classification involves having the individual hypotheses “vote,” with strengths proportional to their weights, and summing overlapping votes. Although this is the first application of </context>
</contexts>
<marker>Freitag, Kushmerick, 2000</marker>
<rawString>D. Freitag and N. Kushmerick. 2000. Boosted wrapper induction. In Proc. 17th National Conference on Artificial Intelligence (AAAI-2000), August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>W Li</author>
</authors>
<title>Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons.</title>
<date>2003</date>
<booktitle>In Proc. 7th Conference on Natural Language Learning (CoNLL-03).</booktitle>
<contexts>
<context position="29897" citStr="McCallum and Li, 2003" startWordPosition="4853" endWordPosition="4857">re research to determine how sensitive, if at all, the NER gains are to the details of the clustering. 5 Conclusion There are several ways in which this work might be extended and improved, both in its particular form and in general: BWI models initial and terminal boundaries, but ignores characteristics of the extracted phrase other than its length. We are exploring mechanisms for modeling relevant phrasal structure. While global statistical approaches, such as se0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 PERSON (clust.) PERSON (base) ORG. (clust.) ORG. (base) quential averaged perceptrons or CRFs (McCallum and Li, 2003), appear better suited to the NER problem than local symbolic learners, the two approaches search different hypothesis spaces. Based on the surmise that, by combining them, we can realize improvements over either in isolation, we are exploring mechanisms for integration. The distributional clusters we find are independent of the problem to which we want to apply them and may sometimes be inappropriate or have the wrong granularity. We are exploring ways to produce groupings that are sensitive to the task at hand. Our results clearly establish that an unsupervised distributional analysis of a t</context>
</contexts>
<marker>McCallum, Li, 2003</marker>
<rawString>A. McCallum and W. Li. 2003. Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons. In Proc. 7th Conference on Natural Language Learning (CoNLL-03).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
<author>J Guinness</author>
<author>A Zamanian</author>
</authors>
<title>Name tagging with word clusters and discriminative training.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL 04.</booktitle>
<contexts>
<context position="27720" citStr="Miller, et al (2004)" startWordPosition="4499" endWordPosition="4502">erzan and Yarowsky do not presuppose a syntactic analysis of the corpus, so their work is perhaps most comparable to this one. Of course, comparisons must be strongly qualified, given the different labeling methods and data sets. Nevertheless, performance of cluster-enhanced BWI at the low end of the horizontal axis compares favorably with the English F1 performance of 0.543 they report using 190 seed words. And, arguably, annotating 10-20 documents is no more labor intensive than assembling a list of 190 seed words. Strong corroboration for the approach advocated in this paper is provided by Miller, et al (2004), in which cluster-based features are combined with a sequential maximum entropy model proposed in Collins (2002) to advance the state of the art. In addition, using active learning, the authors are able to reduce human labeling effort by an order of magnitude. Miller, et al, use a proprietary data set for training and testing, so it is difficult to make a close comparison of outcomes. At roughly comparable training set sizes, they appear to achieve a score of about 0.89 (F1) with a “conventional” HMM, versus 0.93 using the discriminative learner trained with cluster features (compared with 0.</context>
</contexts>
<marker>Miller, Guinness, Zamanian, 2004</marker>
<rawString>S. Miller, J. Guinness, and A. Zamanian. 2004. Name tagging with word clusters and discriminative training. In Proceedings of HLT/NAACL 04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong Kim Sang</author>
</authors>
<title>Introduction to the CoNLL-2002 shared task: Languageindependent named entity recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002.</booktitle>
<contexts>
<context position="6814" citStr="Sang, 2002" startWordPosition="1053" endWordPosition="1054">s problem. Differing from BWI in the details of the application, two recent papers nevertheless demonstrate the effectiveness of the boosting Cap Initial capital AllCap All capitals Uncap Initial lower case Alpha Entirely alphabetic characters ANum Entirely alpha-numeric characters Punc Punctuation Num Entirely numeric characters Schar Single alphabetic character Any Anything Table 2: Default wildcards used in these experiments. paradigm for NER in several languages (Carreras et al., 2002; Wu et al., 2002), one of them achieving the best overall performance in a comparison of several systems (Sang, 2002). 2.2 Boundary Detectors The output of a single invocation of the weak learner in BWI is always an individual pattern, called a boundary detector. A detector has two parts, one to match the text leading up to a boundary, the other for trailing text. Each part is a list of zero or more elements. In order for a boundary to match a detector, the tokens preceding the boundary (or following it) must match the corresponding elements in sequence. For example, the detector [ms .][jones] matches boundaries preceded by the (case-normalized) two-token sequence “ms .” and followed by the single token “jon</context>
</contexts>
<marker>Sang, 2002</marker>
<rawString>E. F. Tjong Kim Sang. 2002. Introduction to the CoNLL-2002 shared task: Languageindependent named entity recognition. In Proceedings of CoNLL-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Schapire</author>
<author>Y Singer</author>
</authors>
<title>Improved boosting algorithms using confidence-rated predictions.</title>
<date>1998</date>
<booktitle>In Proc. 11th Annual Conference on Computational Learning Theory (COLT-98),</booktitle>
<pages>80--91</pages>
<contexts>
<context position="5645" citStr="Schapire and Singer, 1998" startWordPosition="868" endWordPosition="871">sessment is the empirical length probability, which is determined using the length histogram. The confidence of the full prediction is the product of these three individual confidence scores. In the event that overlapping predictions are found in this way (a rare event, empirically), the predictions with lower confidence are discarded. In this section, we sketch those aspects of BWI relevant to the current application. More details are available in the paper in which BWI was defined (Freitag and Kushmerick, 2000). 2.1 Boosting BWI uses generalized AdaBoost to produce each boundary classifier (Schapire and Singer, 1998). Boosting is a procedure for improving the performance of a “weak learner” by repeatedly applying it to a training set, at each step modifying example weights to emphasize those examples on which the learner has done poorly in previous steps. The output is a weighted collection of weak learner hypotheses. Classification involves having the individual hypotheses “vote,” with strengths proportional to their weights, and summing overlapping votes. Although this is the first application of BWI to NER, boosting has previously been shown to work well on this problem. Differing from BWI in the detai</context>
</contexts>
<marker>Schapire, Singer, 1998</marker>
<rawString>R.E. Schapire and Y. Singer. 1998. Improved boosting algorithms using confidence-rated predictions. In Proc. 11th Annual Conference on Computational Learning Theory (COLT-98), pages 80–91, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sch¨utze</author>
</authors>
<title>Distributional part-of-speech tagging.</title>
<date>1995</date>
<booktitle>In Proc. 7th EACL Conference (EACL95),</booktitle>
<marker>Sch¨utze, 1995</marker>
<rawString>H. Sch¨utze. 1995. Distributional part-of-speech tagging. In Proc. 7th EACL Conference (EACL95), March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Thelen</author>
<author>E Riloff</author>
</authors>
<title>A bootstrapping method for learning semantic lexicons using extraction pattern contexts.</title>
<date>2002</date>
<booktitle>In Proc. 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<contexts>
<context position="2508" citStr="Thelen and Riloff, 2002" startWordPosition="368" endWordPosition="371"> the development of a robust NER system, the requirement of a substantial training corpus remains an impediment to the rapid deployment of NER in new domains or new languages. A number bush peters reagan noriega... john robert james david... president chairman head owner... japan california london chicago... Table 1: Sample members of four clusters from the Wall Street Journal corpus. of researchers have therefore sought to exploit the availability of unlabeled documents, typically by bootstrapping a classifier using automatic labellings (Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Thelen and Riloff, 2002). Here, we investigate a different approach. Using a distributional clustering technique called coclustering, we produce clusters which, intuitively, should be useful for NER. Table 1 shows example terms from several sample clusters induced using a collection of documents from the Wall Street Journal (WSJ). Several papers have shown that distributional clustering yields categories that have high agreement with part of speech (Sch¨utze, 1995; Clark, 2000). As the table illustrates, these clusters also tend to have a useful semantic dimension. Clustering on the WSJ portion of the North American </context>
<context position="26214" citStr="Thelen and Riloff (2002)" startWordPosition="4253" endWordPosition="4256">ovement is fairly consistent at all training set sizes. Either the baseline feature set is ultimately too impoverished for this task, or, more likely, the complete MUC 6 training set (318 documents) is small for this class of learner. Techniques to lessen the need for annotation for NER have received a fair amount of attention recently. The prevailing approach to this problem is a bootstrapping technique, in which, starting with a few hand-labeled examples, the system iteratively adds automatic labels to a corpus, training itself, as it were. Examples of this are Cucerzan and Yarowsky (1999), Thelen and Riloff (2002), and Collins and Singer (1999). These techniques address the same problem as this paper, but are otherwise quite different from the work described here. The labeling method (seeding) is an indirect form of corpus annotation. The promise of all such approaches is that, by starting with a small number of seeds, reasonable results can be achieved at low expense. However, it is difficult to tell how much labeling corresponds to a given number of seeds, since this depends on the coverage of the seeds. Note, too, that any bootstrapping approach must confront the problem of instability; poor initial</context>
</contexts>
<marker>Thelen, Riloff, 2002</marker>
<rawString>M. Thelen and E. Riloff. 2002. A bootstrapping method for learning semantic lexicons using extraction pattern contexts. In Proc. 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP 2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
<author>G Ngai</author>
<author>M Carpuat</author>
<author>J Larsen</author>
<author>Y Yang</author>
</authors>
<title>Boosting for named entity recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002,</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="6714" citStr="Wu et al., 2002" startWordPosition="1034" endWordPosition="1037">hough this is the first application of BWI to NER, boosting has previously been shown to work well on this problem. Differing from BWI in the details of the application, two recent papers nevertheless demonstrate the effectiveness of the boosting Cap Initial capital AllCap All capitals Uncap Initial lower case Alpha Entirely alphabetic characters ANum Entirely alpha-numeric characters Punc Punctuation Num Entirely numeric characters Schar Single alphabetic character Any Anything Table 2: Default wildcards used in these experiments. paradigm for NER in several languages (Carreras et al., 2002; Wu et al., 2002), one of them achieving the best overall performance in a comparison of several systems (Sang, 2002). 2.2 Boundary Detectors The output of a single invocation of the weak learner in BWI is always an individual pattern, called a boundary detector. A detector has two parts, one to match the text leading up to a boundary, the other for trailing text. Each part is a list of zero or more elements. In order for a boundary to match a detector, the tokens preceding the boundary (or following it) must match the corresponding elements in sequence. For example, the detector [ms .][jones] matches boundari</context>
</contexts>
<marker>Wu, Ngai, Carpuat, Larsen, Yang, 2002</marker>
<rawString>D. Wu, G. Ngai, M. Carpuat, J. Larsen, and Y. Yang. 2002. Boosting for named entity recognition. In Proceedings of CoNLL-2002, Taipei, Taiwan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>