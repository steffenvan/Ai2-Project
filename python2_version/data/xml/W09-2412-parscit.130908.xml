<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.025253">
<title confidence="0.926922">
SemEval-2010 Task 2: Cross-Lingual Lexical Substitution
</title>
<author confidence="0.996605">
Ravi Sinha Diana McCarthy Rada Mihalcea
</author>
<affiliation confidence="0.998878">
University of North Texas University of Sussex University of North Texas
</affiliation>
<email confidence="0.997882">
ravisinha@unt.edu dianam@sussex.ac.uk rada@cs.unt.edu
</email>
<sectionHeader confidence="0.997054" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.976731944444445">
In this paper we describe the SemEval-
2010 Cross-Lingual Lexical Substitution task,
which is based on the English Lexical Substi-
tution task run at SemEval-2007. In the En-
glish version of the task, annotators and sys-
tems had to find an alternative substitute word
or phrase for a target word in context. In this
paper we propose a task where the target word
and contexts will be in English, but the substi-
tutes will be in Spanish. In this paper we pro-
vide background and motivation for the task
and describe how the dataset will differ from
a machine translation task and previous word
sense disambiguation tasks based on parallel
data. We describe the annotation process and
how we anticipate scoring the system output.
We finish with some ideas for participating
systems.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999732952380952">
The Cross-Lingual Lexical Substitution task is
based on the English Lexical Substitution task run at
SemEval-2007. In the 2007 English Lexical Substi-
tution Task, annotators and systems had to find an al-
ternative substitute word or phrase for a target word
in context. In this cross-lingual task the target word
and contexts will be in English, but the substitutes
will be in Spanish.
An automatic system for cross-lingual lexical sub-
stitution would be useful for a number of applica-
tions. For instance, such a system could be used
to assist human translators in their work, by provid-
ing a number of correct translations that the human
translator can choose from. Similarly, the system
could be used to assist language learners, by pro-
viding them with the interpretation of the unknown
words in a text written in the language they are learn-
ing. Last but not least, the output of a cross-lingual
lexical substitution system could be used as input to
existing systems for cross-language information re-
trieval or automatic machine translation.
</bodyText>
<sectionHeader confidence="0.8893695" genericHeader="introduction">
2 Background: The English Lexical
Substitution Task
</sectionHeader>
<bodyText confidence="0.99948725">
The English Lexical substitution task (hereafter re-
ferred to as LEXSUB) was run at SemEval-2007 fol-
lowing earlier ideas on a method of testing WSD
systems without predetermining the inventory (Mc-
Carthy, 2002). The issue of which inventory is ap-
propriate for the task has been a long standing is-
sue for debate, and while there is hope that coarse-
grained inventories will allow for increased system
performance (Ide and Wilks, 2006) we do not yet
know if these will make the distinctions that will
most benefit practical systems (Stokoe, 2005) or re-
flect cognitive processes (Kilgarriff, 2006). LEXSUB
was proposed as a task which, while requiring con-
textual disambiguation, did not presuppose a spe-
cific sense inventory. In fact, it is quite possible to
use alternative representations of meaning (Sch¨utze,
1998; Pantel and Lin, 2002).
The motivation for a substitution task was that it
would reflect capabilities that might be useful for
natural language processing tasks such as paraphras-
ing and textual entailment, while only focusing on
one aspect of the problem and therefore not requir-
ing a complete system that might mask system capa-
bilities at a lexical level and at the same time make
</bodyText>
<page confidence="0.921428">
76
</page>
<note confidence="0.8066375">
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 76–81,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.99913434375">
participation in the task difficult for small research
teams.
The task required systems to produce a substitute
word for a word in context. For example a substitute
of tournament might be given for the second oc-
currence of match (shown in bold) in the following
sentence:
The ideal preparation would be a light meal
about 2-2 1/2 hours pre-match, followed by a
warm-up hit and perhaps a top-up with extra fluid
before the match.
In LEXSUB, the data was collected for 201 words
from open class parts-of-speech (PoS) (i.e. nouns,
verbs, adjectives and adverbs). Words were selected
that have more than one meaning with at least one
near synonym. Ten sentences for each word were
extracted from the English Internet Corpus (Sharoff,
2006). There were five annotators who annotated
each target word as it occurred in the context of a
sentence. The annotators were each allowed to pro-
vide up to three substitutes, though they could also
provide a NIL response if they could not come up
with a substitute. They had to indicate if the target
word was an integral part of a multiword.
A development and test dataset were provided,
but no training data. Any system that relied on train-
ing data, such as sense annotated corpora, had to use
resources available from other sources. The task had
eight participating teams. Teams were allowed to
submit up to two systems and there were a total of
ten different systems. The scoring was conducted
using recall and precision measures using:
</bodyText>
<listItem confidence="0.993576">
• the frequency distribution of responses from
the annotators and
• the mode of the annotators (the most frequent
response).
</listItem>
<bodyText confidence="0.9987194">
The systems were scored using their best guess as
well as an out-of-ten score which allowed up to 10
attempts. 1 The results are reported in McCarthy and
Navigli (2007) and in more detail in McCarthy and
Navigli (in press).
</bodyText>
<footnote confidence="0.996602666666667">
1The details are available at
http://nlp.cs.swarthmore.edu/semeval/tasks/
task10/task10documentation.pdf.
</footnote>
<sectionHeader confidence="0.725721" genericHeader="method">
3 Motivation and Related Work
</sectionHeader>
<bodyText confidence="0.997015355555556">
While there has been a lot of discussion on the rel-
evant sense distinctions for monolingual WSD sys-
tems, for machine translation applications there is
a consensus that the relevant sense distinctions are
those that reflect different translations. One early
and notable work was the SENSEVAL-2 Japanese
Translation task (Kurohashi, 2001) that obtained al-
ternative translation records of typical usages of a
test word, also referred to as a translation mem-
ory. Systems could either select the most appropri-
ate translation memory record for each instance and
were scored against a gold-standard set of annota-
tions, or they could provide a translation that was
scored by translation experts after the results were
submitted. In contrast to this work, we propose to
provide actual translations for target instances in ad-
vance, rather than predetermine translations using
lexicographers or rely on post-hoc evaluation, which
does not permit evaluation of new systems after the
competition.
Previous standalone WSD tasks based on parallel
data have obtained distinct translations for senses as
listed in a dictionary (Ng and Chan, 2007). In this
way fine-grained senses with the same translations
can be lumped together, however this does not fully
allow for the fact that some senses for the same
words may have some translations in common but
also others that are not. An example from Resnik
and Yarowsky (2000) (table 4 in that paper) is the
first two senses from WordNet for the noun interest:
WordNet sense Spanish Translation
monetary e.g. on loan inter´es, r´edito
stake/share inter´es,participaci´on
For WSD tasks, a decision can be made to lump
senses with such overlap, or split them using the dis-
tinctive translation and then use the distinctive trans-
lations as a sense inventory. This sense inventory is
then used to collect training from parallel data (Ng
and Chan, 2007). We propose that it would be in-
teresting to collect a dataset where the overlap in
translations for an instance can remain and that this
will depend on the token instance rather than map-
ping to a pre-defined sense inventory. Resnik and
Yarowsky (2000) also conducted their experiments
using words in context, rather than a predefined
</bodyText>
<page confidence="0.991461">
77
</page>
<bodyText confidence="0.999958666666667">
sense-inventory as in (Ng and Chan, 2007; Chan and
Ng, 2005), however in these experiments the anno-
tators were asked for a single preferred translation.
We intend to allow annotators to supply as many
translations as they feel are equally valid. This will
allow us to examine more subtle relationships be-
tween usages and to allow partial credit to systems
which get a close approximation to the annotators’
translations. Unlike a full blown machine transla-
tion task (Carpuat and Wu, 2007), annotators and
systems will not be required to translate the whole
context but just the target word.
</bodyText>
<sectionHeader confidence="0.9697265" genericHeader="method">
4 The Cross-Lingual Lexical Substitution
Task
</sectionHeader>
<bodyText confidence="0.9999665">
Here we discuss our proposal for a Cross-Lingual
Lexical Substitution task. The task will follow LEX-
SUB except that the annotations will be translations
rather than paraphrases.
Given a target word in context, the task is to pro-
vide several correct translations for that word in a
given language. We will use English as the source
language and Spanish as the target language. Mul-
tiwords are ‘part and parcel’ of natural language.
For this reason, rather than try and filter multiwords,
which is very hard to do without assuming a fixed
inventory, 2 we will ask annotators to indicate where
the target word is part of a multiword and what that
multiword is. This way, we know what the substitute
translation is replacing.
We will provide both development and test sets,
but no training data. As for LEXSUB, any sys-
tems requiring data will need to obtain it from other
sources. We will include nouns, verbs, adjectives
and adverbs in both development and test data. Un-
like LEXSUB, the annotators will be told the PoS of
the current target word.
</bodyText>
<subsectionHeader confidence="0.980214">
4.1 Annotation
</subsectionHeader>
<bodyText confidence="0.9999816">
We are going to use four annotators for our task, all
native Spanish speakers from Mexico, with a high
level of proficiency in English. The annotation in-
terface is shown in figure 1. We will calculate inter-
tagger agreement as pairwise agreement between
</bodyText>
<footnote confidence="0.7448425">
2The multiword inventories that do exist are far from com-
plete.
</footnote>
<bodyText confidence="0.966951">
sets of substitutes from annotators, as was done in
LEXSUB.
</bodyText>
<subsectionHeader confidence="0.996572">
4.2 An Example
</subsectionHeader>
<bodyText confidence="0.9999942">
One significant outcome of this task is that there
will not necessarily be clear divisions between us-
ages and senses because we do not use a predefined
sense inventory, or restrict the annotations to dis-
tinctive translations. This will mean that there can
be usages that overlap to different extents with each
other but do not have identical translations. An ex-
ample from our preliminary annotation trials is the
target adverb severely. Four sentences are shown in
figure 2 with the translations provided by one an-
notator marked in italics and {} braces. Here, all
the token occurrences seem related to each other in
that they share some translations, but not all. There
are sentences like 1 and 2 that appear not to have
anything in common. However 1, 3, and 4 seem to
be partly related (they share severamente), and 2, 3,
and 4 are also partly related (they share seriamente).
When we look again, sentences 1 and 2, though not
directly related, both have translations in common
with sentences 3 and 4.
</bodyText>
<subsectionHeader confidence="0.997775">
4.3 Scoring
</subsectionHeader>
<bodyText confidence="0.999974333333333">
We will adopt the best and out-of-ten precision and
recall scores from LEXSUB. The systems can supply
as many translations as they feel fit the context. The
system translations will be given credit depending
on the number of annotators that picked each trans-
lation. The credit will be divided by the number of
annotator responses for the item and since for the
best score the credit for the system answers for an
item is also divided by the number of answers the
system provides, this allows more credit to be given
to instances where there is less variation. For that
reason, a system is better guessing the translation
that is most frequent unless it really wants to hedge
its bets. Thus if i is an item in the set of instances
I, and Ti is the multiset of gold standard translations
from the human annotators for i, and a system pro-
vides a set of answers Si for i, then the best score
for item i will be:
</bodyText>
<equation confidence="0.966556666666667">
�s∈S, frequency(s E Ti)
best score(i) = (1)
|Si |· |Ti|
</equation>
<page confidence="0.99465">
78
</page>
<figureCaption confidence="0.999725">
Figure 1: The Cross-Lingual Lexical Substitution Interface
</figureCaption>
<listItem confidence="0.992133777777778">
1. Perhaps the effect of West Nile Virus is sufficient to extinguish endemic birds already severely stressed
by habitat losses. {fuertemente, severamente, duramente, exageradamente}
2. She looked as severely as she could muster at Draco. {rigurosamente, seriamente}
3. A day before he was due to return to the United States Patton was severely injured in a road accident.
{seriamente, duramente, severamente}
4. Use market tools to address environmental issues , such as eliminating subsidies for industries that
severely harm the environment, like coal. {peligrosamente, seriamente, severamente}
5. This picture was severely damaged in the flood of 1913 and has rarely been seen until now. {altamente,
seriamente, exageradamente}
</listItem>
<figureCaption confidence="0.99584">
Figure 2: Translations from one annotator for the adverb severely
</figureCaption>
<bodyText confidence="0.9982065">
Precision is calculated by summing the scores for
each item and dividing by the number of items that
the system attempted whereas recall divides the sum
of scores for each item by |I|. Thus:
</bodyText>
<equation confidence="0.985989">
best score(i)
best precision = Ei (2)
|i E I : defined(Si) |
E best score(i)
best recall Z = (3)
|I|
</equation>
<bodyText confidence="0.9997452">
The out-of-ten scorer will allow up to ten system
responses and will not divide the credit attributed
to each answer by the number of system responses.
This allows the system to be less cautious and for
the fact that there is considerable variation on the
task and there may be cases where systems select a
perfectly good translation that the annotators had not
thought of. By allowing up to ten translations in the
out-of-ten task the systems can hedge their bets to
find the translations that the annotators supplied.
</bodyText>
<equation confidence="0.9799345">
Es∈Si frequency(s E Ti)
oot score(i) = (4)
|Ti|
EZ oot score(i)
oot precision = (5)
|i E I : defined(Si)|
79
score(i)
oot recall = i oot (6)
|I|
</equation>
<bodyText confidence="0.938082925925926">
We will refine the scores before June 2009 when
we will release the development data for this cross-
lingual task. We note that there was an issue that the
original LEXSUB out-of-ten scorer allowed dupli-
cates (McCarthy and Navigli, in press). The effect
of duplicates is that systems can get inflated scores
because the credit for each item is not divided by
the number of substitutes and because the frequency
of each annotator response is used. McCarthy and
Navigli (in press) describe this oversight, identify
the systems that had included duplicates and explain
the implications. For our task there is an option for
the out-of-ten score. Either:
1. we remove duplicates before scoring or,
2. we allow duplicates so that systems can boost
their scores with duplicates on translations with
higher probability
We will probably allow duplicates but make this
clear to participants.
We may calculate additional best and out-of-ten
scores against the mode from the annotators re-
sponses as was done in LEXSUB, but we have not
decided on this yet. We will not run a multiword
task, but we will use the items identified as multi-
words as an optional filter to the scoring i.e. to see
how systems did without these items.
We will provide baselines and upper-bounds.
</bodyText>
<sectionHeader confidence="0.989713" genericHeader="method">
5 Systems
</sectionHeader>
<bodyText confidence="0.9995495">
In the cross-lingual LEXSUB task, the systems will
have to deal with two parts of the problem, namely:
</bodyText>
<listItem confidence="0.9891525">
1. candidate collection
2. candidate selection
</listItem>
<bodyText confidence="0.999975695652174">
The first sub-task, candidate collection, refers to
consulting several resources and coming up with a
list of potential translation candidates for each tar-
get word and part of speech. We do not provide any
inventories, as with the original LEXSUB task, and
thus leave this task of coming up with the most suit-
able translation list (in contrast to the synonym list
required for LEXSUB) to the participants. As was
observed with LEXSUB, it is our intuition that the
quality of this translation list that the systems come
up with will determine to a large extent how well
the final performance of the system will be. Partici-
pants are free to use any ideas. However, a few pos-
sibilities might be to use parallel corpora, bilingual
dictionaries, a translation engine that only translates
the target word, or a machine translation system that
translates the entire sentences. Several of the bilin-
gual dictionaries or even other resources might be
combined together to come up with a comprehen-
sive translation candidate list, if that seems to im-
prove performance.
The second phase, candidate selection, concerns
fitting the translation candidates in context, and thus
coming up with a ranking as to which translations
are the most suitable for each instance. The highest
ranking candidate will be the output for best, and the
list of the top 10 ranking candidates will be the out-
put for out-of-ten. Again, participants are free to use
their creativity in this, while a range of possible al-
gorithms might include using a machine translation
system, using language models, word sense disam-
biguation models, semantic similarity-based tech-
niques, graph-based models etc. Again, combina-
tions of these might be used if they are feasible as
far as time and space are concerned.
We anticipate a minor practical issue to come up
with all participants, and that is the issue of different
character encodings, especially when using bilin-
gual dictionaries from the Web. This is directly re-
lated to the issue of dealing with characters with di-
acritics, and in our experience not all available soft-
ware packages and programs are able to handle dia-
critics and different character encodings in the same
way. This issue is inherent in all cross-lingual tasks,
and we leave it up to the discretion of the partici-
pants to effectively deal with it.
</bodyText>
<sectionHeader confidence="0.978876" genericHeader="method">
6 Post Hoc Issues
</sectionHeader>
<bodyText confidence="0.999960333333333">
In LEXSUB a post hoc evaluation was conducted us-
ing fresh annotators to ensure that the substitutes
the systems came up with were not typically bet-
ter than those of the original annotators. This was
done as a sanity check because there was no fixed
inventory for the task and there will be a lot of varia-
</bodyText>
<page confidence="0.989321">
80
</page>
<bodyText confidence="0.9999813125">
tion in the task and sometimes the systems might do
better than the annotators. The post hoc evaluation
demonstrated that the post hoc annotators typically
preferred the substitutes provided by humans.
We have not yet determined whether we will run
a post hoc evaluation because of the costs of do-
ing this and the time constraints. Another option is
to reannotate a portion of our data using a new set
of annotators but restricting them to the translations
supplied by the initial set of annotations and other
translations from available resources. This would be
worthwhile but it could be done at any stage when
funds permit because we do not intend to supply a
set of candidate translations to the annotators since
we wish to evaluate candidate collection as well as
candidate selection.
</bodyText>
<sectionHeader confidence="0.99943" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999517384615385">
In this paper we have outlined the cross-lingual lex-
ical substitution task to be run under the auspices
of SemEval-2010. The task will require annotators
and systems to find translations for a target word in
context. Unlike machine translation tasks, the whole
text is not translated and annotators are encouraged
to supply as many translations as fit the context. Un-
like previous WSD tasks based on parallel data, be-
cause we allow multiple translations and because we
do not restrict translations to those that provide clear
cut sense distinctions, we will be able to use the
dataset collected to investigate more subtle represen-
tations of meaning.
</bodyText>
<sectionHeader confidence="0.996279" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.919945">
The work of the first and third authors has been partially
supported by a National Science Foundation CAREER
award #0747340. The work of the second author has been
supported by a Royal Society UK Dorothy Hodgkin Fel-
lowship.
</bodyText>
<sectionHeader confidence="0.998114" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999405655737705">
Marine Carpuat and Dekai Wu. 2007. Improving statisti-
cal machine translation using word sense disambigua-
tion. In Proceedings of the Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL 2007), pages 61–72, Prague, Czech Republic,
June. Association for Computational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2005. Word sense
disambiguation with distribution estimation. In Pro-
ceedings of the 19th International Joint Conference on
Artificial Intelligence (IJCAI2005), pages 1010–1015,
Edinburgh, Scotland.
Nancy Ide and Yorick Wilks. 2006. Making sense about
sense. In Eneko Agirre and Phil Edmonds, editors,
Word Sense Disambiguation, Algorithms and Applica-
tions, pages 47–73. Springer.
Adam Kilgarriff. 2006. Word senses. In Eneko
Agirre and Phil Edmonds, editors, Word Sense Disam-
biguation, Algorithms and Applications, pages 29–46.
Springer.
Sadao Kurohashi. 2001. SENSEVAL-2 japanese transla-
tion task. In Proceedings of the SENSEVAL-2 work-
shop, pages 37–44.
Diana McCarthy and Roberto Navigli. 2007. SemEval-
2007 task 10: English lexical substitution task. In Pro-
ceedings of the 4th International Workshop on Seman-
tic Evaluations (SemEval-2007), pages 48–53, Prague,
Czech Republic.
Diana McCarthy and Roberto Navigli. in press. The en-
glish lexical substitution task. Language Resources
and Evaluation Special Issue on Computational Se-
mantic Analysis of Language: SemEval-2007 and Be-
yond.
Diana McCarthy. 2002. Lexical substitution as a task for
wsd evaluation. In Proceedings of the ACL Workshop
on Word Sense Disambiguation: Recent Successes and
Future Directions, pages 109–115, Philadelphia, USA.
Hwee Tou Ng and Yee Seng Chan. 2007. SemEval-
2007 task 11: English lexical sample task via
English-Chinese parallel text. In Proceedings of the
4th International Workshop on Semantic Evaluations
(SemEval-2007), pages 54–58, Prague, Czech Repub-
lic.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of ACM SIGKDD
Conference on Knowledge Discovery and Data Min-
ing, pages 613–619, Edmonton, Canada.
Philip Resnik and David Yarowsky. 2000. Distinguish-
ing systems and distinguishing senses: New evaluation
methods for word sense disambiguation. Natural Lan-
guage Engineering, 5(3):113–133.
Hinrich Sch¨utze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97–123.
Serge Sharoff. 2006. Open-source corpora: Using the
net to fish for linguistic data. International Journal of
Corpus Linguistics, 11(4):435–462.
Christopher Stokoe. 2005. Differentiating homonymy
and polysemy in information retrieval. In Proceedings
of the joint conference on Human Language Technol-
ogy and Empirical methods in Natural Language Pro-
cessing, pages 403–410, Vancouver, B.C., Canada.
</reference>
<page confidence="0.999261">
81
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.977845">
<title confidence="0.998382">SemEval-2010 Task 2: Cross-Lingual Lexical Substitution</title>
<author confidence="0.999961">Ravi Sinha Diana McCarthy Rada Mihalcea</author>
<affiliation confidence="0.997847">University of North Texas University of Sussex University of North Texas</affiliation>
<email confidence="0.997407">ravisinha@unt.edudianam@sussex.ac.ukrada@cs.unt.edu</email>
<abstract confidence="0.999092526315789">In this paper we describe the SemEval- 2010 Cross-Lingual Lexical Substitution task, which is based on the English Lexical Substitution task run at SemEval-2007. In the English version of the task, annotators and systems had to find an alternative substitute word or phrase for a target word in context. In this paper we propose a task where the target word and contexts will be in English, but the substitutes will be in Spanish. In this paper we provide background and motivation for the task and describe how the dataset will differ from a machine translation task and previous word sense disambiguation tasks based on parallel data. We describe the annotation process and how we anticipate scoring the system output. We finish with some ideas for participating systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Improving statistical machine translation using word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</booktitle>
<pages>61--72</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="8169" citStr="Carpuat and Wu, 2007" startWordPosition="1319" endWordPosition="1322">ned sense inventory. Resnik and Yarowsky (2000) also conducted their experiments using words in context, rather than a predefined 77 sense-inventory as in (Ng and Chan, 2007; Chan and Ng, 2005), however in these experiments the annotators were asked for a single preferred translation. We intend to allow annotators to supply as many translations as they feel are equally valid. This will allow us to examine more subtle relationships between usages and to allow partial credit to systems which get a close approximation to the annotators’ translations. Unlike a full blown machine translation task (Carpuat and Wu, 2007), annotators and systems will not be required to translate the whole context but just the target word. 4 The Cross-Lingual Lexical Substitution Task Here we discuss our proposal for a Cross-Lingual Lexical Substitution task. The task will follow LEXSUB except that the annotations will be translations rather than paraphrases. Given a target word in context, the task is to provide several correct translations for that word in a given language. We will use English as the source language and Spanish as the target language. Multiwords are ‘part and parcel’ of natural language. For this reason, rath</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007. Improving statistical machine translation using word sense disambiguation. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL 2007), pages 61–72, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Word sense disambiguation with distribution estimation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI2005),</booktitle>
<pages>1010--1015</pages>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="7741" citStr="Chan and Ng, 2005" startWordPosition="1249" endWordPosition="1252">h such overlap, or split them using the distinctive translation and then use the distinctive translations as a sense inventory. This sense inventory is then used to collect training from parallel data (Ng and Chan, 2007). We propose that it would be interesting to collect a dataset where the overlap in translations for an instance can remain and that this will depend on the token instance rather than mapping to a pre-defined sense inventory. Resnik and Yarowsky (2000) also conducted their experiments using words in context, rather than a predefined 77 sense-inventory as in (Ng and Chan, 2007; Chan and Ng, 2005), however in these experiments the annotators were asked for a single preferred translation. We intend to allow annotators to supply as many translations as they feel are equally valid. This will allow us to examine more subtle relationships between usages and to allow partial credit to systems which get a close approximation to the annotators’ translations. Unlike a full blown machine translation task (Carpuat and Wu, 2007), annotators and systems will not be required to translate the whole context but just the target word. 4 The Cross-Lingual Lexical Substitution Task Here we discuss our pro</context>
</contexts>
<marker>Chan, Ng, 2005</marker>
<rawString>Yee Seng Chan and Hwee Tou Ng. 2005. Word sense disambiguation with distribution estimation. In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI2005), pages 1010–1015, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Yorick Wilks</author>
</authors>
<title>Making sense about sense.</title>
<date>2006</date>
<booktitle>In Eneko Agirre and Phil Edmonds, editors, Word Sense Disambiguation, Algorithms and Applications,</booktitle>
<pages>47--73</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2541" citStr="Ide and Wilks, 2006" startWordPosition="407" endWordPosition="410">substitution system could be used as input to existing systems for cross-language information retrieval or automatic machine translation. 2 Background: The English Lexical Substitution Task The English Lexical substitution task (hereafter referred to as LEXSUB) was run at SemEval-2007 following earlier ideas on a method of testing WSD systems without predetermining the inventory (McCarthy, 2002). The issue of which inventory is appropriate for the task has been a long standing issue for debate, and while there is hope that coarsegrained inventories will allow for increased system performance (Ide and Wilks, 2006) we do not yet know if these will make the distinctions that will most benefit practical systems (Stokoe, 2005) or reflect cognitive processes (Kilgarriff, 2006). LEXSUB was proposed as a task which, while requiring contextual disambiguation, did not presuppose a specific sense inventory. In fact, it is quite possible to use alternative representations of meaning (Sch¨utze, 1998; Pantel and Lin, 2002). The motivation for a substitution task was that it would reflect capabilities that might be useful for natural language processing tasks such as paraphrasing and textual entailment, while only f</context>
</contexts>
<marker>Ide, Wilks, 2006</marker>
<rawString>Nancy Ide and Yorick Wilks. 2006. Making sense about sense. In Eneko Agirre and Phil Edmonds, editors, Word Sense Disambiguation, Algorithms and Applications, pages 47–73. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>Word senses.</title>
<date>2006</date>
<booktitle>In Eneko Agirre and Phil Edmonds, editors, Word Sense Disambiguation, Algorithms and Applications,</booktitle>
<pages>29--46</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2702" citStr="Kilgarriff, 2006" startWordPosition="435" endWordPosition="436">Lexical Substitution Task The English Lexical substitution task (hereafter referred to as LEXSUB) was run at SemEval-2007 following earlier ideas on a method of testing WSD systems without predetermining the inventory (McCarthy, 2002). The issue of which inventory is appropriate for the task has been a long standing issue for debate, and while there is hope that coarsegrained inventories will allow for increased system performance (Ide and Wilks, 2006) we do not yet know if these will make the distinctions that will most benefit practical systems (Stokoe, 2005) or reflect cognitive processes (Kilgarriff, 2006). LEXSUB was proposed as a task which, while requiring contextual disambiguation, did not presuppose a specific sense inventory. In fact, it is quite possible to use alternative representations of meaning (Sch¨utze, 1998; Pantel and Lin, 2002). The motivation for a substitution task was that it would reflect capabilities that might be useful for natural language processing tasks such as paraphrasing and textual entailment, while only focusing on one aspect of the problem and therefore not requiring a complete system that might mask system capabilities at a lexical level and at the same time ma</context>
</contexts>
<marker>Kilgarriff, 2006</marker>
<rawString>Adam Kilgarriff. 2006. Word senses. In Eneko Agirre and Phil Edmonds, editors, Word Sense Disambiguation, Algorithms and Applications, pages 29–46. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
</authors>
<title>SENSEVAL-2 japanese translation task.</title>
<date>2001</date>
<booktitle>In Proceedings of the SENSEVAL-2 workshop,</booktitle>
<pages>37--44</pages>
<contexts>
<context position="5802" citStr="Kurohashi, 2001" startWordPosition="934" endWordPosition="935">ich allowed up to 10 attempts. 1 The results are reported in McCarthy and Navigli (2007) and in more detail in McCarthy and Navigli (in press). 1The details are available at http://nlp.cs.swarthmore.edu/semeval/tasks/ task10/task10documentation.pdf. 3 Motivation and Related Work While there has been a lot of discussion on the relevant sense distinctions for monolingual WSD systems, for machine translation applications there is a consensus that the relevant sense distinctions are those that reflect different translations. One early and notable work was the SENSEVAL-2 Japanese Translation task (Kurohashi, 2001) that obtained alternative translation records of typical usages of a test word, also referred to as a translation memory. Systems could either select the most appropriate translation memory record for each instance and were scored against a gold-standard set of annotations, or they could provide a translation that was scored by translation experts after the results were submitted. In contrast to this work, we propose to provide actual translations for target instances in advance, rather than predetermine translations using lexicographers or rely on post-hoc evaluation, which does not permit e</context>
</contexts>
<marker>Kurohashi, 2001</marker>
<rawString>Sadao Kurohashi. 2001. SENSEVAL-2 japanese translation task. In Proceedings of the SENSEVAL-2 workshop, pages 37–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Roberto Navigli</author>
</authors>
<title>SemEval2007 task 10: English lexical substitution task.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>48--53</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5274" citStr="McCarthy and Navigli (2007)" startWordPosition="859" endWordPosition="862">ta. Any system that relied on training data, such as sense annotated corpora, had to use resources available from other sources. The task had eight participating teams. Teams were allowed to submit up to two systems and there were a total of ten different systems. The scoring was conducted using recall and precision measures using: • the frequency distribution of responses from the annotators and • the mode of the annotators (the most frequent response). The systems were scored using their best guess as well as an out-of-ten score which allowed up to 10 attempts. 1 The results are reported in McCarthy and Navigli (2007) and in more detail in McCarthy and Navigli (in press). 1The details are available at http://nlp.cs.swarthmore.edu/semeval/tasks/ task10/task10documentation.pdf. 3 Motivation and Related Work While there has been a lot of discussion on the relevant sense distinctions for monolingual WSD systems, for machine translation applications there is a consensus that the relevant sense distinctions are those that reflect different translations. One early and notable work was the SENSEVAL-2 Japanese Translation task (Kurohashi, 2001) that obtained alternative translation records of typical usages of a te</context>
</contexts>
<marker>McCarthy, Navigli, 2007</marker>
<rawString>Diana McCarthy and Roberto Navigli. 2007. SemEval2007 task 10: English lexical substitution task. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 48–53, Prague, Czech Republic.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Diana McCarthy</author>
<author>Roberto Navigli</author>
</authors>
<title>in press. The english lexical substitution task.</title>
<booktitle>Language Resources and Evaluation Special Issue on Computational Semantic Analysis of Language: SemEval-2007 and Beyond.</booktitle>
<marker>McCarthy, Navigli, </marker>
<rawString>Diana McCarthy and Roberto Navigli. in press. The english lexical substitution task. Language Resources and Evaluation Special Issue on Computational Semantic Analysis of Language: SemEval-2007 and Beyond.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
</authors>
<title>Lexical substitution as a task for wsd evaluation.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL Workshop on Word Sense Disambiguation: Recent Successes and Future Directions,</booktitle>
<pages>109--115</pages>
<location>Philadelphia, USA.</location>
<contexts>
<context position="2319" citStr="McCarthy, 2002" startWordPosition="369" endWordPosition="371">could be used to assist language learners, by providing them with the interpretation of the unknown words in a text written in the language they are learning. Last but not least, the output of a cross-lingual lexical substitution system could be used as input to existing systems for cross-language information retrieval or automatic machine translation. 2 Background: The English Lexical Substitution Task The English Lexical substitution task (hereafter referred to as LEXSUB) was run at SemEval-2007 following earlier ideas on a method of testing WSD systems without predetermining the inventory (McCarthy, 2002). The issue of which inventory is appropriate for the task has been a long standing issue for debate, and while there is hope that coarsegrained inventories will allow for increased system performance (Ide and Wilks, 2006) we do not yet know if these will make the distinctions that will most benefit practical systems (Stokoe, 2005) or reflect cognitive processes (Kilgarriff, 2006). LEXSUB was proposed as a task which, while requiring contextual disambiguation, did not presuppose a specific sense inventory. In fact, it is quite possible to use alternative representations of meaning (Sch¨utze, 1</context>
</contexts>
<marker>McCarthy, 2002</marker>
<rawString>Diana McCarthy. 2002. Lexical substitution as a task for wsd evaluation. In Proceedings of the ACL Workshop on Word Sense Disambiguation: Recent Successes and Future Directions, pages 109–115, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Yee Seng Chan</author>
</authors>
<title>SemEval2007 task 11: English lexical sample task via English-Chinese parallel text.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>54--58</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="6595" citStr="Ng and Chan, 2007" startWordPosition="1056" endWordPosition="1059">nslation memory record for each instance and were scored against a gold-standard set of annotations, or they could provide a translation that was scored by translation experts after the results were submitted. In contrast to this work, we propose to provide actual translations for target instances in advance, rather than predetermine translations using lexicographers or rely on post-hoc evaluation, which does not permit evaluation of new systems after the competition. Previous standalone WSD tasks based on parallel data have obtained distinct translations for senses as listed in a dictionary (Ng and Chan, 2007). In this way fine-grained senses with the same translations can be lumped together, however this does not fully allow for the fact that some senses for the same words may have some translations in common but also others that are not. An example from Resnik and Yarowsky (2000) (table 4 in that paper) is the first two senses from WordNet for the noun interest: WordNet sense Spanish Translation monetary e.g. on loan inter´es, r´edito stake/share inter´es,participaci´on For WSD tasks, a decision can be made to lump senses with such overlap, or split them using the distinctive translation and then</context>
</contexts>
<marker>Ng, Chan, 2007</marker>
<rawString>Hwee Tou Ng and Yee Seng Chan. 2007. SemEval2007 task 11: English lexical sample task via English-Chinese parallel text. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 54–58, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<booktitle>In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>613--619</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="2945" citStr="Pantel and Lin, 2002" startWordPosition="471" endWordPosition="474"> issue of which inventory is appropriate for the task has been a long standing issue for debate, and while there is hope that coarsegrained inventories will allow for increased system performance (Ide and Wilks, 2006) we do not yet know if these will make the distinctions that will most benefit practical systems (Stokoe, 2005) or reflect cognitive processes (Kilgarriff, 2006). LEXSUB was proposed as a task which, while requiring contextual disambiguation, did not presuppose a specific sense inventory. In fact, it is quite possible to use alternative representations of meaning (Sch¨utze, 1998; Pantel and Lin, 2002). The motivation for a substitution task was that it would reflect capabilities that might be useful for natural language processing tasks such as paraphrasing and textual entailment, while only focusing on one aspect of the problem and therefore not requiring a complete system that might mask system capabilities at a lexical level and at the same time make 76 Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 76–81, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics participation in the task difficult for</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Patrick Pantel and Dekang Lin. 2002. Discovering word senses from text. In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 613–619, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>David Yarowsky</author>
</authors>
<title>Distinguishing systems and distinguishing senses: New evaluation methods for word sense disambiguation.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<volume>5</volume>
<issue>3</issue>
<contexts>
<context position="6872" citStr="Resnik and Yarowsky (2000)" startWordPosition="1104" endWordPosition="1107">slations for target instances in advance, rather than predetermine translations using lexicographers or rely on post-hoc evaluation, which does not permit evaluation of new systems after the competition. Previous standalone WSD tasks based on parallel data have obtained distinct translations for senses as listed in a dictionary (Ng and Chan, 2007). In this way fine-grained senses with the same translations can be lumped together, however this does not fully allow for the fact that some senses for the same words may have some translations in common but also others that are not. An example from Resnik and Yarowsky (2000) (table 4 in that paper) is the first two senses from WordNet for the noun interest: WordNet sense Spanish Translation monetary e.g. on loan inter´es, r´edito stake/share inter´es,participaci´on For WSD tasks, a decision can be made to lump senses with such overlap, or split them using the distinctive translation and then use the distinctive translations as a sense inventory. This sense inventory is then used to collect training from parallel data (Ng and Chan, 2007). We propose that it would be interesting to collect a dataset where the overlap in translations for an instance can remain and t</context>
</contexts>
<marker>Resnik, Yarowsky, 2000</marker>
<rawString>Philip Resnik and David Yarowsky. 2000. Distinguishing systems and distinguishing senses: New evaluation methods for word sense disambiguation. Natural Language Engineering, 5(3):113–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Serge Sharoff</author>
</authors>
<title>Open-source corpora: Using the net to fish for linguistic data.</title>
<date>2006</date>
<journal>International Journal of Corpus Linguistics,</journal>
<volume>11</volume>
<issue>4</issue>
<contexts>
<context position="4241" citStr="Sharoff, 2006" startWordPosition="683" endWordPosition="684">a word in context. For example a substitute of tournament might be given for the second occurrence of match (shown in bold) in the following sentence: The ideal preparation would be a light meal about 2-2 1/2 hours pre-match, followed by a warm-up hit and perhaps a top-up with extra fluid before the match. In LEXSUB, the data was collected for 201 words from open class parts-of-speech (PoS) (i.e. nouns, verbs, adjectives and adverbs). Words were selected that have more than one meaning with at least one near synonym. Ten sentences for each word were extracted from the English Internet Corpus (Sharoff, 2006). There were five annotators who annotated each target word as it occurred in the context of a sentence. The annotators were each allowed to provide up to three substitutes, though they could also provide a NIL response if they could not come up with a substitute. They had to indicate if the target word was an integral part of a multiword. A development and test dataset were provided, but no training data. Any system that relied on training data, such as sense annotated corpora, had to use resources available from other sources. The task had eight participating teams. Teams were allowed to sub</context>
</contexts>
<marker>Sharoff, 2006</marker>
<rawString>Serge Sharoff. 2006. Open-source corpora: Using the net to fish for linguistic data. International Journal of Corpus Linguistics, 11(4):435–462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Stokoe</author>
</authors>
<title>Differentiating homonymy and polysemy in information retrieval.</title>
<date>2005</date>
<booktitle>In Proceedings of the joint conference on Human Language Technology and Empirical methods in Natural Language Processing,</booktitle>
<pages>403--410</pages>
<location>Vancouver, B.C.,</location>
<contexts>
<context position="2652" citStr="Stokoe, 2005" startWordPosition="428" endWordPosition="429">achine translation. 2 Background: The English Lexical Substitution Task The English Lexical substitution task (hereafter referred to as LEXSUB) was run at SemEval-2007 following earlier ideas on a method of testing WSD systems without predetermining the inventory (McCarthy, 2002). The issue of which inventory is appropriate for the task has been a long standing issue for debate, and while there is hope that coarsegrained inventories will allow for increased system performance (Ide and Wilks, 2006) we do not yet know if these will make the distinctions that will most benefit practical systems (Stokoe, 2005) or reflect cognitive processes (Kilgarriff, 2006). LEXSUB was proposed as a task which, while requiring contextual disambiguation, did not presuppose a specific sense inventory. In fact, it is quite possible to use alternative representations of meaning (Sch¨utze, 1998; Pantel and Lin, 2002). The motivation for a substitution task was that it would reflect capabilities that might be useful for natural language processing tasks such as paraphrasing and textual entailment, while only focusing on one aspect of the problem and therefore not requiring a complete system that might mask system capab</context>
</contexts>
<marker>Stokoe, 2005</marker>
<rawString>Christopher Stokoe. 2005. Differentiating homonymy and polysemy in information retrieval. In Proceedings of the joint conference on Human Language Technology and Empirical methods in Natural Language Processing, pages 403–410, Vancouver, B.C., Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>