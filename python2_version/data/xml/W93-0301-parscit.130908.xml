<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.908916">
Robust Bilingual Word Alignment
for Machine Aided Translation
</title>
<author confidence="0.818153">
Ido Dagan Kenneth W. Church William A. Gale
</author>
<affiliation confidence="0.725529">
AT&amp;T Bell Laboratories
</affiliation>
<address confidence="0.8820435">
600 Mountain Avenue
Murray Hill, NJ 07974
</address>
<sectionHeader confidence="0.945316" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999661136363637">
We have developed a new program called
word_align for aligning parallel text, text such
as the Canadian Hansards that are available in
two or more languages. The program takes the
output of char_align (Church, 1993), a robust
alternative to sentence-based alignment pro-
grams, and applies word-level constraints us-
ing a version of Brown et al.&apos;s Model 2 (Brown
et al., 1993), modified and extended to deal
with robustness issues. Word_align was tested
on a subset of Canadian Hansards supplied by
Simard (Simard et al., 1992). The combina-
tion of word_align plus char_align reduces the
variance (average square error) by a factor of
5 over char_align alone. More importantly, be-
cause word_align and char_align were designed
to work robustly on texts that are smaller and
more noisy than the Hansards, it has been pos-
sible to successfully deploy the programs at
AT&amp;T Language Line Services, a commercial
translation service, to help them with difficult
terminology.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999531">
Aligning parallel texts has recently received consid-
erable attention (Warwick et al., 1990; Brown et al.,
1991a; Gale and Church, 1991b; Gale and Church,
1991a; Kay and Rosenschein, 1993; Simard et al.,
1992; Church, 1993; Kupiec, 1993; Matsumoto et
al., 1993). These methods have been used in ma-
chine translation (Brown et al., 1990; Sadler, 1989),
terminology research and translation aids (Isabelle,
1992; Ogden and Gonzales, 1993), bilingual lexi-
cography (Klavans and Tzoukermann, 1990), col-
location studies (Smadja, 1992), word-sense disam-
biguation (Brown et al., 1991b; Gale et al., 1992)
and information retrieval in a multilingual environ-
ment (Landauer and Littman, 1990).
The information retrieval application may be
of particular relevance to this audience. It would
be highly desirable for users to be able to express
queries in whatever language they chose and re-
trieve documents that may or may not have been
written in the same language as the query. Lan-
dauer and Littman used SVD analysis (or Latent
Semantic Indexing) on the Canadian Hansards,
parliamentary debates that are published in both
English and French, in order to estimate a kind of
soft thesaurus. They then showed that these esti-
mates could be used to retrieve documents appro-
priately in the bilingual condition where the query
and the document were written in different lan-
guages.
We have been most interested in the terminol-
ogy application. How does Microsoft, or some other
software vendor, want &amp;quot;dialog box,&amp;quot; &amp;quot;text box,&amp;quot;
and &amp;quot;menu box&amp;quot; to be translated in their man-
uals? Considerable time is spent on terminology
questions, many of which have already been solved
by other translators working on similar texts. It
ought to be possible for a translator to point at
an instance of &amp;quot;dialog box&amp;quot; in the English version
of the Microsoft Windows manual and see how it
was translated in the French version of the same
manual. Alternatively, the translator can ask for a
bilingual concordance as shown in Figure 1. A PC-
based terminology reuse tool is being developed to
do just exactly this. The tool depends crucially
on the results of an alignment program to deter-
mine which parts of the source text correspond with
which parts of the target text.
In working with the translators at AT&amp;T Lan-
guage Line Services, a commercial translation ser-
vice, we discovered that we needed to completely
redesign our alignment programs in order to deal
more effectively with texts supplied by Language
Line&apos;s customers. All too often the texts are not
available in electronic form, and may need to be
scanned in and processed by an OCR (optical char-
acter recognition) device. Even if the texts are
available in electronic form, it may not be worth
the effort to clean them up by hand. Real texts are
not like the Hansards; real texts are much smaller
and not nearly as clean as the ideal texts that have
</bodyText>
<page confidence="0.946762">
1
</page>
<note confidence="0.696100833333333">
displayed . In the Save As
afficha Dans Enregistrer Enregistrer
ainsi que son extension . Dans la boite de
x When you chcose a command button , the
Lorsque commande bouton
sissez un bouton de commande , la boite de
</note>
<tableCaption confidence="0.837688166666667">
button . Or doubl - lick the Control -
r bouton cliquer fois Systeme
ouvez aussi cliquer deux fois sur la case du
ee &apos; aa , &apos; When you move to an empty
Lorsque placez
de Lorsque vous vous placez dans une zone de
dialog box , this area is called Save
dialogue boite cette zone est Enregistr
dialogue Enregistrer sous , cette zone est appele
dialog box closes and the command is
dialogue boite ferme commande execute
dialogue se ferme et le programme execute la corn
menu box . Or press ESC . If a dialog box
menu case Si dialogue boite p
menu Systeme .Il est egalement possible d &apos; a
text box , an iiisertion point ( flastung ve
texte zone insertion (
texte vide , un point d &apos; insertion ( barre vertic
</tableCaption>
<figureCaption confidence="0.780368166666667">
Figure 1: A small sample of a bilingual concordance, based on the output of word_align. Four concordances
for the word &amp;quot;box&amp;quot; are shown, identifying three different translations for the word: boite, case, zone. The
concordances are selected from English and French versions of the Microsoft Windows manual (with some
errors introduced by OCR). There are three lines of text for each instance of &amp;quot;box&amp;quot;: (1) English, (2) glosses,
and (3) French. The glosses are selected from the French text (the third line), and are written underneath
the corresponding English words, as identified by word_align.
</figureCaption>
<bodyText confidence="0.986646117647059">
been used in previous studies.
To deal with these robustness issues, Church
(1993) developed a character-based alignment
method called char_align. The method was in-
tended as a replacement for sentence-based meth-
ods (e.g., (Brown et al., 1991a; Gale and Church,
1991b; Kay and Rosenschein, 1993)), which are
very sensitive to noise. This paper describes a
new program, called word_align, that starts with
an initial &amp;quot;rough&amp;quot; alignment (e.g., the output of
char_a/ign or a sentence-based alignment method),
and produces improved alignments by exploiting
constraints at the word-level. The alignment algo-
rithm consists of two steps: (1) estimate transla-
tion probabilities, and (2) use these probabilities
to search for most probable alignment path. The
two steps are described in the following section.
</bodyText>
<sectionHeader confidence="0.975718" genericHeader="method">
2 The alignment Algorithm
</sectionHeader>
<subsectionHeader confidence="0.996902">
2.1 Estimation of translation
probabilities
</subsectionHeader>
<bodyText confidence="0.999924818181818">
The translation probabilities are estimated using a
method based on Brown et al.&apos;s Model 2 (1993),
which is summarized in the following subsection,
2.1.1. Then, in subsection 2.1.2, we describe
modifications that achieve three goals: (1) en-
able word_align to accept input which may not be
aligned by sentence (e.g. char_al:gn&apos;s output), (2)
reduce the number of parameters that need to be
estimated, and (3) prepare the ground for the sec-
ond step, the search for the best alignment (de-
scribed in section 2.2).
</bodyText>
<subsectionHeader confidence="0.931666">
2.1.1 Brown et al.&apos;s Model
</subsectionHeader>
<bodyText confidence="0.978143727272727">
In the context of their statistical machine trans-
lation project (Brown et al., 1990), Brown et al.
estimate Pr(f le), the probability that f, a sentence
in one language (say French), is the translation of
e, a sentence in the other language (say English).
Pr(f le) is computed using the concept of altgnment,
denoted by a, which is a set of connections between
each French word in f and the corresponding En-
glish word in e. A connection, which we will write
f e
as con- specifies that position j in f is connected
</bodyText>
<equation confidence="0.728011">
1,1 &apos;
</equation>
<bodyText confidence="0.999712307692308">
to position i in e. If a French word in f does not
correspond to any English word in e, then it is
connected to the special word null (position 0 in
e). Notice that this model is directional, as each
French position is connected to exactly one posi-
tion in the English sentence (which might be the
null word), and accordingly the number of connec-
tions in an alignment is equal to the length of the
French sentence. However, an English word may be
connected to several words in the French sentence,
or not connected at all.
Using alignments, the translation probability
for a pair of sentences is expressed as
</bodyText>
<equation confidence="0.8301205">
Pr(fle) = E Pr(f, ale) (1)
aEA
</equation>
<bodyText confidence="0.788919">
where A is the set of all combinatorially possible
alignments for the sentences f and e (calligraphic
font will be used to denote sets).
In their paper, Brown et al. present a series of
5 models of Pr(f le). The first two of these 5 models
are summarized here.
</bodyText>
<page confidence="0.975598">
2
</page>
<bodyText confidence="0.92401">
Model 1
Model 1 assumes that Pr(f, ale) depends pri-
marily on t( f le), the probability that an occurrence
of the English word e is translated as the French
word f. That is,
</bodyText>
<equation confidence="0.98043">
Pr(f le) = E Pr(f , ale) = E Cf.e t(filea,)
aEA aEA j=1
</equation>
<bodyText confidence="0.996472333333333">
(2)
where Cf e, an irrelevant constant, accounts for
certain dependencies on sentence lengths, which
are not important for our purposes here. Except
for Cf e&apos; most of the notation is borrowed from
Brown et al.. The variable, j, is used to refer to a
position in a French sentence, and the variable, i,
is used to refer to a position in an English sentence.
The expression, f, is used to refer to the French
word in position j of a French sentence, and e, is
used to refer to the English word in position i of
an English sentence. An alignment, a, is a set of
pairs (j, i), each of which connects a position in a
French sentence with a corresponding position in
an English sentence. The expression, ai , is used
to refer to the English position that is connected
to the French position j, and the expression,
is used to refer to the English word in position ai.
The variable, m, is used to denote the length of
the French sentence and the variable, I, is used to
denote the length of the English sentence.
There are quite a number of constraints that
could be used to estimate Pr(f, ale). Model 1 de-
pends primarily on the translation probabilities,
t(f(e), and does not make use of constraints in-
volving the positions within an alignment. These
constraints will be exploited in Model 2.
Brown et al. estimate t(fle) on the basis of a
training set, a set of English and French sentences
that have been aligned at the sentence level. Those
values of t(fle) that maximize the probability of
the training set are called the maximum likelihood
estimates. Brown et a/. show that the maximum
likelihood estimates satisfy
It follows from the definition of Model 1 that
the probability of a connection satisfies:
</bodyText>
<equation confidence="0.99954">
Pr(conf.,ie) = i(flei)
Ek=0 t(fj lek )
</equation>
<bodyText confidence="0.987022710526316">
Recall that h refers to the French word in position
j of the French sentence f of length m, and that
ei refers to the English word in position i of the
English sentence e of length I. Also, remember
that position 0 is reserved for the null word.
Equations 3 and 4 are used iteratively to esti-
mate t(f le). That is, we start with an initial guess
for (f le). We then evaluation the right hand side
of equation 4, and compute the probability of the
connections in the training set. Then we evaluate
equation 3, obtain new estimates for the transla-
tion probabilities, and repeat the process, until it
converges. This iterative process is known as the
EM algorithm and has been shown to converge to
a stationary point (Baum, 1972; Dempster et al.,
1977). Moreover, Brown et a/. show that Model
1 has a unique maximum, and therefore, in this
special case, the EM algorithm is guaranteed to
converge to the maximum likelihood solution, and
does not depend on the initial guess.
Model 2
Model 2 improves upon model 1 by making use
of the positions within an alignment. For instance,
it is much more likely that the first word of an En-
glish sentence will be connected to a word near the
beginning of the corresponding French sentence,
than to some word near the end of the French sen-
tence. Model 2 enhances Model 1 with the assump-
tion that the probability of a connection, con,
depends also on j and i (the positions in f and
e), as well as on m and 1 (the lengths of the two
sentences). This dependence is expressed through
the term a(iIj, m,1), which denotes the probabil-
ity of connecting position j in a French sentence of
length m with position i in an English sentence
of length I. Since each French position is con-
nected to exactly one English position, the con-
straint El=c)
</bodyText>
<equation confidence="0.835524857142857">
. a(iIj, m, 1) = 1 should hold for all j,
s
in and I. In place of equation 2, we now have:
Pr(fle) = E Pr(f, ale)
aeA
= E t(file.„)
aEA j=i
</equation>
<bodyText confidence="0.959332666666667">
where e
C&apos;f is an irrelevant constant.
,
As in Model 1, equation 3 holds for the max-
imum likelihood estimates of the translation prob-
abilities. The corresponding equation for the max-
</bodyText>
<figure confidence="0.9811257">
f
E f e Pr(con1
•&apos;•e )
J,
t(fle) = (3)
f
E f.e N. Pr(con •
,se),
con ECO...
3••
</figure>
<bodyText confidence="0.997500777777778">
where CO/tile and CON.,, denote sets of con-
nections: the set CO.Arf,, contains all connections
in the training data between f and e, and the
set CON.,, contains all connections between some
French word and e. The probability of a connec-
tion, con&apos;.e • , is the sum of the probabilities of all
alignments that contain it. Notice that equation
3 satisfies the constraint: E t(fle) = 1, for each
English word e.
</bodyText>
<figure confidence="0.919541333333333">
con &apos; ECOAri,.
3..
3
imum likelihood estimates of a(iIj, in, 1) is:
f ,e Pr(conif :ie)
m , 1) = eon
3.. ECO.A.r&amp;quot;,1
(6)
f
E f e Pr(con &apos; )
1,1e
ton 3,&apos;, ECON. 37&apos;
</figure>
<bodyText confidence="0.999976875">
where CONV denotes the set of connections in the
training data between positions j and i in French
and English sentences of lengths in and 1, respec-
tively. Similarly, CO.A7.3 denotes the set of con-
nections between position j and some English po-
sition, in sentences of these lengths.
Instead of equation 4, we obtain the following
equation for the probability of a connection:
</bodyText>
<equation confidence="0.9963945">
t(fi lei) • a(ilj, m, 1)
Pr(conf &apos;e) = (7)
J
Lk=0i(f)lek)*
</equation>
<bodyText confidence="0.99956875">
Notice that Model 1 is a special case of Model 2,
where a(i1j,m,l) is held fixed at
As before, the EM algorithm is used to com-
pute maximum likelihood estimates for t(fle) and
a(i1j, m, 1) (using first equation 7, and then equa-
tions 3 and 6). However, in this case, Model 2
does not have a unique maximum, and therefore
the results depend on the initial guesses. Brown
et al. therefore use Model 1 to obtain estimates for
t(f le) which do not depend on the initial guesses.
These values are then used as the initial guesses of
t(f)e) in Model 2.
</bodyText>
<subsubsectionHeader confidence="0.871825">
2.1.2 Our model
</subsubsectionHeader>
<bodyText confidence="0.999722371428572">
As mentioned in the introduction, we are interested
in aligning corpora that are smaller and noisier
than the Hansards. This implies severe practical
constraints on the word alignment algorithm. As
mentioned earlier, we chose to start with the out-
put of char_align because it is more robust than al-
ternative sentence-based methods. This choice, of
course, requires certain modifications to the model
of Brown et al. to accommodate as input an initial
rough alignment (such as produced by char_align)
instead of pairs of aligned sentences. It is also
useful to reduce the number of parameters that we
are trying to estimate, because we have much less
data and much more noise. The paragraphs below
describe our modifications which are intended to
meet these somewhat different requirements. The
two major modifications are: (a) replacing the
sentence-by-sentence alignment with a single global
alignment for the entire corpus, and (b) replacing
the set of probabilities a(ijj, in,!) with a small set
of offset probabilities.
Word_align starts with an initial rough align-
ment, I, which maps French positions to English
positions (if the mapping is partial, we use linear
extrapolation to make it complete). Our goal is to
find a global alignment. A, which is more accurate
than I. To achieve this goal, we first use I to deter-
mine which connections will be considered for A.
Let coni,i denote a connection between position j
in the French corpus and position i in the English
corpus (the super-scripts in cof e are omitted, as
there is no notion of sentences). We assume that
coni,i is a possible connection only if i falls within a
limited window which is centered around 1(j), such
that:
</bodyText>
<equation confidence="0.988643">
I(j) — w &lt; i &lt; I(j) + w (8)
</equation>
<bodyText confidence="0.9999005">
where w is a predetermined parameter specifying
the size of the window (we typically set w to 20
words). Connections that fall outside this window
are assumed to have a zero probability. This as-
sumption replaces the assumption of Brown et al.
that connections which cross boundaries of aligned
sentences have a zero probability. In this new
framework, equation 3 becomes:
</bodyText>
<equation confidence="0.996213">
Econ,,,ECOArf,, Pr(coni,i)
t(fle) = (9)
Lcon,„EcoAr.„ Pr(conj,i)
</equation>
<bodyText confidence="0.999907272727273">
where CONL, and CON. ,e are taken from the set
of possible connections, as defined by (8).
Turning to Model 2, the parameters of the form
a(i1j,m,l) are somewhat more problematic. First,
since there are no sentence boundaries, there are no
direct equivalents for i, j, in and 1. Secondly, there
are too many parameters to be estimated, given the
limited size of our corpora (one parameter for each
combination of i, j, in and 1). Fortunately, these
parameters are highly redundant. For example, it
is likely that a(i1j, m, 1) will be very close to a(i +
11j+ 1, m, /) and a(i1j, m + 1,1+1).
In order to deal with these concerns, we re-
place probabilities of the form a(iIj, in, 1) with a
small set of offset probabilities. We use k to denote
the offset between i, an English position which cor-
responds to the French position j, and the English
position which the input alignment I connects to
j: k = i —I(j). An offset probability, o(k), is the
probability of having an offset k for some arbitrary
connection. According to (8), k ranges between
—w and w. Thus, instead of equation 6, we have
</bodyText>
<equation confidence="0.981048">
Econ Pr(conj,i)
o(k) = (10)
Econ,..ecog Pr(conj,i)
</equation>
<bodyText confidence="0.999746">
where CON is the set of all connections and COArk
is the set of all connections with offset k. Instead
of equation 7, we have
</bodyText>
<equation confidence="0.9899736">
Pr(conj,i) = t(fi lei) • o(i — I(j))
a(k1j, rn,1)
Iu)+w
1(1, le h) • o(h — I(j))
(11)
</equation>
<bodyText confidence="0.999710666666667">
The last three equations are used in the EM
algorithm in an iterative fashion as before to es-
timate the translation probabilities and the offset
</bodyText>
<page confidence="0.995019">
4
</page>
<bodyText confidence="0.99991982051282">
probabilities. Table 1 and Figure 2 show some val-
ues that were estimated in this way. The input
consisted of a pair of Microsoft Windows manu-
als in English (125,000 words) and its equivalent in
French (143,000 words). Table 1 shows four French
words and the four most likely translations, sorted
by t(el f)1 . Note that the correct translation(s) are
usually near the front of the list, though there is a
tendency for the program to be confused by collo-
cates such as &amp;quot;information about&amp;quot;. Figure 2 shows
the probability estimates for offsets from the ini-
tial alignment I. Note that smaller offsets are more
likely than larger ones, as we would expect. More-
over, the distribution is reasonably close to normal,
as indicated by the dotted line, which was gener-
ated by a Gaussian with a mean of 0 and standard
deviation of 102.
We have found it useful to make use of three fil-
ters to deal with robustness issues. Empirically, we
found that both high frequency and low frequency
words caused difficulties and therefore connections
involving these words are filtered out. The thresh-
olds are set to exclude the most frequent function
words and punctuations, as well as words with less
than 3 occurrences. In addition, following a similar
filter by Brown et al., small values of t( f le) are set
to 0 after each iteration of the EM algorithm be-
cause these small values often correspond to inap-
propriate translations. Finally, connections to null
are ignored. Such connections model French words
that are often omitted in the English translation.
However, because of OCR errors and other sources
of noise, it was decided that this phenomenon was
too difficult to model.
Some words will not be aligned because of these
heuristics. It may not be necessary, however, to
align all words in order to meet the goal of help-
ing translators (and lexicographers) with difficult
terminology.
</bodyText>
<subsectionHeader confidence="0.997873">
2.2 Finding the most probable
alignment
</subsectionHeader>
<bodyText confidence="0.9987394">
The EM algorithm produces two sets of maxi-
mum likelihood probability estimates: translation
probabilities, 1(1 le), and offset probabilities, o(k).
Brown et al. select their preferred alignment simply
by choosing the most probable alignment according
to the maximum likelihood probabilities, relative to
the given sentence alignment. In the terms of our
&apos;In this example, French is used as the source lan-
guage and English as the target.
2The center of the estimated distribution seems
more fiat than in a normal distribution. This might
be explained by a higher tendency for local changes
of word order within phrases than for order changes
among phrases. This is merely a hypothesis, though,
which requires further testing.
</bodyText>
<equation confidence="0.86629">
model, it is necessary to select the alignment A
that maximizes:
t( fi lei) • o( i — 1(j)). (12)
</equation>
<bodyText confidence="0.999866461538461">
Unfortunately, this method does not model the de-
pendence between connections for French words
that are near one another. For example, the fact
that the French position j was connected to the
English position i will not increase the probability
that j + 1 will be connected to an English position
near i. The absence of such dependence can easily
confuse the program, mainly in aligning adjacent
occurrences of the same word, which are common
in technical texts. Brown et al. introduce such de-
pendence in their Model 4. We have selected a
simpler alternative defined in terms of offset prob-
abilities.
</bodyText>
<subsectionHeader confidence="0.9265705">
2.2.1 Determining the set of relevant
connections
</subsectionHeader>
<bodyText confidence="0.999988153846154">
The first step in finding the most probable align-
ment is to determine the relevant connectzons for
each French position. Relevant connections are re-
quired to be reasonably likely, that is, their trans-
lation probability (t(f le)) should exceed some min-
imal threshold. Moreover, they are required to fall
within a window between 1(j) — w and 1(j) + w in
the English corpus, as in the previous step (param-
eter estimation). We call a French position relevant
if it has at least one relevant connection. Each
alignment A then consists of exactly one connec-
tion for each relevant French position (the irrele-
vant positions are ignored).
</bodyText>
<subsectionHeader confidence="0.990346">
2.2.2 Determining the most probable
alignment
</subsectionHeader>
<bodyText confidence="0.985857">
To model the dependency between connections in
an alignment, we assume that the offset of a con-
nection is determined relative to the preceding con-
nection in A, instead of relative to the initial align-
ment, I. For this purpose, we define A&apos; (j) as a lin-
ear extrapolation from the preceding connection in
A:
</bodyText>
<equation confidence="0.561718">
As(j) = AUprev (j jPrev 7cr7,1VE (13)
</equation>
<bodyText confidence="0.997515857142857">
where previ is the last French position before j
which is aligned by A and NE and NF are the
lengths of the English and French corpora. A&apos;(j)
thus predicts the connection of j, knowing the con-
nection of jprey and assuming that the two lan-
guages have the same word order. Instead of (12),
the most probable alignment maximizes
</bodyText>
<equation confidence="0.746268">
11 t(fi le,) • o(i — (j)). (14)
con,„€ A
</equation>
<page confidence="0.953341">
5
</page>
<bodyText confidence="0.9946604">
French word
zone
fermer
informations
insertion
English translations (with probabilities)
box (0.58) area (0.28) want (0.04) In (0.02)
close (0.44) when (0.08) Close (0.07) selected (0.06)
information (0.66) about (0.15) For (0.12) see (0.04)
insertion (0.61) point (0.23) Edit (0.06) To (0.05)
</bodyText>
<tableCaption confidence="0.996773">
Table 1: Estimated translation probabilities
</tableCaption>
<figure confidence="0.983079">
.0
2 0
0(NI
- • .......
-20 -10 0 10 20
Offset
</figure>
<figureCaption confidence="0.999947">
Figure 2: Estimated offset probabilities (solid line) along with a Gaussian (dashed line) for comparison.
</figureCaption>
<bodyText confidence="0.998789428571429">
We approximate the offset probabilities, o(k), rela-
tive to A&apos;, using the maximum likelihood estimates
which were computed relative to I (as described in
Section 2.1.2).
We use a dynamic programming algorithm to
find the most probable alignment. This enables
us to know the value A/ 1 when dealing with
</bodyText>
<subsectionHeader confidence="0.782009">
,,prev
</subsectionHeader>
<bodyText confidence="0.887495473684211">
position j. To avoid connections with very low
probability (due to a large offset) we require that
i( f, lei ) • o(i — (j)) exceeds a pre-specified thresh-
old T3. If the threshold is not exceeded, the
connection is dropped from the alignment, and
i(file) • o(i — (j)) for that connection is set to
T when computing (14). T can therefore be inter-
preted as a global setting of the probability that
a random position will be connected to the null
&apos;In fact, the threshold on t(hle,), which is used to
determine the relevant connections (described in the
previous subsection), is used just as an efficient early
application of the threshold T. This early application
is possible when t(hle,)- o(k,„„x) &lt; T, where kmax is
the value of k with maximal o(k).
English word&apos;. A similar dynamic programming
approach was used by Gale and Church for word
alignment (Gale and Church, 1991a), to handle de-
pendency between connections.
</bodyText>
<sectionHeader confidence="0.994889" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.98345975">
Word_olign was first evaluated on a representative
sample of Canadian Hansards (160,000 words in
English and French). The sample was kindly pro-
vided by Simard et al., along with alignments of
sentence boundaries as determined by their panel
of 8 judges (Simard et al., 1992).
Ten iterations of the EM algorithm were com-
puted to estimate the parameters of the model.
The window size was set to 20 words in each di-
rection, and the minimal threshold for t(fle) was
set to 0.005. We considered connections whose
source and target words had frequencies between 3
and 1700 (1700 is the highest frequency of a con-
tent word in the corpus. We thus excluded as many
&apos;As mentioned earlier, we do not estimate directly
translation probabilities for the null English word.
</bodyText>
<page confidence="0.996448">
6
</page>
<bodyText confidence="0.958567690140846">
function words as possible, but no content words).
In this experiment, we used French as the source
language and English as the target language.
Figure 3 presents the alignment error rate of
word_align. It is compared with the error rate of
word_align&apos;s input, i.e. the initial rough alignment
which is produced by char_align. The errors are
sampled at sentence boundaries, and are measured
as the relative distance between the output of the
alignment program and the &amp;quot;true&amp;quot; alignment, as
defined by the human judges5. The histograms
present errors in the range of -20-20, which cov-
ers about 95% of the data.6. It can be seen that
word_align decreases the error rate significantly
(notice the different scales of the vertical axes). In
55% of the cases, there is no error in word_align&apos;s
output (distance of 0), in 73% the distance from
the correct alignment is at most 1, and in 84% the
distance is at most 3.
A second evaluation of word_align was per-
formed on noisy technical documents, of the type
typically available for AT&amp;T Language Line Ser-
vices. We used the English and French versions of
a manual of monitoring equipment (about 65,000
words), both scanned by an OCR device. We sam-
pled the English vocabulary with frequency be-
tween three and 450 occurrences, the same vocabu-
lary that was used for alignment. We sampled 100
types from the top fifth by frequency of the vocabu-
lary (quintile), 80 types from the second quintile, 60
from the third, 40 from the fourth, and 20 from the
bottom quintile. We used this stratified sampling
because we wanted to make more accurate state-
ments about our error rate by tokens than we would
have obtained from random sampling, or even from
equal weighting of the quintiles. After choosing the
300 types from the vocabulary list, one token for
each type was chosen at random from the corpus.
By hand, the best corresponding position in the
French version was chosen, to be compared with
word_align&apos;s output.
Table 2 summarizes the results of the second
experiment. The figures indicate the expected rela-
tive frequency of each offset from the correct align-
ment. This relative frequency was computed ac-
cording to the word frequencies in the stratified
sample. As shown in the table, for 60.5% of the to-
kens the alignment is accurate, and in 84% the off-
set from the correct alingment is at most 3. These
figures demonstrate the usefulness of word_align for
constructing bilingual lexicons, and its impact on
&apos;As explained earlier, word_align produces a partial
alignment. For the purpose of the evaluation, we used
linear interpolation to get alignments for all the posi-
tions in the sample.
6Recall that the window size we used is 20 words
in each direction, which means that word_align cannot
recover from larger errors in char_align.
char align errors (in words)
Figure 3: Word_align reduces the variance (average
square error) by a factor of 5 over char_align alone
(notice the vertical scales).
the quality of bilingual concordances (as in Fig-
ure 1). Indeed, using bilingual concordances which
are based on word_align&apos;s output, the translators at
AT&amp;T Language Line Services are now producing
bilingual terminology lexicons at a rate of 60-100
terms per hour! This is compared with the previous
rate of about 30 terms per hour using char_al:gn&apos;s
output, and an extremely lower rate before align-
ment tools were available.
</bodyText>
<sectionHeader confidence="0.99943" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999203545454545">
Compared with other word alignment algorithms
(Brown et al., 1993; Gale and Church, 1991a),
word_align does not require sentence alignment as
input, and was shown to produce useful align-
ments for small and noisy corpora. Its robust-
ness was achieved by modifying Brown et al.&apos;s
Model 2 to handle an initial &amp;quot;rough&amp;quot; alignment,
reducing the number of parameters and introduc-
ing a dependency between alignments of adjacent
words. Taking the output of char_align as in-
put, word_align produces significantly better, word-
</bodyText>
<figure confidence="0.57028725">
fififirinn,
-10 10 20
-20 -10 0 10 20
vrorti_elign errors (in words)
</figure>
<page confidence="0.994609">
7
</page>
<table confidence="0.999742142857143">
Offset from Percentage Accumulative
correct alignment percentage
0 60.5% 60.5%
1 10.8% 71.3%
2 7.5% 78.8%
3 5.2% 84%
4 1.6% 85.6%
</table>
<tableCaption confidence="0.9926005">
Table 2: Word_align&apos;s precision on noisy input,
scanned by an OCR device.
</tableCaption>
<bodyText confidence="0.996821">
level, alignments on the kind of corpora that are
typically available to translators. This improve-
ment increased the rate of constructing bilingual
terminology lexicons at AT&amp;T Language Line Ser-
vices by a factor of 2-3. In addition, the align-
ments may also be helpful to developers of lexicons
for machine translation systems. Word_align thus
provides an example how a model such as Brown
et al.&apos;s Model 2, that was originally designed for
research in statistical machine translation, can be
modified to achieve practical, though less ambi-
tious, goals in the near term.
</bodyText>
<sectionHeader confidence="0.999627" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999772277108434">
L. E. Baum. 1972. An inequality and an associ-
ated maximization technique in statistical es-
timation of probabilistic functions of a markov
process. Inequalities, 3:1-8.
P. Brown, J. Cocke, S. Della Pietra,
V. Della Pietra, F. Jelinek, R.L. Mercer, and
Roossin P.S. 1990. A statistical approach to
language translation. Computational Linguis-
tics, 16(2):79-85.
P. Brown, J. Lai, and R. Mercer. 1991a. Aligning
sentences in parallel corpora. In Proc. of the
Annual Meeting of the ACL.
P. Brown, S. Della Pietra, V. Della Pietra, and
R. Mercer. 1991b. Word sense disambiguation
using statistical methods. In Proc. of the An-
nual Meeting of the ACL.
Peter Brown, Stephen Della Pietra, Vincent Della
Pietra, and Robert Mercer. 1993. The mathe-
matics of machine translation: parameter esti-
mation. Computational Linguistics. to appear.
Kenneth W. Church. 1993. Char_align: A program
for aligning parallel texts at character level. In
Proc. of the Annual Meeting of the ACL.
A. P. Dempster, N. M. Laird, and D. B. Rubin.
1977. Maximum liklihood from incomplete
data via the EM algorithm. Journal of the
Royal Statistical Society, 39(B):1-38.
William Gale and Kenneth Church. 1991a. Identi-
fying word correspondence in parallel text. In
Proc. of the DARPA Workshop on Speech and
Natural Language.
William Gale and Kenneth Church. 1991b. A pro-
gram for aligning sentences in bilingual cor-
pora. In Proc. of the Annual Meeting of the
ACL.
William Gale, Kenneth Church, and David
Yarowsky. 1992. Using bilingual materials
to develop word sense disambiguation meth-
ods. In Proc. of the International Conference
on Theoretical and Methodolgical Issues in Ma-
chine Translation.
P. Isabelle. 1992. Bi-textual aids for translators.
In Proc. of the Annual Conference of the UW
Center for the New OED and Text Research.
M. Kay and M. Rosenschein. 1993. Text-
translation alignment. Computational Linguis-
tics, to appear.
J. Klavans and E. Tzoukermann. 1990. The bicord
system. In Proc. of COLING.
Julian Kupiec. 1993. An algorithm for finding
noun phrase correspondences in bilingual cor-
pora. In Proc. of the Annual Meeting of the
ACL.
Thomas K. Landauer and Michael L. Littman.
1990. Fully automatic cross-language docu-
ment retrieval using latent semantic indexing.
In Proc. of the Annual Conference of the UW
Center for the New OED and Text Research.
Yuji Matsumoto, Hiroyuki Ishimoto, Takehito Ut-
suro, and Makoto Nagao. 1993. Structural
matching of parallel texts. In Proc. of the An-
nual Meeting of the ACL.
William Ogden and Margarita Gonzales. 1993.
Norm - a system for translators. Demonstra-
tion at ARPA Workshop on Human Language
Technology.
V. Sadler. 1989. Working with analogical seman-
tics: Disambiguation techniques in DLT. Foris
Publications.
M. Simard, G. Foster, and P. Isabelle. 1992. Us-
ing cognates to align sentences in bilingual cor-
pora. In Proc. of the International Conference
on Theoretical and Methodolgical Issues in Ma-
chine Translation.
Frank Smadja. 1992. How to compile a bilingual
collocational lexicon automatically. In AAAI
Workshop on Statistically-based Natural Lan-
guage Processing Techniques, July.
S. Warwick, J. Hajic, and G. Russell. 1990. Search-
ing on tagged corpora: linguistically motivated
concordance analysis. In Proc. of the Annual
Conference of the UW Center for the New
OED and Text Research.
</reference>
<page confidence="0.998491">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.870692">
<title confidence="0.997896">Robust Bilingual Word for Machine Aided Translation</title>
<author confidence="0.999992">Ido Dagan Kenneth W Church William A Gale</author>
<affiliation confidence="0.999875">AT&amp;T Bell Laboratories</affiliation>
<address confidence="0.999804">600 Mountain Avenue Murray Hill, NJ 07974</address>
<abstract confidence="0.994431347826087">We have developed a new program called aligning parallel text, text such as the Canadian Hansards that are available in two or more languages. The program takes the of 1993), a robust alternative to sentence-based alignment programs, and applies word-level constraints usa version of Brown Model 2 (Brown et al., 1993), modified and extended to deal robustness issues. tested on a subset of Canadian Hansards supplied by Simard (Simard et al., 1992). The combination of word_align plus char_align reduces the variance (average square error) by a factor of over More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&amp;T Language Line Services, a commercial translation service, to help them with difficult terminology.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L E Baum</author>
</authors>
<title>An inequality and an associated maximization technique in statistical estimation of probabilistic functions of a markov process.</title>
<date>1972</date>
<journal>Inequalities,</journal>
<pages>3--1</pages>
<contexts>
<context position="11062" citStr="Baum, 1972" startWordPosition="1909" endWordPosition="1910">he English word in position i of the English sentence e of length I. Also, remember that position 0 is reserved for the null word. Equations 3 and 4 are used iteratively to estimate t(f le). That is, we start with an initial guess for (f le). We then evaluation the right hand side of equation 4, and compute the probability of the connections in the training set. Then we evaluate equation 3, obtain new estimates for the translation probabilities, and repeat the process, until it converges. This iterative process is known as the EM algorithm and has been shown to converge to a stationary point (Baum, 1972; Dempster et al., 1977). Moreover, Brown et a/. show that Model 1 has a unique maximum, and therefore, in this special case, the EM algorithm is guaranteed to converge to the maximum likelihood solution, and does not depend on the initial guess. Model 2 Model 2 improves upon model 1 by making use of the positions within an alignment. For instance, it is much more likely that the first word of an English sentence will be connected to a word near the beginning of the corresponding French sentence, than to some word near the end of the French sentence. Model 2 enhances Model 1 with the assumptio</context>
</contexts>
<marker>Baum, 1972</marker>
<rawString>L. E. Baum. 1972. An inequality and an associated maximization technique in statistical estimation of probabilistic functions of a markov process. Inequalities, 3:1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>J Cocke</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>F Jelinek</author>
<author>R L Mercer</author>
<author>P S Roossin</author>
</authors>
<title>A statistical approach to language translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<pages>16--2</pages>
<contexts>
<context position="1491" citStr="Brown et al., 1990" startWordPosition="233" endWordPosition="236">and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&amp;T Language Line Services, a commercial translation service, to help them with difficult terminology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval application may be of particular relevance to this audience. It would be highly desirable for users to be able to express queries in whatever language they chose and retrieve documents that may or may not have been written in </context>
<context position="7028" citStr="Brown et al., 1990" startWordPosition="1162" endWordPosition="1165">e translation probabilities are estimated using a method based on Brown et al.&apos;s Model 2 (1993), which is summarized in the following subsection, 2.1.1. Then, in subsection 2.1.2, we describe modifications that achieve three goals: (1) enable word_align to accept input which may not be aligned by sentence (e.g. char_al:gn&apos;s output), (2) reduce the number of parameters that need to be estimated, and (3) prepare the ground for the second step, the search for the best alignment (described in section 2.2). 2.1.1 Brown et al.&apos;s Model In the context of their statistical machine translation project (Brown et al., 1990), Brown et al. estimate Pr(f le), the probability that f, a sentence in one language (say French), is the translation of e, a sentence in the other language (say English). Pr(f le) is computed using the concept of altgnment, denoted by a, which is a set of connections between each French word in f and the corresponding English word in e. A connection, which we will write f e as con- specifies that position j in f is connected 1,1 &apos; to position i in e. If a French word in f does not correspond to any English word in e, then it is connected to the special word null (position 0 in e). Notice that</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Mercer, Roossin, 1990</marker>
<rawString>P. Brown, J. Cocke, S. Della Pietra, V. Della Pietra, F. Jelinek, R.L. Mercer, and Roossin P.S. 1990. A statistical approach to language translation. Computational Linguistics, 16(2):79-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>J Lai</author>
<author>R Mercer</author>
</authors>
<title>Aligning sentences in parallel corpora.</title>
<date>1991</date>
<booktitle>In Proc. of the Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="1268" citStr="Brown et al., 1991" startWordPosition="196" endWordPosition="199">dian Hansards supplied by Simard (Simard et al., 1992). The combination of word_align plus char_align reduces the variance (average square error) by a factor of 5 over char_align alone. More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&amp;T Language Line Services, a commercial translation service, to help them with difficult terminology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval app</context>
<context position="5779" citStr="Brown et al., 1991" startWordPosition="967" endWordPosition="970">rdances are selected from English and French versions of the Microsoft Windows manual (with some errors introduced by OCR). There are three lines of text for each instance of &amp;quot;box&amp;quot;: (1) English, (2) glosses, and (3) French. The glosses are selected from the French text (the third line), and are written underneath the corresponding English words, as identified by word_align. been used in previous studies. To deal with these robustness issues, Church (1993) developed a character-based alignment method called char_align. The method was intended as a replacement for sentence-based methods (e.g., (Brown et al., 1991a; Gale and Church, 1991b; Kay and Rosenschein, 1993)), which are very sensitive to noise. This paper describes a new program, called word_align, that starts with an initial &amp;quot;rough&amp;quot; alignment (e.g., the output of char_a/ign or a sentence-based alignment method), and produces improved alignments by exploiting constraints at the word-level. The alignment algorithm consists of two steps: (1) estimate translation probabilities, and (2) use these probabilities to search for most probable alignment path. The two steps are described in the following section. 2 The alignment Algorithm 2.1 Estimation o</context>
</contexts>
<marker>Brown, Lai, Mercer, 1991</marker>
<rawString>P. Brown, J. Lai, and R. Mercer. 1991a. Aligning sentences in parallel corpora. In Proc. of the Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>R Mercer</author>
</authors>
<title>Word sense disambiguation using statistical methods.</title>
<date>1991</date>
<booktitle>In Proc. of the Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="1268" citStr="Brown et al., 1991" startWordPosition="196" endWordPosition="199">dian Hansards supplied by Simard (Simard et al., 1992). The combination of word_align plus char_align reduces the variance (average square error) by a factor of 5 over char_align alone. More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&amp;T Language Line Services, a commercial translation service, to help them with difficult terminology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval app</context>
<context position="5779" citStr="Brown et al., 1991" startWordPosition="967" endWordPosition="970">rdances are selected from English and French versions of the Microsoft Windows manual (with some errors introduced by OCR). There are three lines of text for each instance of &amp;quot;box&amp;quot;: (1) English, (2) glosses, and (3) French. The glosses are selected from the French text (the third line), and are written underneath the corresponding English words, as identified by word_align. been used in previous studies. To deal with these robustness issues, Church (1993) developed a character-based alignment method called char_align. The method was intended as a replacement for sentence-based methods (e.g., (Brown et al., 1991a; Gale and Church, 1991b; Kay and Rosenschein, 1993)), which are very sensitive to noise. This paper describes a new program, called word_align, that starts with an initial &amp;quot;rough&amp;quot; alignment (e.g., the output of char_a/ign or a sentence-based alignment method), and produces improved alignments by exploiting constraints at the word-level. The alignment algorithm consists of two steps: (1) estimate translation probabilities, and (2) use these probabilities to search for most probable alignment path. The two steps are described in the following section. 2 The alignment Algorithm 2.1 Estimation o</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1991</marker>
<rawString>P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer. 1991b. Word sense disambiguation using statistical methods. In Proc. of the Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Brown</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>Robert Mercer</author>
</authors>
<title>The mathematics of machine translation: parameter estimation. Computational Linguistics.</title>
<date>1993</date>
<note>to appear.</note>
<contexts>
<context position="28443" citStr="Brown et al., 1993" startWordPosition="4923" endWordPosition="4926">reduces the variance (average square error) by a factor of 5 over char_align alone (notice the vertical scales). the quality of bilingual concordances (as in Figure 1). Indeed, using bilingual concordances which are based on word_align&apos;s output, the translators at AT&amp;T Language Line Services are now producing bilingual terminology lexicons at a rate of 60-100 terms per hour! This is compared with the previous rate of about 30 terms per hour using char_al:gn&apos;s output, and an extremely lower rate before alignment tools were available. 4 Conclusions Compared with other word alignment algorithms (Brown et al., 1993; Gale and Church, 1991a), word_align does not require sentence alignment as input, and was shown to produce useful alignments for small and noisy corpora. Its robustness was achieved by modifying Brown et al.&apos;s Model 2 to handle an initial &amp;quot;rough&amp;quot; alignment, reducing the number of parameters and introducing a dependency between alignments of adjacent words. Taking the output of char_align as input, word_align produces significantly better, wordfififirinn, -10 10 20 -20 -10 0 10 20 vrorti_elign errors (in words) 7 Offset from Percentage Accumulative correct alignment percentage 0 60.5% 60.5% 1</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter Brown, Stephen Della Pietra, Vincent Della Pietra, and Robert Mercer. 1993. The mathematics of machine translation: parameter estimation. Computational Linguistics. to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
</authors>
<title>Char_align: A program for aligning parallel texts at character level.</title>
<date>1993</date>
<booktitle>In Proc. of the Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="1379" citStr="Church, 1993" startWordPosition="216" endWordPosition="217">iance (average square error) by a factor of 5 over char_align alone. More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&amp;T Language Line Services, a commercial translation service, to help them with difficult terminology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval application may be of particular relevance to this audience. It would be highly desirable for users to be able to </context>
<context position="5620" citStr="Church (1993)" startWordPosition="945" endWordPosition="946"> output of word_align. Four concordances for the word &amp;quot;box&amp;quot; are shown, identifying three different translations for the word: boite, case, zone. The concordances are selected from English and French versions of the Microsoft Windows manual (with some errors introduced by OCR). There are three lines of text for each instance of &amp;quot;box&amp;quot;: (1) English, (2) glosses, and (3) French. The glosses are selected from the French text (the third line), and are written underneath the corresponding English words, as identified by word_align. been used in previous studies. To deal with these robustness issues, Church (1993) developed a character-based alignment method called char_align. The method was intended as a replacement for sentence-based methods (e.g., (Brown et al., 1991a; Gale and Church, 1991b; Kay and Rosenschein, 1993)), which are very sensitive to noise. This paper describes a new program, called word_align, that starts with an initial &amp;quot;rough&amp;quot; alignment (e.g., the output of char_a/ign or a sentence-based alignment method), and produces improved alignments by exploiting constraints at the word-level. The alignment algorithm consists of two steps: (1) estimate translation probabilities, and (2) use t</context>
</contexts>
<marker>Church, 1993</marker>
<rawString>Kenneth W. Church. 1993. Char_align: A program for aligning parallel texts at character level. In Proc. of the Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum liklihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<pages>39--1</pages>
<contexts>
<context position="11086" citStr="Dempster et al., 1977" startWordPosition="1911" endWordPosition="1914">ord in position i of the English sentence e of length I. Also, remember that position 0 is reserved for the null word. Equations 3 and 4 are used iteratively to estimate t(f le). That is, we start with an initial guess for (f le). We then evaluation the right hand side of equation 4, and compute the probability of the connections in the training set. Then we evaluate equation 3, obtain new estimates for the translation probabilities, and repeat the process, until it converges. This iterative process is known as the EM algorithm and has been shown to converge to a stationary point (Baum, 1972; Dempster et al., 1977). Moreover, Brown et a/. show that Model 1 has a unique maximum, and therefore, in this special case, the EM algorithm is guaranteed to converge to the maximum likelihood solution, and does not depend on the initial guess. Model 2 Model 2 improves upon model 1 by making use of the positions within an alignment. For instance, it is much more likely that the first word of an English sentence will be connected to a word near the beginning of the corresponding French sentence, than to some word near the end of the French sentence. Model 2 enhances Model 1 with the assumption that the probability o</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum liklihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(B):1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Gale</author>
<author>Kenneth Church</author>
</authors>
<title>Identifying word correspondence in parallel text.</title>
<date>1991</date>
<booktitle>In Proc. of the DARPA Workshop on Speech and Natural Language.</booktitle>
<contexts>
<context position="1292" citStr="Gale and Church, 1991" startWordPosition="200" endWordPosition="203">d by Simard (Simard et al., 1992). The combination of word_align plus char_align reduces the variance (average square error) by a factor of 5 over char_align alone. More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&amp;T Language Line Services, a commercial translation service, to help them with difficult terminology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval application may be of parti</context>
<context position="5803" citStr="Gale and Church, 1991" startWordPosition="971" endWordPosition="974">from English and French versions of the Microsoft Windows manual (with some errors introduced by OCR). There are three lines of text for each instance of &amp;quot;box&amp;quot;: (1) English, (2) glosses, and (3) French. The glosses are selected from the French text (the third line), and are written underneath the corresponding English words, as identified by word_align. been used in previous studies. To deal with these robustness issues, Church (1993) developed a character-based alignment method called char_align. The method was intended as a replacement for sentence-based methods (e.g., (Brown et al., 1991a; Gale and Church, 1991b; Kay and Rosenschein, 1993)), which are very sensitive to noise. This paper describes a new program, called word_align, that starts with an initial &amp;quot;rough&amp;quot; alignment (e.g., the output of char_a/ign or a sentence-based alignment method), and produces improved alignments by exploiting constraints at the word-level. The alignment algorithm consists of two steps: (1) estimate translation probabilities, and (2) use these probabilities to search for most probable alignment path. The two steps are described in the following section. 2 The alignment Algorithm 2.1 Estimation of translation probabilit</context>
<context position="24178" citStr="Gale and Church, 1991" startWordPosition="4207" endWordPosition="4210">e) • o(i — (j)) for that connection is set to T when computing (14). T can therefore be interpreted as a global setting of the probability that a random position will be connected to the null &apos;In fact, the threshold on t(hle,), which is used to determine the relevant connections (described in the previous subsection), is used just as an efficient early application of the threshold T. This early application is possible when t(hle,)- o(k,„„x) &lt; T, where kmax is the value of k with maximal o(k). English word&apos;. A similar dynamic programming approach was used by Gale and Church for word alignment (Gale and Church, 1991a), to handle dependency between connections. 3 Evaluation Word_olign was first evaluated on a representative sample of Canadian Hansards (160,000 words in English and French). The sample was kindly provided by Simard et al., along with alignments of sentence boundaries as determined by their panel of 8 judges (Simard et al., 1992). Ten iterations of the EM algorithm were computed to estimate the parameters of the model. The window size was set to 20 words in each direction, and the minimal threshold for t(fle) was set to 0.005. We considered connections whose source and target words had frequ</context>
<context position="28466" citStr="Gale and Church, 1991" startWordPosition="4927" endWordPosition="4930"> (average square error) by a factor of 5 over char_align alone (notice the vertical scales). the quality of bilingual concordances (as in Figure 1). Indeed, using bilingual concordances which are based on word_align&apos;s output, the translators at AT&amp;T Language Line Services are now producing bilingual terminology lexicons at a rate of 60-100 terms per hour! This is compared with the previous rate of about 30 terms per hour using char_al:gn&apos;s output, and an extremely lower rate before alignment tools were available. 4 Conclusions Compared with other word alignment algorithms (Brown et al., 1993; Gale and Church, 1991a), word_align does not require sentence alignment as input, and was shown to produce useful alignments for small and noisy corpora. Its robustness was achieved by modifying Brown et al.&apos;s Model 2 to handle an initial &amp;quot;rough&amp;quot; alignment, reducing the number of parameters and introducing a dependency between alignments of adjacent words. Taking the output of char_align as input, word_align produces significantly better, wordfififirinn, -10 10 20 -20 -10 0 10 20 vrorti_elign errors (in words) 7 Offset from Percentage Accumulative correct alignment percentage 0 60.5% 60.5% 1 10.8% 71.3% 2 7.5% 78.</context>
</contexts>
<marker>Gale, Church, 1991</marker>
<rawString>William Gale and Kenneth Church. 1991a. Identifying word correspondence in parallel text. In Proc. of the DARPA Workshop on Speech and Natural Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Gale</author>
<author>Kenneth Church</author>
</authors>
<title>A program for aligning sentences in bilingual corpora.</title>
<date>1991</date>
<booktitle>In Proc. of the Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="1292" citStr="Gale and Church, 1991" startWordPosition="200" endWordPosition="203">d by Simard (Simard et al., 1992). The combination of word_align plus char_align reduces the variance (average square error) by a factor of 5 over char_align alone. More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&amp;T Language Line Services, a commercial translation service, to help them with difficult terminology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval application may be of parti</context>
<context position="5803" citStr="Gale and Church, 1991" startWordPosition="971" endWordPosition="974">from English and French versions of the Microsoft Windows manual (with some errors introduced by OCR). There are three lines of text for each instance of &amp;quot;box&amp;quot;: (1) English, (2) glosses, and (3) French. The glosses are selected from the French text (the third line), and are written underneath the corresponding English words, as identified by word_align. been used in previous studies. To deal with these robustness issues, Church (1993) developed a character-based alignment method called char_align. The method was intended as a replacement for sentence-based methods (e.g., (Brown et al., 1991a; Gale and Church, 1991b; Kay and Rosenschein, 1993)), which are very sensitive to noise. This paper describes a new program, called word_align, that starts with an initial &amp;quot;rough&amp;quot; alignment (e.g., the output of char_a/ign or a sentence-based alignment method), and produces improved alignments by exploiting constraints at the word-level. The alignment algorithm consists of two steps: (1) estimate translation probabilities, and (2) use these probabilities to search for most probable alignment path. The two steps are described in the following section. 2 The alignment Algorithm 2.1 Estimation of translation probabilit</context>
<context position="24178" citStr="Gale and Church, 1991" startWordPosition="4207" endWordPosition="4210">e) • o(i — (j)) for that connection is set to T when computing (14). T can therefore be interpreted as a global setting of the probability that a random position will be connected to the null &apos;In fact, the threshold on t(hle,), which is used to determine the relevant connections (described in the previous subsection), is used just as an efficient early application of the threshold T. This early application is possible when t(hle,)- o(k,„„x) &lt; T, where kmax is the value of k with maximal o(k). English word&apos;. A similar dynamic programming approach was used by Gale and Church for word alignment (Gale and Church, 1991a), to handle dependency between connections. 3 Evaluation Word_olign was first evaluated on a representative sample of Canadian Hansards (160,000 words in English and French). The sample was kindly provided by Simard et al., along with alignments of sentence boundaries as determined by their panel of 8 judges (Simard et al., 1992). Ten iterations of the EM algorithm were computed to estimate the parameters of the model. The window size was set to 20 words in each direction, and the minimal threshold for t(fle) was set to 0.005. We considered connections whose source and target words had frequ</context>
<context position="28466" citStr="Gale and Church, 1991" startWordPosition="4927" endWordPosition="4930"> (average square error) by a factor of 5 over char_align alone (notice the vertical scales). the quality of bilingual concordances (as in Figure 1). Indeed, using bilingual concordances which are based on word_align&apos;s output, the translators at AT&amp;T Language Line Services are now producing bilingual terminology lexicons at a rate of 60-100 terms per hour! This is compared with the previous rate of about 30 terms per hour using char_al:gn&apos;s output, and an extremely lower rate before alignment tools were available. 4 Conclusions Compared with other word alignment algorithms (Brown et al., 1993; Gale and Church, 1991a), word_align does not require sentence alignment as input, and was shown to produce useful alignments for small and noisy corpora. Its robustness was achieved by modifying Brown et al.&apos;s Model 2 to handle an initial &amp;quot;rough&amp;quot; alignment, reducing the number of parameters and introducing a dependency between alignments of adjacent words. Taking the output of char_align as input, word_align produces significantly better, wordfififirinn, -10 10 20 -20 -10 0 10 20 vrorti_elign errors (in words) 7 Offset from Percentage Accumulative correct alignment percentage 0 60.5% 60.5% 1 10.8% 71.3% 2 7.5% 78.</context>
</contexts>
<marker>Gale, Church, 1991</marker>
<rawString>William Gale and Kenneth Church. 1991b. A program for aligning sentences in bilingual corpora. In Proc. of the Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Gale</author>
<author>Kenneth Church</author>
<author>David Yarowsky</author>
</authors>
<title>Using bilingual materials to develop word sense disambiguation methods.</title>
<date>1992</date>
<booktitle>In Proc. of the International Conference on Theoretical and Methodolgical Issues in Machine Translation.</booktitle>
<contexts>
<context position="1752" citStr="Gale et al., 1992" startWordPosition="269" endWordPosition="272">inology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval application may be of particular relevance to this audience. It would be highly desirable for users to be able to express queries in whatever language they chose and retrieve documents that may or may not have been written in the same language as the query. Landauer and Littman used SVD analysis (or Latent Semantic Indexing) on the Canadian Hansards, parliamentary debates that are published in both English and French, in order to estimate a kind of soft thesaurus. They then showed t</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>William Gale, Kenneth Church, and David Yarowsky. 1992. Using bilingual materials to develop word sense disambiguation methods. In Proc. of the International Conference on Theoretical and Methodolgical Issues in Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Isabelle</author>
</authors>
<title>Bi-textual aids for translators.</title>
<date>1992</date>
<booktitle>In Proc. of the Annual Conference of the UW Center for the New OED and Text</booktitle>
<note>to appear.</note>
<contexts>
<context position="1565" citStr="Isabelle, 1992" startWordPosition="244" endWordPosition="245">re noisy than the Hansards, it has been possible to successfully deploy the programs at AT&amp;T Language Line Services, a commercial translation service, to help them with difficult terminology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval application may be of particular relevance to this audience. It would be highly desirable for users to be able to express queries in whatever language they chose and retrieve documents that may or may not have been written in the same language as the query. Landauer and Littman used SVD analysis (or</context>
</contexts>
<marker>Isabelle, 1992</marker>
<rawString>P. Isabelle. 1992. Bi-textual aids for translators. In Proc. of the Annual Conference of the UW Center for the New OED and Text Research. M. Kay and M. Rosenschein. 1993. Texttranslation alignment. Computational Linguistics, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Klavans</author>
<author>E Tzoukermann</author>
</authors>
<title>The bicord system.</title>
<date>1990</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="1648" citStr="Klavans and Tzoukermann, 1990" startWordPosition="253" endWordPosition="256">ploy the programs at AT&amp;T Language Line Services, a commercial translation service, to help them with difficult terminology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval application may be of particular relevance to this audience. It would be highly desirable for users to be able to express queries in whatever language they chose and retrieve documents that may or may not have been written in the same language as the query. Landauer and Littman used SVD analysis (or Latent Semantic Indexing) on the Canadian Hansards, parliamentary debates that are</context>
</contexts>
<marker>Klavans, Tzoukermann, 1990</marker>
<rawString>J. Klavans and E. Tzoukermann. 1990. The bicord system. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
</authors>
<title>An algorithm for finding noun phrase correspondences in bilingual corpora.</title>
<date>1993</date>
<booktitle>In Proc. of the Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="1393" citStr="Kupiec, 1993" startWordPosition="218" endWordPosition="219"> square error) by a factor of 5 over char_align alone. More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&amp;T Language Line Services, a commercial translation service, to help them with difficult terminology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval application may be of particular relevance to this audience. It would be highly desirable for users to be able to express querie</context>
</contexts>
<marker>Kupiec, 1993</marker>
<rawString>Julian Kupiec. 1993. An algorithm for finding noun phrase correspondences in bilingual corpora. In Proc. of the Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Michael L Littman</author>
</authors>
<title>Fully automatic cross-language document retrieval using latent semantic indexing.</title>
<date>1990</date>
<booktitle>In Proc. of the Annual Conference of the UW Center for the New OED and Text Research.</booktitle>
<contexts>
<context position="1837" citStr="Landauer and Littman, 1990" startWordPosition="281" endWordPosition="284">erable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval application may be of particular relevance to this audience. It would be highly desirable for users to be able to express queries in whatever language they chose and retrieve documents that may or may not have been written in the same language as the query. Landauer and Littman used SVD analysis (or Latent Semantic Indexing) on the Canadian Hansards, parliamentary debates that are published in both English and French, in order to estimate a kind of soft thesaurus. They then showed that these estimates could be used to retrieve documents appropriately in the bilingua</context>
</contexts>
<marker>Landauer, Littman, 1990</marker>
<rawString>Thomas K. Landauer and Michael L. Littman. 1990. Fully automatic cross-language document retrieval using latent semantic indexing. In Proc. of the Annual Conference of the UW Center for the New OED and Text Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuji Matsumoto</author>
<author>Hiroyuki Ishimoto</author>
<author>Takehito Utsuro</author>
<author>Makoto Nagao</author>
</authors>
<title>Structural matching of parallel texts.</title>
<date>1993</date>
<booktitle>In Proc. of the Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="1418" citStr="Matsumoto et al., 1993" startWordPosition="220" endWordPosition="223"> by a factor of 5 over char_align alone. More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&amp;T Language Line Services, a commercial translation service, to help them with difficult terminology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval application may be of particular relevance to this audience. It would be highly desirable for users to be able to express queries in whatever language th</context>
</contexts>
<marker>Matsumoto, Ishimoto, Utsuro, Nagao, 1993</marker>
<rawString>Yuji Matsumoto, Hiroyuki Ishimoto, Takehito Utsuro, and Makoto Nagao. 1993. Structural matching of parallel texts. In Proc. of the Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Ogden</author>
<author>Margarita Gonzales</author>
</authors>
<title>Norm - a system for translators. Demonstration at ARPA Workshop on Human Language Technology.</title>
<date>1993</date>
<contexts>
<context position="1592" citStr="Ogden and Gonzales, 1993" startWordPosition="246" endWordPosition="249">e Hansards, it has been possible to successfully deploy the programs at AT&amp;T Language Line Services, a commercial translation service, to help them with difficult terminology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval application may be of particular relevance to this audience. It would be highly desirable for users to be able to express queries in whatever language they chose and retrieve documents that may or may not have been written in the same language as the query. Landauer and Littman used SVD analysis (or Latent Semantic Indexing) </context>
</contexts>
<marker>Ogden, Gonzales, 1993</marker>
<rawString>William Ogden and Margarita Gonzales. 1993. Norm - a system for translators. Demonstration at ARPA Workshop on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Sadler</author>
</authors>
<title>Working with analogical semantics: Disambiguation techniques in DLT.</title>
<date>1989</date>
<publisher>Foris Publications.</publisher>
<contexts>
<context position="1506" citStr="Sadler, 1989" startWordPosition="237" endWordPosition="238">designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&amp;T Language Line Services, a commercial translation service, to help them with difficult terminology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval application may be of particular relevance to this audience. It would be highly desirable for users to be able to express queries in whatever language they chose and retrieve documents that may or may not have been written in the same langua</context>
</contexts>
<marker>Sadler, 1989</marker>
<rawString>V. Sadler. 1989. Working with analogical semantics: Disambiguation techniques in DLT. Foris Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Simard</author>
<author>G Foster</author>
<author>P Isabelle</author>
</authors>
<title>Using cognates to align sentences in bilingual corpora.</title>
<date>1992</date>
<booktitle>In Proc. of the International Conference on Theoretical and Methodolgical Issues in Machine Translation.</booktitle>
<contexts>
<context position="704" citStr="Simard et al., 1992" startWordPosition="108" endWordPosition="111">hurch William A. Gale AT&amp;T Bell Laboratories 600 Mountain Avenue Murray Hill, NJ 07974 Abstract We have developed a new program called word_align for aligning parallel text, text such as the Canadian Hansards that are available in two or more languages. The program takes the output of char_align (Church, 1993), a robust alternative to sentence-based alignment programs, and applies word-level constraints using a version of Brown et al.&apos;s Model 2 (Brown et al., 1993), modified and extended to deal with robustness issues. Word_align was tested on a subset of Canadian Hansards supplied by Simard (Simard et al., 1992). The combination of word_align plus char_align reduces the variance (average square error) by a factor of 5 over char_align alone. More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&amp;T Language Line Services, a commercial translation service, to help them with difficult terminology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and </context>
<context position="24511" citStr="Simard et al., 1992" startWordPosition="4260" endWordPosition="4263">s an efficient early application of the threshold T. This early application is possible when t(hle,)- o(k,„„x) &lt; T, where kmax is the value of k with maximal o(k). English word&apos;. A similar dynamic programming approach was used by Gale and Church for word alignment (Gale and Church, 1991a), to handle dependency between connections. 3 Evaluation Word_olign was first evaluated on a representative sample of Canadian Hansards (160,000 words in English and French). The sample was kindly provided by Simard et al., along with alignments of sentence boundaries as determined by their panel of 8 judges (Simard et al., 1992). Ten iterations of the EM algorithm were computed to estimate the parameters of the model. The window size was set to 20 words in each direction, and the minimal threshold for t(fle) was set to 0.005. We considered connections whose source and target words had frequencies between 3 and 1700 (1700 is the highest frequency of a content word in the corpus. We thus excluded as many &apos;As mentioned earlier, we do not estimate directly translation probabilities for the null English word. 6 function words as possible, but no content words). In this experiment, we used French as the source language and</context>
</contexts>
<marker>Simard, Foster, Isabelle, 1992</marker>
<rawString>M. Simard, G. Foster, and P. Isabelle. 1992. Using cognates to align sentences in bilingual corpora. In Proc. of the International Conference on Theoretical and Methodolgical Issues in Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Smadja</author>
</authors>
<title>How to compile a bilingual collocational lexicon automatically.</title>
<date>1992</date>
<booktitle>In AAAI Workshop on Statistically-based Natural Language Processing Techniques,</booktitle>
<contexts>
<context position="1684" citStr="Smadja, 1992" startWordPosition="260" endWordPosition="261">ommercial translation service, to help them with difficult terminology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval application may be of particular relevance to this audience. It would be highly desirable for users to be able to express queries in whatever language they chose and retrieve documents that may or may not have been written in the same language as the query. Landauer and Littman used SVD analysis (or Latent Semantic Indexing) on the Canadian Hansards, parliamentary debates that are published in both English and Frenc</context>
</contexts>
<marker>Smadja, 1992</marker>
<rawString>Frank Smadja. 1992. How to compile a bilingual collocational lexicon automatically. In AAAI Workshop on Statistically-based Natural Language Processing Techniques, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Warwick</author>
<author>J Hajic</author>
<author>G Russell</author>
</authors>
<title>Searching on tagged corpora: linguistically motivated concordance analysis.</title>
<date>1990</date>
<booktitle>In Proc. of the Annual Conference of the UW Center for the New OED and Text Research.</booktitle>
<contexts>
<context position="1248" citStr="Warwick et al., 1990" startWordPosition="192" endWordPosition="195">ed on a subset of Canadian Hansards supplied by Simard (Simard et al., 1992). The combination of word_align plus char_align reduces the variance (average square error) by a factor of 5 over char_align alone. More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&amp;T Language Line Services, a commercial translation service, to help them with difficult terminology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The infor</context>
</contexts>
<marker>Warwick, Hajic, Russell, 1990</marker>
<rawString>S. Warwick, J. Hajic, and G. Russell. 1990. Searching on tagged corpora: linguistically motivated concordance analysis. In Proc. of the Annual Conference of the UW Center for the New OED and Text Research.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>