<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005631">
<title confidence="0.974119">
An Enriched MT Grammar for Under $100
</title>
<author confidence="0.929892">
Omar F. Zaidan and Juri Ganitkevitch
</author>
<affiliation confidence="0.988812">
Dept. of Computer Science, Johns Hopkins University
</affiliation>
<address confidence="0.823709">
Baltimore, MD 21218, USA
</address>
<email confidence="0.99932">
{ozaidan,juri}@cs.jhu.edu
</email>
<sectionHeader confidence="0.995646" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999372789473684">
We propose a framework for improving out-
put quality of machine translation systems, by
operating on the level of grammar rule fea-
tures. Our framework aims to give a boost to
grammar rules that appear in the derivations
of translation candidates that are deemed to
be of good quality, hence making those rules
more preferable by the system. To that end, we
ask human annotators on Amazon Mechanical
Turk to compare translation candidates, and
then interpret their preferences of one candi-
date over another as an implicit preference for
one derivation over another, and therefore as
an implicit preference for one or more gram-
mar rules. Our framework also allows us to
generalize these preferences to grammar rules
corresponding to a previously unseen test set,
namely rules for which no candidates have
beenjudged.
</bodyText>
<sectionHeader confidence="0.999137" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997535">
When translating between two languages, state-
of-the-art statistical machine translation sys-
tems (Koehn et al., 2007; Li et al., 2009) generate
candidate translations by relying on a set of relevant
grammar (or phrase table) entries. Each of those
entries, or rules, associates a string in the source
language with a string in the target language, with
these associations typically learned by examining
a large parallel bitext. By the very nature of the
translation process, a target side sentence e can
be a candidate translation for a source sentence f
only if e can be constructed using a small subset
of the grammar, namely the subset of rules with
source side sequences relevant to the word sequence
of f. However, even this limited set of candidates
(call it £(f)) is quite large, with |£(f) |growing
exponentially in the length of f. The system is able
to rank the translations within £(f) by assigning a
score s(e) to each candidate translation. This score
is the dot product:
</bodyText>
<equation confidence="0.999147">
s(e) = Co(e) · w� (1)
</equation>
<bodyText confidence="0.999861714285714">
where cp(e) is a feature vector characterizing e, and
w� is a system-specific weight vector characterizing
the system’s belief of how much the different fea-
tures reflect translation quality. The features of a
candidate e are computed by examining the way e is
constructed (or derived), and so if we let d(e) be the
derivation of e, the feature vector can be denoted:1
</bodyText>
<equation confidence="0.998571">
Co(d(e)) = (cp1(d(e)), ... , cpm(d(e))) (2)
</equation>
<bodyText confidence="0.999932888888889">
where cpi(d(e)) is the value of ith feature function
of d(e) (with a corresponding weight wi in w).
To compute the score for a candidate, we examine
its derivation d(e), enumerating the grammar rules
used to construct e: d(e) = (r1, ... , rk). Typically,
each of the rules will itself have a vector of m fea-
tures, and we calculate the value of a derivation fea-
ture cpi(d(e)) as the sum of the ith feature over all
rules in the derivation:
</bodyText>
<equation confidence="0.989605">
cpi(d(e)) = � cpi(r) (3)
rEd(e)
</equation>
<bodyText confidence="0.933114">
1There are other features computed directly, without ex-
amining the derivation (e.g. candidate length, language model
score), but we omit these features from the motivation discus-
sion for clarity.
</bodyText>
<page confidence="0.988878">
93
</page>
<note confidence="0.7556945">
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 93–98,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999937212121212">
These features are usually either relative frequen-
cies estimated from the training corpus, relating the
rule’s source and target sides, or features that char-
acterize the structure of the rule itself, independently
from the corpus.
Either way, the weight wi is chosen so as to reflect
some belief regarding the correlation between the ith
feature and translation quality. This is usually done
by choosing weights that maximize performance on
a tuning set separate from the training bitext. Un-
like system weights, the grammar rule feature val-
ues are fixed once extracted, and are not modified
during this tuning phase. In this paper, we propose a
framework to augment the feature set to incorporate
additional intuition about how likely a rule is to pro-
duce a translation preferred by a human annotator.
This knowledge is acquired by directly asking hu-
man judges to compare candidate translations, there-
fore determining which subset of grammar rules an-
notators seem to prefer over others. We also seek to
generalize this intuition to rules for which no can-
didates were judged, hence allowing us to impact a
much larger set of rules than just those used in trans-
lating the tuning set.
The paper is organized as follows. We first give
a general description of our framework. We then
discuss our data collection efforts on Amazon Me-
chanical Turk for an Urdu-English translation task,
and make explicit the type of judgments we col-
lect and how they can be used to augment grammar
rules. Before concluding, we propose a framework
for generalizing judgments to unseen grammar rules,
and analyze the data collection process.
</bodyText>
<sectionHeader confidence="0.90731" genericHeader="method">
2 The General Framework
</sectionHeader>
<bodyText confidence="0.999415904761905">
As initially mentioned, when tuning a SMT system
on a development set, we typically only perform
high-level optimization of the system weights. In
this section we outline an approach that could allow
for lower-level optimization, on the level of individ-
ual grammar rules.
We kick off the process by soliciting judgments
from human annotators regarding the quality of a
subset of candidates (the following section outlines
how candidates are chosen). The resulting judg-
ments on sentences are interpreted to be judgments
on individual grammar rules used in the derivations
of these candidates. And so, if an annotator declares
a candidate to be of high quality, this is considered
a vote of confidence on the individual rules giving
rise to this candidate, and if an annotator declares a
candidate to be of lowl quality, this is considered a
vote of no confidence on the individual rules.
To make use of the collected judgments, we ex-
tend the set of features used in the decoder by a new
feature A:
</bodyText>
<equation confidence="0.995633">
_ (�01j... j �omj A) (4)
</equation>
<bodyText confidence="0.999554105263158">
This feature is the cornerstone of our framework,
as it will hold the quantified and cumulated judg-
ments for each rule, and will be used by the system
at decoding time, in addition to the existing m fea-
tures, incorporating the annotators’ judgments into
the translation process.2
The range of possible values for this feature, and
how the feature is computed, depends on how one
chooses to ask annotators to score candidates, and
what form those judgments assume (i.e. are those
judgments scores on a scale? Are they “better”
vs. “worse” judgments, and if so, compared to how
many other possibilities?). At this point, we will
only emphasize that the value of A should reflect
the annotators’ preference for the rule, and that it
should be computed from the collected judgments.
We will propose one such method of computing A in
Section 4, after describing the type of judgments we
collected.
</bodyText>
<sectionHeader confidence="0.990088" genericHeader="method">
3 Data Collection
</sectionHeader>
<bodyText confidence="0.999987166666667">
We apply our approach to an Urdu-to-English trans-
lation task. We used a syntactically rich SAMT
grammar (Venugopal and Zollmann, 2006), where
each rule in the grammar is characterized by 12 fea-
tures. The grammar was provided by Chris Callison-
Burch (personal communication), and was extracted
from a parallel corpus of 88k sentence pairs.3 One
system using this grammar produced significantly
improved output over submissions to the NIST 2009
Urdu-English task (Baker et al., 2009).
We use the Joshua system (Li et al., 2009)
as a decoder, with system weights tuned using
</bodyText>
<footnote confidence="0.752995">
2In fact, the collected judgments can only cover a small por-
tion of the grammar. We address this coverage problem in Sec-
tion 4.
</footnote>
<equation confidence="0.5735105">
3LDC catalog number LDC2009E12.
W
</equation>
<page confidence="0.984421">
94
</page>
<bodyText confidence="0.99917348">
Z-MERT (Zaidan, 2009) on a tuning set of 981 sen-
tences, a subset of the 2008 NIST Urdu-English test
set.4 We choose candidates to be judged from the
300-best candidate lists.5
Asking a worker to make a quantitative judgment
of the quality of a particular candidate translation
(e.g. on a 1–7 scale) is a highly subjective and
annotator-dependent process. Instead, we present
workers with pairs of candidates, and ask them to
judge which candidate is of better quality.
How are candidate pairs chosen? We would like
a judgment to have the maximum potential for be-
ing informative about specific grammar rules. In es-
sense, we prefer a pair of candidates if they have
highly similar derivations, yet differ noticeably in
terms of how the decoder ranks them. In other
words, if a relatively minimal change in derivation
causes a relatively large difference in the score as-
signed by the decoder, we are likely to attribute the
difference to very few rule comparisons (or perhaps
only one), hence focusing the comparison on indi-
vidual rules, all the while shielding the annotators
from having to compare grammar rules directly.
Specifically, each pair of candidates (e, e0) is as-
signed a potential score π(e, e0), defined as:
</bodyText>
<equation confidence="0.80658">
lev(d(e), d(e0)), (5)
</equation>
<bodyText confidence="0.999330214285714">
where s(e) is the score assigned by the decoder,
and lev(d, d�) is a distance measure between two
derivations which we will now descibe in more de-
tail. In Joshua, the derivation of a candidate is cap-
tured fully and exactly by a derivation tree, and so
we define lev(d, d&apos;) as a tree distance metric as fol-
lows. We first represent the trees as strings, using the
familiar nested string representation, then compute
the word-based Levenshtein edit distance between
the two strings. An edit has a cost of 1 in general, but
we assign a cost of zero to edit operations on termi-
nals, since we want to focus on the structure of the
derivation trees, rather than on terminal-level lexi-
cal choices.6 Furthermore, we ignore differences in
</bodyText>
<footnote confidence="0.980781">
4LDC catalog number LDC2009E11.
5We exclude source sentences shorter than 4 words long or
that have fewer than 4 candidate translations. This eliminates
roughly 6% of the development set.
6This is not to say that lexical choices are not important, but
lexical choice is heavily influenced by context, which is not cap-
</footnote>
<bodyText confidence="0.999859777777778">
“pure” pre-terminal rules, that only have terminals
as their right-hand side. These decisions effectively
allow us to focus our efforts on grammar rules with
at least one nonterminal in their right-hand side.
We perform the above potential computation on
all pairs formed by the cross product of the top 10
candidates and the top 300 candidates, and choose
the top five pairs ranked by potential.
Our HIT template is rather simple. Each HIT
screen corresponds to a single source sentence,
which is shown to the worker along with the five
chosen candidate pairs. To aid workers who are not
fluent in Urdu7 better judge translation quality, the
HIT also displays one of the available references
for that source sentence. To eliminate potential bias
associated with the order in which candidates are
presented (an annotator might be biased to choos-
ing the first presented candidate, for example), we
present the two candidates in random or- der. Fur-
thermore, for quality assurance, we embed a sixth
candidate pair to be judged, where we pair up a ran-
domly chosen candidate with another reference for
that sentence.8 Presumably, a faithful worker would
be unlikely to prefer a random candidate over the
reference, and so this functions as an embedded self-
verification test. The order of this test, relative to the
five original pairs, is chosen randomly.
</bodyText>
<sectionHeader confidence="0.993617" genericHeader="method">
4 Incorporating the Judgements
</sectionHeader>
<subsectionHeader confidence="0.985816">
4.1 Judgement Quantification
</subsectionHeader>
<bodyText confidence="0.999381833333333">
The judgments we obtain from the procedure de-
scribed in the previous section relate pairs of can-
didate translations. However, we have defined the
accumulation feature λ as a feature for each rule.
Thus, in order to compute λ, we need to project the
judgments onto the rules that tell the two candidates
apart. A simple way to do this is the following: for
a judged candidate pair (e, e0) let U(e) be the set of
tured well by grammar rules. Furthermore, lexical choice is a
phenomenon already well captured by the score assigned to the
candidate by the language model, a feature typically included
when designing 0.
</bodyText>
<footnote confidence="0.886257">
7We exclude workers from India and restrict the task to
workers with an existing approval rating of 90% or higher.
8The tuning set contains at least three different human refer-
ences for each source sentence, and so the reference “candidate”
shown to the worker is not the same as the sentence already
identified as a reference.
</footnote>
<equation confidence="0.947786">
π(e, e0) = s(e)s(e0)
</equation>
<page confidence="0.980035">
95
</page>
<bodyText confidence="0.999458461538462">
rules that appear in d(e) but not in d(e0), and vice
versa.9 We will assume that the jugdment obtained
for (e, e0) applies for every rule pair in the cartesian
product of U(e) and U(e0). This expansion yields
a set of judged grammar rule pairs J = {(a, b)}
with associated vote counts va&gt;b and vb&gt;a, captur-
ing how often the annotators preferred a candidate
that was set apart by a over a candidate containing
b, and vice versa.
So, following our prior definiton as an expression
of the judges’ preference, we can calculate the value
of A for a rule r as the relative frequency of favorable
judgements:
</bodyText>
<equation confidence="0.9889515">
�
(r,b)∈J vr&gt;b
A(r) = (6)
E(r,b)∈J vb&gt;r + vr&gt;b
</equation>
<subsectionHeader confidence="0.985221">
4.2 Generalization to Unseen Rules
</subsectionHeader>
<bodyText confidence="0.999992727272727">
This approach has a substantial problem: A, com-
puted as given above, is undefined for a rule that
was never judged (i.e. a rule that never set apart
a pair of candidates presented to the annotators).
Furthermore, as described, the coverage of the col-
lected judgments will be limited to a small subset
of the entire grammar, meaning that when the sys-
tem is asked to translate a new source sentence, it
is highly unlikely that the relevant grammar rules
would have already been judged by an annotator.
Therefore, it is necessary to generalize the collected
judgments/votes and propagate them to previously
unexamined rules.
In order to do this, we propose the following gen-
eral approach: when observing a judgment for a pair
of rules (a, b) E J , we view that judgement not as
a vote on one of them specifically, but rather as a
comparison of rules similar to a versus rules similar
to b. When calculating A(r) for any rule r we use
a distance measure over rules, Δ, to estimate how
each judgment in J projects to r. This leads to the
following modified computatio of A(r):
</bodyText>
<equation confidence="0.9938205">
Δ(a, r)v0b&gt;a + Δ(b, r)v0a&gt;b (7)
Δ(a, r) + Δ(b, r)
</equation>
<footnote confidence="0.860307">
9The way we select candidate pairs ensures that U(e) and
U(e&apos;) are both small and expressive in terms of impact on the
decoder ranking. On our data U(e) contained an average of 4
rules.
</footnote>
<bodyText confidence="0.999509">
where v0a&gt;b (and analogously v0b&gt;a) is defined as the
relative frequency of a being preferred over b:
</bodyText>
<subsectionHeader confidence="0.998535">
4.3 A Vector Space Realization
</subsectionHeader>
<bodyText confidence="0.99998684">
Having presented a general framework for judgment
generalization, we will now briefly sketch a concrete
realization of this approach.
In order to be able to use the common distance
metrics on rules, we define a rule vector space. The
basis of this space will be a new set of rule features
designed specifically for the purpose of describing
the structure of a rule, � = (O1, ... , Ok). Provided
the exact features chosen are expressive and well-
distributed over the grammar, we expect any con-
ventional distance metric to correlate with rule sim-
ilarity.
We deem a particular Oi good if it quantifies a
quality of the rule that describes the rule’s nature
rather than the particular lexical choices it makes,
i.e. a statistic (such as the rule length, arity, number
of lexical items in the target or source side or the av-
erage covered span in the training corpus), informa-
tion relevant to the rule’s effect on a derivation (such
as nonterminals occuring in the rule and wheter they
are re-ordered) or features that capture frequent lex-
ical cues that carry syntactic information (such as
the co-occurrence of function words in source and
target language, possibly in conjunction with certain
nonterminal types).
</bodyText>
<sectionHeader confidence="0.999216" genericHeader="evaluation">
5 Results and Analysis
</sectionHeader>
<bodyText confidence="0.999940857142857">
The judgments were collected over a period of
about 12 days (Figure 1). A total of 16,374 labels
were provided (2,729 embedded test labels + 13,645
‘true’ labels) by 658 distinct workers over 83.1 hours
(i.e. each worker completed an average of 4.2 HITs
over 7.6 minutes). The reward for each HIT was
$0.02, with an additional $0.005 incurred for Ama-
zon Fees. Since each HIT provides five labels, we
obtain 200 (true) labels on the dollar. Each HIT
took an average of 1.83 minutes to complete, for a
labeling rate of 164 true labels/hour, and an effec-
tive wage of $0.66/hour. The low reward does not
seem to have deterred Turkers from completing our
HITs faithfully, as the success rate on the embedded
</bodyText>
<equation confidence="0.6504625">
�
A(r) =
(a,b)∈J
va&gt;b
v0 a&gt;b =
va&gt;b + vb&gt;a
</equation>
<page confidence="0.731953">
96
</page>
<figure confidence="0.999712214285714">
% Completed 100 % Time Chosen for Comparison 30
90 25
80 20
70 15
60 10
50 5
40 0
30
20
10
0 1 2 3 4 5 6 7 8 9 10
Candidate Rank
1 2 3 4 5 6 7 8 9 10 11
Time (Days)
</figure>
<figureCaption confidence="0.993346571428571">
Figure 1: Progress of HIT submission over time. There
was a hiatus of about a month during which we collected
no data, which we are omitting for clairty.
Figure 2: Histogram of the rank of the higher-ranked
candidate chosen in pair comparisons. For instance, in
about 29% of chosen pairs, the higher-ranked candidate
was the top candidate (of 300) by decoder score.
</figureCaption>
<table confidence="0.966824466666667">
True Questions
Preferred %
High- 40.0%
Ranked
Low- 24.1%
Ranked
No 35.9%
Difference
Validation Questions
Preferred %
Reference 83.7%
Random 11.7%
Candidate
No 4.65%
Difference
</table>
<figure confidence="0.9958965">
(a) [NP] — ( [NP] [NN+IN] �S I the [NN+IN] [NP] )
(b) [NP] — ( [NP] S [NN] �S I [NN] of [NP] )
</figure>
<figureCaption confidence="0.854415555555555">
Figure 3: A example pair of rules for which judgements
were obtained. The first rule is preferred by the decoder,
while human annotators favor the second rule.
Table 1: Distributions of the collected judgments over
the true questions and over the embedded test questions.
“High-Ranked” (resp. “Low-Ranked”) refers to whether
the decoder assigned a high (low) score to the candidate.
And so, annotators agreed with the decoder 40.0% of the
time, and disagreed 24.1% of the time.
</figureCaption>
<bodyText confidence="0.993717097560975">
questions was quite high (Table 1).10 From our set
of comparatively judged candidate translations we
extracted competing rule pairs. To reduce the in-
fluence of lexical choices and improve comparabil-
ity, we excluded pure preterminal rules and limited
the extraction to rules covering the same span in the
Urdu source. Figure 3 shows an interesting example
of one such rule pair. While the decoder demon-
strates a clear preference for rule (a) (including it
into its higher-ranked translation 100% of the time),
the Turkers tend to prefer translations generated us-
ing rule (b), disagreeing with the SMT system 60%
of the time. This indicates that preferring the second
rule in decoding may yield better results in terms of
human judgment, in this case potentially due to the
10It should be mentioned that the human references them-
selves are of relatively low quality.
cleaner separation of noun phrases from the prepo-
sitional phrase.
We also examine the distribution of the chosen
candidates. Recall that each pair consists of a high-
ranked candidate from the top-ten list, and a low-
ranked candidate from the top-300 list. The His-
togram of the higher rank (Figure 2) shows that the
high-ranked candidate is in fact a top-three candi-
date over 50% of the time. We also see (Figure 4)
that the low-ranked candidate tends to be either close
in rank to the top-ten list, or far away. This again
makes sense given our definition of potential for a
pair: potential is high if the derivations are very
close (left mode) or if the decoder scores differ con-
siderably (right mode).
Finally, we examine inter-annotator agreement,
since we collect multiple judgments per query. We
find that there is full agreement among the anno-
tators in 20.6% of queries. That is, in 20.6% of
queries, all three annotators answering that query
gave the same answer (out of the three provided
answers). This complete agreement rate is signif-
icantly higher than a rate caused by pure chance
(11.5%). This is a positive result, especially given
</bodyText>
<page confidence="0.998262">
97
</page>
<table confidence="0.949592">
% Time Chosen for Comparison 20 lation system remain to be seen.
15 Additionally, introducing A as a new feature
10 makes it necessary to find a viable weight for it.
5 While this can be done trivially in running MERT
0 on arbitrary development data, it may be of interest
</table>
<bodyText confidence="0.856606333333333">
to extend the weight optimization procedure in or-
der to preserve the partial ordering induced by the
judgments as best as possible.
</bodyText>
<figure confidence="0.9816650625">
1-20
21-40
41-60
61-80
81-100
101-120
121-140
141-160
161-180
181-200
201-220
221-240
241-260
261-280
281-300
Candidate Rank
</figure>
<figureCaption confidence="0.9657695">
Figure 4: Histogram of the rank of the lower-ranked can-
didate chosen in pair comparisons. For instance, in about
16% of chosen candidate pairs, the lower-ranked candi-
date was ranked in the top 20.
</figureCaption>
<bodyText confidence="0.999974727272727">
how little diversity usually exists in n-best lists,
a fact (purposely) exacerbated by our strategy of
choosing highly similar pairs of candidates. On the
other hand, we observe complete disagreement in
only 14.9% of queries, which is significantly lower
than a rate caused by pure chance (which is 22.2%).
One thing to note is that these percentages are
calculated after excluding the validation questions,
where the complete agreement rate is an expectedly
even higher 64.9%, and the complete disagreement
rate is an expectedly even lower 3.60%.
</bodyText>
<sectionHeader confidence="0.998589" genericHeader="conclusions">
6 Conclusions and Outlook
</sectionHeader>
<bodyText confidence="0.999949166666667">
We presented a framework that allows us to “tune”
MT systems on a finer level than system-level fea-
ture weights, going instead to the grammar rule level
and augmenting the feature set to reflect collected
human judgments. A system relying on this new fea-
ture during decoding is expected to have a slightly
different ranking of translation candidates that takes
human judgment into account. We presented one
particular judgment collection procedure that relies
on comparing candidate pairs (as opposed to eval-
uating a candidate in isolation) and complemented
it with one possible method of propagating human
judgments to cover grammar rules relevant to new
sentences.
While the presented statistics over the collected
data suggest that the proposed candidate selection
procedure yields consistent and potentially informa-
tive data, the quantitative effects on a machine trans-
</bodyText>
<sectionHeader confidence="0.998279" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998212">
This research was supported by the EuroMatrix-
Plus project funded by the European Commission,
by the DARPA GALE program under Contract No.
HR0011-06-2-0001, and the NSF under grant IIS-
0713448.
</bodyText>
<sectionHeader confidence="0.999431" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999949363636364">
Kathy Baker, Steven Bethard, Michael Bloodgood, Ralf
Brown, Chris Callison-Burch, Glen Coppersmith,
Bonnie Dorr, Wes Filardo, Kendall Giles, Anni Irvine,
Mike Kayser, Lori Levin, Justin Martineau, Jim May-
field, Scott Miller, Aaron Phillips, Andrew Philpot,
Christine Piatko, Lane Schwartz, and David Zajic.
2009. Semantically informed machine translation
(SIMT). In SCALE 2009 Summer Workshop Final Re-
port, pages 135–139.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proc. of ACL, Demonstration Session,
pages 177–180, June.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
N. G. Thornton, Jonathan Weese, and Omar F. Zaidan.
2009. Joshua: An open source toolkit for parsing-
based machine translation. In Proc. of the Fourth
Workshop on Statistical Machine Translation, pages
135–139.
Ashish Venugopal and Andreas Zollmann. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proc. of the NAACL 2006 Workshop on Statistical
Machine Translation, pages 138–141. Association for
Computational Linguistics.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79–88.
</reference>
<page confidence="0.996259">
98
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.883248">
<title confidence="0.97346">An Enriched MT Grammar for Under $100</title>
<author confidence="0.997739">F Zaidan</author>
<affiliation confidence="0.999077">Dept. of Computer Science, Johns Hopkins</affiliation>
<address confidence="0.998288">Baltimore, MD 21218,</address>
<abstract confidence="0.9954209">We propose a framework for improving output quality of machine translation systems, by operating on the level of grammar rule features. Our framework aims to give a boost to grammar rules that appear in the derivations of translation candidates that are deemed to be of good quality, hence making those rules more preferable by the system. To that end, we ask human annotators on Amazon Mechanical Turk to compare translation candidates, and then interpret their preferences of one candidate over another as an implicit preference for one derivation over another, and therefore as an implicit preference for one or more grammar rules. Our framework also allows us to generalize these preferences to grammar rules corresponding to a previously unseen test set, namely rules for which no candidates have beenjudged.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Kathy Baker</author>
<author>Steven Bethard</author>
<author>Michael Bloodgood</author>
<author>Ralf Brown</author>
<author>Chris Callison-Burch</author>
<author>Glen Coppersmith</author>
<author>Bonnie Dorr</author>
<author>Wes Filardo</author>
<author>Kendall Giles</author>
</authors>
<title>Semantically informed machine translation (SIMT).</title>
<date>2009</date>
<booktitle>In SCALE 2009 Summer Workshop Final Report,</booktitle>
<pages>135--139</pages>
<location>Anni Irvine, Mike Kayser, Lori Levin, Justin Martineau, Jim Mayfield, Scott Miller, Aaron Phillips, Andrew Philpot, Christine Piatko, Lane</location>
<contexts>
<context position="7366" citStr="Baker et al., 2009" startWordPosition="1220" endWordPosition="1223">ments. We will propose one such method of computing A in Section 4, after describing the type of judgments we collected. 3 Data Collection We apply our approach to an Urdu-to-English translation task. We used a syntactically rich SAMT grammar (Venugopal and Zollmann, 2006), where each rule in the grammar is characterized by 12 features. The grammar was provided by Chris CallisonBurch (personal communication), and was extracted from a parallel corpus of 88k sentence pairs.3 One system using this grammar produced significantly improved output over submissions to the NIST 2009 Urdu-English task (Baker et al., 2009). We use the Joshua system (Li et al., 2009) as a decoder, with system weights tuned using 2In fact, the collected judgments can only cover a small portion of the grammar. We address this coverage problem in Section 4. 3LDC catalog number LDC2009E12. W 94 Z-MERT (Zaidan, 2009) on a tuning set of 981 sentences, a subset of the 2008 NIST Urdu-English test set.4 We choose candidates to be judged from the 300-best candidate lists.5 Asking a worker to make a quantitative judgment of the quality of a particular candidate translation (e.g. on a 1–7 scale) is a highly subjective and annotator-dependen</context>
</contexts>
<marker>Baker, Bethard, Bloodgood, Brown, Callison-Burch, Coppersmith, Dorr, Filardo, Giles, 2009</marker>
<rawString>Kathy Baker, Steven Bethard, Michael Bloodgood, Ralf Brown, Chris Callison-Burch, Glen Coppersmith, Bonnie Dorr, Wes Filardo, Kendall Giles, Anni Irvine, Mike Kayser, Lori Levin, Justin Martineau, Jim Mayfield, Scott Miller, Aaron Phillips, Andrew Philpot, Christine Piatko, Lane Schwartz, and David Zajic. 2009. Semantically informed machine translation (SIMT). In SCALE 2009 Summer Workshop Final Report, pages 135–139.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar,</location>
<marker>Koehn, Hoang, Mayne, Callison-Burch, Federico, Bertoldi, Cowan, Shen, </marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL, Demonstration Session,</booktitle>
<pages>177--180</pages>
<marker>Constantin, Herbst, 2007</marker>
<rawString>Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of ACL, Demonstration Session, pages 177–180, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Chris Callison-Burch</author>
<author>Chris Dyer</author>
<author>Juri Ganitkevitch</author>
<author>Sanjeev Khudanpur</author>
<author>Lane Schwartz</author>
<author>Wren N G Thornton</author>
<author>Jonathan Weese</author>
<author>Omar F Zaidan</author>
</authors>
<title>Joshua: An open source toolkit for parsingbased machine translation.</title>
<date>2009</date>
<booktitle>In Proc. of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>135--139</pages>
<contexts>
<context position="1149" citStr="Li et al., 2009" startWordPosition="177" endWordPosition="180">t end, we ask human annotators on Amazon Mechanical Turk to compare translation candidates, and then interpret their preferences of one candidate over another as an implicit preference for one derivation over another, and therefore as an implicit preference for one or more grammar rules. Our framework also allows us to generalize these preferences to grammar rules corresponding to a previously unseen test set, namely rules for which no candidates have beenjudged. 1 Introduction When translating between two languages, stateof-the-art statistical machine translation systems (Koehn et al., 2007; Li et al., 2009) generate candidate translations by relying on a set of relevant grammar (or phrase table) entries. Each of those entries, or rules, associates a string in the source language with a string in the target language, with these associations typically learned by examining a large parallel bitext. By the very nature of the translation process, a target side sentence e can be a candidate translation for a source sentence f only if e can be constructed using a small subset of the grammar, namely the subset of rules with source side sequences relevant to the word sequence of f. However, even this limi</context>
<context position="7410" citStr="Li et al., 2009" startWordPosition="1229" endWordPosition="1232">ting A in Section 4, after describing the type of judgments we collected. 3 Data Collection We apply our approach to an Urdu-to-English translation task. We used a syntactically rich SAMT grammar (Venugopal and Zollmann, 2006), where each rule in the grammar is characterized by 12 features. The grammar was provided by Chris CallisonBurch (personal communication), and was extracted from a parallel corpus of 88k sentence pairs.3 One system using this grammar produced significantly improved output over submissions to the NIST 2009 Urdu-English task (Baker et al., 2009). We use the Joshua system (Li et al., 2009) as a decoder, with system weights tuned using 2In fact, the collected judgments can only cover a small portion of the grammar. We address this coverage problem in Section 4. 3LDC catalog number LDC2009E12. W 94 Z-MERT (Zaidan, 2009) on a tuning set of 981 sentences, a subset of the 2008 NIST Urdu-English test set.4 We choose candidates to be judged from the 300-best candidate lists.5 Asking a worker to make a quantitative judgment of the quality of a particular candidate translation (e.g. on a 1–7 scale) is a highly subjective and annotator-dependent process. Instead, we present workers with </context>
</contexts>
<marker>Li, Callison-Burch, Dyer, Ganitkevitch, Khudanpur, Schwartz, Thornton, Weese, Zaidan, 2009</marker>
<rawString>Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren N. G. Thornton, Jonathan Weese, and Omar F. Zaidan. 2009. Joshua: An open source toolkit for parsingbased machine translation. In Proc. of the Fourth Workshop on Statistical Machine Translation, pages 135–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollmann</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proc. of the NAACL 2006 Workshop on Statistical Machine Translation,</booktitle>
<pages>138--141</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7020" citStr="Venugopal and Zollmann, 2006" startWordPosition="1166" endWordPosition="1169">ndidates, and what form those judgments assume (i.e. are those judgments scores on a scale? Are they “better” vs. “worse” judgments, and if so, compared to how many other possibilities?). At this point, we will only emphasize that the value of A should reflect the annotators’ preference for the rule, and that it should be computed from the collected judgments. We will propose one such method of computing A in Section 4, after describing the type of judgments we collected. 3 Data Collection We apply our approach to an Urdu-to-English translation task. We used a syntactically rich SAMT grammar (Venugopal and Zollmann, 2006), where each rule in the grammar is characterized by 12 features. The grammar was provided by Chris CallisonBurch (personal communication), and was extracted from a parallel corpus of 88k sentence pairs.3 One system using this grammar produced significantly improved output over submissions to the NIST 2009 Urdu-English task (Baker et al., 2009). We use the Joshua system (Li et al., 2009) as a decoder, with system weights tuned using 2In fact, the collected judgments can only cover a small portion of the grammar. We address this coverage problem in Section 4. 3LDC catalog number LDC2009E12. W 9</context>
</contexts>
<marker>Venugopal, Zollmann, 2006</marker>
<rawString>Ashish Venugopal and Andreas Zollmann. 2006. Syntax augmented machine translation via chart parsing. In Proc. of the NAACL 2006 Workshop on Statistical Machine Translation, pages 138–141. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
</authors>
<title>Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems.</title>
<date>2009</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>91--79</pages>
<contexts>
<context position="7643" citStr="Zaidan, 2009" startWordPosition="1272" endWordPosition="1273">ch rule in the grammar is characterized by 12 features. The grammar was provided by Chris CallisonBurch (personal communication), and was extracted from a parallel corpus of 88k sentence pairs.3 One system using this grammar produced significantly improved output over submissions to the NIST 2009 Urdu-English task (Baker et al., 2009). We use the Joshua system (Li et al., 2009) as a decoder, with system weights tuned using 2In fact, the collected judgments can only cover a small portion of the grammar. We address this coverage problem in Section 4. 3LDC catalog number LDC2009E12. W 94 Z-MERT (Zaidan, 2009) on a tuning set of 981 sentences, a subset of the 2008 NIST Urdu-English test set.4 We choose candidates to be judged from the 300-best candidate lists.5 Asking a worker to make a quantitative judgment of the quality of a particular candidate translation (e.g. on a 1–7 scale) is a highly subjective and annotator-dependent process. Instead, we present workers with pairs of candidates, and ask them to judge which candidate is of better quality. How are candidate pairs chosen? We would like a judgment to have the maximum potential for being informative about specific grammar rules. In essense, w</context>
</contexts>
<marker>Zaidan, 2009</marker>
<rawString>Omar F. Zaidan. 2009. Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems. The Prague Bulletin of Mathematical Linguistics, 91:79–88.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>