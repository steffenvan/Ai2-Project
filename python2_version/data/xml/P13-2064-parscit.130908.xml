<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.059124">
<title confidence="0.995354">
A Novel Graph-based Compact Representation of Word Alignment
</title>
<author confidence="0.995469">
Qun Liu†‡ Zhaopeng Tu‡ Shouxun Lin‡
</author>
<affiliation confidence="0.989009">
†Centre for Next Generation Locolisation ‡Key Lab. of Intelligent Info. Processing
Dublin City University Institute of Computing Technology, CAS
</affiliation>
<email confidence="0.99664">
qliu@computing.dcu.ie {tuzhaopeng,sxlin}@ict.ac.cn
</email>
<sectionHeader confidence="0.993823" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999725923076923">
In this paper, we propose a novel compact
representation called weighted bipartite
hypergraph to exploit the fertility model,
which plays a critical role in word align-
ment. However, estimating the probabili-
ties of rules extracted from hypergraphs is
an NP-complete problem, which is com-
putationally infeasible. Therefore, we pro-
pose a divide-and-conquer strategy by de-
composing a hypergraph into a set of inde-
pendent subhypergraphs. The experiments
show that our approach outperforms both
1-best and n-best alignments.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999941833333333">
Word alignment is the task of identifying trans-
lational relations between words in parallel cor-
pora, in which a word at one language is usually
translated into several words at the other language
(fertility model) (Brown et al., 1993). Given that
many-to-many links are common in natural lan-
guages (Moore, 2005), it is necessary to pay atten-
tion to the relations among alignment links.
In this paper, we have proposed a novel graph-
based compact representation of word alignment,
which takes into account the joint distribution of
alignment links. We first transform each align-
ment to a bigraph that can be decomposed into a
set of subgraphs, where all interrelated links are
in the same subgraph (§ 2.1). Then we employ
a weighted partite hypergraph to encode multiple
bigraphs (§ 2.2).
The main challenge of this research is to effi-
ciently calculate the fractional counts for rules ex-
tracted from hypergraphs. This is equivalent to the
decision version of set covering problem, which is
NP-complete. Observing that most alignments are
not connected, we propose a divide-and-conquer
strategy by decomposing a hypergraph into a set
</bodyText>
<figureCaption confidence="0.9727545">
Figure 1: A bigraph constructed from an align-
ment (a), and its disjoint MCSs (b).
</figureCaption>
<bodyText confidence="0.9990936">
of independent subhypergraphs, which is compu-
tationally feasible in practice (§ 3.2). Experimen-
tal results show that our approach significantly im-
proves translation performance by up to 1.3 BLEU
points over 1-best alignments (§ 4.3).
</bodyText>
<sectionHeader confidence="0.984233" genericHeader="method">
2 Graph-based Compact Representation
</sectionHeader>
<subsectionHeader confidence="0.950565">
2.1 Word Alignment as a Bigraph
</subsectionHeader>
<bodyText confidence="0.9999033">
Each alignment of a sentence pair can be trans-
formed to a bigraph, in which the two disjoint ver-
tex sets S and T are the source and target words re-
spectively, and the edges are word-by-word links.
For example, Figure 1(a) shows the corresponding
bigraph of an alignment.
The bigraph usually is not connected. A graph
is called connected if there is a path between every
pair of distinct vertices. In an alignment, words in
a specific portion at the source side (i.e. a verb
phrase) usually align to those in the corresponding
portion (i.e. the verb phrase at the target side), and
would never align to other words; and vice versa.
Therefore, there is no edge that connects the words
in the portion to those outside the portion.
Therefore, a bigraph can be decomposed into
a unique set of minimum connected subgraphs
(MCSs), where each subgraph is connected and
does not contain any other MCSs. For example,
the bigraph in Figure 1(a) can be decomposed into
</bodyText>
<page confidence="0.9749">
358
</page>
<note confidence="0.461576">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 358–363,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figure confidence="0.999537857142857">
Җ
൘
Ṽᆀ
к
the
book
is
on
the
desk
Җ
൘
Ṽᆀ
к
the
book
is
on
the
desk
Җ
൘
Ṽᆀ
к
e2 e3
e4
e1
es
the
book
is
on
the
desk
(a) (b) (c)
</figure>
<figureCaption confidence="0.916596">
Figure 2: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair; (c) the
resulting hypergraph that takes the two alignments as samples.
</figureCaption>
<bodyText confidence="0.999457166666667">
the MCSs in Figure 1(b). We can see that all in-
terrelated links are in the same MCS. These MCSs
work as fundamental units in our approach to take
advantage of the relations among the links. Here-
inafter, we use bigraph to denote the alignment of
a sentence pair.
</bodyText>
<subsectionHeader confidence="0.996169">
2.2 Weighted Bipartite Hypergraph
</subsectionHeader>
<bodyText confidence="0.999922538461538">
We believe that offering more alternatives to ex-
tracting translation rules could help improve trans-
lation quality. We propose a new structure called
weighted bipartite hypergraph that compactly en-
codes multiple alignments.
We use an example to illustrate our idea. Fig-
ures 2(a) and 2(b) show two bigraphs of the same
sentence pair. Intuitively, we can encode the
union set of subgraphs in a bipartite hypergraph,
in which each MCS serves as a hyperedge, as in
Figure 2(c). Accordingly, we can calculate how
well a hyperedge is by calculating its relative fre-
quency, which is the probability sum of bigraphs
in which the corresponding MCS occurs divided
by the probability sum of all possible bigraphs.
Suppose that the probabilities of the two bigraphs
in Figures 2(a) and 2(b) are 0.7 and 0.3, respec-
tively. Then the weight of e1 is 1.0 and e2 is
0.7. Therefore, each hyperedge is associated with
a weight to indicate how well it is.
Formally, a weighted bipartite hypergraph H is
a triple (5, T, E) where 5 and T are two sets of
vertices on the source and target sides, and E are
hyperedges associated with weights. Currently,
we estimate the weights of hyperedges from an n-
best list by calculating relative frequencies:
</bodyText>
<equation confidence="0.988903666666667">
�BGEN p(BG) x S(BG,gz)
w(ez) =
EBGEN p(BG)
</equation>
<bodyText confidence="0.9958164">
Here N is an n-best bigraph (i.e., alignment) list,
p(BG) is the probability of a bigraph BG in the n-
best list, gz is the MCS that corresponds to ez, and
S(BG, gz) is an indicator function which equals 1
when gz occurs in BG, and 0 otherwise.
It is worthy mentioning that a hypergraph en-
codes much more alignments than the input n-best
list. For example, we can construct a new align-
ment by using hyperedges from different bigraphs
that cover all vertices.
</bodyText>
<sectionHeader confidence="0.983069" genericHeader="method">
3 Graph-based Rule Extraction
</sectionHeader>
<bodyText confidence="0.999885">
In this section we describe how to extract transla-
tion rules from a hypergraph (§ 3.1) and how to
estimate their probabilities (§ 3.2).
</bodyText>
<subsectionHeader confidence="0.998647">
3.1 Extraction Algorithm
</subsectionHeader>
<bodyText confidence="0.9999968">
We extract translation rules from a hypergraph
for the hierarchical phrase-based system (Chiang,
2007). Chiang (2007) describes a rule extrac-
tion algorithm that involves two steps: (1) extract
phrases from 1-best alignments; (2) obtain vari-
able rules by replacing sub-phrase pairs with non-
terminals. Our extraction algorithm differs at the
first step, in which we extract phrases from hyper-
graphs instead of 1-best alignments. Rather than
restricting ourselves by the alignment consistency
in the traditional algorithm, we extract all possible
candidate target phrases for each source phrase.
To maintain a reasonable rule table size, we fil-
ter out less promising candidates that have a frac-
tional count lower than a threshold.
</bodyText>
<subsectionHeader confidence="0.999938">
3.2 Calculating Fractional Counts
</subsectionHeader>
<bodyText confidence="0.9962088">
The fractional count of a phrase pair is the proba-
bility sum of the alignments with which the phrase
pair is consistent (§3.2.2), divided by the probabil-
ity sum of all alignments encoded in a hypergraph
(§3.2.1) (Liu et al., 2009).
</bodyText>
<page confidence="0.994916">
359
</page>
<bodyText confidence="0.983369">
Intuitively, our approach faces two challenges:
</bodyText>
<listItem confidence="0.9970218">
1. How to calculate the probability sum of all
alignments encoded in a hypergraph (§3.2.1)?
2. How to efficiently calculate the probability
sum of all consistent alignments for each
phrase pair (§3.2.2)?
</listItem>
<subsectionHeader confidence="0.877489">
3.2.1 Enumerating All Alignments
</subsectionHeader>
<bodyText confidence="0.999655111111111">
In theory, a hypergraph can encode all possible
alignments if there are enough hyperedges. How-
ever, since a hypergraph is constructed from an n-
best list, it can only represent partial space of all
alignments (p(A|H) &lt; 1) because of the limiting
size of hyperedges learned from the list. There-
fore, we need to enumerate all possible align-
ments in a hypergraph to obtain the probability
sum p(A|H).
Specifically, generating an alignment from a hy-
pergraph can be modelled as finding a complete
hyperedge matching, which is a set of hyperedges
without common vertices that matches all vertices.
The probability of the alignment is the product of
hyperedge weights. Thus, enumerating all possi-
ble alignments in a hypergraph is reformulated as
finding all complete hypergraph matchings, which
is an NP-complete problem (Valiant, 1979).
Similar to the bigraph, a hypergraph is also usu-
ally not connected. To make the enumeration prac-
tically tractable, we propose a divide-and-conquer
strategy by decomposing a hypergraph H into a set
of independent subhypergraphs {h1, h2, ... , hn}.
Intuitively, the probability of an alignment is the
product of hyperedge weights. According to the
divide-and-conquer strategy, the probability sum
of all alignments A encoded in a hypergraph H is:
</bodyText>
<equation confidence="0.9952535">
p(A|H) = 11 p(Ai|hi)
hiEH
</equation>
<bodyText confidence="0.9681085">
Here p(Ai|hi) is the probability sum of all sub-
alignments Ai encoded in the subhypergraph hi.
</bodyText>
<subsectionHeader confidence="0.82126">
3.2.2 Enumerating Consistent Alignments
</subsectionHeader>
<bodyText confidence="0.9993835">
Since a hypergraph encodes many alignments, it is
unrealistic to enumerate all consistent alignments
explicitly for each phrase pair.
Recall that a hypergraph can be decomposed
to a list of independent subhypergraphs, and an
alignment is a combination of the sub-alignments
from the decompositions. We observe that a
phrase pair is absolutely consistent with the sub-
alignments from some subhypergraphs, while pos-
sibly consistent with the others. As an example,
</bodyText>
<figureCaption confidence="0.932321">
Figure 3: A hypergraph with a candidate phrase
in the grey shadow (a), and its independent subhy-
pergraphs {h1, h2, h3}.
</figureCaption>
<bodyText confidence="0.9998565">
consider the phrase pair in the grey shadow in Fig-
ure 3(a), it is consistent with all sub-alignments
from both h1 and h2 because they are outside and
inside the phrase pair respectively, while not con-
sistent with the sub-alignment that contains hyper-
edge e2 from h3 because it contains an alignment
link that crosses the phrase pair.
Therefore, to calculate the probability sum of all
consistent alignments, we only need to consider
the overlap subhypergraphs, which have at least
one hyperedge that crosses the phrase pair. Given
a overlap subhypergraph, the probability sum of
consistent sub-alignments is calculated by sub-
tracting the probability sum of the sub-alignments
that contain crossed hyperedges, from the proba-
bility sum of all sub-alignments encoded in a hy-
pergraph.
Given a phrase pair P, let OS and NS de-
notes the sets of overlap and non-overlap subhy-
pergraphs respectively (NS = H − OS). Then
</bodyText>
<equation confidence="0.9841775">
p(A|H, P) = 11 p(Ai|hi, P) 11 p(Aj|hj)
hiEOS hjENS
</equation>
<bodyText confidence="0.9965945">
Here the phrase pair is absolutely consistent with
the sub-alignments from non-overlap subhyper-
graphs (IS), and we have p(A|h, P) = p(A|h).
Then the fractional count of a phrase pair is:
</bodyText>
<equation confidence="0.986973333333333">
c(P|H) = p(A|H, P)
p(A|H) HhiEOS p(A|hi, P)
HhiEOS p(A|hi)
</equation>
<bodyText confidence="0.9996036">
After we get the fractional counts of transla-
tion rules, we can estimate their relative frequen-
cies (Och and Ney, 2004). We follow (Liu et al.,
2009; Tu et al., 2011) to learn lexical tables from
n-best lists and then calculate the lexical weights.
</bodyText>
<figure confidence="0.99841915625">
(a) (b)
Җ
൘
Ṽᆀ
к
e2 e3
e4
e1
e5
the
book
is
on
the
desk
Ṽᆀ
Җ
൘
e2 e3
e4
e1
e5
is
the
book
the
desk
on
к
h1
h2
h3
</figure>
<page confidence="0.988091">
360
</page>
<table confidence="0.99945425">
Rules from... Rules MT03 MT04 MT05 Avg.
1-best 257M 33.45 35.25 33.63 34.11
10-best 427M 34.10 35.71 34.04 34.62
Hypergraph 426M 34.71 36.24 34.41 35.12
</table>
<tableCaption confidence="0.999918">
Table 1: Evaluation of translation quality.
</tableCaption>
<sectionHeader confidence="0.997508" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.964547">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.99999496">
We carry out our experiments on Chinese-English
translation tasks using a reimplementation of the
hierarchical phrase-based system (Chiang, 2007).
Our training data contains 1.5 million sentence
pairs from LDC dataset.1 We train a 4-gram
language model on the Xinhua portion of the
GIGAWORD corpus using the SRI Language
Toolkit (Stolcke, 2002) with modified Kneser-Ney
Smoothing (Kneser and Ney, 1995). We use min-
imum error rate training (Och, 2003) to optimize
the feature weights on the MT02 testset, and test
on the MT03/04/05 testsets. For evaluation, case-
insensitive NIST BLEU (Papineni et al., 2002) is
used to measure translation performance.
We first follow Venugopal et al. (2008) to pro-
duce n-best lists via GIZA++. We produce 10-best
lists in two translation directions, and use “grow-
diag-final-and” strategy (Koehn et al., 2003) to
generate the final n-best lists by selecting the
top n alignments. We re-estimated the probabil-
ity of each alignment in the n-best list using re-
normalization (Venugopal et al., 2008). Finally we
construct weighted alignment hypergraphs from
these n-best lists.2 When extracting rules from hy-
pergraphs, we set the pruning threshold t = 0.5.
</bodyText>
<subsectionHeader confidence="0.9889475">
4.2 Tractability of Divide-and-Conquer
Strategy
</subsectionHeader>
<bodyText confidence="0.9994066">
Figure 4 shows the distribution of vertices (hy-
peredges) number of the subhypergraphs. We can
see that most of the subhypergraphs have just less
than two vertices and hyperedges.3 Specifically,
each subhypergraph has 2.0 vertices and 1.4 hy-
</bodyText>
<footnote confidence="0.9872693">
1The corpus includes LDC2002E18, LDC2003E07,
LDC2003E14, Hansards portion of LDC2004T07,
LDC2004T08 and LDC2005T06.
2Here we only use 10-best lists, because the alignments
beyond top 10 have very small probabilities, thus have negli-
gible influence on the hypergraphs.
3It’s interesting that there are few subhypergraphs that
have exactly 2 hyperedges. In this case, the only two hy-
peredges fully cover the vertices and they differ at the word-
by-word links, which is uncommon in n-best lists.
</footnote>
<figureCaption confidence="0.942627">
Figure 4: The distribution of vertices (hyperedges)
number of the subhypergraphs.
</figureCaption>
<bodyText confidence="0.997026">
peredges on average. This suggests that the divide-
and-conquer strategy makes the extraction compu-
tationally tractable, because it greatly reduces the
number of vertices and hyperedges. For computa-
tional tractability, we only allow a subhypergraph
has at most 5 hyperedges. 4
</bodyText>
<subsectionHeader confidence="0.998244">
4.3 Translation Performance
</subsectionHeader>
<bodyText confidence="0.999911611111111">
Table 1 shows the rule table size and transla-
tion quality. Using n-best lists slightly improves
the BLEU score over 1-best alignments, but at
the cost of a larger rule table. This is in ac-
cord with intuition, because all possible transla-
tion rules would be extracted from different align-
ments in n-best lists without pruning. This larger
rule table indeed leads to a high rule coverage, but
in the meanwhile, introduces translation errors be-
cause of the low-quality rules (i.e., rules extracted
only from low-quality alignments in n-best lists).
By contrast, our approach not only significantly
improves the translation performance over 1-best
alignments, but also outperforms n-best lists with
a similar-scale rule table. The absolute improve-
ments of 1.0 BLEU points on average over 1-best
alignments are statistically significant at p &lt; 0.01
using sign-test (Collins et al., 2005).
</bodyText>
<footnote confidence="0.950177333333333">
4If a subhypergraph has more than 5 hyperedges, we
forcibly partition it into small subhypergraphs by iteratively
removing lowest-probability hyperedges.
</footnote>
<figure confidence="0.99812">
1 2 3 4 5 6 7 8 9 10
number of vertices (hyperedges)
percentage
0.8
0.6
0.4
0.2
0
1
vertices
hyperedges
</figure>
<page confidence="0.992415">
361
</page>
<table confidence="0.99164475">
Rules from... Shared Non-shared All
Rules BLEU Rules BLEU Rules BLEU
10-best 1.83M 32.75 2.81M 30.71 4.64M 34.62
Hypergraph 1.83M 33.24 2.89M 31.12 4.72M 35.12
</table>
<tableCaption confidence="0.996135">
Table 2: Comparison of rule tables learned from n-best lists and hypergraphs. “All” denotes the full rule
</tableCaption>
<bodyText confidence="0.998774692307692">
table, “Shared” denotes the intersection of two tables, and “Non-shared” denotes the complement. Note
that the probabilities of “Shared” rules are different for the two approaches.
Why our approach outperforms n-best lists? In
theory, the rule table extracted from n-best lists
is a subset of that from hypergraphs. In prac-
tice, however, this is not true because we pruned
the rules that have fractional counts lower than a
threshold. Therefore, the question arises as to how
many rules are shared by n-best and hypergraph-
based extractions. We try to answer this ques-
tion by comparing the different rule tables (filtered
on the test sets) learned from n-best lists and hy-
pergraphs. Table 2 gives some statistics. “All”
denotes the full rule table, “Shared” denotes the
intersection of two tables, and “Non-shared” de-
notes the complement. Note that the probabil-
ities of “Shared” rules are different for the two
approaches. We can see that both the “Shared”
and “Non-shared” rules learned from hypergraphs
outperform n-best lists, indicating: (1) our ap-
proach has a better estimation of rule probabili-
ties because we estimate the probabilities from a
much larger alignment space that can not be rep-
resented by n-best lists, (2) our approach can ex-
tract good rules that cannot be extracted from any
single alignments in the n-best lists.
</bodyText>
<sectionHeader confidence="0.999855" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.9999796875">
Our research builds on previous work in the field
of graph models and compact representations.
Graph models have been used before in word
alignment: the search space of word alignment can
be structured as a graph and the search problem
can be reformulated as finding the optimal path
though this graph (e.g., (Och and Ney, 2004; Liu et
al., 2010)). In addition, Kumar and Byrne (2002)
define a graph distance as a loss function for
minimum Bayes-risk word alignment, Riesa and
Marcu (2010) open up the word alignment task to
advances in hypergraph algorithms currently used
in parsing. As opposed to the search problem, we
propose a graph-based compact representation that
encodes multiple alignments for machine transla-
tion.
Previous research has demonstrated that com-
pact representations can produce improved re-
sults by offering more alternatives, e.g., using
forests over 1-best trees (Mi and Huang, 2008;
Tu et al., 2010; Tu et al., 2012a), word lattices
over 1-best segmentations (Dyer et al., 2008),
and weighted alignment matrices over 1-best word
alignments (Liu et al., 2009; Tu et al., 2011; Tu et
al., 2012b). Liu et al., (2009) estimate the link
probabilities from n-best lists, while Gispert et
al., (2010) learn the alignment posterior probabil-
ities directly from IBM models. However, both of
them ignore the relations among alignment links.
By contrast, our approach takes into account the
joint distribution of alignment links and explores
the fertility model past the link level.
</bodyText>
<sectionHeader confidence="0.999234" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999936272727273">
We have presented a novel compact representa-
tion of word alignment, named weighted bipar-
tite hypergraph, to exploit the relations among
alignment links. Since estimating the probabil-
ities of rules extracted from hypergraphs is an
NP-complete problem, we propose a computation-
ally tractable divide-and-conquer strategy by de-
composing a hypergraph into a set of independent
subhypergraphs. Experimental results show that
our approach outperforms both 1-best and n-best
alignments.
</bodyText>
<sectionHeader confidence="0.953173" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9950394">
The authors are supported by 863 State Key
Project No. 2011AA01A207, National Key Tech-
nology R&amp;D Program No. 2012BAH39B03 and
National Natural Science Foundation of China
(Contracts 61202216). Qun Liu’s work is partially
supported by Science Foundation Ireland (Grant
No.07/CE/I1142) as part of the CNGL at Dublin
City University. We thank Junhui Li, Yifan He
and the anonymous reviewers for their insightful
comments.
</bodyText>
<page confidence="0.996095">
362
</page>
<sectionHeader confidence="0.975386" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996845618556701">
Peter E. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational linguistics,
19(2):263–311.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
M. Collins, P. Koehn, and I. Kuˇcerov´a. 2005. Clause
restructuring for statistical machine translation. In
Proceedings of the 43rd Annual Meeting on Associa-
tion for Computational Linguistics, pages 531–540.
Adri`a de Gispert, Juan Pino, and William Byrne. 2010.
Hierarchical phrase-based translation grammars ex-
tracted from alignment posterior probabilities. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
545–554.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice translation.
In Proceedings ofACL-08: HLT, pages 1012–1020.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing, volume 1, pages
181–184.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 48–54.
Shankar Kumar and William Byrne. 2002. Mini-
mum Bayes-risk word alignments of bilingual texts.
In Proceedings of the 2002 Conference on Empiri-
cal Methods in Natural Language Processing, pages
140–147.
Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1017–1026.
Yang Liu, Qun Liu, and Shouxun Lin. 2010. Discrim-
inative word alignment by linear modeling. Compu-
tational Linguistics, 36(3):303–339.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 206–214.
Robert C. Moore. 2005. A discriminative framework
for bilingual word alignment. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 81–88, October.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417–449.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318.
Jason Riesa and Daniel Marcu. 2010. Hierarchical
search for word alignment. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 157–166.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proceedings of Seventh Inter-
national Conference on Spoken Language Process-
ing, volume 3, pages 901–904. Citeseer.
Zhaopeng Tu, Yang Liu, Young-Sook Hwang, Qun
Liu, and Shouxun Lin. 2010. Dependency forest
for statistical machine translation. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, pages 1092–1100.
Zhaopeng Tu, Yang Liu, Qun Liu, and Shouxun Lin.
2011. Extracting hierarchical rules from a weighted
alignment matrix. In Proceedings of 5th Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 1294–1303.
Zhaopeng Tu, Wenbin Jiang, Qun Liu, and Shouxun
Lin. 2012a. Dependency forest for sentiment anal-
ysis. In Springer-Verlag Berlin Heidelberg, pages
69–77.
Zhaopeng Tu, Yang Liu, Yifan He, Josef van Genabith,
Qun Liu, and Shouxun Lin. 2012b. Combining mul-
tiple alignments to improve machine translation. In
Proceedings ofthe 24th International Conference on
Computational Linguistics, pages 1249–1260.
Leslie G Valiant. 1979. The complexity of comput-
ing the permanent. Theoretical Computer Science,
8(2):189–201.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2008. Wider pipelines: n-best
alignments and parses in mt training. In Proceed-
ings ofAMTA, pages 192–201.
</reference>
<page confidence="0.999364">
363
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.640979">
<title confidence="0.9730355">A Novel Graph-based Compact Representation of Word Alignment for Next Generation Locolisation Lab. of Intelligent Info. Processing</title>
<author confidence="0.659604">Dublin City University Institute of Computing Technology</author>
<author confidence="0.659604">CAS</author>
<abstract confidence="0.998788714285714">In this paper, we propose a novel compact called bipartite exploit the fertility model, which plays a critical role in word alignment. However, estimating the probabilities of rules extracted from hypergraphs is an NP-complete problem, which is computationally infeasible. Therefore, we propose a divide-and-conquer strategy by decomposing a hypergraph into a set of independent subhypergraphs. The experiments show that our approach outperforms both and alignments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter E Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1070" citStr="Brown et al., 1993" startWordPosition="150" endWordPosition="153">role in word alignment. However, estimating the probabilities of rules extracted from hypergraphs is an NP-complete problem, which is computationally infeasible. Therefore, we propose a divide-and-conquer strategy by decomposing a hypergraph into a set of independent subhypergraphs. The experiments show that our approach outperforms both 1-best and n-best alignments. 1 Introduction Word alignment is the task of identifying translational relations between words in parallel corpora, in which a word at one language is usually translated into several words at the other language (fertility model) (Brown et al., 1993). Given that many-to-many links are common in natural languages (Moore, 2005), it is necessary to pay attention to the relations among alignment links. In this paper, we have proposed a novel graphbased compact representation of word alignment, which takes into account the joint distribution of alignment links. We first transform each alignment to a bigraph that can be decomposed into a set of subgraphs, where all interrelated links are in the same subgraph (§ 2.1). Then we employ a weighted partite hypergraph to encode multiple bigraphs (§ 2.2). The main challenge of this research is to effic</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter E. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="6106" citStr="Chiang, 2007" startWordPosition="1015" endWordPosition="1016">ponds to ez, and S(BG, gz) is an indicator function which equals 1 when gz occurs in BG, and 0 otherwise. It is worthy mentioning that a hypergraph encodes much more alignments than the input n-best list. For example, we can construct a new alignment by using hyperedges from different bigraphs that cover all vertices. 3 Graph-based Rule Extraction In this section we describe how to extract translation rules from a hypergraph (§ 3.1) and how to estimate their probabilities (§ 3.2). 3.1 Extraction Algorithm We extract translation rules from a hypergraph for the hierarchical phrase-based system (Chiang, 2007). Chiang (2007) describes a rule extraction algorithm that involves two steps: (1) extract phrases from 1-best alignments; (2) obtain variable rules by replacing sub-phrase pairs with nonterminals. Our extraction algorithm differs at the first step, in which we extract phrases from hypergraphs instead of 1-best alignments. Rather than restricting ourselves by the alignment consistency in the traditional algorithm, we extract all possible candidate target phrases for each source phrase. To maintain a reasonable rule table size, we filter out less promising candidates that have a fractional coun</context>
<context position="11242" citStr="Chiang, 2007" startWordPosition="1855" endWordPosition="1856">. We follow (Liu et al., 2009; Tu et al., 2011) to learn lexical tables from n-best lists and then calculate the lexical weights. (a) (b) Җ  Ṽᆀ к e2 e3 e4 e1 e5 the book is on the desk Ṽᆀ Җ  e2 e3 e4 e1 e5 is the book the desk on к h1 h2 h3 360 Rules from... Rules MT03 MT04 MT05 Avg. 1-best 257M 33.45 35.25 33.63 34.11 10-best 427M 34.10 35.71 34.04 34.62 Hypergraph 426M 34.71 36.24 34.41 35.12 Table 1: Evaluation of translation quality. 4 Experiments 4.1 Setup We carry out our experiments on Chinese-English translation tasks using a reimplementation of the hierarchical phrase-based system (Chiang, 2007). Our training data contains 1.5 million sentence pairs from LDC dataset.1 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, caseinsensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We first follow Venugopal et al. (2008) to produce n-best lists via GIZA++. We produce 10-best</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>P Koehn</author>
<author>I Kuˇcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>531--540</pages>
<marker>Collins, Koehn, Kuˇcerov´a, 2005</marker>
<rawString>M. Collins, P. Koehn, and I. Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 531–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adri`a de Gispert</author>
<author>Juan Pino</author>
<author>William Byrne</author>
</authors>
<title>Hierarchical phrase-based translation grammars extracted from alignment posterior probabilities.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>545--554</pages>
<marker>de Gispert, Pino, Byrne, 2010</marker>
<rawString>Adri`a de Gispert, Juan Pino, and William Byrne. 2010. Hierarchical phrase-based translation grammars extracted from alignment posterior probabilities. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 545–554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Dyer</author>
<author>Smaranda Muresan</author>
<author>Philip Resnik</author>
</authors>
<title>Generalizing word lattice translation.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>1012--1020</pages>
<contexts>
<context position="17213" citStr="Dyer et al., 2008" startWordPosition="2801" endWordPosition="2804">efine a graph distance as a loss function for minimum Bayes-risk word alignment, Riesa and Marcu (2010) open up the word alignment task to advances in hypergraph algorithms currently used in parsing. As opposed to the search problem, we propose a graph-based compact representation that encodes multiple alignments for machine translation. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g., using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010; Tu et al., 2012a), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices over 1-best word alignments (Liu et al., 2009; Tu et al., 2011; Tu et al., 2012b). Liu et al., (2009) estimate the link probabilities from n-best lists, while Gispert et al., (2010) learn the alignment posterior probabilities directly from IBM models. However, both of them ignore the relations among alignment links. By contrast, our approach takes into account the joint distribution of alignment links and explores the fertility model past the link level. 6 Conclusion We have presented a novel compact representation of word alignment, named weighted bipartite h</context>
</contexts>
<marker>Dyer, Muresan, Resnik, 2008</marker>
<rawString>Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing word lattice translation. In Proceedings ofACL-08: HLT, pages 1012–1020.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<volume>1</volume>
<pages>181--184</pages>
<contexts>
<context position="11499" citStr="Kneser and Ney, 1995" startWordPosition="1893" endWordPosition="1896">les from... Rules MT03 MT04 MT05 Avg. 1-best 257M 33.45 35.25 33.63 34.11 10-best 427M 34.10 35.71 34.04 34.62 Hypergraph 426M 34.71 36.24 34.41 35.12 Table 1: Evaluation of translation quality. 4 Experiments 4.1 Setup We carry out our experiments on Chinese-English translation tasks using a reimplementation of the hierarchical phrase-based system (Chiang, 2007). Our training data contains 1.5 million sentence pairs from LDC dataset.1 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, caseinsensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We first follow Venugopal et al. (2008) to produce n-best lists via GIZA++. We produce 10-best lists in two translation directions, and use “growdiag-final-and” strategy (Koehn et al., 2003) to generate the final n-best lists by selecting the top n alignments. We re-estimated the probability of each alignment in the n-best list using renormalization</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, volume 1, pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Joseph Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="11938" citStr="Koehn et al., 2003" startWordPosition="1964" endWordPosition="1967"> train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, caseinsensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We first follow Venugopal et al. (2008) to produce n-best lists via GIZA++. We produce 10-best lists in two translation directions, and use “growdiag-final-and” strategy (Koehn et al., 2003) to generate the final n-best lists by selecting the top n alignments. We re-estimated the probability of each alignment in the n-best list using renormalization (Venugopal et al., 2008). Finally we construct weighted alignment hypergraphs from these n-best lists.2 When extracting rules from hypergraphs, we set the pruning threshold t = 0.5. 4.2 Tractability of Divide-and-Conquer Strategy Figure 4 shows the distribution of vertices (hyperedges) number of the subhypergraphs. We can see that most of the subhypergraphs have just less than two vertices and hyperedges.3 Specifically, each subhyperg</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Minimum Bayes-risk word alignments of bilingual texts.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>140--147</pages>
<contexts>
<context position="16593" citStr="Kumar and Byrne (2002)" startWordPosition="2705" endWordPosition="2708">we estimate the probabilities from a much larger alignment space that can not be represented by n-best lists, (2) our approach can extract good rules that cannot be extracted from any single alignments in the n-best lists. 5 Related Work Our research builds on previous work in the field of graph models and compact representations. Graph models have been used before in word alignment: the search space of word alignment can be structured as a graph and the search problem can be reformulated as finding the optimal path though this graph (e.g., (Och and Ney, 2004; Liu et al., 2010)). In addition, Kumar and Byrne (2002) define a graph distance as a loss function for minimum Bayes-risk word alignment, Riesa and Marcu (2010) open up the word alignment task to advances in hypergraph algorithms currently used in parsing. As opposed to the search problem, we propose a graph-based compact representation that encodes multiple alignments for machine translation. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g., using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010; Tu et al., 2012a), word lattices over 1-best segmentations</context>
</contexts>
<marker>Kumar, Byrne, 2002</marker>
<rawString>Shankar Kumar and William Byrne. 2002. Minimum Bayes-risk word alignments of bilingual texts. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 140–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Tian Xia</author>
<author>Xinyan Xiao</author>
<author>Qun Liu</author>
</authors>
<title>Weighted alignment matrices for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1017--1026</pages>
<contexts>
<context position="6996" citStr="Liu et al., 2009" startWordPosition="1155" endWordPosition="1158">ases from hypergraphs instead of 1-best alignments. Rather than restricting ourselves by the alignment consistency in the traditional algorithm, we extract all possible candidate target phrases for each source phrase. To maintain a reasonable rule table size, we filter out less promising candidates that have a fractional count lower than a threshold. 3.2 Calculating Fractional Counts The fractional count of a phrase pair is the probability sum of the alignments with which the phrase pair is consistent (§3.2.2), divided by the probability sum of all alignments encoded in a hypergraph (§3.2.1) (Liu et al., 2009). 359 Intuitively, our approach faces two challenges: 1. How to calculate the probability sum of all alignments encoded in a hypergraph (§3.2.1)? 2. How to efficiently calculate the probability sum of all consistent alignments for each phrase pair (§3.2.2)? 3.2.1 Enumerating All Alignments In theory, a hypergraph can encode all possible alignments if there are enough hyperedges. However, since a hypergraph is constructed from an nbest list, it can only represent partial space of all alignments (p(A|H) &lt; 1) because of the limiting size of hyperedges learned from the list. Therefore, we need to </context>
<context position="10658" citStr="Liu et al., 2009" startWordPosition="1746" endWordPosition="1749">l sub-alignments encoded in a hypergraph. Given a phrase pair P, let OS and NS denotes the sets of overlap and non-overlap subhypergraphs respectively (NS = H − OS). Then p(A|H, P) = 11 p(Ai|hi, P) 11 p(Aj|hj) hiEOS hjENS Here the phrase pair is absolutely consistent with the sub-alignments from non-overlap subhypergraphs (IS), and we have p(A|h, P) = p(A|h). Then the fractional count of a phrase pair is: c(P|H) = p(A|H, P) p(A|H) HhiEOS p(A|hi, P) HhiEOS p(A|hi) After we get the fractional counts of translation rules, we can estimate their relative frequencies (Och and Ney, 2004). We follow (Liu et al., 2009; Tu et al., 2011) to learn lexical tables from n-best lists and then calculate the lexical weights. (a) (b) Җ  Ṽᆀ к e2 e3 e4 e1 e5 the book is on the desk Ṽᆀ Җ  e2 e3 e4 e1 e5 is the book the desk on к h1 h2 h3 360 Rules from... Rules MT03 MT04 MT05 Avg. 1-best 257M 33.45 35.25 33.63 34.11 10-best 427M 34.10 35.71 34.04 34.62 Hypergraph 426M 34.71 36.24 34.41 35.12 Table 1: Evaluation of translation quality. 4 Experiments 4.1 Setup We carry out our experiments on Chinese-English translation tasks using a reimplementation of the hierarchical phrase-based system (Chiang, 2007). Our training d</context>
<context position="17292" citStr="Liu et al., 2009" startWordPosition="2813" endWordPosition="2816">Riesa and Marcu (2010) open up the word alignment task to advances in hypergraph algorithms currently used in parsing. As opposed to the search problem, we propose a graph-based compact representation that encodes multiple alignments for machine translation. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g., using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010; Tu et al., 2012a), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices over 1-best word alignments (Liu et al., 2009; Tu et al., 2011; Tu et al., 2012b). Liu et al., (2009) estimate the link probabilities from n-best lists, while Gispert et al., (2010) learn the alignment posterior probabilities directly from IBM models. However, both of them ignore the relations among alignment links. By contrast, our approach takes into account the joint distribution of alignment links and explores the fertility model past the link level. 6 Conclusion We have presented a novel compact representation of word alignment, named weighted bipartite hypergraph, to exploit the relations among alignment links. Since estimating the</context>
</contexts>
<marker>Liu, Xia, Xiao, Liu, 2009</marker>
<rawString>Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu. 2009. Weighted alignment matrices for statistical machine translation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1017–1026.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Discriminative word alignment by linear modeling.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="16555" citStr="Liu et al., 2010" startWordPosition="2699" endWordPosition="2702">on of rule probabilities because we estimate the probabilities from a much larger alignment space that can not be represented by n-best lists, (2) our approach can extract good rules that cannot be extracted from any single alignments in the n-best lists. 5 Related Work Our research builds on previous work in the field of graph models and compact representations. Graph models have been used before in word alignment: the search space of word alignment can be structured as a graph and the search problem can be reformulated as finding the optimal path though this graph (e.g., (Och and Ney, 2004; Liu et al., 2010)). In addition, Kumar and Byrne (2002) define a graph distance as a loss function for minimum Bayes-risk word alignment, Riesa and Marcu (2010) open up the word alignment task to advances in hypergraph algorithms currently used in parsing. As opposed to the search problem, we propose a graph-based compact representation that encodes multiple alignments for machine translation. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g., using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010; Tu et al., 2012a), w</context>
</contexts>
<marker>Liu, Liu, Lin, 2010</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2010. Discriminative word alignment by linear modeling. Computational Linguistics, 36(3):303–339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
</authors>
<title>Forest-based translation rule extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>206--214</pages>
<contexts>
<context position="17116" citStr="Mi and Huang, 2008" startWordPosition="2784" endWordPosition="2787">h this graph (e.g., (Och and Ney, 2004; Liu et al., 2010)). In addition, Kumar and Byrne (2002) define a graph distance as a loss function for minimum Bayes-risk word alignment, Riesa and Marcu (2010) open up the word alignment task to advances in hypergraph algorithms currently used in parsing. As opposed to the search problem, we propose a graph-based compact representation that encodes multiple alignments for machine translation. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g., using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010; Tu et al., 2012a), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices over 1-best word alignments (Liu et al., 2009; Tu et al., 2011; Tu et al., 2012b). Liu et al., (2009) estimate the link probabilities from n-best lists, while Gispert et al., (2010) learn the alignment posterior probabilities directly from IBM models. However, both of them ignore the relations among alignment links. By contrast, our approach takes into account the joint distribution of alignment links and explores the fertility model past the link level. 6 Conclusi</context>
</contexts>
<marker>Mi, Huang, 2008</marker>
<rawString>Haitao Mi and Liang Huang. 2008. Forest-based translation rule extraction. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 206–214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>A discriminative framework for bilingual word alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="1147" citStr="Moore, 2005" startWordPosition="164" endWordPosition="165">m hypergraphs is an NP-complete problem, which is computationally infeasible. Therefore, we propose a divide-and-conquer strategy by decomposing a hypergraph into a set of independent subhypergraphs. The experiments show that our approach outperforms both 1-best and n-best alignments. 1 Introduction Word alignment is the task of identifying translational relations between words in parallel corpora, in which a word at one language is usually translated into several words at the other language (fertility model) (Brown et al., 1993). Given that many-to-many links are common in natural languages (Moore, 2005), it is necessary to pay attention to the relations among alignment links. In this paper, we have proposed a novel graphbased compact representation of word alignment, which takes into account the joint distribution of alignment links. We first transform each alignment to a bigraph that can be decomposed into a set of subgraphs, where all interrelated links are in the same subgraph (§ 2.1). Then we employ a weighted partite hypergraph to encode multiple bigraphs (§ 2.2). The main challenge of this research is to efficiently calculate the fractional counts for rules extracted from hypergraphs. </context>
</contexts>
<marker>Moore, 2005</marker>
<rawString>Robert C. Moore. 2005. A discriminative framework for bilingual word alignment. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 81–88, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="10629" citStr="Och and Ney, 2004" startWordPosition="1740" endWordPosition="1743"> from the probability sum of all sub-alignments encoded in a hypergraph. Given a phrase pair P, let OS and NS denotes the sets of overlap and non-overlap subhypergraphs respectively (NS = H − OS). Then p(A|H, P) = 11 p(Ai|hi, P) 11 p(Aj|hj) hiEOS hjENS Here the phrase pair is absolutely consistent with the sub-alignments from non-overlap subhypergraphs (IS), and we have p(A|h, P) = p(A|h). Then the fractional count of a phrase pair is: c(P|H) = p(A|H, P) p(A|H) HhiEOS p(A|hi, P) HhiEOS p(A|hi) After we get the fractional counts of translation rules, we can estimate their relative frequencies (Och and Ney, 2004). We follow (Liu et al., 2009; Tu et al., 2011) to learn lexical tables from n-best lists and then calculate the lexical weights. (a) (b) Җ  Ṽᆀ к e2 e3 e4 e1 e5 the book is on the desk Ṽᆀ Җ  e2 e3 e4 e1 e5 is the book the desk on к h1 h2 h3 360 Rules from... Rules MT03 MT04 MT05 Avg. 1-best 257M 33.45 35.25 33.63 34.11 10-best 427M 34.10 35.71 34.04 34.62 Hypergraph 426M 34.71 36.24 34.41 35.12 Table 1: Evaluation of translation quality. 4 Experiments 4.1 Setup We carry out our experiments on Chinese-English translation tasks using a reimplementation of the hierarchical phrase-based system (</context>
<context position="16536" citStr="Och and Ney, 2004" startWordPosition="2695" endWordPosition="2698">s a better estimation of rule probabilities because we estimate the probabilities from a much larger alignment space that can not be represented by n-best lists, (2) our approach can extract good rules that cannot be extracted from any single alignments in the n-best lists. 5 Related Work Our research builds on previous work in the field of graph models and compact representations. Graph models have been used before in word alignment: the search space of word alignment can be structured as a graph and the search problem can be reformulated as finding the optimal path though this graph (e.g., (Och and Ney, 2004; Liu et al., 2010)). In addition, Kumar and Byrne (2002) define a graph distance as a loss function for minimum Bayes-risk word alignment, Riesa and Marcu (2010) open up the word alignment task to advances in hypergraph algorithms currently used in parsing. As opposed to the search problem, we propose a graph-based compact representation that encodes multiple alignments for machine translation. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g., using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010; T</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz J. Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="11547" citStr="Och, 2003" startWordPosition="1904" endWordPosition="1905">25 33.63 34.11 10-best 427M 34.10 35.71 34.04 34.62 Hypergraph 426M 34.71 36.24 34.41 35.12 Table 1: Evaluation of translation quality. 4 Experiments 4.1 Setup We carry out our experiments on Chinese-English translation tasks using a reimplementation of the hierarchical phrase-based system (Chiang, 2007). Our training data contains 1.5 million sentence pairs from LDC dataset.1 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, caseinsensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We first follow Venugopal et al. (2008) to produce n-best lists via GIZA++. We produce 10-best lists in two translation directions, and use “growdiag-final-and” strategy (Koehn et al., 2003) to generate the final n-best lists by selecting the top n alignments. We re-estimated the probability of each alignment in the n-best list using renormalization (Venugopal et al., 2008). Finally we construct </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="11703" citStr="Papineni et al., 2002" startWordPosition="1927" endWordPosition="1930">ments 4.1 Setup We carry out our experiments on Chinese-English translation tasks using a reimplementation of the hierarchical phrase-based system (Chiang, 2007). Our training data contains 1.5 million sentence pairs from LDC dataset.1 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, caseinsensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We first follow Venugopal et al. (2008) to produce n-best lists via GIZA++. We produce 10-best lists in two translation directions, and use “growdiag-final-and” strategy (Koehn et al., 2003) to generate the final n-best lists by selecting the top n alignments. We re-estimated the probability of each alignment in the n-best list using renormalization (Venugopal et al., 2008). Finally we construct weighted alignment hypergraphs from these n-best lists.2 When extracting rules from hypergraphs, we set the pruning threshold t = 0.5. 4.2 Tractability of D</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Riesa</author>
<author>Daniel Marcu</author>
</authors>
<title>Hierarchical search for word alignment.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>157--166</pages>
<contexts>
<context position="16698" citStr="Riesa and Marcu (2010)" startWordPosition="2722" endWordPosition="2725">sts, (2) our approach can extract good rules that cannot be extracted from any single alignments in the n-best lists. 5 Related Work Our research builds on previous work in the field of graph models and compact representations. Graph models have been used before in word alignment: the search space of word alignment can be structured as a graph and the search problem can be reformulated as finding the optimal path though this graph (e.g., (Och and Ney, 2004; Liu et al., 2010)). In addition, Kumar and Byrne (2002) define a graph distance as a loss function for minimum Bayes-risk word alignment, Riesa and Marcu (2010) open up the word alignment task to advances in hypergraph algorithms currently used in parsing. As opposed to the search problem, we propose a graph-based compact representation that encodes multiple alignments for machine translation. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g., using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010; Tu et al., 2012a), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices over 1-best word alignments (Liu et al., 2009; Tu e</context>
</contexts>
<marker>Riesa, Marcu, 2010</marker>
<rawString>Jason Riesa and Daniel Marcu. 2010. Hierarchical search for word alignment. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 157–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of Seventh International Conference on Spoken Language Processing,</booktitle>
<volume>3</volume>
<pages>901--904</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="11441" citStr="Stolcke, 2002" startWordPosition="1887" endWordPosition="1888"> e4 e1 e5 is the book the desk on к h1 h2 h3 360 Rules from... Rules MT03 MT04 MT05 Avg. 1-best 257M 33.45 35.25 33.63 34.11 10-best 427M 34.10 35.71 34.04 34.62 Hypergraph 426M 34.71 36.24 34.41 35.12 Table 1: Evaluation of translation quality. 4 Experiments 4.1 Setup We carry out our experiments on Chinese-English translation tasks using a reimplementation of the hierarchical phrase-based system (Chiang, 2007). Our training data contains 1.5 million sentence pairs from LDC dataset.1 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, caseinsensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We first follow Venugopal et al. (2008) to produce n-best lists via GIZA++. We produce 10-best lists in two translation directions, and use “growdiag-final-and” strategy (Koehn et al., 2003) to generate the final n-best lists by selecting the top n alignments. We re-estimated the probability </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In Proceedings of Seventh International Conference on Spoken Language Processing, volume 3, pages 901–904. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhaopeng Tu</author>
<author>Yang Liu</author>
<author>Young-Sook Hwang</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Dependency forest for statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>1092--1100</pages>
<contexts>
<context position="17133" citStr="Tu et al., 2010" startWordPosition="2788" endWordPosition="2791">(Och and Ney, 2004; Liu et al., 2010)). In addition, Kumar and Byrne (2002) define a graph distance as a loss function for minimum Bayes-risk word alignment, Riesa and Marcu (2010) open up the word alignment task to advances in hypergraph algorithms currently used in parsing. As opposed to the search problem, we propose a graph-based compact representation that encodes multiple alignments for machine translation. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g., using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010; Tu et al., 2012a), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices over 1-best word alignments (Liu et al., 2009; Tu et al., 2011; Tu et al., 2012b). Liu et al., (2009) estimate the link probabilities from n-best lists, while Gispert et al., (2010) learn the alignment posterior probabilities directly from IBM models. However, both of them ignore the relations among alignment links. By contrast, our approach takes into account the joint distribution of alignment links and explores the fertility model past the link level. 6 Conclusion We have presen</context>
</contexts>
<marker>Tu, Liu, Hwang, Liu, Lin, 2010</marker>
<rawString>Zhaopeng Tu, Yang Liu, Young-Sook Hwang, Qun Liu, and Shouxun Lin. 2010. Dependency forest for statistical machine translation. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 1092–1100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhaopeng Tu</author>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Extracting hierarchical rules from a weighted alignment matrix.</title>
<date>2011</date>
<booktitle>In Proceedings of 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>1294--1303</pages>
<contexts>
<context position="10676" citStr="Tu et al., 2011" startWordPosition="1750" endWordPosition="1753">ncoded in a hypergraph. Given a phrase pair P, let OS and NS denotes the sets of overlap and non-overlap subhypergraphs respectively (NS = H − OS). Then p(A|H, P) = 11 p(Ai|hi, P) 11 p(Aj|hj) hiEOS hjENS Here the phrase pair is absolutely consistent with the sub-alignments from non-overlap subhypergraphs (IS), and we have p(A|h, P) = p(A|h). Then the fractional count of a phrase pair is: c(P|H) = p(A|H, P) p(A|H) HhiEOS p(A|hi, P) HhiEOS p(A|hi) After we get the fractional counts of translation rules, we can estimate their relative frequencies (Och and Ney, 2004). We follow (Liu et al., 2009; Tu et al., 2011) to learn lexical tables from n-best lists and then calculate the lexical weights. (a) (b) Җ  Ṽᆀ к e2 e3 e4 e1 e5 the book is on the desk Ṽᆀ Җ  e2 e3 e4 e1 e5 is the book the desk on к h1 h2 h3 360 Rules from... Rules MT03 MT04 MT05 Avg. 1-best 257M 33.45 35.25 33.63 34.11 10-best 427M 34.10 35.71 34.04 34.62 Hypergraph 426M 34.71 36.24 34.41 35.12 Table 1: Evaluation of translation quality. 4 Experiments 4.1 Setup We carry out our experiments on Chinese-English translation tasks using a reimplementation of the hierarchical phrase-based system (Chiang, 2007). Our training data contains 1.5 m</context>
<context position="17309" citStr="Tu et al., 2011" startWordPosition="2817" endWordPosition="2820">010) open up the word alignment task to advances in hypergraph algorithms currently used in parsing. As opposed to the search problem, we propose a graph-based compact representation that encodes multiple alignments for machine translation. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g., using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010; Tu et al., 2012a), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices over 1-best word alignments (Liu et al., 2009; Tu et al., 2011; Tu et al., 2012b). Liu et al., (2009) estimate the link probabilities from n-best lists, while Gispert et al., (2010) learn the alignment posterior probabilities directly from IBM models. However, both of them ignore the relations among alignment links. By contrast, our approach takes into account the joint distribution of alignment links and explores the fertility model past the link level. 6 Conclusion We have presented a novel compact representation of word alignment, named weighted bipartite hypergraph, to exploit the relations among alignment links. Since estimating the probabilities of</context>
</contexts>
<marker>Tu, Liu, Liu, Lin, 2011</marker>
<rawString>Zhaopeng Tu, Yang Liu, Qun Liu, and Shouxun Lin. 2011. Extracting hierarchical rules from a weighted alignment matrix. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 1294–1303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhaopeng Tu</author>
<author>Wenbin Jiang</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Dependency forest for sentiment analysis.</title>
<date>2012</date>
<booktitle>In Springer-Verlag</booktitle>
<pages>69--77</pages>
<location>Berlin Heidelberg,</location>
<contexts>
<context position="17150" citStr="Tu et al., 2012" startWordPosition="2792" endWordPosition="2795">4; Liu et al., 2010)). In addition, Kumar and Byrne (2002) define a graph distance as a loss function for minimum Bayes-risk word alignment, Riesa and Marcu (2010) open up the word alignment task to advances in hypergraph algorithms currently used in parsing. As opposed to the search problem, we propose a graph-based compact representation that encodes multiple alignments for machine translation. Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e.g., using forests over 1-best trees (Mi and Huang, 2008; Tu et al., 2010; Tu et al., 2012a), word lattices over 1-best segmentations (Dyer et al., 2008), and weighted alignment matrices over 1-best word alignments (Liu et al., 2009; Tu et al., 2011; Tu et al., 2012b). Liu et al., (2009) estimate the link probabilities from n-best lists, while Gispert et al., (2010) learn the alignment posterior probabilities directly from IBM models. However, both of them ignore the relations among alignment links. By contrast, our approach takes into account the joint distribution of alignment links and explores the fertility model past the link level. 6 Conclusion We have presented a novel compa</context>
</contexts>
<marker>Tu, Jiang, Liu, Lin, 2012</marker>
<rawString>Zhaopeng Tu, Wenbin Jiang, Qun Liu, and Shouxun Lin. 2012a. Dependency forest for sentiment analysis. In Springer-Verlag Berlin Heidelberg, pages 69–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhaopeng Tu</author>
<author>Yang Liu</author>
<author>Yifan He</author>
<author>Josef van Genabith</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Combining multiple alignments to improve machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings ofthe 24th International Conference on Computational Linguistics,</booktitle>
<pages>1249--1260</pages>
<marker>Tu, Liu, He, van Genabith, Liu, Lin, 2012</marker>
<rawString>Zhaopeng Tu, Yang Liu, Yifan He, Josef van Genabith, Qun Liu, and Shouxun Lin. 2012b. Combining multiple alignments to improve machine translation. In Proceedings ofthe 24th International Conference on Computational Linguistics, pages 1249–1260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leslie G Valiant</author>
</authors>
<title>The complexity of computing the permanent.</title>
<date>1979</date>
<journal>Theoretical Computer Science,</journal>
<volume>8</volume>
<issue>2</issue>
<contexts>
<context position="8115" citStr="Valiant, 1979" startWordPosition="1333" endWordPosition="1334">) &lt; 1) because of the limiting size of hyperedges learned from the list. Therefore, we need to enumerate all possible alignments in a hypergraph to obtain the probability sum p(A|H). Specifically, generating an alignment from a hypergraph can be modelled as finding a complete hyperedge matching, which is a set of hyperedges without common vertices that matches all vertices. The probability of the alignment is the product of hyperedge weights. Thus, enumerating all possible alignments in a hypergraph is reformulated as finding all complete hypergraph matchings, which is an NP-complete problem (Valiant, 1979). Similar to the bigraph, a hypergraph is also usually not connected. To make the enumeration practically tractable, we propose a divide-and-conquer strategy by decomposing a hypergraph H into a set of independent subhypergraphs {h1, h2, ... , hn}. Intuitively, the probability of an alignment is the product of hyperedge weights. According to the divide-and-conquer strategy, the probability sum of all alignments A encoded in a hypergraph H is: p(A|H) = 11 p(Ai|hi) hiEH Here p(Ai|hi) is the probability sum of all subalignments Ai encoded in the subhypergraph hi. 3.2.2 Enumerating Consistent Alig</context>
</contexts>
<marker>Valiant, 1979</marker>
<rawString>Leslie G Valiant. 1979. The complexity of computing the permanent. Theoretical Computer Science, 8(2):189–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollmann</author>
<author>Noah A Smith</author>
<author>Stephan Vogel</author>
</authors>
<title>Wider pipelines: n-best alignments and parses in mt training.</title>
<date>2008</date>
<booktitle>In Proceedings ofAMTA,</booktitle>
<pages>192--201</pages>
<contexts>
<context position="11787" citStr="Venugopal et al. (2008)" startWordPosition="1940" endWordPosition="1943">sing a reimplementation of the hierarchical phrase-based system (Chiang, 2007). Our training data contains 1.5 million sentence pairs from LDC dataset.1 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use minimum error rate training (Och, 2003) to optimize the feature weights on the MT02 testset, and test on the MT03/04/05 testsets. For evaluation, caseinsensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We first follow Venugopal et al. (2008) to produce n-best lists via GIZA++. We produce 10-best lists in two translation directions, and use “growdiag-final-and” strategy (Koehn et al., 2003) to generate the final n-best lists by selecting the top n alignments. We re-estimated the probability of each alignment in the n-best list using renormalization (Venugopal et al., 2008). Finally we construct weighted alignment hypergraphs from these n-best lists.2 When extracting rules from hypergraphs, we set the pruning threshold t = 0.5. 4.2 Tractability of Divide-and-Conquer Strategy Figure 4 shows the distribution of vertices (hyperedges) </context>
</contexts>
<marker>Venugopal, Zollmann, Smith, Vogel, 2008</marker>
<rawString>Ashish Venugopal, Andreas Zollmann, Noah A. Smith, and Stephan Vogel. 2008. Wider pipelines: n-best alignments and parses in mt training. In Proceedings ofAMTA, pages 192–201.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>