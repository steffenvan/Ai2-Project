<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000013">
<title confidence="0.8595685">
Dead parrots make bad pets:
Exploring modifier effects in noun phrases
</title>
<author confidence="0.914146">
Germ´an Kruszewski and Marco Baroni
</author>
<affiliation confidence="0.898307">
Center for Mind/Brain Sciences (University of Trento, Italy)
</affiliation>
<email confidence="0.973575">
(german.kruszewski|marco.baroni)@unitn.it
</email>
<sectionHeader confidence="0.997108" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999899285714286">
Sometimes modifiers have a strong effect
on core aspects of the meaning of the
nouns they are attached to: A parrot is
a desirable pet, but a dead parrot is, at
the very least, a rather unusual household
companion. In order to stimulate compu-
tational research into the impact of mod-
ification on phrase meaning, we collected
and made available a large dataset contain-
ing subject ratings for a variety of noun
phrases and the categories they might be-
long to. We propose to use compositional
distributional semantics to model these
data, experimenting with numerous distri-
butional semantic spaces, phrase compo-
sition methods and asymmetric similarity
measures. Our models capture a statis-
tically significant portion of the data, al-
though much work is still needed before
we achieve a full computational account of
modification effects.
</bodyText>
<sectionHeader confidence="0.999514" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999819428571428">
Not all modifiers are created equal. Green parrots
have all essential qualities of parrots, but dead par-
rots don’t. For example, as vocally argued by the
disgruntled costumer in Monty Python’s famous
Dead Parrot Sketch,1 dead parrots make rather
poor pet birds. In modifier-head constructions
(that, for the purpose of this article, we restrict to
right-headed adjective-noun and noun-noun con-
structions), modifiers are not simply picking a sub-
set of the denotation of the head they modify, but
they are often distorting the properties of the head
in a radical manner.
These modifier effects on phrase meaning have
been studied extensively by theoretical linguists,
</bodyText>
<footnote confidence="0.84987">
1http://en.wikipedia.org/wiki/Dead_
Parrot_sketch
</footnote>
<bodyText confidence="0.999873">
who have focused primarily on the extreme case
of intensional modifiers such as fake, alleged and
toy, where the phrase denotes something that is
no longer (or is not necessarily) a head (a toy
gun is not a gun). See McNally (2013) for a re-
cent review of the linguistic literature. Cognitive
scientists have looked at modification phenomena
within the general study of conceptual combina-
tion (see Chapter 12 of Murphy (2002) for an ex-
tensive review). The cognitive tradition has fo-
cused on how modification affects prototypicality:
a guppy is the prototypical pet fish, but it is neither
a typical pet nor a typical fish (Smith and Osher-
son, 1984). This line of research has highlighted
how strong modification effects might be the rule,
rather than the exception: Wisniewski (1997) re-
ports that, when subjects were asked to provide
the meaning for more than 200 novel modifier-
head constructions, “70% [of the answers] in-
volved the construal of a noun’s referent as some-
thing other than the typical category named by the
noun [head].” Indeed, recent research suggests
that even the most stereotypical modifiers affect
prototypicality, so that subjects are less willing
to attribute to quacking ducks such obvious duck
properties as having webbed feet (Connolly et al.,
2007).
The impact of modification on phrase mean-
ing is not only very interesting from a linguistic
and cognitive perspective, but also important from
a practical point of view, as it might affect ex-
pected entailment patterns: If parrot entails pet,
then lively parrot also entails pet. However, as we
saw above, dead parrot doesn’t necessarily entail
pet (at least not from the point of view of a dis-
gruntled costumer who was just sold the corpse).
Being able to track the impact that modifiers have
on heads should thus have a positive effect on im-
portant tasks such as recognizing textual entail-
ment, paraphrasing and anaphora resolution (An-
droutsopoulos and Malakasiotis, 2010; Dagan et
</bodyText>
<page confidence="0.975659">
171
</page>
<note confidence="0.932677666666667">
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 171–181,
Dublin, Ireland, August 23-24 2014.
al., 2009; Poesio et al., 2010).
</note>
<bodyText confidence="0.999659413043478">
Despite their theoretical and practical import,
modification effects have been largely overlooked
in computational linguistics, with the notable ex-
ception of Boleda et al. (2012; 2013), who only
focused on the extreme case of intensional adjec-
tives, studied a limited number of modifiers, and
did not attempt to capture the graded nature of
modification (a dead parrot is not a prototypical
animal, but a toy parrot is not an animal at all).
This paper aims to stimulate computational re-
search into modifier effects on phrase meaning in
two ways. First, we introduce a new, large, pub-
licly available data set of modifier-head phrases
annotated with four kinds of modification-related
subject ratings: whether the concept denoted by
the phrase is an instance of the concept denoted by
its head (is a dead parrot still a parrot?), to what
extent it is a member of one of the larger categories
the head belongs to (is it still a pet?), and typical-
ity ratings for the same questions (how typical is a
dead parrot as a parrot? and as a pet?).
Second, we present a first attempt to model the
collected judgments computationally. We choose
distributional semantics (Erk, 2012) as our frame
of reference, as it produces continuous similarity
scores, in line with the graded nature of the mod-
ification effects we are investigating. In partic-
ular, we look at the compositional extension of
distributional semantics (Baroni, 2013), because
we need representations not only for words, but
also phrases, and we adopt the asymmetric simi-
larity measures developed in the literature on lex-
ical entailment (Kotlerman et al., 2010; Lenci and
Benotto, 2012), because we are interested in an
asymmetric relation (to what extent the concept
denoted by the phrase is a good instance of the tar-
get class, and not vice versa). As far as we know,
this is the first time these asymmetric measures
are applied to composed representations (Baroni
et al. (2012) experimented with entailment mea-
sures applied to phrase representations directly
harvested from corpora, and not derived composi-
tionally). We are thus also providing a novel eval-
uation of compositional models and asymmetric
measures on a challenging task where they could
potentially be very useful.2
</bodyText>
<footnote confidence="0.916359">
2Connell and Ramscar (2001) showed good correlation of
similarity scores produced by the LSA distributional seman-
tic model with human category typicality judgments, how-
ever they did not consider phrases nor adopted an asymmetric
measure to take directionality into account.
</footnote>
<sectionHeader confidence="0.84156" genericHeader="method">
2 The Norwegian Blue Parrot data set
</sectionHeader>
<bodyText confidence="0.999390829787234">
We introduce Norwegian Blue Parrot (NBP),3 a
new, large data set to explore modification effects.
Given a head noun h and a modifier adjective or
noun m, NBP contains average membership and
typicality ratings for the phrase mh both as an
instance of h and as an instance of c (a broader
category h belongs to). As a control, we also
present ratings for unmodified h as an instance
of c (we will use them below to test similarity
measures on their ability to capture the direction
of the membership relation, and to zero in on the
effect of modification vs. more general member-
ship/typicality effects). We include, and indeed fo-
cus on, relations with broader categories because
they are more prone to modification effects: In-
tuitively, a dead parrot is still a parrot, but it is,
at the very least, an atypical pet. The statistics
in Table 1, discussed below, confirm our intuition
that subjects are more likely to assign lower scores
with respect to a broader category than to the head
category itself (although this is, no doubt, in part
by construction, since we started constructing the
dataset by mining examples where mh is atypi-
cal of c, not h). We collect both membership and
typicality ratings because we expect them to have
different implications for sound entailment. If x
is not a member of class y, then x obviously does
not entail y. However, if x is an atypical y, en-
tailment still holds, but some typical properties of
y might not carry over (e.g., in an anaphora reso-
lution setting, we might still consider co-indexing
dead parrot with animal, but not with breathing
creature, despite the fact that breathing is a highly
characteristic property of animals).
In order to make sure that NBP would contain a
fair number of examples affected by strong mod-
ification effects, we first came up with a set of
(m, h, c) tuples where, according to our own in-
tuition, m makes h fairly atypical as an instance
of c. For example, a bottle is a piece of drinkware.
If we add the modifier perfume, we expect that,
while subjects might still agree that a perfume bot-
tle is a bottle, they should generally disagree on
the statement that a perfume bottle belongs to the
drinkware category. We refer to tuples of this
sort (e.g., (perfume, bottle, drinkware)) as dis-
torted tuples in what follows.4
</bodyText>
<footnote confidence="0.999269333333333">
3Available from http://clic.cimec.unitn.it/
composes/
4When creating the tuples, we also used some adjectives
</footnote>
<page confidence="0.997524">
172
</page>
<bodyText confidence="0.991433131313132">
We then constructed a number of tuples that
should not display a strong modification effect. In
particular, in order to insure that any atypical rat-
ing we obtained on the distorted tuples could not
be explained away by characteristics of m or h
alone (rather than by their combination), for each
distorted tuple we constructed a few more tuples
with the same h and c but a different m, that
we did not expect to be strongly distorting (e.g.,
(plastic, bottle, drinkware)). Similarly, for each
distorted tuple we generated a few more with the
same m, but combined with (the same or differ-
ent) h and c on which the m should not exert a
strong effect ((perfume, bottle, container)). In
total, NBP is based on 489 distorted tuples and
1938 more matching tuples.
We constructed NBP to insure that it would
contain many tuples displaying strong modifica-
tion effects, and highly comparable tuples that do
not feature such effects. An alternative approach
would have been to rate phrases that were ran-
domly selected from a corpus. This would have
led to a dataset reflecting a more realistic distribu-
tion of modification effects, but it would not have
guaranteed, for the same number of pairs, a fair
amount of distorted tuples and comparable con-
trols. We leave the study of the natural distribution
of modification strength in text to further work.
To find inspiration for the tuples, we looked into
various databases containing concepts organized
by category, namely BLESS (Baroni and Lenci,
2011), ConceptNet (Speer and Havasi, 2013) and
WordNet (Fellbaum, 1998). We insured that all
words in our tuples occurred at least 200 times in
the large corpus we describe below (phrases were
not filtered by frequency, due to data sparseness).
Finally, when looking for tuples matching the dis-
torted ones, we made sure that the mh phrases in
the new tuples have similar Pointwise Mutual In-
formation to the corresponding phrases in the dis-
torted tuple (or, where the latter were not attested
in the corpus, similar m and h frequencies). Find-
ing meaningful combinations among unattested or
infrequent phrases was not an easy task and there
was not always a perfect candidate. However, the
phrases selected in this way yielded challenging
items for which there is little or no direct cor-
pus evidence, so that compositional models are re-
quired to account for them.
that have been traditionally labeled as intensional by seman-
ticists: artificial, toy, former.
From each source tuple (e.g.,
(plastic, bottle, drinkware)), we generated 3
instance-class combinations to be rated: mh —* c
(plastic bottle —* drinkware), mh —* h (plastic
bottle —* bottle), h —* c (bottle —* drinkware), for
a total of 5,849 pairs, that constitute the final NBP
data set (2,417 mh —* c pairs, 2,115 mh —* h
pairs and 1,317 h —* c pairs).5
For each of these pairs, we collected both mem-
bership and typicality ratings through two surveys
on the CrowdFlower platform.6 Subjects came
exclusively from English speaking countries and
no special qualifications were required from them.
Membership ratings were collected by asking sub-
jects whether the instance is a member of the class
(formulated as a yes/no question). In a separate
study, we asked subjects to rate how typical the in-
stance is as member of the class on a 7-point scale.
For both questions, we collected 10 judgments per
pair and report their averages in NBP. For both sur-
veys, we added 48 control pairs with an expected
answer (yes/no for membership, high/low range
for typicality), that the subjects had to provide in
order for their ratings to be included in the final
set (“gold standard” items in crowd-sourcing par-
lance). These controls included highly prototypi-
cal pairs (dog —* animal), possibly with stereotyp-
ical modifiers (beautiful rose —* flower), and unre-
lated pairs (biology —* dance), also possibly under
modification (popular magazine —* animal).
We asked for binary rather than graded member-
ship judgments because these are more in line with
commonsense intuitions about category member-
ship (we might naturally speak of sparrows being
more typical birds than penguins, but it is strange
to say that they are “more birds”). The standard
view in the psychology of concepts (Hampton,
1991) is that membership judgments are the prod-
uct of a hard threshold we impose on the typicality
scale (x is not y if the typicality of x as y is below
a certain, subject-dependent threshold), although
under certain experimental conditions subjects can
also conceptualize membership as a graded prop-
erty (Kalish, 1995).
Membership and typicality ratings, especially
in borderline cases such as those we constructed,
are the output of complex cognitive processes
where large inter-subject differences are expected,
</bodyText>
<footnote confidence="0.9961135">
5There is a larger number of mh → c pairs because dif-
ferent tuples can lead to the same mh → h or h → c combi-
nations.
6http://crowdflower.com/
</footnote>
<page confidence="0.993497">
173
</page>
<table confidence="0.486401">
measure mh → c mh → h h → c tot.
memb. 0.84 (0.2) 0.97 (0.1) 0.88 (0.2) 0.89 (0.2)
typ. 5.45 (1.1) 6.29 (0.6) 5.81 (1.0) 5.84 (1.0)
</table>
<tableCaption confidence="0.76771">
Table 1: NBP summary statistics: Mean average
</tableCaption>
<bodyText confidence="0.969580854166667">
ratings and their standard deviations across pairs,
itemized by instance-class type and in total. Mem-
bership values range from 0 to 1, typicality values
from 1 to 7.
so it doesn’t make sense to worry about “inter-
annotator agreement” in this context. Still, several
sanity checks indicate that, overall, our subjects
understood our questions as we meant them, and
behaved in a reasonably coherent manner. First,
both average membership and typicality, ratings
are significantly lower (p &lt; 0.001) for the mh →
c pairs deriving from those tuples that we manu-
ally labeled as distorted than for the non-distorted
ones. Moreover, for membership, in 86% of the
cases at least 8 over 10 subjects gave the same re-
sponse. For typicality, the observed average rat-
ing standard deviation across pairs (1.2) is signifi-
cantly below what expected by chance (p &lt; 0.05),
based on a simulated random rating distribution.
Membership and typicality ratings are highly cor-
related, but not identical (r = 0.76)
Table 1 reports mean membership and typicality
scores in NBP. Both ratings are negatively skewed,
that is, subjects had the tendency to respond as-
sertively to the membership question and to give
high typicality scores. This is not surprising: Be-
cause of the way NBP was constructed, there are
about 4 tuples with no expected strong modifica-
tion effect for each distorted tuple. Furthermore,
except for the negative control items (not entered
in NBP), our questions did not feature cases where
a negative/low response would be entirely straight-
forward (of the “is a cat a building?” kind). We
observe moreover that, in accordance with the in-
tuition we discussed at the beginning of this sec-
tion, the ratings are extremely high when the class
is identical to the phrase head. On the other hand,
the mh → c condition displays, as expected, the
lowest averages, suggesting that this will be the
most interesting type to model experimentally.
Table 2 presents a few example entries from
NBP. The first block of the table illustrates cases
with the highest possible membership and typical-
ity scores. At the other extreme, the second block
contains examples with very low membership and
typicality. Interestingly, there are also cases, such
instance class memb. typ.
top membership, top typicality
</bodyText>
<table confidence="0.9677599">
gourmet soup food 1.00 7.00
huge tiger predator 1.00 7.00
sugared soda drink 1.00 7.00
live fish animal 1.00 7.00
Thai rice rice 1.00 7.00
silver spoon spoon 1.00 7.00
low membership, low typicality
fatal shooting sport 0.20 1.40
human egg food 0.40 1.50
perfume bottle drinkware 0.10 1.30
explosive vest commodity 0.30 1.90
lemon water chemical 0.20 1.60
creamy rice bean 0.20 1.30
top membership, (relatively) low typicality
sick tuna tuna 1.00 3.20
explosive vest vest 1.00 3.50
perforated sieve tool 1.00 4.20
bottled oxygen substance 1.00 4.30
grilled trout creature 1.00 4.40
educational toy amusement 1.00 4.50
</table>
<tableCaption confidence="0.957855">
Table 2: Instance-class pairs illustrating various
</tableCaption>
<bodyText confidence="0.964176115384615">
combinations of membership and typicality rat-
ings in NBP.
as the ones in the third block of the table, where all
subjects agreed on class membership, but the typ-
icality scores are relatively low (we did not find
clear cases of the opposite pattern, and indeed we
would have been surprised to find highly typical
instances of a class not being treated as members
of the class).
Some examples in Table 2 illustrate an impor-
tant design choice we made in constructing NBP,
namely, to ignore the issue of whether potential
modification effects are actually due to the modi-
fier and the category pertaining to different word
senses of the head term. One might argue, for
example, that egg has a food sense and a repro-
ductive vessel sense. The human modifier picks
the second sense, and so, obviously, human eggs
are judged as bad instances of food. While we
see the point of this objection, we think it’s im-
possible to draw a clear-cut distinction between
discrete word senses (even in the rather extreme
egg case, the eggs we eat are reproductive ves-
sels from a chicken point of view!). This has
been long recognized in the linguistic and cog-
nitive literature (Kilgarriff, 1997; Murphy, 2002),
</bodyText>
<page confidence="0.992567">
174
</page>
<bodyText confidence="0.999990952380952">
and even by the computational word sense disam-
biguation community, that is currently addressing
the continuous nature of polysemy by shifting to
the lexical-substitution-in-context task (McCarthy
and Navigli, 2009). Context provides fundamen-
tal cues to disambiguating polysemous words, and
noun modifiers typically act as important disam-
biguating contexts for the nouns. Thus, we think
that it is more productive for computational sys-
tems to handle modifier-triggered disambiguation
as a special case of the more general class of mod-
ification effects, than to engage in the quixotic
pursuit to determine, a priori, what’s the bound-
ary between a word-sense and a “pure” modifi-
cation effect. Note in Table 2 that grilled trout
was unanimously rated by subjects as an instance
of the creature category, despite the fact that the
cooking-related grilled modifier cues a classic
shift from an animal (and thus creature) sense to
food (Copestake and Briscoe, 1995). Examples
like this suggest that our agnosticism is warranted.
</bodyText>
<sectionHeader confidence="0.999896" genericHeader="method">
3 Methods
</sectionHeader>
<subsectionHeader confidence="0.99971">
3.1 Composition models
</subsectionHeader>
<bodyText confidence="0.999477510638298">
We experiment with many ways to derive a phrase
vector by combining the vectors of its constituents.
Mitchell and Lapata (2010) proposed a set of sim-
ple models in which each component of the phrase
vector is a function of the corresponding compo-
nents of the constituent vectors. Given vectors a�
and b, the weighted additive model (wadd) returns
their weighted sum: p� = w1d + w2 b. In the dila-
tion model (dil), the output vector is obtained by
decomposing one of the input vectors, say b, into
a vector parallel to a� and its orthogonal counter-
part, and then dilating only the parallel vector by a
factor A before re-combining. The corresponding
formula is: (d·d)b + (A − 1)(d·b)d. In our ex-
periments, we stretch the head vector in the direc-
tion of the modifier (i.e., a� is the modifier, b� is the
head). In the multiplicative model (mult), vectors
are combined by component-wise multiplication,
such that each phrase component pi is given by:
pi = aibi.
Guevara (2010) and Zanzotto et al. (2010) pro-
pose a full form of the additive model (fulladd),
where the two constituent vectors are multiplied
by weight matrices before being added, so that
each phrase component is a weighted sum of all
constituent components: p� = W1 + W2 b.
Finally, the lexical function (lexfunc) model of
Baroni and Zamparelli (2010) and Coecke et al.
(2010) takes inspiration from formal semantics
to characterize composition as function applica-
tion. In particular, in modifier-head phrases, the
modifier is treated as a linear function operating
on the head vector. Given that linear functions
can be expressed by matrices and their application
by matrix-by-vector multiplication, the modifier is
represented by a matrix A to be multiplied with
the modifier vector b, so that: p� = Ab.
We use the DISSECT toolkit7 to estimate the
parameters of the composition methods and de-
rive phrase vectors. In particular, DISSECT finds
optimal parameter settings by learning to approx-
imate corpus-extracted phrase vector examples
with least-squares methods (Dinu et al., 2013).
We use as training examples all the modifier-head
phrases that contain a modifier of interest and oc-
cur at least 50 times in our source corpus (see Sec-
tion 3.3 below).
</bodyText>
<subsectionHeader confidence="0.999397">
3.2 Asymmetric similarity measures
</subsectionHeader>
<bodyText confidence="0.998128888888889">
Several measures to identify word pairs that stand
in an instance-class relationship by comparing
their vectors have been proposed in the recent dis-
tributional semantics literature (Kotlerman et al.,
2010; Lenci and Benotto, 2012; Weeds et al.,
2004).8 While the task of deciding if u is in class v
is typically framed (also by distributional semanti-
cists) in binary, yes-or-no terms, all proposed mea-
sures return a continuous numerical score.9 Con-
sequently, we conjecture that they might be well-
suited to capture the graded notions of class mem-
bership and typicality we recorded in NBP.10
In what follows, we use wx(f) to denote the
weight (value) of feature (dimension) f in the dis-
tributional vector of term x. Fx denotes the set of
features (dimensions) in the vector of x such that
wx(f) &gt; t, where t is a predefined threshold to
decide whether a feature is active.11 Importantly,
</bodyText>
<footnote confidence="0.998349357142857">
7http://clic.cimec.unitn.it/composes/
toolkit/
8We speak of “instance-class relations” in a very broad
and loose sense, to encompass classic relations such as hy-
ponymy but also the fuzzier notion of lexical entailment.
9SVM classifiers have also been shown by Baroni et al.
(2012) to be well-suited for entailment detection, but they do
not naturally return continuous scores.
10Subjects had to answer a yes/no question concerning
class membership, but by averaging their response we derive
continuous membership scores.
11The obvious choice for t is 0. However, when work-
ing with the low-rank spaces described in Section 3.3 below,
we set t to 0.1, since after SVD/NMF smoothing we observe
</footnote>
<page confidence="0.996663">
175
</page>
<bodyText confidence="0.998695083333333">
all measures assume non-negative values.
Most asymmetric measures proposed in the lit-
erature build upon the distributional inclusion hy-
pothesis, stating that “if u is a semantically nar-
rower term than v, then a significant number
of salient distributional features of u is included
in the feature vector of v as well” (Lenci and
Benotto, 2012). In our terminology, u is the poten-
tial instance, and v is the class. We re-implement
all the measures adopted by Lenci and Benotto,
namely weedsprec, cosweeds, clarkede and invcl
(see their paper for the original references):
</bodyText>
<equation confidence="0.993129">
P
weedsprec(u, v) =
pinvcl(u, v) = clarkede(u, v) x (1 − clarkede(u, v))
</equation>
<bodyText confidence="0.997757">
The cosweeds formula combines weedsprec
with the widely used symmetric cosine measure:
</bodyText>
<equation confidence="0.99198025">
P
f∈Fu∩Fv wu(f) x wv(f)
cosine(u, v) =qP qP
f∈Fu wu(f)2 x f∈Fv wv(f)2
</equation>
<bodyText confidence="0.99907">
Finally, we experiment with the carefully
crafted balapinc measure of Kotlerman et al.
(2010):
</bodyText>
<equation confidence="0.512512">
pbalapinc(u, v) = lin(u, v) · apinc(u, v)
</equation>
<bodyText confidence="0.965504">
where the lin term is computed as follows:
</bodyText>
<equation confidence="0.997082">
P
f∈Fu∩Fv wu(f) + wv(f)
lin(u, v) = Pf∈Fu wu(f) + Pf∈Fv wv(f)
</equation>
<bodyText confidence="0.999829857142857">
The balapinc score is the geometric average
of a symmetric similarity measure (lin) and the
strongly asymmetric apinc measure, that takes
large values when dimensions with high values in
the vector of the more specific term are also high
in the vector of the more general term (refer to
Kotlerman et al. (2010) for the apinc formula).
</bodyText>
<footnote confidence="0.347395">
widespread low-frequency noise.
</footnote>
<subsectionHeader confidence="0.997442">
3.3 Distributional semantic spaces
</subsectionHeader>
<bodyText confidence="0.999931257142857">
We extract co-occurrence information from a cor-
pus of about 2.8 billion words obtained by con-
catenating ukWaC,12 Wikipedia13 and the British
National Corpus.14 With DISSECT, we build co-
occurrence vectors for the top 20K most frequent
lemmas in the source corpus (plus any NBP term
missing from this list). We treat the top 10K
most frequent lemmas as context elements. We
consider context windows of 2 and 20 words on
the two sides of the targets. We weight the vec-
tors by non-negative Pointwise Mutual Informa-
tion and Local Mutual Information (Evert, 2005).
We experiment with vectors in the resulting full-
rank (10K-dimensional) semantic spaces as well
as with vectors in spaces of ranks 100 and 300.
Rank reduction is performed by applying the Sin-
gular Value Decomposition (Golub and Van Loan,
1996) or Non-negative Matrix Factorization (Lee
and Seung, 2000). It is customary to represent the
output of these operations directly in a dense low-
dimensional space. However, the asymmetric sim-
ilarity measures we use assume sparse vectors (or
the “inclusion” criterion would be meaningless),
so we project back the outcome of SVD and NMF
to sparse 10K-dimensional but low-rank spaces. In
total, we explore 20 distinct semantic spaces.
We also collect co-occurrence vectors for
the phrases needed to estimate the composi-
tion method parameters (see Section 3.1 above).
We use DISSECT’s “peripheral space” option to
project the phrase raw count vectors into the vari-
ous spaces without affecting their structure.
Due to memory constraints, we restrict evalua-
tion in the full-rank spaces to the wadd and mult
models.
</bodyText>
<sectionHeader confidence="0.999738" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999243">
Given the methods described above, the main
question we want to answer is: Which combina-
tion of compositional model and asymmetric sim-
ilarity measure yields a better fit for the data in the
NBP dataset?
We start however with a sanity check on the
ability of the measures to capture the direction of
the instance-class membership relation. Even a
measure that is good at capturing degrees of mem-
bership/typicality won’t be of much practical use
</bodyText>
<footnote confidence="0.998989666666667">
12http://wacky.sslmit.unibo.it
13http://en.wikipedia.org
14http://www.natcorp.ox.ac.uk
</footnote>
<equation confidence="0.9887418">
f∈F wu(f)
pcosweeds(u, v) = weedsprec(u, v) x cosine(u,v)
clarkede(u, v) = Pf∈Fu∩Fv min(wu (f ), wv(f ))
P
f∈Fu wu(f)
</equation>
<page confidence="0.992297">
176
</page>
<figure confidence="0.4931988">
clarkede weedsprec balapinc cosweeds invcl
Low-rank spaces
10 8 11 8 7
Full-rank spaces
2 4 4 4 2
</figure>
<tableCaption confidence="0.929116">
Table 3: Number of spaces (over totals of 16 low-
</tableCaption>
<bodyText confidence="0.999653161290323">
rank and 4 full-rank spaces) in which each mea-
sure was able to predict class membership direc-
tion significantly above chance.
if it is not able to tell us which item in a pair is the
instance and which is the class.
Detecting membership direction As described
in Section 2 above, NBP also contains single-
word h —* c pairs (parrot—* pet). We extracted
the subset of those that all judges considered to
be in the category membership relation, and we
checked them manually to make sure that the di-
rection was one-way only. This resulted in a set
of 639 pairs where the membership relation holds
unidirectionally. We tested all combination of se-
mantic spaces (Section 3.3) and asymmetric sim-
ilarity measures (Section 3.2) on the task of as-
signing a higher score to the pairs in the h —* c
(vs. c —* h) direction (e.g., (score(parrot —*
pet) &gt; score(pet —* parrot)). Table 3 reports,
for each measure, the number of spaces in which
the measure was able to predict membership di-
rection significantly better than chance (binomial
test, p &lt; 0.05). We report results on full- and
low-rank (SVD, NMF) spaces separately since, as
discussed above, for most composition models we
can only use the latter. We observe that all mea-
sures are able to significantly detect directionality
in at least some spaces. For all the analyses below,
we exclude from further testing the space-measure
combinations that failed to pass this sanity check,
since they are clearly failing to capture properties
pertaining to the instance-class relation (if a com-
bination is not able to tell that it is a parrot that is
a pet, and not vice versa, there is no point in ask-
ing if the same combination is able to model how
typical a dead parrot is as a pet).
Modeling typicality ratings of mh —* c pairs
Next, for each of the remaining spaces, we first
performed composition as described in Section 3.1
above to build the representations for the nominal
phrases in the NBP dataset, and then computed
asymmetric similarity scores for pairs made of a
phrase and the corresponding potential class.
We computed the correlations between mean
human membership or typicality ratings and the
scores produced with each combination of com-
position model, similarity measure and space.
The resulting performance profiles for member-
ship and typicality are very highly correlated (r =
.99), and we thus report only the latter. We leave
it to further work to devise measures that are more
specifically tuned to capture membership or typi-
cality.
Table 4 reports the top correlation coefficients
between typicality judgments and scores of each
mh —* c pair (dead parrot—* pet) across spaces,
organized by measures and composition meth-
ods. The best correlation is achieved with the
weedsprec measure using the mult composition
model in a full-rank space (precisely that of con-
text window size 2 and ppmi weighting). Recall
that mult returns the component-wise product of
the vectors it combines. Thus, modification un-
der mult is carried out by picking only those fea-
tures of the head that are also present in the mod-
ifier, and enhancing them by a factor given by the
modifier’s feature value. The weedsprec measure
is then given by the weighted proportion of active
features in mh that are also active in c. Therefore,
the more the modifier shares features with the par-
ent category, the higher weedsprec will be. This
might explain why weedsprec is a good fit for the
mult model in measuring degrees of category typ-
icality.
Looking at composition methods, there is no ev-
idence that the more complex, matrix-based ful-
ladd and lexfunc approaches are performing any
better than the simple multiplicative and additive
methods. Indeed, mult shows the most consistent
overall performance, confirming the conclusion of
Blacoe and Lapata (2012) that, at the present time,
when it comes to composition, “simpler is better”.
A related point emerges from the comparison of
the low- and full-rank results for mult and wadd.
The smoothing process due to dimensionality re-
duction is quite disruptive for the current asym-
metric measures, that are based on feature inclu-
sion. This is a further reason to stick to simpler
composition methods, that can be applied directly
in the full-rank spaces.
Regarding the measures themselves, we see that
cosweeds, that balances weedsprec with the clas-
sic cosine score, is the most robust, returning good
</bodyText>
<page confidence="0.994223">
177
</page>
<table confidence="0.9718826">
clarkede weedsprec balapinc cosweeds invcl
Low-rank spaces
dil 9* 15* 16* 19* 8*
fulladd 17* 16* 12* 24* −3
lexfunc 17* 12* 12* 27* −2
mult 13* 19* 19* 29* 12*
wadd 14* 14* 16* 27* −2
Full-rank spaces
mult 9* 39* 33* 36* 15*
wadd 30* 34* 31* 35* 14*
</table>
<tableCaption confidence="0.986819">
Table 4: Percentage Pearson r between asymmet-
</tableCaption>
<bodyText confidence="0.976246075">
ric similarity measures and mh → c typicality rat-
ings. *p &lt; 0.001
results across all composition methods. On the
other hand, the related clarkede and invcl mea-
sures turn out to be quite brittle.
The highly significant correlations show that the
measures do capture to some extent the patterns of
variance in the data. However, when considering
potential practical applications, even the highest
reported correlation (.39) is certainly not impres-
sive, indicating that there is plenty of room for fur-
ther research into developing better composition
methods and/or membership/typicality measures.
Focusing on the modifier effect for mh → c
pairs The typicality judgment for dead parrot as
a pet is influenced by two factors: how typical par-
rots are as pets, and how much more or less typical
dead parrots are as pets, as opposed to parrots in
general. A good model must be able to capture
both factors (and this is what we tested above).
However, we are also interested in assessing to
what extent the models are capturing the modifi-
cation effectproper, as opposed to the overall de-
gree of typicality of the h concept as member of
the c category. To focus on the modification fac-
tor, we partialed out the h→c (parrot→pet) rat-
ings from the mh→c (dead parrot→pet) ratings
and from the corresponding model scores (that is,
we correlated the residuals of mh→c ratings and
model-produced scores after regressing the h→c
ratings on both). The results are shown in Table
5. Correlations are lower overall, but the general
picture from the previous analysis still holds, con-
firming that the computational models are (also)
capturing modifier effects. Interestingly, wadd, dil
and fulladd generally undergo larger performance
drops than mult and lexfunc. Evidently, models
like the latter, in which the modifier selects the
relevant features from the head, are better suited
to explain modification than the former, in which
</bodyText>
<table confidence="0.9238609">
clarkede weedsprec balapinc cosweeds invcl
Low-rank spaces
dil 5 −1 −1 −2 7*
fulladd 10* 7* 5+ 7+ −2
lexfunc 15* 9* 10* 18* −2
mult 4+ 14* 13* 15* 9*
wadd 7+ 7* 9* 12+ −2
Full-rank spaces
mult 1 25* 21* 24* 5+
wadd 11* 18* 13* 20* 2
</table>
<tableCaption confidence="0.974205">
Table 5: Percentage Pearson r between asymmet-
</tableCaption>
<bodyText confidence="0.988416114285714">
ric similarity measures and mh → c typicality rat-
ings where h → c scores have been partialed out.
*p &lt; 0.001, +p &lt; 0.05
the modifier features are just added to those of the
head by means of a linear combination.
Modeling typicality ratings of mh → h pairs
We repeated the first analysis for pairs of the type
mh → h (dead parrot→ parrot). The results,
shown in Table 6, are lower than in the previous
analysis. This is probably due to the fact that, as
discussed in Section 2, when the very same con-
cept is used as phrase head and category, judg-
ments are subject to a strong ceiling effect, and
none of our measures is designed to flatten out
above a certain threshold. Indeed, if we measure
the skewness of the typicality ratings,15 we obtain
that, while for h → c and mh → c the skewness is
of −1.9 and −1.5, respectively, for mh→h it gets
to −3.9.
In any case, the results confirm the brittleness of
the clarkede and invcl measures. The linguistically
motivated lexfunc model emerges here as a com-
petitive alternative to the simpler models. Still, the
best results are obtained with mult and cosweeds
(on the full-rank, context window size 20, ppmi
weighted space). Notably, weedsprec applied to a
pair of the type mh → h, where the phrase is con-
structed using the mult model, results in a constant
value of 1, whatever the modifier and the head
noun is. This is due to the fact that the features of
a phrase composed using mult are a subset of the
features of the head,16 and in this case the head is
the same as the category. Therefore, by definition,
weedsprec yields a score of 1 for every pair, the
variance is null and hence the correlation is unde-
</bodyText>
<footnote confidence="0.551984666666667">
15A skewness factor of 0 means that the distribution is bal-
anced around the mean, while the more negative the coeffi-
cient is, the more the left tail is longer and the distribution is
concentrated to the right (toward high typicality values in our
case).
16In set notation: F. n F„ = F. since F. C F„
</footnote>
<page confidence="0.984574">
178
</page>
<table confidence="0.9648077">
clarkede weedsprec balapinc cosweeds invcl
Low-rank spaces
dil 2 −1 −2 −3 4
fulladd 5+ 5+ 2 1 −1
lexfunc 14* 8* 14* 17* −1
mult 3 - 13* 15* 5+
wadd 6+ 8* 7+ 6 −3
Full-rank spaces
mult −2 - 18* 19* −2
wadd 7* 13* 7* 12* −2
</table>
<tableCaption confidence="0.803116333333333">
Table 6: Percentage Pearson r between asymmet-
ric similarity measures and mh → h typicality rat-
ings. *p &lt; 0.001, +p &lt; 0.05
</tableCaption>
<bodyText confidence="0.999866571428571">
fined. As a consequence, in this case cosweeds,
which is the geometric mean between weedsprec
and cosine, reduces to cosine similarity! The latter
might be effective in capturing the degree of simi-
larity between the phrase and its potential category
but, as a symmetric measure, it cannot, alone, pro-
vide a full account of category typicality effects.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999963516666667">
We introduced the challenge of quantifying the
impact of modification on the meaning of noun
phrases to the computational linguistics commu-
nity. We presented a new dataset that collects
membership and typicality ratings for modifier-
head phrases with respect to the category repre-
sented by the head as well as a broader category.
Since accounting for modifier distortion requires
semantic representations of phrases and model-
ing graded judgments, we consider this an ideal
testbed for compositional distributional semantics.
In the interaction between compositional mod-
els and directional similarity measures, we have
observed that simpler models yield better results.
Specifically, mult and wadd are economical com-
position models than can be applied on full-rank
spaces, which in turn work best with our similar-
ity measures.
Psychologists studying modification effects in
concept combination have proposed models that
are usually quite complex, relying on hand-crafted
feature definitions and making very strong as-
sumptions about the combination process (see for
example Cohen and Murphy (1984), Smith et al.
(1988)). Some of these assumptions have led other
researchers to argue that prototypes do not com-
pose at all (Connolly et al., 2007). In contrast,
the approach we borrow from distributional se-
mantics, while only mildly successful for now, has
the advantage of being very simple both in its con-
struction and application, and in the assumptions
that it makes.
Also notable is that we are putting under the
same umbrella tasks that have been traditionally
tackled separately. For example, among the ef-
fects present in the dataset, we can find both word
sense disambiguation (see discussion at the end of
Section 2) and what Murphy (2002) calls “knowl-
edge effects” (e.g., a plane makes a very good ma-
chine, but a paper plane doesn’t). Moreover, these
effects can also interact (people know that a hu-
man egg is actually a single, small cell, and hence
not even cannibals would consider it satisfactory
food). We can thus explore the empirical ques-
tion of whether all these related phenomena can
be tackled together, with a single model account-
ing for all of them.
In conclusion, the challenge that we intro-
duced brings together concept combination and
non-subsective modification phenomena studied
in psychology and theoretical linguistics, and tries
to handle them with the standard machinery of
computational linguistics. This challenge has
proved quite difficult for current tools, but this is
exactly what we expected in the first place. Our
goal, from the outset, was to create a task that
could help us delimiting the boundaries of com-
putational methods for characterizing human con-
cepts, while delimiting, at the same time, the no-
tion of human concepts itself.
</bodyText>
<sectionHeader confidence="0.998636" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9944285">
We acknowledge ERC 2011 Starting Independent
Research Grant n. 283554 (COMPOSES).
</bodyText>
<sectionHeader confidence="0.998847" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997261">
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entail-
ment methods. Journal of Artificial Intelligence Re-
search, 38:135–187.
Marco Baroni and Alessandro Lenci. 2011. How
we BLESSed distributional semantic evaluation. In
Proceedings of the EMNLP GEMS Workshop, pages
1–10, Edinburgh, UK.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183–1193, Boston,
MA.
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,
and Chung-Chieh Shan. 2012. Entailment above
</reference>
<page confidence="0.986721">
179
</page>
<reference confidence="0.9997075">
the word level in distributional semantics. In Pro-
ceedings of EACL, pages 23–32, Avignon, France.
Marco Baroni. 2013. Composition in distributional
semantics. Language and Linguistics Compass,
7(10):511–522.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of EMNLP, pages
546–556, Jeju Island, Korea.
Gemma Boleda, Eva Maria Vecchi, Miquel Cornudella,
and Louise McNally. 2012. First order vs. higher
order modification in distributional semantics. In
Proceedings of EMNLP, pages 1223–1233, Jeju Is-
land, Korea.
Gemma Boleda, Marco Baroni, Louise McNally, and
Nghia The Pham. 2013. Intensionality was only
alleged: On adjective-noun composition in distribu-
tional semantics. In Proceedings of IWCS, pages
35–46, Potsdam, Germany.
Graham Chapman. 1989. The complete Monty
Python’s flying circus : all the words. Pantheon
Books, New York.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. Linguis-
tic Analysis, 36:345–384.
Benjamin Cohen and Gregory L Murphy. 1984. Mod-
els of concepts. Cognitive Science, 8(1):27–58.
Louise Connell and Michael Ramscar. 2001. Using
distributional measures to model typicality in cate-
gorization. In Proceedings of CogSci, pages 226–
231, Edinburgh, UK.
Andrew Connolly, Jerry Fodor, Lila Gleitman, and
Henry Gleitman. 2007. Why stereotypes don’t even
make good defaults. Cognition, 103(1):1–22.
Ann Copestake and Ted Briscoe. 1995. Semi-
productive polysemy and sense extension. Journal
of Semantics, 12:15–67.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2009. Recognizing textual entailment: ratio-
nale, evaluation and approaches. Natural Language
Engineering, 15:459–476.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013. General estimation and evaluation of com-
positional distributional semantic models. In Pro-
ceedings of ACL Workshop on Continuous Vector
Space Models and their Compositionality, pages 50–
58, Sofia, Bulgaria.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass, 6(10):635–653.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Ph.D dissertation, Stuttgart University.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Gene Golub and Charles Van Loan. 1996. Matrix
Computations (3rd ed.). JHU Press, Baltimore, MD.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33–37,
Uppsala, Sweden.
James Hampton. 1991. The combination of prototype
concepts. In Paula Schwanenflugel, editor, The psy-
chology of word meanings, pages 91–116. Erlbaum,
Hillsdale, NJ.
Charles Kalish. 1995. Essentialism and graded mem-
bership in animal and artifact categories. Memory
and Cognition, 23(3):335–353.
Adam Kilgarriff. 1997. I don’t believe in word senses.
Computers and the Humanities, 31:91–113.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359–389.
Daniel Lee and Sebastian Seung. 2000. Algorithms for
Non-negative Matrix Factorization. In Proceedings
of NIPS, pages 556–562.
Alessandro Lenci and Giulia Benotto. 2012. Identi-
fying hypernyms in distributional semantic spaces.
In Proceedings of *SEM, pages 75–79, Montreal,
Canada.
Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language Resources
and Evaluation, 43(2):139–159.
Louise McNally. 2013. Modification. In Maria Aloni
and Paul Dekker, editors, Cambridge Handbook of
Semantics. Cambridge University Press, Cambridge,
UK. In press.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388–1429.
Gregory Murphy. 2002. The Big Book of Concepts.
MIT Press, Cambridge, MA.
Massimo Poesio, Simone Ponzetto, and Yan-
nick Versley. 2010. Computational models
of anaphora resolution: A survey. http:
//clic.cimec.unitn.it/massimo/
Publications/lilt.pdf.
Edward Smith and Daniel Osherson. 1984. Concep-
tual combination with prototype concepts. Cogni-
tive Science, 8(4):337–361.
Edward E Smith, Daniel N Osherson, Lance J Rips,
and Margaret Keane. 1988. Combining prototypes:
A selective modification model. Cognitive Science,
12(4):485–527.
</reference>
<page confidence="0.973197">
180
</page>
<reference confidence="0.999797">
Robert Speer and Catherine Havasi. 2013. Con-
ceptNet 5: A large semantic network for relational
knowledge. In Iryna Gurevych and Jungi Kim, edi-
tors, The People’s Web Meets NLP, pages 161–176.
Springer, Berlin.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of COLING, pages 1015–
1021, Geneva, Switzerland.
Edward Wisniewski. 1997. When concepts combine.
Psychonomic Bulletin &amp; Review, 4(2):167–183.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca
Falucchi, and Suresh Manandhar. 2010. Estimat-
ing linear models for compositional distributional
semantics. In Proceedings of COLING, pages 1263–
1271, Beijing, China.
</reference>
<page confidence="0.998384">
181
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.867059">
<title confidence="0.9343675">Dead parrots make bad Exploring modifier effects in noun phrases</title>
<author confidence="0.999989">Kruszewski Baroni</author>
<affiliation confidence="0.999786">Center for Mind/Brain Sciences (University of Trento,</affiliation>
<email confidence="0.998414">(german.kruszewski|marco.baroni)@unitn.it</email>
<abstract confidence="0.999782136363636">Sometimes modifiers have a strong effect on core aspects of the meaning of the they are attached to: A desirable pet, but a parrot at the very least, a rather unusual household companion. In order to stimulate computational research into the impact of modification on phrase meaning, we collected and made available a large dataset containing subject ratings for a variety of noun phrases and the categories they might beto. We propose to use semantics model these data, experimenting with numerous distributional semantic spaces, phrase composition methods and asymmetric similarity measures. Our models capture a statistically significant portion of the data, although much work is still needed before we achieve a full computational account of modification effects.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ion Androutsopoulos</author>
<author>Prodromos Malakasiotis</author>
</authors>
<title>A survey of paraphrasing and textual entailment methods.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>38--135</pages>
<contexts>
<context position="3731" citStr="Androutsopoulos and Malakasiotis, 2010" startWordPosition="586" endWordPosition="590">se meaning is not only very interesting from a linguistic and cognitive perspective, but also important from a practical point of view, as it might affect expected entailment patterns: If parrot entails pet, then lively parrot also entails pet. However, as we saw above, dead parrot doesn’t necessarily entail pet (at least not from the point of view of a disgruntled costumer who was just sold the corpse). Being able to track the impact that modifiers have on heads should thus have a positive effect on important tasks such as recognizing textual entailment, paraphrasing and anaphora resolution (Androutsopoulos and Malakasiotis, 2010; Dagan et 171 Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 171–181, Dublin, Ireland, August 23-24 2014. al., 2009; Poesio et al., 2010). Despite their theoretical and practical import, modification effects have been largely overlooked in computational linguistics, with the notable exception of Boleda et al. (2012; 2013), who only focused on the extreme case of intensional adjectives, studied a limited number of modifiers, and did not attempt to capture the graded nature of modification (a dead parrot is not a prototypical animal, but a to</context>
</contexts>
<marker>Androutsopoulos, Malakasiotis, 2010</marker>
<rawString>Ion Androutsopoulos and Prodromos Malakasiotis. 2010. A survey of paraphrasing and textual entailment methods. Journal of Artificial Intelligence Research, 38:135–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>How we BLESSed distributional semantic evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP GEMS Workshop,</booktitle>
<pages>1--10</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="10369" citStr="Baroni and Lenci, 2011" startWordPosition="1700" endWordPosition="1703">hly comparable tuples that do not feature such effects. An alternative approach would have been to rate phrases that were randomly selected from a corpus. This would have led to a dataset reflecting a more realistic distribution of modification effects, but it would not have guaranteed, for the same number of pairs, a fair amount of distorted tuples and comparable controls. We leave the study of the natural distribution of modification strength in text to further work. To find inspiration for the tuples, we looked into various databases containing concepts organized by category, namely BLESS (Baroni and Lenci, 2011), ConceptNet (Speer and Havasi, 2013) and WordNet (Fellbaum, 1998). We insured that all words in our tuples occurred at least 200 times in the large corpus we describe below (phrases were not filtered by frequency, due to data sparseness). Finally, when looking for tuples matching the distorted ones, we made sure that the mh phrases in the new tuples have similar Pointwise Mutual Information to the corresponding phrases in the distorted tuple (or, where the latter were not attested in the corpus, similar m and h frequencies). Finding meaningful combinations among unattested or infrequent phras</context>
</contexts>
<marker>Baroni, Lenci, 2011</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2011. How we BLESSed distributional semantic evaluation. In Proceedings of the EMNLP GEMS Workshop, pages 1–10, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1183--1193</pages>
<location>Boston, MA.</location>
<contexts>
<context position="20445" citStr="Baroni and Zamparelli (2010)" startWordPosition="3376" endWordPosition="3379">n our experiments, we stretch the head vector in the direction of the modifier (i.e., a� is the modifier, b� is the head). In the multiplicative model (mult), vectors are combined by component-wise multiplication, such that each phrase component pi is given by: pi = aibi. Guevara (2010) and Zanzotto et al. (2010) propose a full form of the additive model (fulladd), where the two constituent vectors are multiplied by weight matrices before being added, so that each phrase component is a weighted sum of all constituent components: p� = W1 + W2 b. Finally, the lexical function (lexfunc) model of Baroni and Zamparelli (2010) and Coecke et al. (2010) takes inspiration from formal semantics to characterize composition as function application. In particular, in modifier-head phrases, the modifier is treated as a linear function operating on the head vector. Given that linear functions can be expressed by matrices and their application by matrix-by-vector multiplication, the modifier is represented by a matrix A to be multiplied with the modifier vector b, so that: p� = Ab. We use the DISSECT toolkit7 to estimate the parameters of the composition methods and derive phrase vectors. In particular, DISSECT finds optimal</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of EMNLP, pages 1183–1193, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Raffaella Bernardi</author>
<author>Ngoc-Quynh Do</author>
<author>Chung-Chieh Shan</author>
</authors>
<title>Entailment above the word level in distributional semantics.</title>
<date>2012</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>23--32</pages>
<location>Avignon, France.</location>
<contexts>
<context position="5856" citStr="Baroni et al. (2012)" startWordPosition="938" endWordPosition="941">stigating. In particular, we look at the compositional extension of distributional semantics (Baroni, 2013), because we need representations not only for words, but also phrases, and we adopt the asymmetric similarity measures developed in the literature on lexical entailment (Kotlerman et al., 2010; Lenci and Benotto, 2012), because we are interested in an asymmetric relation (to what extent the concept denoted by the phrase is a good instance of the target class, and not vice versa). As far as we know, this is the first time these asymmetric measures are applied to composed representations (Baroni et al. (2012) experimented with entailment measures applied to phrase representations directly harvested from corpora, and not derived compositionally). We are thus also providing a novel evaluation of compositional models and asymmetric measures on a challenging task where they could potentially be very useful.2 2Connell and Ramscar (2001) showed good correlation of similarity scores produced by the LSA distributional semantic model with human category typicality judgments, however they did not consider phrases nor adopted an asymmetric measure to take directionality into account. 2 The Norwegian Blue Par</context>
<context position="22549" citStr="Baroni et al. (2012)" startWordPosition="3713" endWordPosition="3716">ership and typicality we recorded in NBP.10 In what follows, we use wx(f) to denote the weight (value) of feature (dimension) f in the distributional vector of term x. Fx denotes the set of features (dimensions) in the vector of x such that wx(f) &gt; t, where t is a predefined threshold to decide whether a feature is active.11 Importantly, 7http://clic.cimec.unitn.it/composes/ toolkit/ 8We speak of “instance-class relations” in a very broad and loose sense, to encompass classic relations such as hyponymy but also the fuzzier notion of lexical entailment. 9SVM classifiers have also been shown by Baroni et al. (2012) to be well-suited for entailment detection, but they do not naturally return continuous scores. 10Subjects had to answer a yes/no question concerning class membership, but by averaging their response we derive continuous membership scores. 11The obvious choice for t is 0. However, when working with the low-rank spaces described in Section 3.3 below, we set t to 0.1, since after SVD/NMF smoothing we observe 175 all measures assume non-negative values. Most asymmetric measures proposed in the literature build upon the distributional inclusion hypothesis, stating that “if u is a semantically nar</context>
</contexts>
<marker>Baroni, Bernardi, Do, Shan, 2012</marker>
<rawString>Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, and Chung-Chieh Shan. 2012. Entailment above the word level in distributional semantics. In Proceedings of EACL, pages 23–32, Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
</authors>
<title>Composition in distributional semantics.</title>
<date>2013</date>
<journal>Language and Linguistics Compass,</journal>
<volume>7</volume>
<issue>10</issue>
<contexts>
<context position="5343" citStr="Baroni, 2013" startWordPosition="853" endWordPosition="854">rrot still a parrot?), to what extent it is a member of one of the larger categories the head belongs to (is it still a pet?), and typicality ratings for the same questions (how typical is a dead parrot as a parrot? and as a pet?). Second, we present a first attempt to model the collected judgments computationally. We choose distributional semantics (Erk, 2012) as our frame of reference, as it produces continuous similarity scores, in line with the graded nature of the modification effects we are investigating. In particular, we look at the compositional extension of distributional semantics (Baroni, 2013), because we need representations not only for words, but also phrases, and we adopt the asymmetric similarity measures developed in the literature on lexical entailment (Kotlerman et al., 2010; Lenci and Benotto, 2012), because we are interested in an asymmetric relation (to what extent the concept denoted by the phrase is a good instance of the target class, and not vice versa). As far as we know, this is the first time these asymmetric measures are applied to composed representations (Baroni et al. (2012) experimented with entailment measures applied to phrase representations directly harve</context>
</contexts>
<marker>Baroni, 2013</marker>
<rawString>Marco Baroni. 2013. Composition in distributional semantics. Language and Linguistics Compass, 7(10):511–522.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Blacoe</author>
<author>Mirella Lapata</author>
</authors>
<title>A comparison of vector-based representations for semantic composition.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>546--556</pages>
<location>Jeju Island,</location>
<contexts>
<context position="30612" citStr="Blacoe and Lapata (2012)" startWordPosition="5052" endWordPosition="5055">re is then given by the weighted proportion of active features in mh that are also active in c. Therefore, the more the modifier shares features with the parent category, the higher weedsprec will be. This might explain why weedsprec is a good fit for the mult model in measuring degrees of category typicality. Looking at composition methods, there is no evidence that the more complex, matrix-based fulladd and lexfunc approaches are performing any better than the simple multiplicative and additive methods. Indeed, mult shows the most consistent overall performance, confirming the conclusion of Blacoe and Lapata (2012) that, at the present time, when it comes to composition, “simpler is better”. A related point emerges from the comparison of the low- and full-rank results for mult and wadd. The smoothing process due to dimensionality reduction is quite disruptive for the current asymmetric measures, that are based on feature inclusion. This is a further reason to stick to simpler composition methods, that can be applied directly in the full-rank spaces. Regarding the measures themselves, we see that cosweeds, that balances weedsprec with the classic cosine score, is the most robust, returning good 177 clark</context>
</contexts>
<marker>Blacoe, Lapata, 2012</marker>
<rawString>William Blacoe and Mirella Lapata. 2012. A comparison of vector-based representations for semantic composition. In Proceedings of EMNLP, pages 546–556, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gemma Boleda</author>
<author>Eva Maria Vecchi</author>
<author>Miquel Cornudella</author>
<author>Louise McNally</author>
</authors>
<title>First order vs. higher order modification in distributional semantics.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1223--1233</pages>
<location>Jeju Island,</location>
<contexts>
<context position="4101" citStr="Boleda et al. (2012" startWordPosition="641" endWordPosition="644"> sold the corpse). Being able to track the impact that modifiers have on heads should thus have a positive effect on important tasks such as recognizing textual entailment, paraphrasing and anaphora resolution (Androutsopoulos and Malakasiotis, 2010; Dagan et 171 Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 171–181, Dublin, Ireland, August 23-24 2014. al., 2009; Poesio et al., 2010). Despite their theoretical and practical import, modification effects have been largely overlooked in computational linguistics, with the notable exception of Boleda et al. (2012; 2013), who only focused on the extreme case of intensional adjectives, studied a limited number of modifiers, and did not attempt to capture the graded nature of modification (a dead parrot is not a prototypical animal, but a toy parrot is not an animal at all). This paper aims to stimulate computational research into modifier effects on phrase meaning in two ways. First, we introduce a new, large, publicly available data set of modifier-head phrases annotated with four kinds of modification-related subject ratings: whether the concept denoted by the phrase is an instance of the concept deno</context>
</contexts>
<marker>Boleda, Vecchi, Cornudella, McNally, 2012</marker>
<rawString>Gemma Boleda, Eva Maria Vecchi, Miquel Cornudella, and Louise McNally. 2012. First order vs. higher order modification in distributional semantics. In Proceedings of EMNLP, pages 1223–1233, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gemma Boleda</author>
<author>Marco Baroni</author>
<author>Louise McNally</author>
<author>Nghia The Pham</author>
</authors>
<title>Intensionality was only alleged: On adjective-noun composition in distributional semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of IWCS,</booktitle>
<pages>35--46</pages>
<location>Potsdam, Germany.</location>
<marker>Boleda, Baroni, McNally, Pham, 2013</marker>
<rawString>Gemma Boleda, Marco Baroni, Louise McNally, and Nghia The Pham. 2013. Intensionality was only alleged: On adjective-noun composition in distributional semantics. In Proceedings of IWCS, pages 35–46, Potsdam, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Chapman</author>
</authors>
<title>The complete Monty Python’s flying circus : all the words. Pantheon Books,</title>
<date>1989</date>
<location>New York.</location>
<marker>Chapman, 1989</marker>
<rawString>Graham Chapman. 1989. The complete Monty Python’s flying circus : all the words. Pantheon Books, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Clark</author>
</authors>
<title>Mathematical foundations for a compositional distributional model of meaning. Linguistic Analysis,</title>
<date>2010</date>
<pages>36--345</pages>
<contexts>
<context position="20470" citStr="Coecke et al. (2010)" startWordPosition="3381" endWordPosition="3384"> head vector in the direction of the modifier (i.e., a� is the modifier, b� is the head). In the multiplicative model (mult), vectors are combined by component-wise multiplication, such that each phrase component pi is given by: pi = aibi. Guevara (2010) and Zanzotto et al. (2010) propose a full form of the additive model (fulladd), where the two constituent vectors are multiplied by weight matrices before being added, so that each phrase component is a weighted sum of all constituent components: p� = W1 + W2 b. Finally, the lexical function (lexfunc) model of Baroni and Zamparelli (2010) and Coecke et al. (2010) takes inspiration from formal semantics to characterize composition as function application. In particular, in modifier-head phrases, the modifier is treated as a linear function operating on the head vector. Given that linear functions can be expressed by matrices and their application by matrix-by-vector multiplication, the modifier is represented by a matrix A to be multiplied with the modifier vector b, so that: p� = Ab. We use the DISSECT toolkit7 to estimate the parameters of the composition methods and derive phrase vectors. In particular, DISSECT finds optimal parameter settings by le</context>
</contexts>
<marker>Coecke, Sadrzadeh, Clark, 2010</marker>
<rawString>Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. 2010. Mathematical foundations for a compositional distributional model of meaning. Linguistic Analysis, 36:345–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Cohen</author>
<author>Gregory L Murphy</author>
</authors>
<title>Models of concepts.</title>
<date>1984</date>
<journal>Cognitive Science,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="37457" citStr="Cohen and Murphy (1984)" startWordPosition="6240" endWordPosition="6243"> testbed for compositional distributional semantics. In the interaction between compositional models and directional similarity measures, we have observed that simpler models yield better results. Specifically, mult and wadd are economical composition models than can be applied on full-rank spaces, which in turn work best with our similarity measures. Psychologists studying modification effects in concept combination have proposed models that are usually quite complex, relying on hand-crafted feature definitions and making very strong assumptions about the combination process (see for example Cohen and Murphy (1984), Smith et al. (1988)). Some of these assumptions have led other researchers to argue that prototypes do not compose at all (Connolly et al., 2007). In contrast, the approach we borrow from distributional semantics, while only mildly successful for now, has the advantage of being very simple both in its construction and application, and in the assumptions that it makes. Also notable is that we are putting under the same umbrella tasks that have been traditionally tackled separately. For example, among the effects present in the dataset, we can find both word sense disambiguation (see discussio</context>
</contexts>
<marker>Cohen, Murphy, 1984</marker>
<rawString>Benjamin Cohen and Gregory L Murphy. 1984. Models of concepts. Cognitive Science, 8(1):27–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Louise Connell</author>
<author>Michael Ramscar</author>
</authors>
<title>Using distributional measures to model typicality in categorization.</title>
<date>2001</date>
<booktitle>In Proceedings of CogSci,</booktitle>
<pages>226--231</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="6185" citStr="Connell and Ramscar (2001)" startWordPosition="986" endWordPosition="989">12), because we are interested in an asymmetric relation (to what extent the concept denoted by the phrase is a good instance of the target class, and not vice versa). As far as we know, this is the first time these asymmetric measures are applied to composed representations (Baroni et al. (2012) experimented with entailment measures applied to phrase representations directly harvested from corpora, and not derived compositionally). We are thus also providing a novel evaluation of compositional models and asymmetric measures on a challenging task where they could potentially be very useful.2 2Connell and Ramscar (2001) showed good correlation of similarity scores produced by the LSA distributional semantic model with human category typicality judgments, however they did not consider phrases nor adopted an asymmetric measure to take directionality into account. 2 The Norwegian Blue Parrot data set We introduce Norwegian Blue Parrot (NBP),3 a new, large data set to explore modification effects. Given a head noun h and a modifier adjective or noun m, NBP contains average membership and typicality ratings for the phrase mh both as an instance of h and as an instance of c (a broader category h belongs to). As a </context>
</contexts>
<marker>Connell, Ramscar, 2001</marker>
<rawString>Louise Connell and Michael Ramscar. 2001. Using distributional measures to model typicality in categorization. In Proceedings of CogSci, pages 226– 231, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Connolly</author>
<author>Jerry Fodor</author>
<author>Lila Gleitman</author>
<author>Henry Gleitman</author>
</authors>
<title>Why stereotypes don’t even make good defaults.</title>
<date>2007</date>
<journal>Cognition,</journal>
<volume>103</volume>
<issue>1</issue>
<contexts>
<context position="3057" citStr="Connolly et al., 2007" startWordPosition="475" endWordPosition="478">s line of research has highlighted how strong modification effects might be the rule, rather than the exception: Wisniewski (1997) reports that, when subjects were asked to provide the meaning for more than 200 novel modifierhead constructions, “70% [of the answers] involved the construal of a noun’s referent as something other than the typical category named by the noun [head].” Indeed, recent research suggests that even the most stereotypical modifiers affect prototypicality, so that subjects are less willing to attribute to quacking ducks such obvious duck properties as having webbed feet (Connolly et al., 2007). The impact of modification on phrase meaning is not only very interesting from a linguistic and cognitive perspective, but also important from a practical point of view, as it might affect expected entailment patterns: If parrot entails pet, then lively parrot also entails pet. However, as we saw above, dead parrot doesn’t necessarily entail pet (at least not from the point of view of a disgruntled costumer who was just sold the corpse). Being able to track the impact that modifiers have on heads should thus have a positive effect on important tasks such as recognizing textual entailment, pa</context>
<context position="37604" citStr="Connolly et al., 2007" startWordPosition="6266" endWordPosition="6269">served that simpler models yield better results. Specifically, mult and wadd are economical composition models than can be applied on full-rank spaces, which in turn work best with our similarity measures. Psychologists studying modification effects in concept combination have proposed models that are usually quite complex, relying on hand-crafted feature definitions and making very strong assumptions about the combination process (see for example Cohen and Murphy (1984), Smith et al. (1988)). Some of these assumptions have led other researchers to argue that prototypes do not compose at all (Connolly et al., 2007). In contrast, the approach we borrow from distributional semantics, while only mildly successful for now, has the advantage of being very simple both in its construction and application, and in the assumptions that it makes. Also notable is that we are putting under the same umbrella tasks that have been traditionally tackled separately. For example, among the effects present in the dataset, we can find both word sense disambiguation (see discussion at the end of Section 2) and what Murphy (2002) calls “knowledge effects” (e.g., a plane makes a very good machine, but a paper plane doesn’t). M</context>
</contexts>
<marker>Connolly, Fodor, Gleitman, Gleitman, 2007</marker>
<rawString>Andrew Connolly, Jerry Fodor, Lila Gleitman, and Henry Gleitman. 2007. Why stereotypes don’t even make good defaults. Cognition, 103(1):1–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Ted Briscoe</author>
</authors>
<title>Semiproductive polysemy and sense extension.</title>
<date>1995</date>
<journal>Journal of Semantics,</journal>
<pages>12--15</pages>
<contexts>
<context position="19036" citStr="Copestake and Briscoe, 1995" startWordPosition="3133" endWordPosition="3136">t disambiguating contexts for the nouns. Thus, we think that it is more productive for computational systems to handle modifier-triggered disambiguation as a special case of the more general class of modification effects, than to engage in the quixotic pursuit to determine, a priori, what’s the boundary between a word-sense and a “pure” modification effect. Note in Table 2 that grilled trout was unanimously rated by subjects as an instance of the creature category, despite the fact that the cooking-related grilled modifier cues a classic shift from an animal (and thus creature) sense to food (Copestake and Briscoe, 1995). Examples like this suggest that our agnosticism is warranted. 3 Methods 3.1 Composition models We experiment with many ways to derive a phrase vector by combining the vectors of its constituents. Mitchell and Lapata (2010) proposed a set of simple models in which each component of the phrase vector is a function of the corresponding components of the constituent vectors. Given vectors a� and b, the weighted additive model (wadd) returns their weighted sum: p� = w1d + w2 b. In the dilation model (dil), the output vector is obtained by decomposing one of the input vectors, say b, into a vector</context>
</contexts>
<marker>Copestake, Briscoe, 1995</marker>
<rawString>Ann Copestake and Ted Briscoe. 1995. Semiproductive polysemy and sense extension. Journal of Semantics, 12:15–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
<author>Bernardo Magnini</author>
<author>Dan Roth</author>
</authors>
<title>Recognizing textual entailment: rationale, evaluation and approaches.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<pages>15--459</pages>
<marker>Dagan, Dolan, Magnini, Roth, 2009</marker>
<rawString>Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. 2009. Recognizing textual entailment: rationale, evaluation and approaches. Natural Language Engineering, 15:459–476.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Nghia The Pham</author>
<author>Marco Baroni</author>
</authors>
<title>General estimation and evaluation of compositional distributional semantic models.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL Workshop on Continuous Vector Space Models and their Compositionality,</booktitle>
<pages>50--58</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="21178" citStr="Dinu et al., 2013" startWordPosition="3488" endWordPosition="3491">on. In particular, in modifier-head phrases, the modifier is treated as a linear function operating on the head vector. Given that linear functions can be expressed by matrices and their application by matrix-by-vector multiplication, the modifier is represented by a matrix A to be multiplied with the modifier vector b, so that: p� = Ab. We use the DISSECT toolkit7 to estimate the parameters of the composition methods and derive phrase vectors. In particular, DISSECT finds optimal parameter settings by learning to approximate corpus-extracted phrase vector examples with least-squares methods (Dinu et al., 2013). We use as training examples all the modifier-head phrases that contain a modifier of interest and occur at least 50 times in our source corpus (see Section 3.3 below). 3.2 Asymmetric similarity measures Several measures to identify word pairs that stand in an instance-class relationship by comparing their vectors have been proposed in the recent distributional semantics literature (Kotlerman et al., 2010; Lenci and Benotto, 2012; Weeds et al., 2004).8 While the task of deciding if u is in class v is typically framed (also by distributional semanticists) in binary, yes-or-no terms, all propos</context>
</contexts>
<marker>Dinu, Pham, Baroni, 2013</marker>
<rawString>Georgiana Dinu, Nghia The Pham, and Marco Baroni. 2013. General estimation and evaluation of compositional distributional semantic models. In Proceedings of ACL Workshop on Continuous Vector Space Models and their Compositionality, pages 50– 58, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>Vector space models of word meaning and phrase meaning: A survey.</title>
<date>2012</date>
<journal>Language and Linguistics Compass,</journal>
<volume>6</volume>
<issue>10</issue>
<contexts>
<context position="5093" citStr="Erk, 2012" startWordPosition="814" endWordPosition="815">troduce a new, large, publicly available data set of modifier-head phrases annotated with four kinds of modification-related subject ratings: whether the concept denoted by the phrase is an instance of the concept denoted by its head (is a dead parrot still a parrot?), to what extent it is a member of one of the larger categories the head belongs to (is it still a pet?), and typicality ratings for the same questions (how typical is a dead parrot as a parrot? and as a pet?). Second, we present a first attempt to model the collected judgments computationally. We choose distributional semantics (Erk, 2012) as our frame of reference, as it produces continuous similarity scores, in line with the graded nature of the modification effects we are investigating. In particular, we look at the compositional extension of distributional semantics (Baroni, 2013), because we need representations not only for words, but also phrases, and we adopt the asymmetric similarity measures developed in the literature on lexical entailment (Kotlerman et al., 2010; Lenci and Benotto, 2012), because we are interested in an asymmetric relation (to what extent the concept denoted by the phrase is a good instance of the t</context>
</contexts>
<marker>Erk, 2012</marker>
<rawString>Katrin Erk. 2012. Vector space models of word meaning and phrase meaning: A survey. Language and Linguistics Compass, 6(10):635–653.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Evert</author>
</authors>
<title>The Statistics of Word Cooccurrences.</title>
<date>2005</date>
<institution>Stuttgart University.</institution>
<note>Ph.D dissertation,</note>
<contexts>
<context position="24966" citStr="Evert, 2005" startWordPosition="4115" endWordPosition="4116">ow-frequency noise. 3.3 Distributional semantic spaces We extract co-occurrence information from a corpus of about 2.8 billion words obtained by concatenating ukWaC,12 Wikipedia13 and the British National Corpus.14 With DISSECT, we build cooccurrence vectors for the top 20K most frequent lemmas in the source corpus (plus any NBP term missing from this list). We treat the top 10K most frequent lemmas as context elements. We consider context windows of 2 and 20 words on the two sides of the targets. We weight the vectors by non-negative Pointwise Mutual Information and Local Mutual Information (Evert, 2005). We experiment with vectors in the resulting fullrank (10K-dimensional) semantic spaces as well as with vectors in spaces of ranks 100 and 300. Rank reduction is performed by applying the Singular Value Decomposition (Golub and Van Loan, 1996) or Non-negative Matrix Factorization (Lee and Seung, 2000). It is customary to represent the output of these operations directly in a dense lowdimensional space. However, the asymmetric similarity measures we use assume sparse vectors (or the “inclusion” criterion would be meaningless), so we project back the outcome of SVD and NMF to sparse 10K-dimensi</context>
</contexts>
<marker>Evert, 2005</marker>
<rawString>Stefan Evert. 2005. The Statistics of Word Cooccurrences. Ph.D dissertation, Stuttgart University.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gene Golub</author>
<author>Charles Van Loan</author>
</authors>
<date>1996</date>
<journal>Matrix Computations</journal>
<volume>3</volume>
<editor>ed.).</editor>
<publisher>JHU Press,</publisher>
<location>Baltimore, MD.</location>
<marker>Golub, Van Loan, 1996</marker>
<rawString>Gene Golub and Charles Van Loan. 1996. Matrix Computations (3rd ed.). JHU Press, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiliano Guevara</author>
</authors>
<title>A regression model of adjective-noun compositionality in distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of GEMS,</booktitle>
<pages>33--37</pages>
<location>Uppsala,</location>
<contexts>
<context position="20104" citStr="Guevara (2010)" startWordPosition="3320" endWordPosition="3321">ted sum: p� = w1d + w2 b. In the dilation model (dil), the output vector is obtained by decomposing one of the input vectors, say b, into a vector parallel to a� and its orthogonal counterpart, and then dilating only the parallel vector by a factor A before re-combining. The corresponding formula is: (d·d)b + (A − 1)(d·b)d. In our experiments, we stretch the head vector in the direction of the modifier (i.e., a� is the modifier, b� is the head). In the multiplicative model (mult), vectors are combined by component-wise multiplication, such that each phrase component pi is given by: pi = aibi. Guevara (2010) and Zanzotto et al. (2010) propose a full form of the additive model (fulladd), where the two constituent vectors are multiplied by weight matrices before being added, so that each phrase component is a weighted sum of all constituent components: p� = W1 + W2 b. Finally, the lexical function (lexfunc) model of Baroni and Zamparelli (2010) and Coecke et al. (2010) takes inspiration from formal semantics to characterize composition as function application. In particular, in modifier-head phrases, the modifier is treated as a linear function operating on the head vector. Given that linear functi</context>
</contexts>
<marker>Guevara, 2010</marker>
<rawString>Emiliano Guevara. 2010. A regression model of adjective-noun compositionality in distributional semantics. In Proceedings of GEMS, pages 33–37, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Hampton</author>
</authors>
<title>The combination of prototype concepts.</title>
<date>1991</date>
<booktitle>The psychology of word meanings,</booktitle>
<pages>91--116</pages>
<editor>In Paula Schwanenflugel, editor,</editor>
<publisher>Erlbaum,</publisher>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="13102" citStr="Hampton, 1991" startWordPosition="2155" endWordPosition="2156">d” items in crowd-sourcing parlance). These controls included highly prototypical pairs (dog —* animal), possibly with stereotypical modifiers (beautiful rose —* flower), and unrelated pairs (biology —* dance), also possibly under modification (popular magazine —* animal). We asked for binary rather than graded membership judgments because these are more in line with commonsense intuitions about category membership (we might naturally speak of sparrows being more typical birds than penguins, but it is strange to say that they are “more birds”). The standard view in the psychology of concepts (Hampton, 1991) is that membership judgments are the product of a hard threshold we impose on the typicality scale (x is not y if the typicality of x as y is below a certain, subject-dependent threshold), although under certain experimental conditions subjects can also conceptualize membership as a graded property (Kalish, 1995). Membership and typicality ratings, especially in borderline cases such as those we constructed, are the output of complex cognitive processes where large inter-subject differences are expected, 5There is a larger number of mh → c pairs because different tuples can lead to the same m</context>
</contexts>
<marker>Hampton, 1991</marker>
<rawString>James Hampton. 1991. The combination of prototype concepts. In Paula Schwanenflugel, editor, The psychology of word meanings, pages 91–116. Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Kalish</author>
</authors>
<title>Essentialism and graded membership in animal and artifact categories.</title>
<date>1995</date>
<journal>Memory and Cognition,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="13417" citStr="Kalish, 1995" startWordPosition="2207" endWordPosition="2208">bership judgments because these are more in line with commonsense intuitions about category membership (we might naturally speak of sparrows being more typical birds than penguins, but it is strange to say that they are “more birds”). The standard view in the psychology of concepts (Hampton, 1991) is that membership judgments are the product of a hard threshold we impose on the typicality scale (x is not y if the typicality of x as y is below a certain, subject-dependent threshold), although under certain experimental conditions subjects can also conceptualize membership as a graded property (Kalish, 1995). Membership and typicality ratings, especially in borderline cases such as those we constructed, are the output of complex cognitive processes where large inter-subject differences are expected, 5There is a larger number of mh → c pairs because different tuples can lead to the same mh → h or h → c combinations. 6http://crowdflower.com/ 173 measure mh → c mh → h h → c tot. memb. 0.84 (0.2) 0.97 (0.1) 0.88 (0.2) 0.89 (0.2) typ. 5.45 (1.1) 6.29 (0.6) 5.81 (1.0) 5.84 (1.0) Table 1: NBP summary statistics: Mean average ratings and their standard deviations across pairs, itemized by instance-class </context>
</contexts>
<marker>Kalish, 1995</marker>
<rawString>Charles Kalish. 1995. Essentialism and graded membership in animal and artifact categories. Memory and Cognition, 23(3):335–353.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>I don’t believe in word senses.</title>
<date>1997</date>
<booktitle>Computers and the Humanities,</booktitle>
<pages>31--91</pages>
<contexts>
<context position="18057" citStr="Kilgarriff, 1997" startWordPosition="2984" endWordPosition="2985"> to the modifier and the category pertaining to different word senses of the head term. One might argue, for example, that egg has a food sense and a reproductive vessel sense. The human modifier picks the second sense, and so, obviously, human eggs are judged as bad instances of food. While we see the point of this objection, we think it’s impossible to draw a clear-cut distinction between discrete word senses (even in the rather extreme egg case, the eggs we eat are reproductive vessels from a chicken point of view!). This has been long recognized in the linguistic and cognitive literature (Kilgarriff, 1997; Murphy, 2002), 174 and even by the computational word sense disambiguation community, that is currently addressing the continuous nature of polysemy by shifting to the lexical-substitution-in-context task (McCarthy and Navigli, 2009). Context provides fundamental cues to disambiguating polysemous words, and noun modifiers typically act as important disambiguating contexts for the nouns. Thus, we think that it is more productive for computational systems to handle modifier-triggered disambiguation as a special case of the more general class of modification effects, than to engage in the quixo</context>
</contexts>
<marker>Kilgarriff, 1997</marker>
<rawString>Adam Kilgarriff. 1997. I don’t believe in word senses. Computers and the Humanities, 31:91–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lili Kotlerman</author>
<author>Ido Dagan</author>
<author>Idan Szpektor</author>
<author>Maayan Zhitomirsky-Geffet</author>
</authors>
<title>Directional distributional similarity for lexical inference.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<volume>16</volume>
<issue>4</issue>
<contexts>
<context position="5536" citStr="Kotlerman et al., 2010" startWordPosition="882" endWordPosition="885">l is a dead parrot as a parrot? and as a pet?). Second, we present a first attempt to model the collected judgments computationally. We choose distributional semantics (Erk, 2012) as our frame of reference, as it produces continuous similarity scores, in line with the graded nature of the modification effects we are investigating. In particular, we look at the compositional extension of distributional semantics (Baroni, 2013), because we need representations not only for words, but also phrases, and we adopt the asymmetric similarity measures developed in the literature on lexical entailment (Kotlerman et al., 2010; Lenci and Benotto, 2012), because we are interested in an asymmetric relation (to what extent the concept denoted by the phrase is a good instance of the target class, and not vice versa). As far as we know, this is the first time these asymmetric measures are applied to composed representations (Baroni et al. (2012) experimented with entailment measures applied to phrase representations directly harvested from corpora, and not derived compositionally). We are thus also providing a novel evaluation of compositional models and asymmetric measures on a challenging task where they could potenti</context>
<context position="21587" citStr="Kotlerman et al., 2010" startWordPosition="3553" endWordPosition="3556">position methods and derive phrase vectors. In particular, DISSECT finds optimal parameter settings by learning to approximate corpus-extracted phrase vector examples with least-squares methods (Dinu et al., 2013). We use as training examples all the modifier-head phrases that contain a modifier of interest and occur at least 50 times in our source corpus (see Section 3.3 below). 3.2 Asymmetric similarity measures Several measures to identify word pairs that stand in an instance-class relationship by comparing their vectors have been proposed in the recent distributional semantics literature (Kotlerman et al., 2010; Lenci and Benotto, 2012; Weeds et al., 2004).8 While the task of deciding if u is in class v is typically framed (also by distributional semanticists) in binary, yes-or-no terms, all proposed measures return a continuous numerical score.9 Consequently, we conjecture that they might be wellsuited to capture the graded notions of class membership and typicality we recorded in NBP.10 In what follows, we use wx(f) to denote the weight (value) of feature (dimension) f in the distributional vector of term x. Fx denotes the set of features (dimensions) in the vector of x such that wx(f) &gt; t, where </context>
<context position="23858" citStr="Kotlerman et al. (2010)" startWordPosition="3926" endWordPosition="3929">s included in the feature vector of v as well” (Lenci and Benotto, 2012). In our terminology, u is the potential instance, and v is the class. We re-implement all the measures adopted by Lenci and Benotto, namely weedsprec, cosweeds, clarkede and invcl (see their paper for the original references): P weedsprec(u, v) = pinvcl(u, v) = clarkede(u, v) x (1 − clarkede(u, v)) The cosweeds formula combines weedsprec with the widely used symmetric cosine measure: P f∈Fu∩Fv wu(f) x wv(f) cosine(u, v) =qP qP f∈Fu wu(f)2 x f∈Fv wv(f)2 Finally, we experiment with the carefully crafted balapinc measure of Kotlerman et al. (2010): pbalapinc(u, v) = lin(u, v) · apinc(u, v) where the lin term is computed as follows: P f∈Fu∩Fv wu(f) + wv(f) lin(u, v) = Pf∈Fu wu(f) + Pf∈Fv wv(f) The balapinc score is the geometric average of a symmetric similarity measure (lin) and the strongly asymmetric apinc measure, that takes large values when dimensions with high values in the vector of the more specific term are also high in the vector of the more general term (refer to Kotlerman et al. (2010) for the apinc formula). widespread low-frequency noise. 3.3 Distributional semantic spaces We extract co-occurrence information from a corpu</context>
</contexts>
<marker>Kotlerman, Dagan, Szpektor, Zhitomirsky-Geffet, 2010</marker>
<rawString>Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2010. Directional distributional similarity for lexical inference. Natural Language Engineering, 16(4):359–389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Lee</author>
<author>Sebastian Seung</author>
</authors>
<title>Algorithms for Non-negative Matrix Factorization.</title>
<date>2000</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>556--562</pages>
<contexts>
<context position="25269" citStr="Lee and Seung, 2000" startWordPosition="4161" endWordPosition="4164"> in the source corpus (plus any NBP term missing from this list). We treat the top 10K most frequent lemmas as context elements. We consider context windows of 2 and 20 words on the two sides of the targets. We weight the vectors by non-negative Pointwise Mutual Information and Local Mutual Information (Evert, 2005). We experiment with vectors in the resulting fullrank (10K-dimensional) semantic spaces as well as with vectors in spaces of ranks 100 and 300. Rank reduction is performed by applying the Singular Value Decomposition (Golub and Van Loan, 1996) or Non-negative Matrix Factorization (Lee and Seung, 2000). It is customary to represent the output of these operations directly in a dense lowdimensional space. However, the asymmetric similarity measures we use assume sparse vectors (or the “inclusion” criterion would be meaningless), so we project back the outcome of SVD and NMF to sparse 10K-dimensional but low-rank spaces. In total, we explore 20 distinct semantic spaces. We also collect co-occurrence vectors for the phrases needed to estimate the composition method parameters (see Section 3.1 above). We use DISSECT’s “peripheral space” option to project the phrase raw count vectors into the var</context>
</contexts>
<marker>Lee, Seung, 2000</marker>
<rawString>Daniel Lee and Sebastian Seung. 2000. Algorithms for Non-negative Matrix Factorization. In Proceedings of NIPS, pages 556–562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Lenci</author>
<author>Giulia Benotto</author>
</authors>
<title>Identifying hypernyms in distributional semantic spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of *SEM,</booktitle>
<pages>75--79</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="5562" citStr="Lenci and Benotto, 2012" startWordPosition="886" endWordPosition="889">parrot? and as a pet?). Second, we present a first attempt to model the collected judgments computationally. We choose distributional semantics (Erk, 2012) as our frame of reference, as it produces continuous similarity scores, in line with the graded nature of the modification effects we are investigating. In particular, we look at the compositional extension of distributional semantics (Baroni, 2013), because we need representations not only for words, but also phrases, and we adopt the asymmetric similarity measures developed in the literature on lexical entailment (Kotlerman et al., 2010; Lenci and Benotto, 2012), because we are interested in an asymmetric relation (to what extent the concept denoted by the phrase is a good instance of the target class, and not vice versa). As far as we know, this is the first time these asymmetric measures are applied to composed representations (Baroni et al. (2012) experimented with entailment measures applied to phrase representations directly harvested from corpora, and not derived compositionally). We are thus also providing a novel evaluation of compositional models and asymmetric measures on a challenging task where they could potentially be very useful.2 2Con</context>
<context position="21612" citStr="Lenci and Benotto, 2012" startWordPosition="3557" endWordPosition="3560">ive phrase vectors. In particular, DISSECT finds optimal parameter settings by learning to approximate corpus-extracted phrase vector examples with least-squares methods (Dinu et al., 2013). We use as training examples all the modifier-head phrases that contain a modifier of interest and occur at least 50 times in our source corpus (see Section 3.3 below). 3.2 Asymmetric similarity measures Several measures to identify word pairs that stand in an instance-class relationship by comparing their vectors have been proposed in the recent distributional semantics literature (Kotlerman et al., 2010; Lenci and Benotto, 2012; Weeds et al., 2004).8 While the task of deciding if u is in class v is typically framed (also by distributional semanticists) in binary, yes-or-no terms, all proposed measures return a continuous numerical score.9 Consequently, we conjecture that they might be wellsuited to capture the graded notions of class membership and typicality we recorded in NBP.10 In what follows, we use wx(f) to denote the weight (value) of feature (dimension) f in the distributional vector of term x. Fx denotes the set of features (dimensions) in the vector of x such that wx(f) &gt; t, where t is a predefined thresho</context>
<context position="23307" citStr="Lenci and Benotto, 2012" startWordPosition="3835" endWordPosition="3838">stion concerning class membership, but by averaging their response we derive continuous membership scores. 11The obvious choice for t is 0. However, when working with the low-rank spaces described in Section 3.3 below, we set t to 0.1, since after SVD/NMF smoothing we observe 175 all measures assume non-negative values. Most asymmetric measures proposed in the literature build upon the distributional inclusion hypothesis, stating that “if u is a semantically narrower term than v, then a significant number of salient distributional features of u is included in the feature vector of v as well” (Lenci and Benotto, 2012). In our terminology, u is the potential instance, and v is the class. We re-implement all the measures adopted by Lenci and Benotto, namely weedsprec, cosweeds, clarkede and invcl (see their paper for the original references): P weedsprec(u, v) = pinvcl(u, v) = clarkede(u, v) x (1 − clarkede(u, v)) The cosweeds formula combines weedsprec with the widely used symmetric cosine measure: P f∈Fu∩Fv wu(f) x wv(f) cosine(u, v) =qP qP f∈Fu wu(f)2 x f∈Fv wv(f)2 Finally, we experiment with the carefully crafted balapinc measure of Kotlerman et al. (2010): pbalapinc(u, v) = lin(u, v) · apinc(u, v) where</context>
</contexts>
<marker>Lenci, Benotto, 2012</marker>
<rawString>Alessandro Lenci and Giulia Benotto. 2012. Identifying hypernyms in distributional semantic spaces. In Proceedings of *SEM, pages 75–79, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Roberto Navigli</author>
</authors>
<title>The English lexical substitution task.</title>
<date>2009</date>
<journal>Language Resources and Evaluation,</journal>
<volume>43</volume>
<issue>2</issue>
<contexts>
<context position="18292" citStr="McCarthy and Navigli, 2009" startWordPosition="3014" endWordPosition="3017">so, obviously, human eggs are judged as bad instances of food. While we see the point of this objection, we think it’s impossible to draw a clear-cut distinction between discrete word senses (even in the rather extreme egg case, the eggs we eat are reproductive vessels from a chicken point of view!). This has been long recognized in the linguistic and cognitive literature (Kilgarriff, 1997; Murphy, 2002), 174 and even by the computational word sense disambiguation community, that is currently addressing the continuous nature of polysemy by shifting to the lexical-substitution-in-context task (McCarthy and Navigli, 2009). Context provides fundamental cues to disambiguating polysemous words, and noun modifiers typically act as important disambiguating contexts for the nouns. Thus, we think that it is more productive for computational systems to handle modifier-triggered disambiguation as a special case of the more general class of modification effects, than to engage in the quixotic pursuit to determine, a priori, what’s the boundary between a word-sense and a “pure” modification effect. Note in Table 2 that grilled trout was unanimously rated by subjects as an instance of the creature category, despite the fa</context>
</contexts>
<marker>McCarthy, Navigli, 2009</marker>
<rawString>Diana McCarthy and Roberto Navigli. 2009. The English lexical substitution task. Language Resources and Evaluation, 43(2):139–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Louise McNally</author>
</authors>
<title>Modification. In Maria Aloni and Paul Dekker, editors, Cambridge Handbook of Semantics.</title>
<date>2013</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="2014" citStr="McNally (2013)" startWordPosition="309" endWordPosition="310">we restrict to right-headed adjective-noun and noun-noun constructions), modifiers are not simply picking a subset of the denotation of the head they modify, but they are often distorting the properties of the head in a radical manner. These modifier effects on phrase meaning have been studied extensively by theoretical linguists, 1http://en.wikipedia.org/wiki/Dead_ Parrot_sketch who have focused primarily on the extreme case of intensional modifiers such as fake, alleged and toy, where the phrase denotes something that is no longer (or is not necessarily) a head (a toy gun is not a gun). See McNally (2013) for a recent review of the linguistic literature. Cognitive scientists have looked at modification phenomena within the general study of conceptual combination (see Chapter 12 of Murphy (2002) for an extensive review). The cognitive tradition has focused on how modification affects prototypicality: a guppy is the prototypical pet fish, but it is neither a typical pet nor a typical fish (Smith and Osherson, 1984). This line of research has highlighted how strong modification effects might be the rule, rather than the exception: Wisniewski (1997) reports that, when subjects were asked to provid</context>
</contexts>
<marker>McNally, 2013</marker>
<rawString>Louise McNally. 2013. Modification. In Maria Aloni and Paul Dekker, editors, Cambridge Handbook of Semantics. Cambridge University Press, Cambridge, UK. In press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="19260" citStr="Mitchell and Lapata (2010)" startWordPosition="3168" endWordPosition="3171"> to engage in the quixotic pursuit to determine, a priori, what’s the boundary between a word-sense and a “pure” modification effect. Note in Table 2 that grilled trout was unanimously rated by subjects as an instance of the creature category, despite the fact that the cooking-related grilled modifier cues a classic shift from an animal (and thus creature) sense to food (Copestake and Briscoe, 1995). Examples like this suggest that our agnosticism is warranted. 3 Methods 3.1 Composition models We experiment with many ways to derive a phrase vector by combining the vectors of its constituents. Mitchell and Lapata (2010) proposed a set of simple models in which each component of the phrase vector is a function of the corresponding components of the constituent vectors. Given vectors a� and b, the weighted additive model (wadd) returns their weighted sum: p� = w1d + w2 b. In the dilation model (dil), the output vector is obtained by decomposing one of the input vectors, say b, into a vector parallel to a� and its orthogonal counterpart, and then dilating only the parallel vector by a factor A before re-combining. The corresponding formula is: (d·d)b + (A − 1)(d·b)d. In our experiments, we stretch the head vect</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Murphy</author>
</authors>
<title>The Big Book of Concepts.</title>
<date>2002</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2207" citStr="Murphy (2002)" startWordPosition="339" endWordPosition="340">operties of the head in a radical manner. These modifier effects on phrase meaning have been studied extensively by theoretical linguists, 1http://en.wikipedia.org/wiki/Dead_ Parrot_sketch who have focused primarily on the extreme case of intensional modifiers such as fake, alleged and toy, where the phrase denotes something that is no longer (or is not necessarily) a head (a toy gun is not a gun). See McNally (2013) for a recent review of the linguistic literature. Cognitive scientists have looked at modification phenomena within the general study of conceptual combination (see Chapter 12 of Murphy (2002) for an extensive review). The cognitive tradition has focused on how modification affects prototypicality: a guppy is the prototypical pet fish, but it is neither a typical pet nor a typical fish (Smith and Osherson, 1984). This line of research has highlighted how strong modification effects might be the rule, rather than the exception: Wisniewski (1997) reports that, when subjects were asked to provide the meaning for more than 200 novel modifierhead constructions, “70% [of the answers] involved the construal of a noun’s referent as something other than the typical category named by the nou</context>
<context position="18072" citStr="Murphy, 2002" startWordPosition="2986" endWordPosition="2987">nd the category pertaining to different word senses of the head term. One might argue, for example, that egg has a food sense and a reproductive vessel sense. The human modifier picks the second sense, and so, obviously, human eggs are judged as bad instances of food. While we see the point of this objection, we think it’s impossible to draw a clear-cut distinction between discrete word senses (even in the rather extreme egg case, the eggs we eat are reproductive vessels from a chicken point of view!). This has been long recognized in the linguistic and cognitive literature (Kilgarriff, 1997; Murphy, 2002), 174 and even by the computational word sense disambiguation community, that is currently addressing the continuous nature of polysemy by shifting to the lexical-substitution-in-context task (McCarthy and Navigli, 2009). Context provides fundamental cues to disambiguating polysemous words, and noun modifiers typically act as important disambiguating contexts for the nouns. Thus, we think that it is more productive for computational systems to handle modifier-triggered disambiguation as a special case of the more general class of modification effects, than to engage in the quixotic pursuit to </context>
<context position="38106" citStr="Murphy (2002)" startWordPosition="6352" endWordPosition="6353">e assumptions have led other researchers to argue that prototypes do not compose at all (Connolly et al., 2007). In contrast, the approach we borrow from distributional semantics, while only mildly successful for now, has the advantage of being very simple both in its construction and application, and in the assumptions that it makes. Also notable is that we are putting under the same umbrella tasks that have been traditionally tackled separately. For example, among the effects present in the dataset, we can find both word sense disambiguation (see discussion at the end of Section 2) and what Murphy (2002) calls “knowledge effects” (e.g., a plane makes a very good machine, but a paper plane doesn’t). Moreover, these effects can also interact (people know that a human egg is actually a single, small cell, and hence not even cannibals would consider it satisfactory food). We can thus explore the empirical question of whether all these related phenomena can be tackled together, with a single model accounting for all of them. In conclusion, the challenge that we introduced brings together concept combination and non-subsective modification phenomena studied in psychology and theoretical linguistics</context>
</contexts>
<marker>Murphy, 2002</marker>
<rawString>Gregory Murphy. 2002. The Big Book of Concepts. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Simone Ponzetto</author>
<author>Yannick Versley</author>
</authors>
<title>Computational models of anaphora resolution: A survey.</title>
<date>2010</date>
<note>http: //clic.cimec.unitn.it/massimo/ Publications/lilt.pdf.</note>
<contexts>
<context position="3922" citStr="Poesio et al., 2010" startWordPosition="616" endWordPosition="619">hen lively parrot also entails pet. However, as we saw above, dead parrot doesn’t necessarily entail pet (at least not from the point of view of a disgruntled costumer who was just sold the corpse). Being able to track the impact that modifiers have on heads should thus have a positive effect on important tasks such as recognizing textual entailment, paraphrasing and anaphora resolution (Androutsopoulos and Malakasiotis, 2010; Dagan et 171 Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 171–181, Dublin, Ireland, August 23-24 2014. al., 2009; Poesio et al., 2010). Despite their theoretical and practical import, modification effects have been largely overlooked in computational linguistics, with the notable exception of Boleda et al. (2012; 2013), who only focused on the extreme case of intensional adjectives, studied a limited number of modifiers, and did not attempt to capture the graded nature of modification (a dead parrot is not a prototypical animal, but a toy parrot is not an animal at all). This paper aims to stimulate computational research into modifier effects on phrase meaning in two ways. First, we introduce a new, large, publicly availabl</context>
</contexts>
<marker>Poesio, Ponzetto, Versley, 2010</marker>
<rawString>Massimo Poesio, Simone Ponzetto, and Yannick Versley. 2010. Computational models of anaphora resolution: A survey. http: //clic.cimec.unitn.it/massimo/ Publications/lilt.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Smith</author>
<author>Daniel Osherson</author>
</authors>
<title>Conceptual combination with prototype concepts.</title>
<date>1984</date>
<journal>Cognitive Science,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="2430" citStr="Smith and Osherson, 1984" startWordPosition="375" endWordPosition="379">rily on the extreme case of intensional modifiers such as fake, alleged and toy, where the phrase denotes something that is no longer (or is not necessarily) a head (a toy gun is not a gun). See McNally (2013) for a recent review of the linguistic literature. Cognitive scientists have looked at modification phenomena within the general study of conceptual combination (see Chapter 12 of Murphy (2002) for an extensive review). The cognitive tradition has focused on how modification affects prototypicality: a guppy is the prototypical pet fish, but it is neither a typical pet nor a typical fish (Smith and Osherson, 1984). This line of research has highlighted how strong modification effects might be the rule, rather than the exception: Wisniewski (1997) reports that, when subjects were asked to provide the meaning for more than 200 novel modifierhead constructions, “70% [of the answers] involved the construal of a noun’s referent as something other than the typical category named by the noun [head].” Indeed, recent research suggests that even the most stereotypical modifiers affect prototypicality, so that subjects are less willing to attribute to quacking ducks such obvious duck properties as having webbed f</context>
</contexts>
<marker>Smith, Osherson, 1984</marker>
<rawString>Edward Smith and Daniel Osherson. 1984. Conceptual combination with prototype concepts. Cognitive Science, 8(4):337–361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward E Smith</author>
<author>Daniel N Osherson</author>
<author>Lance J Rips</author>
<author>Margaret Keane</author>
</authors>
<title>Combining prototypes: A selective modification model.</title>
<date>1988</date>
<journal>Cognitive Science,</journal>
<volume>12</volume>
<issue>4</issue>
<contexts>
<context position="37478" citStr="Smith et al. (1988)" startWordPosition="6244" endWordPosition="6247">l distributional semantics. In the interaction between compositional models and directional similarity measures, we have observed that simpler models yield better results. Specifically, mult and wadd are economical composition models than can be applied on full-rank spaces, which in turn work best with our similarity measures. Psychologists studying modification effects in concept combination have proposed models that are usually quite complex, relying on hand-crafted feature definitions and making very strong assumptions about the combination process (see for example Cohen and Murphy (1984), Smith et al. (1988)). Some of these assumptions have led other researchers to argue that prototypes do not compose at all (Connolly et al., 2007). In contrast, the approach we borrow from distributional semantics, while only mildly successful for now, has the advantage of being very simple both in its construction and application, and in the assumptions that it makes. Also notable is that we are putting under the same umbrella tasks that have been traditionally tackled separately. For example, among the effects present in the dataset, we can find both word sense disambiguation (see discussion at the end of Secti</context>
</contexts>
<marker>Smith, Osherson, Rips, Keane, 1988</marker>
<rawString>Edward E Smith, Daniel N Osherson, Lance J Rips, and Margaret Keane. 1988. Combining prototypes: A selective modification model. Cognitive Science, 12(4):485–527.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Speer</author>
<author>Catherine Havasi</author>
</authors>
<title>ConceptNet 5: A large semantic network for relational knowledge.</title>
<date>2013</date>
<booktitle>In Iryna Gurevych</booktitle>
<pages>161--176</pages>
<editor>and Jungi Kim, editors,</editor>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="10406" citStr="Speer and Havasi, 2013" startWordPosition="1705" endWordPosition="1708">ture such effects. An alternative approach would have been to rate phrases that were randomly selected from a corpus. This would have led to a dataset reflecting a more realistic distribution of modification effects, but it would not have guaranteed, for the same number of pairs, a fair amount of distorted tuples and comparable controls. We leave the study of the natural distribution of modification strength in text to further work. To find inspiration for the tuples, we looked into various databases containing concepts organized by category, namely BLESS (Baroni and Lenci, 2011), ConceptNet (Speer and Havasi, 2013) and WordNet (Fellbaum, 1998). We insured that all words in our tuples occurred at least 200 times in the large corpus we describe below (phrases were not filtered by frequency, due to data sparseness). Finally, when looking for tuples matching the distorted ones, we made sure that the mh phrases in the new tuples have similar Pointwise Mutual Information to the corresponding phrases in the distorted tuple (or, where the latter were not attested in the corpus, similar m and h frequencies). Finding meaningful combinations among unattested or infrequent phrases was not an easy task and there was</context>
</contexts>
<marker>Speer, Havasi, 2013</marker>
<rawString>Robert Speer and Catherine Havasi. 2013. ConceptNet 5: A large semantic network for relational knowledge. In Iryna Gurevych and Jungi Kim, editors, The People’s Web Meets NLP, pages 161–176. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
<author>Diana McCarthy</author>
</authors>
<title>Characterising measures of lexical distributional similarity.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1015--1021</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="21633" citStr="Weeds et al., 2004" startWordPosition="3561" endWordPosition="3564">rticular, DISSECT finds optimal parameter settings by learning to approximate corpus-extracted phrase vector examples with least-squares methods (Dinu et al., 2013). We use as training examples all the modifier-head phrases that contain a modifier of interest and occur at least 50 times in our source corpus (see Section 3.3 below). 3.2 Asymmetric similarity measures Several measures to identify word pairs that stand in an instance-class relationship by comparing their vectors have been proposed in the recent distributional semantics literature (Kotlerman et al., 2010; Lenci and Benotto, 2012; Weeds et al., 2004).8 While the task of deciding if u is in class v is typically framed (also by distributional semanticists) in binary, yes-or-no terms, all proposed measures return a continuous numerical score.9 Consequently, we conjecture that they might be wellsuited to capture the graded notions of class membership and typicality we recorded in NBP.10 In what follows, we use wx(f) to denote the weight (value) of feature (dimension) f in the distributional vector of term x. Fx denotes the set of features (dimensions) in the vector of x such that wx(f) &gt; t, where t is a predefined threshold to decide whether </context>
</contexts>
<marker>Weeds, Weir, McCarthy, 2004</marker>
<rawString>Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarity. In Proceedings of COLING, pages 1015– 1021, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Wisniewski</author>
</authors>
<title>When concepts combine.</title>
<date>1997</date>
<journal>Psychonomic Bulletin &amp; Review,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="2565" citStr="Wisniewski (1997)" startWordPosition="398" endWordPosition="399">t necessarily) a head (a toy gun is not a gun). See McNally (2013) for a recent review of the linguistic literature. Cognitive scientists have looked at modification phenomena within the general study of conceptual combination (see Chapter 12 of Murphy (2002) for an extensive review). The cognitive tradition has focused on how modification affects prototypicality: a guppy is the prototypical pet fish, but it is neither a typical pet nor a typical fish (Smith and Osherson, 1984). This line of research has highlighted how strong modification effects might be the rule, rather than the exception: Wisniewski (1997) reports that, when subjects were asked to provide the meaning for more than 200 novel modifierhead constructions, “70% [of the answers] involved the construal of a noun’s referent as something other than the typical category named by the noun [head].” Indeed, recent research suggests that even the most stereotypical modifiers affect prototypicality, so that subjects are less willing to attribute to quacking ducks such obvious duck properties as having webbed feet (Connolly et al., 2007). The impact of modification on phrase meaning is not only very interesting from a linguistic and cognitive </context>
</contexts>
<marker>Wisniewski, 1997</marker>
<rawString>Edward Wisniewski. 1997. When concepts combine. Psychonomic Bulletin &amp; Review, 4(2):167–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Zanzotto</author>
<author>Ioannis Korkontzelos</author>
<author>Francesca Falucchi</author>
<author>Suresh Manandhar</author>
</authors>
<title>Estimating linear models for compositional distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1263--1271</pages>
<location>Beijing, China.</location>
<contexts>
<context position="20131" citStr="Zanzotto et al. (2010)" startWordPosition="3323" endWordPosition="3326"> w2 b. In the dilation model (dil), the output vector is obtained by decomposing one of the input vectors, say b, into a vector parallel to a� and its orthogonal counterpart, and then dilating only the parallel vector by a factor A before re-combining. The corresponding formula is: (d·d)b + (A − 1)(d·b)d. In our experiments, we stretch the head vector in the direction of the modifier (i.e., a� is the modifier, b� is the head). In the multiplicative model (mult), vectors are combined by component-wise multiplication, such that each phrase component pi is given by: pi = aibi. Guevara (2010) and Zanzotto et al. (2010) propose a full form of the additive model (fulladd), where the two constituent vectors are multiplied by weight matrices before being added, so that each phrase component is a weighted sum of all constituent components: p� = W1 + W2 b. Finally, the lexical function (lexfunc) model of Baroni and Zamparelli (2010) and Coecke et al. (2010) takes inspiration from formal semantics to characterize composition as function application. In particular, in modifier-head phrases, the modifier is treated as a linear function operating on the head vector. Given that linear functions can be expressed by mat</context>
</contexts>
<marker>Zanzotto, Korkontzelos, Falucchi, Manandhar, 2010</marker>
<rawString>Fabio Zanzotto, Ioannis Korkontzelos, Francesca Falucchi, and Suresh Manandhar. 2010. Estimating linear models for compositional distributional semantics. In Proceedings of COLING, pages 1263– 1271, Beijing, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>