<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000083">
<title confidence="0.9981195">
Improving Statistical Machine Translation Using
Domain Bilingual Multiword Expressions
</title>
<author confidence="0.999002">
Zhixiang Ren1 Yajuan L¨u1 Jie Cao1 Qun Liu1 Yun Huang2
</author>
<affiliation confidence="0.962378333333333">
1Key Lab. of Intelligent Info. Processing 2Department of Computer Science
Institute of Computing Technology School of Computing
Chinese Academy of Sciences National University of Singapore
</affiliation>
<address confidence="0.665382">
P.O. Box 2704, Beijing 100190, China Computing 1, Law Link, Singapore 117590
</address>
<email confidence="0.987845">
{renzhixiang,lvyajuan huangyun@comp.nus.edu.sg
caojie,liuqun}@ict.ac.cn
</email>
<sectionHeader confidence="0.993721" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999262714285714">
Multiword expressions (MWEs) have
been proved useful for many natural lan-
guage processing tasks. However, how to
use them to improve performance of statis-
tical machine translation (SMT) is not well
studied. This paper presents a simple yet
effective strategy to extract domain bilin-
gual multiword expressions. In addition,
we implement three methods to integrate
bilingual MWEs to Moses, the state-of-
the-art phrase-based machine translation
system. Experiments show that bilingual
MWEs could improve translation perfor-
mance significantly.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999809483870968">
Phrase-based machine translation model has been
proved a great improvement over the initial word-
based approaches (Brown et al., 1993). Recent
syntax-based models perform even better than
phrase-based models. However, when syntax-
based models are applied to new domain with few
syntax-annotated corpus, the translation perfor-
mance would decrease. To utilize the robustness
of phrases and make up the lack of syntax or se-
mantic information in phrase-based model for do-
main translation, we study domain bilingual mul-
tiword expressions and integrate them to the exist-
ing phrase-based model.
A multiword expression (MWE) can be consid-
ered as word sequence with relatively fixed struc-
ture representing special meanings. There is no
uniform definition of MWE, and many researchers
give different properties of MWE. Sag et al. (2002)
roughly defined MWE as “idiosyncratic interpre-
tations that cross word boundaries (or spaces)”.
Cruys and Moir´on (2007) focused on the non-
compositional property of MWE, i.e. the property
that whole expression cannot be derived from their
component words. Stanford university launched
a MWE project1, in which different qualities of
MWE were presented. For bilingual multiword
expression (BiMWE), we define a bilingual phrase
as a bilingual MWE if (1) the source phrase is a
MWE in source language; (2) the source phrase
and the target phrase must be translated to each
other exactly, i.e. there is no additional (boundary)
word in target phrase which cannot find the corre-
sponding word in source phrase, and vice versa.
In recent years, many useful methods have been
proposed to extract MWEs or BiMWEs automati-
cally (Piao et al., 2005; Bannard, 2007; Fazly and
Stevenson, 2006). Since MWE usually constrains
possible senses of a polysemous word in context,
they can be used in many NLP applications such
as information retrieval, question answering, word
sense disambiguation and so on.
For machine translation, Piao et al. (2005) have
noted that the issue of MWE identification and
accurate interpretation from source to target lan-
guage remained an unsolved problem for existing
MT systems. This problem is more severe when
MT systems are used to translate domain-specific
texts, since they may include technical terminol-
ogy as well as more general fixed expressions and
idioms. Although some MT systems may employ
a machine-readable bilingual dictionary of MWE,
it is time-consuming and inefficient to obtain this
resource manually. Therefore, some researchers
have tried to use automatically extracted bilingual
MWEs in SMT. Tanaka and Baldwin (2003) de-
scribed an approach of noun-noun compound ma-
chine translation, but no significant comparison
was presented. Lambert and Banchs (2005) pre-
sented a method in which bilingual MWEs were
used to modify the word alignment so as to im-
prove the SMT quality. In their work, a bilin-
gual MWE in training corpus was grouped as
</bodyText>
<footnote confidence="0.984217">
1http://mwe.stanford.edu/
</footnote>
<page confidence="0.986187">
47
</page>
<note confidence="0.9989895">
Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 47–54,
Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999863363636363">
one unique token before training alignment mod-
els. They reported that both alignment quality
and translation accuracy were improved on a small
corpus. However, in their further study, they re-
ported even lower BLEU scores after grouping
MWEs according to part-of-speech on a large cor-
pus (Lambert and Banchs, 2006). Nonetheless,
since MWE represents liguistic knowledge, the
role and usefulness of MWE in full-scale SMT
is intuitively positive. The difficulty lies in how
to integrate bilingual MWEs into existing SMT
system to improve SMT performance, especially
when translating domain texts.
In this paper, we implement three methods that
integrate domain bilingual MWEs into a phrase-
based SMT system, and show that these ap-
proaches improve translation quality significantly.
The main difference between our methods and
Lambert and Banchs’ work is that we directly aim
at improving the SMT performance rather than im-
proving the word alignment quality. In detail, dif-
ferences are listed as follows:
</bodyText>
<listItem confidence="0.89404748">
• Instead of using the bilingual n-gram trans-
lation model, we choose the phrase-based
SMT system, Moses2, which achieves sig-
nificantly better translation performance than
many other SMT systems and is a state-of-
the-art SMT system.
• Instead of improving translation indirectly
by improving the word alignment quality,
we directly target at the quality of transla-
tion. Some researchers have argued that large
gains of alignment performance under many
metrics only led to small gains in translation
performance (Ayan and Dorr, 2006; Fraser
and Marcu, 2007).
Besides the above differences, there are some
advantages of our approaches:
• In our method, automatically extracted
MWEs are used as additional resources rather
than as phrase-table filter. Since bilingual
MWEs are extracted according to noisy au-
tomatic word alignment, errors in word align-
ment would further propagate to the SMT and
hurt SMT performance.
• We conduct experiments on domain-specific
corpus. For one thing, domain-specific
</listItem>
<footnote confidence="0.84596">
2http://www.statmt.org/moses/
</footnote>
<bodyText confidence="0.999595052631579">
corpus potentially includes a large number
of technical terminologies as well as more
general fixed expressions and idioms, i.e.
domain-specific corpus has high MWE cov-
erage. For another, after the investigation,
current SMT system could not effectively
deal with these domain-specific MWEs es-
pecially for Chinese, since these MWEs are
more flexible and concise. Take the Chi-
nese term “R OZ- &amp; gr-&apos;t” for example. The
meaning of this term is “soften hard mass
and dispel pathogenic accumulation”. Ev-
ery word of this term represents a special
meaning and cannot be understood literally
or without this context. These terms are dif-
ficult to be translated even for humans, let
alone machine translation. So, treating these
terms as MWEs and applying them in SMT
system have practical significance.
</bodyText>
<listItem confidence="0.647450571428571">
• In our approach, no additional corpus is intro-
duced. We attempt to extract useful MWEs
from the training corpus and adopt suitable
methods to apply them. Thus, it benefits
for the full exploitation of available resources
without increasing great time and space com-
plexities of SMT system.
</listItem>
<bodyText confidence="0.9998228">
The remainder of the paper is organized as fol-
lows. Section 2 describes the bilingual MWE ex-
traction technique. Section 3 proposes three meth-
ods to apply bilingual MWEs in SMT system.
Section 4 presents the experimental results. Sec-
tion 5 draws conclusions and describes the future
work. Since this paper mainly focuses on the ap-
plication of BiMWE in SMT, we only give a brief
introduction on monolingual and bilingual MWE
extraction.
</bodyText>
<sectionHeader confidence="0.9648975" genericHeader="method">
2 Bilingual Multiword Expression
Extraction
</sectionHeader>
<bodyText confidence="0.9999322">
In this section we describe our approach of bilin-
gual MWE extraction. In the first step, we obtain
monolingual MWEs from the Chinese part of par-
allel corpus. After that, we look for the translation
of the extracted MWEs from parallel corpus.
</bodyText>
<subsectionHeader confidence="0.992845">
2.1 Automatic Extraction of MWEs
</subsectionHeader>
<bodyText confidence="0.9997838">
In the past two decades, many different ap-
proaches on automatic MWE identification were
reported. In general, those approaches can be
classified into three main trends: (1) statisti-
cal approaches (Pantel and Lin, 2001; Piao et
</bodyText>
<page confidence="0.998487">
48
</page>
<bodyText confidence="0.993346133333333">
al., 2005), (2) syntactic approaches (Fazly and
Stevenson, 2006; Bannard, 2007), and (3) seman-
tic approaches (Baldwin et al., 2003; Cruys and
Moir´on, 2007). Syntax-based and semantic-based
methods achieve high precision, but syntax or se-
mantic analysis has to be introduced as preparing
step, so it is difficult to apply them to domains with
few syntactical or semantic annotation. Statistical
approaches only consider frequency information,
so they can be used to obtain MWEs from bilin-
gual corpora without deeper syntactic or semantic
analysis. Most statistical measures only take two
words into account, so it not easy to extract MWEs
containing three or more than three words.
Log Likelihood Ratio (LLR) has been proved a
good statistical measurement of the association of
two random variables (Chang et al., 2002). We
adopt the idea of statistical approaches, and pro-
pose a new algorithm named LLR-based Hierar-
chical Reducing Algorithm (HRA for short) to ex-
tract MWEs with arbitrary lengths. To illustrate
our algorithm, firstly we define some useful items.
In the following definitions, we assume the given
sentence is “A B C D E”.
Definition 1 Unit: A unit is any sub-string of the
given sentence. For example, “A B”, “C”, “C D E”
are all units, but “A B D” is not a unit.
Definition 2 List: A list is an ordered sequence of
units which exactly cover the given sentence. For
example, {“A”,“B C D”,“E”} forms a list.
Definition 3 Score: The score function only de-
fines on two adjacent units and return the LLR
between the last word of first unit and the first
word of the second unit3. For example, the score
of adjacent unit “B C” and “D E” is defined as
LLR(“C”,“D”).
Definition 4 Select: The selecting operator is to
find the two adjacent units with maximum score
in a list.
Definition 5 Reduce: The reducing operator is to
remove two specific adjacent units, concatenate
them, and put back the result unit to the removed
position. For example, if we want to reduce unit
“B C” and unit “D” in list {“A”,“B C”,“D”,“E”},
we will get the list {“A”,“B C D”,“E”}.
Initially, every word in the sentence is consid-
ered as one unit and all these units form a initial
list L. If the sentence is of length N, then the
3we use a stoplist to eliminate the units containing func-
tion words by setting their score to 0
list contains N units, of course. The final set of
MWEs, S, is initialized to empty set. After initial-
ization, the algorithm will enter an iterating loop
with two steps: (1) select the two adjacent units
with maximum score in L, naming U1 and U2; and
(2) reduce U1 and U2 in L, and insert the reducing
result into the final set S. Our algorithm termi-
nates on two conditions: (1) if the maximum score
after selection is less than a given threshold; or (2)
if L contains only one unit.
</bodyText>
<equation confidence="0.985028857142857">
c1(“£ p É• w ~è �)
c1(“£ p É• w M)
c1(“£ p É• w)
c2(p É• w)
c2(p É•)
c2(p) c3(É•) c4(w) c5(j)
“£ 147.1 p 6755.2 É• 1059.6 w 0 OJ 0 IMè 809.6 �
</equation>
<figureCaption confidence="0.914641">
Figure 1: Example of Hierarchical Reducing Al-
gorithm
</figureCaption>
<bodyText confidence="0.977868928571429">
Let us make the algorithm clearer with an ex-
ample. Assume the threshold of score is 20, the
given sentence is ““£ p É• w j �è ~”4.
Figure 1 shows the hierarchical structure of given
sentence (based on LLR of adjacent words). In
this example, four MWEs (“p É•”, “p É•
w”, “Nè V,”, ““£ p É• w”) are extracted
in the order, and sub-strings over dotted line in fig-
ure 1 are not extracted.
From the above example, we can see that the
extracted MWEs correspond to human intuition.
In general, the basic idea of HRA is to reflect
the hierarchical structure pattern of natural lan-
guage. Furthermore, in the HRA, MWEs are mea-
sured with the minimum LLR of adjacent words
in them, which gives lexical confidence of ex-
tracted MWEs. Finally, suppose given sentence
has length N, HRA would definitely terminate
within N − 1 iterations, which is very efficient.
However, HRA has a problem that it would ex-
tract substrings before extracting the whole string,
even if the substrings only appear in the particu-
lar whole string, which we consider useless. To
solve this problem, we use contextual features,
4The whole sentence means “healthy tea for preventing
hyperlipidemia”, and we give the meaning for each Chi-
nese word: �iR(preventing), n(hyper-), É•(-lipid-), 4(-
emia), 0(for), **_(healthy), *(tea).
</bodyText>
<equation confidence="0.984842">
c1(“£)
c6(,Wè *)
c6(W c7(�)
</equation>
<page confidence="0.989983">
49
</page>
<table confidence="0.343176">
contextual entropy (Luo and Sun, 2003) and C-
value (Frantzi and Ananiadou, 1996), to filter out
those substrings which exist only in few MWEs.
2.2 Automatic Extraction of MWE’s
Translation
</table>
<bodyText confidence="0.999877571428571">
We select and annotate 33000 phrase pairs ran-
domly, of which 30000 pairs are used as training
set and 3000 pairs are used as test set. We use the
perceptron training algorithm to train the model.
As the experiments reveal, the classification preci-
sion of this model is 91.67%.
In subsection 2.1, we described the algorithm to
obtain MWEs, and we would like to introduce the
procedure to find their translations from parallel
corpus in this subsection.
For mining the English translations of Chinese
MWEs, we first obtain the candidate translations
of a given MWE from the parallel corpus. Steps
are listed as follows:
</bodyText>
<listItem confidence="0.972601375">
1. Run GIZA++5 to align words in the training
parallel corpus.
2. For a given MWE, find the bilingual sentence
pairs where the source language sentences in-
clude the MWE.
3. Extract the candidate translations of the
MWE from the above sentence pairs accord-
ing to the algorithm described by Och (2002).
</listItem>
<bodyText confidence="0.99985592">
After the above procedure, we have already
extracted all possible candidate translations of a
given MWE. The next step is to distinguish right
candidates from wrong candidates. We construct
perceptron-based classification model (Collins,
2002) to solve the problem. We design two
groups of features: translation features, which
describe the mutual translating chance between
source phrase and target phrase, and the language
features, which refer to how well a candidate
is a reasonable translation. The translation fea-
tures include: (1) the logarithm of source-target
translation probability; (2) the logarithm of target-
source translation probability; (3) the logarithm
of source-target lexical weighting; (4) the loga-
rithm of target-source lexical weighting; and (5)
the logarithm of the phrase pair’s LLR (Dunning,
1993). The first four features are exactly the same
as the four translation probabilities used in tradi-
tional phrase-based system (Koehn et al., 2003).
The language features include: (1) the left entropy
of the target phrase (Luo and Sun, 2003); (2) the
right entropy of the target phrase; (3) the first word
of the target phrase; (4) the last word of the target
phrase; and (5) all words in the target phrase.
</bodyText>
<footnote confidence="0.892589">
5http://www.fjoch.com/GIZA++.html
</footnote>
<sectionHeader confidence="0.697946" genericHeader="method">
3 Application of Bilingual MWEs
</sectionHeader>
<bodyText confidence="0.992854857142857">
Intuitively, bilingual MWE is useful to improve
the performance of SMT. However, as we de-
scribed in section 1, it still needs further research
on how to integrate bilingual MWEs into SMT
system. In this section, we propose three methods
to utilize bilingual MWEs, and we will compare
their performance in section 4.
</bodyText>
<subsectionHeader confidence="0.995964">
3.1 Model Retraining with Bilingual MWEs
</subsectionHeader>
<bodyText confidence="0.999932384615385">
Bilingual phrase table is very important for
phrase-based MT system. However, due to the er-
rors in automatic word alignment and unaligned
word extension in phrase extraction (Och, 2002),
many meaningless phrases would be extracted,
which results in inaccuracy of phrase probability
estimation. To alleviate this problem, we take the
automatically extracted bilingual MWEs as paral-
lel sentence pairs, add them into the training cor-
pus, and retrain the model using GIZA++. By
increasing the occurrences of bilingual MWEs,
which are good phrases, we expect that the align-
ment would be modified and the probability es-
timation would be more reasonable. Wu et al.
(2008) also used this method to perform domain
adaption for SMT. Different from their approach,
in which bilingual MWEs are extracted from ad-
ditional corpus, we extract bilingual MWEs from
the original training set. The fact that additional
resources can improve the domain-specific SMT
performance was proved by many researchers
(Wu et al., 2008; Eck et al., 2004). However,
our method shows that making better use of the
resources in hand could also enhance the quality
of SMT system. We use “Baseline+BiMWE” to
represent this method.
</bodyText>
<subsectionHeader confidence="0.999373">
3.2 New Feature for Bilingual MWEs
</subsectionHeader>
<bodyText confidence="0.999973714285714">
Lopez and Resnik (2006) once pointed out that
better feature mining can lead to substantial gain
in translation quality. Inspired by this idea, we
append one feature into bilingual phrase table to
indicate that whether a bilingual phrase contains
bilingual MWEs. In other words, if the source lan-
guage phrase contains a MWE (as substring) and
</bodyText>
<page confidence="0.990577">
50
</page>
<bodyText confidence="0.999992714285714">
the target language phrase contains the translation
of the MWE (as substring), the feature value is 1,
otherwise the feature value is set to 0. Due to the
high reliability of bilingual MWEs, we expect that
this feature could help SMT system to select bet-
ter and reasonable phrase pairs during translation.
We use “Baseline+Feat” to represent this method.
</bodyText>
<subsectionHeader confidence="0.967911">
3.3 Additional Phrase Table of bilingual
MWEs
</subsectionHeader>
<bodyText confidence="0.999939642857143">
Wu et al. (2008) proposed a method to construct a
phrase table by a manually-made translation dic-
tionary. Instead of manually constructing transla-
tion dictionary, we construct an additional phrase
table containing automatically extracted bilingual
MWEs. As to probability assignment, we just as-
sign 1 to the four translation probabilities for sim-
plicity. Since Moses supports multiple bilingual
phrase tables, we combine the original phrase ta-
ble and new constructed bilingual MWE table. For
each phrase in input sentence during translation,
the decoder would search all candidate transla-
tion phrases in both phrase tables. We use “Base-
line+NewBP” to represent this method.
</bodyText>
<sectionHeader confidence="0.999438" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.944482">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.9999405">
We run experiments on two domain-specific patent
corpora: one is for traditional medicine domain,
and the other is for chemical industry domain. Our
translation tasks are Chinese-to-English.
In the traditional medicine domain, table 1
shows the data statistics. For language model, we
use SRI Language Modeling Toolkit6 to train a tri-
gram model with modified Kneser-Ney smoothing
(Chen and Goodman, 1998) on the target side of
training corpus. Using our bilingual MWE ex-
tracting algorithm, 80287 bilingual MWEs are ex-
tracted from the training set.
</bodyText>
<table confidence="0.995967285714286">
Chinese English
Training Sentences 120,355
Words 4,688,873 4,737,843
Dev Sentences 1,000
Words 31,722 32,390
Test Sentences 1,000
Words 41,643 40,551
</table>
<tableCaption confidence="0.999796">
Table 1: Traditional medicine corpus
</tableCaption>
<footnote confidence="0.891365">
6http://www.speech.sri.com/projects/srilm/
</footnote>
<bodyText confidence="0.998708333333333">
In the chemical industry domain, table 2 gives
the detail information of the data. In this experi-
ment, 59466 bilingual MWEs are extracted.
</bodyText>
<table confidence="0.998278857142857">
Chinese English
Training Sentences 120,856
Words 4,532,503 4,311,682
Dev Sentences 1,099
Words 42,122 40,521
Test Sentences 1,099
Words 41,069 39,210
</table>
<tableCaption confidence="0.998654">
Table 2: Chemical industry corpus
</tableCaption>
<bodyText confidence="0.999984888888889">
We test translation quality on test set and use the
open source tool mteval-vllb.pl7 to calculate case-
sensitive BLEU 4 score (Papineni et al., 2002) as
our evaluation criteria. For this evaluation, there is
only one reference per test sentence. We also per-
form statistical significant test between two trans-
lation results (Collins et al., 2005). The mean of
all scores and relative standard deviation are calcu-
lated with a 99% confidence interval of the mean.
</bodyText>
<subsectionHeader confidence="0.990103">
4.2 MT Systems
</subsectionHeader>
<bodyText confidence="0.999947764705882">
We use the state-of-the-art phrase-based SMT sys-
tem, Moses, as our baseline system. The features
used in baseline system include: (1) four transla-
tion probability features; (2) one language model
feature; (3) distance-based and lexicalized distor-
tion model feature; (4) word penalty; (5) phrase
penalty. For “Baseline+BiMWE” method, bilin-
gual MWEs are added into training corpus, as a
result, new alignment and new phrase table are
obtained. For “Baseline+Feat” method, one ad-
ditional 0/1 feature are introduced to each entry in
phrase table. For “Baseline+NewBP”, additional
phrase table constructed by bilingual MWEs is
used.
Features are combined in the log-linear model.
To obtain the best translation e� of the source sen-
tence f, log-linear model uses following equation:
</bodyText>
<equation confidence="0.998584333333333">
e� = arg max
e
amhm(e,f) (1)
</equation>
<bodyText confidence="0.997664">
in which hm and am denote the mth feature and
weight. The weights are automatically turned by
minimum error rate training (Och, 2002) on devel-
opment set.
</bodyText>
<footnote confidence="0.679825">
7http://www.nist.gov/speech/tests/mt/resources/scoring.htm
</footnote>
<equation confidence="0.5834458">
Ae|f)
= arg max
e
M
m=1
</equation>
<page confidence="0.992767">
51
</page>
<table confidence="0.916745333333333">
4.3 Results
Methods BLEU
Baseline 0.2658
Baseline+BiMWE 0.2661
Baseline+Feat 0.2675
Baseline+NewBP 0.2719
</table>
<tableCaption confidence="0.865287">
Table 3: Translation results of using bilingual
MWEs in traditional medicine domain
Table 3 gives our experiment results. From
</tableCaption>
<bodyText confidence="0.99782275">
this table, we can see that, bilingual MWEs
improve translation quality in all cases. The
Baseline+NewBP method achieves the most im-
provement of 0.61% BLEU score compared
with the baseline system. The Baseline+Feat
method comes next with 0.17% BLEU score im-
provement. And the Baseline+BiMWE achieves
slightly higher translation quality than the baseline
system.
To our disappointment, however, none of these
improvements are statistical significant. We
manually examine the extracted bilingual MWEs
which are labeled positive by perceptron algorithm
and find that although the classification precision
is high (91.67%), the proportion of positive exam-
ple is relatively lower (76.69%). The low positive
proportion means that many negative instances
have been wrongly classified to positive, which in-
troduce noises. To remove noisy bilingual MWEs,
we use the length ratio x of the source phrase over
the target phrase to rank the bilingual MWEs la-
beled positive. Assume x follows Gaussian distri-
butions, then the ranking score of phrase pair (s, t)
is defined as the following formula:
</bodyText>
<equation confidence="0.9981205">
1 (x�µ)2
�core(s, t) = log(LLR(s, t))�
</equation>
<bodyText confidence="0.993685071428572">
(2)
Here the mean p and variance Q2 are estimated
from the training set. After ranking by score, we
select the top 50000, 60000 and 70000 bilingual
MWEs to perform the three methods mentioned in
section 3. The results are showed in table 4.
From this table, we can conclude that: (1) All
the three methods on all settings improve BLEU
score; (2) Except the Baseline+BiMWE method,
the other two methods obtain significant improve-
ment of BLEU score (0.2728, 0.2734, 0.2724)
over baseline system (0.2658); (3) When the scale
of bilingual MWEs is relatively small (50000,
60000), the Baseline+Feat method performs better
</bodyText>
<table confidence="0.994973">
Methods 50000 60000 70000
Baseline 0.2658
Baseline+BiMWE 0.2671 0.2686 0.2715
Baseline+Feat 0.2728 0.2734 0.2712
Baseline+NewBP 0.2662 0.2706 0.2724
</table>
<tableCaption confidence="0.917929">
Table 4: Translation results of using bilingual
MWEs in traditional medicine domain
</tableCaption>
<bodyText confidence="0.989407266666667">
than others; (4) As the number of bilingual MWEs
increasing, the Baseline+NewBP method outper-
forms the Baseline+Feat method; (5) Comparing
table 4 and 3, we can see it is not true that the
more bilingual MWEs, the better performance of
phrase-based SMT. This conclusion is the same as
(Lambert and Banchs, 2005).
To verify the assumption that bilingual MWEs
do indeed improve the SMT performance not only
on particular domain, we also perform some ex-
periments on chemical industry domain. Table 5
shows the results. From this table, we can see that
these three methods can improve the translation
performance on chemical industry domain as well
as on the traditional medicine domain.
</bodyText>
<table confidence="0.9891812">
Methods BLEU
Baseline 0.1882
Baseline+BiMWE 0.1928
Baseline+Feat 0.1917
Baseline+Newbp 0.1914
</table>
<tableCaption confidence="0.982948">
Table 5: Translation results of using bilingual
MWEs in chemical industry domain
</tableCaption>
<subsectionHeader confidence="0.977764">
4.4 Discussion
</subsectionHeader>
<bodyText confidence="0.97984475">
In order to know in what respects our methods im-
prove performance of translation, we manually an-
alyze some test sentences and gives some exam-
ples in this subsection.
</bodyText>
<listItem confidence="0.703725">
(1) For the first example in table 6, “�
</listItem>
<bodyText confidence="0.992813666666667">
�”
is aligned to other words and not correctly trans-
lated in baseline system, while it is aligned to cor-
rect target phrase “dredging meridians” in Base-
line+BiMWE, since the bilingual MWE (“A )W” ,
“dredging meridians”) has been added into train-
ing corpus and then aligned by GIZA++.
(2) For the second example in table 6, “�
V,” has two candidate translation in phrase table:
“tea” and “medicated tea”. The baseline system
chooses the “tea” as the translation of “4 V,”,
while the Baseline+Feat system chooses the “med-
</bodyText>
<equation confidence="0.883225">
.\/27rQ
Xe− 2σ2
</equation>
<page confidence="0.988983">
52
</page>
<table confidence="0.780681846153846">
Src *RA-_J� �h_&amp; ! i*-- ! i&amp; � ! �9 ! � 7K ! �
Sc,7ik— 911 A4 -f0 00 &amp;quot;
Ref the obtained product is effective in tonifying blood, expelling cold, dredging meridians
, promoting production of body fluid, promoting urination, and tranquilizing mind;
and can be used for supplementing nutrition and protecting health .
Baseline the food has effects in tonifying blood, dispelling cold, promoting salivation and water
, and tranquilizing , and tonic effects , and making nutritious health .
+Bimwe the food has effects in tonifying blood, dispelling cold, dredging meridians, promoting
salivation , promoting urination , and tranquilizing tonic , nutritious pulverizing .
Src Z 7 _$11A 14411 ! fit.-Ad ! 40,11 ! * ! i1* -d &amp;quot;
Ref the product can also be made into tablet, pill, powder, medicated tea, or injection.
Baseline may also be made into tablet, pill, powder, tea, or injection.
+Feat may also be made into tablet, pill, powder, medicated tea, or injection.
</table>
<tableCaption confidence="0.996043">
Table 6: Translation example
</tableCaption>
<bodyText confidence="0.999093666666667">
icated tea” because the additional feature gives
high probability of the correct translation “medi-
cated tea”.
</bodyText>
<sectionHeader confidence="0.993982" genericHeader="conclusions">
5 Conclusion and Future Works
</sectionHeader>
<bodyText confidence="0.9999794">
This paper presents the LLR-based hierarchical
reducing algorithm to automatically extract bilin-
gual MWEs and investigates the performance of
three different application strategies in applying
bilingual MWEs for SMT system. The translation
results show that using an additional feature to rep-
resent whether a bilingual phrase contains bilin-
gual MWEs performs the best in most cases. The
other two strategies can also improve the quality
of SMT system, although not as much as the first
one. These results are encouraging and motivated
to do further research in this area.
The strategies of bilingual MWE application is
roughly simply and coarse in this paper. Com-
plicated approaches should be taken into account
during applying bilingual MWEs. For example,
we may consider other features of the bilingual
MWEs and examine their effect on the SMT per-
formance. Besides application in phrase-based
SMT system, bilingual MWEs may also be inte-
grated into other MT models such as hierarchical
phrase-based models or syntax-based translation
models. We will do further studies on improving
statistical machine translation using domain bilin-
gual MWEs.
</bodyText>
<sectionHeader confidence="0.998295" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997554">
This work is supported by National Natural Sci-
ence Foundation of China, Contracts 60603095
and 60873167. We would like to thank the anony-
mous reviewers for their insightful comments on
an earlier draft of this paper.
</bodyText>
<sectionHeader confidence="0.99567" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.96723821875">
Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going
beyond aer: an extensive analysis of word align-
ments and their impact on mt. In Proceedings of the
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 9–16.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model
of multiword expression decomposability. In Pro-
ceedings of the ACL-2003 Workshop on Multiword
Expressions: Analysis, Acquisiton and Treatment,
pages 89–96.
Colin Bannard. 2007. A measure of syntactic flex-
ibility for automatically identifying multiword ex-
pressions in corpora. In Proceedings of the ACL
Workshop on A Broader Perspective on Multiword
Expressions, pages 1–8.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263–
311.
Baobao Chang, Pernilla Danielsson, and Wolfgang
Teubert. 2002. Extraction of translation unit from
chinese-english parallel corpora. In Proceedings of
the first SIGHAN workshop on Chinese language
processing, pages 1–5.
Stanley F. Chen and Joshua Goodman. 1998. Am em-
pirical study of smoothing techniques for language
modeling. Technical report.
Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
</reference>
<page confidence="0.987174">
53
</page>
<reference confidence="0.999465076086956">
Meeting on Association for Computational Linguis-
tics, pages 531–540.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the Empirical Methods in Natural Language Pro-
cessing Conference, pages 1–8.
Tim Van de Cruys and Bego˜na Villada Moir´on. 2007.
Semantics-based multiword expression extraction.
In Proceedings of the Workshop on A Broader Per-
spective on Multiword Expressions, pages 25–32.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61–74.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2004.
Improving statistical machine translation in the med-
ical domain using the unified medical language sys-
tem. In Proceedings of the 20th international con-
ference on Computational Linguistics table of con-
tents, pages 792–798.
Afsaneh Fazly and Suzanne Stevenson. 2006. Auto-
matically constructing a lexicon of verb phrase id-
iomatic combinations. In Proceedings of the EACL,
pages 337–344.
Katerina T. Frantzi and Sophia Ananiadou. 1996. Ex-
tracting nested collocations. In Proceedings of the
16th conference on Computational linguistics, pages
41–46.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine trans-
lation. Computational Linguistics, 33(3):293–303.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology, pages
48–54.
Patrik Lambert and Rafael Banchs. 2005. Data in-
ferred multi-word expressions for statistical machine
translation. In Proceedings of Machine Translation
SummitX, pages 396–403.
Patrik Lambert and Rafael Banchs. 2006. Grouping
multi-word expressions according to part-of-speech
in statistical machine translation. In Proceedings of
the Workshop on Multi-word-expressions in a multi-
lingual context, pages 9–16.
Adam Lopez and Philip Resnik. 2006. Word-based
alignment, phrase-based translation: What’s the
link? In proceedings of the 7th conference of the as-
sociation for machine translation in the Americas:
visions for the future of machine translation, pages
90–99.
Shengfen Luo and Maosong Sun. 2003. Two-character
chinese word extraction based on hybrid of internal
and contextual measures. In Proceedings of the sec-
ond SIGHAN workshop on Chinese language pro-
cessing, pages 24–30.
Franz Josef Och. 2002. Statistical Machine Transla-
tion: From Single-Word Models to Alignment Tem-
plates. Ph.d. thesis, Computer Science Department,
RWTH Aachen, Germany.
Patrick Pantel and Dekang Lin. 2001. A statistical cor-
pus based term extractor. In AI ’01: Proceedings of
the 14th Biennial Conference of the Canadian Soci-
ety on Computational Studies of Intelligence, pages
36–46.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Conference of the Association for Com-
putational Linguistics, pages 311–318.
Scott Songlin Piao, Paul Rayson, Dawn Archer, and
Tony McEnery. 2005. Comparing and combining a
semantic tagger and a statistical tool for mwe extrac-
tion. Computer Speech and Language, 19(4):378–
397.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multi-
word expressions: A pain in the neck for nlp. In
Proceedings of the 3th International Conference
on Intelligent Text Processing and Computational
Linguistics(CICLing-2002), pages 1–15.
Takaaki Tanaka and Timothy Baldwin. 2003. Noun-
noun compound machine translation: A feasibility
study on shallow processing. In Proceedings of
the ACL-2003 Workshop on Multiword Expressions:
Analysis, Acquisition and Treatment, pages 17–24.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine transla-
tion with domain dictionary and monolingual cor-
pora. In Proceedings of Conference on Computa-
tional Linguistics (COLING), pages 993–1000.
</reference>
<page confidence="0.999007">
54
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.613882">
<title confidence="0.9991365">Improving Statistical Machine Translation Domain Bilingual Multiword Expressions</title>
<author confidence="0.99518">Yajuan Jie Qun Yun</author>
<affiliation confidence="0.878980666666667">Lab. of Intelligent Info. Processing of Computer Science Institute of Computing Technology School of Computing Chinese Academy of Sciences National University of Singapore</affiliation>
<address confidence="0.980737">P.O. Box 2704, Beijing 100190, China Computing 1, Law Link, Singapore 117590</address>
<abstract confidence="0.998543866666667">Multiword expressions (MWEs) have been proved useful for many natural language processing tasks. However, how to use them to improve performance of statistical machine translation (SMT) is not well studied. This paper presents a simple yet effective strategy to extract domain bilingual multiword expressions. In addition, we implement three methods to integrate bilingual MWEs to Moses, the state-ofthe-art phrase-based machine translation system. Experiments show that bilingual MWEs could improve translation performance significantly.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Necip Fazil Ayan</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Going beyond aer: an extensive analysis of word alignments and their impact on mt.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="5641" citStr="Ayan and Dorr, 2006" startWordPosition="852" endWordPosition="855"> than improving the word alignment quality. In detail, differences are listed as follows: • Instead of using the bilingual n-gram translation model, we choose the phrase-based SMT system, Moses2, which achieves significantly better translation performance than many other SMT systems and is a state-ofthe-art SMT system. • Instead of improving translation indirectly by improving the word alignment quality, we directly target at the quality of translation. Some researchers have argued that large gains of alignment performance under many metrics only led to small gains in translation performance (Ayan and Dorr, 2006; Fraser and Marcu, 2007). Besides the above differences, there are some advantages of our approaches: • In our method, automatically extracted MWEs are used as additional resources rather than as phrase-table filter. Since bilingual MWEs are extracted according to noisy automatic word alignment, errors in word alignment would further propagate to the SMT and hurt SMT performance. • We conduct experiments on domain-specific corpus. For one thing, domain-specific 2http://www.statmt.org/moses/ corpus potentially includes a large number of technical terminologies as well as more general fixed exp</context>
</contexts>
<marker>Ayan, Dorr, 2006</marker>
<rawString>Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going beyond aer: an extensive analysis of word alignments and their impact on mt. In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Colin Bannard</author>
<author>Takaaki Tanaka</author>
<author>Dominic Widdows</author>
</authors>
<title>An empirical model of multiword expression decomposability.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL-2003 Workshop on Multiword Expressions: Analysis, Acquisiton and Treatment,</booktitle>
<pages>89--96</pages>
<contexts>
<context position="8338" citStr="Baldwin et al., 2003" startWordPosition="1280" endWordPosition="1283"> we describe our approach of bilingual MWE extraction. In the first step, we obtain monolingual MWEs from the Chinese part of parallel corpus. After that, we look for the translation of the extracted MWEs from parallel corpus. 2.1 Automatic Extraction of MWEs In the past two decades, many different approaches on automatic MWE identification were reported. In general, those approaches can be classified into three main trends: (1) statistical approaches (Pantel and Lin, 2001; Piao et 48 al., 2005), (2) syntactic approaches (Fazly and Stevenson, 2006; Bannard, 2007), and (3) semantic approaches (Baldwin et al., 2003; Cruys and Moir´on, 2007). Syntax-based and semantic-based methods achieve high precision, but syntax or semantic analysis has to be introduced as preparing step, so it is difficult to apply them to domains with few syntactical or semantic annotation. Statistical approaches only consider frequency information, so they can be used to obtain MWEs from bilingual corpora without deeper syntactic or semantic analysis. Most statistical measures only take two words into account, so it not easy to extract MWEs containing three or more than three words. Log Likelihood Ratio (LLR) has been proved a goo</context>
</contexts>
<marker>Baldwin, Bannard, Tanaka, Widdows, 2003</marker>
<rawString>Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and Dominic Widdows. 2003. An empirical model of multiword expression decomposability. In Proceedings of the ACL-2003 Workshop on Multiword Expressions: Analysis, Acquisiton and Treatment, pages 89–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
</authors>
<title>A measure of syntactic flexibility for automatically identifying multiword expressions in corpora.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL Workshop on A Broader Perspective on Multiword Expressions,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="2717" citStr="Bannard, 2007" startWordPosition="401" endWordPosition="402">r component words. Stanford university launched a MWE project1, in which different qualities of MWE were presented. For bilingual multiword expression (BiMWE), we define a bilingual phrase as a bilingual MWE if (1) the source phrase is a MWE in source language; (2) the source phrase and the target phrase must be translated to each other exactly, i.e. there is no additional (boundary) word in target phrase which cannot find the corresponding word in source phrase, and vice versa. In recent years, many useful methods have been proposed to extract MWEs or BiMWEs automatically (Piao et al., 2005; Bannard, 2007; Fazly and Stevenson, 2006). Since MWE usually constrains possible senses of a polysemous word in context, they can be used in many NLP applications such as information retrieval, question answering, word sense disambiguation and so on. For machine translation, Piao et al. (2005) have noted that the issue of MWE identification and accurate interpretation from source to target language remained an unsolved problem for existing MT systems. This problem is more severe when MT systems are used to translate domain-specific texts, since they may include technical terminology as well as more general</context>
<context position="8287" citStr="Bannard, 2007" startWordPosition="1273" endWordPosition="1274">ltiword Expression Extraction In this section we describe our approach of bilingual MWE extraction. In the first step, we obtain monolingual MWEs from the Chinese part of parallel corpus. After that, we look for the translation of the extracted MWEs from parallel corpus. 2.1 Automatic Extraction of MWEs In the past two decades, many different approaches on automatic MWE identification were reported. In general, those approaches can be classified into three main trends: (1) statistical approaches (Pantel and Lin, 2001; Piao et 48 al., 2005), (2) syntactic approaches (Fazly and Stevenson, 2006; Bannard, 2007), and (3) semantic approaches (Baldwin et al., 2003; Cruys and Moir´on, 2007). Syntax-based and semantic-based methods achieve high precision, but syntax or semantic analysis has to be introduced as preparing step, so it is difficult to apply them to domains with few syntactical or semantic annotation. Statistical approaches only consider frequency information, so they can be used to obtain MWEs from bilingual corpora without deeper syntactic or semantic analysis. Most statistical measures only take two words into account, so it not easy to extract MWEs containing three or more than three word</context>
</contexts>
<marker>Bannard, 2007</marker>
<rawString>Colin Bannard. 2007. A measure of syntactic flexibility for automatically identifying multiword expressions in corpora. In Proceedings of the ACL Workshop on A Broader Perspective on Multiword Expressions, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>311</pages>
<contexts>
<context position="1176" citStr="Brown et al., 1993" startWordPosition="155" endWordPosition="158">l language processing tasks. However, how to use them to improve performance of statistical machine translation (SMT) is not well studied. This paper presents a simple yet effective strategy to extract domain bilingual multiword expressions. In addition, we implement three methods to integrate bilingual MWEs to Moses, the state-ofthe-art phrase-based machine translation system. Experiments show that bilingual MWEs could improve translation performance significantly. 1 Introduction Phrase-based machine translation model has been proved a great improvement over the initial wordbased approaches (Brown et al., 1993). Recent syntax-based models perform even better than phrase-based models. However, when syntaxbased models are applied to new domain with few syntax-annotated corpus, the translation performance would decrease. To utilize the robustness of phrases and make up the lack of syntax or semantic information in phrase-based model for domain translation, we study domain bilingual multiword expressions and integrate them to the existing phrase-based model. A multiword expression (MWE) can be considered as word sequence with relatively fixed structure representing special meanings. There is no uniform </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263– 311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baobao Chang</author>
<author>Pernilla Danielsson</author>
<author>Wolfgang Teubert</author>
</authors>
<title>Extraction of translation unit from chinese-english parallel corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of the first SIGHAN workshop on Chinese language processing,</booktitle>
<pages>1--5</pages>
<contexts>
<context position="9027" citStr="Chang et al., 2002" startWordPosition="1388" endWordPosition="1391">hieve high precision, but syntax or semantic analysis has to be introduced as preparing step, so it is difficult to apply them to domains with few syntactical or semantic annotation. Statistical approaches only consider frequency information, so they can be used to obtain MWEs from bilingual corpora without deeper syntactic or semantic analysis. Most statistical measures only take two words into account, so it not easy to extract MWEs containing three or more than three words. Log Likelihood Ratio (LLR) has been proved a good statistical measurement of the association of two random variables (Chang et al., 2002). We adopt the idea of statistical approaches, and propose a new algorithm named LLR-based Hierarchical Reducing Algorithm (HRA for short) to extract MWEs with arbitrary lengths. To illustrate our algorithm, firstly we define some useful items. In the following definitions, we assume the given sentence is “A B C D E”. Definition 1 Unit: A unit is any sub-string of the given sentence. For example, “A B”, “C”, “C D E” are all units, but “A B D” is not a unit. Definition 2 List: A list is an ordered sequence of units which exactly cover the given sentence. For example, {“A”,“B C D”,“E”} forms a l</context>
</contexts>
<marker>Chang, Danielsson, Teubert, 2002</marker>
<rawString>Baobao Chang, Pernilla Danielsson, and Wolfgang Teubert. 2002. Extraction of translation unit from chinese-english parallel corpora. In Proceedings of the first SIGHAN workshop on Chinese language processing, pages 1–5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>Am empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical report.</tech>
<contexts>
<context position="18357" citStr="Chen and Goodman, 1998" startWordPosition="2953" endWordPosition="2956">WE table. For each phrase in input sentence during translation, the decoder would search all candidate translation phrases in both phrase tables. We use “Baseline+NewBP” to represent this method. 4 Experiments 4.1 Data We run experiments on two domain-specific patent corpora: one is for traditional medicine domain, and the other is for chemical industry domain. Our translation tasks are Chinese-to-English. In the traditional medicine domain, table 1 shows the data statistics. For language model, we use SRI Language Modeling Toolkit6 to train a trigram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) on the target side of training corpus. Using our bilingual MWE extracting algorithm, 80287 bilingual MWEs are extracted from the training set. Chinese English Training Sentences 120,355 Words 4,688,873 4,737,843 Dev Sentences 1,000 Words 31,722 32,390 Test Sentences 1,000 Words 41,643 40,551 Table 1: Traditional medicine corpus 6http://www.speech.sri.com/projects/srilm/ In the chemical industry domain, table 2 gives the detail information of the data. In this experiment, 59466 bilingual MWEs are extracted. Chinese English Training Sentences 120,856 Words 4,532,503 4,311,682 Dev Sentences 1,09</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. Am empirical study of smoothing techniques for language modeling. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kuˇcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>531--540</pages>
<marker>Collins, Koehn, Kuˇcerov´a, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 531–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the Empirical Methods in Natural Language Processing Conference,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="13870" citStr="Collins, 2002" startWordPosition="2248" endWordPosition="2249">of a given MWE from the parallel corpus. Steps are listed as follows: 1. Run GIZA++5 to align words in the training parallel corpus. 2. For a given MWE, find the bilingual sentence pairs where the source language sentences include the MWE. 3. Extract the candidate translations of the MWE from the above sentence pairs according to the algorithm described by Och (2002). After the above procedure, we have already extracted all possible candidate translations of a given MWE. The next step is to distinguish right candidates from wrong candidates. We construct perceptron-based classification model (Collins, 2002) to solve the problem. We design two groups of features: translation features, which describe the mutual translating chance between source phrase and target phrase, and the language features, which refer to how well a candidate is a reasonable translation. The translation features include: (1) the logarithm of source-target translation probability; (2) the logarithm of targetsource translation probability; (3) the logarithm of source-target lexical weighting; (4) the logarithm of target-source lexical weighting; and (5) the logarithm of the phrase pair’s LLR (Dunning, 1993). The first four fea</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the Empirical Methods in Natural Language Processing Conference, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
<author>Bego˜na Villada Moir´on</author>
</authors>
<title>Semantics-based multiword expression extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on A Broader Perspective on Multiword Expressions,</booktitle>
<pages>25--32</pages>
<marker>Van de Cruys, Moir´on, 2007</marker>
<rawString>Tim Van de Cruys and Bego˜na Villada Moir´on. 2007. Semantics-based multiword expression extraction. In Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="14450" citStr="Dunning, 1993" startWordPosition="2333" endWordPosition="2334">assification model (Collins, 2002) to solve the problem. We design two groups of features: translation features, which describe the mutual translating chance between source phrase and target phrase, and the language features, which refer to how well a candidate is a reasonable translation. The translation features include: (1) the logarithm of source-target translation probability; (2) the logarithm of targetsource translation probability; (3) the logarithm of source-target lexical weighting; (4) the logarithm of target-source lexical weighting; and (5) the logarithm of the phrase pair’s LLR (Dunning, 1993). The first four features are exactly the same as the four translation probabilities used in traditional phrase-based system (Koehn et al., 2003). The language features include: (1) the left entropy of the target phrase (Luo and Sun, 2003); (2) the right entropy of the target phrase; (3) the first word of the target phrase; (4) the last word of the target phrase; and (5) all words in the target phrase. 5http://www.fjoch.com/GIZA++.html 3 Application of Bilingual MWEs Intuitively, bilingual MWE is useful to improve the performance of SMT. However, as we described in section 1, it still needs fu</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Improving statistical machine translation in the medical domain using the unified medical language system.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics table of contents,</booktitle>
<pages>792--798</pages>
<contexts>
<context position="16302" citStr="Eck et al., 2004" startWordPosition="2629" endWordPosition="2632">em into the training corpus, and retrain the model using GIZA++. By increasing the occurrences of bilingual MWEs, which are good phrases, we expect that the alignment would be modified and the probability estimation would be more reasonable. Wu et al. (2008) also used this method to perform domain adaption for SMT. Different from their approach, in which bilingual MWEs are extracted from additional corpus, we extract bilingual MWEs from the original training set. The fact that additional resources can improve the domain-specific SMT performance was proved by many researchers (Wu et al., 2008; Eck et al., 2004). However, our method shows that making better use of the resources in hand could also enhance the quality of SMT system. We use “Baseline+BiMWE” to represent this method. 3.2 New Feature for Bilingual MWEs Lopez and Resnik (2006) once pointed out that better feature mining can lead to substantial gain in translation quality. Inspired by this idea, we append one feature into bilingual phrase table to indicate that whether a bilingual phrase contains bilingual MWEs. In other words, if the source language phrase contains a MWE (as substring) and 50 the target language phrase contains the transla</context>
</contexts>
<marker>Eck, Vogel, Waibel, 2004</marker>
<rawString>Matthias Eck, Stephan Vogel, and Alex Waibel. 2004. Improving statistical machine translation in the medical domain using the unified medical language system. In Proceedings of the 20th international conference on Computational Linguistics table of contents, pages 792–798.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Afsaneh Fazly</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Automatically constructing a lexicon of verb phrase idiomatic combinations.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL,</booktitle>
<pages>337--344</pages>
<contexts>
<context position="2745" citStr="Fazly and Stevenson, 2006" startWordPosition="403" endWordPosition="406">ds. Stanford university launched a MWE project1, in which different qualities of MWE were presented. For bilingual multiword expression (BiMWE), we define a bilingual phrase as a bilingual MWE if (1) the source phrase is a MWE in source language; (2) the source phrase and the target phrase must be translated to each other exactly, i.e. there is no additional (boundary) word in target phrase which cannot find the corresponding word in source phrase, and vice versa. In recent years, many useful methods have been proposed to extract MWEs or BiMWEs automatically (Piao et al., 2005; Bannard, 2007; Fazly and Stevenson, 2006). Since MWE usually constrains possible senses of a polysemous word in context, they can be used in many NLP applications such as information retrieval, question answering, word sense disambiguation and so on. For machine translation, Piao et al. (2005) have noted that the issue of MWE identification and accurate interpretation from source to target language remained an unsolved problem for existing MT systems. This problem is more severe when MT systems are used to translate domain-specific texts, since they may include technical terminology as well as more general fixed expressions and idiom</context>
<context position="8271" citStr="Fazly and Stevenson, 2006" startWordPosition="1269" endWordPosition="1272"> extraction. 2 Bilingual Multiword Expression Extraction In this section we describe our approach of bilingual MWE extraction. In the first step, we obtain monolingual MWEs from the Chinese part of parallel corpus. After that, we look for the translation of the extracted MWEs from parallel corpus. 2.1 Automatic Extraction of MWEs In the past two decades, many different approaches on automatic MWE identification were reported. In general, those approaches can be classified into three main trends: (1) statistical approaches (Pantel and Lin, 2001; Piao et 48 al., 2005), (2) syntactic approaches (Fazly and Stevenson, 2006; Bannard, 2007), and (3) semantic approaches (Baldwin et al., 2003; Cruys and Moir´on, 2007). Syntax-based and semantic-based methods achieve high precision, but syntax or semantic analysis has to be introduced as preparing step, so it is difficult to apply them to domains with few syntactical or semantic annotation. Statistical approaches only consider frequency information, so they can be used to obtain MWEs from bilingual corpora without deeper syntactic or semantic analysis. Most statistical measures only take two words into account, so it not easy to extract MWEs containing three or more</context>
</contexts>
<marker>Fazly, Stevenson, 2006</marker>
<rawString>Afsaneh Fazly and Suzanne Stevenson. 2006. Automatically constructing a lexicon of verb phrase idiomatic combinations. In Proceedings of the EACL, pages 337–344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katerina T Frantzi</author>
<author>Sophia Ananiadou</author>
</authors>
<title>Extracting nested collocations.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational linguistics,</booktitle>
<pages>41--46</pages>
<contexts>
<context position="12599" citStr="Frantzi and Ananiadou, 1996" startWordPosition="2039" endWordPosition="2042">would definitely terminate within N − 1 iterations, which is very efficient. However, HRA has a problem that it would extract substrings before extracting the whole string, even if the substrings only appear in the particular whole string, which we consider useless. To solve this problem, we use contextual features, 4The whole sentence means “healthy tea for preventing hyperlipidemia”, and we give the meaning for each Chinese word: �iR(preventing), n(hyper-), É•(-lipid-), 4(- emia), 0(for), **_(healthy), *(tea). c1(“£) c6(,Wè *) c6(W c7(�) 49 contextual entropy (Luo and Sun, 2003) and Cvalue (Frantzi and Ananiadou, 1996), to filter out those substrings which exist only in few MWEs. 2.2 Automatic Extraction of MWE’s Translation We select and annotate 33000 phrase pairs randomly, of which 30000 pairs are used as training set and 3000 pairs are used as test set. We use the perceptron training algorithm to train the model. As the experiments reveal, the classification precision of this model is 91.67%. In subsection 2.1, we described the algorithm to obtain MWEs, and we would like to introduce the procedure to find their translations from parallel corpus in this subsection. For mining the English translations of </context>
</contexts>
<marker>Frantzi, Ananiadou, 1996</marker>
<rawString>Katerina T. Frantzi and Sophia Ananiadou. 1996. Extracting nested collocations. In Proceedings of the 16th conference on Computational linguistics, pages 41–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>Measuring word alignment quality for statistical machine translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="5666" citStr="Fraser and Marcu, 2007" startWordPosition="856" endWordPosition="859">ord alignment quality. In detail, differences are listed as follows: • Instead of using the bilingual n-gram translation model, we choose the phrase-based SMT system, Moses2, which achieves significantly better translation performance than many other SMT systems and is a state-ofthe-art SMT system. • Instead of improving translation indirectly by improving the word alignment quality, we directly target at the quality of translation. Some researchers have argued that large gains of alignment performance under many metrics only led to small gains in translation performance (Ayan and Dorr, 2006; Fraser and Marcu, 2007). Besides the above differences, there are some advantages of our approaches: • In our method, automatically extracted MWEs are used as additional resources rather than as phrase-table filter. Since bilingual MWEs are extracted according to noisy automatic word alignment, errors in word alignment would further propagate to the SMT and hurt SMT performance. • We conduct experiments on domain-specific corpus. For one thing, domain-specific 2http://www.statmt.org/moses/ corpus potentially includes a large number of technical terminologies as well as more general fixed expressions and idioms, i.e.</context>
</contexts>
<marker>Fraser, Marcu, 2007</marker>
<rawString>Alexander Fraser and Daniel Marcu. 2007. Measuring word alignment quality for statistical machine translation. Computational Linguistics, 33(3):293–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="14595" citStr="Koehn et al., 2003" startWordPosition="2354" endWordPosition="2357">ranslating chance between source phrase and target phrase, and the language features, which refer to how well a candidate is a reasonable translation. The translation features include: (1) the logarithm of source-target translation probability; (2) the logarithm of targetsource translation probability; (3) the logarithm of source-target lexical weighting; (4) the logarithm of target-source lexical weighting; and (5) the logarithm of the phrase pair’s LLR (Dunning, 1993). The first four features are exactly the same as the four translation probabilities used in traditional phrase-based system (Koehn et al., 2003). The language features include: (1) the left entropy of the target phrase (Luo and Sun, 2003); (2) the right entropy of the target phrase; (3) the first word of the target phrase; (4) the last word of the target phrase; and (5) all words in the target phrase. 5http://www.fjoch.com/GIZA++.html 3 Application of Bilingual MWEs Intuitively, bilingual MWE is useful to improve the performance of SMT. However, as we described in section 1, it still needs further research on how to integrate bilingual MWEs into SMT system. In this section, we propose three methods to utilize bilingual MWEs, and we wi</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrik Lambert</author>
<author>Rafael Banchs</author>
</authors>
<title>Data inferred multi-word expressions for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of Machine Translation SummitX,</booktitle>
<pages>396--403</pages>
<contexts>
<context position="3757" citStr="Lambert and Banchs (2005)" startWordPosition="556" endWordPosition="559">em for existing MT systems. This problem is more severe when MT systems are used to translate domain-specific texts, since they may include technical terminology as well as more general fixed expressions and idioms. Although some MT systems may employ a machine-readable bilingual dictionary of MWE, it is time-consuming and inefficient to obtain this resource manually. Therefore, some researchers have tried to use automatically extracted bilingual MWEs in SMT. Tanaka and Baldwin (2003) described an approach of noun-noun compound machine translation, but no significant comparison was presented. Lambert and Banchs (2005) presented a method in which bilingual MWEs were used to modify the word alignment so as to improve the SMT quality. In their work, a bilingual MWE in training corpus was grouped as 1http://mwe.stanford.edu/ 47 Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 47–54, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP one unique token before training alignment models. They reported that both alignment quality and translation accuracy were improved on a small corpus. However, in their further study, they reported even lower BLEU scores after grouping MWEs acco</context>
<context position="23092" citStr="Lambert and Banchs, 2005" startWordPosition="3679" endWordPosition="3682">WEs is relatively small (50000, 60000), the Baseline+Feat method performs better Methods 50000 60000 70000 Baseline 0.2658 Baseline+BiMWE 0.2671 0.2686 0.2715 Baseline+Feat 0.2728 0.2734 0.2712 Baseline+NewBP 0.2662 0.2706 0.2724 Table 4: Translation results of using bilingual MWEs in traditional medicine domain than others; (4) As the number of bilingual MWEs increasing, the Baseline+NewBP method outperforms the Baseline+Feat method; (5) Comparing table 4 and 3, we can see it is not true that the more bilingual MWEs, the better performance of phrase-based SMT. This conclusion is the same as (Lambert and Banchs, 2005). To verify the assumption that bilingual MWEs do indeed improve the SMT performance not only on particular domain, we also perform some experiments on chemical industry domain. Table 5 shows the results. From this table, we can see that these three methods can improve the translation performance on chemical industry domain as well as on the traditional medicine domain. Methods BLEU Baseline 0.1882 Baseline+BiMWE 0.1928 Baseline+Feat 0.1917 Baseline+Newbp 0.1914 Table 5: Translation results of using bilingual MWEs in chemical industry domain 4.4 Discussion In order to know in what respects our</context>
</contexts>
<marker>Lambert, Banchs, 2005</marker>
<rawString>Patrik Lambert and Rafael Banchs. 2005. Data inferred multi-word expressions for statistical machine translation. In Proceedings of Machine Translation SummitX, pages 396–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrik Lambert</author>
<author>Rafael Banchs</author>
</authors>
<title>Grouping multi-word expressions according to part-of-speech in statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Multi-word-expressions in a multilingual context,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="4425" citStr="Lambert and Banchs, 2006" startWordPosition="665" endWordPosition="668"> were used to modify the word alignment so as to improve the SMT quality. In their work, a bilingual MWE in training corpus was grouped as 1http://mwe.stanford.edu/ 47 Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 47–54, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP one unique token before training alignment models. They reported that both alignment quality and translation accuracy were improved on a small corpus. However, in their further study, they reported even lower BLEU scores after grouping MWEs according to part-of-speech on a large corpus (Lambert and Banchs, 2006). Nonetheless, since MWE represents liguistic knowledge, the role and usefulness of MWE in full-scale SMT is intuitively positive. The difficulty lies in how to integrate bilingual MWEs into existing SMT system to improve SMT performance, especially when translating domain texts. In this paper, we implement three methods that integrate domain bilingual MWEs into a phrasebased SMT system, and show that these approaches improve translation quality significantly. The main difference between our methods and Lambert and Banchs’ work is that we directly aim at improving the SMT performance rather th</context>
</contexts>
<marker>Lambert, Banchs, 2006</marker>
<rawString>Patrik Lambert and Rafael Banchs. 2006. Grouping multi-word expressions according to part-of-speech in statistical machine translation. In Proceedings of the Workshop on Multi-word-expressions in a multilingual context, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
<author>Philip Resnik</author>
</authors>
<title>Word-based alignment, phrase-based translation: What’s the link?</title>
<date>2006</date>
<booktitle>In proceedings of the 7th</booktitle>
<pages>90--99</pages>
<contexts>
<context position="16532" citStr="Lopez and Resnik (2006)" startWordPosition="2667" endWordPosition="2670"> more reasonable. Wu et al. (2008) also used this method to perform domain adaption for SMT. Different from their approach, in which bilingual MWEs are extracted from additional corpus, we extract bilingual MWEs from the original training set. The fact that additional resources can improve the domain-specific SMT performance was proved by many researchers (Wu et al., 2008; Eck et al., 2004). However, our method shows that making better use of the resources in hand could also enhance the quality of SMT system. We use “Baseline+BiMWE” to represent this method. 3.2 New Feature for Bilingual MWEs Lopez and Resnik (2006) once pointed out that better feature mining can lead to substantial gain in translation quality. Inspired by this idea, we append one feature into bilingual phrase table to indicate that whether a bilingual phrase contains bilingual MWEs. In other words, if the source language phrase contains a MWE (as substring) and 50 the target language phrase contains the translation of the MWE (as substring), the feature value is 1, otherwise the feature value is set to 0. Due to the high reliability of bilingual MWEs, we expect that this feature could help SMT system to select better and reasonable phra</context>
</contexts>
<marker>Lopez, Resnik, 2006</marker>
<rawString>Adam Lopez and Philip Resnik. 2006. Word-based alignment, phrase-based translation: What’s the link? In proceedings of the 7th conference of the association for machine translation in the Americas: visions for the future of machine translation, pages 90–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shengfen Luo</author>
<author>Maosong Sun</author>
</authors>
<title>Two-character chinese word extraction based on hybrid of internal and contextual measures.</title>
<date>2003</date>
<booktitle>In Proceedings of the second SIGHAN workshop on Chinese language processing,</booktitle>
<pages>24--30</pages>
<contexts>
<context position="12558" citStr="Luo and Sun, 2003" startWordPosition="2032" endWordPosition="2035">ven sentence has length N, HRA would definitely terminate within N − 1 iterations, which is very efficient. However, HRA has a problem that it would extract substrings before extracting the whole string, even if the substrings only appear in the particular whole string, which we consider useless. To solve this problem, we use contextual features, 4The whole sentence means “healthy tea for preventing hyperlipidemia”, and we give the meaning for each Chinese word: �iR(preventing), n(hyper-), É•(-lipid-), 4(- emia), 0(for), **_(healthy), *(tea). c1(“£) c6(,Wè *) c6(W c7(�) 49 contextual entropy (Luo and Sun, 2003) and Cvalue (Frantzi and Ananiadou, 1996), to filter out those substrings which exist only in few MWEs. 2.2 Automatic Extraction of MWE’s Translation We select and annotate 33000 phrase pairs randomly, of which 30000 pairs are used as training set and 3000 pairs are used as test set. We use the perceptron training algorithm to train the model. As the experiments reveal, the classification precision of this model is 91.67%. In subsection 2.1, we described the algorithm to obtain MWEs, and we would like to introduce the procedure to find their translations from parallel corpus in this subsection</context>
<context position="14689" citStr="Luo and Sun, 2003" startWordPosition="2370" endWordPosition="2373">er to how well a candidate is a reasonable translation. The translation features include: (1) the logarithm of source-target translation probability; (2) the logarithm of targetsource translation probability; (3) the logarithm of source-target lexical weighting; (4) the logarithm of target-source lexical weighting; and (5) the logarithm of the phrase pair’s LLR (Dunning, 1993). The first four features are exactly the same as the four translation probabilities used in traditional phrase-based system (Koehn et al., 2003). The language features include: (1) the left entropy of the target phrase (Luo and Sun, 2003); (2) the right entropy of the target phrase; (3) the first word of the target phrase; (4) the last word of the target phrase; and (5) all words in the target phrase. 5http://www.fjoch.com/GIZA++.html 3 Application of Bilingual MWEs Intuitively, bilingual MWE is useful to improve the performance of SMT. However, as we described in section 1, it still needs further research on how to integrate bilingual MWEs into SMT system. In this section, we propose three methods to utilize bilingual MWEs, and we will compare their performance in section 4. 3.1 Model Retraining with Bilingual MWEs Bilingual </context>
</contexts>
<marker>Luo, Sun, 2003</marker>
<rawString>Shengfen Luo and Maosong Sun. 2003. Two-character chinese word extraction based on hybrid of internal and contextual measures. In Proceedings of the second SIGHAN workshop on Chinese language processing, pages 24–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Statistical Machine Translation: From Single-Word Models to Alignment Templates.</title>
<date>2002</date>
<tech>Ph.d. thesis,</tech>
<institution>Computer Science Department, RWTH Aachen,</institution>
<contexts>
<context position="13625" citStr="Och (2002)" startWordPosition="2214" endWordPosition="2215">d the algorithm to obtain MWEs, and we would like to introduce the procedure to find their translations from parallel corpus in this subsection. For mining the English translations of Chinese MWEs, we first obtain the candidate translations of a given MWE from the parallel corpus. Steps are listed as follows: 1. Run GIZA++5 to align words in the training parallel corpus. 2. For a given MWE, find the bilingual sentence pairs where the source language sentences include the MWE. 3. Extract the candidate translations of the MWE from the above sentence pairs according to the algorithm described by Och (2002). After the above procedure, we have already extracted all possible candidate translations of a given MWE. The next step is to distinguish right candidates from wrong candidates. We construct perceptron-based classification model (Collins, 2002) to solve the problem. We design two groups of features: translation features, which describe the mutual translating chance between source phrase and target phrase, and the language features, which refer to how well a candidate is a reasonable translation. The translation features include: (1) the logarithm of source-target translation probability; (2) </context>
<context position="15464" citStr="Och, 2002" startWordPosition="2498" endWordPosition="2499">. 5http://www.fjoch.com/GIZA++.html 3 Application of Bilingual MWEs Intuitively, bilingual MWE is useful to improve the performance of SMT. However, as we described in section 1, it still needs further research on how to integrate bilingual MWEs into SMT system. In this section, we propose three methods to utilize bilingual MWEs, and we will compare their performance in section 4. 3.1 Model Retraining with Bilingual MWEs Bilingual phrase table is very important for phrase-based MT system. However, due to the errors in automatic word alignment and unaligned word extension in phrase extraction (Och, 2002), many meaningless phrases would be extracted, which results in inaccuracy of phrase probability estimation. To alleviate this problem, we take the automatically extracted bilingual MWEs as parallel sentence pairs, add them into the training corpus, and retrain the model using GIZA++. By increasing the occurrences of bilingual MWEs, which are good phrases, we expect that the alignment would be modified and the probability estimation would be more reasonable. Wu et al. (2008) also used this method to perform domain adaption for SMT. Different from their approach, in which bilingual MWEs are ext</context>
<context position="20468" citStr="Och, 2002" startWordPosition="3280" endWordPosition="3281">od, bilingual MWEs are added into training corpus, as a result, new alignment and new phrase table are obtained. For “Baseline+Feat” method, one additional 0/1 feature are introduced to each entry in phrase table. For “Baseline+NewBP”, additional phrase table constructed by bilingual MWEs is used. Features are combined in the log-linear model. To obtain the best translation e� of the source sentence f, log-linear model uses following equation: e� = arg max e amhm(e,f) (1) in which hm and am denote the mth feature and weight. The weights are automatically turned by minimum error rate training (Och, 2002) on development set. 7http://www.nist.gov/speech/tests/mt/resources/scoring.htm Ae|f) = arg max e M m=1 51 4.3 Results Methods BLEU Baseline 0.2658 Baseline+BiMWE 0.2661 Baseline+Feat 0.2675 Baseline+NewBP 0.2719 Table 3: Translation results of using bilingual MWEs in traditional medicine domain Table 3 gives our experiment results. From this table, we can see that, bilingual MWEs improve translation quality in all cases. The Baseline+NewBP method achieves the most improvement of 0.61% BLEU score compared with the baseline system. The Baseline+Feat method comes next with 0.17% BLEU score impro</context>
</contexts>
<marker>Och, 2002</marker>
<rawString>Franz Josef Och. 2002. Statistical Machine Translation: From Single-Word Models to Alignment Templates. Ph.d. thesis, Computer Science Department, RWTH Aachen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>A statistical corpus based term extractor.</title>
<date>2001</date>
<booktitle>In AI ’01: Proceedings of the 14th Biennial Conference of the Canadian Society on Computational Studies of Intelligence,</booktitle>
<pages>36--46</pages>
<contexts>
<context position="8195" citStr="Pantel and Lin, 2001" startWordPosition="1257" endWordPosition="1260">SMT, we only give a brief introduction on monolingual and bilingual MWE extraction. 2 Bilingual Multiword Expression Extraction In this section we describe our approach of bilingual MWE extraction. In the first step, we obtain monolingual MWEs from the Chinese part of parallel corpus. After that, we look for the translation of the extracted MWEs from parallel corpus. 2.1 Automatic Extraction of MWEs In the past two decades, many different approaches on automatic MWE identification were reported. In general, those approaches can be classified into three main trends: (1) statistical approaches (Pantel and Lin, 2001; Piao et 48 al., 2005), (2) syntactic approaches (Fazly and Stevenson, 2006; Bannard, 2007), and (3) semantic approaches (Baldwin et al., 2003; Cruys and Moir´on, 2007). Syntax-based and semantic-based methods achieve high precision, but syntax or semantic analysis has to be introduced as preparing step, so it is difficult to apply them to domains with few syntactical or semantic annotation. Statistical approaches only consider frequency information, so they can be used to obtain MWEs from bilingual corpora without deeper syntactic or semantic analysis. Most statistical measures only take two</context>
</contexts>
<marker>Pantel, Lin, 2001</marker>
<rawString>Patrick Pantel and Dekang Lin. 2001. A statistical corpus based term extractor. In AI ’01: Proceedings of the 14th Biennial Conference of the Canadian Society on Computational Studies of Intelligence, pages 36–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Weijing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Conference of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="19202" citStr="Papineni et al., 2002" startWordPosition="3076" endWordPosition="3079">ces 1,000 Words 31,722 32,390 Test Sentences 1,000 Words 41,643 40,551 Table 1: Traditional medicine corpus 6http://www.speech.sri.com/projects/srilm/ In the chemical industry domain, table 2 gives the detail information of the data. In this experiment, 59466 bilingual MWEs are extracted. Chinese English Training Sentences 120,856 Words 4,532,503 4,311,682 Dev Sentences 1,099 Words 42,122 40,521 Test Sentences 1,099 Words 41,069 39,210 Table 2: Chemical industry corpus We test translation quality on test set and use the open source tool mteval-vllb.pl7 to calculate casesensitive BLEU 4 score (Papineni et al., 2002) as our evaluation criteria. For this evaluation, there is only one reference per test sentence. We also perform statistical significant test between two translation results (Collins et al., 2005). The mean of all scores and relative standard deviation are calculated with a 99% confidence interval of the mean. 4.2 MT Systems We use the state-of-the-art phrase-based SMT system, Moses, as our baseline system. The features used in baseline system include: (1) four translation probability features; (2) one language model feature; (3) distance-based and lexicalized distortion model feature; (4) wor</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Conference of the Association for Computational Linguistics, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Songlin Piao</author>
<author>Paul Rayson</author>
<author>Dawn Archer</author>
<author>Tony McEnery</author>
</authors>
<title>Comparing and combining a semantic tagger and a statistical tool for mwe extraction.</title>
<date>2005</date>
<journal>Computer Speech and Language,</journal>
<volume>19</volume>
<issue>4</issue>
<pages>397</pages>
<contexts>
<context position="2702" citStr="Piao et al., 2005" startWordPosition="397" endWordPosition="400">e derived from their component words. Stanford university launched a MWE project1, in which different qualities of MWE were presented. For bilingual multiword expression (BiMWE), we define a bilingual phrase as a bilingual MWE if (1) the source phrase is a MWE in source language; (2) the source phrase and the target phrase must be translated to each other exactly, i.e. there is no additional (boundary) word in target phrase which cannot find the corresponding word in source phrase, and vice versa. In recent years, many useful methods have been proposed to extract MWEs or BiMWEs automatically (Piao et al., 2005; Bannard, 2007; Fazly and Stevenson, 2006). Since MWE usually constrains possible senses of a polysemous word in context, they can be used in many NLP applications such as information retrieval, question answering, word sense disambiguation and so on. For machine translation, Piao et al. (2005) have noted that the issue of MWE identification and accurate interpretation from source to target language remained an unsolved problem for existing MT systems. This problem is more severe when MT systems are used to translate domain-specific texts, since they may include technical terminology as well </context>
</contexts>
<marker>Piao, Rayson, Archer, McEnery, 2005</marker>
<rawString>Scott Songlin Piao, Paul Rayson, Dawn Archer, and Tony McEnery. 2005. Comparing and combining a semantic tagger and a statistical tool for mwe extraction. Computer Speech and Language, 19(4):378– 397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan A Sag</author>
<author>Timothy Baldwin</author>
<author>Francis Bond</author>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
</authors>
<title>Multiword expressions: A pain in the neck for nlp.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3th International Conference on Intelligent Text Processing and Computational Linguistics(CICLing-2002),</booktitle>
<pages>1--15</pages>
<contexts>
<context position="1867" citStr="Sag et al. (2002)" startWordPosition="262" endWordPosition="265"> However, when syntaxbased models are applied to new domain with few syntax-annotated corpus, the translation performance would decrease. To utilize the robustness of phrases and make up the lack of syntax or semantic information in phrase-based model for domain translation, we study domain bilingual multiword expressions and integrate them to the existing phrase-based model. A multiword expression (MWE) can be considered as word sequence with relatively fixed structure representing special meanings. There is no uniform definition of MWE, and many researchers give different properties of MWE. Sag et al. (2002) roughly defined MWE as “idiosyncratic interpretations that cross word boundaries (or spaces)”. Cruys and Moir´on (2007) focused on the noncompositional property of MWE, i.e. the property that whole expression cannot be derived from their component words. Stanford university launched a MWE project1, in which different qualities of MWE were presented. For bilingual multiword expression (BiMWE), we define a bilingual phrase as a bilingual MWE if (1) the source phrase is a MWE in source language; (2) the source phrase and the target phrase must be translated to each other exactly, i.e. there is n</context>
</contexts>
<marker>Sag, Baldwin, Bond, Copestake, Flickinger, 2002</marker>
<rawString>Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann Copestake, and Dan Flickinger. 2002. Multiword expressions: A pain in the neck for nlp. In Proceedings of the 3th International Conference on Intelligent Text Processing and Computational Linguistics(CICLing-2002), pages 1–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takaaki Tanaka</author>
<author>Timothy Baldwin</author>
</authors>
<title>Nounnoun compound machine translation: A feasibility study on shallow processing.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL-2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="3621" citStr="Tanaka and Baldwin (2003)" startWordPosition="536" endWordPosition="539">5) have noted that the issue of MWE identification and accurate interpretation from source to target language remained an unsolved problem for existing MT systems. This problem is more severe when MT systems are used to translate domain-specific texts, since they may include technical terminology as well as more general fixed expressions and idioms. Although some MT systems may employ a machine-readable bilingual dictionary of MWE, it is time-consuming and inefficient to obtain this resource manually. Therefore, some researchers have tried to use automatically extracted bilingual MWEs in SMT. Tanaka and Baldwin (2003) described an approach of noun-noun compound machine translation, but no significant comparison was presented. Lambert and Banchs (2005) presented a method in which bilingual MWEs were used to modify the word alignment so as to improve the SMT quality. In their work, a bilingual MWE in training corpus was grouped as 1http://mwe.stanford.edu/ 47 Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 47–54, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP one unique token before training alignment models. They reported that both alignment quality and translation </context>
</contexts>
<marker>Tanaka, Baldwin, 2003</marker>
<rawString>Takaaki Tanaka and Timothy Baldwin. 2003. Nounnoun compound machine translation: A feasibility study on shallow processing. In Proceedings of the ACL-2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
<author>Chengqing Zong</author>
</authors>
<title>Domain adaptation for statistical machine translation with domain dictionary and monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of Conference on Computational Linguistics (COLING),</booktitle>
<pages>993--1000</pages>
<contexts>
<context position="15943" citStr="Wu et al. (2008)" startWordPosition="2572" endWordPosition="2575">ase-based MT system. However, due to the errors in automatic word alignment and unaligned word extension in phrase extraction (Och, 2002), many meaningless phrases would be extracted, which results in inaccuracy of phrase probability estimation. To alleviate this problem, we take the automatically extracted bilingual MWEs as parallel sentence pairs, add them into the training corpus, and retrain the model using GIZA++. By increasing the occurrences of bilingual MWEs, which are good phrases, we expect that the alignment would be modified and the probability estimation would be more reasonable. Wu et al. (2008) also used this method to perform domain adaption for SMT. Different from their approach, in which bilingual MWEs are extracted from additional corpus, we extract bilingual MWEs from the original training set. The fact that additional resources can improve the domain-specific SMT performance was proved by many researchers (Wu et al., 2008; Eck et al., 2004). However, our method shows that making better use of the resources in hand could also enhance the quality of SMT system. We use “Baseline+BiMWE” to represent this method. 3.2 New Feature for Bilingual MWEs Lopez and Resnik (2006) once point</context>
<context position="17272" citStr="Wu et al. (2008)" startWordPosition="2790" endWordPosition="2793">append one feature into bilingual phrase table to indicate that whether a bilingual phrase contains bilingual MWEs. In other words, if the source language phrase contains a MWE (as substring) and 50 the target language phrase contains the translation of the MWE (as substring), the feature value is 1, otherwise the feature value is set to 0. Due to the high reliability of bilingual MWEs, we expect that this feature could help SMT system to select better and reasonable phrase pairs during translation. We use “Baseline+Feat” to represent this method. 3.3 Additional Phrase Table of bilingual MWEs Wu et al. (2008) proposed a method to construct a phrase table by a manually-made translation dictionary. Instead of manually constructing translation dictionary, we construct an additional phrase table containing automatically extracted bilingual MWEs. As to probability assignment, we just assign 1 to the four translation probabilities for simplicity. Since Moses supports multiple bilingual phrase tables, we combine the original phrase table and new constructed bilingual MWE table. For each phrase in input sentence during translation, the decoder would search all candidate translation phrases in both phrase </context>
</contexts>
<marker>Wu, Wang, Zong, 2008</marker>
<rawString>Hua Wu, Haifeng Wang, and Chengqing Zong. 2008. Domain adaptation for statistical machine translation with domain dictionary and monolingual corpora. In Proceedings of Conference on Computational Linguistics (COLING), pages 993–1000.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>