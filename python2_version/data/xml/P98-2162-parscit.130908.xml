<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004739">
<title confidence="0.8932985">
Improving Statistical Natural Language Translation with
Categories and Rules
</title>
<author confidence="0.869781">
Franz Josef Och and Hans Weber
</author>
<affiliation confidence="0.863129">
FAU Erlangen - Computer Science Institute,
</affiliation>
<address confidence="0.791234">
IMMD VIII - Artificial Intelligence,
Am Weichselgarten 9, 91058 Erlangen - Tennenlohe, Germany
</address>
<email confidence="0.982765">
Ifaoch,weberl@immd8.informatik.uni-erlangen.de
</email>
<sectionHeader confidence="0.997014" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999821357142857">
This paper describes an all level approach on
statistical natural language translation (SNLT).
Without any predefined knowledge the system
learns a statistical translation lexicon (STL),
word classes (WCs) and translation rules (TRs)
from a parallel corpus thereby producing a gen-
eralized form of a word alignment (WA). The
translation process itself is realized as a beam
search. In our method example-based tech-
niques enter an overall statistical approach lead-
ing to about 50 percent correctly translated
sentences applied to the very difficult English-
German VERBMOBIL spontaneous speech cor-
pus.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999906">
In SNLT the transfer itself is realized as a max-
imization process of the form
</bodyText>
<equation confidence="0.991625">
Trans(d) = argmaxe P(eld) (1)
</equation>
<bodyText confidence="0.999668870967742">
Here d is a given source language (SL) sentence
which has to be translated into a target lan-
guage (TL) sentence e. In order to model the
distributions P(eld) all approaches in SNLT use
a &amp;quot;divide and conquer&amp;quot; strategy of approximat-
ing P(eld) by a combination of simpler models.
The problem is to reduce parameters in a suffi-
cient way but end up with a model still able to
describe the linguistic facts of natural language
translation.
The work presented here uses two approxi-
mations for P(eld). One approximation is used
for to gain the relevant parameters in training
while a modified formula is subject of decoding
translations. In detail, we impose the following
modifications with respect to approaches pub-
lished in the last decade: 1. A refined distance
weight for the STL probabilities is used which
allows for a good modeling of the effects caused
by syntactic phrases. 2. In order to account for
collocations a WA technique is used, where one-
to-n and n-to-one WAs are allowed. 3. For
the translation WCs are used which are con-
structed using clustering techniques, where the
STL forms a part of the optimization criterion.
4. A set of TRs is learned mapping sequences
of SL WCs to sequences of TL WCs.
Throughout the paper the four topics above
are described in more detail. Finally we report
on experimental results produced on the VERB-
MOBIL corpus.
</bodyText>
<sectionHeader confidence="0.7920175" genericHeader="introduction">
2 Learning of the Translation
Lexicon
</sectionHeader>
<bodyText confidence="0.999493125">
In order to determine the STL, we use a sta-
tistical model for translation and the EM algo-
rithm to adjust its model parameters. The sim-
ple model 1 (Brown et al., 1993) for the trans-
lation of a SL sentence d = c11 di in a TL
sentence e = el ... em assumes that every TL
word is generated independently as a mixture
of the SL words:
</bodyText>
<equation confidence="0.959306">
m
P(eld) lETE t(eildi) (2)
j=1 z=o
</equation>
<bodyText confidence="0.999895166666667">
In the equation above t(eildi) stands for the
probability that ei is generated by di.
The assumption that each SL word influences
every TL word with the same strength appears
to be too simple. In the refined model 2 (Brown
et al., 1993) alignment probabilities a(i1j,l,m)
are included to model the effect that the po-
sition of a word influences the position of its
translation.
The phrasal organization of natural languages
is well known and has been described by (Jack-
endorff, 1977) among many others. The tra-
</bodyText>
<page confidence="0.998455">
985
</page>
<bodyText confidence="0.9985969">
ditional alignment probabilities depend on ab-
solute positions and do not take that into ac-
count, as has already been noted by (Vogel et
al., 1996). Therefore we developed a kind of
relative weighting probability. The following
model — which we will call the model 2&apos; —
makes the weight between the words di and ej
dependent on the relative distances between the
words dk which generated the previous word
ej_1:
</bodyText>
<equation confidence="0.985893">
s(iIj, ei_i, d) E d(i - 1c11) • t(ei-i kik) (3)
k=0
</equation>
<bodyText confidence="0.999627">
Here d(i - kl/) is the probability that word di
influences a word ej if the previous word e3_1 is
influenced by dk. As an effect of such a weight
a (phrase-)cluster of words being moved over a
long distance receives additional &apos;cost&apos; only at
the ends of the cluster. So we have the final
translation probability for model 2&apos;:
</bodyText>
<equation confidence="0.994822666666667">
m
P(eld) H E ei-i, d) (4)
j=1
</equation>
<bodyText confidence="0.999867166666667">
The parameters involved can be determined us-
ing the EM algorithm (Baum, 1972). The ap-
plication of this algorithm to the basic prob-
lem using a parallel bilingual corpus aligned on
the sentence level is described in (Brown et al.,
1993).
</bodyText>
<sectionHeader confidence="0.945417" genericHeader="method">
3 Determining a Word Alignment
</sectionHeader>
<bodyText confidence="0.9999513125">
The kind of WA we use is more general than
the often used WA through a vector, where ev-
ery TL word is generated by exactly one SL
word. We use a matrix Z for every sentence
pair, whose fields describe whether or not two
words are aligned. In this approach, multiple
words can be aligned to one TL word, which is
motivated by collocation phenomena as for in-
stance German compound nouns. Alignments
may look like the one in figure 1 according to our
method. The matrix Z contains i + 1 lines and
j rows with binary values. The value zij = 1
(zij = 0) means that the word i influences (not)
the word j. In figure 1 every link stands for
zij = 1.
The models 1, 2 and 2&apos; and some similar mod-
</bodyText>
<figureCaption confidence="0.9759115">
Figure 1: Alignment example.
els can be described in the form
</figureCaption>
<equation confidence="0.947596">
m (5)
P(eld) H E xii
</equation>
<bodyText confidence="0.999594291666666">
where the value xij is the strength of the influ-
ence of word di to word ej. We use a thresh-
old 0 &lt; 1 in such a way that while the sum
EZ=0
xi of the first s values is smaller than
0. EL0 xki we set zisj = 0. The other values
are set to 1. The permutation io, , it sorts the
so that xzoj &lt; &lt; x.
Interestingly using such a WA technique does
not in general lead to the same results when
applied from TL to SL and vice versa. If we
use P(eld) or P(dle) we receive different WAs
43d and 43e. Intuitively the relation between the
words of the sentences should be symmetric and
there should be the same WA. It is possible to
enforce the symmetry with z = 431 • 4/3e, in
order to make a link between two words only if
there is a link in both WAs.
It is possible to include the WA into the EM
algorithm for the estimation of the model prob-
abilities. This can be done by replacing t(ei Idi)
by t(e3Idi) • z. The resulting STL becomes
much cleaner in the sense that it does not con-
tain so many wrong entries (see section 7).
</bodyText>
<sectionHeader confidence="0.829768" genericHeader="method">
4 Learning of Translation Rules
</sectionHeader>
<bodyText confidence="0.999902357142857">
The incorporation of TRs adds an &amp;quot;example-
based&amp;quot; touch to the statistical approach. In a
very naive approach a TR could be represented
by a translation example. The obvious advan-
tage is an expectable good quality of the trans-
lated sentences. The disadvantage is the fact
that almost no sentence can be translated be-
cause every corpus would have too few examples
— the generalization capability of the naive ap-
proach is very limited.
We desired a general kind of TR which does
not use explicit linguistic properties of the used
languages. In addition the rules should general-
ize from very sparse data. Therefore it seemed
</bodyText>
<figure confidence="0.789695">
Pfingstmontag
about Monday
</figure>
<page confidence="0.994142">
986
</page>
<bodyText confidence="0.999865666666667">
natural to use WCs and shorter sequences to
end up with a set of rather general rules. In or-
der to achieve a good learning performance, all
the WCs of a language are pairwise disjoint (see
section 5). The function C(-) gives the class of
a word or the sequence of WCs of a sequence of
words.
Our TRs are triples (D, E, Z) where D is a
sequence of SL WCs, E is a sequence of TL WCs
and Z is a WA matrix between D and E. For
using one rule in the translation process we first
rewrite the probability P(eld):
</bodyText>
<equation confidence="0.9181615">
P(eld) = E P(E, Z1d) • P(elE, Z, d) (6)
E,Z
</equation>
<bodyText confidence="0.999778333333333">
In order to simplify the maximization (equation
1) we use only the TR which gives the maximum
probability.
During the learning of those TRs we count all
extractable rules occurring in the aligned cor-
pus and define the probability p(E, ZIC(d))
P(E, Z Id) in terms of the relative frequency.
We approximate P(elE, Z, d) by simpler
probabilities, so that we finally need a language
model p(eile31-1), a translation model p(ei Id, Z)
and a probability p(eilEi). For p(ei 16.31-1) we
use a class-based polygram language model
(Schukat-Talamazzini, 1994). For the transla-
tion probability p(e3 Id, Z) we use model 1 and
include the information of the WA:
</bodyText>
<equation confidence="0.5185335">
p(eild, Z) := E t(eildi) • zii (7)
i=o
</equation>
<bodyText confidence="0.999581142857143">
Figure 2 shows how the application of those
rules works in principle. We arrive at a list of
word hypotheses with probabilities for each po-
sition. Neglecting the language model, the best
decision would be to independently choose the
most probable word for every position.
In general the translation of a sentence in-
volves more than one rule and usually there are
many rules applicable. An applicable rule is one
where the sequence of SL WCs matches a se-
quence of WCs in the sentence. So in the gen-
eral case we have to decide for a set of rules we
want to apply. This set of rules has to cover the
sentence, this means that every word is used in
a rule and that no word is used twice or more
times. The next step is to decide how to ar-
range the generated units to get the translated
sentence. Finally we have to decide for every
position which word to use. We want all those
decisions to be optimal in the sense that the
following product is maximized:
</bodyText>
<equation confidence="0.8679935">
p(e01) o • • 0 eUL)) . 11 p(z(k),E(k)r(d(k))
d(k)) (8)
Here L is the number of SL units, d(k) is the k-th
SL unit, e(k) is the k-th TL unit and ii,... ,
</equation>
<bodyText confidence="0.885791">
is a permutation of the numbers 1, , L.
</bodyText>
<sectionHeader confidence="0.798693" genericHeader="method">
5 Learning of Category Systems
</sectionHeader>
<bodyText confidence="0.999623621621621">
During the last decade some publications have
discussed the problem of learning WCs using
clustering techniques based on maximum like-
lihood criteria applied to single language cor-
pora. The question which we pose in addition
is: Which WCs are suitable for translation? It
seems to make sense to require that the used
WCs in the two languages are correlated, so
that the information about the class of a SL
word gives much information about the class of
the generated TL word. Therefore it has been
argued in (Fung and Wu, 1995) that indepen-
dently generated WCs are not good for the use
in translation.
For the automatic generation of class systems
exists a well known procedure (see (Kneser and
Ney, 1993), (Och, 1995)) which maximizes the
perplexity of the language model for a training
corpus by moving one word from a class to an-
other in an iterative procedure. The function
ML(CIN,) which has to be optimized de-
pends only on the count function Art„,, which
counts the frequency that the word w&apos; comes
after the word w.
Using two sets of WCs for the TL and SL
which are independent (method INDEP) does
not guarantee that those WCs are much cor-
related. The resulting WCs have only the prop-
erty that the information about the class of a
word w has much information about the class
of the following word w&apos;. We want for the
WCs used for translation that the information
about the WC of a word has much information
about the WC of the translation. For the use
of the standard method for optimizing WCs we
need only define a count function Nd..4e, which
we do by Nd,e(d,e) := t(eld) • n(e). In the
</bodyText>
<page confidence="0.980547">
987
</page>
<figure confidence="0.999345857142857">
source text
FM
CM
MI
word hypotheses E&gt; translated text
translation rule
CI=
</figure>
<figureCaption confidence="0.999961">
Figure 2: Application of a Rule.
</figureCaption>
<bodyText confidence="0.999251266666667">
same way a count function Ned can be deter-
mined and we get the new optimization criterion
ML(Cdl-t1C,INde+Ne_4(1). The resulting classes
are strongly correlated, but rarely contain words
with similar syntactic/semantic properties. To
arrive at WCs having both (method COMB), we
determine TL WCs with the first method and
afterwards we determine SL WCs with the sec-
ond method.
So we can use the well known iterative
method to end up with WCs in different lan-
guages which are correlated. From those WCs
we expect that they are more suitable for build-
ing the TRs from section 4 and finally result in
a better overall translation performance.
</bodyText>
<sectionHeader confidence="0.739344" genericHeader="method">
6 Translation as a Search Problem
</sectionHeader>
<bodyText confidence="0.999943444444444">
The problem of finding the translation of a sen-
tence can be viewed as a search problem for a
path with minimal cost in a tree. If we apply
the negative logarithm to the product of proba-
bilities in equation 8 we arrive at a sum of costs
which has to be minimized. The costs stem from
the language model, the rule probabilities and
the translation probabilities. In the search tree
every node represents a partial translation for
the first words or a full translation. The leaves
of the tree are the nodes where the applied rules
define a complete cover of the SL sentence. To
reduce the search space we use additional costs
for changing the order of the fragments.
We use a beam search strategy (Greer et al.,
1982) to find a good path in this tree. To make
the search feasible we had to implement some
problem specific heuristics.
</bodyText>
<sectionHeader confidence="0.999852" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.999386375">
The experiments in this section have all been
carried out on the bilingual German-English
VERBMOBIL corpus. This corpus consists of
spontaneous utterances from negotiation di-
alogs which had originally been produced in
German. For training we used 11 500 randomly
chosen sentence pairs.
The first experiment shall be understood as
an illustration for our improved technique in
generating a STL using the WA in the EM-
algorithm. We generated a STL using 10 EM-
iterations for model 1 and 10 iterations for
model 2&apos;. The whole process took about 4 hours
for our corpus. Below are given some STL en-
tries for German words. The probabilities t(eld)
are written in parentheses.
</bodyText>
<listItem confidence="0.9961682">
• Tuesday-Dienstag (0.83), den (0.05),
COMMA (0.042), am (0.038), dienstags
(0.018), der (0.009), also (0.0069), passen
(0.0019), diesem (0.0013), steht (0.0012)
• Frankfurt-Frankfurt (0.67), nach (0.12),
</listItem>
<bodyText confidence="0.870552555555556">
in (0.081), mit (0.068), um (0.031),
habe (0.02), besuchen (0.0078), wiederum
(0.0036)
The top positions are always plausible trans-
lations. But there are many improper transla-
tions produced. When we include the WA in the
EM algorithm as described in section 3 we can
produce fewer lexicon entries of a much better
quality:
</bodyText>
<listItem confidence="0.999451666666667">
• Tuesday-&gt;Dienstag (0.97), dienstags
(0.029)
• Frankfurt-Frankfurt (1)
</listItem>
<bodyText confidence="0.827863">
The following two corresponding WCs (out of
600) show a typical result of the method COMB
to determine correlated WCs:
</bodyText>
<listItem confidence="0.620125">
• Mittwoch, Donnerstag, Freitag,
</listItem>
<reference confidence="0.956307">
Sonnabend, Friihlingsanfang, Karsamstag,
Volkstrauertag, Weihnachtsferien, Som-
merschule, Thomas, einschlieBen
• Wednesday, Thursday, Friday, Thursdays,
Fridays, Thomas, Veterans&apos;, mourning, na-
tional, spending, spring, summer-school
</reference>
<page confidence="0.998767">
988
</page>
<bodyText confidence="0.999877538461539">
To evaluate the complete system we translated
200 randomly chosen sentences drawn from an
independent test corpus and checked manually
how many of them constituted acceptable trans-
lations. Since we used a spontaneous speech
corpus many sentences were grammatically in-
correct. A translation is classified &apos;correct&apos; if
the translation is an error-free (spontaneaous
speech) utterance and classified &apos;understand-
able&apos; if the intention of the utterance is trans-
lated. The 100 sentences had a mean sentence
length of 10 words. The used STL was gener-
ated using model 2&apos; (see section 2).
</bodyText>
<table confidence="0.992761666666667">
correct understandable
INDEP 46.5 % 64 %
COMB 52% 71%
</table>
<tableCaption confidence="0.96725">
Table 1: Quality of Translation.
Some example translations:
</tableCaption>
<bodyText confidence="0.915989">
• was haltst du von zweiter Februar nachmit-
tags, nach fiinfzehn Uhr -4 what do you
think about the second of February in the
afternoon, after three o&apos;clock
• I wanted to fix a time with you for a five-
day business trip to Stuttgart -+ ich wollte
mit Ihnen einen Termin ausmachen fiir eine
fiinftagige Geschaftsreise nach Stuttgart
</bodyText>
<sectionHeader confidence="0.999192" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999991368421053">
We have presented a couple of improvements
to SNLT. The most important changes are the
translation model 2&apos;, the representation of WA
using a matrix, a method to determine corre-
lated WCs and the use of TRs to constrain
search. In the future, the rule mechanism
should be extended. So far the rules learned
are only loop-free finite state transducers. Still
many translation errors stem from the inability
to model long distance dependencies. We intend
to move to finite state cascades or context free
grammars in future work. With respect to the
category sets we feel that an additional morpho-
logical model could further improve the transla-
tion quality. As it stands the system still makes
many errors concerning the number of nominals
and verbs. This is especially important when
the language pairs differ with respect to the pro-
ductivity of their inflectional systems.
</bodyText>
<sectionHeader confidence="0.993282" genericHeader="acknowledgments">
9 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999411">
We have to thank Stefan Vogel from the RWTH
Aachen explicitly, for the material he provided
and Gunther G6rz for general promotion. The
work is part of the German Joint Project VERB-
MOBIL. This work was funded by the German
Federal Ministry for Research and Technology
(BMBF) in the framework of the Verbmobil
Project under Grant BMBF 01 IV 701 K 5. The
responsibility for the contents of this study lies
with the authors.
</bodyText>
<sectionHeader confidence="0.999226" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999918459459459">
L.E. Baum. 1972. An Inequality and Asso-
ciated Maximization Technique in Statisti-
cal Estimation for Probabilistic Functions of
Markov Processes. Inequalities, 3:1-8.
P. F. Brown, S. A. Della Pietra, V. J.
Della Pietra, and R. L. Mercer. 1993. The
mathematics of statistical machine transla-
tion: Parameter estimation. Computational
Linguistics, 19(2):263-311.
P. Fung and D. Wu. 1995. Coerced markov
models for cross-lingual lexical-tag relations.
In The Sixth Int. Conf on Theor. and Method-
ological Issues in Machine Translation, pages
240-255, Leuven, Belgium, July.
K. Greer, B. Lowerre, and L. Wilcox. 1982.
Acoustic Pattern Matching and Beam Search-
ing. In Proc. Int. Conf. on Acoustics, Speech,
and Signal Processing, pages 1251-1254,
Paris.
R. Jackendorff. 1977. X-bar-syntax: A study
of phrase structure. In Linguistic Inquiry
Monograph 2.
R. Kneser and H. Ney. 1993. Improved Clus-
tering Techniques for Class-Based Statistical
Language Modelling. In Eurospeech, pages
973-976.
F. J. Och. 1995. Maximum-Likelihood-
Schatzung von Wortkategorien mit Verfahren
der kombinatorischen Optimierung. Studien-
arbeit, FAU Erlangen-Nurnberg.
E.G. Schukat-Talamazzini. 1994. Automatische
Spracherkennung. Vieweg, Wiesbaden.
S. Vogel, H. Ney, and C. Tillmann. 1996.
HMM-Based Word Alignment in Statistical
Translation. In Proc. Int. Conf. on Compu-
tational Linguistics, pages 836-841, Kopen-
hagen, August.
</reference>
<page confidence="0.998786">
989
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.558652">
<title confidence="0.999505">Improving Statistical Natural Language Translation with Categories and Rules</title>
<author confidence="0.99992">Franz Josef Och</author>
<author confidence="0.99992">Hans Weber</author>
<affiliation confidence="0.9880955">FAU Erlangen - Computer Science Institute, IMMD VIII - Artificial Intelligence,</affiliation>
<address confidence="0.984262">Am Weichselgarten 9, 91058 Erlangen - Tennenlohe, Germany</address>
<email confidence="0.9105">Ifaoch,weberl@immd8.informatik.uni-erlangen.de</email>
<abstract confidence="0.958639466666667">This paper describes an all level approach on statistical natural language translation (SNLT). Without any predefined knowledge the system learns a statistical translation lexicon (STL), word classes (WCs) and translation rules (TRs) from a parallel corpus thereby producing a generalized form of a word alignment (WA). The translation process itself is realized as a beam search. In our method example-based techniques enter an overall statistical approach leading to about 50 percent correctly translated sentences applied to the very difficult English- German VERBMOBIL spontaneous speech corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Friihlingsanfang Sonnabend</author>
</authors>
<location>Karsamstag, Volkstrauertag, Weihnachtsferien, Sommerschule, Thomas, einschlieBen</location>
<marker>Sonnabend, </marker>
<rawString>Sonnabend, Friihlingsanfang, Karsamstag, Volkstrauertag, Weihnachtsferien, Sommerschule, Thomas, einschlieBen</rawString>
</citation>
<citation valid="false">
<authors>
<author>Wednesday</author>
</authors>
<location>Thursday, Friday, Thursdays, Fridays, Thomas, Veterans&apos;, mourning,</location>
<note>national, spending, spring, summer-school</note>
<marker>Wednesday, </marker>
<rawString>• Wednesday, Thursday, Friday, Thursdays, Fridays, Thomas, Veterans&apos;, mourning, national, spending, spring, summer-school</rawString>
</citation>
<citation valid="true">
<authors>
<author>L E Baum</author>
</authors>
<title>An Inequality and Associated Maximization Technique</title>
<date>1972</date>
<booktitle>in Statistical Estimation for Probabilistic Functions of Markov Processes. Inequalities,</booktitle>
<pages>3--1</pages>
<contexts>
<context position="4192" citStr="Baum, 1972" startWordPosition="713" endWordPosition="714"> between the words di and ej dependent on the relative distances between the words dk which generated the previous word ej_1: s(iIj, ei_i, d) E d(i - 1c11) • t(ei-i kik) (3) k=0 Here d(i - kl/) is the probability that word di influences a word ej if the previous word e3_1 is influenced by dk. As an effect of such a weight a (phrase-)cluster of words being moved over a long distance receives additional &apos;cost&apos; only at the ends of the cluster. So we have the final translation probability for model 2&apos;: m P(eld) H E ei-i, d) (4) j=1 The parameters involved can be determined using the EM algorithm (Baum, 1972). The application of this algorithm to the basic problem using a parallel bilingual corpus aligned on the sentence level is described in (Brown et al., 1993). 3 Determining a Word Alignment The kind of WA we use is more general than the often used WA through a vector, where every TL word is generated by exactly one SL word. We use a matrix Z for every sentence pair, whose fields describe whether or not two words are aligned. In this approach, multiple words can be aligned to one TL word, which is motivated by collocation phenomena as for instance German compound nouns. Alignments may look like</context>
</contexts>
<marker>Baum, 1972</marker>
<rawString>L.E. Baum. 1972. An Inequality and Associated Maximization Technique in Statistical Estimation for Probabilistic Functions of Markov Processes. Inequalities, 3:1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="2584" citStr="Brown et al., 1993" startWordPosition="416" endWordPosition="419">ere oneto-n and n-to-one WAs are allowed. 3. For the translation WCs are used which are constructed using clustering techniques, where the STL forms a part of the optimization criterion. 4. A set of TRs is learned mapping sequences of SL WCs to sequences of TL WCs. Throughout the paper the four topics above are described in more detail. Finally we report on experimental results produced on the VERBMOBIL corpus. 2 Learning of the Translation Lexicon In order to determine the STL, we use a statistical model for translation and the EM algorithm to adjust its model parameters. The simple model 1 (Brown et al., 1993) for the translation of a SL sentence d = c11 di in a TL sentence e = el ... em assumes that every TL word is generated independently as a mixture of the SL words: m P(eld) lETE t(eildi) (2) j=1 z=o In the equation above t(eildi) stands for the probability that ei is generated by di. The assumption that each SL word influences every TL word with the same strength appears to be too simple. In the refined model 2 (Brown et al., 1993) alignment probabilities a(i1j,l,m) are included to model the effect that the position of a word influences the position of its translation. The phrasal organization</context>
<context position="4349" citStr="Brown et al., 1993" startWordPosition="739" endWordPosition="742"> 1c11) • t(ei-i kik) (3) k=0 Here d(i - kl/) is the probability that word di influences a word ej if the previous word e3_1 is influenced by dk. As an effect of such a weight a (phrase-)cluster of words being moved over a long distance receives additional &apos;cost&apos; only at the ends of the cluster. So we have the final translation probability for model 2&apos;: m P(eld) H E ei-i, d) (4) j=1 The parameters involved can be determined using the EM algorithm (Baum, 1972). The application of this algorithm to the basic problem using a parallel bilingual corpus aligned on the sentence level is described in (Brown et al., 1993). 3 Determining a Word Alignment The kind of WA we use is more general than the often used WA through a vector, where every TL word is generated by exactly one SL word. We use a matrix Z for every sentence pair, whose fields describe whether or not two words are aligned. In this approach, multiple words can be aligned to one TL word, which is motivated by collocation phenomena as for instance German compound nouns. Alignments may look like the one in figure 1 according to our method. The matrix Z contains i + 1 lines and j rows with binary values. The value zij = 1 (zij = 0) means that the wor</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
<author>D Wu</author>
</authors>
<title>Coerced markov models for cross-lingual lexical-tag relations.</title>
<date>1995</date>
<booktitle>In The Sixth Int. Conf on Theor. and Methodological Issues in Machine Translation,</booktitle>
<pages>240--255</pages>
<location>Leuven, Belgium,</location>
<contexts>
<context position="9787" citStr="Fung and Wu, 1995" startWordPosition="1775" endWordPosition="1778">TL unit and ii,... , is a permutation of the numbers 1, , L. 5 Learning of Category Systems During the last decade some publications have discussed the problem of learning WCs using clustering techniques based on maximum likelihood criteria applied to single language corpora. The question which we pose in addition is: Which WCs are suitable for translation? It seems to make sense to require that the used WCs in the two languages are correlated, so that the information about the class of a SL word gives much information about the class of the generated TL word. Therefore it has been argued in (Fung and Wu, 1995) that independently generated WCs are not good for the use in translation. For the automatic generation of class systems exists a well known procedure (see (Kneser and Ney, 1993), (Och, 1995)) which maximizes the perplexity of the language model for a training corpus by moving one word from a class to another in an iterative procedure. The function ML(CIN,) which has to be optimized depends only on the count function Art„,, which counts the frequency that the word w&apos; comes after the word w. Using two sets of WCs for the TL and SL which are independent (method INDEP) does not guarantee that tho</context>
</contexts>
<marker>Fung, Wu, 1995</marker>
<rawString>P. Fung and D. Wu. 1995. Coerced markov models for cross-lingual lexical-tag relations. In The Sixth Int. Conf on Theor. and Methodological Issues in Machine Translation, pages 240-255, Leuven, Belgium, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Greer</author>
<author>B Lowerre</author>
<author>L Wilcox</author>
</authors>
<title>Acoustic Pattern Matching and Beam Searching.</title>
<date>1982</date>
<booktitle>In Proc. Int. Conf. on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>1251--1254</pages>
<location>Paris.</location>
<contexts>
<context position="12357" citStr="Greer et al., 1982" startWordPosition="2235" endWordPosition="2238">th with minimal cost in a tree. If we apply the negative logarithm to the product of probabilities in equation 8 we arrive at a sum of costs which has to be minimized. The costs stem from the language model, the rule probabilities and the translation probabilities. In the search tree every node represents a partial translation for the first words or a full translation. The leaves of the tree are the nodes where the applied rules define a complete cover of the SL sentence. To reduce the search space we use additional costs for changing the order of the fragments. We use a beam search strategy (Greer et al., 1982) to find a good path in this tree. To make the search feasible we had to implement some problem specific heuristics. 7 Results The experiments in this section have all been carried out on the bilingual German-English VERBMOBIL corpus. This corpus consists of spontaneous utterances from negotiation dialogs which had originally been produced in German. For training we used 11 500 randomly chosen sentence pairs. The first experiment shall be understood as an illustration for our improved technique in generating a STL using the WA in the EMalgorithm. We generated a STL using 10 EMiterations for mo</context>
</contexts>
<marker>Greer, Lowerre, Wilcox, 1982</marker>
<rawString>K. Greer, B. Lowerre, and L. Wilcox. 1982. Acoustic Pattern Matching and Beam Searching. In Proc. Int. Conf. on Acoustics, Speech, and Signal Processing, pages 1251-1254, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jackendorff</author>
</authors>
<title>X-bar-syntax: A study of phrase structure.</title>
<date>1977</date>
<journal>In Linguistic Inquiry Monograph</journal>
<volume>2</volume>
<contexts>
<context position="3265" citStr="Jackendorff, 1977" startWordPosition="542" endWordPosition="544">ce e = el ... em assumes that every TL word is generated independently as a mixture of the SL words: m P(eld) lETE t(eildi) (2) j=1 z=o In the equation above t(eildi) stands for the probability that ei is generated by di. The assumption that each SL word influences every TL word with the same strength appears to be too simple. In the refined model 2 (Brown et al., 1993) alignment probabilities a(i1j,l,m) are included to model the effect that the position of a word influences the position of its translation. The phrasal organization of natural languages is well known and has been described by (Jackendorff, 1977) among many others. The tra985 ditional alignment probabilities depend on absolute positions and do not take that into account, as has already been noted by (Vogel et al., 1996). Therefore we developed a kind of relative weighting probability. The following model — which we will call the model 2&apos; — makes the weight between the words di and ej dependent on the relative distances between the words dk which generated the previous word ej_1: s(iIj, ei_i, d) E d(i - 1c11) • t(ei-i kik) (3) k=0 Here d(i - kl/) is the probability that word di influences a word ej if the previous word e3_1 is influenc</context>
</contexts>
<marker>Jackendorff, 1977</marker>
<rawString>R. Jackendorff. 1977. X-bar-syntax: A study of phrase structure. In Linguistic Inquiry Monograph 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>H Ney</author>
</authors>
<title>Improved Clustering Techniques for Class-Based Statistical Language Modelling.</title>
<date>1993</date>
<booktitle>In Eurospeech,</booktitle>
<pages>973--976</pages>
<contexts>
<context position="9965" citStr="Kneser and Ney, 1993" startWordPosition="1805" endWordPosition="1808"> using clustering techniques based on maximum likelihood criteria applied to single language corpora. The question which we pose in addition is: Which WCs are suitable for translation? It seems to make sense to require that the used WCs in the two languages are correlated, so that the information about the class of a SL word gives much information about the class of the generated TL word. Therefore it has been argued in (Fung and Wu, 1995) that independently generated WCs are not good for the use in translation. For the automatic generation of class systems exists a well known procedure (see (Kneser and Ney, 1993), (Och, 1995)) which maximizes the perplexity of the language model for a training corpus by moving one word from a class to another in an iterative procedure. The function ML(CIN,) which has to be optimized depends only on the count function Art„,, which counts the frequency that the word w&apos; comes after the word w. Using two sets of WCs for the TL and SL which are independent (method INDEP) does not guarantee that those WCs are much correlated. The resulting WCs have only the property that the information about the class of a word w has much information about the class of the following word w</context>
</contexts>
<marker>Kneser, Ney, 1993</marker>
<rawString>R. Kneser and H. Ney. 1993. Improved Clustering Techniques for Class-Based Statistical Language Modelling. In Eurospeech, pages 973-976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<date>1995</date>
<booktitle>Maximum-LikelihoodSchatzung von Wortkategorien mit Verfahren der kombinatorischen Optimierung. Studienarbeit, FAU Erlangen-Nurnberg.</booktitle>
<contexts>
<context position="9978" citStr="Och, 1995" startWordPosition="1809" endWordPosition="1810">ques based on maximum likelihood criteria applied to single language corpora. The question which we pose in addition is: Which WCs are suitable for translation? It seems to make sense to require that the used WCs in the two languages are correlated, so that the information about the class of a SL word gives much information about the class of the generated TL word. Therefore it has been argued in (Fung and Wu, 1995) that independently generated WCs are not good for the use in translation. For the automatic generation of class systems exists a well known procedure (see (Kneser and Ney, 1993), (Och, 1995)) which maximizes the perplexity of the language model for a training corpus by moving one word from a class to another in an iterative procedure. The function ML(CIN,) which has to be optimized depends only on the count function Art„,, which counts the frequency that the word w&apos; comes after the word w. Using two sets of WCs for the TL and SL which are independent (method INDEP) does not guarantee that those WCs are much correlated. The resulting WCs have only the property that the information about the class of a word w has much information about the class of the following word w&apos;. We want fo</context>
</contexts>
<marker>Och, 1995</marker>
<rawString>F. J. Och. 1995. Maximum-LikelihoodSchatzung von Wortkategorien mit Verfahren der kombinatorischen Optimierung. Studienarbeit, FAU Erlangen-Nurnberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E G Schukat-Talamazzini</author>
</authors>
<title>Automatische Spracherkennung.</title>
<date>1994</date>
<location>Vieweg, Wiesbaden.</location>
<contexts>
<context position="7946" citStr="Schukat-Talamazzini, 1994" startWordPosition="1430" endWordPosition="1431">t rewrite the probability P(eld): P(eld) = E P(E, Z1d) • P(elE, Z, d) (6) E,Z In order to simplify the maximization (equation 1) we use only the TR which gives the maximum probability. During the learning of those TRs we count all extractable rules occurring in the aligned corpus and define the probability p(E, ZIC(d)) P(E, Z Id) in terms of the relative frequency. We approximate P(elE, Z, d) by simpler probabilities, so that we finally need a language model p(eile31-1), a translation model p(ei Id, Z) and a probability p(eilEi). For p(ei 16.31-1) we use a class-based polygram language model (Schukat-Talamazzini, 1994). For the translation probability p(e3 Id, Z) we use model 1 and include the information of the WA: p(eild, Z) := E t(eildi) • zii (7) i=o Figure 2 shows how the application of those rules works in principle. We arrive at a list of word hypotheses with probabilities for each position. Neglecting the language model, the best decision would be to independently choose the most probable word for every position. In general the translation of a sentence involves more than one rule and usually there are many rules applicable. An applicable rule is one where the sequence of SL WCs matches a sequence o</context>
</contexts>
<marker>Schukat-Talamazzini, 1994</marker>
<rawString>E.G. Schukat-Talamazzini. 1994. Automatische Spracherkennung. Vieweg, Wiesbaden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>H Ney</author>
<author>C Tillmann</author>
</authors>
<title>HMM-Based Word Alignment in Statistical Translation.</title>
<date>1996</date>
<booktitle>In Proc. Int. Conf. on Computational Linguistics,</booktitle>
<pages>836--841</pages>
<location>Kopenhagen,</location>
<contexts>
<context position="3442" citStr="Vogel et al., 1996" startWordPosition="573" endWordPosition="576">r the probability that ei is generated by di. The assumption that each SL word influences every TL word with the same strength appears to be too simple. In the refined model 2 (Brown et al., 1993) alignment probabilities a(i1j,l,m) are included to model the effect that the position of a word influences the position of its translation. The phrasal organization of natural languages is well known and has been described by (Jackendorff, 1977) among many others. The tra985 ditional alignment probabilities depend on absolute positions and do not take that into account, as has already been noted by (Vogel et al., 1996). Therefore we developed a kind of relative weighting probability. The following model — which we will call the model 2&apos; — makes the weight between the words di and ej dependent on the relative distances between the words dk which generated the previous word ej_1: s(iIj, ei_i, d) E d(i - 1c11) • t(ei-i kik) (3) k=0 Here d(i - kl/) is the probability that word di influences a word ej if the previous word e3_1 is influenced by dk. As an effect of such a weight a (phrase-)cluster of words being moved over a long distance receives additional &apos;cost&apos; only at the ends of the cluster. So we have the f</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-Based Word Alignment in Statistical Translation. In Proc. Int. Conf. on Computational Linguistics, pages 836-841, Kopenhagen, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>