<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001022">
<title confidence="0.9907405">
Identifying Emotions, Intentions, and Attitudes in Text
Using a Game with a Purpose
</title>
<author confidence="0.997173">
Lisa Pearl
</author>
<affiliation confidence="0.9991165">
Department of Cognitive Sciences
University of California, Irvine
</affiliation>
<address confidence="0.909143">
3151 Social Science Plaza
Irvine, CA 92697, USA
</address>
<email confidence="0.999426">
lpearl@uci.edu
</email>
<author confidence="0.995621">
Mark Steyvers
</author>
<affiliation confidence="0.999131">
Department of Cognitive Sciences
University of California, Irvine
</affiliation>
<address confidence="0.909194">
3151 Social Science Plaza
Irvine, CA 92697, USA
</address>
<email confidence="0.99966">
msteyver@uci.edu
</email>
<sectionHeader confidence="0.995649" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999531923076923">
Subtle social information is available in text
such as a speaker’s emotional state, intentions,
and attitude, but current information extrac-
tion systems are unable to extract this infor-
mation at the level that humans can. We de-
scribe a methodology for creating databases
of messages annotated with social information
based on interactive games between humans
trying to generate and interpret messages for a
number of different social information types.
We then present some classification results
achieved by using a small-scale database cre-
ated with this methodology.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.98019258">
A focus of much information extraction research
has been identifying surface-level semantic content
(e.g., identifying who did what to whom when).
In recent years, research on sentiment analysis and
opinion mining has recognized that more subtle in-
formation can be communicated via linguistic fea-
tures in the text (see Pang and Lee (2008) for a re-
view), such as whether text (e.g., a movie review)
is positive or negative (Turney 2002, Pang, Lee,
and Vaithyanathan 2002, Dave, Lawrence, and Pen-
nock 2003, Wiebe et al. 2004, Kennedy and Inkpen
2006, Agarwal, Biadsy, and Mckeown 2009, Greene
and Resnik 2009, among many others). However,
other subtle information available in text, such as a
speaker’s emotional states (e.g., anger, embarrass-
ment), intentions (e.g., persuasion, deception), and
attitudes (e.g., disbelief, confidence), has not been
explored as much, though there has been some work
71
in detecting emotion (e.g., Subasic and Huettner
2001, Alm, Roth, and Sproat 2005, Nicolov et al.
2006, Abbasi 2007) and detecting deception (e.g.,
Annolli, Balconi, and Ciceri 2002, Zhou et al. 2004,
Gupta and Skillicorn 2006, Zhou and Sung 2008).
This latter kind of social information is useful for
identifying the “tone” of a message, i.e., for un-
derstanding the underlying intention behind a mes-
sage’s creation, and also for predicting how this
message will be interpreted by humans reading it.
A technical barrier to extracting this kind of social
information is that there are currently no large-scale
text databases that are annotated with social infor-
mation from which to learn the relevant linguistic
cues. That is, there are few examples of social in-
formation “ground truth” - text annotated with hu-
man perceptions of the social information contained
within the text. Given the success of sentiment anal-
ysis, we believe this social information could also
be retrievable once the relevant linguistic cues are
identified.
One way to create the necessary annotated data
is to draw from computational social science (Lazer
et al. 2009), and make use of human-based com-
putation (Kosurokoff 2001, von Ahn 2006, among
others) since humans are used to transmitting so-
cial information through language. In this paper,
we describe a methodology for creating this kind
of database, and then present the results from a
small-scale database created using this methodol-
ogy1. In addition, we show one example of us-
</bodyText>
<footnote confidence="0.998564333333333">
1The database can be obtained by downloading it
from http://www.socsci.uci.edu/˜lpearl/CoLaLab/projects.html
or contacting Lisa Pearl at lpearl@uci.edu.
</footnote>
<note confidence="0.985242">
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 71–79,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.991688945054946">
ing this database by training a Sparse Multinomial this.” From only the text itself, we can readily in-
Logistic Regression classifier (Krishnapuram et al. fer that the speaker intends to persuade the listener.
2005) on these data. Human-based computation can leverage this ability
2 Reliable databases of social information from the population, and use it to construct a reli-
2.1 The need for databases able database of social information. Interestingly,
In general, reliable databases are required to de- groups of humans are sometimes capable of pro-
velop reliable machine learning algorithms. Unfor- ducing much more precise and reliable results than
tunately, very few databases annotated with social any particular individual in the group. For example,
information exist, and the few that do are small in Steyvers et al. (2009) has shown that such “wisdom
size. A recent addition to the Linguistic Data Con- of crowds” phenomena occur in many knowledge
sortium demonstrates this: The Language Under- domains, including human memory, problem solv-
standing Annotation Corpus (LUAC) by Diab et al. ing, and prediction. In addition, Snow et al. (2008)
(2009) includes text annotated with committed be- have demonstrated that a relatively small number of
lief, which “distinguishes between statements which non-expert annotations in natural language tasks can
assert belief or opinion, those which contain spec- achieve the same results as expert annotation.
ulation, and statements which convey fact or oth- 2.2 Games with a purpose
erwise do not convey belief.” This is meant to aid One approach is to use a game with a purpose
in determining which beliefs can be ascribed to a (GWAP) (von Ahn and Dabbish 2004, von Ahn
communicator and how strongly the communicator 2006, von Ahn, Kedia, and Blum 2006) that is de-
holds those beliefs. Nonetheless, this is still a small signed to encourage people to provide the infor-
sample of the possible social information contained mation needed in the database. GWAPs are cur-
in text. Moreover, the LUAC contains only about rently being used to accumulate information about
9000 words across two languages (6949 English, many things that humans find easy to identify (see
2183 Arabic), which is small compared to the cor- http://www.gwap.com/gwap/ for several examples),
pora generally available for natural language pro- such as objects in images (von Ahn and Dabbish
cessing (e.g., the English Gigaword corpus (Graff 2004), the musical style of songs, impressions of
2003) contains 1756504 words). sights and sounds in videos, and common sense re-
Another tack taken by researchers has been to use lationships between concepts (von Ahn, Kedia, and
open-source data that are likely to demonstrate cer- Blum 2006). In addition, as the collected data comes
tain social information by happenstance, e.g., online from and is vetted by a large number of participants,
gaming forums with games that happen to involve we can gauge which messages are reliable examples
the intent to deceive (e.g., Zhou and Sung 2008: of particular social information and which are con-
Mafia game forums). While these data sets are larger fusing examples.
in size, they do not have the breadth of coverage in 2.3 A GWAP for social information in text
terms of what social information they can capture We designed a GWAP to create a database of mes-
because, by nature, the games only explicitly involve sages annotated with social information, where un-
one kind of social information (e.g., intentions: de- paid participants provide knowledge about the social
ception); other social information cannot reliably be information in text. The GWAP encourages partici-
attributed to the text. In general, real world data sets pants to both generate messages that reflect specific
present the problem of ground truth, i.e., knowing social information and to label messages created by
for certain which emotions, intentions, and attitudes other participants as reflecting specific social infor-
are conveyed by a particular message. mation. Participants are given points for every mes-
However, people can often detect social informa- sage they create that is correctly labeled by another
tion conveyed through text (perhaps parsing it as participant, and for every message created by an-
the “tone” of the message). For example, consider other participant that they correctly label.
the following message: “Come on...you have to buy Message generators were instructed to generate a
72
message expressing some particular social informa-
tion type (such as persuading), and were allowed to
use a displayed picture as context to guide their mes-
sage, so they would not need to rely completely on
their own imaginations. All context pictures used
in our GWAP were meant to be generic enough
that they could be a basis for a message express-
ing a variety of social information types. Context
pictures were randomly assigned when participants
were asked to generate messages; this meant that, for
example, a picture could be used to generate a per-
suasive message and be used again later to generate
a deceptive message. Generators were also warned
not to use ”taboo” words that would make the social
information too easy to guess 2, but were encour-
aged to express the social information as clearly as
possible. The generator was told that if another par-
ticipant perceived the correct social information type
from the message, the generator would be rewarded
with game points.
Message annotators were instructed to guess
which social information type was being expressed
by the displayed message. They were also shown
the image the generator used as context for the mes-
sage, and were rewarded with points for successful
detection of the intended social information.
As an example of the GWAP in action, one par-
ticipant might generate the message “Won’t you con-
sider joining our campaign? It’s for a good cause.”
for the social information of persuading; a differ-
ent participant would see this message and might la-
bel it as an example of persuading. A participant
can only label a message with one social informa-
tion type (e.g., a participant could not choose both
persuading and formal for the same message).3
With enough game players, many messages are
created that clearly reflect different social informa-
tion. Without any of the participants necessarily
2Taboo words were chosen as morphological variants of the
social information type description. For example, persuade,
persuades, persuaded, and persuading were considered taboo
words for “persuading”. Future versions of the GWAP could
allow the taboo word list to be influenced by which words are
often associated with a particular social information type.
</bodyText>
<footnote confidence="0.939508">
3We note that this is a restriction that might be relaxed in
future versions of the GWAP. For instance, participants might
decide whether a message expresses a social information type
or not from their perspective, so the task is more like binary
classification for each social information type.
</footnote>
<bodyText confidence="0.999933071428571">
having expert knowledge or training, we expect that
the cumulative knowledge to be quite reliable (for
example, see Steyvers et al. (2009) and work by von
Ahn (von Ahn and Dabbish 2004, von Ahn 2006,
von Ahn, Kedia, and Blum 2006) for other success-
ful cases involving the “wisdom of the crowds”, and
Snow et al. (2008) for non-expert annotation in nat-
ural language tasks such as affect recognition). Be-
cause the same text can be evaluated by many differ-
ent people, this can reduce the effect of idiosyncratic
responses from a few individuals.
An advantage of this kind of database is that many
different kinds of social information can be gen-
erated and labeled by the participants so that the
database contains examples of many different kinds
of social information in text, even if only a single
label is given to a particular message (perhaps ex-
pressing that message’s most obvious social infor-
mation from the perspective of the labeler). We can
gauge how clearly a message reflects social informa-
tion by how often it is labeled by others as reflect-
ing that social information. In addition, by the very
nature of the GWAP, we can also assess which so-
cial information is easily confused by humans, e.g.,
politeness with embarrassment, or confidence with
deception. This can aid the development of models
that extract social information and could also iden-
tify messages likely to be ambiguous to humans.
</bodyText>
<subsectionHeader confidence="0.990172">
2.4 A GWAP study
</subsectionHeader>
<bodyText confidence="0.999994611111111">
Below we report data from an offline GWAP that in-
volves eight types of social information indicative of
several social aspects that we thought would be of
interest: politeness (indicates emotional state, atti-
tude), rudeness (indicates emotional state, attitude),
embarrassment (indicates emotional state), formal-
ity (indicates attitude), persuading (indicates intent),
deception (indicates intent), confidence (indicates
emotional state, attitude), and disbelief (indicates
attitude). Fifty eight English-speaking adults par-
ticipated in the GWAP, consisting of a mix of un-
dergraduate students, graduate students, the authors,
friends of the students, and friends of the authors,
in order to simulate the varied mix of participants in
an online GWAP. The undergraduate students were
compensated with course credit. Together, these 58
participants created 1176 messages and made 3198
annotations. Note that a participant would label
</bodyText>
<page confidence="0.993435">
73
</page>
<bodyText confidence="0.9275615">
more messages than that participant would be asked
to generate, and more than one participant would la-
bel the same message (though no participant would
label a message that s/he created, nor would any par-
ticipant label the same message more than once).
Participants were encouraged to play the GWAP
multiple times if they were inclined, to simulate the
experience of playing a favorite game. There was no
limit on message length, though most participants
tended to keep messages fairly brief. Some sample
messages (with the participants’ own spelling and
punctuation) that were correctly and incorrectly la-
beled are shown in Table 1.
Social Information Message
Generated
Labeled
deception “Oh yeah...your hair looks really
deception great like that...yup, I love it...it,
uh, really suits you...”
embarrassment “Oh... we’re not dating. I would
embarrassment never date him... he’s like a
brother to me..”
disbelief “Are you and him really
disbelief friends?”
rudeness “James, Bree doesn’t like you.
persuading She never did and never will!”
deception “I wasn’t going to take anything
persuading from your storeroom, I swear!
Really, I won’t try to get inside
again!’
politeness “Your orange hair matches your
deception sweater nicely”
</bodyText>
<tableCaption confidence="0.995171">
Table 1: Sample messages from the offline GWAP.
</tableCaption>
<bodyText confidence="0.999974">
The GWAP as currently designed allows us to
gauge two interesting aspects of social information
transmission via text. First, we can assess our non-
expert participants’ performance. Second, we can
assess the messages themselves.
For the participants, we can gauge their accuracy
as message generators by measuring how often a
message they created was successfully perceived as
expressing the intended social information type (that
is, their “expressive accuracy”). On average, mes-
sage generators were able to generate reliable mes-
sages 56% of the time. Figure 1 displays the expres-
sive accuracy of participants, while also showing
how many messages participants generated. Most
participants created less than 30 messages, and were
accurate more than half the time.
</bodyText>
<figureCaption confidence="0.999242">
Figure 1: Expressive accuracy of GWAP participants.
</figureCaption>
<bodyText confidence="0.999890142857143">
At the same time, we can also gauge the accu-
racy of the participants as non-expert annotators by
measuring how often a participant perceived the in-
tended social information (that is, their “perceptive
accuracy”). On average, annotators were able to per-
ceive the intended social information 58% of the
time. Figure 2 displays the perceptive accuracy,
while also showing how many messages partici-
pants annotated. Most participants annotated around
20 messages or between 80 and 100 messages and
were accurate more than half the time. Average
inter-annotator agreement was 0.44, calculated us-
ing Fleiss’ Kappa (Fleiss 1971), suggesting moder-
ate agreement.
</bodyText>
<figureCaption confidence="0.991219">
Figure 2: Perceptive accuracy of GWAP participants.
</figureCaption>
<bodyText confidence="0.9992862">
Turning to the messages, we can gauge how often
messages were able to successfully express a par-
ticular social information type, and how often they
were confused as expressing some other type. Table
2 shows a confusion matrix of social information de-
</bodyText>
<page confidence="0.994822">
74
</page>
<table confidence="0.918309">
rived from this database.
deception
politeness
rudeness
embarrassment
confidence
disbelief
formality
persuading
.07 .10 .03 .09 .10 .04 .20
.53 .05 .02 .03 .01 .20 .10
.01 .78 .02 .04 .04 .03 .03
.09 .05 .56 .02 .13 .05 .03
.04 .03 .01 .67 .05 .02 .13
.05 .05 .04 .07 .62 .02 .06
.34 .04 .02 .06 .03 .39 .10
.06 .03 .01 .12 .03 .04 .61
</table>
<tableCaption confidence="0.998945">
Table 2: Confusion matrix for the human participants.
</tableCaption>
<bodyText confidence="0.973033857142857">
The rows represent the intended social information for a
message while the columns represent the labeled social
information, averaged over messages and participants.
The matrix shows the likelihood that a message
will be labeled as expressing specific social infor-
mation (in a column), given that it has been gener-
ated with specific social information in mind (in a
row), averaged over messages and participants. In
other words, we show the probability distribution
p(labeledIgenerated). The diagonal probabilities
indicate how often a message’s social information
was correctly labeled for each social information
type; this shows how often social information trans-
mission was successful. Messages were perceived
correctly by human participants about 57% of the
time. More particular observations about the data in
Table 2 are that people are more likely to correctly
identify a message expressing rudeness (p = .78)
and confidence (p = .67) and less likely to correctly
identify a message expressing deception (p = .37)
or formality (p = .39). Also, we can see that a
deceptive message can often be mistaken for a per-
suading message (p = 0.20), a formal message mis-
taken for a polite message (p = 0.34), a message
expressing disbelief mistaken for a message express-
ing deception (p = .10), and a persuading message
mistaken for a deceptive message (p = .09) or con-
fidence (p = .12), among other observations. Some
of these may be expected, e.g., confusing confidence
with persuading since someone who is trying to per-
suade will likely be confident about the topic, or
formality with politeness since many formal expres-
sions are used to indicate politeness (e.g., “if you
would be so kind”). Others may be unexpected a
priori, such as mistaking disbelief for deception.
</bodyText>
<subsectionHeader confidence="0.994031">
2.5 Human reliability and message reliability
</subsectionHeader>
<bodyText confidence="0.99907017948718">
Given that humans were believed to be good at iden-
tifying social information in text, the low percep-
tive accuracy rates for participants and low anno-
tation accuracy rates for messages may seem unex-
pected. However, we believe it indicates that some
messages are better than others at expressing social
information in a way obvious to humans. That is,
messages confusing to human participants (e.g., the
lower three examples in Table 1, as well as the con-
fusing messages represented by the probabilities in
Table 2) would be consistently mislabeled.
It may be that some messages are created such
that many annotators agree with each other, but they
all perceive a social information type other than the
one intended.4 In a similar vein, messages with
low inter-annotator agreement may simply be poorly
generated messages that should be removed from the
database. To this end, we can assess how often ma-
jority annotator agreement correlates with percep-
tion of the message’s intended social information
type. Table 3 shows the confusion matrix for mes-
sages where over 50% of the annotators agreed with
each other on which social information type was in-
tended, and at least two annotators labeled the mes-
sage. A total of 866 messages satisfied these criteria.
The confusion matrix, as before, shows the like-
lihood that a message will be labeled as express-
ing specific social information (in a column), given
that it has been generated with specific social in-
formation in mind (in a row), averaged over mes-
sages and participants. The diagonal probabilities
indicate how often a message’s social information
was correctly labeled for each social information
type; this shows how often social information trans-
mission was successful. The messages in this sub-
set were perceived correctly by human participants
about 71% of the time, a significant improvement
over 57%. This demonstrates how even a modest
pooling of non-expert opinion can significantly in-
</bodyText>
<footnote confidence="0.93730325">
4Messages consistently perceived as expressing a different
social information type than intended should perhaps be con-
sidered as actually expressing that social information type rather
than the intended one.
deception .37
politeness .05
rudeness .04
embarrassment .07
confidence .04
disbelief .10
formality .02
persuading .09
</footnote>
<page confidence="0.985846">
75
</page>
<table confidence="0.99770575">
.05 .10 .01 .07 .07 .03 .21
.71 .03 .00 .01 .00 .13 .09
.00 .92 .00 .01 .02 .02 .00
.08 .05 .69 .00 .11 .01 .02
.04 .02 .01 .82 .01 .01 .09
.03 .02 .02 .05 .82 .00 .02
.34 .02 .01 .03 .03 .46 .10
.05 .01 .00 .05 .03 .01 .82
</table>
<tableCaption confidence="0.996167">
Table 3: Confusion matrix for the human participants,
</tableCaption>
<figureCaption confidence="0.387547">
where the majority of participants agreed on a message’s
intended social information and at least two participants
labeled the message. The rows represent the intended so-
cial information for a message while the columns repre-
sent the labeled social information, averaged over mes-
sages and participants.
</figureCaption>
<bodyText confidence="0.951023170731707">
crease the accuracy of social information identifica-
tion in text.
We can observe similar trends to what we saw in
Table 2, in many cases sharpened from what they
were previously. People are still more likely to iden-
tify messages expressing rudeness (p = .92) and
confidence (p = .82), though they are also now more
likely to accurately identify persuading (p = .82).
The ability to identify politeness (p = .71) and em-
barrassment (p = .69) has also improved, though
a polite message can still be mistaken for a formal
message (p = .13). Formality (p = .46) and de-
ception (p = .45) remain more difficult to iden-
tify, with formal messages mistaken for politeness
(p = .34) and deceptive messages mistaken for per-
suading (p = .21) and rudeness (p=.10) 5. Note,
however, that messages of disbelief and persuad-
ing are now rarely mistaken for deceptive messages
(p = .05 and p = .03, respectively). It is likely
then that the confusions arising in this data set are
more representative of the actual confusion humans
encounter when perceiving these social information
5We note that people’s precision on deceptive messages was
higher: 0.67. That is, when they labeled a message as deceptive,
it was deceptive 2/3 of the time. However, the probabilities in
Table 3 represent deceptive message recall, i.e., how well they
were able to label all deceptive messages as deceptive.
types.
Identifying messages likely to be misperceived by
humans is useful for two reasons. First, from a cog-
nitive standpoint, we can identify what features of
those messages are the source of the confusion if
the messages are consistently misperceived, which
tells us what linguistic cues humans are (mistakenly)
keying into. This then leads to designing better ma-
chine learning algorithms that do not key into those
misleading cues. Second, this aids the design of cog-
nitive systems that predict how a message is likely
to be interpreted by humans, and can warn a human
reader if a message’s intent is likely to be interpreted
incorrectly.
</bodyText>
<sectionHeader confidence="0.835531" genericHeader="introduction">
3 Training a classifier with the database
</sectionHeader>
<bodyText confidence="0.999916294117647">
To demonstrate the utility of the created database for
developing computational approaches to social in-
formation identification in text, we applied a Sparse
Multinomial Logistic Regression (SMLR) classifier
(Krishnapuram et al. 2005) to the the subset of mes-
sages where two or more participants labeled the
message and more than 50% of the participants per-
ceived the intended social information type. This
subset consisted of 624 messages (these messages
make up the messages in the diagonals of table 3).
While we realize that there are many other machine
learning techniques that could be used, we thought
this classifier would be a reasonable one to start
with to demonstrate the utility of the database. As a
first pass measure for identifying diagnostic linguis-
tic cues, we examined a number of fairly shallow
features:
</bodyText>
<listItem confidence="0.997904111111111">
• unigrams, bigrams, and trigrams
• number of word types, word tokens, and sen-
tences
• number of exclamation marks, questions
marks, and punctuation marks
• average sentence and word length
• word type to word token ratio
• average word log frequency for words appear-
ing more than once in the database
</listItem>
<figure confidence="0.996602375">
deception
politeness
rudeness
embarrassment
confidence
disbelief
formality
persuading
deception .45
politeness .03
rudeness .03
embarrassment .04
confidence .01
disbelief .05
formality .02
persuading .03
</figure>
<page confidence="0.959188">
76
</page>
<bodyText confidence="0.999950894736842">
The use of shallow linguistic features seemed a
reasonable first investigation as prior research in-
volving linguistic cues for identifying information
in text has often used word-level cues. For exam-
ple, positive and negative affect words (e.g., excel-
lent vs. poor) have been used in sentiment analysis
to summarize whether a document is positive or neg-
ative (Turney 2002, Pang, Lee, and Vaithyanathan
2002, among others). In deception detection re-
search, informative word-level cues include count-
ing first and third person pronoun usage (e.g., me vs.
them) (Anolli, Balconi, and Ciceri 2002), and noting
the number of “exception words” (e.g., but, except,
without) (Gupta and Skillicorn 2006). In addition,
informative shallow text properties have also been
identified (Zhou et al. 2004), such as (a) number of
verbs, words, noun phrases, and sentences, (b) aver-
age sentence and word length, and (c) word type to
word token ratio.
The SMLR classifier model was trained to pro-
duce the label (one of eight) corresponding to the
generated social information using all the text fea-
tures as input. Using a 10-fold cross-validation pro-
cedure, the model was trained on 90% of the mes-
sages and tested on the remaining 10%. The sparse
classifier favors a small number of features in the
regression solution and sets the weight of a large
fraction of features to zero. Some of the non-zero
weights learned by the model for each social infor-
mation type are listed below (though each type has
other features that also had non-zero weights). Posi-
tive weights indicate positive correlations while neg-
ative weights indicate negative correlations. Cues
that are negatively correlated are italicized. Bigrams
and trigrams are indicated by + in between the rele-
vant words (e.g., no+way). BEGIN and END indi-
cate the beginning and the end of the message, re-
spectively.
</bodyText>
<listItem confidence="0.998090375">
• deception: #-of-question-marks (-0.5),
actually (1.4), at+all (0.6), if (0.8), me (-0.9),
my (-0.2), not (1.6), of+course (1.1), trying+to
(0.8), you+END (1.0)
• politeness: BEGIN+please (2.1), help (2.1),
may+i (1.2), nice (2.3), nicely+END (1.1),
so+sorry (1.5), would+you+like (1.0)
• rudeness: annoying (1.2), good (-1.1), great
</listItem>
<bodyText confidence="0.570634">
(-0.6), hurry+up (1.0), loud (2.7), mean (0.9),
pretty (-2.0), ugly (1.6)
</bodyText>
<listItem confidence="0.912881333333333">
• embarrassment: BEGIN+oh (2.0),
can’t+believe (1.0), can’t+believe+i (0.6),
forgot (2.1), good (-.9), my (2.0), oh (1.1)
• confidence: i+believe (2.1), i+know (2.4),
positive (3.5), really+good (2.9), sure (3.3),
the+best (2.5), think (-0.8)
• disbelief: #-of-question-marks (2.4),
BEGIN+are (3.8), like (-0.6), never (1.4),
no+way (3.0), shocked (1.1), such+a (1.1)
• formality: #-of-exclamation-marks (-0.8),
BEGIN+excuse (2.1), don’t (-0.8), miss (4.1),
mr (3.7), please (2.7), sir (5.1), very+nice (1.0)
• persuading: BEGIN+if+you (2.3), buy (1.3),
come (3.5), have+to (1.6), we+can (1.3),
would+look (2.9), you+should (3.4)
</listItem>
<bodyText confidence="0.999955703703704">
Some of the feature-label correlations discovered
by the model fit with our intuitions about the so-
cial information types. For example, deceptive mes-
sages are negatively correlated with some of the
first person pronouns (me, my), in accordance with
Anolli, Balconi, and Ciceri (2002)’s results. Sev-
eral polite and formal words appear correlated with
polite and formal messages respectively (may+i,
nice, so+sorry, would+you+like; BEGIN+excuse,
miss, mr, sir), and formal messages tend not to in-
clude exclamation points. Negative words tend to
be associated with rude messages (annoying, loud,
mean, ugly), while positive words tend to be asso-
ciated with confident messages (really+good, sure,
the+best). Messages conveying disbelief tend to
have more question marks and contain expressions
of surprise (never, no+way, shocked), and persua-
sive messages tend to contain coercive expressions
(come, have+to, you+should). As this is a relatively
small data set, these cues are unlikely to be defini-
tive – however, it is promising for the approach as a
whole that the classifier can identify these cues using
fairly shallow linguistic analyses.
We can also examine the classifier’s ability to la-
bel messages, given the features it has deemed di-
agnostic for each social information type (i.e., those
features it gave non-zero weight). For each message
</bodyText>
<page confidence="0.996623">
77
</page>
<bodyText confidence="0.999948173913044">
in the dataset, the classifier predicted what the in-
tended social information type was. A correct pre-
diction for a message’s type matches the intended
type for the message. A confusion matrix for the
classifier based on the messages from the 624 mes-
sage test set is shown in Table 4. Overall, the clas-
sifier was able to correctly label 59% of the mes-
sages. This is 12% less than humans were able to
correctly label, but far better than chance perfor-
mance (13%) and the performance of a simple al-
gorithm that chooses the most frequent data type in
the training set (17%).
The classifier shows some patterns similar to the
human participants: (1) deception and formality are
harder to detect than other social information types,
(2) confidence and embarrassment are easier to de-
tect than other social information types, and (3) for-
mality is often mistaken for politeness (p = .26).
However, some differences from the human partici-
pants are that deception is often mistaken for rude-
ness (p = .19) and politeness is often confused with
rudeness and embarrassment, in addition to formal-
ity (all p = .12).
</bodyText>
<table confidence="0.9981510625">
deception
politeness
rudeness
embarrassment
confidence
disbelief
formality
persuading
.08 .19 .08 .08 .09 .06 .08
.49 .12 .12 .05 .01 .12 .05
.06 .63 .04 .07 .07 .01 .07
.01 .11 .76 .06 .03 .01 .00
.01 .04 .08 .68 .02 .03 .08
.03 .08 .02 .09 .56 .02 .12
.26 .06 .03 .00 .06 .43 .15
.06 .09 .03 .11 .03 .02 .61
</table>
<tableCaption confidence="0.95168925">
Table 4: Confusion matrix for the machine learning clas-
sifier. The rows represent the intended social information
for a message while the columns represent the labeled so-
cial information.
</tableCaption>
<bodyText confidence="0.998469">
As the classifier’s behavior was similar to hu-
man behavior in some cases, and the classifier used
only these shallow linguistic features to make its
decision, this suggests that humans may be key-
ing into some of these shallower linguistic features
when deciding a message’s social information con-
tent. Given this, a classifier trained on such linguis-
tic features may be able to predict which messages
are likely to be ambiguous to humans.
</bodyText>
<sectionHeader confidence="0.995656" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999981233333333">
We have described a methodology using GWAPs
to create a database containing messages labeled
with social information such as emotions, inten-
tions, and attitudes, which can be valuable to the
information extraction research community. Hav-
ing implemented this methodology on a small scale,
we discovered that non-expert annotators were able
to identify the social information of interest fairly
well when their collective perceptions were com-
bined. However, we also noted that certain social
information types are easily confusable by humans.
We also used the database created by the GWAP
to investigate shallow linguistic cues to social in-
formation in text and attempt to automatically la-
bel messages as expressing particular social infor-
mation. The fact that the social information types
we used in our GWAP can be identified automati-
cally with some success suggests that these social
information types are useful to pursue, though of
course there are many other emotional states, atti-
tudes, and intentions that could be explored in fu-
ture work. In addition, other classifiers, particularly
those using deeper-level properties like phrase struc-
ture, may be able to identify more subtle cues to
social information in text. We also foresee extend-
ing the GWAP methodology to create large-scale
databases both in English and in other languages in
order to continue fostering the development of com-
putational approaches to social information identifi-
cation.
</bodyText>
<sectionHeader confidence="0.997486" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9968962">
This paper has benefited from discussion and ad-
vice from Padhraic Smyth, Pierre Isabelle, and three
anonymous reviewers. In addition, this work is
supported by NSF grant BCS-0843896 to LP and
CORCL grant MI 14B-2009-2010 to LP and MS.
</bodyText>
<sectionHeader confidence="0.99893" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9919776">
Abbasi, A. 2007. Affect intensity analysis of dark web
forums. Proceedings of Intelligence and Security In-
formatics (ISI): 282-288.
Agarwal, A., Biadsy, F., and Mckeown, K. 2009. Con-
textual Phrase-Level Polarity Analysis using Lexical
</reference>
<footnote confidence="0.774722625">
deception .36
politeness .05
rudeness .06
embarrassment .02
confidence .06
disbelief .08
formality .00
persuading .05
</footnote>
<page confidence="0.994757">
78
</page>
<reference confidence="0.999280419047619">
Affect Scoring and Syntactic N-grams. Proceedings
of the 12th Conference of the European Chapter of the
ACL, Athens, Green: 24-32.
Alm, C. O., Roth, D., and Sproat, R. 2005. Emo-
tions from text: Machine learning for text-based emo-
tion prediction. Proceedings of the Human Lan-
guage Technology Conference and the Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP).
Anolli, L., Balconi, M., and Ciceri, R. 2002. De-
ceptive Miscommunication Theory (DeMiT): A New
Model for the Analysis of Deceptive Communication.
In Anolli, L., Ciceri, R. and Rivs, G. (eds)., Say not
to say: new perspectives on miscommunication. IOS
Press: 73-100.
Dave, K., Lawrence, S., and Pennock, D. 2003. Min-
ing the peanut gallery: Opinion extraction and seman-
tic classication of product reviews, Proceedings of
WWW: 519-528.
Diab, M., Dorr, B., Levin, L., Mitamura, T., Passonneau,
R., Rambow, O., and Ramshaw, L. 2009. Language
Understanding Annotation Corpus. LDC, Philadel-
phia.
Fleiss, J. L. 1971. Measuring nominal scale agreement
among many raters. Psychological Bulletin 76(5):
378382.
Graff, D. 2003. English Gigaword. Linguistic Data Con-
sortium, Philadelphia.
Greene, S. and Resnik, P. 2009. More than Words:
Syntactic Packaging and Implicit Sentiment. Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the ACL, Boulder,
Colorado: 503-511.
Gupta, S. and Skillicorn, D. 2006. Improving a Textual
Deception Detection Model, Proceedings of the 2006
conference of the Center for Advanced Studies on Col-
laborative research. Toronto, Canada.
Kennedy, A. and Inkpen, D. 2006. Sentiment clas-
sication of movie reviews using contextual valence
shifters. Computational Intelligence, 22: 110-125.
Kosorukoff, A. 2001. Human-based Genetic Algorithm.
IEEE Transactions on Systems, Man, and Cybernetics,
SMC-2001: 3464-3469.
Krishnapuram, B., Figueiredo, M., Carin, L., and
Hartemink, A. 2005. Sparse Multinomial Logistic Re-
gression: Fast Algorithms and Generalization Bounds.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 27: 957-968.
Lazer, D., Pentland, A., Adamic, L., Aral, S., Barabsi,
A., Brewer, D., Christakis, N., Contractor, N., Fowler,
J., Gutmann, M., Jebara, T., King, G., Macy, M., Roy,
D., and Val Alstyne, M. 2009. Computational Social
Science, Science, 323: 721-723.
Nicolov, N., Salvetti, F., Liberman, M., and Martin, J. H.
(eds.) 2006. AAAI Symposium on Computational Ap-
proaches to Analysing Weblogs (AAAI-CAAW). AAAI
Press.
Pang, B. and Lee, L. 2008. Opinion Mining and Sen-
timent Analysis. Foundations and Trends in Informa-
tion Retrieval 2(1-2): 1-135.
Pang, B., Lee, L., and Vaithyanathan, S. 2002. Thumbs
up? Sentiment Classification using Machine Learn-
ing Techniques. Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP): 79-86.
Snow, R., O’Connor, B., Jurafsky, D., and Ng, A. 2008.
Cheap and Fast - But is it Good? Evaluating Non-
Expert Annotations for Natural Language Tasks. Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP), 254-263.
Steyvers, M., Lee, M., Miller, B., and Hemmer, P. 2009.
The Wisdom of Crowds in the Recollection of Order
Information. In J. Lafferty, C. Williams (Eds.) Ad-
vances in Neural Information Processing Systems, 23,
MIT Press.
Subasic, P. and Huettner A. 2001. A?ect analysis of text
using fuzzy semantic typing. IEEE Transactions on
Fuzzy Systems, 9: 483-496.
Turney, P. 2002. Thumbs Up or Thumbs Down? Seman-
tic Orientation Applied to Unsupervised Classification
of Reviews. Proceedings of the Association for Com-
putational Linguistics (ACL): 417-424.
von Ahn, L. 2006. Games With A Purpose. IEEE Com-
puter Magazine, June 2006: 96-98.
von Ahn, L. and Dabbish, L. 2004. Labeling Im-
ages with a Computer Game. Proceedings of the
SIGCHI Conference on Human Factors in Computing
Systems (Association for Computing Machinery, New
York, 2004): 319-326.
von Ahn, L., Kedia, M. and Blum, M. 2006. Verbosity:
A Game for Collecting Common-Sense Facts, In Pro-
ceedings of the SIGCHI conference on Human Factors
in computing systems, Montral, Quebec, Canada.
Wiebe, J.M., Wilson, T., Bruce, R., Bell, M., and Martin,
M. 2004. Learning subjective language. Computa-
tional Linguistics, 30: 277-308.
Zhou, L., Burgoon, J., Nunamaker, J., and Twitchell, D.
2004. Automating linguistics-based cues for detect-
ing deception in text-based asynchronous computer-
mediated communication. Group Decision and Nego-
tiation, 13: 81-106.
Zhou, L. and Sung, Y. 2008. Cues to deception in on-
line Chinese groups. Proceedings of the 41st Annual
Hawaii international Conference on System Sciences,
146. Washington, DC: IEEE Computer Society.
</reference>
<page confidence="0.999037">
79
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.723600">
<title confidence="0.9954435">Identifying Emotions, Intentions, and Attitudes in Using a Game with a Purpose</title>
<author confidence="0.996754">Lisa</author>
<affiliation confidence="0.9999745">Department of Cognitive University of California,</affiliation>
<address confidence="0.9807355">3151 Social Science Irvine, CA 92697,</address>
<email confidence="0.999777">lpearl@uci.edu</email>
<author confidence="0.996472">Mark</author>
<affiliation confidence="0.9999825">Department of Cognitive University of California,</affiliation>
<address confidence="0.9685695">3151 Social Science Irvine, CA 92697,</address>
<email confidence="0.999725">msteyver@uci.edu</email>
<abstract confidence="0.986231357142857">Subtle social information is available in text such as a speaker’s emotional state, intentions, and attitude, but current information extraction systems are unable to extract this information at the level that humans can. We describe a methodology for creating databases of messages annotated with social information based on interactive games between humans trying to generate and interpret messages for a number of different social information types. We then present some classification results achieved by using a small-scale database created with this methodology.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Abbasi</author>
</authors>
<title>Affect intensity analysis of dark web forums.</title>
<date>2007</date>
<booktitle>Proceedings of Intelligence and Security Informatics (ISI):</booktitle>
<pages>282--288</pages>
<contexts>
<context position="1979" citStr="Abbasi 2007" startWordPosition="298" endWordPosition="299">tive or negative (Turney 2002, Pang, Lee, and Vaithyanathan 2002, Dave, Lawrence, and Pennock 2003, Wiebe et al. 2004, Kennedy and Inkpen 2006, Agarwal, Biadsy, and Mckeown 2009, Greene and Resnik 2009, among many others). However, other subtle information available in text, such as a speaker’s emotional states (e.g., anger, embarrassment), intentions (e.g., persuasion, deception), and attitudes (e.g., disbelief, confidence), has not been explored as much, though there has been some work 71 in detecting emotion (e.g., Subasic and Huettner 2001, Alm, Roth, and Sproat 2005, Nicolov et al. 2006, Abbasi 2007) and detecting deception (e.g., Annolli, Balconi, and Ciceri 2002, Zhou et al. 2004, Gupta and Skillicorn 2006, Zhou and Sung 2008). This latter kind of social information is useful for identifying the “tone” of a message, i.e., for understanding the underlying intention behind a message’s creation, and also for predicting how this message will be interpreted by humans reading it. A technical barrier to extracting this kind of social information is that there are currently no large-scale text databases that are annotated with social information from which to learn the relevant linguistic cues.</context>
</contexts>
<marker>Abbasi, 2007</marker>
<rawString>Abbasi, A. 2007. Affect intensity analysis of dark web forums. Proceedings of Intelligence and Security Informatics (ISI): 282-288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Agarwal</author>
<author>F Biadsy</author>
<author>K Mckeown</author>
</authors>
<title>Contextual Phrase-Level Polarity Analysis using Lexical Affect Scoring and Syntactic N-grams.</title>
<date>2009</date>
<booktitle>Proceedings of the 12th Conference of the European Chapter of the ACL,</booktitle>
<pages>24--32</pages>
<location>Athens, Green:</location>
<marker>Agarwal, Biadsy, Mckeown, 2009</marker>
<rawString>Agarwal, A., Biadsy, F., and Mckeown, K. 2009. Contextual Phrase-Level Polarity Analysis using Lexical Affect Scoring and Syntactic N-grams. Proceedings of the 12th Conference of the European Chapter of the ACL, Athens, Green: 24-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C O Alm</author>
<author>D Roth</author>
<author>R Sproat</author>
</authors>
<title>Emotions from text: Machine learning for text-based emotion prediction.</title>
<date>2005</date>
<booktitle>Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP).</booktitle>
<marker>Alm, Roth, Sproat, 2005</marker>
<rawString>Alm, C. O., Roth, D., and Sproat, R. 2005. Emotions from text: Machine learning for text-based emotion prediction. Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Anolli</author>
<author>M Balconi</author>
<author>R Ciceri</author>
</authors>
<title>Deceptive Miscommunication Theory (DeMiT): A New Model for the Analysis of Deceptive Communication. In</title>
<date>2002</date>
<pages>73--100</pages>
<publisher>IOS Press:</publisher>
<marker>Anolli, Balconi, Ciceri, 2002</marker>
<rawString>Anolli, L., Balconi, M., and Ciceri, R. 2002. Deceptive Miscommunication Theory (DeMiT): A New Model for the Analysis of Deceptive Communication. In Anolli, L., Ciceri, R. and Rivs, G. (eds)., Say not to say: new perspectives on miscommunication. IOS Press: 73-100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Dave</author>
<author>S Lawrence</author>
<author>D Pennock</author>
</authors>
<title>Mining the peanut gallery: Opinion extraction and semantic classication of product reviews,</title>
<date>2003</date>
<booktitle>Proceedings of WWW:</booktitle>
<pages>519--528</pages>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>Dave, K., Lawrence, S., and Pennock, D. 2003. Mining the peanut gallery: Opinion extraction and semantic classication of product reviews, Proceedings of WWW: 519-528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Diab</author>
<author>B Dorr</author>
<author>L Levin</author>
<author>T Mitamura</author>
<author>R Passonneau</author>
<author>O Rambow</author>
<author>L Ramshaw</author>
</authors>
<title>Language Understanding Annotation Corpus.</title>
<date>2009</date>
<location>LDC, Philadelphia.</location>
<marker>Diab, Dorr, Levin, Mitamura, Passonneau, Rambow, Ramshaw, 2009</marker>
<rawString>Diab, M., Dorr, B., Levin, L., Mitamura, T., Passonneau, R., Rambow, O., and Ramshaw, L. 2009. Language Understanding Annotation Corpus. LDC, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>Psychological Bulletin</journal>
<volume>76</volume>
<issue>5</issue>
<pages>378382</pages>
<contexts>
<context position="15806" citStr="Fleiss 1971" startWordPosition="2480" endWordPosition="2481">ame time, we can also gauge the accuracy of the participants as non-expert annotators by measuring how often a participant perceived the intended social information (that is, their “perceptive accuracy”). On average, annotators were able to perceive the intended social information 58% of the time. Figure 2 displays the perceptive accuracy, while also showing how many messages participants annotated. Most participants annotated around 20 messages or between 80 and 100 messages and were accurate more than half the time. Average inter-annotator agreement was 0.44, calculated using Fleiss’ Kappa (Fleiss 1971), suggesting moderate agreement. Figure 2: Perceptive accuracy of GWAP participants. Turning to the messages, we can gauge how often messages were able to successfully express a particular social information type, and how often they were confused as expressing some other type. Table 2 shows a confusion matrix of social information de74 rived from this database. deception politeness rudeness embarrassment confidence disbelief formality persuading .07 .10 .03 .09 .10 .04 .20 .53 .05 .02 .03 .01 .20 .10 .01 .78 .02 .04 .04 .03 .03 .09 .05 .56 .02 .13 .05 .03 .04 .03 .01 .67 .05 .02 .13 .05 .05 .0</context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>Fleiss, J. L. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin 76(5): 378382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Graff</author>
</authors>
<title>English Gigaword. Linguistic Data Consortium,</title>
<date>2003</date>
<location>Philadelphia.</location>
<marker>Graff, 2003</marker>
<rawString>Graff, D. 2003. English Gigaword. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Greene</author>
<author>P Resnik</author>
</authors>
<title>More than Words: Syntactic Packaging and Implicit Sentiment. Human Language Technologies: The</title>
<date>2009</date>
<booktitle>Annual Conference of the North American Chapter of the ACL,</booktitle>
<pages>503--511</pages>
<location>Boulder, Colorado:</location>
<contexts>
<context position="1568" citStr="Greene and Resnik 2009" startWordPosition="235" endWordPosition="238"> 1 Introduction A focus of much information extraction research has been identifying surface-level semantic content (e.g., identifying who did what to whom when). In recent years, research on sentiment analysis and opinion mining has recognized that more subtle information can be communicated via linguistic features in the text (see Pang and Lee (2008) for a review), such as whether text (e.g., a movie review) is positive or negative (Turney 2002, Pang, Lee, and Vaithyanathan 2002, Dave, Lawrence, and Pennock 2003, Wiebe et al. 2004, Kennedy and Inkpen 2006, Agarwal, Biadsy, and Mckeown 2009, Greene and Resnik 2009, among many others). However, other subtle information available in text, such as a speaker’s emotional states (e.g., anger, embarrassment), intentions (e.g., persuasion, deception), and attitudes (e.g., disbelief, confidence), has not been explored as much, though there has been some work 71 in detecting emotion (e.g., Subasic and Huettner 2001, Alm, Roth, and Sproat 2005, Nicolov et al. 2006, Abbasi 2007) and detecting deception (e.g., Annolli, Balconi, and Ciceri 2002, Zhou et al. 2004, Gupta and Skillicorn 2006, Zhou and Sung 2008). This latter kind of social information is useful for ide</context>
</contexts>
<marker>Greene, Resnik, 2009</marker>
<rawString>Greene, S. and Resnik, P. 2009. More than Words: Syntactic Packaging and Implicit Sentiment. Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, Boulder, Colorado: 503-511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gupta</author>
<author>D Skillicorn</author>
</authors>
<title>Improving a Textual Deception Detection Model,</title>
<date>2006</date>
<booktitle>Proceedings of the 2006 conference of the Center for Advanced Studies on Collaborative research.</booktitle>
<location>Toronto, Canada.</location>
<contexts>
<context position="2089" citStr="Gupta and Skillicorn 2006" startWordPosition="313" endWordPosition="316">3, Wiebe et al. 2004, Kennedy and Inkpen 2006, Agarwal, Biadsy, and Mckeown 2009, Greene and Resnik 2009, among many others). However, other subtle information available in text, such as a speaker’s emotional states (e.g., anger, embarrassment), intentions (e.g., persuasion, deception), and attitudes (e.g., disbelief, confidence), has not been explored as much, though there has been some work 71 in detecting emotion (e.g., Subasic and Huettner 2001, Alm, Roth, and Sproat 2005, Nicolov et al. 2006, Abbasi 2007) and detecting deception (e.g., Annolli, Balconi, and Ciceri 2002, Zhou et al. 2004, Gupta and Skillicorn 2006, Zhou and Sung 2008). This latter kind of social information is useful for identifying the “tone” of a message, i.e., for understanding the underlying intention behind a message’s creation, and also for predicting how this message will be interpreted by humans reading it. A technical barrier to extracting this kind of social information is that there are currently no large-scale text databases that are annotated with social information from which to learn the relevant linguistic cues. That is, there are few examples of social information “ground truth” - text annotated with human perceptions </context>
<context position="25287" citStr="Gupta and Skillicorn 2006" startWordPosition="4039" endWordPosition="4042">vestigation as prior research involving linguistic cues for identifying information in text has often used word-level cues. For example, positive and negative affect words (e.g., excellent vs. poor) have been used in sentiment analysis to summarize whether a document is positive or negative (Turney 2002, Pang, Lee, and Vaithyanathan 2002, among others). In deception detection research, informative word-level cues include counting first and third person pronoun usage (e.g., me vs. them) (Anolli, Balconi, and Ciceri 2002), and noting the number of “exception words” (e.g., but, except, without) (Gupta and Skillicorn 2006). In addition, informative shallow text properties have also been identified (Zhou et al. 2004), such as (a) number of verbs, words, noun phrases, and sentences, (b) average sentence and word length, and (c) word type to word token ratio. The SMLR classifier model was trained to produce the label (one of eight) corresponding to the generated social information using all the text features as input. Using a 10-fold cross-validation procedure, the model was trained on 90% of the messages and tested on the remaining 10%. The sparse classifier favors a small number of features in the regression sol</context>
</contexts>
<marker>Gupta, Skillicorn, 2006</marker>
<rawString>Gupta, S. and Skillicorn, D. 2006. Improving a Textual Deception Detection Model, Proceedings of the 2006 conference of the Center for Advanced Studies on Collaborative research. Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kennedy</author>
<author>D Inkpen</author>
</authors>
<title>Sentiment classication of movie reviews using contextual valence shifters.</title>
<date>2006</date>
<journal>Computational Intelligence,</journal>
<volume>22</volume>
<pages>110--125</pages>
<contexts>
<context position="1509" citStr="Kennedy and Inkpen 2006" startWordPosition="226" endWordPosition="229"> using a small-scale database created with this methodology. 1 Introduction A focus of much information extraction research has been identifying surface-level semantic content (e.g., identifying who did what to whom when). In recent years, research on sentiment analysis and opinion mining has recognized that more subtle information can be communicated via linguistic features in the text (see Pang and Lee (2008) for a review), such as whether text (e.g., a movie review) is positive or negative (Turney 2002, Pang, Lee, and Vaithyanathan 2002, Dave, Lawrence, and Pennock 2003, Wiebe et al. 2004, Kennedy and Inkpen 2006, Agarwal, Biadsy, and Mckeown 2009, Greene and Resnik 2009, among many others). However, other subtle information available in text, such as a speaker’s emotional states (e.g., anger, embarrassment), intentions (e.g., persuasion, deception), and attitudes (e.g., disbelief, confidence), has not been explored as much, though there has been some work 71 in detecting emotion (e.g., Subasic and Huettner 2001, Alm, Roth, and Sproat 2005, Nicolov et al. 2006, Abbasi 2007) and detecting deception (e.g., Annolli, Balconi, and Ciceri 2002, Zhou et al. 2004, Gupta and Skillicorn 2006, Zhou and Sung 2008</context>
</contexts>
<marker>Kennedy, Inkpen, 2006</marker>
<rawString>Kennedy, A. and Inkpen, D. 2006. Sentiment classication of movie reviews using contextual valence shifters. Computational Intelligence, 22: 110-125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kosorukoff</author>
</authors>
<title>Human-based Genetic Algorithm.</title>
<date>2001</date>
<journal>IEEE Transactions on Systems, Man, and Cybernetics,</journal>
<volume>2001</volume>
<pages>3464--3469</pages>
<marker>Kosorukoff, 2001</marker>
<rawString>Kosorukoff, A. 2001. Human-based Genetic Algorithm. IEEE Transactions on Systems, Man, and Cybernetics, SMC-2001: 3464-3469.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Krishnapuram</author>
<author>M Figueiredo</author>
<author>L Carin</author>
<author>A Hartemink</author>
</authors>
<title>Sparse Multinomial Logistic Regression: Fast Algorithms and Generalization Bounds.</title>
<date>2005</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>27</volume>
<pages>957--968</pages>
<contexts>
<context position="23493" citStr="Krishnapuram et al. 2005" startWordPosition="3754" endWordPosition="3757">mans are (mistakenly) keying into. This then leads to designing better machine learning algorithms that do not key into those misleading cues. Second, this aids the design of cognitive systems that predict how a message is likely to be interpreted by humans, and can warn a human reader if a message’s intent is likely to be interpreted incorrectly. 3 Training a classifier with the database To demonstrate the utility of the created database for developing computational approaches to social information identification in text, we applied a Sparse Multinomial Logistic Regression (SMLR) classifier (Krishnapuram et al. 2005) to the the subset of messages where two or more participants labeled the message and more than 50% of the participants perceived the intended social information type. This subset consisted of 624 messages (these messages make up the messages in the diagonals of table 3). While we realize that there are many other machine learning techniques that could be used, we thought this classifier would be a reasonable one to start with to demonstrate the utility of the database. As a first pass measure for identifying diagnostic linguistic cues, we examined a number of fairly shallow features: • unigra</context>
</contexts>
<marker>Krishnapuram, Figueiredo, Carin, Hartemink, 2005</marker>
<rawString>Krishnapuram, B., Figueiredo, M., Carin, L., and Hartemink, A. 2005. Sparse Multinomial Logistic Regression: Fast Algorithms and Generalization Bounds. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27: 957-968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lazer</author>
<author>A Pentland</author>
<author>L Adamic</author>
<author>S Aral</author>
<author>A Barabsi</author>
<author>D Brewer</author>
<author>N Christakis</author>
<author>N Contractor</author>
<author>J Fowler</author>
<author>M Gutmann</author>
<author>T Jebara</author>
<author>G King</author>
<author>M Macy</author>
<author>D Roy</author>
<author>Val Alstyne</author>
<author>M</author>
</authors>
<date>2009</date>
<journal>Computational Social Science, Science,</journal>
<volume>323</volume>
<pages>721--723</pages>
<contexts>
<context position="3005" citStr="Lazer et al. 2009" startWordPosition="461" endWordPosition="464">xtracting this kind of social information is that there are currently no large-scale text databases that are annotated with social information from which to learn the relevant linguistic cues. That is, there are few examples of social information “ground truth” - text annotated with human perceptions of the social information contained within the text. Given the success of sentiment analysis, we believe this social information could also be retrievable once the relevant linguistic cues are identified. One way to create the necessary annotated data is to draw from computational social science (Lazer et al. 2009), and make use of human-based computation (Kosurokoff 2001, von Ahn 2006, among others) since humans are used to transmitting social information through language. In this paper, we describe a methodology for creating this kind of database, and then present the results from a small-scale database created using this methodology1. In addition, we show one example of us1The database can be obtained by downloading it from http://www.socsci.uci.edu/˜lpearl/CoLaLab/projects.html or contacting Lisa Pearl at lpearl@uci.edu. Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analy</context>
</contexts>
<marker>Lazer, Pentland, Adamic, Aral, Barabsi, Brewer, Christakis, Contractor, Fowler, Gutmann, Jebara, King, Macy, Roy, Alstyne, M, 2009</marker>
<rawString>Lazer, D., Pentland, A., Adamic, L., Aral, S., Barabsi, A., Brewer, D., Christakis, N., Contractor, N., Fowler, J., Gutmann, M., Jebara, T., King, G., Macy, M., Roy, D., and Val Alstyne, M. 2009. Computational Social Science, Science, 323: 721-723.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Nicolov</author>
<author>F Salvetti</author>
<author>M Liberman</author>
<author>J H Martin</author>
</authors>
<date>2006</date>
<booktitle>AAAI Symposium on Computational Approaches to Analysing Weblogs (AAAI-CAAW).</booktitle>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="1965" citStr="Nicolov et al. 2006" startWordPosition="294" endWordPosition="297">movie review) is positive or negative (Turney 2002, Pang, Lee, and Vaithyanathan 2002, Dave, Lawrence, and Pennock 2003, Wiebe et al. 2004, Kennedy and Inkpen 2006, Agarwal, Biadsy, and Mckeown 2009, Greene and Resnik 2009, among many others). However, other subtle information available in text, such as a speaker’s emotional states (e.g., anger, embarrassment), intentions (e.g., persuasion, deception), and attitudes (e.g., disbelief, confidence), has not been explored as much, though there has been some work 71 in detecting emotion (e.g., Subasic and Huettner 2001, Alm, Roth, and Sproat 2005, Nicolov et al. 2006, Abbasi 2007) and detecting deception (e.g., Annolli, Balconi, and Ciceri 2002, Zhou et al. 2004, Gupta and Skillicorn 2006, Zhou and Sung 2008). This latter kind of social information is useful for identifying the “tone” of a message, i.e., for understanding the underlying intention behind a message’s creation, and also for predicting how this message will be interpreted by humans reading it. A technical barrier to extracting this kind of social information is that there are currently no large-scale text databases that are annotated with social information from which to learn the relevant li</context>
</contexts>
<marker>Nicolov, Salvetti, Liberman, Martin, 2006</marker>
<rawString>Nicolov, N., Salvetti, F., Liberman, M., and Martin, J. H. (eds.) 2006. AAAI Symposium on Computational Approaches to Analysing Weblogs (AAAI-CAAW). AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Opinion Mining and Sentiment Analysis. Foundations and Trends in Information Retrieval</title>
<date>2008</date>
<pages>2--1</pages>
<contexts>
<context position="1300" citStr="Pang and Lee (2008)" startWordPosition="189" endWordPosition="192">l information based on interactive games between humans trying to generate and interpret messages for a number of different social information types. We then present some classification results achieved by using a small-scale database created with this methodology. 1 Introduction A focus of much information extraction research has been identifying surface-level semantic content (e.g., identifying who did what to whom when). In recent years, research on sentiment analysis and opinion mining has recognized that more subtle information can be communicated via linguistic features in the text (see Pang and Lee (2008) for a review), such as whether text (e.g., a movie review) is positive or negative (Turney 2002, Pang, Lee, and Vaithyanathan 2002, Dave, Lawrence, and Pennock 2003, Wiebe et al. 2004, Kennedy and Inkpen 2006, Agarwal, Biadsy, and Mckeown 2009, Greene and Resnik 2009, among many others). However, other subtle information available in text, such as a speaker’s emotional states (e.g., anger, embarrassment), intentions (e.g., persuasion, deception), and attitudes (e.g., disbelief, confidence), has not been explored as much, though there has been some work 71 in detecting emotion (e.g., Subasic a</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Pang, B. and Lee, L. 2008. Opinion Mining and Sentiment Analysis. Foundations and Trends in Information Retrieval 2(1-2): 1-135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment Classification using Machine Learning Techniques.</title>
<date>2002</date>
<booktitle>Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP):</booktitle>
<pages>79--86</pages>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Pang, B., Lee, L., and Vaithyanathan, S. 2002. Thumbs up? Sentiment Classification using Machine Learning Techniques. Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP): 79-86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>B O’Connor</author>
<author>D Jurafsky</author>
<author>A Ng</author>
</authors>
<title>Cheap and Fast - But is it Good? Evaluating NonExpert Annotations for Natural Language Tasks.</title>
<date>2008</date>
<booktitle>Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>254--263</pages>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Snow, R., O’Connor, B., Jurafsky, D., and Ng, A. 2008. Cheap and Fast - But is it Good? Evaluating NonExpert Annotations for Natural Language Tasks. Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 254-263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steyvers</author>
<author>M Lee</author>
<author>B Miller</author>
<author>P Hemmer</author>
</authors>
<title>The Wisdom of Crowds in the Recollection of Order Information. In</title>
<date>2009</date>
<booktitle>Advances in Neural Information Processing Systems, 23,</booktitle>
<editor>J. Lafferty, C. Williams (Eds.)</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4571" citStr="Steyvers et al. (2009)" startWordPosition="696" endWordPosition="699">. 2005) on these data. Human-based computation can leverage this ability 2 Reliable databases of social information from the population, and use it to construct a reli2.1 The need for databases able database of social information. Interestingly, In general, reliable databases are required to de- groups of humans are sometimes capable of provelop reliable machine learning algorithms. Unfor- ducing much more precise and reliable results than tunately, very few databases annotated with social any particular individual in the group. For example, information exist, and the few that do are small in Steyvers et al. (2009) has shown that such “wisdom size. A recent addition to the Linguistic Data Con- of crowds” phenomena occur in many knowledge sortium demonstrates this: The Language Under- domains, including human memory, problem solvstanding Annotation Corpus (LUAC) by Diab et al. ing, and prediction. In addition, Snow et al. (2008) (2009) includes text annotated with committed be- have demonstrated that a relatively small number of lief, which “distinguishes between statements which non-expert annotations in natural language tasks can assert belief or opinion, those which contain spec- achieve the same resu</context>
<context position="10870" citStr="Steyvers et al. (2009)" startWordPosition="1705" endWordPosition="1708">d taboo words for “persuading”. Future versions of the GWAP could allow the taboo word list to be influenced by which words are often associated with a particular social information type. 3We note that this is a restriction that might be relaxed in future versions of the GWAP. For instance, participants might decide whether a message expresses a social information type or not from their perspective, so the task is more like binary classification for each social information type. having expert knowledge or training, we expect that the cumulative knowledge to be quite reliable (for example, see Steyvers et al. (2009) and work by von Ahn (von Ahn and Dabbish 2004, von Ahn 2006, von Ahn, Kedia, and Blum 2006) for other successful cases involving the “wisdom of the crowds”, and Snow et al. (2008) for non-expert annotation in natural language tasks such as affect recognition). Because the same text can be evaluated by many different people, this can reduce the effect of idiosyncratic responses from a few individuals. An advantage of this kind of database is that many different kinds of social information can be generated and labeled by the participants so that the database contains examples of many different </context>
</contexts>
<marker>Steyvers, Lee, Miller, Hemmer, 2009</marker>
<rawString>Steyvers, M., Lee, M., Miller, B., and Hemmer, P. 2009. The Wisdom of Crowds in the Recollection of Order Information. In J. Lafferty, C. Williams (Eds.) Advances in Neural Information Processing Systems, 23, MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Subasic</author>
<author>A Huettner</author>
</authors>
<title>A?ect analysis of text using fuzzy semantic typing.</title>
<date>2001</date>
<journal>IEEE Transactions on Fuzzy Systems,</journal>
<volume>9</volume>
<pages>483--496</pages>
<contexts>
<context position="1916" citStr="Subasic and Huettner 2001" startWordPosition="285" endWordPosition="288">ee (2008) for a review), such as whether text (e.g., a movie review) is positive or negative (Turney 2002, Pang, Lee, and Vaithyanathan 2002, Dave, Lawrence, and Pennock 2003, Wiebe et al. 2004, Kennedy and Inkpen 2006, Agarwal, Biadsy, and Mckeown 2009, Greene and Resnik 2009, among many others). However, other subtle information available in text, such as a speaker’s emotional states (e.g., anger, embarrassment), intentions (e.g., persuasion, deception), and attitudes (e.g., disbelief, confidence), has not been explored as much, though there has been some work 71 in detecting emotion (e.g., Subasic and Huettner 2001, Alm, Roth, and Sproat 2005, Nicolov et al. 2006, Abbasi 2007) and detecting deception (e.g., Annolli, Balconi, and Ciceri 2002, Zhou et al. 2004, Gupta and Skillicorn 2006, Zhou and Sung 2008). This latter kind of social information is useful for identifying the “tone” of a message, i.e., for understanding the underlying intention behind a message’s creation, and also for predicting how this message will be interpreted by humans reading it. A technical barrier to extracting this kind of social information is that there are currently no large-scale text databases that are annotated with socia</context>
</contexts>
<marker>Subasic, Huettner, 2001</marker>
<rawString>Subasic, P. and Huettner A. 2001. A?ect analysis of text using fuzzy semantic typing. IEEE Transactions on Fuzzy Systems, 9: 483-496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews.</title>
<date>2002</date>
<booktitle>Proceedings of the Association for Computational Linguistics (ACL):</booktitle>
<pages>417--424</pages>
<contexts>
<context position="1396" citStr="Turney 2002" startWordPosition="209" endWordPosition="210">number of different social information types. We then present some classification results achieved by using a small-scale database created with this methodology. 1 Introduction A focus of much information extraction research has been identifying surface-level semantic content (e.g., identifying who did what to whom when). In recent years, research on sentiment analysis and opinion mining has recognized that more subtle information can be communicated via linguistic features in the text (see Pang and Lee (2008) for a review), such as whether text (e.g., a movie review) is positive or negative (Turney 2002, Pang, Lee, and Vaithyanathan 2002, Dave, Lawrence, and Pennock 2003, Wiebe et al. 2004, Kennedy and Inkpen 2006, Agarwal, Biadsy, and Mckeown 2009, Greene and Resnik 2009, among many others). However, other subtle information available in text, such as a speaker’s emotional states (e.g., anger, embarrassment), intentions (e.g., persuasion, deception), and attitudes (e.g., disbelief, confidence), has not been explored as much, though there has been some work 71 in detecting emotion (e.g., Subasic and Huettner 2001, Alm, Roth, and Sproat 2005, Nicolov et al. 2006, Abbasi 2007) and detecting de</context>
<context position="24965" citStr="Turney 2002" startWordPosition="3993" endWordPosition="3994">g more than once in the database deception politeness rudeness embarrassment confidence disbelief formality persuading deception .45 politeness .03 rudeness .03 embarrassment .04 confidence .01 disbelief .05 formality .02 persuading .03 76 The use of shallow linguistic features seemed a reasonable first investigation as prior research involving linguistic cues for identifying information in text has often used word-level cues. For example, positive and negative affect words (e.g., excellent vs. poor) have been used in sentiment analysis to summarize whether a document is positive or negative (Turney 2002, Pang, Lee, and Vaithyanathan 2002, among others). In deception detection research, informative word-level cues include counting first and third person pronoun usage (e.g., me vs. them) (Anolli, Balconi, and Ciceri 2002), and noting the number of “exception words” (e.g., but, except, without) (Gupta and Skillicorn 2006). In addition, informative shallow text properties have also been identified (Zhou et al. 2004), such as (a) number of verbs, words, noun phrases, and sentences, (b) average sentence and word length, and (c) word type to word token ratio. The SMLR classifier model was trained t</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Turney, P. 2002. Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews. Proceedings of the Association for Computational Linguistics (ACL): 417-424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L von Ahn</author>
</authors>
<title>Games With A Purpose.</title>
<date>2006</date>
<journal>IEEE Computer Magazine,</journal>
<pages>96--98</pages>
<marker>von Ahn, 2006</marker>
<rawString>von Ahn, L. 2006. Games With A Purpose. IEEE Computer Magazine, June 2006: 96-98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L von Ahn</author>
<author>L Dabbish</author>
</authors>
<title>Labeling Images with a Computer Game.</title>
<date>2004</date>
<booktitle>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Association for Computing Machinery,</booktitle>
<pages>319--326</pages>
<location>New York,</location>
<marker>von Ahn, Dabbish, 2004</marker>
<rawString>von Ahn, L. and Dabbish, L. 2004. Labeling Images with a Computer Game. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Association for Computing Machinery, New York, 2004): 319-326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L von Ahn</author>
<author>M Kedia</author>
<author>M Blum</author>
</authors>
<title>Verbosity: A Game for Collecting Common-Sense Facts,</title>
<date>2006</date>
<booktitle>In Proceedings of the SIGCHI conference on Human Factors in computing systems,</booktitle>
<location>Montral, Quebec, Canada.</location>
<marker>von Ahn, Kedia, Blum, 2006</marker>
<rawString>von Ahn, L., Kedia, M. and Blum, M. 2006. Verbosity: A Game for Collecting Common-Sense Facts, In Proceedings of the SIGCHI conference on Human Factors in computing systems, Montral, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Wiebe</author>
<author>T Wilson</author>
<author>R Bruce</author>
<author>M Bell</author>
<author>M Martin</author>
</authors>
<title>Learning subjective language.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<pages>277--308</pages>
<contexts>
<context position="1484" citStr="Wiebe et al. 2004" startWordPosition="222" endWordPosition="225">results achieved by using a small-scale database created with this methodology. 1 Introduction A focus of much information extraction research has been identifying surface-level semantic content (e.g., identifying who did what to whom when). In recent years, research on sentiment analysis and opinion mining has recognized that more subtle information can be communicated via linguistic features in the text (see Pang and Lee (2008) for a review), such as whether text (e.g., a movie review) is positive or negative (Turney 2002, Pang, Lee, and Vaithyanathan 2002, Dave, Lawrence, and Pennock 2003, Wiebe et al. 2004, Kennedy and Inkpen 2006, Agarwal, Biadsy, and Mckeown 2009, Greene and Resnik 2009, among many others). However, other subtle information available in text, such as a speaker’s emotional states (e.g., anger, embarrassment), intentions (e.g., persuasion, deception), and attitudes (e.g., disbelief, confidence), has not been explored as much, though there has been some work 71 in detecting emotion (e.g., Subasic and Huettner 2001, Alm, Roth, and Sproat 2005, Nicolov et al. 2006, Abbasi 2007) and detecting deception (e.g., Annolli, Balconi, and Ciceri 2002, Zhou et al. 2004, Gupta and Skillicorn</context>
</contexts>
<marker>Wiebe, Wilson, Bruce, Bell, Martin, 2004</marker>
<rawString>Wiebe, J.M., Wilson, T., Bruce, R., Bell, M., and Martin, M. 2004. Learning subjective language. Computational Linguistics, 30: 277-308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zhou</author>
<author>J Burgoon</author>
<author>J Nunamaker</author>
<author>D Twitchell</author>
</authors>
<title>Automating linguistics-based cues for detecting deception in text-based asynchronous computermediated communication.</title>
<date>2004</date>
<journal>Group Decision and Negotiation,</journal>
<volume>13</volume>
<pages>81--106</pages>
<contexts>
<context position="2062" citStr="Zhou et al. 2004" startWordPosition="309" endWordPosition="312">e, and Pennock 2003, Wiebe et al. 2004, Kennedy and Inkpen 2006, Agarwal, Biadsy, and Mckeown 2009, Greene and Resnik 2009, among many others). However, other subtle information available in text, such as a speaker’s emotional states (e.g., anger, embarrassment), intentions (e.g., persuasion, deception), and attitudes (e.g., disbelief, confidence), has not been explored as much, though there has been some work 71 in detecting emotion (e.g., Subasic and Huettner 2001, Alm, Roth, and Sproat 2005, Nicolov et al. 2006, Abbasi 2007) and detecting deception (e.g., Annolli, Balconi, and Ciceri 2002, Zhou et al. 2004, Gupta and Skillicorn 2006, Zhou and Sung 2008). This latter kind of social information is useful for identifying the “tone” of a message, i.e., for understanding the underlying intention behind a message’s creation, and also for predicting how this message will be interpreted by humans reading it. A technical barrier to extracting this kind of social information is that there are currently no large-scale text databases that are annotated with social information from which to learn the relevant linguistic cues. That is, there are few examples of social information “ground truth” - text annota</context>
<context position="25382" citStr="Zhou et al. 2004" startWordPosition="4053" endWordPosition="4056">ed word-level cues. For example, positive and negative affect words (e.g., excellent vs. poor) have been used in sentiment analysis to summarize whether a document is positive or negative (Turney 2002, Pang, Lee, and Vaithyanathan 2002, among others). In deception detection research, informative word-level cues include counting first and third person pronoun usage (e.g., me vs. them) (Anolli, Balconi, and Ciceri 2002), and noting the number of “exception words” (e.g., but, except, without) (Gupta and Skillicorn 2006). In addition, informative shallow text properties have also been identified (Zhou et al. 2004), such as (a) number of verbs, words, noun phrases, and sentences, (b) average sentence and word length, and (c) word type to word token ratio. The SMLR classifier model was trained to produce the label (one of eight) corresponding to the generated social information using all the text features as input. Using a 10-fold cross-validation procedure, the model was trained on 90% of the messages and tested on the remaining 10%. The sparse classifier favors a small number of features in the regression solution and sets the weight of a large fraction of features to zero. Some of the non-zero weights</context>
</contexts>
<marker>Zhou, Burgoon, Nunamaker, Twitchell, 2004</marker>
<rawString>Zhou, L., Burgoon, J., Nunamaker, J., and Twitchell, D. 2004. Automating linguistics-based cues for detecting deception in text-based asynchronous computermediated communication. Group Decision and Negotiation, 13: 81-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zhou</author>
<author>Y Sung</author>
</authors>
<title>Cues to deception in online Chinese groups.</title>
<date>2008</date>
<booktitle>Proceedings of the 41st Annual Hawaii international Conference on System Sciences, 146.</booktitle>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC:</location>
<contexts>
<context position="2110" citStr="Zhou and Sung 2008" startWordPosition="317" endWordPosition="320">dy and Inkpen 2006, Agarwal, Biadsy, and Mckeown 2009, Greene and Resnik 2009, among many others). However, other subtle information available in text, such as a speaker’s emotional states (e.g., anger, embarrassment), intentions (e.g., persuasion, deception), and attitudes (e.g., disbelief, confidence), has not been explored as much, though there has been some work 71 in detecting emotion (e.g., Subasic and Huettner 2001, Alm, Roth, and Sproat 2005, Nicolov et al. 2006, Abbasi 2007) and detecting deception (e.g., Annolli, Balconi, and Ciceri 2002, Zhou et al. 2004, Gupta and Skillicorn 2006, Zhou and Sung 2008). This latter kind of social information is useful for identifying the “tone” of a message, i.e., for understanding the underlying intention behind a message’s creation, and also for predicting how this message will be interpreted by humans reading it. A technical barrier to extracting this kind of social information is that there are currently no large-scale text databases that are annotated with social information from which to learn the relevant linguistic cues. That is, there are few examples of social information “ground truth” - text annotated with human perceptions of the social informa</context>
<context position="6784" citStr="Zhou and Sung 2008" startWordPosition="1050" endWordPosition="1053">nglish Gigaword corpus (Graff 2004), the musical style of songs, impressions of 2003) contains 1756504 words). sights and sounds in videos, and common sense reAnother tack taken by researchers has been to use lationships between concepts (von Ahn, Kedia, and open-source data that are likely to demonstrate cer- Blum 2006). In addition, as the collected data comes tain social information by happenstance, e.g., online from and is vetted by a large number of participants, gaming forums with games that happen to involve we can gauge which messages are reliable examples the intent to deceive (e.g., Zhou and Sung 2008: of particular social information and which are conMafia game forums). While these data sets are larger fusing examples. in size, they do not have the breadth of coverage in 2.3 A GWAP for social information in text terms of what social information they can capture We designed a GWAP to create a database of mesbecause, by nature, the games only explicitly involve sages annotated with social information, where unone kind of social information (e.g., intentions: de- paid participants provide knowledge about the social ception); other social information cannot reliably be information in text. Th</context>
</contexts>
<marker>Zhou, Sung, 2008</marker>
<rawString>Zhou, L. and Sung, Y. 2008. Cues to deception in online Chinese groups. Proceedings of the 41st Annual Hawaii international Conference on System Sciences, 146. Washington, DC: IEEE Computer Society.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>