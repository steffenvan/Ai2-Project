<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001419">
<title confidence="0.9987035">
A Practical Nonmonotonic Theory
for Reasoning about Speech Acts
</title>
<author confidence="0.990239">
Douglas Appelt, Kurt Konolige
</author>
<affiliation confidence="0.6604725">
Artificial Intelligence Center and
Center for the Study of Language and Information
SRI International
Menlo Park, California
</affiliation>
<sectionHeader confidence="0.96718" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999905576923077">
A prerequisite to a theory of the way agents un-
derstand speech acts is a theory of how their be-
liefs and intentions are revised as a consequence
of events. This process of attitude revision is
an interesting domain for the application of non-
monotonic reasoning because speech acts have a
conventional aspect that is readily represented by
defaults, but that interacts with an agent&apos;s be-
liefs and intentions in many complex ways that
may override the defaults. Perrault has devel-
oped a theory of speech acts, based on Rieter&apos;s
default logic, that captures the conventional as-
pect; it does not, however, adequately account for
certain easily observed facts about attitude revi-
sion resulting from speech acts. A natural the-
ory of attitude revision seems to require a method
of stating preferences among competing defaults.
We present here a speech act theory, formalized
in hierarchic autoepistemic logic (a refinement of
Moore&apos;s autoepistemic logic), in which revision of
both the speaker&apos;s and hearer&apos;s attitudes can be
adequately described. As a collateral benefit, effi-
cient automatic reasoning methods for the formal-
ism exist. The theory has been implemented and
is now being employed by an utterance-planning
system.
</bodyText>
<sectionHeader confidence="0.998905" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975027027027">
The general idea of utterance planning has been
at the focus of much NL processing research for
the last ten years. The central thesis of this
approach is that utterances are actions that are
planned to satisfy particular speaker goals. This
has led researchers to formalize speech acts in a
way that would permit them to be used as op-
erators in a planning system [1,4 The central
problem in formalizing speech acts is to correctly
capture the pertinent facts about the revision of
the speaker&apos;s and hearer&apos;s attitudes that ensues
as a consequence of the act. This turns out to be
quite difficult because the results of the attitude
revision are highly conditional upon the context of
the utterance.
To consider just a small number of the contin-
gencies that may arise, consider a speaker S utter-
ing a declarative sentence with propositional con-
tent P to hearer H. One is inclined to say that,
if H believes S is sincere, H will believe P. How-
ever, if H believes initially, he may not be
convinced, even if he thinks S is sincere. On the
other hand, he may change his beliefs, or he may
suspend belief as to whether P is true. H may
not believe —43, but simply believe that S is neiter
competent nor sincere, and so may not come to
believe P. The problem one is then faced with
is this: How does one describe the effect of ut-
tering the declarative sentence so that given the
appropriate contextual elements, any one of these
possibilities can follow from the description?
One possible approach to this problem would be
to find some fundamental, context-independent ef-
fect of informing that is true every time a declara-
tive sentence is uttered. If one&apos;s general theory of
the world and of rational behavior were sufficiently
strong and detailed, any of the consequences of
</bodyText>
<page confidence="0.993438">
170
</page>
<bodyText confidence="0.996941537313433">
attitude revision would be derivable from the ba-
sic effect in combination with the elaborate theory
of rationality. The initial efforts made along this
path [3,5] entailed the axiomatization the effects
of speech acts as producing in the hearer the be-
lief that the speaker wants him to recognize the
latter&apos;s intention to hold some other belief. The
effects were characterized by nestings of Goal and
Bel operators, as in
Bel(H, Goal(S, Bel(H, P))).
If the right conditions for attitude revision ob-
tained, the conclusion Bel(H, P) would follow
from the above assumption.
This general approach proved inadequate be-
cause there is in fact no such statement about be-
liefs about goals about beliefs that is true in every
performance of a speech act. It is possible to con-
struct a counterexample contradicting any such ef-
fect that might be postulated. In addition, long
and complicated chains of reasoning are required
to derive the simplest, most basic consequences of
an utterance in situations in which all of the &amp;quot;nor-
mal&amp;quot; conditions obtain — a consequence that runs
counter to one&apos;s intuitive expectations.
Cohen and Levesque [4] developed a speech act
theory in a monotonic modal logic that incorpo-
rates context-dependent preconditions in the ax-
ioms that state the effects of a speech act. Their
approach overcomes the theoretical difficulties of
earlier context-independent attempts; however, if
one desires to apply their theory in a practical
computational system for reasoning about speech
acts, one is faced with serious difficulties. Some
of the context-dependent conditions that deter-
mine the effects of a speech act, according to their
theory, involve statements about what an agent
does not believe, as well as what he does believe.
This means that for conclusions about the effect of
speech acts to follow from the theory, it must in-
clude an explicit representation of an agent&apos;s igno-
rance as well as of his knowledge, which in practice
is difficult or even impossible to achieve.
A further complication arises from the type of
reasoning necessary for adequate characterization
of the attitude revision process. A theory based on
monotonic reasoning can only distinguish between
belief and lack thereof, whereas one based on non-
monotonic reasoning can distinguish between be-
lief (or its absence) as a consequence of known
facts, and belief that follows as a default because
more specific information is absent. To the extent
that such a distinction plays a role in the attitude
revision process, it argues for a formalization with
a nonmonotonic character.
Our research is therefore motivated by the fol-
lowing observations: (1) earlier work demonstrates
convincingly that any adequate speech-act theory
must relate the effects of a speech act to context-
dependent preconditions; (2) these preconditions
must depend on the ignorance as well as on the
knowledge of the relevant agents; (3) any prac-
tical system for reasoning about ignorance must
be based on nonmonotonic reasoning; (4) existing
speech act theories based on nonmonotonic rea-
soning cannot account for the facts of attitude re-
vision resulting from the performance of speech
acts.
</bodyText>
<sectionHeader confidence="0.951851" genericHeader="introduction">
2 Perrault&apos;s Default Theory
</sectionHeader>
<subsectionHeader confidence="0.647172">
of Speech Acts
</subsectionHeader>
<bodyText confidence="0.999508714285714">
As an alternative to monotonic theories, Perrault
has proposed a theory of speech acts, based on an
extension of Reiter&apos;s default logic [11] extended
to include- default-rule schemata. We shall sum-
marize Perrault&apos;s theory briefly as it relates to in-
forming and belief. The notation p q is intended
as an abbreviation of the default rule of inference,
</bodyText>
<listItem confidence="0.719958">
p: Mq
•
</listItem>
<bodyText confidence="0.996907555555556">
Default theories of this form are called normal.
Every normal default theory has at least one ex-
tension, i.e., a mutually consistent set of sentences
sanctioned by the theory.
The operator Bz,t represents Agent x&apos;s beliefs at
time t and is assumed to posess all the properties
of the modal system weak 55 (that is, S5 without
the schema Bz,ty6 D 0), plus the following axioms:
Persistence:
</bodyText>
<equation confidence="0.943003454545455">
D
Memory:
BP D Br,t+iBz,tP
171
Observability:
Doz,toe A Doy,t(Obs(Doz,t(a))) (3)
By,t÷iDoz,t(a)
Belief Transfer:
Bz,tBy,tP BP (4)
Declarative:
Doz,t(Utter(P)) BP (5)
</equation>
<bodyText confidence="0.994304722222222">
In addition, there is a default-rule schema stat-
ing that, if p q is a default rule, then so is
for any agent x and time t.
Perrault could demonstrate that, given his the-
ory, there is an extension containing all of the
desired conclusions regarding the beliefs of the
speaker and hearer, starting from the fact that
a speaker utters a declarative sentence and the
hearer observes him uttering it. Furthermore, the
theory can make correct predictions in cases in
which the usual preconditions of the speech act
do not obtain. For example, if the speaker is ly-
ing, but the hearer does not recognize the lie, then
the hearer&apos;s beliefs are exactly the same as when
the speaker tells the truth; moreover the speaker&apos;s
beliefs about mutual belief are the same, but he
still does not believe the proposition he uttered —
that is, he fails to be convinced by his own lie.
</bodyText>
<sectionHeader confidence="0.9701195" genericHeader="method">
3 Problems with Perrault&apos;s
Theory
</sectionHeader>
<bodyText confidence="0.999965158730159">
A serious problem arises with Perrault&apos;s theory
concerning reasoning about an agent&apos;s ignorance.
His theory predicts that a speaker can convince
himself of any unsupported proposition simply by
asserting it, which is clearly at odds with our in-
tuitions. Suppose that it is true of speaker s that
,.133,tP. Suppose furthermore that, for whatever
reason, s utters P. In the absence of any further
information about the speaker&apos;s and hearer&apos;s be-
liefs, it is a consequence of axioms (1)-(5) that
B,,t+iBh,t+IP. From this consequence and the
belief transfer rule (4) it is possible to conclude
B„,t+IP. The strongest conclusion that can be
derived about s&apos;s beliefs at t + 1 without using
this default rule is B,,t+i -43,4 P, which is not suf-
ficient to override the default.
This problem does not admit of any simple fixes.
One clearly does not want an axiom or default rule
of the form that asserts what amounts to &amp;quot;igno-
rance persists&amp;quot; to defeat conclusions drawn from
speech acts. In that case, one could never con-
clude that anyone ever learns anything as a result
of a speech act. The alternative is to weaken the
conditions under which the default rules can be
defeated. However, by adopting this strategy we
are giving up the advantage of using normal de-
faults. In general, nonnormal default theories do
not necessarily have extensions, nor is there any
proof procedure for such logics.
Perrault has intentionally left open the question
of how a speech act theory should be integrated
with a general theory of action and belief revision.
He finesses this problem by introducing the per-
sistence axiom, which states that beliefs always
persist across changes in state. Clearly this is not
true in general, because actions typically change
our beliefs about what is true of the world. Even
if one considers only speech acts, in some cases •
one can get an agent to change his beliefs by say-
ing something, and in other cases not. Whether
one can or not, however, depends on what be-
lief revision strategy is adopted by the respective
agents in a given situation. The problem cannot
be solved by simply adding a few more axioms
and default rules to the theory. Any theory that
allows for the possibility of describing belief revi-
sion must of necessity confront the problem of in-
consistent extensions. This means that, if a hearer
initially believes the default theory will have
(at least) one extension for the case in which his
belief that -1p persists, and one extension in which
he changes his mind and believes p. Perhaps it
will even have an extension in which he suspends
belief as to whether p.
The source of the difficulties surrounding Per-
rault&apos;s theory is that the default logic he adopts
is unable to describe the attitude revision that oc-
curs in consequence of a speech act. It is not our
purpose here to state what an agent&apos;s belief re-
vision strategy should be. Rather we introduce a
framework within which a variety of belief revision
strategies can be accomodated efficiently, and we
demonstrate that this framework can be applied in
</bodyText>
<page confidence="0.989216">
172
</page>
<bodyText confidence="0.999044352941177">
a way that eliminates the problems with Perrault&apos;s
theory.
Finally, there is a serious practical problem
faced by anyone who wishes to implement Per-
rault&apos;s theory in a system that reasons about
speech acts. There is no way the belief transfer
rule can be used efficiently by a reasoning sys-
tem; even if it is assumed that its application is
restricted to the speaker and hearer, with no other
agents in the domain involved. If it is used in a
backward direction, it applies to its own result. In-
voking the rule in a forward direction is also prob-
lematic, because in general one agent will have a
very large number of beliefs (even an infinite num-
ber, if introspection is taken into account) about
another agent&apos;s beliefs, most of which will be ir-
relevant to the problem at hand.
</bodyText>
<sectionHeader confidence="0.977046" genericHeader="method">
4 Hierarchic Autoepistemic
Logic
</sectionHeader>
<bodyText confidence="0.993852444444444">
Autoepistemic (AE) logic was developed by Moore
[10] as a reconstruction of McDermott&apos;s nonmono-
tonic logic [9]. An autoepistemic logic is based on
a first-order language augmented by a modal op-
erator L, which is interpreted intuitively as self
belief. A stable expansion (analogous to an exten-
sion of a default theory) of an autoepistemic base
set A is a set of formulas T satisfying the following
conditions:
</bodyText>
<listItem confidence="0.9970968">
1. T contains all the sentences of the base the-
ory A
2. T is closed under first-order consequence
3. If 0 T , then L0 T
4. If 0 T, then -ILO E T
</listItem>
<bodyText confidence="0.999921762711865">
Hierarchic autoepistemic logic (HAEL) was de-
veloped in response to two deficiences of autoepis-
temic logic, when the latter is viewed as a logic
for automated nonmonotonic reasoning. The first
is a representational problem: how to incorporate
preferences among default inferences in a natural
way within the logic. Such preferences arise in
many disparate settings in nonmonotonic reason-
ing — for example, in taxonomic hierarchies [6]
or in reasoning about events over time [12]. To
some extent, preferences among defaults can be
encoded in AE logic by introducing auxiliary in-
formation into the statements of the defaults, but
this method does not always accord satisfactorily
with our intuitions. The most natural statement
of preferences is with respect to the multiple ex-
pansions of a particular base set, that is, we pre-
fer certain expansions because the defaults used in
them have a higher priority than the ones used in
alternative expansions.
The second problem is computational: how to
tell whether a proposition is contained within the
desired expansion of a base set. As can be seen
from the above definition, a stable expansion of
an autoepistemic theory is defined as a fixedpoint;
the question of whether a formula belongs to this
fixedpoint is not even semidecidable. This prob-
lem is shared by all of the most popular nonmono-
tonic logics. The usual recourse is to restrict the
expressive power of the language, e.g., normal de-
fault theories [11] and separable circumscriptive
theories [8]. However, as exemplified by the diffi-
culties of Perrault&apos;s approach, it may not be easy
or even possible to express the relevant facts with
a restricted language.
Hierarchical autoepistemic logic is a modifica-
tion of autoepistemic logic that addresses these
two considerations. In HAEL, the primary struc-
ture is not a single uniform theory, but a collection
of subtheories linked in a hierarchy. Subtheories
represent different sources of information available
to an agent, while the hierarchy expresses the way
in which this information is combined. For ex-
ample, in representing taxonomic defaults, more
specific information would take precedence over
general attributes. HAEL thus permits a natural
expression of preferences among defaults. Further-
more, given the hierarchical nature of the subthe-
ory relation, it is possible to give a constructive
semantics for the autoepistemic operator, in con-
trast to the usual self-referential fixedpoints. We
can then arrive easily at computational realiza-
tions of the logic.
The language of HAEL consists of a standard
first-order language, augmented by a indexed set
of unary modal operators Li. If 0 is any sentence
(containing no free variables) of the first-order lan-
guage, then Lick is also a sentence. Note that nei-
ther nesting of modal operators nor quantifying
</bodyText>
<page confidence="0.995493">
173
</page>
<bodyText confidence="0.9800035">
into a modal context is allowed. Sentences with-
out modal operators are called ordinary.
An HAEL structure r consists of an indexed
set of subtheories r, together with a partial order
on the set. We write ri 75 if ri precedes ri in
the order. Associated with every subtheory ri is a
base set Ai, the initial sentences of the structure.
Within Ai, the occurrence of Li is restricted by
the following condition:
If Li occurs positively (negatively) in
</bodyText>
<equation confidence="0.58469">
(6)
</equation>
<bodyText confidence="0.994269230769231">
Ai, then ri -4 ri ri).
This restriction prevents the modal operator from
referring to subtheories that succeed it in the hier-
archy, since Li0 is intended to mean that 0 is an
element of the subtheory rj. The distinction be-
tween positive and negative occurrences is simply
that a subtheory may represent (using L) which
sentences it contains, but is forbidden from repre-
senting what it does not contain.
A complex stable expansion of an HAEL struc-
ture r is a set of sets of sentences Ti corresponding
to the subtheories of r. It obeys the following con-
ditions (0 is an ordinary sentence):
</bodyText>
<listItem confidence="0.9996675">
1. Each Ti contains Ai
2. Each Ti is closed under first-order conse-
quence
3. If E Ti, and ri, then LiO E
4. If 0 Ti, and ri -4 ri, then -4,10 E
5. If E Tj , and ri , then t;6 E
</listItem>
<bodyText confidence="0.981518666666667">
These conditions are similar to those for AE sta-
ble expansions. Note that, in (3) and (4), Ti con-
tains modal atoms describing the contents of sub-
theories beneath it in the hierarchy. In addition,
according to (5) it also inherits all the ordinary
sentences of preceeding subtheories.
Unlike AE base sets, which may have more
than one stable expansion, HAEL structures have
a unique minimal complex stable expansion (see
Konolige [7]). So we are justified in speaking of
&amp;quot;the&amp;quot; theory of an HAEL structure and, from this
point on, we shall identify the subtheory ri of a
structure with the set of sentences in the complex
stable expansion for that subtheory.
Here is a simple example, which can be inter-
preted as the standard &amp;quot;typically birds fly&amp;quot; default
scenario by letting F(x) be &amp;quot;x flies,&amp;quot; B(x) be &amp;quot;x
is a bird,&amp;quot; and P(x) be &amp;quot;x is a penguin.&amp;quot;
</bodyText>
<equation confidence="0.999678">
A0 = {P(a),B(a)}
(7)
= {LIP(a) A --.L0F(a) j ,F(a))
A2 = {L2B(a) A -1,1,F(a) j F(a)}
</equation>
<bodyText confidence="0.999572565217391">
Theory To contains all of the first-order con-
sequences of P(a), B(a), LoP(a), and LoB(a).
--.L0F(a) is not in 0, but it is in r1, as is LoP(a),
-,L0-,P(a), etc. Note that P(a) is inherited by
71; hence LiP(a) is in r1. Given this, by first-
order closure -,F(a) is in Ti and, by inheritance,
Li-,F(a) is in r2, so that F(a) cannot be derived
there. On the other hand, r2 inherits F(a) from
Note from this example that information
present in the lowest subtheories of the hierarchy
percolates to its top. More specific evidence, or
preferred defaults, should be placed lower in the
hierarchy, so that their effects will block the action
of higher-placed evidence or defaults.
HAEL can be given a constructive semantics
that is in accord with the closure conditions.
When the inference procedure of each subtheory
is decidable, an obvious decidable proof method
for the logic exists. The details of this develop-
ment are too complicated to be included here, but
are described by Konolige [7]. For the rest of this
paper, we shall use a propositional base language;
the derivations can be readily checked.
</bodyText>
<sectionHeader confidence="0.997175" genericHeader="method">
5 A HAEL Theory of Speech
Acts
</sectionHeader>
<bodyText confidence="0.999912">
We demonstrate here how to construct a hierarchic
autoepistemic theory of speech acts. We assume
that there is a hierarchy of autoepistemic subthe-
ories as illustrated in Figure 1. The lowest subthe-
ory, 7.0, contains the strongest evidence about the
speaker&apos;s and hearer&apos;s mental states. For exam-
ple, if it is known to the hearer that the speaker
is lying, this information goes into To.
In subtheory Ti, defaults are collected about the
effects of the speech act on the beliefs of both
hearer and speaker. These defaults can be over-
ridden by the particular evidence of TO. Together
</bodyText>
<page confidence="0.99653">
174
</page>
<bodyText confidence="0.999018696969697">
TO and 71 constitute the first level of reasoning
about the speech act. At Level 2, the beliefs of
the speaker and hearer that can be deduced in
r1 are used as evidence to guide defaults about
nested beliefs, that is, the speaker&apos;s beliefs about
the hearer&apos;s beliefs, and vice versa. These results
are collected in 7-2. In a similar manner, successive
levels contain the result of one agent&apos;s reflection
upon his and his interlocutor&apos;s beliefs and inten-
tions at the next lower level. We shall discuss here
how Levels TO and 71 of the HAEL theory are ax-
iomatized, and shall extend the axiomatization to
the higher theories by means of axiom schemata.
An agent&apos;s belief revision strategy is represented
by two features of the model. The position of
the speech act theory in the general hierarchy of
theories determines the way in which conclusions
drawn in those theories can defeat conclusions that
follow from speech acts. In our model, the speech
act defaults will go into the subtheory 71, while
evidence that will be used to defeat these defaults
will go in TO. In addition, the axioms that relate
Ti to ro determine precisely what each agent is
willing to accept from ro as evidence against the
default conclusions of the speech act theory.
It is easy to duplicate the details of Perrault&apos;s
analysis within this framework. Theory ro would
contain all the agents&apos; beliefs prior to the speech
act, while the defaults of 7-1 would state that an
agent believed the utterance P if he did not be-
lieve its negation in 70. As we have noted, this
analysis does not allow for the situation in which
the speaker utters P without believing either it
or its opposite, and then becomes convinced of its
truth by the very fact of having uttered it — nor
does it allow the hearer to change his belief in
as a result of the utterance.
We choose a more complicated and realistic ex-
pression of belief revision. Specifically, we allow
an agent to believe P (in 7-1) by virtue of the ut-
terance of P only if he does not have any evidence
(in 7-0) against believing it. Using this scheme,
we can accommodate the hearer&apos;s change of be-
lief, and show that the speaker is not convinced
by his own efforts.
We now present the axioms of the HAEL theory
for the declarative utterance of the proposition P.
The language we use is a propositional modal one
for the beliefs of the speaker and hearer. Agents s
and h represent the speaker and hearer; the sub-
scripts i and f represent the initial situation and
the situation resulting from the utterance, respec-
tively. There are two operators: [a] for a&apos;s belief
and {a} for a&apos;s goals. The formula [h1](/), for exam-
ple, means that the hearer believes in the final
situation, while {si}4) means that the speaker in-
tended in the initial situation. In addition, we
use a phantom agent u to represent the content of
the utterance and certain assumptions about the
speaker&apos;s intentions. We do not argue here as to
what constitutes the correct logic of these opera-
tors; a convenient one is weak S5.
The following axioms are assumed to hold in all
subtheories.
[u]P, P the propositional content of ut- (8)
terance
</bodyText>
<equation confidence="0.9818355">
[u]b3 [ti]lsi}[hi]cb (9)
[a]{a}0 {a}0, where a is any (10)
</equation>
<bodyText confidence="0.909376130434783">
agent in any sit-
uation.
The contents of the u theory are essentially the
same for all types of speech acts. The precise ef-
fects upon the speaker&apos;s and hearer&apos;s mental states
is determined by the propositional content of the
utterance and its mood. We assume here that the
speaker utters a simple declarative sentence, (Ax-
iom 8), although a similar analysis could be done
for other types of sentences, given a suitable repre-
sentation of their propositional content. Proposi-
tions that are true in u generally become believed
by the speaker and hearer in 7-1, provided that
these propositions bear the proper relationship to
their beliefs in T. Finally, the speaker intends to
bring about each of the beliefs the hearer acquires
in ri , also subject to the caveat that it is consistent
with his beliefs in TO.
Relation between subtheories:
To -‹ Tj (11)
Speaker&apos;s beliefs as a consequence of the speech
act:
in Ai: [u](/) A -,Lo-l[si]fk3 IsfJO (12)
</bodyText>
<page confidence="0.996634">
175
</page>
<figureCaption confidence="0.999491">
Figure 1: A Hierarchic Autoepistemic Theory
</figureCaption>
<figure confidence="0.982878409090909">
•
•
•
roe.&amp;quot;
re,/
r.■••••
ove,
r••••••
•
•
/Ivo
0/0
eeo
•■•••
ee•
Level 3
Level 2
Hearer&apos;s beliefs as a consequence of the speech act:
in Ai:
aid(/&apos; A (13)
--,Lo-qhM A -,Lo[hi]-1[s1]0 A
--1.Lo[hi]-4s1lliti]O)D [hijO
</figure>
<bodyText confidence="0.999486784313726">
The asymmetry between Axioms 12 and 13 is a
consequence of the fact that a speech act has dif-
ferent effects on the speaker&apos;s and hearer&apos;s mental
states. The intuition behind these axioms is that
a speech act should never change the speaker&apos;s
mental attitudes with regard to the proposition
he utters. If he utters a sentence, regardless of
whether he is lying, or in any other way insincere,
he should believe P after the utterance if and only
if he believed it before. However, in the hearer&apos;s
case, whether he believes P depends not only on
his prior mental state with respect to P, but also
on whether he believes that the speaker is being
sincere. Axiom 13 states that a hearer is willing to
believe what a speaker says if it does not conflict
with his own beliefs in To, and if the utterance does
not conflict with what the hearer believes about
the speaker&apos;s mental state, (i.e., that the speaker
is not lying), and if he believes that believing P
is consistent with his beliefs about the speaker&apos;s
prior intentions (i.e., that the speaker is using the
utterance with communicative intent, as distinct
from, say, testing a microphone).
As a first example of the use of the theory, con-
sider the normal case in which Ao contains no evi-
dence about the speaker&apos;s and hearer&apos;s beliefs after
the speech act. In this event, Ao is empty and Ai
contains Axioms 8-1.0. By the inheritance condi-
tions, Ti contains --4.0-[sf]P, and so must contain
[s/]/3 by axiom 12. Similarly, from Axiom 13 it fol-
lows that [hAP is in T1. Further derivations lead
to {s1}[hf1P, {si}[IgHsi}[lif]P, and so on.
As a second example, consider the case in which
the speaker utters P, perhaps to convince the
hearer of it, but does not himself believe either
P or its negation. In this case, To contains -[sf]/&apos;
and and 71 must contain Lo-isf)P by
the inheritance condition. Hence, the application
of Axiom 12 will be blocked, and so we cannot
conclude in r1 that the speaker believes P. On
the other hand, since none of the antecedents of
Axiom 13 are affected, the hearer does come to
believe it.
Finally, consider belief revision on the part of
the hearer. The precise path belief revision takes
depends on the contents of To. If we consider the
hearer&apos;s belief to be stronger evidence than that of
the utterance, we would transfer the hearer&apos;s ini-
tial belief [111],P to [h.f]-iP in To, and block the de-
fault Axiom 13. But suppose the hearer does not
believe ,P strongly in the initial situation. Then
</bodyText>
<page confidence="0.995818">
176
</page>
<bodyText confidence="0.999929666666667">
we would transfer (by default) the belief [111]-42 177 resents the speaker&apos;s reasoning about the hearer&apos;s
to a subtheory higher than pi , since the evidence beliefs about what the speaker&apos; believes.
furnished by the utterance is meant to override the In general, agents may have quite complicated
initial belief. Thus, by making the proper choices theories about how other agents apply defaults.
regarding the transfer of initial beliefs in various The simplest assumption we can make is that they
subtheories, it becomes possible to represent the reason in a uniform manner, exactly the same as
revision of the hearer&apos;s beliefs. the way we axiomatized Level 1. Therefore, we ex-
This theory of speech acts has been presented tend the analysis just presented to arbitrary reflec-
with respect to declarative sentences and repre- tion of agents on one another&apos;s belief by proposing
sentative speech acts. To analyze imperative sen- axiom schemata for the speaker&apos;s and hearer&apos;s be-
tences and directive speech acts, it is clear in liefs at each level, of which Axioms 12 and 13 are
what direction one should proceed, although the the Level 1 instances. We introduce a schematic
required augmentation to the theory is quite com- operator [(s, h)n] which can be thought of as n lev-
plex. The change in the utterance theory that is els of alternation of s&apos;s and h&apos;s beliefs about each
brought about by an imperative sentence is the other. This is stated more precisely as
addition of the belief that the speaker intends the [(s, h)]0 f •. .[s][h] . . .[s] (15)
hearer to bring about the propositional content of n times
the utterance. That would entail substituting the Then, for example, Axiom 12 can be restated as
following effect for that stated by Axiom 8: the general schema
[u] {sf}P, P the propositional con- (14) in .A.4.1:
tent of utterance ats]ok (16)
One then needs to axiomatize a theory of intention —1L„ [(hf f )n]—o[Sf]cb)
revision as well as belief revision, which entails de- [(11 sI)Ta][si]O.
scribing how agents adopt and abandon intentions, 7 Conclusion
and how these intentions are related to their be- A theory of speech acts based on default reasoning
liefs about one another. Cohen and Levesque have is elegant and desirable. Unfortunately, the only
advanced an excellent proposal for such a theory existing proposal that explains how this should be
[4], but any discussion of it is far beyond the scope done suffers from three serious problems: (1) the
of this article. theory makes some incorrect predictions; (2) the
6 Reflecting on the Theory theory cannot be integrated easily with a theory
When agents perform speech acts, not only are of action; (3) there seems to be no efficient imple-
their beliefs about the uttered proposition af- mentation strategy. The problems are stem from
fected, but also their beliefs about one another, the theory&apos;s formulation in normal default logic.
to arbitrayr levels of reflection. We have demonstrated how these difficulties can
If a speaker reflects on what a hearer believes be overcome by formulating the theory instead in
about the speaker&apos;s own beliefs, he takes into ac- a version of autoepistemic logic that is designed to
count not only the beliefs themselves, but also combine reasoning about belief with autoepisternic
what he believes to be the hearer&apos;s belief revi- reasoning. Such a logic makes it possible to for-
sion strategy, which, according to our theory, is malize a description of the agents&apos; belief revision
reflected in the hierarchical relationship among the processes that can capture observed facts about
theories. Therefore, reflection on the speech-act- attitude revision correctly in response to speech
understanding process takes place at higher levels acts. This theory has been tested and imple-
of the hierarchy illustrated in Figure 1. For exam- mented as a central component of the GENESYS
ple, if Level 1 represents the speaker&apos;s reasoning utterance-planning system.
about what the hearer believes, then Level 2 rep-
</bodyText>
<sectionHeader confidence="0.992142" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999842461538462">
This research was supported in part by a contract
with the Nippon Telegraph and Telephone Cor-
poration, in part by the Office of Naval Research
under Contract N00014-85—C-0251, and in part
under subcontract with Stanford University un-
der Contract N00039-84—C-0211 with the Defense
Advanced Research Projects Agency. The original
draft of this paper has been substantially improved
by comments from Phil Cohen, Shozo Naito, and
Ray Perrault. The authors are also grateful to the
participants in the Artificial Intelligence Principia
seminar at Stanford for providing their stimulat-
ing discussion of these and related issues.
</bodyText>
<sectionHeader confidence="0.999439" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999944447368421">
[1] Douglas E. Appelt. Planning English Sen-
tences. Cambridge University Press, Cam-
bridge, England, 1985.
[2] Philip R. Cohen. On Knowning What to Say:
Planning Speech Acts. PhD thesis, University
of Toronto, 1978.
[3] Philip R. Cohen and H. Levesque. Speech
acts and rationality. In Proceedings of the
23rd Annual Meeting, pages 49-59, Associ-
ation for Computational Linguistics, 1985.
[4] Philip R. Cohen and H. Levesque. Rational
Interaction as the Basis for Communication.
Technical Report, Center for the Study of
Language and Information, 1987.
[6] Philip R. Cohen and C. Raymond Perrault.
Elements of a plan-based theory of speech
acts. Cognitive Science, 3:117-212, 1979.
[6] D. W. Etherington and R. Reiter. On inheri-
tance hierarchies with exceptions. In Proceed-
ings of AAAI, 1983.
[7] Kurt Konolige. A Hierarchic Autoepistemic
Logic. Forthcoming technical note, 1988.
[8] Vladmir Lifschitz. Computing circumscrip-
tion. In Proceedings of AAAI, pages 121-127,
1985.
[9] Drew McDermott. Nonmonotonic logic II:
nonmonotonic modal theories. Journal of
the Association for Computing Machinery,
29(1):33-57, 1982.
[10] Robert C. Moore. Semantical considerations
on nonmonotonic logic. Artificial Intelli-
gence, 25(1), 1985.
[11] Raymond Reiter. A logic for default reason-
ing. Artificial Intelligence, 13, 1980.
[12] Yoav Shoham. Reasoning about Change:
Time and Causation from the Standpoint of
Artificial Intelligence. MIT Press, Cam-
bridge, Massachusetss, 1987.
</reference>
<page confidence="0.997187">
178
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.976626">
<title confidence="0.999521">A Practical Nonmonotonic Theory for Reasoning about Speech Acts</title>
<author confidence="0.999842">Douglas Appelt</author>
<author confidence="0.999842">Kurt Konolige</author>
<affiliation confidence="0.999214">Artificial Intelligence Center and Center for the Study of Language and Information SRI International</affiliation>
<address confidence="0.998112">Menlo Park, California</address>
<abstract confidence="0.999322370370371">A prerequisite to a theory of the way agents understand speech acts is a theory of how their beliefs and intentions are revised as a consequence of events. This process of attitude revision is an interesting domain for the application of nonmonotonic reasoning because speech acts have a conventional aspect that is readily represented by defaults, but that interacts with an agent&apos;s beliefs and intentions in many complex ways that may override the defaults. Perrault has developed a theory of speech acts, based on Rieter&apos;s default logic, that captures the conventional aspect; it does not, however, adequately account for certain easily observed facts about attitude revision resulting from speech acts. A natural theory of attitude revision seems to require a method of stating preferences among competing defaults. We present here a speech act theory, formalized in hierarchic autoepistemic logic (a refinement of Moore&apos;s autoepistemic logic), in which revision of both the speaker&apos;s and hearer&apos;s attitudes can be adequately described. As a collateral benefit, efficient automatic reasoning methods for the formalism exist. The theory has been implemented and is now being employed by an utterance-planning system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Douglas E Appelt</author>
</authors>
<title>Planning English Sentences.</title>
<date>1985</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England,</location>
<marker>[1]</marker>
<rawString>Douglas E. Appelt. Planning English Sentences. Cambridge University Press, Cambridge, England, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip R Cohen</author>
</authors>
<title>On Knowning What to Say: Planning Speech Acts.</title>
<date>1978</date>
<tech>PhD thesis,</tech>
<institution>University of Toronto,</institution>
<marker>[2]</marker>
<rawString>Philip R. Cohen. On Knowning What to Say: Planning Speech Acts. PhD thesis, University of Toronto, 1978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip R Cohen</author>
<author>H Levesque</author>
</authors>
<title>Speech acts and rationality.</title>
<date>1985</date>
<booktitle>In Proceedings of the 23rd Annual Meeting,</booktitle>
<pages>49--59</pages>
<institution>Association for Computational Linguistics,</institution>
<contexts>
<context position="3400" citStr="[3,5]" startWordPosition="568" endWordPosition="568">he declarative sentence so that given the appropriate contextual elements, any one of these possibilities can follow from the description? One possible approach to this problem would be to find some fundamental, context-independent effect of informing that is true every time a declarative sentence is uttered. If one&apos;s general theory of the world and of rational behavior were sufficiently strong and detailed, any of the consequences of 170 attitude revision would be derivable from the basic effect in combination with the elaborate theory of rationality. The initial efforts made along this path [3,5] entailed the axiomatization the effects of speech acts as producing in the hearer the belief that the speaker wants him to recognize the latter&apos;s intention to hold some other belief. The effects were characterized by nestings of Goal and Bel operators, as in Bel(H, Goal(S, Bel(H, P))). If the right conditions for attitude revision obtained, the conclusion Bel(H, P) would follow from the above assumption. This general approach proved inadequate because there is in fact no such statement about beliefs about goals about beliefs that is true in every performance of a speech act. It is possible to</context>
</contexts>
<marker>[3]</marker>
<rawString>Philip R. Cohen and H. Levesque. Speech acts and rationality. In Proceedings of the 23rd Annual Meeting, pages 49-59, Association for Computational Linguistics, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip R Cohen</author>
<author>H Levesque</author>
</authors>
<title>Rational Interaction as the Basis for Communication. Technical Report, Center for the Study of Language and Information,</title>
<date>1987</date>
<contexts>
<context position="4365" citStr="[4]" startWordPosition="728" endWordPosition="728">(H, P) would follow from the above assumption. This general approach proved inadequate because there is in fact no such statement about beliefs about goals about beliefs that is true in every performance of a speech act. It is possible to construct a counterexample contradicting any such effect that might be postulated. In addition, long and complicated chains of reasoning are required to derive the simplest, most basic consequences of an utterance in situations in which all of the &amp;quot;normal&amp;quot; conditions obtain — a consequence that runs counter to one&apos;s intuitive expectations. Cohen and Levesque [4] developed a speech act theory in a monotonic modal logic that incorporates context-dependent preconditions in the axioms that state the effects of a speech act. Their approach overcomes the theoretical difficulties of earlier context-independent attempts; however, if one desires to apply their theory in a practical computational system for reasoning about speech acts, one is faced with serious difficulties. Some of the context-dependent conditions that determine the effects of a speech act, according to their theory, involve statements about what an agent does not believe, as well as what he </context>
<context position="28583" citStr="[4]" startWordPosition="4888" endWordPosition="4888">chema [u] {sf}P, P the propositional con- (14) in .A.4.1: tent of utterance ats]ok (16) One then needs to axiomatize a theory of intention —1L„ [(hf f )n]—o[Sf]cb) revision as well as belief revision, which entails de- [(11 sI)Ta][si]O. scribing how agents adopt and abandon intentions, 7 Conclusion and how these intentions are related to their be- A theory of speech acts based on default reasoning liefs about one another. Cohen and Levesque have is elegant and desirable. Unfortunately, the only advanced an excellent proposal for such a theory existing proposal that explains how this should be [4], but any discussion of it is far beyond the scope done suffers from three serious problems: (1) the of this article. theory makes some incorrect predictions; (2) the 6 Reflecting on the Theory theory cannot be integrated easily with a theory When agents perform speech acts, not only are of action; (3) there seems to be no efficient impletheir beliefs about the uttered proposition af- mentation strategy. The problems are stem from fected, but also their beliefs about one another, the theory&apos;s formulation in normal default logic. to arbitrayr levels of reflection. We have demonstrated how these</context>
</contexts>
<marker>[4]</marker>
<rawString>Philip R. Cohen and H. Levesque. Rational Interaction as the Basis for Communication. Technical Report, Center for the Study of Language and Information, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip R Cohen</author>
<author>C Raymond Perrault</author>
</authors>
<title>Elements of a plan-based theory of speech acts.</title>
<date>1979</date>
<journal>Cognitive Science,</journal>
<pages>3--117</pages>
<contexts>
<context position="13090" citStr="[6]" startWordPosition="2201" endWordPosition="2201">he following conditions: 1. T contains all the sentences of the base theory A 2. T is closed under first-order consequence 3. If 0 T , then L0 T 4. If 0 T, then -ILO E T Hierarchic autoepistemic logic (HAEL) was developed in response to two deficiences of autoepistemic logic, when the latter is viewed as a logic for automated nonmonotonic reasoning. The first is a representational problem: how to incorporate preferences among default inferences in a natural way within the logic. Such preferences arise in many disparate settings in nonmonotonic reasoning — for example, in taxonomic hierarchies [6] or in reasoning about events over time [12]. To some extent, preferences among defaults can be encoded in AE logic by introducing auxiliary information into the statements of the defaults, but this method does not always accord satisfactorily with our intuitions. The most natural statement of preferences is with respect to the multiple expansions of a particular base set, that is, we prefer certain expansions because the defaults used in them have a higher priority than the ones used in alternative expansions. The second problem is computational: how to tell whether a proposition is contained</context>
</contexts>
<marker>[6]</marker>
<rawString>Philip R. Cohen and C. Raymond Perrault. Elements of a plan-based theory of speech acts. Cognitive Science, 3:117-212, 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D W Etherington</author>
<author>R Reiter</author>
</authors>
<title>On inheritance hierarchies with exceptions.</title>
<date>1983</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<contexts>
<context position="13090" citStr="[6]" startWordPosition="2201" endWordPosition="2201">he following conditions: 1. T contains all the sentences of the base theory A 2. T is closed under first-order consequence 3. If 0 T , then L0 T 4. If 0 T, then -ILO E T Hierarchic autoepistemic logic (HAEL) was developed in response to two deficiences of autoepistemic logic, when the latter is viewed as a logic for automated nonmonotonic reasoning. The first is a representational problem: how to incorporate preferences among default inferences in a natural way within the logic. Such preferences arise in many disparate settings in nonmonotonic reasoning — for example, in taxonomic hierarchies [6] or in reasoning about events over time [12]. To some extent, preferences among defaults can be encoded in AE logic by introducing auxiliary information into the statements of the defaults, but this method does not always accord satisfactorily with our intuitions. The most natural statement of preferences is with respect to the multiple expansions of a particular base set, that is, we prefer certain expansions because the defaults used in them have a higher priority than the ones used in alternative expansions. The second problem is computational: how to tell whether a proposition is contained</context>
</contexts>
<marker>[6]</marker>
<rawString>D. W. Etherington and R. Reiter. On inheritance hierarchies with exceptions. In Proceedings of AAAI, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Konolige</author>
</authors>
<title>A Hierarchic Autoepistemic Logic. Forthcoming technical note,</title>
<date>1988</date>
<contexts>
<context position="17126" citStr="[7]" startWordPosition="2885" endWordPosition="2885">h Ti contains Ai 2. Each Ti is closed under first-order consequence 3. If E Ti, and ri, then LiO E 4. If 0 Ti, and ri -4 ri, then -4,10 E 5. If E Tj , and ri , then t;6 E These conditions are similar to those for AE stable expansions. Note that, in (3) and (4), Ti contains modal atoms describing the contents of subtheories beneath it in the hierarchy. In addition, according to (5) it also inherits all the ordinary sentences of preceeding subtheories. Unlike AE base sets, which may have more than one stable expansion, HAEL structures have a unique minimal complex stable expansion (see Konolige [7]). So we are justified in speaking of &amp;quot;the&amp;quot; theory of an HAEL structure and, from this point on, we shall identify the subtheory ri of a structure with the set of sentences in the complex stable expansion for that subtheory. Here is a simple example, which can be interpreted as the standard &amp;quot;typically birds fly&amp;quot; default scenario by letting F(x) be &amp;quot;x flies,&amp;quot; B(x) be &amp;quot;x is a bird,&amp;quot; and P(x) be &amp;quot;x is a penguin.&amp;quot; A0 = {P(a),B(a)} (7) = {LIP(a) A --.L0F(a) j ,F(a)) A2 = {L2B(a) A -1,1,F(a) j F(a)} Theory To contains all of the first-order consequences of P(a), B(a), LoP(a), and LoB(a). --.L0F(a) i</context>
<context position="18611" citStr="[7]" startWordPosition="3149" endWordPosition="3149">from Note from this example that information present in the lowest subtheories of the hierarchy percolates to its top. More specific evidence, or preferred defaults, should be placed lower in the hierarchy, so that their effects will block the action of higher-placed evidence or defaults. HAEL can be given a constructive semantics that is in accord with the closure conditions. When the inference procedure of each subtheory is decidable, an obvious decidable proof method for the logic exists. The details of this development are too complicated to be included here, but are described by Konolige [7]. For the rest of this paper, we shall use a propositional base language; the derivations can be readily checked. 5 A HAEL Theory of Speech Acts We demonstrate here how to construct a hierarchic autoepistemic theory of speech acts. We assume that there is a hierarchy of autoepistemic subtheories as illustrated in Figure 1. The lowest subtheory, 7.0, contains the strongest evidence about the speaker&apos;s and hearer&apos;s mental states. For example, if it is known to the hearer that the speaker is lying, this information goes into To. In subtheory Ti, defaults are collected about the effects of the spe</context>
</contexts>
<marker>[7]</marker>
<rawString>Kurt Konolige. A Hierarchic Autoepistemic Logic. Forthcoming technical note, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladmir Lifschitz</author>
</authors>
<title>Computing circumscription.</title>
<date>1985</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>121--127</pages>
<contexts>
<context position="14159" citStr="[8]" startWordPosition="2378" endWordPosition="2378">rity than the ones used in alternative expansions. The second problem is computational: how to tell whether a proposition is contained within the desired expansion of a base set. As can be seen from the above definition, a stable expansion of an autoepistemic theory is defined as a fixedpoint; the question of whether a formula belongs to this fixedpoint is not even semidecidable. This problem is shared by all of the most popular nonmonotonic logics. The usual recourse is to restrict the expressive power of the language, e.g., normal default theories [11] and separable circumscriptive theories [8]. However, as exemplified by the difficulties of Perrault&apos;s approach, it may not be easy or even possible to express the relevant facts with a restricted language. Hierarchical autoepistemic logic is a modification of autoepistemic logic that addresses these two considerations. In HAEL, the primary structure is not a single uniform theory, but a collection of subtheories linked in a hierarchy. Subtheories represent different sources of information available to an agent, while the hierarchy expresses the way in which this information is combined. For example, in representing taxonomic defaults,</context>
</contexts>
<marker>[8]</marker>
<rawString>Vladmir Lifschitz. Computing circumscription. In Proceedings of AAAI, pages 121-127, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Drew McDermott</author>
</authors>
<title>Nonmonotonic logic II: nonmonotonic modal theories.</title>
<date>1982</date>
<journal>Journal of the Association for Computing Machinery,</journal>
<pages>29--1</pages>
<contexts>
<context position="12212" citStr="[9]" startWordPosition="2049" endWordPosition="2049">at its application is restricted to the speaker and hearer, with no other agents in the domain involved. If it is used in a backward direction, it applies to its own result. Invoking the rule in a forward direction is also problematic, because in general one agent will have a very large number of beliefs (even an infinite number, if introspection is taken into account) about another agent&apos;s beliefs, most of which will be irrelevant to the problem at hand. 4 Hierarchic Autoepistemic Logic Autoepistemic (AE) logic was developed by Moore [10] as a reconstruction of McDermott&apos;s nonmonotonic logic [9]. An autoepistemic logic is based on a first-order language augmented by a modal operator L, which is interpreted intuitively as self belief. A stable expansion (analogous to an extension of a default theory) of an autoepistemic base set A is a set of formulas T satisfying the following conditions: 1. T contains all the sentences of the base theory A 2. T is closed under first-order consequence 3. If 0 T , then L0 T 4. If 0 T, then -ILO E T Hierarchic autoepistemic logic (HAEL) was developed in response to two deficiences of autoepistemic logic, when the latter is viewed as a logic for automat</context>
</contexts>
<marker>[9]</marker>
<rawString>Drew McDermott. Nonmonotonic logic II: nonmonotonic modal theories. Journal of the Association for Computing Machinery, 29(1):33-57, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>Semantical considerations on nonmonotonic logic.</title>
<date>1985</date>
<journal>Artificial Intelligence,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="12154" citStr="[10]" startWordPosition="2040" endWordPosition="2040">efficiently by a reasoning system; even if it is assumed that its application is restricted to the speaker and hearer, with no other agents in the domain involved. If it is used in a backward direction, it applies to its own result. Invoking the rule in a forward direction is also problematic, because in general one agent will have a very large number of beliefs (even an infinite number, if introspection is taken into account) about another agent&apos;s beliefs, most of which will be irrelevant to the problem at hand. 4 Hierarchic Autoepistemic Logic Autoepistemic (AE) logic was developed by Moore [10] as a reconstruction of McDermott&apos;s nonmonotonic logic [9]. An autoepistemic logic is based on a first-order language augmented by a modal operator L, which is interpreted intuitively as self belief. A stable expansion (analogous to an extension of a default theory) of an autoepistemic base set A is a set of formulas T satisfying the following conditions: 1. T contains all the sentences of the base theory A 2. T is closed under first-order consequence 3. If 0 T , then L0 T 4. If 0 T, then -ILO E T Hierarchic autoepistemic logic (HAEL) was developed in response to two deficiences of autoepistem</context>
</contexts>
<marker>[10]</marker>
<rawString>Robert C. Moore. Semantical considerations on nonmonotonic logic. Artificial Intelligence, 25(1), 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond Reiter</author>
</authors>
<title>A logic for default reasoning.</title>
<date>1980</date>
<journal>Artificial Intelligence,</journal>
<volume>13</volume>
<contexts>
<context position="6571" citStr="[11]" startWordPosition="1078" endWordPosition="1078">ffects of a speech act to contextdependent preconditions; (2) these preconditions must depend on the ignorance as well as on the knowledge of the relevant agents; (3) any practical system for reasoning about ignorance must be based on nonmonotonic reasoning; (4) existing speech act theories based on nonmonotonic reasoning cannot account for the facts of attitude revision resulting from the performance of speech acts. 2 Perrault&apos;s Default Theory of Speech Acts As an alternative to monotonic theories, Perrault has proposed a theory of speech acts, based on an extension of Reiter&apos;s default logic [11] extended to include- default-rule schemata. We shall summarize Perrault&apos;s theory briefly as it relates to informing and belief. The notation p q is intended as an abbreviation of the default rule of inference, p: Mq • Default theories of this form are called normal. Every normal default theory has at least one extension, i.e., a mutually consistent set of sentences sanctioned by the theory. The operator Bz,t represents Agent x&apos;s beliefs at time t and is assumed to posess all the properties of the modal system weak 55 (that is, S5 without the schema Bz,ty6 D 0), plus the following axioms: Pers</context>
<context position="14116" citStr="[11]" startWordPosition="2373" endWordPosition="2373">the defaults used in them have a higher priority than the ones used in alternative expansions. The second problem is computational: how to tell whether a proposition is contained within the desired expansion of a base set. As can be seen from the above definition, a stable expansion of an autoepistemic theory is defined as a fixedpoint; the question of whether a formula belongs to this fixedpoint is not even semidecidable. This problem is shared by all of the most popular nonmonotonic logics. The usual recourse is to restrict the expressive power of the language, e.g., normal default theories [11] and separable circumscriptive theories [8]. However, as exemplified by the difficulties of Perrault&apos;s approach, it may not be easy or even possible to express the relevant facts with a restricted language. Hierarchical autoepistemic logic is a modification of autoepistemic logic that addresses these two considerations. In HAEL, the primary structure is not a single uniform theory, but a collection of subtheories linked in a hierarchy. Subtheories represent different sources of information available to an agent, while the hierarchy expresses the way in which this information is combined. For e</context>
</contexts>
<marker>[11]</marker>
<rawString>Raymond Reiter. A logic for default reasoning. Artificial Intelligence, 13, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Shoham</author>
</authors>
<title>Reasoning about Change: Time and Causation from the Standpoint of Artificial Intelligence.</title>
<date>1987</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetss,</location>
<contexts>
<context position="13134" citStr="[12]" startWordPosition="2209" endWordPosition="2209">the sentences of the base theory A 2. T is closed under first-order consequence 3. If 0 T , then L0 T 4. If 0 T, then -ILO E T Hierarchic autoepistemic logic (HAEL) was developed in response to two deficiences of autoepistemic logic, when the latter is viewed as a logic for automated nonmonotonic reasoning. The first is a representational problem: how to incorporate preferences among default inferences in a natural way within the logic. Such preferences arise in many disparate settings in nonmonotonic reasoning — for example, in taxonomic hierarchies [6] or in reasoning about events over time [12]. To some extent, preferences among defaults can be encoded in AE logic by introducing auxiliary information into the statements of the defaults, but this method does not always accord satisfactorily with our intuitions. The most natural statement of preferences is with respect to the multiple expansions of a particular base set, that is, we prefer certain expansions because the defaults used in them have a higher priority than the ones used in alternative expansions. The second problem is computational: how to tell whether a proposition is contained within the desired expansion of a base set.</context>
</contexts>
<marker>[12]</marker>
<rawString>Yoav Shoham. Reasoning about Change: Time and Causation from the Standpoint of Artificial Intelligence. MIT Press, Cambridge, Massachusetss, 1987.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>