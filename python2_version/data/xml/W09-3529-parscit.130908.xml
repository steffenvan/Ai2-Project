<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000078">
<title confidence="0.984458">
Maximum N-gram HMM-based Name Transliteration: Experiment in
NEWS 2009 on English-Chinese Corpus
</title>
<author confidence="0.992004">
Yilu Zhou
</author>
<affiliation confidence="0.983965">
George Washington University
</affiliation>
<email confidence="0.996616">
yzhou@gwu.edu
</email>
<sectionHeader confidence="0.993838" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99377555">
We propose an English-Chinese name transli-
teration system using a maximum N-gram
Hidden Markov Model. To handle special
challenges with alphabet-based and character-
based language pair, we apply a two-phase
transliteration model by building two HMM
models, one between English and Chinese Pi-
nyin and another between Chinese Pinyin and
Chinese characters. Our model improves tradi-
tional HMM by assigning the longest prior
translation sequence of syllables the largest
weight. In our non-standard runs, we use a
Web-mining module to boost the performance
by adding online popularity information of
candidate translations. The entire model does
not rely on any dictionaries and the probability
tables are derived merely from training corpus.
In participation of NEWS 2009 experiment,
our model achieved 0.462 Top-1 accuracy and
0.764 Mean F-score.
</bodyText>
<sectionHeader confidence="0.99884" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999984765957447">
It is in general difficult for human to translate
unfamiliar personal names, place names and
names of organizations (Lee et al., 2006). One
reason is the variability in name translation. In
many situations, there is more than one correct
translation for the same name. In some languag-
es, such as Arabic, it can go up to as many as
forty (Arbabi et al., 1994). Even professional
translators find it difficult to identify all varia-
tions. For example, when translating “Phelps”
into Chinese, there are at least 5 different ways to
translate this name: “费尔普斯,” “菲尔普斯,”
“弗尔普斯,” “菲尔普思,” and “菲尔普丝,” with
some more popular than others.
The variability in translation implies the
complexity in name translation that can hardly be
addressed in typical machine translation systems.
Machine translation systems are often black box-
es where only one translation is provided, which
do not offer a solution to variability issue. The
accuracy of a machine translation system,
whether statistical or example-based, largely de-
pends on sentence context information. This con-
text information is often not available with name
translation. Furthermore, emerging names are
difficult to capture in regular machine translation
systems if they have not been included in train-
ing corpus or translation dictionary. Thus, being
able to translate proper names not only has its
own application area, it will also enhance the
performance of current machine translation sys-
tems.
In our previous English-Arabic name trans-
literation work (Zhou et al., 2008), we proposed
a framework for name transliteration using a 2-
gram and a 3-gram Hidden Markov Model
(HMM). In this research, we extend our 2-gram
and 3-gram HMM to an N-gram HMM where N
is the maximum number of prior translation
mapping sequence that can be identified in the
training corpus. In our non-standard runs, we
also integrated a Web mining module. The rest
of the paper is structured as follows. Section 2
reviews related work; Section 3 describes our
algorithm; Section 4 discusses implementation
and evaluation results are provided in Section 5.
Section 6 concludes our work.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999093">
Research in translating proper names has focused
on two strategies: One is to mine translation pairs
from bilingual online resources or corpora (Lee
et. al, 2006). The second approach is a direct
translation approach (Chen and Zong, 2008).
The first approach is based on the assumption
that the two name equivalents should share simi-
lar relevant context words in their languages.
Correct transliteration is then extracted from the
closest matching proper nouns. The second ap-
proach, direct translation, is often done by trans-
literation. Transliteration is the representation of
a word or phrase in the closest corresponding
letters or characters of a language with different
alphabet so that the pronunciation is as close as
possible to the original word or phrase (AbdulJa-
leel and Larkey, 2003). Unlike mining-based ap-
proach, transliteration can deal with low-
frequency proper names, but may generate ill-
formed translations.
</bodyText>
<page confidence="0.954196">
128
</page>
<note confidence="0.983721">
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 128–131,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999921904761905">
Transliteration models can be categorized into
rule-based approach and statistical approach. A
rule-based approach maps each letter or a set of
letters in the source language to the closest
sounding letter or letters in the target language
according to pre-defined rules or mapping tables.
It relies on manual identification of all translite-
ration rules and heuristics, which can be very
complex and time consuming to build (Darwish
et al., 2001). A statistical approach obtains trans-
lation probabilities from a training corpus: pairs
of transliterated words. When new words come,
the statistical approach picks the transliteration
candidate with the highest transliteration proba-
bilities generated as the correct transliteration.
Most statistical-based research used phoneme-
based transliteration, relying on a pronunciation
dictionary. Al-Onaizan and Knight showed that a
grapheme-based approach out-performed a pho-
neme-based approach in Arabic-English transli-
teration (Al-Onaizan and Knight, 2002).
</bodyText>
<sectionHeader confidence="0.95347" genericHeader="method">
3 Challenges with Chinese Language
</sectionHeader>
<bodyText confidence="0.999930888888889">
There are several challenges in transliterating
English names into Chinese. First, written Chi-
nese is a logogram language. Thus, a phonetic
representation of Chinese characters, Pinyin, is
used as an intermediate Romanization. Our
process of translating an English name into Chi-
nese consists of two steps: translating English
word into Pinyin and then mapping Pinyin into
Chinese characters.
Second, Chinese is not only monosyllabic, but
the pronunciation of each Chinese character is
always composed of one (or none) Consonant
unit and one Vowel unit with the Consonant al-
ways appears at the beginning. For example,
/EKS/ is one syllable in English but is three syl-
lables in Chinese (/E/ + /KE/ + /SI/). English syl-
lables need to be processed in a way that can be
mapped to Chinese Pinyin.
</bodyText>
<sectionHeader confidence="0.981352" genericHeader="method">
4 Proposed Maximum N-gram HMM
</sectionHeader>
<bodyText confidence="0.9965698">
Figure 1 illustrates our name translation
framework. The framework consists of three ma-
jor components: 1) Training, 2) Hidden Markov
Model-based Transliteration, and 3) Web Min-
ing-enhanced ranking.
</bodyText>
<subsectionHeader confidence="0.99086">
4.1 Training
</subsectionHeader>
<bodyText confidence="0.999927555555556">
The training process (Figure 1 Module 1) gene-
rates two transliteration probability tables based
on a training corpus of English-Pinyin pair and
Pinyin-Chinese name pairs. Pinyin is not pro-
vided in the training corpus, but is easy to obtain
from a Chinese Pinyin table.
In order to perform mapping from English
names to Chinese Pinyin, an English name is
divided into sub-syllables and this process is
called Syllabification. Although many English
syllabification algorithms have been proposed,
they need to be adjusted. During syllabification,
light vowels are inserted between two continuous
consonants and silent letters are deleted. We use
a finite state machine to implement the syllabifi-
cation process. For example, “Phelps” becomes
{/ph/ /e/ /l/ /@/ /p/ /@/ /s/ /@/} with “@” being
inserted light vowels.
Alignment process maps each sub-syllable in
an English name to target Pinyin. The accuracy
of Alignment process largely depends on the
accuracy of Syllabification. Pinyin to Chinese
character alignment is more straightforward
where each Pinyin syllable (consonant + vowel)
is mapped to the corresponding Chinese charac-
ter. Once the alignment is done, occurrence of
each translation pair can be calculated. Using this
occurrence information, we can derive probabili-
ties under various situations to support probabili-
ty models.
We use the Hidden Markov Model which is
one of the most popular probability models and
has been used in speech recognition, the human
genome project, consumer decision modeling,
etc. (Rabiner, 1989). In transliteration, traditional
HMM can be viewed as a 2-gram model where
the current mapping selection depends on the
previous mapping pair. We expand it to an N-
gram model and use the combination of 1-gram,
2-gram, ... , (N-1)-gram and N-gram HMM
where N is the maximum number of mapping
sequence that can be found in training corpus.
The goal of our model is to find the candi-
date transliteration with the highest translitera-
tion probabilities:
</bodyText>
<equation confidence="0.959088">
argmax (  |) argmax ( ..  |... )
P t s = P t1t2 tn s1s2 s n
</equation>
<bodyText confidence="0.9986264">
Where s is the source name to be transliterated,
which contains letter string s1s2... si; t is the tar-
get name, which contains letter string t1t2... ti.
In a simple statistical model, or a 1-gram
model, transliteration probability is estimated as:
</bodyText>
<equation confidence="0.79994825">
P(t1,t2,t3 , , tn  |s1 , s2 , s3 , , sn )
Where
s translates to t in corpus
i i
of times s appears in corpus
i
=
P
(
t1  |s1
)P(t2 )
S )...... P(tn Sn
2
P(ti  |si) #
# of
times
</equation>
<page confidence="0.887286">
129
130
</page>
<equation confidence="0.608017833333333">
)
s3, , sn
,
t3 , , tn  |s1 , s 2
t p t s t t
1 ) (  |, ,
n n n− −
1 n 2
,
to ti
# of times
s translates
i
)
,,......,  |,
t t s s
3 n 1 2
,s3, ,sn
=P(t1 |s1)p(t2 |s2, t1
)P(t3  |s3 , t2
s − &gt; t
i−2 i−2
s − &gt; t and
i−1 i−1
</equation>
<bodyText confidence="0.863799857142857">
The bigram HMM improves the simple sta-
tistical model in that it incorporates context in-
formation into a probability calculation. The
transliteration of the current letter is dependent
on the transliteration of ONE previous letter (one
previous state in HMM). Transliteration proba-
bility is estimated as:
</bodyText>
<figure confidence="0.880089066666667">
(3)
s t t s t
, )(  |, ) (  |,
p t s t
2 1 3 3 2 n n n
Where
and
(  |, ) =
P t s t
i i i − 1# of times s translates to t
i−1
# of
times
s translates to t given
i i
s − &gt; t
i − 1 i
i
−
1
−
1
P(ti |si) =
#
# of
times s translates to t
i i
of times
s occurs
i
</figure>
<bodyText confidence="0.7861918">
The trigram HMM intends to capture even
more context information by translating the cur-
rent letter dependent on the TWO previous let-
ters. Transliteration probability is estimated as:
(4)
</bodyText>
<equation confidence="0.680074117647059">
P t t
( ,
1
Where
P(ti  |si) =
#
of times
s occurs
i
P(ti  |si,ti−1) =
#of timess translates tot
i−1 i−1
P(ti  |si,ti−1,ti
s translates to t given
3 i
s translates to t and
i − 1 i − 1
</equation>
<bodyText confidence="0.99701925">
This process is continued until the maximum
mapping sequence is found in the transliteration
corpus. The final probability estimation is a
weighted combination of all N-grams:
</bodyText>
<equation confidence="0.6092895">
FinalTransliterationScore=
α1 (1− gramHMM) +α2 (2 − gramHMM) + + αn (N − gramHMM)
</equation>
<bodyText confidence="0.999859727272727">
In our submitted results, we applied α1=1, α2=2,
...., αn=N such that longer matched sequence has
a larger contribution in the final probability. The
rationale is that the longer the prior sequence
identified in training data, the higher probability
that the translation sequence is the correct tone.
These α parameters can be tuned in the future.
We call this approach Maximum N-gram
HMM. The same process is conducted for Pinyin
to Chinese character translation as shown in the
lower part of Figure 1 Module 1.
</bodyText>
<subsectionHeader confidence="0.949141">
4.2 Translation and Ranking
</subsectionHeader>
<bodyText confidence="0.9999473">
Once the two Maximum N-gram HMM Model
are obtained, new incoming names are translated
by obtaining a letter sequence that maximizes the
overall probability through the HMM (Figure 1
Module 2). This step uses a modified Viterbi’s
search algorithm (Viterbi 1967). The original
Viterbi’s algorithm only keeps the most optimal
path. To cope with name translation variations,
we keep the top-20 optimal paths for further
analysis.
</bodyText>
<subsectionHeader confidence="0.977313">
4.3 Web Mining Component
</subsectionHeader>
<bodyText confidence="0.980547">
To boost the transliteration performance we pro-
pose to use the Web mining approach, which
analyzes candidates’ occurrence on the Web
</bodyText>
<figure confidence="0.652515615384615">
)
and
s translates to t
i−2 i−2
,
P t t
( ,
1 2
) ( |
P t 2
= P(t1  |s1
1
−
)
2
=
)
# of
times
# of
times
timess translates tot givens t
− − &gt;
i i i 1 i
1
#of
</figure>
<bodyText confidence="0.9977728">
(Figure 1 Module 3). Each one of the top-20
transliterations obtained from the previous step is
sent to a Web search engine using a meta-search
program which records the number of documents
retrieved, referred to as Web frequency. By ex-
amining the popularity of all possible translitera-
tions on the Internet, bad transliterations can be
filtered and their online popularity can serve as
an indicator of transliteration correctness. The
popularity is estimated by acquiring the number
of documents returned from a search engine us-
ing the translation candidate as query. The final
rank of transliterations is derived from a
weighted score of the normalized Web frequency
and the probability score.
</bodyText>
<sectionHeader confidence="0.999364" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.967387666666667">
Named Entity Workshop (NEWS) 2009 Machine
Transliteration Shared Task provided a training
corpus with 31,961 pairs of English and Chinese
name translations and 2,896 testing cases. We
submitted one standard run with Maximum N-
gram HMM (N-HMM) setting, and two non-
standard runs with 3-gram HMM (3-HMM), and
Maximum N-gram HMM + Web mining (N-
HMM+W). There are two other runs that we
submitted which contains error in the results and
they are not discussed here. We present our eval-
uation results in Table 1.
</bodyText>
<table confidence="0.9994925">
Top-1 F- MRR MAP MAP
Acc score (Ref) (10)
N-HMM 0.456 0.763 0.587 0.456 0.185
N- 0.462 0.764 0.564 0.462 0.175
HMM+W
3-HMM 0.458 0.763 0.602 0.458 0.191
</table>
<tableCaption confidence="0.999948">
Table 1: Evaluation Results with Top-10 Candidates
</tableCaption>
<bodyText confidence="0.999922666666667">
It is confirmed that Web-mining module
boosted the performance of N-gram HMM in all
measure except for MAP(10). However, the boost-
ing effect is small (1.3%). To our surprise, 3-
gram HMM outperformed Maximum N-gram
HMM slightly (3% in MAP(10)). Our best Top-1
accuracy is 0.462, and best Mean F-score is
0.764 both achieved by N-gram HMM with Web
mining module. We believe this slightly lower
performance of Maximum N-gram HMM can be
improved with some tuning of weight parame-
ters.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999993882352941">
We propose an English-Chinese name translite-
ration system using a maximum N-gram Hidden
Markov Model. To handle special challenges
with alphabet-based and character-based lan-
guage pair, we apply a two-phase transliteration
model by building two HMM models, one be-
tween English and Chinese Pinyin and another
between Chinese Pinyin and Chinese characters.
In participation of NEWS 2009 experiment, our
model achieved 0.462 Top-1 accuracy and 0.764
Mean F-score. We plan to conduct further study
the impact of Web mining component and find
optimal set of parameters. Our model does not
rely on any existing dictionary and the transla-
tion results are entirely based on learning the
corpus data. In the future, this framework can be
extended to other language pairs.
</bodyText>
<sectionHeader confidence="0.966265" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.893968">
We thank the data source provider of this shared
task from
</bodyText>
<reference confidence="0.819477333333333">
English-Chinese (EnCh): Haizhou Li, Min Zhang,
Jian Su, &amp;quot;A joint source channel model for machine
transliteration&amp;quot;, Proc. of the 42nd ACL, 2004
</reference>
<sectionHeader confidence="0.92713" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99988227027027">
AbdulJaleel, N., and Larkey, L. S., Statistical transli-
teration for English-Arabic Cross Language In-
formation Retrieval, in Proceedings of (CIKM)
New Orleans, LA, pp. 139 (2003).
Al-Onaizan, Y., and Knight, K., Machine Translitera-
tion of Names in Arabic Text, in Proceedings
of the ACL-02 Workshop on Computational
Approaches to Semitic Languages Philadel-
phia, Pennsylvania pp. 1 (2002).
Arbabi, M., Fischthal, S. M., Cheng, V. C., and Bart,
E., Algorithms for Arabic Name Translitera-
tion, IBM Journal of Research and Develop-
ment, 38, 183 (1994).
Chen, Y., and Zong, C., A Structure-based Model for
Chinese Organization Name Translation, ACM
Transactions on ACL, 7, 1 (2008).
Darwish, K., Doermann, D., Jones, R., Oard, D., and
Rautiainen, M., TREC-10 Experiments at Uni-
versity of Maryland CLIR and Video in TREC,
Gaithersburg, Maryland (2001).
Lee, C.J., Chang, J. S., Jang, J.S.R, Extraction of
transliteration pairs from parallel corpora using
a statistical transliteration model, Information
Sciences, 176(1), 67-90 (2006).
Rabiner, L. R., A Tutorial on Hidden Markov Models
and Selected Applications in Speech Recogni-
tion, Proceedings of the IEEE, 77, 257–286
(1989).
Viterbi, A. J., Error Bounds for Convolutional Codes
and an Asymptotically Optimum Decoding Al-
gorithm, IEEE Transactions on Information
Theory, 13, 260 (1967).
Zhou, Y., Huang, F., and Chen, H., Combining prob-
ability Models and Web Mining Models: A
Framework for Proper Name transliteration, In-
formation Technology and Management, 9, 91
(2008).
</reference>
<page confidence="0.998306">
131
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.496355">
<title confidence="0.9934155">Maximum N-gram HMM-based Name Transliteration: Experiment NEWS 2009 on English-Chinese Corpus</title>
<author confidence="0.995332">Yilu Zhou</author>
<affiliation confidence="0.947951">George Washington University</affiliation>
<email confidence="0.999154">yzhou@gwu.edu</email>
<abstract confidence="0.975159380952381">We propose an English-Chinese name transliteration system using a maximum N-gram Hidden Markov Model. To handle special challenges with alphabet-based and characterbased language pair, we apply a two-phase transliteration model by building two HMM models, one between English and Chinese Pinyin and another between Chinese Pinyin and Chinese characters. Our model improves traditional HMM by assigning the longest prior translation sequence of syllables the largest weight. In our non-standard runs, we use a Web-mining module to boost the performance by adding online popularity information of candidate translations. The entire model does not rely on any dictionaries and the probability tables are derived merely from training corpus. In participation of NEWS 2009 experiment, our model achieved 0.462 Top-1 accuracy and 0.764 Mean F-score.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>English-Chinese Haizhou Li</author>
<author>Min Zhang</author>
<author>Jian Su</author>
</authors>
<title>A joint source channel model for machine transliteration&amp;quot;,</title>
<date>2004</date>
<booktitle>Proc. of the 42nd ACL,</booktitle>
<marker>Li, Zhang, Su, 2004</marker>
<rawString>English-Chinese (EnCh): Haizhou Li, Min Zhang, Jian Su, &amp;quot;A joint source channel model for machine transliteration&amp;quot;, Proc. of the 42nd ACL, 2004</rawString>
</citation>
<citation valid="true">
<authors>
<author>N AbdulJaleel</author>
<author>L S Larkey</author>
</authors>
<title>Statistical transliteration for English-Arabic Cross Language Information Retrieval,</title>
<date>2003</date>
<booktitle>in Proceedings of (CIKM)</booktitle>
<pages>139</pages>
<location>New Orleans, LA,</location>
<contexts>
<context position="3939" citStr="AbdulJaleel and Larkey, 2003" startWordPosition="609" endWordPosition="613">e second approach is a direct translation approach (Chen and Zong, 2008). The first approach is based on the assumption that the two name equivalents should share similar relevant context words in their languages. Correct transliteration is then extracted from the closest matching proper nouns. The second approach, direct translation, is often done by transliteration. Transliteration is the representation of a word or phrase in the closest corresponding letters or characters of a language with different alphabet so that the pronunciation is as close as possible to the original word or phrase (AbdulJaleel and Larkey, 2003). Unlike mining-based approach, transliteration can deal with lowfrequency proper names, but may generate illformed translations. 128 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 128–131, Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP Transliteration models can be categorized into rule-based approach and statistical approach. A rule-based approach maps each letter or a set of letters in the source language to the closest sounding letter or letters in the target language according to pre-defined rules or mapping tables. It relies on manual identification of al</context>
</contexts>
<marker>AbdulJaleel, Larkey, 2003</marker>
<rawString>AbdulJaleel, N., and Larkey, L. S., Statistical transliteration for English-Arabic Cross Language Information Retrieval, in Proceedings of (CIKM) New Orleans, LA, pp. 139 (2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Al-Onaizan</author>
<author>K Knight</author>
</authors>
<title>Machine Transliteration of Names in Arabic Text,</title>
<date>2002</date>
<booktitle>in Proceedings of the ACL-02 Workshop on Computational Approaches to Semitic Languages</booktitle>
<pages>1</pages>
<location>Philadelphia, Pennsylvania</location>
<contexts>
<context position="5208" citStr="Al-Onaizan and Knight, 2002" startWordPosition="789" endWordPosition="792">hich can be very complex and time consuming to build (Darwish et al., 2001). A statistical approach obtains translation probabilities from a training corpus: pairs of transliterated words. When new words come, the statistical approach picks the transliteration candidate with the highest transliteration probabilities generated as the correct transliteration. Most statistical-based research used phonemebased transliteration, relying on a pronunciation dictionary. Al-Onaizan and Knight showed that a grapheme-based approach out-performed a phoneme-based approach in Arabic-English transliteration (Al-Onaizan and Knight, 2002). 3 Challenges with Chinese Language There are several challenges in transliterating English names into Chinese. First, written Chinese is a logogram language. Thus, a phonetic representation of Chinese characters, Pinyin, is used as an intermediate Romanization. Our process of translating an English name into Chinese consists of two steps: translating English word into Pinyin and then mapping Pinyin into Chinese characters. Second, Chinese is not only monosyllabic, but the pronunciation of each Chinese character is always composed of one (or none) Consonant unit and one Vowel unit with the Co</context>
</contexts>
<marker>Al-Onaizan, Knight, 2002</marker>
<rawString>Al-Onaizan, Y., and Knight, K., Machine Transliteration of Names in Arabic Text, in Proceedings of the ACL-02 Workshop on Computational Approaches to Semitic Languages Philadelphia, Pennsylvania pp. 1 (2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Arbabi</author>
<author>S M Fischthal</author>
<author>V C Cheng</author>
<author>E Bart</author>
</authors>
<title>Algorithms for Arabic Name Transliteration,</title>
<date>1994</date>
<journal>IBM Journal of Research and Development,</journal>
<volume>38</volume>
<pages>183</pages>
<contexts>
<context position="1376" citStr="Arbabi et al., 1994" startWordPosition="207" endWordPosition="210">ndidate translations. The entire model does not rely on any dictionaries and the probability tables are derived merely from training corpus. In participation of NEWS 2009 experiment, our model achieved 0.462 Top-1 accuracy and 0.764 Mean F-score. 1 Introduction It is in general difficult for human to translate unfamiliar personal names, place names and names of organizations (Lee et al., 2006). One reason is the variability in name translation. In many situations, there is more than one correct translation for the same name. In some languages, such as Arabic, it can go up to as many as forty (Arbabi et al., 1994). Even professional translators find it difficult to identify all variations. For example, when translating “Phelps” into Chinese, there are at least 5 different ways to translate this name: “费尔普斯,” “菲尔普斯,” “弗尔普斯,” “菲尔普思,” and “菲尔普丝,” with some more popular than others. The variability in translation implies the complexity in name translation that can hardly be addressed in typical machine translation systems. Machine translation systems are often black boxes where only one translation is provided, which do not offer a solution to variability issue. The accuracy of a machine translation system</context>
</contexts>
<marker>Arbabi, Fischthal, Cheng, Bart, 1994</marker>
<rawString>Arbabi, M., Fischthal, S. M., Cheng, V. C., and Bart, E., Algorithms for Arabic Name Transliteration, IBM Journal of Research and Development, 38, 183 (1994).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chen</author>
<author>C Zong</author>
</authors>
<title>A Structure-based Model for Chinese Organization Name Translation,</title>
<date>2008</date>
<journal>ACM Transactions on ACL,</journal>
<volume>7</volume>
<contexts>
<context position="3382" citStr="Chen and Zong, 2008" startWordPosition="522" endWordPosition="525">lation mapping sequence that can be identified in the training corpus. In our non-standard runs, we also integrated a Web mining module. The rest of the paper is structured as follows. Section 2 reviews related work; Section 3 describes our algorithm; Section 4 discusses implementation and evaluation results are provided in Section 5. Section 6 concludes our work. 2 Related Work Research in translating proper names has focused on two strategies: One is to mine translation pairs from bilingual online resources or corpora (Lee et. al, 2006). The second approach is a direct translation approach (Chen and Zong, 2008). The first approach is based on the assumption that the two name equivalents should share similar relevant context words in their languages. Correct transliteration is then extracted from the closest matching proper nouns. The second approach, direct translation, is often done by transliteration. Transliteration is the representation of a word or phrase in the closest corresponding letters or characters of a language with different alphabet so that the pronunciation is as close as possible to the original word or phrase (AbdulJaleel and Larkey, 2003). Unlike mining-based approach, translitera</context>
</contexts>
<marker>Chen, Zong, 2008</marker>
<rawString>Chen, Y., and Zong, C., A Structure-based Model for Chinese Organization Name Translation, ACM Transactions on ACL, 7, 1 (2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Darwish</author>
<author>D Doermann</author>
<author>R Jones</author>
<author>D Oard</author>
<author>M Rautiainen</author>
</authors>
<date>2001</date>
<booktitle>TREC-10 Experiments at University of Maryland CLIR and Video in TREC,</booktitle>
<location>Gaithersburg, Maryland</location>
<contexts>
<context position="4655" citStr="Darwish et al., 2001" startWordPosition="719" endWordPosition="722">y generate illformed translations. 128 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 128–131, Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP Transliteration models can be categorized into rule-based approach and statistical approach. A rule-based approach maps each letter or a set of letters in the source language to the closest sounding letter or letters in the target language according to pre-defined rules or mapping tables. It relies on manual identification of all transliteration rules and heuristics, which can be very complex and time consuming to build (Darwish et al., 2001). A statistical approach obtains translation probabilities from a training corpus: pairs of transliterated words. When new words come, the statistical approach picks the transliteration candidate with the highest transliteration probabilities generated as the correct transliteration. Most statistical-based research used phonemebased transliteration, relying on a pronunciation dictionary. Al-Onaizan and Knight showed that a grapheme-based approach out-performed a phoneme-based approach in Arabic-English transliteration (Al-Onaizan and Knight, 2002). 3 Challenges with Chinese Language There are </context>
</contexts>
<marker>Darwish, Doermann, Jones, Oard, Rautiainen, 2001</marker>
<rawString>Darwish, K., Doermann, D., Jones, R., Oard, D., and Rautiainen, M., TREC-10 Experiments at University of Maryland CLIR and Video in TREC, Gaithersburg, Maryland (2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Lee</author>
<author>J S Chang</author>
<author>J S R Jang</author>
</authors>
<title>Extraction of transliteration pairs from parallel corpora using a statistical transliteration model,</title>
<date>2006</date>
<journal>Information Sciences,</journal>
<volume>176</volume>
<issue>1</issue>
<pages>67--90</pages>
<contexts>
<context position="1152" citStr="Lee et al., 2006" startWordPosition="165" endWordPosition="168">ditional HMM by assigning the longest prior translation sequence of syllables the largest weight. In our non-standard runs, we use a Web-mining module to boost the performance by adding online popularity information of candidate translations. The entire model does not rely on any dictionaries and the probability tables are derived merely from training corpus. In participation of NEWS 2009 experiment, our model achieved 0.462 Top-1 accuracy and 0.764 Mean F-score. 1 Introduction It is in general difficult for human to translate unfamiliar personal names, place names and names of organizations (Lee et al., 2006). One reason is the variability in name translation. In many situations, there is more than one correct translation for the same name. In some languages, such as Arabic, it can go up to as many as forty (Arbabi et al., 1994). Even professional translators find it difficult to identify all variations. For example, when translating “Phelps” into Chinese, there are at least 5 different ways to translate this name: “费尔普斯,” “菲尔普斯,” “弗尔普斯,” “菲尔普思,” and “菲尔普丝,” with some more popular than others. The variability in translation implies the complexity in name translation that can hardly be addressed in</context>
</contexts>
<marker>Lee, Chang, Jang, 2006</marker>
<rawString>Lee, C.J., Chang, J. S., Jang, J.S.R, Extraction of transliteration pairs from parallel corpora using a statistical transliteration model, Information Sciences, 176(1), 67-90 (2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
</authors>
<title>A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition,</title>
<date>1989</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<volume>77</volume>
<pages>257--286</pages>
<contexts>
<context position="7811" citStr="Rabiner, 1989" startWordPosition="1196" endWordPosition="1197">ss largely depends on the accuracy of Syllabification. Pinyin to Chinese character alignment is more straightforward where each Pinyin syllable (consonant + vowel) is mapped to the corresponding Chinese character. Once the alignment is done, occurrence of each translation pair can be calculated. Using this occurrence information, we can derive probabilities under various situations to support probability models. We use the Hidden Markov Model which is one of the most popular probability models and has been used in speech recognition, the human genome project, consumer decision modeling, etc. (Rabiner, 1989). In transliteration, traditional HMM can be viewed as a 2-gram model where the current mapping selection depends on the previous mapping pair. We expand it to an Ngram model and use the combination of 1-gram, 2-gram, ... , (N-1)-gram and N-gram HMM where N is the maximum number of mapping sequence that can be found in training corpus. The goal of our model is to find the candidate transliteration with the highest transliteration probabilities: argmax ( |) argmax ( .. |... ) P t s = P t1t2 tn s1s2 s n Where s is the source name to be transliterated, which contains letter string s1s2... si; t i</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Rabiner, L. R., A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition, Proceedings of the IEEE, 77, 257–286 (1989).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Viterbi</author>
</authors>
<title>Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm,</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>13</volume>
<pages>260</pages>
<contexts>
<context position="10953" citStr="Viterbi 1967" startWordPosition="1832" endWordPosition="1833"> sequence identified in training data, the higher probability that the translation sequence is the correct tone. These α parameters can be tuned in the future. We call this approach Maximum N-gram HMM. The same process is conducted for Pinyin to Chinese character translation as shown in the lower part of Figure 1 Module 1. 4.2 Translation and Ranking Once the two Maximum N-gram HMM Model are obtained, new incoming names are translated by obtaining a letter sequence that maximizes the overall probability through the HMM (Figure 1 Module 2). This step uses a modified Viterbi’s search algorithm (Viterbi 1967). The original Viterbi’s algorithm only keeps the most optimal path. To cope with name translation variations, we keep the top-20 optimal paths for further analysis. 4.3 Web Mining Component To boost the transliteration performance we propose to use the Web mining approach, which analyzes candidates’ occurrence on the Web ) and s translates to t i−2 i−2 , P t t ( , 1 2 ) ( | P t 2 = P(t1 |s1 1 − ) 2 = ) # of times # of times timess translates tot givens t − − &gt; i i i 1 i 1 #of (Figure 1 Module 3). Each one of the top-20 transliterations obtained from the previous step is sent to a Web search e</context>
</contexts>
<marker>Viterbi, 1967</marker>
<rawString>Viterbi, A. J., Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm, IEEE Transactions on Information Theory, 13, 260 (1967).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhou</author>
<author>F Huang</author>
<author>H Chen</author>
</authors>
<title>Combining probability Models and Web Mining Models: A Framework for Proper Name transliteration,</title>
<date>2008</date>
<journal>Information Technology and Management,</journal>
<volume>9</volume>
<contexts>
<context position="2540" citStr="Zhou et al., 2008" startWordPosition="383" endWordPosition="386">ty issue. The accuracy of a machine translation system, whether statistical or example-based, largely depends on sentence context information. This context information is often not available with name translation. Furthermore, emerging names are difficult to capture in regular machine translation systems if they have not been included in training corpus or translation dictionary. Thus, being able to translate proper names not only has its own application area, it will also enhance the performance of current machine translation systems. In our previous English-Arabic name transliteration work (Zhou et al., 2008), we proposed a framework for name transliteration using a 2- gram and a 3-gram Hidden Markov Model (HMM). In this research, we extend our 2-gram and 3-gram HMM to an N-gram HMM where N is the maximum number of prior translation mapping sequence that can be identified in the training corpus. In our non-standard runs, we also integrated a Web mining module. The rest of the paper is structured as follows. Section 2 reviews related work; Section 3 describes our algorithm; Section 4 discusses implementation and evaluation results are provided in Section 5. Section 6 concludes our work. 2 Related W</context>
</contexts>
<marker>Zhou, Huang, Chen, 2008</marker>
<rawString>Zhou, Y., Huang, F., and Chen, H., Combining probability Models and Web Mining Models: A Framework for Proper Name transliteration, Information Technology and Management, 9, 91 (2008).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>