<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001206">
<title confidence="0.9735065">
Diagnosing meaning errors in
short answers to reading comprehension questions
</title>
<author confidence="0.9935">
Stacey Bailey
</author>
<affiliation confidence="0.996352">
Department of Linguistics
The Ohio State University
</affiliation>
<address confidence="0.6018665">
1712 Neil Avenue
Columbus, Ohio 43210, USA
</address>
<email confidence="0.995224">
s.bailey@ling.osu.edu
</email>
<author confidence="0.722591">
Detmar Meurers
</author>
<affiliation confidence="0.632704">
Seminar f¨ur Sprachwissenschaft
</affiliation>
<address confidence="0.839078333333333">
Universit¨at T¨ubingen
Wilhelmstrasse 19
72074 T¨ubingen, Germany
</address>
<email confidence="0.997027">
dm@sfs.uni-tuebingen.de
</email>
<sectionHeader confidence="0.995601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998977">
A common focus of systems in Intelli-
gent Computer-Assisted Language Learning
(ICALL) is to provide immediate feedback to
language learners working on exercises. Most
of this research has focused on providing feed-
back on the form of the learner input. Foreign
language practice and second language acqui-
sition research, on the other hand, emphasizes
the importance of exercises that require the
learner to manipulate meaning.
The ability of an ICALL system to diag-
nose and provide feedback on the mean-
ing conveyed by a learner response depends
on how well it can deal with the response
variation allowed by an activity. We focus
on short-answer reading comprehension ques-
tions which have a clearly defined target re-
sponse but the learner may convey the mean-
ing of the target in multiple ways. As empiri-
cal basis of our work, we collected an English
as a Second Language (ESL) learner corpus
of short-answer reading comprehension ques-
tions, for which two graders provided target
answers and correctness judgments. On this
basis, we developed a Content-Assessment
Module (CAM), which performs shallow se-
mantic analysis to diagnose meaning errors. It
reaches an accuracy of 88% for semantic error
detection and 87% on semantic error diagno-
sis on a held-out test data set.
</bodyText>
<sectionHeader confidence="0.999331" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997670054054054">
Language practice that includes meaningful interac-
tion is a critical component of many current lan-
guage teaching theories. At the same time, exist-
ing research on intelligent computer-aided language
learning (ICALL) systems has focused primarily on
providing practice with grammatical forms. For
most ICALL systems, although form assessment of-
ten involves the use of natural language processing
(NLP) techniques, the need for sophisticated con-
tent assessment of a learner response is limited by
restricting the kinds of activities offered in order to
tightly control the variation allowed in learner re-
sponses, i.e., only one or very few forms can be used
by the learner to express the correct content. Yet
many of the activities that language instructors typ-
ically use in real language-learning settings support
a significant degree of variation in correct answers
and in turn require both form and content assess-
ment for answer evaluation. Thus, there is a real
need for ICALL systems that provide accurate con-
tent assessment.
While some meaningful activities are too unre-
stricted for ICALL systems to provide effective con-
tent assessment, where the line should be drawn on
a spectrum of language exercises is an open ques-
tion. Different language-learning exercises carry
different expectations with respect to the level and
type of linguistic variation possible across learner
responses. In turn, these expectations may be linked
to the learning goals underlying the activity design,
the cognitive skills required to respond to the ac-
tivity, or other properties of the activity. To de-
velop adequate processing strategies for content as-
sessment, it is important to understand the connec-
tion between exercises and expected variation, as
conceptualized by the exercise spectrum shown in
Figure 1, because the level of variation imposes re-
</bodyText>
<page confidence="0.975996">
107
</page>
<note confidence="0.8377095">
Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 107–115,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</note>
<figureCaption confidence="0.998831">
Figure 1: Language Learning Exercise Spectrum
</figureCaption>
<figure confidence="0.672458">
Tightly Restricted Responses Loosely Restricted Responses
Decontextualized Short-answer reading Essays on
grammar fill-in- comprehension individualized
the-blanks questions topics
The Middle Ground
Viable Processing Ground
</figure>
<bodyText confidence="0.999920941176471">
quirements and limitations on different processing
strategies. At one extreme of the spectrum, there are
tightly restricted exercises requiring minimal analy-
sis in order to assess content. At the other extreme
are unrestricted exercises requiring extensive form
and content analysis to assess content. In this work,
we focus on determining whether shallow content-
analysis techniques can be used to perform content
assessment for activities in the space between the
extremes. A good test case in this middle ground
are loosely restricted reading comprehension (RC)
questions. From a teaching perspective, they are a
task that is common in real-life learning situations,
they combine elements of comprehension and pro-
duction, and they are a meaningful activity suited
to an ICALL setting. From a processing perspec-
tive, responses exhibit linguistic variation on lexical,
morphological, syntactic and semantic levels – yet
the intended contents of the answer is predictable so
that an instructor can define target responses.
Since variation is possible across learner re-
sponses in activities in the middle ground of the
spectrum, we propose a shallow content assessment
approach which supports the comparison of target
and learner responses on several levels including to-
ken, chunk and relation. We present an architec-
ture for a content assessment module (CAM) which
provides this flexibility using multiple surface-based
matching strategies and existing language process-
ing tools. For an empirical evaluation, we collected
a corpus of language learner data consisting exclu-
sively of responses to short-answer reading compre-
hension questions by intermediate English language
learners.
</bodyText>
<sectionHeader confidence="0.961911" genericHeader="method">
2 The Data
</sectionHeader>
<bodyText confidence="0.99997435483871">
The learner corpus consists of 566 responses to
short-answer comprehension questions. The re-
sponses, written by intermediate ESL students as
part of their regular homework assignments, were
typically 1-3 sentences in length. Students had ac-
cess to their textbooks for all activities. For devel-
opment and testing, the corpus was divided into two
sets. The development set contains 311 responses
from 11 students answering 47 different questions;
the test set contains 255 responses from 15 students
to 28 questions. The development and test sets were
collected in two different classes of the same inter-
mediate reading/writing course.
Two graders annotated the learner answers with
a binary code for semantic correctness and one of
several diagnosis codes to be discussed below. Tar-
get responses (i.e., correct answers) and keywords
from the target responses were also identified by
the graders.1 Because we focus on content assess-
ment, learner responses containing grammatical er-
rors were only marked as incorrect if the grammat-
ical errors impacted the understanding of the mean-
ing.
The graders did not agree on correctness judg-
ments for 31 responses (12%) in the test set. These
were eliminated from the test set in order to obtain a
gold standard for evaluation.
The remaining responses in the development and
test sets showed a range of variation for many of the
prompts. As the following example from the corpus
illustrates, even straightforward questions based on
</bodyText>
<footnote confidence="0.986278">
1Keywords refer to terms in the target response essential to
a correct answer.
</footnote>
<page confidence="0.997741">
108
</page>
<bodyText confidence="0.996555666666667">
an explicit short reading passage yield both linguis-
tic and content variation:
in meaning from what was conveyed by the target
answer.2
CUE: What are the methods of propaganda men-
tioned in the article?
TARGET: The methods include use of labels, visual
images, and beautiful or famous people promoting
the idea or product. Also used is linking the product
to concepts that are admired or desired and to create
the impression that everyone supports the product or
idea.
</bodyText>
<listItem confidence="0.972913375">
LEARNER RESPONSES:
• A number of methods of propaganda are used
in the media.
• Bositive or negative labels.
• Giving positive or negative labels. Using vi-
sual images. Having a beautiful or famous per-
son to promote. Creating the impression that
everyone supports the product or idea.
</listItem>
<bodyText confidence="0.993020296296296">
While the third answer was judged to be correct,
the syntactic structures, word order, forms, and lexi-
cal items used (e.g., famous person vs. famous peo-
ple) vary from the string provided as target. Of the
learner responses in the corpus, only one was string
identical with the teacher-provided target and nine
were identical when treated as bags-of-words. In the
test set, none of the learner responses was string or
bag-of-word identical with the corresponding target
sentence.
To classify the variation exhibited in learner re-
sponses, we developed an annotation scheme based
on target modification, with the meaning error la-
bels being adapted from those identified by James
(1998) for grammatical mistakes. Target modifica-
tion encodes how the learner response varies from
the target, but makes the sometimes incorrect as-
sumption that the learner is actually trying to “hit”
the meaning of the target. The annotation scheme
distinguishes correct answers, omissions (of rele-
vant concepts), overinclusions (of incorrect con-
cepts), blends (both omissions and overinclusions),
and non-answers. These error types are exempli-
fied below with examples from the corpus. In ad-
dition, the graders used the label alternate answer
for responses that were correct given the question
and reading passage, but that differed significantly
</bodyText>
<listItem confidence="0.983915">
1. Necessary concepts left out of learner response.
</listItem>
<bodyText confidence="0.6676416">
CUE: Name the features that are used in the
design of advertisements.
TARGET: The features are eye contact, color,
famous people, language and cultural refer-
ences.
</bodyText>
<listItem confidence="0.843633833333333">
RESPONSE: Eye contact, color
2. Response with extraneous, incorrect concepts.
CUE: Which form of programming on TV
shows that highest level of violence?
TARGET: Cartoons show the most violent acts.
RESPONSE: Television drama, children’s pro-
grams and cartoons.
3. An incorrect blend/substitution (correct con-
cept missing, incorrect one present).
CUE: What is alliteration?
TARGET: Alliteration is where sequential
words begin with the same letter or sound.
RESPONSE: The worlds are often chosen to
make some pattern or play on works. Sequen-
tial works begins with the same letter or sound.
4. Multiple incorrect concepts.
CUE: What was the major moral question
raised by the Clinton incident?3
</listItem>
<bodyText confidence="0.991265">
TARGET: The moral question raised by the
Clinton incident was whether a politician’s
personal life is relevant to their job perfor-
mance.
RESPONSE: The scandal was about the rela-
tionship between Clinton and Lewinsky.
</bodyText>
<sectionHeader confidence="0.971121" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.9999846">
The CAM design integrates multiple matching
strategies at different levels of representation and
various abstractions from the surface form to com-
pare meanings across a range of response varia-
tions. The approach is related to the methods used in
</bodyText>
<footnote confidence="0.930844">
2We use the term concept to refer to an entity or a relation
between entities in a representation of the meaning of a sen-
tence. Thus, a response generally contains multiple concepts.
3Note the incorrect presupposition in the cue provided by
the instructor.
</footnote>
<page confidence="0.998389">
109
</page>
<bodyText confidence="0.997072571428571">
machine translation evaluation (e.g., Banerjee and
Lavie, 2005; Lin and Och, 2004), paraphrase recog-
nition (e.g., Brockett and Dolan, 2005; Hatzivas-
siloglou et al., 1999), and automatic grading (e.g.,
Leacock, 2004; Marin, 2004).
To illustrate the general idea, consider the exam-
ple from our corpus in Figure 2.
</bodyText>
<figureCaption confidence="0.987188">
Figure 2: Basic matching example
</figureCaption>
<bodyText confidence="0.999925321428572">
We find one string identical match between the token
was occurring in the target and the learner response.
At the noun chunk level we can match home with
his house. And finally, after pronoun resolution it is
possible to match Bob Hope with he.
The overall architecture of CAM is shown in Fig-
ure 3. Generally speaking, CAM compares the
learner response to a stored target response and de-
cides whether the two responses are possibly differ-
ent realizations of the same semantic content. The
design relies on a series of increasingly complex
comparison modules to “align” or match compatible
concepts. Aligned and unaligned concepts are used
to diagnose content errors. The CAM design sup-
ports the comparison of target and learner responses
on token, chunk and relation levels. At the token
level, the nature of the comparison includes abstrac-
tions of the string to its lemma (i.e., uninflected root
form of a word), semantic type (e.g., date, location),
synonyms, and a more general notion of similarity
supporting comparison across part-of-speech.
The system takes as input the learner response and
one or more target responses, along with the ques-
tion and the source reading passage. The compari-
son of the target and learner input pair proceeds first
with an analysis filter, which determines whether
linguistic analysis is required for diagnosis. Essen-
tially, this filter identifies learner responses that were
copied directly from the source text.
Then, for any learner-target response pair that
requires linguistic analysis, CAM assessment pro-
ceeds in three phases – Annotation, Alignment and
Diagnosis. The Annotation phase uses NLP tools to
enrich the learner and target responses, as well as
the question text, with linguistic information, such
as lemmas and part-of-speech tags. The question
text is used for pronoun resolution and to eliminate
concepts that are “given” (cf. Halliday, 1967, p. 204
and many others since). Here “given” information
refers to concepts from the question text that are re-
used in the learner response. They may be neces-
sary for forming complete sentences, but contribute
no new information. For example, if the question is
What is alliteration? and the response is Allitera-
tion is the repetition of initial letters or sounds, then
the concept represented by the word alliteration is
given and the rest is new. For CAM, responses are
neither penalized nor rewarded for containing given
information.
Table 1 contains an overview of the annotations
and the resources, tools or algorithms used. The
choice of the particular algorithm or implementation
was primarily based on availability and performance
on our development corpus – other implementations
could generally be substituted without changing the
overall approach.
</bodyText>
<table confidence="0.999860357142857">
Annotation Task Language Processing Tool
Sentence Detection, MontyLingua (Liu, 2004)
Tokenization,
Lemmatization
Lemmatization PC-KIMMO (Antworth, 1993)
Spell Checking Edit distance (Levenshtein, 1966),
SCOWL word list (Atkinson, 2004)
Part-of-speech Tagging TreeTagger (Schmid, 1994)
Noun Phrase Chunking CASS (Abney, 1997)
Lexical Relations WordNet (Miller, 1995)
Similarity Scores PMI-IR (Turney, 2001;
Mihalcea et al., 2006)
Dependency Relations Stanford Parser
(Klein and Manning, 2003)
</table>
<tableCaption confidence="0.999899">
Table 1: NLP Tools used in CAM
</tableCaption>
<bodyText confidence="0.9998184">
After the Annotation phase, Alignment maps new
(i.e., not given) concepts in the learner response to
concepts in the target response using the annotated
information. The final Diagnosis phase analyzes
the alignment to determine whether the learner re-
</bodyText>
<page confidence="0.994998">
110
</page>
<figureCaption confidence="0.999733">
Figure 3: Architecture of the Content Assessment Module (CAM)
</figureCaption>
<figure confidence="0.999139513513513">
Output
Input
Annotation Alignment Diagnosis
Analysis Filter
Givenness
Pre-Alignment Filters
Punctuation
Token-level
Alignment
Detection
Classification
Chunk-level
Alignment
Diagnosis
Classification
Relation-level
Alignment
Source Text
Learner
Response
Activity Model
Target
Response(s)
Question
Settings
Sentence Detection
Tokenization
Lemmatization
Dependency Parsing
Spelling Correction
Similarity Scoring
Pronoun Resolution
Type Recognition
POS Tagging
Chunking
Error
Reporting
</figure>
<bodyText confidence="0.998397714285714">
sponse contains content errors. If multiple target re-
sponses are supplied, then each is compared to the
learner response and the target response with the
most matches is selected as the model used in di-
agnosis. The output is a diagnosis of the input pair,
which might be used in a number of ways to provide
feedback to the learner.
</bodyText>
<subsectionHeader confidence="0.999761">
3.1 Combining the evidence
</subsectionHeader>
<bodyText confidence="0.999933108108108">
To combine the evidence from these different lev-
els of analysis for content evaluation and diagno-
sis, we tried two methods. In the first, we hand-
wrote rules and set thresholds to maximize perfor-
mance on the development set. On the development
set, the hand-tuned method resulted in an accuracy
of 81% for the semantic error detection task, a bi-
nary judgment task. However, performance on the
test set (which was collected in a later quarter with
a different instructor and different students) made
clear that the rules and thresholds thus obtained were
overly specific to the development set, as accuracy
dropped down to 63% on the test set. The hand-
written rules apparently were not general enough to
transfer well from the development set to the test set,
i.e., they relied on properties of the development set
that where not shared across data sets. Given the va-
riety of features and the many different options for
combining and weighing them that might have been
explored, we decided that rather than hand-tuning
the rules to additional data, we would try to machine
learn the best way of combining the evidence col-
lected. We thus decided to explore machine learn-
ing, even though the set of development data for
training clearly is very small.
Machine learning has been used for equivalence
recognition in related fields. For instance, Hatzivas-
siloglou et al. (1999) trained a classifier for para-
phrase detection, though their performance only
reached roughly 37% recall and 61% precision. In
a different approach, Finch et al. (2005) found that
MT evaluation techniques combined with machine
learning improves equivalence recognition. They
used the output of several MT evaluation approaches
based on matching concepts (e.g., BLEU) as fea-
tures/values for training a support vector machine
(SVM) classifier. Matched concepts and unmatched
</bodyText>
<page confidence="0.997608">
111
</page>
<bodyText confidence="0.999352466666667">
concepts alike were used as features for training the
classifier. Tested against the Microsoft Research
Paraphrase (MSRP) Corpus, the SVM classifier ob-
tained 75% accuracy on identifying paraphrases.
But it does not appear that machine learning tech-
niques have so far been applied to or even discussed
in the context of language learner corpora, where the
available data sets typically are very small.
To begin to address the application of machine
learning to meaning error diagnosis, the alignment
data computed by CAM was converted into features
suitable for machine learning. For example, the first
feature calculated is the relative overlap of aligned
keywords from the target response. The full list of
features are listed in Table 2.
</bodyText>
<listItem confidence="0.678280571428571">
Features Description
1. Keyword Overlap Percent of keywords aligned
(relative to target)
2. Target Overlap Percent of aligned target tokens
3. Learner Overlap Percent of aligned learner tokens
4. T-Chunk Percent of aligned target chunks
5. L-Chunk Percent of aligned learner chunks
6. T-Triple Percent of aligned target triples
7. L-Triple Percent of aligned learner triples
8. Token Match Percent of token alignments
that were token-identical
9. Similarity Match Percent of token alignments
that were similarity-resolved
10. Type Match Percent of token alignments
that were type-resolved
11. Lemma Match Percent of token alignments
that were lemma-resolved
12. Synonym Match Percent of token alignments
that were synonym-resolved
13. Variety of Match Number of kinds of token-level
(0-5) alignments
</listItem>
<tableCaption confidence="0.990198">
Table 2: Features used for Machine Learning
</tableCaption>
<bodyText confidence="0.999701236842105">
Features 1-7 reflect relative numbers of matches (rel-
ative to length of either the target or learner re-
sponse). Features 2, 4, and 6 are related to the target
response overlap. Features 3, 5, and 7 are related to
overlap in the learner response. Features 8–13 re-
flect the nature of the matches.
The values for the 13 features in Table 2 were used
to train the detection classifier. For diagnosis, a four-
teenth feature – a detection feature (1 or 0 depending
on whether the detection classifier detected an error)
– was added to the development data to train the di-
agnosis classifier. Given that token-level alignments
are used in identifying chunk- and triple-level align-
ments, that kinds of alignments are related to variety
of matches, etc., there is clear redundancy and inter-
dependence among features. But each feature adds
some new information to the overall diagnosis pic-
ture.
The machine learning suite used in all the devel-
opment and testing runs is TiMBL (Daelemans et al.,
2007). As with the NLP tools used, TiMBL was cho-
sen mainly to illustrate the approach. It was not eval-
uated against several learning algorithms to deter-
mine the best performing algorithm for the task, al-
though this is certainly an avenue for future research.
In fact, TiMBL itself offers several algorithms and
options for training and testing. Experiments with
these options on the development set included vary-
ing how similarity between instances was measured,
how importance (i.e., weight) was assigned to fea-
tures and how many neighbors (i.e., instances) were
examined in classifying new instances. Given the
very small development set available, making em-
pirical tuning on the development set difficult, we
decided to use the default learning algorithm (k-
nearest neighbor) and majority voting based on the
top-performing training runs for each available dis-
tance measure.
</bodyText>
<sectionHeader confidence="0.999953" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999790722222222">
Turning to the results obtained by the machine-
learning based CAM, for the binary semantic error
detection task, the system obtains an overall 87% ac-
curacy on the development set (using the leave-one-
out option of TiMBL to avoid training on the test
item). Interestingly, even for this small development
set, machine learning thus outperforms the accuracy
obtained for the manual method of combining the
evidence reported above. On the test set, the final
TiMBL-based CAM performance for detection im-
proved slightly to 88% accuracy. These results sug-
gest that detection using the CAM design is viable,
though more extensive testing with a larger corpus
is needed.
Balanced sets Both the development and test sets
contained a high proportion of correct answers –
71% of the development set and 84% of the test set
were marked as correct by the human graders. Thus,
</bodyText>
<page confidence="0.996159">
112
</page>
<bodyText confidence="0.999954742857143">
we also sampled a balanced set consisting of 50%
correct and 50% incorrect answers by randomly in-
cluding correct answers plus all the incorrect an-
swers to obtain a set with 152 cases (development
subset) and 72 (test subset) sentences. The accuracy
obtained for this balanced set was 78% (leave-one-
out-testing with development set) and 67% (test set).
The fact that the results for the balanced develop-
ment set using leave-one-out-testing are comparable
to the general results shows that the machine learner
was not biased towards the ratio of correct and in-
correct responses, even though there is a clear drop
from development to test set, possibly related to the
small size of the data sets available for training and
testing.
Alternate answers Another interesting aspect to
discuss is the treatment of alternate answers. Recall
that alternate answers are those learner responses
that are correct but significantly dissimilar from the
given target. Of the development set response pairs,
15 were labeled as alternate answers. One would
expect that given that these responses violate the as-
sumption that the learner is trying to hit the given
target, using these items in training would negatively
effect the results. This turns out to be the case; per-
formance on the training set drops slightly when the
alternate answer pairs are included. We thus did not
include them in the development set used for train-
ing the classifier. In other words, the diagnosis clas-
sifier was trained to label the data with one of five
codes – correct, omissions (of relevant concepts),
overinclusions (of incorrect concepts), blends (both
omissions and overinclusions), and non-answers.
Because it cannot be determined beforehand which
items in unseen data are alternate answer pairs, these
pairs were not removed from the test set in the final
evaluation. Were these items eliminated, the detec-
tion performance would improve slightly to 89%.
Form errors Interestingly, the form errors fre-
quently occurring in the student utterances did not
negatively impact the CAM results. On average, a
learner response in the test set contained 2.7 form
errors. Yet, 68% of correctly diagnosed sentences
included at least one form error, but only 53% of
incorrectly diagnosed ones did so. In other words,
correct responses had more form errors than incor-
rect responses. Looking at numbers and combina-
tions of form errors, no clear pattern emerges that
would suggest that form errors are linked to mean-
ing errors in a clear way. One conclusion to draw
based on these data is that form and content assess-
ment can be treated as distinct in the evaluation of
learner responses. Even in the presence of a range
of form-based errors, human graders can clearly ex-
tract the intended meaning to be able to evaluate se-
mantic correctness. The CAM approach is similarly
able to provide meaning evaluation in the presence
of grammatical errors.
Diagnosis For diagnosis with five codes, CAM
obtained overall 87% accuracy both on the devel-
opment and on the test set. Given that the number of
labels increases from 2 to 5, the slight drop in overall
performance in diagnosis as compared to the detec-
tion of semantic errors (from 88% to 87%) is both
unsurprising in the decline and encouraging in the
smallness of the decline. However, given the sample
size and few numbers of instances of any given error
in the test (and development) set, additional quanti-
tative analysis of the diagnosis results would not be
particularly meaningful.
</bodyText>
<sectionHeader confidence="0.99985" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.9999535">
The need for semantic error diagnosis in previous
CALL work has been limited by the narrow range
of acceptable response variation in the supported
language activity types. The few ICALL systems
that have been successfully integrated into real-life
language teaching, such as German Tutor (Heift,
2001) and BANZAI (Nagata, 2002), also tightly
control expected response variation through delib-
erate exercise type choices that limit acceptable re-
sponses. Content assessment in the German Tutor
is performed by string matching against the stored
targets. Because of the tightly controlled exercise
types and lack of variation in the expected input,
the assumption that any variation in a learner re-
sponse is due to form error, rather than legitimate
variation, is a reasonable one. The recently de-
veloped TAGARELA system for learners of Por-
tuguese (Amaral and Meurers, 2006; Amaral, 2007)
lifts some of the restrictions on exercise types, while
relying on shallow semantic processing. Using
strategies inspired by our work, TAGARELA in-
corporates simple content assessment for evaluating
</bodyText>
<page confidence="0.998055">
113
</page>
<bodyText confidence="0.998797636363636">
learner responses in short-answer questions.
ICALL system designs that do incorporate more
sophisticated content assessment include FreeText
(L’Haire and Faltin, 2003), the Military Language
Tutor (MILT) Program (Kaplan et al., 1998), and
Herr Kommissar (DeSmedt, 1995). These systems
restrict both the exercise types and domains to make
content assessment feasible using deeper semantic
processing strategies.
Beyond the ICALL domain, work in automatic
grading of short answers and essays has addressed
whether the students answers convey the correct
meaning, but these systems focus on largely scor-
ing rather than diagnosis (e.g., E-rater, Burstein
and Chodorow, 1999), do not specifically address
language learning contexts and/or are designed to
work specifically with longer texts (e.g., AutoTu-
tor, Wiemer-Hastings et al., 1999). Thus, the extent
to which ICALL systems can diagnose meaning er-
rors in language learner responses has been far from
clear.
As far as we are aware, no directly comparable
systems performing content-assessment on related
language learner data exist. The closest related sys-
tem that does a similar kind of detection is the C-
rater system (Leacock, 2004). That system obtains
85% accuracy. However, the test set and scoring sys-
tem were different, and the system was applied to
responses from native English speakers. In addition,
their work focused on detection of errors rather than
diagnosis. So, the results are not directly compara-
ble. Nevertheless, the CAM detection results clearly
are competitive.
</bodyText>
<sectionHeader confidence="0.998965" genericHeader="conclusions">
6 Summary
</sectionHeader>
<bodyText confidence="0.999977857142857">
After motivating the need for content assessment in
ICALL, in this paper we have discussed an approach
for content assessment of English language learner
responses to short answer reading comprehension
questions, which is worked out in detail in Bailey
(2008). We discussed an architecture which relies on
shallow processing strategies and achieves an accu-
racy approaching 90% for content error detection on
a learner corpus we collected from learners complet-
ing the exercises assigned in a real-life ESL class.
Even for the small data sets available in the area of
language learning, it turns out that machine learn-
ing can be effective for combining the evidence from
various shallow matching features. The good perfor-
mance confirms the viability of using shallow NLP
techniques for meaning error detection. By devel-
oping and testing this model, we hope to contribute
to bridging the gap between what is practical and
feasible from a processing perspective and what is
desirable from the perspective of current theories of
language instruction.
</bodyText>
<sectionHeader confidence="0.99853" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999447854166667">
Steven Abney, 1997. Partial Parsing via Finite-State Cas-
cades. Natural Language Engineering, 2(4):337–344.
http://vinartus.net/spa/97a.pdf.
Luiz Amaral, 2007. Designing Intelligent Language Tu-
toring Systems: Integrating Natural Language Pro-
cessing Technology into Foreign Language Teaching.
Ph.D. thesis, The Ohio State University.
Luiz Amaral and Detmar Meurers, 2006. Where
does ICALL Fit into Foreign Language Teaching?
Presentation at the 23rd Annual Conference of the
Computer Assisted Language Instruction Consortium
(CALICO), May 19, 2006. University of Hawaii.
http://purl.org/net/icall/handouts/
calico06-amaral-meurers.pdf.
Evan L. Antworth, 1993. Glossing Text with the PC-
KIMMO Morphological Parser. Computers and the
Humanities, 26:475–484.
Kevin Atkinson, 2004. Spell Checking Oriented
Word Lists (SCOWL). http://wordlist.
sourceforge.net/.
Stacey Bailey, 2008. Content Assessment in Intelligent
Computer-Aided Language Learning: Meaning Error
Diagnosis for English as a Second Language. Ph.D.
thesis, The Ohio State University.
Satanjeev Banerjee and Alon Lavie, 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of Workshop on Intrinsic and Extrinsic Eval-
uation Measures for Machine Translation and/or Sum-
marization at the 43th Annual Meeting of the Associ-
ation of Computational Linguistics (ACL-2005). Ann
Arbor, Michigan, pp. 65–72. http://aclweb.
org/anthology/W05-0909.
Chris Brockett and William B. Dolan, 2005. Support Vec-
tor Machines for Paraphrase Identification and Cor-
pus Construction. In Proceedings of the Third In-
ternational Workshop on Paraphrasing (IWP2005).
pp. 1–8. http://aclweb.org/anthology/
I05-5001.
Jill Burstein and Martin Chodorow, 1999. Automated
Essay Scoring for Nonnative English Speakers. In
Proceedings of a Workshop on Computer-Mediated
Language Assessment and Evaluation of Natural Lan-
guage Processing, Joint Symposium of the Asso-
ciation of Computational Linguistics (ACL-99) and
the International Association of Language Learning
Technologies. pp. 68–75. http://aclweb.org/
anthology/W99-0411.
</reference>
<page confidence="0.988291">
114
</page>
<reference confidence="0.999866567010309">
Walter Daelemans, Jakub Zavrel, Kovan der Sloot and
Antal van den Bosch, 2007. TiMBL: Tilburg Memory-
Based Learner Reference Guide, ILK Technical Re-
port ILK 07-03. Induction of Linguistic Knowledge
Research Group Department of Communication and
Information Sciences, Tilburg University, P.O. Box
90153, NL-5000 LE, Tilburg, The Netherlands, ver-
sion 6.0 edition.
William DeSmedt, 1995. Herr Kommissar: An ICALL
Conversation Simulator for Intermediate German. In
V. Melissa Holland, Jonathan Kaplan and Michelle
Sams (eds.), Intelligent Language Tutors: Theory
Shaping Technology, Lawrence Erlbaum Associates,
pp. 153–174.
Andrew Finch, Young-Sook Hwang and Eiichiro Sumita,
2005. Using Machine Translation Evaluation Tech-
niques to Determine Sentence-level Semantic Equiva-
lence. In Proceedings of the Third International Work-
shop on Paraphrasing (IWP2005). pp. 17–24. http:
//aclweb.org/anthology/I05-5003.
Michael Halliday, 1967. Notes on Transitivity and Theme
in English. Part 1 and 2. Journal of Linguistics, 3:37–
81, 199–244.
Vasileios Hatzivassiloglou, Judith Klavans and Eleazar
Eskin, 1999. Detecting Text Similarity over Short Pas-
sages: Exploring Linguistic Feature Combinations via
Machine Learning. In Proceedings of Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora (EMNLP’99). College Park, Maryland, pp.
203–212. http://aclweb.org/anthology/
W99-0625.
Trude Heift, 2001. Intelligent Language Tutoring
Systems for Grammar Practice. Zeitschrift f¨ur In-
terkulturellen Fremdsprachenunterricht, 6(2). http:
//www.spz.tu-darmstadt.de/projekt_
ejournal/jg-06-2/beitrag/heift2.htm.
Carl James, 1998. Errors in Language Learning and Use:
Exploring Error Analysis. Longman Publishers.
Jonathan Kaplan, Mark Sobol, Robert Wisher and Robert
Seidel, 1998. The Military Language Tutor (MILT)
Program: An Advanced Authoring System. Computer
Assisted Language Learning, 11(3):265–287.
Dan Klein and Christopher D. Manning, 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of the 41st
Meeting of the Association for Computational Linguis-
tics (ACL 2003). Sapporo, Japan, pp. 423–430. http:
//aclweb.org/anthology/P03-1054.
Claudia Leacock, 2004. Scoring Free-Responses Auto-
matically: A Case Study of a Large-Scale Assessment.
Examens, 1(3).
Vladimir I. Levenshtein, 1966. Binary Codes Capable of
Correcting Deletions, Insertions, and Reversals. Soviet
Physics Doklady, 10(8):707–710.
S´ebastien L’Haire and Anne Vandeventer Faltin, 2003.
Error Diagnosis in the FreeText Project. CALICO
Journal, 20(3):481–495.
Chin-Yew Lin and Franz Josef Och, 2004. Auto-
matic Evaluation of Machine Translation Quality Us-
ing Longest Common Subsequence and Skip-Bigram
Statistics. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-04). pp. 605–612. http://aclweb.org/
anthology/P04-1077.
Hugo Liu, 2004. MontyLingua: An End-to-
End Natural Language Processor with Common
Sense. http://web.media.mit.edu/˜hugo/
montylingua, accessed October 30, 2006.
Diana Rosario P´erez Mar´ın, 2004. Automatic Evaluation
of Users’ Short Essays by Using Statistical and Shal-
low Natural Language Processing Techniques. Mas-
ter’s thesis, Universidad Aut´onoma de Madrid. http:
//www.ii.uam.es/˜dperez/tea.pdf.
Rada Mihalcea, Courtney Corley and Carlo Strapparava,
2006. Corpus-based and Knowledge-based Measures
of Text Semantic Similarity. In Proceedings of the Na-
tional Conference on Artificial Intelligence. American
Association for Artificial Intelligence (AAAI) Press,
Menlo Park, CA, volume 21(1), pp. 775–780.
George Miller, 1995. WordNet: A Lexical Database for
English. Communications of the ACM, 38(11):39–41.
Noriko Nagata, 2002. BANZAI: An Application of Nat-
ural Language Processing to Web-Based Language
Learning. CALICO Journal, 19(3):583–599.
Helmut Schmid, 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In International Con-
ference on New Methods in Language Processing.
Manchester, United Kingdom, pp. 44–49.
Peter Turney, 2001. Mining the Web for Synonyms: PMI-
IR Versus LSA on TOEFL. In Proceedings of the
Twelfth European Conference on Machine Learning
(ECML-2001). Freiburg, Germany, pp. 491–502.
Peter Wiemer-Hastings, Katja Wiemer-Hastings and
Arthur Graesser, 1999. Improving an Intelligent Tu-
tor’s Comprehension of Students with Latent Seman-
tic Analysis. In Susanne Lajoie and Martial Vivet
(eds.), Artificial Intelligence in Education, IOS Press,
pp. 535–542.
</reference>
<page confidence="0.999025">
115
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.233254">
<title confidence="0.984337">Diagnosing meaning errors in short answers to reading comprehension questions</title>
<author confidence="0.997763">Stacey</author>
<affiliation confidence="0.9780425">Department of The Ohio State</affiliation>
<address confidence="0.9956955">1712 Neil Columbus, Ohio 43210,</address>
<email confidence="0.999551">s.bailey@ling.osu.edu</email>
<note confidence="0.5633892">Detmar Seminar f¨ur Universit¨at Wilhelmstrasse 72074 T¨ubingen,</note>
<email confidence="0.993711">dm@sfs.uni-tuebingen.de</email>
<abstract confidence="0.998243967741936">A common focus of systems in Intelligent Computer-Assisted Language Learning (ICALL) is to provide immediate feedback to language learners working on exercises. Most of this research has focused on providing feedback on the form of the learner input. Foreign language practice and second language acquisition research, on the other hand, emphasizes the importance of exercises that require the learner to manipulate meaning. The ability of an ICALL system to diagnose and provide feedback on the meaning conveyed by a learner response depends on how well it can deal with the response variation allowed by an activity. We focus on short-answer reading comprehension questions which have a clearly defined target response but the learner may convey the meaning of the target in multiple ways. As empirical basis of our work, we collected an English as a Second Language (ESL) learner corpus of short-answer reading comprehension questions, for which two graders provided target answers and correctness judgments. On this basis, we developed a Content-Assessment Module (CAM), which performs shallow semantic analysis to diagnose meaning errors. It reaches an accuracy of 88% for semantic error detection and 87% on semantic error diagnosis on a held-out test data set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Partial Parsing via Finite-State Cascades.</title>
<date>1997</date>
<journal>Natural Language Engineering,</journal>
<volume>2</volume>
<issue>4</issue>
<pages>97</pages>
<contexts>
<context position="14331" citStr="Abney, 1997" startWordPosition="2208" endWordPosition="2209">e annotations and the resources, tools or algorithms used. The choice of the particular algorithm or implementation was primarily based on availability and performance on our development corpus – other implementations could generally be substituted without changing the overall approach. Annotation Task Language Processing Tool Sentence Detection, MontyLingua (Liu, 2004) Tokenization, Lemmatization Lemmatization PC-KIMMO (Antworth, 1993) Spell Checking Edit distance (Levenshtein, 1966), SCOWL word list (Atkinson, 2004) Part-of-speech Tagging TreeTagger (Schmid, 1994) Noun Phrase Chunking CASS (Abney, 1997) Lexical Relations WordNet (Miller, 1995) Similarity Scores PMI-IR (Turney, 2001; Mihalcea et al., 2006) Dependency Relations Stanford Parser (Klein and Manning, 2003) Table 1: NLP Tools used in CAM After the Annotation phase, Alignment maps new (i.e., not given) concepts in the learner response to concepts in the target response using the annotated information. The final Diagnosis phase analyzes the alignment to determine whether the learner re110 Figure 3: Architecture of the Content Assessment Module (CAM) Output Input Annotation Alignment Diagnosis Analysis Filter Givenness Pre-Alignment F</context>
</contexts>
<marker>Abney, 1997</marker>
<rawString>Steven Abney, 1997. Partial Parsing via Finite-State Cascades. Natural Language Engineering, 2(4):337–344. http://vinartus.net/spa/97a.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luiz Amaral</author>
</authors>
<title>Designing Intelligent Language Tutoring Systems: Integrating Natural Language Processing Technology into Foreign Language Teaching.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>The Ohio State University.</institution>
<contexts>
<context position="26242" citStr="Amaral, 2007" startWordPosition="4107" endWordPosition="4108"> as German Tutor (Heift, 2001) and BANZAI (Nagata, 2002), also tightly control expected response variation through deliberate exercise type choices that limit acceptable responses. Content assessment in the German Tutor is performed by string matching against the stored targets. Because of the tightly controlled exercise types and lack of variation in the expected input, the assumption that any variation in a learner response is due to form error, rather than legitimate variation, is a reasonable one. The recently developed TAGARELA system for learners of Portuguese (Amaral and Meurers, 2006; Amaral, 2007) lifts some of the restrictions on exercise types, while relying on shallow semantic processing. Using strategies inspired by our work, TAGARELA incorporates simple content assessment for evaluating 113 learner responses in short-answer questions. ICALL system designs that do incorporate more sophisticated content assessment include FreeText (L’Haire and Faltin, 2003), the Military Language Tutor (MILT) Program (Kaplan et al., 1998), and Herr Kommissar (DeSmedt, 1995). These systems restrict both the exercise types and domains to make content assessment feasible using deeper semantic processin</context>
</contexts>
<marker>Amaral, 2007</marker>
<rawString>Luiz Amaral, 2007. Designing Intelligent Language Tutoring Systems: Integrating Natural Language Processing Technology into Foreign Language Teaching. Ph.D. thesis, The Ohio State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luiz Amaral</author>
<author>Detmar Meurers</author>
</authors>
<title>Where does ICALL Fit into Foreign Language Teaching?</title>
<date>2006</date>
<booktitle>Presentation at the 23rd Annual Conference of the Computer Assisted Language Instruction Consortium (CALICO),</booktitle>
<institution>University of Hawaii.</institution>
<note>http://purl.org/net/icall/handouts/ calico06-amaral-meurers.pdf.</note>
<contexts>
<context position="26227" citStr="Amaral and Meurers, 2006" startWordPosition="4103" endWordPosition="4106">fe language teaching, such as German Tutor (Heift, 2001) and BANZAI (Nagata, 2002), also tightly control expected response variation through deliberate exercise type choices that limit acceptable responses. Content assessment in the German Tutor is performed by string matching against the stored targets. Because of the tightly controlled exercise types and lack of variation in the expected input, the assumption that any variation in a learner response is due to form error, rather than legitimate variation, is a reasonable one. The recently developed TAGARELA system for learners of Portuguese (Amaral and Meurers, 2006; Amaral, 2007) lifts some of the restrictions on exercise types, while relying on shallow semantic processing. Using strategies inspired by our work, TAGARELA incorporates simple content assessment for evaluating 113 learner responses in short-answer questions. ICALL system designs that do incorporate more sophisticated content assessment include FreeText (L’Haire and Faltin, 2003), the Military Language Tutor (MILT) Program (Kaplan et al., 1998), and Herr Kommissar (DeSmedt, 1995). These systems restrict both the exercise types and domains to make content assessment feasible using deeper sem</context>
</contexts>
<marker>Amaral, Meurers, 2006</marker>
<rawString>Luiz Amaral and Detmar Meurers, 2006. Where does ICALL Fit into Foreign Language Teaching? Presentation at the 23rd Annual Conference of the Computer Assisted Language Instruction Consortium (CALICO), May 19, 2006. University of Hawaii. http://purl.org/net/icall/handouts/ calico06-amaral-meurers.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evan L Antworth</author>
</authors>
<date>1993</date>
<booktitle>Glossing Text with the PCKIMMO Morphological Parser. Computers and the Humanities,</booktitle>
<pages>26--475</pages>
<contexts>
<context position="14159" citStr="Antworth, 1993" startWordPosition="2186" endWordPosition="2187"> the word alliteration is given and the rest is new. For CAM, responses are neither penalized nor rewarded for containing given information. Table 1 contains an overview of the annotations and the resources, tools or algorithms used. The choice of the particular algorithm or implementation was primarily based on availability and performance on our development corpus – other implementations could generally be substituted without changing the overall approach. Annotation Task Language Processing Tool Sentence Detection, MontyLingua (Liu, 2004) Tokenization, Lemmatization Lemmatization PC-KIMMO (Antworth, 1993) Spell Checking Edit distance (Levenshtein, 1966), SCOWL word list (Atkinson, 2004) Part-of-speech Tagging TreeTagger (Schmid, 1994) Noun Phrase Chunking CASS (Abney, 1997) Lexical Relations WordNet (Miller, 1995) Similarity Scores PMI-IR (Turney, 2001; Mihalcea et al., 2006) Dependency Relations Stanford Parser (Klein and Manning, 2003) Table 1: NLP Tools used in CAM After the Annotation phase, Alignment maps new (i.e., not given) concepts in the learner response to concepts in the target response using the annotated information. The final Diagnosis phase analyzes the alignment to determine w</context>
</contexts>
<marker>Antworth, 1993</marker>
<rawString>Evan L. Antworth, 1993. Glossing Text with the PCKIMMO Morphological Parser. Computers and the Humanities, 26:475–484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Atkinson</author>
</authors>
<title>Spell Checking Oriented Word Lists (SCOWL).</title>
<date>2004</date>
<note>http://wordlist. sourceforge.net/.</note>
<contexts>
<context position="14242" citStr="Atkinson, 2004" startWordPosition="2197" endWordPosition="2198"> penalized nor rewarded for containing given information. Table 1 contains an overview of the annotations and the resources, tools or algorithms used. The choice of the particular algorithm or implementation was primarily based on availability and performance on our development corpus – other implementations could generally be substituted without changing the overall approach. Annotation Task Language Processing Tool Sentence Detection, MontyLingua (Liu, 2004) Tokenization, Lemmatization Lemmatization PC-KIMMO (Antworth, 1993) Spell Checking Edit distance (Levenshtein, 1966), SCOWL word list (Atkinson, 2004) Part-of-speech Tagging TreeTagger (Schmid, 1994) Noun Phrase Chunking CASS (Abney, 1997) Lexical Relations WordNet (Miller, 1995) Similarity Scores PMI-IR (Turney, 2001; Mihalcea et al., 2006) Dependency Relations Stanford Parser (Klein and Manning, 2003) Table 1: NLP Tools used in CAM After the Annotation phase, Alignment maps new (i.e., not given) concepts in the learner response to concepts in the target response using the annotated information. The final Diagnosis phase analyzes the alignment to determine whether the learner re110 Figure 3: Architecture of the Content Assessment Module (C</context>
</contexts>
<marker>Atkinson, 2004</marker>
<rawString>Kevin Atkinson, 2004. Spell Checking Oriented Word Lists (SCOWL). http://wordlist. sourceforge.net/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stacey Bailey</author>
</authors>
<title>Content Assessment in Intelligent Computer-Aided Language Learning: Meaning Error Diagnosis for English as a Second Language.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>The Ohio State University.</institution>
<contexts>
<context position="28249" citStr="Bailey (2008)" startWordPosition="4408" endWordPosition="4409">system obtains 85% accuracy. However, the test set and scoring system were different, and the system was applied to responses from native English speakers. In addition, their work focused on detection of errors rather than diagnosis. So, the results are not directly comparable. Nevertheless, the CAM detection results clearly are competitive. 6 Summary After motivating the need for content assessment in ICALL, in this paper we have discussed an approach for content assessment of English language learner responses to short answer reading comprehension questions, which is worked out in detail in Bailey (2008). We discussed an architecture which relies on shallow processing strategies and achieves an accuracy approaching 90% for content error detection on a learner corpus we collected from learners completing the exercises assigned in a real-life ESL class. Even for the small data sets available in the area of language learning, it turns out that machine learning can be effective for combining the evidence from various shallow matching features. The good performance confirms the viability of using shallow NLP techniques for meaning error detection. By developing and testing this model, we hope to c</context>
</contexts>
<marker>Bailey, 2008</marker>
<rawString>Stacey Bailey, 2008. Content Assessment in Intelligent Computer-Aided Language Learning: Meaning Error Diagnosis for English as a Second Language. Ph.D. thesis, The Ohio State University.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization at the 43th Annual Meeting of the Association of Computational Linguistics (ACL-2005).</booktitle>
<pages>65--72</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="10968" citStr="Banerjee and Lavie, 2005" startWordPosition="1686" endWordPosition="1689">he relationship between Clinton and Lewinsky. 3 Method The CAM design integrates multiple matching strategies at different levels of representation and various abstractions from the surface form to compare meanings across a range of response variations. The approach is related to the methods used in 2We use the term concept to refer to an entity or a relation between entities in a representation of the meaning of a sentence. Thus, a response generally contains multiple concepts. 3Note the incorrect presupposition in the cue provided by the instructor. 109 machine translation evaluation (e.g., Banerjee and Lavie, 2005; Lin and Och, 2004), paraphrase recognition (e.g., Brockett and Dolan, 2005; Hatzivassiloglou et al., 1999), and automatic grading (e.g., Leacock, 2004; Marin, 2004). To illustrate the general idea, consider the example from our corpus in Figure 2. Figure 2: Basic matching example We find one string identical match between the token was occurring in the target and the learner response. At the noun chunk level we can match home with his house. And finally, after pronoun resolution it is possible to match Bob Hope with he. The overall architecture of CAM is shown in Figure 3. Generally speaking</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie, 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proceedings of Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization at the 43th Annual Meeting of the Association of Computational Linguistics (ACL-2005). Ann Arbor, Michigan, pp. 65–72. http://aclweb. org/anthology/W05-0909.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Brockett</author>
<author>William B Dolan</author>
</authors>
<title>Support Vector Machines for Paraphrase Identification and Corpus Construction.</title>
<date>2005</date>
<booktitle>In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).</booktitle>
<pages>1--8</pages>
<contexts>
<context position="11044" citStr="Brockett and Dolan, 2005" startWordPosition="1698" endWordPosition="1701">ates multiple matching strategies at different levels of representation and various abstractions from the surface form to compare meanings across a range of response variations. The approach is related to the methods used in 2We use the term concept to refer to an entity or a relation between entities in a representation of the meaning of a sentence. Thus, a response generally contains multiple concepts. 3Note the incorrect presupposition in the cue provided by the instructor. 109 machine translation evaluation (e.g., Banerjee and Lavie, 2005; Lin and Och, 2004), paraphrase recognition (e.g., Brockett and Dolan, 2005; Hatzivassiloglou et al., 1999), and automatic grading (e.g., Leacock, 2004; Marin, 2004). To illustrate the general idea, consider the example from our corpus in Figure 2. Figure 2: Basic matching example We find one string identical match between the token was occurring in the target and the learner response. At the noun chunk level we can match home with his house. And finally, after pronoun resolution it is possible to match Bob Hope with he. The overall architecture of CAM is shown in Figure 3. Generally speaking, CAM compares the learner response to a stored target response and decides </context>
</contexts>
<marker>Brockett, Dolan, 2005</marker>
<rawString>Chris Brockett and William B. Dolan, 2005. Support Vector Machines for Paraphrase Identification and Corpus Construction. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005). pp. 1–8. http://aclweb.org/anthology/ I05-5001.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jill Burstein</author>
<author>Martin Chodorow</author>
</authors>
<title>Automated Essay Scoring for Nonnative English Speakers.</title>
<date>1999</date>
<booktitle>In Proceedings of a Workshop on Computer-Mediated Language Assessment and Evaluation of Natural Language Processing, Joint Symposium of the Association of Computational Linguistics (ACL-99) and the International Association of Language Learning Technologies.</booktitle>
<pages>68--75</pages>
<contexts>
<context position="27115" citStr="Burstein and Chodorow, 1999" startWordPosition="4229" endWordPosition="4232">s. ICALL system designs that do incorporate more sophisticated content assessment include FreeText (L’Haire and Faltin, 2003), the Military Language Tutor (MILT) Program (Kaplan et al., 1998), and Herr Kommissar (DeSmedt, 1995). These systems restrict both the exercise types and domains to make content assessment feasible using deeper semantic processing strategies. Beyond the ICALL domain, work in automatic grading of short answers and essays has addressed whether the students answers convey the correct meaning, but these systems focus on largely scoring rather than diagnosis (e.g., E-rater, Burstein and Chodorow, 1999), do not specifically address language learning contexts and/or are designed to work specifically with longer texts (e.g., AutoTutor, Wiemer-Hastings et al., 1999). Thus, the extent to which ICALL systems can diagnose meaning errors in language learner responses has been far from clear. As far as we are aware, no directly comparable systems performing content-assessment on related language learner data exist. The closest related system that does a similar kind of detection is the Crater system (Leacock, 2004). That system obtains 85% accuracy. However, the test set and scoring system were diff</context>
</contexts>
<marker>Burstein, Chodorow, 1999</marker>
<rawString>Jill Burstein and Martin Chodorow, 1999. Automated Essay Scoring for Nonnative English Speakers. In Proceedings of a Workshop on Computer-Mediated Language Assessment and Evaluation of Natural Language Processing, Joint Symposium of the Association of Computational Linguistics (ACL-99) and the International Association of Language Learning Technologies. pp. 68–75. http://aclweb.org/ anthology/W99-0411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Kovan der Sloot</author>
<author>Antal van den Bosch</author>
</authors>
<title>TiMBL: Tilburg MemoryBased Learner Reference Guide, ILK</title>
<date>2007</date>
<tech>Technical Report ILK 07-03.</tech>
<institution>Induction of Linguistic Knowledge Research Group Department of Communication and Information Sciences, Tilburg University, P.O. Box</institution>
<marker>Daelemans, Zavrel, der Sloot, van den Bosch, 2007</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Kovan der Sloot and Antal van den Bosch, 2007. TiMBL: Tilburg MemoryBased Learner Reference Guide, ILK Technical Report ILK 07-03. Induction of Linguistic Knowledge Research Group Department of Communication and Information Sciences, Tilburg University, P.O. Box 90153, NL-5000 LE, Tilburg, The Netherlands, version 6.0 edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William DeSmedt</author>
</authors>
<title>Herr Kommissar: An ICALL Conversation Simulator for Intermediate German. In</title>
<date>1995</date>
<booktitle>Intelligent Language Tutors: Theory Shaping Technology, Lawrence Erlbaum Associates,</booktitle>
<pages>153--174</pages>
<editor>V. Melissa Holland, Jonathan Kaplan and Michelle Sams (eds.),</editor>
<contexts>
<context position="26714" citStr="DeSmedt, 1995" startWordPosition="4172" endWordPosition="4173">ate variation, is a reasonable one. The recently developed TAGARELA system for learners of Portuguese (Amaral and Meurers, 2006; Amaral, 2007) lifts some of the restrictions on exercise types, while relying on shallow semantic processing. Using strategies inspired by our work, TAGARELA incorporates simple content assessment for evaluating 113 learner responses in short-answer questions. ICALL system designs that do incorporate more sophisticated content assessment include FreeText (L’Haire and Faltin, 2003), the Military Language Tutor (MILT) Program (Kaplan et al., 1998), and Herr Kommissar (DeSmedt, 1995). These systems restrict both the exercise types and domains to make content assessment feasible using deeper semantic processing strategies. Beyond the ICALL domain, work in automatic grading of short answers and essays has addressed whether the students answers convey the correct meaning, but these systems focus on largely scoring rather than diagnosis (e.g., E-rater, Burstein and Chodorow, 1999), do not specifically address language learning contexts and/or are designed to work specifically with longer texts (e.g., AutoTutor, Wiemer-Hastings et al., 1999). Thus, the extent to which ICALL sy</context>
</contexts>
<marker>DeSmedt, 1995</marker>
<rawString>William DeSmedt, 1995. Herr Kommissar: An ICALL Conversation Simulator for Intermediate German. In V. Melissa Holland, Jonathan Kaplan and Michelle Sams (eds.), Intelligent Language Tutors: Theory Shaping Technology, Lawrence Erlbaum Associates, pp. 153–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Finch</author>
</authors>
<title>Young-Sook Hwang and Eiichiro Sumita,</title>
<date>2005</date>
<booktitle>In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).</booktitle>
<pages>17--24</pages>
<marker>Finch, 2005</marker>
<rawString>Andrew Finch, Young-Sook Hwang and Eiichiro Sumita, 2005. Using Machine Translation Evaluation Techniques to Determine Sentence-level Semantic Equivalence. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005). pp. 17–24. http: //aclweb.org/anthology/I05-5003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Halliday</author>
</authors>
<date>1967</date>
<journal>Journal of Linguistics,</journal>
<booktitle>Notes on Transitivity and Theme in English. Part</booktitle>
<volume>1</volume>
<pages>199--244</pages>
<contexts>
<context position="13148" citStr="Halliday, 1967" startWordPosition="2036" endWordPosition="2037">etermines whether linguistic analysis is required for diagnosis. Essentially, this filter identifies learner responses that were copied directly from the source text. Then, for any learner-target response pair that requires linguistic analysis, CAM assessment proceeds in three phases – Annotation, Alignment and Diagnosis. The Annotation phase uses NLP tools to enrich the learner and target responses, as well as the question text, with linguistic information, such as lemmas and part-of-speech tags. The question text is used for pronoun resolution and to eliminate concepts that are “given” (cf. Halliday, 1967, p. 204 and many others since). Here “given” information refers to concepts from the question text that are reused in the learner response. They may be necessary for forming complete sentences, but contribute no new information. For example, if the question is What is alliteration? and the response is Alliteration is the repetition of initial letters or sounds, then the concept represented by the word alliteration is given and the rest is new. For CAM, responses are neither penalized nor rewarded for containing given information. Table 1 contains an overview of the annotations and the resourc</context>
</contexts>
<marker>Halliday, 1967</marker>
<rawString>Michael Halliday, 1967. Notes on Transitivity and Theme in English. Part 1 and 2. Journal of Linguistics, 3:37– 81, 199–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Judith Klavans</author>
<author>Eleazar Eskin</author>
</authors>
<title>Detecting Text Similarity over Short Passages: Exploring Linguistic Feature Combinations via Machine Learning.</title>
<date>1999</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP’99). College</booktitle>
<pages>203--212</pages>
<location>Park, Maryland,</location>
<contexts>
<context position="11076" citStr="Hatzivassiloglou et al., 1999" startWordPosition="1702" endWordPosition="1706">ategies at different levels of representation and various abstractions from the surface form to compare meanings across a range of response variations. The approach is related to the methods used in 2We use the term concept to refer to an entity or a relation between entities in a representation of the meaning of a sentence. Thus, a response generally contains multiple concepts. 3Note the incorrect presupposition in the cue provided by the instructor. 109 machine translation evaluation (e.g., Banerjee and Lavie, 2005; Lin and Och, 2004), paraphrase recognition (e.g., Brockett and Dolan, 2005; Hatzivassiloglou et al., 1999), and automatic grading (e.g., Leacock, 2004; Marin, 2004). To illustrate the general idea, consider the example from our corpus in Figure 2. Figure 2: Basic matching example We find one string identical match between the token was occurring in the target and the learner response. At the noun chunk level we can match home with his house. And finally, after pronoun resolution it is possible to match Bob Hope with he. The overall architecture of CAM is shown in Figure 3. Generally speaking, CAM compares the learner response to a stored target response and decides whether the two responses are po</context>
<context position="17056" citStr="Hatzivassiloglou et al. (1999)" startWordPosition="2627" endWordPosition="2631"> to the test set, i.e., they relied on properties of the development set that where not shared across data sets. Given the variety of features and the many different options for combining and weighing them that might have been explored, we decided that rather than hand-tuning the rules to additional data, we would try to machine learn the best way of combining the evidence collected. We thus decided to explore machine learning, even though the set of development data for training clearly is very small. Machine learning has been used for equivalence recognition in related fields. For instance, Hatzivassiloglou et al. (1999) trained a classifier for paraphrase detection, though their performance only reached roughly 37% recall and 61% precision. In a different approach, Finch et al. (2005) found that MT evaluation techniques combined with machine learning improves equivalence recognition. They used the output of several MT evaluation approaches based on matching concepts (e.g., BLEU) as features/values for training a support vector machine (SVM) classifier. Matched concepts and unmatched 111 concepts alike were used as features for training the classifier. Tested against the Microsoft Research Paraphrase (MSRP) C</context>
</contexts>
<marker>Hatzivassiloglou, Klavans, Eskin, 1999</marker>
<rawString>Vasileios Hatzivassiloglou, Judith Klavans and Eleazar Eskin, 1999. Detecting Text Similarity over Short Passages: Exploring Linguistic Feature Combinations via Machine Learning. In Proceedings of Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP’99). College Park, Maryland, pp. 203–212. http://aclweb.org/anthology/ W99-0625.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trude Heift</author>
</authors>
<title>Intelligent Language Tutoring Systems for Grammar Practice.</title>
<date>2001</date>
<journal>Zeitschrift f¨ur Interkulturellen Fremdsprachenunterricht,</journal>
<volume>6</volume>
<issue>2</issue>
<pages>06--2</pages>
<contexts>
<context position="25659" citStr="Heift, 2001" startWordPosition="4016" endWordPosition="4017"> is both unsurprising in the decline and encouraging in the smallness of the decline. However, given the sample size and few numbers of instances of any given error in the test (and development) set, additional quantitative analysis of the diagnosis results would not be particularly meaningful. 5 Related Work The need for semantic error diagnosis in previous CALL work has been limited by the narrow range of acceptable response variation in the supported language activity types. The few ICALL systems that have been successfully integrated into real-life language teaching, such as German Tutor (Heift, 2001) and BANZAI (Nagata, 2002), also tightly control expected response variation through deliberate exercise type choices that limit acceptable responses. Content assessment in the German Tutor is performed by string matching against the stored targets. Because of the tightly controlled exercise types and lack of variation in the expected input, the assumption that any variation in a learner response is due to form error, rather than legitimate variation, is a reasonable one. The recently developed TAGARELA system for learners of Portuguese (Amaral and Meurers, 2006; Amaral, 2007) lifts some of th</context>
</contexts>
<marker>Heift, 2001</marker>
<rawString>Trude Heift, 2001. Intelligent Language Tutoring Systems for Grammar Practice. Zeitschrift f¨ur Interkulturellen Fremdsprachenunterricht, 6(2). http: //www.spz.tu-darmstadt.de/projekt_ ejournal/jg-06-2/beitrag/heift2.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl James</author>
</authors>
<title>Errors in Language Learning and Use: Exploring Error Analysis.</title>
<date>1998</date>
<publisher>Longman Publishers.</publisher>
<contexts>
<context position="8625" citStr="James (1998)" startWordPosition="1326" endWordPosition="1327">res, word order, forms, and lexical items used (e.g., famous person vs. famous people) vary from the string provided as target. Of the learner responses in the corpus, only one was string identical with the teacher-provided target and nine were identical when treated as bags-of-words. In the test set, none of the learner responses was string or bag-of-word identical with the corresponding target sentence. To classify the variation exhibited in learner responses, we developed an annotation scheme based on target modification, with the meaning error labels being adapted from those identified by James (1998) for grammatical mistakes. Target modification encodes how the learner response varies from the target, but makes the sometimes incorrect assumption that the learner is actually trying to “hit” the meaning of the target. The annotation scheme distinguishes correct answers, omissions (of relevant concepts), overinclusions (of incorrect concepts), blends (both omissions and overinclusions), and non-answers. These error types are exemplified below with examples from the corpus. In addition, the graders used the label alternate answer for responses that were correct given the question and reading </context>
</contexts>
<marker>James, 1998</marker>
<rawString>Carl James, 1998. Errors in Language Learning and Use: Exploring Error Analysis. Longman Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Kaplan</author>
<author>Mark Sobol</author>
<author>Robert Wisher</author>
<author>Robert Seidel</author>
</authors>
<title>The Military Language Tutor (MILT) Program: An Advanced Authoring System.</title>
<date>1998</date>
<journal>Computer Assisted Language Learning,</journal>
<volume>11</volume>
<issue>3</issue>
<contexts>
<context position="26678" citStr="Kaplan et al., 1998" startWordPosition="4165" endWordPosition="4168"> is due to form error, rather than legitimate variation, is a reasonable one. The recently developed TAGARELA system for learners of Portuguese (Amaral and Meurers, 2006; Amaral, 2007) lifts some of the restrictions on exercise types, while relying on shallow semantic processing. Using strategies inspired by our work, TAGARELA incorporates simple content assessment for evaluating 113 learner responses in short-answer questions. ICALL system designs that do incorporate more sophisticated content assessment include FreeText (L’Haire and Faltin, 2003), the Military Language Tutor (MILT) Program (Kaplan et al., 1998), and Herr Kommissar (DeSmedt, 1995). These systems restrict both the exercise types and domains to make content assessment feasible using deeper semantic processing strategies. Beyond the ICALL domain, work in automatic grading of short answers and essays has addressed whether the students answers convey the correct meaning, but these systems focus on largely scoring rather than diagnosis (e.g., E-rater, Burstein and Chodorow, 1999), do not specifically address language learning contexts and/or are designed to work specifically with longer texts (e.g., AutoTutor, Wiemer-Hastings et al., 1999)</context>
</contexts>
<marker>Kaplan, Sobol, Wisher, Seidel, 1998</marker>
<rawString>Jonathan Kaplan, Mark Sobol, Robert Wisher and Robert Seidel, 1998. The Military Language Tutor (MILT) Program: An Advanced Authoring System. Computer Assisted Language Learning, 11(3):265–287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Meeting of the Association for Computational Linguistics (ACL 2003).</booktitle>
<pages>423--430</pages>
<location>Sapporo,</location>
<contexts>
<context position="14498" citStr="Klein and Manning, 2003" startWordPosition="2228" endWordPosition="2231"> performance on our development corpus – other implementations could generally be substituted without changing the overall approach. Annotation Task Language Processing Tool Sentence Detection, MontyLingua (Liu, 2004) Tokenization, Lemmatization Lemmatization PC-KIMMO (Antworth, 1993) Spell Checking Edit distance (Levenshtein, 1966), SCOWL word list (Atkinson, 2004) Part-of-speech Tagging TreeTagger (Schmid, 1994) Noun Phrase Chunking CASS (Abney, 1997) Lexical Relations WordNet (Miller, 1995) Similarity Scores PMI-IR (Turney, 2001; Mihalcea et al., 2006) Dependency Relations Stanford Parser (Klein and Manning, 2003) Table 1: NLP Tools used in CAM After the Annotation phase, Alignment maps new (i.e., not given) concepts in the learner response to concepts in the target response using the annotated information. The final Diagnosis phase analyzes the alignment to determine whether the learner re110 Figure 3: Architecture of the Content Assessment Module (CAM) Output Input Annotation Alignment Diagnosis Analysis Filter Givenness Pre-Alignment Filters Punctuation Token-level Alignment Detection Classification Chunk-level Alignment Diagnosis Classification Relation-level Alignment Source Text Learner Response </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning, 2003. Accurate Unlexicalized Parsing. In Proceedings of the 41st Meeting of the Association for Computational Linguistics (ACL 2003). Sapporo, Japan, pp. 423–430. http: //aclweb.org/anthology/P03-1054.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
</authors>
<title>Scoring Free-Responses Automatically: A Case Study of a Large-Scale Assessment.</title>
<date>2004</date>
<journal>Examens,</journal>
<volume>1</volume>
<issue>3</issue>
<contexts>
<context position="11120" citStr="Leacock, 2004" startWordPosition="1711" endWordPosition="1712">tractions from the surface form to compare meanings across a range of response variations. The approach is related to the methods used in 2We use the term concept to refer to an entity or a relation between entities in a representation of the meaning of a sentence. Thus, a response generally contains multiple concepts. 3Note the incorrect presupposition in the cue provided by the instructor. 109 machine translation evaluation (e.g., Banerjee and Lavie, 2005; Lin and Och, 2004), paraphrase recognition (e.g., Brockett and Dolan, 2005; Hatzivassiloglou et al., 1999), and automatic grading (e.g., Leacock, 2004; Marin, 2004). To illustrate the general idea, consider the example from our corpus in Figure 2. Figure 2: Basic matching example We find one string identical match between the token was occurring in the target and the learner response. At the noun chunk level we can match home with his house. And finally, after pronoun resolution it is possible to match Bob Hope with he. The overall architecture of CAM is shown in Figure 3. Generally speaking, CAM compares the learner response to a stored target response and decides whether the two responses are possibly different realizations of the same se</context>
<context position="27629" citStr="Leacock, 2004" startWordPosition="4312" endWordPosition="4313"> systems focus on largely scoring rather than diagnosis (e.g., E-rater, Burstein and Chodorow, 1999), do not specifically address language learning contexts and/or are designed to work specifically with longer texts (e.g., AutoTutor, Wiemer-Hastings et al., 1999). Thus, the extent to which ICALL systems can diagnose meaning errors in language learner responses has been far from clear. As far as we are aware, no directly comparable systems performing content-assessment on related language learner data exist. The closest related system that does a similar kind of detection is the Crater system (Leacock, 2004). That system obtains 85% accuracy. However, the test set and scoring system were different, and the system was applied to responses from native English speakers. In addition, their work focused on detection of errors rather than diagnosis. So, the results are not directly comparable. Nevertheless, the CAM detection results clearly are competitive. 6 Summary After motivating the need for content assessment in ICALL, in this paper we have discussed an approach for content assessment of English language learner responses to short answer reading comprehension questions, which is worked out in det</context>
</contexts>
<marker>Leacock, 2004</marker>
<rawString>Claudia Leacock, 2004. Scoring Free-Responses Automatically: A Case Study of a Large-Scale Assessment. Examens, 1(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir I Levenshtein</author>
</authors>
<title>Binary Codes Capable of Correcting Deletions, Insertions, and Reversals.</title>
<date>1966</date>
<journal>Soviet Physics Doklady,</journal>
<volume>10</volume>
<issue>8</issue>
<contexts>
<context position="14208" citStr="Levenshtein, 1966" startWordPosition="2192" endWordPosition="2193">s new. For CAM, responses are neither penalized nor rewarded for containing given information. Table 1 contains an overview of the annotations and the resources, tools or algorithms used. The choice of the particular algorithm or implementation was primarily based on availability and performance on our development corpus – other implementations could generally be substituted without changing the overall approach. Annotation Task Language Processing Tool Sentence Detection, MontyLingua (Liu, 2004) Tokenization, Lemmatization Lemmatization PC-KIMMO (Antworth, 1993) Spell Checking Edit distance (Levenshtein, 1966), SCOWL word list (Atkinson, 2004) Part-of-speech Tagging TreeTagger (Schmid, 1994) Noun Phrase Chunking CASS (Abney, 1997) Lexical Relations WordNet (Miller, 1995) Similarity Scores PMI-IR (Turney, 2001; Mihalcea et al., 2006) Dependency Relations Stanford Parser (Klein and Manning, 2003) Table 1: NLP Tools used in CAM After the Annotation phase, Alignment maps new (i.e., not given) concepts in the learner response to concepts in the target response using the annotated information. The final Diagnosis phase analyzes the alignment to determine whether the learner re110 Figure 3: Architecture o</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir I. Levenshtein, 1966. Binary Codes Capable of Correcting Deletions, Insertions, and Reversals. Soviet Physics Doklady, 10(8):707–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S´ebastien L’Haire</author>
<author>Anne Vandeventer Faltin</author>
</authors>
<title>Error Diagnosis in the FreeText Project.</title>
<date>2003</date>
<journal>CALICO Journal,</journal>
<volume>20</volume>
<issue>3</issue>
<marker>L’Haire, Faltin, 2003</marker>
<rawString>S´ebastien L’Haire and Anne Vandeventer Faltin, 2003. Error Diagnosis in the FreeText Project. CALICO Journal, 20(3):481–495.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04).</booktitle>
<pages>605--612</pages>
<contexts>
<context position="10988" citStr="Lin and Och, 2004" startWordPosition="1690" endWordPosition="1693">inton and Lewinsky. 3 Method The CAM design integrates multiple matching strategies at different levels of representation and various abstractions from the surface form to compare meanings across a range of response variations. The approach is related to the methods used in 2We use the term concept to refer to an entity or a relation between entities in a representation of the meaning of a sentence. Thus, a response generally contains multiple concepts. 3Note the incorrect presupposition in the cue provided by the instructor. 109 machine translation evaluation (e.g., Banerjee and Lavie, 2005; Lin and Och, 2004), paraphrase recognition (e.g., Brockett and Dolan, 2005; Hatzivassiloglou et al., 1999), and automatic grading (e.g., Leacock, 2004; Marin, 2004). To illustrate the general idea, consider the example from our corpus in Figure 2. Figure 2: Basic matching example We find one string identical match between the token was occurring in the target and the learner response. At the noun chunk level we can match home with his house. And finally, after pronoun resolution it is possible to match Bob Hope with he. The overall architecture of CAM is shown in Figure 3. Generally speaking, CAM compares the l</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och, 2004. Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04). pp. 605–612. http://aclweb.org/ anthology/P04-1077.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Liu</author>
</authors>
<title>MontyLingua: An End-toEnd Natural Language Processor with Common Sense.</title>
<date>2004</date>
<note>http://web.media.mit.edu/˜hugo/ montylingua, accessed</note>
<contexts>
<context position="14091" citStr="Liu, 2004" startWordPosition="2180" endWordPosition="2181">n of initial letters or sounds, then the concept represented by the word alliteration is given and the rest is new. For CAM, responses are neither penalized nor rewarded for containing given information. Table 1 contains an overview of the annotations and the resources, tools or algorithms used. The choice of the particular algorithm or implementation was primarily based on availability and performance on our development corpus – other implementations could generally be substituted without changing the overall approach. Annotation Task Language Processing Tool Sentence Detection, MontyLingua (Liu, 2004) Tokenization, Lemmatization Lemmatization PC-KIMMO (Antworth, 1993) Spell Checking Edit distance (Levenshtein, 1966), SCOWL word list (Atkinson, 2004) Part-of-speech Tagging TreeTagger (Schmid, 1994) Noun Phrase Chunking CASS (Abney, 1997) Lexical Relations WordNet (Miller, 1995) Similarity Scores PMI-IR (Turney, 2001; Mihalcea et al., 2006) Dependency Relations Stanford Parser (Klein and Manning, 2003) Table 1: NLP Tools used in CAM After the Annotation phase, Alignment maps new (i.e., not given) concepts in the learner response to concepts in the target response using the annotated informat</context>
</contexts>
<marker>Liu, 2004</marker>
<rawString>Hugo Liu, 2004. MontyLingua: An End-toEnd Natural Language Processor with Common Sense. http://web.media.mit.edu/˜hugo/ montylingua, accessed October 30, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana Rosario P´erez Mar´ın</author>
</authors>
<title>Automatic Evaluation of Users’ Short Essays by Using Statistical and Shallow Natural Language Processing Techniques. Master’s thesis, Universidad Aut´onoma de Madrid.</title>
<date>2004</date>
<note>http: //www.ii.uam.es/˜dperez/tea.pdf.</note>
<marker>Mar´ın, 2004</marker>
<rawString>Diana Rosario P´erez Mar´ın, 2004. Automatic Evaluation of Users’ Short Essays by Using Statistical and Shallow Natural Language Processing Techniques. Master’s thesis, Universidad Aut´onoma de Madrid. http: //www.ii.uam.es/˜dperez/tea.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and Knowledge-based Measures of Text Semantic Similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence. American Association for Artificial Intelligence</booktitle>
<volume>21</volume>
<issue>1</issue>
<pages>775--780</pages>
<publisher>(AAAI) Press,</publisher>
<location>Menlo Park, CA,</location>
<contexts>
<context position="14435" citStr="Mihalcea et al., 2006" startWordPosition="2220" endWordPosition="2223">thm or implementation was primarily based on availability and performance on our development corpus – other implementations could generally be substituted without changing the overall approach. Annotation Task Language Processing Tool Sentence Detection, MontyLingua (Liu, 2004) Tokenization, Lemmatization Lemmatization PC-KIMMO (Antworth, 1993) Spell Checking Edit distance (Levenshtein, 1966), SCOWL word list (Atkinson, 2004) Part-of-speech Tagging TreeTagger (Schmid, 1994) Noun Phrase Chunking CASS (Abney, 1997) Lexical Relations WordNet (Miller, 1995) Similarity Scores PMI-IR (Turney, 2001; Mihalcea et al., 2006) Dependency Relations Stanford Parser (Klein and Manning, 2003) Table 1: NLP Tools used in CAM After the Annotation phase, Alignment maps new (i.e., not given) concepts in the learner response to concepts in the target response using the annotated information. The final Diagnosis phase analyzes the alignment to determine whether the learner re110 Figure 3: Architecture of the Content Assessment Module (CAM) Output Input Annotation Alignment Diagnosis Analysis Filter Givenness Pre-Alignment Filters Punctuation Token-level Alignment Detection Classification Chunk-level Alignment Diagnosis Classi</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley and Carlo Strapparava, 2006. Corpus-based and Knowledge-based Measures of Text Semantic Similarity. In Proceedings of the National Conference on Artificial Intelligence. American Association for Artificial Intelligence (AAAI) Press, Menlo Park, CA, volume 21(1), pp. 775–780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
</authors>
<title>WordNet: A Lexical Database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="14372" citStr="Miller, 1995" startWordPosition="2213" endWordPosition="2214">r algorithms used. The choice of the particular algorithm or implementation was primarily based on availability and performance on our development corpus – other implementations could generally be substituted without changing the overall approach. Annotation Task Language Processing Tool Sentence Detection, MontyLingua (Liu, 2004) Tokenization, Lemmatization Lemmatization PC-KIMMO (Antworth, 1993) Spell Checking Edit distance (Levenshtein, 1966), SCOWL word list (Atkinson, 2004) Part-of-speech Tagging TreeTagger (Schmid, 1994) Noun Phrase Chunking CASS (Abney, 1997) Lexical Relations WordNet (Miller, 1995) Similarity Scores PMI-IR (Turney, 2001; Mihalcea et al., 2006) Dependency Relations Stanford Parser (Klein and Manning, 2003) Table 1: NLP Tools used in CAM After the Annotation phase, Alignment maps new (i.e., not given) concepts in the learner response to concepts in the target response using the annotated information. The final Diagnosis phase analyzes the alignment to determine whether the learner re110 Figure 3: Architecture of the Content Assessment Module (CAM) Output Input Annotation Alignment Diagnosis Analysis Filter Givenness Pre-Alignment Filters Punctuation Token-level Alignment </context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George Miller, 1995. WordNet: A Lexical Database for English. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noriko Nagata</author>
</authors>
<title>BANZAI: An Application of Natural Language Processing to Web-Based Language Learning.</title>
<date>2002</date>
<journal>CALICO Journal,</journal>
<volume>19</volume>
<issue>3</issue>
<contexts>
<context position="25685" citStr="Nagata, 2002" startWordPosition="4020" endWordPosition="4021">the decline and encouraging in the smallness of the decline. However, given the sample size and few numbers of instances of any given error in the test (and development) set, additional quantitative analysis of the diagnosis results would not be particularly meaningful. 5 Related Work The need for semantic error diagnosis in previous CALL work has been limited by the narrow range of acceptable response variation in the supported language activity types. The few ICALL systems that have been successfully integrated into real-life language teaching, such as German Tutor (Heift, 2001) and BANZAI (Nagata, 2002), also tightly control expected response variation through deliberate exercise type choices that limit acceptable responses. Content assessment in the German Tutor is performed by string matching against the stored targets. Because of the tightly controlled exercise types and lack of variation in the expected input, the assumption that any variation in a learner response is due to form error, rather than legitimate variation, is a reasonable one. The recently developed TAGARELA system for learners of Portuguese (Amaral and Meurers, 2006; Amaral, 2007) lifts some of the restrictions on exercise</context>
</contexts>
<marker>Nagata, 2002</marker>
<rawString>Noriko Nagata, 2002. BANZAI: An Application of Natural Language Processing to Web-Based Language Learning. CALICO Journal, 19(3):583–599.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic Part-of-Speech Tagging Using Decision Trees.</title>
<date>1994</date>
<booktitle>In International Conference on New Methods in Language Processing.</booktitle>
<pages>44--49</pages>
<location>Manchester, United Kingdom,</location>
<contexts>
<context position="14291" citStr="Schmid, 1994" startWordPosition="2202" endWordPosition="2203">ation. Table 1 contains an overview of the annotations and the resources, tools or algorithms used. The choice of the particular algorithm or implementation was primarily based on availability and performance on our development corpus – other implementations could generally be substituted without changing the overall approach. Annotation Task Language Processing Tool Sentence Detection, MontyLingua (Liu, 2004) Tokenization, Lemmatization Lemmatization PC-KIMMO (Antworth, 1993) Spell Checking Edit distance (Levenshtein, 1966), SCOWL word list (Atkinson, 2004) Part-of-speech Tagging TreeTagger (Schmid, 1994) Noun Phrase Chunking CASS (Abney, 1997) Lexical Relations WordNet (Miller, 1995) Similarity Scores PMI-IR (Turney, 2001; Mihalcea et al., 2006) Dependency Relations Stanford Parser (Klein and Manning, 2003) Table 1: NLP Tools used in CAM After the Annotation phase, Alignment maps new (i.e., not given) concepts in the learner response to concepts in the target response using the annotated information. The final Diagnosis phase analyzes the alignment to determine whether the learner re110 Figure 3: Architecture of the Content Assessment Module (CAM) Output Input Annotation Alignment Diagnosis A</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid, 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. In International Conference on New Methods in Language Processing. Manchester, United Kingdom, pp. 44–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Mining the Web for Synonyms: PMIIR Versus LSA on TOEFL.</title>
<date>2001</date>
<booktitle>In Proceedings of the Twelfth European Conference on Machine Learning (ECML-2001).</booktitle>
<pages>491--502</pages>
<location>Freiburg, Germany,</location>
<contexts>
<context position="14411" citStr="Turney, 2001" startWordPosition="2218" endWordPosition="2219">ticular algorithm or implementation was primarily based on availability and performance on our development corpus – other implementations could generally be substituted without changing the overall approach. Annotation Task Language Processing Tool Sentence Detection, MontyLingua (Liu, 2004) Tokenization, Lemmatization Lemmatization PC-KIMMO (Antworth, 1993) Spell Checking Edit distance (Levenshtein, 1966), SCOWL word list (Atkinson, 2004) Part-of-speech Tagging TreeTagger (Schmid, 1994) Noun Phrase Chunking CASS (Abney, 1997) Lexical Relations WordNet (Miller, 1995) Similarity Scores PMI-IR (Turney, 2001; Mihalcea et al., 2006) Dependency Relations Stanford Parser (Klein and Manning, 2003) Table 1: NLP Tools used in CAM After the Annotation phase, Alignment maps new (i.e., not given) concepts in the learner response to concepts in the target response using the annotated information. The final Diagnosis phase analyzes the alignment to determine whether the learner re110 Figure 3: Architecture of the Content Assessment Module (CAM) Output Input Annotation Alignment Diagnosis Analysis Filter Givenness Pre-Alignment Filters Punctuation Token-level Alignment Detection Classification Chunk-level Al</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter Turney, 2001. Mining the Web for Synonyms: PMIIR Versus LSA on TOEFL. In Proceedings of the Twelfth European Conference on Machine Learning (ECML-2001). Freiburg, Germany, pp. 491–502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Wiemer-Hastings</author>
</authors>
<title>Katja Wiemer-Hastings and Arthur Graesser,</title>
<date>1999</date>
<booktitle>In Susanne Lajoie and Martial Vivet (eds.), Artificial Intelligence in Education,</booktitle>
<pages>535--542</pages>
<publisher>IOS Press,</publisher>
<marker>Wiemer-Hastings, 1999</marker>
<rawString>Peter Wiemer-Hastings, Katja Wiemer-Hastings and Arthur Graesser, 1999. Improving an Intelligent Tutor’s Comprehension of Students with Latent Semantic Analysis. In Susanne Lajoie and Martial Vivet (eds.), Artificial Intelligence in Education, IOS Press, pp. 535–542.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>