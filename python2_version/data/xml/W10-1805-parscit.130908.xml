<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000034">
<title confidence="0.99653">
Consistency Checking for Treebank Alignment
</title>
<author confidence="0.995668">
Markus Dickinson Yvonne Samuelsson
</author>
<affiliation confidence="0.999881">
Indiana University Stockholm University
</affiliation>
<email confidence="0.990238">
md7@indiana.edu yvonne.samuelsson@ling.su.se
</email>
<sectionHeader confidence="0.993648" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998180625">
This paper explores ways to detect errors
in aligned corpora, using very little tech-
nology. In the first method, applicable
to any aligned corpus, we consider align-
ment as a string-to-string mapping. Treat-
ing the target string as a label, we ex-
amine each source string to find incon-
sistencies in alignment. Despite setting
up the problem on a par with grammat-
ical annotation, we demonstrate crucial
differences in sorting errors from legiti-
mate variations. The second method ex-
amines phrase nodes which are predicted
to be aligned, based on the alignment of
their yields. Both methods are effective in
complementary ways.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999949684210527">
Parallel corpora—texts and their translations—
have become essential in the development of
machine translation (MT) systems. Alignment
quality is crucial to these corpora; as Tiede-
mann (2003) states, “[t]he most important fea-
ture of texts and their translations is the corre-
spondence between source and target segments”
(p. 2). While being useful for translation studies
and foreign language pedagogy (see, e.g., Botley
et al., 2000; McEnery and Wilson, 1996), PARAL-
LEL TREEBANKS—syntactically-annotated paral-
lel corpora—offer additional useful information
for machine translation, cross-language infor-
mation retrieval, and word-sense disambiguation
(see, e.g., Tiedemann, 2003),
While high-quality alignments are desirable,
even gold standard annotation can contain anno-
tation errors. For other forms of linguistic an-
notation, the presence of errors has been shown
to create various problems, from unreliable train-
ing and evaluation of NLP technology (e.g., Padro
and Marquez, 1998) to low precision and recall
of queries for already rare linguistic phenomena
(e.g., Meurers and M¨uller, 2008). Even a small
number of errors can have a significant impact
on the uses of linguistic annotation, e.g., chang-
ing the assessment of parsers (e.g., Habash et al.,
2007). One could remove potentially unfavorable
sentence pairs when training a statistical MT sys-
tem, to avoid incorrect word alignments (Okita,
2009), but this removes all relevant data from
those sentences and does not help evaluation.
We thus focus on detecting errors in the anno-
tation of alignments. Annotation error detection
has been explored for part-of-speech (POS) anno-
tation (e.g., Loftsson, 2009) and syntactic anno-
tation (e.g., Ule and Simov, 2004; Dickinson and
Meurers, 2005), but there have been few, if any, at-
tempts to develop general approaches to error de-
tection for aligned corpora. Alignments are differ-
ent in nature, as the annotation does not introduce
abstract categories such as POS, but relies upon
defining translation units with equivalent mean-
ings.
We use the idea that variation in annotation can
indicate errors (section 2), for consistency check-
ing of alignments, as detailed in section 3. In sec-
tion 4, we outline language-independent heuristics
to sort true ambiguities from errors, and evaluate
them on a parallel treebank in section 5. In sec-
tion 6 we turn to a complementary method, ex-
ploiting compositional properties of aligned tree-
banks, to align more nodes. The methods are sim-
ple, effective, and applicable to any aligned tree-
bank. As far as we know, this is the first attempt to
thoroughly investigate and empirically verify er-
ror detection methods for aligned corpora.
</bodyText>
<page confidence="0.988463">
38
</page>
<note confidence="0.970527">
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 38–46,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.981859" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.997299">
2.1 Variation N-gram Method
</subsectionHeader>
<bodyText confidence="0.99995308">
As a starting point for an error detection method
for aligned corpora, we use the variation n-gram
approach for syntactic annotation (Dickinson and
Meurers, 2003, 2005). The approach is based on
detecting strings which occur multiple times in
the corpus with varying annotation, the so-called
VARIATION NUCLEI. The nucleus with repeated
surrounding context is referred to as a VARIATION
n-GRAM. The basic heuristic for detecting anno-
tation errors requires one word of recurring con-
text on each side of the nucleus, which is suffi-
cient for detecting errors in grammatical annota-
tion with high precision (Dickinson, 2008).
The approach detects bracketing and labeling
errors in constituency annotation. For example,
the variation nucleus last month occurs once in
the Penn Treebank (Taylor et al., 2003) with the
label NP and once as a non-constituent, handled
through a special label NIL. As a labeling error
example, next Tuesday occurs three times, twice
as NP and once as PP (Dickinson and Meur-
ers, 2003). The method works for discontinuous
constituency annotation (Dickinson and Meurers,
2005), allowing one to apply it to alignments,
which may span over several words.
</bodyText>
<subsectionHeader confidence="0.991704">
2.2 Parallel Treebank Consistency Checking
</subsectionHeader>
<bodyText confidence="0.999941882352941">
For the experiments in this paper we will use
the SMULTRON parallel treebank of Swedish,
German, and English (Gustafson-ˇCapkov´a et al.,
2007), containing syntactic annotation and align-
ment on both word and phrase levels.1 Addition-
ally, alignments are marked as showing either an
EXACT or a FUZZY (approximate) equivalence.
Corpora with alignments often have under-
gone some error-checking. Previous consistency
checks for SMULTRON, for example, consisted
of running one script for comparing differences
in length between the source and target language
items, and one script for comparing alignment
labels, to detect variation between EXACT and
FUZZY links. For example, the pair and (English)
and samt (German, ‘together with’) had 20 FUZZY
matches and 1 (erroneous) EXACT match. Such
</bodyText>
<footnote confidence="0.990757">
1SMULTRON is freely available for research purposes, see
http://www.cl.uzh.ch/kitt/smultron/.
</footnote>
<bodyText confidence="0.995465636363636">
methods are limited, in that they do not, e.g., han-
dle missing alignments.
The TreeAligner2 tool for annotating and
querying aligned parallel treebanks (Volk et al.,
2007) employs its own consistency checking, re-
cently developed by Torsten Marek. One method
uses 2 x 2 contingency tables over words, look-
ing, e.g., at the word-word or POS-POS combina-
tions, pinpointing anomalous translation equiva-
lents. While potentially effective, this does not ad-
dress the use of alignments in context, i.e., when
we might expect to see a rare translation.
A second, more treebank-specific method
checks for so-called branch link locality: if two
nodes are aligned, any node dominating one of
them can only be aligned to a node dominating the
other one. While this constraint can flag erroneous
links, it too does not address missing alignments.
The two methods we propose in this paper address
these limitations and can be used to complement
this work. Furthermore, these methods have not
been evaluated, whereas we evaluate our methods.
</bodyText>
<sectionHeader confidence="0.944176" genericHeader="method">
3 Consistency of Alignment
</sectionHeader>
<bodyText confidence="0.999944166666667">
To adapt the variation n-gram method and deter-
mine whether strings in a corpus are consistently
aligned, we must: 1) define the units of data we
expect to be consistently annotated (this section),
and 2) define which information effectively iden-
tifies the erroneous cases (section 4).
</bodyText>
<subsectionHeader confidence="0.999944">
3.1 Units of Data
</subsectionHeader>
<bodyText confidence="0.9999643125">
Alignment relates words in a source language and
words in a target language, potentially mediated
by phrase nodes. Following the variation n-gram
method, we define the units of data, i.e., the vari-
ation nuclei, as strings. Then, we break the prob-
lem into two different source-to-target mappings,
mapping a source variation nucleus to a target lan-
guage label. With a German-English aligned cor-
pus, for example, we look for the consistency of
aligning German words to their English counter-
parts and separately examine the consistency of
aligning English words with their German “la-
bels.” Because a translated word can be used in
different parts of a sentence, we also normalize all
target labels into lower-case, preventing variation
between, e.g., the and The.
</bodyText>
<footnote confidence="0.985427">
2http://www.cl.uzh.ch/kitt/treealigner
</footnote>
<page confidence="0.999097">
39
</page>
<figure confidence="0.999790893939394">
dense
with
SB
NP
MNR
PP
NK
HD
HD
NK
AP
NP
HD
HD
dichte
ADJA
Kränze
NN
Osterglocken
NN
von
APPR
IN
NNS
IN
NNS
JJ
clusters
of
daffodils
NP NP
PP
NP
PP
DT
DT
IN
NN
NN
The
in
mirror
the
girl
SB
NP
NK HD
MNR
PP
HD
NK
NP
HD
Das
ART
Mädchen
NN
Spiegel
NN
im
APPRART
NP NP
PP
LOC
NP
PD
</figure>
<figureCaption confidence="0.990602">
Figure 2: The word someone aligned as a phrase
</figureCaption>
<figure confidence="0.977498724137931">
P
on the left, but not a phrase by itself on the right.
VP
CLR
NK
steNN
sse
RB
N else
someone
ADJA
NP
HD
V jemanden
NK P
PIS
NN
someone
IN ADVP RB
NPA
NP
NK
eine andere
KART PIS
D
ADVP
edge
NP N
PRD
</figure>
<figureCaption confidence="0.990702">
Figure 1: Word and phrase alignments span the
same string on the left, but not on the right.
</figureCaption>
<bodyText confidence="0.986712566666667">
SBJ
Although alignment maps strings to strings for
this method, complications arise when mediated
by phrase nodes: if a phrase node spans over only
one word, it could have two distinct mappings,
one as a word and one as a phrase, which may
or may not result in the same yield. Figure 1 il-
lustrates this. On the left side, Osterglocken is
aligned to daffodils at the word level, and the same
string is aligned on the phrase level (NP to NP).
In contrast, on the right side, the word Spiegel is
aligned to the word mirror, while at the phrase
level, Spiegel (NP) is aligned to the mirror (NP).
As word and phrase level strings can behave dif-
ferently, we split error detection into word-level
and phrase-level methods, to avoid unnecessary
variation. By splitting the problem first into differ-
ent source-to-target mappings and then into words
and phrases, we do not have to change the under-
lying way of finding consistency.
Multiple Alignment The mapping between
source strings and target labels handles n-to-m
alignments. For example, if G¨arten maps to the
gardens, the and gardens is considered one string.
Likewise, in the opposite direction, the gardens
maps as a unit to G¨arten, even if discontinuous.
Unary Branches With syntactic annotation,
unary branches present a potential difficulty, in
that a single string could have more than one la-
bel, violating the assumption that the string-to-
</bodyText>
<equation confidence="0.551462777777778">
label mapping is a function. For example, in
PP
Penn Treebank-style annotation, an NP node can
S CLR
dominate a QP (quantifierVphrase) node via a
unary branch. Thus, an annotator could (likely
S
erroneously) assign different alignments to each
S
</equation>
<bodyText confidence="0.998222">
phrasal node, one for the NP and one for the QP,
resulting in different target labels.
</bodyText>
<sectionHeader confidence="0.393569" genericHeader="method">
VP
</sectionHeader>
<bodyText confidence="0.9999774">
We handle all the (source) unary branch align-
ments as a conjunction of possibilities, ordered
from top to bottom. Just as the syntactic struc-
ture can be relabeled as NP/QP (Dickinson and
Meurers, 2003), we can relabel a string as, e.g.,
</bodyText>
<subsectionHeader confidence="0.931445">
S
</subsectionHeader>
<bodyText confidence="0.9989555">
the man/man. If different unary nodes result in the
same string (the man/the man), we combine them
</bodyText>
<sectionHeader confidence="0.501484" genericHeader="method">
VROOT
</sectionHeader>
<bodyText confidence="0.994022">
(the man). Note that unary branches are unprob-
lematic in the target language since they always
yield the same string, i.e., are still one label.
</bodyText>
<subsectionHeader confidence="0.999593">
3.2 Consistency and Completeness
</subsectionHeader>
<bodyText confidence="0.999990785714286">
Error detection for syntactic annotation finds in-
consistencies in constituent labeling (e.g., NP vs.
QP) and inconsistencies in bracketing (e.g., NP vs.
NIL). Likewise, we can distinguish inconsistency
in labeling (different translations) from inconsis-
tency in alignment (aligned/unaligned). Detecting
inconsistency in alignment deals with the com-
pleteness of the annotation, by using the label NIL
for unaligned strings.
We use the method from Dickinson and Meur-
ers (2005) to generate NILs, but using NIL for un-
aligned strings is too coarse-grained for phrase-
level alignment. A string mapping to NIL might
be a phrase which has no alignment, or it might
not be a phrase and thus could not possibly have
an alignment. Thus, we create NIL-C as a new
label, indicating a constituent with no alignment,
differing from NIL strings which do not even form
a phrase. For example, on the left side of Fig-
ure 2, the string someone aligns to jemanden on
the phrase level. On the right side of Figure 2,
the string someone by itself does not constitute a
phrase (even though the alignment in this instance
is correct) and is labeled NIL. If there were in-
stances of someone as an NP with no alignment,
this would be NIL-C. NIL-C cases seem to be use-
ful for inconsistency detection, as we expect con-
sistency for items annotated as a phrase.
</bodyText>
<subsectionHeader confidence="0.999624">
3.3 Alignment Types
</subsectionHeader>
<bodyText confidence="0.999979958333333">
Aligned corpora often specify additional informa-
tion about each alignment, e.g., a “sure” or “pos-
sible” alignment (Och and Ney, 2003). In SMUL-
TRON, for instance, an EXACT alignment means
that the strings are considered direct translation
equivalents outside the current sentence context,
whereas a FUZZY one is not as strict an equiva-
lent. For example, something in English EXACT-
aligns with etwas in German. However, if some-
thing and irgend etwas (‘something or other’) are
constituents on the phrase level, &lt;something, ir-
gend etwas&gt; is an acceptable alignment (since the
corpus aligns as much as possible), but is FUZZY.
Since EXACT alignments are the ones we expect
to consistently align with the same string across
the corpus, we attach information about the align-
ment type to each corpus position. This can be
used to filter out variations involving, e.g., FUZZY
alignments (see section 4.4). When multiple
alignments form a single variation nucleus, there
could be different types of alignment for each link,
e.g., dog EXACT-aligning and the FUZZY-aligning
with Hund. We did not observe this, but one can
easily allow for a mixed type (EXACT-FUZZY).
</bodyText>
<subsectionHeader confidence="0.939146">
3.4 Algorithm
</subsectionHeader>
<bodyText confidence="0.9992835">
The algorithm first splits the data into appropriate
units (SL=source language, TL=target language):
</bodyText>
<listItem confidence="0.984989111111111">
1. Divide the alignments into two SL-to-TL mappings.
2. Divide each SL-to-TL alignment set into word-level
and phrase-level alignments.
For each of the four sets of alignments:
1. Map each string in SL with an alignment to a label
• Label = &lt;(lower-cased) TL translation, EX-
ACT|FUZZY|EXACT-FUZZY&gt;
• (For phrases) Constituent phrases with no align-
ment are given the special label, NIL-C.
• (For phrases) Constituent phrases which are
unary branches are given a single, normalized la-
bel representing all target strings.
2. Generate NIL alignments for string tokens which occur
in SL, but have no alignment to TL, using the method
described in Dickinson and Meurers (2005).
3. Find SL strings which have variation in labeling.
4. Filter the variations from step 3, based on likelihood of
being an error (see section 4).
</listItem>
<sectionHeader confidence="0.996198" genericHeader="method">
4 Identifying Inconsistent Alignments
</sectionHeader>
<bodyText confidence="0.9999775">
As words and phrases have acceptable variants for
translation, the method in section 3 will lead to
detecting acceptable variations. We use several
heuristics to filter the set of variations.
</bodyText>
<subsectionHeader confidence="0.93932">
4.1 NIL-only Variation
</subsectionHeader>
<bodyText confidence="0.99987725">
As discussed in section 3.2, we use the label NIL-
C to refer to syntactic constituents which do not
receive an alignment, while NIL refers to non-
constituent strings without an alignment. A string
which varies between NIL and NIL-C, then, is not
really varying in its alignment—i.e., it is always
unaligned. We thus remove cases varying only be-
tween NIL and NIL-C.
</bodyText>
<subsectionHeader confidence="0.966793">
4.2 Context-based Filtering
</subsectionHeader>
<bodyText confidence="0.999995823529412">
The variation n-gram method has generally relied
upon immediate lexical context around the vari-
ation nucleus, in order to sort errors from ambi-
guities (Dickinson, 2008). However, while use-
ful for grammatical annotation, it is not clear how
useful the surrounding context is for translation
tasks, given the wide range of possible translations
for the same context. Further, requiring identical
context around source words is very strict, leading
to sparse data problems, and it ignores alignment-
specific information (see sections 4.3 and 4.4).
We test three different notions of context.
Matching the variation n-gram method, we first
employ a filter identifying those nuclei which
share the “shortest” identical context, i.e., one
word of context on every side of a nucleus. Sec-
ondly, we relax this to require only one word of
</bodyText>
<page confidence="0.997291">
41
</page>
<bodyText confidence="0.9998665">
context, on either the left or right side. Finally, we
require no identical context in the source language
and rely only on other filters. For example, with
the nucleus come in the context Where does the
world come from, the first notion requires world
come from to recur, the second either world come
or come from, and the third only requires that the
nucleus itself recur (come).
</bodyText>
<subsectionHeader confidence="0.998802">
4.3 Target Language Filtering
</subsectionHeader>
<bodyText confidence="0.99996432">
Because translation is open-ended, there can be
different translations in a corpus. We want to
filter out cases where there is variation in align-
ment stemming from multiple translation possibil-
ities. We implement a TARGET LANGUAGE FIL-
TER, which keeps only the variations where the
target words are present in the same sentence. If
word x is sometimes aligned to y1 and sometimes
to y2, and word y2 occurs in at least one sentence
where y1 is the chosen target, then we keep the
variation. If y1 and y2 do not occur in any of the
same sentences, we remove the variation: given
the translations, there is no possibility of having
the same alignment.
This also works for NIL labels, given sentence
alignments.3 For NILs, the check is in only one
direction: the aligned sentence must contain the
target string used as the label elsewhere in the cor-
pus. For instance, the word All aligns once with
alle and twice with NIL. We check the two NIL
cases to see whether one of them contains alle.
Sentences which are completely unaligned lead
to NILs for every word and phrase, and we always
keep the variation. In practice, the issue of having
no alignment should be handled separately.
</bodyText>
<subsectionHeader confidence="0.9991">
4.4 Alignment Type Filtering
</subsectionHeader>
<bodyText confidence="0.9999692">
A final filter relies on alignment type informa-
tion. Namely, the FUZZY label already indicates
that the alignment is not perfect, i.e., not nec-
essarily applicable in other contexts. For exam-
ple, the English word dead FUZZY-aligns with the
German verschwunden (‘gone, missing’), the best
translation in its context. In another part of the
corpus, dead EXACT-aligns with leblosen (‘life-
less’). While this is variation between verschwun-
den and leblosen, the presence of the FUZZY label
</bodyText>
<footnote confidence="0.94224">
3In SMULTRON, sentence alignments are not given di-
rectly, but can be deduced from the set of word alignments.
</footnote>
<table confidence="0.999450857142857">
word phrase
all 540 251
oneword 340 182
shortest 96 21
all-TL 194 140
oneword-TL 130 94
shortest-TL 30 16
</table>
<tableCaption confidence="0.999754">
Table 1: Number of variations across contexts
</tableCaption>
<bodyText confidence="0.80529475">
alerts us to the fact that it should vary with another
word. The ALIGNMENT TYPE FILTER removes
cases varying between one EXACT label and one
or more FUZZY labels.
</bodyText>
<sectionHeader confidence="0.998607" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.99998371875">
Evaluation was done for English to German on
half of SMULTRON (the part taken from the novel
Sophie’s World), with approximately 7500 words
from each language and 7600 alignments (roughly
4800 word-level and 2800 phrase-level). Basic
statistics are in Table 1. We filter based on the
target language (TL) and provide three different
contextual definitions: no context, i.e., all varia-
tions (all); one word of context on the left or right
(oneword); and one word of context on the left and
right, i.e., the shortest surrounding context (short-
est). The filters reduce the number of variations,
with a dramatic loss for the shortest contexts.
A main question concerns the impact of the fil-
tering conditions on error detection. To gauge this,
we randomly selected 50 (all) variations for the
word level and 50 for the phrase level, each corre-
sponding to just under 400 corpus instances. The
variations were checked manually to see which
were true variations and which were errors.
We report the effect of different filters on preci-
sion and recall in Table 2, where recall is with re-
spect to the all condition.4 Adding too much lexi-
cal context in the source language (i.e., the short-
est conditions) results in too low a recall to be
practically effective. Using one word of context
on either side has higher recall, but the precision
is no better than using no source language con-
text at all. What seems to be most effective is to
only use the target language filter (all-TL). Here,
we find higher precision—higher than any source
language filter—and the recall is respectable.
</bodyText>
<footnote confidence="0.931834">
4Future work should test for recall of all alignment errors,
by first manually checking a small section of the corpus.
</footnote>
<page confidence="0.9994">
42
</page>
<tableCaption confidence="0.990748">
Table 2: Error precision and recall
</tableCaption>
<figure confidence="0.998370785714286">
Word
Cases Errors P R
Phrase
Cases Errors P R
all
30%
100%
34%
100%
oneword
shortest
all-TL
oneword-TL
shortest-TL
50 17
33
8
20
15
2
12
2
11
6
1
36%
25%
71%
12%
33
4
8
1
24%
25%
40%
55%
50%
65%
35%
6%
27
14
3
12
7
1
44%
50%
33%
50 15
53%
7%
80%
47%
7%
</figure>
<bodyText confidence="0.993295166666667">
TL filter An advantage of the target language
filter is its ability to handle lexical (e.g., case) vari-
ations. One example of this is the English phrase
a dog, which varies between German einem Hund
(dative singular), einen Hund (accusative singu-
lar) and Hunde (accusative plural). Similar to us-
ing lower-case labels, one could map strings to
canonical forms. However, the target language
filter naturally eliminates such unwanted varia-
tion, without any language-specific information,
because the other forms do not appear across sen-
tences.
Several of the variations which the target lan-
guage filter incorrectly removes would, once the
error is fixed, still have variation. As an example,
consider cat, which varies between Katze (5 to-
kens) and NIL (2 tokens). In one of the NIL cases,
the word needs to be FUZZY-aligned with the Ger-
man Tigerkatze. The variation points out the error,
but there would still be variation (between Katze,
Tigerkatze, and NIL) after correction. This shows
the limitation of the heuristic in identifying the re-
quired non-exact alignments.
Another case the filter misses is the variation
nucleus heard, which varies between geh¨ort (2 to-
kens) and h¨oren (1 token). In this case, one of the
instances of &lt;heard, geh¨ort&gt; should be &lt;heard,
geh¨ort hatte&gt;. Note that here the erroneous case
is not variation-based at all; it is a problem with
the label geh¨ort. What is needed is a method to
detect more translation possibilities.
As an example of a problem for phrases, con-
sider the variation for the nucleus end with 5 in-
stances of NIL and 1 of ein Ende. In one NIL
instance, the proper alignment should be &lt;the
end, Ende&gt;, with a longer source string. Since
the target label is Ende and not ein Ende, the fil-
ter removes this variation. One might explore
more fuzzily matching NIL strings, so that Ende
matches with ein Ende. We explore a different
method for phrases next, which deals with some
of these NIL cases.
</bodyText>
<sectionHeader confidence="0.970187" genericHeader="method">
6 A Complementary Method
</sectionHeader>
<bodyText confidence="0.999948222222222">
Although it works for any type of aligned corpus,
the string-based variation method of detecting er-
rors is limited in the types of errors it can de-
tect. There might be ways to generalize the vari-
ation n-gram method (cf. Dickinson, 2008), but
this does not exploit properties inherent to aligned
treebanks. We pursue a complementary approach,
as this can fill in some gaps a string-based method
cannot deal with (cf. Loftsson, 2009).
</bodyText>
<subsectionHeader confidence="0.999049">
6.1 Phrase Alignment Based on Word Links
</subsectionHeader>
<bodyText confidence="0.999896722222222">
Using the existing word alignments, we can search
for missing or erroneous phrase alignments. If
the words dominated by a phrase are aligned, the
phrases generally should be, too (cf. Lavie et al.,
2008). We take the yield of a constituent in one
side of a corpus, find the word alignments of this
yield, and use these alignments to predict a phrasal
alignment for the constituent. If the predicted
alignment is not annotated, it is flagged as a possi-
ble error. This is similar to the branch link locality
of the TreeAligner (see section 2.2), but here as a
prediction, rather than a restriction, of alignment.
For example, consider the English VP choose
her own friends in (1). Most of the words are
aligned to words within Ihre Freunde vielleicht
w¨ahlen (‘possibly choose her friends’), with no
alignment to words outside of this German VP. We
want to predict that the phrases be aligned.
</bodyText>
<equation confidence="0.309616">
(1) a. [VP choose1 her2 own friends3]
b. [VP Ihre2 Freunde3 vielleicht w¨ahlen1 ]
</equation>
<bodyText confidence="0.834823">
The algorithm works as follows:
</bodyText>
<listItem confidence="0.854064">
1. For every phrasal node s in the source treebank:
(a) Predict a target phrase node t to align with,
</listItem>
<bodyText confidence="0.709039">
where t could be non-alignment (NIL):
</bodyText>
<page confidence="0.999301">
43
</page>
<bodyText confidence="0.897595142857143">
i. Obtain the yield (i.e., child nodes) of the
phrase node s: s1, ... sn.
ii. Obtain the alignments for each child node
si, resulting in a set of child nodes in the
target language (t1, ... tm).
iii. Store every mother node t&apos; covering all the
target child nodes, i.e., all &lt;s, t&apos;&gt; pairs.
</bodyText>
<listItem confidence="0.9344878">
(b) If a predicted alignment (&lt;s, t&apos;&gt;) is not in the
set of actual alignments (&lt;s, t&gt;), add it to the
set of potential alignments, AS—T.
i. For nodes which are predicted to have non-
alignment (but are actually aligned), output
them to a separate file.
2. Perform step 1 with the source and target reversed,
thereby generating both AS�--.T and AT�--.S.
3. Intersect AS—T and AT—S, to obtain the set of pre-
dicted phrasal alignments not currently aligned.
</listItem>
<bodyText confidence="0.999546230769231">
The main idea in 1a is to find the children of a
source node and their alignments and then obtain
the target nodes which have all of these aligned
nodes as children. A node covering all these target
children is a plausible candidate for alignment.
Consider example (2). Within the 8-word En-
glish ADVP (almost twice ... ), there are six words
which align to words in the corresponding Ger-
man sentence, all under the same NP.5 It does not
matter that some words are unaligned; the fact
that the English ADVP and the German NP cover
basically the same set of words suggests that the
phrases should be aligned, as is the case here.
</bodyText>
<figureCaption confidence="0.84380575">
(2) a. Sophie lived on2 [NP1 the2 outskirts3 of a4
sprawling5* suburb6*] and had [ADVP almost7
twice8 as9 far10 to school as11 Joanna12 *] .
b. Sophie wohnte am2 [NP1 Ende3 eines4
ausgedehnten5* Viertels6* mit Einfam-
ilienh¨ausern] und hatte [NP einen fast7
doppelt8 so9 langen10 Schulweg wie11
Jorunn12*] .
</figureCaption>
<bodyText confidence="0.999400916666667">
The prediction of an aligned node in 1a allows
for multiple possibilities: in 1aiii, we only check
that a mother node t&apos; covers all the target children,
disregarding extra children, since translations can
contain extra words. In general, many such dom-
inating nodes exist, and most are poor candidates
for alignment of the node in question. This is the
reason for the bidirectional check in steps 2 and 3.
For example, in (3), we correctly predict align-
ment between the NP dominating you in English
and the NP dominating man in German. From
the word alignment, we generate a list of mother
</bodyText>
<footnote confidence="0.700806">
5FUZZY labels are marked by an asterisk, but are not used.
</footnote>
<bodyText confidence="0.997724625">
nodes of man as potential alignments for the you
NP. Two of these (six) nodes are shown in (3b).
In the other direction, there are eight nodes con-
taining you; two are shown in (3a). These are the
predicted alignment nodes for the NP dominating
man. In either direction, this overgenerates; the
intersection, however, only contains alignment be-
tween the lowest NPs.
</bodyText>
<listItem confidence="0.530923">
(3) a. But it ’s just as impossible to realize [S [NP
</listItem>
<bodyText confidence="0.997058658536585">
you1 ] have to die without thinking how incred-
ibly amazing it is to be alive ] .
b. [S Und es ist genauso unm¨oglich , dar¨uber
nachzudenken , dass [NP man1 ] sterben muss
, ohne zugleich daran zu denken , wie phan-
tastisch das Leben ist . ]
While generally effective, certain predictions
are less likely to be errors. In figure 3, for ex-
ample, the sentence pair is an entire rephrasing;
&lt;her, ihr&gt; is the only word alignment. For each
phrasal node in the SL, the method only requires
that all its words be aligned with the words under
the TL node. Thus, the English PP on her, the VP
had just been dumped on her, and the two VPs in
between are predicted as possible alignments with
the German VP ihr einfach in die Wiege gelegt
worden or its immediate VP daughter: they all
have her and ihr aligned, and no contradicting
alignments. Sparse word alignments lead to mul-
tiple possible phrase alignments. After intersect-
ing, we mark cases with more than one predicted
source or target phrase and do not evaluate them.
If in step 1aiii, no target mother (t&apos;) exists, but
there is alignment in the corpus, then in step 1bi,
we output predicted non-alignment. In Example
(2), for instance, the English NP the outskirts of
a sprawling suburb is (incorrectly) predicted to
have no alignment, although most words align to
words within the same German NP. This predic-
tion arises because the aligns to a word (am) out-
side of the German NP, due to am being a contrac-
tion of the preposition an and the article dem, (cf.
on and the, respectively). The method for predict-
ing phrase alignments, however, relies upon words
being within the constituent. We thus conclude
that: 1) the cases in step 1bi are unlikely to be er-
rors, and 2) there are types of alignments which
we simply will not find, a problem also for au-
tomatic alignment based on similar assumptions
(e.g., Zhechev and Way, 2008). In (2), for in-
stance, were there not already alignment between
</bodyText>
<page confidence="0.998835">
44
</page>
<figureCaption confidence="0.999734">
Figure 3: A sentence with minimal alignment
</figureCaption>
<bodyText confidence="0.874245">
the NPs, we would not predict it.
</bodyText>
<subsectionHeader confidence="0.96964">
6.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999953366666667">
The method returns 318 cases, in addition to 135
cases with multiple source/target phrases and 104
predicted non-alignments. To evaluate, we sam-
pled 55 of the 318 flagged phrases and found that
25 should have been aligned as suggested. 21
of the phrases have zero difference in length be-
tween source and target, while 34 have differences
of up to 9 tokens. Of the phrases with zero-
length difference, 18 should have been aligned
(precision=85.7%), while only 7 with length dif-
ferences should have been aligned. This is in line
with previous findings that length difference can
help predict alignment (cf., e.g., Gale and Church,
1993). About half of all phrase pairs that should
be aligned should be EXACT, regardless of the
length difference.
The method is good at predicting the alignment
of one-word phrases, e.g., pronouns, as in (3). Of
the 11 suggested alignments where both source
and target have a length of 1, all were correct sug-
gestions. This is not surprising, since all words
under the phrases are (trivially) aligned. Although
shorter phrases with short length differences gen-
erally means a higher rate of correct suggestions,
we do not want to filter out items based on phrase
length, since there are outliers that are correct sug-
gestions, e.g., phrase pairs with lengths of 15 and
13 (difference=2) or 31 and 36 (difference=5). It
is worth noting that checking the suggestions took
very little time.
</bodyText>
<sectionHeader confidence="0.980415" genericHeader="conclusions">
7 Summary and Outlook
</sectionHeader>
<bodyText confidence="0.9999773">
This paper explores two simple, language-
independent ways to detect errors in aligned cor-
pora. In the first method, applicable to any aligned
corpus, we consider alignment as a string-to-string
mapping, where a string could be the yield of a
phrase. Treating the target string as a label, we
find inconsistencies in the labeling of each source
string. Despite setting the problem up in a similar
way to grammatical annotation, we also demon-
strated that new heuristics are needed to sort er-
rors. The second method examines phrase nodes
which are predicted to be aligned, based on the
alignment of their yields. Both methods are ef-
fective, in complementary ways, and can be used
to suggest alignments for annotators or to suggest
revisions for incorrect alignments.
The wide range of possible translations and the
linguistic information which goes into them indi-
cate that there should be other ways of finding er-
rors. One possibility is to use more abstract source
or target language representations, such as POS,
to overcome the limitations of string-based meth-
ods. This will likely also be a useful avenue to
explore for language pairs more dissimilar than
English and German. By investigating different
ways to ensure alignment consistency, one can be-
gin to provide insights into automatic alignment
(Zhechev and Way, 2008). Additionally, by cor-
recting the errors, one can determine the effect on
machine translation evaluation.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998911">
We would like to thank Martin Volk and Thorsten
Marek for useful discussion and feedback of ear-
lier versions of this paper and three anonymous
reviewers for their comments.
</bodyText>
<figure confidence="0.999385337837838">
SB
S
HD
OC
VP
OC
HD
VP
DA
MO
MO
HD
PP
HD
NK
NP
NP
AVP
NP
HD
HD
HD
NK HD
Das
PDS
war
VAFIN
ihr
PPER
einfach
ADV
in
APPR
die
ART
Wiege
NN
gelegt
VVPP
worden
VAPP
PRP$
NNS
VBD
RB
VBN
VBN
-NONE-
IN
PRP
.
Her
looks
had
just
been
dumped
--
on
her
.
NP
ADVP
NP NP
*
PP
CLR
VP
VP
SBJ
VP
S
.
$.
</figure>
<sectionHeader confidence="0.90843" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992464464285714">
Botley, S. P., McEnery, A. M., and Wilson, A.,
editors (2000). Multilingual Corpora in Teach-
ing and Research. Rodopi, Amsterdam, Atlanta
GA.
Dickinson, M. (2008). Representations for cat-
egory disambiguation. In Proceedings of
COLING-08, pages 201–208, Manchester.
Dickinson, M. and Meurers, W. D. (2003). Detect-
ing inconsistencies in treebanks. In Proceed-
ings of TLT-03, pages 45–56, V¨axj¨o, Sweden.
Dickinson, M. and Meurers, W. D. (2005). De-
tecting errors in discontinuous structural anno-
tation. In Proceedings of ACL-05, pages 322–
329.
Gale, W. A. and Church, K. W. (1993). A pro-
gram for aligning sentences in bilingual cor-
pora. Computational Linguistics, 19(1):75–
102.
Gustafson-ˇCapkov´a, S., Samuelsson,
Y., and Volk, M. (2007). SMUL-
TRON (version 1.0) - The Stock-
holm MULtilingual parallel TReebank.
www.ling.su.se/dali/research/smultron/index.htm.
Habash, N., Gabbard, R., Rambow, O., Kulick, S.,
and Marcus, M. (2007). Determining case in
Arabic: Learning complex linguistic behavior
requires complex linguistic features. In Pro-
ceedings of EMNLP-CoNLL-07, pages 1084–
1092.
Lavie, A., Parlikar, A., and Ambati, V. (2008).
Syntax-driven learning of sub-sentential trans-
lation equivalents and translation rules from
parsed parallel corpora. In Proceedings of the
ACL-08: HLT Second Workshop on Syntax and
Structure in Statistical Translation (SSST-2),
pages 87–95, Columbus, OH.
Loftsson, H. (2009). Correcting a POS-tagged
corpus using three complementary methods.
In Proceedings of EACL-09, pages 523–531,
Athens, Greece.
McEnery, T. and Wilson, A. (1996). Corpus Lin-
guistics. Edinburgh University Press, Edin-
burgh.
Meurers, D. and M¨uller, S. (2008). Corpora
and syntax (article 44). In L¨udeling, A. and
Kyt¨o, M., editors, Corpus Linguistics. An In-
ternational Handbook, Handbooks of Linguis-
tics and Communication Science. Mouton de
Gruyter, Berlin.
Och, F. J. and Ney, H. (2003). A systematic com-
parison of various statistical alignment models.
Computational Linguistics, 29(1):19–51.
Okita, T. (2009). Data cleaning for word align-
ment. In Proceedings of the ACL-IJCNLP 2009
Student Research Workshop, pages 72–80, Sun-
tec, Singapore.
Padro, L. and Marquez, L. (1998). On the eval-
uation and comparison of taggers: the effect
of noise in testing corpora. In Proceedings of
ACL-COLING-98, pages 997–1002, San Fran-
cisco, California.
Taylor, A., Marcus, M., and Santorini, B. (2003).
The penn treebank: An overview. In Abeill´e,
A., editor, Treebanks: Building and using syn-
tactically annotated corpora, chapter 1, pages
5–22. Kluwer, Dordrecht.
Tiedemann, J. (2003). Recycling Translations -
Extraction of Lexical Data from Parallel Cor-
pora and their Application in Natural Lan-
guage Processing. PhD thesis, Uppsala univer-
sity.
Ule, T. and Simov, K. (2004). Unexpected pro-
ductions may well be errors. In Proceedings of
LREC-04, Lisbon, Portugal.
Volk, M., Lundborg, J., and Mettler, M. (2007).
A search tool for parallel treebanks. In Pro-
ceedings of the Linguistic Annotation Workshop
(LAW) at ACL, pages 85–92, Prague, Czech Re-
public. Association for Computational Linguis-
tics.
Zhechev, V. and Way, A. (2008). Automatic gen-
eration of parallel treebanks. In Proceedings
of Coling 2008, pages 1105–1112, Manchester,
UK.
</reference>
<page confidence="0.999588">
46
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.705745">
<title confidence="0.998975">Consistency Checking for Treebank Alignment</title>
<author confidence="0.995881">Markus Dickinson Yvonne Samuelsson</author>
<affiliation confidence="0.999997">Indiana University Stockholm University</affiliation>
<email confidence="0.716402">md7@indiana.eduyvonne.samuelsson@ling.su.se</email>
<abstract confidence="0.999412058823529">This paper explores ways to detect errors in aligned corpora, using very little technology. In the first method, applicable to any aligned corpus, we consider alignment as a string-to-string mapping. Treating the target string as a label, we examine each source string to find inconsistencies in alignment. Despite setting up the problem on a par with grammatical annotation, we demonstrate crucial differences in sorting errors from legitimate variations. The second method examines phrase nodes which are predicted to be aligned, based on the alignment of their yields. Both methods are effective in complementary ways.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S P Botley</author>
<author>A M McEnery</author>
<author>A Wilson</author>
</authors>
<date>2000</date>
<booktitle>Multilingual Corpora in Teaching and Research. Rodopi,</booktitle>
<location>Amsterdam, Atlanta GA.</location>
<contexts>
<context position="1242" citStr="Botley et al., 2000" startWordPosition="181" endWordPosition="184">tions. The second method examines phrase nodes which are predicted to be aligned, based on the alignment of their yields. Both methods are effective in complementary ways. 1 Introduction Parallel corpora—texts and their translations— have become essential in the development of machine translation (MT) systems. Alignment quality is crucial to these corpora; as Tiedemann (2003) states, “[t]he most important feature of texts and their translations is the correspondence between source and target segments” (p. 2). While being useful for translation studies and foreign language pedagogy (see, e.g., Botley et al., 2000; McEnery and Wilson, 1996), PARALLEL TREEBANKS—syntactically-annotated parallel corpora—offer additional useful information for machine translation, cross-language information retrieval, and word-sense disambiguation (see, e.g., Tiedemann, 2003), While high-quality alignments are desirable, even gold standard annotation can contain annotation errors. For other forms of linguistic annotation, the presence of errors has been shown to create various problems, from unreliable training and evaluation of NLP technology (e.g., Padro and Marquez, 1998) to low precision and recall of queries for alrea</context>
</contexts>
<marker>Botley, McEnery, Wilson, 2000</marker>
<rawString>Botley, S. P., McEnery, A. M., and Wilson, A., editors (2000). Multilingual Corpora in Teaching and Research. Rodopi, Amsterdam, Atlanta GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dickinson</author>
</authors>
<title>Representations for category disambiguation.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING-08,</booktitle>
<pages>201--208</pages>
<location>Manchester.</location>
<contexts>
<context position="4308" citStr="Dickinson, 2008" startWordPosition="654" endWordPosition="655">a starting point for an error detection method for aligned corpora, we use the variation n-gram approach for syntactic annotation (Dickinson and Meurers, 2003, 2005). The approach is based on detecting strings which occur multiple times in the corpus with varying annotation, the so-called VARIATION NUCLEI. The nucleus with repeated surrounding context is referred to as a VARIATION n-GRAM. The basic heuristic for detecting annotation errors requires one word of recurring context on each side of the nucleus, which is sufficient for detecting errors in grammatical annotation with high precision (Dickinson, 2008). The approach detects bracketing and labeling errors in constituency annotation. For example, the variation nucleus last month occurs once in the Penn Treebank (Taylor et al., 2003) with the label NP and once as a non-constituent, handled through a special label NIL. As a labeling error example, next Tuesday occurs three times, twice as NP and once as PP (Dickinson and Meurers, 2003). The method works for discontinuous constituency annotation (Dickinson and Meurers, 2005), allowing one to apply it to alignments, which may span over several words. 2.2 Parallel Treebank Consistency Checking For</context>
<context position="15058" citStr="Dickinson, 2008" startWordPosition="2435" endWordPosition="2436"> to filter the set of variations. 4.1 NIL-only Variation As discussed in section 3.2, we use the label NILC to refer to syntactic constituents which do not receive an alignment, while NIL refers to nonconstituent strings without an alignment. A string which varies between NIL and NIL-C, then, is not really varying in its alignment—i.e., it is always unaligned. We thus remove cases varying only between NIL and NIL-C. 4.2 Context-based Filtering The variation n-gram method has generally relied upon immediate lexical context around the variation nucleus, in order to sort errors from ambiguities (Dickinson, 2008). However, while useful for grammatical annotation, it is not clear how useful the surrounding context is for translation tasks, given the wide range of possible translations for the same context. Further, requiring identical context around source words is very strict, leading to sparse data problems, and it ignores alignmentspecific information (see sections 4.3 and 4.4). We test three different notions of context. Matching the variation n-gram method, we first employ a filter identifying those nuclei which share the “shortest” identical context, i.e., one word of context on every side of a n</context>
<context position="22449" citStr="Dickinson, 2008" startWordPosition="3707" endWordPosition="3708"> NIL instance, the proper alignment should be &lt;the end, Ende&gt;, with a longer source string. Since the target label is Ende and not ein Ende, the filter removes this variation. One might explore more fuzzily matching NIL strings, so that Ende matches with ein Ende. We explore a different method for phrases next, which deals with some of these NIL cases. 6 A Complementary Method Although it works for any type of aligned corpus, the string-based variation method of detecting errors is limited in the types of errors it can detect. There might be ways to generalize the variation n-gram method (cf. Dickinson, 2008), but this does not exploit properties inherent to aligned treebanks. We pursue a complementary approach, as this can fill in some gaps a string-based method cannot deal with (cf. Loftsson, 2009). 6.1 Phrase Alignment Based on Word Links Using the existing word alignments, we can search for missing or erroneous phrase alignments. If the words dominated by a phrase are aligned, the phrases generally should be, too (cf. Lavie et al., 2008). We take the yield of a constituent in one side of a corpus, find the word alignments of this yield, and use these alignments to predict a phrasal alignment f</context>
</contexts>
<marker>Dickinson, 2008</marker>
<rawString>Dickinson, M. (2008). Representations for category disambiguation. In Proceedings of COLING-08, pages 201–208, Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dickinson</author>
<author>W D Meurers</author>
</authors>
<title>Detecting inconsistencies in treebanks.</title>
<date>2003</date>
<booktitle>In Proceedings of TLT-03,</booktitle>
<pages>45--56</pages>
<location>V¨axj¨o,</location>
<contexts>
<context position="3850" citStr="Dickinson and Meurers, 2003" startWordPosition="580" endWordPosition="583"> of aligned treebanks, to align more nodes. The methods are simple, effective, and applicable to any aligned treebank. As far as we know, this is the first attempt to thoroughly investigate and empirically verify error detection methods for aligned corpora. 38 Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 38–46, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics 2 Background 2.1 Variation N-gram Method As a starting point for an error detection method for aligned corpora, we use the variation n-gram approach for syntactic annotation (Dickinson and Meurers, 2003, 2005). The approach is based on detecting strings which occur multiple times in the corpus with varying annotation, the so-called VARIATION NUCLEI. The nucleus with repeated surrounding context is referred to as a VARIATION n-GRAM. The basic heuristic for detecting annotation errors requires one word of recurring context on each side of the nucleus, which is sufficient for detecting errors in grammatical annotation with high precision (Dickinson, 2008). The approach detects bracketing and labeling errors in constituency annotation. For example, the variation nucleus last month occurs once in</context>
<context position="10490" citStr="Dickinson and Meurers, 2003" startWordPosition="1685" endWordPosition="1688">a single string could have more than one label, violating the assumption that the string-tolabel mapping is a function. For example, in PP Penn Treebank-style annotation, an NP node can S CLR dominate a QP (quantifierVphrase) node via a unary branch. Thus, an annotator could (likely S erroneously) assign different alignments to each S phrasal node, one for the NP and one for the QP, resulting in different target labels. VP We handle all the (source) unary branch alignments as a conjunction of possibilities, ordered from top to bottom. Just as the syntactic structure can be relabeled as NP/QP (Dickinson and Meurers, 2003), we can relabel a string as, e.g., S the man/man. If different unary nodes result in the same string (the man/the man), we combine them VROOT (the man). Note that unary branches are unproblematic in the target language since they always yield the same string, i.e., are still one label. 3.2 Consistency and Completeness Error detection for syntactic annotation finds inconsistencies in constituent labeling (e.g., NP vs. QP) and inconsistencies in bracketing (e.g., NP vs. NIL). Likewise, we can distinguish inconsistency in labeling (different translations) from inconsistency in alignment (aligned</context>
</contexts>
<marker>Dickinson, Meurers, 2003</marker>
<rawString>Dickinson, M. and Meurers, W. D. (2003). Detecting inconsistencies in treebanks. In Proceedings of TLT-03, pages 45–56, V¨axj¨o, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dickinson</author>
<author>W D Meurers</author>
</authors>
<title>Detecting errors in discontinuous structural annotation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-05,</booktitle>
<pages>322--329</pages>
<contexts>
<context position="2557" citStr="Dickinson and Meurers, 2005" startWordPosition="374" endWordPosition="377">errors can have a significant impact on the uses of linguistic annotation, e.g., changing the assessment of parsers (e.g., Habash et al., 2007). One could remove potentially unfavorable sentence pairs when training a statistical MT system, to avoid incorrect word alignments (Okita, 2009), but this removes all relevant data from those sentences and does not help evaluation. We thus focus on detecting errors in the annotation of alignments. Annotation error detection has been explored for part-of-speech (POS) annotation (e.g., Loftsson, 2009) and syntactic annotation (e.g., Ule and Simov, 2004; Dickinson and Meurers, 2005), but there have been few, if any, attempts to develop general approaches to error detection for aligned corpora. Alignments are different in nature, as the annotation does not introduce abstract categories such as POS, but relies upon defining translation units with equivalent meanings. We use the idea that variation in annotation can indicate errors (section 2), for consistency checking of alignments, as detailed in section 3. In section 4, we outline language-independent heuristics to sort true ambiguities from errors, and evaluate them on a parallel treebank in section 5. In section 6 we t</context>
<context position="4785" citStr="Dickinson and Meurers, 2005" startWordPosition="727" endWordPosition="730">recurring context on each side of the nucleus, which is sufficient for detecting errors in grammatical annotation with high precision (Dickinson, 2008). The approach detects bracketing and labeling errors in constituency annotation. For example, the variation nucleus last month occurs once in the Penn Treebank (Taylor et al., 2003) with the label NP and once as a non-constituent, handled through a special label NIL. As a labeling error example, next Tuesday occurs three times, twice as NP and once as PP (Dickinson and Meurers, 2003). The method works for discontinuous constituency annotation (Dickinson and Meurers, 2005), allowing one to apply it to alignments, which may span over several words. 2.2 Parallel Treebank Consistency Checking For the experiments in this paper we will use the SMULTRON parallel treebank of Swedish, German, and English (Gustafson-ˇCapkov´a et al., 2007), containing syntactic annotation and alignment on both word and phrase levels.1 Additionally, alignments are marked as showing either an EXACT or a FUZZY (approximate) equivalence. Corpora with alignments often have undergone some error-checking. Previous consistency checks for SMULTRON, for example, consisted of running one script fo</context>
<context position="11284" citStr="Dickinson and Meurers (2005)" startWordPosition="1807" endWordPosition="1811">ary branches are unproblematic in the target language since they always yield the same string, i.e., are still one label. 3.2 Consistency and Completeness Error detection for syntactic annotation finds inconsistencies in constituent labeling (e.g., NP vs. QP) and inconsistencies in bracketing (e.g., NP vs. NIL). Likewise, we can distinguish inconsistency in labeling (different translations) from inconsistency in alignment (aligned/unaligned). Detecting inconsistency in alignment deals with the completeness of the annotation, by using the label NIL for unaligned strings. We use the method from Dickinson and Meurers (2005) to generate NILs, but using NIL for unaligned strings is too coarse-grained for phraselevel alignment. A string mapping to NIL might be a phrase which has no alignment, or it might not be a phrase and thus could not possibly have an alignment. Thus, we create NIL-C as a new label, indicating a constituent with no alignment, differing from NIL strings which do not even form a phrase. For example, on the left side of Figure 2, the string someone aligns to jemanden on the phrase level. On the right side of Figure 2, the string someone by itself does not constitute a phrase (even though the align</context>
<context position="14098" citStr="Dickinson and Meurers (2005)" startWordPosition="2278" endWordPosition="2281">L mappings. 2. Divide each SL-to-TL alignment set into word-level and phrase-level alignments. For each of the four sets of alignments: 1. Map each string in SL with an alignment to a label • Label = &lt;(lower-cased) TL translation, EXACT|FUZZY|EXACT-FUZZY&gt; • (For phrases) Constituent phrases with no alignment are given the special label, NIL-C. • (For phrases) Constituent phrases which are unary branches are given a single, normalized label representing all target strings. 2. Generate NIL alignments for string tokens which occur in SL, but have no alignment to TL, using the method described in Dickinson and Meurers (2005). 3. Find SL strings which have variation in labeling. 4. Filter the variations from step 3, based on likelihood of being an error (see section 4). 4 Identifying Inconsistent Alignments As words and phrases have acceptable variants for translation, the method in section 3 will lead to detecting acceptable variations. We use several heuristics to filter the set of variations. 4.1 NIL-only Variation As discussed in section 3.2, we use the label NILC to refer to syntactic constituents which do not receive an alignment, while NIL refers to nonconstituent strings without an alignment. A string whic</context>
</contexts>
<marker>Dickinson, Meurers, 2005</marker>
<rawString>Dickinson, M. and Meurers, W. D. (2005). Detecting errors in discontinuous structural annotation. In Proceedings of ACL-05, pages 322– 329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>K W Church</author>
</authors>
<title>A program for aligning sentences in bilingual corpora.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<pages>102</pages>
<contexts>
<context position="29248" citStr="Gale and Church, 1993" startWordPosition="4902" endWordPosition="4905">returns 318 cases, in addition to 135 cases with multiple source/target phrases and 104 predicted non-alignments. To evaluate, we sampled 55 of the 318 flagged phrases and found that 25 should have been aligned as suggested. 21 of the phrases have zero difference in length between source and target, while 34 have differences of up to 9 tokens. Of the phrases with zerolength difference, 18 should have been aligned (precision=85.7%), while only 7 with length differences should have been aligned. This is in line with previous findings that length difference can help predict alignment (cf., e.g., Gale and Church, 1993). About half of all phrase pairs that should be aligned should be EXACT, regardless of the length difference. The method is good at predicting the alignment of one-word phrases, e.g., pronouns, as in (3). Of the 11 suggested alignments where both source and target have a length of 1, all were correct suggestions. This is not surprising, since all words under the phrases are (trivially) aligned. Although shorter phrases with short length differences generally means a higher rate of correct suggestions, we do not want to filter out items based on phrase length, since there are outliers that are </context>
</contexts>
<marker>Gale, Church, 1993</marker>
<rawString>Gale, W. A. and Church, K. W. (1993). A program for aligning sentences in bilingual corpora. Computational Linguistics, 19(1):75– 102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gustafson-ˇCapkov´a</author>
<author>Y Samuelsson</author>
<author>M Volk</author>
</authors>
<title>SMULTRON (version 1.0) - The Stockholm MULtilingual parallel TReebank.</title>
<date>2007</date>
<note>www.ling.su.se/dali/research/smultron/index.htm.</note>
<marker>Gustafson-ˇCapkov´a, Samuelsson, Volk, 2007</marker>
<rawString>Gustafson-ˇCapkov´a, S., Samuelsson, Y., and Volk, M. (2007). SMULTRON (version 1.0) - The Stockholm MULtilingual parallel TReebank. www.ling.su.se/dali/research/smultron/index.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>R Gabbard</author>
<author>O Rambow</author>
<author>S Kulick</author>
<author>M Marcus</author>
</authors>
<title>Determining case in Arabic: Learning complex linguistic behavior requires complex linguistic features.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL-07,</booktitle>
<pages>1084--1092</pages>
<contexts>
<context position="2072" citStr="Habash et al., 2007" startWordPosition="300" endWordPosition="303">guation (see, e.g., Tiedemann, 2003), While high-quality alignments are desirable, even gold standard annotation can contain annotation errors. For other forms of linguistic annotation, the presence of errors has been shown to create various problems, from unreliable training and evaluation of NLP technology (e.g., Padro and Marquez, 1998) to low precision and recall of queries for already rare linguistic phenomena (e.g., Meurers and M¨uller, 2008). Even a small number of errors can have a significant impact on the uses of linguistic annotation, e.g., changing the assessment of parsers (e.g., Habash et al., 2007). One could remove potentially unfavorable sentence pairs when training a statistical MT system, to avoid incorrect word alignments (Okita, 2009), but this removes all relevant data from those sentences and does not help evaluation. We thus focus on detecting errors in the annotation of alignments. Annotation error detection has been explored for part-of-speech (POS) annotation (e.g., Loftsson, 2009) and syntactic annotation (e.g., Ule and Simov, 2004; Dickinson and Meurers, 2005), but there have been few, if any, attempts to develop general approaches to error detection for aligned corpora. A</context>
</contexts>
<marker>Habash, Gabbard, Rambow, Kulick, Marcus, 2007</marker>
<rawString>Habash, N., Gabbard, R., Rambow, O., Kulick, S., and Marcus, M. (2007). Determining case in Arabic: Learning complex linguistic behavior requires complex linguistic features. In Proceedings of EMNLP-CoNLL-07, pages 1084– 1092.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie</author>
<author>A Parlikar</author>
<author>V Ambati</author>
</authors>
<title>Syntax-driven learning of sub-sentential translation equivalents and translation rules from parsed parallel corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL-08: HLT Second Workshop on Syntax and Structure in Statistical Translation (SSST-2),</booktitle>
<pages>87--95</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="22890" citStr="Lavie et al., 2008" startWordPosition="3777" endWordPosition="3780">ng-based variation method of detecting errors is limited in the types of errors it can detect. There might be ways to generalize the variation n-gram method (cf. Dickinson, 2008), but this does not exploit properties inherent to aligned treebanks. We pursue a complementary approach, as this can fill in some gaps a string-based method cannot deal with (cf. Loftsson, 2009). 6.1 Phrase Alignment Based on Word Links Using the existing word alignments, we can search for missing or erroneous phrase alignments. If the words dominated by a phrase are aligned, the phrases generally should be, too (cf. Lavie et al., 2008). We take the yield of a constituent in one side of a corpus, find the word alignments of this yield, and use these alignments to predict a phrasal alignment for the constituent. If the predicted alignment is not annotated, it is flagged as a possible error. This is similar to the branch link locality of the TreeAligner (see section 2.2), but here as a prediction, rather than a restriction, of alignment. For example, consider the English VP choose her own friends in (1). Most of the words are aligned to words within Ihre Freunde vielleicht w¨ahlen (‘possibly choose her friends’), with no align</context>
</contexts>
<marker>Lavie, Parlikar, Ambati, 2008</marker>
<rawString>Lavie, A., Parlikar, A., and Ambati, V. (2008). Syntax-driven learning of sub-sentential translation equivalents and translation rules from parsed parallel corpora. In Proceedings of the ACL-08: HLT Second Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 87–95, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Loftsson</author>
</authors>
<title>Correcting a POS-tagged corpus using three complementary methods.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL-09,</booktitle>
<pages>523--531</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="2475" citStr="Loftsson, 2009" startWordPosition="363" endWordPosition="364"> phenomena (e.g., Meurers and M¨uller, 2008). Even a small number of errors can have a significant impact on the uses of linguistic annotation, e.g., changing the assessment of parsers (e.g., Habash et al., 2007). One could remove potentially unfavorable sentence pairs when training a statistical MT system, to avoid incorrect word alignments (Okita, 2009), but this removes all relevant data from those sentences and does not help evaluation. We thus focus on detecting errors in the annotation of alignments. Annotation error detection has been explored for part-of-speech (POS) annotation (e.g., Loftsson, 2009) and syntactic annotation (e.g., Ule and Simov, 2004; Dickinson and Meurers, 2005), but there have been few, if any, attempts to develop general approaches to error detection for aligned corpora. Alignments are different in nature, as the annotation does not introduce abstract categories such as POS, but relies upon defining translation units with equivalent meanings. We use the idea that variation in annotation can indicate errors (section 2), for consistency checking of alignments, as detailed in section 3. In section 4, we outline language-independent heuristics to sort true ambiguities fro</context>
<context position="22644" citStr="Loftsson, 2009" startWordPosition="3738" endWordPosition="3739">re fuzzily matching NIL strings, so that Ende matches with ein Ende. We explore a different method for phrases next, which deals with some of these NIL cases. 6 A Complementary Method Although it works for any type of aligned corpus, the string-based variation method of detecting errors is limited in the types of errors it can detect. There might be ways to generalize the variation n-gram method (cf. Dickinson, 2008), but this does not exploit properties inherent to aligned treebanks. We pursue a complementary approach, as this can fill in some gaps a string-based method cannot deal with (cf. Loftsson, 2009). 6.1 Phrase Alignment Based on Word Links Using the existing word alignments, we can search for missing or erroneous phrase alignments. If the words dominated by a phrase are aligned, the phrases generally should be, too (cf. Lavie et al., 2008). We take the yield of a constituent in one side of a corpus, find the word alignments of this yield, and use these alignments to predict a phrasal alignment for the constituent. If the predicted alignment is not annotated, it is flagged as a possible error. This is similar to the branch link locality of the TreeAligner (see section 2.2), but here as a</context>
</contexts>
<marker>Loftsson, 2009</marker>
<rawString>Loftsson, H. (2009). Correcting a POS-tagged corpus using three complementary methods. In Proceedings of EACL-09, pages 523–531, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T McEnery</author>
<author>A Wilson</author>
</authors>
<title>Corpus Linguistics.</title>
<date>1996</date>
<publisher>Edinburgh University Press,</publisher>
<location>Edinburgh.</location>
<contexts>
<context position="1269" citStr="McEnery and Wilson, 1996" startWordPosition="185" endWordPosition="188">hod examines phrase nodes which are predicted to be aligned, based on the alignment of their yields. Both methods are effective in complementary ways. 1 Introduction Parallel corpora—texts and their translations— have become essential in the development of machine translation (MT) systems. Alignment quality is crucial to these corpora; as Tiedemann (2003) states, “[t]he most important feature of texts and their translations is the correspondence between source and target segments” (p. 2). While being useful for translation studies and foreign language pedagogy (see, e.g., Botley et al., 2000; McEnery and Wilson, 1996), PARALLEL TREEBANKS—syntactically-annotated parallel corpora—offer additional useful information for machine translation, cross-language information retrieval, and word-sense disambiguation (see, e.g., Tiedemann, 2003), While high-quality alignments are desirable, even gold standard annotation can contain annotation errors. For other forms of linguistic annotation, the presence of errors has been shown to create various problems, from unreliable training and evaluation of NLP technology (e.g., Padro and Marquez, 1998) to low precision and recall of queries for already rare linguistic phenomen</context>
</contexts>
<marker>McEnery, Wilson, 1996</marker>
<rawString>McEnery, T. and Wilson, A. (1996). Corpus Linguistics. Edinburgh University Press, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Meurers</author>
<author>S M¨uller</author>
</authors>
<title>Corpora and syntax (article 44).</title>
<date>2008</date>
<booktitle>Corpus Linguistics. An International Handbook, Handbooks of Linguistics and Communication Science. Mouton de Gruyter,</booktitle>
<editor>In L¨udeling, A. and Kyt¨o, M., editors,</editor>
<location>Berlin.</location>
<marker>Meurers, M¨uller, 2008</marker>
<rawString>Meurers, D. and M¨uller, S. (2008). Corpora and syntax (article 44). In L¨udeling, A. and Kyt¨o, M., editors, Corpus Linguistics. An International Handbook, Handbooks of Linguistics and Communication Science. Mouton de Gruyter, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="12292" citStr="Och and Ney, 2003" startWordPosition="1990" endWordPosition="1993"> example, on the left side of Figure 2, the string someone aligns to jemanden on the phrase level. On the right side of Figure 2, the string someone by itself does not constitute a phrase (even though the alignment in this instance is correct) and is labeled NIL. If there were instances of someone as an NP with no alignment, this would be NIL-C. NIL-C cases seem to be useful for inconsistency detection, as we expect consistency for items annotated as a phrase. 3.3 Alignment Types Aligned corpora often specify additional information about each alignment, e.g., a “sure” or “possible” alignment (Och and Ney, 2003). In SMULTRON, for instance, an EXACT alignment means that the strings are considered direct translation equivalents outside the current sentence context, whereas a FUZZY one is not as strict an equivalent. For example, something in English EXACTaligns with etwas in German. However, if something and irgend etwas (‘something or other’) are constituents on the phrase level, &lt;something, irgend etwas&gt; is an acceptable alignment (since the corpus aligns as much as possible), but is FUZZY. Since EXACT alignments are the ones we expect to consistently align with the same string across the corpus, we </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Och, F. J. and Ney, H. (2003). A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Okita</author>
</authors>
<title>Data cleaning for word alignment.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Student Research Workshop,</booktitle>
<pages>72--80</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="2217" citStr="Okita, 2009" startWordPosition="323" endWordPosition="324"> forms of linguistic annotation, the presence of errors has been shown to create various problems, from unreliable training and evaluation of NLP technology (e.g., Padro and Marquez, 1998) to low precision and recall of queries for already rare linguistic phenomena (e.g., Meurers and M¨uller, 2008). Even a small number of errors can have a significant impact on the uses of linguistic annotation, e.g., changing the assessment of parsers (e.g., Habash et al., 2007). One could remove potentially unfavorable sentence pairs when training a statistical MT system, to avoid incorrect word alignments (Okita, 2009), but this removes all relevant data from those sentences and does not help evaluation. We thus focus on detecting errors in the annotation of alignments. Annotation error detection has been explored for part-of-speech (POS) annotation (e.g., Loftsson, 2009) and syntactic annotation (e.g., Ule and Simov, 2004; Dickinson and Meurers, 2005), but there have been few, if any, attempts to develop general approaches to error detection for aligned corpora. Alignments are different in nature, as the annotation does not introduce abstract categories such as POS, but relies upon defining translation uni</context>
</contexts>
<marker>Okita, 2009</marker>
<rawString>Okita, T. (2009). Data cleaning for word alignment. In Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 72–80, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Padro</author>
<author>L Marquez</author>
</authors>
<title>On the evaluation and comparison of taggers: the effect of noise in testing corpora.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL-COLING-98,</booktitle>
<pages>997--1002</pages>
<location>San Francisco, California.</location>
<contexts>
<context position="1793" citStr="Padro and Marquez, 1998" startWordPosition="254" endWordPosition="257">ion studies and foreign language pedagogy (see, e.g., Botley et al., 2000; McEnery and Wilson, 1996), PARALLEL TREEBANKS—syntactically-annotated parallel corpora—offer additional useful information for machine translation, cross-language information retrieval, and word-sense disambiguation (see, e.g., Tiedemann, 2003), While high-quality alignments are desirable, even gold standard annotation can contain annotation errors. For other forms of linguistic annotation, the presence of errors has been shown to create various problems, from unreliable training and evaluation of NLP technology (e.g., Padro and Marquez, 1998) to low precision and recall of queries for already rare linguistic phenomena (e.g., Meurers and M¨uller, 2008). Even a small number of errors can have a significant impact on the uses of linguistic annotation, e.g., changing the assessment of parsers (e.g., Habash et al., 2007). One could remove potentially unfavorable sentence pairs when training a statistical MT system, to avoid incorrect word alignments (Okita, 2009), but this removes all relevant data from those sentences and does not help evaluation. We thus focus on detecting errors in the annotation of alignments. Annotation error dete</context>
</contexts>
<marker>Padro, Marquez, 1998</marker>
<rawString>Padro, L. and Marquez, L. (1998). On the evaluation and comparison of taggers: the effect of noise in testing corpora. In Proceedings of ACL-COLING-98, pages 997–1002, San Francisco, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Taylor</author>
<author>M Marcus</author>
<author>B Santorini</author>
</authors>
<title>The penn treebank: An overview.</title>
<date>2003</date>
<pages>5--22</pages>
<editor>In Abeill´e, A., editor,</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="4490" citStr="Taylor et al., 2003" startWordPosition="679" endWordPosition="682">h is based on detecting strings which occur multiple times in the corpus with varying annotation, the so-called VARIATION NUCLEI. The nucleus with repeated surrounding context is referred to as a VARIATION n-GRAM. The basic heuristic for detecting annotation errors requires one word of recurring context on each side of the nucleus, which is sufficient for detecting errors in grammatical annotation with high precision (Dickinson, 2008). The approach detects bracketing and labeling errors in constituency annotation. For example, the variation nucleus last month occurs once in the Penn Treebank (Taylor et al., 2003) with the label NP and once as a non-constituent, handled through a special label NIL. As a labeling error example, next Tuesday occurs three times, twice as NP and once as PP (Dickinson and Meurers, 2003). The method works for discontinuous constituency annotation (Dickinson and Meurers, 2005), allowing one to apply it to alignments, which may span over several words. 2.2 Parallel Treebank Consistency Checking For the experiments in this paper we will use the SMULTRON parallel treebank of Swedish, German, and English (Gustafson-ˇCapkov´a et al., 2007), containing syntactic annotation and alig</context>
</contexts>
<marker>Taylor, Marcus, Santorini, 2003</marker>
<rawString>Taylor, A., Marcus, M., and Santorini, B. (2003). The penn treebank: An overview. In Abeill´e, A., editor, Treebanks: Building and using syntactically annotated corpora, chapter 1, pages 5–22. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tiedemann</author>
</authors>
<title>Recycling Translations -Extraction of Lexical Data from Parallel Corpora and their Application in Natural Language Processing.</title>
<date>2003</date>
<tech>PhD thesis,</tech>
<institution>Uppsala university.</institution>
<contexts>
<context position="1001" citStr="Tiedemann (2003)" startWordPosition="144" endWordPosition="146"> target string as a label, we examine each source string to find inconsistencies in alignment. Despite setting up the problem on a par with grammatical annotation, we demonstrate crucial differences in sorting errors from legitimate variations. The second method examines phrase nodes which are predicted to be aligned, based on the alignment of their yields. Both methods are effective in complementary ways. 1 Introduction Parallel corpora—texts and their translations— have become essential in the development of machine translation (MT) systems. Alignment quality is crucial to these corpora; as Tiedemann (2003) states, “[t]he most important feature of texts and their translations is the correspondence between source and target segments” (p. 2). While being useful for translation studies and foreign language pedagogy (see, e.g., Botley et al., 2000; McEnery and Wilson, 1996), PARALLEL TREEBANKS—syntactically-annotated parallel corpora—offer additional useful information for machine translation, cross-language information retrieval, and word-sense disambiguation (see, e.g., Tiedemann, 2003), While high-quality alignments are desirable, even gold standard annotation can contain annotation errors. For o</context>
</contexts>
<marker>Tiedemann, 2003</marker>
<rawString>Tiedemann, J. (2003). Recycling Translations -Extraction of Lexical Data from Parallel Corpora and their Application in Natural Language Processing. PhD thesis, Uppsala university.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Ule</author>
<author>K Simov</author>
</authors>
<title>Unexpected productions may well be errors.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC-04,</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="2527" citStr="Ule and Simov, 2004" startWordPosition="370" endWordPosition="373">en a small number of errors can have a significant impact on the uses of linguistic annotation, e.g., changing the assessment of parsers (e.g., Habash et al., 2007). One could remove potentially unfavorable sentence pairs when training a statistical MT system, to avoid incorrect word alignments (Okita, 2009), but this removes all relevant data from those sentences and does not help evaluation. We thus focus on detecting errors in the annotation of alignments. Annotation error detection has been explored for part-of-speech (POS) annotation (e.g., Loftsson, 2009) and syntactic annotation (e.g., Ule and Simov, 2004; Dickinson and Meurers, 2005), but there have been few, if any, attempts to develop general approaches to error detection for aligned corpora. Alignments are different in nature, as the annotation does not introduce abstract categories such as POS, but relies upon defining translation units with equivalent meanings. We use the idea that variation in annotation can indicate errors (section 2), for consistency checking of alignments, as detailed in section 3. In section 4, we outline language-independent heuristics to sort true ambiguities from errors, and evaluate them on a parallel treebank i</context>
</contexts>
<marker>Ule, Simov, 2004</marker>
<rawString>Ule, T. and Simov, K. (2004). Unexpected productions may well be errors. In Proceedings of LREC-04, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Volk</author>
<author>J Lundborg</author>
<author>M Mettler</author>
</authors>
<title>A search tool for parallel treebanks.</title>
<date>2007</date>
<booktitle>In Proceedings of the Linguistic Annotation Workshop (LAW) at ACL,</booktitle>
<pages>85--92</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5956" citStr="Volk et al., 2007" startWordPosition="899" endWordPosition="902">for example, consisted of running one script for comparing differences in length between the source and target language items, and one script for comparing alignment labels, to detect variation between EXACT and FUZZY links. For example, the pair and (English) and samt (German, ‘together with’) had 20 FUZZY matches and 1 (erroneous) EXACT match. Such 1SMULTRON is freely available for research purposes, see http://www.cl.uzh.ch/kitt/smultron/. methods are limited, in that they do not, e.g., handle missing alignments. The TreeAligner2 tool for annotating and querying aligned parallel treebanks (Volk et al., 2007) employs its own consistency checking, recently developed by Torsten Marek. One method uses 2 x 2 contingency tables over words, looking, e.g., at the word-word or POS-POS combinations, pinpointing anomalous translation equivalents. While potentially effective, this does not address the use of alignments in context, i.e., when we might expect to see a rare translation. A second, more treebank-specific method checks for so-called branch link locality: if two nodes are aligned, any node dominating one of them can only be aligned to a node dominating the other one. While this constraint can flag </context>
</contexts>
<marker>Volk, Lundborg, Mettler, 2007</marker>
<rawString>Volk, M., Lundborg, J., and Mettler, M. (2007). A search tool for parallel treebanks. In Proceedings of the Linguistic Annotation Workshop (LAW) at ACL, pages 85–92, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Zhechev</author>
<author>A Way</author>
</authors>
<title>Automatic generation of parallel treebanks.</title>
<date>2008</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>1105--1112</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="28454" citStr="Zhechev and Way, 2008" startWordPosition="4768" endWordPosition="4771">edicted to have no alignment, although most words align to words within the same German NP. This prediction arises because the aligns to a word (am) outside of the German NP, due to am being a contraction of the preposition an and the article dem, (cf. on and the, respectively). The method for predicting phrase alignments, however, relies upon words being within the constituent. We thus conclude that: 1) the cases in step 1bi are unlikely to be errors, and 2) there are types of alignments which we simply will not find, a problem also for automatic alignment based on similar assumptions (e.g., Zhechev and Way, 2008). In (2), for instance, were there not already alignment between 44 Figure 3: A sentence with minimal alignment the NPs, we would not predict it. 6.2 Evaluation The method returns 318 cases, in addition to 135 cases with multiple source/target phrases and 104 predicted non-alignments. To evaluate, we sampled 55 of the 318 flagged phrases and found that 25 should have been aligned as suggested. 21 of the phrases have zero difference in length between source and target, while 34 have differences of up to 9 tokens. Of the phrases with zerolength difference, 18 should have been aligned (precision=</context>
</contexts>
<marker>Zhechev, Way, 2008</marker>
<rawString>Zhechev, V. and Way, A. (2008). Automatic generation of parallel treebanks. In Proceedings of Coling 2008, pages 1105–1112, Manchester, UK.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>