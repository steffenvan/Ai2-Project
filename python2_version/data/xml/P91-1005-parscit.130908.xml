<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002593">
<title confidence="0.920208">
AN ALGORITHM FOR PLAN RECOGNITION IN
COLLABORATIVE DISCOURSE*
Karen E. Lochbaum
</title>
<author confidence="0.885873">
Aiken Computation Lab
</author>
<affiliation confidence="0.92713">
Harvard University
</affiliation>
<address confidence="0.982965">
33 Oxford Street
Cambridge, MA 02138
</address>
<email confidence="0.918871">
kel©harvard.harvard.edu
</email>
<sectionHeader confidence="0.875696" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999808444444444">
A model of plan recognition in discourse must be based
on intended recognition, distinguish each agent&apos;s be-
liefs and intentions from the other&apos;s, and avoid as-
sumptions about the correctness or completeness of
the agents&apos; beliefs. In this paper, we present an algo-
rithm for plan recognition that is based on the Shared-
Plan model of collaboration (Grosz and Sidner, 1990;
Lochbaum et al., 1990) and that satisfies these con-
straints.
</bodyText>
<sectionHeader confidence="0.997666" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.985876849056604">
To make sense of each other&apos;s utterances, conversa-
tional participants must recognize the intentions be-
hind those utterances. Thus, a model of intended plan
recognition is an important component of a theory of
discourse understanding. The model must distinguish
each agent&apos;s beliefs and intentions from the other&apos;s and
avoid assumptions about the correctness or complete-
ness of the agents&apos; beliefs.
Early work on plan recognition in discourse, e.g.
Allen &amp; Perrault (1980); Sidner &amp; Israel (1981), was
based on work in Al planning systems, in particu-
lar the STRIPS formalism (Pikes and Nilsson, 1971).
However, as Pollack (1986) has argued, because these
systems do not differentiate between the beliefs and
intentions of the different conversational participants,
they are insufficient for modelling discourse. Although
Pollack proposes a model that does make this distinc-
tion, her model has other shortcomings. In particular,
it assumes a master/slave relationship between agents
(Grosz and Sidner, 1990) and that the inferring agent
has complete and accurate knowledge of domain ac-
tions. In addition, like many earlier systems, it relies
upon a set of heuristics to control the application of
plan inference rules.
In contrast, Kautz (1987; 1990) presented a theo-
retical formalization of the plan recognition problem,
*This research has been supported by U S WEST Ad-
vanced Technologies and by a Bellcore Graduate Fellow-
ship.
and a corresponding algorithm, in which the only con-
clusions that are drawn are those that are &amp;quot;absolutely
justified.&amp;quot; Although Kautz&apos;s work is quite elegant, it
too has several deficiencies as a model of plan recogni-
tion for discourse. In particular, it is a model of keyhole
recognition — the inferring agent observes the actions
of another agent without that second agent&apos;s knowl-
edge — rather than a model of intended recognition.
Furthermore, both the inferring and performing agents
are assumed to have complete and correct knowledge
of the domain.
In this paper, we present an algorithm for intended
recognition that is based on the SharedPlan model of
collaboration (Grosz and Sidner, 1990; Lochbaum et
al., 1990) and that, as a result, overcomes the limita-
tions of these previous models. We begin by briefly
presenting the action representation used by the algo-
rithm and then discussing the type of plan recogni-
tion necessary for the construction of a SharedPlan.
Next, we present the algorithm itself, and discuss an
initial implementation. Finally, because Kautz&apos;s plan
recognition algorithms are not necessarily tied to the
assumptions made by his formal model, we directly
compare our algorithm to his.
</bodyText>
<sectionHeader confidence="0.998314" genericHeader="method">
ACTION REPRESENTATION
</sectionHeader>
<bodyText confidence="0.98762375">
We use the action representation formally defined by
Balkanski (1990) for modelling collaborative actions.
We use the term act-type to refer to a type of action;
e.g. boiling water is an act-type that will be repre-
sented by boil(water). In addition to types of actions,
we also need to refer to the agents who will perform
those actions and the time interval over which they will
do so. We use the term activity to refer to this type
of informationl; e.g. Carol&apos;s boiling water over some
time interval (t1) is an activity that will be represented
by (boil(water),carol,t1). Throughout the rest of this
paper, we will follow the convention of denoting ar-
bitrary activities using uppercase Greek letters, while
using lowercase Greek letters to denote act-types. In
&apos;This terminology supersedes that used in (Lochbaum
et al., 1990).
</bodyText>
<page confidence="0.998865">
33
</page>
<table confidence="0.993658857142857">
Act-type Activity
Relations CGEN(71,72,C) GEN(r1,r2)
CENABLES(71,-y2,C) ENABLES(fi,r2)
Constructors sequence(71,...,7.) mri,...,r.)
simult(-yi ,...,-y.)
conjoined(71 ,...,-yn)
iteration(AX.7[X],{X1,...X.}) I(AX.11XJ,IX1,...X.})
</table>
<tableCaption confidence="0.999744">
Table 1: Act-type/Activity Relations and Constructors defined by Balkanski (1990)
</tableCaption>
<bodyText confidence="0.994944">
addition, lowercase letters denote the act-type of the
activity represented by the corresponding uppercase
letter, e.g. 7 act-type(F).
Balkanski also defines act-type and activity con-
structors and relations; e.g. sequence(boil(water),
add(noodles,water)) represents the sequence of doing
an act of type boil(water) followed by an act of type
add(noodles,water), while CGEN(mix(sauce,noodles),
make(pasta_dish),C) represents that the first act-type
conditionally generates the second (Goldman, 1970;
Pollack, 1986). Table 1 lists the act-type and corre-
sponding activity relations and constructors that will
be used in this paper.
Act-type constructors and relations are used in
specifying recipes. Following Pollack (1990), we use
the term recipe to refer to what an agent knows
when the agent knows a way of doing something.
As an example, a particular agent&apos;s recipe for lift-
ing a piano might be CGEN(simult(lift(foot(piano)),
lift (keyb oard(piano))) , lift (piano) , AG .[IG1=21); this
recipe encodes that simultaneously lifting the foot- and
keyboard ends of a piano results in lifting the piano,
provided that there are two agents doing the lifting.
For ease of presentation, we will sometimes represent
recipes graphically using different types of arrows to
represent specific act-type relations and constructors.
Figure 1 contains the graphical presentation of the pi-
ano lifting recipe.
</bodyText>
<equation confidence="0.941586714285714">
lift(piano)
= 2]
simult(lift(foot(piano)),lift(keyboard(piano)))
cit \c2
lift(foot(piano)) lift(keyboard(piano))
IC indicates generation subject to the condition C
ci/ indicates constituent i of a complex act-type
</equation>
<figureCaption confidence="0.998287">
Figure 1: A recipe for lifting a piano
</figureCaption>
<sectionHeader confidence="0.9985585" genericHeader="method">
THE SHARED PLAN AUGMENTATION
ALGORITHM
</sectionHeader>
<bodyText confidence="0.952843933333334">
A previous paper (Lochbaum et aL, 1990) describes
an augmentation algorithm based on Grosz and Sid-
ner&apos;s SharedPlan model of collaboration (Grosz and
Sidner, 1990) that delineates the ways in which an
agent&apos;s beliefs are affected by utterances made in the
context of collaboration. A portion of that algorithm
is repeated in Figure 2. In the discussion that follows,
we will assume the context specified by the algorithm.
SharedPlan*(Gi,G2,A,T1,T2) represents that G1 and
G2 have a partial SharedPlan at time T1 to perform
act-type A at time T2 (Grosz and Sidner, 1990).
Assume:
Act is an action of type 7,
G, designates the agent who communicates Prop(Act),
G3 designates the agent being modelled
</bodyText>
<equation confidence="0.645763">
j E {1,2}, i j,
SharedPlan*(Gi ,G2,A,T1 ,T2).
</equation>
<figureCaption confidence="0.535185">
4. Search own beliefs for Contributes(7,A) and where pos-
sible, more specific information as to how -y contributes
to A.
Figure 2: The SharedPlan Augmentation Algorithm
</figureCaption>
<bodyText confidence="0.999645333333333">
Step (4) of this algorithm is closely related to the
standard plan recognition problem. In this step, agent
Gj is trying to determine why agent Gi has mentioned
an act of type -y, i.e. Gj is trying to identify the role
Gi believes -y will play in their SharedPlan. In our
previous work, we did not specify the details of how
this reasoning was modelled. In this paper, we present
an algorithm that does so. The algorithm uses a new
construct: augmented rgraphs.
</bodyText>
<sectionHeader confidence="0.993251" genericHeader="method">
AUGMENTED RGRAPH CONSTRUCTION
</sectionHeader>
<bodyText confidence="0.9893965">
Agents Gi and Gj each bring to their collaboration pri-
vate beliefs about how to perform types of actions, i.e.
recipes for those actions. As they collaborate, a signifi-
cant portion of their communication is concerned with
deciding upon the types of actions that need to be per-
formed and how those actions are related. Thus, they
establish mutual belief in a recipe for action2. In ad-
dition, however, the agents must also determine which
2Agents do not necessarily discuss actions in a fixed or-
der (e.g. the order in which they appear in a recipe). Con-
sequently, our algorithm is not constrained to reasoning
about actions in a fixed order.
</bodyText>
<page confidence="0.995122">
34
</page>
<bodyText confidence="0.999994411764706">
agents will perform each action and the time inter-
val over which they will do so, in accordance with the
agency and timing constraints specified by their evolv-
ing jointly-held recipe. To model an agent&apos;s reasoning
in this collaborative situation, we introduce a dynamic
representation called an augmented recipe graph. The
construction of an augmented recipe graph corresponds
to the reasoning that an agent performs to determine
whether or not the performance of a particular activ-
ity makes sense in terms of the agent&apos;s recipes and the
evolving SharedPlan.
Augmented recipe graphs are comprised of two
parts, a recipe graph or rgraph, representing activities
and relations among them, and a set of constraints,
representing conditions on the agents and times of
those activities. An rgraph corresponds to a partic-
ular specification of a recipe. Whereas a recipe rep-
resents information about the performance, in the ab-
stract, of act-types, an rgraph represents more spe-
cialized information by including act-type performance
agents and times. An rgraph is a tree-like representa-
tion comprised of (1) nodes, representing activities and
(2) links between nodes, representing activity relations.
The structure of an rgraph mirrors the structure of the
recipe to which it corresponds: each activity and ac-
tivity relation in an rgraph is derived from the corre-
sponding act-type and act-type relation in its associ-
ated recipe, based on the correspondences in Table 1.
Because the constructors and relations used in specify-
ing recipes may impose agency and timing constraints
on the successful performance of act-types, the rgraph
representation is augmented by a set of constraints.
Following Kautz, we will use the term explaining to
refer to the process of creating an augmented rgraph.
</bodyText>
<sectionHeader confidence="0.993676" genericHeader="method">
AUGMENTED RGRAPH SCHEMAS
</sectionHeader>
<bodyText confidence="0.996656166666667">
To describe the explanation process, we will assume
that agents Gi and Gi are collaborating to achieve an
act-type A and Gi communicates a proposition from
which an activity F can be derived3 (cf. the assump-
tions of Figure 2). Gi &apos;s reasoning in this context is
modelled by building an augmented rgraph that ex-
plains how r might be related to A. This representa-
tion is constructed by searching each of G3 &apos;s recipes for
A to find a sequence of relations and constructors link-
ing 7 to A. Augmented rgraphs are constructed during
this search by creating appropriate nodes and links as
each act-type and relation in a recipe is encountered.
By considering each type of relation and construc-
tor that may appear in a recipe, we can specify gen-
eral schemas expressing the form that the correspond-
ing augmented rgraph must take. Table 2 contains
the schemas for each of the act-type relations and
31.&apos; need not include a complete agent or time specifica-
tion.
constructors4.
The algorithm for explaining an activity r according
to a particular recipe for A thus consists of consider-
ing in turn each relation and constructor in the recipe
linking 7 and A and using the appropriate schema
to incrementally build an augmented rgraph. Each
schema specifies an rgraph portion to create and the
constraints to associate with that rgraph. If agent
Gi knows multiple recipes for A, then the algorithm
attempts to create an augmented rgraph from each
recipe. Those augmented rgraphs that are successfully
created are maintained as possible explanations for F
until more information becomes available; they repre-
sent Gi &apos;s current beliefs about Gi&apos;s possible beliefs.
If at any time the set of constraints associated with
an augmented rgraph becomes unsatisfiable, a failure
occurs: the constraints stipulated by the recipe are not
met by the activities in the corresponding rgraph. This
failure corresponds to a discrepancy between agent
Gi &apos;s beliefs and those Gj has attributed to agent G.
On the basis of such a discrepancy, agent Gj might
query Gi, or might first consider the other recipes that
she knows for A (i.e. in an attempt to produce a suc-
cessful explanation using another recipe). The algo-
rithm follows the latter course of action. When a recipe
does not provide an explanation for F, it is eliminated
from consideration and the algorithm continues look-
ing for &amp;quot;valid&amp;quot; recipes.
To illustrate the algorithm, we will consider the
reasoning done by agent Pam in the dialogue in
Figure 3; we assume that Pam knows the recipe
given in Figure 1. To begin, we consider the ac-
tivity derived from utterance (3) of this discourse:
Fi=(lift(foot(piano)),{joe},t1), where ti is the time in-
terval over which the agents will lift the piano. To ex-
plain F1, the algorithm creates the augmented rgraph
shown in Figure 4. It begins by considering the other
act-types in the recipe to which 71=lift(foot(piano)) is
related. Because 71 is a component of a simultaneous
act-type, the simult schema is used to create nodes Ni,
N2, and the link between them. A constraint of this
schema is that the constituents of the complex activ-
ity represented by node N2 have the same time. This
constraint is modelled directly in the rgraph by creat-
ing the activity corresponding to lift(keyboard(piano))
to have the same time as F1 . No information about
the agent of this activity is known, however, so a vari-
able, G1, is used to represent the agent. Next, because
the simultaneous act-type is related by a CGEN rela-
tion to lift (piano), the CGEN schema is used to create
node N3 and the link between N2 and N3. The first
two constraints of the schema are satisfied by creating
node N3 such that its activity&apos;s agent and time are the
</bodyText>
<footnote confidence="0.781917">
4The technical report (Lochbaum, 1991) contains a more
detailed discussion of the derivation of these schemas from
the definitions given by Balkanski (1990).
</footnote>
<page confidence="0.967566">
35
</page>
<table confidence="0.999812692307692">
Recipe Augmented Rgraph
Rgraph Constraints
CGEN(-y, 6 ,C) (6, G ,T) G=agent(r)
I GEN T=time(r)
r HOLDSI(C,G,T)
HOLDS&apos;(C,agent(r),time(r))
BEFORE(time(r),T)
CENABLES(7, 6,C) (6, G,T)
-ft ENABLES
r
sequence(n &apos;72, •..7n) Kw, r2, ..., r,0=A v.; BEFORE(time(rAtime(r3+1))
I c, agent(A)=U jagent(F,)
F, time(A)=coverinterval({time(r )})5
agent(A)=U jagent(r3)
time(A)=cover_interval({time(F, )))
conjoined(ri , 72, ...7n) ic(ri, r2, ...ro)---6,
I c,
I&apos;,
simult(71, 72, —70 K(ri, r2, ...rn)=6, vi time(r,)=time(r,+, )
1 c, agent(A)=U3agent(F,)
r, time(11)=cover_interva1({time(r, )})
agent(A)=agent(r)
time(A)=time(r)
iteration( X .-y[X], iox.r[x], {X1, ...X,,})=A
{Xi , X2, ...Xn)) I ci
[AX.rmx,
</table>
<tableCaption confidence="0.99463">
Table 2: Rgraph Schemas
</tableCaption>
<bodyText confidence="0.9996235">
same as node N2&apos;s. The third constraint is instantiated
and associated with the rgraph.
</bodyText>
<listItem confidence="0.999844875">
(1) Joe: I want to lift the piano.
(2) Pam: OK.
(3) Joe: On the count of three, I&apos;ll pick up this
[deictic to foot] end,
(4) and you pick up that
[deictic to keyboard] end.
(5) Pam: OK.
(6) Joe: One, two, three!
</listItem>
<figureCaption confidence="0.992437">
Figure 3: A sample discourse
</figureCaption>
<equation confidence="0.783313857142857">
Rgraph:
N3:(lift(piano),{joe} U GI ,t1)
GEN
N2:KWift(foot(piano)),{joe},t1),(lift(keyboard(piano)),G1 ,t1))
ci
N1: (lift(foot(piano)), {joe} ,ti)
Constraints: {HOLDS&apos;(AG.[IGI = 2],{joe} u
</equation>
<figureCaption confidence="0.9581145">
Figure 4: Augmented rgraph explaining (lift(foot(pi-
ano)),{joebtl)
</figureCaption>
<sectionHeader confidence="0.985526" genericHeader="method">
MERGING AUGMENTED RGRAPHS
</sectionHeader>
<bodyText confidence="0.997564428571429">
As discussed thus far, the construction algorithm pro-
duces an explanation for how an activity r is related
to a goal A. However, to properly model collaboration,
one must also take into account the context of previ-
ously discussed activities. Thus, we now address how
the algorithm explains an activity r in this context.
Because Gi and Gj are collaborating, it is appropri-
ate for Gj to assume that any activity mentioned by
Gi is part of doing A (or at least that Gi believes that
it is). If this is not the case, then Gi must explicitly
indicate that to Gj (Grosz and Sidner, 1990). Given
this assumption, G, &apos;s task is to produce a coherent ex-
planation, based upon her recipes, for how all of the
activities that she and Gi discuss are related to A.
We incorporate this model of G, &apos;s task into the algo-
rithm by requiring that each recipe have at most one
corresponding augmented rgraph, and implement this
restriction as follows: whenever an rgraph node corre-
sponding to a particular act-type in a recipe is created,
the construction algorithm checks to see whether there
is already another node (in a previously constructed
rgraph) corresponding to that act-type. If so, the al-
gorithm tries to merge the augmented rgraph currently
under construction with the previous one, in part by
merging these two nodes. In so doing, it combines the
information contained in the separate explanations.
The processing of utterance (4) in the sample di-
alogue illustrates this procedure. The activity de-
rived from utterance (4) is r2=-(lift(keyboard(piano)),
{pam}, t1). The initial augmented rgraph portion cre-
ated in explaining this activity is shown in Figure
5. Node N5 of the rgraph corresponds to the act-
type simult(lift(foot(piano)),lift(keyboard(piano))) and
includes information derived from F2. But the rgraph
(in Figure 4) previously constructed in explaining 1&apos;1
also includes a node, N2, corresponding to this act-type
(and containing information derived from F1). Rather
than continuing with an independent explanation for
F2, the algorithm attempts to combine the information
5The function coverintervaI takes a set of time intervals
as an argument and returns a time interval spanning the
set (Balkanski, 1990).
</bodyText>
<page confidence="0.995639">
36
</page>
<bodyText confidence="0.949242">
from the two activities by merging their augmented
rgraphs.
</bodyText>
<figure confidence="0.802629">
Rgraph:
N5:KWift(foot(piano)),G2,t1),(lift(keyboard(piano)),{pam},t1))
C2
N4:(lift(keyboard(piano)),{pam} tl)
Constraints:{}
</figure>
<figureCaption confidence="0.9662985">
Figure 5: Augmented rgraph partially explaining
(lift(keyboard(piano)),{pam},t1)
</figureCaption>
<bodyText confidence="0.999916">
Two augmented rgraphs are merged by first merg-
ing their rgraphs at the two nodes corresponding to
the same act-type (e.g. nodes N5 and N2), and then
merging their constraints. Two nodes are merged by
unifying the activities they represent. If this unifica-
tion is successful, then the two sets of constraints are
merged by taking their union and adding to the result-
ing set the equality constraints expressing the bindings
used in the unification. If this new set of constraints
is satisfiable, then the bindings used in the unification
are applied to the remainder of the two rgraphs. Oth-
erwise, the algorithm fails: the activities represented in
the two rgraphs are not compatible. In this case, be-
cause the recipe corresponding to the rgraphs does not
provide an explanation for all of the activities discussed
by the agents, it is removed from further consideration.
The augmented rgraph resulting from merging the two
augmented rgraphs in Figures 4 and Figure 5 is shown
in Figure 6.
</bodyText>
<equation confidence="0.851034714285714">
Rgraph:
N3:(lift(piano),{joe,pam},t1)
T GEN
N2:KWift(foot(piano)),{joe},t1),(lift(keyboard(piano)),{pam),t1))
/ \c2
NI:(lift(foot(piano)),{joe},t1) N4:(lift(keyboard(piano)),Ipam},t1)
Constraints: {HOLDV(AG.[IGI = 2],{joe} U Gi,t1), Gi={Pam}}
</equation>
<figureCaption confidence="0.975744">
Figure 6: Augmented rgraph resulting from merging
the augmented rgraphs in Figures 4 and 5
</figureCaption>
<sectionHeader confidence="0.978524" genericHeader="method">
IMPLEMENTATION
</sectionHeader>
<bodyText confidence="0.9999343">
An implementation of the algorithm is currently un-
derway using the constraint logic programming lan-
guage, CLP(R) (Jaffar and Lasses, 1987; Jaffar and
Michaylov, 1987). Syntactically, this language is very
similar to Prolog, except that constraints on real-
valued variables may be intermixed with literals in
rules and goals. Semantically, CLP(R) is a generaliza-
tion of Prolog in which unifiability is replaced by solv-
ability of constraints. For example, in Prolog, the pred-
icate X &lt; 3 fails if X is uninstantiated. In CLP(R),
however, X &lt; 3 is a constraint, which is solvable if
there exists a substitution for X that makes it true.
Because many of the augmented rgraph constraints
are relations over real-valued variables (e.g. the time
of one activity must be before the time of another),
CLP(R) is a very appealing language in which to im-
plement the augmented rgraph construction process.
The algorithm for implementing this process in a logic
programming language, however, differs markedly from
the intuitive algorithm described in this paper.
</bodyText>
<sectionHeader confidence="0.883926" genericHeader="method">
RGRAPHS AND CONSTRAINTS VS. EGRAPHS
</sectionHeader>
<bodyText confidence="0.9999366875">
Kautz (1987) presented several graph-based algorithms
derived from his formal model of plan recognition. In
Kautz&apos;s algorithms, an explanation for an observation
is represented in the form of an explanation graph or
egraph. Although the term rgraph was chosen to par-
allel Kautz&apos;s terminology, the two representations and
algorithms are quite different in scope.
Two capabilities that an algorithm for plan recog-
nition in collaborative discourse must possess are the
abilities to represent joint actions of multiple agents
and to reason about hypothetical actions. In addition,
such an algorithm may, and for efficiency should, ex-
ploit assumptions of the communicative situation. The
augmented rgraph representation and algorithm meet
these qualifications, whereas the egraph representation
and algorithms do not.
The underlying action representation used in r-
graphs is capable of representing complex relations
among acts, including simultaneity and sequentiality.
In addition, relations among the agents and times of
acts may also be expressed. The action representation
used in egraphs is, like that in STRIPS, simple step de-
composition. Though it is possible to represent simul-
taneous or sequential actions, the egraph representa-
tion can only model such actions if they are performed
by the same agent. This restriction is in keeping with
Kautz&apos;s model of keyhole recognition, but is insuffi-
cient for modelling intended recognition in multiagent
settings.
Rgraphs are only a part of our representation. Aug-
mented rgraphs also include constraints on the activ-
ities represented in the rgraph. Kautz does not have
such an extended representation. Although he uses
constraints to guide egraph construction, because they
are not part of his representation, his algorithm can
only check their satisfaction locally. In contrast, by col-
lecting together all of the constraints introduced by the
different relations or constructors in a recipe, we can
exploit interactions among them to determine unsat-
isfiability earlier than an algorithm which checks con-
straints locally. Kautz&apos;s algorithm checks each event&apos;s
constraints independently and hence cannot determine
satisfiability until a constraint is ground; it cannot, for
example, reason that one constraint makes another un-
satisfiable.
Because agents involved in collaboration dedicate a
significant portion of their time to discussing the ac-
tions they need to perform, an algorithm for mod-
</bodyText>
<page confidence="0.997915">
37
</page>
<bodyText confidence="0.999971181818182">
elling plan recognition in discourse must model rea-
soning about hypothetical and only partially specified
activities. Because the augmented rgraph representa-
tion allows variables to stand for agents and times in
both activities and constraints, it meets this criteria.
Kautz&apos;s algorithm, however, models reasoning about
actual event occurrences. Consequently, the egraph
representation does not include a means of referring to
indefinite specifications.
In modelling collaboration, unless explicitly indi-
cated otherwise, it is appropriate to assume that all
acts are related. In the augmented rgraph construction
algorithm, we exploit this by restricting the reasoning
done by the algorithm to recipes for A, and by combin-
ing explanations for acts as soon as possible. Kautz&apos;s
algorithm, however, because it is based on a model of
keyhole recognition, does not and cannot make use of
this assumption. Upon each observation, an indepen-
dent egraph must be created explaining all possible
uses of the observed action. Various hypotheses are
then drawn and maintained as to how the action might
be related to other observed actions.
</bodyText>
<sectionHeader confidence="0.998019" genericHeader="conclusions">
CONCLUSIONS Sz FUTURE DIRECTIONS
</sectionHeader>
<bodyText confidence="0.999992608695652">
To achieve their joint goal, collaborating agents must
have mutual beliefs about the types of actions they will
perform to achieve that goal, the relations among those
actions, the agents who will perform the actions, and
the time interval over which they will do so. In this
paper, we have presented a representation, augmented
rgraphs, modelling this information and have provided
an algorithm for constructing and reasoning with it.
The steps of the construction algorithm parallel the
reasoning that an agent performs in determining the
relevance of an activity. The algorithm does not re-
quire that activities be discussed in a fixed order and
allows for reasoning about hypothetical or only par-
tially specified activities.
Future work includes: (1) adding other types of con-
straints (e.g. restrictions on the parameters of actions)
to the representation; (2) using the augmented rgraph
representation in identifying, on the basis of unsatisfi-
able constraints, particular discrepancies in the agents&apos;
beliefs; (3) identifying information conveyed in Gi&apos;s
utterances as to how he believes two acts are related
(Balkanski, 1991) and incorporating that information
into our model of Gi&apos;s reasoning.
</bodyText>
<sectionHeader confidence="0.999209" genericHeader="acknowledgments">
ACKNOWLEDGMENTS
</sectionHeader>
<bodyText confidence="0.9993005">
I would like to thank Cecile Balkanski, Barbara Grosz,
Stuart Shieber, and Candy Sidner for many helpful
discussions and comments on the research presented
in this paper.
</bodyText>
<sectionHeader confidence="0.997846" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.996703645833334">
Allen, J. and Perrault, C. 1980. Analyzing intention
in utterances. Artificial Intelligence, 15(3):143-178.
Balkanski, C. T. 1990. Modelling act-type relations
in collaborative activity. Technical Report TR-23-
90, Harvard University.
Balkanski, C. T. 1991. Logical form of complex sen-
tences in task-oriented dialogues. In Proceedings of
the 29th Annual Meeting of the ACL, Student Ses-
sion, Berkeley, CA.
Fikes, R. E. and Nilsson, N. J. 1971. STRIPS: A new
approach to the application of theorem proving to
problem solving. Artificial Intelligence, 2:189-208.
Goldman, A. I. 1970. A Theory Of Human Action.
Princeton University Press.
Grosz, B. and Sidner, C. 1990. Plans for discourse.
In Cohen, P., Morgan, J., and Pollack, M., editors,
Intentions in Communication. MIT Press.
Jaffar, J. and Lassez, J.-L. 1987. Constraint logic
programming. In Proceedings of the 14th ACM
Symposium on the Principles of Programming Lan-
guages, pages 111-119, Munich.
Jaffar, J. and Michaylov, S. 1987. Methodology and
implementation of a CLP system. In Proceedings of
the 4th International Conference on Logic Program-
ming, pages 196-218, Melbourne. MIT Press.
Kautz, H. A. 1987. A Formal Theory of Plan Recog-
nition. PhD thesis, University of Rochester.
Kautz, H. A. 1990. A circumscriptive theory of
plan recognition. In Cohen, P., Morgan, J., and
Pollack, M., editors, Intentions in Communication.
MIT Press.
Lochbaum, K. E., Grosz, B. J., and Sidner, C. L.
1990. Models of plans to support communica-
tion: An initial report. In Proceedings of AAAI-90,
Boston, MA.
Lochbaum, K. E. 1991. Plan recognition in collabo-
rative discourse. Technical report, Harvard Univer-
sity.
Pollack, M. E. June 1986. A model of plan inference
that distinguishes between the beliefs of actors and
observers. In Proceedings of the 24th Annual Meeting
of the ACL.
Pollack, M. E. 1990. Plans as complex mental at-
titudes. In Cohen, P., Morgan, J., and Pollack, M.,
editors, Intentions in Communication. MIT Press.
Sidner, C. and Israel, D. J. 1981. Recognizing in-
tended meaning and speakers&apos; plans. In Proceedings
of IJCAI-81.
</reference>
<page confidence="0.999328">
38
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.616376">
<title confidence="0.9895635">AN ALGORITHM FOR PLAN RECOGNITION IN COLLABORATIVE DISCOURSE*</title>
<author confidence="0.999633">Karen E Lochbaum</author>
<affiliation confidence="0.9954365">Aiken Computation Lab Harvard University</affiliation>
<address confidence="0.9992765">33 Oxford Street Cambridge, MA 02138</address>
<email confidence="0.999782">kel©harvard.harvard.edu</email>
<abstract confidence="0.9258953">A model of plan recognition in discourse must be based on intended recognition, distinguish each agent&apos;s beliefs and intentions from the other&apos;s, and avoid assumptions about the correctness or completeness of the agents&apos; beliefs. In this paper, we present an algorithm for plan recognition that is based on the Shared- Plan model of collaboration (Grosz and Sidner, 1990; al., and that satisfies these constraints.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allen</author>
<author>C Perrault</author>
</authors>
<title>Analyzing intention in utterances.</title>
<date>1980</date>
<journal>Artificial Intelligence,</journal>
<pages>15--3</pages>
<contexts>
<context position="1106" citStr="Allen &amp; Perrault (1980)" startWordPosition="164" endWordPosition="167">at is based on the SharedPlan model of collaboration (Grosz and Sidner, 1990; Lochbaum et al., 1990) and that satisfies these constraints. INTRODUCTION To make sense of each other&apos;s utterances, conversational participants must recognize the intentions behind those utterances. Thus, a model of intended plan recognition is an important component of a theory of discourse understanding. The model must distinguish each agent&apos;s beliefs and intentions from the other&apos;s and avoid assumptions about the correctness or completeness of the agents&apos; beliefs. Early work on plan recognition in discourse, e.g. Allen &amp; Perrault (1980); Sidner &amp; Israel (1981), was based on work in Al planning systems, in particular the STRIPS formalism (Pikes and Nilsson, 1971). However, as Pollack (1986) has argued, because these systems do not differentiate between the beliefs and intentions of the different conversational participants, they are insufficient for modelling discourse. Although Pollack proposes a model that does make this distinction, her model has other shortcomings. In particular, it assumes a master/slave relationship between agents (Grosz and Sidner, 1990) and that the inferring agent has complete and accurate knowledge </context>
</contexts>
<marker>Allen, Perrault, 1980</marker>
<rawString>Allen, J. and Perrault, C. 1980. Analyzing intention in utterances. Artificial Intelligence, 15(3):143-178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C T Balkanski</author>
</authors>
<title>Modelling act-type relations in collaborative activity.</title>
<date>1990</date>
<tech>Technical Report TR-23-90,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="3368" citStr="Balkanski (1990)" startWordPosition="521" endWordPosition="522">, 1990; Lochbaum et al., 1990) and that, as a result, overcomes the limitations of these previous models. We begin by briefly presenting the action representation used by the algorithm and then discussing the type of plan recognition necessary for the construction of a SharedPlan. Next, we present the algorithm itself, and discuss an initial implementation. Finally, because Kautz&apos;s plan recognition algorithms are not necessarily tied to the assumptions made by his formal model, we directly compare our algorithm to his. ACTION REPRESENTATION We use the action representation formally defined by Balkanski (1990) for modelling collaborative actions. We use the term act-type to refer to a type of action; e.g. boiling water is an act-type that will be represented by boil(water). In addition to types of actions, we also need to refer to the agents who will perform those actions and the time interval over which they will do so. We use the term activity to refer to this type of informationl; e.g. Carol&apos;s boiling water over some time interval (t1) is an activity that will be represented by (boil(water),carol,t1). Throughout the rest of this paper, we will follow the convention of denoting arbitrary activiti</context>
<context position="13837" citStr="Balkanski (1990)" startWordPosition="2200" endWordPosition="2201">ing to lift(keyboard(piano)) to have the same time as F1 . No information about the agent of this activity is known, however, so a variable, G1, is used to represent the agent. Next, because the simultaneous act-type is related by a CGEN relation to lift (piano), the CGEN schema is used to create node N3 and the link between N2 and N3. The first two constraints of the schema are satisfied by creating node N3 such that its activity&apos;s agent and time are the 4The technical report (Lochbaum, 1991) contains a more detailed discussion of the derivation of these schemas from the definitions given by Balkanski (1990). 35 Recipe Augmented Rgraph Rgraph Constraints CGEN(-y, 6 ,C) (6, G ,T) G=agent(r) I GEN T=time(r) r HOLDSI(C,G,T) HOLDS&apos;(C,agent(r),time(r)) BEFORE(time(r),T) CENABLES(7, 6,C) (6, G,T) -ft ENABLES r sequence(n &apos;72, •..7n) Kw, r2, ..., r,0=A v.; BEFORE(time(rAtime(r3+1)) I c, agent(A)=U jagent(F,) F, time(A)=coverinterval({time(r )})5 agent(A)=U jagent(r3) time(A)=cover_interval({time(F, ))) conjoined(ri , 72, ...7n) ic(ri, r2, ...ro)---6, I c, I&apos;, simult(71, 72, —70 K(ri, r2, ...rn)=6, vi time(r,)=time(r,+, ) 1 c, agent(A)=U3agent(F,) r, time(11)=cover_interva1({time(r, )}) agent(A)=agent(r)</context>
<context position="17381" citStr="Balkanski, 1990" startWordPosition="2744" endWordPosition="2745">eated in explaining this activity is shown in Figure 5. Node N5 of the rgraph corresponds to the acttype simult(lift(foot(piano)),lift(keyboard(piano))) and includes information derived from F2. But the rgraph (in Figure 4) previously constructed in explaining 1&apos;1 also includes a node, N2, corresponding to this act-type (and containing information derived from F1). Rather than continuing with an independent explanation for F2, the algorithm attempts to combine the information 5The function coverintervaI takes a set of time intervals as an argument and returns a time interval spanning the set (Balkanski, 1990). 36 from the two activities by merging their augmented rgraphs. Rgraph: N5:KWift(foot(piano)),G2,t1),(lift(keyboard(piano)),{pam},t1)) C2 N4:(lift(keyboard(piano)),{pam} tl) Constraints:{} Figure 5: Augmented rgraph partially explaining (lift(keyboard(piano)),{pam},t1) Two augmented rgraphs are merged by first merging their rgraphs at the two nodes corresponding to the same act-type (e.g. nodes N5 and N2), and then merging their constraints. Two nodes are merged by unifying the activities they represent. If this unification is successful, then the two sets of constraints are merged by taking </context>
</contexts>
<marker>Balkanski, 1990</marker>
<rawString>Balkanski, C. T. 1990. Modelling act-type relations in collaborative activity. Technical Report TR-23-90, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C T Balkanski</author>
</authors>
<title>Logical form of complex sentences in task-oriented dialogues.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the ACL,</booktitle>
<location>Student Session, Berkeley, CA.</location>
<marker>Balkanski, 1991</marker>
<rawString>Balkanski, C. T. 1991. Logical form of complex sentences in task-oriented dialogues. In Proceedings of the 29th Annual Meeting of the ACL, Student Session, Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Fikes</author>
<author>N J Nilsson</author>
</authors>
<title>STRIPS: A new approach to the application of theorem proving to problem solving.</title>
<date>1971</date>
<journal>Artificial Intelligence,</journal>
<pages>2--189</pages>
<marker>Fikes, Nilsson, 1971</marker>
<rawString>Fikes, R. E. and Nilsson, N. J. 1971. STRIPS: A new approach to the application of theorem proving to problem solving. Artificial Intelligence, 2:189-208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A I Goldman</author>
</authors>
<title>A Theory Of Human Action.</title>
<date>1970</date>
<publisher>Princeton University Press.</publisher>
<contexts>
<context position="4939" citStr="Goldman, 1970" startWordPosition="737" endWordPosition="738">{X1,...X.}) I(AX.11XJ,IX1,...X.}) Table 1: Act-type/Activity Relations and Constructors defined by Balkanski (1990) addition, lowercase letters denote the act-type of the activity represented by the corresponding uppercase letter, e.g. 7 act-type(F). Balkanski also defines act-type and activity constructors and relations; e.g. sequence(boil(water), add(noodles,water)) represents the sequence of doing an act of type boil(water) followed by an act of type add(noodles,water), while CGEN(mix(sauce,noodles), make(pasta_dish),C) represents that the first act-type conditionally generates the second (Goldman, 1970; Pollack, 1986). Table 1 lists the act-type and corresponding activity relations and constructors that will be used in this paper. Act-type constructors and relations are used in specifying recipes. Following Pollack (1990), we use the term recipe to refer to what an agent knows when the agent knows a way of doing something. As an example, a particular agent&apos;s recipe for lifting a piano might be CGEN(simult(lift(foot(piano)), lift (keyb oard(piano))) , lift (piano) , AG .[IG1=21); this recipe encodes that simultaneously lifting the foot- and keyboard ends of a piano results in lifting the pia</context>
</contexts>
<marker>Goldman, 1970</marker>
<rawString>Goldman, A. I. 1970. A Theory Of Human Action. Princeton University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Grosz</author>
<author>C Sidner</author>
</authors>
<title>Plans for discourse.</title>
<date>1990</date>
<booktitle>Intentions in Communication.</booktitle>
<editor>In Cohen, P., Morgan, J., and Pollack, M., editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1640" citStr="Grosz and Sidner, 1990" startWordPosition="243" endWordPosition="246">gents&apos; beliefs. Early work on plan recognition in discourse, e.g. Allen &amp; Perrault (1980); Sidner &amp; Israel (1981), was based on work in Al planning systems, in particular the STRIPS formalism (Pikes and Nilsson, 1971). However, as Pollack (1986) has argued, because these systems do not differentiate between the beliefs and intentions of the different conversational participants, they are insufficient for modelling discourse. Although Pollack proposes a model that does make this distinction, her model has other shortcomings. In particular, it assumes a master/slave relationship between agents (Grosz and Sidner, 1990) and that the inferring agent has complete and accurate knowledge of domain actions. In addition, like many earlier systems, it relies upon a set of heuristics to control the application of plan inference rules. In contrast, Kautz (1987; 1990) presented a theoretical formalization of the plan recognition problem, *This research has been supported by U S WEST Advanced Technologies and by a Bellcore Graduate Fellowship. and a corresponding algorithm, in which the only conclusions that are drawn are those that are &amp;quot;absolutely justified.&amp;quot; Although Kautz&apos;s work is quite elegant, it too has several </context>
<context position="6290" citStr="Grosz and Sidner, 1990" startWordPosition="937" endWordPosition="940">lly using different types of arrows to represent specific act-type relations and constructors. Figure 1 contains the graphical presentation of the piano lifting recipe. lift(piano) = 2] simult(lift(foot(piano)),lift(keyboard(piano))) cit \c2 lift(foot(piano)) lift(keyboard(piano)) IC indicates generation subject to the condition C ci/ indicates constituent i of a complex act-type Figure 1: A recipe for lifting a piano THE SHARED PLAN AUGMENTATION ALGORITHM A previous paper (Lochbaum et aL, 1990) describes an augmentation algorithm based on Grosz and Sidner&apos;s SharedPlan model of collaboration (Grosz and Sidner, 1990) that delineates the ways in which an agent&apos;s beliefs are affected by utterances made in the context of collaboration. A portion of that algorithm is repeated in Figure 2. In the discussion that follows, we will assume the context specified by the algorithm. SharedPlan*(Gi,G2,A,T1,T2) represents that G1 and G2 have a partial SharedPlan at time T1 to perform act-type A at time T2 (Grosz and Sidner, 1990). Assume: Act is an action of type 7, G, designates the agent who communicates Prop(Act), G3 designates the agent being modelled j E {1,2}, i j, SharedPlan*(Gi ,G2,A,T1 ,T2). 4. Search own belie</context>
<context position="15748" citStr="Grosz and Sidner, 1990" startWordPosition="2486" endWordPosition="2489">ano)),{joebtl) MERGING AUGMENTED RGRAPHS As discussed thus far, the construction algorithm produces an explanation for how an activity r is related to a goal A. However, to properly model collaboration, one must also take into account the context of previously discussed activities. Thus, we now address how the algorithm explains an activity r in this context. Because Gi and Gj are collaborating, it is appropriate for Gj to assume that any activity mentioned by Gi is part of doing A (or at least that Gi believes that it is). If this is not the case, then Gi must explicitly indicate that to Gj (Grosz and Sidner, 1990). Given this assumption, G, &apos;s task is to produce a coherent explanation, based upon her recipes, for how all of the activities that she and Gi discuss are related to A. We incorporate this model of G, &apos;s task into the algorithm by requiring that each recipe have at most one corresponding augmented rgraph, and implement this restriction as follows: whenever an rgraph node corresponding to a particular act-type in a recipe is created, the construction algorithm checks to see whether there is already another node (in a previously constructed rgraph) corresponding to that act-type. If so, the alg</context>
</contexts>
<marker>Grosz, Sidner, 1990</marker>
<rawString>Grosz, B. and Sidner, C. 1990. Plans for discourse. In Cohen, P., Morgan, J., and Pollack, M., editors, Intentions in Communication. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jaffar</author>
<author>J-L Lassez</author>
</authors>
<title>Constraint logic programming.</title>
<date>1987</date>
<booktitle>In Proceedings of the 14th ACM Symposium on the Principles of Programming Languages,</booktitle>
<pages>111--119</pages>
<location>Munich.</location>
<marker>Jaffar, Lassez, 1987</marker>
<rawString>Jaffar, J. and Lassez, J.-L. 1987. Constraint logic programming. In Proceedings of the 14th ACM Symposium on the Principles of Programming Languages, pages 111-119, Munich.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jaffar</author>
<author>S Michaylov</author>
</authors>
<title>Methodology and implementation of a CLP system.</title>
<date>1987</date>
<booktitle>In Proceedings of the 4th International Conference on Logic Programming,</booktitle>
<pages>196--218</pages>
<publisher>MIT Press.</publisher>
<location>Melbourne.</location>
<contexts>
<context position="19155" citStr="Jaffar and Michaylov, 1987" startWordPosition="2988" endWordPosition="2991">gmented rgraph resulting from merging the two augmented rgraphs in Figures 4 and Figure 5 is shown in Figure 6. Rgraph: N3:(lift(piano),{joe,pam},t1) T GEN N2:KWift(foot(piano)),{joe},t1),(lift(keyboard(piano)),{pam),t1)) / \c2 NI:(lift(foot(piano)),{joe},t1) N4:(lift(keyboard(piano)),Ipam},t1) Constraints: {HOLDV(AG.[IGI = 2],{joe} U Gi,t1), Gi={Pam}} Figure 6: Augmented rgraph resulting from merging the augmented rgraphs in Figures 4 and 5 IMPLEMENTATION An implementation of the algorithm is currently underway using the constraint logic programming language, CLP(R) (Jaffar and Lasses, 1987; Jaffar and Michaylov, 1987). Syntactically, this language is very similar to Prolog, except that constraints on realvalued variables may be intermixed with literals in rules and goals. Semantically, CLP(R) is a generalization of Prolog in which unifiability is replaced by solvability of constraints. For example, in Prolog, the predicate X &lt; 3 fails if X is uninstantiated. In CLP(R), however, X &lt; 3 is a constraint, which is solvable if there exists a substitution for X that makes it true. Because many of the augmented rgraph constraints are relations over real-valued variables (e.g. the time of one activity must be befor</context>
</contexts>
<marker>Jaffar, Michaylov, 1987</marker>
<rawString>Jaffar, J. and Michaylov, S. 1987. Methodology and implementation of a CLP system. In Proceedings of the 4th International Conference on Logic Programming, pages 196-218, Melbourne. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H A Kautz</author>
</authors>
<title>A Formal Theory of Plan Recognition.</title>
<date>1987</date>
<tech>PhD thesis,</tech>
<institution>University of Rochester.</institution>
<contexts>
<context position="1876" citStr="Kautz (1987" startWordPosition="284" endWordPosition="285">s argued, because these systems do not differentiate between the beliefs and intentions of the different conversational participants, they are insufficient for modelling discourse. Although Pollack proposes a model that does make this distinction, her model has other shortcomings. In particular, it assumes a master/slave relationship between agents (Grosz and Sidner, 1990) and that the inferring agent has complete and accurate knowledge of domain actions. In addition, like many earlier systems, it relies upon a set of heuristics to control the application of plan inference rules. In contrast, Kautz (1987; 1990) presented a theoretical formalization of the plan recognition problem, *This research has been supported by U S WEST Advanced Technologies and by a Bellcore Graduate Fellowship. and a corresponding algorithm, in which the only conclusions that are drawn are those that are &amp;quot;absolutely justified.&amp;quot; Although Kautz&apos;s work is quite elegant, it too has several deficiencies as a model of plan recognition for discourse. In particular, it is a model of keyhole recognition — the inferring agent observes the actions of another agent without that second agent&apos;s knowledge — rather than a model of in</context>
<context position="20085" citStr="Kautz (1987)" startWordPosition="3140" endWordPosition="3141">fails if X is uninstantiated. In CLP(R), however, X &lt; 3 is a constraint, which is solvable if there exists a substitution for X that makes it true. Because many of the augmented rgraph constraints are relations over real-valued variables (e.g. the time of one activity must be before the time of another), CLP(R) is a very appealing language in which to implement the augmented rgraph construction process. The algorithm for implementing this process in a logic programming language, however, differs markedly from the intuitive algorithm described in this paper. RGRAPHS AND CONSTRAINTS VS. EGRAPHS Kautz (1987) presented several graph-based algorithms derived from his formal model of plan recognition. In Kautz&apos;s algorithms, an explanation for an observation is represented in the form of an explanation graph or egraph. Although the term rgraph was chosen to parallel Kautz&apos;s terminology, the two representations and algorithms are quite different in scope. Two capabilities that an algorithm for plan recognition in collaborative discourse must possess are the abilities to represent joint actions of multiple agents and to reason about hypothetical actions. In addition, such an algorithm may, and for effi</context>
</contexts>
<marker>Kautz, 1987</marker>
<rawString>Kautz, H. A. 1987. A Formal Theory of Plan Recognition. PhD thesis, University of Rochester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H A Kautz</author>
</authors>
<title>A circumscriptive theory of plan recognition.</title>
<date>1990</date>
<booktitle>Intentions in Communication.</booktitle>
<editor>In Cohen, P., Morgan, J., and Pollack, M., editors,</editor>
<publisher>MIT Press.</publisher>
<marker>Kautz, 1990</marker>
<rawString>Kautz, H. A. 1990. A circumscriptive theory of plan recognition. In Cohen, P., Morgan, J., and Pollack, M., editors, Intentions in Communication. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K E Lochbaum</author>
<author>B J Grosz</author>
<author>C L Sidner</author>
</authors>
<title>Models of plans to support communication: An initial report.</title>
<date>1990</date>
<booktitle>In Proceedings of AAAI-90,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="2782" citStr="Lochbaum et al., 1990" startWordPosition="429" endWordPosition="432">solutely justified.&amp;quot; Although Kautz&apos;s work is quite elegant, it too has several deficiencies as a model of plan recognition for discourse. In particular, it is a model of keyhole recognition — the inferring agent observes the actions of another agent without that second agent&apos;s knowledge — rather than a model of intended recognition. Furthermore, both the inferring and performing agents are assumed to have complete and correct knowledge of the domain. In this paper, we present an algorithm for intended recognition that is based on the SharedPlan model of collaboration (Grosz and Sidner, 1990; Lochbaum et al., 1990) and that, as a result, overcomes the limitations of these previous models. We begin by briefly presenting the action representation used by the algorithm and then discussing the type of plan recognition necessary for the construction of a SharedPlan. Next, we present the algorithm itself, and discuss an initial implementation. Finally, because Kautz&apos;s plan recognition algorithms are not necessarily tied to the assumptions made by his formal model, we directly compare our algorithm to his. ACTION REPRESENTATION We use the action representation formally defined by Balkanski (1990) for modelling</context>
<context position="4127" citStr="Lochbaum et al., 1990" startWordPosition="646" endWordPosition="649">be represented by boil(water). In addition to types of actions, we also need to refer to the agents who will perform those actions and the time interval over which they will do so. We use the term activity to refer to this type of informationl; e.g. Carol&apos;s boiling water over some time interval (t1) is an activity that will be represented by (boil(water),carol,t1). Throughout the rest of this paper, we will follow the convention of denoting arbitrary activities using uppercase Greek letters, while using lowercase Greek letters to denote act-types. In &apos;This terminology supersedes that used in (Lochbaum et al., 1990). 33 Act-type Activity Relations CGEN(71,72,C) GEN(r1,r2) CENABLES(71,-y2,C) ENABLES(fi,r2) Constructors sequence(71,...,7.) mri,...,r.) simult(-yi ,...,-y.) conjoined(71 ,...,-yn) iteration(AX.7[X],{X1,...X.}) I(AX.11XJ,IX1,...X.}) Table 1: Act-type/Activity Relations and Constructors defined by Balkanski (1990) addition, lowercase letters denote the act-type of the activity represented by the corresponding uppercase letter, e.g. 7 act-type(F). Balkanski also defines act-type and activity constructors and relations; e.g. sequence(boil(water), add(noodles,water)) represents the sequence of doi</context>
</contexts>
<marker>Lochbaum, Grosz, Sidner, 1990</marker>
<rawString>Lochbaum, K. E., Grosz, B. J., and Sidner, C. L. 1990. Models of plans to support communication: An initial report. In Proceedings of AAAI-90, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K E Lochbaum</author>
</authors>
<title>Plan recognition in collaborative discourse.</title>
<date>1991</date>
<tech>Technical report,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="13719" citStr="Lochbaum, 1991" startWordPosition="2182" endWordPosition="2183">by node N2 have the same time. This constraint is modelled directly in the rgraph by creating the activity corresponding to lift(keyboard(piano)) to have the same time as F1 . No information about the agent of this activity is known, however, so a variable, G1, is used to represent the agent. Next, because the simultaneous act-type is related by a CGEN relation to lift (piano), the CGEN schema is used to create node N3 and the link between N2 and N3. The first two constraints of the schema are satisfied by creating node N3 such that its activity&apos;s agent and time are the 4The technical report (Lochbaum, 1991) contains a more detailed discussion of the derivation of these schemas from the definitions given by Balkanski (1990). 35 Recipe Augmented Rgraph Rgraph Constraints CGEN(-y, 6 ,C) (6, G ,T) G=agent(r) I GEN T=time(r) r HOLDSI(C,G,T) HOLDS&apos;(C,agent(r),time(r)) BEFORE(time(r),T) CENABLES(7, 6,C) (6, G,T) -ft ENABLES r sequence(n &apos;72, •..7n) Kw, r2, ..., r,0=A v.; BEFORE(time(rAtime(r3+1)) I c, agent(A)=U jagent(F,) F, time(A)=coverinterval({time(r )})5 agent(A)=U jagent(r3) time(A)=cover_interval({time(F, ))) conjoined(ri , 72, ...7n) ic(ri, r2, ...ro)---6, I c, I&apos;, simult(71, 72, —70 K(ri, r2,</context>
</contexts>
<marker>Lochbaum, 1991</marker>
<rawString>Lochbaum, K. E. 1991. Plan recognition in collaborative discourse. Technical report, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Pollack</author>
</authors>
<title>A model of plan inference that distinguishes between the beliefs of actors and observers.</title>
<date>1986</date>
<booktitle>In Proceedings of the 24th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="1262" citStr="Pollack (1986)" startWordPosition="192" endWordPosition="193"> of each other&apos;s utterances, conversational participants must recognize the intentions behind those utterances. Thus, a model of intended plan recognition is an important component of a theory of discourse understanding. The model must distinguish each agent&apos;s beliefs and intentions from the other&apos;s and avoid assumptions about the correctness or completeness of the agents&apos; beliefs. Early work on plan recognition in discourse, e.g. Allen &amp; Perrault (1980); Sidner &amp; Israel (1981), was based on work in Al planning systems, in particular the STRIPS formalism (Pikes and Nilsson, 1971). However, as Pollack (1986) has argued, because these systems do not differentiate between the beliefs and intentions of the different conversational participants, they are insufficient for modelling discourse. Although Pollack proposes a model that does make this distinction, her model has other shortcomings. In particular, it assumes a master/slave relationship between agents (Grosz and Sidner, 1990) and that the inferring agent has complete and accurate knowledge of domain actions. In addition, like many earlier systems, it relies upon a set of heuristics to control the application of plan inference rules. In contras</context>
<context position="4955" citStr="Pollack, 1986" startWordPosition="739" endWordPosition="740">X.11XJ,IX1,...X.}) Table 1: Act-type/Activity Relations and Constructors defined by Balkanski (1990) addition, lowercase letters denote the act-type of the activity represented by the corresponding uppercase letter, e.g. 7 act-type(F). Balkanski also defines act-type and activity constructors and relations; e.g. sequence(boil(water), add(noodles,water)) represents the sequence of doing an act of type boil(water) followed by an act of type add(noodles,water), while CGEN(mix(sauce,noodles), make(pasta_dish),C) represents that the first act-type conditionally generates the second (Goldman, 1970; Pollack, 1986). Table 1 lists the act-type and corresponding activity relations and constructors that will be used in this paper. Act-type constructors and relations are used in specifying recipes. Following Pollack (1990), we use the term recipe to refer to what an agent knows when the agent knows a way of doing something. As an example, a particular agent&apos;s recipe for lifting a piano might be CGEN(simult(lift(foot(piano)), lift (keyb oard(piano))) , lift (piano) , AG .[IG1=21); this recipe encodes that simultaneously lifting the foot- and keyboard ends of a piano results in lifting the piano, provided tha</context>
</contexts>
<marker>Pollack, 1986</marker>
<rawString>Pollack, M. E. June 1986. A model of plan inference that distinguishes between the beliefs of actors and observers. In Proceedings of the 24th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Pollack</author>
</authors>
<title>Plans as complex mental attitudes.</title>
<date>1990</date>
<booktitle>Intentions in Communication.</booktitle>
<editor>In Cohen, P., Morgan, J., and Pollack, M., editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5163" citStr="Pollack (1990)" startWordPosition="770" endWordPosition="771">letter, e.g. 7 act-type(F). Balkanski also defines act-type and activity constructors and relations; e.g. sequence(boil(water), add(noodles,water)) represents the sequence of doing an act of type boil(water) followed by an act of type add(noodles,water), while CGEN(mix(sauce,noodles), make(pasta_dish),C) represents that the first act-type conditionally generates the second (Goldman, 1970; Pollack, 1986). Table 1 lists the act-type and corresponding activity relations and constructors that will be used in this paper. Act-type constructors and relations are used in specifying recipes. Following Pollack (1990), we use the term recipe to refer to what an agent knows when the agent knows a way of doing something. As an example, a particular agent&apos;s recipe for lifting a piano might be CGEN(simult(lift(foot(piano)), lift (keyb oard(piano))) , lift (piano) , AG .[IG1=21); this recipe encodes that simultaneously lifting the foot- and keyboard ends of a piano results in lifting the piano, provided that there are two agents doing the lifting. For ease of presentation, we will sometimes represent recipes graphically using different types of arrows to represent specific act-type relations and constructors. F</context>
</contexts>
<marker>Pollack, 1990</marker>
<rawString>Pollack, M. E. 1990. Plans as complex mental attitudes. In Cohen, P., Morgan, J., and Pollack, M., editors, Intentions in Communication. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sidner</author>
<author>D J Israel</author>
</authors>
<title>Recognizing intended meaning and speakers&apos; plans.</title>
<date>1981</date>
<booktitle>In Proceedings of IJCAI-81.</booktitle>
<contexts>
<context position="1130" citStr="Sidner &amp; Israel (1981)" startWordPosition="168" endWordPosition="171">Plan model of collaboration (Grosz and Sidner, 1990; Lochbaum et al., 1990) and that satisfies these constraints. INTRODUCTION To make sense of each other&apos;s utterances, conversational participants must recognize the intentions behind those utterances. Thus, a model of intended plan recognition is an important component of a theory of discourse understanding. The model must distinguish each agent&apos;s beliefs and intentions from the other&apos;s and avoid assumptions about the correctness or completeness of the agents&apos; beliefs. Early work on plan recognition in discourse, e.g. Allen &amp; Perrault (1980); Sidner &amp; Israel (1981), was based on work in Al planning systems, in particular the STRIPS formalism (Pikes and Nilsson, 1971). However, as Pollack (1986) has argued, because these systems do not differentiate between the beliefs and intentions of the different conversational participants, they are insufficient for modelling discourse. Although Pollack proposes a model that does make this distinction, her model has other shortcomings. In particular, it assumes a master/slave relationship between agents (Grosz and Sidner, 1990) and that the inferring agent has complete and accurate knowledge of domain actions. In ad</context>
</contexts>
<marker>Sidner, Israel, 1981</marker>
<rawString>Sidner, C. and Israel, D. J. 1981. Recognizing intended meaning and speakers&apos; plans. In Proceedings of IJCAI-81.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>