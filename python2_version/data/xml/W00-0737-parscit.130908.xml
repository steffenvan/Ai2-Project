<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.056138">
<note confidence="0.819239">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 163-165, Lisbon, Portugal, 2000.
</note>
<title confidence="0.733224">
Hybrid Text Chunking
</title>
<author confidence="0.83528">
GuoDong Zhou and Jian Su and TongGuan Tey
</author>
<affiliation confidence="0.829215">
Kent Ridge Digital Labs
</affiliation>
<address confidence="0.9688675">
21 Heng Mui Keng Terrace
Singapore 119613
</address>
<email confidence="0.819658">
lzhougd, sujian, tongguanlOkrdl.org.sg
</email>
<sectionHeader confidence="0.991513" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999978583333333">
This paper proposes an error-driven HMM-
based text chunk tagger with context-dependent
lexicon. Compared with standard HMM-based
tagger, this tagger incorporates more contextual
information into a lexical entry. Moreover, an
error-driven learning approach is adopted to de-
crease the memory requirement by keeping only
positive lexical entries and makes it possible
to further incorporate more context-dependent
lexical entries. Finally, memory-based learning
is adopted to further improve the performance
of the chunk tagger.
</bodyText>
<sectionHeader confidence="0.998428" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999976181818182">
The idea of using statistics for chunking goes
back to Church(1988), who used corpus frequen-
cies to determine the boundaries of simple non-
recursive noun phrases. Skut and Brants(1998)
modified Church&apos;s approach in a way permitting
efficient and reliable recognition of structures of
limited depth and encoded the structure in such
a way that it can be recognised by a Viterbi
tagger. Our approach follows Skut and Brants&apos;
way by employing HMM-based tagging method
to model the chunking process.
</bodyText>
<sectionHeader confidence="0.942167" genericHeader="method">
2 HMM-based Chunk Tagger with
Context-dependent Lexicon
</sectionHeader>
<bodyText confidence="0.934795">
Given a token sequence GI/ = 9192. • -9n ,
the goal is to find an optimal tag sequence
Tr = tit2. • •t7, which maximizes log P(7J&apos;IG7):
</bodyText>
<equation confidence="0.951578">
log P(T111G7) = log P(Tr) + logP(Tr)P(00
</equation>
<bodyText confidence="0.948715333333333">
The second item in the above equation is the
mutual information between the tag sequence
Tr and the given token sequence G. By as-
suming that the mutual information between
and Tr is equal to the summation of mutual
information between G711 and the individual tag
</bodyText>
<equation confidence="0.955525222222222">
ti (1&lt;i&lt;n):
P(Tr, G7) n P(ti, G7)
E log
logP(T11)P(G?) i=i P(ti)P(G7)
Mgr, G11) = E mgti,
we have:
log P(T111G7) = n p(ti,GD
log(p Tr)+Elog
= log P(T11) - E log P(ti) + slog P(tilq)
</equation>
<bodyText confidence="0.999969227272727">
The first item of above equation can be solved
by chain rules. Normally, each tag is assumed
to be probabilistic dependent on the N-1 previ-
ous tags. Here, backoff bigram(N=2) model is
used. The second item is the summation of log
probabilities of all the tags. Both the first item
and second item constitute the language model
component while the third item constitutes the
lexicon component. Ideally the third item can
be estimated by the forward-backward algo-
rithm(Rabiner 1989) recursively for the first-
order(Rabiner 1989) or second-order HMMs.
However, several approximations on it will be
attempted later in this paper instead. The
stochastic optimal tag sequence can be found
by maximizing the above equation over all the
possible tag sequences using the Viterbi algo-
rithm.
The main difference between our tagger and
the standard taggers lies in our tagger has a
context-dependent lexicon while others use a
context-independent lexicon.
</bodyText>
<page confidence="0.994713">
163
</page>
<bodyText confidence="0.997962619047619">
For chunk tagger, we have gi = piwz where
= wiw2. • •wn, is the word sequence and
Pi = P1P2- •-Pn is the part-of-speech(POS)
sequence. Here, we use structural tags to
representing chunking(bracketing and labeling)
structure. The basic idea of representing
the structural tags is similar to Skut and
Brants(1998) and the structural tag consists of
three parts:
1) Structural relation. The basic idea is sim-
ple: structures of limited depth are encoded
using a finite number of flags. Given a se-
quence of input tokens(here, the word and POS
pairs), we consider the structural relation be-
tween the previous input token and the current
one. For the recognition of chunks, it is suffi-
cient to distinguish the following four different
structural relations which uniquely identify the
sub-structures of depth l(Skut and Brants used
seven different structural relations to identify
the sub-structures of depth 2).
</bodyText>
<listItem confidence="0.920250090909091">
• 00: the current input token and the previ-
ous one have the same parent
• 90: one ancestor of the current input token
and the previous input token have the same
parent
• 09: the current input token and one an-
cestor of the previous input token have the
same parent
• 99 one ancestor of the current input token
and one ancestor of the previous input to-
ken have the same parent
</listItem>
<bodyText confidence="0.99965356">
Compared with the B-Chunk and I-Chunk
used in Ramshaw and Marcus (1995)., structural
relations 99 and 90 correspond to B-Chunk
which represents the first word of the chunk,
and structural relations 00 and 09 correspond
to I-Chunk which represents each other in the
chunk while 90 also means the beginning of the
sentence and 09 means the end of the sentence.
2)Phrase category. This is used to identify
the phrase categories of input tokens.
3)Part-of-speech. Because of the limited
number of structural relations and phrase cate-
gories, the POS is added into the structural tag
to represent more accurate models.
Principally, the current chunk is dependent
on all the context words and their POSs. How-
ever, in order to decrease memory require-
ment and computational complexity, our base-
line HMM-based chunk tagger only considers
previous POS, current POS and their word to-
kens whose POSs are of certain kinds, such as
preposition and determiner etc. The overall
precision, recall and Fo=1 rates of our baseline
tagger on the test data of the shared task are
89.58%, 89.56% and 89.57%.
</bodyText>
<sectionHeader confidence="0.99837" genericHeader="method">
3 Error-driven Learning
</sectionHeader>
<bodyText confidence="0.99990552631579">
After analysing the chunking results, we find
many errors are caused by a limited number of
words. In order to overcome such errors, we
include such words in the chunk dependence
context by using error-driven learning. First,
the above HMM-based chunk tagger is used to
chunk the training data. Secondly, the chunk
tags determined by the chunk tagger are com-
pared with the given chunk tags in the training
data. For each word, its chunking error number
is summed. Finally, those words whose chunk-
ing error numbers are equal to or above a given
threshold(i.e. 3) are kept. The HMM-based
chunk tagger is re-trained with those words con-
sidered in the chunk dependence context.
The overall precision, recall and Fo=1 rates
of our error-driven HMM-based chunk tagger
on the test data of the shared task are 91.53%,
92.02% and 91.77
</bodyText>
<sectionHeader confidence="0.941114" genericHeader="method">
4 Memory based Learning
</sectionHeader>
<bodyText confidence="0.999903631578947">
Memory-based learning has been widely used
in NLP tasks in the last decade. Principally, it
falls into two paradigms. First paradigm rep-
resents examples as sets of features and car-
ries out induction by finding the most simi-
lar cases. Such works include Daelemans et
al.(1996) for POS tagging and Cardie(1993)
for syntactic and semantic tagging. Second
paradigm makes use of raw sequential data
and generalises by reconstructing test examples
from different pieces of the training data. Such
works include Bod(1992) for parsing, Argamon
et al.(1998) for shallow natural language pat-
terns and Daelemans et al.(1999) for shallow
parsing.
The memory-based method presented here
follows the second paradigm and makes use of
raw sequential data. Here, generalization is per-
formed online at recognition time by comparing
</bodyText>
<page confidence="0.996843">
164
</page>
<bodyText confidence="0.9936424">
the new pattern to the ones in the training cor-
pus.
Given one of the N most probable chunk se-
quences extracted by the error-driven HMM-
based chunk tagger, we can extract a set of
chunk patterns, each of them with the format:
XP = porlArnn±lpn+i, where ril+1 is the
structural relation between pi and pi+1.
As an example, from the bracketed and la-
beled sentence:
</bodyText>
<equation confidence="0.981538533333333">
[NP He/PRP [VP reckons/VBZ ]
[NP the/DT current/JJ account/NN
deficit/NN ] [VP will/MD narrow/VB
[ PP to/TO ] [NP only/RB #/#
1.8/CD billion/CD [PP in/IN [NP
September/NNP 1 [0 ./.
we can extract following chunk patterns:
NP=NULL 90 PRP 99 VBZ
VP=PRP 99 VBZ 99 DT
NP=VBZ 99 DT JJ NN NN 99 MD
PP=VB 99 TO 99 RB
NP=TO 99 RB # CD CD 99 IN
PP=CD 99 IN 99 NNP
NP=IN 99 NNP 99.
0=NNP 99. 09 NULL
</equation>
<bodyText confidence="0.999911076923077">
For every chunk pattern, we estimate its proba-
bility by using memory-based learning. If the
chunk pattern exists in the training corpus,
its probability is computed by the probability
of such pattern among all the chunk patterns.
Otherwise, its probability is estimated by the
multiply of its overlapped sub-patterns. Then
the probability of each of the N most probable
chunk sequences is adjusted by multiplying the
probabilities of its extracted chunk patterns.
Table 1 shows the performance of error-driven
HMM-based chunk tagger with memory-based
learning.
</bodyText>
<sectionHeader confidence="0.998398" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999520333333333">
It is found that the performance with the help of
error-driven learning is improved by 2.20% and
integration of memory-based learning further
improves the performance by 0.35% to 92.12%.
For future work, the experimentation on large
scale task will be speculated in the near future.
Finally, a closer integration of memory-based
method with HMM-based chunk tagger will also
be conducted.
</bodyText>
<table confidence="0.999905666666667">
test data precision recall Fi3=1
ADJP 76.17% 70.78% 73.37
ADVP 78.25% 78.52% 78.39
CONJP 46.67% 77.78% 58.33
INTJ 20.00% 50.00% 28.57
LST 00.00% 00.00% 00.00
NP 92.19% 92.59% 92.39
PP 96.09% 96.94% 96.51
PRT 72.36% 83.96% 77.73
SBAR 83.56% 79.81% 81.64
VP 92.77% 92.85% 92.81
all 91.99% 92.25% 92.12
</table>
<tableCaption confidence="0.99962">
Table 1: performance of chunking
</tableCaption>
<sectionHeader confidence="0.9836" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998663210526316">
S. Argamon, I. Dagan, and Y. Krymolowski. 1998.
A memory-based approach to learning shallow
natural language patterns. In COLING/ACL-
1998, pages 67-73. Montreal, Canada.
R. Bod. 1992. A computational model of lan-
guage performance: Data-oriented parsing. In
COLING-1992, pages 855-859. Nantes, France.
C. Cardie. 1993. A case-based approach to knowl-
edge acquisition for domain-specific sentence anal-
ysis. In Proceeding of the 11th National Con-
ference on Artificial Intelligence, pages 798-803.
Menlo Park, CA, USA. AAAI Press.
K.W. Church. 1988. A stochastic parts program and
noun phrase parser for unrestricted text. In Pro-
ceedings of Second Conference on Applied Natu-
ral Language Processing, pages 136-143. Austin,
Texas, USA.
W. Daelemans, J. Zavrel, P. Berck, and S. Gillis.
1996. Mbt: A memory-based part-of-speech tag-
ger generator. In Proceeding of the Fourth Work-
shop on Large Scale Corpora, pages 14-27. ACL
SIGDAT.
W. Daelemans, S. Buchholz, and J. Veenstra. 1999.
Memory-based shallow parsing. In CoNLL-1999,
pages 53-60. Bergen, Norway.
L.R. Rabiner. 1989. A tutorial on hidden markov
models and selected applications in speech recog-
nition. In Proceedings of the IEEE, volume 77,
pages 257-286.
Lance A. Ramshaw and Mitchell P. Marcus. 1995.
Text chunking using transformation-based learn-
ing. In Proceedings of the Third ACL Work-
shop on Very Large Corpora. Cambridge, Mas-
sachusetts, USA.
W. Skut and T. Brants. 1998. Chunk tagger: sta-
tistical recognition of noun phrases. In ESSLLI-
1998 Workshop on Automated Acquisition of Syn-
tax and Parsing. Saarbruucken, Germany.
</reference>
<page confidence="0.998736">
165
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.817223">
<note confidence="0.90922">of CoNLL-2000 and LLL-2000, 163-165, Lisbon, Portugal, 2000.</note>
<title confidence="0.996401">Hybrid Text Chunking</title>
<author confidence="0.997051">Zhou Su</author>
<affiliation confidence="0.95404">Kent Ridge Digital</affiliation>
<address confidence="0.9698245">21 Heng Mui Keng Singapore</address>
<email confidence="0.992883">lzhougd,sujian,tongguanlOkrdl.org.sg</email>
<abstract confidence="0.999808692307692">This paper proposes an error-driven HMMbased text chunk tagger with context-dependent lexicon. Compared with standard HMM-based tagger, this tagger incorporates more contextual information into a lexical entry. Moreover, an error-driven learning approach is adopted to decrease the memory requirement by keeping only positive lexical entries and makes it possible to further incorporate more context-dependent lexical entries. Finally, memory-based learning is adopted to further improve the performance of the chunk tagger.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Argamon</author>
<author>I Dagan</author>
<author>Y Krymolowski</author>
</authors>
<title>A memory-based approach to learning shallow natural language patterns.</title>
<date>1998</date>
<booktitle>In COLING/ACL1998,</booktitle>
<pages>67--73</pages>
<location>Montreal, Canada.</location>
<marker>Argamon, Dagan, Krymolowski, 1998</marker>
<rawString>S. Argamon, I. Dagan, and Y. Krymolowski. 1998. A memory-based approach to learning shallow natural language patterns. In COLING/ACL1998, pages 67-73. Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bod</author>
</authors>
<title>A computational model of language performance: Data-oriented parsing.</title>
<date>1992</date>
<booktitle>In COLING-1992,</booktitle>
<pages>855--859</pages>
<location>Nantes, France.</location>
<marker>Bod, 1992</marker>
<rawString>R. Bod. 1992. A computational model of language performance: Data-oriented parsing. In COLING-1992, pages 855-859. Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cardie</author>
</authors>
<title>A case-based approach to knowledge acquisition for domain-specific sentence analysis.</title>
<date>1993</date>
<booktitle>In Proceeding of the 11th National Conference on Artificial Intelligence,</booktitle>
<pages>798--803</pages>
<publisher>AAAI Press.</publisher>
<location>Menlo Park, CA, USA.</location>
<marker>Cardie, 1993</marker>
<rawString>C. Cardie. 1993. A case-based approach to knowledge acquisition for domain-specific sentence analysis. In Proceeding of the 11th National Conference on Artificial Intelligence, pages 798-803. Menlo Park, CA, USA. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.</title>
<date>1988</date>
<booktitle>In Proceedings of Second Conference on Applied Natural Language Processing,</booktitle>
<pages>136--143</pages>
<location>Austin, Texas, USA.</location>
<marker>Church, 1988</marker>
<rawString>K.W. Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of Second Conference on Applied Natural Language Processing, pages 136-143. Austin, Texas, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>P Berck</author>
<author>S Gillis</author>
</authors>
<title>Mbt: A memory-based part-of-speech tagger generator.</title>
<date>1996</date>
<booktitle>In Proceeding of the Fourth Workshop on Large Scale Corpora,</booktitle>
<pages>14--27</pages>
<publisher>ACL SIGDAT.</publisher>
<marker>Daelemans, Zavrel, Berck, Gillis, 1996</marker>
<rawString>W. Daelemans, J. Zavrel, P. Berck, and S. Gillis. 1996. Mbt: A memory-based part-of-speech tagger generator. In Proceeding of the Fourth Workshop on Large Scale Corpora, pages 14-27. ACL SIGDAT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>S Buchholz</author>
<author>J Veenstra</author>
</authors>
<title>Memory-based shallow parsing.</title>
<date>1999</date>
<booktitle>In CoNLL-1999,</booktitle>
<pages>53--60</pages>
<location>Bergen,</location>
<marker>Daelemans, Buchholz, Veenstra, 1999</marker>
<rawString>W. Daelemans, S. Buchholz, and J. Veenstra. 1999. Memory-based shallow parsing. In CoNLL-1999, pages 53-60. Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
</authors>
<title>A tutorial on hidden markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>In Proceedings of the IEEE,</booktitle>
<volume>77</volume>
<pages>257--286</pages>
<contexts>
<context position="2453" citStr="Rabiner 1989" startWordPosition="390" endWordPosition="392">P(G?) i=i P(ti)P(G7) Mgr, G11) = E mgti, we have: log P(T111G7) = n p(ti,GD log(p Tr)+Elog = log P(T11) - E log P(ti) + slog P(tilq) The first item of above equation can be solved by chain rules. Normally, each tag is assumed to be probabilistic dependent on the N-1 previous tags. Here, backoff bigram(N=2) model is used. The second item is the summation of log probabilities of all the tags. Both the first item and second item constitute the language model component while the third item constitutes the lexicon component. Ideally the third item can be estimated by the forward-backward algorithm(Rabiner 1989) recursively for the firstorder(Rabiner 1989) or second-order HMMs. However, several approximations on it will be attempted later in this paper instead. The stochastic optimal tag sequence can be found by maximizing the above equation over all the possible tag sequences using the Viterbi algorithm. The main difference between our tagger and the standard taggers lies in our tagger has a context-dependent lexicon while others use a context-independent lexicon. 163 For chunk tagger, we have gi = piwz where = wiw2. • •wn, is the word sequence and Pi = P1P2- •-Pn is the part-of-speech(POS) sequence</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>L.R. Rabiner. 1989. A tutorial on hidden markov models and selected applications in speech recognition. In Proceedings of the IEEE, volume 77, pages 257-286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance A Ramshaw</author>
<author>Mitchell P Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third ACL Workshop on Very Large Corpora.</booktitle>
<location>Cambridge, Massachusetts, USA.</location>
<contexts>
<context position="4271" citStr="Ramshaw and Marcus (1995)" startWordPosition="692" endWordPosition="695">t structural relations which uniquely identify the sub-structures of depth l(Skut and Brants used seven different structural relations to identify the sub-structures of depth 2). • 00: the current input token and the previous one have the same parent • 90: one ancestor of the current input token and the previous input token have the same parent • 09: the current input token and one ancestor of the previous input token have the same parent • 99 one ancestor of the current input token and one ancestor of the previous input token have the same parent Compared with the B-Chunk and I-Chunk used in Ramshaw and Marcus (1995)., structural relations 99 and 90 correspond to B-Chunk which represents the first word of the chunk, and structural relations 00 and 09 correspond to I-Chunk which represents each other in the chunk while 90 also means the beginning of the sentence and 09 means the end of the sentence. 2)Phrase category. This is used to identify the phrase categories of input tokens. 3)Part-of-speech. Because of the limited number of structural relations and phrase categories, the POS is added into the structural tag to represent more accurate models. Principally, the current chunk is dependent on all the con</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of the Third ACL Workshop on Very Large Corpora. Cambridge, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Skut</author>
<author>T Brants</author>
</authors>
<title>Chunk tagger: statistical recognition of noun phrases.</title>
<date>1998</date>
<booktitle>In ESSLLI1998 Workshop on Automated Acquisition of Syntax and Parsing. Saarbruucken,</booktitle>
<location>Germany.</location>
<marker>Skut, Brants, 1998</marker>
<rawString>W. Skut and T. Brants. 1998. Chunk tagger: statistical recognition of noun phrases. In ESSLLI1998 Workshop on Automated Acquisition of Syntax and Parsing. Saarbruucken, Germany.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>