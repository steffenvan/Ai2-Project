<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.981793">
SPRITE: Generalizing Topic Models with Structured Priors
</title>
<author confidence="0.970109">
Michael J. Paul and Mark Dredze
</author>
<affiliation confidence="0.952796666666667">
Department of Computer Science
Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, MD 21218
</affiliation>
<email confidence="0.999226">
mpaul@cs.jhu.edu, mdredze@cs.jhu.edu
</email>
<sectionHeader confidence="0.99391" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997768">
We introduce SPRITE, a family of topic
models that incorporates structure into
model priors as a function of underlying
components. The structured priors can
be constrained to model topic hierarchies,
factorizations, correlations, and supervi-
sion, allowing SPRITE to be tailored to
particular settings. We demonstrate this
flexibility by constructing a SPRITE-based
model to jointly infer topic hierarchies and
author perspective, which we apply to cor-
pora of political debates and online re-
views. We show that the model learns in-
tuitive topics, outperforming several other
topic models at predictive tasks.
</bodyText>
<sectionHeader confidence="0.998983" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999074266666667">
Topic models can be a powerful aid for analyzing
large collections of text by uncovering latent in-
terpretable structures without manual supervision.
Yet people often have expectations about topics in
a given corpus and how they should be structured
for a particular task. It is crucial for the user expe-
rience that topics meet these expectations (Mimno
et al., 2011; Talley et al., 2011) yet black box topic
models provide no control over the desired output.
This paper presents SPRITE, a family of topic
models that provide a flexible framework for en-
coding preferences as priors for how topics should
be structured. SPRITE can incorporate many types
of structure that have been considered in prior
work, including hierarchies (Blei et al., 2003a;
</bodyText>
<note confidence="0.655909">
Mimno et al., 2007), factorizations (Paul and
Dredze, 2012; Eisenstein et al., 2011), sparsity
(Wang and Blei, 2009; Balasubramanyan and Co-
hen, 2013), correlations between topics (Blei and
Lafferty, 2007; Li and McCallum, 2006), pref-
erences over word choices (Andrzejewski et al.,
2009; Paul and Dredze, 2013), and associations
</note>
<bodyText confidence="0.996047363636363">
between topics and document attributes (Ramage
et al., 2009; Mimno and McCallum, 2008).
SPRITE builds on a standard topic model,
adding structure to the priors over the model pa-
rameters. The priors are given by log-linear func-
tions of underlying components (§2), which pro-
vide additional latent structure that we will show
can enrich the model in many ways. By apply-
ing particular constraints and priors to the compo-
nent hyperparameters, a variety of structures can
be induced such as hierarchies and factorizations
(§3), and we will show that this framework cap-
tures many existing topic models (§4).
After describing the general form of the model,
we show how SPRITE can be tailored to partic-
ular settings by describing a specific model for
the applied task of jointly inferring topic hierar-
chies and perspective (§6). We experiment with
this topic+perspective model on sets of political
debates and online reviews (§7), and demonstrate
that SPRITE learns desired structures while outper-
forming many baselines at predictive tasks.
</bodyText>
<sectionHeader confidence="0.780741" genericHeader="introduction">
2 Topic Modeling with Structured Priors
</sectionHeader>
<bodyText confidence="0.999325176470588">
Our model family generalizes latent Dirichlet al-
location (LDA) (Blei et al., 2003b). Under LDA,
there are K topics, where a topic is a categor-
ical distribution over V words parameterized by
φk. Each document has a categorical distribution
over topics, parameterized by 8,,,, for the mth doc-
ument. Each observed word in a document is gen-
erated by drawing a topic z from 8,,,,, then drawing
the word from φz. 8 and φ have priors given by
Dirichlet distributions.
Our generalization adds structure to the gener-
ation of the Dirichlet parameters. The priors for
these parameters are modeled as log-linear com-
binations of underlying components. Components
are real-valued vectors of length equal to the vo-
cabulary size V (for priors over word distribu-
tions) or length equal to the number of topics K
</bodyText>
<page confidence="0.998996">
43
</page>
<bodyText confidence="0.987452283018868">
Transactions of the Association for Computational Linguistics, vol. 3, pp. 43–57, 2015. Action Editor: Janyce Wiebe.
Submission batch: 7/2014; Revision batch 12/2014; Published 1/2015. c�2015 Association for Computational Linguistics.
(for priors over topic distributions).
For example, we might assume that topics about
sports like baseball and football share a common
prior – given by a component – with general words
about sports. A fine-grained topic about steroid
use in sports might be created by combining com-
ponents about broader topics like sports, medicine,
and crime. By modeling the priors as combina-
tions of components that are shared across all top-
ics, we can learn interesting connections between
topics, where components provide an additional
latent layer for corpus understanding.
As we’ll show in the next section, by imposing
certain requirements on which components feed
into which topics (or documents), we can induce
a variety of model structures. For example, if we
want to model a topic hierarchy, we require that
each topic depend on exactly one parent compo-
nent. If we want to jointly model topic and ide-
ology in a corpus of political documents (§6), we
make topic priors a combination of one component
from each of two groups: a topical component and
an ideological component, resulting in ideology-
specific topics like “conservative economics”.
Components construct priors as follows. For the
topic-specific word distributions φ, there are C(φ)
topic components. The kth topic’s prior over φk
is a weighted combination (with coefficient vector
βk) of the C(φ) components (where component c is
denoted ωc). For the document-specific topic dis-
tributions θ, there are C(θ) document components.
The mth document’s prior over θm is a weighted
combination (coefficients αm) of the C(θ) compo-
nents (where component c is denoted δc).
Once conditioned on these priors, the model
is identical to LDA. The generative story is de-
scribed in Figure 1. We call this family of models
SPRITE: Structured PRIor Topic modEls.
To illustrate the role that components can play,
consider an example in which we are modeling re-
search topics in a corpus of NLP abstracts (as we
do in §7.3). Consider three speech-related topics:
signal processing, automatic speech recognition,
and dialog systems. Conceptualized as a hierar-
chy, these topics might belong to a higher level
category of spoken language processing. SPRITE
allows the relationship between these three topics
to be defined in two ways. One, we can model that
these topics will all have words in common. This
is handled by the topic components – these three
topics could all draw from a common “spoken lan-
</bodyText>
<listItem confidence="0.9998035">
• Generate hyperparameters: α, β, δ, ω (§3)
• For each document m, generate parameters:
</listItem>
<equation confidence="0.7399565">
θ)
θmk = exp(E,1 αmc δck), 1&lt;k&lt;K
</equation>
<listItem confidence="0.95978025">
2. θm — Dirichlet(˜θm)
• For each topic k, generate parameters:
«)
1. ˜φkv = exp(Ec C(&amp;quot; βkc ωcv), 1&lt;v&lt;V
2. φk — Dirichlet( ˜φk)
• For each token (m, n), generate data:
1. Topic (unobserved): zm,n — θm
2. Word (observed): wm,n — φzm,n
</listItem>
<figureCaption confidence="0.991143333333333">
Figure 1: The generative story of SPRITE. The difference
from latent Dirichlet allocation (Blei et al., 2003b) is the gen-
eration of the Dirichlet parameters.
</figureCaption>
<bodyText confidence="0.9999369">
guage” topic component, with high-weight words
such as speech and spoken, which informs the
prior of all three topics. Second, we can model that
these topics are likely to occur together in docu-
ments. For example, articles about dialog systems
are likely to discuss automatic speech recognition
as a subroutine. This is handled by the document
components – there could be a “spoken language”
document component that gives high weight to all
three topics, so that if a document draw its prior
from this component, then it is more likely to give
probability to these topics together.
The next section will describe how particular
priors over the coefficients can induce various
structures such as hierarchies and factorizations,
and components and coefficients can also be pro-
vided as input to incorporate supervision and prior
knowledge. The general prior structure used in
SPRITE can be used to represent a wide array of
existing topic models, outlined in Section 4.
</bodyText>
<sectionHeader confidence="0.99659" genericHeader="method">
3 Topic Structures
</sectionHeader>
<bodyText confidence="0.999901666666667">
By changing the particular configuration of the hy-
perparameters – the component coefficients (α and
β) and the component weights (δ and ω) – we ob-
tain a diverse range of model structures and behav-
iors. We now describe possible structures and the
corresponding priors.
</bodyText>
<subsectionHeader confidence="0.998027">
3.1 Component Structures
</subsectionHeader>
<bodyText confidence="0.999953">
This subsection discusses various graph structures
that can describe the relation between topic com-
ponents and topics, and between document com-
ponents and documents, illustrated in Figure 2.
</bodyText>
<figure confidence="0.456196">
1.
</figure>
<page confidence="0.849297">
44
</page>
<figure confidence="0.988282">
(a) Dense DAG (b) Sparse DAG (c) Tree (d) Factored Forest
</figure>
<figureCaption confidence="0.998815">
Figure 2: Example graph structures describing possible relations between components (middle row) and topics or documents
(bottom row). Edges correspond to non-zero values for α or β (the component coefficients defining priors over the document
and topic distributions). The root node is a shared prior over the component weights (with other possibilities discussed in §3.3).
</figureCaption>
<subsectionHeader confidence="0.485487">
3.1.1 Directed Acyclic Graph
</subsectionHeader>
<bodyText confidence="0.9998772">
The general SPRITE model can be thought of as a
dense directed acyclic graph (DAG), where every
document or topic is connected to every compo-
nent with some weight α or �. When many of
the α or � coefficients are zero, the DAG becomes
sparse. A sparse DAG has an intuitive interpre-
tation: each document or topic depends on some
subset of components.
The default prior over coefficients that we use
in this study is a 0-mean Gaussian distribution,
which encourages the weights to be small. We
note that to induce a sparse graph, one could use
a 0-mean Laplace distribution as the prior over α
and �, which prefers parameters such that some
components are zero.
</bodyText>
<subsectionHeader confidence="0.813079">
3.1.2 Tree
</subsectionHeader>
<bodyText confidence="0.9999519">
When each document or topic has exactly one par-
ent (one nonzero coefficient) we obtain a two-level
tree structure. This structure naturally arises in
topic hierarchies, for example, where fine-grained
topics are children of coarse-grained topics.
To create an (unweighted) tree, we require
αmc ∈ {0, 1} and Ec αmc = 1 for each docu-
ment m. Similarly, �kc ∈ {0,1} and Ec �kc = 1
for each topic k. In this setting, αm and �k are
indicator vectors which select a single component.
In this study, rather than strictly requiring αm
and �k to be binary-valued indicator vectors, we
create a relaxation that allows for easier parameter
estimation. We let αm and �k to real-valued vari-
ables in a simplex, but place a prior over their val-
ues to encourage sparse values, favoring vectors
with a single component near 1 and others near 0.
This is achieved using a Dirichlet(p &lt; 1) distribu-
tion as the prior over α and �, which has higher
density near the boundaries of the simplex.1
</bodyText>
<footnote confidence="0.778997333333333">
1This generalizes the technique used in Paul and Dredze
(2012), who approximated binary variables with real-valued
variables in (0, 1), by using a “U-shaped” Beta(p &lt; 1) distri-
</footnote>
<bodyText confidence="0.999563666666667">
For a weighted tree, α and � could be a product
of two variables: an “integer-like” indicator vec-
tor with sparse Dirichlet prior as suggested above,
combined with a real-valued weight (e.g., with a
Gaussian prior). We take this approach in our
model of topic and perspective (§6).
</bodyText>
<subsectionHeader confidence="0.558334">
3.1.3 Factored Forest
</subsectionHeader>
<bodyText confidence="0.999911259259259">
By using structured sparsity over the DAG, we can
obtain a structure where components are grouped
into G factors, and each document or topic has
one parent from each group. Figure 2(d) illus-
trates this: the left three components belong to one
group, the right two belong to another, and each
bottom node has exactly one parent from each.
This is a DAG that we call a “factored forest” be-
cause the subgraphs associated with each group in
isolation are trees. This structure arises in “multi-
dimensional” models like SAGE (Eisenstein et al.,
2011) and Factorial LDA (Paul and Dredze, 2012),
which allow tokens to be associated with multiple
variables (e.g. a topic along with a variable denot-
ing positive or negative sentiment). This allows
word distributions to depend on both factors.
The “exactly one parent” indicator constraint is
the same as in the tree structure but enforces a
tree only within each group. This can therefore be
(softly) modeled using a sparse Dirichlet prior as
described in the previous subsection. In this case,
the subsets of components belonging to each fac-
tor have separate sparse Dirichlet priors. Using
the example from Figure 2(d), the first three com-
ponent indicators would come from one Dirichlet,
while the latter two component indicators would
come from a second.
</bodyText>
<subsectionHeader confidence="0.999991">
3.2 Tying Topic and Document Components
</subsectionHeader>
<bodyText confidence="0.94389725">
A desirable property for many situations is for the
topic and document components to correspond to
bution as the prior to encourage sparsity. The Dirichlet distri-
bution is the multivariate extension of the Beta distribution.
</bodyText>
<page confidence="0.998764">
45
</page>
<bodyText confidence="0.999925653846154">
each other. For example, if we think of the com-
ponents as coarse-grained topics in a hierarchy,
then the coefficients � enforce that topic word dis-
tributions share a prior defined by their parent w
component, while the coefficients α represent a
document’s proportions of coarse-grained topics,
which effects the document’s prior over child top-
ics (through the 8 vectors). Consider the example
with spoken language topics in §2: these three top-
ics (signal processing, speech recognition, and di-
alog systems) are a priori likely both to share the
same words and to occur together in documents.
By tying these together, we ensure that the pat-
terns are consistent across the two types of com-
ponents, and the patterns from both types can re-
inforce each other during inference.
In this case, the number of topic components
is the same as the number of document compo-
nents (C(φ) = C(θ)), and the coefficients (�cz)
of the topic components should correlate with the
weights of the document components (8zc). The
approach we take (§6) is to define 8 and � as a
product of two variables (suggested in §3.1.2): a
binary mask variable (with sparse Dirichlet prior),
which we let be identical for both 8 and �, and a
real-valued positive weight.
</bodyText>
<subsectionHeader confidence="0.998789">
3.3 Deep Components
</subsectionHeader>
<bodyText confidence="0.99994725">
As for priors over the component weights 8 and
w, we assume they are generated from a 0-mean
Gaussian. While not experimented with in this
study, it is also possible to allow the components
themselves to have rich priors which are functions
of higher level components. For example, rather
than assuming a mean of zero, the mean could be a
weighted combination of higher level weight vec-
tors. This approach was used by Paul and Dredze
(2013) in Factorial LDA, in which each w compo-
nent had its own Gaussian prior provided as input
to guide the parameters.
</bodyText>
<sectionHeader confidence="0.972083" genericHeader="method">
4 Special Cases and Extensions
</sectionHeader>
<bodyText confidence="0.999590285714286">
We now describe several existing Dirichlet prior
topic models and show how they are special cases
of SPRITE. Table 1 summarizes these models and
their relation to SPRITE. In almost every case, we
also describe how the SPRITE representation of
the model offers improvements over the original
model or can lead to novel extensions.
</bodyText>
<table confidence="0.975652">
Model Sec. Document priors Topic priors
LDA 4.1 Single component Single component
SCTM 4.2 Single component Sparse binary Q
SAGE 4.3 Single component Sparse w
FLDA 4.3 Binary b is transpose of Q Factored binary Q
PAM 4.4 α are supertopic weights Single component
DMR 4.5 α are feature values Single component
</table>
<tableCaption confidence="0.991333142857143">
Table 1: Topic models with Dirichlet priors that are gen-
eralized by SPRITE. The description of each model can be
found in the noted section number. PAM is not equivalent,
but captures very similar behavior. The described component
formulations of SCTM and SAGE are equivalent, but these
differ from SPRITE in that the components directly define the
parameters, rather than priors over the parameters.
</tableCaption>
<subsectionHeader confidence="0.998391">
4.1 Latent Dirichlet Allocation
</subsectionHeader>
<bodyText confidence="0.99882825">
In LDA (Blei et al., 2003b), all 0 vectors are
drawn from the same prior, as are all O vectors.
This is a basic instance of our model with only
one component at the topic and document levels,
</bodyText>
<equation confidence="0.53242">
C(θ) = C(φ) = 1, with coefficients α = � = 1.
</equation>
<subsectionHeader confidence="0.977075">
4.2 Shared Components Topic Models
</subsectionHeader>
<bodyText confidence="0.996437708333333">
Shared components topic models (SCTM) (Gorm-
ley et al., 2010) define topics as products of “com-
ponents”, where components are word distribu-
tions. To use the notation of our paper, the kth
topic’s word distribution in SCTM is parameter-
ized by Okv ∝ f1c wβkc
cv , where the w vectors are
word distributions (rather than vectors in RV ), and
the �kc E {0, 1} variables are indicators denoting
whether component c is in topic k.
This is closely related to SPRITE, where top-
ics also depend on products of underlying com-
ponents. A major difference is that in SCTM,
the topic-specific word distributions are exactly
defined as a product of components, whereas in
SPRITE, it is only the prior that is a product of
components.2 Another difference is that SCTM
has an unweighted product of components (� is bi-
nary), whereas SPRITE allows for weighted prod-
ucts. The log-linear parameterization leads to sim-
pler optimization procedures than the product pa-
rameterization. Finally, the components in SCTM
only apply to the word distributions, and not the
topic distributions in documents.
</bodyText>
<subsectionHeader confidence="0.998508">
4.3 Factored Topic Models
</subsectionHeader>
<bodyText confidence="0.981215">
Factored topic models combine multiple aspects
of the text to generate the document (instead of
just topics). One such topic model is Factorial
LDA (FLDA) (Paul and Dredze, 2012). In FLDA,
</bodyText>
<footnote confidence="0.997642333333333">
2The posterior becomes concentrated around the prior
when the Dirichlet variance is low, in which case SPRITE be-
haves like SCTM. SPRITE is therefore more general.
</footnote>
<page confidence="0.999707">
46
</page>
<bodyText confidence="0.998883345454546">
“topics” are actually tuples of potentially multiple
variables, such as aspect and sentiment in online
reviews (Paul et al., 2013). Each document distri-
bution θm is a distribution over pairs (or higher-
dimensional tuples if there are more than two fac-
tors), and each pair (j, k) has a word distribu-
tion φ(j,k). FLDA uses a similar log-linear pa-
rameterization of the Dirichlet priors as SPRITE.
Using our notation, the Dirichlet(˜φ(j,k)) prior for
φ(j,k) is defined as ˜φ(j,k),v=exp(ωjv+ωkv), where
ωj is a weight vector over the vocabulary for the
jth component of the first factor, and ωk encodes
the weights for the kth component of the second
factor. (Some bias terms are omitted for sim-
plicity.) The prior over θm has a similar form:
˜θm,(j,k)=exp(αmj + αmk), where αmj is docu-
ment m’s preference for component j of the first
factor (and likewise for k of the second).
This corresponds to an instantiation of SPRITE
using an unweighted factored forest (§3.1.3),
where βzc = δcz
(§3.2, recall that δ are document
components while β are the topic coefficients).
Each subtopic z (which is a pair of variables in
the two-factor model) has one parent component
from each factor, indicated by βz which is binary-
valued. At the document level in the two-factor
example, δj is an indicator vector with values of 1
for all pairs with j as the first component, and thus
the coefficient αmj controls the prior for all such
pairs of the form (j, ·), and likewise δk indicates
pairs with k as the second component, controlling
the prior over (·, k).
The SPRITE representation offers a benefit over
the original FLDA model. FLDA assumes that the
entire Cartesian product of the different factors is
represented in the model (e.g. φ parameters for ev-
ery possible tuple), which leads to issues with effi-
ciency and overparameterization with higher num-
bers of factors. With SPRITE, we can simply fix
the number of “topics” to a number smaller than
the size of the Cartesian product, and the model
will learn which subset of tuples are included,
through the values of β and δ.
Finally, another existing model family that al-
lows for topic factorization is the sparse additive
generative model (SAGE) (Eisenstein et al., 2011).
SAGE uses a log-linear parameterization to define
word distributions. SAGE is a general family of
models that need not be factored, but is presented
as an efficient solution for including multiple fac-
tors, such as topic and geography or topic and au-
thor ideology. Like SCTM, φ is exactly defined as
a product of ω weights, rather than our approach
of using the product to define a prior over φ.
</bodyText>
<subsectionHeader confidence="0.999281">
4.4 Topic Hierarchies and Correlations
</subsectionHeader>
<bodyText confidence="0.999957893617021">
While the two previous subsections primarily fo-
cused on word distributions (with FLDA being an
exception that focused on both), SPRITE’s priors
over topic distributions also have useful charac-
teristics. The component-specific δ vectors can
be interpreted as common topic distribution pat-
terns, where each component is likely to give high
weight to groups of topics that tend to occur to-
gether. Each document’s α weights encode which
of the topic groups are present in that document.
Similar properties are captured by the Pachinko
allocation model (PAM) (Li and McCallum,
2006). Under PAM, each document has a distri-
bution over supertopics. Each supertopic is as-
sociated with a Dirichlet prior over subtopic dis-
tributions, where subtopics are the low level top-
ics that are associated with word parameters φ.
Documents also have supertopic-specific distribu-
tions over subtopics (drawn from each supertopic-
specific Dirichlet prior). Each topic in a document
is drawn by first drawing a supertopic from the
document’s distribution, then drawing a subtopic
from that supertopic’s document distribution.
While not equivalent, this is quite similar to
SPRITE where document components correspond
to supertopics. Each document’s α weights can
be interpreted to be similar to a distribution over
supertopics, and each δ vector is that supertopic’s
contribution to the prior over subtopics. The prior
over the document’s topic distribution is thus af-
fected by the document’s supertopic weights α.
The SPRITE formulation naturally allows for
powerful extensions to PAM. One possibility is
to include topic components for the word distri-
butions, in addition to document components, and
to tie together δcz
and βzc (§3.2). This models the
intuitive characteristic that subtopics belonging to
similar supertopics (encoded by δ) should come
from similar priors over their word distributions
(since they will have similar β values). That is,
children of a supertopic are topically related – they
are likely to share words. This is a richer alterna-
tive to the hierarchical variant of PAM proposed
by Mimno et al. (2007), which modeled separate
word distributions for supertopics and subtopics,
but the subtopics were not dependent on the super-
</bodyText>
<page confidence="0.99604">
47
</page>
<bodyText confidence="0.999189666666667">
topic word distributions. Another extension is to
form a strict tree structure, making each subtopic
belong to exactly one supertopic: a true hierarchy.
</bodyText>
<subsectionHeader confidence="0.997537">
4.5 Conditioning on Document Attributes
</subsectionHeader>
<bodyText confidence="0.99999419047619">
SPRITE also naturally provides the ability to con-
dition document topic distributions on features of
the document, such as a user rating in a review.
To do this, let the number of document compo-
nents be the number of features, and the value of
αmc is the mth document’s value of the cth fea-
ture. The δ vectors then influence the document’s
topic prior based on the feature values. For exam-
ple, increasing αmc will increase the prior for topic
z if δcz is positive and decrease the prior if δcz is
negative. This is similar to the structure used for
PAM (§4.4), but here the α weights are fixed and
provided as input, rather than learned and inter-
preted as supertopic weights. This is identical to
the Dirichlet-multinomial regression (DMR) topic
model (Mimno and McCallum, 2008). The DMR
topic model define’s each document’s Dirichlet
prior over topics as a log-linear function of the
document’s feature values and regression coeffi-
cients for each topic. The cth feature’s regression
coefficients correspond to the δc vector in SPRITE.
</bodyText>
<sectionHeader confidence="0.967086" genericHeader="method">
5 Inference and Parameter Estimation
</sectionHeader>
<bodyText confidence="0.9999956">
We now discuss how to infer the posterior of the
latent variables z and parameters θ and φ, and find
maximum a posteriori (MAP) estimates of the hy-
perparameters α, β, δ, and ω, given their hyperpri-
ors. We take a Monte Carlo EM approach, using a
collapsed Gibbs sampler to sample from the pos-
terior of the topic assignments z conditioned on
the hyperparameters, then optimizing the hyperpa-
rameters using gradient-based optimization condi-
tioned on the samples.
Given the hyperparameters, the sampling equa-
tions are identical to the standard LDA sampler
(Griffiths and Steyvers, 2004). The partial deriva-
tive of the collapsed log likelihood L of the corpus
with respect to each hyperparameter βkc is:
</bodyText>
<equation confidence="0.999515666666667">
∂L ∂P(β) +
∂βkc = ∂βkc v
(Ψ(nkv+˜φkv) −Ψ(˜φkv) +Ψ(Ekl˜φklv) −Ψ(Eklnvl+˜φklv))
</equation>
<bodyText confidence="0.985761111111111">
where ˜φkv=exp(Ec, βkc,ωc,v), nkv is the number
of times word v is assigned to topic k (in the
samples from the E-step), and T is the digamma
function, the derivative of the log of the gamma
function. The digamma terms arise from the
Dirichlet-multinomial distribution, when integrat-
ing out the parameters φ. P(β) is the hyperprior.
For a 0-mean Gaussian hyperprior with variance
σ2, ∂P (β)
</bodyText>
<equation confidence="0.856702">
∂βkc = −βkc
</equation>
<bodyText confidence="0.919905">
σ2 . Under a Dirchlet(ρ) hyper-
prior, when we want β to represent an indicator
vector (§3.1.2), ∂P(β) ∂βkc = ρ−1 βkc .
The partial derivatives for the other hyperpa-
rameters are similar. Rather than involving a sum
over the vocabulary, ∂L
</bodyText>
<equation confidence="0.812373">
∂δck sums over documents,
while ∂L
∂ωcv and ∂L
∂αmc sum over topics.
</equation>
<bodyText confidence="0.987040181818182">
Our inference algorithm alternates between one
Gibbs iteration and one iteration of gradient as-
cent, so that the parameters change gradually. For
unconstrained parameters, we use the update rule:
xt+1=xt + ηt∇L(xt), for some variable x and
a step size ηt at iteration t. For parameters con-
strained to the simplex (such as when β is a soft
indicator vector), we use exponentiated gradient
ascent (Kivinen and Warmuth, 1997) with the up-
date rule: xt+1
i a xti exp(ηt∇iL(xt)).
</bodyText>
<subsectionHeader confidence="0.991439">
5.1 Tightening the Constraints
</subsectionHeader>
<bodyText confidence="0.950154071428572">
For variables that we prefer to be binary but
have softened to continuous variables using sparse
Beta or Dirichlet priors, we can straightforwardly
strengthen the preference to be binary by modify-
ing the objective function to favor the prior more
heavily. Specifically, under a Dirichlet(ρ&lt;1) prior
we will introduce a scaling parameter τt ≥ 1
to the prior log likelihood: τt log P(β) with par-
tial derivative τt ρ−1
βkc , which adds extra weight to
the sparse Dirichlet prior in the objective. The
algorithm used in our experiments begins with
τ1 = 1 and optionally increases τ over time. This
is a deterministic annealing approach, where τ
corresponds to an inverse temperature (Ueda and
Nakano, 1998; Smith and Eisner, 2006).
As τ approaches infinity, the prior-annealed
MAP objective maxβ P(φ|β)P(β)τ approaches
maxβ P(φ|β) maxβ P(β). Annealing only the
prior P(β) results in maximization of this term
only, while the outer max chooses a good β under
P(φ|β) as a tie-breaker among all β values that
maximize the inner max (binary-valued β).3
We show experimentally (§7.2.2) that annealing
the prior yields values that satisfy the constraints.
3Other modifications could be made to the objective func-
tion to induce sparsity, such as entropy regularization (Bala-
subramanyan and Cohen, 2013).
</bodyText>
<equation confidence="0.95391">
ωcv ˜φkv X (1)
</equation>
<page confidence="0.996758">
48
</page>
<sectionHeader confidence="0.8613875" genericHeader="method">
6 A Factored Hierarchical Model of
Topic and Perspective
</sectionHeader>
<bodyText confidence="0.999161632653061">
We will now describe a SPRITE model that en-
compasses nearly all of the structures and exten-
sions described in §3–4, followed by experimen-
tal results using this model to jointly capture topic
and “perspective” in a corpus of political debates
(where perspective corresponds to ideology) and
a corpus of online doctor reviews (where perspec-
tive corresponds to the review sentiment).
First, we will create a topic hierarchy (§4.4).
The hierarchy will model both topics and docu-
ments, where αm is document m’s supertopic pro-
portions, δc is the cth supertopic’s subtopic prior,
ωc is the cth supertopic’s word prior, and βk is
the weight vector that selects the kth topic’s par-
ent supertopic, which incorporates (soft) indicator
vectors to encode a tree structure (§3.1.2).
We want a weighted tree; while each βk has
only one nonzero element, the nonzero element
can be a value other than 1. We do this by replac-
ing the single coefficient βkc with a product of two
variables: bkc ˆβkc. Here, ˆβk is a real-valued weight
vector, while bkc is a binary indicator vector which
zeroes out all but one element of βk. We do the
same with the δ vectors, replacing δck with bkc ˆδck.
The b variables are shared across both topic and
document components, which is how we tie these
together (§3.2). We relax the binary requirement
and instead allow a positive real-valued vector
whose elements sum to 1, with a Dirichlet(ρ&lt;1)
prior to encourage sparsity (§3.1.2).
To be properly interpreted as a hierarchy, we
constrain the coefficients α and β (and by ex-
tension, δ) to be positive. To optimize these pa-
rameters in a mathematically convenient way, we
write βkc as exp(log βkc), and instead optimize
log βkc E R rather than βkc E R+.
Second, we factorize (§4.3) our hierarchy such
that each topic depends not only on its supertopic,
but also on a value indicating perspective. For ex-
ample, a conservative topic about energy will ap-
pear differently from a liberal topic about energy.
The prior for a topic will be a log-linear combina-
tion of both a supertopic (e.g. energy) and a per-
spective (e.g. liberal) weight vector. The variables
associated with the perspective component are de-
noted with superscript (P) rather than subscript c.
To learn meaningful perspective parameters, we
include supervision in the form of document at-
tributes (§4.5). Each document includes a pos-
</bodyText>
<listItem confidence="0.989119333333333">
• bk ∼ Dirichlet(ρ &lt; 1) (soft indicator)
• α(P) is given as input (perspective value)
• δ(P)
</listItem>
<equation confidence="0.996266375">
k = β(P)
k
• ˜φkv = exp(ω(B)
v +β(P )
k ω(P )
v +Ec bkc ˆβkcωcv)
• ˜θmk = exp(δ(B)
k + αm )δkP) +Ecbkcαmcˆδck)
</equation>
<figureCaption confidence="0.9973715">
Figure 3: Summary of the hyperparameters in our SPRITE-
based topic and perspective model (§6).
</figureCaption>
<bodyText confidence="0.9991465">
itive or negative score denoting the perspective,
which is the variable α(P) mfor document m. Since
α(P) are the coefficients for δ(P), positive values
of δ(P)
k indicate that topic k is more likely if the au-
thor is conservative (which has a positive α score
in our data), and less likely if the author is liberal
(which has a negative score). There is only a single
perspective component, but it represents two ends
of a spectrum with positive and negative weights;
β(P )and δ(P) are not constrained to be positive,
unlike the supertopics. We also set β(P)
</bodyText>
<equation confidence="0.998941">
k = δ(P )
</equation>
<bodyText confidence="0.961431666666667">
k .
This means that topics with positive δ(P)
k will also
have a positive β coefficient that is multiplied with
the perspective word vector ω(P).
Finally, we include “bias” component vectors
denoted ω(B) and δ(B), which act as overall
weights over the vocabulary and topics, so that the
component-specific ω and δ weights can be inter-
preted as deviations from the global bias weights.
Figure 3 summarizes the model. This includes
most of the features described above (trees, fac-
tored structures, tying topic and document compo-
nents, and document attributes), so we can ablate
model features to measure their effect.
</bodyText>
<sectionHeader confidence="0.999878" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<subsectionHeader confidence="0.995251">
7.1 Datasets and Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999899">
We applied our models to two corpora:
</bodyText>
<listItem confidence="0.99998">
• Debates: A set of floor debates from the 109th–
</listItem>
<bodyText confidence="0.996181181818182">
112th U.S. Congress, collected by Nguyen et
al. (2013), who also applied a hierarchical topic
model to this data. Each document is a tran-
script of one speaker’s turn in a debate, and each
document includes the first dimension of the
DW-NOMINATE score (Lewis and Poole, 2004),
a real-valued score indicating how conservative
(positive) or liberal (negative) the speaker is.
This value is α(P). We took a sample of 5,000
documents from the House debates (850,374 to-
kens; 7,426 types), balanced across party affilia-
</bodyText>
<page confidence="0.997299">
49
</page>
<bodyText confidence="0.982145">
tion. We sampled from the most partisan speak-
ers, removing scores below the median value.
</bodyText>
<listItem confidence="0.827873333333333">
• Reviews: Doctor reviews from RateMDs.com,
previously analyzed using FLDA (Paul et al.,
2013; Wallace et al., 2014). The reviews con-
</listItem>
<bodyText confidence="0.993162481481482">
tain ratings on a 1–5 scale for multiple aspects.
We centered the ratings around the middle value
3, then took reviews that had the same sign for
all aspects, and averaged the scores to produce
a value for α(P). Our corpus contains 20,000
documents (476,991 tokens; 10,158 types), bal-
anced across positive/negative scores.
Unless otherwise specified, K=50 topics and
C=10 components (excluding the perspective
component) for Debates, and K=20 and C=5 for
Reviews. These values were chosen as a qualita-
tive preference, not optimized for predictive per-
formance, but we experiment with different values
in §7.2.2. We set the step size qt according to Ada-
Grad (Duchi et al., 2011), where the step size is
the inverse of the sum of squared historical gradi-
ents.4 We place a sparse Dirichlet(p=0.01) prior
on the b variables, and apply weak regulariza-
tion to all other hyperparameters via a JV(0,102)
prior. These hyperparameters were chosen after
only minimal tuning, and were selected because
they showed stable and reasonable output qualita-
tively during preliminary development.
We ran our inference algorithm for 5000 itera-
tions, estimating the parameters 0 and 0 by aver-
aging the final 100 iterations. Our results are aver-
aged across 10 randomly initialized samplers.5
</bodyText>
<subsectionHeader confidence="0.9989035">
7.2 Evaluating the Topic Perspective Model
7.2.1 Analysis of Output
</subsectionHeader>
<bodyText confidence="0.99978">
Figure 4 shows examples of topics learned from
the Reviews corpus. The figure includes the high-
est probability words in various topics as well as
the highest weight words in the supertopic com-
ponents and perspective component, which feed
into the priors over the topic parameters. We see
that one supertopic includes many words related to
surgery, such as procedure and performed, and has
multiple children, including a topic about dental
work. Another supertopic includes words describ-
ing family members such as kids and husband.
</bodyText>
<footnote confidence="0.9658512">
4AdaGrad decayed too quickly for the b variables. For
these, we used a variant suggested by Zeiler (2012) which
uses an average of historical gradients rather than a sum.
5Our code and the data will be available at:
http://cs.jhu.edu/˜mpaul.
</footnote>
<bodyText confidence="0.99962047368421">
One topic has both supertopics as parents, which
appears to describe surgeries that saved a family
member’s life, with top words including {saved,
life, husband, cancer}. The figure also illustrates
which topics are associated more with positive or
negative reviews, as indicated by the value of 8(P).
Interpretable parameters were also learned from
the Debates corpus. Consider two topics about
energy that have polar values of 8(P). The
conservative-leaning topic is about oil and gas,
with top words including {oil, gas, companies,
prices, drilling}. The liberal-leaning topic is
about renewable energy, with top words includ-
ing {energy, new, technology, future, renewable}.
Both of these topics share a common parent of an
industry-related supertopic whose top words are
{industry, companies, market, price}. A nonparti-
san topic under this same supertopic has top words
{credit, financial, loan, mortgage, loans}.
</bodyText>
<subsubsectionHeader confidence="0.586544">
7.2.2 Quantitative Evaluation
</subsubsectionHeader>
<bodyText confidence="0.9999296">
We evaluated the model on two predictive tasks as
well as topic quality. The first metric is perplex-
ity of held-out text. The held-out set is based on
tokens rather than documents: we trained on even
numbered tokens and tested on odd tokens. This is
a type of “document completion” evaluation (Wal-
lach et al., 2009b) which measures how well the
model can predict held-out tokens of a document
after observing only some.
We also evaluated how well the model can pre-
dict the attribute value (DW-NOMINATE score or
user rating) of the document. We trained a linear
regression model using the document topic distri-
butions 0 as features. We held out half of the docu-
ments for testing and measured the mean absolute
error. When estimating document-specific SPRITE
parameters for held-out documents, we fix the fea-
ture value α(P) m= 0 for that document.
These predictive experiments do not directly
measure performance at many of the particular
tasks that topic models are well suited for, like
data exploration, summarization, and visualiza-
tion. We therefore also include a metric that more
directly measures the quality and interpretability
of topics. We use the topic coherence metric intro-
duced by Mimno et al. (2011), which is based on
co-occurrence statistics among each topic’s most
probable words and has been shown to correlate
with human judgments of topic quality. This met-
ric measures the quality of each topic, and we
</bodyText>
<page confidence="0.967924">
50
</page>
<figure confidence="0.999797516393443">
pain
surgery
dr
went
knee
foot
neck
mri
injury
shoulder
bone
months
told
surgeon
therapy
told
hospital
dr
blood
went
later
days
mother
said
er
cancer
weight
home
father
months
dentist
teeth
dental
work
tooth
root
mouth
pain
dentists
went
filling
canal
dr
crown
cleaning
“Surgery”
surgery
pain
went
dr
surgeon
told
procedure
months
performed
removed
left
fix
said
later
years
dr
life
thank
saved
god
husband
heart
cancer
years
helped
doctors
hospital
father
man
able
“Family”
dr
best
children
years
kids
cares
hes
care
old
daughter
child
husband
family
pediatrician
trust
dr
best
years
doctor
love
cares
ive
children
patients
hes
family
kids
seen
doctors
son
baby
son
pregnancy
dr
child
pregnant
ob
daughter
first
delivered
gyn
birth
delivery
section
hospital
</figure>
<figureCaption confidence="0.83045925">
Figure 4: Examples of topics (gray boxes) and components (colored boxes) learned on the Reviews corpus with 20 topics and
5 components. Words with the highest and lowest values of w(P), the perspective component, are shown on the left, reflecting
positive and negative sentiment words. The words with largest w values in two supertopic components are also shown, with
manually given labels. Arrows from components to topics indicate that the topic’s word distribution draws from that component
</figureCaption>
<bodyText confidence="0.9405835">
in its prior (with non-zero β value). There are also implicit arrows from the perspective component to all topics (omitted for
clarity). The vertical positions of topics reflect the topic’s perspective value B(P). Topics centered above the middle line are
more likely to occur in reviews with positive scores, while topics below the middle line are more likely in negative reviews.
Note that this is a “soft” hierarchy because the tree structure is not strictly enforced, so some topics have multiple parent
components. Table 3 shows how strict trees can be learned by tuning the annealing parameter.
measure the average coherence across all topics:
</bodyText>
<equation confidence="0.998749666666667">
DF(vkm, vkl) + 1
log (2)
DF (vkl)
</equation>
<bodyText confidence="0.9894472">
where DF(v, w) is the document frequency of
words v and w (the number of documents in which
they both occur), DF(v) is the document fre-
quency of word v, and vki is the ith most probable
word in topic k. We use the top M = 20 words.
This metric is limited to measuring only the qual-
ity of word clusters, ignoring the potentially im-
proved interpretability of organizing the data into
certain structures. However, it is still useful as an
alternative measure of performance and utility, in-
dependent of the models’ predictive abilities.
Using these three metrics, we compared to sev-
eral variants (denoted in bold) of the full model
to understand how the different parts of the model
affect performance:
</bodyText>
<listItem confidence="0.824239458333333">
• Variants that contain the hierarchy components
but not the perspective component (Hierarchy
only), and vice versa (Perspective only).
• The “hierarchy only” model using only docu-
ment components δ and no topic components.
This is a PAM-style model because it exhibits
similar behavior to PAM (§4.4). We also com-
pared to the original PAM model.
• The “hierarchy only” model using only topic
components ω and no document components.
This is a SCTM-style model because it exhibits
similar behavior to SCTM (§4.2).
• The full model where α(P) is learned rather than
given as input. This is a FLDA-style model that
has similar behavior to FLDA (§4.3). We also
compared to the original FLDA model.
• The “perspective only” model but without the
ω(P) topic component, so the attribute value af-
fects only the topic distributions and not the
word distributions. This is identical to the DMR
model of Mimno and McCallum (2008) (§4.5).
• A model with no components except for the
bias vectors ω(B) and δ(B). This is equiva-
lent to LDA with optimized hyperparameters
</listItem>
<bodyText confidence="0.979038625">
(learned). We also experimented with using
fixed symmetric hyperparameters, using val-
ues suggested by Griffiths and Steyvers (2004):
50/K and 0.01 for topic and word distributions.
To put the results in context, we also compare to
two types of baselines: (1) “bag of words” base-
lines, where we measure the perplexity of add-one
smoothed unigram language models, we measure
</bodyText>
<equation confidence="0.764711">
1 K
E
k=1
M
E
m=2
m−1�
l=1
K
</equation>
<page confidence="0.842052">
51
</page>
<figure confidence="0.982295935483871">
Perspective
best
love
years
caring
children
really
wonderful
hes
great
family
comfortable
listens
thank
amazing
É
É
went
pay
later
staff
asked
money
company
refused
pain
office
didn’t
said
told
doctor
</figure>
<table confidence="0.988708866666667">
Debates Reviews
Model Perplexity Prediction error Coherence Perplexity Prediction error Coherence
Full model †1555.5 f 2.3 †0.615 f 0.001 -342.8 f 0.9 †1421.3 f 8.4 †0.787 f 0.006 -512.7 f 1.6
Hierarchy only †1561.8 f 1.4 0.620 f 0.002 -342.6 f 1.1 †1457.2 f 6.9 †0.804 f 0.007 -509.1 f 1.9
Perspective only †1567.3 f 2.3 †0.613 f 0.002 -342.1 f 1.2 †1413.7 f 2.2 †0.800 f 0.002 -512.0 f 1.7
SCTM-style 1572.5 f 1.6 0.620 f 0.002 †-335.8 f 1.1 1504.0 f 1.9 †0.837 f 0.002 †-490.8 f 0.9
PAM-style †1567.4 f 1.9 0.620 f 0.002 -347.6 f 1.4 †1440.4 f 2.7 †0.835 f 0.004 -542.9 f 6.7
FLDA-style †1559.5 f 2.0 0.617 f 0.002 -340.8 f 1.4 †1451.1 f 5.4 †0.809 f 0.006 -505.3 f 2.3
DMR 1578.0 f 1.1 0.618 f 0.002 -343.1 f 1.0 †1416.4 f 3.0 †0.799 f 0.003 -511.6 f 2.0
PAM 1578.9 f 0.3 0.622 f 0.003 †-336.0 f 1.1 1514.8 f 0.9 †0.835 f 0.003 †-493.3 f 1.2
FLDA 1574.1 f 2.2 0.618 f 0.002 -344.4 f 1.3 1541.9 f 2.3 0.856 f 0.003 -502.2 f 3.1
LDA (learned) 1579.6 f 1.5 0.620 f 0.001 -342.6 f 0.6 1507.9 f 2.4 0.846 f 0.002 -501.4 f 1.2
LDA (fixed) 1659.3 f 0.9 0.622 f 0.002 -349.5 f 0.8 1517.2 f 0.4 0.920 f 0.003 -585.2 f 0.9
Bag of words 2521.6 f 0.0 0.617 f 0.000 †-196.2 f 0.0 1633.5 f 0.0 0.813 f 0.000 †-408.1 f 0.0
Naive baseline 7426.0 f 0.0 0.677 f 0.000 -852.9 f 7.4 10158.0 f 0.0 1.595 f 0.000 -795.2 f 13.0
</table>
<tableCaption confidence="0.999672">
Table 2: Perplexity of held-out tokens and mean absolute error for attribute prediction using various models (f std. error).
† indicates significant improvement (p &lt; 0.05) over optimized LDA under a two-sided t-test.
</tableCaption>
<bodyText confidence="0.998839175675676">
the prediction error using bag of words features,
and we measure coherence of the unigram distri-
bution; (2) naive baselines, where we measure the
perplexity of the uniform distribution over each
dataset’s vocabulary, the prediction error when
simply predicting each attribute as the mean value
in the training set, and the coherence of 20 ran-
domly selected words (repeated for 10 trials).
Table 2 shows that the full SPRITE model sub-
stantially outperforms the LDA baseline at both
predictive tasks. Generally, model variants with
more structure perform better predictively.
The difference between SCTM-style and
PAM-style is that the former uses only topic com-
ponents (for word distributions) and the latter uses
only document components (for the topic distri-
butions). Results show that the structured priors
are more important for topic than word distribu-
tions, since PAM-style has lower perplexity on
both datasets. However, models with both topic
and document components generally outperform
either alone, including comparing the Perspec-
tive only and DMR models. The former includes
both topic and document perspective components,
while DMR has only a document level component.
PAM does not significantly outperform opti-
mized LDA in most measures, likely because it up-
dates the hyperparameters using a moment-based
approximation, which is less accurate than our
gradient-based optimization. FLDA perplexity
is 2.3% higher than optimized LDA on Reviews,
comparable to the 4% reported by Paul and Dredze
(2012) on a different corpus. The FLDA-style
SPRITE variant, which is more flexible, signifi-
cantly outperforms FLDA in most measures.
The results are quite different under the co-
herence metric. It seems that topic components
(which influence the word distributions) improve
coherence over LDA, while document compo-
nents worsen coherence. SCTM-style (which uses
only topic components) does the best in both
datasets, while PAM-style (which uses only doc-
uments) does the worst. PAM also significantly
improves over LDA, despite worse perplexity.
The LDA (learned) baseline substantially out-
performs LDA (fixed) in all cases, highlighting the
importance of optimizing hyperparameters, con-
sistent with prior research (Wallach et al., 2009a).
Surprisingly, many SPRITE variants also outper-
form the bag of words regression baseline, even
though the latter was tuned to optimize perfor-
mance using heavy t2 regularization, which we
applied only weakly (without tuning) to the topic
model features. We also point out that the “bag
of words” version of the coherence metric (the co-
herence of the top 20 words) is higher than the av-
erage topic coherence, which is an artifact of how
the metric is defined: the most probable words in
the corpus also tend to co-occur together in most
documents, so these words are considered to be
highly coherent when grouped together.
Parameter Sensitivity We evaluated the full
model at the two predictive tasks with varying
numbers of topics ({12,25,50,100} for Debates
and {5,10,20,40} for Reviews) and components
({2,5,10,20}). Figure 5 shows that performance is
more sensitive to the number of topics than com-
ponents, with generally less variance among the
latter. More topics improve performance mono-
tonically on Debates, while performance declines
at 40 topics on Reviews. The middle range of com-
ponents (5–10) tends to perform better than too
few (2) or too many (20) components.
Regardless of quantitative differences, the
</bodyText>
<page confidence="0.997814">
52
</page>
<figureCaption confidence="0.996825">
Figure 5: Predictive performance of full model with differ-
ent numbers of topics K across different numbers of compo-
nents, represented on the x-axis (log scale).
</figureCaption>
<table confidence="0.9995432">
τt Debates Reviews
0.000 (Sparse DAG) 58.1% 42.4%
1.000 (Soft Tree) 93.2% 74.6%
1.001t (Hard Tree) 99.8% 99.4%
1.003t (Hard Tree) 100% 100%
</table>
<tableCaption confidence="0.923907">
Table 3: The percentage of indicator values that are sparse
(near 0 or 1) when using different annealing schedules.
</tableCaption>
<bodyText confidence="0.999942285714286">
choice of parameters may depend on the end ap-
plication and the particular structures that the user
has in mind, if interpretability is important. For
example, if the topic model is used as a visual-
ization tool, then 2 components would not likely
result in an interesting hierarchy to the user, even
if this setting produces low perplexity.
Structured Sparsity We use a relaxation of the
binary b that induces a “soft” tree structure. Ta-
ble 3 shows the percentage of b values which are
within c = .001 of 0 or 1 under various anneal-
ing schedules, increasing the inverse temperature
T by 0.1% after each iteration (i.e. Tt = 1.001t)
as well as 0.3% and no annealing at all (T = 1).
At T = 0, we model a DAG rather than a tree, be-
cause the model has no preference that b is sparse.
Many of the values are binary in the DAG case, but
the sparse prior substantially increases the number
of binary values, obtaining fully binary structures
with sufficient annealing. We compare the DAG
and tree structures more in the next subsection.
</bodyText>
<subsectionHeader confidence="0.998967">
7.3 Structure Comparison
</subsectionHeader>
<bodyText confidence="0.999962916666667">
The previous subsection experimented with mod-
els that included a variety of structures, but did
not provide a comparison of each structure in iso-
lation, since most model variants were part of a
complex joint model. In this section, we exper-
iment with the basic SPRITE model for the three
structures described in §3: a DAG, a tree, and a
factored forest. For each structure, we also exper-
iment with each type of component: document,
topic, and both types (combined).
For this set of experiments, we included a third
dataset that does not contain a perspective value:
</bodyText>
<listItem confidence="0.930035666666667">
• Abstracts: A set of 957 abstracts from the ACL
anthology (97,168 tokens; 8,246 types). These
abstracts have previously been analyzed with
FLDA (Paul and Dredze, 2012), so we include
it here to see if the factored structure that we
explore in this section learns similar patterns.
</listItem>
<bodyText confidence="0.999799368421053">
Based on our sparsity experiments in the pre-
vious subsection, we set Tt = 1.003t to induce
hard structures (tree and factored) and T = 0 to in-
duce a DAG. We keep the same parameters as the
previous subsection: K=50 and C=10 for Debates
and K=20 and C=5 for Reviews. For the factored
structures, we use two factors, with one factor hav-
ing more components than the other: 3 and 7 com-
ponents for Debates, and 2 and 3 components for
Reviews (the total number of components across
the two factors is therefore the same as for the
DAG and tree experiments). The Abstracts exper-
iments use the same parameters as with Debates.
Since the Abstracts dataset does not have a per-
spective value to predict, we do not include predic-
tion error as a metric, instead focusing on held-out
perplexity and topic coherence (Eq. 2). Table 4
shows the results of these two metrics.
Some trends are clear and consistent. Topic
components always hurt perplexity, while these
components typically improve coherence, as was
observed in the previous subsection. It has pre-
viously been observed that perplexity and topic
quality are not correlated (Chang et al., 2009).
These results show that the choice of components
depends on the task at hand. Combining the two
components tends to produce results somewhere
in between, suggesting that using both component
types is a reasonable “default” setting.
Document components usually improve per-
plexity, likely due to the nature of the document
completion setup, in which half of each document
is held out. The document components capture
correlations between topics, so by inferring the
components that generated the first half of the doc-
ument, the prior is adjusted to give more probabil-
ity to topics that are likely to occur in the unseen
second half. Another interesting trend is that the
</bodyText>
<figure confidence="0.998335266666667">
Prediction error
K=5 K=20
K=10 K=40
2 5 10 20
.630 .92
.625 .88
.620 .84
.615 .80
.610 .76
.605
2 5 10 20 2 5 10 20
Number of components Number of components
Perplexity
Reviews
1600
1550
1500
1450
1400
2 5 10 20
Debates
1900
K=12
K=50
K=25
K=100
1800
1700
1600
1500
</figure>
<page confidence="0.991185">
53
</page>
<table confidence="0.999592857142857">
Perplexity Coherence
DAG Tree Factored DAG Tree Factored
Debates
Document 1572.0 f 0.9 1568.7 f 2.0 1566.8 f 2.0 -342.9 f 1.2 -346.0 f 0.9 -343.2 f 1.0
Topic 1575.0 f 1.5 1573.4 f 1.8 1559.3 f 1.5 -342.4 f 0.6 -339.2 f 1.7 -333.9 f 0.9
Combined 1566.7 f 1.7 1559.9 f 1.9 1552.5 f 1.9 -342.9 f 1.3 -342.6 f 1.2 -340.3 f 1.0
Reviews
Document 1456.9 f 3.8 1446.4 f 4.0 1450.4 f 5.5 -512.2 f 4.6 -527.9 f 6.5 -535.4 f 7.4
Topic 1508.5 f 1.7 1517.9 f 2.0 1502.0 f 1.9 -500.1 f 1.2 -499.0 f 0.9 -486.1 f 1.5
Combined 1464.1 f 3.3 1455.1 f 5.6 1448.5 f 8.5 -504.9 f 1.4 -527.8 f 6.1 -535.5 f 8.2
Abstracts
Document 3107.7 f 7.7 3089.5 f 9.1 3098.7 f 10.2 -393.2 f 0.8 -390.8 f 0.9 -392.8 f 1.5
Topic 3241.7 f 2.1 3455.9 f 10.2 3507.4 f 9.7 -389.0 f 0.8 -388.8 f 0.7 -332.2 f 1.1
Combined 3200.8 f 3.5 3307.2 f 7.8 3364.9 f 19.1 -373.1 f 0.8 -360.6 f 0.9 -342.3 f 0.9
</table>
<tableCaption confidence="0.988587">
Table 4: Quantitative results for different structures (columns) and different components (rows) for two metrics (f std. error)
across three datasets. The best (structure, component) pair for each dataset and metric is in bold.
</tableCaption>
<bodyText confidence="0.998949942307692">
factored structure tends to perform well under both
metrics, with the lowest perplexity and highest co-
herence in a majority of the nine comparisons (i.e.
each row). Perhaps the models are capturing a nat-
ural factorization present in the data.
To understand the factored structure qualita-
tively, Figure 6 shows examples of components
from each factor along with example topics that
draw from all pairs of these components, learned
on Abstracts. We find that the factor with the
smaller number of components (left of the figure)
seems to decompose into components represent-
ing the major themes or disciplines found in ACL
abstracts, with one component expressing compu-
tational approaches (top) and the other expressing
linguistic theory (bottom). The third component
(not shown) has words associated with speech, in-
cluding {spoken, speech, recognition}.
The factor shown on the right seems to decom-
pose into different research topics: one compo-
nent represents semantics (top), another syntax
(bottom), with others including morphology (top
words including {segmentation, chinese, morphol-
ogy}) and information retrieval (top words includ-
ing {documents, retrieval, ir}).
Many of the topics intuitively follow from the
components of these two factors. For example,
the two topics expressing vector space models and
distributional semantics (top left and right) both
draw from the “computational” and “semantics”
components, while the topics expressing ontolo-
gies and question answering (middle left and right)
draw from “linguistics” and “semantics”.
The factorization is similar to what had been
previously been induced by FLDA. Figure 3 of
Paul and Dredze (2012) shows components that
look similar to the computational methods and
linguistic theory components here, and the factor
with the largest number of components also de-
composes by research topic. These results show
that SPRITE is capable of recovering similar struc-
tures as FLDA, a more specialized model. SPRITE
is also much more flexible than FLDA. While
FLDA strictly models a one-to-one mapping of
topics to each pair of components, SPRITE allows
multiple topics to belong to the same pair (as in
the semantics examples above), and conversely
SPRITE does not require that all pairs have an as-
sociated topic. This property allows SPRITE to
scale to larger numbers of factors than FLDA, be-
cause the number of topics is not required to grow
with the number of all possible tuples.
</bodyText>
<sectionHeader confidence="0.9999" genericHeader="related work">
8 Related Work
</sectionHeader>
<bodyText confidence="0.995669454545455">
Our topic and perspective model is related to su-
pervised hierarchical LDA (SHLDA) (Nguyen et
al., 2013), which learns a topic hierarchy while
also learning regression parameters to associate
topics with feature values such as political per-
spective. This model does not explicitly incorpo-
rate perspective-specific word priors into the top-
ics (as in our factorized approach). The regression
structure is also different. SHLDA is a “down-
stream” model, where the perspective value is a re-
sponse variable conditioned on the topics. In con-
trast, SPRITE is an “upstream” model, where the
topics are conditioned on the perspective value.
We argue that the latter is more accurate as a gen-
erative story (the emitted words depend on the
author’s perspective, not the other way around).
Moreover, in our model the perspective influences
both the word and topic distributions (through the
topic and document components, respectively).
Inverse regression topic models (Rabinovich
and Blei, 2014) use document feature values (such
as political ideology) to alter the parameters of the
</bodyText>
<page confidence="0.997379">
54
</page>
<figureCaption confidence="0.999681333333333">
Figure 6: Examples of topics (gray boxes) and components (colored boxes) learned on the Abstracts corpus with 50 topics
using a factored structure. The components have been grouped into two factors, one factor with 3 components (left) and one
with 7 (right), with two examples shown from each. Each topic prior draws from exactly one component from each factor.
</figureCaption>
<figure confidence="0.999333311111111">
similarity
words
word
vector
semantic
similar
based
method
words
corpus
word
multiword
paper
based
frequency
expressions
question
questions
answer
answering
answers
qa
systems
type
training
learning
corpus
large
unsupervised
corpora
method
data
semantic
knowledge
semantics
ontology
relations
lexical
concepts
concept
parsing
parser
parse
treebank
grammar
tree
trees
structure
german
languages
french
english
multilingual
italian
structure
spanish
“Semantics”
semantic
knowledge
domain
ontology
systems
words
information
wordnet
question
dialogue
“Syntax”
parse
treebank
parser
penn
parsers
trees
dependencies
acoustic
corpus
parsing
“Computational”
method
words
word
corpus
learning
performance
approaches
training
proposed
based
“Linguistics”
</figure>
<bodyText confidence="0.95684946875">
grammar
parsing
representation
structure
grammars
parse
syntax
representations
semantics
formalism
topic-specific word distributions. This is an alter-
native to the more common approach to regression
based topic modeling, where the variables affect
the topic distributions rather than the word distri-
butions. Our SPRITE-based model does both: the
document features adjust the prior over topic dis-
tributions (through δ), but by tying together the
document and topic components (with β), the doc-
ument features also affect the prior over word dis-
tributions. To the best of our knowledge, this is the
first topic model to condition both topic and word
distributions on the same features.
The topic aspect model (Paul and Girju, 2010a)
is also a two-dimensional factored model that has
been used to jointly model topic and perspective
(Paul and Girju, 2010b). However, this model
does not use structured priors over the parameters,
unlike most of the models discussed in §4.
An alternative approach to incorporating user
preferences and expertise are interactive topic
models (Hu et al., 2013), a complimentary ap-
proach to SPRITE.
</bodyText>
<sectionHeader confidence="0.994769" genericHeader="conclusions">
9 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.999920875">
We have presented SPRITE, a family of topic mod-
els that utilize structured priors to induce pre-
ferred topic structures. Specific instantiations of
SPRITE are similar or equivalent to several exist-
ing topic models. We demonstrated the utility of
SPRITE by constructing a single model with many
different characteristics, including a topic hierar-
chy, a factorization of topic and perspective, and
supervision in the form of document attributes.
These structures were incorporated into the pri-
ors of both the word and topic distributions, unlike
most prior work that considered one or the other.
Our experiments explored how each of these var-
ious model features affect performance, and our
results showed that models with structured priors
perform better than baseline LDA models.
Our framework has made clear advancements
with respect to existing structured topic models.
For example, SPRITE is more general and of-
fers simpler inference than the shared compo-
nents topic model (Gormley et al., 2010), and
SPRITE allows for more flexible and scalable fac-
tored structures than FLDA, as described in earlier
sections. Both of these models were motivated by
their ability to learn interesting structures, rather
than their performance at any predictive task. Sim-
ilarly, our goal in this study was not to provide
state of the art results for a particular task, but
to demonstrate a framework for learning struc-
tures that are richer than previous structured mod-
els. Therefore, our experiments focused on un-
derstanding how SPRITE compares to commonly
used models with similar structures, and how the
different variants compare under different metrics.
Ultimately, the model design choice depends on
the application and the user needs. By unifying
such a wide variety of topic models, SPRITE can
serve as a common framework for enabling model
exploration and bringing application-specific pref-
erences and structure into topic models.
</bodyText>
<page confidence="0.998031">
55
</page>
<sectionHeader confidence="0.998817" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9984212">
We thank Jason Eisner and Hanna Wallach for
helpful discussions, and Viet-An Nguyen for pro-
viding the Congressional debates data. Michael
Paul is supported by a Microsoft Research PhD
fellowship.
</bodyText>
<sectionHeader confidence="0.999074" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999889282608696">
D. Andrzejewski, X. Zhu, and M. Craven. 2009. In-
corporating domain knowledge into topic modeling
via Dirichlet forest priors. In ICML.
R. Balasubramanyan and W. Cohen. 2013. Regular-
ization of latent variable models to obtain sparsity.
In SIAM Conference on Data Mining.
D. Blei and J. Lafferty. 2007. A correlated topic model
of Science. Annals ofApplied Statistics, 1(1):17–35.
D. Blei, T. Griffiths, M. Jordan, and J. Tenenbaum.
2003a. Hierarchical topic models and the nested
Chinese restaurant process. In NIPS.
D. Blei, A. Ng, and M. Jordan. 2003b. Latent Dirichlet
allocation. JMLR.
J. Chang, J. Boyd-Graber, S. Gerrish, C. Wang, and
D. Blei. 2009. Reading tea leaves: How humans
interpret topic models. In NIPS.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic
optimization. JMLR, 12:2121–2159.
J. Eisenstein, A. Ahmed, and E. P. Xing. 2011. Sparse
additive generative models of text. In ICML.
M.R. Gormley, M. Dredze, B. Van Durme, and J. Eis-
ner. 2010. Shared components topic models. In
NAACL.
T. Griffiths and M. Steyvers. 2004. Finding scientific
topics. In Proceedings of the National Academy of
Sciences of the United States of America.
Y. Hu, J. Boyd-Graber, B. Satinoff, and A. Smith.
2013. Interactive topic modeling. Machine Learn-
ing, 95:423–469.
J. Kivinen and M.K. Warmuth. 1997. Exponentiated
gradient versus gradient descent for linear predic-
tors. Information and Computation, 132:1–63.
J.B. Lewis and K.T. Poole. 2004. Measuring bias and
uncertainty in ideal point estimates via the paramet-
ric bootstrap. Political Analysis, 12(2):105–127.
W. Li and A. McCallum. 2006. Pachinko alloca-
tion: DAG-structured mixture models of topic cor-
relations. In International Conference on Machine
Learning.
D. Mimno and A. McCallum. 2008. Topic mod-
els conditioned on arbitrary features with Dirichlet-
multinomial regression. In UAI.
D. Mimno, W. Li, and A. McCallum. 2007. Mixtures
of hierarchical topics with Pachinko allocation. In
International Conference on Machine Learning.
D. Mimno, H.M. Wallach, E. Talley, M. Leenders, and
A. McCallum. 2011. Optimizing semantic coher-
ence in topic models. In EMNLP.
V. Nguyen, J. Boyd-Graber, and P. Resnik. 2013. Lex-
ical and hierarchical topic regression. In Neural In-
formation Processing Systems.
M.J. Paul and M. Dredze. 2012. Factorial LDA: Sparse
multi-dimensional text models. In Neural Informa-
tion Processing Systems (NIPS).
M.J. Paul and M. Dredze. 2013. Drug extraction from
the web: Summarizing drug experiences with multi-
dimensional topic models. In NAACL.
M. Paul and R. Girju. 2010a. A two-dimensional
topic-aspect model for discovering multi-faceted
topics. In AAAI.
M.J. Paul and R. Girju. 2010b. Summarizing con-
trastive viewpoints in opinionated text. In Empirical
Methods in Natural Language Processing.
M.J. Paul, B.C. Wallace, and M. Dredze. 2013. What
affects patient (dis)satisfaction? Analyzing online
doctor ratings with a joint topic-sentiment model.
In AAAI Workshop on Expanding the Boundaries of
Health Informatics Using AI.
M. Rabinovich and D. Blei. 2014. The inverse regres-
sion topic model. In International Conference on
Machine Learning.
D. Ramage, D. Hall, R. Nallapati, and C.D. Man-
ning. 2009. Labeled LDA: a supervised topic model
for credit attribution in multi-labeled corpora. In
EMNLP.
N.A. Smith and J. Eisner. 2006. Annealing structural
bias in multilingual weighted grammar induction. In
COLING-ACL.
E.M. Talley, D. Newman, D. Mimno, B.W. Herr II,
H.M. Wallach, G.A.P.C. Burns, M. Leenders, and
A. McCallum. 2011. Database of NIH grants us-
ing machine-learned categories and graphical clus-
tering. Nature Methods, 8(6):443–444.
N. Ueda and R. Nakano. 1998. Deterministic anneal-
ing EM algorithm. Neural Networks, 11(2):271–
282.
B.C. Wallace, M.J. Paul, U. Sarkar, T.A. Trikalinos,
and M. Dredze. 2014. A large-scale quantitative
analysis of latent factors and sentiment in online
doctor reviews. Journal of the American Medical
Informatics Association, 21(6):1098–1103.
</reference>
<page confidence="0.964862">
56
</page>
<reference confidence="0.9991761">
H.M. Wallach, D. Mimno, and A. McCallum. 2009a.
Rethinking LDA: Why priors matter. In NIPS.
H.M. Wallach, I. Murray, R. Salakhutdinov, and
D. Mimno. 2009b. Evaluation methods for topic
models. In ICML.
C. Wang and D. Blei. 2009. Decoupling sparsity
and smoothness in the discrete hierarchical Dirich-
let process. In NIPS.
M.D. Zeiler. 2012. ADADELTA: An adaptive learning
rate method. CoRR, abs/1212.5701.
</reference>
<page confidence="0.9996225">
57
58
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.429720">
<title confidence="0.999922">Generalizing Topic Models with Structured Priors</title>
<author confidence="0.999895">J Paul</author>
<affiliation confidence="0.9538035">Department of Computer Human Language Technology Center of</affiliation>
<address confidence="0.437781">Johns Hopkins University, Baltimore, MD</address>
<abstract confidence="0.9997485625">introduce a family of topic models that incorporates structure into model priors as a function of underlying components. The structured priors can be constrained to model topic hierarchies, factorizations, correlations, and superviallowing be tailored to particular settings. We demonstrate this by constructing a model to jointly infer topic hierarchies and author perspective, which we apply to corpora of political debates and online reviews. We show that the model learns intuitive topics, outperforming several other topic models at predictive tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Andrzejewski</author>
<author>X Zhu</author>
<author>M Craven</author>
</authors>
<title>Incorporating domain knowledge into topic modeling via Dirichlet forest priors.</title>
<date>2009</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="1916" citStr="Andrzejewski et al., 2009" startWordPosition="285" endWordPosition="288">odels provide no control over the desired output. This paper presents SPRITE, a family of topic models that provide a flexible framework for encoding preferences as priors for how topics should be structured. SPRITE can incorporate many types of structure that have been considered in prior work, including hierarchies (Blei et al., 2003a; Mimno et al., 2007), factorizations (Paul and Dredze, 2012; Eisenstein et al., 2011), sparsity (Wang and Blei, 2009; Balasubramanyan and Cohen, 2013), correlations between topics (Blei and Lafferty, 2007; Li and McCallum, 2006), preferences over word choices (Andrzejewski et al., 2009; Paul and Dredze, 2013), and associations between topics and document attributes (Ramage et al., 2009; Mimno and McCallum, 2008). SPRITE builds on a standard topic model, adding structure to the priors over the model parameters. The priors are given by log-linear functions of underlying components (§2), which provide additional latent structure that we will show can enrich the model in many ways. By applying particular constraints and priors to the component hyperparameters, a variety of structures can be induced such as hierarchies and factorizations (§3), and we will show that this framewor</context>
</contexts>
<marker>Andrzejewski, Zhu, Craven, 2009</marker>
<rawString>D. Andrzejewski, X. Zhu, and M. Craven. 2009. Incorporating domain knowledge into topic modeling via Dirichlet forest priors. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Balasubramanyan</author>
<author>W Cohen</author>
</authors>
<title>Regularization of latent variable models to obtain sparsity.</title>
<date>2013</date>
<booktitle>In SIAM Conference on Data Mining.</booktitle>
<contexts>
<context position="1780" citStr="Balasubramanyan and Cohen, 2013" startWordPosition="264" endWordPosition="268">task. It is crucial for the user experience that topics meet these expectations (Mimno et al., 2011; Talley et al., 2011) yet black box topic models provide no control over the desired output. This paper presents SPRITE, a family of topic models that provide a flexible framework for encoding preferences as priors for how topics should be structured. SPRITE can incorporate many types of structure that have been considered in prior work, including hierarchies (Blei et al., 2003a; Mimno et al., 2007), factorizations (Paul and Dredze, 2012; Eisenstein et al., 2011), sparsity (Wang and Blei, 2009; Balasubramanyan and Cohen, 2013), correlations between topics (Blei and Lafferty, 2007; Li and McCallum, 2006), preferences over word choices (Andrzejewski et al., 2009; Paul and Dredze, 2013), and associations between topics and document attributes (Ramage et al., 2009; Mimno and McCallum, 2008). SPRITE builds on a standard topic model, adding structure to the priors over the model parameters. The priors are given by log-linear functions of underlying components (§2), which provide additional latent structure that we will show can enrich the model in many ways. By applying particular constraints and priors to the component </context>
<context position="26692" citStr="Balasubramanyan and Cohen, 2013" startWordPosition="4389" endWordPosition="4393">inverse temperature (Ueda and Nakano, 1998; Smith and Eisner, 2006). As τ approaches infinity, the prior-annealed MAP objective maxβ P(φ|β)P(β)τ approaches maxβ P(φ|β) maxβ P(β). Annealing only the prior P(β) results in maximization of this term only, while the outer max chooses a good β under P(φ|β) as a tie-breaker among all β values that maximize the inner max (binary-valued β).3 We show experimentally (§7.2.2) that annealing the prior yields values that satisfy the constraints. 3Other modifications could be made to the objective function to induce sparsity, such as entropy regularization (Balasubramanyan and Cohen, 2013). ωcv ˜φkv X (1) 48 6 A Factored Hierarchical Model of Topic and Perspective We will now describe a SPRITE model that encompasses nearly all of the structures and extensions described in §3–4, followed by experimental results using this model to jointly capture topic and “perspective” in a corpus of political debates (where perspective corresponds to ideology) and a corpus of online doctor reviews (where perspective corresponds to the review sentiment). First, we will create a topic hierarchy (§4.4). The hierarchy will model both topics and documents, where αm is document m’s supertopic propor</context>
</contexts>
<marker>Balasubramanyan, Cohen, 2013</marker>
<rawString>R. Balasubramanyan and W. Cohen. 2013. Regularization of latent variable models to obtain sparsity. In SIAM Conference on Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>J Lafferty</author>
</authors>
<title>A correlated topic model of Science.</title>
<date>2007</date>
<journal>Annals ofApplied Statistics,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="1834" citStr="Blei and Lafferty, 2007" startWordPosition="272" endWordPosition="275">ese expectations (Mimno et al., 2011; Talley et al., 2011) yet black box topic models provide no control over the desired output. This paper presents SPRITE, a family of topic models that provide a flexible framework for encoding preferences as priors for how topics should be structured. SPRITE can incorporate many types of structure that have been considered in prior work, including hierarchies (Blei et al., 2003a; Mimno et al., 2007), factorizations (Paul and Dredze, 2012; Eisenstein et al., 2011), sparsity (Wang and Blei, 2009; Balasubramanyan and Cohen, 2013), correlations between topics (Blei and Lafferty, 2007; Li and McCallum, 2006), preferences over word choices (Andrzejewski et al., 2009; Paul and Dredze, 2013), and associations between topics and document attributes (Ramage et al., 2009; Mimno and McCallum, 2008). SPRITE builds on a standard topic model, adding structure to the priors over the model parameters. The priors are given by log-linear functions of underlying components (§2), which provide additional latent structure that we will show can enrich the model in many ways. By applying particular constraints and priors to the component hyperparameters, a variety of structures can be induce</context>
</contexts>
<marker>Blei, Lafferty, 2007</marker>
<rawString>D. Blei and J. Lafferty. 2007. A correlated topic model of Science. Annals ofApplied Statistics, 1(1):17–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>T Griffiths</author>
<author>M Jordan</author>
<author>J Tenenbaum</author>
</authors>
<title>Hierarchical topic models and the nested Chinese restaurant process.</title>
<date>2003</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="1628" citStr="Blei et al., 2003" startWordPosition="242" endWordPosition="245">manual supervision. Yet people often have expectations about topics in a given corpus and how they should be structured for a particular task. It is crucial for the user experience that topics meet these expectations (Mimno et al., 2011; Talley et al., 2011) yet black box topic models provide no control over the desired output. This paper presents SPRITE, a family of topic models that provide a flexible framework for encoding preferences as priors for how topics should be structured. SPRITE can incorporate many types of structure that have been considered in prior work, including hierarchies (Blei et al., 2003a; Mimno et al., 2007), factorizations (Paul and Dredze, 2012; Eisenstein et al., 2011), sparsity (Wang and Blei, 2009; Balasubramanyan and Cohen, 2013), correlations between topics (Blei and Lafferty, 2007; Li and McCallum, 2006), preferences over word choices (Andrzejewski et al., 2009; Paul and Dredze, 2013), and associations between topics and document attributes (Ramage et al., 2009; Mimno and McCallum, 2008). SPRITE builds on a standard topic model, adding structure to the priors over the model parameters. The priors are given by log-linear functions of underlying components (§2), which </context>
<context position="3112" citStr="Blei et al., 2003" startWordPosition="475" endWordPosition="478">ow that this framework captures many existing topic models (§4). After describing the general form of the model, we show how SPRITE can be tailored to particular settings by describing a specific model for the applied task of jointly inferring topic hierarchies and perspective (§6). We experiment with this topic+perspective model on sets of political debates and online reviews (§7), and demonstrate that SPRITE learns desired structures while outperforming many baselines at predictive tasks. 2 Topic Modeling with Structured Priors Our model family generalizes latent Dirichlet allocation (LDA) (Blei et al., 2003b). Under LDA, there are K topics, where a topic is a categorical distribution over V words parameterized by φk. Each document has a categorical distribution over topics, parameterized by 8,,,, for the mth document. Each observed word in a document is generated by drawing a topic z from 8,,,,, then drawing the word from φz. 8 and φ have priors given by Dirichlet distributions. Our generalization adds structure to the generation of the Dirichlet parameters. The priors for these parameters are modeled as log-linear combinations of underlying components. Components are real-valued vectors of leng</context>
<context position="6959" citStr="Blei et al., 2003" startWordPosition="1107" endWordPosition="1110">del that these topics will all have words in common. This is handled by the topic components – these three topics could all draw from a common “spoken lan• Generate hyperparameters: α, β, δ, ω (§3) • For each document m, generate parameters: θ) θmk = exp(E,1 αmc δck), 1&lt;k&lt;K 2. θm — Dirichlet(˜θm) • For each topic k, generate parameters: «) 1. ˜φkv = exp(Ec C(&amp;quot; βkc ωcv), 1&lt;v&lt;V 2. φk — Dirichlet( ˜φk) • For each token (m, n), generate data: 1. Topic (unobserved): zm,n — θm 2. Word (observed): wm,n — φzm,n Figure 1: The generative story of SPRITE. The difference from latent Dirichlet allocation (Blei et al., 2003b) is the generation of the Dirichlet parameters. guage” topic component, with high-weight words such as speech and spoken, which informs the prior of all three topics. Second, we can model that these topics are likely to occur together in documents. For example, articles about dialog systems are likely to discuss automatic speech recognition as a subroutine. This is handled by the document components – there could be a “spoken language” document component that gives high weight to all three topics, so that if a document draw its prior from this component, then it is more likely to give probab</context>
<context position="15570" citStr="Blei et al., 2003" startWordPosition="2549" endWordPosition="2552">Sparse w FLDA 4.3 Binary b is transpose of Q Factored binary Q PAM 4.4 α are supertopic weights Single component DMR 4.5 α are feature values Single component Table 1: Topic models with Dirichlet priors that are generalized by SPRITE. The description of each model can be found in the noted section number. PAM is not equivalent, but captures very similar behavior. The described component formulations of SCTM and SAGE are equivalent, but these differ from SPRITE in that the components directly define the parameters, rather than priors over the parameters. 4.1 Latent Dirichlet Allocation In LDA (Blei et al., 2003b), all 0 vectors are drawn from the same prior, as are all O vectors. This is a basic instance of our model with only one component at the topic and document levels, C(θ) = C(φ) = 1, with coefficients α = � = 1. 4.2 Shared Components Topic Models Shared components topic models (SCTM) (Gormley et al., 2010) define topics as products of “components”, where components are word distributions. To use the notation of our paper, the kth topic’s word distribution in SCTM is parameterized by Okv ∝ f1c wβkc cv , where the w vectors are word distributions (rather than vectors in RV ), and the �kc E {0, </context>
</contexts>
<marker>Blei, Griffiths, Jordan, Tenenbaum, 2003</marker>
<rawString>D. Blei, T. Griffiths, M. Jordan, and J. Tenenbaum. 2003a. Hierarchical topic models and the nested Chinese restaurant process. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>A Ng</author>
<author>M Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<publisher>JMLR.</publisher>
<contexts>
<context position="1628" citStr="Blei et al., 2003" startWordPosition="242" endWordPosition="245">manual supervision. Yet people often have expectations about topics in a given corpus and how they should be structured for a particular task. It is crucial for the user experience that topics meet these expectations (Mimno et al., 2011; Talley et al., 2011) yet black box topic models provide no control over the desired output. This paper presents SPRITE, a family of topic models that provide a flexible framework for encoding preferences as priors for how topics should be structured. SPRITE can incorporate many types of structure that have been considered in prior work, including hierarchies (Blei et al., 2003a; Mimno et al., 2007), factorizations (Paul and Dredze, 2012; Eisenstein et al., 2011), sparsity (Wang and Blei, 2009; Balasubramanyan and Cohen, 2013), correlations between topics (Blei and Lafferty, 2007; Li and McCallum, 2006), preferences over word choices (Andrzejewski et al., 2009; Paul and Dredze, 2013), and associations between topics and document attributes (Ramage et al., 2009; Mimno and McCallum, 2008). SPRITE builds on a standard topic model, adding structure to the priors over the model parameters. The priors are given by log-linear functions of underlying components (§2), which </context>
<context position="3112" citStr="Blei et al., 2003" startWordPosition="475" endWordPosition="478">ow that this framework captures many existing topic models (§4). After describing the general form of the model, we show how SPRITE can be tailored to particular settings by describing a specific model for the applied task of jointly inferring topic hierarchies and perspective (§6). We experiment with this topic+perspective model on sets of political debates and online reviews (§7), and demonstrate that SPRITE learns desired structures while outperforming many baselines at predictive tasks. 2 Topic Modeling with Structured Priors Our model family generalizes latent Dirichlet allocation (LDA) (Blei et al., 2003b). Under LDA, there are K topics, where a topic is a categorical distribution over V words parameterized by φk. Each document has a categorical distribution over topics, parameterized by 8,,,, for the mth document. Each observed word in a document is generated by drawing a topic z from 8,,,,, then drawing the word from φz. 8 and φ have priors given by Dirichlet distributions. Our generalization adds structure to the generation of the Dirichlet parameters. The priors for these parameters are modeled as log-linear combinations of underlying components. Components are real-valued vectors of leng</context>
<context position="6959" citStr="Blei et al., 2003" startWordPosition="1107" endWordPosition="1110">del that these topics will all have words in common. This is handled by the topic components – these three topics could all draw from a common “spoken lan• Generate hyperparameters: α, β, δ, ω (§3) • For each document m, generate parameters: θ) θmk = exp(E,1 αmc δck), 1&lt;k&lt;K 2. θm — Dirichlet(˜θm) • For each topic k, generate parameters: «) 1. ˜φkv = exp(Ec C(&amp;quot; βkc ωcv), 1&lt;v&lt;V 2. φk — Dirichlet( ˜φk) • For each token (m, n), generate data: 1. Topic (unobserved): zm,n — θm 2. Word (observed): wm,n — φzm,n Figure 1: The generative story of SPRITE. The difference from latent Dirichlet allocation (Blei et al., 2003b) is the generation of the Dirichlet parameters. guage” topic component, with high-weight words such as speech and spoken, which informs the prior of all three topics. Second, we can model that these topics are likely to occur together in documents. For example, articles about dialog systems are likely to discuss automatic speech recognition as a subroutine. This is handled by the document components – there could be a “spoken language” document component that gives high weight to all three topics, so that if a document draw its prior from this component, then it is more likely to give probab</context>
<context position="15570" citStr="Blei et al., 2003" startWordPosition="2549" endWordPosition="2552">Sparse w FLDA 4.3 Binary b is transpose of Q Factored binary Q PAM 4.4 α are supertopic weights Single component DMR 4.5 α are feature values Single component Table 1: Topic models with Dirichlet priors that are generalized by SPRITE. The description of each model can be found in the noted section number. PAM is not equivalent, but captures very similar behavior. The described component formulations of SCTM and SAGE are equivalent, but these differ from SPRITE in that the components directly define the parameters, rather than priors over the parameters. 4.1 Latent Dirichlet Allocation In LDA (Blei et al., 2003b), all 0 vectors are drawn from the same prior, as are all O vectors. This is a basic instance of our model with only one component at the topic and document levels, C(θ) = C(φ) = 1, with coefficients α = � = 1. 4.2 Shared Components Topic Models Shared components topic models (SCTM) (Gormley et al., 2010) define topics as products of “components”, where components are word distributions. To use the notation of our paper, the kth topic’s word distribution in SCTM is parameterized by Okv ∝ f1c wβkc cv , where the w vectors are word distributions (rather than vectors in RV ), and the �kc E {0, </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D. Blei, A. Ng, and M. Jordan. 2003b. Latent Dirichlet allocation. JMLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chang</author>
<author>J Boyd-Graber</author>
<author>S Gerrish</author>
<author>C Wang</author>
<author>D Blei</author>
</authors>
<title>Reading tea leaves: How humans interpret topic models.</title>
<date>2009</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="48709" citStr="Chang et al., 2009" startWordPosition="8074" endWordPosition="8077">r the DAG and tree experiments). The Abstracts experiments use the same parameters as with Debates. Since the Abstracts dataset does not have a perspective value to predict, we do not include prediction error as a metric, instead focusing on held-out perplexity and topic coherence (Eq. 2). Table 4 shows the results of these two metrics. Some trends are clear and consistent. Topic components always hurt perplexity, while these components typically improve coherence, as was observed in the previous subsection. It has previously been observed that perplexity and topic quality are not correlated (Chang et al., 2009). These results show that the choice of components depends on the task at hand. Combining the two components tends to produce results somewhere in between, suggesting that using both component types is a reasonable “default” setting. Document components usually improve perplexity, likely due to the nature of the document completion setup, in which half of each document is held out. The document components capture correlations between topics, so by inferring the components that generated the first half of the document, the prior is adjusted to give more probability to topics that are likely to </context>
</contexts>
<marker>Chang, Boyd-Graber, Gerrish, Wang, Blei, 2009</marker>
<rawString>J. Chang, J. Boyd-Graber, S. Gerrish, C. Wang, and D. Blei. 2009. Reading tea leaves: How humans interpret topic models. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Duchi</author>
<author>E Hazan</author>
<author>Y Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>JMLR,</journal>
<pages>12--2121</pages>
<contexts>
<context position="32159" citStr="Duchi et al., 2011" startWordPosition="5322" endWordPosition="5325">ered the ratings around the middle value 3, then took reviews that had the same sign for all aspects, and averaged the scores to produce a value for α(P). Our corpus contains 20,000 documents (476,991 tokens; 10,158 types), balanced across positive/negative scores. Unless otherwise specified, K=50 topics and C=10 components (excluding the perspective component) for Debates, and K=20 and C=5 for Reviews. These values were chosen as a qualitative preference, not optimized for predictive performance, but we experiment with different values in §7.2.2. We set the step size qt according to AdaGrad (Duchi et al., 2011), where the step size is the inverse of the sum of squared historical gradients.4 We place a sparse Dirichlet(p=0.01) prior on the b variables, and apply weak regularization to all other hyperparameters via a JV(0,102) prior. These hyperparameters were chosen after only minimal tuning, and were selected because they showed stable and reasonable output qualitatively during preliminary development. We ran our inference algorithm for 5000 iterations, estimating the parameters 0 and 0 by averaging the final 100 iterations. Our results are averaged across 10 randomly initialized samplers.5 7.2 Eval</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. JMLR, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisenstein</author>
<author>A Ahmed</author>
<author>E P Xing</author>
</authors>
<title>Sparse additive generative models of text.</title>
<date>2011</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="1715" citStr="Eisenstein et al., 2011" startWordPosition="255" endWordPosition="258">orpus and how they should be structured for a particular task. It is crucial for the user experience that topics meet these expectations (Mimno et al., 2011; Talley et al., 2011) yet black box topic models provide no control over the desired output. This paper presents SPRITE, a family of topic models that provide a flexible framework for encoding preferences as priors for how topics should be structured. SPRITE can incorporate many types of structure that have been considered in prior work, including hierarchies (Blei et al., 2003a; Mimno et al., 2007), factorizations (Paul and Dredze, 2012; Eisenstein et al., 2011), sparsity (Wang and Blei, 2009; Balasubramanyan and Cohen, 2013), correlations between topics (Blei and Lafferty, 2007; Li and McCallum, 2006), preferences over word choices (Andrzejewski et al., 2009; Paul and Dredze, 2013), and associations between topics and document attributes (Ramage et al., 2009; Mimno and McCallum, 2008). SPRITE builds on a standard topic model, adding structure to the priors over the model parameters. The priors are given by log-linear functions of underlying components (§2), which provide additional latent structure that we will show can enrich the model in many ways</context>
<context position="11611" citStr="Eisenstein et al., 2011" startWordPosition="1888" endWordPosition="1891">this approach in our model of topic and perspective (§6). 3.1.3 Factored Forest By using structured sparsity over the DAG, we can obtain a structure where components are grouped into G factors, and each document or topic has one parent from each group. Figure 2(d) illustrates this: the left three components belong to one group, the right two belong to another, and each bottom node has exactly one parent from each. This is a DAG that we call a “factored forest” because the subgraphs associated with each group in isolation are trees. This structure arises in “multidimensional” models like SAGE (Eisenstein et al., 2011) and Factorial LDA (Paul and Dredze, 2012), which allow tokens to be associated with multiple variables (e.g. a topic along with a variable denoting positive or negative sentiment). This allows word distributions to depend on both factors. The “exactly one parent” indicator constraint is the same as in the tree structure but enforces a tree only within each group. This can therefore be (softly) modeled using a sparse Dirichlet prior as described in the previous subsection. In this case, the subsets of components belonging to each factor have separate sparse Dirichlet priors. Using the example </context>
<context position="19480" citStr="Eisenstein et al., 2011" startWordPosition="3217" endWordPosition="3220">ver the original FLDA model. FLDA assumes that the entire Cartesian product of the different factors is represented in the model (e.g. φ parameters for every possible tuple), which leads to issues with efficiency and overparameterization with higher numbers of factors. With SPRITE, we can simply fix the number of “topics” to a number smaller than the size of the Cartesian product, and the model will learn which subset of tuples are included, through the values of β and δ. Finally, another existing model family that allows for topic factorization is the sparse additive generative model (SAGE) (Eisenstein et al., 2011). SAGE uses a log-linear parameterization to define word distributions. SAGE is a general family of models that need not be factored, but is presented as an efficient solution for including multiple factors, such as topic and geography or topic and author ideology. Like SCTM, φ is exactly defined as a product of ω weights, rather than our approach of using the product to define a prior over φ. 4.4 Topic Hierarchies and Correlations While the two previous subsections primarily focused on word distributions (with FLDA being an exception that focused on both), SPRITE’s priors over topic distribut</context>
</contexts>
<marker>Eisenstein, Ahmed, Xing, 2011</marker>
<rawString>J. Eisenstein, A. Ahmed, and E. P. Xing. 2011. Sparse additive generative models of text. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Gormley</author>
<author>M Dredze</author>
<author>B Van Durme</author>
<author>J Eisner</author>
</authors>
<title>Shared components topic models.</title>
<date>2010</date>
<booktitle>In NAACL.</booktitle>
<marker>Gormley, Dredze, Van Durme, Eisner, 2010</marker>
<rawString>M.R. Gormley, M. Dredze, B. Van Durme, and J. Eisner. 2010. Shared components topic models. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>In Proceedings of the National Academy of Sciences of the United States of America.</booktitle>
<contexts>
<context position="23992" citStr="Griffiths and Steyvers, 2004" startWordPosition="3944" endWordPosition="3947">in SPRITE. 5 Inference and Parameter Estimation We now discuss how to infer the posterior of the latent variables z and parameters θ and φ, and find maximum a posteriori (MAP) estimates of the hyperparameters α, β, δ, and ω, given their hyperpriors. We take a Monte Carlo EM approach, using a collapsed Gibbs sampler to sample from the posterior of the topic assignments z conditioned on the hyperparameters, then optimizing the hyperparameters using gradient-based optimization conditioned on the samples. Given the hyperparameters, the sampling equations are identical to the standard LDA sampler (Griffiths and Steyvers, 2004). The partial derivative of the collapsed log likelihood L of the corpus with respect to each hyperparameter βkc is: ∂L ∂P(β) + ∂βkc = ∂βkc v (Ψ(nkv+˜φkv) −Ψ(˜φkv) +Ψ(Ekl˜φklv) −Ψ(Eklnvl+˜φklv)) where ˜φkv=exp(Ec, βkc,ωc,v), nkv is the number of times word v is assigned to topic k (in the samples from the E-step), and T is the digamma function, the derivative of the log of the gamma function. The digamma terms arise from the Dirichlet-multinomial distribution, when integrating out the parameters φ. P(β) is the hyperprior. For a 0-mean Gaussian hyperprior with variance σ2, ∂P (β) ∂βkc = −βkc σ2</context>
<context position="39788" citStr="Griffiths and Steyvers (2004)" startWordPosition="6557" endWordPosition="6560"> than given as input. This is a FLDA-style model that has similar behavior to FLDA (§4.3). We also compared to the original FLDA model. • The “perspective only” model but without the ω(P) topic component, so the attribute value affects only the topic distributions and not the word distributions. This is identical to the DMR model of Mimno and McCallum (2008) (§4.5). • A model with no components except for the bias vectors ω(B) and δ(B). This is equivalent to LDA with optimized hyperparameters (learned). We also experimented with using fixed symmetric hyperparameters, using values suggested by Griffiths and Steyvers (2004): 50/K and 0.01 for topic and word distributions. To put the results in context, we also compare to two types of baselines: (1) “bag of words” baselines, where we measure the perplexity of add-one smoothed unigram language models, we measure 1 K E k=1 M E m=2 m−1� l=1 K 51 Perspective best love years caring children really wonderful hes great family comfortable listens thank amazing É É went pay later staff asked money company refused pain office didn’t said told doctor Debates Reviews Model Perplexity Prediction error Coherence Perplexity Prediction error Coherence Full model †1555.5 f 2.3 †0</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>T. Griffiths and M. Steyvers. 2004. Finding scientific topics. In Proceedings of the National Academy of Sciences of the United States of America.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Hu</author>
<author>J Boyd-Graber</author>
<author>B Satinoff</author>
<author>A Smith</author>
</authors>
<title>Interactive topic modeling.</title>
<date>2013</date>
<booktitle>Machine Learning,</booktitle>
<pages>95--423</pages>
<contexts>
<context position="56457" citStr="Hu et al., 2013" startWordPosition="9325" endWordPosition="9328">nts (with β), the document features also affect the prior over word distributions. To the best of our knowledge, this is the first topic model to condition both topic and word distributions on the same features. The topic aspect model (Paul and Girju, 2010a) is also a two-dimensional factored model that has been used to jointly model topic and perspective (Paul and Girju, 2010b). However, this model does not use structured priors over the parameters, unlike most of the models discussed in §4. An alternative approach to incorporating user preferences and expertise are interactive topic models (Hu et al., 2013), a complimentary approach to SPRITE. 9 Discussion and Conclusion We have presented SPRITE, a family of topic models that utilize structured priors to induce preferred topic structures. Specific instantiations of SPRITE are similar or equivalent to several existing topic models. We demonstrated the utility of SPRITE by constructing a single model with many different characteristics, including a topic hierarchy, a factorization of topic and perspective, and supervision in the form of document attributes. These structures were incorporated into the priors of both the word and topic distributions</context>
</contexts>
<marker>Hu, Boyd-Graber, Satinoff, Smith, 2013</marker>
<rawString>Y. Hu, J. Boyd-Graber, B. Satinoff, and A. Smith. 2013. Interactive topic modeling. Machine Learning, 95:423–469.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kivinen</author>
<author>M K Warmuth</author>
</authors>
<title>Exponentiated gradient versus gradient descent for linear predictors.</title>
<date>1997</date>
<journal>Information and Computation,</journal>
<pages>132--1</pages>
<contexts>
<context position="25318" citStr="Kivinen and Warmuth, 1997" startWordPosition="4168" endWordPosition="4171">βkc = ρ−1 βkc . The partial derivatives for the other hyperparameters are similar. Rather than involving a sum over the vocabulary, ∂L ∂δck sums over documents, while ∂L ∂ωcv and ∂L ∂αmc sum over topics. Our inference algorithm alternates between one Gibbs iteration and one iteration of gradient ascent, so that the parameters change gradually. For unconstrained parameters, we use the update rule: xt+1=xt + ηt∇L(xt), for some variable x and a step size ηt at iteration t. For parameters constrained to the simplex (such as when β is a soft indicator vector), we use exponentiated gradient ascent (Kivinen and Warmuth, 1997) with the update rule: xt+1 i a xti exp(ηt∇iL(xt)). 5.1 Tightening the Constraints For variables that we prefer to be binary but have softened to continuous variables using sparse Beta or Dirichlet priors, we can straightforwardly strengthen the preference to be binary by modifying the objective function to favor the prior more heavily. Specifically, under a Dirichlet(ρ&lt;1) prior we will introduce a scaling parameter τt ≥ 1 to the prior log likelihood: τt log P(β) with partial derivative τt ρ−1 βkc , which adds extra weight to the sparse Dirichlet prior in the objective. The algorithm used in o</context>
</contexts>
<marker>Kivinen, Warmuth, 1997</marker>
<rawString>J. Kivinen and M.K. Warmuth. 1997. Exponentiated gradient versus gradient descent for linear predictors. Information and Computation, 132:1–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Lewis</author>
<author>K T Poole</author>
</authors>
<title>Measuring bias and uncertainty in ideal point estimates via the parametric bootstrap. Political Analysis,</title>
<date>2004</date>
<contexts>
<context position="31019" citStr="Lewis and Poole, 2004" startWordPosition="5138" endWordPosition="5141">he model. This includes most of the features described above (trees, factored structures, tying topic and document components, and document attributes), so we can ablate model features to measure their effect. 7 Experiments 7.1 Datasets and Experimental Setup We applied our models to two corpora: • Debates: A set of floor debates from the 109th– 112th U.S. Congress, collected by Nguyen et al. (2013), who also applied a hierarchical topic model to this data. Each document is a transcript of one speaker’s turn in a debate, and each document includes the first dimension of the DW-NOMINATE score (Lewis and Poole, 2004), a real-valued score indicating how conservative (positive) or liberal (negative) the speaker is. This value is α(P). We took a sample of 5,000 documents from the House debates (850,374 tokens; 7,426 types), balanced across party affilia49 tion. We sampled from the most partisan speakers, removing scores below the median value. • Reviews: Doctor reviews from RateMDs.com, previously analyzed using FLDA (Paul et al., 2013; Wallace et al., 2014). The reviews contain ratings on a 1–5 scale for multiple aspects. We centered the ratings around the middle value 3, then took reviews that had the same</context>
</contexts>
<marker>Lewis, Poole, 2004</marker>
<rawString>J.B. Lewis and K.T. Poole. 2004. Measuring bias and uncertainty in ideal point estimates via the parametric bootstrap. Political Analysis, 12(2):105–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Li</author>
<author>A McCallum</author>
</authors>
<title>Pachinko allocation: DAG-structured mixture models of topic correlations.</title>
<date>2006</date>
<booktitle>In International Conference on Machine Learning.</booktitle>
<contexts>
<context position="1858" citStr="Li and McCallum, 2006" startWordPosition="276" endWordPosition="279">t al., 2011; Talley et al., 2011) yet black box topic models provide no control over the desired output. This paper presents SPRITE, a family of topic models that provide a flexible framework for encoding preferences as priors for how topics should be structured. SPRITE can incorporate many types of structure that have been considered in prior work, including hierarchies (Blei et al., 2003a; Mimno et al., 2007), factorizations (Paul and Dredze, 2012; Eisenstein et al., 2011), sparsity (Wang and Blei, 2009; Balasubramanyan and Cohen, 2013), correlations between topics (Blei and Lafferty, 2007; Li and McCallum, 2006), preferences over word choices (Andrzejewski et al., 2009; Paul and Dredze, 2013), and associations between topics and document attributes (Ramage et al., 2009; Mimno and McCallum, 2008). SPRITE builds on a standard topic model, adding structure to the priors over the model parameters. The priors are given by log-linear functions of underlying components (§2), which provide additional latent structure that we will show can enrich the model in many ways. By applying particular constraints and priors to the component hyperparameters, a variety of structures can be induced such as hierarchies an</context>
<context position="20493" citStr="Li and McCallum, 2006" startWordPosition="3383" endWordPosition="3386">er φ. 4.4 Topic Hierarchies and Correlations While the two previous subsections primarily focused on word distributions (with FLDA being an exception that focused on both), SPRITE’s priors over topic distributions also have useful characteristics. The component-specific δ vectors can be interpreted as common topic distribution patterns, where each component is likely to give high weight to groups of topics that tend to occur together. Each document’s α weights encode which of the topic groups are present in that document. Similar properties are captured by the Pachinko allocation model (PAM) (Li and McCallum, 2006). Under PAM, each document has a distribution over supertopics. Each supertopic is associated with a Dirichlet prior over subtopic distributions, where subtopics are the low level topics that are associated with word parameters φ. Documents also have supertopic-specific distributions over subtopics (drawn from each supertopicspecific Dirichlet prior). Each topic in a document is drawn by first drawing a supertopic from the document’s distribution, then drawing a subtopic from that supertopic’s document distribution. While not equivalent, this is quite similar to SPRITE where document component</context>
</contexts>
<marker>Li, McCallum, 2006</marker>
<rawString>W. Li and A. McCallum. 2006. Pachinko allocation: DAG-structured mixture models of topic correlations. In International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mimno</author>
<author>A McCallum</author>
</authors>
<title>Topic models conditioned on arbitrary features with Dirichletmultinomial regression.</title>
<date>2008</date>
<booktitle>In UAI.</booktitle>
<contexts>
<context position="2045" citStr="Mimno and McCallum, 2008" startWordPosition="304" endWordPosition="307">amework for encoding preferences as priors for how topics should be structured. SPRITE can incorporate many types of structure that have been considered in prior work, including hierarchies (Blei et al., 2003a; Mimno et al., 2007), factorizations (Paul and Dredze, 2012; Eisenstein et al., 2011), sparsity (Wang and Blei, 2009; Balasubramanyan and Cohen, 2013), correlations between topics (Blei and Lafferty, 2007; Li and McCallum, 2006), preferences over word choices (Andrzejewski et al., 2009; Paul and Dredze, 2013), and associations between topics and document attributes (Ramage et al., 2009; Mimno and McCallum, 2008). SPRITE builds on a standard topic model, adding structure to the priors over the model parameters. The priors are given by log-linear functions of underlying components (§2), which provide additional latent structure that we will show can enrich the model in many ways. By applying particular constraints and priors to the component hyperparameters, a variety of structures can be induced such as hierarchies and factorizations (§3), and we will show that this framework captures many existing topic models (§4). After describing the general form of the model, we show how SPRITE can be tailored to</context>
<context position="23116" citStr="Mimno and McCallum, 2008" startWordPosition="3804" endWordPosition="3807">do this, let the number of document components be the number of features, and the value of αmc is the mth document’s value of the cth feature. The δ vectors then influence the document’s topic prior based on the feature values. For example, increasing αmc will increase the prior for topic z if δcz is positive and decrease the prior if δcz is negative. This is similar to the structure used for PAM (§4.4), but here the α weights are fixed and provided as input, rather than learned and interpreted as supertopic weights. This is identical to the Dirichlet-multinomial regression (DMR) topic model (Mimno and McCallum, 2008). The DMR topic model define’s each document’s Dirichlet prior over topics as a log-linear function of the document’s feature values and regression coefficients for each topic. The cth feature’s regression coefficients correspond to the δc vector in SPRITE. 5 Inference and Parameter Estimation We now discuss how to infer the posterior of the latent variables z and parameters θ and φ, and find maximum a posteriori (MAP) estimates of the hyperparameters α, β, δ, and ω, given their hyperpriors. We take a Monte Carlo EM approach, using a collapsed Gibbs sampler to sample from the posterior of the </context>
<context position="39519" citStr="Mimno and McCallum (2008)" startWordPosition="6515" endWordPosition="6518"> (§4.4). We also compared to the original PAM model. • The “hierarchy only” model using only topic components ω and no document components. This is a SCTM-style model because it exhibits similar behavior to SCTM (§4.2). • The full model where α(P) is learned rather than given as input. This is a FLDA-style model that has similar behavior to FLDA (§4.3). We also compared to the original FLDA model. • The “perspective only” model but without the ω(P) topic component, so the attribute value affects only the topic distributions and not the word distributions. This is identical to the DMR model of Mimno and McCallum (2008) (§4.5). • A model with no components except for the bias vectors ω(B) and δ(B). This is equivalent to LDA with optimized hyperparameters (learned). We also experimented with using fixed symmetric hyperparameters, using values suggested by Griffiths and Steyvers (2004): 50/K and 0.01 for topic and word distributions. To put the results in context, we also compare to two types of baselines: (1) “bag of words” baselines, where we measure the perplexity of add-one smoothed unigram language models, we measure 1 K E k=1 M E m=2 m−1� l=1 K 51 Perspective best love years caring children really wonder</context>
</contexts>
<marker>Mimno, McCallum, 2008</marker>
<rawString>D. Mimno and A. McCallum. 2008. Topic models conditioned on arbitrary features with Dirichletmultinomial regression. In UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mimno</author>
<author>W Li</author>
<author>A McCallum</author>
</authors>
<title>Mixtures of hierarchical topics with Pachinko allocation.</title>
<date>2007</date>
<booktitle>In International Conference on Machine Learning.</booktitle>
<contexts>
<context position="1650" citStr="Mimno et al., 2007" startWordPosition="246" endWordPosition="249">Yet people often have expectations about topics in a given corpus and how they should be structured for a particular task. It is crucial for the user experience that topics meet these expectations (Mimno et al., 2011; Talley et al., 2011) yet black box topic models provide no control over the desired output. This paper presents SPRITE, a family of topic models that provide a flexible framework for encoding preferences as priors for how topics should be structured. SPRITE can incorporate many types of structure that have been considered in prior work, including hierarchies (Blei et al., 2003a; Mimno et al., 2007), factorizations (Paul and Dredze, 2012; Eisenstein et al., 2011), sparsity (Wang and Blei, 2009; Balasubramanyan and Cohen, 2013), correlations between topics (Blei and Lafferty, 2007; Li and McCallum, 2006), preferences over word choices (Andrzejewski et al., 2009; Paul and Dredze, 2013), and associations between topics and document attributes (Ramage et al., 2009; Mimno and McCallum, 2008). SPRITE builds on a standard topic model, adding structure to the priors over the model parameters. The priors are given by log-linear functions of underlying components (§2), which provide additional lat</context>
<context position="22019" citStr="Mimno et al. (2007)" startWordPosition="3620" endWordPosition="3623"> α. The SPRITE formulation naturally allows for powerful extensions to PAM. One possibility is to include topic components for the word distributions, in addition to document components, and to tie together δcz and βzc (§3.2). This models the intuitive characteristic that subtopics belonging to similar supertopics (encoded by δ) should come from similar priors over their word distributions (since they will have similar β values). That is, children of a supertopic are topically related – they are likely to share words. This is a richer alternative to the hierarchical variant of PAM proposed by Mimno et al. (2007), which modeled separate word distributions for supertopics and subtopics, but the subtopics were not dependent on the super47 topic word distributions. Another extension is to form a strict tree structure, making each subtopic belong to exactly one supertopic: a true hierarchy. 4.5 Conditioning on Document Attributes SPRITE also naturally provides the ability to condition document topic distributions on features of the document, such as a user rating in a review. To do this, let the number of document components be the number of features, and the value of αmc is the mth document’s value of th</context>
</contexts>
<marker>Mimno, Li, McCallum, 2007</marker>
<rawString>D. Mimno, W. Li, and A. McCallum. 2007. Mixtures of hierarchical topics with Pachinko allocation. In International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mimno</author>
<author>H M Wallach</author>
<author>E Talley</author>
<author>M Leenders</author>
<author>A McCallum</author>
</authors>
<title>Optimizing semantic coherence in topic models.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1247" citStr="Mimno et al., 2011" startWordPosition="180" endWordPosition="183">l to jointly infer topic hierarchies and author perspective, which we apply to corpora of political debates and online reviews. We show that the model learns intuitive topics, outperforming several other topic models at predictive tasks. 1 Introduction Topic models can be a powerful aid for analyzing large collections of text by uncovering latent interpretable structures without manual supervision. Yet people often have expectations about topics in a given corpus and how they should be structured for a particular task. It is crucial for the user experience that topics meet these expectations (Mimno et al., 2011; Talley et al., 2011) yet black box topic models provide no control over the desired output. This paper presents SPRITE, a family of topic models that provide a flexible framework for encoding preferences as priors for how topics should be structured. SPRITE can incorporate many types of structure that have been considered in prior work, including hierarchies (Blei et al., 2003a; Mimno et al., 2007), factorizations (Paul and Dredze, 2012; Eisenstein et al., 2011), sparsity (Wang and Blei, 2009; Balasubramanyan and Cohen, 2013), correlations between topics (Blei and Lafferty, 2007; Li and McCa</context>
<context position="35753" citStr="Mimno et al. (2011)" startWordPosition="5891" endWordPosition="5894">nt topic distributions 0 as features. We held out half of the documents for testing and measured the mean absolute error. When estimating document-specific SPRITE parameters for held-out documents, we fix the feature value α(P) m= 0 for that document. These predictive experiments do not directly measure performance at many of the particular tasks that topic models are well suited for, like data exploration, summarization, and visualization. We therefore also include a metric that more directly measures the quality and interpretability of topics. We use the topic coherence metric introduced by Mimno et al. (2011), which is based on co-occurrence statistics among each topic’s most probable words and has been shown to correlate with human judgments of topic quality. This metric measures the quality of each topic, and we 50 pain surgery dr went knee foot neck mri injury shoulder bone months told surgeon therapy told hospital dr blood went later days mother said er cancer weight home father months dentist teeth dental work tooth root mouth pain dentists went filling canal dr crown cleaning “Surgery” surgery pain went dr surgeon told procedure months performed removed left fix said later years dr life than</context>
</contexts>
<marker>Mimno, Wallach, Talley, Leenders, McCallum, 2011</marker>
<rawString>D. Mimno, H.M. Wallach, E. Talley, M. Leenders, and A. McCallum. 2011. Optimizing semantic coherence in topic models. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Nguyen</author>
<author>J Boyd-Graber</author>
<author>P Resnik</author>
</authors>
<title>Lexical and hierarchical topic regression.</title>
<date>2013</date>
<booktitle>In Neural Information Processing Systems.</booktitle>
<contexts>
<context position="30799" citStr="Nguyen et al. (2013)" startWordPosition="5100" endWordPosition="5103">rs denoted ω(B) and δ(B), which act as overall weights over the vocabulary and topics, so that the component-specific ω and δ weights can be interpreted as deviations from the global bias weights. Figure 3 summarizes the model. This includes most of the features described above (trees, factored structures, tying topic and document components, and document attributes), so we can ablate model features to measure their effect. 7 Experiments 7.1 Datasets and Experimental Setup We applied our models to two corpora: • Debates: A set of floor debates from the 109th– 112th U.S. Congress, collected by Nguyen et al. (2013), who also applied a hierarchical topic model to this data. Each document is a transcript of one speaker’s turn in a debate, and each document includes the first dimension of the DW-NOMINATE score (Lewis and Poole, 2004), a real-valued score indicating how conservative (positive) or liberal (negative) the speaker is. This value is α(P). We took a sample of 5,000 documents from the House debates (850,374 tokens; 7,426 types), balanced across party affilia49 tion. We sampled from the most partisan speakers, removing scores below the median value. • Reviews: Doctor reviews from RateMDs.com, previ</context>
<context position="53292" citStr="Nguyen et al., 2013" startWordPosition="8857" endWordPosition="8860">a more specialized model. SPRITE is also much more flexible than FLDA. While FLDA strictly models a one-to-one mapping of topics to each pair of components, SPRITE allows multiple topics to belong to the same pair (as in the semantics examples above), and conversely SPRITE does not require that all pairs have an associated topic. This property allows SPRITE to scale to larger numbers of factors than FLDA, because the number of topics is not required to grow with the number of all possible tuples. 8 Related Work Our topic and perspective model is related to supervised hierarchical LDA (SHLDA) (Nguyen et al., 2013), which learns a topic hierarchy while also learning regression parameters to associate topics with feature values such as political perspective. This model does not explicitly incorporate perspective-specific word priors into the topics (as in our factorized approach). The regression structure is also different. SHLDA is a “downstream” model, where the perspective value is a response variable conditioned on the topics. In contrast, SPRITE is an “upstream” model, where the topics are conditioned on the perspective value. We argue that the latter is more accurate as a generative story (the emit</context>
</contexts>
<marker>Nguyen, Boyd-Graber, Resnik, 2013</marker>
<rawString>V. Nguyen, J. Boyd-Graber, and P. Resnik. 2013. Lexical and hierarchical topic regression. In Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Paul</author>
<author>M Dredze</author>
</authors>
<title>Factorial LDA: Sparse multi-dimensional text models.</title>
<date>2012</date>
<booktitle>In Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="1689" citStr="Paul and Dredze, 2012" startWordPosition="251" endWordPosition="254">out topics in a given corpus and how they should be structured for a particular task. It is crucial for the user experience that topics meet these expectations (Mimno et al., 2011; Talley et al., 2011) yet black box topic models provide no control over the desired output. This paper presents SPRITE, a family of topic models that provide a flexible framework for encoding preferences as priors for how topics should be structured. SPRITE can incorporate many types of structure that have been considered in prior work, including hierarchies (Blei et al., 2003a; Mimno et al., 2007), factorizations (Paul and Dredze, 2012; Eisenstein et al., 2011), sparsity (Wang and Blei, 2009; Balasubramanyan and Cohen, 2013), correlations between topics (Blei and Lafferty, 2007; Li and McCallum, 2006), preferences over word choices (Andrzejewski et al., 2009; Paul and Dredze, 2013), and associations between topics and document attributes (Ramage et al., 2009; Mimno and McCallum, 2008). SPRITE builds on a standard topic model, adding structure to the priors over the model parameters. The priors are given by log-linear functions of underlying components (§2), which provide additional latent structure that we will show can enr</context>
<context position="10650" citStr="Paul and Dredze (2012)" startWordPosition="1725" endWordPosition="1728"> αm and �k are indicator vectors which select a single component. In this study, rather than strictly requiring αm and �k to be binary-valued indicator vectors, we create a relaxation that allows for easier parameter estimation. We let αm and �k to real-valued variables in a simplex, but place a prior over their values to encourage sparse values, favoring vectors with a single component near 1 and others near 0. This is achieved using a Dirichlet(p &lt; 1) distribution as the prior over α and �, which has higher density near the boundaries of the simplex.1 1This generalizes the technique used in Paul and Dredze (2012), who approximated binary variables with real-valued variables in (0, 1), by using a “U-shaped” Beta(p &lt; 1) distriFor a weighted tree, α and � could be a product of two variables: an “integer-like” indicator vector with sparse Dirichlet prior as suggested above, combined with a real-valued weight (e.g., with a Gaussian prior). We take this approach in our model of topic and perspective (§6). 3.1.3 Factored Forest By using structured sparsity over the DAG, we can obtain a structure where components are grouped into G factors, and each document or topic has one parent from each group. Figure 2(d</context>
<context position="17096" citStr="Paul and Dredze, 2012" startWordPosition="2812" endWordPosition="2815">PRITE, it is only the prior that is a product of components.2 Another difference is that SCTM has an unweighted product of components (� is binary), whereas SPRITE allows for weighted products. The log-linear parameterization leads to simpler optimization procedures than the product parameterization. Finally, the components in SCTM only apply to the word distributions, and not the topic distributions in documents. 4.3 Factored Topic Models Factored topic models combine multiple aspects of the text to generate the document (instead of just topics). One such topic model is Factorial LDA (FLDA) (Paul and Dredze, 2012). In FLDA, 2The posterior becomes concentrated around the prior when the Dirichlet variance is low, in which case SPRITE behaves like SCTM. SPRITE is therefore more general. 46 “topics” are actually tuples of potentially multiple variables, such as aspect and sentiment in online reviews (Paul et al., 2013). Each document distribution θm is a distribution over pairs (or higherdimensional tuples if there are more than two factors), and each pair (j, k) has a word distribution φ(j,k). FLDA uses a similar log-linear parameterization of the Dirichlet priors as SPRITE. Using our notation, the Dirich</context>
<context position="43300" citStr="Paul and Dredze (2012)" startWordPosition="7169" endWordPosition="7172">perplexity on both datasets. However, models with both topic and document components generally outperform either alone, including comparing the Perspective only and DMR models. The former includes both topic and document perspective components, while DMR has only a document level component. PAM does not significantly outperform optimized LDA in most measures, likely because it updates the hyperparameters using a moment-based approximation, which is less accurate than our gradient-based optimization. FLDA perplexity is 2.3% higher than optimized LDA on Reviews, comparable to the 4% reported by Paul and Dredze (2012) on a different corpus. The FLDA-style SPRITE variant, which is more flexible, significantly outperforms FLDA in most measures. The results are quite different under the coherence metric. It seems that topic components (which influence the word distributions) improve coherence over LDA, while document components worsen coherence. SCTM-style (which uses only topic components) does the best in both datasets, while PAM-style (which uses only documents) does the worst. PAM also significantly improves over LDA, despite worse perplexity. The LDA (learned) baseline substantially outperforms LDA (fixe</context>
<context position="47457" citStr="Paul and Dredze, 2012" startWordPosition="7856" endWordPosition="7859">mparison of each structure in isolation, since most model variants were part of a complex joint model. In this section, we experiment with the basic SPRITE model for the three structures described in §3: a DAG, a tree, and a factored forest. For each structure, we also experiment with each type of component: document, topic, and both types (combined). For this set of experiments, we included a third dataset that does not contain a perspective value: • Abstracts: A set of 957 abstracts from the ACL anthology (97,168 tokens; 8,246 types). These abstracts have previously been analyzed with FLDA (Paul and Dredze, 2012), so we include it here to see if the factored structure that we explore in this section learns similar patterns. Based on our sparsity experiments in the previous subsection, we set Tt = 1.003t to induce hard structures (tree and factored) and T = 0 to induce a DAG. We keep the same parameters as the previous subsection: K=50 and C=10 for Debates and K=20 and C=5 for Reviews. For the factored structures, we use two factors, with one factor having more components than the other: 3 and 7 components for Debates, and 2 and 3 components for Reviews (the total number of components across the two fa</context>
<context position="52396" citStr="Paul and Dredze (2012)" startWordPosition="8707" endWordPosition="8710"> words including {segmentation, chinese, morphology}) and information retrieval (top words including {documents, retrieval, ir}). Many of the topics intuitively follow from the components of these two factors. For example, the two topics expressing vector space models and distributional semantics (top left and right) both draw from the “computational” and “semantics” components, while the topics expressing ontologies and question answering (middle left and right) draw from “linguistics” and “semantics”. The factorization is similar to what had been previously been induced by FLDA. Figure 3 of Paul and Dredze (2012) shows components that look similar to the computational methods and linguistic theory components here, and the factor with the largest number of components also decomposes by research topic. These results show that SPRITE is capable of recovering similar structures as FLDA, a more specialized model. SPRITE is also much more flexible than FLDA. While FLDA strictly models a one-to-one mapping of topics to each pair of components, SPRITE allows multiple topics to belong to the same pair (as in the semantics examples above), and conversely SPRITE does not require that all pairs have an associated</context>
</contexts>
<marker>Paul, Dredze, 2012</marker>
<rawString>M.J. Paul and M. Dredze. 2012. Factorial LDA: Sparse multi-dimensional text models. In Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Paul</author>
<author>M Dredze</author>
</authors>
<title>Drug extraction from the web: Summarizing drug experiences with multidimensional topic models.</title>
<date>2013</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="1940" citStr="Paul and Dredze, 2013" startWordPosition="289" endWordPosition="292">er the desired output. This paper presents SPRITE, a family of topic models that provide a flexible framework for encoding preferences as priors for how topics should be structured. SPRITE can incorporate many types of structure that have been considered in prior work, including hierarchies (Blei et al., 2003a; Mimno et al., 2007), factorizations (Paul and Dredze, 2012; Eisenstein et al., 2011), sparsity (Wang and Blei, 2009; Balasubramanyan and Cohen, 2013), correlations between topics (Blei and Lafferty, 2007; Li and McCallum, 2006), preferences over word choices (Andrzejewski et al., 2009; Paul and Dredze, 2013), and associations between topics and document attributes (Ramage et al., 2009; Mimno and McCallum, 2008). SPRITE builds on a standard topic model, adding structure to the priors over the model parameters. The priors are given by log-linear functions of underlying components (§2), which provide additional latent structure that we will show can enrich the model in many ways. By applying particular constraints and priors to the component hyperparameters, a variety of structures can be induced such as hierarchies and factorizations (§3), and we will show that this framework captures many existing</context>
<context position="14327" citStr="Paul and Dredze (2013)" startWordPosition="2343" endWordPosition="2346">ables (suggested in §3.1.2): a binary mask variable (with sparse Dirichlet prior), which we let be identical for both 8 and �, and a real-valued positive weight. 3.3 Deep Components As for priors over the component weights 8 and w, we assume they are generated from a 0-mean Gaussian. While not experimented with in this study, it is also possible to allow the components themselves to have rich priors which are functions of higher level components. For example, rather than assuming a mean of zero, the mean could be a weighted combination of higher level weight vectors. This approach was used by Paul and Dredze (2013) in Factorial LDA, in which each w component had its own Gaussian prior provided as input to guide the parameters. 4 Special Cases and Extensions We now describe several existing Dirichlet prior topic models and show how they are special cases of SPRITE. Table 1 summarizes these models and their relation to SPRITE. In almost every case, we also describe how the SPRITE representation of the model offers improvements over the original model or can lead to novel extensions. Model Sec. Document priors Topic priors LDA 4.1 Single component Single component SCTM 4.2 Single component Sparse binary Q </context>
</contexts>
<marker>Paul, Dredze, 2013</marker>
<rawString>M.J. Paul and M. Dredze. 2013. Drug extraction from the web: Summarizing drug experiences with multidimensional topic models. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Paul</author>
<author>R Girju</author>
</authors>
<title>A two-dimensional topic-aspect model for discovering multi-faceted topics.</title>
<date>2010</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="56097" citStr="Paul and Girju, 2010" startWordPosition="9269" endWordPosition="9272">ecific word distributions. This is an alternative to the more common approach to regression based topic modeling, where the variables affect the topic distributions rather than the word distributions. Our SPRITE-based model does both: the document features adjust the prior over topic distributions (through δ), but by tying together the document and topic components (with β), the document features also affect the prior over word distributions. To the best of our knowledge, this is the first topic model to condition both topic and word distributions on the same features. The topic aspect model (Paul and Girju, 2010a) is also a two-dimensional factored model that has been used to jointly model topic and perspective (Paul and Girju, 2010b). However, this model does not use structured priors over the parameters, unlike most of the models discussed in §4. An alternative approach to incorporating user preferences and expertise are interactive topic models (Hu et al., 2013), a complimentary approach to SPRITE. 9 Discussion and Conclusion We have presented SPRITE, a family of topic models that utilize structured priors to induce preferred topic structures. Specific instantiations of SPRITE are similar or equiv</context>
</contexts>
<marker>Paul, Girju, 2010</marker>
<rawString>M. Paul and R. Girju. 2010a. A two-dimensional topic-aspect model for discovering multi-faceted topics. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Paul</author>
<author>R Girju</author>
</authors>
<title>Summarizing contrastive viewpoints in opinionated text.</title>
<date>2010</date>
<booktitle>In Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="56097" citStr="Paul and Girju, 2010" startWordPosition="9269" endWordPosition="9272">ecific word distributions. This is an alternative to the more common approach to regression based topic modeling, where the variables affect the topic distributions rather than the word distributions. Our SPRITE-based model does both: the document features adjust the prior over topic distributions (through δ), but by tying together the document and topic components (with β), the document features also affect the prior over word distributions. To the best of our knowledge, this is the first topic model to condition both topic and word distributions on the same features. The topic aspect model (Paul and Girju, 2010a) is also a two-dimensional factored model that has been used to jointly model topic and perspective (Paul and Girju, 2010b). However, this model does not use structured priors over the parameters, unlike most of the models discussed in §4. An alternative approach to incorporating user preferences and expertise are interactive topic models (Hu et al., 2013), a complimentary approach to SPRITE. 9 Discussion and Conclusion We have presented SPRITE, a family of topic models that utilize structured priors to induce preferred topic structures. Specific instantiations of SPRITE are similar or equiv</context>
</contexts>
<marker>Paul, Girju, 2010</marker>
<rawString>M.J. Paul and R. Girju. 2010b. Summarizing contrastive viewpoints in opinionated text. In Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Paul</author>
<author>B C Wallace</author>
<author>M Dredze</author>
</authors>
<title>What affects patient (dis)satisfaction? Analyzing online doctor ratings with a joint topic-sentiment model.</title>
<date>2013</date>
<booktitle>In AAAI Workshop on Expanding the Boundaries of Health Informatics Using AI.</booktitle>
<contexts>
<context position="17403" citStr="Paul et al., 2013" startWordPosition="2861" endWordPosition="2864">the components in SCTM only apply to the word distributions, and not the topic distributions in documents. 4.3 Factored Topic Models Factored topic models combine multiple aspects of the text to generate the document (instead of just topics). One such topic model is Factorial LDA (FLDA) (Paul and Dredze, 2012). In FLDA, 2The posterior becomes concentrated around the prior when the Dirichlet variance is low, in which case SPRITE behaves like SCTM. SPRITE is therefore more general. 46 “topics” are actually tuples of potentially multiple variables, such as aspect and sentiment in online reviews (Paul et al., 2013). Each document distribution θm is a distribution over pairs (or higherdimensional tuples if there are more than two factors), and each pair (j, k) has a word distribution φ(j,k). FLDA uses a similar log-linear parameterization of the Dirichlet priors as SPRITE. Using our notation, the Dirichlet(˜φ(j,k)) prior for φ(j,k) is defined as ˜φ(j,k),v=exp(ωjv+ωkv), where ωj is a weight vector over the vocabulary for the jth component of the first factor, and ωk encodes the weights for the kth component of the second factor. (Some bias terms are omitted for simplicity.) The prior over θm has a similar</context>
<context position="31443" citStr="Paul et al., 2013" startWordPosition="5205" endWordPosition="5208">chical topic model to this data. Each document is a transcript of one speaker’s turn in a debate, and each document includes the first dimension of the DW-NOMINATE score (Lewis and Poole, 2004), a real-valued score indicating how conservative (positive) or liberal (negative) the speaker is. This value is α(P). We took a sample of 5,000 documents from the House debates (850,374 tokens; 7,426 types), balanced across party affilia49 tion. We sampled from the most partisan speakers, removing scores below the median value. • Reviews: Doctor reviews from RateMDs.com, previously analyzed using FLDA (Paul et al., 2013; Wallace et al., 2014). The reviews contain ratings on a 1–5 scale for multiple aspects. We centered the ratings around the middle value 3, then took reviews that had the same sign for all aspects, and averaged the scores to produce a value for α(P). Our corpus contains 20,000 documents (476,991 tokens; 10,158 types), balanced across positive/negative scores. Unless otherwise specified, K=50 topics and C=10 components (excluding the perspective component) for Debates, and K=20 and C=5 for Reviews. These values were chosen as a qualitative preference, not optimized for predictive performance, </context>
</contexts>
<marker>Paul, Wallace, Dredze, 2013</marker>
<rawString>M.J. Paul, B.C. Wallace, and M. Dredze. 2013. What affects patient (dis)satisfaction? Analyzing online doctor ratings with a joint topic-sentiment model. In AAAI Workshop on Expanding the Boundaries of Health Informatics Using AI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rabinovich</author>
<author>D Blei</author>
</authors>
<title>The inverse regression topic model.</title>
<date>2014</date>
<booktitle>In International Conference on Machine Learning.</booktitle>
<contexts>
<context position="54171" citStr="Rabinovich and Blei, 2014" startWordPosition="8993" endWordPosition="8996">orized approach). The regression structure is also different. SHLDA is a “downstream” model, where the perspective value is a response variable conditioned on the topics. In contrast, SPRITE is an “upstream” model, where the topics are conditioned on the perspective value. We argue that the latter is more accurate as a generative story (the emitted words depend on the author’s perspective, not the other way around). Moreover, in our model the perspective influences both the word and topic distributions (through the topic and document components, respectively). Inverse regression topic models (Rabinovich and Blei, 2014) use document feature values (such as political ideology) to alter the parameters of the 54 Figure 6: Examples of topics (gray boxes) and components (colored boxes) learned on the Abstracts corpus with 50 topics using a factored structure. The components have been grouped into two factors, one factor with 3 components (left) and one with 7 (right), with two examples shown from each. Each topic prior draws from exactly one component from each factor. similarity words word vector semantic similar based method words corpus word multiword paper based frequency expressions question questions answer</context>
</contexts>
<marker>Rabinovich, Blei, 2014</marker>
<rawString>M. Rabinovich and D. Blei. 2014. The inverse regression topic model. In International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ramage</author>
<author>D Hall</author>
<author>R Nallapati</author>
<author>C D Manning</author>
</authors>
<title>Labeled LDA: a supervised topic model for credit attribution in multi-labeled corpora.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2018" citStr="Ramage et al., 2009" startWordPosition="300" endWordPosition="303">provide a flexible framework for encoding preferences as priors for how topics should be structured. SPRITE can incorporate many types of structure that have been considered in prior work, including hierarchies (Blei et al., 2003a; Mimno et al., 2007), factorizations (Paul and Dredze, 2012; Eisenstein et al., 2011), sparsity (Wang and Blei, 2009; Balasubramanyan and Cohen, 2013), correlations between topics (Blei and Lafferty, 2007; Li and McCallum, 2006), preferences over word choices (Andrzejewski et al., 2009; Paul and Dredze, 2013), and associations between topics and document attributes (Ramage et al., 2009; Mimno and McCallum, 2008). SPRITE builds on a standard topic model, adding structure to the priors over the model parameters. The priors are given by log-linear functions of underlying components (§2), which provide additional latent structure that we will show can enrich the model in many ways. By applying particular constraints and priors to the component hyperparameters, a variety of structures can be induced such as hierarchies and factorizations (§3), and we will show that this framework captures many existing topic models (§4). After describing the general form of the model, we show ho</context>
</contexts>
<marker>Ramage, Hall, Nallapati, Manning, 2009</marker>
<rawString>D. Ramage, D. Hall, R. Nallapati, and C.D. Manning. 2009. Labeled LDA: a supervised topic model for credit attribution in multi-labeled corpora. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
<author>J Eisner</author>
</authors>
<title>Annealing structural bias in multilingual weighted grammar induction.</title>
<date>2006</date>
<booktitle>In COLING-ACL.</booktitle>
<contexts>
<context position="26127" citStr="Smith and Eisner, 2006" startWordPosition="4303" endWordPosition="4306">r Dirichlet priors, we can straightforwardly strengthen the preference to be binary by modifying the objective function to favor the prior more heavily. Specifically, under a Dirichlet(ρ&lt;1) prior we will introduce a scaling parameter τt ≥ 1 to the prior log likelihood: τt log P(β) with partial derivative τt ρ−1 βkc , which adds extra weight to the sparse Dirichlet prior in the objective. The algorithm used in our experiments begins with τ1 = 1 and optionally increases τ over time. This is a deterministic annealing approach, where τ corresponds to an inverse temperature (Ueda and Nakano, 1998; Smith and Eisner, 2006). As τ approaches infinity, the prior-annealed MAP objective maxβ P(φ|β)P(β)τ approaches maxβ P(φ|β) maxβ P(β). Annealing only the prior P(β) results in maximization of this term only, while the outer max chooses a good β under P(φ|β) as a tie-breaker among all β values that maximize the inner max (binary-valued β).3 We show experimentally (§7.2.2) that annealing the prior yields values that satisfy the constraints. 3Other modifications could be made to the objective function to induce sparsity, such as entropy regularization (Balasubramanyan and Cohen, 2013). ωcv ˜φkv X (1) 48 6 A Factored Hi</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>N.A. Smith and J. Eisner. 2006. Annealing structural bias in multilingual weighted grammar induction. In COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Talley</author>
<author>D Newman</author>
<author>D Mimno</author>
<author>B W Herr H M Wallach</author>
<author>G A P C Burns</author>
<author>M Leenders</author>
<author>A McCallum</author>
</authors>
<title>Database of NIH grants using machine-learned categories and graphical clustering.</title>
<date>2011</date>
<journal>Nature Methods,</journal>
<volume>8</volume>
<issue>6</issue>
<contexts>
<context position="1269" citStr="Talley et al., 2011" startWordPosition="184" endWordPosition="187">opic hierarchies and author perspective, which we apply to corpora of political debates and online reviews. We show that the model learns intuitive topics, outperforming several other topic models at predictive tasks. 1 Introduction Topic models can be a powerful aid for analyzing large collections of text by uncovering latent interpretable structures without manual supervision. Yet people often have expectations about topics in a given corpus and how they should be structured for a particular task. It is crucial for the user experience that topics meet these expectations (Mimno et al., 2011; Talley et al., 2011) yet black box topic models provide no control over the desired output. This paper presents SPRITE, a family of topic models that provide a flexible framework for encoding preferences as priors for how topics should be structured. SPRITE can incorporate many types of structure that have been considered in prior work, including hierarchies (Blei et al., 2003a; Mimno et al., 2007), factorizations (Paul and Dredze, 2012; Eisenstein et al., 2011), sparsity (Wang and Blei, 2009; Balasubramanyan and Cohen, 2013), correlations between topics (Blei and Lafferty, 2007; Li and McCallum, 2006), preferenc</context>
</contexts>
<marker>Talley, Newman, Mimno, Wallach, Burns, Leenders, McCallum, 2011</marker>
<rawString>E.M. Talley, D. Newman, D. Mimno, B.W. Herr II, H.M. Wallach, G.A.P.C. Burns, M. Leenders, and A. McCallum. 2011. Database of NIH grants using machine-learned categories and graphical clustering. Nature Methods, 8(6):443–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ueda</author>
<author>R Nakano</author>
</authors>
<title>Deterministic annealing EM algorithm.</title>
<date>1998</date>
<journal>Neural Networks,</journal>
<volume>11</volume>
<issue>2</issue>
<pages>282</pages>
<contexts>
<context position="26102" citStr="Ueda and Nakano, 1998" startWordPosition="4299" endWordPosition="4302">les using sparse Beta or Dirichlet priors, we can straightforwardly strengthen the preference to be binary by modifying the objective function to favor the prior more heavily. Specifically, under a Dirichlet(ρ&lt;1) prior we will introduce a scaling parameter τt ≥ 1 to the prior log likelihood: τt log P(β) with partial derivative τt ρ−1 βkc , which adds extra weight to the sparse Dirichlet prior in the objective. The algorithm used in our experiments begins with τ1 = 1 and optionally increases τ over time. This is a deterministic annealing approach, where τ corresponds to an inverse temperature (Ueda and Nakano, 1998; Smith and Eisner, 2006). As τ approaches infinity, the prior-annealed MAP objective maxβ P(φ|β)P(β)τ approaches maxβ P(φ|β) maxβ P(β). Annealing only the prior P(β) results in maximization of this term only, while the outer max chooses a good β under P(φ|β) as a tie-breaker among all β values that maximize the inner max (binary-valued β).3 We show experimentally (§7.2.2) that annealing the prior yields values that satisfy the constraints. 3Other modifications could be made to the objective function to induce sparsity, such as entropy regularization (Balasubramanyan and Cohen, 2013). ωcv ˜φkv</context>
</contexts>
<marker>Ueda, Nakano, 1998</marker>
<rawString>N. Ueda and R. Nakano. 1998. Deterministic annealing EM algorithm. Neural Networks, 11(2):271– 282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B C Wallace</author>
<author>M J Paul</author>
<author>U Sarkar</author>
<author>T A Trikalinos</author>
<author>M Dredze</author>
</authors>
<title>A large-scale quantitative analysis of latent factors and sentiment in online doctor reviews.</title>
<date>2014</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>21</volume>
<issue>6</issue>
<contexts>
<context position="31466" citStr="Wallace et al., 2014" startWordPosition="5209" endWordPosition="5212">to this data. Each document is a transcript of one speaker’s turn in a debate, and each document includes the first dimension of the DW-NOMINATE score (Lewis and Poole, 2004), a real-valued score indicating how conservative (positive) or liberal (negative) the speaker is. This value is α(P). We took a sample of 5,000 documents from the House debates (850,374 tokens; 7,426 types), balanced across party affilia49 tion. We sampled from the most partisan speakers, removing scores below the median value. • Reviews: Doctor reviews from RateMDs.com, previously analyzed using FLDA (Paul et al., 2013; Wallace et al., 2014). The reviews contain ratings on a 1–5 scale for multiple aspects. We centered the ratings around the middle value 3, then took reviews that had the same sign for all aspects, and averaged the scores to produce a value for α(P). Our corpus contains 20,000 documents (476,991 tokens; 10,158 types), balanced across positive/negative scores. Unless otherwise specified, K=50 topics and C=10 components (excluding the perspective component) for Debates, and K=20 and C=5 for Reviews. These values were chosen as a qualitative preference, not optimized for predictive performance, but we experiment with </context>
</contexts>
<marker>Wallace, Paul, Sarkar, Trikalinos, Dredze, 2014</marker>
<rawString>B.C. Wallace, M.J. Paul, U. Sarkar, T.A. Trikalinos, and M. Dredze. 2014. A large-scale quantitative analysis of latent factors and sentiment in online doctor reviews. Journal of the American Medical Informatics Association, 21(6):1098–1103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H M Wallach</author>
<author>D Mimno</author>
<author>A McCallum</author>
</authors>
<title>Rethinking LDA: Why priors matter.</title>
<date>2009</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="34854" citStr="Wallach et al., 2009" startWordPosition="5744" endWordPosition="5748">ew, technology, future, renewable}. Both of these topics share a common parent of an industry-related supertopic whose top words are {industry, companies, market, price}. A nonpartisan topic under this same supertopic has top words {credit, financial, loan, mortgage, loans}. 7.2.2 Quantitative Evaluation We evaluated the model on two predictive tasks as well as topic quality. The first metric is perplexity of held-out text. The held-out set is based on tokens rather than documents: we trained on even numbered tokens and tested on odd tokens. This is a type of “document completion” evaluation (Wallach et al., 2009b) which measures how well the model can predict held-out tokens of a document after observing only some. We also evaluated how well the model can predict the attribute value (DW-NOMINATE score or user rating) of the document. We trained a linear regression model using the document topic distributions 0 as features. We held out half of the documents for testing and measured the mean absolute error. When estimating document-specific SPRITE parameters for held-out documents, we fix the feature value α(P) m= 0 for that document. These predictive experiments do not directly measure performance at </context>
<context position="44028" citStr="Wallach et al., 2009" startWordPosition="7276" endWordPosition="7279">A in most measures. The results are quite different under the coherence metric. It seems that topic components (which influence the word distributions) improve coherence over LDA, while document components worsen coherence. SCTM-style (which uses only topic components) does the best in both datasets, while PAM-style (which uses only documents) does the worst. PAM also significantly improves over LDA, despite worse perplexity. The LDA (learned) baseline substantially outperforms LDA (fixed) in all cases, highlighting the importance of optimizing hyperparameters, consistent with prior research (Wallach et al., 2009a). Surprisingly, many SPRITE variants also outperform the bag of words regression baseline, even though the latter was tuned to optimize performance using heavy t2 regularization, which we applied only weakly (without tuning) to the topic model features. We also point out that the “bag of words” version of the coherence metric (the coherence of the top 20 words) is higher than the average topic coherence, which is an artifact of how the metric is defined: the most probable words in the corpus also tend to co-occur together in most documents, so these words are considered to be highly coherent</context>
</contexts>
<marker>Wallach, Mimno, McCallum, 2009</marker>
<rawString>H.M. Wallach, D. Mimno, and A. McCallum. 2009a. Rethinking LDA: Why priors matter. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H M Wallach</author>
<author>I Murray</author>
<author>R Salakhutdinov</author>
<author>D Mimno</author>
</authors>
<title>Evaluation methods for topic models.</title>
<date>2009</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="34854" citStr="Wallach et al., 2009" startWordPosition="5744" endWordPosition="5748">ew, technology, future, renewable}. Both of these topics share a common parent of an industry-related supertopic whose top words are {industry, companies, market, price}. A nonpartisan topic under this same supertopic has top words {credit, financial, loan, mortgage, loans}. 7.2.2 Quantitative Evaluation We evaluated the model on two predictive tasks as well as topic quality. The first metric is perplexity of held-out text. The held-out set is based on tokens rather than documents: we trained on even numbered tokens and tested on odd tokens. This is a type of “document completion” evaluation (Wallach et al., 2009b) which measures how well the model can predict held-out tokens of a document after observing only some. We also evaluated how well the model can predict the attribute value (DW-NOMINATE score or user rating) of the document. We trained a linear regression model using the document topic distributions 0 as features. We held out half of the documents for testing and measured the mean absolute error. When estimating document-specific SPRITE parameters for held-out documents, we fix the feature value α(P) m= 0 for that document. These predictive experiments do not directly measure performance at </context>
<context position="44028" citStr="Wallach et al., 2009" startWordPosition="7276" endWordPosition="7279">A in most measures. The results are quite different under the coherence metric. It seems that topic components (which influence the word distributions) improve coherence over LDA, while document components worsen coherence. SCTM-style (which uses only topic components) does the best in both datasets, while PAM-style (which uses only documents) does the worst. PAM also significantly improves over LDA, despite worse perplexity. The LDA (learned) baseline substantially outperforms LDA (fixed) in all cases, highlighting the importance of optimizing hyperparameters, consistent with prior research (Wallach et al., 2009a). Surprisingly, many SPRITE variants also outperform the bag of words regression baseline, even though the latter was tuned to optimize performance using heavy t2 regularization, which we applied only weakly (without tuning) to the topic model features. We also point out that the “bag of words” version of the coherence metric (the coherence of the top 20 words) is higher than the average topic coherence, which is an artifact of how the metric is defined: the most probable words in the corpus also tend to co-occur together in most documents, so these words are considered to be highly coherent</context>
</contexts>
<marker>Wallach, Murray, Salakhutdinov, Mimno, 2009</marker>
<rawString>H.M. Wallach, I. Murray, R. Salakhutdinov, and D. Mimno. 2009b. Evaluation methods for topic models. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wang</author>
<author>D Blei</author>
</authors>
<title>Decoupling sparsity and smoothness in the discrete hierarchical Dirichlet process.</title>
<date>2009</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="1746" citStr="Wang and Blei, 2009" startWordPosition="260" endWordPosition="263">red for a particular task. It is crucial for the user experience that topics meet these expectations (Mimno et al., 2011; Talley et al., 2011) yet black box topic models provide no control over the desired output. This paper presents SPRITE, a family of topic models that provide a flexible framework for encoding preferences as priors for how topics should be structured. SPRITE can incorporate many types of structure that have been considered in prior work, including hierarchies (Blei et al., 2003a; Mimno et al., 2007), factorizations (Paul and Dredze, 2012; Eisenstein et al., 2011), sparsity (Wang and Blei, 2009; Balasubramanyan and Cohen, 2013), correlations between topics (Blei and Lafferty, 2007; Li and McCallum, 2006), preferences over word choices (Andrzejewski et al., 2009; Paul and Dredze, 2013), and associations between topics and document attributes (Ramage et al., 2009; Mimno and McCallum, 2008). SPRITE builds on a standard topic model, adding structure to the priors over the model parameters. The priors are given by log-linear functions of underlying components (§2), which provide additional latent structure that we will show can enrich the model in many ways. By applying particular constr</context>
</contexts>
<marker>Wang, Blei, 2009</marker>
<rawString>C. Wang and D. Blei. 2009. Decoupling sparsity and smoothness in the discrete hierarchical Dirichlet process. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M D Zeiler</author>
</authors>
<title>ADADELTA: An adaptive learning rate method.</title>
<date>2012</date>
<location>CoRR, abs/1212.5701.</location>
<contexts>
<context position="33455" citStr="Zeiler (2012)" startWordPosition="5530" endWordPosition="5531"> of topics learned from the Reviews corpus. The figure includes the highest probability words in various topics as well as the highest weight words in the supertopic components and perspective component, which feed into the priors over the topic parameters. We see that one supertopic includes many words related to surgery, such as procedure and performed, and has multiple children, including a topic about dental work. Another supertopic includes words describing family members such as kids and husband. 4AdaGrad decayed too quickly for the b variables. For these, we used a variant suggested by Zeiler (2012) which uses an average of historical gradients rather than a sum. 5Our code and the data will be available at: http://cs.jhu.edu/˜mpaul. One topic has both supertopics as parents, which appears to describe surgeries that saved a family member’s life, with top words including {saved, life, husband, cancer}. The figure also illustrates which topics are associated more with positive or negative reviews, as indicated by the value of 8(P). Interpretable parameters were also learned from the Debates corpus. Consider two topics about energy that have polar values of 8(P). The conservative-leaning top</context>
</contexts>
<marker>Zeiler, 2012</marker>
<rawString>M.D. Zeiler. 2012. ADADELTA: An adaptive learning rate method. CoRR, abs/1212.5701.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>