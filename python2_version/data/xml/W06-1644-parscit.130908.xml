<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000025">
<title confidence="0.693255">
Style &amp; Topic Language Model Adaptation Using HMM-LDA
</title>
<author confidence="0.485135">
Bo-June (Paul) Hsu, James Glass
</author>
<affiliation confidence="0.198807">
MIT Computer Science and Artificial Intelligence Laboratory
</affiliation>
<address confidence="0.465421">
32 Vassar Street, Cambridge, MA 02139, USA
</address>
<email confidence="0.998773">
{bohsu,glass}@mit.edu
</email>
<sectionHeader confidence="0.995636" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999936636363636">
Adapting language models across styles
and topics, such as for lecture transcrip-
tion, involves combining generic style
models with topic-specific content rele-
vant to the target document. In this
work, we investigate the use of the Hid-
den Markov Model with Latent Dirichlet
Allocation (HMM-LDA) to obtain syn-
tactic state and semantic topic assign-
ments to word instances in the training
corpus. From these context-dependent
labels, we construct style and topic mod-
els that better model the target document,
and extend the traditional bag-of-words
topic models to n-grams. Experiments
with static model interpolation yielded a
perplexity and relative word error rate
(WER) reduction of 7.1% and 2.1%, re-
spectively, over an adapted trigram base-
line. Adaptive interpolation of mixture
components further reduced perplexity
by 9.5% and WER by a modest 0.3%.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969018867925">
With the rapid growth of audio-visual materials
available over the web, effective language mod-
eling of the diverse content, both in style and
topic, becomes essential for efficient access and
management of this information. As a prime
example, successful language modeling for aca-
demic lectures not only enables the initial tran-
scription via automatic speech recognition, but
also assists educators and students in the creation
and navigation of these materials through annota-
tion, retrieval, summarization, and even transla-
tion of the embedded content.
Compared with other types of audio content,
lecture speech often exhibits a high degree of
spontaneity and focuses on narrow topics with
specific terminology (Furui, 2003; Glass et al,
2004). Unfortunately, training corpora available
for language modeling rarely match the target
lecture in both style and topic. While transcripts
from other lectures better match the style of the
target lecture than written text, it is often difficult
to find transcripts on the target topic. On the
other hand, although topic-specific vocabulary
can be gleaned from related text materials, such
as the textbook and lecture slides, written lan-
guage is a poor predictor of how words are actu-
ally spoken. Furthermore, given that the precise
topic of a target lecture is often unknown a priori
and may even shift over time, it is generally dif-
ficult to identify topically related documents.
Thus, an effective language model (LM) need to
not only account for the casual speaking style of
lectures, but also accommodate the topic-specific
vocabulary of the subject matter. Moreover, the
ability of the language model to dynamically
adapt over the course of the lecture could prove
extremely useful for both increasing transcription
accuracy, as well as providing evidence for lec-
ture segmentation and information retrieval.
In this paper, we investigate the application of
the syntactic state and semantic topic assign-
ments from the Hidden Markov Model with La-
tent Dirichlet Allocation model to the problem of
language modeling. We explore the use of these
context-dependent labels to identify style and
learn topics from both a large number of spoken
lectures as well as written text. By dynamically
interpolating lecture style models with topic-
specific models, we obtain language models that
better describe the subtopic structure within a
lecture. Initial experiments demonstrate a 16.1%
perplexity reduction and a 2.4% WER reduction
over an adapted trigram baseline.
</bodyText>
<page confidence="0.989484">
373
</page>
<note confidence="0.853743">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 373–381,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999965470588235">
In the following sections, we first summarize
related research on adaptive and topic-mixture
language models, and describe previous work on
the HMM-LDA model. We then examine the
ability of the model to learn syntactic classes as
well as topics from textbook materials and lec-
ture transcripts. Next, we describe a variety of
language model experiments we performed to
combine style and topic models constructed from
the state and topic labels with conventional tri-
gram models trained from both spoken and writ-
ten materials. We also demonstrate the use of
the combined model in an on-line adaptive mode.
Finally, we summarize the results of this research
and suggest future opportunities for related mod-
eling techniques in spoken lecture and other con-
tent processing research.
</bodyText>
<sectionHeader confidence="0.96894" genericHeader="introduction">
2 Adaptive and Topic-Mixture LMs
</sectionHeader>
<bodyText confidence="0.999977655555556">
The concept of adaptive and topic-mixture lan-
guage models has been previously explored by
many researchers. Adaptive language modeling
exploits the property that words appearing earlier
in a document are likely to appear again. Cache
language models (Kuhn and De Mori, 1990;
Clarkson and Robinson, 1997) leverage this ob-
servation and increase the probability of previ-
ously observed words in a document when pre-
dicting the next word. By interpolating with a
conditional trigram cache model, Goodman
(2001) demonstrated up to 34% decrease in per-
plexity over a trigram baseline for small training
sets.
The cache intuition has been extended by at-
tempting to increase the probability of unob-
served but topically related words. Specifically,
given a mixture model with topic-specific com-
ponents, we can increase the mixture weights of
the topics corresponding to previously observed
words to better predict the next word. Some of
the early work in this area used a maximum en-
tropy language model framework to trigger in-
creases in likelihood of related words (Lau et al.,
1993; Rosenfeld, 1996).
A variety of methods has been used to explore
topic-mixture models. To model a mixture of
topics within a document, the sentence mixture
model (Iyer and Ostendorf, 1999) builds multiple
topic models from clusters of training sentences
and defines the probability of a target sentence as
a weighted combination of its probability under
each topic model. Latent Semantic Analysis
(LSA) has been used to cluster topically related
words and has demonstrated significant reduc-
tion in perplexity and word error rate (Belle-
garda, 2000). Probabilistic LSA (PLSA) has
been used to decompose documents into compo-
nent word distributions and create unigram topic
models from these distributions. Gildea and
Hofmann (1999) demonstrated noticeable per-
plexity reduction via dynamic combination of
these unigram topic models with a generic tri-
gram model.
To identify topics from an unlabeled corpus,
(Blei et al., 2003) extends PLSA with the Latent
Dirichlet Allocation (LDA) model that describes
each document in a corpus as generated from a
mixture of topics, each characterized by a word
unigram distribution. Hidden Markov Model
with LDA (HMM-LDA) (Griffiths et al., 2004)
further extends this topic mixture model to sepa-
rate syntactic words from content words whose
distributions depend primarily on local context
and document topic, respectively.
In the specific area of lecture processing, pre-
vious work in language model adaptation has
primarily focused on customizing a fixed n-gram
language model for each lecture by combining n-
gram statistics from general conversational
speech, other lectures, textbooks, and other re-
sources related to the target lecture (Nanjo and
Kawahara, 2002, 2004; Leeuwis et al., 2003;
Park et al., 2005).
Most of the previous work on topic-mixture
models focuses on in-domain adaptation using
large amounts of matched training data. How-
ever, most, if not all, of the data available to train
a lecture language model are either cross-domain
or cross-style. Furthermore, although adaptive
models have been shown to yield significant per-
plexity reduction on clean transcripts, the im-
provements tend to diminish when working with
speech recognizer hypotheses with high WER.
In this work, we apply the concept of dynamic
topic adaptation to the lecture transcription task.
Unlike previous work, we first construct a style
model and a topic-domain model using the clas-
sification of word instances into syntactic states
and topics provided by HMM-LDA. Further-
more, we leverage the context-dependent labels
to extend topic models from unigrams to n-
grams, allowing for better prediction of transi-
tions involving topic words. Note that although
this work focuses on the use of HMM-LDA to
generate the state and topic labels, any method
that yields such labels suffices for the purpose of
the language modeling experiments. The follow-
ing section describes the HMM-LDA framework
in more detail.
</bodyText>
<page confidence="0.999293">
374
</page>
<sectionHeader confidence="0.996746" genericHeader="method">
3 HMM-LDA
</sectionHeader>
<subsectionHeader confidence="0.999925">
3.1 Latent Dirichlet Allocation
</subsectionHeader>
<bodyText confidence="0.999899326923077">
Discrete Principal Component Analysis describes
a family of models that decompose a set of fea-
ture vectors into its principal components (Bun-
tine and Jakulin, 2005). Describing feature vec-
tors via their components reduces the number of
parameters required to model the data, hence im-
proving the quality of the estimated parameters
when given limited training data. LSA, PLSA,
and LDA are all examples from this family.
Given a predefined number of desired compo-
nents, LSA models feature vectors by finding a
set of orthonormal components that maximize
the variance using singular value decomposition
(Deerwester et al., 1990). Unfortunately, the
component vectors may contain non-interpret-
able negative values when working with word
occurrence counts as feature vectors. PLSA
eliminates this problem by using non-negative
matrix factorization to model each document as a
weighted combination of a set of non-negative
feature vectors (Hofmann, 1999). However, be-
cause the number of parameters grows linearly
with the number of documents, the model is
prone to overfitting. Furthermore, because each
training document has its own set of topic weight
parameters, PLSA does not provide a generative
framework for describing the probability of an
unseen document (Blei et al., 2003).
To address the shortcomings of PLSA, Blei et
al. (2003) introduced the LDA model, which fur-
ther imposes a Dirichlet distribution on the topic
mixture weights corresponding to the documents
in the corpus. With the number of model pa-
rameters dependent only on the number of topic
mixtures and vocabulary size, LDA is less prone
to overfitting and is capable of estimating the
probability of unobserved test documents.
Empirically, LDA has been shown to outper-
form PLSA in corpus perplexity, collaborative
filtering, and text classification experiments (Blei
et al., 2003). Various extensions to the basic
LDA model have since been proposed. The Au-
thor Topic model adds an additional dependency
on the author(s) to the topic mixture weights of
each document (Rosen-Zvi et al., 2005). The
Hierarchical Dirichlet Process is a nonparametric
model that generalizes distribution parameter
modeling to multiple levels. Without having to
estimate the number of mixture components, this
model has been shown to match the best result
from LDA on a document modeling task (Teh et
al., 2004).
</bodyText>
<subsectionHeader confidence="0.998688">
3.2 Hidden Markov Model with LDA
</subsectionHeader>
<bodyText confidence="0.962379558823529">
HMM-LDA model proposed by Griffiths et al.
(2004) combines the HMM and LDA models to
separate syntactic words with local dependencies
from topic-dependent content words without re-
quiring any labeled data. Similar to HMM-based
part-of-speech taggers, HMM-LDA maps each
word in the document to a hidden syntactic state.
Each state generates words according to a uni-
gram distribution except the special topic state,
where words are modeled by document-specific
mixtures of topic distributions, as in LDA.
Figure 1 describes this generative process in
more detail.
Figure 1: Generative framework and graphical
model representation of HMM-LDA. The num-
ber of states and topics are pre-specified. The
topic mixture for each document is modeled with
a Dirichlet distribution. Each word wi in the n-
word document is generated from its hidden state
si or hidden topic zi if si is the special topic state.
Unlike vocabulary selection techniques that
separate domain-independent words from topic-
specific keywords using word collocation statis-
tics, HMM-LDA classifies each word instance
according to its context. Thus, an instance of the
word “return” may be assigned to a syntactic
state in “to return a”, but classified as a topic
keyword in “expected return for”. By labeling
each word in the training set with its syntactic
state and mixture topic, HMM-LDA not only
separates stylistic words from content words in a
context-dependent manner, but also decomposes
the corpus into a set of topic word distributions.
This form of soft, context-dependent classifica-
</bodyText>
<figure confidence="0.978524238095238">
For each document d in the corpus:
1. Draw topic weights d
B from Dirichlet (a)
2. For each word wi in document d:
a. Draw topic zi from Multinomia l( d )
B
b. Draw state si from M ultinomial ( si �1 )
z
c. Draw word wi from:
Multinomial( )
# zi s =
i stopi c
Multinomial(ysi) otherwise
d
z2 w2
z1 w1
zn wn
... ... ...
s1
s2
sn
</figure>
<page confidence="0.994379">
375
</page>
<bodyText confidence="0.9986725">
tion has many potential uses for language model-
ing, topic segmentation, and indexing.
</bodyText>
<subsectionHeader confidence="0.997975">
3.3 Training
</subsectionHeader>
<bodyText confidence="0.999989892857143">
To train an HMM-LDA model, we employ the
MATLAB Topic Modeling Toolbox 1.3 (Grif-
fiths and Steyvers, 2004; Griffiths et al., 2004).
This particular implementation performs Gibbs
sampling, a form of Markov chain Monte Carlo
(MCMC), to estimate the optimal model parame-
ters fitted to the training data. Specifically, the
algorithm creates a Markov chain whose station-
ary distribution matches the expected distribution
of the state and topic labels for each word in the
training corpus. Starting from random labels,
Gibbs sampling sequentially samples the label
for each hidden variable conditioned on the cur-
rent value of all other variables. After a suffi-
cient number of iterations, the Markov chain
converges to the stationary distribution. We can
easily compute the posterior word distribution
for each state and topic from a single sample by
averaging over the label counts and prior pa-
rameters. With a sufficiently large training set,
we will have enough words assigned to each
state and topic to yield a reasonable approxima-
tion to the underlying distribution.
In the following sections, we examine the ap-
plication of models derived from the HMM-LDA
labels to the task of spoken lecture transcription
and explore techniques on adaptive topic model-
ing to construct a better lecture language model.
</bodyText>
<sectionHeader confidence="0.999016" genericHeader="method">
4 HMM-LDA Analysis
</sectionHeader>
<bodyText confidence="0.997421590909091">
Our language modeling experiments have been
conducted on high-fidelity transcripts of ap-
proximately 168 hours of lectures from three un-
dergraduate subjects in math, physics, and com-
puter science (CS), as well as 79 seminars cover-
ing a wide range of topics (Glass et al., 2004).
For evaluation, we withheld the set of 20 CS lec-
tures and used the first 10 lectures as a develop-
ment set and the last 10 lectures for the test set.
The remainder of these data was used for training
and will be referred to as the Lectures dataset.
To supplement the out-of-domain lecture tran-
scripts with topic-specific textual resources, we
added the CS course textbook (Textbook) as ad-
ditional training data for learning the target top-
ics. To create topic-cohesive documents, the
textbook is divided at every section heading to
form 271 documents. Next, the text is heuristi-
cally segmented at sentence-like boundaries and
normalized into the words corresponding to the
spoken form of the text. Table 1 summarizes the
data used in this evaluation.
</bodyText>
<table confidence="0.9994336">
Dataset Documents Sentences Vocabulary Words
Lectures 150 58,626 25,654 1,390,039
Textbook 271 6,762 4,686 131,280
CS Dev 10 4,102 3,285 93,348
CS Test 10 3,595 3,357 87,518
</table>
<tableCaption confidence="0.999898">
Table 1: Summary of evaluation datasets.
</tableCaption>
<bodyText confidence="0.9999785">
In the following analysis, we ran the Gibbs
sampler against the Lectures dataset for a total of
2800 iterations, computing a model every 10 it-
erations, and took the model with the lowest per-
plexity as the final model. We built the model
with 20 states and 100 topics based on prelimi-
nary experiments. We also trained an HMM-
LDA model on the Textbook dataset using the
same model parameters. We ran the sampler for
a total of 2000 iterations, computing the perplex-
ity every 100 iterations. Again, we selected the
lowest perplexity model as the final model.
</bodyText>
<subsectionHeader confidence="0.993152">
4.1 Semantic Topics
</subsectionHeader>
<bodyText confidence="0.976282869565217">
HMM-LDA extracts words whose distributions
vary across documents and clusters them into a
set of components. In Figure 2, we list the top
10 words from a random selection of 10 topics
computed from the Lectures dataset. As shown,
the words assigned to the LDA topic state are
representative of content words and are grouped
into broad semantic topics. For example, topic 4,
8, and 9 correspond to machine learning, linear
algebra, and magnetism, respectively.
Since the Lectures dataset consists of speech
transcripts with disfluencies, it is interesting to
1 2 3 4 5 6 7 8 9 10
center work rights system &lt;laugh&gt; &lt;partial&gt; class basis magnetic light
world research human things her memory people v current red
and right U. robot children ah tax &lt;eh&gt; field water
ideas people S. systems book brain wealth vector loop colors
new computing government work Cambridge animal social matrix surface white
technology network international example books okay American transformation direction angle
innovation system countries person street eye power linear e blue
community information president robots city synaptic world eight law here
place software world learning library receptors &lt;unintelligible&gt; output flux rainbow
building computers support machine brother mouse society t m sun
</bodyText>
<figureCaption confidence="0.996692">
Figure 2: The top 10 words from 10 randomly selected topics computed from the Lectures dataset.
</figureCaption>
<page confidence="0.994176">
376
</page>
<bodyText confidence="0.993045419354839">
observe that “&lt;laugh&gt;” is the top word in a
topic corresponding to childhood memories.
Cursory examination of the data suggests that the
speakers talking about children tend to laugh
more during the lecture. Although it may not be
desirable to capture speaker idiosyncrasies in the
topic mixtures, HMM-LDA has clearly demon-
strated its ability to capture distinctive semantic
topics in a corpus. By leveraging all documents
in the corpus, the model yields smoother topic
word distributions that are less vulnerable to
overfitting.
Since HMM-LDA labels the state and topic of
each word in the training corpus, we can also
visualize the results by color-coding the words
by their topic assignments. Figure 3 shows a
color-coded excerpt from a topically coherent
paragraph in the Textbook dataset. Notice how
most of the content words (uppercase) are as-
signed to the same topic/color. Furthermore, of
the 7 instances of the words “and” and “or”
(underlined), 6 are correctly classified as syntac-
tic or topic words, demonstrating the context-
dependent labeling capabilities of the HMM-
LDA model. Moreover, from these labels, we
can identify multi-word topic key phrases (e.g.
output signals, input signal, “and” gate) in addi-
tion to standalone keywords, an observation we
will leverage later on with n-gram topic models.
We draw an INVERTER SYMBOLICALLY as in Figure 3.24.
An AND GATE, also shown in Figure 3.24, is a PRIMITIVE
</bodyText>
<figureCaption confidence="0.848765181818182">
FUNCTION box with two INPUTS and ONE OUTPUT. It
drives its OUTPUT SIGNAL to a value that is the LOGICAL
AND of the INPUTS. That is, if both of its INPUT SIGNALS
BECOME 1. Then ONE and GATE DELAY time later the AND
GATE will force its OUTPUT SIGNAL TO be 1; otherwise the
OUTPUT will be 0. An OR GATE is a SIMILAR two INPUT
PRIMITIVE FUNCTION box that drives its OUTPUT SIGNAL
to a value that is the LOGICAL OR of the INPUTS. That is, the
OUTPUT will BECOME 1 if at least ONE of the INPUT
SIGNALS is 1; otherwise the OUTPUT will BECOME 0.
Figure 3: Color-coded excerpt from the Textbook
</figureCaption>
<bodyText confidence="0.9638596">
dataset showing the context-dependent topic la-
bels. Syntactic words appear black in lowercase.
Topic words are shown in uppercase with their
respective topic colors. All instances of the
words “and” and “or” are underlined.
</bodyText>
<subsectionHeader confidence="0.988515">
4.2 Syntactic States
</subsectionHeader>
<bodyText confidence="0.999983347826087">
Since the syntactic states are shared across all
documents, we expect words associated with the
syntactic states when applying HMM-LDA to the
Lectures dataset to reflect the lecture style vo-
cabulary.
In Figure 4, we list the top 10 words from each
of the 19 syntactic states (state 20 is the topic
state). Note that each state plays a clear syntactic
role. For example, state 2 contains prepositions
while state 7 contains verbs. Since the model is
trained on transcriptions of spontaneous speech,
hesitation disfluencies (&lt;uh&gt;, &lt;um&gt;, &lt;partial&gt;)
are all grouped in state 3 along with other words
(so, if, okay) that frequently indicate hesitation.
While many of these hesitation words are con-
junctions, the words in state 6 show that most
conjunctions are actually assigned to a different
state representing different syntactic behavior
from hesitations. As demonstrated with sponta-
neous speech, HMM-LDA yields syntactic states
that have a good correspondence to part-of-
speech labels, without requiring any labeled
training data.
</bodyText>
<subsectionHeader confidence="0.993807">
4.3 Discussions
</subsectionHeader>
<bodyText confidence="0.999934526315789">
Although MCMC techniques converge to the
global stationary distribution, we cannot guaran-
tee convergence from observation of the perplex-
ity alone. Unlike EM algorithms, random sam-
pling may actually temporarily decrease the
model likelihood. Thus, in the above analysis,
the number of iterations was chosen to be at least
double the point at which the perplexity first ap-
peared to converge.
In addition to the number of iterations, the
choice of the number of states and topics, as well
as the values of the hyper-parameters on the
Dirichlet prior, also impact the quality and effec-
tiveness of the resulting model. Ideally, we run
the algorithm with different combinations of the
parameter values and perform model selection to
choose the model with the best complexity-
penalized likelihood. However, given finite
computing resources, this approach is often im-
</bodyText>
<table confidence="0.3976175">
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19
the of so know I and is it&apos;s it a way it two going that can very to have
</table>
<tableCaption confidence="0.682882166666667">
this in &lt;uh&gt; see you but are not you an time this one doing what will more just be
a for if do we or was that&apos;s out some thing that three one how would little longer want
that on &lt;um&gt; think they because has I&apos;m up one lot there hundred looking where don&apos;t much doesn&apos;t had
these with &lt;partial&gt; go let as were just them no question which m sort when could good never get
my at now get let&apos;s that goes there&apos;s that in kind he t done if do different go like
our from then say he where had &lt;uh&gt; me two point here five able why just than physically got
</tableCaption>
<footnote confidence="0.716011333333333">
your by okay make I&apos;ll thank comes we&apos;re about any case course d coming which me important that&apos;ll need
those about well look people which means also here this idea who years talking as should long anybody&apos;s try
their as but take I&apos;d is says you&apos;re all another problem they four trying because may as with take
</footnote>
<figureCaption confidence="0.99836">
Figure 4: The top 10 words from the 19 syntactic states computed from the Lectures dataset.
</figureCaption>
<page confidence="0.994865">
377
</page>
<bodyText confidence="0.999952333333333">
practical. As an alternative for future work, we
would like to perform Gibbs sampling on the
hyper-parameters (Griffiths et al., 2004) and ap-
ply the Dirichlet process to estimate the number
of states and topics (Teh et al., 2004).
Despite the suboptimal choice of parameters
and potential lack of convergence, the labels de-
rived from HMM-LDA are still effective for lan-
guage modeling applications, as described next.
</bodyText>
<sectionHeader confidence="0.988949" genericHeader="method">
5 Language Modeling Experiments
</sectionHeader>
<bodyText confidence="0.999501523809524">
To evaluate the effectiveness of models derived
from the separation of syntax from content, we
performed experiments that compare the per-
plexities and WERs of various model combina-
tions. For a baseline, we used an adapted model
(L+T) that linearly interpolates trigram models
trained on the Lectures (L) and Textbook (T)
datasets. In all models, all interpolation weights
and additional parameters are tuned on a devel-
opment set consisting of the first half of the CS
lectures and tested on the second half. Unless
otherwise noted, modified Kneser-Ney discount-
ing (Chen and Goodman, 1998) is applied with
the respective training set vocabulary using the
SRILM Toolkit (Stolcke, 2002).
To compute the word error rates associated
with a specific language model, we used a
speaker-independent speech recognizer (Glass,
2003). The lectures were pre-segmented into
utterances by forced alignment of the reference
transcription.
</bodyText>
<subsectionHeader confidence="0.996922">
5.1 Lecture Style
</subsectionHeader>
<bodyText confidence="0.999515923076923">
In general, an n-gram model trained on a limited
set of topic-specific documents tends to overem-
phasize words from the observed topics instead
of evenly distributing weights over all potential
topics. Specifically, given the list of words fol-
lowing an n-gram context, we would like to
deemphasize the observed occurrences of topic
words and ideally redistribute these counts to all
potential topic words. As an approximation, we
can build such a topic-deemphasized style tri-
gram model (S) by using counts of only n-gram
sequences that do not end on a topic word,
smoothed over the Lectures vocabulary. Figure
5 shows the n-grams corresponding to an utter-
ance used to build the style trigram model. Note
that the counts of topic to style word transitions
are not altered as these probabilities are mostly
independent of the observed topic distribution.
By interpolating the style model (S) from
above with the smoothed trigram model based on
the Lectures dataset (L), the combined model
(L+S) achieves a 3.6% perplexity reduction and
1.0% WER reduction over (L), as shown in Table
2. Without introducing topic-specific training
data, we can already improve the generic lecture
LM performance using the HMM-LDA labels.
</bodyText>
<figure confidence="0.4244432">
&lt;s&gt; for the SPATIAL MEMORY &lt;/s&gt;
unigrams: for, the, spatial, memory, &lt;/s&gt;
bigrams: &lt;s&gt; for, for the, the spatial, spatial memory, memory &lt;/s&gt;
trigrams: &lt;s&gt; &lt;s&gt; for, &lt;s&gt; for the, for the spatial,
the spatial memory, spatial memory &lt;/s&gt;
</figure>
<figureCaption confidence="0.9959745">
Figure 5: Style model n-grams. Topic words in
the utterance are in uppercase.
</figureCaption>
<subsectionHeader confidence="0.99633">
5.2 Topic Domain
</subsectionHeader>
<bodyText confidence="0.999958379310345">
Unlike Lectures, the Textbook dataset contains
content words relevant to the target lectures, but
in a mismatched style. Commonly, the Textbook
trigram model is interpolated with the generic
model to improve the probability estimates of the
transitions involving topic words. The interpola-
tion weight is chosen to best fit the probabilities
of these n-gram sequences while minimizing the
mismatch in style. However, with only one pa-
rameter, all n-gram contexts must share the same
mixture weight. Because transitions from con-
texts containing topic words are rarely observed
in the off-topic Lectures, the Textbook model (T)
should ideally have higher weight in these con-
texts than contexts that are more equally ob-
served in both datasets.
One heuristic approach for adjusting the
weight in these contexts is to build a topic-
domain trigram model (D) from the Textbook n-
gram counts with Witten-Bell smoothing (Chen
and Goodman, 1998) where we emphasize the
sequences containing a topic word in the context
by doubling their counts. In effect, this reduces
the smoothing on words following topic contexts
with respect to lower-order models without sig-
nificantly affecting the transitions from non-topic
words. Figure 6 shows the adjusted counts for an
utterance used to build the domain trigram
model.
</bodyText>
<table confidence="0.539383571428572">
&lt;s&gt; HUFFMAN CODE can be represented as a BINARY TREE ...
unigrams: huffman, code, can, be, represented, as, binary, tree, ...
bigrams: &lt;s&gt; huffman, huffman code (2×), code can (2×),
can be, be represented, represented as, a binary,
binary tree (2×), ...
trigrams: &lt;s&gt; &lt;s&gt; hufmann, &lt;s&gt; hufmann code (2×),
hufmann code can (2×), code can be (2×),
</table>
<footnote confidence="0.8108765">
can be represented, be represented as,
represented as a, as a binary, a binary tree (2×), ...
</footnote>
<figureCaption confidence="0.7647095">
Figure 6: Domain model n-grams. Topic words
in the utterance are in uppercase.
</figureCaption>
<page confidence="0.996369">
378
</page>
<bodyText confidence="0.9999815">
Empirically, interpolating the lectures, text-
book, and style models with the domain model
(L+T+S+D) further decreases the perplexity by
1.4% and WER by 0.3% over (L+T+S), validat-
ing our intuition. Overall, the addition of the
style and domain models reduces perplexity and
WER by a noticeable 7.1% and 2.1%, respec-
tively, as shown in Table 2.
</bodyText>
<table confidence="0.999117260869565">
Model Perplexity
Development Test
L: Lectures Trigram 180.2 (0.0%) 199.6 (0.0%)
T: Textbook Trigram 291.7 (+61.8%) 331.7 (+66.2%)
S: Style Trigram 207.0 (+14.9%) 224.6 (+12.5%)
D: Domain Trigram 354.1 (+96.5%) 411.6 (+106.3%)
L+S 174.2 (–3.3%) 192.4 (–3.6%)
L+T: Baseline 138.3 (0.0%) 154.4 (0.0%)
L+T+S 131.0 (–5.3%) 145.6 (–5.7%)
L+T+S+D 128.8 (–6.9%) 143.6 (–7.1%)
L+T+S+D+Topic100
• Static Mixture (cheat) 118.1 (–14.6%) 131.3 (–15.0%)
• Dynamic Mixture 115.7 (–16.4%) 129.5 (–16.1%)
Word Error Rate
Model Development Test
L: Lectures Trigram 49.5% (0.0%) 50.2% (0.0%)
L+S 49.2% (–0.7%) 49.7% (–1.0%)
L+T: Baseline 46.6% (0.0%) 46.7% (0.0%)
L+T+S 46.0% (–1.2%) 45.8% (–1.8%)
L+T+S+D 45.8% (–1.8%) 45.7% (–2.1%)
L+T+S+D+Topic100
• Static Mixture (cheat) 45.5% (–2.4%) 45.4% (–2.8%)
• Dynamic Mixture 45.4% (–2.6%) 45.6% (–2.4%)
</table>
<tableCaption confidence="0.992842">
Table 2: Perplexity (top) and WER (bottom) per-
</tableCaption>
<bodyText confidence="0.803888">
formance of various model combinations. Rela-
tive reduction is shown in parentheses.
</bodyText>
<subsectionHeader confidence="0.998717">
5.3 Textbook Topics
</subsectionHeader>
<bodyText confidence="0.992617923076923">
In addition to identifying content words, HMM-
LDA also assigns words to a topic based on their
distribution across documents. Thus, we can
apply HMM-LDA with 100 topics to the Text-
book dataset to identify representative words and
their associated contexts for each topic. From
these labels, we can build unsmoothed trigram
language models (Topic100) for each topic from
the counts of observed n-gram sequences that
end in a word assigned to the respective topic.
Figure 7 shows a sample of the word n-grams
identified via this approach for a few topics.
Note that some of the n-grams are key phrases
for the topic while others contain a mixture of
syntactic and topic words. Unlike bag-of-words
models that only identify the unigram distribu-
tion for each topic, the use of context-dependent
labels enables the construction of n-gram topic
models that not only characterize the frequencies
of topic words, but also describe the transition
contexts leading up to these words.
Huffman tree Monte Carlo time segment assoc key
relative frequency rand update the agenda the table
relative frequencies random numbers segment time local table
the tree trials remaining current time a table
one hundred trials passed first agenda of records
</bodyText>
<figureCaption confidence="0.993812">
Figure 7: Sample of n-grams from select topics.
</figureCaption>
<subsectionHeader confidence="0.978542">
5.4 Topic Mixtures
</subsectionHeader>
<bodyText confidence="0.999941111111111">
Since each target lecture generally only covers a
subset of the available topics, it will be ideal to
identify the specific topics corresponding to a
target lecture and assign those topic models more
weight in a linearly interpolated mixture model.
As an ideal case, we performed a cheating ex-
periment to measure the best performance of a
statically interpolated topic mixture model
(L+T+S+D+Topic100) where we tuned the
mixture weights of all mixture components, in-
cluding the lectures, textbook, style, domain, and
the 100 individual topic trigram models on indi-
vidual target lectures.
Table 2 shows that by weighting the compo-
nent models appropriately, we can reduce the
perplexity and WER by an additional 7.9% and
0.7%, respectively, over the (L+T+S+D) model
even with simple linear interpolation for model
combination.
To gain further insight into the topic mixture
model, we examine the breakdown of the nor-
malized topic weights for a specific lecture. As
shown in Figure 8, of the 100 topic models, 15 of
them account for over 90% of the total weight.
Thus, lectures tend to show a significant topic
skew which topic adaptation approaches can
model effectively.
</bodyText>
<figureCaption confidence="0.995286">
Figure 8: Topic mixture weight breakdown.
</figureCaption>
<subsectionHeader confidence="0.898348">
5.5 Topic Adaptation
</subsectionHeader>
<bodyText confidence="0.9999283125">
Unfortunately, since different lectures cover dif-
ferent topics, we generally cannot tune the topic
mixture weights ahead of time. One approach,
without any a priori knowledge of the target lec-
ture, is to adaptively estimate the optimal mix-
ture weights as we process the lecture (Gildea
and Hofmann, 1999). However, since the topic
distribution shifts over a long lecture, modeling a
lecture as an interpolation of components with
fixed weights may not be the most optimal. In-
stead, we employ an exponential decay strategy
where we update the current mixture distribution
by linearly interpolating it with the posterior
topic distribution given the current word. Spe-
cifically, applying Bayes’ rule, the probability of
topic t generating the current word w is given by:
</bodyText>
<equation confidence="0.597077">
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
</equation>
<page confidence="0.768644">
379
</page>
<equation confidence="0.887867">
(t  |w) = P(wjt,)P(t′ )
</equation>
<bodyText confidence="0.990607884615384">
To achieve the exponential decay, we update the
topic distribution after each word according to
Pi+1(t) = (1 – )•Pi(t) + •P(t  |wi), where is the
adaptation rate.
We evaluated this approach of dynamic mix-
ture weight adaptation on the (L+T+S+D+Topic
100) model, with the same set of components as
the cheating experiment with static weights. As
shown in Table 2, the dynamic model actually
outperforms the static model by more than 1% in
perplexity, by better modeling the dynamic topic
substructure within the lecture.
To run the recognizer with a dynamic LM, we
rescored the top 100 hypotheses generated with
the (L+T+S+D) model using the dynamic LM.
The WER obtained through such n-best rescoring
yielded noticeable improvements over the
(L+T+S+D) model without a priori knowledge
of the topic distribution, but did not beat the op-
timal static model on the test set.
To further gain an intuition for mixture weight
adaptation, we plotted the normalized adapted
weights of the topic models across the first lec-
ture of the test set in Figure 9. Note that the
topic mixture varies greatly across the lecture. In
this particular lecture, the lecturer starts out with
a review of the previous lecture. Subsequently,
he shows an example of computation using ac-
cumulators. Finally, he focuses the lecture on
stream as a data structure, with an intervening
example that finds pairs of i and j that sum up to
a prime. By comparing the topic labels in Figure
9 with the top words from the corresponding top-
ics in Figure 10, we observe that the topic
weights obtained via dynamic adaptation match
the subject matter of the lecture fairly closely.
Finally, to assess the effect that word error rate
has on adaptation performance, we applied the
adaptation algorithm to the corresponding tran-
script from the automatic speech recognizer
(ASR). Traditional cache language models tend
to be vulnerable to recognition errors since incor-
rect words in the history negatively bias the pre-
diction of the current word. However, by adapt-
ing at a topic level, which reduces the number of
dynamic parameters, the dynamic topic model is
less sensitive to recognition errors. As seen in
Figure 9, even with a word error rate around
40%, the normalized topic mixture weights from
the ASR transcript still show a strong resem-
blance to the original weights from the manual
reference transcript.
</bodyText>
<figureCaption confidence="0.932372666666667">
Figure 9: Adaptation of topic model weights on
manual and ASR transcription of a single lecture.
T12 T35 T98 T99
stream pairs sequence of
s i enumerate see
streams j accumulate and
integers k map in
series pair interval for
prime s filter vs
filter integers sequences register
delayed sum operations data
interleave queens odd as
infinite t nil make
Figure 10: Top 10 words from select Textbook
topics appearing in Figure 9.
</figureCaption>
<sectionHeader confidence="0.984432" genericHeader="conclusions">
6 Summary and Conclusions
</sectionHeader>
<bodyText confidence="0.999971111111111">
In this paper, we have shown how to leverage
context-dependent state and topic labels, such as
the ones generated by the HMM-LDA model, to
construct better language models for lecture tran-
scription and extend topic models beyond tradi-
tional unigrams. Although the WER of the top
recognizer hypotheses exceeds 45%, by dynami-
cally updating the mixture weights to model the
topic substructure within individual lectures, we
are able to reduce the test set perplexity and
WER by over 16% and 2.4%, respectively, rela-
tive to the combined Lectures and Textbook
(L+T) baseline.
Although we primarily focused on lecture
transcription in this work, the techniques extend
to language modeling scenarios where exactly
matched training data are often limited or non-
existent. Instead, we have to rely on appropriate
combination of models derived from partially
matched data. HMM-LDA and related tech-
niques show great promise for finding structure
in unlabeled data, from which we can build more
sophisticated models.
The experiments in this paper combine models
primarily through simple linear interpolation. As
motivated in section 5.2, allowing for context-
dependent interpolation weights based on topic
</bodyText>
<page confidence="0.988825">
380
</page>
<bodyText confidence="0.999837545454545">
labels may yield significant improvement for
both perplexity and WER. Thus, in future work,
we would like to study algorithms for automati-
cally learning appropriate context-dependent in-
terpolation weights. Furthermore, we hope to
improve the convergence properties of the dy-
namic adaptation scheme at the start of lectures
and across topic transitions. Lastly, we would
like to extend the LDA framework to support
speaker-specific adaptation and apply the result-
ing topic distributions to lecture segmentation.
</bodyText>
<sectionHeader confidence="0.996363" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99997775">
We would like to thank the anonymous review-
ers for their useful comments and feedback.
Support for this research was provided in part by
the National Science Foundation under grant
#IIS-0415865. Any opinions, findings, and con-
clusions, or recommendations expressed in this
material are those of the authors and do not nec-
essarily reflect the views of the NSF.
</bodyText>
<sectionHeader confidence="0.990517" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.999627512195122">
Y. Akita and T. Kawahara. 2004. Language Model
Adaptation Based on PLSA of Topics and Speak-
ers. In Proc. ICSLP.
J. Bellegarda. 2000. Exploiting Latent Semantic In-
formation in Statistical Language Modeling. In
Proc. IEEE, 88(8):1279-1296.
D. Blei, A. Ng, and M. Jordan. 1993. Latent
Dirichlet Allocation. Journal of Machine Learning
Research, 3:993-1022.
W. Buntine and A. Jakulin. 2005. Discrete Principal
Component Analysis. Technical Report, Helsinki
Institute for Information Technology.
S. Chen and J. Goodman. 1996. An Empirical Study
of Smoothing Techniques for Language Modeling.
In Proc. ACL, 310-318.
P. Clarkson and A. Robinson. 1997. Language
Model Adaptation Using Mixtures and an Expo-
nentially Decaying Cache. In Proc. ICASSP.
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, R.
Harshman. 1990. Indexing by Latent Semantic
Analysis. Journal of the American Society for In-
formation Science, 41(6):391-407.
S. Furui. 2003. Recent Advances in Spontaneous
Speech Recognition and Understanding. In Proc.
IEEE Workshop on Spontaneous Speech Proc. and
Rec, 1-6.
D. Gildea and T. Hofmann. 1999. Topic-Based Lan-
guage Models Using EM. In Proc. Eurospeech.
J. Glass. 2003. A Probabilistic Framework for Seg-
ment-based Speech Recognition. Computer, Speech
and Language, 17:137-152.
J. Glass, T.J. Hazen, L. Hetherington, and C. Wang.
2004. Analysis and Processing of Lecture Audio
Data: Preliminary Investigations. In Proc. HLT-
NAACL Workshop on Interdisciplinary Approaches
to Speech Indexing and Retrieval, 9-12.
J. Goodman. 2001. A Bit of Progress in Language
Modeling (Extended Version). Technical Report,
Microsoft Research.
T. Griffiths and M. Steyvers. 2004. Finding Scien-
tific Topics. In Proc. National Academy of Sci-
ence, 101(Suppl. 1):5228-5235.
T. Griffiths, M. Steyvers, D. Blei, and J. Tenenbaum.
2004. Integrating Topics and Syntax. Adv. in Neu-
ral Information Processing Systems, 17:537-544.
R. Iyer and M. Ostendorf. 1999. Modeling Long
Distance Dependence in Language: Topic Mixtures
Versus Dynamic Cache. In IEEE Transactions on
Speech and Audio Processing, 7:30-39.
R. Kuhn and R. De Mori. 1990. A Cache-Based
Natural Language Model for Speech Recognition.
In IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 12:570-583.
R. Lau, R. Rosenfeld, S. Roukos. 1993. Trigger-
Based Language Models: a Maximum Entropy
Approach. In Proc. ICASSP.
E. Leeuwis, M. Federico, and M. Cettolo. 2003. Lan-
guage Modeling and Transcription of the TED
Corpus Lectures. In Proc. ICASSP.
H. Nanjo and T. Kawahara. 2002. Unsupervised
Language Model Adaptation for Lecture Speech
Recognition. In Proc. ICSLP.
H. Nanjo and T. Kawahara. 2004. Language Model
and Speaking Rate Adaptation for Spontaneous
Presentation Speech Recognition. In IEEE Trans.
SAP, 12(4):391-400.
A. Park, T. Hazen, and J. Glass. 2005. Automatic
Processing of Audio Lectures for Information Re-
trieval: Vocabulary Selection and Language Mod-
eling. In Proc. ICASSP.
M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P.
Smyth. 2004. The Author-Topic Model for Au-
thors and Documents. 20th Conference on Uncer-
tainty in Artificial Intelligence.
R. Rosenfeld. 1996. A Maximum Entropy Approach
to Adaptive Statistical Language Modeling. Com-
puter, Speech and Language, 10:187-228.
A. Stolcke. 2002. SRILM – An Extensible Language
Modeling Toolkit. In Proc. ICSLP.
Y. Teh, M. Jordan, M. Beal, and D. Blei. 2006. Hi-
erarchical Dirichlet Processes. To appear in Jour-
nal of the American Statistical Association.
</reference>
<page confidence="0.998652">
381
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.974016">
<title confidence="0.998908">Style &amp; Topic Language Model Adaptation Using HMM-LDA</title>
<author confidence="0.999256">Bo-June Hsu</author>
<author confidence="0.999256">James Glass</author>
<affiliation confidence="0.999738">MIT Computer Science and Artificial Intelligence</affiliation>
<address confidence="0.999099">32 Vassar Street, Cambridge, MA 02139, USA</address>
<email confidence="0.999785">bohsu@mit.edu</email>
<email confidence="0.999785">glass@mit.edu</email>
<abstract confidence="0.998971739130435">Adapting language models across styles and topics, such as for lecture transcription, involves combining generic style models with topic-specific content relevant to the target document. In this work, we investigate the use of the Hidden Markov Model with Latent Dirichlet Allocation (HMM-LDA) to obtain syntactic state and semantic topic assignments to word instances in the training corpus. From these context-dependent labels, we construct style and topic models that better model the target document, and extend the traditional bag-of-words topic models to n-grams. Experiments with static model interpolation yielded a perplexity and relative word error rate (WER) reduction of 7.1% and 2.1%, respectively, over an adapted trigram baseline. Adaptive interpolation of mixture components further reduced perplexity by 9.5% and WER by a modest 0.3%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Akita</author>
<author>T Kawahara</author>
</authors>
<title>Language Model Adaptation Based on PLSA of Topics and Speakers. In</title>
<date>2004</date>
<booktitle>Proc. ICSLP.</booktitle>
<marker>Akita, Kawahara, 2004</marker>
<rawString>Y. Akita and T. Kawahara. 2004. Language Model Adaptation Based on PLSA of Topics and Speakers. In Proc. ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bellegarda</author>
</authors>
<title>Exploiting Latent Semantic Information in Statistical Language Modeling.</title>
<date>2000</date>
<booktitle>In Proc. IEEE,</booktitle>
<pages>88--8</pages>
<contexts>
<context position="6208" citStr="Bellegarda, 2000" startWordPosition="957" endWordPosition="959">to trigger increases in likelihood of related words (Lau et al., 1993; Rosenfeld, 1996). A variety of methods has been used to explore topic-mixture models. To model a mixture of topics within a document, the sentence mixture model (Iyer and Ostendorf, 1999) builds multiple topic models from clusters of training sentences and defines the probability of a target sentence as a weighted combination of its probability under each topic model. Latent Semantic Analysis (LSA) has been used to cluster topically related words and has demonstrated significant reduction in perplexity and word error rate (Bellegarda, 2000). Probabilistic LSA (PLSA) has been used to decompose documents into component word distributions and create unigram topic models from these distributions. Gildea and Hofmann (1999) demonstrated noticeable perplexity reduction via dynamic combination of these unigram topic models with a generic trigram model. To identify topics from an unlabeled corpus, (Blei et al., 2003) extends PLSA with the Latent Dirichlet Allocation (LDA) model that describes each document in a corpus as generated from a mixture of topics, each characterized by a word unigram distribution. Hidden Markov Model with LDA (H</context>
</contexts>
<marker>Bellegarda, 2000</marker>
<rawString>J. Bellegarda. 2000. Exploiting Latent Semantic Information in Statistical Language Modeling. In Proc. IEEE, 88(8):1279-1296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>A Ng</author>
<author>M Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>1993</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<marker>Blei, Ng, Jordan, 1993</marker>
<rawString>D. Blei, A. Ng, and M. Jordan. 1993. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993-1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Buntine</author>
<author>A Jakulin</author>
</authors>
<title>Discrete Principal Component Analysis.</title>
<date>2005</date>
<tech>Technical Report,</tech>
<institution>Helsinki Institute for Information Technology.</institution>
<contexts>
<context position="8794" citStr="Buntine and Jakulin, 2005" startWordPosition="1355" endWordPosition="1359">ge the context-dependent labels to extend topic models from unigrams to ngrams, allowing for better prediction of transitions involving topic words. Note that although this work focuses on the use of HMM-LDA to generate the state and topic labels, any method that yields such labels suffices for the purpose of the language modeling experiments. The following section describes the HMM-LDA framework in more detail. 374 3 HMM-LDA 3.1 Latent Dirichlet Allocation Discrete Principal Component Analysis describes a family of models that decompose a set of feature vectors into its principal components (Buntine and Jakulin, 2005). Describing feature vectors via their components reduces the number of parameters required to model the data, hence improving the quality of the estimated parameters when given limited training data. LSA, PLSA, and LDA are all examples from this family. Given a predefined number of desired components, LSA models feature vectors by finding a set of orthonormal components that maximize the variance using singular value decomposition (Deerwester et al., 1990). Unfortunately, the component vectors may contain non-interpretable negative values when working with word occurrence counts as feature ve</context>
</contexts>
<marker>Buntine, Jakulin, 2005</marker>
<rawString>W. Buntine and A. Jakulin. 2005. Discrete Principal Component Analysis. Technical Report, Helsinki Institute for Information Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>J Goodman</author>
</authors>
<title>An Empirical Study of Smoothing Techniques for Language Modeling. In</title>
<date>1996</date>
<booktitle>Proc. ACL,</booktitle>
<pages>310--318</pages>
<marker>Chen, Goodman, 1996</marker>
<rawString>S. Chen and J. Goodman. 1996. An Empirical Study of Smoothing Techniques for Language Modeling. In Proc. ACL, 310-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Clarkson</author>
<author>A Robinson</author>
</authors>
<title>Language Model Adaptation Using Mixtures and an Exponentially Decaying Cache. In</title>
<date>1997</date>
<booktitle>Proc. ICASSP.</booktitle>
<contexts>
<context position="4893" citStr="Clarkson and Robinson, 1997" startWordPosition="745" endWordPosition="748">from both spoken and written materials. We also demonstrate the use of the combined model in an on-line adaptive mode. Finally, we summarize the results of this research and suggest future opportunities for related modeling techniques in spoken lecture and other content processing research. 2 Adaptive and Topic-Mixture LMs The concept of adaptive and topic-mixture language models has been previously explored by many researchers. Adaptive language modeling exploits the property that words appearing earlier in a document are likely to appear again. Cache language models (Kuhn and De Mori, 1990; Clarkson and Robinson, 1997) leverage this observation and increase the probability of previously observed words in a document when predicting the next word. By interpolating with a conditional trigram cache model, Goodman (2001) demonstrated up to 34% decrease in perplexity over a trigram baseline for small training sets. The cache intuition has been extended by attempting to increase the probability of unobserved but topically related words. Specifically, given a mixture model with topic-specific components, we can increase the mixture weights of the topics corresponding to previously observed words to better predict t</context>
</contexts>
<marker>Clarkson, Robinson, 1997</marker>
<rawString>P. Clarkson and A. Robinson. 1997. Language Model Adaptation Using Mixtures and an Exponentially Decaying Cache. In Proc. ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S Dumais</author>
<author>G Furnas</author>
<author>T Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>41--6</pages>
<contexts>
<context position="9255" citStr="Deerwester et al., 1990" startWordPosition="1428" endWordPosition="1431">iscrete Principal Component Analysis describes a family of models that decompose a set of feature vectors into its principal components (Buntine and Jakulin, 2005). Describing feature vectors via their components reduces the number of parameters required to model the data, hence improving the quality of the estimated parameters when given limited training data. LSA, PLSA, and LDA are all examples from this family. Given a predefined number of desired components, LSA models feature vectors by finding a set of orthonormal components that maximize the variance using singular value decomposition (Deerwester et al., 1990). Unfortunately, the component vectors may contain non-interpretable negative values when working with word occurrence counts as feature vectors. PLSA eliminates this problem by using non-negative matrix factorization to model each document as a weighted combination of a set of non-negative feature vectors (Hofmann, 1999). However, because the number of parameters grows linearly with the number of documents, the model is prone to overfitting. Furthermore, because each training document has its own set of topic weight parameters, PLSA does not provide a generative framework for describing the p</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>S. Deerwester, S. Dumais, G. Furnas, T. Landauer, R. Harshman. 1990. Indexing by Latent Semantic Analysis. Journal of the American Society for Information Science, 41(6):391-407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Furui</author>
</authors>
<title>Recent Advances in Spontaneous Speech Recognition and Understanding.</title>
<date>2003</date>
<booktitle>In Proc. IEEE Workshop on Spontaneous Speech Proc. and Rec,</booktitle>
<pages>1--6</pages>
<contexts>
<context position="1811" citStr="Furui, 2003" startWordPosition="269" endWordPosition="270">e content, both in style and topic, becomes essential for efficient access and management of this information. As a prime example, successful language modeling for academic lectures not only enables the initial transcription via automatic speech recognition, but also assists educators and students in the creation and navigation of these materials through annotation, retrieval, summarization, and even translation of the embedded content. Compared with other types of audio content, lecture speech often exhibits a high degree of spontaneity and focuses on narrow topics with specific terminology (Furui, 2003; Glass et al, 2004). Unfortunately, training corpora available for language modeling rarely match the target lecture in both style and topic. While transcripts from other lectures better match the style of the target lecture than written text, it is often difficult to find transcripts on the target topic. On the other hand, although topic-specific vocabulary can be gleaned from related text materials, such as the textbook and lecture slides, written language is a poor predictor of how words are actually spoken. Furthermore, given that the precise topic of a target lecture is often unknown a p</context>
</contexts>
<marker>Furui, 2003</marker>
<rawString>S. Furui. 2003. Recent Advances in Spontaneous Speech Recognition and Understanding. In Proc. IEEE Workshop on Spontaneous Speech Proc. and Rec, 1-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>T Hofmann</author>
</authors>
<title>Topic-Based Language Models Using EM.</title>
<date>1999</date>
<booktitle>In Proc. Eurospeech.</booktitle>
<contexts>
<context position="6389" citStr="Gildea and Hofmann (1999)" startWordPosition="982" endWordPosition="985">re of topics within a document, the sentence mixture model (Iyer and Ostendorf, 1999) builds multiple topic models from clusters of training sentences and defines the probability of a target sentence as a weighted combination of its probability under each topic model. Latent Semantic Analysis (LSA) has been used to cluster topically related words and has demonstrated significant reduction in perplexity and word error rate (Bellegarda, 2000). Probabilistic LSA (PLSA) has been used to decompose documents into component word distributions and create unigram topic models from these distributions. Gildea and Hofmann (1999) demonstrated noticeable perplexity reduction via dynamic combination of these unigram topic models with a generic trigram model. To identify topics from an unlabeled corpus, (Blei et al., 2003) extends PLSA with the Latent Dirichlet Allocation (LDA) model that describes each document in a corpus as generated from a mixture of topics, each characterized by a word unigram distribution. Hidden Markov Model with LDA (HMM-LDA) (Griffiths et al., 2004) further extends this topic mixture model to separate syntactic words from content words whose distributions depend primarily on local context and do</context>
<context position="31667" citStr="Gildea and Hofmann, 1999" startWordPosition="5085" endWordPosition="5088">ormalized topic weights for a specific lecture. As shown in Figure 8, of the 100 topic models, 15 of them account for over 90% of the total weight. Thus, lectures tend to show a significant topic skew which topic adaptation approaches can model effectively. Figure 8: Topic mixture weight breakdown. 5.5 Topic Adaptation Unfortunately, since different lectures cover different topics, we generally cannot tune the topic mixture weights ahead of time. One approach, without any a priori knowledge of the target lecture, is to adaptively estimate the optimal mixture weights as we process the lecture (Gildea and Hofmann, 1999). However, since the topic distribution shifts over a long lecture, modeling a lecture as an interpolation of components with fixed weights may not be the most optimal. Instead, we employ an exponential decay strategy where we update the current mixture distribution by linearly interpolating it with the posterior topic distribution given the current word. Specifically, applying Bayes’ rule, the probability of topic t generating the current word w is given by: 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 379 (t |w) = P(wjt,)P(t′ ) To achieve the exponential decay, we update the topic distribution af</context>
</contexts>
<marker>Gildea, Hofmann, 1999</marker>
<rawString>D. Gildea and T. Hofmann. 1999. Topic-Based Language Models Using EM. In Proc. Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Glass</author>
</authors>
<title>A Probabilistic Framework for Segment-based Speech Recognition.</title>
<date>2003</date>
<journal>Computer, Speech and Language,</journal>
<pages>17--137</pages>
<contexts>
<context position="24020" citStr="Glass, 2003" startWordPosition="3873" endWordPosition="3874">d an adapted model (L+T) that linearly interpolates trigram models trained on the Lectures (L) and Textbook (T) datasets. In all models, all interpolation weights and additional parameters are tuned on a development set consisting of the first half of the CS lectures and tested on the second half. Unless otherwise noted, modified Kneser-Ney discounting (Chen and Goodman, 1998) is applied with the respective training set vocabulary using the SRILM Toolkit (Stolcke, 2002). To compute the word error rates associated with a specific language model, we used a speaker-independent speech recognizer (Glass, 2003). The lectures were pre-segmented into utterances by forced alignment of the reference transcription. 5.1 Lecture Style In general, an n-gram model trained on a limited set of topic-specific documents tends to overemphasize words from the observed topics instead of evenly distributing weights over all potential topics. Specifically, given the list of words following an n-gram context, we would like to deemphasize the observed occurrences of topic words and ideally redistribute these counts to all potential topic words. As an approximation, we can build such a topic-deemphasized style trigram m</context>
</contexts>
<marker>Glass, 2003</marker>
<rawString>J. Glass. 2003. A Probabilistic Framework for Segment-based Speech Recognition. Computer, Speech and Language, 17:137-152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Glass</author>
<author>T J Hazen</author>
<author>L Hetherington</author>
<author>C Wang</author>
</authors>
<title>Analysis and Processing of Lecture Audio Data: Preliminary Investigations.</title>
<date>2004</date>
<booktitle>In Proc. HLTNAACL Workshop on Interdisciplinary Approaches to Speech Indexing and Retrieval,</booktitle>
<pages>9--12</pages>
<contexts>
<context position="1831" citStr="Glass et al, 2004" startWordPosition="271" endWordPosition="274">th in style and topic, becomes essential for efficient access and management of this information. As a prime example, successful language modeling for academic lectures not only enables the initial transcription via automatic speech recognition, but also assists educators and students in the creation and navigation of these materials through annotation, retrieval, summarization, and even translation of the embedded content. Compared with other types of audio content, lecture speech often exhibits a high degree of spontaneity and focuses on narrow topics with specific terminology (Furui, 2003; Glass et al, 2004). Unfortunately, training corpora available for language modeling rarely match the target lecture in both style and topic. While transcripts from other lectures better match the style of the target lecture than written text, it is often difficult to find transcripts on the target topic. On the other hand, although topic-specific vocabulary can be gleaned from related text materials, such as the textbook and lecture slides, written language is a poor predictor of how words are actually spoken. Furthermore, given that the precise topic of a target lecture is often unknown a priori and may even s</context>
<context position="14597" citStr="Glass et al., 2004" startWordPosition="2291" endWordPosition="2294">ach state and topic to yield a reasonable approximation to the underlying distribution. In the following sections, we examine the application of models derived from the HMM-LDA labels to the task of spoken lecture transcription and explore techniques on adaptive topic modeling to construct a better lecture language model. 4 HMM-LDA Analysis Our language modeling experiments have been conducted on high-fidelity transcripts of approximately 168 hours of lectures from three undergraduate subjects in math, physics, and computer science (CS), as well as 79 seminars covering a wide range of topics (Glass et al., 2004). For evaluation, we withheld the set of 20 CS lectures and used the first 10 lectures as a development set and the last 10 lectures for the test set. The remainder of these data was used for training and will be referred to as the Lectures dataset. To supplement the out-of-domain lecture transcripts with topic-specific textual resources, we added the CS course textbook (Textbook) as additional training data for learning the target topics. To create topic-cohesive documents, the textbook is divided at every section heading to form 271 documents. Next, the text is heuristically segmented at sen</context>
</contexts>
<marker>Glass, Hazen, Hetherington, Wang, 2004</marker>
<rawString>J. Glass, T.J. Hazen, L. Hetherington, and C. Wang. 2004. Analysis and Processing of Lecture Audio Data: Preliminary Investigations. In Proc. HLTNAACL Workshop on Interdisciplinary Approaches to Speech Indexing and Retrieval, 9-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>A Bit of Progress in Language Modeling (Extended Version).</title>
<date>2001</date>
<tech>Technical Report, Microsoft Research.</tech>
<contexts>
<context position="5094" citStr="Goodman (2001)" startWordPosition="779" endWordPosition="780">modeling techniques in spoken lecture and other content processing research. 2 Adaptive and Topic-Mixture LMs The concept of adaptive and topic-mixture language models has been previously explored by many researchers. Adaptive language modeling exploits the property that words appearing earlier in a document are likely to appear again. Cache language models (Kuhn and De Mori, 1990; Clarkson and Robinson, 1997) leverage this observation and increase the probability of previously observed words in a document when predicting the next word. By interpolating with a conditional trigram cache model, Goodman (2001) demonstrated up to 34% decrease in perplexity over a trigram baseline for small training sets. The cache intuition has been extended by attempting to increase the probability of unobserved but topically related words. Specifically, given a mixture model with topic-specific components, we can increase the mixture weights of the topics corresponding to previously observed words to better predict the next word. Some of the early work in this area used a maximum entropy language model framework to trigger increases in likelihood of related words (Lau et al., 1993; Rosenfeld, 1996). A variety of m</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>J. Goodman. 2001. A Bit of Progress in Language Modeling (Extended Version). Technical Report, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding Scientific Topics.</title>
<date>2004</date>
<booktitle>In Proc. National Academy of Science,</booktitle>
<volume>101</volume>
<pages>1--5228</pages>
<contexts>
<context position="13106" citStr="Griffiths and Steyvers, 2004" startWordPosition="2052" endWordPosition="2056">orpus into a set of topic word distributions. This form of soft, context-dependent classificaFor each document d in the corpus: 1. Draw topic weights d B from Dirichlet (a) 2. For each word wi in document d: a. Draw topic zi from Multinomia l( d ) B b. Draw state si from M ultinomial ( si �1 ) z c. Draw word wi from: Multinomial( ) # zi s = i stopi c Multinomial(ysi) otherwise d z2 w2 z1 w1 zn wn ... ... ... s1 s2 sn 375 tion has many potential uses for language modeling, topic segmentation, and indexing. 3.3 Training To train an HMM-LDA model, we employ the MATLAB Topic Modeling Toolbox 1.3 (Griffiths and Steyvers, 2004; Griffiths et al., 2004). This particular implementation performs Gibbs sampling, a form of Markov chain Monte Carlo (MCMC), to estimate the optimal model parameters fitted to the training data. Specifically, the algorithm creates a Markov chain whose stationary distribution matches the expected distribution of the state and topic labels for each word in the training corpus. Starting from random labels, Gibbs sampling sequentially samples the label for each hidden variable conditioned on the current value of all other variables. After a sufficient number of iterations, the Markov chain conver</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>T. Griffiths and M. Steyvers. 2004. Finding Scientific Topics. In Proc. National Academy of Science, 101(Suppl. 1):5228-5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Griffiths</author>
<author>M Steyvers</author>
<author>D Blei</author>
<author>J Tenenbaum</author>
</authors>
<date>2004</date>
<booktitle>Integrating Topics and Syntax. Adv. in Neural Information Processing Systems,</booktitle>
<pages>17--537</pages>
<contexts>
<context position="6840" citStr="Griffiths et al., 2004" startWordPosition="1052" endWordPosition="1055">listic LSA (PLSA) has been used to decompose documents into component word distributions and create unigram topic models from these distributions. Gildea and Hofmann (1999) demonstrated noticeable perplexity reduction via dynamic combination of these unigram topic models with a generic trigram model. To identify topics from an unlabeled corpus, (Blei et al., 2003) extends PLSA with the Latent Dirichlet Allocation (LDA) model that describes each document in a corpus as generated from a mixture of topics, each characterized by a word unigram distribution. Hidden Markov Model with LDA (HMM-LDA) (Griffiths et al., 2004) further extends this topic mixture model to separate syntactic words from content words whose distributions depend primarily on local context and document topic, respectively. In the specific area of lecture processing, previous work in language model adaptation has primarily focused on customizing a fixed n-gram language model for each lecture by combining ngram statistics from general conversational speech, other lectures, textbooks, and other resources related to the target lecture (Nanjo and Kawahara, 2002, 2004; Leeuwis et al., 2003; Park et al., 2005). Most of the previous work on topic</context>
<context position="11070" citStr="Griffiths et al. (2004)" startWordPosition="1710" endWordPosition="1713">experiments (Blei et al., 2003). Various extensions to the basic LDA model have since been proposed. The Author Topic model adds an additional dependency on the author(s) to the topic mixture weights of each document (Rosen-Zvi et al., 2005). The Hierarchical Dirichlet Process is a nonparametric model that generalizes distribution parameter modeling to multiple levels. Without having to estimate the number of mixture components, this model has been shown to match the best result from LDA on a document modeling task (Teh et al., 2004). 3.2 Hidden Markov Model with LDA HMM-LDA model proposed by Griffiths et al. (2004) combines the HMM and LDA models to separate syntactic words with local dependencies from topic-dependent content words without requiring any labeled data. Similar to HMM-based part-of-speech taggers, HMM-LDA maps each word in the document to a hidden syntactic state. Each state generates words according to a unigram distribution except the special topic state, where words are modeled by document-specific mixtures of topic distributions, as in LDA. Figure 1 describes this generative process in more detail. Figure 1: Generative framework and graphical model representation of HMM-LDA. The number</context>
<context position="13131" citStr="Griffiths et al., 2004" startWordPosition="2057" endWordPosition="2060"> distributions. This form of soft, context-dependent classificaFor each document d in the corpus: 1. Draw topic weights d B from Dirichlet (a) 2. For each word wi in document d: a. Draw topic zi from Multinomia l( d ) B b. Draw state si from M ultinomial ( si �1 ) z c. Draw word wi from: Multinomial( ) # zi s = i stopi c Multinomial(ysi) otherwise d z2 w2 z1 w1 zn wn ... ... ... s1 s2 sn 375 tion has many potential uses for language modeling, topic segmentation, and indexing. 3.3 Training To train an HMM-LDA model, we employ the MATLAB Topic Modeling Toolbox 1.3 (Griffiths and Steyvers, 2004; Griffiths et al., 2004). This particular implementation performs Gibbs sampling, a form of Markov chain Monte Carlo (MCMC), to estimate the optimal model parameters fitted to the training data. Specifically, the algorithm creates a Markov chain whose stationary distribution matches the expected distribution of the state and topic labels for each word in the training corpus. Starting from random labels, Gibbs sampling sequentially samples the label for each hidden variable conditioned on the current value of all other variables. After a sufficient number of iterations, the Markov chain converges to the stationary dis</context>
<context position="22884" citStr="Griffiths et al., 2004" startWordPosition="3693" endWordPosition="3696">en say he where had &lt;uh&gt; me two point here five able why just than physically got your by okay make I&apos;ll thank comes we&apos;re about any case course d coming which me important that&apos;ll need those about well look people which means also here this idea who years talking as should long anybody&apos;s try their as but take I&apos;d is says you&apos;re all another problem they four trying because may as with take Figure 4: The top 10 words from the 19 syntactic states computed from the Lectures dataset. 377 practical. As an alternative for future work, we would like to perform Gibbs sampling on the hyper-parameters (Griffiths et al., 2004) and apply the Dirichlet process to estimate the number of states and topics (Teh et al., 2004). Despite the suboptimal choice of parameters and potential lack of convergence, the labels derived from HMM-LDA are still effective for language modeling applications, as described next. 5 Language Modeling Experiments To evaluate the effectiveness of models derived from the separation of syntax from content, we performed experiments that compare the perplexities and WERs of various model combinations. For a baseline, we used an adapted model (L+T) that linearly interpolates trigram models trained o</context>
</contexts>
<marker>Griffiths, Steyvers, Blei, Tenenbaum, 2004</marker>
<rawString>T. Griffiths, M. Steyvers, D. Blei, and J. Tenenbaum. 2004. Integrating Topics and Syntax. Adv. in Neural Information Processing Systems, 17:537-544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Iyer</author>
<author>M Ostendorf</author>
</authors>
<title>Modeling Long Distance Dependence in Language: Topic Mixtures Versus Dynamic Cache.</title>
<date>1999</date>
<booktitle>In IEEE Transactions on Speech and Audio Processing,</booktitle>
<pages>7--30</pages>
<contexts>
<context position="5849" citStr="Iyer and Ostendorf, 1999" startWordPosition="901" endWordPosition="904">ded by attempting to increase the probability of unobserved but topically related words. Specifically, given a mixture model with topic-specific components, we can increase the mixture weights of the topics corresponding to previously observed words to better predict the next word. Some of the early work in this area used a maximum entropy language model framework to trigger increases in likelihood of related words (Lau et al., 1993; Rosenfeld, 1996). A variety of methods has been used to explore topic-mixture models. To model a mixture of topics within a document, the sentence mixture model (Iyer and Ostendorf, 1999) builds multiple topic models from clusters of training sentences and defines the probability of a target sentence as a weighted combination of its probability under each topic model. Latent Semantic Analysis (LSA) has been used to cluster topically related words and has demonstrated significant reduction in perplexity and word error rate (Bellegarda, 2000). Probabilistic LSA (PLSA) has been used to decompose documents into component word distributions and create unigram topic models from these distributions. Gildea and Hofmann (1999) demonstrated noticeable perplexity reduction via dynamic co</context>
</contexts>
<marker>Iyer, Ostendorf, 1999</marker>
<rawString>R. Iyer and M. Ostendorf. 1999. Modeling Long Distance Dependence in Language: Topic Mixtures Versus Dynamic Cache. In IEEE Transactions on Speech and Audio Processing, 7:30-39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kuhn</author>
<author>R De Mori</author>
</authors>
<title>A Cache-Based Natural Language Model for Speech Recognition.</title>
<date>1990</date>
<booktitle>In IEEE Transactions on Pattern Analysis and Machine Intelligence,</booktitle>
<pages>12--570</pages>
<marker>Kuhn, De Mori, 1990</marker>
<rawString>R. Kuhn and R. De Mori. 1990. A Cache-Based Natural Language Model for Speech Recognition. In IEEE Transactions on Pattern Analysis and Machine Intelligence, 12:570-583.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Lau</author>
<author>R Rosenfeld</author>
<author>S Roukos</author>
</authors>
<title>TriggerBased Language Models: a Maximum Entropy Approach. In</title>
<date>1993</date>
<booktitle>Proc. ICASSP.</booktitle>
<contexts>
<context position="5660" citStr="Lau et al., 1993" startWordPosition="871" endWordPosition="874"> a conditional trigram cache model, Goodman (2001) demonstrated up to 34% decrease in perplexity over a trigram baseline for small training sets. The cache intuition has been extended by attempting to increase the probability of unobserved but topically related words. Specifically, given a mixture model with topic-specific components, we can increase the mixture weights of the topics corresponding to previously observed words to better predict the next word. Some of the early work in this area used a maximum entropy language model framework to trigger increases in likelihood of related words (Lau et al., 1993; Rosenfeld, 1996). A variety of methods has been used to explore topic-mixture models. To model a mixture of topics within a document, the sentence mixture model (Iyer and Ostendorf, 1999) builds multiple topic models from clusters of training sentences and defines the probability of a target sentence as a weighted combination of its probability under each topic model. Latent Semantic Analysis (LSA) has been used to cluster topically related words and has demonstrated significant reduction in perplexity and word error rate (Bellegarda, 2000). Probabilistic LSA (PLSA) has been used to decompos</context>
</contexts>
<marker>Lau, Rosenfeld, Roukos, 1993</marker>
<rawString>R. Lau, R. Rosenfeld, S. Roukos. 1993. TriggerBased Language Models: a Maximum Entropy Approach. In Proc. ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Leeuwis</author>
<author>M Federico</author>
<author>M Cettolo</author>
</authors>
<title>Language Modeling and Transcription of the TED Corpus Lectures. In</title>
<date>2003</date>
<booktitle>Proc. ICASSP.</booktitle>
<contexts>
<context position="7384" citStr="Leeuwis et al., 2003" startWordPosition="1134" endWordPosition="1137">istribution. Hidden Markov Model with LDA (HMM-LDA) (Griffiths et al., 2004) further extends this topic mixture model to separate syntactic words from content words whose distributions depend primarily on local context and document topic, respectively. In the specific area of lecture processing, previous work in language model adaptation has primarily focused on customizing a fixed n-gram language model for each lecture by combining ngram statistics from general conversational speech, other lectures, textbooks, and other resources related to the target lecture (Nanjo and Kawahara, 2002, 2004; Leeuwis et al., 2003; Park et al., 2005). Most of the previous work on topic-mixture models focuses on in-domain adaptation using large amounts of matched training data. However, most, if not all, of the data available to train a lecture language model are either cross-domain or cross-style. Furthermore, although adaptive models have been shown to yield significant perplexity reduction on clean transcripts, the improvements tend to diminish when working with speech recognizer hypotheses with high WER. In this work, we apply the concept of dynamic topic adaptation to the lecture transcription task. Unlike previous</context>
</contexts>
<marker>Leeuwis, Federico, Cettolo, 2003</marker>
<rawString>E. Leeuwis, M. Federico, and M. Cettolo. 2003. Language Modeling and Transcription of the TED Corpus Lectures. In Proc. ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Nanjo</author>
<author>T Kawahara</author>
</authors>
<title>Unsupervised Language Model Adaptation for Lecture Speech Recognition. In</title>
<date>2002</date>
<booktitle>Proc. ICSLP.</booktitle>
<contexts>
<context position="7356" citStr="Nanjo and Kawahara, 2002" startWordPosition="1129" endWordPosition="1132">haracterized by a word unigram distribution. Hidden Markov Model with LDA (HMM-LDA) (Griffiths et al., 2004) further extends this topic mixture model to separate syntactic words from content words whose distributions depend primarily on local context and document topic, respectively. In the specific area of lecture processing, previous work in language model adaptation has primarily focused on customizing a fixed n-gram language model for each lecture by combining ngram statistics from general conversational speech, other lectures, textbooks, and other resources related to the target lecture (Nanjo and Kawahara, 2002, 2004; Leeuwis et al., 2003; Park et al., 2005). Most of the previous work on topic-mixture models focuses on in-domain adaptation using large amounts of matched training data. However, most, if not all, of the data available to train a lecture language model are either cross-domain or cross-style. Furthermore, although adaptive models have been shown to yield significant perplexity reduction on clean transcripts, the improvements tend to diminish when working with speech recognizer hypotheses with high WER. In this work, we apply the concept of dynamic topic adaptation to the lecture transcr</context>
</contexts>
<marker>Nanjo, Kawahara, 2002</marker>
<rawString>H. Nanjo and T. Kawahara. 2002. Unsupervised Language Model Adaptation for Lecture Speech Recognition. In Proc. ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Nanjo</author>
<author>T Kawahara</author>
</authors>
<title>Language Model and Speaking Rate Adaptation for Spontaneous Presentation Speech Recognition.</title>
<date>2004</date>
<booktitle>In IEEE Trans. SAP,</booktitle>
<pages>12--4</pages>
<marker>Nanjo, Kawahara, 2004</marker>
<rawString>H. Nanjo and T. Kawahara. 2004. Language Model and Speaking Rate Adaptation for Spontaneous Presentation Speech Recognition. In IEEE Trans. SAP, 12(4):391-400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Park</author>
<author>T Hazen</author>
<author>J Glass</author>
</authors>
<title>Automatic Processing of Audio Lectures for Information Retrieval: Vocabulary Selection and Language Modeling. In</title>
<date>2005</date>
<booktitle>Proc. ICASSP.</booktitle>
<contexts>
<context position="7404" citStr="Park et al., 2005" startWordPosition="1138" endWordPosition="1141">rkov Model with LDA (HMM-LDA) (Griffiths et al., 2004) further extends this topic mixture model to separate syntactic words from content words whose distributions depend primarily on local context and document topic, respectively. In the specific area of lecture processing, previous work in language model adaptation has primarily focused on customizing a fixed n-gram language model for each lecture by combining ngram statistics from general conversational speech, other lectures, textbooks, and other resources related to the target lecture (Nanjo and Kawahara, 2002, 2004; Leeuwis et al., 2003; Park et al., 2005). Most of the previous work on topic-mixture models focuses on in-domain adaptation using large amounts of matched training data. However, most, if not all, of the data available to train a lecture language model are either cross-domain or cross-style. Furthermore, although adaptive models have been shown to yield significant perplexity reduction on clean transcripts, the improvements tend to diminish when working with speech recognizer hypotheses with high WER. In this work, we apply the concept of dynamic topic adaptation to the lecture transcription task. Unlike previous work, we first cons</context>
</contexts>
<marker>Park, Hazen, Glass, 2005</marker>
<rawString>A. Park, T. Hazen, and J. Glass. 2005. Automatic Processing of Audio Lectures for Information Retrieval: Vocabulary Selection and Language Modeling. In Proc. ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rosen-Zvi</author>
<author>T Griffiths</author>
<author>M Steyvers</author>
<author>P Smyth</author>
</authors>
<title>The Author-Topic Model for Authors and Documents.</title>
<date>2004</date>
<booktitle>20th Conference on Uncertainty in Artificial Intelligence.</booktitle>
<marker>Rosen-Zvi, Griffiths, Steyvers, Smyth, 2004</marker>
<rawString>M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth. 2004. The Author-Topic Model for Authors and Documents. 20th Conference on Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>A Maximum Entropy Approach to Adaptive Statistical Language Modeling.</title>
<date>1996</date>
<journal>Computer, Speech and Language,</journal>
<pages>10--187</pages>
<contexts>
<context position="5678" citStr="Rosenfeld, 1996" startWordPosition="875" endWordPosition="876">gram cache model, Goodman (2001) demonstrated up to 34% decrease in perplexity over a trigram baseline for small training sets. The cache intuition has been extended by attempting to increase the probability of unobserved but topically related words. Specifically, given a mixture model with topic-specific components, we can increase the mixture weights of the topics corresponding to previously observed words to better predict the next word. Some of the early work in this area used a maximum entropy language model framework to trigger increases in likelihood of related words (Lau et al., 1993; Rosenfeld, 1996). A variety of methods has been used to explore topic-mixture models. To model a mixture of topics within a document, the sentence mixture model (Iyer and Ostendorf, 1999) builds multiple topic models from clusters of training sentences and defines the probability of a target sentence as a weighted combination of its probability under each topic model. Latent Semantic Analysis (LSA) has been used to cluster topically related words and has demonstrated significant reduction in perplexity and word error rate (Bellegarda, 2000). Probabilistic LSA (PLSA) has been used to decompose documents into c</context>
</contexts>
<marker>Rosenfeld, 1996</marker>
<rawString>R. Rosenfeld. 1996. A Maximum Entropy Approach to Adaptive Statistical Language Modeling. Computer, Speech and Language, 10:187-228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM – An Extensible Language Modeling Toolkit. In</title>
<date>2002</date>
<booktitle>Proc. ICSLP.</booktitle>
<contexts>
<context position="23882" citStr="Stolcke, 2002" startWordPosition="3853" endWordPosition="3854">f syntax from content, we performed experiments that compare the perplexities and WERs of various model combinations. For a baseline, we used an adapted model (L+T) that linearly interpolates trigram models trained on the Lectures (L) and Textbook (T) datasets. In all models, all interpolation weights and additional parameters are tuned on a development set consisting of the first half of the CS lectures and tested on the second half. Unless otherwise noted, modified Kneser-Ney discounting (Chen and Goodman, 1998) is applied with the respective training set vocabulary using the SRILM Toolkit (Stolcke, 2002). To compute the word error rates associated with a specific language model, we used a speaker-independent speech recognizer (Glass, 2003). The lectures were pre-segmented into utterances by forced alignment of the reference transcription. 5.1 Lecture Style In general, an n-gram model trained on a limited set of topic-specific documents tends to overemphasize words from the observed topics instead of evenly distributing weights over all potential topics. Specifically, given the list of words following an n-gram context, we would like to deemphasize the observed occurrences of topic words and i</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM – An Extensible Language Modeling Toolkit. In Proc. ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Teh</author>
<author>M Jordan</author>
<author>M Beal</author>
<author>D Blei</author>
</authors>
<title>Hierarchical Dirichlet Processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association.</journal>
<note>To appear in</note>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Y. Teh, M. Jordan, M. Beal, and D. Blei. 2006. Hierarchical Dirichlet Processes. To appear in Journal of the American Statistical Association.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>