<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000318">
<title confidence="0.9991465">
Revisiting the impact of different annotation schemes on PCFG parsing:
A grammatical dependency evaluation
</title>
<author confidence="0.98723">
Adriane Boyd
</author>
<affiliation confidence="0.9940965">
Department of Linguistics
The Ohio State University
</affiliation>
<address confidence="0.6685915">
1712 Neil Avenue
Columbus, Ohio 43210, USA
</address>
<email confidence="0.997715">
adriane@ling.osu.edu
</email>
<author confidence="0.644064">
Detmar Meurers
</author>
<affiliation confidence="0.531061">
Seminar f¨ur Sprachwissenschaft
</affiliation>
<address confidence="0.799612333333333">
Universit¨at T¨ubingen
Wilhelmstrasse 19
72074 T¨ubingen, Germany
</address>
<email confidence="0.997871">
dm@sfs.uni-tuebingen.de
</email>
<sectionHeader confidence="0.99562" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999792545454546">
Recent parsing research has started addressing
the questions a) how parsers trained on differ-
ent syntactic resources differ in their perfor-
mance and b) how to conduct a meaningful
evaluation of the parsing results across such
a range of syntactic representations. Two Ger-
man treebanks, Negra and T¨uBa-D/Z, consti-
tute an interesting testing ground for such re-
search given that the two treebanks make very
different representational choices for this lan-
guage, which also is of general interest given
that German is situated between the extremes
of fixed and free word order. We show that
previous work comparing PCFG parsing with
these two treebanks employed PARSEVAL
and grammatical function comparisons which
were skewed by differences between the two
corpus annotation schemes. Focusing on the
grammatical dependency triples as an essen-
tial dimension of comparison, we show that
the two very distinct corpora result in compa-
rable parsing performance.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996622363636364">
Syntactically annotated corpora have been produced
for a range of languages and they differ significantly
regarding which language properties are encoded
and how they are represented. Between the two ex-
tremes of constituency treebanks for English and de-
pendency treebanks for free word order languages
such as Czech lie languages such as German, for
which two different treebanks have explored differ-
ent options for encoding topology and dependency,
Negra (Brants et al., 1999) and T¨uBa-D/Z (Telljo-
hann et al., 2005).
</bodyText>
<page confidence="0.980361">
24
</page>
<bodyText confidence="0.999773826086957">
Recent research has started addressing the ques-
tion of how parsers trained on these different syntac-
tic resources differ in their performance. Such work
must also address the question of how to conduct a
meaningful evaluation of the parsing results across
such a range of syntactic representations. In this pa-
per, we show that previous work comparing PCFG
parsing for the two German treebanks used represen-
tations which cannot adequately be compared using
the given PARSEVAL measures and that a grammat-
ical dependency evaluation is more meaningful than
the grammatical function evaluation provided.
We present the first comparison of Negra and
T¨uBa-D/Z using a labeled dependency evaluation
based on the grammatical function labels provided
in the corpora. We show that, in contrast to previ-
ous literature, a labeled dependency evaluation es-
tablishes that PCFG parsers trained on the two cor-
pora give similar parsing performance. The focus on
labeled dependencies also provides a direct link to
recent work on dependency-based evaluation (e.g.,
Clark and Curran, 2007) and dependency parsing
(e.g., CoNLL shared tasks 2006, 2007).
</bodyText>
<subsectionHeader confidence="0.87975">
1.1 Previous work
</subsectionHeader>
<bodyText confidence="0.999811333333333">
The question of how to evaluate parser output has
naturally already arisen in earlier work on parsing
English. As discussed by Lin (1995) and others, the
PARSEVAL evaluation typically used to analyze the
performance of statistical parsing models has many
drawbacks. Bracketing evaluation may count a sin-
gle error multiple times and does not differentiate
between errors that significantly affect the interpre-
tation of the sentence and those that are less crucial.
</bodyText>
<note confidence="0.7328525">
Proceedings of the ACL-08: HLT Workshop on Parsing German (PaGe-08), pages 24–32,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999795447368421">
It also does not allow for evaluation of particular
syntactic structures or provide meaningful informa-
tion about where the parser is failing. In addition,
and most directly relevant for this paper, PARSE-
VAL scores are difficult to compare across syntactic
annotation schemes (Carroll et al., 2003).
At the same time, previous research on PCFG
parsing using treebank training data present PAR-
SEVAL measures in comparing the parsing per-
formance for different languages and annotation
schemes, reporting a number of striking differences.
For example, Levy and Manning (2003), K¨ubler
(2005), and K¨ubler et al. (2006) highlight the sig-
nificant effect of language properties and annotation
schemes for German and Chinese treebanks. In re-
lated work, parser enhancements that provide a sig-
nificant performance boost for English, such as head
lexicalization, are reported not to provide the same
kind of improvement, if any, for German (Dubey and
Keller, 2003; Dubey, 2004; K¨ubler et al., 2006).
Previous work has compared the similar Negra
and Tiger corpora of German to the very different
T¨uBa-D/Z corpus. K¨ubler et al. (2006) compares
the Negra and T¨uBa-D/Z corpora of German using
a PARSEVAL evaluation and an evaluation on core
grammatical function labels that is included to ad-
dress concerns about the PARSEVAL measure.1 Us-
ing the Stanford Parser (Klein and Manning, 2002),
which employs a factored PCFG and dependency
model, they claim that the model trained on T¨uBa-
D/Z consistently outperforms that trained on Ne-
gra in PARSEVAL and grammatical function evalu-
ations. Dubey (2004) also includes an evaluation on
grammatical function for statistical models trained
on Negra, but obtains very different results from
K¨ubler et al. (2006).2
In recent related work, Rehbein and van Genabith
(2007a) demonstrate using the Tiger and T¨uBa-D/Z
</bodyText>
<footnote confidence="0.980116666666667">
1The evaluation is based only on the grammatical function;
it does not identify the dependency pair that it labels.
2While the focus of K¨ubler et al. (2006) is on comparing
</footnote>
<bodyText confidence="0.979878171428571">
parsing results across corpora, Dubey (2004) focuses on im-
proving parsing for Negra, including corpus-specific enhance-
ments leading to better results. This difference in focus and
additional differences in experimental setup mean that a fine-
grained comparison of the results is inappropriate – the rele-
vant point here is that the gap between the results (23% for sub-
jects, 35% for accusative objects) warrants further attention in
the context of comparing parsing results across corpora.
corpora of German that PARSEVAL is inappropri-
ate for comparisons of the output of PCFG parsers
trained on different treebank annotation schemes be-
cause PARSEVAL scores are affected by the ratio
of terminal to non-terminal nodes. A dependency-
based evaluation on triples of the form word-POS-
head shows better results for the parser trained
on Tiger even though the much lower PARSEVAL
scores, if meaningful, would predict that the out-
put for Tiger is of lower quality. However, their
dependency-based evaluation does not make use
of the grammatical function labels, which are pro-
vided in the corpora and closely correspond to the
representations used in recent work on formalism-
independent evaluation of parsers (e.g., Clark and
Curran, 2007).3
Addressing these issues, we resolve the apparent
discrepancy between K¨ubler et al. (2006) and Dubey
(2004) and establish a firm grammatical function
comparison of Negra and T¨uBa-D/Z. We also ex-
tend the evaluation to a labeled dependency evalu-
ation based on grammatical relations for both cor-
pora. Such an evaluation, which abstracts away from
the specifics of the annotation schemes, shows that,
in contrast to the claims made in K¨ubler et al. (2006),
the parsing results for PCFG parsers trained on these
heterogeneous corpora are very similar.
</bodyText>
<sectionHeader confidence="0.687331" genericHeader="method">
2 The corpora used
</sectionHeader>
<bodyText confidence="0.99999">
As motivated in the introduction, the work discussed
in this paper is based on two German corpora, Ne-
gra and T¨uBa-D/Z, which differ significantly in the
syntactic representations used – thereby offering an
interesting test bed for investigating the influence of
an annotation scheme on the parsers trained.
</bodyText>
<subsectionHeader confidence="0.954096">
2.1 Negra
</subsectionHeader>
<bodyText confidence="0.99945075">
The Negra corpus (Brants et al., 1999) consists of
newspaper text from the Frankfurter Rundschau, a
German newspaper. Version 2 of the corpus contains
20,602 sentences. It uses the STTS tag set (Schiller
et al., 1995) for part-of-speech annotation. There are
25 non-terminal node labels and 46 edge labels.
The syntactic annotation of Negra combines fea-
tures from phrase structure grammar and depen-
</bodyText>
<footnote confidence="0.999314333333333">
3Their evaluation also introduces an additional level of com-
plexity by finding heads heuristically rather than relying on the
head labels present on some elements in each corpus.
</footnote>
<page confidence="0.999245">
25
</page>
<bodyText confidence="0.9653015">
dency grammar using a tree-like syntactic structure
with grammatical functions labeled on the edges of
the tree. Flat sentence structures are used in many
places to avoid attachment ambiguities and non-
branching phrases are not used.
The annotation scheme emphasizes the use of the
tree structure to encode grammatical dependencies,
representing a head and all its dependents within a
local tree regardless of whether a dependent is real-
ized near its head or not, e.g., because it has been
extraposed or fronted. Since traditional syntax trees
do not permit the crossing branches needed to li-
cense discontinuous constituents, Negra uses a “syn-
tax graph” data structure to represent the annotation.
An example of a syntax graph with a discontinuous
constituent (VP) due to a fronted dative object (NP)
is shown in Figure 1.
this opinion can I only completely agree
</bodyText>
<figureCaption confidence="0.988114">
Figure 1: Negra tree for ‘I can only agree with this opin-
ion completely.’
</figureCaption>
<bodyText confidence="0.999940703703704">
Negra uses flat NP and PP annotation with no
marked heads. For example, both Dieser and Mein-
ung in Figure 1 have the grammatical function label
“NK”. Since unary branching is not used in Negra, a
bare noun or pronoun argument is not dominated by
an NP node, as shown by the pronoun ich above.
A verbal head in Negra is always marked with the
edge label “HD” and its arguments are its sisters in
the local tree. The subject is always the sister of the
finite verb, which is a daughter of S. If the finite verb
is the main verb in the clause, the objects are also its
sisters, i.e., the finite verb, subject and objects are
all daughters of S. If the main verb is an auxiliary
governing a non-finite main verb, the non-finite verb
and its objects and modifiers form a VP where the
objects are sisters of the non-finite verb as in Fig-
ure 1. The VP is then a sister of the finite verb.
The finite verb in a German declarative clause ap-
pears in the so-called verb-second position, immedi-
ately following the fronted constituent. As a result,
the VP in Negra is discontinuous whenever one of
its children has been fronted, as in the common word
orders exemplified in (1a) and (1b).
The sentence we saw in Figure 1 contains a dis-
continuous VP with a fronted dative object (Dieser
Meinung). The dative object and a modifier (voll)
form a VP with the non-finite verb (zustimmen).
</bodyText>
<subsectionHeader confidence="0.997733">
2.2 T¨uBa-D/Z
</subsectionHeader>
<bodyText confidence="0.999966222222222">
The T¨uBa-D/Z corpus, version 2, (Telljohann et al.,
2005) consists of 22,091 sentences of newspaper
text from the German newspaper die tageszeitung.
Like Negra, it uses the STTS tag set (Schiller et al.,
1995) for part-of-speech annotation. Syntactically it
uses 27 non-terminal node labels and 47 edge labels.
The syntactic annotation incorporates a topologi-
cal field analysis of the German clause (Reis, 1980;
H¨ohle, 1986), which segments a sentence into topo-
logical units depending on the position of the finite
verb (verb-first, verb-second, verb-last). In a verb-
first and verb-second sentence, the finite verb is the
left bracket (LK), whereas in a verb-last subordinate
clause, the subordinating conjunction occupies that
field. In all clauses, the non-finite verb cluster forms
the right bracket (VC), and arguments and modifiers
can appear in the middle field (MF) between the two
brackets. Extraposed material is found to the right
of the right bracket, and in a verb-second sentence
one constituent appears in the fronted field (VF) pre-
ceding the finite verb. By specifying constraints on
the elements that can occur in the different fields,
the word order in any type of German clause can be
concisely characterized.
Each clause in the T¨uBa-D/Z corpus is divided
into topological fields at the top level, and each topo-
logical field contains phrase-level annotation. An
</bodyText>
<figure confidence="0.980289708333334">
Dieser Meinung
PDAT NN
kann ich nur
VMFIN PPER ADV
HD SB
S
VP
OC
MO
voll zustimmen
ADJD VVINF
.
$.
VROOT
NK NK
NP
DA
MO HD
(1) a. Die T¨ur hat Anna wieder zugeschlagen.
the door has Anna again slammed-shut
‘Anna slammed the door shut again.’
b. Wieder hat Anna die T¨ur zugeschlagen.
again has Anna the door slammed-shut
‘Anna slammed the door shut again.’
</figure>
<page confidence="0.980635">
26
</page>
<bodyText confidence="0.91390525">
example sentence from T¨uBa-D/Z is shown in Fig-
ure 2, where the topological fields VF, LK, MF, and
VC are visible under the SIMPX clause node.
for it will Andrea Fischer little time have
</bodyText>
<figureCaption confidence="0.947844">
Figure 2: T¨uBa-D/Z tree for ‘Andrea Fischer will have
little time for it.’
</figureCaption>
<bodyText confidence="0.999975">
Edge labels are used to mark heads and gram-
matical functions, even though it can be nontrivial
to figure out which grammatical function belongs
to which head given that heads and their arguments
often are in separate topological fields. For exam-
ple, in Figure 2 the subject noun chunk (NX) has
the edge label ON (object - nominative) and the ob-
ject noun chunk has the edge label OA (object - ac-
cusative); both are realized within the middle field
(MF), while the finite verb (VXFIN) marked as HD
(head) is in the left sentence bracket (LK). This is-
sue becomes relevant in section 3.4.2, discussing an
evaluation based on labeled dependency triples.
Where Negra uses discontinuous constituents,
T¨uBa-D/Z uses special edge labels to annotate gram-
matical relations which are not locally realized. For
example, the fronted prepositional phrase (PX) in
Figure 2 has the edge label OA-MOD which needs
to be matched with the noun phrase (NX) with label
OA that is found in the MF field.
</bodyText>
<subsectionHeader confidence="0.999973">
2.3 Comparing Negra and T¨uBa-D/Z
</subsectionHeader>
<bodyText confidence="0.999303">
To give an impression of how the different anno-
tation schemes affect the appearance of a typical
tree in the two corpora, Table 1 provides statistics
on average sentence length and the number of non-
terminals per sentence.
</bodyText>
<table confidence="0.99832875">
Negra T¨uBa-D/Z
No. of Sentences 20,602 22,091
Terminals/Sentence 17.2 17.3
Non-terminals/Sentence 7.0 20.7
</table>
<tableCaption confidence="0.999885">
Table 1: General Characteristics of the Corpora
</tableCaption>
<bodyText confidence="0.999162375">
While the sentences in Negra and T¨uBa-D/Z on
average have the same number of words, the average
T¨uBa-D/Z sentence has nearly three times as many
non-terminal nodes as the average Negra sentence.
This difference is mainly due to the extra level of
topological fields annotation and the use of more
contoured structures in many places where Negra
uses flatter structures.
</bodyText>
<sectionHeader confidence="0.999804" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.99912625">
The goal of the following experiments is a compar-
ison of parsing performance across different types
of evaluation metrics for parsers trained on Negra
(Ver. 2) and T¨uBa-D/Z (Ver. 2).
</bodyText>
<subsectionHeader confidence="0.996677">
3.1 Data Preparation
</subsectionHeader>
<bodyText confidence="0.999780045454545">
Following K¨ubler et al. (2006), only sentences with
fewer than 35 words were used, which results in
20,002 sentences for Negra and 21,365 sentences
for T¨uBa-D/Z. Because punctuation is not attached
within the sentence in the corpus annotation, punc-
tuation was removed.
To be able to train PCFG parsing models, it is nec-
essary to convert the syntax graphs encoding trees
with discontinuities in Negra into traditional syntax
trees. Around 30% of sentences in Negra contain at
least one discontinuity. To remove discontinuities,
we used the conversion program included with the
Negra corpus annotation tools (Brants and Plaehn,
2000), the same tool used in K¨ubler et al. (2006),
which raises non-head elements to a higher tree un-
til there are no more discontinuities. For example,
for the discontinuous tree with a fronted object we
saw in Figure 1, the PP containing the fronted NP
Dieser Meinung is raised to become a daughter of
the top S node.4
Additionally, the edge labels used in both corpora
need to be folded into the node labels to become a
</bodyText>
<footnote confidence="0.966283">
4An alternate method that avoids certain problems with this
raising method is discussed in Boyd (2007).
</footnote>
<figure confidence="0.998230128205128">
OA-MOD
VF
PX
HD
- - - -
VXFIN
LK
HD
HD
- -
EN-ADD
NX
ON OA
-
SIMPX
MF
- HD
NX
VXINF
VC
OV
HD
Dafür
PROP
wird
VAFIN
Andrea
NE
Fischer
NE
wenig
PIAT
Zeit
NN
haben
VAINF
.
$.
VROOT
</figure>
<page confidence="0.990291">
27
</page>
<bodyText confidence="0.999954205128205">
part of context-free grammar rules used by a PCFG
parser. In the Penn Treebank-style versions of the
corpora appropriate for training a PCFG parser, each
edge label is joined with the phrase or POS label
on the phrase or word immediately below it. Both
corpora include edge labels above all phrases and
words. However the flatter structures in Negra result
in 39 different edge labels on words while T¨uBa-D/Z
has only 5.
Unlike K¨ubler et al. (2006), which ignored edge
labels on words, we incorporate all edge labels
present in both corpora. As a consequence of this,
providing a parser with perfect lexical tags would
also provide the edge label for that word. T¨uBa-D/Z
does not annotate grammatical functions other than
HD on words, but Negra includes many grammati-
cal functions on words. Including edge labels in the
perfect lexical tags would artificially boost the re-
sults of a grammatical function evaluation for Negra
since it amounts to providing the correct grammati-
cal function for the 38% of arguments in Negra that
are single words.
To avoid this problem, we introduced non-
branching phrasal nodes into Negra to prevent the
correct grammatical function label from being pro-
vided with the perfect lexical tag in the cases
of single-word arguments, which are mostly bare
nouns and pronouns. We added phrasal nodes above
all single-word subject, accusative object, dative ob-
ject, and genitive object5 arguments, with the cate-
gory of the inserted phrase depending on the POS
tag on the word. The introduced phrasal node is
given the word’s original grammatical function la-
bel; the grammatical function label of the word itself
becomes NK for NPs and HD for APs and VPs. In
total, 14,580 nodes were inserted into Negra in this
way. T¨uBa-D/Z has non-branching phrases above all
single-word arguments, so that no such modification
was needed.6
</bodyText>
<subsectionHeader confidence="0.994554">
3.2 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.994446">
We trained unlexicalized PCFG parsing models us-
ing LoPar (Schmid, 2000). Unlexicalized models
</bodyText>
<footnote confidence="0.529427">
5Genitive objects are modified for the sake of consistency
among arguments even though there are too few genitive objects
to provide reliable results in the evaluation.
6The addition of edge labels to terminal POS labels results
in 337 lexical tags for Negra and 91 for T¨uBa-D/Z.
</footnote>
<bodyText confidence="0.994835666666667">
were used to minimize the impact of other corpus
differences on parsing. A ten-fold cross validation
was performed for all experiments.7
</bodyText>
<subsectionHeader confidence="0.991104">
3.3 PARSEVAL Evaluation
</subsectionHeader>
<bodyText confidence="0.998112">
As a reference point for comparison with previous
work, the PARSEVAL results8 are given in Table 2.
</bodyText>
<table confidence="0.998943666666667">
Negra T¨uBa-D/Z
Unlabeled Precision 78.69 89.92
Unlabeled Recall 82.29 86.48
Labeled Precision 64.08 75.36
Labeled Recall 67.01 72.47
Coverage 97.00 99.90
</table>
<tableCaption confidence="0.997488">
Table 2: PARSEVAL Evaluation
</tableCaption>
<bodyText confidence="0.999629733333333">
The parser trained on T¨uBa-D/Z performs much
better than the one trained on Negra on all labeled
and unlabeled bracketing scores. As we saw in
section 2, Negra and T¨uBa-D/Z use very different
syntactic annotation schemes, resulting in over 2.5
times as many non-terminals per sentence in T¨uBa-
D/Z as in Negra with the additional unary nodes.
As mentioned previously, Rehbein and van Genabith
(2007a) showed that PARSEVAL is affected by the
ratio of terminal to non-terminal nodes, so these re-
sults are not expected to indicate the quality of the
parses. The comparison with grammatical function
and dependency evaluations we turn to next show-
cases that PARSEVAL does not provide a meaning-
ful evaluation metric across annotation schemes.
</bodyText>
<subsectionHeader confidence="0.966615">
3.4 Dependency Evaluation
</subsectionHeader>
<bodyText confidence="0.999947714285714">
Complementing the issue of the ratio of terminals
to non-terminals raised in the last section, one can
question whether counting all brackets in the sen-
tence equally, as done by the PARSEVAL metric,
provides a good measure of how accurately the ba-
sic functor-argument structure of the sentence has
been captured in a parse. Thus, it is useful to per-
</bodyText>
<footnote confidence="0.604792625">
7Our experimental setup is designed to support a compari-
son between Negra and T¨uBa-D/Z for the three evaluation met-
rics and is intended to be comparable to the setup of K¨ubler
et al. (2006). For Negra, Dubey (2004) explores a range of pars-
ing models and the corpus preparation he uses differs from the
one discussed in this paper so that a discussion of his results is
beyond the scope of the corpus comparison in this paper.
8Scores were calculated using evalb.
</footnote>
<page confidence="0.998971">
28
</page>
<bodyText confidence="0.999731428571428">
form an evaluation based on the grammatical func-
tion labels that are important for determining the
functor-argument structure of the sentence: subjects,
accusative objects, and dative objects.9 The first
step in an evaluation of functor-argument structure
is to identify whether an argument bears the correct
grammatical function label.
</bodyText>
<subsectionHeader confidence="0.84659">
3.4.1 Grammatical Function Label Evaluation
</subsectionHeader>
<bodyText confidence="0.996932142857143">
K¨ubler et al. (2006) present the results shown in Ta-
ble 3 for the parsing performance of the unlexical-
ized model of the Stanford Parser (Klein and Man-
ning, 2002). In this grammatical function label eval-
uation, T¨uBa-D/Z outperforms Negra for subjects,
accusative objects, and dative objects based on an
evaluation of phrasal arguments.
</bodyText>
<table confidence="0.9983708">
Negra T¨uBa-D/Z
Prec Rec F Prec Rec F
Subj 52.50 58.02 55.26 66.82 75.93 72.38
Acc 35.14 36.30 35.72 43.84 47.31 45.58
Dat 8.38 3.58 5.98 24.46 9.96 17.21
</table>
<tableCaption confidence="0.89922">
Table 3: Grammatical Function Label Evaluation for
Phrasal Arguments from K¨ubler et al. (2006)
</tableCaption>
<bodyText confidence="0.999943555555556">
Note that this grammatical function label evalua-
tion is restricted to labels on phrases; grammatical
function labels on words are ignored in training and
testing. This results in an unbalanced comparison
between Negra and T¨uBa-D/Z since, as discussed
in section 2, T¨uBa-D/Z includes unary-branching
phrases above all single-word arguments whereas
Negra does not. In effect, single-word arguments
in Negra – mainly pronouns and bare nouns – are
not considered in the evaluation from K¨ubler et al.
(2006). The result is thus a comparison of multi-
word arguments in Negra to both single- and multi-
word arguments in T¨uBa-D/Z. Recall from section
3.1 that this is not a minor difference: single-word
arguments account for 38% of subjects, accusative
objects, and dative objects in Negra.
As discussed in the data preparation section, Ne-
gra was modified for our experiment so as not to
</bodyText>
<footnote confidence="0.899774">
9Genitive objects are also annotated in both corpora, but
they are too infrequent to provide meaningful results. As dis-
cussed in Rehbein and van Genabith (2007b), labels such as
subject (SB for Negra, ON for T¨uBa-D/Z) are not necessarily
comparable in all instances, but such cases are infrequent.
</footnote>
<bodyText confidence="0.998885555555555">
provide the parser with the grammatical function la-
bels for single word phrases as part of the perfect
tags provided. This evaluation handles multiple cat-
egories of arguments, not just NPs, so it focuses
solely on the grammatical function labels, ignoring
the phrasal categories. For example, in Negra an NP-
OA in a parse is considered a correct accusative ob-
ject even if the OA label in the gold standard has the
category MPN. The results are shown in Table 4.
</bodyText>
<table confidence="0.9991592">
Negra T¨uBa-D/Z
Prec Rec F Prec Rec F
Subj 69.69 69.12 69.42 65.74 72.24 68.99
Acc 48.17 50.97 49.57 41.37 46.81 44.09
Dat 20.93 15.22 18.08 21.40 11.51 16.46
</table>
<tableCaption confidence="0.999225">
Table 4: Grammatical Function Label Evaluation
</tableCaption>
<bodyText confidence="0.99992725">
In contrast to the results for NP grammatical func-
tions of K¨ubler et al. (2006) we saw in Table 3, Ne-
gra and T¨uBa-D/Z perform quite similarly overall,
with Negra slightly outperforming T¨uBa-D/Z for all
types of arguments.
These results also form a clear contrast to the
PARSEVAL results we saw in Table 2. Contrary
to the finding in K¨ubler et al. (2006), the PAR-
SEVAL evaluation does not echo the grammatical
function label evaluation. In keeping with the re-
sults from Rehbein and van Genabith (2007a), we
find that PARSEVAL is not an adequate predictor of
performance in an evaluation targeting the functor-
argument structure of the sentence for comparisons
between PCFG parsers trained on corpora with dif-
ferent annotation schemes.
</bodyText>
<subsectionHeader confidence="0.982572">
3.4.2 Labeled Dependency Triple Evaluation
</subsectionHeader>
<bodyText confidence="0.999753230769231">
While determining the grammatical function of an
element is an important part of determining the
functor-argument structure of a sentence, the other
necessary component is determining the head of
each function. To evaluate whether both the functor
and the argument have been correctly found, an eval-
uation of labeled dependency triples is needed. As
in the previous section, we focus on the grammatical
function labels for arguments of verbs. To complete
a labeled dependency triple for each argument, we
additionally need to locate the lexical verbal head.
In Negra, the head is the sister of an argu-
ment marked with the function label “HD”, however
</bodyText>
<page confidence="0.996772">
29
</page>
<bodyText confidence="0.999724114285714">
heads are only marked for a subset of the phrase cat-
egories: S, VP, AP, and AVP.10 This subset includes
the phrase categories that contain verbs and their ar-
guments, S and VP. In our experiment, the parser
finds the HD grammatical function labels with a very
high f-score: 99.5% precision and 96.5% recall. If
the sister with the label HD is a word, then that word
is the lexical head for the purposes of this depen-
dency evaluation. If the sister with the label HD is
a phrase, then a recursive search for heads within
that phrase finds a lexical head. In 3.2% of cases in
the gold standard, it is not possible to find a lexical
head for an argument. Further methods could be ap-
plied to find the remaining heads heuristically, but
we avoid the additional parameters this introduces
for this evaluation by ignoring these cases.
For T¨uBa-D/Z, finding the head is not as simple
because the verbal head and its arguments are in dif-
ferent topological fields. To create a parallel com-
parison to Negra, the finite verb from the local clause
is chosen as the head for all subjects. The (finite or
non-finite) main full verb is designated as the head
for the accusative and dative objects. It is possible
to automatically find an appropriate head verb for all
but 2.7% of subjects, accusative objects, and dative
objects.11 As with Negra, only cases where a head
verb can be found in the gold standard are consid-
ered in the evaluation.
As in the grammatical function evaluation in the
previous section, only the grammatical function la-
bel, not the phrase category is considered in the eval-
uation. The results for the labeled dependency eval-
uation are shown in Table 5. The parser trained on
Negra outperforms the one trained on T¨uBa-D/Z for
all types of arguments.
</bodyText>
<sectionHeader confidence="0.951679" genericHeader="method">
4 Discussion of Results
</sectionHeader>
<bodyText confidence="0.960311909090909">
Comparing PARSEVAL scores for a parser trained
on the Negra and the T¨uBa-D/Z corpus with a gram-
matical function and a labeled dependency evalua-
10However, some strings labeled as S and VP do not contain
a head and thus lack a daughter with a HD function label.
11The relative numbers of instances where a lexical head is
not found are comparable for Negra and T¨uBa-D/Z. Heads are
not found for approximately 4% of subjects, 1% of accusative
objects, and 1% of dative objects. These instances are fre-
quently due to elision of the verb in headlines and coordinated
clauses.
</bodyText>
<table confidence="0.9993462">
Negra T¨uBa-D/Z
Prec Rec F Prec Rec F
Subj 72.84 69.03 70.93 60.52 65.98 63.25
Acc 47.96 48.80 48.38 37.39 40.83 39.11
Dat 19.56 14.01 16.79 19.32 10.39 14.85
</table>
<tableCaption confidence="0.998648">
Table 5: Labeled Dependency Evaluation
</tableCaption>
<bodyText confidence="0.999987487179487">
tion, we confirm that the PARSEVAL scores do not
correlate with the scores in the other two evalua-
tions, which given their closeness to the semantic
functor argument structure make meaningful targets
for evaluating parsers.
Shifting the focus to the grammatical function
evaluation, we showed that a grammatical function
evaluation based on phrasal arguments as provided
by K¨ubler et al. (2006) is inadequate for compar-
ing parsers trained on the Negra and T¨uBa-D/Z cor-
pora. By introducing non-branching phrase nodes
above single-word arguments in Negra, it is possi-
ble to provide a balanced comparison for the gram-
matical function label evaluation between Negra and
T¨uBa-D/Z on both phrasal and single-word argu-
ments. The models trained on both corpora perform
very similarly in the grammatical function evalua-
tion, in contrast to the claims in K¨ubler et al. (2006).
When the grammatical function label evaluation
is extended into a labeled dependency evaluation by
finding the verbal head to complete the labeled de-
pendency triple, the parser trained on Negra outper-
forms that trained on T¨uBa-D/Z. The more signifi-
cant drop in results for T¨uBa-D/Z compared to the
grammatical function label evaluation may be due
to the fact that a verbal lexical head in T¨uBa-D/Z is
not in the same local tree as its dependents, whereas
it is in Negra. The presence of intervening topolog-
ical field nodes in T¨uBa-D/Z may make it difficult
for the parser to consistently identify the elements
of the dependency triple across several subtrees.
The Negra corpus annotation scheme makes it
simple to identify the heads of verb arguments, but
the flat NP and PP structures make it difficult to ex-
tend a labeled dependency analysis beyond verb ar-
guments. On the other hand, T¨uBa-D/Z has marked
heads in NPs and PPs, but it is not as easy to pair
verb arguments with their heads because the verbs
are in separate topological fields from their argu-
</bodyText>
<page confidence="0.993768">
30
</page>
<bodyText confidence="0.999974606060606">
ments. For a constituent-based corpus annotation
scheme to lend itself to a thorough labeled depen-
dency evaluation, heads should be marked clearly
for all phrase categories and all non-head elements
need to have marked grammatical functions.
The presence of topological field nodes in T¨uBa-
D/Z deserves more discussion in relation to a gram-
matical dependency evaluation. The corpus con-
tains two very different types of nodes in its syntac-
tic trees: nodes such as NP and PP that correspond
to constituents and nodes such as VF (Vorfeld) and
MF (Mittelfeld) that correspond to word order do-
mains. Constituents such as NP have grammatical
relations to other elements in the sentence and have
identifiable heads within them, whereas nodes en-
coding word order domains have neither.12 While
constituents and word order domains sometimes co-
incide, such as the Vorfeld normally consisting of a
single constituent, this is not the general case. For
example, the Mittelfeld often contains multiple con-
stituents which each stand in different grammatical
relations to the verb(s) in the left and right sentence
brackets (LK and VC).
Returning to the issue of finding dependencies be-
tween constituents, the intervening word order do-
main nodes can make it non-trivial to determine
these relations in T¨uBa-D/Z. For example, word or-
der domain nodes will always intervene between a
verb and its arguments. In order to have all gram-
matical dependencies directly encoded in the tree-
bank, it would be preferable for corpus annotation
schemes to ensure that a homogeneous constituency
representation can be easily obtained.
</bodyText>
<sectionHeader confidence="0.999711" genericHeader="method">
5 Future Work
</sectionHeader>
<bodyText confidence="0.938908636363636">
An evaluation on arguments of verbs is just a first
step in working towards a more complete labeled
dependency evaluation. Because Negra and T¨uBa-
D/Z do not have parallel uses of many grammatical
function labels beyond arguments of verbs, a more
detailed evaluation on more types of dependency re-
lations will require a complex dependency conver-
sion method to provide comparable results.
12While the focus in this work is on unlexicalized parsing,
this also calls into question the effect of head lexicalization for
a corpus that contains elements that by their nature are not the
types of elements that have heads.
Since previous work on head-lexicalized pars-
ing models for German has focused on PARSEVAL
evaluations, it would also be useful to perform a la-
beled dependency evaluation to determine what ef-
fect head lexicalization has on particular construc-
tions for the parsers. Because of the concerns dis-
cussed in the previous section and the difference in
which types of clauses have marked heads in Negra
and T¨uBa-D/Z, the effect of head lexicalization on
the parsing results may differ for the two corpora.
</bodyText>
<sectionHeader confidence="0.999174" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999983142857143">
Addressing the general question of how to compare
parsing results for different annotation schemes, we
revisited the comparison of PCFG parsing results for
the Negra and T¨uBa-D/Z corpora. We show that
these different annotation schemes lead to very sig-
nificant differences in PARSEVAL scores for un-
lexicalized PCFG parsing models, but grammatical
function label and labeled dependency evaluations
for arguments of verbs show that this difference does
not carry over to measures which are relevant to the
semantic functor-argument structure. In contrast to
K¨ubler et al. (2006) a grammatical function evalua-
tion on subjects, accusative objects, and dative ob-
jects establishes that Negra and T¨uBa-D/Z perform
similarly when all types of words and phrases ap-
pearing as arguments are taken into consideration. A
labeled dependency evaluation based on grammati-
cal relations, which links this work to current work
on formalism-independent parser evaluation (e.g.,
Clark and Curran, 2007), shows that the parsing per-
formance for Negra and T¨uBa-D/Z is comparable.
</bodyText>
<sectionHeader confidence="0.999278" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997963125">
Adriane Boyd, 2007. Discontinuity Revisited: An Im-
proved Conversion to Context-Free Representations.
In Proceedings of the Linguistic Annotation Workshop
(LAW). Prague, Czech Republic.
Thorsten Brants and Oliver Plaehn, 2000. Interactive
Corpus Annotation. In Proceedings of the Second In-
ternational Conference on Language Resources and
Evaluation (LREC). Athens, Greece.
Thorsten Brants, Wojciech Skut and Hans Uszkoreit,
1999. Syntactic Annotation of a German Newspaper
Corpus. In Proceedings of the ATALA Treebank Work-
shop. Paris, France.
John Carroll, Guido Minnen and Ted Briscoe, 2003.
Parser evaluation: using a grammatical relation anno-
tation scheme. In A. Abeill´e (ed.), Treebanks: Build-
ing and Using Parsed Corpora, Kluwer, Dordrecht.
</reference>
<page confidence="0.997663">
31
</page>
<reference confidence="0.999834121212121">
Stephen Clark and James Curran, 2007. Formalism-
Independent Parser Evaluation with CCG and Dep-
Bank. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics (ACL).
Prague, Czech Republic.
Amit Dubey, 2004. Statistical Parsing for German: Mod-
eling Syntactic Properties and Annotation Differences.
Ph.D. thesis, Universit¨at des Saarlandes.
Amit Dubey and Frank Keller, 2003. Probabilistic Pars-
ing Using Sister-Head Dependencies. In Proceedings
of the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL). Sapporo, Japan.
Tilman H¨ohle, 1986. Der Begriff “Mittelfeld”, An-
merkungen ¨uber die Theorie der topologischen Felder.
InAkten des Siebten Internationalen Germanistenkon-
gresses 1985. G¨ottingen, Germany.
Dan Klein and Christopher D. Manning, 2002. Fast Exact
Inference with a Factored Model for Natural Language
Parsing. In Advances in Neural Information Process-
ing Systems 15 (NIPS). Vancouver, British Columbia,
Canada.
Sandra K¨ubler, 2005. How do treebank annotation
schemes influence parsing results? Or how not to com-
pare apples and oranges. In Proceedings of the Con-
ference on RecentAdvances in Natural Language Pro-
cessing (RANLP). Borovets, Bulgaria.
Sandra K¨ubler, Erhard W. Hinrichs and Wolfgang Maier,
2006. Is it really that difficult to parse German? In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP). Sydney,
Australia.
Roger Levy and Christopher Manning, 2003. Is it harder
to parse Chinese, or the Chinese Treebank? In Pro-
ceedings of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL).
Dekang Lin, 1995. A Dependency-based Method for
Evaluating Broad-Coverage Parsers. In Proceedings
of the International Joint Conference on Artificial In-
telligence (IJCAI). Montreal, Quebec, Canada.
Ines Rehbein and Josef van Genabith, 2007a. Treebank
Annotation Schemes and Parser Evaluation for Ger-
man. In Proceedings of the Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning. Prague,
Czech Republic.
Ines Rehbein and Josef van Genabith, 2007b. Why is it so
difficult to compare treebanks? TIGER and T¨uBa-D/Z
revisited. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories (TLT). Bergen, Norway.
Marga Reis, 1980. On Justifying Topological Frames:
‘Positional Field’ and the Order of Nonverbal Con-
stituents in German. Documentation et Recherche
en Linguistique Allemande Contemporaine Vincennes
(DRLAV), 22/23.
Anne Schiller, Simone Teufel and Christine Thielen,
1995. Guidelines f¨ur das Tagging deutscher Textcor-
pora mit STTS. Technical report, Universit¨at Stuttgart,
Universit¨at T¨ubingen, Germany.
Helmut Schmid, 2000. LoPar: Design and Implemen-
tation. Arbeitspapiere des Sonderforschungsbereiches
340 No. 149, Universit¨at Stuttgart.
Heike Telljohann, Erhard W. Hinrichs, Sandra K¨ubler and
Heike Zinsmeister, 2005. Stylebook for the T¨ubingen
Treebank of Written German (T¨uBa-D/Z). Technical
report, Seminar f¨ur Sprachwissenschaft, Universit¨at
T¨ubingen, Germany.
</reference>
<page confidence="0.999298">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.225273">
<title confidence="0.9984955">Revisiting the impact of different annotation schemes on PCFG A grammatical dependency evaluation</title>
<author confidence="0.974317">Adriane</author>
<affiliation confidence="0.969138">Department of The Ohio State</affiliation>
<address confidence="0.9958255">1712 Neil Columbus, Ohio 43210,</address>
<email confidence="0.998701">adriane@ling.osu.edu</email>
<note confidence="0.5334154">Detmar Seminar f¨ur Universit¨at Wilhelmstrasse 72074 T¨ubingen,</note>
<email confidence="0.993849">dm@sfs.uni-tuebingen.de</email>
<abstract confidence="0.999428434782609">Recent parsing research has started addressing the questions a) how parsers trained on different syntactic resources differ in their performance and b) how to conduct a meaningful evaluation of the parsing results across such a range of syntactic representations. Two German treebanks, Negra and T¨uBa-D/Z, constitute an interesting testing ground for such research given that the two treebanks make very different representational choices for this language, which also is of general interest given that German is situated between the extremes of fixed and free word order. We show that previous work comparing PCFG parsing with these two treebanks employed PARSEVAL and grammatical function comparisons which were skewed by differences between the two corpus annotation schemes. Focusing on the grammatical dependency triples as an essential dimension of comparison, we show that the two very distinct corpora result in comparable parsing performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adriane Boyd</author>
</authors>
<title>Discontinuity Revisited: An Improved Conversion to Context-Free Representations.</title>
<date>2007</date>
<booktitle>In Proceedings of the Linguistic Annotation Workshop (LAW).</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="15826" citStr="Boyd (2007)" startWordPosition="2548" endWordPosition="2549">nversion program included with the Negra corpus annotation tools (Brants and Plaehn, 2000), the same tool used in K¨ubler et al. (2006), which raises non-head elements to a higher tree until there are no more discontinuities. For example, for the discontinuous tree with a fronted object we saw in Figure 1, the PP containing the fronted NP Dieser Meinung is raised to become a daughter of the top S node.4 Additionally, the edge labels used in both corpora need to be folded into the node labels to become a 4An alternate method that avoids certain problems with this raising method is discussed in Boyd (2007). OA-MOD VF PX HD - - - - VXFIN LK HD HD - - EN-ADD NX ON OA - SIMPX MF - HD NX VXINF VC OV HD Dafür PROP wird VAFIN Andrea NE Fischer NE wenig PIAT Zeit NN haben VAINF . $. VROOT 27 part of context-free grammar rules used by a PCFG parser. In the Penn Treebank-style versions of the corpora appropriate for training a PCFG parser, each edge label is joined with the phrase or POS label on the phrase or word immediately below it. Both corpora include edge labels above all phrases and words. However the flatter structures in Negra result in 39 different edge labels on words while T¨uBa-D/Z has onl</context>
</contexts>
<marker>Boyd, 2007</marker>
<rawString>Adriane Boyd, 2007. Discontinuity Revisited: An Improved Conversion to Context-Free Representations. In Proceedings of the Linguistic Annotation Workshop (LAW). Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Oliver Plaehn</author>
</authors>
<title>Interactive Corpus Annotation.</title>
<date>2000</date>
<booktitle>In Proceedings of the Second International Conference on Language Resources and Evaluation (LREC).</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="15305" citStr="Brants and Plaehn, 2000" startWordPosition="2453" endWordPosition="2456"> K¨ubler et al. (2006), only sentences with fewer than 35 words were used, which results in 20,002 sentences for Negra and 21,365 sentences for T¨uBa-D/Z. Because punctuation is not attached within the sentence in the corpus annotation, punctuation was removed. To be able to train PCFG parsing models, it is necessary to convert the syntax graphs encoding trees with discontinuities in Negra into traditional syntax trees. Around 30% of sentences in Negra contain at least one discontinuity. To remove discontinuities, we used the conversion program included with the Negra corpus annotation tools (Brants and Plaehn, 2000), the same tool used in K¨ubler et al. (2006), which raises non-head elements to a higher tree until there are no more discontinuities. For example, for the discontinuous tree with a fronted object we saw in Figure 1, the PP containing the fronted NP Dieser Meinung is raised to become a daughter of the top S node.4 Additionally, the edge labels used in both corpora need to be folded into the node labels to become a 4An alternate method that avoids certain problems with this raising method is discussed in Boyd (2007). OA-MOD VF PX HD - - - - VXFIN LK HD HD - - EN-ADD NX ON OA - SIMPX MF - HD NX</context>
</contexts>
<marker>Brants, Plaehn, 2000</marker>
<rawString>Thorsten Brants and Oliver Plaehn, 2000. Interactive Corpus Annotation. In Proceedings of the Second International Conference on Language Resources and Evaluation (LREC). Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Wojciech Skut</author>
<author>Hans Uszkoreit</author>
</authors>
<title>Syntactic Annotation of a German Newspaper Corpus.</title>
<date>1999</date>
<booktitle>In Proceedings of the ATALA Treebank Workshop.</booktitle>
<location>Paris, France.</location>
<contexts>
<context position="1827" citStr="Brants et al., 1999" startWordPosition="264" endWordPosition="267">s as an essential dimension of comparison, we show that the two very distinct corpora result in comparable parsing performance. 1 Introduction Syntactically annotated corpora have been produced for a range of languages and they differ significantly regarding which language properties are encoded and how they are represented. Between the two extremes of constituency treebanks for English and dependency treebanks for free word order languages such as Czech lie languages such as German, for which two different treebanks have explored different options for encoding topology and dependency, Negra (Brants et al., 1999) and T¨uBa-D/Z (Telljohann et al., 2005). 24 Recent research has started addressing the question of how parsers trained on these different syntactic resources differ in their performance. Such work must also address the question of how to conduct a meaningful evaluation of the parsing results across such a range of syntactic representations. In this paper, we show that previous work comparing PCFG parsing for the two German treebanks used representations which cannot adequately be compared using the given PARSEVAL measures and that a grammatical dependency evaluation is more meaningful than th</context>
<context position="7819" citStr="Brants et al., 1999" startWordPosition="1201" endWordPosition="1204">an evaluation, which abstracts away from the specifics of the annotation schemes, shows that, in contrast to the claims made in K¨ubler et al. (2006), the parsing results for PCFG parsers trained on these heterogeneous corpora are very similar. 2 The corpora used As motivated in the introduction, the work discussed in this paper is based on two German corpora, Negra and T¨uBa-D/Z, which differ significantly in the syntactic representations used – thereby offering an interesting test bed for investigating the influence of an annotation scheme on the parsers trained. 2.1 Negra The Negra corpus (Brants et al., 1999) consists of newspaper text from the Frankfurter Rundschau, a German newspaper. Version 2 of the corpus contains 20,602 sentences. It uses the STTS tag set (Schiller et al., 1995) for part-of-speech annotation. There are 25 non-terminal node labels and 46 edge labels. The syntactic annotation of Negra combines features from phrase structure grammar and depen3Their evaluation also introduces an additional level of complexity by finding heads heuristically rather than relying on the head labels present on some elements in each corpus. 25 dency grammar using a tree-like syntactic structure with g</context>
</contexts>
<marker>Brants, Skut, Uszkoreit, 1999</marker>
<rawString>Thorsten Brants, Wojciech Skut and Hans Uszkoreit, 1999. Syntactic Annotation of a German Newspaper Corpus. In Proceedings of the ATALA Treebank Workshop. Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Guido Minnen</author>
<author>Ted Briscoe</author>
</authors>
<title>Parser evaluation: using a grammatical relation annotation scheme.</title>
<date>2003</date>
<editor>In A. Abeill´e (ed.),</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="3946" citStr="Carroll et al., 2003" startWordPosition="590" endWordPosition="593">gle error multiple times and does not differentiate between errors that significantly affect the interpretation of the sentence and those that are less crucial. Proceedings of the ACL-08: HLT Workshop on Parsing German (PaGe-08), pages 24–32, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics It also does not allow for evaluation of particular syntactic structures or provide meaningful information about where the parser is failing. In addition, and most directly relevant for this paper, PARSEVAL scores are difficult to compare across syntactic annotation schemes (Carroll et al., 2003). At the same time, previous research on PCFG parsing using treebank training data present PARSEVAL measures in comparing the parsing performance for different languages and annotation schemes, reporting a number of striking differences. For example, Levy and Manning (2003), K¨ubler (2005), and K¨ubler et al. (2006) highlight the significant effect of language properties and annotation schemes for German and Chinese treebanks. In related work, parser enhancements that provide a significant performance boost for English, such as head lexicalization, are reported not to provide the same kind of </context>
</contexts>
<marker>Carroll, Minnen, Briscoe, 2003</marker>
<rawString>John Carroll, Guido Minnen and Ted Briscoe, 2003. Parser evaluation: using a grammatical relation annotation scheme. In A. Abeill´e (ed.), Treebanks: Building and Using Parsed Corpora, Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James Curran</author>
</authors>
<title>FormalismIndependent Parser Evaluation with CCG and DepBank.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL).</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2941" citStr="Clark and Curran, 2007" startWordPosition="438" endWordPosition="441">red using the given PARSEVAL measures and that a grammatical dependency evaluation is more meaningful than the grammatical function evaluation provided. We present the first comparison of Negra and T¨uBa-D/Z using a labeled dependency evaluation based on the grammatical function labels provided in the corpora. We show that, in contrast to previous literature, a labeled dependency evaluation establishes that PCFG parsers trained on the two corpora give similar parsing performance. The focus on labeled dependencies also provides a direct link to recent work on dependency-based evaluation (e.g., Clark and Curran, 2007) and dependency parsing (e.g., CoNLL shared tasks 2006, 2007). 1.1 Previous work The question of how to evaluate parser output has naturally already arisen in earlier work on parsing English. As discussed by Lin (1995) and others, the PARSEVAL evaluation typically used to analyze the performance of statistical parsing models has many drawbacks. Bracketing evaluation may count a single error multiple times and does not differentiate between errors that significantly affect the interpretation of the sentence and those that are less crucial. Proceedings of the ACL-08: HLT Workshop on Parsing Germ</context>
<context position="6892" citStr="Clark and Curran, 2007" startWordPosition="1053" endWordPosition="1056">reebank annotation schemes because PARSEVAL scores are affected by the ratio of terminal to non-terminal nodes. A dependencybased evaluation on triples of the form word-POShead shows better results for the parser trained on Tiger even though the much lower PARSEVAL scores, if meaningful, would predict that the output for Tiger is of lower quality. However, their dependency-based evaluation does not make use of the grammatical function labels, which are provided in the corpora and closely correspond to the representations used in recent work on formalismindependent evaluation of parsers (e.g., Clark and Curran, 2007).3 Addressing these issues, we resolve the apparent discrepancy between K¨ubler et al. (2006) and Dubey (2004) and establish a firm grammatical function comparison of Negra and T¨uBa-D/Z. We also extend the evaluation to a labeled dependency evaluation based on grammatical relations for both corpora. Such an evaluation, which abstracts away from the specifics of the annotation schemes, shows that, in contrast to the claims made in K¨ubler et al. (2006), the parsing results for PCFG parsers trained on these heterogeneous corpora are very similar. 2 The corpora used As motivated in the introduct</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James Curran, 2007. FormalismIndependent Parser Evaluation with CCG and DepBank. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL). Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Dubey</author>
</authors>
<title>Statistical Parsing for German: Modeling Syntactic Properties and Annotation Differences.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Universit¨at des Saarlandes.</institution>
<contexts>
<context position="4614" citStr="Dubey, 2004" startWordPosition="695" endWordPosition="696">ng treebank training data present PARSEVAL measures in comparing the parsing performance for different languages and annotation schemes, reporting a number of striking differences. For example, Levy and Manning (2003), K¨ubler (2005), and K¨ubler et al. (2006) highlight the significant effect of language properties and annotation schemes for German and Chinese treebanks. In related work, parser enhancements that provide a significant performance boost for English, such as head lexicalization, are reported not to provide the same kind of improvement, if any, for German (Dubey and Keller, 2003; Dubey, 2004; K¨ubler et al., 2006). Previous work has compared the similar Negra and Tiger corpora of German to the very different T¨uBa-D/Z corpus. K¨ubler et al. (2006) compares the Negra and T¨uBa-D/Z corpora of German using a PARSEVAL evaluation and an evaluation on core grammatical function labels that is included to address concerns about the PARSEVAL measure.1 Using the Stanford Parser (Klein and Manning, 2002), which employs a factored PCFG and dependency model, they claim that the model trained on T¨uBaD/Z consistently outperforms that trained on Negra in PARSEVAL and grammatical function evalua</context>
<context position="7002" citStr="Dubey (2004)" startWordPosition="1071" endWordPosition="1072">ncybased evaluation on triples of the form word-POShead shows better results for the parser trained on Tiger even though the much lower PARSEVAL scores, if meaningful, would predict that the output for Tiger is of lower quality. However, their dependency-based evaluation does not make use of the grammatical function labels, which are provided in the corpora and closely correspond to the representations used in recent work on formalismindependent evaluation of parsers (e.g., Clark and Curran, 2007).3 Addressing these issues, we resolve the apparent discrepancy between K¨ubler et al. (2006) and Dubey (2004) and establish a firm grammatical function comparison of Negra and T¨uBa-D/Z. We also extend the evaluation to a labeled dependency evaluation based on grammatical relations for both corpora. Such an evaluation, which abstracts away from the specifics of the annotation schemes, shows that, in contrast to the claims made in K¨ubler et al. (2006), the parsing results for PCFG parsers trained on these heterogeneous corpora are very similar. 2 The corpora used As motivated in the introduction, the work discussed in this paper is based on two German corpora, Negra and T¨uBa-D/Z, which differ signif</context>
<context position="20034" citStr="Dubey (2004)" startWordPosition="3248" endWordPosition="3249"> annotation schemes. 3.4 Dependency Evaluation Complementing the issue of the ratio of terminals to non-terminals raised in the last section, one can question whether counting all brackets in the sentence equally, as done by the PARSEVAL metric, provides a good measure of how accurately the basic functor-argument structure of the sentence has been captured in a parse. Thus, it is useful to per7Our experimental setup is designed to support a comparison between Negra and T¨uBa-D/Z for the three evaluation metrics and is intended to be comparable to the setup of K¨ubler et al. (2006). For Negra, Dubey (2004) explores a range of parsing models and the corpus preparation he uses differs from the one discussed in this paper so that a discussion of his results is beyond the scope of the corpus comparison in this paper. 8Scores were calculated using evalb. 28 form an evaluation based on the grammatical function labels that are important for determining the functor-argument structure of the sentence: subjects, accusative objects, and dative objects.9 The first step in an evaluation of functor-argument structure is to identify whether an argument bears the correct grammatical function label. 3.4.1 Gramm</context>
</contexts>
<marker>Dubey, 2004</marker>
<rawString>Amit Dubey, 2004. Statistical Parsing for German: Modeling Syntactic Properties and Annotation Differences. Ph.D. thesis, Universit¨at des Saarlandes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Dubey</author>
<author>Frank Keller</author>
</authors>
<title>Probabilistic Parsing Using Sister-Head Dependencies.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="4601" citStr="Dubey and Keller, 2003" startWordPosition="691" endWordPosition="694">arch on PCFG parsing using treebank training data present PARSEVAL measures in comparing the parsing performance for different languages and annotation schemes, reporting a number of striking differences. For example, Levy and Manning (2003), K¨ubler (2005), and K¨ubler et al. (2006) highlight the significant effect of language properties and annotation schemes for German and Chinese treebanks. In related work, parser enhancements that provide a significant performance boost for English, such as head lexicalization, are reported not to provide the same kind of improvement, if any, for German (Dubey and Keller, 2003; Dubey, 2004; K¨ubler et al., 2006). Previous work has compared the similar Negra and Tiger corpora of German to the very different T¨uBa-D/Z corpus. K¨ubler et al. (2006) compares the Negra and T¨uBa-D/Z corpora of German using a PARSEVAL evaluation and an evaluation on core grammatical function labels that is included to address concerns about the PARSEVAL measure.1 Using the Stanford Parser (Klein and Manning, 2002), which employs a factored PCFG and dependency model, they claim that the model trained on T¨uBaD/Z consistently outperforms that trained on Negra in PARSEVAL and grammatical fu</context>
</contexts>
<marker>Dubey, Keller, 2003</marker>
<rawString>Amit Dubey and Frank Keller, 2003. Probabilistic Parsing Using Sister-Head Dependencies. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL). Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tilman H¨ohle</author>
</authors>
<title>Der Begriff “Mittelfeld”, Anmerkungen ¨uber die Theorie der topologischen Felder.</title>
<date>1986</date>
<booktitle>InAkten des Siebten Internationalen Germanistenkongresses</booktitle>
<location>G¨ottingen, Germany.</location>
<marker>H¨ohle, 1986</marker>
<rawString>Tilman H¨ohle, 1986. Der Begriff “Mittelfeld”, Anmerkungen ¨uber die Theorie der topologischen Felder. InAkten des Siebten Internationalen Germanistenkongresses 1985. G¨ottingen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast Exact Inference with a Factored Model for Natural Language Parsing.</title>
<date>2002</date>
<booktitle>In Advances in Neural Information Processing Systems 15 (NIPS).</booktitle>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="5024" citStr="Klein and Manning, 2002" startWordPosition="759" endWordPosition="762"> enhancements that provide a significant performance boost for English, such as head lexicalization, are reported not to provide the same kind of improvement, if any, for German (Dubey and Keller, 2003; Dubey, 2004; K¨ubler et al., 2006). Previous work has compared the similar Negra and Tiger corpora of German to the very different T¨uBa-D/Z corpus. K¨ubler et al. (2006) compares the Negra and T¨uBa-D/Z corpora of German using a PARSEVAL evaluation and an evaluation on core grammatical function labels that is included to address concerns about the PARSEVAL measure.1 Using the Stanford Parser (Klein and Manning, 2002), which employs a factored PCFG and dependency model, they claim that the model trained on T¨uBaD/Z consistently outperforms that trained on Negra in PARSEVAL and grammatical function evaluations. Dubey (2004) also includes an evaluation on grammatical function for statistical models trained on Negra, but obtains very different results from K¨ubler et al. (2006).2 In recent related work, Rehbein and van Genabith (2007a) demonstrate using the Tiger and T¨uBa-D/Z 1The evaluation is based only on the grammatical function; it does not identify the dependency pair that it labels. 2While the focus o</context>
<context position="20829" citStr="Klein and Manning, 2002" startWordPosition="3373" endWordPosition="3377"> of the corpus comparison in this paper. 8Scores were calculated using evalb. 28 form an evaluation based on the grammatical function labels that are important for determining the functor-argument structure of the sentence: subjects, accusative objects, and dative objects.9 The first step in an evaluation of functor-argument structure is to identify whether an argument bears the correct grammatical function label. 3.4.1 Grammatical Function Label Evaluation K¨ubler et al. (2006) present the results shown in Table 3 for the parsing performance of the unlexicalized model of the Stanford Parser (Klein and Manning, 2002). In this grammatical function label evaluation, T¨uBa-D/Z outperforms Negra for subjects, accusative objects, and dative objects based on an evaluation of phrasal arguments. Negra T¨uBa-D/Z Prec Rec F Prec Rec F Subj 52.50 58.02 55.26 66.82 75.93 72.38 Acc 35.14 36.30 35.72 43.84 47.31 45.58 Dat 8.38 3.58 5.98 24.46 9.96 17.21 Table 3: Grammatical Function Label Evaluation for Phrasal Arguments from K¨ubler et al. (2006) Note that this grammatical function label evaluation is restricted to labels on phrases; grammatical function labels on words are ignored in training and testing. This result</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Christopher D. Manning, 2002. Fast Exact Inference with a Factored Model for Natural Language Parsing. In Advances in Neural Information Processing Systems 15 (NIPS). Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra K¨ubler</author>
</authors>
<title>How do treebank annotation schemes influence parsing results? Or how not to compare apples and oranges.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on RecentAdvances in Natural Language Processing (RANLP). Borovets,</booktitle>
<marker>K¨ubler, 2005</marker>
<rawString>Sandra K¨ubler, 2005. How do treebank annotation schemes influence parsing results? Or how not to compare apples and oranges. In Proceedings of the Conference on RecentAdvances in Natural Language Processing (RANLP). Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra K¨ubler</author>
<author>Erhard W Hinrichs</author>
<author>Wolfgang Maier</author>
</authors>
<title>Is it really that difficult to parse German?</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<location>Sydney, Australia.</location>
<marker>K¨ubler, Hinrichs, Maier, 2006</marker>
<rawString>Sandra K¨ubler, Erhard W. Hinrichs and Wolfgang Maier, 2006. Is it really that difficult to parse German? In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Christopher Manning</author>
</authors>
<title>Is it harder to parse Chinese, or the Chinese Treebank?</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="4220" citStr="Levy and Manning (2003)" startWordPosition="631" endWordPosition="634">8. c�2008 Association for Computational Linguistics It also does not allow for evaluation of particular syntactic structures or provide meaningful information about where the parser is failing. In addition, and most directly relevant for this paper, PARSEVAL scores are difficult to compare across syntactic annotation schemes (Carroll et al., 2003). At the same time, previous research on PCFG parsing using treebank training data present PARSEVAL measures in comparing the parsing performance for different languages and annotation schemes, reporting a number of striking differences. For example, Levy and Manning (2003), K¨ubler (2005), and K¨ubler et al. (2006) highlight the significant effect of language properties and annotation schemes for German and Chinese treebanks. In related work, parser enhancements that provide a significant performance boost for English, such as head lexicalization, are reported not to provide the same kind of improvement, if any, for German (Dubey and Keller, 2003; Dubey, 2004; K¨ubler et al., 2006). Previous work has compared the similar Negra and Tiger corpora of German to the very different T¨uBa-D/Z corpus. K¨ubler et al. (2006) compares the Negra and T¨uBa-D/Z corpora of Ge</context>
</contexts>
<marker>Levy, Manning, 2003</marker>
<rawString>Roger Levy and Christopher Manning, 2003. Is it harder to parse Chinese, or the Chinese Treebank? In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>A Dependency-based Method for Evaluating Broad-Coverage Parsers.</title>
<date>1995</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI).</booktitle>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="3159" citStr="Lin (1995)" startWordPosition="475" endWordPosition="476">ncy evaluation based on the grammatical function labels provided in the corpora. We show that, in contrast to previous literature, a labeled dependency evaluation establishes that PCFG parsers trained on the two corpora give similar parsing performance. The focus on labeled dependencies also provides a direct link to recent work on dependency-based evaluation (e.g., Clark and Curran, 2007) and dependency parsing (e.g., CoNLL shared tasks 2006, 2007). 1.1 Previous work The question of how to evaluate parser output has naturally already arisen in earlier work on parsing English. As discussed by Lin (1995) and others, the PARSEVAL evaluation typically used to analyze the performance of statistical parsing models has many drawbacks. Bracketing evaluation may count a single error multiple times and does not differentiate between errors that significantly affect the interpretation of the sentence and those that are less crucial. Proceedings of the ACL-08: HLT Workshop on Parsing German (PaGe-08), pages 24–32, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics It also does not allow for evaluation of particular syntactic structures or provide meaningful information abo</context>
</contexts>
<marker>Lin, 1995</marker>
<rawString>Dekang Lin, 1995. A Dependency-based Method for Evaluating Broad-Coverage Parsers. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI). Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ines Rehbein</author>
<author>Josef van Genabith</author>
</authors>
<title>Treebank Annotation Schemes and Parser Evaluation for German.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<location>Prague, Czech Republic.</location>
<marker>Rehbein, van Genabith, 2007</marker>
<rawString>Ines Rehbein and Josef van Genabith, 2007a. Treebank Annotation Schemes and Parser Evaluation for German. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ines Rehbein</author>
<author>Josef van Genabith</author>
</authors>
<title>Why is it so difficult to compare treebanks? TIGER and T¨uBa-D/Z revisited.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Treebanks and Linguistic Theories (TLT).</booktitle>
<location>Bergen, Norway.</location>
<marker>Rehbein, van Genabith, 2007</marker>
<rawString>Ines Rehbein and Josef van Genabith, 2007b. Why is it so difficult to compare treebanks? TIGER and T¨uBa-D/Z revisited. In Proceedings of the Workshop on Treebanks and Linguistic Theories (TLT). Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marga Reis</author>
</authors>
<title>On Justifying Topological Frames: ‘Positional Field’ and the Order of Nonverbal Constituents</title>
<date>1980</date>
<booktitle>in German. Documentation et Recherche en Linguistique Allemande Contemporaine Vincennes (DRLAV),</booktitle>
<pages>22--23</pages>
<contexts>
<context position="11093" citStr="Reis, 1980" startWordPosition="1760" endWordPosition="1761">ce we saw in Figure 1 contains a discontinuous VP with a fronted dative object (Dieser Meinung). The dative object and a modifier (voll) form a VP with the non-finite verb (zustimmen). 2.2 T¨uBa-D/Z The T¨uBa-D/Z corpus, version 2, (Telljohann et al., 2005) consists of 22,091 sentences of newspaper text from the German newspaper die tageszeitung. Like Negra, it uses the STTS tag set (Schiller et al., 1995) for part-of-speech annotation. Syntactically it uses 27 non-terminal node labels and 47 edge labels. The syntactic annotation incorporates a topological field analysis of the German clause (Reis, 1980; H¨ohle, 1986), which segments a sentence into topological units depending on the position of the finite verb (verb-first, verb-second, verb-last). In a verbfirst and verb-second sentence, the finite verb is the left bracket (LK), whereas in a verb-last subordinate clause, the subordinating conjunction occupies that field. In all clauses, the non-finite verb cluster forms the right bracket (VC), and arguments and modifiers can appear in the middle field (MF) between the two brackets. Extraposed material is found to the right of the right bracket, and in a verb-second sentence one constituent </context>
</contexts>
<marker>Reis, 1980</marker>
<rawString>Marga Reis, 1980. On Justifying Topological Frames: ‘Positional Field’ and the Order of Nonverbal Constituents in German. Documentation et Recherche en Linguistique Allemande Contemporaine Vincennes (DRLAV), 22/23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Schiller</author>
<author>Simone Teufel</author>
<author>Christine Thielen</author>
</authors>
<title>Guidelines f¨ur das Tagging deutscher Textcorpora mit STTS.</title>
<date>1995</date>
<tech>Technical report,</tech>
<institution>Universit¨at Stuttgart, Universit¨at T¨ubingen,</institution>
<contexts>
<context position="7998" citStr="Schiller et al., 1995" startWordPosition="1230" endWordPosition="1233">FG parsers trained on these heterogeneous corpora are very similar. 2 The corpora used As motivated in the introduction, the work discussed in this paper is based on two German corpora, Negra and T¨uBa-D/Z, which differ significantly in the syntactic representations used – thereby offering an interesting test bed for investigating the influence of an annotation scheme on the parsers trained. 2.1 Negra The Negra corpus (Brants et al., 1999) consists of newspaper text from the Frankfurter Rundschau, a German newspaper. Version 2 of the corpus contains 20,602 sentences. It uses the STTS tag set (Schiller et al., 1995) for part-of-speech annotation. There are 25 non-terminal node labels and 46 edge labels. The syntactic annotation of Negra combines features from phrase structure grammar and depen3Their evaluation also introduces an additional level of complexity by finding heads heuristically rather than relying on the head labels present on some elements in each corpus. 25 dency grammar using a tree-like syntactic structure with grammatical functions labeled on the edges of the tree. Flat sentence structures are used in many places to avoid attachment ambiguities and nonbranching phrases are not used. The </context>
<context position="10892" citStr="Schiller et al., 1995" startWordPosition="1729" endWordPosition="1732">on, immediately following the fronted constituent. As a result, the VP in Negra is discontinuous whenever one of its children has been fronted, as in the common word orders exemplified in (1a) and (1b). The sentence we saw in Figure 1 contains a discontinuous VP with a fronted dative object (Dieser Meinung). The dative object and a modifier (voll) form a VP with the non-finite verb (zustimmen). 2.2 T¨uBa-D/Z The T¨uBa-D/Z corpus, version 2, (Telljohann et al., 2005) consists of 22,091 sentences of newspaper text from the German newspaper die tageszeitung. Like Negra, it uses the STTS tag set (Schiller et al., 1995) for part-of-speech annotation. Syntactically it uses 27 non-terminal node labels and 47 edge labels. The syntactic annotation incorporates a topological field analysis of the German clause (Reis, 1980; H¨ohle, 1986), which segments a sentence into topological units depending on the position of the finite verb (verb-first, verb-second, verb-last). In a verbfirst and verb-second sentence, the finite verb is the left bracket (LK), whereas in a verb-last subordinate clause, the subordinating conjunction occupies that field. In all clauses, the non-finite verb cluster forms the right bracket (VC),</context>
</contexts>
<marker>Schiller, Teufel, Thielen, 1995</marker>
<rawString>Anne Schiller, Simone Teufel and Christine Thielen, 1995. Guidelines f¨ur das Tagging deutscher Textcorpora mit STTS. Technical report, Universit¨at Stuttgart, Universit¨at T¨ubingen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>LoPar: Design and Implementation.</title>
<date>2000</date>
<journal>Arbeitspapiere des Sonderforschungsbereiches</journal>
<volume>340</volume>
<institution>Universit¨at Stuttgart.</institution>
<contexts>
<context position="17955" citStr="Schmid, 2000" startWordPosition="2914" endWordPosition="2915">ll single-word subject, accusative object, dative object, and genitive object5 arguments, with the category of the inserted phrase depending on the POS tag on the word. The introduced phrasal node is given the word’s original grammatical function label; the grammatical function label of the word itself becomes NK for NPs and HD for APs and VPs. In total, 14,580 nodes were inserted into Negra in this way. T¨uBa-D/Z has non-branching phrases above all single-word arguments, so that no such modification was needed.6 3.2 Experimental Setup We trained unlexicalized PCFG parsing models using LoPar (Schmid, 2000). Unlexicalized models 5Genitive objects are modified for the sake of consistency among arguments even though there are too few genitive objects to provide reliable results in the evaluation. 6The addition of edge labels to terminal POS labels results in 337 lexical tags for Negra and 91 for T¨uBa-D/Z. were used to minimize the impact of other corpus differences on parsing. A ten-fold cross validation was performed for all experiments.7 3.3 PARSEVAL Evaluation As a reference point for comparison with previous work, the PARSEVAL results8 are given in Table 2. Negra T¨uBa-D/Z Unlabeled Precision</context>
</contexts>
<marker>Schmid, 2000</marker>
<rawString>Helmut Schmid, 2000. LoPar: Design and Implementation. Arbeitspapiere des Sonderforschungsbereiches 340 No. 149, Universit¨at Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heike Telljohann</author>
<author>Erhard W Hinrichs</author>
<author>Sandra K¨ubler</author>
<author>Heike Zinsmeister</author>
</authors>
<title>Stylebook for the T¨ubingen Treebank of Written German (T¨uBa-D/Z).</title>
<date>2005</date>
<tech>Technical report, Seminar f¨ur Sprachwissenschaft,</tech>
<institution>Universit¨at T¨ubingen,</institution>
<marker>Telljohann, Hinrichs, K¨ubler, Zinsmeister, 2005</marker>
<rawString>Heike Telljohann, Erhard W. Hinrichs, Sandra K¨ubler and Heike Zinsmeister, 2005. Stylebook for the T¨ubingen Treebank of Written German (T¨uBa-D/Z). Technical report, Seminar f¨ur Sprachwissenschaft, Universit¨at T¨ubingen, Germany.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>