<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.947689">
Decoding Algorithm in Statistical Machine Translation
</title>
<author confidence="0.997105">
Ye-Yi Wang and Alex Waibel
</author>
<affiliation confidence="0.991439">
Language Technology Institute
School of Computer Science
Carnegie Mellon University
</affiliation>
<address confidence="0.5987765">
5000 Forbes Avenue
Pittsburgh, PA 15213, USA
</address>
<email confidence="0.922753">
{yyw,waibel}Ocs.cmu.edu
</email>
<sectionHeader confidence="0.992966" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999706">
Decoding algorithm is a crucial part in sta-
tistical machine translation. We describe
a stack decoding algorithm in this paper.
We present the hypothesis scoring method
and the heuristics used in our algorithm.
We report several techniques deployed to
improve the performance of the decoder.
We also introduce a simplified model to
moderate the sparse data problem and to
speed up the decoding process. We evalu-
ate and compare these techniques/models
in our statistical machine translation sys-
tem.
</bodyText>
<sectionHeader confidence="0.999383" genericHeader="introduction">
1 Introduction
</sectionHeader>
<subsectionHeader confidence="0.994328">
1.1 Statistical Machine Translation
</subsectionHeader>
<bodyText confidence="0.999937692307692">
Statistical machine translation is based on a channel
model. Given a sentence T in one language (Ger-
man) to be translated into another language (En-
glish), it considers T as the target of a communi-
cation channel, and its translation S as the source
of the channel. Hence the machine translation task
becomes to recover the source from the target. Ba-
sically every English sentence is a possible source for
a German target sentence. If we assign a probability
P(S I T) to each pair of sentences (S, T), then the
problem of translation is to find the source S for a
given target T, such that P(S T) is the maximum.
According to Bayes rule,
</bodyText>
<equation confidence="0.954534666666667">
P(S)P(T I S)
P(SIT) I T) = (1)
P(T)
</equation>
<bodyText confidence="0.99369375">
Since the denominator is independent of S, we have
= arg max P(S)P(T I S) (2)
Therefore a statistical machine translation system
must deal with the following three problems:
</bodyText>
<listItem confidence="0.989453">
• Modeling Problem: How to depict the process
of generating a sentence in a source language,
and the process used by a channel to generate
</listItem>
<bodyText confidence="0.994691">
a target sentence upon receiving a source sen-
tence? The former is the problem of language
modeling, and the later is the problem of trans-
lation modeling. They provide a framework for
calculating P(S) and P(T I S) in (2).
</bodyText>
<listItem confidence="0.999770333333333">
• Learning Problem: Given a statistical language
model P(S) and a statistical translation model
P(T I S), how to estimate the parameters in
these models from a bilingual corpus of sen-
tences?
• Decoding Problem: With a fully specified
</listItem>
<bodyText confidence="0.978081571428571">
(framework and parameters) language and
translation model, given a target sentence T,
how to efficiently search for the source sentence
S&apos; that satisfies (2).
The modeling and learning issues have been dis-
cussed in (Brown et al., 1993), where ngram model
was used for language modeling, and five different
translation models were introduced for the transla-
tion process. We briefly introduce the model 2 here,
for which we built our decoder.
In model 2, upon receiving a source English sen-
tence e = el , • • , el, the channel generates a German
sentence g = gi, • • • , gni at the target end in the fol-
lowing way:
</bodyText>
<listItem confidence="0.999059">
1. With a distribution P(in I e), randomly choose
the length m of the German translation g. In
model 2, the distribution is independent of m
and e:
</listItem>
<equation confidence="0.940591">
P(rn I e) =
</equation>
<bodyText confidence="0.882231875">
where is a small, fixed number.
2. For each position i (0 &lt; i &lt; m) in g, find the
corresponding position cti in e according to an
alignment distribution P(ai I i, m, e). In
model 2, the distribution only depends on i, ai
and the length of the English and German sen-
tences:
P(ai I i, a1, m, e) = a(cti I i,m, I)
</bodyText>
<listItem confidence="0.830497">
3. Generate the word gi at the position i of the
German sentence from the English word ea, at
</listItem>
<page confidence="0.998455">
366
</page>
<bodyText confidence="0.9516445">
the aligned position ai of gi, according to a
translation distribution P(gi I ar e) =
t(giI e„,). The distribution here only depends
on gi and ea,.
Therefore, P(g I e) is the sum of the probabilities
of generating g from e over all possible alignments
A, in which the position i in the target sentence g is
aligned to the position ai in the source sentence e:
</bodyText>
<figure confidence="0.946675">
P (g I e) =
I m
ef•••E ilt(gi I eoi)a(ai I j,l,m) =
ni=0 a„,=0 j=1
m 1
11 E t(gi ei)a(i I j, 1 , m)
j=1. i=o
</figure>
<figureCaption confidence="0.661807">
(Brown et al., 1993) also described how to use
the EM algorithm to estimate the parameters a(i I
j, 1, in) and t(g I e) in the aforementioned model.
</figureCaption>
<subsectionHeader confidence="0.9976335">
1.2 Decoding in Statistical Machine
Translation
</subsectionHeader>
<bodyText confidence="0.9999725">
(Brown et al., 1993) and (Vogel, Ney, and Tillman,
1996) have discussed the first two of the three prob-
lems in statistical machine translation. Although
the authors of (Brown et al., 1993) stated that they
would discuss the search problem in a follow-up arti-
cle, so far there have no publications devoted to the
decoding issue for statistical machine translation.
On the other side, decoding algorithm is a crucial
part in statistical machine translation. Its perfor-
mance directly affects the quality and efficiency of
translation. Without a good and efficient decoding
algorithm, a statistical machine translation system
may miss the best translation of an input sentence
even if it is perfectly predicted by the model.
</bodyText>
<sectionHeader confidence="0.813681" genericHeader="method">
2 Stack Decoding Algorithm
</sectionHeader>
<bodyText confidence="0.998718">
Stack decoders are widely used in speech recognition
systems. The basic algorithm can be described as
following:
</bodyText>
<listItem confidence="0.999732454545455">
1. Initialize the stack with a null hypothesis.
2. Pop the hypothesis with the highest score off
the stack, name it as current-hypothesis.
3. if current-hypothesis is a complete sentence,
output it and terminate.
4. extend current-hypothesis by appending a
word in the lexicon to its end. Compute the
score of the new hypothesis and insert it into
the stack. Do this for all the words in the lexi-
con.
5. Go to 2.
</listItem>
<subsectionHeader confidence="0.999919">
2.1 Scoring the hypotheses
</subsectionHeader>
<bodyText confidence="0.9582135">
In stack search for statistical machine translation,
a hypothesis H includes (a) the length 1 of the
source sentence, and (b) the prefix words in the
sentence. Thus a hypothesis can be written as
H = 1 : e1e2 • • • ek , which postulates a source sen-
tence of length 1 and its first k words. The score
of H, fH , consists of two parts: the prefix score 9H
for e1 e2 • • • ek and the heuristic score hH for the part
ek+iek+2 • • el that is yet to be appended to H to
complete the sentence.
</bodyText>
<subsectionHeader confidence="0.619809">
2.1.1 Prefix score gH
</subsectionHeader>
<bodyText confidence="0.9748567">
(3) can be used to assess a hypothesis. Although
it was obtained from the alignment model, it would
be easier for us to describe the scoring method if
we interpret the last expression in the equation in
the following way: each word ei in the hypothesis
contributes the amount E t(g I e)a(i I j, 1, m) to the
probability of the target sentence word gi. For each
hypothesis H = 1 : e2,• • , ek, we use SH(j) to
denote the probability mass for the target word gi
contributed by the words in the hypothesis:
</bodyText>
<equation confidence="0.958683">
Ic
si,(j) = EE tcgi eoci(i I i, 1, m) (4)
i=0
</equation>
<bodyText confidence="0.975276777777778">
Extending H with a new word will increase
SH(j), 1 &lt; j &lt; m.
To make the score additive, the logarithm of the
probability in (3) was used. So the prefix score con-
tributed by the translation model is ET....0 log SH(j).
Because our objective is to maximize P(e, g), we
have to include as well the logarithm of the language
model probability of the hypothesis in the score,
therefore we have
</bodyText>
<equation confidence="0.978904">
E log SH(j)
j=0
</equation>
<bodyText confidence="0.82314825">
Elog P (et I ei-N+1 • • • ei-1)•
i=o
here N is the order of the ngram language model.
The above 9-score gH of a hypothesis H = 1 :
e1e2 • • • ek can be calculated from the 9-score of its
parent hypothesis P = 1: e1 e2 • • • ek- 1:
gp + log P(ek I ek-N-1-1 &amp;quot; • ek-1)
rn
</bodyText>
<construct confidence="0.623511666666667">
logo. ct(gi I ek)a(k I j, in)
Sp (j)
i=o
</construct>
<equation confidence="0.521364">
SH(j) = Sp(j) + et(gi ek)a(k j,1, in) (5)
</equation>
<bodyText confidence="0.9993045">
A practical problem arises here. For a many early
stage hypothesis P, Sp(j) is close to 0. This causes
problems because it appears as a denominator in (5)
and the argument of the log function when calculat-
ing gp. We dealt with this by either limiting the
translation probability from the null word (Brown
</bodyText>
<equation confidence="0.995947666666667">
(3)
9H =
gH =
</equation>
<page confidence="0.982156">
367
</page>
<bodyText confidence="0.999987">
et al., 1993) at the hypothetical 0-position(Brown et
al., 1993) over a threshold during the EM training,
or setting SH,,(j) to a small probability r instead of
0 for the initial null hypothesis Ho. Our experiments
show that r = 10-4 gives the best result.
</bodyText>
<subsectionHeader confidence="0.798758">
2.1.2 Heuristics
</subsectionHeader>
<bodyText confidence="0.999940444444445">
To guarantee an optimal search result, the heuris-
tic function must be an upper-bound of the score
for all possible extensions ek+iek+2 • • • el (Nilsson,
1971) of a hypothesis. In other words, the benefit
of extending a hypothesis should never be under-
estimated. Otherwise the search algorithm will con-
clude prematurely with a non-optimal hypothesis.
On the other hand, if the heuristic function over-
estimates the merit of extending a hypothesis too
much, the search algorithm will waste a huge amount
of time after it hits a correct result to safeguard the
optimality.
To estimate the language model score hm of the
unrealized part of a hypothesis, we used the nega-
tive of the language model perplexity PPtrain on the
training data as the logarithm of the average proba-
bility of predicting a new word in the extension from
a history. So we have
</bodyText>
<equation confidence="0.976231">
hfim = —(1 — k)PPtroin + C. (6)
</equation>
<bodyText confidence="0.999952727272727">
Here is the motivation behind this. We assume that
the perplexity on training data overestimates the
likelihood of the forthcoming word string on av-
erage. However, when there are only a few words
to be extended (k is close to 1), the language model
probability for the words to be extended may be
much higher than the average. This is why the con-
stant term C was introduced in (6). When k &lt;&lt; 1,
—(1— k)P Ptrain is the dominating term in (6), so the
heuristic language model score is close to the aver-
age. This can avoid overestimating the score too
much. As k is getting closer to 1, the constant term
C plays a more important role in (6) to avoid un-
derestimating the language model score. In our ex-
periments, we used C = P Ptrain+10g(Prnax), where
Pmar is the maximum ngram probability in the lan-
guage model.
To estimate the translation model score, we intro-
duce a variable vii (j), the maximum contribution to
the probability of the target sentence word gi from
any possible source language words at any position
between i and 1:
</bodyText>
<equation confidence="0.834335333333333">
vii(j) = max t(gj I e)a(k I j,l,m). (7)
i&lt;k&lt;I,eELE
here LE is the English lexicon.
</equation>
<bodyText confidence="0.9726384">
Since vii (j) is independent of hypotheses, it only
needs to be calculated once for a given target sen-
tence.
When k &lt;1, the heuristic function for the hypoth-
esis H = 1 : eie2 • • ek , is
</bodyText>
<equation confidence="0.998979666666667">
hH = E max{0, log(v(k+i)/ (j)) — log SH(j)}
j =1
—(1 — k)P Pfrain 4- C (8)
</equation>
<bodyText confidence="0.999947666666667">
where log(v(k4.1)/ (j)) — log SH(j)) is the maximum
increasement that a new word can bring to the like-
lihood of the j-th target word.
When k = 1, since no words can be appended to
the hypothesis, it is obvious that hit = 0.
This heuristic function over-estimates the score
of the upcoming words. Because of the constraints
from language model and from the fact that a posi-
tion in a source sentence cannot be occupied by two
different words, normally the placement of words in
those unfilled positions cannot maximize the likeli-
hood of all the target words simultaneously.
</bodyText>
<subsectionHeader confidence="0.99762">
2.2 Pruning and aborting search
</subsectionHeader>
<bodyText confidence="0.999994916666667">
Due to physical space limitation, we cannot keep all
hypotheses alive. We set a constant M, and when-
ever the number of hypotheses exceeds M, the al-
gorithm will prune the hypotheses with the lowest
scores. In our experiments, M was set to 20,000.
There is time limitation too. It is of little practical
interest to keep a seemingly endless search alive too
long. So we set a constant T, whenever the decoder
extends more than T hypotheses, it will abort the
search and register a failure. In our experiments, T
was set to 6000, which roughly corresponded to 2
and half hours of search effort.
</bodyText>
<subsectionHeader confidence="0.985315">
2.3 Multi-Stack Search
</subsectionHeader>
<bodyText confidence="0.99999012">
The above decoder has one problem: since the
heuristic function overestimates the merit of ex-
tending a hypothesis, the decoder always prefers
hypotheses of a long sentence, which have a bet-
ter chance to maximize the likelihood of the target
words. The decoder will extend the hypothesis with
large 1 first, and their children will soon occupy the
stack and push the hypotheses of a shorter source
sentence out of the stack. If the source sentence is
a short one, the decoder will never be able to find
it, for the hypotheses leading to it have been pruned
permanently.
This &amp;quot;incomparable&amp;quot; problem was solved with
multi-stack search(Magerman, 1994). A separate
stack was used for each hypothesized source sentence
length 1. We do compare hypotheses in different
stacks in the following cases. First, we compare a
complete sentence in a stack with the hypotheses in
other stacks to safeguard the optimality of search
result; Second, the top hypothesis in a stack is com-
pared with that of another stack. If the difference
is greater than a constant (S, then the less probable
one will not be extended. This is called soft-pruning,
since whenever the scores of the hypotheses in other
stacks go down, this hypothesis may revive.
</bodyText>
<page confidence="0.980276">
368
</page>
<figure confidence="0.998408666666667">
English —
5 10 15 20 25 30 35 40
Sentence Length
German —
10 15 20 25 30 35 40
Sentence Length
</figure>
<figureCaption confidence="0.999699">
Figure 1: Sentence Length Distribution
</figureCaption>
<sectionHeader confidence="0.9415465" genericHeader="method">
3 Stack Search with a Simplified
Model
</sectionHeader>
<bodyText confidence="0.99861425">
In the IBM translation model 2, the alignment pa-
rameters depend on the source and target sentence
length 1 and m. While this is an accurate model, it
causes the following difficulties:
</bodyText>
<listItem confidence="0.6935592">
1. there are too many parameters and therefore
too few training data per parameter. This may
not be a problem when massive training data
are available. However, in our application, this
is a severe problem. Figure 1 plots the length
distribution for the English and German sen-
tences. When sentences get longer, there are
fewer training data available.
2. the search algorithm has to make multiple hy-
potheses of different source sentence length. For
each source sentence length, it searches through
almost the same prefix words and finally set-
tles on a sentence length. This is a very time
consuming process and makes the decoder very
inefficient.
</listItem>
<bodyText confidence="0.9996402">
To solve the first problem, we adjusted the count
for the parameter a(i I i, 1, m) in the EM parameter
estimation by adding to it the counts for the pa-
rameters a(i I j,1&apos;, m&apos;), assuming (1, m) and (1&apos;, m&apos;)
are close enough. The closeness were measured in
</bodyText>
<figure confidence="0.924109333333333">
.. 9- •
. *** •
1&apos; 1
</figure>
<figureCaption confidence="0.9699285">
Figure 2: Each x/y position represents a different
source/target sentence length. The dark dot at the
intersection (1, m) corresponds to the set of counts
for the alignment parameters a(o I •,1, m) in the
EM estimation. The adjusted counts are the sum
of the counts in the neighboring sets residing inside
the circle centered at (1, m) with radius r. We took
r = 3 in our experiment.
</figureCaption>
<bodyText confidence="0.726497">
Euclidean distance (Figure 2). So we have
</bodyText>
<equation confidence="0.892685333333333">
e(i m) =
c(i nil; e, g) (9)
—ti 2&lt;r2;e,g
</equation>
<bodyText confidence="0.998002">
where e(i I j,1, m) is the adjusted count for the pa-
rameter a(i I j, 1, m), c(i I j, 1, m; e, g) is the expected
count for a(i I j,1, m) from a paired sentence (e g),
and c(i I j, I, m; e, g) = 0 when lel 0 1, or Igl m,
or i &gt; 1, or j&gt; m.
Although (9) can moderate the severity of the first
data sparse problem, it does not ease the second
inefficiency problem at all. We thus made a radi-
cal change to (9) by removing the precondition that
(1,m) and (/&apos;, m&apos;) must be close enough. This re-
sults in a simplified translation model, in which the
alignment parameters are independent of the sen-
tence length I and m:
</bodyText>
<equation confidence="0.943186">
P(i I j,m,e) = P(i I I, In)
= a(i I j)
</equation>
<bodyText confidence="0.906253222222222">
here i, j &lt; Lm, and Lm is the maximum sentence
length allowed in the translation system. A slight
change to the EM algorithm was made to estimate
the parameters.
There is a problem with this model: given a sen-
tence pair g and e, when the length of e is smaller
than Lm, then the alignment parameters do not sum
to 1:
lel
</bodyText>
<equation confidence="0.941364">
E a(i 1:7) &lt; 1. (10)
</equation>
<bodyText confidence="0.964217">
i=o
We deal with this problem by padding e to length
Lm with dummy words that never gives rise to any
word in the target of the channel.
Since the parameters are independent of the
source sentence length, we do not have to make an
</bodyText>
<figure confidence="0.998683363636364">
Number of sentences
5000
4000
3000
2000
1000
5000
4000
3000
2000
1000
</figure>
<page confidence="0.996784">
369
</page>
<bodyText confidence="0.99997">
assumption about the length in a hypothesis. When-
ever a hypothesis ends with the sentence end sym-
bol &lt;/s&gt; and its score is the highest, the decoder
reports it as the search result. In this case, a hypoth-
esis can be expressed as H = e1, e2, • • • , ek , and I H
is used to denote the length of the sentence prefix of
the hypothesis H, in this case, k.
</bodyText>
<subsectionHeader confidence="0.995052">
3.1 Heuristics
</subsectionHeader>
<bodyText confidence="0.99781975">
Since we do not make assumption of the source sen-
tence length, the heuristics described above can no
longer be applied. Instead, we used the following
heuristic function:
</bodyText>
<equation confidence="0.985294">
= E max{0, log(4111+1)(11/1+n)(i)
)}
SH
—n * PPtrain + C (11)
L--11/1
hH = E Pp(lm + rn) * .h7.1 (12)
n=1
</equation>
<bodyText confidence="0.9995891">
here hik is the heuristics for the hypothesis that ex-
tend H with n more words to complete the source
sentence (thus the final source sentence length is
IHI n.) Pp(x I y) is the Poisson distribution of the
source sentence length conditioned on the target sen-
tence length. It is used to calculate the mean of the
heuristics over all possible source sentence length. m
is the target sentence length. The parameters of the
Poisson distributions can be estimated from training
data.
</bodyText>
<sectionHeader confidence="0.994664" genericHeader="method">
4 Implementation
</sectionHeader>
<bodyText confidence="0.995008">
Due to historical reasons, stack search got its current
name. Unfortunately, the requirement for search
states organization is far beyond what a stack and
its push pop operations can handle. What we really
need is a dynamic set which supports the following
operations:
</bodyText>
<listItem confidence="0.999857">
1. INSERT: to insert a new hypothesis into the
set.
2. DELETE: to delete a state in hard pruning.
3. MAXIMUM: to find the state with the best
score to extend.
4. MINIMUM: to find the state to be pruned.
</listItem>
<bodyText confidence="0.9998838">
We used the Red-Black tree data structure (Cor-
men, Leiserson, and Rivest, 1990) to implement the
dynamic set, which guarantees that the above oper-
ations take 0(log n) time in the worst case, where n
is the number of search states in the set.
</bodyText>
<sectionHeader confidence="0.994726" genericHeader="evaluation">
5 Performance
</sectionHeader>
<bodyText confidence="0.99998375">
We tested the performance of the decoders with
the scheduling corpus(Suhm et al., 1995). Around
30,000 parallel sentences (400,000 words altogether
for both languages) were used to train the IBM
model 2 and the simplified model with the EM algo-
rithm. A larger English monolingual corpus with
around 0.5 million words was used to train a bi-
gram for language modelling. The lexicon contains
2,800 English and 4,800 German words in morpho-
logically inflected form. We did not do any prepro-
cessing/analysis of the data as reported in (Brown
et al., 1992).
</bodyText>
<subsectionHeader confidence="0.978372">
5.1 Decoder Success Rate
</subsectionHeader>
<bodyText confidence="0.9999705">
Table 1 shows the success rate of three mod-
els/decoders. As we mentioned before, the compari-
son between hypotheses of different sentence length
made the single stack search for the IBM model 2
fail (return without a result) on a majority of the
test sentences. While the multi-stack decoder im-
proved this, the simplified model/decoder produced
an output for all the 120 test sentences.
</bodyText>
<subsectionHeader confidence="0.999121">
5.2 Translation Accuracy
</subsectionHeader>
<bodyText confidence="0.9989581">
Unlike the case in speech recognition, it is quite
arguable what &amp;quot;accurate translations&amp;quot; means. In
speech recognition an output can be compared with
the sample transcript of the test data. In machine
translation, a sentence may have several legitimate
translations. It is difficult to compare an output
from a decoder with a designated translation. In-
stead, we used human subjects to judge the machine-
made translations. The translations are classified
into three categories&apos;.
</bodyText>
<listItem confidence="0.997034875">
1. Correct translations: translations that are
grammatical and convey the same meaning as
the inputs.
2. Okay translations: translations that convey the
same meaning but with small grammatical mis-
takes or translations that convey most but not
the entire meaning of the input.
3. Incorrect translations: Translations that are
</listItem>
<bodyText confidence="0.987104388888889">
ungrammatical or convey little meaningful in-
formation or the information is different from
the input.
Examples of correct, okay, and incorrect transla-
tions are shown in Table 2.
Table 3 shows the statistics of the translation re-
sults. The accuracy was calculate by crediting a cor-
rect translation 1 point and an okay translation 1/2
point.
There are two different kinds of errors in statis-
tical machine translation. A modeling error occurs
when the model assigns a higher score to an incor-
rect translation than a correct one. We cannot do
anything about this with the decoder. A decoding
&apos;This is roughly the same as the classification in IBM
statistical translation, except we do not have &amp;quot;legitimate
translation that conveys different meaning from the in-
put&amp;quot; — we did not observed this case in our outputs.
</bodyText>
<page confidence="0.98964">
370
</page>
<table confidence="0.998022">
Total Test Sentences Decoded Sentenced Failed sentences
Model 2, Single Stack 120 32 88
Model 2, Multi-Stack 120 83 37
Simplified Model 120 120 0
</table>
<tableCaption confidence="0.837928">
Table 1: Decoder Success Rate
</tableCaption>
<table confidence="0.999900888888889">
Correct German ich habe em n Meeting von halb zehn bis um zwolf
English (target) I have a meeting from nine thirty to twelve
English (output) I have a meeting from nine thirty to twelve
German versuchen wir sollten es vielleicht mit einem anderen Termin
English (target) we might want to try for some other time
English (output) we should try another time
Okay German ich glaube nicht das ich noch irgend etwas im Januar frei habe
English (target) I do not think I have got anything open in January
English (output) I think I will not free in January
German ich glaube wir sollten em n weiteres Meeting vereinbaren
English (target) I think we have to have another meeting
English (output) I think we should fix a meeting
Incorrect German schlagen Sie doch einen Termin vor
English (target) why don&apos;t you suggest a time
English (output) why you an appointment
German ich habe Zeit fiir den Rest des Tages
English (target) I am free the rest of it
English (output) I have time for the rest of July
</table>
<tableCaption confidence="0.979556">
Table 2: Examples of Correct, Okay, and Incorrect Translations: for each translation, the first line is an
</tableCaption>
<bodyText confidence="0.999727702702703">
input German sentence, the second line is the human made (target) translation for that input sentence, and
the third line is the output from the decoder.
error or search error happens when the search al-
gorithm misses a correct translation with a higher
score.
When evaluating a decoding algorithm, it would
be attractive if we can tell how many errors are
caused by the decoder. Unfortunately, this is not
attainable. Suppose that we are going to translate a
German sentence g, and we know from the sample
that e is one of its possible English translations. The
decoder outputs an incorrect e&apos; as the translation of
g. If the score of e&apos; is lower than that of e, we know
that a search error has occurred. On the other hand,
if the score of e&apos; is higher, we cannot decide if it is a
modeling error or not, since there may still be other
legitimate translations with a score higher than e&apos;
— we just do not know what they are.
Although we cannot distinguish a modeling error
from a search error, the comparison between the de-
coder output&apos;s score and that of a sample transla-
tion can still reveal some information about the per-
formance of the decoder. If we know that the de-
coder can find a sentence with a better score than
a &amp;quot;correct&amp;quot; translation, we will be more confident
that the decoder is less prone to cause errors. Ta-
ble 4 shows the comparison between the score of the
outputs from the decoder and the score of the sam-
ple translations when the outputs are incorrect. In
most cases, the incorrect outputs have a higher score
than the sample translations. Again, we consider a
&amp;quot;okay&amp;quot; translation a half error here.
This result hints that model deficiencies may be a
major source of errors. The models we used here are
very simple. With a more sophisticated model, more
training data, and possibly some preprocessing, the
total error rate is expected to decrease.
</bodyText>
<subsectionHeader confidence="0.998931">
5.3 Decoding Speed
</subsectionHeader>
<bodyText confidence="0.999954818181818">
Another important issue is the efficiency of the de-
coder. Figure 3 plots the average number of states
being extended by the decoders. It is grouped ac-
cording to the input sentence length, and evaluated
on those sentences on which the decoder succeeded.
The average number of states being extended in
the model 2 single stack search is not available for
long sentences, since the decoder failed on most of
the long sentences.
The figure shows that the simplified model/decoder
works much more efficiently than the other mod-
</bodyText>
<page confidence="0.996849">
371
</page>
<table confidence="0.984628">
Total Correct Okay Incorrect Accuracy
Model 2, Multi-Stack 83 39 12 32 54.2%
Simplified Model 120 64 15 41 59.6%
</table>
<tableCaption confidence="0.986972">
Table 3: Translation Accuracy
</tableCaption>
<table confidence="0.997904666666667">
Total Errors Scoree &gt; Scoree, Scoree &lt; Scoree,
Model 2, Multi-Stack 38 3.5 (7.9%) 34.5 ( 92.1%)
Simplified Model 48.5 4.5 (9.3%) 44 (90.7%)
</table>
<tableCaption confidence="0.99952">
Table 4: Sample Translations versus Machine-Made Translations
</tableCaption>
<figure confidence="0.994283333333333">
6000
Average Number of Expanded States 5000
4000
3000
2000
1000
</figure>
<figureCaption confidence="0.9633665">
Figure 3: Extended States versus Target Sentence
Length
</figureCaption>
<bodyText confidence="0.935981">
els/decoders.
</bodyText>
<sectionHeader confidence="0.999421" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999855">
We have reported a stack decoding algorithm for the
IBM statistical translation model 2 and a simpli-
fied model. Because the simplified model has fewer
Darameters and does not have to posit hypotheses
with the same prefixes but different length, it out-
performed the IBM model 2 with regard to both
accuracy and efficiency, especially in our application
that lacks a massive amount of training data. In
most cases, the erroneous outputs from the decoder
have a higher score than the human made transla-
tions. Therefore it is less likely that the decoder is
a major contributor of translation errors.
</bodyText>
<sectionHeader confidence="0.999145" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999919571428571">
We would like to thank John Lafferty for enlight-
ening discussions on this work. We would also like
to thank the anonymous ACL reviewers for valuable
comments. This research was partly supported by
ATR and the Verbmobil Project. The views and
conclusions in this document are those of the au-
thors.
</bodyText>
<sectionHeader confidence="0.998899" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998369">
Brown, P. F., S. A. Della-Pietra, V. J Della-Pietra,
and R. L. Mercer. 1993. The Mathematics of Sta-
tistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19(2):263-311.
Brown, P. F., S. A. Della Pietra, V. J. Della Pietra,
J. D. Lafferty, and R. L. Mercer. 1992. Analy-
sis, Statistical Transfer, and Synthesis in Machine
Translation. In Proceedings of the fourth Interna-
tional Conference on Theoretical and Methodolog-
ical Issues in Machine Translation, pages 83-100.
Cormen, Thomas H., Charles E. Leiserson, and
Ronald L. Rivest. 1990. Introduction to Al-
gorithms. The MIT Press, Cambridge, Mas-
sachusetts.
Magerman, D. 1994. Natural Language Parsing
as Statistical Pattern Recognition. Ph.D. thesis,
Stanford University.
Nilsson, N. 1971. Problem-Solving Methods in Arti-
ficial Intelligence. McGraw Hill, New York, New
York.
Suhm, B., P.Geutner, T. Kemp, A. Lavie, L. May-
field, A. McNair, I. Rogina, T. Schultz, T. Slo-
boda, W. Ward, M. Woszczyna, and A. Waibel.
1995. JANUS: Towards multilingual spoken lan-
guage translation. In Proceedings of the ARPA
Speech Spoken Language Technology Workshop,
Austin, TX, 1995.
Vogel, S., H. Ney, and C. Tillman. 1996. HMM-
Based Word Alignment in Statistical Transla-
tion. In Proceedings of the Seventeenth Interna-
tional Conference on Computational Linguistics:
COLING-96, pages 836-841, Copenhagen, Den-
mark.
</reference>
<figure confidence="0.9968516">
&amp;quot;Mode12-Single-Stack&amp;quot;
&amp;quot;Mode12-Multi-Stack&amp;quot;
&amp;quot;Simplified-Model&amp;quot; -----
1-4 5-8 9-12 13-16
Target Sentence Length
</figure>
<page confidence="0.967497">
372
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.783492">
<title confidence="0.999964">Decoding Algorithm in Statistical Machine Translation</title>
<author confidence="0.999941">Ye-Yi Wang</author>
<author confidence="0.999941">Alex Waibel</author>
<affiliation confidence="0.999683">Language Technology Institute School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.997958">5000 Forbes Avenue Pittsburgh, PA 15213, USA</address>
<email confidence="0.999786">yywOcs.cmu.edu</email>
<email confidence="0.999786">waibelOcs.cmu.edu</email>
<abstract confidence="0.984804857142857">Decoding algorithm is a crucial part in statistical machine translation. We describe a stack decoding algorithm in this paper. We present the hypothesis scoring method and the heuristics used in our algorithm. We report several techniques deployed to improve the performance of the decoder. We also introduce a simplified model to moderate the sparse data problem and to speed up the decoding process. We evaluate and compare these techniques/models in our statistical machine translation system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della-Pietra</author>
<author>V J Della-Pietra</author>
<author>R L Mercer</author>
</authors>
<date>1993</date>
<booktitle>The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics,</booktitle>
<pages>19--2</pages>
<contexts>
<context position="2466" citStr="Brown et al., 1993" startWordPosition="406" endWordPosition="409">roblem of language modeling, and the later is the problem of translation modeling. They provide a framework for calculating P(S) and P(T I S) in (2). • Learning Problem: Given a statistical language model P(S) and a statistical translation model P(T I S), how to estimate the parameters in these models from a bilingual corpus of sentences? • Decoding Problem: With a fully specified (framework and parameters) language and translation model, given a target sentence T, how to efficiently search for the source sentence S&apos; that satisfies (2). The modeling and learning issues have been discussed in (Brown et al., 1993), where ngram model was used for language modeling, and five different translation models were introduced for the translation process. We briefly introduce the model 2 here, for which we built our decoder. In model 2, upon receiving a source English sentence e = el , • • , el, the channel generates a German sentence g = gi, • • • , gni at the target end in the following way: 1. With a distribution P(in I e), randomly choose the length m of the German translation g. In model 2, the distribution is independent of m and e: P(rn I e) = where is a small, fixed number. 2. For each position i (0 &lt; i </context>
<context position="3902" citStr="Brown et al., 1993" startWordPosition="700" endWordPosition="703"> i, a1, m, e) = a(cti I i,m, I) 3. Generate the word gi at the position i of the German sentence from the English word ea, at 366 the aligned position ai of gi, according to a translation distribution P(gi I ar e) = t(giI e„,). The distribution here only depends on gi and ea,. Therefore, P(g I e) is the sum of the probabilities of generating g from e over all possible alignments A, in which the position i in the target sentence g is aligned to the position ai in the source sentence e: P (g I e) = I m ef•••E ilt(gi I eoi)a(ai I j,l,m) = ni=0 a„,=0 j=1 m 1 11 E t(gi ei)a(i I j, 1 , m) j=1. i=o (Brown et al., 1993) also described how to use the EM algorithm to estimate the parameters a(i I j, 1, in) and t(g I e) in the aforementioned model. 1.2 Decoding in Statistical Machine Translation (Brown et al., 1993) and (Vogel, Ney, and Tillman, 1996) have discussed the first two of the three problems in statistical machine translation. Although the authors of (Brown et al., 1993) stated that they would discuss the search problem in a follow-up article, so far there have no publications devoted to the decoding issue for statistical machine translation. On the other side, decoding algorithm is a crucial part in </context>
<context position="7582" citStr="Brown et al., 1993" startWordPosition="1392" endWordPosition="1395">is H = 1 : e1e2 • • • ek can be calculated from the 9-score of its parent hypothesis P = 1: e1 e2 • • • ek- 1: gp + log P(ek I ek-N-1-1 &amp;quot; • ek-1) rn logo. ct(gi I ek)a(k I j, in) Sp (j) i=o SH(j) = Sp(j) + et(gi ek)a(k j,1, in) (5) A practical problem arises here. For a many early stage hypothesis P, Sp(j) is close to 0. This causes problems because it appears as a denominator in (5) and the argument of the log function when calculating gp. We dealt with this by either limiting the translation probability from the null word (Brown (3) 9H = gH = 367 et al., 1993) at the hypothetical 0-position(Brown et al., 1993) over a threshold during the EM training, or setting SH,,(j) to a small probability r instead of 0 for the initial null hypothesis Ho. Our experiments show that r = 10-4 gives the best result. 2.1.2 Heuristics To guarantee an optimal search result, the heuristic function must be an upper-bound of the score for all possible extensions ek+iek+2 • • • el (Nilsson, 1971) of a hypothesis. In other words, the benefit of extending a hypothesis should never be underestimated. Otherwise the search algorithm will conclude prematurely with a non-optimal hypothesis. On the other hand, if the heuristic fun</context>
</contexts>
<marker>Brown, Della-Pietra, Della-Pietra, Mercer, 1993</marker>
<rawString>Brown, P. F., S. A. Della-Pietra, V. J Della-Pietra, and R. L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2):263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>J D Lafferty</author>
<author>R L Mercer</author>
</authors>
<title>Analysis, Statistical Transfer, and Synthesis in Machine Translation.</title>
<date>1992</date>
<booktitle>In Proceedings of the fourth International Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<pages>83--100</pages>
<contexts>
<context position="17955" citStr="Brown et al., 1992" startWordPosition="3291" endWordPosition="3294">ase, where n is the number of search states in the set. 5 Performance We tested the performance of the decoders with the scheduling corpus(Suhm et al., 1995). Around 30,000 parallel sentences (400,000 words altogether for both languages) were used to train the IBM model 2 and the simplified model with the EM algorithm. A larger English monolingual corpus with around 0.5 million words was used to train a bigram for language modelling. The lexicon contains 2,800 English and 4,800 German words in morphologically inflected form. We did not do any preprocessing/analysis of the data as reported in (Brown et al., 1992). 5.1 Decoder Success Rate Table 1 shows the success rate of three models/decoders. As we mentioned before, the comparison between hypotheses of different sentence length made the single stack search for the IBM model 2 fail (return without a result) on a majority of the test sentences. While the multi-stack decoder improved this, the simplified model/decoder produced an output for all the 120 test sentences. 5.2 Translation Accuracy Unlike the case in speech recognition, it is quite arguable what &amp;quot;accurate translations&amp;quot; means. In speech recognition an output can be compared with the sample tr</context>
</contexts>
<marker>Brown, Pietra, Pietra, Lafferty, Mercer, 1992</marker>
<rawString>Brown, P. F., S. A. Della Pietra, V. J. Della Pietra, J. D. Lafferty, and R. L. Mercer. 1992. Analysis, Statistical Transfer, and Synthesis in Machine Translation. In Proceedings of the fourth International Conference on Theoretical and Methodological Issues in Machine Translation, pages 83-100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas H Cormen</author>
<author>Charles E Leiserson</author>
<author>Ronald L Rivest</author>
</authors>
<title>Introduction to Algorithms.</title>
<date>1990</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="17228" citStr="Cormen, Leiserson, and Rivest, 1990" startWordPosition="3164" endWordPosition="3169">s of the Poisson distributions can be estimated from training data. 4 Implementation Due to historical reasons, stack search got its current name. Unfortunately, the requirement for search states organization is far beyond what a stack and its push pop operations can handle. What we really need is a dynamic set which supports the following operations: 1. INSERT: to insert a new hypothesis into the set. 2. DELETE: to delete a state in hard pruning. 3. MAXIMUM: to find the state with the best score to extend. 4. MINIMUM: to find the state to be pruned. We used the Red-Black tree data structure (Cormen, Leiserson, and Rivest, 1990) to implement the dynamic set, which guarantees that the above operations take 0(log n) time in the worst case, where n is the number of search states in the set. 5 Performance We tested the performance of the decoders with the scheduling corpus(Suhm et al., 1995). Around 30,000 parallel sentences (400,000 words altogether for both languages) were used to train the IBM model 2 and the simplified model with the EM algorithm. A larger English monolingual corpus with around 0.5 million words was used to train a bigram for language modelling. The lexicon contains 2,800 English and 4,800 German wo</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, 1990</marker>
<rawString>Cormen, Thomas H., Charles E. Leiserson, and Ronald L. Rivest. 1990. Introduction to Algorithms. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Magerman</author>
</authors>
<title>Natural Language Parsing as Statistical Pattern Recognition.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="11913" citStr="Magerman, 1994" startWordPosition="2171" endWordPosition="2172">m: since the heuristic function overestimates the merit of extending a hypothesis, the decoder always prefers hypotheses of a long sentence, which have a better chance to maximize the likelihood of the target words. The decoder will extend the hypothesis with large 1 first, and their children will soon occupy the stack and push the hypotheses of a shorter source sentence out of the stack. If the source sentence is a short one, the decoder will never be able to find it, for the hypotheses leading to it have been pruned permanently. This &amp;quot;incomparable&amp;quot; problem was solved with multi-stack search(Magerman, 1994). A separate stack was used for each hypothesized source sentence length 1. We do compare hypotheses in different stacks in the following cases. First, we compare a complete sentence in a stack with the hypotheses in other stacks to safeguard the optimality of search result; Second, the top hypothesis in a stack is compared with that of another stack. If the difference is greater than a constant (S, then the less probable one will not be extended. This is called soft-pruning, since whenever the scores of the hypotheses in other stacks go down, this hypothesis may revive. 368 English — 5 10 15 </context>
</contexts>
<marker>Magerman, 1994</marker>
<rawString>Magerman, D. 1994. Natural Language Parsing as Statistical Pattern Recognition. Ph.D. thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Nilsson</author>
</authors>
<title>Problem-Solving Methods in Artificial Intelligence.</title>
<date>1971</date>
<publisher>McGraw Hill,</publisher>
<location>New York, New York.</location>
<contexts>
<context position="7951" citStr="Nilsson, 1971" startWordPosition="1459" endWordPosition="1460">nator in (5) and the argument of the log function when calculating gp. We dealt with this by either limiting the translation probability from the null word (Brown (3) 9H = gH = 367 et al., 1993) at the hypothetical 0-position(Brown et al., 1993) over a threshold during the EM training, or setting SH,,(j) to a small probability r instead of 0 for the initial null hypothesis Ho. Our experiments show that r = 10-4 gives the best result. 2.1.2 Heuristics To guarantee an optimal search result, the heuristic function must be an upper-bound of the score for all possible extensions ek+iek+2 • • • el (Nilsson, 1971) of a hypothesis. In other words, the benefit of extending a hypothesis should never be underestimated. Otherwise the search algorithm will conclude prematurely with a non-optimal hypothesis. On the other hand, if the heuristic function overestimates the merit of extending a hypothesis too much, the search algorithm will waste a huge amount of time after it hits a correct result to safeguard the optimality. To estimate the language model score hm of the unrealized part of a hypothesis, we used the negative of the language model perplexity PPtrain on the training data as the logarithm of the av</context>
</contexts>
<marker>Nilsson, 1971</marker>
<rawString>Nilsson, N. 1971. Problem-Solving Methods in Artificial Intelligence. McGraw Hill, New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Suhm</author>
<author>T Kemp P Geutner</author>
<author>A Lavie</author>
<author>L Mayfield</author>
<author>A McNair</author>
<author>I Rogina</author>
<author>T Schultz</author>
<author>T Sloboda</author>
<author>W Ward</author>
<author>M Woszczyna</author>
<author>A Waibel</author>
</authors>
<title>JANUS: Towards multilingual spoken language translation.</title>
<date>1995</date>
<booktitle>In Proceedings of the ARPA Speech Spoken Language Technology Workshop,</booktitle>
<location>Austin, TX,</location>
<contexts>
<context position="17493" citStr="Suhm et al., 1995" startWordPosition="3213" endWordPosition="3216">e really need is a dynamic set which supports the following operations: 1. INSERT: to insert a new hypothesis into the set. 2. DELETE: to delete a state in hard pruning. 3. MAXIMUM: to find the state with the best score to extend. 4. MINIMUM: to find the state to be pruned. We used the Red-Black tree data structure (Cormen, Leiserson, and Rivest, 1990) to implement the dynamic set, which guarantees that the above operations take 0(log n) time in the worst case, where n is the number of search states in the set. 5 Performance We tested the performance of the decoders with the scheduling corpus(Suhm et al., 1995). Around 30,000 parallel sentences (400,000 words altogether for both languages) were used to train the IBM model 2 and the simplified model with the EM algorithm. A larger English monolingual corpus with around 0.5 million words was used to train a bigram for language modelling. The lexicon contains 2,800 English and 4,800 German words in morphologically inflected form. We did not do any preprocessing/analysis of the data as reported in (Brown et al., 1992). 5.1 Decoder Success Rate Table 1 shows the success rate of three models/decoders. As we mentioned before, the comparison between hypothe</context>
</contexts>
<marker>Suhm, Geutner, Lavie, Mayfield, McNair, Rogina, Schultz, Sloboda, Ward, Woszczyna, Waibel, 1995</marker>
<rawString>Suhm, B., P.Geutner, T. Kemp, A. Lavie, L. Mayfield, A. McNair, I. Rogina, T. Schultz, T. Sloboda, W. Ward, M. Woszczyna, and A. Waibel. 1995. JANUS: Towards multilingual spoken language translation. In Proceedings of the ARPA Speech Spoken Language Technology Workshop, Austin, TX, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>H Ney</author>
<author>C Tillman</author>
</authors>
<title>HMMBased Word Alignment in Statistical Translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the Seventeenth International Conference on Computational Linguistics: COLING-96,</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="4134" citStr="Vogel, Ney, and Tillman, 1996" startWordPosition="740" endWordPosition="744">e„,). The distribution here only depends on gi and ea,. Therefore, P(g I e) is the sum of the probabilities of generating g from e over all possible alignments A, in which the position i in the target sentence g is aligned to the position ai in the source sentence e: P (g I e) = I m ef•••E ilt(gi I eoi)a(ai I j,l,m) = ni=0 a„,=0 j=1 m 1 11 E t(gi ei)a(i I j, 1 , m) j=1. i=o (Brown et al., 1993) also described how to use the EM algorithm to estimate the parameters a(i I j, 1, in) and t(g I e) in the aforementioned model. 1.2 Decoding in Statistical Machine Translation (Brown et al., 1993) and (Vogel, Ney, and Tillman, 1996) have discussed the first two of the three problems in statistical machine translation. Although the authors of (Brown et al., 1993) stated that they would discuss the search problem in a follow-up article, so far there have no publications devoted to the decoding issue for statistical machine translation. On the other side, decoding algorithm is a crucial part in statistical machine translation. Its performance directly affects the quality and efficiency of translation. Without a good and efficient decoding algorithm, a statistical machine translation system may miss the best translation of </context>
</contexts>
<marker>Vogel, Ney, Tillman, 1996</marker>
<rawString>Vogel, S., H. Ney, and C. Tillman. 1996. HMMBased Word Alignment in Statistical Translation. In Proceedings of the Seventeenth International Conference on Computational Linguistics: COLING-96, pages 836-841, Copenhagen, Denmark.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>