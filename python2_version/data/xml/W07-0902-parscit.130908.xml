<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000136">
<title confidence="0.900577">
Viterbi Based Alignment between Text Images and their Transcripts∗
Alejandro H. Toselli, Ver´onica Romero and Enrique Vidal
Institut Tecnol`ogic d’Inform`atica
Universitat Polit`ecnica de Val`encia
Camide Vera s/n
</title>
<address confidence="0.621961">
46071 - Val`encia, Spain
</address>
<email confidence="0.99236">
[ahector,vromero,evidal]@iti.upv.es
</email>
<sectionHeader confidence="0.998548" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99987025">
An alignment method based on the Viterbi
algorithm is proposed to find mappings be-
tween word images of a given handwrit-
ten document and their respective (ASCII)
words on its transcription. The approach
takes advantage of the underlying segmen-
tation made by Viterbi decoding in hand-
written text recognition based on Hidden
Markov Models (HMMs). Two HMMs
modelling schemes are evaluated: one using
78-HMMs (one HMM per character class)
and other using a unique HMM to model all
the characters and another to model blank
spaces. According to various metrics used
to measure the quality of the alignments, en-
couraging results are obtained.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998655444444445">
Recently, many on-line digital libraries have been
publishing large quantities of digitized ancient hand-
written documents, which allows the general pub-
lic to access this kind of cultural heritage resources.
This is a new, comfortable way of consulting and
querying this material. The Biblioteca Valenciana
Digital (BiValDi)1 is an example of one such digital
library, which provides an interesting collection of
handwritten documents.
</bodyText>
<note confidence="0.89323475">
This work has been supported by the EC (FEDER), the
Spanish MEC under grant TIN2006-15694-C02-01, and by the
Consellerta d’Empresa, Universitat i CiMcia - Generalitat Va-
lenciana under contract GV06/252.
</note>
<footnote confidence="0.747734">
1http://bv2.gva.es
</footnote>
<bodyText confidence="0.999903058823529">
Several of these handwritten documents include
both, the handwritten material and its proper tran-
scription (in ASCII format). This fact has moti-
vated the development of methodologies to align
these documents and their transcripts; i.e. to gen-
erate a mapping between each word image on a doc-
ument page with its respective ASCII word on its
transcript. This word by word alignment would al-
low users to easily find the place of a word in the
manuscript when reading the corresponding tran-
script. For example, one could display both the
handwritten page and the transcript and whenever
the mouse is held over a word in the transcript, the
corresponding word in the handwritten image would
be outlined using a box. In a similar way, whenever
the mouse is held over a word in the handwritten im-
age, the corresponding word in the transcript would
be highlighted (see figure 1). This kind of alignment
can help paleography experts to quickly locate im-
age text while reading a transcript, with useful ap-
plications to editing, indexing, etc. In the opposite
direction, the alignment can also be useful for people
trying to read the image text directly, when arriving
to complex or damaged parts of the document.
Creating such alignments is challenging since the
transcript is an ASCII text file while the manuscript
page is an image. Some recent works address this
problem by relying on a previous explicit image-
processing based word pre-segmentation of the page
image, before attempting the transcription align-
ments. For example, in (Kornfield et al., 2004),
the set of previously segmented word images and
their corresponding transcriptions are transformed
into two different times series, which are aligned
</bodyText>
<page confidence="0.976134">
9
</page>
<note confidence="0.9120365">
Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 9–16,
Prague, 28 June 2007. c�2007 Association for Computational Linguistics
</note>
<figureCaption confidence="0.984871">
Figure 1: Screen-shot of the alignment prototype interface displaying an outlined word (using a box) in the
manuscript (left) and the corresponding highlighted word in the transcript (right).
</figureCaption>
<bodyText confidence="0.999605323529412">
using dynamic time warping (DTW). In this same
direction, (Huang and Srihari, 2006), in addition to
the word pre-segmentation, attempt a (rough) recog-
nition of the word images. The resulting word string
is then aligned with the transcription using dynamic
programming.
The alignment method presented here (hencefor-
ward called Viterbi alignment), relies on the Viterbi
decoding approach to handwritten text recogni-
tion (HTR) based on Hidden Markov Models
(HMMs) (Bazzi et al., 1999; Toselli et al., 2004).
These techniques are based on methods originally
introduced for speech recognition (Jelinek, 1998).
In such HTR systems, the alignment is actually a
byproduct of the proper recognition process, i.e. an
implicit segmentation of each text image line is ob-
tained where each segment successively corresponds
to one recognized word. In our case, word recogni-
tion is not actually needed, as we do already have
the correct transcription. Therefore, to obtain the
segmentations for the given word sequences, the so-
called “forced-recognition” approach is employed
(see section 2.2). This idea has been previously ex-
plored in (Zimmermann and Bunke, 2002).
Alignments can be computed line by line in cases
where the beginning and end positions of lines are
known or, in a more general case, for whole pages.
We show line-by-line results on a set of 53 pages
from the “Cristo-Salvador” handwritten document
(see section 5.2). To evaluate the quality of the ob-
tained alignments, two metrics were used which give
information at different alignment levels: one mea-
sures the accuracy of alignment mark placements
and the other measures the amount of erroneous as-
</bodyText>
<page confidence="0.996627">
10
</page>
<figureCaption confidence="0.9352318">
Figure 2: Example of 5-states HMM modeling (feature vectors sequences) of instances of the character “a”
within the Spanish word “cuarenta” (forty). The states are shared among all instances of characters of the
same class. The zones modelled by each state show graphically subsequences of feature vectors (see details
in the magnifying-glass view) compounded by stacking the normalized grey level and its both derivatives
features.
</figureCaption>
<figure confidence="0.994143111111111">
0.2
0.1
0.2
0.3
0.9
0.8
0.7
0.3
0.7 0.8
</figure>
<bodyText confidence="0.998934666666667">
signments produced between word images and tran-
scriptions (see section 4).
The remainder of this paper is organized as fol-
lows. First, the alignment framework is introduced
and formalized in section 2. Then, an implemented
prototype is described in section 3. The alignment
evaluation metrics are presented in section 4. The
experiments and results are commented in section 5.
Finally, some conclusions are drawn in section 6.
</bodyText>
<sectionHeader confidence="0.849529" genericHeader="method">
2 HMM-based HTR and Viterbi alignment
</sectionHeader>
<bodyText confidence="0.999833">
HMM-based handwritten text recognition is briefly
outlined in this section, followed by a more detailed
presentation of the Viterbi alignment approach.
</bodyText>
<subsectionHeader confidence="0.95246">
2.1 HMM HTR Basics
</subsectionHeader>
<bodyText confidence="0.997975833333333">
The traditional handwritten text recognition problem
can be formulated as the problem of finding a most
likely word sequence w� = (w1, w2,... , wn), for
a given handwritten sentence (or line) image rep-
resented by a feature vector sequence x = xp 1 =
(x1, x2,. . . , xp), that is:
</bodyText>
<equation confidence="0.99874">
w� = arg max Pr(w|x)
W
= arg max Pr(x|w) · Pr(w) (1)
W
</equation>
<bodyText confidence="0.9998215">
where Pr(x|w) is usually approximated by
concatenated character Hidden Markov Models
(HMMs) (Jelinek, 1998; Bazzi et al., 1999),
whereas Pr(w) is approximated typically by an
n-gram word language model (Jelinek, 1998).
Thus, each character class is modeled by a con-
tinuous density left-to-right HMM, characterized by
a set of states and a Gaussian mixture per state. The
Gaussian mixture serves as a probabilistic law to
model the emission of feature vectors by each HMM
state. Figure 2 shows an example of how a HMM
models a feature vector sequence corresponding to
</bodyText>
<page confidence="0.982473">
11
</page>
<figure confidence="0.873829">
b0 b1 b2 b3 b4 b5 b6 bn=7
</figure>
<figureCaption confidence="0.844005666666667">
Figure 3: Example of segmented text line image along with its resulting deslanted and size-normalized
image. Moreover, the alignment marks (b0 ... b8) which delimit each of the words (including word-spaces)
over the text image feature vectors sequence x.
</figureCaption>
<figure confidence="0.4660665">
x1
xp
w1 w2 w3 w4 w5 w6
wn=7
</figure>
<bodyText confidence="0.980565538461538">
character “a”. The process to obtain feature vector
sequences from text images as well as the training of
HMMs are explained in section 3.
HMMs as well as n-grams models can be rep-
resented by stochastic finite state networks (SFN),
which are integrated into a single global SFN by re-
placing each word character of the n-gram model by
the corresponding HMM. The search involved in the
equation (1) to decode the input feature vectors se-
quence x into the more likely output word sequence
w, is performed over this global SFN. This search
problem is adequately solved by the Viterbi algo-
rithm (Jelinek, 1998).
</bodyText>
<subsectionHeader confidence="0.997732">
2.2 Viterbi Alignment
</subsectionHeader>
<bodyText confidence="0.971062181818182">
As a byproduct of the Viterbi solution to (1), the
feature vectors subsequences of x aligned with each
of the recognized words w1, w2, ... , wn can be ob-
tained. These implicit subsequences can be visual-
ized into the equation (1) as follows:
where b is the optimal alignment. In our case,
we are not really interested in proper text recogni-
tion because the transcription is known beforehand.
Let w� be the given transcription. Now, Pr(w) in
equation 3 is zero for all w except w, for which
Pr( w) = 1. Therefore,
</bodyText>
<figure confidence="0.990799142857143">
b = arg max Pr(x, b |w) (4)
b
which can be expanded to,
b = arg max Pr(x, b1 |w)Pr(x, b2|b1, w) ...
b
... Pr(x, bn|b1b2 ... bn−1, W)
(5)
</figure>
<bodyText confidence="0.98027825">
Assuming independence of each bi mark from
b1b2 ... bi−1 and assuming that each subsequence
xb b1 depends only of wi, equation (5) can be rewrit-
ten as,
</bodyText>
<equation confidence="0.993262333333333">
w� = arg max � Pr(x, b|w) · Pr(w) (2) b = arg max Pr(xb1
w b b b� |w1) ... Pr(xb�
b��1 |�wn) (6)
</equation>
<bodyText confidence="0.999759142857143">
where b is an alignment; that is, an ordered se-
quence of n+1 marks (b0, b1, ... , bn), used to de-
marcate the subsequences belonging to each recog-
nized word. The marks b0 and bn always point out
to the first and last components of x (see figure 3).
Now, approximating the sum in (2) by the domi-
nant term:
</bodyText>
<equation confidence="0.95326">
Pr(x, b|w) · Pr(w) (3)
</equation>
<bodyText confidence="0.976082">
This simpler Viterbi search problem is known as
“forced recognition”.
</bodyText>
<sectionHeader confidence="0.951199" genericHeader="method">
3 Overview of the Alignment Prototype
</sectionHeader>
<bodyText confidence="0.9995525">
The implementation of the alignment prototype in-
volved four different parts: document image prepro-
cessing, line image feature extraction, HMMs train-
ing and alignment map generation.
</bodyText>
<equation confidence="0.607948">
w� Pz� arg max
w
</equation>
<page confidence="0.565895333333333">
max
b
12
</page>
<bodyText confidence="0.99991878125">
Document image preprocessing encompasses the
following steps: first, skew correction is carried out
on each document page image; then background
removal and noise reduction is performed by ap-
plying a bi-dimensional median filter (Kavalliera-
tou and Stamatatos, 2006) on the whole page im-
age. Next, a text line extraction process based on
local minimums of the horizontal projection profile
of page image, divides the page into separate line
images (Marti and Bunke, 2001). In addition con-
nected components has been used to solve the situ-
ations where local minimum values are greater than
zero, making impossible to obtain a clear text line
separation. Finally, slant correction and non-linear
size normalization are applied (Toselli et al., 2004;
Romero et al., 2006) on each extracted line image.
An example of extracted text line image is shown
in the top panel of figure 3, along with the result-
ing deslanted and size-normalized image. Note how
non-linear normalization leads to reduced sizes of
ascenders and descenders, as well as to a thiner un-
derline of the word “ciudadanos”.
As our alignment prototype is based on Hid-
den Markov Models (HMMs), each preprocessed
line image is represented as a sequence of feature
vectors. To do this, the feature extraction mod-
ule applies a grid to divide line image into N x
M squared cells. In this work, N = 40 is cho-
sen empirically (using the corpus described further
on) and M must satisfy the condition M/N =
original image aspect ratio. From each cell, three
features are calculated: normalized gray level, hor-
izontal gray level derivative and vertical gray level
derivative. The way these three features are deter-
mined is described in (Toselli et al., 2004). Columns
of cells or frames are processed from left to right
and a feature vector is constructed for each frame
by stacking the three features computed in its con-
stituent cells.
Hence, at the end of this process, a sequence of
M 120-dimensional feature vectors (40 normalized
gray-level components, 40 horizontal and 40 vertical
derivatives components) is obtained. An example of
feature vectors sequence, representing an image of
the Spanish word “cuarenta” (forty) is shown in fig-
ure 2.
As it was explained in section 2.1, characters are
modeled by continuous density left-to-right HMMs
with 6 states and 64 Gaussian mixture components
per state. This topology (number of HMM states and
Gaussian densities per state) was determined by tun-
ing empirically the system on the corpus described
in section 5.1. Once a HMM “topology” has been
adopted, the model parameters can be easily trained
from images of continuously handwritten text (with-
out any kind of segmentation) accompanied by the
transcription of these images into the correspond-
ing sequence of characters. This training process is
carried out using a well known instance of the EM
algorithm called forward-backward or Baum-Welch
re-estimation (Jelinek, 1998).
The last phase in the alignment process is the gen-
eration of the mapping proper by means of Viterbi
“forced recognition”, as discussed in section 2.2.
</bodyText>
<sectionHeader confidence="0.992638" genericHeader="method">
4 Alignment Evaluation Metrics
</sectionHeader>
<bodyText confidence="0.999722571428571">
Two kinds of measures have been adopted to evalu-
ate the quality of alignments. On the one hand, the
average value and standard deviation (henceforward
called MEAN-STD) of the absolute differences be-
tween the system-proposed word alignment marks
and their corresponding (correct) references. This
gives us an idea of the geometrical accuracy of the
alignments obtained. On the other hand, the align-
ment error rate (AER), which measures the amount
of erroneous assignments produced between word
images and transcriptions.
Given a reference mark sequence r =
(r0, r1, ... , rn) along with an associated to-
kens sequence w = (w1, w2,... , wn), and a
segmentation marks sequence b = (b0, b1, ... , bn)
(with r0 =b0 n rn =bn), we define the MEAN-STD
and AER metrics as follows:
MEAN-STD: The average value and standard devi-
ation of absolute differences between reference and
proposed alignment marks, are given by:
where di = |ri − bi |.
</bodyText>
<equation confidence="0.966825555555556">
in
1 di i=1
(di − µ)2
µ =
n − 1
Q =
−
n
1 (7)
</equation>
<page confidence="0.638587">
13
</page>
<equation confidence="0.7741968">
w1 w2 w3 w4 w5 w6 wn=7
x1 xp
r0 r1 r2 r3 r4 r5 r6 r7
m1 m3 m5 m7 b7
b0 b1 b2 b3 b4 b5 b6
</equation>
<figureCaption confidence="0.9829065">
Figure 4: Example of AER computation. In this case N = 4 (only no word-space are considered:
w1, w3, w5, w7) and w5 is erroneously aligned with the subsequence xb�
</figureCaption>
<figure confidence="0.593345111111111">
b� (m5 ∈/ (b4, b5)). The resulting
AER is 25%.
AER: Defined as:
100 �
AER(%) =
N
j:wj=,4b
�0 bj−1 &lt;mj &lt;bj
1 otherwise
</figure>
<bodyText confidence="0.999910545454545">
where b stands for the blank-space token, N &lt; n is
the number of real words (i.e., tokens which are not
b, and mj = (rj−1 + rj)/2.
A good alignment will have a µ value close to 0
and small σ. Thus, MEAN-STD gives us an idea of
how accurate are the automatically computed align-
ment marks. On the other hand, AER assesses align-
ments at a higher level; that is, it measures mis-
matches between word-images and ASCII transcrip-
tions (tokens), excluding word-space tokens. This is
illustrated in figure 4, where the AER would be 25%.
</bodyText>
<sectionHeader confidence="0.998892" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999628">
In order to test the effectiveness of the presented
alignment approach, different experiments were car-
ried out. The corpus used, as well as the experiments
carried out and the obtained results, are reported in
the following subsections.
</bodyText>
<subsectionHeader confidence="0.997526">
5.1 Corpus description
</subsectionHeader>
<bodyText confidence="0.975761714285714">
The corpus was compiled from the legacy handwrit-
ing document identified as Cristo-Salvador, which
was kindly provided by the Biblioteca Valenciana
Digital (BIVALDI). It is composed of 53 text page
images, scanned at 300dpi and written by only one
writer. Some of these page images are shown in the
figure 5.
As has been explained in section 3, the page im-
ages have been preprocessed and divided into lines,
resulting in a data-set of 1,172 text line images.
In this phase, around 4% of the automatically ex-
tracted line-separation marks were manually cor-
rected. The transcriptions corresponding to each line
image are also available, containing 10,911 running
words with a vocabulary of 3,408 different words.
To test the quality of the computed alignments, 12
pages were randomly chosen from the whole corpus
pages to be used as references. For these pages the
true locations of alignment marks were set manually.
Table 1 summarized the basic statistics of this cor-
pus and its reference pages.
</bodyText>
<table confidence="0.9984382">
Number of: References Total Lexicon
pages 12 53 –
text lines 312 1,172 –
words 2,955 10,911 3,408
characters 16,893 62,159 78
</table>
<tableCaption confidence="0.999741">
Table 1: Basic statistics of the database
</tableCaption>
<subsectionHeader confidence="0.965494">
5.2 Experiments and Results
</subsectionHeader>
<bodyText confidence="0.9993129">
As mentioned above, experiments were carried out
computing the alignments line-by-line. Two differ-
ent HMM modeling schemes were employed. The
first one models each of the 78 character classes us-
ing a different HMM per class. The second scheme
uses 2 HMMs, one to model all the 77 no-blank
character classes, and the other to model only the
blank “character” class. The HMM topology was
identical for all HMMs in both schemes: left-to-
right with 6 states and 64 Gaussian mixture com-
</bodyText>
<equation confidence="0.562802666666667">
ej
(8)
ej =
</equation>
<page confidence="0.914785">
14
</page>
<figureCaption confidence="0.944152666666667">
Figure 5: Examples page images of the corpus “Cristo-Salvador”, which show backgrounds of big variations
and uneven illumination, spots due to the humidity, marks resulting from the ink that goes through the paper
(called bleed-through), etc.
</figureCaption>
<bodyText confidence="0.981391714285714">
ponents per state.
As has been explained in section 4, two different
measures have been adopted to evaluate the quality
of the obtained alignments: the MEAN-STD and the
AER. Table 2 shows the different alignment evalu-
ation results obtained for the different schemes of
HMM modeling.
</bodyText>
<table confidence="0.9970125">
78-HMMs 2-HMMs
AER (%) 7.20 25.98
µ (mm) 1.15 2.95
Q (mm) 3.90 6.56
</table>
<tableCaption confidence="0.92519">
Table 2: Alignment evaluation results 78-HMMs
and 2-HMMs.
</tableCaption>
<bodyText confidence="0.999788">
From the results we can see that using the 78
HMMs scheme the best AER is obtained (7.20%).
Moreover, the relative low values of µ and Q (in mil-
limeters) show that the quality of the obtained align-
ments (marks) is quite acceptable, that is they are
very close to their respective references. This is il-
lustrated on the left histogram of figure 6.
The two typical alignment errors are known as
over-segmentation and under-segmentation respec-
tively. The over-segmentation error is when one
word image is separated into two or more fragments.
The under-segmentation error occurs when two or
more images are grouped together and returned as
one word. Figure 7 shows some of them.
</bodyText>
<sectionHeader confidence="0.998305" genericHeader="conclusions">
6 Remarks and Conclusions
</sectionHeader>
<bodyText confidence="0.999818416666667">
Given a manuscript and its transcription, we propose
an alignment method to map every word image on
the manuscript with its respective ASCII word on
the transcript. This method takes advantage of the
implicit alignment made by Viterbi decoding used
in text recognition with HMMs.
The results reported in the last section should be
considered preliminary.
Current work is under way to apply this align-
ment approach to the whole pages, which represents
a more general case where the most corpora do not
have transcriptions set at line level.
</bodyText>
<sectionHeader confidence="0.998951" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.961804">
I. Bazzi, R. Schwartz, and J. Makhoul. 1999. An Om-
nifont Open-Vocabulary OCR System for English and
</reference>
<page confidence="0.955866">
15
</page>
<figure confidence="0.999224727272727">
Frequency (%)
Frequency (%)
0 1 2 3 4 5 6
|Segi − Refi |(mm)
0 1 2 3 4 5 6
|Segi − Refi |(mm)
12
10
8
6
4
2
0
mean
6
5
4
3
2
1
0
mean
</figure>
<figureCaption confidence="0.987032333333333">
Figure 6:  |r − bi |distribution histograms for 78-HMMs (left) and 2-HMMs (right) modelling schemes.
Figure 7: Word alignment for 6 lines of a particularly noisy part of the corpus. The four last words on the
second line as well as the last line illustrate some of over-segmentation and under-segmentation error types.
</figureCaption>
<reference confidence="0.994299731707317">
Arabic. IEEE Trans. on PAMI, 21(6):495–504.
Chen Huang and Sargur N. Srihari. 2006. Mapping Tran-
scripts to Handwritten Text. In Suvisoft Ltd., editor,
Tenth International Workshop on Frontiers in Hand-
writing Recognition, pages 15–20, La Baule, France,
October.
F. Jelinek. 1998. Statistical Methods for Speech Recog-
nition. MIT Press.
Ergina Kavallieratou and Efstathios Stamatatos. 2006.
Improving the quality of degraded document images.
In DIAL ’06: Proceedings of the Second International
Conference on Document Image Analysis for Libraries
(DIAL’06), pages 340–349, Washington, DC, USA.
IEEE Computer Society.
E. M. Kornfield, R. Manmatha, and J. Allan. 2004. Text
Alignment with Handwritten Documents. In First In-
ternational Workshop on Document Image Analysis
for Libraries (DIAL), pages 195–209, Palo Alto, CA,
USA, January.
U.-V. Marti and H. Bunke. 2001. Using a Statistical Lan-
guage Model to improve the preformance of an HMM-
Based Cursive Handwriting Recognition System. Int.
Journal of Pattern Recognition and Artificial In telli-
gence, 15(1):65–90.
V. Romero, M. Pastor, A. H. Toselli, and E. Vidal. 2006.
Criteria for handwritten off-line text size normaliza-
tion. In Procc. of The Sixth IASTED international
Conference on Visualization, Imaging, and Image Pro-
cessing (VIIP 06), Palma de Mallorca, Spain, August.
A. H. Toselli, A. Juan, D. Keysers, J. Gonzlez, I. Sal-
vador, H. Ney, E. Vidal, and F. Casacuberta. 2004.
Integrated Handwriting Recognition and Interpretation
using Finite-State Models. Int. Journal of Pattern
Recognition and Artificial Intelligence, 18(4):519–
539, June.
M. Zimmermann and H. Bunke. 2002. Automatic Seg-
mentation of the IAM Off-Line Database for Hand-
written English Text. In ICPR ’02: Proceedings of
the 16 th International Conference on Pattern Recog-
nition (ICPR’02) Volume 4, page 40035, Washington,
DC, USA. IEEE Computer Society.
</reference>
<page confidence="0.998697">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.685108">
<title confidence="0.996056">Based Alignment between Text Images and their</title>
<author confidence="0.999391">Alejandro H Toselli</author>
<author confidence="0.999391">Ver´onica Romero</author>
<author confidence="0.999391">Enrique</author>
<affiliation confidence="0.901285">Institut Tecnol`ogic Universitat Polit`ecnica de Camide Vera</affiliation>
<address confidence="0.995887">46071 - Val`encia,</address>
<email confidence="0.99281">[ahector,vromero,evidal]@iti.upv.es</email>
<abstract confidence="0.999107235294118">An alignment method based on the Viterbi algorithm is proposed to find mappings between word images of a given handwritten document and their respective (ASCII) words on its transcription. The approach takes advantage of the underlying segmentation made by Viterbi decoding in handwritten text recognition based on Hidden Markov Models (HMMs). Two HMMs modelling schemes are evaluated: one using 78-HMMs (one HMM per character class) and other using a unique HMM to model all the characters and another to model blank spaces. According to various metrics used to measure the quality of the alignments, encouraging results are obtained.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>I Bazzi</author>
<author>R Schwartz</author>
<author>J Makhoul</author>
</authors>
<title>An Omnifont Open-Vocabulary OCR System for English and Arabic.</title>
<date>1999</date>
<journal>IEEE Trans. on PAMI,</journal>
<volume>21</volume>
<issue>6</issue>
<contexts>
<context position="4142" citStr="Bazzi et al., 1999" startWordPosition="633" endWordPosition="636">face displaying an outlined word (using a box) in the manuscript (left) and the corresponding highlighted word in the transcript (right). using dynamic time warping (DTW). In this same direction, (Huang and Srihari, 2006), in addition to the word pre-segmentation, attempt a (rough) recognition of the word images. The resulting word string is then aligned with the transcription using dynamic programming. The alignment method presented here (henceforward called Viterbi alignment), relies on the Viterbi decoding approach to handwritten text recognition (HTR) based on Hidden Markov Models (HMMs) (Bazzi et al., 1999; Toselli et al., 2004). These techniques are based on methods originally introduced for speech recognition (Jelinek, 1998). In such HTR systems, the alignment is actually a byproduct of the proper recognition process, i.e. an implicit segmentation of each text image line is obtained where each segment successively corresponds to one recognized word. In our case, word recognition is not actually needed, as we do already have the correct transcription. Therefore, to obtain the segmentations for the given word sequences, the socalled “forced-recognition” approach is employed (see section 2.2). T</context>
<context position="6887" citStr="Bazzi et al., 1999" startWordPosition="1079" endWordPosition="1082">written text recognition is briefly outlined in this section, followed by a more detailed presentation of the Viterbi alignment approach. 2.1 HMM HTR Basics The traditional handwritten text recognition problem can be formulated as the problem of finding a most likely word sequence w� = (w1, w2,... , wn), for a given handwritten sentence (or line) image represented by a feature vector sequence x = xp 1 = (x1, x2,. . . , xp), that is: w� = arg max Pr(w|x) W = arg max Pr(x|w) · Pr(w) (1) W where Pr(x|w) is usually approximated by concatenated character Hidden Markov Models (HMMs) (Jelinek, 1998; Bazzi et al., 1999), whereas Pr(w) is approximated typically by an n-gram word language model (Jelinek, 1998). Thus, each character class is modeled by a continuous density left-to-right HMM, characterized by a set of states and a Gaussian mixture per state. The Gaussian mixture serves as a probabilistic law to model the emission of feature vectors by each HMM state. Figure 2 shows an example of how a HMM models a feature vector sequence corresponding to 11 b0 b1 b2 b3 b4 b5 b6 bn=7 Figure 3: Example of segmented text line image along with its resulting deslanted and size-normalized image. Moreover, the alignmen</context>
</contexts>
<marker>Bazzi, Schwartz, Makhoul, 1999</marker>
<rawString>I. Bazzi, R. Schwartz, and J. Makhoul. 1999. An Omnifont Open-Vocabulary OCR System for English and Arabic. IEEE Trans. on PAMI, 21(6):495–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Huang</author>
<author>Sargur N Srihari</author>
</authors>
<title>Mapping Transcripts to Handwritten Text.</title>
<date>2006</date>
<booktitle>In Suvisoft Ltd., editor, Tenth International Workshop on Frontiers in Handwriting Recognition,</booktitle>
<pages>15--20</pages>
<location>La Baule, France,</location>
<contexts>
<context position="3745" citStr="Huang and Srihari, 2006" startWordPosition="573" endWordPosition="576">field et al., 2004), the set of previously segmented word images and their corresponding transcriptions are transformed into two different times series, which are aligned 9 Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 9–16, Prague, 28 June 2007. c�2007 Association for Computational Linguistics Figure 1: Screen-shot of the alignment prototype interface displaying an outlined word (using a box) in the manuscript (left) and the corresponding highlighted word in the transcript (right). using dynamic time warping (DTW). In this same direction, (Huang and Srihari, 2006), in addition to the word pre-segmentation, attempt a (rough) recognition of the word images. The resulting word string is then aligned with the transcription using dynamic programming. The alignment method presented here (henceforward called Viterbi alignment), relies on the Viterbi decoding approach to handwritten text recognition (HTR) based on Hidden Markov Models (HMMs) (Bazzi et al., 1999; Toselli et al., 2004). These techniques are based on methods originally introduced for speech recognition (Jelinek, 1998). In such HTR systems, the alignment is actually a byproduct of the proper recog</context>
</contexts>
<marker>Huang, Srihari, 2006</marker>
<rawString>Chen Huang and Sargur N. Srihari. 2006. Mapping Transcripts to Handwritten Text. In Suvisoft Ltd., editor, Tenth International Workshop on Frontiers in Handwriting Recognition, pages 15–20, La Baule, France, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Statistical Methods for Speech Recognition.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4265" citStr="Jelinek, 1998" startWordPosition="652" endWordPosition="653">t (right). using dynamic time warping (DTW). In this same direction, (Huang and Srihari, 2006), in addition to the word pre-segmentation, attempt a (rough) recognition of the word images. The resulting word string is then aligned with the transcription using dynamic programming. The alignment method presented here (henceforward called Viterbi alignment), relies on the Viterbi decoding approach to handwritten text recognition (HTR) based on Hidden Markov Models (HMMs) (Bazzi et al., 1999; Toselli et al., 2004). These techniques are based on methods originally introduced for speech recognition (Jelinek, 1998). In such HTR systems, the alignment is actually a byproduct of the proper recognition process, i.e. an implicit segmentation of each text image line is obtained where each segment successively corresponds to one recognized word. In our case, word recognition is not actually needed, as we do already have the correct transcription. Therefore, to obtain the segmentations for the given word sequences, the socalled “forced-recognition” approach is employed (see section 2.2). This idea has been previously explored in (Zimmermann and Bunke, 2002). Alignments can be computed line by line in cases whe</context>
<context position="6866" citStr="Jelinek, 1998" startWordPosition="1077" endWordPosition="1078"> HMM-based handwritten text recognition is briefly outlined in this section, followed by a more detailed presentation of the Viterbi alignment approach. 2.1 HMM HTR Basics The traditional handwritten text recognition problem can be formulated as the problem of finding a most likely word sequence w� = (w1, w2,... , wn), for a given handwritten sentence (or line) image represented by a feature vector sequence x = xp 1 = (x1, x2,. . . , xp), that is: w� = arg max Pr(w|x) W = arg max Pr(x|w) · Pr(w) (1) W where Pr(x|w) is usually approximated by concatenated character Hidden Markov Models (HMMs) (Jelinek, 1998; Bazzi et al., 1999), whereas Pr(w) is approximated typically by an n-gram word language model (Jelinek, 1998). Thus, each character class is modeled by a continuous density left-to-right HMM, characterized by a set of states and a Gaussian mixture per state. The Gaussian mixture serves as a probabilistic law to model the emission of feature vectors by each HMM state. Figure 2 shows an example of how a HMM models a feature vector sequence corresponding to 11 b0 b1 b2 b3 b4 b5 b6 bn=7 Figure 3: Example of segmented text line image along with its resulting deslanted and size-normalized image. M</context>
<context position="8245" citStr="Jelinek, 1998" startWordPosition="1316" endWordPosition="1317"> w6 wn=7 character “a”. The process to obtain feature vector sequences from text images as well as the training of HMMs are explained in section 3. HMMs as well as n-grams models can be represented by stochastic finite state networks (SFN), which are integrated into a single global SFN by replacing each word character of the n-gram model by the corresponding HMM. The search involved in the equation (1) to decode the input feature vectors sequence x into the more likely output word sequence w, is performed over this global SFN. This search problem is adequately solved by the Viterbi algorithm (Jelinek, 1998). 2.2 Viterbi Alignment As a byproduct of the Viterbi solution to (1), the feature vectors subsequences of x aligned with each of the recognized words w1, w2, ... , wn can be obtained. These implicit subsequences can be visualized into the equation (1) as follows: where b is the optimal alignment. In our case, we are not really interested in proper text recognition because the transcription is known beforehand. Let w� be the given transcription. Now, Pr(w) in equation 3 is zero for all w except w, for which Pr( w) = 1. Therefore, b = arg max Pr(x, b |w) (4) b which can be expanded to, b = arg </context>
<context position="12734" citStr="Jelinek, 1998" startWordPosition="2088" endWordPosition="2089">nd 64 Gaussian mixture components per state. This topology (number of HMM states and Gaussian densities per state) was determined by tuning empirically the system on the corpus described in section 5.1. Once a HMM “topology” has been adopted, the model parameters can be easily trained from images of continuously handwritten text (without any kind of segmentation) accompanied by the transcription of these images into the corresponding sequence of characters. This training process is carried out using a well known instance of the EM algorithm called forward-backward or Baum-Welch re-estimation (Jelinek, 1998). The last phase in the alignment process is the generation of the mapping proper by means of Viterbi “forced recognition”, as discussed in section 2.2. 4 Alignment Evaluation Metrics Two kinds of measures have been adopted to evaluate the quality of alignments. On the one hand, the average value and standard deviation (henceforward called MEAN-STD) of the absolute differences between the system-proposed word alignment marks and their corresponding (correct) references. This gives us an idea of the geometrical accuracy of the alignments obtained. On the other hand, the alignment error rate (AE</context>
</contexts>
<marker>Jelinek, 1998</marker>
<rawString>F. Jelinek. 1998. Statistical Methods for Speech Recognition. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergina Kavallieratou</author>
<author>Efstathios Stamatatos</author>
</authors>
<title>Improving the quality of degraded document images.</title>
<date>2006</date>
<booktitle>In DIAL ’06: Proceedings of the Second International Conference on Document Image Analysis for Libraries (DIAL’06),</booktitle>
<pages>340--349</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="10075" citStr="Kavallieratou and Stamatatos, 2006" startWordPosition="1649" endWordPosition="1653">m in (2) by the dominant term: Pr(x, b|w) · Pr(w) (3) This simpler Viterbi search problem is known as “forced recognition”. 3 Overview of the Alignment Prototype The implementation of the alignment prototype involved four different parts: document image preprocessing, line image feature extraction, HMMs training and alignment map generation. w� Pz� arg max w max b 12 Document image preprocessing encompasses the following steps: first, skew correction is carried out on each document page image; then background removal and noise reduction is performed by applying a bi-dimensional median filter (Kavallieratou and Stamatatos, 2006) on the whole page image. Next, a text line extraction process based on local minimums of the horizontal projection profile of page image, divides the page into separate line images (Marti and Bunke, 2001). In addition connected components has been used to solve the situations where local minimum values are greater than zero, making impossible to obtain a clear text line separation. Finally, slant correction and non-linear size normalization are applied (Toselli et al., 2004; Romero et al., 2006) on each extracted line image. An example of extracted text line image is shown in the top panel of</context>
</contexts>
<marker>Kavallieratou, Stamatatos, 2006</marker>
<rawString>Ergina Kavallieratou and Efstathios Stamatatos. 2006. Improving the quality of degraded document images. In DIAL ’06: Proceedings of the Second International Conference on Document Image Analysis for Libraries (DIAL’06), pages 340–349, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Kornfield</author>
<author>R Manmatha</author>
<author>J Allan</author>
</authors>
<title>Text Alignment with Handwritten Documents.</title>
<date>2004</date>
<booktitle>In First International Workshop on Document Image Analysis for Libraries (DIAL),</booktitle>
<pages>195--209</pages>
<location>Palo Alto, CA, USA,</location>
<contexts>
<context position="3140" citStr="Kornfield et al., 2004" startWordPosition="486" endWordPosition="489"> quickly locate image text while reading a transcript, with useful applications to editing, indexing, etc. In the opposite direction, the alignment can also be useful for people trying to read the image text directly, when arriving to complex or damaged parts of the document. Creating such alignments is challenging since the transcript is an ASCII text file while the manuscript page is an image. Some recent works address this problem by relying on a previous explicit imageprocessing based word pre-segmentation of the page image, before attempting the transcription alignments. For example, in (Kornfield et al., 2004), the set of previously segmented word images and their corresponding transcriptions are transformed into two different times series, which are aligned 9 Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 9–16, Prague, 28 June 2007. c�2007 Association for Computational Linguistics Figure 1: Screen-shot of the alignment prototype interface displaying an outlined word (using a box) in the manuscript (left) and the corresponding highlighted word in the transcript (right). using dynamic time warping (DTW). In this same direction, (Huang and Srihari, </context>
</contexts>
<marker>Kornfield, Manmatha, Allan, 2004</marker>
<rawString>E. M. Kornfield, R. Manmatha, and J. Allan. 2004. Text Alignment with Handwritten Documents. In First International Workshop on Document Image Analysis for Libraries (DIAL), pages 195–209, Palo Alto, CA, USA, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U-V Marti</author>
<author>H Bunke</author>
</authors>
<title>Using a Statistical Language Model to improve the preformance of an HMMBased Cursive Handwriting Recognition System.</title>
<date>2001</date>
<journal>Int. Journal of Pattern Recognition and Artificial In telligence,</journal>
<pages>15--1</pages>
<contexts>
<context position="10280" citStr="Marti and Bunke, 2001" startWordPosition="1685" endWordPosition="1688">our different parts: document image preprocessing, line image feature extraction, HMMs training and alignment map generation. w� Pz� arg max w max b 12 Document image preprocessing encompasses the following steps: first, skew correction is carried out on each document page image; then background removal and noise reduction is performed by applying a bi-dimensional median filter (Kavallieratou and Stamatatos, 2006) on the whole page image. Next, a text line extraction process based on local minimums of the horizontal projection profile of page image, divides the page into separate line images (Marti and Bunke, 2001). In addition connected components has been used to solve the situations where local minimum values are greater than zero, making impossible to obtain a clear text line separation. Finally, slant correction and non-linear size normalization are applied (Toselli et al., 2004; Romero et al., 2006) on each extracted line image. An example of extracted text line image is shown in the top panel of figure 3, along with the resulting deslanted and size-normalized image. Note how non-linear normalization leads to reduced sizes of ascenders and descenders, as well as to a thiner underline of the word “</context>
</contexts>
<marker>Marti, Bunke, 2001</marker>
<rawString>U.-V. Marti and H. Bunke. 2001. Using a Statistical Language Model to improve the preformance of an HMMBased Cursive Handwriting Recognition System. Int. Journal of Pattern Recognition and Artificial In telligence, 15(1):65–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Romero</author>
<author>M Pastor</author>
<author>A H Toselli</author>
<author>E Vidal</author>
</authors>
<title>Criteria for handwritten off-line text size normalization.</title>
<date>2006</date>
<booktitle>In Procc. of The Sixth IASTED international Conference on Visualization, Imaging, and Image Processing (VIIP 06), Palma de Mallorca,</booktitle>
<location>Spain,</location>
<contexts>
<context position="10576" citStr="Romero et al., 2006" startWordPosition="1732" endWordPosition="1735">emoval and noise reduction is performed by applying a bi-dimensional median filter (Kavallieratou and Stamatatos, 2006) on the whole page image. Next, a text line extraction process based on local minimums of the horizontal projection profile of page image, divides the page into separate line images (Marti and Bunke, 2001). In addition connected components has been used to solve the situations where local minimum values are greater than zero, making impossible to obtain a clear text line separation. Finally, slant correction and non-linear size normalization are applied (Toselli et al., 2004; Romero et al., 2006) on each extracted line image. An example of extracted text line image is shown in the top panel of figure 3, along with the resulting deslanted and size-normalized image. Note how non-linear normalization leads to reduced sizes of ascenders and descenders, as well as to a thiner underline of the word “ciudadanos”. As our alignment prototype is based on Hidden Markov Models (HMMs), each preprocessed line image is represented as a sequence of feature vectors. To do this, the feature extraction module applies a grid to divide line image into N x M squared cells. In this work, N = 40 is chosen em</context>
</contexts>
<marker>Romero, Pastor, Toselli, Vidal, 2006</marker>
<rawString>V. Romero, M. Pastor, A. H. Toselli, and E. Vidal. 2006. Criteria for handwritten off-line text size normalization. In Procc. of The Sixth IASTED international Conference on Visualization, Imaging, and Image Processing (VIIP 06), Palma de Mallorca, Spain, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A H Toselli</author>
<author>A Juan</author>
<author>D Keysers</author>
<author>J Gonzlez</author>
<author>I Salvador</author>
<author>H Ney</author>
<author>E Vidal</author>
<author>F Casacuberta</author>
</authors>
<title>Integrated Handwriting Recognition and Interpretation using Finite-State Models.</title>
<date>2004</date>
<journal>Int. Journal of Pattern Recognition and Artificial Intelligence,</journal>
<volume>18</volume>
<issue>4</issue>
<pages>539</pages>
<contexts>
<context position="4165" citStr="Toselli et al., 2004" startWordPosition="637" endWordPosition="640">utlined word (using a box) in the manuscript (left) and the corresponding highlighted word in the transcript (right). using dynamic time warping (DTW). In this same direction, (Huang and Srihari, 2006), in addition to the word pre-segmentation, attempt a (rough) recognition of the word images. The resulting word string is then aligned with the transcription using dynamic programming. The alignment method presented here (henceforward called Viterbi alignment), relies on the Viterbi decoding approach to handwritten text recognition (HTR) based on Hidden Markov Models (HMMs) (Bazzi et al., 1999; Toselli et al., 2004). These techniques are based on methods originally introduced for speech recognition (Jelinek, 1998). In such HTR systems, the alignment is actually a byproduct of the proper recognition process, i.e. an implicit segmentation of each text image line is obtained where each segment successively corresponds to one recognized word. In our case, word recognition is not actually needed, as we do already have the correct transcription. Therefore, to obtain the segmentations for the given word sequences, the socalled “forced-recognition” approach is employed (see section 2.2). This idea has been previ</context>
<context position="10554" citStr="Toselli et al., 2004" startWordPosition="1728" endWordPosition="1731">age; then background removal and noise reduction is performed by applying a bi-dimensional median filter (Kavallieratou and Stamatatos, 2006) on the whole page image. Next, a text line extraction process based on local minimums of the horizontal projection profile of page image, divides the page into separate line images (Marti and Bunke, 2001). In addition connected components has been used to solve the situations where local minimum values are greater than zero, making impossible to obtain a clear text line separation. Finally, slant correction and non-linear size normalization are applied (Toselli et al., 2004; Romero et al., 2006) on each extracted line image. An example of extracted text line image is shown in the top panel of figure 3, along with the resulting deslanted and size-normalized image. Note how non-linear normalization leads to reduced sizes of ascenders and descenders, as well as to a thiner underline of the word “ciudadanos”. As our alignment prototype is based on Hidden Markov Models (HMMs), each preprocessed line image is represented as a sequence of feature vectors. To do this, the feature extraction module applies a grid to divide line image into N x M squared cells. In this wor</context>
</contexts>
<marker>Toselli, Juan, Keysers, Gonzlez, Salvador, Ney, Vidal, Casacuberta, 2004</marker>
<rawString>A. H. Toselli, A. Juan, D. Keysers, J. Gonzlez, I. Salvador, H. Ney, E. Vidal, and F. Casacuberta. 2004. Integrated Handwriting Recognition and Interpretation using Finite-State Models. Int. Journal of Pattern Recognition and Artificial Intelligence, 18(4):519– 539, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zimmermann</author>
<author>H Bunke</author>
</authors>
<title>Automatic Segmentation of the IAM Off-Line Database for Handwritten English Text.</title>
<date>2002</date>
<booktitle>In ICPR ’02: Proceedings of the 16 th International Conference on Pattern Recognition (ICPR’02)</booktitle>
<volume>4</volume>
<pages>40035</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="4811" citStr="Zimmermann and Bunke, 2002" startWordPosition="736" endWordPosition="739">are based on methods originally introduced for speech recognition (Jelinek, 1998). In such HTR systems, the alignment is actually a byproduct of the proper recognition process, i.e. an implicit segmentation of each text image line is obtained where each segment successively corresponds to one recognized word. In our case, word recognition is not actually needed, as we do already have the correct transcription. Therefore, to obtain the segmentations for the given word sequences, the socalled “forced-recognition” approach is employed (see section 2.2). This idea has been previously explored in (Zimmermann and Bunke, 2002). Alignments can be computed line by line in cases where the beginning and end positions of lines are known or, in a more general case, for whole pages. We show line-by-line results on a set of 53 pages from the “Cristo-Salvador” handwritten document (see section 5.2). To evaluate the quality of the obtained alignments, two metrics were used which give information at different alignment levels: one measures the accuracy of alignment mark placements and the other measures the amount of erroneous as10 Figure 2: Example of 5-states HMM modeling (feature vectors sequences) of instances of the char</context>
</contexts>
<marker>Zimmermann, Bunke, 2002</marker>
<rawString>M. Zimmermann and H. Bunke. 2002. Automatic Segmentation of the IAM Off-Line Database for Handwritten English Text. In ICPR ’02: Proceedings of the 16 th International Conference on Pattern Recognition (ICPR’02) Volume 4, page 40035, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>