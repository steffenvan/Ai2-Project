<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.035846">
<title confidence="0.5053125">
Identifying Unknown Proper Names in Newswire
Text
</title>
<author confidence="0.772604333333333">
Inderjeet Mani, T. Richard Macmillan,
Susann Luperfoy, Elaine P. Lusher,
Sharon J. Laskowski
</author>
<affiliation confidence="0.811285666666667">
Artificial Intelligence Technical Center
The MITRE Corporation, Mail Stop Z401
7525 Colshire Drive, McLean, Virginia 221024481
</affiliation>
<email confidence="0.986154">
mani@starbase.mitre.org
</email>
<sectionHeader confidence="0.966441" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999666818181818">
The identification of unknown proper names in text is a significant challenge for
NLP systems operating on unrestricted text. A system which indexes documents
according to name references can be useful for information retrieval or as a pre-
processor for more knowledge intensive tasks such as database extraction. This paper
describes a system which uses text skimming techniques for deriving proper names
and their semantic attributes automatically from newswire text, without relying on
any listing of name elements. In order to identify new names, the system treats
proper names as (potentially) context-dependent linguistic expressions. In addition
to using information in the local context, the system exploits a computational model
of discourse which identifies individuals based on the way they are described in the
text, instead of relying on their description in a pre-existing knowledge base.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995165">
The identification of unknown proper names in text is a significant challenge for NLP
systems operating on unrestricted text. A system which indexes documents according
to name references can be useful for information retrieval or as a pre-processor for more
knowledge intensive tasks such as database extraction. With the growing use of tagged
corpora in a variety of language-related research areas, being able to reliably tag proper
names is an obvious advantage. In addition, the development of practical techniques for
name identification help to shed light on the various uses of proper names in text.
Traditional approaches to unknown proper name identification involve, broadly speak-
ing, the lexical lookup of names or name fragments in a name database. For example,
approaches such as [Aone et al., 92], [Aberdeen et al., 92], and [Cowie et al., 92], iden-
tify person names by marking off phrases which contain unknown words close to known
name elements like first or last names, and (in [Cowie et al., 92]) unknown words close
to specific title-words. As the above studies show, name databases such as cross-cultural
listings of common first and last names as well as existing geographical gazetteers, are
helpful in name recognition. However, approaches based exclusively on unknown words
and known name elements can be confused by known common nouns (or other parts of
speech) which occur in proper names, even person names. More importantly, such ap-
proaches require an initial name element database. Creating such databases can be a
labor-intensive task. Furthermore, no matter how large the database one can manually
construct, the problem still arises of identifying names which don&apos;t happen to be present
</bodyText>
<page confidence="0.998439">
44
</page>
<bodyText confidence="0.9996885625">
in any given name database. The fact that proper names form, lexically speaking, an open
class whose elements grow far more rapidly than other open classes, and the fact that they
often contain other open-class elements, makes the incompleteness of such databases an
obvious problem.
Our approach aims at deriving proper names and their semantic attributes automat-
ically from large corpora, without relying on any listing of name elements. The overall
approach is based on two main ideas. Firstly, we hypothesize that for certain genres of
text (for example, Wall Street Journal news stories), new references are introduced by in-
formation occurring in the immediate syntactic environment of the proper name. (What
the precise set of such genres is remains to be determined, but our initial set includes
the most common forms of news stories and excludes literary narratives.) Many of these
local contextual clues reflect felicity conventions for introducing new names. New names
of people (as well as organization names, and to some extent location names) are gen-
erally accompanied by honorifics and various appositive phrases which help anchor the
new name reference to mutually assumed knowledge. Further contextual clues come from
selectional restrictions, for example, given &amp;quot;Kambomambo murdered Zombaluma&amp;quot; (from
[Radford, 88]), the verb is the main clue to the hypothesis that the two names are those
of people.
Although the idea of exploiting local context to identify semantic attributes in new
names is in itself not new (e.g. [Coates-Stephens, 91], [Paik et al., 93]), little attention
has been paid in name identification work to the discourse properties of names. Our
second, and more general idea is to view proper names as linguistic expressions whose
interpretation often depends on the discourse context. For example, in the discourse
&amp;quot;U.S. President Bill Clinton....Clinton....Mr. Clinton....President Clinton&amp;quot;, the interpre-
tations of &amp;quot;Clinton&amp;quot;, &amp;quot;Mr. Clinton&amp;quot; and &amp;quot;President Clinton&amp;quot; are dependent on the prior
reference to &amp;quot;U.S. President Bill Clinton&amp;quot;, much as &amp;quot;the president&amp;quot;, &amp;quot;he&amp;quot; and &amp;quot;him-
self&amp;quot; are dependent on prior context in the discourse &amp;quot;U.S. President Bill Clintoni....the
presidenti....hei....himselfi&amp;quot;. The need for text-driven extraction of names presupposes in
turn a computational model of discourse which identifies individuals based on the way
they are described in the text, instead of relying on their description in a pre-existing
knowledge base. The overall discourse representation framework which we use is Luper-
foy&apos;s three-tiered model [Luperfoy, 91], which in turn is a computational adaptation of
Landman&apos;s pegs model of NP semantics [Landman 86].
The idea of the three-tiered model is that there are three significant levels of represen-
tation: linguistic expressions, Discourse Pegs, and knowledge base objects. A distinctive
feature of Discourse Pegs (hereafter referred to as Pegs) as opposed to similar constructs
in the literature, like File Cards ([Heim, 81]), Database Objects ([Sidner, 79]), Discourse
Referents ({Karttunen, 68]), and Discourse Entities ([Webber, 78], [Dahl and Ball, 90]),
is that they describe unique objects with respect to the current discourse, rather than
with respect to the underlying belief system or world model. Thus, in an article mention-
ing Bill Clinton there may be two guises in which he may appear, as Governor Clinton
and President Clinton; these would correspond to two distinct pegs. It is important to
stress that pegs, as a result, do not correspond to equivalence classes of coreferential men-
tions; rather, there is one peg for each distinct object under discussion, irrespective of the
number of entities in the world of reference. Objects which are distinct in the text may
still need to be related to each other for their interpretation; for example, in the discourse
&amp;quot;President Bill Clinton... the Clintons....Hilary&amp;quot;, the expressions &amp;quot;President Bill Clinton&amp;quot;,
&amp;quot;the Clintons&amp;quot; and &amp;quot;Hilary&amp;quot; each introduce new pegs, but these pegs are each linked, as
</bodyText>
<page confidence="0.998667">
45
</page>
<bodyText confidence="0.999986590909091">
&amp;quot;partial dependents&amp;quot;, to the previous one. An interesting subcase of this involves name
mergers, e.g. an article describing a joint venture between two companies may use the
two individual company names followed by a merged name for the joint venture.
In applying this framework to the unknown name problem, we first distinguish three
types of entities: (i) Mentions - these are text segments which are tokens of proper names
in text; (ii) Contexts - these are text segments which provide information about syn-
tactic and semantic properties associated with a name; and (iii) Hypotheses - these are
hypotheses about individuals and their semantic attributes, associated with a Mention.
Given this framework, the goal of unknown name identification is to use the text itself to
generate Hypotheses about possible individuals distinguished by a Mention. In a given
text context, descriptions from earlier Mentions of a name may be further specified by
new information associated with subsequent Mentions of the name (which may take a
somewhat different form from previous Mentions). In general, two Hypotheses, each asso-
ciated with a different Mention, are linked together (by means of a common Peg) whenever
they are mutually compatible. Thus, two Mentions, Mention 1 and Mention 2, can be
considered to be indirectly anchored together to a common Peg whenever hypothetical in-
formation associated with each is mutually compatible. For ease of presentation, we may
speak of these coanchored mentions as &amp;quot;coreferential&amp;quot; (when what we really mean is this
more specific sense of coanchoring); also, we will use the capitalized word &amp;quot;Coreference&amp;quot;
for the process of computing pegs for a mention, a process which may result in either the
coanchoring of the mention to one or more existing pegs, or the allocation of a new peg.
We describe the Coreference process in more detail in Section 4.
</bodyText>
<sectionHeader confidence="0.5553695" genericHeader="method">
2 Proper Names - Syntactic Forms and Semantic At-
tributes
</sectionHeader>
<bodyText confidence="0.999937809523809">
We first need to describe more precisely what we mean by proper names. In terms
of syntactic categories, proper names are commonly identified as lexical NPs. In the
examples in this paper, we use j to identify an internal proper name constituent of interest.
Proper names often occur inside definite NPs, where the proper name can function as the
syntactic head (&amp;quot;the [President of France]&amp;quot; , &amp;quot;the [Gulf of California]&amp;quot;, &amp;quot;the Reagan [White
House]&amp;quot;, &amp;quot;Iraq&apos;s president [Saddam Hussein]&amp;quot;, &amp;quot;Lake [George]&amp;quot;), a complement (&amp;quot;the
president of [France]&amp;quot; ), or an adjunct or attributive NP (&amp;quot;the [Reagan] White House&amp;quot;, &amp;quot;the
[Bush] administration&amp;quot;). They can also occur with indefinite determiners (&amp;quot;an [Arnold
Schwartznegger]&amp;quot; , &amp;quot;a [Washington Redskin]&amp;quot;, &amp;quot;an [IBM]&amp;quot;). As lexical NPs, proper names
have substantial internal structure: they can be formed out of primitive proper name
elements (&amp;quot;Oliver North&amp;quot;, &amp;quot;Gramm-Rudman&amp;quot; &amp;quot;Villa-Lobos&amp;quot;), other proper names (&amp;quot;Lake
George&amp;quot;, &amp;quot;the [President of France]&amp;quot;, &amp;quot;the [Reagan White House]&amp;quot;, &amp;quot;Anne of a Thousand
Days&amp;quot;) and also out of non-proper names (&amp;quot;the [Savings and Loan] crisis&amp;quot;, &amp;quot;General
Electric Co.&amp;quot;, &amp;quot;Federal Savings and Loan Insurance Corporation&amp;quot;, &amp;quot;Committee for the
Protection of Public Welfare&amp;quot;). A common resulting form is the open compound proper
name (&amp;quot;the [Carter Administration National Energy Conservation Committee]&amp;quot; ).
Given an occurrence of a proper name in text, we can use the text itself to extract
semantic attributes associated with that name. As mentioned earlier, the local context
frequently offers valuable clues. Also, for certain varieties of names, such as organization
names (&amp;quot;Microelectronics and Computer Technology Corporation&amp;quot;) and geographical lo-
cation names (&amp;quot;Easter Island&amp;quot;), the internal structure of the name can be used to hy-
</bodyText>
<footnote confidence="0.588201">
4 6
</footnote>
<bodyText confidence="0.99975375">
pothesize various semantic attributes. A study reported in [Amsler, 87] on proper names
in the New York Times containing the word &amp;quot;center&amp;quot; (such as &amp;quot;Grand Forks Energy Re-
search Center&amp;quot; and &amp;quot;Boston University&apos;s Center for Adaptive Systems&amp;quot;) is suggestive of
the scope of such techniques. Identifying idiomatic uses is obviously a problem: as [Am-
sler, 87] points out, &amp;quot;Grand Funk Railroad&amp;quot; is the name of a rock group. In keeping with
such an approach, we have developed subgrammars which model the internal syntax and
semantics of geographical names, which, in combination with information from the local
Context, can be used to guess the type of location.
</bodyText>
<sectionHeader confidence="0.990569" genericHeader="method">
3 Overall Algorithm
</sectionHeader>
<bodyText confidence="0.999780666666667">
The approach of text skimming is associated with much recent work on data extraction
from text (e.g. [Mauldin 891, [Jacobs 881, and many others). In general, this means that
different parts of the text can be processed to different depths, with some parts being
skipped over lightly. The text skimming approach also implies, in our case, that we lighten
the burden of lexical semantics: in contrast to approaches like [Coates-Stephens, 91], we
need only represent word meanings for words closely related in meaning to the semantic
attributes we are attempting to extract. While we were attracted to such an approach,
our work also explores some of the practical tradeoffs associated with text skimming.
The overall algorithm involves first tokenizing the text into sentences and words, then
proposing candidate name mentions, and finally allowing various knowledge sources (KSs)
to vote on and propose hypotheses about a given mention. Each KS can generate multiple
scored hypotheses about a given mention. The KSs are applied in a pre-determined
order to a mention, with each KS refining the hypotheses generated by the previous KS.
Names which are identified beyond a certain confidence level (a variable recall/precision
threshold) are added to a hypothetical lexicon after asking the user about them. Over
time, learnt names (or name elements) in the hypothetical lexicon increase the likelihood
of recognizing a name mention.
The system assumes a shallow knowledge base representing the specific concepts and
attributes to be extracted. For example, a president is either a head-of-state or a corporate-
officer, and a person has age, title, gender and occupation; a place may be a continent,
country, state, city, etc. The semantic lexicon associated with this knowledge base is a
small one, of the order of a few hundred words, consisting of titles, honorifics, location
nouns and organizational suffixes extracted from phrases tagged as NP in the Penn Tree-
bank Wall Street Journal (WSJ) corpus. Words associated with these entities are the
only ones which currently have any lexical semantics in our system. (A noteable excep-
tion comes from our work on place names, which exploits, for comparison purposes, a
TIPSTER gazetteer). This small lexicon is complemented by the very large syntactic lex-
icon derived from the Lancaster-Oslo-Bergen corpus, which is used by our part-of-speech
tagger and parser [de Marcken, 90].
A variety of different grammars are used by the system. The simpler kind are reg-
ular expression grammars which rely on part-of-speech, some specific key lexical items
from our semantic lexicon, and punctuation - these grammars drive a pattern matcher
which is an extension of the one described in [Norvig, 92]. Such grammars are used for
modeling the internal syntax and semantics of geographical names and person names,
and also for locating various Context boundaries - for example, identifying an appositive
construction. Further segmentation of the appositive (see Section 3.3) is done by a mix-
</bodyText>
<page confidence="0.996822">
47
</page>
<bodyText confidence="0.999947666666667">
ture of pattern-matching of the above kind and NP parsing (into head, pre-modifiers, and
post-modifiers) using the MIT Fast Parser [de Marcken, 90] and its associated syntactic
grammar. At present, we perform only a rudimentary analysis of organization names,
merely hypothesizing whether a mention is a likely organization name or not.
We have used the WSJ as a training corpus. The mode of knowledge engineering has
involved building a rudimentary proper name tagger, followed by iterations through a cycle
of tagging the corpus with records of Mentions and their occurrence Contexts, examining
the tagged corpus to improve the knowledge sources, and retagging. It is envisaged that
over time, certain hypothesized individuals will be incorporated into the knowledge base.
</bodyText>
<subsectionHeader confidence="0.99495">
3.1 The Mention Generator
</subsectionHeader>
<bodyText confidence="0.999919315789474">
Given text which distinguished between upper-case and lower-case, the KS which proposes
candidate mentions is based on finding contiguous capitalized words including lower-case
function words (e.g. &amp;quot;of&amp;quot;, &amp;quot;and&amp;quot;, &amp;quot;de&amp;quot;, etc.). Only those sentences containing such
mentions are processed (partially) by other KSs. This capitalization heuristic recalls all
the proper names, but it is slightly imprecise, especially since sentence-initial words are
always capitalized in case distinguished text. To eliminate these, a part-of-speech based
filter is applied to each sentence-initial candidate sequence, discarding the initial word
unless it is from a designated set (a noun, and adjective, a NP, the definite determiner
&amp;quot;the&amp;quot;, or an unknown word) and excluding isolated definite determiners. In practice, this
filter works extremely well. However, mentions may need to be split up later when more
knowledge is available, since titles may need to be extracted, and function words like
conjunctions and prepositions introduce attachment ambiguities (e.g. &amp;quot;Democratic Sens.
Dennnis De Concini and Alan Cranston&amp;quot;, &amp;quot;Food and Drug Administration&amp;quot;).
Given newswire text which makes no reliable case distinction (e.g. all-uppercase or
all-lowercase text), the proposer proposes contiguous sequences of words with categories in
the above designated set. The proposals include all the mentions proposed in case-sensitive
mode, but the use of shallow processing here is obviously far less precise, generating 3
to 4 times as many mentions. However, incorrect candidates get filtered out eventually,
since there are no significant hypotheses about them.
</bodyText>
<subsectionHeader confidence="0.998479">
3.2 Knowledge Sources
</subsectionHeader>
<bodyText confidence="0.999832928571428">
Each KS can have multiple hypotheses with different confidences. For example, the men-
tion &amp;quot;General Electric Co.&amp;quot;, may result in an initial hypothesis that it could be a person,
based on interpreting &amp;quot;General&amp;quot; as a title, and other hypotheses that it could be a com-
pany or a county, based on the abbreviated suffix &amp;quot;Co.&amp;quot;. Each distinct filling of attributes
corresponds to a distinct hypothesis. We currently use a somewhat crude thresholding
scheme: viewing an attribute-KS as filling a single attribute, the confidence of a particular
attribute-KS&apos;s hypothesis is a weighted sum of the match strength and the attribute-KS&apos;s
strength, the latter being based on an initial global ranking followed by later calibration.
The KSes are based on simple heuristics, which, except for Coreference, are interesting
more in terms of their combined effect than in themselves. For example, Organization?
is a KS which trivially determines organizationhood by the presence of certain company
suffixes like &amp;quot;Inc.&amp;quot;. Honorifics uses the text occurrence of honorifics (&amp;quot;Mr.&amp;quot;, &amp;quot;His Holi-
ness&amp;quot;, &amp;quot;Lt. Col.&amp;quot;) from the small semantic lexicon to make inferences about personhood,
as well as gender and job occupation.
</bodyText>
<page confidence="0.996269">
48
</page>
<bodyText confidence="0.9999146">
The Job-Title and Age KSes extract their data from appositive constructions and
premodifying adjective phrases and noun compounds. A job-title (a surface string like
&amp;quot;president-for-life&amp;quot;) may or may not be in the syntactic or semantic lexicon; if it is present
in the semantic lexicon, an effort is made to infer, based on context, the person&apos;s job-
occupation, as discussed in the next section. Person-Name is a weak KS which segments
potential person-names without being able to determine personhood with any confidence.
Name-Element upgrades the confidence of names which match learned name elements.
Agent-of-Human-Action looks for verbs like &amp;quot;lead&amp;quot;, &amp;quot;head&amp;quot;, &amp;quot;say&amp;quot;, &amp;quot;explain&amp;quot;, &amp;quot;think&amp;quot;,
&amp;quot;admit&amp;quot; in the syntactic context to estimate whether a given mention could be a person,
though the assignment of agent role to the mention is only approximate; the frequent use of
metonymy involving companies as agents makes this a relatively weak KS. A Short-Name?
KS reflects a newspaper honorific convention of not using single-word titleless names in
introductory people mentions (as in &amp;quot;Yesterday [Kennedy] said..&amp;quot;). The Location KS uses
patterns involving locational category nouns from the semantic lexicon like &amp;quot;town&amp;quot;, &amp;quot;sea&amp;quot;,
&amp;quot;gulf&amp;quot;, &amp;quot;north&amp;quot; to flag location mentions like &amp;quot;town of Beit Sahoud&amp;quot;.
</bodyText>
<subsectionHeader confidence="0.990126">
3.3 Appositives
</subsectionHeader>
<bodyText confidence="0.999942541666667">
Appositives are important linguistic devices for introducing new mentions. We limit our-
selves to constituents of the form &lt;NP, NP&gt;. These are of the form name-comma-
appositive (e.g. &amp;quot;&lt;name&gt;, &lt;ORG&gt;&apos;s top managing director&amp;quot;, &amp;quot;&lt;name&gt;, a small Bay
Area town&amp;quot;), and appositive-comma-name (e.g. &amp;quot;a top Japanese executive, &lt;name&gt;&amp;quot;).
We ignore double appositives, except for simple ones involving age, as in &amp;quot;Osamu Na-
gayama, 33, senior vice president and chief financial officer of Chugai.&amp;quot;. Therefore, given
a candidate name mention, the appositive modifier is a NP to the right or the left of the
name. (A &lt;NP, NP&gt; constituent can of course be part of an enumerated, conjoined NP;
however, if one conjunct is a name, it&apos;s likely that the other one may be too. Of course,
a &lt;NP, NP&gt; sequence may not be a constituent in the first place).
To identify appositive boundaries, we experimented with both (a) a regular expres-
sion grammar tuned to find appositives in the training corpus, and (b) syntactic-grammar
based parsing using the MIT Fast Parser. Here we found pattern matching, based on look-
ing for left and right delimiters such as comma and certain parts of speech, to be far more
accurate. For example, given &amp;quot;said Chugai&apos;s senior vice president for international trade,
Osamu Nagayama&amp;quot;, the appositive identifier would find &amp;quot;Chugai&apos;s senior vice president for
international trade&amp;quot;. For extracting premodifiers, head and postmodifiers, we have found
technique (b) to be somewhat more useful, though attachment errors still occur. The
extracted premodifiers and head (or maximal fragment thereof) are then looked up in the
semantic lexicon ontology; looking up &amp;quot;senior vice president&amp;quot; would yield corporate-officer
or government-official. Hypotheses about &amp;quot;Chugai&amp;quot;, based on information from Corefer-
ence linking it to an earlier mention of &amp;quot;Chugai Pharmaceutical Corp.&amp;quot;, can be used to
infer that &amp;quot;Osamu Nagayama&amp;quot; is more likely to be a corporate officer than a government
official.
</bodyText>
<page confidence="0.999565">
49
</page>
<sectionHeader confidence="0.998478" genericHeader="method">
4 Coreference
</sectionHeader>
<subsectionHeader confidence="0.998543">
4.1 Normalized Names
</subsectionHeader>
<bodyText confidence="0.999996710526316">
When a new mention is processed by the Coreference KS, pegs from previous mentions
seen earlier in the document are considered as candidate coanchored mentions. Obviously,
we wish to avoid considering the set of all previous pegs in the discourse. The use of focus
information at some level can be used to constrain this set, but that would require in turn
strong assumptions about the discourse structure of texts - which could severely limit our
applicable domains. Still, it seems unreasonable, given a mention of &amp;quot;Bill Clinton&amp;quot;, to
consider a peg for &amp;quot;New York City&amp;quot; as a possible antecedent. This suggests we consider
only previous mentions which are similar in some way. We do this by indexing each
mention by a normalized name, and considering only pegs for mentions which have the
same normalized name. This raises the issue of the choice of a normalized name key.
Obviously, there can be considerable variability in the form of a name across different
mentions. For example, a mention of &amp;quot;President Clinton&amp;quot; could be followed by &amp;quot;Bill
Clinton&amp;quot;; one of &amp;quot;Georgetown University&amp;quot; by &amp;quot;Georgetown&amp;quot;; &amp;quot;the Los Angeles Lakers&amp;quot;
by &amp;quot;the Lakers&amp;quot;. (See [Carroll, 85] for a discussion of the regularities and numerous
irregularities in alternations in name forms, many of which involve metonymic reference).
In the training corpus, the heuristic of choosing the last name element in the surface
form of a name as a normalized name works well for people. This may reflect the fact
that newspapers often impose their own normalization conventions. There are obvious
exceptions to the last name element heuristic; for example, in the WSJ, a mention of
&amp;quot;Roh Tae Woo&amp;quot; is followed by a co-referential mention of &amp;quot;Mr. Roh&amp;quot;. For organization
names, our heuristic is to choose all but the last element as the normalized name, but
to allow a degree of partial matching. Given a new name mention, upon failure to find
a partition cell having previous mentions with the same normalized name, partition cells
with neighboring normalized names are searched. (The closeness metric here involves
having a high percentage of sequential words in common). Thus the WSJ mentions
of &amp;quot;Leaseway Transportation Corp&amp;quot; followed by &amp;quot;Leaseway&amp;quot; would be tied together, as
would &amp;quot;Canadian Technical Tape Inc.&amp;quot; and &amp;quot;Technical Tape&amp;quot;. Of course, at the time
of invoking Coreference for a hypothesis associated with a mention, we may or may not
have (depending in part on the ordering of knowledge sources) enough information to
decide which normalized name heuristic to invoke, in which case we use the last name as
a default.
In practice the matching on normalized names works well, except for cases like Mr.
Roh above, and in cases of spelling errors. If necessary, the system can use a strategy of
iterative widening; if the system fails to find a coreferring mention, in iterative widening
mode it attempts to search through the space of all other previous mentions. In this
mode, the system can also separately collect and warn about mentions whose names are
close to (using the Damerau-Levenshtein similarity metric) but not identical in spelling
to the current mention.
</bodyText>
<subsectionHeader confidence="0.998335">
4.2 Coreference Algorithm
</subsectionHeader>
<bodyText confidence="0.99989075">
At each peg site, the system unifies information from Hypotheses associated with the new
mention with information accumulated from the other mentions at the peg site. As a rule,
successful unification results in coanchoring. The Coreference procedure terminates when
all the pegs in the relevant normalized name partition cell have been considered. A failure
</bodyText>
<page confidence="0.990063">
50
</page>
<bodyText confidence="0.99995705">
of unification, which results from a conflict from a new mention at a peg site, can lead to
three possible outcomes: (i) Ignoring of the conflict, in which case coanchoring of the new
mention to the peg is established; (ii) Overriding of earlier information accumulating at
the peg in question, in which case coanchoring of the new mention to the peg is established,
and coanchoring links from any other conflicting mentions to the peg are broken; or (iii)
Honoring of the conflict, leading to (a) considering some other peg, or if none remains,
(b) the creation of a new peg. The decision whether to Ignore or Override is based on the
relative strength of the hypotheses emanating from different mentions: (i) Conflicts are
Ignored when the information from the new mention has low confidence. (ii) Conflicts are
Overriden when (a) (Weak-Opposition-Loses) the conflicting information from the new
mention has high confidence and the conflicting information from the old mention has low
confidence, or (b) (Strong-Majority-Wins) all the other evidence at the peg (there must
be some) strongly confirms the new mention&apos;s hypothesis. Strong-Majority-Wins requires
that there are at least two old mentions at the peg, with only one old mention giving rise
to the conflict, and with all the other old mentions at the peg being compatible with the
new mention at a high level of confidence for each attribute. Once a link from a mention
is broken, the mention can be relinked to some other peg (either existing, or a new one).
(iii) Otherwise, the conflict is Honored.
Figure 1 shows an example of Coreference and ambiguity resolution. To simplify the
presentation, only one hypothesis is shown per mention, appositives are ignored, and
each attribute of each hypothesis is assumed to have the same confidence. (A Mention is
identified as a string, with the hypothesis directly below it.)
Assume Mention 1 is discourse-initial; assume further that Person-Name and Age have
fired. Coreference on Mention 1 leads to the creation of a new peg, Peg 1, representing
the hypothetical entity Bill Clinton. Coreference on Mention 2 leads to a search in the
normalized-name partition for Clinton. The system unifies the properties associated with
Mention 2 with Mention l&apos;s properties. In this case, since there is no conflict, both
mentions are anchored to Peg 1. Mention 3 results in Coreference attempting a link to
Peg 1. This leads to a conflict in unification with the properties from one of the other
links to Mention 1, arising specifically from the full name and gender information extracted
from Mention 1. These are conflicts because they violate a single-valued constraint for
these attributes. The conflict with Mention 3 is honored, since there is no disparity in
confidence measures. This results in Mention 3 being anchored to a new peg Peg 2,
representing a hypothetical entity Hilary Clinton. Mention 4&apos;s properties are compatible
with both pegs, hence it is coanchored to both, making it ambiguous. Mention 5 leads
to a conflict on name at Peg 1. There is no confidence disparity at Peg 1, so the conflict
is honored, resulting in a search for some other peg. At Peg 2, there is a conflict on
occupation, but since Mention 3 is compatible with Mention 5, by Strong-Majority-Wins,
Mention 3 overrides the information from Mention 4. This leads to breaking of the link
of the conflicting mention with Peg 2, disambiguating Mention 4.
</bodyText>
<sectionHeader confidence="0.998953" genericHeader="method">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999063">
The system has been run on one million words of text (two years of WSJ training cor-
pus as well as the [Kahaner, 91] email corpus). The identification of person names and
geographical locations is in place, as well as a rudimentary organization tagger (which
does not extract any interesting attributes regarding the organization). The pegs-based
</bodyText>
<page confidence="0.994817">
51
</page>
<bodyText confidence="0.999640347826087">
Coreference KS has been implemented, but the breaking of a link from a mention to a
peg is not as yet propagated to other pegs. We have not yet implemented a treatment
of partial dependents, which involve modeling inter-relationships among pegs. Problems
we are currently working on include conjunctions (e.g. is &amp;quot;AVX and Kyocera&amp;quot; a single
entity?), the treatment of partial dependents and references to sets (e.g. the discourse &amp;quot;In-
dira Gandhi....Rajiv Gandhi....the Gandhis&amp;quot;). We are also investigating the applicability
of Bayesian inference networks to the overall problem.
Recently, we conducted an empirical evaluation of the system. In a nutshell (details
are deferred to a separate paper), the evaluation was carried out on a test set of 42 hand-
tagged WSJ articles, using a scoring program we developed. The hand-tagging marked
only the type of the tag (person, organization, or location), ignoring attributes. Scores
on &lt;precision, recall&gt; varied from &lt;76%, 72%&gt; to &lt;84%, 80%&gt;, depending on whether
partial matches (e.g. only a fragment of a name in the program&apos;s tag, or a title identified as
part of a name) were accepted. We soon expect to more directly evaluate the Coreference
KS, but in the meantime we can offer the observation that the Coreference KS has been
observed to be extremely effective (apart from the exceptions we mentioned earlier) for
name mentions in the WSJ, especially for people mentions.
In conclusion, then, we have found that a treatment of proper names as potentially
context-dependent linguistic expressions can be effectively applied to the problem of un-
known name identification in newswire text, especially when combined with local-context
based text skimming. In addition to determining more precisely the genre limitations of
such an approach, one future direction would be to consider porting the system to another
language.
</bodyText>
<sectionHeader confidence="0.976498" genericHeader="method">
References
</sectionHeader>
<bodyText confidence="0.995951277777778">
[Aberdeen et al., 92] J. Aberdeen, J. Burger, D. Connolly, S. Roberts, and M. Vilain,
&amp;quot;Description of the Alembic System as used in MUC-4&amp;quot;, Proceedings of the Fourth
Message Understanding Conference, 1992, pp. 215-222.
[Amsler, 871 Robert A. Amsler, &amp;quot;Research Towards the Development of a Lexical Knowl-
edge Base for Natural Language Processing&amp;quot;, SIGIR Forum, 123, (1-2), 1989.
[Aone et al., 92] C. Aone, D. McKee, S. Shinn, H. Blejer, &amp;quot;Description of the Solomon
System as Used for MUC-4&amp;quot;, Proceedings of the Fourth Message Understanding Con-
ference, 1992, pp. 259-267.
[Carroll, 85] John M. Carroll, &amp;quot;What&apos;s in a Name?&amp;quot;, Freeman and Company, New York,
1985.
[Coates-Stephens, 91] Sam Coates-Stephens, &amp;quot;Automatic Lexical Acquisition Using
Within-Text Descriptions of Proper Nouns&amp;quot;, Proceedings of the Seventh Annual Con-
ference of the UW Centre for the New OED and Text Research, 1991, pp. 154-169.
[Cowie et al., 92] J. Cowie, L. Guthrie, Y. Wilks, J. Pustejovsky, and S. Waterman, &amp;quot;De-
scription of the Solomon System as Used for MUC-4&amp;quot;, Proceedings of the Fourth
Message Understanding Conference, 1992, pp. 223-232.
[Dahl and Ball, 90] D. Dahl and C.N. Ball, &amp;quot;Reference Resolution in PUNDIT&amp;quot;, Technical
Report, Unisys, 1987.
</bodyText>
<page confidence="0.997538">
52
</page>
<reference confidence="0.995715518518518">
[de Marcken, 90] C. G. de Marcken, &amp;quot;Parsing the LOB Corpus&amp;quot;, Proceedings of the 28th
Annual Meeting of the Association for Computational Linguistics, 1990, pp. 243-251.
[Heim, 81] I. Heim, The Semantics of Definite and Indefinite Noun Phrases, Ph.D. Dis-
sertation, Department of Linguistics, University of Massachusetts, 1981.
[Jacobs 88] P. Jacobs, &amp;quot;Relation Driven Text Skimming&amp;quot;, General Electric Co. Technical
Report, 1988.
[Kahaner, 91] The Kahaner email corpus.
[Karttunen, 681 Lauri Karttunen, Discourse Referents, in J. McCawley, (ed.), Syntax and
Semantics, Academic Press, New York.
[Landman 861 F. Landman, &amp;quot;Pegs and Alecs.&amp;quot;, Linguistics and Philosophy, 97-155, 1986.
[Luperfoy, 91] Susann Luperfoy, &amp;quot;Discourse Pegs: A Computational Treatment of
Context-Dependent Referring Expressions&amp;quot;, Ph.D. Dissertation, Department of Lin-
guistics, University of Texas at Austin.
[Mauldin 89] Michael L. Mauldin, &amp;quot;Information Retrieval by Text Skimming&amp;quot;, Carnegie
Mellon University Technical Report CMU-CS-89-193.
[Norvig, 92] Peter Norvig, &amp;quot;Paradigms of Artificial Intelligence Programming: Case Stud-
ies in Common Lisp&amp;quot;, Morgan Kaufmann, 1992.
[Paik et al., 931 Woojin Paik, Elizabeth D. Liddy, Edmund Yu, and Mary McKenna, &amp;quot;In-
terpretation of Proper Nouns for Information Retrieval&amp;quot;, Preliminary Proceedings of
the ARPA Workshop on Human Language Technology, Princeton, March 21-24, 1993.
[Radford, 88] Andrew Radford, &amp;quot;Transformational Grammar&amp;quot;, Cambridge University
Press, 1988.
[Sidner, 79] C. L. Sidner, &amp;quot;Towards a Computational Theory of Definite Anaphora Com-
prehension in Discourse&amp;quot;, Ph.D Thesis, Electrical Engineering and Computer Science,
M.I.T., 1979.
[Webber, 78] B. Webber, &amp;quot;A Formal Approach to Discourse Anaphora&amp;quot;, Ph.D. Thesis,
Department of Applied Mathematics, Harvard University, 1978.
</reference>
<page confidence="0.992261">
53
</page>
<sectionHeader confidence="0.797868" genericHeader="method">
MENTIONS AND HYPOTHESES PEGS
</sectionHeader>
<reference confidence="0.908281476190476">
1. &amp;quot;Bill Clinton, 45&amp;quot; [1. Bill.Clinton]
Name: Bill.Clinton
Age: 45
Norm: Clinton
2. &amp;quot;Mr. Clinton&amp;quot; [1]
Name: .Clinton
Gender: Male
Norm: Clinton
3. &amp;quot;Ms. Hilary Clinton&amp;quot; [2. Hilary.Clinton]
Name: Hilary.Clinton
Gender: Female
Norm: Clinton
4. &amp;quot;U.S. President Clinton&amp;quot; [2, 1]
Name: .Clinton
Occupation: HeadofState
Norm: Clinton
5. &amp;quot;First Lady Hilary Clinton&amp;quot; [2]
Name: Hilary.Clinton Leads to breaking of link from
Gender: Female Mention 4 to Peg 2.
Occupation: FirstLady
Norm: Clinton
</reference>
<figureCaption confidence="0.987376">
Figure 1: Coreference and disambiguation
</figureCaption>
<page confidence="0.9955">
54
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.808626">
<title confidence="0.9483615">Identifying Unknown Proper Names in Newswire Text</title>
<author confidence="0.992673">Inderjeet Mani</author>
<author confidence="0.992673">T Richard Susann Luperfoy</author>
<author confidence="0.992673">Elaine P Lusher</author>
<author confidence="0.992673">Sharon J Laskowski</author>
<affiliation confidence="0.9729625">Artificial Intelligence Technical The MITRE Corporation, Mail Stop</affiliation>
<address confidence="0.998709">7525 Colshire Drive, McLean, Virginia</address>
<email confidence="0.995357">mani@starbase.mitre.org</email>
<abstract confidence="0.998102416666667">The identification of unknown proper names in text is a significant challenge for NLP systems operating on unrestricted text. A system which indexes documents according to name references can be useful for information retrieval or as a preprocessor for more knowledge intensive tasks such as database extraction. This paper describes a system which uses text skimming techniques for deriving proper names and their semantic attributes automatically from newswire text, without relying on any listing of name elements. In order to identify new names, the system treats proper names as (potentially) context-dependent linguistic expressions. In addition to using information in the local context, the system exploits a computational model of discourse which identifies individuals based on the way they are described in the text, instead of relying on their description in a pre-existing knowledge base.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C G de Marcken</author>
</authors>
<title>Parsing the LOB Corpus&amp;quot;,</title>
<date>1990</date>
<booktitle>Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>243--251</pages>
<marker>[de Marcken, 90]</marker>
<rawString>C. G. de Marcken, &amp;quot;Parsing the LOB Corpus&amp;quot;, Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics, 1990, pp. 243-251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Heim</author>
</authors>
<title>The Semantics of Definite and Indefinite Noun Phrases,</title>
<date>1981</date>
<tech>Ph.D. Dissertation,</tech>
<institution>Department of Linguistics, University of Massachusetts,</institution>
<marker>[Heim, 81]</marker>
<rawString>I. Heim, The Semantics of Definite and Indefinite Noun Phrases, Ph.D. Dissertation, Department of Linguistics, University of Massachusetts, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jacobs</author>
</authors>
<title>Relation Driven Text Skimming&amp;quot;, General Electric Co.</title>
<date>1988</date>
<tech>Technical Report,</tech>
<marker>[Jacobs 88]</marker>
<rawString>P. Jacobs, &amp;quot;Relation Driven Text Skimming&amp;quot;, General Electric Co. Technical Report, 1988.</rawString>
</citation>
<citation valid="true">
<title>The Kahaner email corpus. [Karttunen, 681 Lauri Karttunen, Discourse Referents,</title>
<date>1986</date>
<booktitle>Syntax and Semantics,</booktitle>
<pages>97--155</pages>
<editor>in J. McCawley, (ed.),</editor>
<publisher>Academic Press,</publisher>
<location>New York. [Landman</location>
<marker>[Kahaner, 91]</marker>
<rawString>The Kahaner email corpus. [Karttunen, 681 Lauri Karttunen, Discourse Referents, in J. McCawley, (ed.), Syntax and Semantics, Academic Press, New York. [Landman 861 F. Landman, &amp;quot;Pegs and Alecs.&amp;quot;, Linguistics and Philosophy, 97-155, 1986.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Susann Luperfoy</author>
</authors>
<title>Discourse Pegs: A Computational Treatment of Context-Dependent Referring Expressions&amp;quot;,</title>
<tech>Ph.D. Dissertation,</tech>
<institution>Department of Linguistics, University of Texas at Austin.</institution>
<marker>[Luperfoy, 91]</marker>
<rawString>Susann Luperfoy, &amp;quot;Discourse Pegs: A Computational Treatment of Context-Dependent Referring Expressions&amp;quot;, Ph.D. Dissertation, Department of Linguistics, University of Texas at Austin.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Michael L Mauldin</author>
</authors>
<title>Information Retrieval by Text Skimming&amp;quot;,</title>
<tech>Technical Report CMU-CS-89-193.</tech>
<institution>Carnegie Mellon University</institution>
<marker>[Mauldin 89]</marker>
<rawString>Michael L. Mauldin, &amp;quot;Information Retrieval by Text Skimming&amp;quot;, Carnegie Mellon University Technical Report CMU-CS-89-193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Norvig</author>
</authors>
<title>Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp&amp;quot;,</title>
<date>1992</date>
<booktitle>Preliminary Proceedings of the ARPA Workshop on Human Language Technology,</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>Elizabeth</location>
<marker>[Norvig, 92]</marker>
<rawString>Peter Norvig, &amp;quot;Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp&amp;quot;, Morgan Kaufmann, 1992. [Paik et al., 931 Woojin Paik, Elizabeth D. Liddy, Edmund Yu, and Mary McKenna, &amp;quot;Interpretation of Proper Nouns for Information Retrieval&amp;quot;, Preliminary Proceedings of the ARPA Workshop on Human Language Technology, Princeton, March 21-24, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Radford</author>
</authors>
<title>Transformational Grammar&amp;quot;,</title>
<date>1988</date>
<publisher>Cambridge University Press,</publisher>
<marker>[Radford, 88]</marker>
<rawString>Andrew Radford, &amp;quot;Transformational Grammar&amp;quot;, Cambridge University Press, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Sidner</author>
</authors>
<title>Towards a Computational Theory of Definite Anaphora Comprehension in Discourse&amp;quot;, Ph.D Thesis, Electrical Engineering and Computer Science,</title>
<date>1979</date>
<location>M.I.T.,</location>
<marker>[Sidner, 79]</marker>
<rawString>C. L. Sidner, &amp;quot;Towards a Computational Theory of Definite Anaphora Comprehension in Discourse&amp;quot;, Ph.D Thesis, Electrical Engineering and Computer Science, M.I.T., 1979.</rawString>
</citation>
<citation valid="false">
<authors>
<author>B Webber</author>
</authors>
<title>A Formal Approach to Discourse Anaphora&amp;quot;,</title>
<date>1978</date>
<tech>Ph.D. Thesis,</tech>
<institution>Department of Applied Mathematics, Harvard University,</institution>
<location>Clinton</location>
<marker>[Webber, 78]</marker>
<rawString>B. Webber, &amp;quot;A Formal Approach to Discourse Anaphora&amp;quot;, Ph.D. Thesis, Department of Applied Mathematics, Harvard University, 1978. 1. &amp;quot;Bill Clinton, 45&amp;quot; [1. Bill.Clinton] Name: Bill.Clinton Age: 45 Norm: Clinton 2. &amp;quot;Mr. Clinton&amp;quot; [1] Name: .Clinton Gender: Male Norm: Clinton 3. &amp;quot;Ms. Hilary Clinton&amp;quot; [2. Hilary.Clinton] Name: Hilary.Clinton Gender: Female Norm: Clinton 4. &amp;quot;U.S. President Clinton&amp;quot; [2, 1] Name: .Clinton Occupation: HeadofState Norm: Clinton 5. &amp;quot;First Lady Hilary Clinton&amp;quot; [2] Name: Hilary.Clinton Leads to breaking of link from Gender: Female Mention 4 to Peg 2. Occupation: FirstLady Norm: Clinton</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>