<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005116">
<title confidence="0.9992495">
A Wizard-of-Oz environment to study Referring Expression Generation
in a Situated Spoken Dialogue Task
</title>
<author confidence="0.997318">
Srinivasan Janarthanam
</author>
<affiliation confidence="0.9979695">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.803062">
Edinburgh EH8 9AB
</address>
<email confidence="0.997826">
s.janarthanam@ed.ac.uk
</email>
<sectionHeader confidence="0.993856" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998837285714286">
We present a Wizard-of-Oz environment
for data collection on Referring Expres-
sion Generation (REG) in a real situated
spoken dialogue task. The collected data
will be used to build user simulation mod-
els for reinforcement learning of referring
expression generation strategies.
</bodyText>
<sectionHeader confidence="0.999267" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.944484025">
In this paper, we present a Wizard-of-Oz (WoZ)
environment for data collection in a real situated
spoken dialogue task for referring expression gen-
eration (REG). Our primary objective is to study
how participants (hereafter called users) with dif-
ferent domain knowledge and expertise interpret
and resolve different types of referring expressions
(RE) in a situated dialogue context. We also study
the effect of the system’s lexical alignment due
to priming (Pickering and Garrod, 2004) by the
user’s choice of REs. The users follow instruc-
tions from an implemented dialogue manager and
realiser to perform a technical but realistic task –
setting up a home Internet connection. The dia-
logue system’s utterances are manipulated to con-
tain different types of REs - descriptive, technical,
tutorial or lexically aligned REs, to refer to various
domain objects in the task. The users’ responses
to different REs are then logged and studied.
(Janarthanam and Lemon, 2009) presented a
framework for reinforcement learning of optimal
natural language generation strategies to choose
appropriate REs to users with different domain
knowledge expertise. For this, we need user sim-
ulations with different domain knowledge profiles
that are sensitive to the system’s choice of REs. A
WoZ environment is an ideal tool for data collec-
tion to build data-driven user simulations. How-
ever, our study requires a novel WoZ environment.
In section 2, we present prior related work. Sec-
tion 3 describes the task performed by partici-
Oliver Lemon
School of Informatics
University of Edinburgh
Edinburgh EH8 9AB
olemon@inf.ed.ac.uk
pants. In section 4, we describe the WoZ envi-
ronment in detail. Section 5 describes the data
collected in this experiment and section 6 presents
some preliminary results from pilot studies.
</bodyText>
<sectionHeader confidence="0.999777" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99983535">
(Whittaker et al., 2002) present a WoZ environ-
ment to collect data concerning dialogue strate-
gies for presenting restaurant information to users.
This study collects data on strategies used by users
and human expert wizards to obtain and present in-
formation respectively. (van Deemter et al., 2006)
present methods to collect data (the TUNA cor-
pus) for REG using artificially constructed pic-
tures of furniture and photographs of real people.
(Arts, 2004) presents a study choosing between
technical and descriptive expressions for instruc-
tion writing.
In contrast to the above studies, our study is
novel in that it collects data from users having dif-
ferent levels of expertise in a real situated task do-
main, and for spontaneous spoken dialogue. Our
focus is on choosing between technical, descrip-
tive, tutorial, and lexically aligned expressions
rather than selecting different attributes for gen-
erating descriptions.
</bodyText>
<sectionHeader confidence="0.990917" genericHeader="method">
3 The Domain Task
</sectionHeader>
<bodyText confidence="0.957965857142857">
In this experiment, the task for each user is to lis-
ten to and follow the instructions from the WoZ
system and set up their home broadband Internet
connection. We provide the users with a home-
like environment with a desktop computer, phone
socket and a Livebox package from Orange con-
taining cables and components such as the mo-
dem, broadband filters and a power adaptor. Dur-
ing the experiment, they set up the Internet con-
nection by connecting these components to each
other. Prior to the task, the users are informed that
they are interacting with a spoken dialogue system
Proceedings of the 12th European Workshop on Natural Language Generation, pages 94–97,
Athens, Greece, 30 – 31 March 2009. c�2009 Association for Computational Linguistics
</bodyText>
<page confidence="0.990608">
94
</page>
<figure confidence="0.969433923076923">
yes
no
ok
req description
req location
req verify jargon
req verify desc
req repeat
req rephrase
req wait
“Yes it is on”
“No, its not flashing”
“Ok. I did that”
</figure>
<bodyText confidence="0.99186047826087">
“Whats an ethernet cable?”
“Where is the filter?”
“Is it the ethernet cable?”
“Is it the white cable?”
“Please repeat”
“What do you mean?”
“Give me a minute?”
that will give them instructions to set up the con-
nection. However, their utterances are intercepted
by a human wizard. The users are requested to
have a conversation as if they were talking to a hu-
man operator, asking for clarifications if they are
confused or fail to understand the system’s utter-
ances. The system’s utterances are converted au-
tomatically to speech using the Cereproc Speech
Synthesiser and played back to the user. The user
follows the instructions and assembles the compo-
nents. The setup is examined by the wizard at the
end of the experiment to measure the percentage of
task success. The user also fills in questionnaires
prior to and after the task answering questions on
his background, quality of the system during the
task and the knowledge gained during the task.
</bodyText>
<sectionHeader confidence="0.987921" genericHeader="method">
4 The Wizard-of-Oz environment
</sectionHeader>
<bodyText confidence="0.999985909090909">
The Wizard-of-Oz environment facilitates the en-
tire experiment as described in the section above.
The environment consists of the Wizard Interac-
tion Tool, the dialogue system and the wizard. The
users wear a headset with a microphone. Their ut-
terances are relayed to the wizard who then anno-
tates it using the Wizard Interaction Tool (shown
in figure 1) and sends it to the dialogue system.
The system responds with a natural language ut-
terance which is automatically converted to speech
and is played back to the user and the wizard.
</bodyText>
<subsectionHeader confidence="0.938367">
4.1 Wizard Interaction Tool (WIT)
</subsectionHeader>
<bodyText confidence="0.796797">
The Wizard Interaction Tool (WIT) (shown in fig-
ure 1) allows the wizard to interact with the dia-
logue system and the user. The GUI is divided in
to several panels.
a. System Response Panel - This panel displays
the dialogue system’s utterances and RE choices
for the domain objects in the utterance. It also dis-
plays the strategy adopted by the system currently
and a visual indicator of whether the system’s ut-
terance is being played to the user.
</bodyText>
<listItem confidence="0.9589821">
b. Confirmation Request Panel - This panel lets
the wizard handle issues in communication (for
e.g. noise). The wizard can ask the user to repeat,
speak louder, confirm his responses, etc using ap-
propriate pre-recorded messages or build his own
custom messages.
c. Confirmation Panel - This panel lets the wiz-
ard handle confirmation questions from the user.
The wizard can choose ’yes’ or ’no’ or build a cus-
tom message.
</listItem>
<tableCaption confidence="0.811294">
Table 1: User Dialogue Acts.
</tableCaption>
<listItem confidence="0.9398242">
d. Annotation panel - This panel lets the wizard
annotate the content of participant’s utterances.
User responses (dialogue acts and example utter-
ances) that can be annotated using this panel are
given in Table 1. In addition to these, other be-
haviours, like remaining silent or saying irrelevant
things are also accommodated.
e. User’s RE Choice panel - The user’s choice
of REs to refer to the domain objects are annotated
by the wizard using this panel.
</listItem>
<subsectionHeader confidence="0.980368">
4.2 The Instructional Dialogue Manager
</subsectionHeader>
<bodyText confidence="0.999972714285714">
The dialogue manager drives the conversation by
giving instructions to the users. It follows a deter-
ministic dialogue management policy so that we
only study variation in the decisions concerning
the choice of REs. It should be noted that typi-
cal WoZ environments (Whittaker et al., 2002) do
not have dialogue managers and the strategic de-
cisions will be taken by the wizard. Our dialogue
system has three main responsibilities - choosing
the RE strategy, giving instructions and handling
clarification requests.
The dialogue system, initially randomly
chooses the RE strategy at the start of the
dialogue. The list of strategies are as follows.
</bodyText>
<listItem confidence="0.936982166666667">
1. Jargon: Choose technical terms for every ref-
erence to the domain objects.
2. Descriptive: Choose descriptive terms for ev-
ery reference to the domain objects.
3. Tutorial: Use technical terms, but also aug-
ment the description for every reference.
</listItem>
<bodyText confidence="0.999882833333333">
The above three strategies are also augmented
with an alignment feature, so that the system can
either align or not align with the user’s prior choice
of REs. In aligned strategies, the system abandons
the existing strategy (jargon, descriptive or tuto-
rial) for a domain object reference when the user
</bodyText>
<page confidence="0.998248">
95
</page>
<figureCaption confidence="0.999937">
Figure 1: Wizard Interaction Tool
</figureCaption>
<bodyText confidence="0.999980575">
uses a different expression from that of the system
to refer to the domain object. For instance, under
the descriptive strategy, the ethernet cable is re-
ferred to as “the thick cable with red ends”. But
if the user refers to it as “ethernet cable”, then the
system uses “ethernet cable” in subsequent turns
instead of the descriptive expression. In case of
non-aligned strategies, the system simply ignores
user’s use of novel REs and continues to use its
own strategy.
The step-by-step instructions to set up the
broadband connection are hand-coded as a dia-
logue script. The script is a simple determinis-
tic finite state automaton, which contains execu-
tion instruction acts(e.g. Plug in the cable in to
the socket) and observation instruction acts(e.g. Is
the ethernet light flashing?) for the user. Based
on the user’s response, the system identifies the
next instruction. However, the script only con-
tains the dialogue acts. The dialogue acts are then
processed by a built-in realiser component to cre-
ate the system utterances. The realiser uses tem-
plates in which references to domain objects are
changed based on the selected strategy to create
final utterances. By using a fixed dialogue man-
agement policy and by changing the REs, we only
explore users’ reactions to various RE strategies.
The utterances are finally converted to speech and
are played back to the user.
The dialogue system handles two kinds of clar-
ification requests - open requests and closed re-
quests. With open CRs, users request the sys-
tem for location of various domain objects (e.g.
“where is the ethernet cable?”) or to describe
them. With closed CRs, users verify the intended
reference, in case of ambiguity (e.g. “Do you
mean the thin white cable with grey ends?”, “Is
it the broadband filter?”, etc.). The system han-
dles these requests using a knowledge base of the
domain objects.
</bodyText>
<subsectionHeader confidence="0.999636">
4.3 Wizard Activities
</subsectionHeader>
<bodyText confidence="0.999963">
The primary responsibility of the wizard is to un-
derstand the participant’s utterance and annotate
it as one of the dialogue acts in the Annotation
panel, and send the dialogue act to the dialogue
system for response. In addition to the primary
responsibility, the wizard also requests confirma-
tion from the user (if needed) and also responds to
confirmation requests from the user. The wizard
also observes the user’s usage of novel REs and
records them in the User’s RE Choice panel. As
mentioned earlier, our wizard neither decides on
which strategy to use to choose REs nor chooses
</bodyText>
<page confidence="0.979878">
96
</page>
<bodyText confidence="0.941317">
the next task instruction to give the user.
</bodyText>
<sectionHeader confidence="0.847174" genericHeader="method">
5 Data collected
</sectionHeader>
<bodyText confidence="0.914832071428571">
Several different kinds of data are collected before,
during and after the experiment. This data will be
used to build user simulations and reward func-
tions for learning REG strategies and language
models for speech recognition.
1. WIT log - The WIT logs the whole conversa-
tion as an XML file. The log contains system and
user dialogue acts, time of system utterance, sys-
tem’s choice of REs and its utterance at every turn.
It also contains the dialogue start time, total time
elapsed, total number of turns, number of words
in system utterances, number of clarification re-
quests, number of technical, descriptive and tuto-
rial REs, number of confirmations etc.
</bodyText>
<listItem confidence="0.8074422">
2. Background of the user - The user is asked to fill
in a pre-task background questionnaire containing
queries on their experience with computers, Inter-
net and dialogue systems.
3. User satisfaction survey - The user is re-
</listItem>
<bodyText confidence="0.9694983">
quested to fill in a post-task questionnaire contain-
ing queries on the performance of the system dur-
ing the task. Each question is answered in a four
point Likert scale on how strongly the user agrees
or disagrees with the given statement. Statements
like, “Conversation with the system was easy”,
“I would use such a system in future”, etc are
judged by the user which will be used to build re-
ward functions for reinforcement learning of REG
strategies.
</bodyText>
<listItem confidence="0.958714125">
4. Knowledge pre-test - Users’ initial domain
knowledge is tested by asking them to match a list
of technical terms to their respective descriptive
expressions.
5. Knowledge gain post-test - Users’ knowledge
gain during the dialogue task is measured by ask-
ing them to redo the matching task.
6. Percentage of task completion - The wizard
</listItem>
<bodyText confidence="0.972638454545455">
examines the final set up on the user’s table to
determine the percentage of task success using a
form containing declarative statements describing
the ideal broadband set up (for e.g. “the broad-
band filter is plugged in to the phone socket on
the wall”). The wizard awards one point to every
statement that is true of the user’s set up.
7. User’s utterances WAV file - The user’s ut-
terances are recorded in WAV format for build-
ing language models for automatic speech recog-
nition.
</bodyText>
<sectionHeader confidence="0.999534" genericHeader="evaluation">
6 Results from pilot studies
</sectionHeader>
<bodyText confidence="0.999937">
We are currently running pilot studies (with 6 par-
ticipants so far) and have collected around 60 min-
utes of spoken dialogue data. We found that in
the jargon strategy, some users take a lot longer to
finish the task than others (max 59 turns, min 26
turns). We found that besides requesting clarifi-
cations, sometimes novice users assume incorrect
references to some domain objects, affecting their
task completion rates.
</bodyText>
<sectionHeader confidence="0.998901" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999984">
We have presented a novel Wizard-of-Oz environ-
ment to collect spoken data in a real situated task
environment, and to study user reactions to a va-
riety of REG strategies, including system align-
ment. The data will be used for training user sim-
ulations for reinforcement learning of REG strate-
gies to choose between technical, descriptive, tu-
torial, and aligned REs based on a user’s expertise
in the task domain.
</bodyText>
<sectionHeader confidence="0.996039" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999908714285714">
The research leading to these results has re-
ceived funding from the European Community’s
Seventh Framework (FP7) under grant agree-
ment no. 216594 (CLASSiC Project www.classic-
project.org), EPSRC project no. EP/E019501/1,
and the British Council (UKIERI PhD Scholar-
ships 2007-08).
</bodyText>
<sectionHeader confidence="0.999255" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995418529411765">
A. Arts. 2004. Overspeci�cation in Instructive Text.
Ph.D. thesis, Tilburg University, The Netherlands.
S. Janarthanam and O. Lemon. 2009. Learning Lexi-
cal Alignment Policies for Generating Referring Ex-
pressions for Spoken Dialogue Systems. In Proc.
ENLG’09.
M. J. Pickering and S. Garrod. 2004. Toward a mech-
anistic psychology of dialogue. Behavioral and
Brain Sciences, 27:169–225.
K. van Deemter, I. van der Sluis, and A. Gatt.
2006. Building a semantically transparent corpus
for the generation of referring expressions. In Proc.
INLG’06.
S. Whittaker, M. Walker, and J. Moore. 2002. Fish
or Fowl: A Wizard of Oz Evaluation of Dialogue
Strategies in the Restaurant Domain. In Language
Resources and Evaluation Conference.
</reference>
<page confidence="0.999673">
97
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.919116">
<title confidence="0.9995965">A Wizard-of-Oz environment to study Referring Expression in a Situated Spoken Dialogue Task</title>
<author confidence="0.995357">Srinivasan</author>
<affiliation confidence="0.9993585">School of University of</affiliation>
<address confidence="0.931014">Edinburgh EH8</address>
<email confidence="0.998389">s.janarthanam@ed.ac.uk</email>
<abstract confidence="0.99927975">We present a Wizard-of-Oz environment for data collection on Referring Expression Generation (REG) in a real situated spoken dialogue task. The collected data will be used to build user simulation models for reinforcement learning of referring expression generation strategies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Arts</author>
</authors>
<title>Overspeci�cation in Instructive Text.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Tilburg University, The Netherlands.</institution>
<contexts>
<context position="2783" citStr="Arts, 2004" startWordPosition="424" endWordPosition="425">ment in detail. Section 5 describes the data collected in this experiment and section 6 presents some preliminary results from pilot studies. 2 Related Work (Whittaker et al., 2002) present a WoZ environment to collect data concerning dialogue strategies for presenting restaurant information to users. This study collects data on strategies used by users and human expert wizards to obtain and present information respectively. (van Deemter et al., 2006) present methods to collect data (the TUNA corpus) for REG using artificially constructed pictures of furniture and photographs of real people. (Arts, 2004) presents a study choosing between technical and descriptive expressions for instruction writing. In contrast to the above studies, our study is novel in that it collects data from users having different levels of expertise in a real situated task domain, and for spontaneous spoken dialogue. Our focus is on choosing between technical, descriptive, tutorial, and lexically aligned expressions rather than selecting different attributes for generating descriptions. 3 The Domain Task In this experiment, the task for each user is to listen to and follow the instructions from the WoZ system and set u</context>
</contexts>
<marker>Arts, 2004</marker>
<rawString>A. Arts. 2004. Overspeci�cation in Instructive Text. Ph.D. thesis, Tilburg University, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Janarthanam</author>
<author>O Lemon</author>
</authors>
<title>Learning Lexical Alignment Policies for Generating Referring Expressions for Spoken Dialogue Systems.</title>
<date>2009</date>
<booktitle>In Proc. ENLG’09.</booktitle>
<contexts>
<context position="1482" citStr="Janarthanam and Lemon, 2009" startWordPosition="218" endWordPosition="221">essions (RE) in a situated dialogue context. We also study the effect of the system’s lexical alignment due to priming (Pickering and Garrod, 2004) by the user’s choice of REs. The users follow instructions from an implemented dialogue manager and realiser to perform a technical but realistic task – setting up a home Internet connection. The dialogue system’s utterances are manipulated to contain different types of REs - descriptive, technical, tutorial or lexically aligned REs, to refer to various domain objects in the task. The users’ responses to different REs are then logged and studied. (Janarthanam and Lemon, 2009) presented a framework for reinforcement learning of optimal natural language generation strategies to choose appropriate REs to users with different domain knowledge expertise. For this, we need user simulations with different domain knowledge profiles that are sensitive to the system’s choice of REs. A WoZ environment is an ideal tool for data collection to build data-driven user simulations. However, our study requires a novel WoZ environment. In section 2, we present prior related work. Section 3 describes the task performed by particiOliver Lemon School of Informatics University of Edinbu</context>
</contexts>
<marker>Janarthanam, Lemon, 2009</marker>
<rawString>S. Janarthanam and O. Lemon. 2009. Learning Lexical Alignment Policies for Generating Referring Expressions for Spoken Dialogue Systems. In Proc. ENLG’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Pickering</author>
<author>S Garrod</author>
</authors>
<title>Toward a mechanistic psychology of dialogue.</title>
<date>2004</date>
<booktitle>Behavioral and Brain Sciences,</booktitle>
<pages>27--169</pages>
<contexts>
<context position="1001" citStr="Pickering and Garrod, 2004" startWordPosition="140" endWordPosition="143">d data will be used to build user simulation models for reinforcement learning of referring expression generation strategies. 1 Introduction In this paper, we present a Wizard-of-Oz (WoZ) environment for data collection in a real situated spoken dialogue task for referring expression generation (REG). Our primary objective is to study how participants (hereafter called users) with different domain knowledge and expertise interpret and resolve different types of referring expressions (RE) in a situated dialogue context. We also study the effect of the system’s lexical alignment due to priming (Pickering and Garrod, 2004) by the user’s choice of REs. The users follow instructions from an implemented dialogue manager and realiser to perform a technical but realistic task – setting up a home Internet connection. The dialogue system’s utterances are manipulated to contain different types of REs - descriptive, technical, tutorial or lexically aligned REs, to refer to various domain objects in the task. The users’ responses to different REs are then logged and studied. (Janarthanam and Lemon, 2009) presented a framework for reinforcement learning of optimal natural language generation strategies to choose appropria</context>
</contexts>
<marker>Pickering, Garrod, 2004</marker>
<rawString>M. J. Pickering and S. Garrod. 2004. Toward a mechanistic psychology of dialogue. Behavioral and Brain Sciences, 27:169–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K van Deemter</author>
<author>I van der Sluis</author>
<author>A Gatt</author>
</authors>
<title>Building a semantically transparent corpus for the generation of referring expressions.</title>
<date>2006</date>
<booktitle>In Proc. INLG’06.</booktitle>
<marker>van Deemter, van der Sluis, Gatt, 2006</marker>
<rawString>K. van Deemter, I. van der Sluis, and A. Gatt. 2006. Building a semantically transparent corpus for the generation of referring expressions. In Proc. INLG’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Whittaker</author>
<author>M Walker</author>
<author>J Moore</author>
</authors>
<title>Fish or Fowl: A Wizard of Oz Evaluation</title>
<date>2002</date>
<booktitle>of Dialogue Strategies in the Restaurant Domain. In Language Resources and Evaluation Conference.</booktitle>
<contexts>
<context position="2353" citStr="Whittaker et al., 2002" startWordPosition="354" endWordPosition="357">files that are sensitive to the system’s choice of REs. A WoZ environment is an ideal tool for data collection to build data-driven user simulations. However, our study requires a novel WoZ environment. In section 2, we present prior related work. Section 3 describes the task performed by particiOliver Lemon School of Informatics University of Edinburgh Edinburgh EH8 9AB olemon@inf.ed.ac.uk pants. In section 4, we describe the WoZ environment in detail. Section 5 describes the data collected in this experiment and section 6 presents some preliminary results from pilot studies. 2 Related Work (Whittaker et al., 2002) present a WoZ environment to collect data concerning dialogue strategies for presenting restaurant information to users. This study collects data on strategies used by users and human expert wizards to obtain and present information respectively. (van Deemter et al., 2006) present methods to collect data (the TUNA corpus) for REG using artificially constructed pictures of furniture and photographs of real people. (Arts, 2004) presents a study choosing between technical and descriptive expressions for instruction writing. In contrast to the above studies, our study is novel in that it collects</context>
<context position="7411" citStr="Whittaker et al., 2002" startWordPosition="1201" endWordPosition="1204"> be annotated using this panel are given in Table 1. In addition to these, other behaviours, like remaining silent or saying irrelevant things are also accommodated. e. User’s RE Choice panel - The user’s choice of REs to refer to the domain objects are annotated by the wizard using this panel. 4.2 The Instructional Dialogue Manager The dialogue manager drives the conversation by giving instructions to the users. It follows a deterministic dialogue management policy so that we only study variation in the decisions concerning the choice of REs. It should be noted that typical WoZ environments (Whittaker et al., 2002) do not have dialogue managers and the strategic decisions will be taken by the wizard. Our dialogue system has three main responsibilities - choosing the RE strategy, giving instructions and handling clarification requests. The dialogue system, initially randomly chooses the RE strategy at the start of the dialogue. The list of strategies are as follows. 1. Jargon: Choose technical terms for every reference to the domain objects. 2. Descriptive: Choose descriptive terms for every reference to the domain objects. 3. Tutorial: Use technical terms, but also augment the description for every refe</context>
</contexts>
<marker>Whittaker, Walker, Moore, 2002</marker>
<rawString>S. Whittaker, M. Walker, and J. Moore. 2002. Fish or Fowl: A Wizard of Oz Evaluation of Dialogue Strategies in the Restaurant Domain. In Language Resources and Evaluation Conference.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>