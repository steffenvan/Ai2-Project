<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000307">
<title confidence="0.993716">
Improving Word Alignment Models
Using Structured Monolingual Corpora
</title>
<author confidence="0.966939">
Wei Wang *
</author>
<affiliation confidence="0.634985">
New York University
</affiliation>
<address confidence="0.957664">
719 Broadway Room 716
New York, NY
10003-6806 USA
</address>
<email confidence="0.820173">
weAcs.nyu.edu
</email>
<author confidence="0.812666">
Ming Zhou
</author>
<affiliation confidence="0.878446">
Microsoft Research Asia
5/F, Beijing Sigma Center
</affiliation>
<address confidence="0.9205215">
No.49, Zhichun Road, Haidian District
Beijing 100080 China
</address>
<email confidence="0.615654">
mingzhou@microsoft .com
</email>
<sectionHeader confidence="0.964348" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999895208333333">
We propose a new method to improve the per-
formance of word alignment algorithms, in par-
ticular the recall, using structured monolin-
gual corpora as sources to estimate cross lan-
guage word similarities. Normally, cross lan-
guage word similarities, i.e., the similarity be-
tween a source language word and a target
language word, can be estimated with a bilin-
gual corpus of enough size. We use a method
to estimate them from two structured mono-
lingual corpora based on the dependency cor-
respondence assumption justified on large and
balanced bilingual corpora. We selected three
typical word alignment models ranging over
statistical-based ones and heuristic-based ones,
to test whether cross language similarities can
improve the performance of word alignment
models. The crosslingual word similarities are
simply interpolated into these models. The ex-
periments show that crosslingual word similari-
ties estimated from structured monolingual cor-
pora can effectively improve the performance
of word alignment models, in particular the re-
call.
</bodyText>
<sectionHeader confidence="0.996394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998890230769231">
Word alignment algorithms, e.g., (P. Brown
et al., 1993), accept a bilingual sentence pair
as input, and output the links between words
across sentences of the pair. Such links are very
useful knowledge for machine translation.
Many of previous works implicitly assume
that bilingual resources (e.g., bilingual corpora,
bilingual dictionaries) on a large scale are avail-
able. Better performance of word alignment al-
gorithms can be achieved if the training is con-
ducted on larger parallel corpora. The increas-
ing requirement for bilingual resources thus be-
comes a bottleneck of constructing practical
</bodyText>
<footnote confidence="0.981103">
* This work was done while the author was visiting
Microsoft Research Asia.
</footnote>
<bodyText confidence="0.999830768292683">
word aligners for a general domain. To alleviate
the problem, some works, like (Resnik &amp; Smith,
2003; J. Nie et al., 1999), try to obtain parallel
corpora by automatically scrawling them from
the web.
Instead of heavily relying on bilingual cor-
pora, some works try to solve the bottleneck
problem in a different way: to mine bilingual
knowledge from monolingual corpora, which can
be more easily obtained in a large volume.
Koehn &amp; Knight (2000) present an approach
to estimating word translation probabilities by
using unrelated monolingual corpora based on
the EM algorithm. Promising results of their
method are exhibited in selecting the right
translation among several options provided by
a bilingual dictionary. However, their method
focuses on how to stochastize a bilingual dictio-
nary instead of how to extend it by adding new
translations. They discussed that better lan-
guage modeling, e.g., lexical dependencies, may
be useful for further improvement.
Fung &amp; Lee (1998) use an IR approach to in-
ducing new word translations from comparable
corpora. Their method is able to induce new
translations, thus to extend a bilingual dictio-
nary. As discussed in their paper, their method
suffers form low recall. They gave some sug-
gestions in achieving better results, including
introducing PoS tagging, improving word seg-
mentation accuracy. Using a better language
model will certainly do these.
Zhou et al. (2001) compute crosslingual
word similarities from two dependency triple
databases, each of which is obtained by parsing
a large monolingual corpus with a dependency
parser. Since bilexical dependency correspon-
dences are considered in the computation, the
resulting similarities are expected to be more
accurate. Better language modeling imposes
more constraints on the confidences/weights of
translation candidates. Encouraging results are
shown in the application of the similarities to
translation selection.
In this paper, we are interested in the ques-
tion how and to what extent the word align-
ment task can benefit from the work in mono-
lingual language processing, e.g., monolingual
parsers and monolingual corpora, so that the
requirement for large bilingual corpora could be
minimized. Specifically, we are concerned with
how to &amp;quot;convert&amp;quot; structured monolingual cor-
pora into bilingual word similarities (or trans-
lation probabilities) and how to use the simi-
larities to enhance the existing word alignment
models.
To do this, we first empirically justify the as-
sumption exploiting the structural correspon-
dence between different languages.&apos; Then
crosslingual word similarities are computed
from structured corpora using the method in
(Zhou et al., 2001) based on this assumption.
After that, the crosslingual word similarities are
normalized and incorporated into word align-
ment models. Experiments are conducted on
different types of alignment models ranging
from statistical-based ones and heuristic-based
ones. Experimental results show that word
alignment models can be consistently improved
with the crosslingual similarities estimated from
structured monolingual corpora.
The remainder of this paper is organized as
follows. Section 2 lists the related research. Sec-
tion 3 justifies the dependency correspondence
assumption. Section 4 presents the method
to estimate crosslingual word similarities from
structured monolingual corpora. Sections 5 and
6 describe how to incorporate estimated simi-
larities into word alignment methods. Experi-
ments are reported in Section 7. The last sec-
tion makes conclusions and points out future
works.
</bodyText>
<sectionHeader confidence="0.996062" genericHeader="introduction">
2 Related Research
</sectionHeader>
<bodyText confidence="0.998852326530613">
One of the related topics is word alignment.
Word alignment models can generally be classi-
fied into two categories (Och &amp; Ney, 2003): sta-
tistical alignment models and heuristic ones. A
statistical alignment model p(c, ale) describes
the relationship (e.g., word alignment a) be-
tween a source language string e and a tar-
get language string c. Different decompositions
of p(c, ale) result in different variants of word
alignment models. Heuristic based word align-
&apos;Although the language pair used in our paper is En-
glish and Chinese, the basic idea of this paper, however,
can be generalized to other language pairs.
ment approaches use similarity functions of two
languages. Readers might want to refer to
(Och &amp; Ney, 2003) for a comprehensive exami-
nation of word alignment models.
Another related research topic is the auto-
matic estimation of crosslingual word similar-
ities (or probabilities) from monolingual cor-
pora. For example, the works (Koehn &amp; Knight,
2000; Fung &amp; Lee, 1998; Zhou et al., 2001) that
we have mentioned in Section 1.
Methods to estimate crosslingual word sim-
ilarities are sometimes related to methods of
monolingual word clustering. For instance, the
method used in (Zhou et al., 2001) is motivated
by the work in (Lin, 1998). Lin (1998) presents
a method to cluster monolingual words based
on similarities between words in the same lan-
guage. The word similarities are estimated from
a parsed monolingual corpus.
The third related topic is the justification
of Direct Correspondence Assumption (DCA),
which underlies the models/applications ex-
ploiting high level linguistic structures. For ex-
ample, tree-tree alignment, e.g., (Matsumoto,
1993), in example-based machine translation;
synchronous grammars, e.g., (Wu, 1997), for
statistical machine translation (SMT). R. Hwa
et al. (2002) formulate the DCA, and evaluate it
in terms of the accuracies (precision and recall)
of the Chinese syntactic parses projected from
the corresponding English parse trees, which
are the output of an English parser. Their
Experiments are done on small newswire cor-
pora. They conclude that DCA is useful
with some principled transformation of syntac-
tic structures.
</bodyText>
<sectionHeader confidence="0.9973205" genericHeader="method">
3 Dependency Correspondence
Assumption
</sectionHeader>
<bodyText confidence="0.999621">
Methods, e.g., (Zhou et al., 2001), to estimate
crosslingual word similarities from structured
monolingual corpora also take advantage of this
assumption. The justification of this assump-
tion using a large scale and balanced data is
thus necessary.
The Dependency Correspondence Assump-
tion can be formally expressed as follows. Let
triple (wl, R, w2) be a syntactic dependency
consisting of two words wi and w2 and a depen-
dency relation R between them. Given a pair
of sentences E and C that are translation of
each other with syntactic structures TreeE and
Treec, if TreeE contains a dependency triple
(ei, R, e2), and el and e2 are aligned to words
</bodyText>
<table confidence="0.953197666666667">
Dependency Correct Incorrect Map Ratio
VO(E-to-C) 9,991 2,088 82.71%
VO(C-to-E) 8,823 1,697 83.87%
</table>
<tableCaption confidence="0.9312795">
Table 1: Verb-Object dependency correspon-
dence between English and Chinese.
</tableCaption>
<bodyText confidence="0.996474583333333">
Cl and c2 in C, respectively, then there is a de-
pendency triple (ci, R, c2) in Treec.
The experiment that we have done differen-
tiates from that in (R. Hwa et al., 2002) in the
following aspects.
First, we use a much larger and balanced
corpora consisting of 10, 000 English-Chinese
sentence pairs,2 coming from newswire, nov-
els, general bilingual dictionaries, and software
product manuals.
Second, instead of examining all the types
of dependency relations, we examine only the
Verb-Object (VO) dependency type since we
think that VO is one of the dependency types
that are often preserved across languages, and
DCA will thus mostly hold among these depen-
dency types.
Third, we evaluate the DCA in terms of map-
ping ratio: the ratio of the number of correct
crosslingual dependency mappings versus the
number of overall mappings. To do this, we
first manually add the word alignment informa-
tion to the 10,000 sentence pairs. Then, we run
the English parser MiniPar (Lin, 1993) and the
Chinese dependency parser BlockParser (Zhou,
2000) on both sides of the corpora, respectively.
Next, we extract all the mappings from En-
glish dependency triples to Chinese dependency
triples (and vice versa) based on the word align-
ment and parsing results.
Table 1 lists the mapping results. The first
column shows the dependency type and the
mapping directions. The correct mapping ratio
reaches 82.71% from English to Chinese, and
83.87% from Chinese to English. Note that we
treated the dependency type Verb-Object-Prep
as VO.
The intent of this experiment is not to com-
pare with the method and results in (R. Hwa
et al., 2002), but to evaluate the dependency
correspondence assumption for a certain type
of dependency concerned by us.
The numbers in Table 1 are very encourag-
ing because it indicates the feasibility of DCA.
This also suggests that translating the sentences
in the way of keeping DCA on some key depen-
dency types can normally get understandable
translation results.
</bodyText>
<sectionHeader confidence="0.979611" genericHeader="method">
4 Crosslingual Word Similarities
</sectionHeader>
<bodyText confidence="0.980138142857143">
We now briefly describe the method that we use
to estimate crosslingual words similarities from
structured monolingual corpora. The method
was proposed in (Zhou et al., 2001). We shall
use slightly different notations.
The information of a dependency triple T =
R,w2) is defined as:
</bodyText>
<equation confidence="0.979094">
c(w 1, R,w2)c(• , R, .)
I(T) = log2 ( (1)
R, •)c(• , R, w2)
</equation>
<bodyText confidence="0.999980466666667">
where c(...) is the counting function. T is called
a supportive dependency of wi (or w2) if I(T) &gt;
0. Let D(e) be the set of e&apos;s supportive de-
pendencies collected from a structured corpus
in language L1. Let D(c) be the set of c&apos;s
supportive dependencies collected from a struc-
tured corpus in language L2. Let S(r) be a
function returning 1 if dependency T E D(e)
has a corresponding dependency in D(c), and e
corresponds to c;3 and returning 0 otherwise.
Note that we need a bilingual dictionary to
&amp;quot;bridge&amp;quot; corresponding dependencies in differ-
ent languages, and this bilingual dictionary is
the only bilingual resource used. The common
information between c and e is then defined as:
</bodyText>
<equation confidence="0.9980215">
/4,6= s(T)/(7)+ E s(T)/(T) (2)
TED(e) TED(c)
</equation>
<bodyText confidence="0.919757">
The overall information between e and c is de-
fined as:
</bodyText>
<equation confidence="0.996279">
I 0(e, = I (r) E I(T) (3)
TED(e) TED(c)
</equation>
<bodyText confidence="0.937589333333333">
The similarity sim(c, e) between a pair of
crosslingual words c in language L1 and e in
language L2 is defined as the ratio of ic(c, e) to
</bodyText>
<equation confidence="0.81588075">
1-0(C, e):
sim(c, e) = (c,
10(c,
c (4)
</equation>
<bodyText confidence="0.667383">
Readers might want to refer to (Zhou et al.,
2001) for detailed derivations.
</bodyText>
<footnote confidence="0.728554333333333">
2Created by Microsoft Research Asia, and not pub- 30r in the other direction, returning 1 if 7 E D(C) has
a corresponding dependency in D(e).
licly available.
</footnote>
<bodyText confidence="0.999733">
It is worth mentioning that this method has
the ability of inducing new translations. In prin-
ciple, the similarity of any pair of crosslingual
words can be computed using Formula 4. Since
the information used in the computation is dis-
tributedly encoded in the relevant dependency
triples in the entire treebank, this method is also
very robust.
Although the value of function sim(c, e) com-
puted by their method ranges over [0,1], it, how-
ever, is not a probability distribution because
of the fact that Ec,esim(c,e) 1. We thus
use the following normalization so that sim(c, e)
can be incorporated into a statistical translation
model:
</bodyText>
<equation confidence="0.97103">
sim(c, e)
sim(c&apos;, e)
</equation>
<bodyText confidence="0.999626666666667">
In the following two sections, we are go-
ing to present methods to integrate g(cle) into
word alignment models for performance im-
provements. Word alignment models that will
be involved range from statistical-based ones to
heuristic-based ones.
</bodyText>
<sectionHeader confidence="0.8475145" genericHeader="method">
5 Improving Statistical-Based Word
Alignment Models
</sectionHeader>
<bodyText confidence="0.999525785714286">
IBM Model 2 is used to test the usefulness of
q(cle) to statistical word alignment algorithms.
The reason why we have not used more complex
models is that our objective is not the compar-
ison of different word alignment models. It is
reasonable to conclude that, if Model 2 can be
improved by integrating g(cle), more complex
models can be improved, too.4 For brevity, we
shall call the statistical word alignment model
STATS hereafter.
We use a simple interpolated model to com-
bine g(cle) with the the word-to-word transla-
tion probabilities p(cle) estimated from bilin-
gual corpora:
</bodyText>
<equation confidence="0.979536">
Pstats-tnterp(cle) = A g(cle) ± (1 — A) p(cle) (6)
</equation>
<bodyText confidence="0.753194941176471">
where 0 &lt; A &lt; 1. We have made A a constant
for all (c, e) pairs to get around the data sparse-
ness problem in estimation.
40f course, word alignment models (or translation
models) can always be improved by exploiting more in-
formation, e.g., the structural information; but this does
not conflict with out objective because a more complex
model is usually composed of more parameters, and thus
requires larger size of bilingual corpora for training. How
to extract bilingual knowledge from monolingual corpora
and how to combine them into word alignment models
is exactly our objective.
Like the estimation of interpolated monolin-
gual language models, the optimal interpolation
coefficient A can be estimated via the EM algo-
rithm from held-out bilingual corpora X such
that
</bodyText>
<equation confidence="0.99187">
A* = arg 17,XPstats—tnterp(X) (7)
</equation>
<bodyText confidence="0.9999035">
We shall refer to the interpolated statistical
model as STATS-interp hereafter.
</bodyText>
<sectionHeader confidence="0.8432515" genericHeader="method">
6 Improving Heuristic-Based Word
Alignment Models
</sectionHeader>
<bodyText confidence="0.9999535">
We consider the usefulness of q(elc) to the
heuristic-based word alignment models. We
use two types of heuristic-based word align-
ment methods: the dictionary-based method
and the class-based method. They exploit dif-
ferent types of bilingual knowledge.
</bodyText>
<subsectionHeader confidence="0.681418">
6.1 Dictionary-Based Models
</subsectionHeader>
<bodyText confidence="0.99997">
A way to examine the degree to which the cover-
age of a translation dictionary can be improved
by g(cle) is to show the word alignment accu-
racy (e.g., precision and recall). The baseline for
comparison is the dictionary-based word align-
ment method (DICT) in (Ker &amp; Zhang, 1997),
which is briefly described in Figure 1. The in-
put of DICT is a pair (S, T) of sentences and a
bilingual dictionary. The following method can
be used to incorporate g(cle) into DICT.
</bodyText>
<equation confidence="0.9999325">
Pdict—interp(cle) —
A(c,e)q(elc) + [1 — A(c,e)]sign((c, e) E BD) (8)
</equation>
<bodyText confidence="0.999950666666667">
where BD stands for the bilingual dictionary.
A(c,e) is chosen such that if g(cle) is larger than
a threshold, it is a non-zero value (e.g., empir-
ically chosen as 0.3), and 0 otherwise. When
A(c,e) is 0, the bilingual dictionary will take full
control sign functor returns 1 if (c, e) is in
the bilingual dictionary, and 0, otherwise. We
shall refer to the interpolated DICT as DICT-
interp.
</bodyText>
<subsectionHeader confidence="0.909894">
6.2 Class-Based Models
</subsectionHeader>
<bodyText confidence="0.999577875">
Class-based word alignment method
(Ker &amp; Zhang, 1997) attempts to broaden
the word alignment coverage/recall using
crosslingual concept similarities. Concepts are
classes defined in a monolingual thesaurus.
Crosslingual concept similarities are estimated
from bilingual corpora by generalizing the
words into their classes.
</bodyText>
<equation confidence="0.8725625">
q(cle) =
(5)
</equation>
<listItem confidence="0.983508636363636">
1. Enumerate all words Ws in S and words
WT in T.
2. Foreach s in W8, find the set of translations
DT, of s based on a bilingual dictionary.
3. For d E DT, and t E WT, calculate the
dictionary
based similarity (DTSim(s, t))
4. Foreach word s, if DTSim(s, t) is max-
imized over t E WT, produce a connection
(s,t).
5. Compile the list ALN of word alignments.
</listItem>
<figureCaption confidence="0.9948395">
Figure 1: Dictionary-based word alignment
method (DICT) (Ker &amp; Zhang, 1997)
</figureCaption>
<bodyText confidence="0.991682705882353">
The basic idea of the class-based word align-
ment algorithm is as follows. Taking a bilingual
sentence pair as input, the algorithm first con-
ducts DICT (see Figure 1), resulting in a list
ALN of word alignments. Words that are not in
ALN are are aligned using the following model:
a* = arg maxaconceptsim(c3le,)d(i, j) (9)
where conceptsim(c3le,) is the concept similar-
ity between word c3 in one language and e,
in the other language. d(i, j) is the distortion
model. We shall use CLASS to refer to the
class-based method.
As with the integration of q(cle) into statisti-
cal word alignment models in Section 5, q(cle)
can play a role in the improvement on class-
based word alignment models by interpolating
q(cle) with conceptsim(cle) as follows.
</bodyText>
<equation confidence="0.9992505">
Pclass—interp(Cle)
=Aconceptsim(cle)+(1 — ).)q(cle) (10)
</equation>
<bodyText confidence="0.9999614">
where 0 &lt; A &lt; 1. The optimal interpolated co-
efficient A* can be computed via the EM algo-
rithm with held-out bilingual data. The class-
based model interpolating with q(cle) will be
referred to as CLASS-interp.
</bodyText>
<sectionHeader confidence="0.999319" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.9999196">
The experiments include the estimation of
crosslingual word similarities from structured
monolingual corpora (Section 4), and the ap-
plication of the estimated similarities to word
alignment algorithms.
</bodyText>
<subsectionHeader confidence="0.995088">
7.1 Estimation of Crosslingual Word
Similarities
</subsectionHeader>
<bodyText confidence="0.999761642857143">
To obtain two sets of dependency triples, each
set per language, the English dependency parser
MiniPar (Lin, 1993) is applied to 750M bytes of
English corpora of Wall Street Journal (1980-
1990), resulting in 1.9 x107 English dependency
triples. The Chinese dependency parser Block-
Parser (Zhou, 2000) is applied to 1,200M bytes
of Chinese corpora of People&apos;s Daily (1980-
1998), resulting in 3.3 x107 Chinese dependency
triples.
The HIT English-Chinese bilingual dictio-
nary5 consisting of 66,248 Chinese words, 73,693
English words, and 164,794 translation links is
used.
</bodyText>
<subsectionHeader confidence="0.9899385">
7.2 Improving Word Alignment Models
Evaluation metrics
</subsectionHeader>
<bodyText confidence="0.999970333333333">
Let A denote the set of word alignments from
a word alignment method for a pair of bilin-
gual sentences, and let R denote the set of word
alignments of the same sentence pair in the ref-
erence corpora, let I • I denote the size of set •,
then we use
</bodyText>
<equation confidence="0.999339">
Precision = IA n RI
1AI
IA n RI
Recall IRI
2 x Precision x Recall
(13)
F—measure = Precision + Recall
</equation>
<bodyText confidence="0.999799">
where n is a functor of two sets of word align-
ments, returning the number of matched align-
ments.
</bodyText>
<subsectionHeader confidence="0.964204">
Experimental settings
</subsectionHeader>
<bodyText confidence="0.999443052631579">
215,347 pairs of English and Chinese sentences
are used to train the statistical word alignment
algorithm. Since the interpolation coefficients
are manually assigned in our experiments, no
held-out data were reserved.
The HIT English-Chinese bilingual dictionary
is used in the dictionary-based word alignment
methods DICT and DICT-interp.
The same 215,347 bilingual sentences as used
in the training of statistical-based models are
used to train the crosslingual concept similar-
ities conceptsim(elc) and the distortion model
d(i, j). Monolingual thesauri are WordNet (G.
Miller, 1990) with 45,784 classes for English and
Xian Dai Han Yu Tong Yi Ci Dian (Mei, 2002)
with 3,724 classes (40,289 words) for Chinese.
A single test set is used for all the word align-
ment methods. It consists of 1,000 sentence
pairs that are disjoint with the training data.
</bodyText>
<footnote confidence="0.933493">
5Created by Harbin Institute of Technology (HIT).
</footnote>
<figure confidence="0.784956">
(12)
E:
C:
Align:
</figure>
<tableCaption confidence="0.789696">
Table 2: Word alignment methods STATS vs.
STATS-interp
</tableCaption>
<table confidence="0.997302466666667">
20000/1 is/2 a/3 very/4 re-
spectable/5 salary/6 ./7
20000/1 YinglBang4/2 De0/3
Xin1Jin1/4 Shi4/5 FeilChang2/6
Ke3Guan1/7 De0/8 ./9
[1:1] [4:6] [5:7] [6:4] [7:9]
Algorithms
Prec. (%) Rec. (%) F
STATS
STATS-interp
73.69 67.31 70.36
74.54 68.63 71.46
Algorithms Prec. (%) Rec. (%) F
DICT 95.87 44.60 60.88
DICT-interp 95.28 46.84 62.80
</table>
<tableCaption confidence="0.9787945">
Table 3: Word alignment methods DICT vs.
DICT-interp
</tableCaption>
<subsectionHeader confidence="0.931663">
Experimental results
</subsectionHeader>
<bodyText confidence="0.984358972972973">
The performance comparison between STATS
and STATS-interp are shown in Table 2. All
metrics including the precision, recall and thus
F-measure have been improved. The inter-
polation of crosslingual word similarities esti-
mated from monolingual corpora into the sta-
tistical word alignment method STATS makes
the word-to-word translation probability more
reliable than otherwise. Although the training
bilingual corpus is relatively large, there still ex-
ists the data sparseness problem. Probabilities
of rare word translations are often inaccurate.
These probabilities are &amp;quot;adjusted&amp;quot; by being in-
terpolated with crosslingual word similarities
from a completely different knowledge source
monolingual copora. The (optimized) interpo-
lated coefficient A decides how the knowledge
from difference sources (monolingual corpora or
bilingual corpora) are weighted.
The performance comparison between DICT
and DICT-interp are shown in Table 3. We
see that the recall and F-measure metrics of
DICT have been improved by DICT-interp. It
is worth mentioning that the recall is even im-
proved by more than 2% with only a slight
drop of precsion. This implies that the crosslin-
gual word similarities provide more chances for
words to be aligned when the bilingual dictio-
nary fails. Table 4 shows an example. The
words and alignments in bold fonts are those
where the crosslingual word similarities help,
and the bilingual dictionary fails. Chinese are
written in pinyin.
Table 5 lists the top 8 Chinese words (in
pinyin) mined from the monolingual corpora
that are the possible translations of the English
word &amp;quot;salary&amp;quot;. The rightmost column shows the
</bodyText>
<tableCaption confidence="0.9954065">
Table 4: An example of word alignment output
from DICT-interp.
</tableCaption>
<table confidence="0.999961875">
Gongl Zil [salary] 0.178
Nian2 [year] 0.150
XinlJinl [salary] 0.150
Xin1Shui3 [salary] 0.112
Yue4 [month] 0.060
ShoulYi4 [benefit] 0.059
JinlNian2 [this year] 0.044
JialXinl [raise] 0.004
</table>
<tableCaption confidence="0.999283">
Table 5: Crosslingual word similarities.
</tableCaption>
<bodyText confidence="0.9998455">
q(cle). Words in brackets are the real transla-
tions of the corresponding pinyin (in the same
rows).
The performance comparisons between
CLASS and CLASS-interp are shown in Ta-
ble 6. There are two reasons why CLASS is
improved: First, although the generalization
of words into their classes alleviates the data
sparseness problem in CLASS, it gives rise to
the overgeneralization problem.6 The com-
bination with crosslingual word similarities
makes the crosslingual concept similarities
more informative.
Second, the thesauri used are not large
enough for a general domain. For example,
the Chinese thesaurus provides classes only for
40,289 Chinese words. These words even cannot
cover all the Chinese words in the bilingual dic-
tionary we used. Furthermore, the definition of
Chinese words is not consistent, e.g, between
the Chinese thesaurus and the Chinese word
segmentor. In the case where thesauri fails, our
crosslingual word similarities takes control, rais-
ing the recall.
</bodyText>
<subsectionHeader confidence="0.951199">
7.3 Discussions
</subsectionHeader>
<bodyText confidence="0.999672">
One of essential reasons why the performance of
different types (statistical-based and heuristic-
based) of alignment algorithms can be improved
</bodyText>
<footnote confidence="0.932013666666667">
6Like in monolingual language modeling, too small
number of classes will not guarantee the reduction of
perplexity.
</footnote>
<table confidence="0.992235666666667">
Algorithms Prec. (%) Rec. (%) F
CLASS 86.84 54.96 67.32
CLASS-interp 86.03 56.53 68.23
</table>
<tableCaption confidence="0.8703945">
Table 6: Word alignment methods CLASS vs.
CLASS-interp
</tableCaption>
<bodyText confidence="0.999983333333333">
is that the crosslingual word similarities esti-
mated from monolingual corpora are able to
broaden the coverage of, or adjust the knowl-
edge learned from bilingual corpora. The cov-
erage is broadened because we can induce from
structured monolingual corpora reliable new
translations that do not co-occur in the same
sentence pair in bilingual corpora. The knowl-
edge in bilingual corpora is adjusted in a way
that probabilities of translations with low fre-
quencies can be smoothed by simply being in-
terpolated with the normalized word similarities
estimated from structured monolingual corpora.
It is worth emphasizing that the improve-
ments that we have achieved are valuable for
the word alignment task in a general domain.
With the large training corpus, and a large bilin-
gual dictionary, the word alignment methods we
have used in experiments have set a high base-
line for comparisons. In a general domain, a
slight improvement on the recall metric of the
word alignment result usually requires a large
increment of the size of training bilingual cor-
pora. By utilizing the crosslingual word similar-
ities that are estimated from monolingual cor-
pora, we have got around the problem of how
to obtain bilingual corpora on a large scale.
The limitation of our experiments is that we
used two independent monolingual corpora for
the estimation of crosslingual word similarities.
The corpora are independent in the following
ways: first, they are collected independently;
second, they are collected during different dates;
third, their major topics are different, e.g., po-
litical versus business. Better results could be
expected if we use comparable corpora, because,
in comparable copora, crosslingual phrases with
similar structures will provide more information
for the estimation of crosslingual word similari-
ties.
Our approach to estimating crosslingual
word similarities assumes that two monolingual
parsers are available. Although the construc-
tion of monolingual parsers is expensive, there
are free parsers available on the web.
</bodyText>
<sectionHeader confidence="0.998381" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999990616666667">
We have been concerned with how to &amp;quot;convert&amp;quot;
structured monolingual corpora into bilingual
word similarities (or translation probabilities)
and how to incorporate them into word align-
ment models for performance improvement.
Our aim is to take advantage of monolingual
resources, e.g., corpora, parsers, treebank, for
bilingual tasks, e.g., word alignment, so that
the requirement for large amounts of training
bilingual corpora could be alleviated.
To do this, we first empirically justified
the dependency correspondence assumption be-
tween different languages using large and bal-
anced corpora. Then, we computed crosslingual
word similarities using the method in (Zhou
et al., 2001) based on this assumption. After
that, we normalized the crosslingual word sim-
ilarities and integrated them into word align-
ment algorithms using interpolated models.
Experiments are conducted on word align-
ment algorithms ranging from statistical-based
ones and heuristic-based ones. Experimental
results show that word alignment models have
been consistently improved, in particular the re-
call metric.
The main contribution of this paper is that
we have presented an approach to combining
the knowledge mined from structured monolin-
gual corpora with the knowledge mined from
bilingual corpora, and showed the usefulness
of the approach to the improvement on word
alignment performance, and thus showed the
usefulness of monolingual resources to bilingual
tasks. Moreover, we also justified the depen-
dency correspondence assumption based on a
large and balanced corpora. This assumption
underlies the method that we have used to esti-
mated crosslingual word similarities from struc-
tured monolingual corpora.
One of the interesting topics deserving study
in the future could be to divide the crosslingual
word pairs into clusters and estimate the opti-
mal interpolation coefficient A for each of them
using the minimum error rate (MER) as the op-
timization goal, instead of maximum likelihood.
Results in machine translation (Och, 2003) and
speech recognition have shown the advantage of
discriminative training.
Another topic will be to compute the (nor-
malized) word similarities q(elc) in an boot-
strapping manner, for instance, using the EM
algorithm. In each iteration, the (normalized)
word similarities output from the previous it-
eration are used as weights of two correspond-
ing dependency triples in two languages. These
weights are involved in the computation of the
common information (Section 4) between two
crosslingual words. We also desire to improve
statistical models of the state of art with these
re-estimated similarities.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997855">
We would like to thank the reviewers for their
valuable comments.
</bodyText>
<sectionHeader confidence="0.999231" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999670147540984">
Brown, P.F., Della Pietra, S.A., Della Pietra
V.J., and Mercer, R.L. (1993) &amp;quot;The math-
ematics of Statistical Machine Translation:
Parameter Estimation,&amp;quot; Computational Lin-
guistics, 19(2), pp.263-311.
P. Fung and Y. Lee (1998) &amp;quot;Translating Un-
known Words Using Nonparallel, Comparable
Texts,&amp;quot; In Proceedings of COLING-ACL98,
Montreal, Canada: Aug. 1998, 414-420.
R. Hwa, P. Resnik, A. Weinberg, and 0. Kolak
(2002) &amp;quot;Evaluating Translational Correspon-
dence using Annotation Projection&amp;quot; In pro-
ceedings of ACL-O, Philadelphia, July, 2002.
S. Ker and J. Zhang (1997) &amp;quot;A class-based ap-
proach to word alignment,&amp;quot; Computational
Linguistics, Vol. 23, No. 2, pp 313-343.
P. Koehn and K. Knight (2000) &amp;quot;Estimating
Word Translation Probabilities from Unre-
lated Monolingual Corpora Using the EM Al-
gorithm,&amp;quot; AAAI 2000.
D. Lin (1993) Principle-Based Parsing without
Oyergeneration. Proceedings of ACL-93, pp
112-120, Columbus, Ohio.
D. Lin (1998) &amp;quot;Automatic retrieval and cluster-
ing of similar words,&amp;quot; COLING-ACL98, Men-
treal, Canada.
Y. Matsumoto (1993) &amp;quot;Structural Matching of
Parallel Texts,&amp;quot; In Proceedings of ACL 93.
J. Mei (2002) &amp;quot;Xiandai Hanyu TongYi CiDian,&amp;quot;
The Commercial Press LTD. of China.
G. Miller (1990). &amp;quot;WordNet: An on-line lexical
database,&amp;quot; International Journal of Lexicog-
raphy, 3(4):235-312, 1990.
J. Nie, M. Simard, P. Isabelle, R. Du-
rand (1999) &amp;quot;Cross-Language Information
Retrieval based on Parallel Texts and Auto-
matic Mining of Parallel Texts in the Web&amp;quot;,
22nd ACM-SIGIR, pp. 74-81.
F. Och and H. Ney (2003) &amp;quot;A Systematic
Comparison of Various Statistical Alignment
Models,&amp;quot; In Computational Linguistics.
F. Och (2003) &amp;quot;Minimum Error Rate Training
in Statistical Machine Translation&amp;quot;, in ACL
2003.
P. Resnik and N. Smith (2003) &amp;quot;The Web as a
Parallel Corpus,&amp;quot; Computational Linguistics,
Vol. 29, Issue 3, pp 349-380.
D. Wu (1997) &amp;quot;Stochastic inversion trans-
duction grammars and bilingual parsing of
parallel corpora,&amp;quot; Computational Linguistics
23(3):377-404, September.
M. Zhou (2000) &amp;quot;A Block-Based Robust Depen-
dency Parser for Unrestricted Chinese Text,&amp;quot;
2nd Workshop on Chinese language process-
ing, Hong Kong.
M. Zhou, Y. Ding, and C. Huang (2001) &amp;quot;Im-
proving Translation Selection with a New
Translation Model Trained by Independent
Monolingual Corpora,&amp;quot; Computational Lin-
guistics and Chinese Language Processing,
Vol.6, No.1, Feb. 2001, pp. 1-26.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.439452">
<title confidence="0.9980425">Improving Word Alignment Using Structured Monolingual Corpora</title>
<author confidence="0.997073">Wei Wang</author>
<address confidence="0.892838">New York 719 Broadway Room New York, 10003-6806</address>
<email confidence="0.999312">weAcs.nyu.edu</email>
<author confidence="0.855615">Ming</author>
<affiliation confidence="0.999679">Microsoft Research</affiliation>
<address confidence="0.975740666666667">5/F, Beijing Sigma No.49, Zhichun Road, Haidian Beijing 100080</address>
<email confidence="0.978783">mingzhou@microsoft.com</email>
<abstract confidence="0.99632184">We propose a new method to improve the perof word algorithms, in particular the recall, using structured monolingual corpora as sources to estimate cross language word similarities. Normally, cross language word similarities, i.e., the similarity between a source language word and a target language word, can be estimated with a bilingual corpus of enough size. We use a method to estimate them from two structured monolingual corpora based on the dependency correspondence assumption justified on large and balanced bilingual corpora. We selected three typical word alignment models ranging over statistical-based ones and heuristic-based ones, to test whether cross language similarities can improve the performance of word alignment models. The crosslingual word similarities are simply interpolated into these models. The experiments show that crosslingual word similarities estimated from structured monolingual corpora can effectively improve the performance of word alignment models, in particular the recall.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>Della Pietra</author>
<author>Della Pietra V J S A</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of Statistical Machine Translation: Parameter Estimation,&amp;quot;</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<contexts>
<context position="1424" citStr="Brown et al., 1993" startWordPosition="209" endWordPosition="212">ence assumption justified on large and balanced bilingual corpora. We selected three typical word alignment models ranging over statistical-based ones and heuristic-based ones, to test whether cross language similarities can improve the performance of word alignment models. The crosslingual word similarities are simply interpolated into these models. The experiments show that crosslingual word similarities estimated from structured monolingual corpora can effectively improve the performance of word alignment models, in particular the recall. 1 Introduction Word alignment algorithms, e.g., (P. Brown et al., 1993), accept a bilingual sentence pair as input, and output the links between words across sentences of the pair. Such links are very useful knowledge for machine translation. Many of previous works implicitly assume that bilingual resources (e.g., bilingual corpora, bilingual dictionaries) on a large scale are available. Better performance of word alignment algorithms can be achieved if the training is conducted on larger parallel corpora. The increasing requirement for bilingual resources thus becomes a bottleneck of constructing practical * This work was done while the author was visiting Micro</context>
</contexts>
<marker>Brown, Pietra, A, Mercer, 1993</marker>
<rawString>Brown, P.F., Della Pietra, S.A., Della Pietra V.J., and Mercer, R.L. (1993) &amp;quot;The mathematics of Statistical Machine Translation: Parameter Estimation,&amp;quot; Computational Linguistics, 19(2), pp.263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
<author>Y Lee</author>
</authors>
<title>Translating Unknown Words Using Nonparallel, Comparable Texts,&amp;quot;</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL98,</booktitle>
<pages>414--420</pages>
<location>Montreal, Canada:</location>
<contexts>
<context position="3023" citStr="Fung &amp; Lee (1998)" startWordPosition="460" endWordPosition="463">corpora, which can be more easily obtained in a large volume. Koehn &amp; Knight (2000) present an approach to estimating word translation probabilities by using unrelated monolingual corpora based on the EM algorithm. Promising results of their method are exhibited in selecting the right translation among several options provided by a bilingual dictionary. However, their method focuses on how to stochastize a bilingual dictionary instead of how to extend it by adding new translations. They discussed that better language modeling, e.g., lexical dependencies, may be useful for further improvement. Fung &amp; Lee (1998) use an IR approach to inducing new word translations from comparable corpora. Their method is able to induce new translations, thus to extend a bilingual dictionary. As discussed in their paper, their method suffers form low recall. They gave some suggestions in achieving better results, including introducing PoS tagging, improving word segmentation accuracy. Using a better language model will certainly do these. Zhou et al. (2001) compute crosslingual word similarities from two dependency triple databases, each of which is obtained by parsing a large monolingual corpus with a dependency pars</context>
<context position="6599" citStr="Fung &amp; Lee, 1998" startWordPosition="1002" endWordPosition="1005">ns of p(c, ale) result in different variants of word alignment models. Heuristic based word align&apos;Although the language pair used in our paper is English and Chinese, the basic idea of this paper, however, can be generalized to other language pairs. ment approaches use similarity functions of two languages. Readers might want to refer to (Och &amp; Ney, 2003) for a comprehensive examination of word alignment models. Another related research topic is the automatic estimation of crosslingual word similarities (or probabilities) from monolingual corpora. For example, the works (Koehn &amp; Knight, 2000; Fung &amp; Lee, 1998; Zhou et al., 2001) that we have mentioned in Section 1. Methods to estimate crosslingual word similarities are sometimes related to methods of monolingual word clustering. For instance, the method used in (Zhou et al., 2001) is motivated by the work in (Lin, 1998). Lin (1998) presents a method to cluster monolingual words based on similarities between words in the same language. The word similarities are estimated from a parsed monolingual corpus. The third related topic is the justification of Direct Correspondence Assumption (DCA), which underlies the models/applications exploiting high le</context>
</contexts>
<marker>Fung, Lee, 1998</marker>
<rawString>P. Fung and Y. Lee (1998) &amp;quot;Translating Unknown Words Using Nonparallel, Comparable Texts,&amp;quot; In Proceedings of COLING-ACL98, Montreal, Canada: Aug. 1998, 414-420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hwa</author>
<author>P Resnik</author>
<author>A Weinberg</author>
</authors>
<title>Evaluating Translational Correspondence using Annotation Projection&amp;quot;</title>
<date>2002</date>
<booktitle>In proceedings of ACL-O,</booktitle>
<location>Philadelphia,</location>
<contexts>
<context position="7426" citStr="Hwa et al. (2002)" startWordPosition="1125" endWordPosition="1128">ou et al., 2001) is motivated by the work in (Lin, 1998). Lin (1998) presents a method to cluster monolingual words based on similarities between words in the same language. The word similarities are estimated from a parsed monolingual corpus. The third related topic is the justification of Direct Correspondence Assumption (DCA), which underlies the models/applications exploiting high level linguistic structures. For example, tree-tree alignment, e.g., (Matsumoto, 1993), in example-based machine translation; synchronous grammars, e.g., (Wu, 1997), for statistical machine translation (SMT). R. Hwa et al. (2002) formulate the DCA, and evaluate it in terms of the accuracies (precision and recall) of the Chinese syntactic parses projected from the corresponding English parse trees, which are the output of an English parser. Their Experiments are done on small newswire corpora. They conclude that DCA is useful with some principled transformation of syntactic structures. 3 Dependency Correspondence Assumption Methods, e.g., (Zhou et al., 2001), to estimate crosslingual word similarities from structured monolingual corpora also take advantage of this assumption. The justification of this assumption using </context>
<context position="8822" citStr="Hwa et al., 2002" startWordPosition="1352" endWordPosition="1355">consisting of two words wi and w2 and a dependency relation R between them. Given a pair of sentences E and C that are translation of each other with syntactic structures TreeE and Treec, if TreeE contains a dependency triple (ei, R, e2), and el and e2 are aligned to words Dependency Correct Incorrect Map Ratio VO(E-to-C) 9,991 2,088 82.71% VO(C-to-E) 8,823 1,697 83.87% Table 1: Verb-Object dependency correspondence between English and Chinese. Cl and c2 in C, respectively, then there is a dependency triple (ci, R, c2) in Treec. The experiment that we have done differentiates from that in (R. Hwa et al., 2002) in the following aspects. First, we use a much larger and balanced corpora consisting of 10, 000 English-Chinese sentence pairs,2 coming from newswire, novels, general bilingual dictionaries, and software product manuals. Second, instead of examining all the types of dependency relations, we examine only the Verb-Object (VO) dependency type since we think that VO is one of the dependency types that are often preserved across languages, and DCA will thus mostly hold among these dependency types. Third, we evaluate the DCA in terms of mapping ratio: the ratio of the number of correct crosslingu</context>
<context position="10269" citStr="Hwa et al., 2002" startWordPosition="1589" endWordPosition="1592">dency parser BlockParser (Zhou, 2000) on both sides of the corpora, respectively. Next, we extract all the mappings from English dependency triples to Chinese dependency triples (and vice versa) based on the word alignment and parsing results. Table 1 lists the mapping results. The first column shows the dependency type and the mapping directions. The correct mapping ratio reaches 82.71% from English to Chinese, and 83.87% from Chinese to English. Note that we treated the dependency type Verb-Object-Prep as VO. The intent of this experiment is not to compare with the method and results in (R. Hwa et al., 2002), but to evaluate the dependency correspondence assumption for a certain type of dependency concerned by us. The numbers in Table 1 are very encouraging because it indicates the feasibility of DCA. This also suggests that translating the sentences in the way of keeping DCA on some key dependency types can normally get understandable translation results. 4 Crosslingual Word Similarities We now briefly describe the method that we use to estimate crosslingual words similarities from structured monolingual corpora. The method was proposed in (Zhou et al., 2001). We shall use slightly different not</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, 2002</marker>
<rawString>R. Hwa, P. Resnik, A. Weinberg, and 0. Kolak (2002) &amp;quot;Evaluating Translational Correspondence using Annotation Projection&amp;quot; In proceedings of ACL-O, Philadelphia, July, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ker</author>
<author>J Zhang</author>
</authors>
<title>A class-based approach to word alignment,&amp;quot;</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<pages>313--343</pages>
<contexts>
<context position="15316" citStr="Ker &amp; Zhang, 1997" startWordPosition="2432" endWordPosition="2435"> hereafter. 6 Improving Heuristic-Based Word Alignment Models We consider the usefulness of q(elc) to the heuristic-based word alignment models. We use two types of heuristic-based word alignment methods: the dictionary-based method and the class-based method. They exploit different types of bilingual knowledge. 6.1 Dictionary-Based Models A way to examine the degree to which the coverage of a translation dictionary can be improved by g(cle) is to show the word alignment accuracy (e.g., precision and recall). The baseline for comparison is the dictionary-based word alignment method (DICT) in (Ker &amp; Zhang, 1997), which is briefly described in Figure 1. The input of DICT is a pair (S, T) of sentences and a bilingual dictionary. The following method can be used to incorporate g(cle) into DICT. Pdict—interp(cle) — A(c,e)q(elc) + [1 — A(c,e)]sign((c, e) E BD) (8) where BD stands for the bilingual dictionary. A(c,e) is chosen such that if g(cle) is larger than a threshold, it is a non-zero value (e.g., empirically chosen as 0.3), and 0 otherwise. When A(c,e) is 0, the bilingual dictionary will take full control sign functor returns 1 if (c, e) is in the bilingual dictionary, and 0, otherwise. We shall ref</context>
<context position="16750" citStr="Ker &amp; Zhang, 1997" startWordPosition="2677" endWordPosition="2680">Concepts are classes defined in a monolingual thesaurus. Crosslingual concept similarities are estimated from bilingual corpora by generalizing the words into their classes. q(cle) = (5) 1. Enumerate all words Ws in S and words WT in T. 2. Foreach s in W8, find the set of translations DT, of s based on a bilingual dictionary. 3. For d E DT, and t E WT, calculate the dictionary based similarity (DTSim(s, t)) 4. Foreach word s, if DTSim(s, t) is maximized over t E WT, produce a connection (s,t). 5. Compile the list ALN of word alignments. Figure 1: Dictionary-based word alignment method (DICT) (Ker &amp; Zhang, 1997) The basic idea of the class-based word alignment algorithm is as follows. Taking a bilingual sentence pair as input, the algorithm first conducts DICT (see Figure 1), resulting in a list ALN of word alignments. Words that are not in ALN are are aligned using the following model: a* = arg maxaconceptsim(c3le,)d(i, j) (9) where conceptsim(c3le,) is the concept similarity between word c3 in one language and e, in the other language. d(i, j) is the distortion model. We shall use CLASS to refer to the class-based method. As with the integration of q(cle) into statistical word alignment models in S</context>
</contexts>
<marker>Ker, Zhang, 1997</marker>
<rawString>S. Ker and J. Zhang (1997) &amp;quot;A class-based approach to word alignment,&amp;quot; Computational Linguistics, Vol. 23, No. 2, pp 313-343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>K Knight</author>
</authors>
<title>Estimating Word Translation Probabilities from Unrelated Monolingual Corpora Using the EM Algorithm,&amp;quot; AAAI</title>
<date>2000</date>
<contexts>
<context position="2489" citStr="Koehn &amp; Knight (2000)" startWordPosition="380" endWordPosition="383"> increasing requirement for bilingual resources thus becomes a bottleneck of constructing practical * This work was done while the author was visiting Microsoft Research Asia. word aligners for a general domain. To alleviate the problem, some works, like (Resnik &amp; Smith, 2003; J. Nie et al., 1999), try to obtain parallel corpora by automatically scrawling them from the web. Instead of heavily relying on bilingual corpora, some works try to solve the bottleneck problem in a different way: to mine bilingual knowledge from monolingual corpora, which can be more easily obtained in a large volume. Koehn &amp; Knight (2000) present an approach to estimating word translation probabilities by using unrelated monolingual corpora based on the EM algorithm. Promising results of their method are exhibited in selecting the right translation among several options provided by a bilingual dictionary. However, their method focuses on how to stochastize a bilingual dictionary instead of how to extend it by adding new translations. They discussed that better language modeling, e.g., lexical dependencies, may be useful for further improvement. Fung &amp; Lee (1998) use an IR approach to inducing new word translations from compara</context>
<context position="6581" citStr="Koehn &amp; Knight, 2000" startWordPosition="998" endWordPosition="1001">Different decompositions of p(c, ale) result in different variants of word alignment models. Heuristic based word align&apos;Although the language pair used in our paper is English and Chinese, the basic idea of this paper, however, can be generalized to other language pairs. ment approaches use similarity functions of two languages. Readers might want to refer to (Och &amp; Ney, 2003) for a comprehensive examination of word alignment models. Another related research topic is the automatic estimation of crosslingual word similarities (or probabilities) from monolingual corpora. For example, the works (Koehn &amp; Knight, 2000; Fung &amp; Lee, 1998; Zhou et al., 2001) that we have mentioned in Section 1. Methods to estimate crosslingual word similarities are sometimes related to methods of monolingual word clustering. For instance, the method used in (Zhou et al., 2001) is motivated by the work in (Lin, 1998). Lin (1998) presents a method to cluster monolingual words based on similarities between words in the same language. The word similarities are estimated from a parsed monolingual corpus. The third related topic is the justification of Direct Correspondence Assumption (DCA), which underlies the models/applications </context>
</contexts>
<marker>Koehn, Knight, 2000</marker>
<rawString>P. Koehn and K. Knight (2000) &amp;quot;Estimating Word Translation Probabilities from Unrelated Monolingual Corpora Using the EM Algorithm,&amp;quot; AAAI 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Principle-Based Parsing without Oyergeneration.</title>
<date>1993</date>
<booktitle>Proceedings of ACL-93,</booktitle>
<pages>112--120</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="9630" citStr="Lin, 1993" startWordPosition="1485" endWordPosition="1486">oftware product manuals. Second, instead of examining all the types of dependency relations, we examine only the Verb-Object (VO) dependency type since we think that VO is one of the dependency types that are often preserved across languages, and DCA will thus mostly hold among these dependency types. Third, we evaluate the DCA in terms of mapping ratio: the ratio of the number of correct crosslingual dependency mappings versus the number of overall mappings. To do this, we first manually add the word alignment information to the 10,000 sentence pairs. Then, we run the English parser MiniPar (Lin, 1993) and the Chinese dependency parser BlockParser (Zhou, 2000) on both sides of the corpora, respectively. Next, we extract all the mappings from English dependency triples to Chinese dependency triples (and vice versa) based on the word alignment and parsing results. Table 1 lists the mapping results. The first column shows the dependency type and the mapping directions. The correct mapping ratio reaches 82.71% from English to Chinese, and 83.87% from Chinese to English. Note that we treated the dependency type Verb-Object-Prep as VO. The intent of this experiment is not to compare with the meth</context>
<context position="18139" citStr="Lin, 1993" startWordPosition="2901" endWordPosition="2902">e)+(1 — ).)q(cle) (10) where 0 &lt; A &lt; 1. The optimal interpolated coefficient A* can be computed via the EM algorithm with held-out bilingual data. The classbased model interpolating with q(cle) will be referred to as CLASS-interp. 7 Experiments The experiments include the estimation of crosslingual word similarities from structured monolingual corpora (Section 4), and the application of the estimated similarities to word alignment algorithms. 7.1 Estimation of Crosslingual Word Similarities To obtain two sets of dependency triples, each set per language, the English dependency parser MiniPar (Lin, 1993) is applied to 750M bytes of English corpora of Wall Street Journal (1980- 1990), resulting in 1.9 x107 English dependency triples. The Chinese dependency parser BlockParser (Zhou, 2000) is applied to 1,200M bytes of Chinese corpora of People&apos;s Daily (1980- 1998), resulting in 3.3 x107 Chinese dependency triples. The HIT English-Chinese bilingual dictionary5 consisting of 66,248 Chinese words, 73,693 English words, and 164,794 translation links is used. 7.2 Improving Word Alignment Models Evaluation metrics Let A denote the set of word alignments from a word alignment method for a pair of bili</context>
</contexts>
<marker>Lin, 1993</marker>
<rawString>D. Lin (1993) Principle-Based Parsing without Oyergeneration. Proceedings of ACL-93, pp 112-120, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words,&amp;quot; COLING-ACL98,</title>
<date>1998</date>
<location>Mentreal, Canada.</location>
<contexts>
<context position="6865" citStr="Lin, 1998" startWordPosition="1049" endWordPosition="1050">milarity functions of two languages. Readers might want to refer to (Och &amp; Ney, 2003) for a comprehensive examination of word alignment models. Another related research topic is the automatic estimation of crosslingual word similarities (or probabilities) from monolingual corpora. For example, the works (Koehn &amp; Knight, 2000; Fung &amp; Lee, 1998; Zhou et al., 2001) that we have mentioned in Section 1. Methods to estimate crosslingual word similarities are sometimes related to methods of monolingual word clustering. For instance, the method used in (Zhou et al., 2001) is motivated by the work in (Lin, 1998). Lin (1998) presents a method to cluster monolingual words based on similarities between words in the same language. The word similarities are estimated from a parsed monolingual corpus. The third related topic is the justification of Direct Correspondence Assumption (DCA), which underlies the models/applications exploiting high level linguistic structures. For example, tree-tree alignment, e.g., (Matsumoto, 1993), in example-based machine translation; synchronous grammars, e.g., (Wu, 1997), for statistical machine translation (SMT). R. Hwa et al. (2002) formulate the DCA, and evaluate it in </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin (1998) &amp;quot;Automatic retrieval and clustering of similar words,&amp;quot; COLING-ACL98, Mentreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Matsumoto</author>
</authors>
<title>Structural Matching of Parallel Texts,&amp;quot;</title>
<date>1993</date>
<booktitle>In Proceedings of ACL 93.</booktitle>
<contexts>
<context position="7283" citStr="Matsumoto, 1993" startWordPosition="1108" endWordPosition="1109"> estimate crosslingual word similarities are sometimes related to methods of monolingual word clustering. For instance, the method used in (Zhou et al., 2001) is motivated by the work in (Lin, 1998). Lin (1998) presents a method to cluster monolingual words based on similarities between words in the same language. The word similarities are estimated from a parsed monolingual corpus. The third related topic is the justification of Direct Correspondence Assumption (DCA), which underlies the models/applications exploiting high level linguistic structures. For example, tree-tree alignment, e.g., (Matsumoto, 1993), in example-based machine translation; synchronous grammars, e.g., (Wu, 1997), for statistical machine translation (SMT). R. Hwa et al. (2002) formulate the DCA, and evaluate it in terms of the accuracies (precision and recall) of the Chinese syntactic parses projected from the corresponding English parse trees, which are the output of an English parser. Their Experiments are done on small newswire corpora. They conclude that DCA is useful with some principled transformation of syntactic structures. 3 Dependency Correspondence Assumption Methods, e.g., (Zhou et al., 2001), to estimate crossli</context>
</contexts>
<marker>Matsumoto, 1993</marker>
<rawString>Y. Matsumoto (1993) &amp;quot;Structural Matching of Parallel Texts,&amp;quot; In Proceedings of ACL 93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mei</author>
</authors>
<title>Xiandai Hanyu TongYi CiDian,&amp;quot; The Commercial Press LTD. of China.</title>
<date>2002</date>
<contexts>
<context position="19786" citStr="Mei, 2002" startWordPosition="3171" endWordPosition="3172"> to train the statistical word alignment algorithm. Since the interpolation coefficients are manually assigned in our experiments, no held-out data were reserved. The HIT English-Chinese bilingual dictionary is used in the dictionary-based word alignment methods DICT and DICT-interp. The same 215,347 bilingual sentences as used in the training of statistical-based models are used to train the crosslingual concept similarities conceptsim(elc) and the distortion model d(i, j). Monolingual thesauri are WordNet (G. Miller, 1990) with 45,784 classes for English and Xian Dai Han Yu Tong Yi Ci Dian (Mei, 2002) with 3,724 classes (40,289 words) for Chinese. A single test set is used for all the word alignment methods. It consists of 1,000 sentence pairs that are disjoint with the training data. 5Created by Harbin Institute of Technology (HIT). (12) E: C: Align: Table 2: Word alignment methods STATS vs. STATS-interp 20000/1 is/2 a/3 very/4 respectable/5 salary/6 ./7 20000/1 YinglBang4/2 De0/3 Xin1Jin1/4 Shi4/5 FeilChang2/6 Ke3Guan1/7 De0/8 ./9 [1:1] [4:6] [5:7] [6:4] [7:9] Algorithms Prec. (%) Rec. (%) F STATS STATS-interp 73.69 67.31 70.36 74.54 68.63 71.46 Algorithms Prec. (%) Rec. (%) F DICT 95.87</context>
</contexts>
<marker>Mei, 2002</marker>
<rawString>J. Mei (2002) &amp;quot;Xiandai Hanyu TongYi CiDian,&amp;quot; The Commercial Press LTD. of China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>WordNet: An on-line lexical database,&amp;quot;</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<pages>3--4</pages>
<contexts>
<context position="19706" citStr="Miller, 1990" startWordPosition="3155" endWordPosition="3156">ents. Experimental settings 215,347 pairs of English and Chinese sentences are used to train the statistical word alignment algorithm. Since the interpolation coefficients are manually assigned in our experiments, no held-out data were reserved. The HIT English-Chinese bilingual dictionary is used in the dictionary-based word alignment methods DICT and DICT-interp. The same 215,347 bilingual sentences as used in the training of statistical-based models are used to train the crosslingual concept similarities conceptsim(elc) and the distortion model d(i, j). Monolingual thesauri are WordNet (G. Miller, 1990) with 45,784 classes for English and Xian Dai Han Yu Tong Yi Ci Dian (Mei, 2002) with 3,724 classes (40,289 words) for Chinese. A single test set is used for all the word alignment methods. It consists of 1,000 sentence pairs that are disjoint with the training data. 5Created by Harbin Institute of Technology (HIT). (12) E: C: Align: Table 2: Word alignment methods STATS vs. STATS-interp 20000/1 is/2 a/3 very/4 respectable/5 salary/6 ./7 20000/1 YinglBang4/2 De0/3 Xin1Jin1/4 Shi4/5 FeilChang2/6 Ke3Guan1/7 De0/8 ./9 [1:1] [4:6] [5:7] [6:4] [7:9] Algorithms Prec. (%) Rec. (%) F STATS STATS-inter</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>G. Miller (1990). &amp;quot;WordNet: An on-line lexical database,&amp;quot; International Journal of Lexicography, 3(4):235-312, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nie</author>
<author>M Simard</author>
<author>P Isabelle</author>
<author>R Durand</author>
</authors>
<title>Cross-Language Information Retrieval based on Parallel Texts and Automatic Mining of Parallel Texts in the Web&amp;quot;, 22nd ACM-SIGIR,</title>
<date>1999</date>
<pages>74--81</pages>
<contexts>
<context position="2166" citStr="Nie et al., 1999" startWordPosition="327" endWordPosition="330">very useful knowledge for machine translation. Many of previous works implicitly assume that bilingual resources (e.g., bilingual corpora, bilingual dictionaries) on a large scale are available. Better performance of word alignment algorithms can be achieved if the training is conducted on larger parallel corpora. The increasing requirement for bilingual resources thus becomes a bottleneck of constructing practical * This work was done while the author was visiting Microsoft Research Asia. word aligners for a general domain. To alleviate the problem, some works, like (Resnik &amp; Smith, 2003; J. Nie et al., 1999), try to obtain parallel corpora by automatically scrawling them from the web. Instead of heavily relying on bilingual corpora, some works try to solve the bottleneck problem in a different way: to mine bilingual knowledge from monolingual corpora, which can be more easily obtained in a large volume. Koehn &amp; Knight (2000) present an approach to estimating word translation probabilities by using unrelated monolingual corpora based on the EM algorithm. Promising results of their method are exhibited in selecting the right translation among several options provided by a bilingual dictionary. Howe</context>
</contexts>
<marker>Nie, Simard, Isabelle, Durand, 1999</marker>
<rawString>J. Nie, M. Simard, P. Isabelle, R. Durand (1999) &amp;quot;Cross-Language Information Retrieval based on Parallel Texts and Automatic Mining of Parallel Texts in the Web&amp;quot;, 22nd ACM-SIGIR, pp. 74-81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models,&amp;quot; In Computational Linguistics.</title>
<date>2003</date>
<contexts>
<context position="5751" citStr="Och &amp; Ney, 2003" startWordPosition="865" endWordPosition="868">. The remainder of this paper is organized as follows. Section 2 lists the related research. Section 3 justifies the dependency correspondence assumption. Section 4 presents the method to estimate crosslingual word similarities from structured monolingual corpora. Sections 5 and 6 describe how to incorporate estimated similarities into word alignment methods. Experiments are reported in Section 7. The last section makes conclusions and points out future works. 2 Related Research One of the related topics is word alignment. Word alignment models can generally be classified into two categories (Och &amp; Ney, 2003): statistical alignment models and heuristic ones. A statistical alignment model p(c, ale) describes the relationship (e.g., word alignment a) between a source language string e and a target language string c. Different decompositions of p(c, ale) result in different variants of word alignment models. Heuristic based word align&apos;Although the language pair used in our paper is English and Chinese, the basic idea of this paper, however, can be generalized to other language pairs. ment approaches use similarity functions of two languages. Readers might want to refer to (Och &amp; Ney, 2003) for a comp</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. Och and H. Ney (2003) &amp;quot;A Systematic Comparison of Various Statistical Alignment Models,&amp;quot; In Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation&amp;quot;,</title>
<date>2003</date>
<booktitle>in ACL</booktitle>
<contexts>
<context position="27882" citStr="Och, 2003" startWordPosition="4399" endWordPosition="4400">onolingual resources to bilingual tasks. Moreover, we also justified the dependency correspondence assumption based on a large and balanced corpora. This assumption underlies the method that we have used to estimated crosslingual word similarities from structured monolingual corpora. One of the interesting topics deserving study in the future could be to divide the crosslingual word pairs into clusters and estimate the optimal interpolation coefficient A for each of them using the minimum error rate (MER) as the optimization goal, instead of maximum likelihood. Results in machine translation (Och, 2003) and speech recognition have shown the advantage of discriminative training. Another topic will be to compute the (normalized) word similarities q(elc) in an bootstrapping manner, for instance, using the EM algorithm. In each iteration, the (normalized) word similarities output from the previous iteration are used as weights of two corresponding dependency triples in two languages. These weights are involved in the computation of the common information (Section 4) between two crosslingual words. We also desire to improve statistical models of the state of art with these re-estimated similariti</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. Och (2003) &amp;quot;Minimum Error Rate Training in Statistical Machine Translation&amp;quot;, in ACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
<author>N Smith</author>
</authors>
<title>The Web as a Parallel Corpus,&amp;quot;</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<pages>349--380</pages>
<contexts>
<context position="2144" citStr="Resnik &amp; Smith, 2003" startWordPosition="322" endWordPosition="325">the pair. Such links are very useful knowledge for machine translation. Many of previous works implicitly assume that bilingual resources (e.g., bilingual corpora, bilingual dictionaries) on a large scale are available. Better performance of word alignment algorithms can be achieved if the training is conducted on larger parallel corpora. The increasing requirement for bilingual resources thus becomes a bottleneck of constructing practical * This work was done while the author was visiting Microsoft Research Asia. word aligners for a general domain. To alleviate the problem, some works, like (Resnik &amp; Smith, 2003; J. Nie et al., 1999), try to obtain parallel corpora by automatically scrawling them from the web. Instead of heavily relying on bilingual corpora, some works try to solve the bottleneck problem in a different way: to mine bilingual knowledge from monolingual corpora, which can be more easily obtained in a large volume. Koehn &amp; Knight (2000) present an approach to estimating word translation probabilities by using unrelated monolingual corpora based on the EM algorithm. Promising results of their method are exhibited in selecting the right translation among several options provided by a bili</context>
</contexts>
<marker>Resnik, Smith, 2003</marker>
<rawString>P. Resnik and N. Smith (2003) &amp;quot;The Web as a Parallel Corpus,&amp;quot; Computational Linguistics, Vol. 29, Issue 3, pp 349-380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora,&amp;quot;</title>
<date>1997</date>
<journal>Computational Linguistics</journal>
<pages>23--3</pages>
<contexts>
<context position="7361" citStr="Wu, 1997" startWordPosition="1117" endWordPosition="1118">ual word clustering. For instance, the method used in (Zhou et al., 2001) is motivated by the work in (Lin, 1998). Lin (1998) presents a method to cluster monolingual words based on similarities between words in the same language. The word similarities are estimated from a parsed monolingual corpus. The third related topic is the justification of Direct Correspondence Assumption (DCA), which underlies the models/applications exploiting high level linguistic structures. For example, tree-tree alignment, e.g., (Matsumoto, 1993), in example-based machine translation; synchronous grammars, e.g., (Wu, 1997), for statistical machine translation (SMT). R. Hwa et al. (2002) formulate the DCA, and evaluate it in terms of the accuracies (precision and recall) of the Chinese syntactic parses projected from the corresponding English parse trees, which are the output of an English parser. Their Experiments are done on small newswire corpora. They conclude that DCA is useful with some principled transformation of syntactic structures. 3 Dependency Correspondence Assumption Methods, e.g., (Zhou et al., 2001), to estimate crosslingual word similarities from structured monolingual corpora also take advantag</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>D. Wu (1997) &amp;quot;Stochastic inversion transduction grammars and bilingual parsing of parallel corpora,&amp;quot; Computational Linguistics 23(3):377-404, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zhou</author>
</authors>
<title>A Block-Based Robust Dependency Parser for Unrestricted Chinese Text,&amp;quot; 2nd Workshop on Chinese language processing, Hong Kong.</title>
<date>2000</date>
<contexts>
<context position="9689" citStr="Zhou, 2000" startWordPosition="1493" endWordPosition="1494">the types of dependency relations, we examine only the Verb-Object (VO) dependency type since we think that VO is one of the dependency types that are often preserved across languages, and DCA will thus mostly hold among these dependency types. Third, we evaluate the DCA in terms of mapping ratio: the ratio of the number of correct crosslingual dependency mappings versus the number of overall mappings. To do this, we first manually add the word alignment information to the 10,000 sentence pairs. Then, we run the English parser MiniPar (Lin, 1993) and the Chinese dependency parser BlockParser (Zhou, 2000) on both sides of the corpora, respectively. Next, we extract all the mappings from English dependency triples to Chinese dependency triples (and vice versa) based on the word alignment and parsing results. Table 1 lists the mapping results. The first column shows the dependency type and the mapping directions. The correct mapping ratio reaches 82.71% from English to Chinese, and 83.87% from Chinese to English. Note that we treated the dependency type Verb-Object-Prep as VO. The intent of this experiment is not to compare with the method and results in (R. Hwa et al., 2002), but to evaluate th</context>
<context position="18325" citStr="Zhou, 2000" startWordPosition="2930" endWordPosition="2931">h q(cle) will be referred to as CLASS-interp. 7 Experiments The experiments include the estimation of crosslingual word similarities from structured monolingual corpora (Section 4), and the application of the estimated similarities to word alignment algorithms. 7.1 Estimation of Crosslingual Word Similarities To obtain two sets of dependency triples, each set per language, the English dependency parser MiniPar (Lin, 1993) is applied to 750M bytes of English corpora of Wall Street Journal (1980- 1990), resulting in 1.9 x107 English dependency triples. The Chinese dependency parser BlockParser (Zhou, 2000) is applied to 1,200M bytes of Chinese corpora of People&apos;s Daily (1980- 1998), resulting in 3.3 x107 Chinese dependency triples. The HIT English-Chinese bilingual dictionary5 consisting of 66,248 Chinese words, 73,693 English words, and 164,794 translation links is used. 7.2 Improving Word Alignment Models Evaluation metrics Let A denote the set of word alignments from a word alignment method for a pair of bilingual sentences, and let R denote the set of word alignments of the same sentence pair in the reference corpora, let I • I denote the size of set •, then we use Precision = IA n RI 1AI I</context>
</contexts>
<marker>Zhou, 2000</marker>
<rawString>M. Zhou (2000) &amp;quot;A Block-Based Robust Dependency Parser for Unrestricted Chinese Text,&amp;quot; 2nd Workshop on Chinese language processing, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zhou</author>
<author>Y Ding</author>
<author>C Huang</author>
</authors>
<title>Improving Translation Selection with a New Translation Model Trained by Independent Monolingual Corpora,&amp;quot;</title>
<date>2001</date>
<booktitle>Computational Linguistics and Chinese Language Processing, Vol.6, No.1,</booktitle>
<pages>1--26</pages>
<contexts>
<context position="3459" citStr="Zhou et al. (2001)" startWordPosition="530" endWordPosition="533">ead of how to extend it by adding new translations. They discussed that better language modeling, e.g., lexical dependencies, may be useful for further improvement. Fung &amp; Lee (1998) use an IR approach to inducing new word translations from comparable corpora. Their method is able to induce new translations, thus to extend a bilingual dictionary. As discussed in their paper, their method suffers form low recall. They gave some suggestions in achieving better results, including introducing PoS tagging, improving word segmentation accuracy. Using a better language model will certainly do these. Zhou et al. (2001) compute crosslingual word similarities from two dependency triple databases, each of which is obtained by parsing a large monolingual corpus with a dependency parser. Since bilexical dependency correspondences are considered in the computation, the resulting similarities are expected to be more accurate. Better language modeling imposes more constraints on the confidences/weights of translation candidates. Encouraging results are shown in the application of the similarities to translation selection. In this paper, we are interested in the question how and to what extent the word alignment tas</context>
<context position="4712" citStr="Zhou et al., 2001" startWordPosition="714" endWordPosition="717">lingual language processing, e.g., monolingual parsers and monolingual corpora, so that the requirement for large bilingual corpora could be minimized. Specifically, we are concerned with how to &amp;quot;convert&amp;quot; structured monolingual corpora into bilingual word similarities (or translation probabilities) and how to use the similarities to enhance the existing word alignment models. To do this, we first empirically justify the assumption exploiting the structural correspondence between different languages.&apos; Then crosslingual word similarities are computed from structured corpora using the method in (Zhou et al., 2001) based on this assumption. After that, the crosslingual word similarities are normalized and incorporated into word alignment models. Experiments are conducted on different types of alignment models ranging from statistical-based ones and heuristic-based ones. Experimental results show that word alignment models can be consistently improved with the crosslingual similarities estimated from structured monolingual corpora. The remainder of this paper is organized as follows. Section 2 lists the related research. Section 3 justifies the dependency correspondence assumption. Section 4 presents the</context>
<context position="6619" citStr="Zhou et al., 2001" startWordPosition="1006" endWordPosition="1009">sult in different variants of word alignment models. Heuristic based word align&apos;Although the language pair used in our paper is English and Chinese, the basic idea of this paper, however, can be generalized to other language pairs. ment approaches use similarity functions of two languages. Readers might want to refer to (Och &amp; Ney, 2003) for a comprehensive examination of word alignment models. Another related research topic is the automatic estimation of crosslingual word similarities (or probabilities) from monolingual corpora. For example, the works (Koehn &amp; Knight, 2000; Fung &amp; Lee, 1998; Zhou et al., 2001) that we have mentioned in Section 1. Methods to estimate crosslingual word similarities are sometimes related to methods of monolingual word clustering. For instance, the method used in (Zhou et al., 2001) is motivated by the work in (Lin, 1998). Lin (1998) presents a method to cluster monolingual words based on similarities between words in the same language. The word similarities are estimated from a parsed monolingual corpus. The third related topic is the justification of Direct Correspondence Assumption (DCA), which underlies the models/applications exploiting high level linguistic struc</context>
<context position="7862" citStr="Zhou et al., 2001" startWordPosition="1192" endWordPosition="1195">tree alignment, e.g., (Matsumoto, 1993), in example-based machine translation; synchronous grammars, e.g., (Wu, 1997), for statistical machine translation (SMT). R. Hwa et al. (2002) formulate the DCA, and evaluate it in terms of the accuracies (precision and recall) of the Chinese syntactic parses projected from the corresponding English parse trees, which are the output of an English parser. Their Experiments are done on small newswire corpora. They conclude that DCA is useful with some principled transformation of syntactic structures. 3 Dependency Correspondence Assumption Methods, e.g., (Zhou et al., 2001), to estimate crosslingual word similarities from structured monolingual corpora also take advantage of this assumption. The justification of this assumption using a large scale and balanced data is thus necessary. The Dependency Correspondence Assumption can be formally expressed as follows. Let triple (wl, R, w2) be a syntactic dependency consisting of two words wi and w2 and a dependency relation R between them. Given a pair of sentences E and C that are translation of each other with syntactic structures TreeE and Treec, if TreeE contains a dependency triple (ei, R, e2), and el and e2 are </context>
<context position="10832" citStr="Zhou et al., 2001" startWordPosition="1677" endWordPosition="1680">re with the method and results in (R. Hwa et al., 2002), but to evaluate the dependency correspondence assumption for a certain type of dependency concerned by us. The numbers in Table 1 are very encouraging because it indicates the feasibility of DCA. This also suggests that translating the sentences in the way of keeping DCA on some key dependency types can normally get understandable translation results. 4 Crosslingual Word Similarities We now briefly describe the method that we use to estimate crosslingual words similarities from structured monolingual corpora. The method was proposed in (Zhou et al., 2001). We shall use slightly different notations. The information of a dependency triple T = R,w2) is defined as: c(w 1, R,w2)c(• , R, .) I(T) = log2 ( (1) R, •)c(• , R, w2) where c(...) is the counting function. T is called a supportive dependency of wi (or w2) if I(T) &gt; 0. Let D(e) be the set of e&apos;s supportive dependencies collected from a structured corpus in language L1. Let D(c) be the set of c&apos;s supportive dependencies collected from a structured corpus in language L2. Let S(r) be a function returning 1 if dependency T E D(e) has a corresponding dependency in D(c), and e corresponds to c;3 an</context>
<context position="12055" citStr="Zhou et al., 2001" startWordPosition="1905" endWordPosition="1908">urning 0 otherwise. Note that we need a bilingual dictionary to &amp;quot;bridge&amp;quot; corresponding dependencies in different languages, and this bilingual dictionary is the only bilingual resource used. The common information between c and e is then defined as: /4,6= s(T)/(7)+ E s(T)/(T) (2) TED(e) TED(c) The overall information between e and c is defined as: I 0(e, = I (r) E I(T) (3) TED(e) TED(c) The similarity sim(c, e) between a pair of crosslingual words c in language L1 and e in language L2 is defined as the ratio of ic(c, e) to 1-0(C, e): sim(c, e) = (c, 10(c, c (4) Readers might want to refer to (Zhou et al., 2001) for detailed derivations. 2Created by Microsoft Research Asia, and not pub- 30r in the other direction, returning 1 if 7 E D(C) has a corresponding dependency in D(e). licly available. It is worth mentioning that this method has the ability of inducing new translations. In principle, the similarity of any pair of crosslingual words can be computed using Formula 4. Since the information used in the computation is distributedly encoded in the relevant dependency triples in the entire treebank, this method is also very robust. Although the value of function sim(c, e) computed by their method ran</context>
<context position="26551" citStr="Zhou et al., 2001" startWordPosition="4197" endWordPosition="4200">al corpora into bilingual word similarities (or translation probabilities) and how to incorporate them into word alignment models for performance improvement. Our aim is to take advantage of monolingual resources, e.g., corpora, parsers, treebank, for bilingual tasks, e.g., word alignment, so that the requirement for large amounts of training bilingual corpora could be alleviated. To do this, we first empirically justified the dependency correspondence assumption between different languages using large and balanced corpora. Then, we computed crosslingual word similarities using the method in (Zhou et al., 2001) based on this assumption. After that, we normalized the crosslingual word similarities and integrated them into word alignment algorithms using interpolated models. Experiments are conducted on word alignment algorithms ranging from statistical-based ones and heuristic-based ones. Experimental results show that word alignment models have been consistently improved, in particular the recall metric. The main contribution of this paper is that we have presented an approach to combining the knowledge mined from structured monolingual corpora with the knowledge mined from bilingual corpora, and sh</context>
</contexts>
<marker>Zhou, Ding, Huang, 2001</marker>
<rawString>M. Zhou, Y. Ding, and C. Huang (2001) &amp;quot;Improving Translation Selection with a New Translation Model Trained by Independent Monolingual Corpora,&amp;quot; Computational Linguistics and Chinese Language Processing, Vol.6, No.1, Feb. 2001, pp. 1-26.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>