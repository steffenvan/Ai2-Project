<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000020">
<title confidence="0.9936355">
ECNU: One Stone Two Birds: Ensemble of Heterogenous Measures for
Semantic Relatedness and Textual Entailment
</title>
<author confidence="0.999599">
Jiang Zhao, Tian Tian Zhu, Man Lan*
</author>
<affiliation confidence="0.974875">
Department of Computer Science and Technology
East China Normal University
</affiliation>
<email confidence="0.992453">
51121201042,51111201046@ecnu.cn; mlan@cs.ecnu.edu.cn*
</email>
<sectionHeader confidence="0.997278" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998886">
This paper presents our approach to se-
mantic relatedness and textual entailment
subtasks organized as task 1 in SemEval
2014. Specifically, we address two ques-
tions: (1) Can we solve these two sub-
tasks together? (2) Are features proposed
for textual entailment task still effective
for semantic relatedness task? To address
them, we extracted seven types of features
including text difference measures pro-
posed in entailment judgement subtask, as
well as common text similarity measures
used in both subtasks. Then we exploited
the same feature set to solve the both sub-
tasks by considering them as a regression
and a classification task respectively and
performed a study of influence of differ-
ent features. We achieved the first and the
second rank for relatedness and entailment
task respectively.
</bodyText>
<sectionHeader confidence="0.999468" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.957530924528302">
Distributional Semantic Models (DSMs)(surveyed
in (Turney et al., 2010)) exploit the co-occurrences
of other words with the word being modeled to
compute the semantic meaning of the word un-
der the distributional hypothesis: “similar words
share similar contexts” (Harris, 1954). Despite
their success, DSMs are severely limited to model
the semantic of long phrases or sentences since
they ignore grammatical structures and logical
words. Compositional Distributional Semantic
Models (CDSMs)(Zanzotto et al., 2010; Socher et
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
al., 2012) extend DSMs to sentence level to cap-
ture the compositionality in the semantic vector
space, which has seen a rapidly growing interest
in recent years. Although several CDSMs have
been proposed, benchmarks are lagging behind.
Previous work (Grefenstette and Sadrzadeh, 2011;
Socher et al., 2012) performed experiments on
their own datasets or on the same datasets which
are limited to a few hundred instances of very short
sentences with a fixed structure.
To provide a benchmark so as to compare dif-
ferent CDSMs, the sentences involving composi-
tional knowledge task in SemEval 2014 (Marelli et
al., 2014) develops a large dataset which is full of
lexical, syntactic and semantic phenomena. It con-
sists of two subtasks: semantic relatedness task,
which measures the degree of semantic relatedness
of a sentence pair by assigning a relatedness score
ranging from 1 (completely unrelated) to 5 (very
related); and textual entailment (TE) task, which
determines whether one of the following three re-
lationships holds between two given sentences A
and B: (1) entailment: the meaning of B can be
inferred from A; (2) contradiction: A contradicts
B; (3) neutral: the truth of B cannot be inferred on
the basis of A.
Semantic textual similarity (STS) (Lintean and
Rus, 2012) and semantic relatedness are closely
related and interchangeably used in many liter-
atures except that the concept of semantic simi-
larity is more specific than semantic relatedness
and the latter includes concepts as antonymy and
meronymy. In this paper we regard the semantic
relatedness task as a STS task. Besides, regardless
of the original intention of this task, we adopted
the mainstream machine learning methods instead
of CDSMs to solve these two tasks by extracting
heterogenous features.
</bodyText>
<page confidence="0.966745">
271
</page>
<note confidence="0.980606">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 271–277,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.999875617647059">
Like semantic relatedness, TE task (surveyed
in (Androutsopoulos and Malakasiotis, 2009)) is
also closely related to STS task since in TE task
lots of similarity measures at different levels are
exploited to boost classification. For example,
(Malakasiotis and Androutsopoulos, 2007) used
ten string similarity measures such as cosine sim-
ilarity at the word and the character level. There-
fore, the first fundamental question arises, i.e.,
“Can we solve both of these two tasks together?”
At the same time, since high similarity does not
mean entailment holds, the TE task also utilizes
other features besides similarity measures. For ex-
ample, in our previous work (Zhao et al., 2014)
text difference features were proposed and proved
to be effective. Therefore, the second question sur-
faces here, i.e., “Are features proposed for TE task
still effective for STS task?” To answer the first
question, we extracted seven types of features in-
cluding text similarity and text difference and then
fed them to classifiers and regressors to solve TE
and STS task respectively. Regarding the second
question, we conducted a series of experiments
to study the performance of different features for
these two tasks.
The rest of the paper is organized as follows.
Section 2 briefly describes the related work on
STS and TE tasks. Section 3 presents our systems
including features, learning methods, etc. Section
4 shows the experimental results on training data
and Section 5 reports the results of our submitted
systems on test data and gives a detailed analysis.
Finally, Section 6 concludes this paper with future
work.
</bodyText>
<sectionHeader confidence="0.99989" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.980794272727273">
Existing work on STS can be divided into 4
categories according to the similarity measures
used (Gomaa and Fahmy, 2013): (1) string-based
method (B¨ar et al., 2012; Malakasiotis and An-
droutsopoulos, 2007) which calculates similarities
using surface strings at either character level or
word level; (2) corpus-based method (Li et al.,
2006) which measures word or sentence similar-
ities using the information gained from large cor-
pora, including Latent Semantic Analysis (LSA),
pointwise mutual information (PMI), etc. (3)
knowledge-based method (Mihalcea et al., 2006)
which estimates similarities with the aid of ex-
ternal resources, such as WordNet1; (4) hybrid
lhttp://wordnet.princeton.edu/
method (Zhu and Lan, 2013; Croce et al., 2013)
which integrates multiple similarity measures and
adopts supervised machine learning algorithms to
learn the different contributions of different fea-
tures.
The approaches to the task of TE can be roughly
divided into two groups: (1) logic inference
method (Bos and Markert, 2005) where automatic
reasoning tools are used to check the logical repre-
sentations derived from sentences and (2) machine
learning method (Zhao et al., 2013; Gomaa and
Fahmy, 2013) where a supervised model is built
using a variety of similarity scores.
Unlike previous work which separately ad-
dressed these two closely related tasks by using
simple feature types, in this paper we endeavor to
simultaneously solve these two tasks by using het-
erogenous features.
</bodyText>
<sectionHeader confidence="0.990659" genericHeader="method">
3 Our Systems
</sectionHeader>
<bodyText confidence="0.999994444444444">
We consider the two tasks as one by exploiting the
same set of features but using different learning
methods, i.e., classification and regression. Seven
types of features are extracted and most of them
are based on our previous work on TE (Zhao et
al., 2014) and STS (Zhu and Lan, 2013). Many
learning algorithms and parameters are examined
and the final submitted systems are configured ac-
cording to the preliminary results on training data.
</bodyText>
<subsectionHeader confidence="0.999426">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999986428571429">
Three text preprocessing operations were per-
formed before we extracted features, which in-
cluded: (1) we converted the contractions to their
formal writings, for example, doesn’t is rewrit-
ten as does not. (2) the WordNet-based Lemma-
tizer implemented in Natural Language Toolkit2
was used to lemmatize all words to their nearest
base forms in WordNet, for example, was is lem-
matized to be. (3) we replaced a word from one
sentence with another word from the other sen-
tence if the two words share the same meaning,
where WordNet was used to look up synonyms.
No word sense disambiguation was performed and
all synsets for a particular lemma were considered.
</bodyText>
<subsectionHeader confidence="0.999243">
3.2 Feature Representations
3.2.1 Length Features (len)
</subsectionHeader>
<bodyText confidence="0.994529">
Given two sentences A and B, this feature type
records the length information using the follow-
</bodyText>
<footnote confidence="0.967676">
2http://nltk.org/
</footnote>
<page confidence="0.990303">
272
</page>
<bodyText confidence="0.822023">
ing eight measure functions:
</bodyText>
<equation confidence="0.942366333333333">
|A|, |B|, |A − B|, |B − A|, |A ∪ B|, |A ∩ B|, (|A|−|B|)
|B |, (|B|−|A|)
|A|
</equation>
<bodyText confidence="0.999988538461539">
where |A |stands for the number of non-repeated
words in sentence A , |A − B |means the number of
unmatched words found in A but not in B , |A u B|
stands for the set size of non-repeated words found
in either A or B and |A n B |means the set size of
shared words found in both A and B .
Moreover, in consideration of different types of
words make different contributions to text similar-
ity, we also recorded the number of words in set
A − B and B − A whose POS tags are noun, verb,
adjective and adverb respectively. We used Stan-
ford POS Tagger3 for POS tagging. Finally, we
collected a total of sixteen features.
</bodyText>
<subsectionHeader confidence="0.702106">
3.2.2 Surface Text Similarity (st)
</subsectionHeader>
<bodyText confidence="0.9988602">
As shown in Table 1, we adopted six commonly
used functions to calculate the similarity between
sentence A and B based on their surface forms,
where x and y are vectorial representations of
sentences A and B in tf * idf schema.
</bodyText>
<equation confidence="0.771694272727273">
Measure Definition
Jaccard Sjacc = |A ∩ B|/|A ∪ B|
Dice Sdice = 2 ∗ |A ∩ B|/(|A |+ |B|)
Overlap Sover = |A ∩ B|/|A |and |A ∩ B|/|B|
Cosine Scos = →−x · →−Y /(∥ →−x ∥ · ∥ →−Y ∥)
n
Manhattan m(−→ x , →−Y ) = ∑ |xi − Yi|
i=1
n
Euclidean E(−→x , →−Y ) =√ ∑ (xi − Yi)2
i=1
</equation>
<tableCaption confidence="0.917315">
Table 1: Surface text similarity measures and their
definitions used in our experiments.
</tableCaption>
<bodyText confidence="0.9996376">
We also used three statistical correlation coef-
ficients (i.e., Pearson, Spearmanr, Kendalltau) to
measure similarity by regarding the vectorial rep-
resentations as different variables. Thus we got ten
features at last.
</bodyText>
<subsectionHeader confidence="0.701629">
3.2.3 Semantic Similarity (ss)
</subsectionHeader>
<bodyText confidence="0.999944363636364">
The above surface text similarity features only
consider the surface words rather than their ac-
tual meanings in sentences. In order to build the
semantic representations of sentences, we used a
latent model to capture the contextual meanings
of words. Specifically, we adopted the weighted
textual matrix factorization (WTMF) (Guo and
Diab, 2012) to model the semantics of sentences
due to its reported good ability to model short
texts. This model first factorizes the original term-
sentence matrix X into two matrices such that
</bodyText>
<footnote confidence="0.498838">
3http://nlp.stanford.edu/software/tagger.shtml
</footnote>
<bodyText confidence="0.993967625">
Xi,j Pz� PT*,i.Q*,j, where P*,i is a latent seman-
tic vector profile for word wi and Q*,j is a vector
profile that represents the sentence sj. Then we
employed the new representations of sentences,
i.e., Q, to calculate the semantic similarity be-
tween sentences using Cosine, Manhattan, Eu-
clidean, Pearson, Spearmanr, Kendalltau measures
respectively, which results in six features.
</bodyText>
<subsectionHeader confidence="0.755821">
3.2.4 Grammatical Relationship (gr)
</subsectionHeader>
<bodyText confidence="0.999986357142857">
The grammatical relationship feature measures
the semantic similarity between two sentences
at the grammar level and this feature type was
also explored in our previous work (Zhao et al.,
2013; Zhu and Lan, 2013). We used Stanford
Parser4 to acquire the dependency information
from sentences and the grammatical information
are represented in the form of relation unit, e.g.
nsubj(example, this), where nsubj stands for a de-
pendency relationship between example and this.
We obtained a sequence of relation units for each
sentence and then used them to estimate similarity
by adopting eight measure functions described in
Section 3.2.1, resulting in eight features.
</bodyText>
<subsectionHeader confidence="0.933497">
3.2.5 Text Difference Measures (td)
</subsectionHeader>
<bodyText confidence="0.999889545454545">
There are two types of text difference measures.
The first feature type is specially designed for
the contradiction entailment relationship, which
is based on the following observation: there ex-
ist antonyms between two sentences or the nega-
tion status is not consistent (i.e., one sentence has
a negation word while the other does not have) if
contradiction holds. Therefore we examined each
sentence pair and set this feature as 1 if at least one
of these conditions is met, otherwise -1. WordNet
was used to look up antonyms and a negation list
with 28 words was used.
The second feature type is extracted from two
word sets A−B and B −A as follows: we first cal-
culated the similarities between every word from
A − B and every word from B − A , then took the
maximum, minimum and average value of them as
features. In our experiments, four WordNet-based
similarity measures (i.e., path, lch, wup, jcn (Go-
maa and Fahmy, 2013)) were used to calculate the
similarity between two words.
Totally, we got 13 text difference features.
</bodyText>
<footnote confidence="0.972165">
4http://nlp.stanford.edu/software/lex-parser.shtml
</footnote>
<page confidence="0.992322">
273
</page>
<subsectionHeader confidence="0.652301">
3.2.6 String Features (str)
</subsectionHeader>
<bodyText confidence="0.999981347826087">
This set of features is taken from our previous
work (Zhu and Lan, 2013) due to its superior per-
formance.
Longest common sequence (LCS) We computed
the LCS similarity on the original and lemmatized
sentences. It was calculated by finding the maxi-
mum length of a common contiguous subsequence
of two strings and then dividing it by the smaller
length of two strings to eliminate the impacts of
length imbalance.
Jaccard similarity using n-grams We obtained
n-grams at three different levels, i.e., the origi-
nal word level, the lemmatized word level and the
character level. Then these n-grams were used for
calculating Jaccard similarity defined in Table 1.
In our experiments, n = {1, 2,3} were used for
the word level and n = {2, 3, 4} were used for the
character level.
Weighted word overlap (WWO) Since not all
words are equally important, the traditional Over-
lap similarity may not be always reasonable. Thus
we used the information content of word w to es-
timate the importance of word w as follows:
</bodyText>
<equation confidence="0.997271666666667">
∑w′EC freq(wl)
ic(w) = ln
freq(w)
</equation>
<bodyText confidence="0.9999346">
where C is the set of words in the corpus and
freq(w) is the frequency of the word w in the
corpus. To compute ic(w), we used the Web 1T
5-gram Corpus 5. Then the WWO similarity of
two sentence s1 and s2 was calculated as follows:
</bodyText>
<equation confidence="0.896148">
∑
wEs,ns2 ic(w)
∑w′Es2 ic(wl)
</equation>
<bodyText confidence="0.999943714285714">
Due to its asymmetry, we used the harmonic mean
of 5imwwo(s1, s2) and 5imwwo(s2, s1) as the fi-
nal WWO similarity. The WWO similarity is cal-
culated on the original and lemmatized strings re-
spectively.
Finally, we got two LCS features, nine Jaccard
n-gram features and two WWO features.
</bodyText>
<subsectionHeader confidence="0.533673">
3.2.7 Corpus-based Features (cps)
</subsectionHeader>
<bodyText confidence="0.9999016">
Two types of corpus-based feature are also bor-
rowed from our previous work (Zhu and Lan,
2013), i.e., vector space sentence similarity and
co-occurrence retrieval model (CRM), which re-
sults in six features.
</bodyText>
<footnote confidence="0.848465">
5https://catalog.ldc.upenn.edu/LDC2006T13
</footnote>
<note confidence="0.470081">
Co-occurrence retrieval model (CRM) The
CRM word similarity is calculated as follows:
</note>
<equation confidence="0.988576">
5imCRM(w1,w2) = |c(w1) |+ |c(w2)|
</equation>
<bodyText confidence="0.999954583333333">
where c(w) is the set of words that co-occur with
word w. We used the 5-gram part of the Web 1T
5-gram Corpus to obtain c(w). We only consid-
ered the word w with |c(w) |&gt; T and then took
the top 200 co-occurring words ranked by the co-
occurrence frequency as its c(w). In our experi-
ment, we set T = {50, 200}. To propagate the
similarity from words to sentences, we adopted
the best alignment strategy used in (Banea et al.,
2012) to align two sentences.
Vector space sentence similarity This feature set
is taken from (ˇSari´c et al., 2012), which is based
on distributional vectors of words. First we per-
formed latent semantic analysis (LSA) over two
corpora, i.e., the New York Times Annotated Cor-
pus (NYT) (Sandhaus, 2008) and Wikipedia, to es-
timate the distributions of words. Then we used
two strategies to convert the distributional mean-
ings of words to sentence level: (i) simply sum-
ming up the distributional vector of each word w
in the sentence, (ii) using the information content
ic(w) to weigh the LSA vector of each word w and
summing them up. Then we used cosine similarity
to measure the similarity of two sentences.
</bodyText>
<subsectionHeader confidence="0.999341">
3.3 Learning Algorithms
</subsectionHeader>
<bodyText confidence="0.999903095238095">
We explored several classification algorithms to
classify entailment relationships and regression
algorithms to predict similarity scores using the
above 72 features after performing max-min stan-
dardization procedure by scaling them to [-1,1].
Five supervised learning methods were explored:
Support Vector Machine (SVM) which makes the
decisions according to the hyperplanes, Random
Forest (RF) which constructs a multitude of de-
cision trees at training time and selects the mode
of the classes output by individual trees, Gradient
Boosting (GB) that produces a prediction model
in the form of an ensemble of weak prediction
models, k-nearest neighbors (kNN) that decides
the class labels with the aid of the classes of k
nearest neighbors, and Stochastic Gradient De-
scent (SGD) which uses SGD technique to min-
imize loss functions. These supervised learning
methods are implemented in scikit-learn toolkit
(Pedregosa et al., 2011). Besides, we also used
a semi-supervised learning strategy for both tasks
</bodyText>
<equation confidence="0.9986815">
5imwwo(s1, s2) =
2 ∗ |c(w1) ∩ c(w2)|
</equation>
<page confidence="0.993656">
274
</page>
<bodyText confidence="0.999964090909091">
in order to make full use of unlabeled test data.
Specifically, the co-training algorithm was used to
address TE task according to (Zhao et al., 2014).
Its strategy is to train two classifiers with two data
views and to add the top confident predicted in-
stances by one classifier to expand the training set
of another classifier and then to re-train the two
classifiers on the expanded training sets. For STS
task, we utilized CoReg algorithm (Zhou and Li,
2005) which uses two kNN regressors to perform
co-training paradigm.
</bodyText>
<subsectionHeader confidence="0.950265">
3.4 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.9998655">
In order to evaluate the performance of differ-
ent algorithms, we adopted the official evaluation
measures, i.e., Pearson correlation coefficient for
STS task and accuracy for TE task.
</bodyText>
<sectionHeader confidence="0.99848" genericHeader="method">
4 Experiments on Training Data
</sectionHeader>
<bodyText confidence="0.98401385">
To make a reasonable comparison between differ-
ent algorithms, we performed 5-fold cross valida-
tion on training data with 5000 sentence pairs. The
parameters tuned in different algorithms are listed
below: the trade-off parameter c in SVM, the num-
ber of trees n in RF, the number of boosting stages
n in GB, the number of nearest neighbors k in kNN
and the number of passes over the training data n
in SGD. The rest parameters are set to be default.
Table 2: The 5-fold cross validation results on
training data with mean and standard deviation for
each algorithm.
Table 2 reports the experimental results of 5-
fold cross validation with mean and standard devi-
ation and the optimal parameters on training data.
The results of semi-supervised learning methods
are not listed because only a few parameters are
tried due to the limit of time. From this table we
see that SVM, RF and GB perform comparable re-
sults to each other.
</bodyText>
<sectionHeader confidence="0.999924" genericHeader="evaluation">
5 Results on Test Data
</sectionHeader>
<subsectionHeader confidence="0.99582">
5.1 Submitted System Configurations
</subsectionHeader>
<bodyText confidence="0.9999298">
According to the above preliminary experimental
results, we configured five final systems for each
task. Table 3 presents the classification and regres-
sion algorithms with their parameters used in the
five systems for each task.
</bodyText>
<table confidence="0.797592166666667">
System STS task TE task
1 SVR, c=10 SVC, c=100
2 GB, n=210 GB, n=140
3 RF, n=40 RF, n=30
4 CoReg, k=13 co-training, k=40
5 majority voting majority voting
</table>
<tableCaption confidence="0.8320695">
Table 3: Five system configurations for test data
for two tasks.
</tableCaption>
<bodyText confidence="0.999944555555556">
Among them, System 1 acts as our primary
and baseline system that employs SVM algorithm
and as comparison System 2 and System 3 exploit
GB and RF algorithm respectively. Unlike super-
vised settings in the aforementioned systems, Sys-
tem 4 employs a semi-supervised learning strategy
to make use of unlabeled test data. For CoReg,
the number of iteration and the number of near-
est neighbors are set as 100 and 13 respectively,
and for each iteration in co-training, the number
of confident predictions is set as 40. To further
improve performance, System 5 combines the re-
sults of 5 different algorithms (i.e. MaxEnt, SVM,
kNN, GB, RF) through majority voting. We used
the averaged values of the outputs from different
regressors as final similarity scores for semantic
similarity measurement task and chose the major
class label for entailment judgement task.
</bodyText>
<subsectionHeader confidence="0.972581">
5.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.993807875">
Table 4 lists the final results officially released by
the organizers in terms of Pearson and accuracy.
The best performance among these five systems is
shown in bold font. All participants can submit a
maximum of five runs for each task and only one
primary system is involved in official ranking. The
lower part of Table 4 presents the top 3 results and
the results with * are achieved by our systems.
</bodyText>
<table confidence="0.996143666666667">
System STS task TE task(%)
1 0.8279 83.641
2 0.8389 84.128
3 0.8414 83.945
4 0.8210 81.165
5 0.8349 83.986
rank 1st 0.8279* 84.575
rank 2nd 0.8272 83.641*
rank 3rd 0.8268 83.053
</table>
<tableCaption confidence="0.970814">
Table 4: The results of our five systems for two
tasks and the officially top-ranked systems.
</tableCaption>
<bodyText confidence="0.451176">
From this table, we found that (1) System 3 (us-
</bodyText>
<figure confidence="0.9984041875">
STS task
TE task
Pearson
Accuracy
para.
.807±.058
83.46±2.09
c=10
83.16±2.64
n=40
.805±.052
.806±.055
83.22±2.48
n=210
.797±.062
82.54±2.45
k=25
.765±.064
78.88±1.99
n=29
para.
c=100
n=30
n=140
k=17
n=15
Algorithm
SVM
RF
GB
kNN
SGD
</figure>
<page confidence="0.996639">
275
</page>
<bodyText confidence="0.999954033333333">
ing GB algorithm) and System 2 (using RF algo-
rithm) achieve the best performance among three
supervised systems in STS and TE task respec-
tively. However, there is no significant difference
among these systems. (2) Surprisingly, the semi-
supervised system (i.e., System 4) that employs
the co-training strategy to make use of test data
performs the worst, which is beyond our expecta-
tion. Based on our further observation in TE task,
the possible reason is that a lot of misclassified ex-
amples are added into the training pool in the ini-
tial iteration, which results in worse models built
in the subsequent iterations. And we speculate that
the weak learner kNN employed in CoReg may
lead to poor performance as well. (3) The major-
ity voting strategy fails to boost the performance
since GB and RF algorithm obtain the best perfor-
mance among these algorithms. (4) Our systems
obtain very good results on both STS and TE task,
i.e., we rank 1st out of 17 participants in STS task
and rank 2nd out of 18 participants in TE task ac-
cording to the results of primary systems and as
shown in Table 4 our primary system (i.e., System
1) do not achieve the best performance.
In a nutshell, our systems rank first and second
in STS and TE task respectively. Therefore the
answer to the first question raised in Section 1 is
yes. For two tasks, i.e., STS and TE, which are
very closely related but slightly different, we can
use the same features to solve them together.
</bodyText>
<subsectionHeader confidence="0.995658">
5.3 Feature Combination Experiments
</subsectionHeader>
<bodyText confidence="0.9839471">
To answer the second question and explore the in-
fluences of different feature types, we performed
a series of experiments under the best system set-
ting. Table 5 shows the results of different feature
combinations where for each time we selected and
added one best feature type. From this table, we
find that for STS the most effective feature is cps
and for TE task is td. Almost all feature types have
positive effects on performance. Specifically, td
alone achieves 81.063% in TE task which is quite
close to the best performance (84.128%) and cps
alone achieves 0.7544 in STS task. Moreover, the
td feature proposed for TE task is quite effective
in STS task as well, which suggests that text se-
mantic difference measures are also crucial when
measuring sentence similarity.
Therefore the answer to the second question is
yes. It is clear that the features proposed for TE are
also effective for STS and heterogenous features
yield better performance than a single feature type.
</bodyText>
<table confidence="0.997550533333333">
len st ss gr td str cps result
+ 0.7544 (STS)
+ + 0.8057(+5.13)
+ + + 0.8280(+2.23)
+ + + + 0.8365(+0.85)
+ + + + + 0.8426(+0.61)
+ + + + + + 0.8432(+0.06)
+ + + + + + + 0.8429(-0.03)
+ 81.063 (TE)
+ + 82.484(+1.421)
+ + + 82.992(+0.508)
+ + + + 83.844(+0.852)
+ + + + + 83.925(+0.081)
+ + + + + + 84.067(+0.142)
+ + + + + + + 84.128(+0.061)
</table>
<tableCaption confidence="0.99008">
Table 5: Results of feature combinations, the num-
</tableCaption>
<bodyText confidence="0.9261375">
bers in the brackets are the performance incre-
ments compared with the previous results.
</bodyText>
<sectionHeader confidence="0.999668" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999994142857143">
We set up five state-of-the-art systems and each
system employs different classifiers or regressors
using the same feature set. Our submitted systems
rank the 1st out of 17 teams in STS task with the
best performance of 0.8414 in terms of Pearson
coefficient and rank the 2nd out of 18 teams in
TE task with 84.128% in terms of accuracy. This
result indicates that (1) we can use the same fea-
ture set to solve these two tasks together, (2) the
features proposed for TE task are also effective
for STS task and (3) heterogenous features out-
perform a single feature. For future work, we may
explore the underlying relationships between these
two tasks to boost their performance by each other.
</bodyText>
<sectionHeader confidence="0.99816" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.981911">
This research is supported by grants from Na-
tional Natural Science Foundation of China
(No.60903093) and Shanghai Knowledge Service
Platform Project (No. ZF1213).
</bodyText>
<sectionHeader confidence="0.999191" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.98864025">
Ion Androutsopoulos and Prodromos Malakasiotis.
2009. A survey of paraphrasing and textual entail-
ment methods. arXiv preprint arXiv:0912.3747.
Carmen Banea, Samer Hassan, Michael Mohler, and
Rada Mihalcea. 2012. Unt:a supervised synergistic
approach to semantictext similarity. In First Joint
Conference on Lexical and Computational Seman-
tics (*SEM.
</reference>
<page confidence="0.99">
276
</page>
<reference confidence="0.999832383928572">
Daniel B¨ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. Ukp: Computing seman-
tic textual similarity by combining multiple content
similarity measures. In Proceedings of the First
Joint Conference on Lexical and Computational Se-
mantics, pages 435–440. Association for Computa-
tional Linguistics.
Johan Bos and Katja Markert. 2005. Recognising tex-
tual entailment with logical inference. In Proceed-
ings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 628–635. Association for Compu-
tational Linguistics.
Danilo Croce, Valerio Storch, and Roberto Basili.
2013. Unitor-core typed: Combining text similarity
and semantic filters through sv regression. In Pro-
ceedings of the 2nd Joint Conference on Lexical and
Computational Semantics, page 59.
Wael H Gomaa and Aly A Fahmy. 2013. A survey of
text similarity approaches. International Journal of
Computer Applications, 68(13):13–18.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1394–1404. Asso-
ciation for Computational Linguistics.
Weiwei Guo and Mona Diab. 2012. Modeling sen-
tences in the latent space. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics.
Zellig S Harris. 1954. Distributional structure. The
Philosophy of Linguistics,.
Yuhua Li, David McLean, Zuhair A Bandar, James D
O’shea, and Keeley Crockett. 2006. Sentence sim-
ilarity based on semantic nets and corpus statistics.
Knowledge and Data Engineering, IEEE Transac-
tions on, 18(8):1138–1150.
Mihai C. Lintean and Vasile Rus. 2012. Measuring se-
mantic similarity in short texts through greedy pair-
ing and word semantics. In FLAIRS Conference.
AAAI Press.
Prodromos Malakasiotis and Ion Androutsopoulos.
2007. Learning textual entailment using svms and
string similarity measures. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 42–47. Association for Com-
putational Linguistics.
M. Marelli, L. Bentivogli, M. Baroni, R. Bernardi,
S. Menini, and R. Zamparelli. 2014. Semeval-2014
task 1: Evaluation of compositional distributional
semantic models on full sentences through seman-
tic relatedness and textual entailment. In Proceed-
ings of SemEval 2014: International Workshop on
Semantic Evaluation.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In AAAI, vol-
ume 6, pages 775–780.
Fabian Pedregosa, Ga¨el. Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and ´Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825–2830.
Evan Sandhaus. 2008. The new york times annotated
corpus ldc2008t19. Philadelphia: Linguistic Data
Consortium.
Socher, Richard, Huval Brody, Manning Christopher,
and Ng Andrew. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, Jeju Island, Korea.
Peter D Turney, Patrick Pantel, et al. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37(1):141–188.
Frane ˇSari´c, Goran Glavaˇs, Mladen Karan, Jan ˇSnajder,
and Bojana Dalbelo Baˇsi´c. 2012. Takelab: Systems
for measuring semantic text similarity. In Proceed-
ings of the First Joint Conference on Lexical and
Computational Semantics, pages 441–448, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional distri-
butional semantics. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics, pages 1263–1271. Association for Computa-
tional Linguistics.
Jiang Zhao, Man Lan, and Zheng-Yu Niu. 2013. Ec-
nucs: Recognizing cross-lingual textual entailment
using multiple text similarity and text difference
measures. In Proceedings of the Seventh Interna-
tional Workshop on Semantic Evaluation (SemEval
2013), pages 118–123, Atlanta, Georgia, USA, June.
Association for Computational Linguistics.
Jiang Zhao, Man Lan, Zheng-Yu Niu, and Donghong
Ji. 2014. Recognizing cross-lingual textual entail-
ment with co-training using similarity and difference
views. In The 2014 International Joint Conference
on Neural Networks (IJCNN2014). IEEE.
Zhi-Hua Zhou and Ming Li. 2005. Semi-supervised
regression with co-training. In IJCAI, pages 908–
916.
Tian Tian Zhu and Man Lan. 2013. Ecnucs: Measur-
ing short text semantic equivalence using multiple
similarity measurements. In Proceedings of the 2nd
Joint Conference on Lexical and Computational Se-
mantics, page 124.
</reference>
<page confidence="0.997265">
277
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.603037">
<title confidence="0.9682355">ECNU: One Stone Two Birds: Ensemble of Heterogenous Measures Semantic Relatedness and Textual Entailment</title>
<author confidence="0.998061">Tian Tian Zhu Zhao</author>
<author confidence="0.998061">Man</author>
<affiliation confidence="0.8275555">Department of Computer Science and East China Normal</affiliation>
<abstract confidence="0.998996666666667">This paper presents our approach to semantic relatedness and textual entailment subtasks organized as task 1 in SemEval 2014. Specifically, we address two questions: (1) Can we solve these two subtasks together? (2) Are features proposed for textual entailment task still effective for semantic relatedness task? To address them, we extracted seven types of features including text difference measures proposed in entailment judgement subtask, as well as common text similarity measures used in both subtasks. Then we exploited the same feature set to solve the both subtasks by considering them as a regression and a classification task respectively and performed a study of influence of different features. We achieved the first and the second rank for relatedness and entailment task respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ion Androutsopoulos</author>
<author>Prodromos Malakasiotis</author>
</authors>
<title>A survey of paraphrasing and textual entailment methods. arXiv preprint arXiv:0912.3747.</title>
<date>2009</date>
<contexts>
<context position="3841" citStr="Androutsopoulos and Malakasiotis, 2009" startWordPosition="575" endWordPosition="578">s except that the concept of semantic similarity is more specific than semantic relatedness and the latter includes concepts as antonymy and meronymy. In this paper we regard the semantic relatedness task as a STS task. Besides, regardless of the original intention of this task, we adopted the mainstream machine learning methods instead of CDSMs to solve these two tasks by extracting heterogenous features. 271 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 271–277, Dublin, Ireland, August 23-24, 2014. Like semantic relatedness, TE task (surveyed in (Androutsopoulos and Malakasiotis, 2009)) is also closely related to STS task since in TE task lots of similarity measures at different levels are exploited to boost classification. For example, (Malakasiotis and Androutsopoulos, 2007) used ten string similarity measures such as cosine similarity at the word and the character level. Therefore, the first fundamental question arises, i.e., “Can we solve both of these two tasks together?” At the same time, since high similarity does not mean entailment holds, the TE task also utilizes other features besides similarity measures. For example, in our previous work (Zhao et al., 2014) text</context>
</contexts>
<marker>Androutsopoulos, Malakasiotis, 2009</marker>
<rawString>Ion Androutsopoulos and Prodromos Malakasiotis. 2009. A survey of paraphrasing and textual entailment methods. arXiv preprint arXiv:0912.3747.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carmen Banea</author>
<author>Samer Hassan</author>
<author>Michael Mohler</author>
<author>Rada Mihalcea</author>
</authors>
<title>Unt:a supervised synergistic approach to semantictext similarity.</title>
<date>2012</date>
<booktitle>In First Joint Conference on Lexical and Computational Semantics (*SEM.</booktitle>
<contexts>
<context position="14971" citStr="Banea et al., 2012" startWordPosition="2430" endWordPosition="2433">ich results in six features. 5https://catalog.ldc.upenn.edu/LDC2006T13 Co-occurrence retrieval model (CRM) The CRM word similarity is calculated as follows: 5imCRM(w1,w2) = |c(w1) |+ |c(w2)| where c(w) is the set of words that co-occur with word w. We used the 5-gram part of the Web 1T 5-gram Corpus to obtain c(w). We only considered the word w with |c(w) |&gt; T and then took the top 200 co-occurring words ranked by the cooccurrence frequency as its c(w). In our experiment, we set T = {50, 200}. To propagate the similarity from words to sentences, we adopted the best alignment strategy used in (Banea et al., 2012) to align two sentences. Vector space sentence similarity This feature set is taken from (ˇSari´c et al., 2012), which is based on distributional vectors of words. First we performed latent semantic analysis (LSA) over two corpora, i.e., the New York Times Annotated Corpus (NYT) (Sandhaus, 2008) and Wikipedia, to estimate the distributions of words. Then we used two strategies to convert the distributional meanings of words to sentence level: (i) simply summing up the distributional vector of each word w in the sentence, (ii) using the information content ic(w) to weigh the LSA vector of each </context>
</contexts>
<marker>Banea, Hassan, Mohler, Mihalcea, 2012</marker>
<rawString>Carmen Banea, Samer Hassan, Michael Mohler, and Rada Mihalcea. 2012. Unt:a supervised synergistic approach to semantictext similarity. In First Joint Conference on Lexical and Computational Semantics (*SEM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel B¨ar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>Ukp: Computing semantic textual similarity by combining multiple content similarity measures.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>435--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>B¨ar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. Ukp: Computing semantic textual similarity by combining multiple content similarity measures. In Proceedings of the First Joint Conference on Lexical and Computational Semantics, pages 435–440. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Katja Markert</author>
</authors>
<title>Recognising textual entailment with logical inference.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>628--635</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6398" citStr="Bos and Markert, 2005" startWordPosition="975" endWordPosition="978">formation gained from large corpora, including Latent Semantic Analysis (LSA), pointwise mutual information (PMI), etc. (3) knowledge-based method (Mihalcea et al., 2006) which estimates similarities with the aid of external resources, such as WordNet1; (4) hybrid lhttp://wordnet.princeton.edu/ method (Zhu and Lan, 2013; Croce et al., 2013) which integrates multiple similarity measures and adopts supervised machine learning algorithms to learn the different contributions of different features. The approaches to the task of TE can be roughly divided into two groups: (1) logic inference method (Bos and Markert, 2005) where automatic reasoning tools are used to check the logical representations derived from sentences and (2) machine learning method (Zhao et al., 2013; Gomaa and Fahmy, 2013) where a supervised model is built using a variety of similarity scores. Unlike previous work which separately addressed these two closely related tasks by using simple feature types, in this paper we endeavor to simultaneously solve these two tasks by using heterogenous features. 3 Our Systems We consider the two tasks as one by exploiting the same set of features but using different learning methods, i.e., classificati</context>
</contexts>
<marker>Bos, Markert, 2005</marker>
<rawString>Johan Bos and Katja Markert. 2005. Recognising textual entailment with logical inference. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 628–635. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Croce</author>
<author>Valerio Storch</author>
<author>Roberto Basili</author>
</authors>
<title>Unitor-core typed: Combining text similarity and semantic filters through sv regression.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2nd Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>59</pages>
<contexts>
<context position="6118" citStr="Croce et al., 2013" startWordPosition="933" endWordPosition="936"> (1) string-based method (B¨ar et al., 2012; Malakasiotis and Androutsopoulos, 2007) which calculates similarities using surface strings at either character level or word level; (2) corpus-based method (Li et al., 2006) which measures word or sentence similarities using the information gained from large corpora, including Latent Semantic Analysis (LSA), pointwise mutual information (PMI), etc. (3) knowledge-based method (Mihalcea et al., 2006) which estimates similarities with the aid of external resources, such as WordNet1; (4) hybrid lhttp://wordnet.princeton.edu/ method (Zhu and Lan, 2013; Croce et al., 2013) which integrates multiple similarity measures and adopts supervised machine learning algorithms to learn the different contributions of different features. The approaches to the task of TE can be roughly divided into two groups: (1) logic inference method (Bos and Markert, 2005) where automatic reasoning tools are used to check the logical representations derived from sentences and (2) machine learning method (Zhao et al., 2013; Gomaa and Fahmy, 2013) where a supervised model is built using a variety of similarity scores. Unlike previous work which separately addressed these two closely relat</context>
</contexts>
<marker>Croce, Storch, Basili, 2013</marker>
<rawString>Danilo Croce, Valerio Storch, and Roberto Basili. 2013. Unitor-core typed: Combining text similarity and semantic filters through sv regression. In Proceedings of the 2nd Joint Conference on Lexical and Computational Semantics, page 59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Wael</author>
</authors>
<title>Gomaa and Aly A Fahmy.</title>
<date>2013</date>
<journal>International Journal of Computer Applications,</journal>
<volume>68</volume>
<issue>13</issue>
<marker>Wael, 2013</marker>
<rawString>Wael H Gomaa and Aly A Fahmy. 2013. A survey of text similarity approaches. International Journal of Computer Applications, 68(13):13–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimental support for a categorical compositional distributional model of meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1394--1404</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2119" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="304" endWordPosition="307">y ignore grammatical structures and logical words. Compositional Distributional Semantic Models (CDSMs)(Zanzotto et al., 2010; Socher et This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ al., 2012) extend DSMs to sentence level to capture the compositionality in the semantic vector space, which has seen a rapidly growing interest in recent years. Although several CDSMs have been proposed, benchmarks are lagging behind. Previous work (Grefenstette and Sadrzadeh, 2011; Socher et al., 2012) performed experiments on their own datasets or on the same datasets which are limited to a few hundred instances of very short sentences with a fixed structure. To provide a benchmark so as to compare different CDSMs, the sentences involving compositional knowledge task in SemEval 2014 (Marelli et al., 2014) develops a large dataset which is full of lexical, syntactic and semantic phenomena. It consists of two subtasks: semantic relatedness task, which measures the degree of semantic relatedness of a sentence pair by assigning a relatedness score ranging from 1 (complete</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Experimental support for a categorical compositional distributional model of meaning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1394–1404. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>Modeling sentences in the latent space.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="10093" citStr="Guo and Diab, 2012" startWordPosition="1624" endWordPosition="1627">sed in our experiments. We also used three statistical correlation coefficients (i.e., Pearson, Spearmanr, Kendalltau) to measure similarity by regarding the vectorial representations as different variables. Thus we got ten features at last. 3.2.3 Semantic Similarity (ss) The above surface text similarity features only consider the surface words rather than their actual meanings in sentences. In order to build the semantic representations of sentences, we used a latent model to capture the contextual meanings of words. Specifically, we adopted the weighted textual matrix factorization (WTMF) (Guo and Diab, 2012) to model the semantics of sentences due to its reported good ability to model short texts. This model first factorizes the original termsentence matrix X into two matrices such that 3http://nlp.stanford.edu/software/tagger.shtml Xi,j Pz� PT*,i.Q*,j, where P*,i is a latent semantic vector profile for word wi and Q*,j is a vector profile that represents the sentence sj. Then we employed the new representations of sentences, i.e., Q, to calculate the semantic similarity between sentences using Cosine, Manhattan, Euclidean, Pearson, Spearmanr, Kendalltau measures respectively, which results in si</context>
</contexts>
<marker>Guo, Diab, 2012</marker>
<rawString>Weiwei Guo and Mona Diab. 2012. Modeling sentences in the latent space. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
</authors>
<title>Distributional structure. The Philosophy of Linguistics,.</title>
<date>1954</date>
<contexts>
<context position="1376" citStr="Harris, 1954" startWordPosition="202" endWordPosition="203">asures used in both subtasks. Then we exploited the same feature set to solve the both subtasks by considering them as a regression and a classification task respectively and performed a study of influence of different features. We achieved the first and the second rank for relatedness and entailment task respectively. 1 Introduction Distributional Semantic Models (DSMs)(surveyed in (Turney et al., 2010)) exploit the co-occurrences of other words with the word being modeled to compute the semantic meaning of the word under the distributional hypothesis: “similar words share similar contexts” (Harris, 1954). Despite their success, DSMs are severely limited to model the semantic of long phrases or sentences since they ignore grammatical structures and logical words. Compositional Distributional Semantic Models (CDSMs)(Zanzotto et al., 2010; Socher et This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ al., 2012) extend DSMs to sentence level to capture the compositionality in the semantic vector space, which has seen a rapidly growing inte</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig S Harris. 1954. Distributional structure. The Philosophy of Linguistics,.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuhua Li</author>
<author>David McLean</author>
<author>Zuhair A Bandar</author>
<author>James D O’shea</author>
<author>Keeley Crockett</author>
</authors>
<title>Sentence similarity based on semantic nets and corpus statistics. Knowledge and Data Engineering,</title>
<date>2006</date>
<journal>IEEE Transactions on,</journal>
<volume>18</volume>
<issue>8</issue>
<marker>Li, McLean, Bandar, O’shea, Crockett, 2006</marker>
<rawString>Yuhua Li, David McLean, Zuhair A Bandar, James D O’shea, and Keeley Crockett. 2006. Sentence similarity based on semantic nets and corpus statistics. Knowledge and Data Engineering, IEEE Transactions on, 18(8):1138–1150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai C Lintean</author>
<author>Vasile Rus</author>
</authors>
<title>Measuring semantic similarity in short texts through greedy pairing and word semantics.</title>
<date>2012</date>
<booktitle>In FLAIRS Conference.</booktitle>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="3113" citStr="Lintean and Rus, 2012" startWordPosition="468" endWordPosition="471">of lexical, syntactic and semantic phenomena. It consists of two subtasks: semantic relatedness task, which measures the degree of semantic relatedness of a sentence pair by assigning a relatedness score ranging from 1 (completely unrelated) to 5 (very related); and textual entailment (TE) task, which determines whether one of the following three relationships holds between two given sentences A and B: (1) entailment: the meaning of B can be inferred from A; (2) contradiction: A contradicts B; (3) neutral: the truth of B cannot be inferred on the basis of A. Semantic textual similarity (STS) (Lintean and Rus, 2012) and semantic relatedness are closely related and interchangeably used in many literatures except that the concept of semantic similarity is more specific than semantic relatedness and the latter includes concepts as antonymy and meronymy. In this paper we regard the semantic relatedness task as a STS task. Besides, regardless of the original intention of this task, we adopted the mainstream machine learning methods instead of CDSMs to solve these two tasks by extracting heterogenous features. 271 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 271–27</context>
</contexts>
<marker>Lintean, Rus, 2012</marker>
<rawString>Mihai C. Lintean and Vasile Rus. 2012. Measuring semantic similarity in short texts through greedy pairing and word semantics. In FLAIRS Conference. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prodromos Malakasiotis</author>
<author>Ion Androutsopoulos</author>
</authors>
<title>Learning textual entailment using svms and string similarity measures.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>42--47</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4036" citStr="Malakasiotis and Androutsopoulos, 2007" startWordPosition="604" endWordPosition="607">ness task as a STS task. Besides, regardless of the original intention of this task, we adopted the mainstream machine learning methods instead of CDSMs to solve these two tasks by extracting heterogenous features. 271 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 271–277, Dublin, Ireland, August 23-24, 2014. Like semantic relatedness, TE task (surveyed in (Androutsopoulos and Malakasiotis, 2009)) is also closely related to STS task since in TE task lots of similarity measures at different levels are exploited to boost classification. For example, (Malakasiotis and Androutsopoulos, 2007) used ten string similarity measures such as cosine similarity at the word and the character level. Therefore, the first fundamental question arises, i.e., “Can we solve both of these two tasks together?” At the same time, since high similarity does not mean entailment holds, the TE task also utilizes other features besides similarity measures. For example, in our previous work (Zhao et al., 2014) text difference features were proposed and proved to be effective. Therefore, the second question surfaces here, i.e., “Are features proposed for TE task still effective for STS task?” To answer the </context>
<context position="5583" citStr="Malakasiotis and Androutsopoulos, 2007" startWordPosition="855" endWordPosition="859">se two tasks. The rest of the paper is organized as follows. Section 2 briefly describes the related work on STS and TE tasks. Section 3 presents our systems including features, learning methods, etc. Section 4 shows the experimental results on training data and Section 5 reports the results of our submitted systems on test data and gives a detailed analysis. Finally, Section 6 concludes this paper with future work. 2 Related Work Existing work on STS can be divided into 4 categories according to the similarity measures used (Gomaa and Fahmy, 2013): (1) string-based method (B¨ar et al., 2012; Malakasiotis and Androutsopoulos, 2007) which calculates similarities using surface strings at either character level or word level; (2) corpus-based method (Li et al., 2006) which measures word or sentence similarities using the information gained from large corpora, including Latent Semantic Analysis (LSA), pointwise mutual information (PMI), etc. (3) knowledge-based method (Mihalcea et al., 2006) which estimates similarities with the aid of external resources, such as WordNet1; (4) hybrid lhttp://wordnet.princeton.edu/ method (Zhu and Lan, 2013; Croce et al., 2013) which integrates multiple similarity measures and adopts supervi</context>
</contexts>
<marker>Malakasiotis, Androutsopoulos, 2007</marker>
<rawString>Prodromos Malakasiotis and Ion Androutsopoulos. 2007. Learning textual entailment using svms and string similarity measures. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 42–47. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marelli</author>
<author>L Bentivogli</author>
<author>M Baroni</author>
<author>R Bernardi</author>
<author>S Menini</author>
<author>R Zamparelli</author>
</authors>
<title>Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment.</title>
<date>2014</date>
<booktitle>In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="2451" citStr="Marelli et al., 2014" startWordPosition="360" endWordPosition="363">/4.0/ al., 2012) extend DSMs to sentence level to capture the compositionality in the semantic vector space, which has seen a rapidly growing interest in recent years. Although several CDSMs have been proposed, benchmarks are lagging behind. Previous work (Grefenstette and Sadrzadeh, 2011; Socher et al., 2012) performed experiments on their own datasets or on the same datasets which are limited to a few hundred instances of very short sentences with a fixed structure. To provide a benchmark so as to compare different CDSMs, the sentences involving compositional knowledge task in SemEval 2014 (Marelli et al., 2014) develops a large dataset which is full of lexical, syntactic and semantic phenomena. It consists of two subtasks: semantic relatedness task, which measures the degree of semantic relatedness of a sentence pair by assigning a relatedness score ranging from 1 (completely unrelated) to 5 (very related); and textual entailment (TE) task, which determines whether one of the following three relationships holds between two given sentences A and B: (1) entailment: the meaning of B can be inferred from A; (2) contradiction: A contradicts B; (3) neutral: the truth of B cannot be inferred on the basis o</context>
</contexts>
<marker>Marelli, Bentivogli, Baroni, Bernardi, Menini, Zamparelli, 2014</marker>
<rawString>M. Marelli, L. Bentivogli, M. Baroni, R. Bernardi, S. Menini, and R. Zamparelli. 2014. Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In AAAI,</booktitle>
<volume>6</volume>
<pages>775--780</pages>
<contexts>
<context position="5946" citStr="Mihalcea et al., 2006" startWordPosition="908" endWordPosition="911"> concludes this paper with future work. 2 Related Work Existing work on STS can be divided into 4 categories according to the similarity measures used (Gomaa and Fahmy, 2013): (1) string-based method (B¨ar et al., 2012; Malakasiotis and Androutsopoulos, 2007) which calculates similarities using surface strings at either character level or word level; (2) corpus-based method (Li et al., 2006) which measures word or sentence similarities using the information gained from large corpora, including Latent Semantic Analysis (LSA), pointwise mutual information (PMI), etc. (3) knowledge-based method (Mihalcea et al., 2006) which estimates similarities with the aid of external resources, such as WordNet1; (4) hybrid lhttp://wordnet.princeton.edu/ method (Zhu and Lan, 2013; Croce et al., 2013) which integrates multiple similarity measures and adopts supervised machine learning algorithms to learn the different contributions of different features. The approaches to the task of TE can be roughly divided into two groups: (1) logic inference method (Bos and Markert, 2005) where automatic reasoning tools are used to check the logical representations derived from sentences and (2) machine learning method (Zhao et al., </context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In AAAI, volume 6, pages 775–780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Gramfort Varoquaux</author>
<author>Vincent Michel</author>
<author>Bertrand Thirion</author>
<author>Olivier Grisel</author>
<author>Mathieu Blondel</author>
<author>Peter Prettenhofer</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<location>Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David</location>
<marker>Varoquaux, Michel, Thirion, Grisel, Blondel, Prettenhofer, 2011</marker>
<rawString>Fabian Pedregosa, Ga¨el. Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and ´Edouard Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evan Sandhaus</author>
</authors>
<title>The new york times annotated corpus ldc2008t19. Philadelphia: Linguistic Data Consortium.</title>
<date>2008</date>
<contexts>
<context position="15267" citStr="Sandhaus, 2008" startWordPosition="2480" endWordPosition="2481"> to obtain c(w). We only considered the word w with |c(w) |&gt; T and then took the top 200 co-occurring words ranked by the cooccurrence frequency as its c(w). In our experiment, we set T = {50, 200}. To propagate the similarity from words to sentences, we adopted the best alignment strategy used in (Banea et al., 2012) to align two sentences. Vector space sentence similarity This feature set is taken from (ˇSari´c et al., 2012), which is based on distributional vectors of words. First we performed latent semantic analysis (LSA) over two corpora, i.e., the New York Times Annotated Corpus (NYT) (Sandhaus, 2008) and Wikipedia, to estimate the distributions of words. Then we used two strategies to convert the distributional meanings of words to sentence level: (i) simply summing up the distributional vector of each word w in the sentence, (ii) using the information content ic(w) to weigh the LSA vector of each word w and summing them up. Then we used cosine similarity to measure the similarity of two sentences. 3.3 Learning Algorithms We explored several classification algorithms to classify entailment relationships and regression algorithms to predict similarity scores using the above 72 features aft</context>
</contexts>
<marker>Sandhaus, 2008</marker>
<rawString>Evan Sandhaus. 2008. The new york times annotated corpus ldc2008t19. Philadelphia: Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Huval Brody</author>
<author>Manning Christopher</author>
<author>Ng Andrew</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP, Jeju Island,</booktitle>
<contexts>
<context position="2141" citStr="Socher et al., 2012" startWordPosition="308" endWordPosition="311">d logical words. Compositional Distributional Semantic Models (CDSMs)(Zanzotto et al., 2010; Socher et This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ al., 2012) extend DSMs to sentence level to capture the compositionality in the semantic vector space, which has seen a rapidly growing interest in recent years. Although several CDSMs have been proposed, benchmarks are lagging behind. Previous work (Grefenstette and Sadrzadeh, 2011; Socher et al., 2012) performed experiments on their own datasets or on the same datasets which are limited to a few hundred instances of very short sentences with a fixed structure. To provide a benchmark so as to compare different CDSMs, the sentences involving compositional knowledge task in SemEval 2014 (Marelli et al., 2014) develops a large dataset which is full of lexical, syntactic and semantic phenomena. It consists of two subtasks: semantic relatedness task, which measures the degree of semantic relatedness of a sentence pair by assigning a relatedness score ranging from 1 (completely unrelated) to 5 (ve</context>
</contexts>
<marker>Socher, Brody, Christopher, Andrew, 2012</marker>
<rawString>Socher, Richard, Huval Brody, Manning Christopher, and Ng Andrew. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of EMNLP, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of artificial intelligence research,</journal>
<volume>37</volume>
<issue>1</issue>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D Turney, Patrick Pantel, et al. 2010. From frequency to meaning: Vector space models of semantics. Journal of artificial intelligence research, 37(1):141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frane ˇSari´c</author>
<author>Goran Glavaˇs</author>
<author>Mladen Karan</author>
<author>Jan ˇSnajder</author>
<author>Bojana Dalbelo Baˇsi´c</author>
</authors>
<title>Takelab: Systems for measuring semantic text similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>441--448</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>ˇSari´c, Glavaˇs, Karan, ˇSnajder, Baˇsi´c, 2012</marker>
<rawString>Frane ˇSari´c, Goran Glavaˇs, Mladen Karan, Jan ˇSnajder, and Bojana Dalbelo Baˇsi´c. 2012. Takelab: Systems for measuring semantic text similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics, pages 441–448, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Ioannis Korkontzelos</author>
<author>Francesca Fallucchi</author>
<author>Suresh Manandhar</author>
</authors>
<title>Estimating linear models for compositional distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>1263--1271</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1612" citStr="Zanzotto et al., 2010" startWordPosition="232" endWordPosition="235"> We achieved the first and the second rank for relatedness and entailment task respectively. 1 Introduction Distributional Semantic Models (DSMs)(surveyed in (Turney et al., 2010)) exploit the co-occurrences of other words with the word being modeled to compute the semantic meaning of the word under the distributional hypothesis: “similar words share similar contexts” (Harris, 1954). Despite their success, DSMs are severely limited to model the semantic of long phrases or sentences since they ignore grammatical structures and logical words. Compositional Distributional Semantic Models (CDSMs)(Zanzotto et al., 2010; Socher et This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ al., 2012) extend DSMs to sentence level to capture the compositionality in the semantic vector space, which has seen a rapidly growing interest in recent years. Although several CDSMs have been proposed, benchmarks are lagging behind. Previous work (Grefenstette and Sadrzadeh, 2011; Socher et al., 2012) performed experiments on their own datasets or on the same datasets wh</context>
</contexts>
<marker>Zanzotto, Korkontzelos, Fallucchi, Manandhar, 2010</marker>
<rawString>Fabio Massimo Zanzotto, Ioannis Korkontzelos, Francesca Fallucchi, and Suresh Manandhar. 2010. Estimating linear models for compositional distributional semantics. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 1263–1271. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Zhao</author>
<author>Man Lan</author>
<author>Zheng-Yu Niu</author>
</authors>
<title>Ecnucs: Recognizing cross-lingual textual entailment using multiple text similarity and text difference measures.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>118--123</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="6550" citStr="Zhao et al., 2013" startWordPosition="999" endWordPosition="1002">et al., 2006) which estimates similarities with the aid of external resources, such as WordNet1; (4) hybrid lhttp://wordnet.princeton.edu/ method (Zhu and Lan, 2013; Croce et al., 2013) which integrates multiple similarity measures and adopts supervised machine learning algorithms to learn the different contributions of different features. The approaches to the task of TE can be roughly divided into two groups: (1) logic inference method (Bos and Markert, 2005) where automatic reasoning tools are used to check the logical representations derived from sentences and (2) machine learning method (Zhao et al., 2013; Gomaa and Fahmy, 2013) where a supervised model is built using a variety of similarity scores. Unlike previous work which separately addressed these two closely related tasks by using simple feature types, in this paper we endeavor to simultaneously solve these two tasks by using heterogenous features. 3 Our Systems We consider the two tasks as one by exploiting the same set of features but using different learning methods, i.e., classification and regression. Seven types of features are extracted and most of them are based on our previous work on TE (Zhao et al., 2014) and STS (Zhu and Lan,</context>
<context position="10933" citStr="Zhao et al., 2013" startWordPosition="1749" endWordPosition="1752">.shtml Xi,j Pz� PT*,i.Q*,j, where P*,i is a latent semantic vector profile for word wi and Q*,j is a vector profile that represents the sentence sj. Then we employed the new representations of sentences, i.e., Q, to calculate the semantic similarity between sentences using Cosine, Manhattan, Euclidean, Pearson, Spearmanr, Kendalltau measures respectively, which results in six features. 3.2.4 Grammatical Relationship (gr) The grammatical relationship feature measures the semantic similarity between two sentences at the grammar level and this feature type was also explored in our previous work (Zhao et al., 2013; Zhu and Lan, 2013). We used Stanford Parser4 to acquire the dependency information from sentences and the grammatical information are represented in the form of relation unit, e.g. nsubj(example, this), where nsubj stands for a dependency relationship between example and this. We obtained a sequence of relation units for each sentence and then used them to estimate similarity by adopting eight measure functions described in Section 3.2.1, resulting in eight features. 3.2.5 Text Difference Measures (td) There are two types of text difference measures. The first feature type is specially desig</context>
</contexts>
<marker>Zhao, Lan, Niu, 2013</marker>
<rawString>Jiang Zhao, Man Lan, and Zheng-Yu Niu. 2013. Ecnucs: Recognizing cross-lingual textual entailment using multiple text similarity and text difference measures. In Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 118–123, Atlanta, Georgia, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Zhao</author>
<author>Man Lan</author>
<author>Zheng-Yu Niu</author>
<author>Donghong Ji</author>
</authors>
<title>Recognizing cross-lingual textual entailment with co-training using similarity and difference views.</title>
<date>2014</date>
<booktitle>In The 2014 International Joint Conference on Neural Networks (IJCNN2014).</booktitle>
<publisher>IEEE.</publisher>
<contexts>
<context position="4436" citStr="Zhao et al., 2014" startWordPosition="671" endWordPosition="674">and Malakasiotis, 2009)) is also closely related to STS task since in TE task lots of similarity measures at different levels are exploited to boost classification. For example, (Malakasiotis and Androutsopoulos, 2007) used ten string similarity measures such as cosine similarity at the word and the character level. Therefore, the first fundamental question arises, i.e., “Can we solve both of these two tasks together?” At the same time, since high similarity does not mean entailment holds, the TE task also utilizes other features besides similarity measures. For example, in our previous work (Zhao et al., 2014) text difference features were proposed and proved to be effective. Therefore, the second question surfaces here, i.e., “Are features proposed for TE task still effective for STS task?” To answer the first question, we extracted seven types of features including text similarity and text difference and then fed them to classifiers and regressors to solve TE and STS task respectively. Regarding the second question, we conducted a series of experiments to study the performance of different features for these two tasks. The rest of the paper is organized as follows. Section 2 briefly describes the</context>
<context position="7128" citStr="Zhao et al., 2014" startWordPosition="1096" endWordPosition="1099">chine learning method (Zhao et al., 2013; Gomaa and Fahmy, 2013) where a supervised model is built using a variety of similarity scores. Unlike previous work which separately addressed these two closely related tasks by using simple feature types, in this paper we endeavor to simultaneously solve these two tasks by using heterogenous features. 3 Our Systems We consider the two tasks as one by exploiting the same set of features but using different learning methods, i.e., classification and regression. Seven types of features are extracted and most of them are based on our previous work on TE (Zhao et al., 2014) and STS (Zhu and Lan, 2013). Many learning algorithms and parameters are examined and the final submitted systems are configured according to the preliminary results on training data. 3.1 Preprocessing Three text preprocessing operations were performed before we extracted features, which included: (1) we converted the contractions to their formal writings, for example, doesn’t is rewritten as does not. (2) the WordNet-based Lemmatizer implemented in Natural Language Toolkit2 was used to lemmatize all words to their nearest base forms in WordNet, for example, was is lemmatized to be. (3) we re</context>
<context position="16895" citStr="Zhao et al., 2014" startWordPosition="2737" endWordPosition="2740"> in the form of an ensemble of weak prediction models, k-nearest neighbors (kNN) that decides the class labels with the aid of the classes of k nearest neighbors, and Stochastic Gradient Descent (SGD) which uses SGD technique to minimize loss functions. These supervised learning methods are implemented in scikit-learn toolkit (Pedregosa et al., 2011). Besides, we also used a semi-supervised learning strategy for both tasks 5imwwo(s1, s2) = 2 ∗ |c(w1) ∩ c(w2)| 274 in order to make full use of unlabeled test data. Specifically, the co-training algorithm was used to address TE task according to (Zhao et al., 2014). Its strategy is to train two classifiers with two data views and to add the top confident predicted instances by one classifier to expand the training set of another classifier and then to re-train the two classifiers on the expanded training sets. For STS task, we utilized CoReg algorithm (Zhou and Li, 2005) which uses two kNN regressors to perform co-training paradigm. 3.4 Evaluation Measures In order to evaluate the performance of different algorithms, we adopted the official evaluation measures, i.e., Pearson correlation coefficient for STS task and accuracy for TE task. 4 Experiments on</context>
</contexts>
<marker>Zhao, Lan, Niu, Ji, 2014</marker>
<rawString>Jiang Zhao, Man Lan, Zheng-Yu Niu, and Donghong Ji. 2014. Recognizing cross-lingual textual entailment with co-training using similarity and difference views. In The 2014 International Joint Conference on Neural Networks (IJCNN2014). IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi-Hua Zhou</author>
<author>Ming Li</author>
</authors>
<title>Semi-supervised regression with co-training.</title>
<date>2005</date>
<booktitle>In IJCAI,</booktitle>
<pages>908--916</pages>
<contexts>
<context position="17207" citStr="Zhou and Li, 2005" startWordPosition="2791" endWordPosition="2794">t-learn toolkit (Pedregosa et al., 2011). Besides, we also used a semi-supervised learning strategy for both tasks 5imwwo(s1, s2) = 2 ∗ |c(w1) ∩ c(w2)| 274 in order to make full use of unlabeled test data. Specifically, the co-training algorithm was used to address TE task according to (Zhao et al., 2014). Its strategy is to train two classifiers with two data views and to add the top confident predicted instances by one classifier to expand the training set of another classifier and then to re-train the two classifiers on the expanded training sets. For STS task, we utilized CoReg algorithm (Zhou and Li, 2005) which uses two kNN regressors to perform co-training paradigm. 3.4 Evaluation Measures In order to evaluate the performance of different algorithms, we adopted the official evaluation measures, i.e., Pearson correlation coefficient for STS task and accuracy for TE task. 4 Experiments on Training Data To make a reasonable comparison between different algorithms, we performed 5-fold cross validation on training data with 5000 sentence pairs. The parameters tuned in different algorithms are listed below: the trade-off parameter c in SVM, the number of trees n in RF, the number of boosting stages</context>
</contexts>
<marker>Zhou, Li, 2005</marker>
<rawString>Zhi-Hua Zhou and Ming Li. 2005. Semi-supervised regression with co-training. In IJCAI, pages 908– 916.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tian Tian Zhu</author>
<author>Man Lan</author>
</authors>
<title>Ecnucs: Measuring short text semantic equivalence using multiple similarity measurements.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2nd Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>124</pages>
<contexts>
<context position="6097" citStr="Zhu and Lan, 2013" startWordPosition="929" endWordPosition="932">a and Fahmy, 2013): (1) string-based method (B¨ar et al., 2012; Malakasiotis and Androutsopoulos, 2007) which calculates similarities using surface strings at either character level or word level; (2) corpus-based method (Li et al., 2006) which measures word or sentence similarities using the information gained from large corpora, including Latent Semantic Analysis (LSA), pointwise mutual information (PMI), etc. (3) knowledge-based method (Mihalcea et al., 2006) which estimates similarities with the aid of external resources, such as WordNet1; (4) hybrid lhttp://wordnet.princeton.edu/ method (Zhu and Lan, 2013; Croce et al., 2013) which integrates multiple similarity measures and adopts supervised machine learning algorithms to learn the different contributions of different features. The approaches to the task of TE can be roughly divided into two groups: (1) logic inference method (Bos and Markert, 2005) where automatic reasoning tools are used to check the logical representations derived from sentences and (2) machine learning method (Zhao et al., 2013; Gomaa and Fahmy, 2013) where a supervised model is built using a variety of similarity scores. Unlike previous work which separately addressed th</context>
<context position="10953" citStr="Zhu and Lan, 2013" startWordPosition="1753" endWordPosition="1756">,i.Q*,j, where P*,i is a latent semantic vector profile for word wi and Q*,j is a vector profile that represents the sentence sj. Then we employed the new representations of sentences, i.e., Q, to calculate the semantic similarity between sentences using Cosine, Manhattan, Euclidean, Pearson, Spearmanr, Kendalltau measures respectively, which results in six features. 3.2.4 Grammatical Relationship (gr) The grammatical relationship feature measures the semantic similarity between two sentences at the grammar level and this feature type was also explored in our previous work (Zhao et al., 2013; Zhu and Lan, 2013). We used Stanford Parser4 to acquire the dependency information from sentences and the grammatical information are represented in the form of relation unit, e.g. nsubj(example, this), where nsubj stands for a dependency relationship between example and this. We obtained a sequence of relation units for each sentence and then used them to estimate similarity by adopting eight measure functions described in Section 3.2.1, resulting in eight features. 3.2.5 Text Difference Measures (td) There are two types of text difference measures. The first feature type is specially designed for the contradi</context>
<context position="12628" citStr="Zhu and Lan, 2013" startWordPosition="2025" endWordPosition="2028">ed. The second feature type is extracted from two word sets A−B and B −A as follows: we first calculated the similarities between every word from A − B and every word from B − A , then took the maximum, minimum and average value of them as features. In our experiments, four WordNet-based similarity measures (i.e., path, lch, wup, jcn (Gomaa and Fahmy, 2013)) were used to calculate the similarity between two words. Totally, we got 13 text difference features. 4http://nlp.stanford.edu/software/lex-parser.shtml 273 3.2.6 String Features (str) This set of features is taken from our previous work (Zhu and Lan, 2013) due to its superior performance. Longest common sequence (LCS) We computed the LCS similarity on the original and lemmatized sentences. It was calculated by finding the maximum length of a common contiguous subsequence of two strings and then dividing it by the smaller length of two strings to eliminate the impacts of length imbalance. Jaccard similarity using n-grams We obtained n-grams at three different levels, i.e., the original word level, the lemmatized word level and the character level. Then these n-grams were used for calculating Jaccard similarity defined in Table 1. In our experime</context>
<context position="14268" citStr="Zhu and Lan, 2013" startWordPosition="2312" endWordPosition="2315"> the frequency of the word w in the corpus. To compute ic(w), we used the Web 1T 5-gram Corpus 5. Then the WWO similarity of two sentence s1 and s2 was calculated as follows: ∑ wEs,ns2 ic(w) ∑w′Es2 ic(wl) Due to its asymmetry, we used the harmonic mean of 5imwwo(s1, s2) and 5imwwo(s2, s1) as the final WWO similarity. The WWO similarity is calculated on the original and lemmatized strings respectively. Finally, we got two LCS features, nine Jaccard n-gram features and two WWO features. 3.2.7 Corpus-based Features (cps) Two types of corpus-based feature are also borrowed from our previous work (Zhu and Lan, 2013), i.e., vector space sentence similarity and co-occurrence retrieval model (CRM), which results in six features. 5https://catalog.ldc.upenn.edu/LDC2006T13 Co-occurrence retrieval model (CRM) The CRM word similarity is calculated as follows: 5imCRM(w1,w2) = |c(w1) |+ |c(w2)| where c(w) is the set of words that co-occur with word w. We used the 5-gram part of the Web 1T 5-gram Corpus to obtain c(w). We only considered the word w with |c(w) |&gt; T and then took the top 200 co-occurring words ranked by the cooccurrence frequency as its c(w). In our experiment, we set T = {50, 200}. To propagate the </context>
</contexts>
<marker>Zhu, Lan, 2013</marker>
<rawString>Tian Tian Zhu and Man Lan. 2013. Ecnucs: Measuring short text semantic equivalence using multiple similarity measurements. In Proceedings of the 2nd Joint Conference on Lexical and Computational Semantics, page 124.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>