<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000186">
<title confidence="0.9992375">
Exploiting Full Parsing Information to Label Semantic Roles Using an
Ensemble of ME and SVM via Integer Linear Programming
</title>
<author confidence="0.999146">
Tzong-Han Tsai, Chia-Wei Wu, Yu-Chun Lin, Wen-Lian Hsu
</author>
<affiliation confidence="0.998711">
Institute of Information Science
</affiliation>
<address confidence="0.491886">
Academia Sinica
Taipei 115, Taiwan
</address>
<email confidence="0.860713">
{thtsai, cwwu, sbb, hsu}@iis.sinica.edu.tw
</email>
<sectionHeader confidence="0.995385" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999869380952381">
In this paper, we propose a method that
exploits full parsing information by repre-
senting it as features of argument classifi-
cation models and as constraints in integer
linear learning programs. In addition, to
take advantage of SVM-based and Maxi-
mum Entropy-based argument classifica-
tion models, we incorporate their scoring
matrices, and use the combined matrix in
the above-mentioned integer linear pro-
grams. The experimental results show that
full parsing information not only in-
creases the F-score of argument classifi-
cation models by 0.7%, but also
effectively removes all labeling inconsis-
tencies, which increases the F-score by
0.64%. The ensemble of SVM and ME
also boosts the F-score by 0.77%. Our
system achieves an F-score of 76.53% in
the development set and 76.38% in Test
WSJ.
</bodyText>
<sectionHeader confidence="0.999127" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999895638888889">
The Semantic Role Labeling problem can be for-
mulated as a sentence tagging problem. A sentence
can be represented as a sequence of words, as
phrases (chunks), or as a parsing tree. The basic
units of a sentence are words, phrases, and con-
stituents in these representations, respectively..
Pradhan et al. (2004) established that Constituent-
by-Constituent (C-by-C) is better than Phrase-by-
Phrase (P-by-P), which is better than Word-by-
Word (W-by-W). This is probably because the
boundaries of the constituents coincide with the
arguments; therefore, C-by-C has the highest ar-
gument identification F-score among the three ap-
proaches.
In addition, a full parsing tree also provides
richer syntactic information than a sequence of
chunks or words. Pradhan et al. (2004) compared
the seven most common features as well as several
features related to the target constituent’s parent
and sibling constituents. Their experimental results
show that using other constituents’ information
increases the F-score by 6%. Punyakanok et al.
(2004) represent full parsing information as con-
straints in integer linear programs. Their experi-
mental results show that using such information
increases the argument classification accuracy by
1%.
In this paper, we not only add more full parsing
features to argument classification models, but also
represent full parsing information as constraints in
integer linear programs (ILP) to resolve label in-
consistencies. We also build an ensemble of two
argument classification models: Maximum Entropy
and SVM by combining their argument classifica-
tion results and applying them to the above-
mentioned ILPs.
</bodyText>
<sectionHeader confidence="0.922587" genericHeader="method">
2 System Architecture
</sectionHeader>
<bodyText confidence="0.999150166666667">
Our SRL system is comprised of four stages: prun-
ing, argument classification, classification model
incorporation, and integer linear programming.
This section describes how we build these stages,
including the features used in training the argu-
ment classification models.
</bodyText>
<subsectionHeader confidence="0.857394">
2.1 Pruning
</subsectionHeader>
<page confidence="0.907414">
233
</page>
<note confidence="0.4610015">
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 233–236, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.999984956521739">
When the full parsing tree of a sentence is avail-
able, only the constituents in the tree are consid-
ered as argument candidates. In CoNLL-2005, full
parsing trees are provided by two full parsers: the
Collins parser (Collins, 1999) and the Charniak
parser (Charniak, 2000). According to Punyakanok
et al. (2005), the boundary agreement of Charniak
is higher than that of Collins; therefore, we choose
the Charniak parser’s results. However, there are
two million nodes on the full parsing trees in the
training corpus, which makes the training time of
machine learning algorithms extremely long. Be-
sides, noisy information from unrelated parts of a
sentence could also affect the training of machine
learning models. Therefore, our system exploits the
heuristic rules introduced by Xue and Palmer
(2004) to filter out simple constituents that are
unlikely to be arguments. Applying pruning heuris-
tics to the output of Charniak’s parser effectively
eliminates 61% of the training data and 61.3% of
the development data, while still achieves 93% and
85.5% coverage of the correct arguments in the
training and development sets, respectively.
</bodyText>
<subsectionHeader confidence="0.9573265">
2.2 Argument Classification
Full Parsing Features
</subsectionHeader>
<listItem confidence="0.8382746">
• Sub-categorization – The phrase structure
rule that expands the predicate’s parent
node in the parsing tree.
• First and Last Word/POS
• Named Entities – LOC, ORG, PER, and
MISC.
• Level – The level in the parsing tree.
Combination Features
• Predicate Distance Combination
• Predicate Phrase Type Combination
• Head Word and Predicate Combination
• Voice Position Combination
Context Features
• Context Word/POS – The two words pre-
ceding and the two words following the
target phrase, as well as their correspond-
ing POSs.
• Context Chunk Type – The two chunks
preceding and the two chunks following
the target phrase.
</listItem>
<bodyText confidence="0.999866375">
This stage assigns the final labels to the candidates
derived in Section 2.1. A multi-class classifier is
trained to classify the types of the arguments sup-
plied by the pruning stage. In addition, to reduce
the number of excess candidates mistakenly output
by the previous stage, these candidates can be la-
beled as null (meaning “not an argument”). The
features used in this stage are as follows.
</bodyText>
<subsectionHeader confidence="0.395153">
Basic Features
</subsectionHeader>
<listItem confidence="0.999808928571429">
• Predicate – The predicate lemma.
• Path – The syntactic path through the
parsing tree from the parse constituent be-
ing classified to the predicate.
• Constituent Type
• Position – Whether the phrase is located
before or after the predicate.
• Voice – passive: if the predicate has a POS
tag VBN, and its chunk is not a VP, or it is
preceded by a form of “to be” or “to get”
within its chunk; otherwise, it is active.
• Head Word – calculated using the head
word table described by Collins (1999).
• Head POS – The POS of the Head Word.
</listItem>
<bodyText confidence="0.999955625">
We believe that information from related constitu-
ents in the full parsing tree helps in labeling the
target constituent. Denote the target constituent by
t. The following features are the most common
baseline features of t’s parent and sibling constitu-
ents. For example, Parent/ Left Sibling/ Right Sib-
ling Path denotes t’s parents’, left sibling’s, and
right sibling’s Path features.
</bodyText>
<listItem confidence="0.996844">
• Parent / Left Sibling / Right Sibling
Path
• Parent / Left Sibling / Right Sibling
Constituent Type
• Parent / Left Sibling / Right Sibling Po-
sition
• Parent / Left Sibling / Right Sibling
Head Word
• Parent / Left Sibling / Right Sibling
Head POS
• Head of PP parent – If the parent is a PP,
then the head of this PP is also used as a
feature.
</listItem>
<sectionHeader confidence="0.829536" genericHeader="method">
Argument Classification Models
</sectionHeader>
<page confidence="0.986738">
234
</page>
<bodyText confidence="0.999926857142857">
We use all the features of the SVM-based and ME-
based argument classification models. All SVM
classifiers are realized using SVM-Light with a
polynomial kernel of degree 2. The ME-based
model is implemented based on Zhang’s MaxEnt
toolkit1 and L-BFGS (Nocedal and Wright, 1999)
method to perform parameter estimation.
</bodyText>
<subsectionHeader confidence="0.968829">
2.3 Classification Model Incorporation
</subsectionHeader>
<bodyText confidence="0.999687666666667">
We now explain how we incorporate the SVM-
based and ME-based argument classification mod-
els. After argument classification, we acquire two
scoring matrices, PME and PSVM, respectively. In-
corporation of these two models is realized by
weighted summation of PME and PSVM as follows:
</bodyText>
<equation confidence="0.607994">
P’ = wMEPME + wSVMPSVM
</equation>
<bodyText confidence="0.978209">
We use P’ for the objective coefficients of the
ILP described in Section 2.4.
</bodyText>
<subsectionHeader confidence="0.904118">
2.4 Integer Linear Programming (ILP)
</subsectionHeader>
<bodyText confidence="0.9937405">
To represent full parsing information as features,
there are still several syntactic constraints on a
parsing tree in the SRL problem. For example, on a
path of the parsing tree, there can be only one con-
stituent annotated as a non-null argument. How-
ever, it is difficult to encode this constraint in the
argument classification models. Therefore, we ap-
ply integer linear programming to resolve inconsis-
tencies produced in the argument classification
stage.
According to Punyakanok et al. (2004), given a
set of constituents, S, and a set of semantic role
labels, A, the SRL problem can be formulated as
an ILP as follows:
Let zia be the indicator variable that represents
whether or not an argument, a, is assigned to any
Si ∈ S; and let pia = score(Si = a). The scoring ma-
trix P composed of all pia is calculated by the ar-
gument classification models. The goal of this ILP
is to find a set of assignments for all zia that maxi-
mizes the following function:
piazia .
Each Si∈ S should have one of these argument
types, or no type (null). Therefore, we have
zia =1.
Next, we show how to transform the constraints in
</bodyText>
<footnote confidence="0.742004">
1 http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.html
</footnote>
<bodyText confidence="0.981711875">
the filter function into linear equalities or inequali-
ties, and use them in this ILP.
Constraint I: No overlapping or embedding
For arguments Sj1 , . . . , Sjk on the same path in a
full parsing tree, only one argument can be as-
signed to an argument type. Thus, at least k − 1
arguments will be null, which is represented by φ
in the following linear equality:
</bodyText>
<equation confidence="0.994787833333333">
k
≥ −
i
∑= z φ
j k
i 1
</equation>
<bodyText confidence="0.992066666666667">
Constraint II: No duplicate argument classes
Within the same sentence, A0-A5 cannot appear
more than once. The inequality for A0 is therefore:
</bodyText>
<equation confidence="0.997777333333333">
k
ziA0 1.
i=1
</equation>
<sectionHeader confidence="0.416176" genericHeader="method">
Constraint III: R-XXX arguments
</sectionHeader>
<bodyText confidence="0.98536">
The linear inequalities that represent A0 and its
reference type R-A0 are:
</bodyText>
<equation confidence="0.90497175">
k
ziA0 ≥ zmR−
i=1
Constraint IV: C-XXX arguments
</equation>
<bodyText confidence="0.9986535">
The continued argument XXX has to occur before
C-XXX. The linear inequalities for A0 are:
</bodyText>
<equation confidence="0.934943625">
m 1
∀ ∈
m {2,..., }:
M ∑ zj i z m
A0≥ C A0
− .
1
Constraint V: Illegal arguments
</equation>
<bodyText confidence="0.995443666666667">
For each verb, we look up its allowed roles. This
constraint is represented by summing all the corre-
sponding indicator variables to 0.
</bodyText>
<sectionHeader confidence="0.997512" genericHeader="evaluation">
3 Experiment Results
</sectionHeader>
<subsectionHeader confidence="0.999909">
3.1 Data and Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999978142857143">
The data, which is part of the PropBank corpus,
consists of sections from the Wall Street Journal
part of the Penn Treebank. All experiments were
carried out using Section 2 to Section 21 for train-
ing, Section 24 for development, and Section 23
for testing. Unlike CoNLL-2004, part of the Brown
corpus is also included in the test set.
</bodyText>
<subsectionHeader confidence="0.93481">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.9973386">
Table 1 shows that our system makes little differ-
ence to the development set and Test WSJ. How-
ever, due to the intrinsic difference between the
WSJ and Brown corpora, our system performs bet-
ter on Test WSJ than on Test Brown.
</bodyText>
<figure confidence="0.980592083333334">
∑∑
Si∈ Sa∈A
∑
a A
∈
1 .
∀ m ∈ {1, . . ., } :
M
.
A0
i
=
</figure>
<page confidence="0.991329">
235
</page>
<table confidence="0.999996131578947">
Precision Recall Fβ_1
Development 81.13% 72.42% 76.53
Test WSJ 82.77% 70.90% 76.38
Test Brown 73.21% 59.49% 65.64
Test WSJ+Brown 81.55% 69.37% 74.97
Test WSJ Precision Recall Fβ_1
Overall 82.77% 70.90% 76.38
A0 88.25% 84.93% 86.56
A1 82.21% 72.21% 76.89
A2 74.68% 52.34% 61.55
A3 78.30% 47.98% 59.50
A4 84.29% 57.84% 68.60
A5 100.00% 60.00% 75.00
AM-ADV 64.19% 47.83% 54.81
AM-CAU 70.00% 38.36% 49.56
AM-DIR 38.20% 40.00% 39.08
AM-DIS 83.33% 71.88% 77.18
AM-EXT 86.67% 40.62% 55.32
AM-LOC 63.71% 41.60% 50.33
AM-MNR 63.36% 48.26% 54.79
AM-MOD 98.00% 97.64% 97.82
AM-NEG 99.53% 92.61% 95.95
AM-PNC 44.44% 17.39% 25.00
AM-PRD 50.00% 20.00% 28.57
AM-REC 0.00% 0.00% 0.00
AM-TMP 83.21% 61.09% 70.45
R-A0 91.08% 86.61% 88.79
R-A1 79.49% 79.49% 79.49
R-A2 87.50% 43.75% 58.33
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 100.00% 25.00% 40.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 92.31% 57.14% 70.59
R-AM-MNR 25.00% 16.67% 20.00
R-AM-TMP 72.73% 61.54% 66.67
V 97.32% 97.32% 97.32
</table>
<tableCaption confidence="0.991416">
Table 1. Overall results (top) and detailed results
on the WSJ test (bottom).
</tableCaption>
<table confidence="0.999981166666667">
Precision Recall Fβ_1
ME w/o parsing 77.28% 70.55% 73.76%
ME 78.19% 71.08% 74.46%
ME with ILP 79.57% 71.11% 75.10%
SVM 79.88% 72.03% 75.76%
Hybrid 81.13% 72.42% 76.53%
</table>
<tableCaption confidence="0.945844">
Table 2. Results of all configurations on the devel-
opment set.
</tableCaption>
<bodyText confidence="0.99922675">
From Table 2, we can see that the model with
full parsing features outperforms the model with-
out the features in all three performance matrices.
After applying ILP, the performance is improved
further. We also observe that SVM slightly outper-
forms ME. However, the hybrid argument classifi-
cation model achieves the best results in all three
metrics.
</bodyText>
<sectionHeader confidence="0.999078" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999984833333334">
In this paper, we add more full parsing features to
argument classification models, and represent full
parsing information as constraints in ILPs to re-
solve labeling inconsistencies. We also integrate
two argument classification models, ME and SVM,
by combining their argument classification results
and applying them to the above-mentioned ILPs.
The results show full parsing information increases
the total F-score by 1.34%. The ensemble of SVM
and ME also boosts the F-score by 0.77%. Finally,
our system achieves an F-score of 76.53% in the
development set and 76.38% in Test WSJ.
</bodyText>
<sectionHeader confidence="0.987618" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999928">
We are indebted to Wen Shong Lin and Prof. Fu
Chang for their invaluable advice in data pruning,
which greatly speeds up the training of our ma-
chine learning models.
</bodyText>
<sectionHeader confidence="0.999113" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999857192307692">
X. Carreras and L. Màrquez. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic Role Labeling.
In Proceedings of the CoNLL-2005.
E. Charniak. 2000. A Maximum-Entropy-Inspired
Parser. Proceedings of the NAACL-2000.
M. J. Collins. 1999. Head-driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
J. Nocedal and S. J. Wright. 1999. Numerical Optimiza-
tion, Springer.
S. Pradhan, K. Hacioglu, V. Kruglery, W. Ward,J. H.
Martin, and D. Jurafsky. 2004. Support Vector
Learning for Semantic Argument Classification.
Journal of Machine Learning.
V. Punyakanok, D. Roth, and W. Yih. 2005. The Neces-
sity of Syntactic Parsing for Semantic Role Labeling.
In Proceedings of the 19th International Joint Con-
ference on Artificial Intelligence (IJCAI-05).
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004.
Semantic Role Labeling via Integer Linear Pro-
gramming Inference. In Proceedings of the 20th In-
ternational Conference on Computational Linguistics
(COLING-04).
N. Xue and M. Palmer. 2004. Calibrating Features for
Semantic Role Labeling. In Proceedings of the
EMNLP 2004.
</reference>
<page confidence="0.998546">
236
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.386474">
<title confidence="0.9997675">Exploiting Full Parsing Information to Label Semantic Roles Using Ensemble of ME and SVM via Integer Linear Programming</title>
<author confidence="0.997647">Tzong-Han Tsai</author>
<author confidence="0.997647">Chia-Wei Wu</author>
<author confidence="0.997647">Yu-Chun Lin</author>
<author confidence="0.997647">Wen-Lian</author>
<affiliation confidence="0.8183355">Institute of Information Academia</affiliation>
<address confidence="0.872684">Taipei 115,</address>
<email confidence="0.96161">thtsai@iis.sinica.edu.tw</email>
<email confidence="0.96161">cwwu@iis.sinica.edu.tw</email>
<email confidence="0.96161">sbb@iis.sinica.edu.tw</email>
<email confidence="0.96161">hsu@iis.sinica.edu.tw</email>
<abstract confidence="0.982186772727273">In this paper, we propose a method that exploits full parsing information by representing it as features of argument classification models and as constraints in integer linear learning programs. In addition, to take advantage of SVM-based and Maximum Entropy-based argument classification models, we incorporate their scoring matrices, and use the combined matrix in the above-mentioned integer linear programs. The experimental results show that full parsing information not only increases the F-score of argument classification models by 0.7%, but also effectively removes all labeling inconsistencies, which increases the F-score by 0.64%. The ensemble of SVM and ME also boosts the F-score by 0.77%. Our system achieves an F-score of 76.53% in the development set and 76.38% in Test WSJ.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L Màrquez</author>
</authors>
<title>Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of the CoNLL-2005.</booktitle>
<marker>Carreras, Màrquez, 2005</marker>
<rawString>X. Carreras and L. Màrquez. 2005. Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling. In Proceedings of the CoNLL-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A Maximum-Entropy-Inspired Parser.</title>
<date>2000</date>
<booktitle>Proceedings of the NAACL-2000.</booktitle>
<contexts>
<context position="3485" citStr="Charniak, 2000" startWordPosition="524" endWordPosition="525">d integer linear programming. This section describes how we build these stages, including the features used in training the argument classification models. 2.1 Pruning 233 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), pages 233–236, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics When the full parsing tree of a sentence is available, only the constituents in the tree are considered as argument candidates. In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000). According to Punyakanok et al. (2005), the boundary agreement of Charniak is higher than that of Collins; therefore, we choose the Charniak parser’s results. However, there are two million nodes on the full parsing trees in the training corpus, which makes the training time of machine learning algorithms extremely long. Besides, noisy information from unrelated parts of a sentence could also affect the training of machine learning models. Therefore, our system exploits the heuristic rules introduced by Xue and Palmer (2004) to filter out simple constituents that are unlikely to be arguments.</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A Maximum-Entropy-Inspired Parser. Proceedings of the NAACL-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Collins</author>
</authors>
<title>Head-driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="3444" citStr="Collins, 1999" startWordPosition="518" endWordPosition="519">, classification model incorporation, and integer linear programming. This section describes how we build these stages, including the features used in training the argument classification models. 2.1 Pruning 233 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), pages 233–236, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics When the full parsing tree of a sentence is available, only the constituents in the tree are considered as argument candidates. In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000). According to Punyakanok et al. (2005), the boundary agreement of Charniak is higher than that of Collins; therefore, we choose the Charniak parser’s results. However, there are two million nodes on the full parsing trees in the training corpus, which makes the training time of machine learning algorithms extremely long. Besides, noisy information from unrelated parts of a sentence could also affect the training of machine learning models. Therefore, our system exploits the heuristic rules introduced by Xue and Palmer (2004) to filter out simple consti</context>
<context position="5933" citStr="Collins (1999)" startWordPosition="932" endWordPosition="933">s can be labeled as null (meaning “not an argument”). The features used in this stage are as follows. Basic Features • Predicate – The predicate lemma. • Path – The syntactic path through the parsing tree from the parse constituent being classified to the predicate. • Constituent Type • Position – Whether the phrase is located before or after the predicate. • Voice – passive: if the predicate has a POS tag VBN, and its chunk is not a VP, or it is preceded by a form of “to be” or “to get” within its chunk; otherwise, it is active. • Head Word – calculated using the head word table described by Collins (1999). • Head POS – The POS of the Head Word. We believe that information from related constituents in the full parsing tree helps in labeling the target constituent. Denote the target constituent by t. The following features are the most common baseline features of t’s parent and sibling constituents. For example, Parent/ Left Sibling/ Right Sibling Path denotes t’s parents’, left sibling’s, and right sibling’s Path features. • Parent / Left Sibling / Right Sibling Path • Parent / Left Sibling / Right Sibling Constituent Type • Parent / Left Sibling / Right Sibling Position • Parent / Left Sibling</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. J. Collins. 1999. Head-driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nocedal</author>
<author>S J Wright</author>
</authors>
<date>1999</date>
<publisher>Numerical Optimization, Springer.</publisher>
<contexts>
<context position="7017" citStr="Nocedal and Wright, 1999" startWordPosition="1121" endWordPosition="1124">ibling Path • Parent / Left Sibling / Right Sibling Constituent Type • Parent / Left Sibling / Right Sibling Position • Parent / Left Sibling / Right Sibling Head Word • Parent / Left Sibling / Right Sibling Head POS • Head of PP parent – If the parent is a PP, then the head of this PP is also used as a feature. Argument Classification Models 234 We use all the features of the SVM-based and MEbased argument classification models. All SVM classifiers are realized using SVM-Light with a polynomial kernel of degree 2. The ME-based model is implemented based on Zhang’s MaxEnt toolkit1 and L-BFGS (Nocedal and Wright, 1999) method to perform parameter estimation. 2.3 Classification Model Incorporation We now explain how we incorporate the SVMbased and ME-based argument classification models. After argument classification, we acquire two scoring matrices, PME and PSVM, respectively. Incorporation of these two models is realized by weighted summation of PME and PSVM as follows: P’ = wMEPME + wSVMPSVM We use P’ for the objective coefficients of the ILP described in Section 2.4. 2.4 Integer Linear Programming (ILP) To represent full parsing information as features, there are still several syntactic constraints on a </context>
</contexts>
<marker>Nocedal, Wright, 1999</marker>
<rawString>J. Nocedal and S. J. Wright. 1999. Numerical Optimization, Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pradhan</author>
<author>K Hacioglu</author>
<author>V Kruglery</author>
<author>W Ward</author>
<author>J H Martin</author>
<author>D Jurafsky</author>
</authors>
<title>Support Vector Learning for Semantic Argument Classification.</title>
<date>2004</date>
<journal>Journal of Machine Learning.</journal>
<contexts>
<context position="1415" citStr="Pradhan et al. (2004)" startWordPosition="217" endWordPosition="220">-score of argument classification models by 0.7%, but also effectively removes all labeling inconsistencies, which increases the F-score by 0.64%. The ensemble of SVM and ME also boosts the F-score by 0.77%. Our system achieves an F-score of 76.53% in the development set and 76.38% in Test WSJ. 1 Introduction The Semantic Role Labeling problem can be formulated as a sentence tagging problem. A sentence can be represented as a sequence of words, as phrases (chunks), or as a parsing tree. The basic units of a sentence are words, phrases, and constituents in these representations, respectively.. Pradhan et al. (2004) established that Constituentby-Constituent (C-by-C) is better than Phrase-byPhrase (P-by-P), which is better than Word-byWord (W-by-W). This is probably because the boundaries of the constituents coincide with the arguments; therefore, C-by-C has the highest argument identification F-score among the three approaches. In addition, a full parsing tree also provides richer syntactic information than a sequence of chunks or words. Pradhan et al. (2004) compared the seven most common features as well as several features related to the target constituent’s parent and sibling constituents. Their exp</context>
</contexts>
<marker>Pradhan, Hacioglu, Kruglery, Ward, Martin, Jurafsky, 2004</marker>
<rawString>S. Pradhan, K. Hacioglu, V. Kruglery, W. Ward,J. H. Martin, and D. Jurafsky. 2004. Support Vector Learning for Semantic Argument Classification. Journal of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>The Necessity of Syntactic Parsing for Semantic Role Labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI-05).</booktitle>
<contexts>
<context position="3524" citStr="Punyakanok et al. (2005)" startWordPosition="528" endWordPosition="531">This section describes how we build these stages, including the features used in training the argument classification models. 2.1 Pruning 233 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), pages 233–236, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics When the full parsing tree of a sentence is available, only the constituents in the tree are considered as argument candidates. In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000). According to Punyakanok et al. (2005), the boundary agreement of Charniak is higher than that of Collins; therefore, we choose the Charniak parser’s results. However, there are two million nodes on the full parsing trees in the training corpus, which makes the training time of machine learning algorithms extremely long. Besides, noisy information from unrelated parts of a sentence could also affect the training of machine learning models. Therefore, our system exploits the heuristic rules introduced by Xue and Palmer (2004) to filter out simple constituents that are unlikely to be arguments. Applying pruning heuristics to the out</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2005</marker>
<rawString>V. Punyakanok, D. Roth, and W. Yih. 2005. The Necessity of Syntactic Parsing for Semantic Role Labeling. In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI-05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
<author>D Zimak</author>
</authors>
<title>Semantic Role Labeling via Integer Linear Programming Inference.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING-04).</booktitle>
<contexts>
<context position="2134" citStr="Punyakanok et al. (2004)" startWordPosition="322" endWordPosition="325">hich is better than Word-byWord (W-by-W). This is probably because the boundaries of the constituents coincide with the arguments; therefore, C-by-C has the highest argument identification F-score among the three approaches. In addition, a full parsing tree also provides richer syntactic information than a sequence of chunks or words. Pradhan et al. (2004) compared the seven most common features as well as several features related to the target constituent’s parent and sibling constituents. Their experimental results show that using other constituents’ information increases the F-score by 6%. Punyakanok et al. (2004) represent full parsing information as constraints in integer linear programs. Their experimental results show that using such information increases the argument classification accuracy by 1%. In this paper, we not only add more full parsing features to argument classification models, but also represent full parsing information as constraints in integer linear programs (ILP) to resolve label inconsistencies. We also build an ensemble of two argument classification models: Maximum Entropy and SVM by combining their argument classification results and applying them to the abovementioned ILPs. 2 </context>
<context position="8010" citStr="Punyakanok et al. (2004)" startWordPosition="1277" endWordPosition="1280">ME + wSVMPSVM We use P’ for the objective coefficients of the ILP described in Section 2.4. 2.4 Integer Linear Programming (ILP) To represent full parsing information as features, there are still several syntactic constraints on a parsing tree in the SRL problem. For example, on a path of the parsing tree, there can be only one constituent annotated as a non-null argument. However, it is difficult to encode this constraint in the argument classification models. Therefore, we apply integer linear programming to resolve inconsistencies produced in the argument classification stage. According to Punyakanok et al. (2004), given a set of constituents, S, and a set of semantic role labels, A, the SRL problem can be formulated as an ILP as follows: Let zia be the indicator variable that represents whether or not an argument, a, is assigned to any Si ∈ S; and let pia = score(Si = a). The scoring matrix P composed of all pia is calculated by the argument classification models. The goal of this ILP is to find a set of assignments for all zia that maximizes the following function: piazia . Each Si∈ S should have one of these argument types, or no type (null). Therefore, we have zia =1. Next, we show how to transform</context>
</contexts>
<marker>Punyakanok, Roth, Yih, Zimak, 2004</marker>
<rawString>V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004. Semantic Role Labeling via Integer Linear Programming Inference. In Proceedings of the 20th International Conference on Computational Linguistics (COLING-04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>M Palmer</author>
</authors>
<title>Calibrating Features for Semantic Role Labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of the EMNLP</booktitle>
<contexts>
<context position="4016" citStr="Xue and Palmer (2004)" startWordPosition="604" endWordPosition="607"> full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000). According to Punyakanok et al. (2005), the boundary agreement of Charniak is higher than that of Collins; therefore, we choose the Charniak parser’s results. However, there are two million nodes on the full parsing trees in the training corpus, which makes the training time of machine learning algorithms extremely long. Besides, noisy information from unrelated parts of a sentence could also affect the training of machine learning models. Therefore, our system exploits the heuristic rules introduced by Xue and Palmer (2004) to filter out simple constituents that are unlikely to be arguments. Applying pruning heuristics to the output of Charniak’s parser effectively eliminates 61% of the training data and 61.3% of the development data, while still achieves 93% and 85.5% coverage of the correct arguments in the training and development sets, respectively. 2.2 Argument Classification Full Parsing Features • Sub-categorization – The phrase structure rule that expands the predicate’s parent node in the parsing tree. • First and Last Word/POS • Named Entities – LOC, ORG, PER, and MISC. • Level – The level in the parsi</context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>N. Xue and M. Palmer. 2004. Calibrating Features for Semantic Role Labeling. In Proceedings of the EMNLP 2004.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>