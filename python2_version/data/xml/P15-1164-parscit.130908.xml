<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000683">
<title confidence="0.972504">
Text Categorization as a Graph Classification Problem
</title>
<author confidence="0.788956">
François Rousseau Emmanouil Kiagias Michalis Vazirgiannis
</author>
<affiliation confidence="0.736686">
LIX, École Polytechnique, France
</affiliation>
<sectionHeader confidence="0.975076" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999926">
In this paper, we consider the task of
text categorization as a graph classifica-
tion problem. By representing textual doc-
uments as graph-of-words instead of his-
torical n-gram bag-of-words, we extract
more discriminative features that corre-
spond to long-distance n-grams through
frequent subgraph mining. Moreover, by
capitalizing on the concept of k-core, we
reduce the graph representation to its dens-
est part – its main core – speeding up the
feature extraction step for little to no cost
in prediction performances. Experiments
on four standard text classification datasets
show statistically significant higher accu-
racy and macro-averaged F1-score com-
pared to baseline approaches.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999926803278688">
The task of text categorization finds applications
in a wide variety of domains, from news filter-
ing and document organization to opinion mining
and spam detection. With the ever-growing quan-
tity of information available online nowadays, it
is crucial to provide effective systems capable of
classifying text in a timely fashion. Compared
to other application domains of classification, its
specificity lies in its high number of features, its
sparse feature vectors and its skewed multiclass
scenario. For instance, when dealing with thou-
sands of news articles, it is not uncommon to have
millions of n-gram features, only a few hundreds
actually present in each document and tens of class
labels – some of them with thousands of articles
and some others will only a few hundreds. These
particularities have to be taken into account when
envisaging a different representation for a docu-
ment and in our case when considering the task as
a graph classification problem.
Graphs are powerful data structures that are
used to represent complex information about en-
tities and interaction between them and we think
text makes no exception. Historically, following
the traditional bag-of-words representation, uni-
grams have been considered as the natural features
and later extended to n-grams to capture some
word dependency and word order. However, n-
grams correspond to sequences of words and thus
fail to capture word inversion and subset match-
ing (e. g., “article about news” vs. “news article”).
We believe graphs can help solve these issues like
they did for instance with chemical compounds
where repeating substructure patterns are good in-
dicators of belonging to one particular class, e. g.,
predicting carcinogenicity in molecules (Helma et
al., 2001). Graph classification has received a
lot of attention this past decade and various tech-
niques have been developed to deal with the task
but rarely applied on textual data and at its scale.
In our work, we explored a graph representation
of text, namely graph-of-words, to challenge the
traditional bag-of-words representation and help
better classify textual documents into categories.
We first trained a classifier using frequent sub-
graphs as features for increased effectiveness. We
then reduced each graph-of-words to its main core
before mining the features for increased efficiency.
Finally, we also used this technique to reduce the
total number of n-gram features considered in the
baselines for little to no loss in prediction perfor-
mances.
The rest of the paper is organized as follows.
Section 2 provides a review of the related work.
Section 3 defines the preliminary concepts upon
which our work is built. Section 4 introduces the
proposed approaches. Section 5 describes the ex-
perimental settings and presents the results we ob-
tained on four standard datasets. Finally, Section
6 concludes our paper and mentions future work
directions.
</bodyText>
<page confidence="0.96379">
1702
</page>
<note confidence="0.983316">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1702–1712,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.997291" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999924666666667">
In this section, we present the related work in text
categorization, graph classification and the com-
bination of the two fields like in our case.
</bodyText>
<subsectionHeader confidence="0.963234">
2.1 Text categorization
</subsectionHeader>
<bodyText confidence="0.999952347826087">
Text categorization, a.k.a. text classification, cor-
responds to the task of automatically predicting
the class label of a given textual document. We
refer to (Sebastiani, 2002) for an in-depth re-
view of the earliest works in the field and (Ag-
garwal and Zhai, 2012) for a survey of the more
recent works that capitalize on additional meta-
information. We note in particular the seminal
work of Joachims (1998) who was the first to pro-
pose the use of a linear SVM with TF×IDF term
features for the task. This approach is one of the
standard baselines because of its simplicity yet ef-
fectiveness (unsupervised n-gram feature mining
followed by standard supervised learning). An-
other popular approach is the use of Naive Bayes
and its multiple variants (McCallum and Nigam,
1998), in particular for the subtask of spam de-
tection (Androutsopoulos et al., 2000). Finally,
there are a couple of works such as (Hassan et
al., 2007) that used the graph-of-words representa-
tion to propose alternative weights for the n-gram
features but still without considering the task as a
graph classification problem.
</bodyText>
<subsectionHeader confidence="0.99295">
2.2 Graph classification
</subsectionHeader>
<bodyText confidence="0.999934625">
Graph classification corresponds to the task of au-
tomatically predicting the class label of a given
graph. The learning part in itself does not differ
from other supervised learning problems and most
proposed methods deal with the feature extrac-
tion part. They fall into two main categories: ap-
proaches that consider subgraphs as features and
graph kernels.
</bodyText>
<subsectionHeader confidence="0.735995">
2.2.1 Subgraphs as features
</subsectionHeader>
<bodyText confidence="0.99997034375">
The main idea is to mine frequent subgraphs and
use them as features for classification, be it with
Adaboost (Kudo et al., 2004) or a linear SVM
(Deshpande et al., 2005). Indeed, most datasets
that were used in the associated experiments cor-
respond to chemical compounds where repeating
substructure patterns are good indicators of be-
longing to one particular class. Some popular
graph pattern mining algorithms are gSpan (Yan
and Han, 2002), FFSM (Huan et al., 2003) and
Gaston (Nijssen and Kok, 2004). The number of
frequent subgraphs can be enormous, especially
for large graph collections, and handling such a
feature set can be very expensive. To overcome
this issue, recent works have proposed to retain
or even only mine the discriminative subgraphs,
i. e. features that contribute to the classification
decision, in particular gBoost (Saigo et al., 2009),
CORK (Thoma et al., 2009) and GAIA (Jin et
al., 2010). However, when experimenting, gBoost
did not converge on our larger datasets while
GAIA and CORK consider subgraphs of node
size at least 2, which exclude unigrams, result-
ing in poorer performances. Moreover, all these
approaches have been developed for binary clas-
sification, which meant mining features as many
times as the number of classes instead of just once
(one-vs-all learning strategy). In this paper, we
tackle the scalability issue differently through an
unsupervised feature selection approach to reduce
the size of the graphs and a fortiori the number of
frequent subgraphs.
</bodyText>
<subsectionHeader confidence="0.818246">
2.2.2 Graph kernels
</subsectionHeader>
<bodyText confidence="0.999982230769231">
Gärtner et al. (2003) proposed the first kernels be-
tween graphs (as opposed to previous kernels on
graphs, i. e. between nodes) based on either ran-
dom walks or cycles to tackle the problem of clas-
sification between graphs. In parallel, the idea of
marginalized kernels was extended to graphs by
Kashima et al. (2003) and by Mahé et al. (2004).
We refer to (Vishwanathan et al., 2010) for an in-
depth review of the topic and in particular its lim-
itations in terms of number of unique node labels,
which make them unsuitable for our problem as
tested in practice (limited to a few tens of unique
labels compared to hundreds of thousands for us).
</bodyText>
<subsectionHeader confidence="0.99461">
2.3 Similar works
</subsectionHeader>
<bodyText confidence="0.999867538461538">
The work of Markov et al. (2007) is perhaps the
closest to ours since they also perform subgraph
feature mining on graph-of-words representations
but with non-standard datasets and baselines. The
works of Jiang et al. (2010) and Arora et al. (2010)
are also related but their representations are dif-
ferent and closer to parse and dependency trees
used as base features for text categorization by
Kudo and Matsumoto (2004) and Matsumoto et al.
(2005). Moreover, they do not discuss the choice
of the support value, which controls the total num-
ber of features and can potentially lead to millions
of subgraphs on standard datasets.
</bodyText>
<page confidence="0.989333">
1703
</page>
<sectionHeader confidence="0.988556" genericHeader="method">
3 Preliminary concepts
</sectionHeader>
<bodyText confidence="0.999823">
In this section, we introduce the preliminary con-
cepts upon which our work is built.
</bodyText>
<subsectionHeader confidence="0.999661">
3.1 Graph-of-words
</subsectionHeader>
<bodyText confidence="0.999569">
We model a textual document as a graph-of-words,
which corresponds to a graph whose vertices rep-
resent unique terms of the document and whose
edges represent co-occurrences between the terms
within a fixed-size sliding window. The under-
lying assumption is that all the words present in
a document have some undirected relationships
with the others, modulo a window size outside of
which the relationship is not considered. This rep-
resentation was first used in keyword extraction
and summarization (Ohsawa et al., 1998; Mihal-
cea and Tarau, 2004) and more recently in ad hoc
IR (Blanco and Lioma, 2012; Rousseau and Vazir-
giannis, 2013). We refer to (Blanco and Lioma,
2012) for an in-depth review of the graph repre-
sentations of text in NLP.
As a discipline, computer science spans a range of topics
from theoretical studies of algorithms and the limits of
computation to the practical issues of implementing
computing systems in hardware and software.
</bodyText>
<figureCaption confidence="0.988829">
Figure 1: Graph-of-words representation of a tex-
</figureCaption>
<bodyText confidence="0.977808137931034">
hde me
tual document – in bold font, its main core.
Figure 1 illustrates the graph-of-words repre-
sentation of a textual document. The vertices cor-
respond to the remaining terms after standard pre-
processing steps have been applied (tokenization,
stop word removal and stemming). The undirected
edges were drawn between terms co-occurring
within a sliding window over the processed text of
size 4, value consistently reported as working well
in the references aforementioned and validated in
our experiments. Edge direction was used by Fil-
ippova (2010) so as to extract valid sentences but
not here in order to capture some word inversion.
Note that for small-enough window sizes
(which is typically the case in practice), we can
consider that two terms linked represent a long-
distance bigram (Bassiou and Kotropoulos, 2010),
if not a bigram. Furthermore, by extending the
denomination, we can consider that a subgraph
of size n is a long-distance n-gram, if not an n-
gram. Indeed, the nodes belonging to a subgraph
do not necessarily appear in a sequence in the doc-
ument like for a n-gram. Moreover, this enables us
to “merge” together n-grams that share the same
terms but maybe not in the same order. In the ex-
periments, by abusing the terminology, we will re-
fer to them as n-grams to adopt a common termi-
nology with the baseline approaches.
</bodyText>
<subsectionHeader confidence="0.999897">
3.2 Node/edge labels and subgraph matching
</subsectionHeader>
<bodyText confidence="0.999989210526316">
In graph classification, it is common to introduce
a node labeling function p to map a node id to its
label. For instance, consider the case of chemi-
cal compounds (e. g., the benzene C6H6). Then in
its graph representation (its “structural formula”),
it is crucial to differentiate between the multiple
nodes labeled the same (e. g., C or H). In the case
of graph-of-words, node labels are unique inside
a graph since they represent unique terms of the
document and we can therefore omit these func-
tions since they are injective in our case and we
can substitute node ids for node labels. In partic-
ular, the general problem of subgraph matching,
which defines an isomorphism between a graph
and a subgraph and is NP-complete (Garey and
Johnson, 1990), can be reduced to a polynomial
problem when node labels are unique. In our ex-
periments, we used the standard algorithm VF2
developed by Cordella et al. (2001).
</bodyText>
<subsectionHeader confidence="0.999471">
3.3 K-core and main core
</subsectionHeader>
<bodyText confidence="0.999971384615385">
Seidman (1983) defined the k-core of a graph as
the maximal connected subgraph whose vertices
are at least of degree k within the subgraph. The
non-empty k-core of largest k is called the main
core and corresponds to the most cohesive set(s)
of vertices. The corresponding value of k may dif-
fer from one graph to another. Batagelj and Za-
veršnik (2003) proposed an algorithm to extract
the main core of an unweighted graph in time lin-
ear in the number of edges, complexity similar
in our case to the other NLP preprocessing steps.
Bold font on Figure 1 indicates that a vertex be-
longs to the main core of the graph.
</bodyText>
<figure confidence="0.989364133333333">
issu
hardwar
disciplin
scienc
comput
system softwar
implement
practic
rang
algorithm
span
topic
studi
limit
theoret
</figure>
<page confidence="0.991066">
1704
</page>
<sectionHeader confidence="0.992274" genericHeader="method">
4 Graph-of-words classification
</sectionHeader>
<bodyText confidence="0.999988">
In this section, we present our work and the sev-
eral approaches we explored, from unsupervised
feature mining using gSpan to propose more dis-
criminative features than standard n-grams to un-
supervised feature selection using k-core to reduce
the total number of subgraph and n-gram features.
</bodyText>
<subsectionHeader confidence="0.995711">
4.1 Unsupervised feature mining using gSpan
</subsectionHeader>
<bodyText confidence="0.99999634375">
We considered the task of text categorization as a
graph classification problem by representing tex-
tual documents as graph-of-words and then ex-
tracting subgraph features to train a graph classi-
fier. Each document is a separate graph-of-words
and the collection of documents thus corresponds
to a set of graphs. Therefore, for larger datasets,
the total number of graphs increases but not the
average graph size (the average number of unique
terms in a text), assuming homogeneous datasets.
Because the total number of unique node la-
bels corresponds to the number of unique terms
in the collection in our case, graph kernels are
not suitable for us as verified in practice using the
MATLAB code made available by Shervashidze
(2009). We therefore only explored the meth-
ods that consider subgraphs as features. Repeat-
ing substructure patterns between graphs are intu-
itively good candidates for classification since, at
least for chemical compounds, shared subparts of
molecules are good indicators of belonging to one
particular class. We assumed it would the same for
text. Indeed, subgraphs of graph-of-words corre-
spond to sets of words co-occurring together, just
not necessarily always as the same sequence like
for n-grams – it can be seen as a relaxed definition
of a n-gram to capture additional variants.
We used gSpan (graph-based Substructure
pattern (Yan and Han, 2002)) as frequent sub-
graph miner like (Jiang et al., 2010; Arora et al.,
2010) mostly because of its fast available C++
implementation from gBoost (Saigo et al., 2009).
Briefly, the key idea behind gSpan is that in-
stead of enumerating all the subgraphs and test-
ing for isomorphism throughout the collection, it
first builds for each graph a lexicographic order
of all the edges using depth-first-search (DFS)
traversal and assigns to it a unique minimum DFS
code. Based on all these DFS codes, a hierarchical
search tree is constructed at the collection-level.
By pre-order traversal of this tree, gSpan discov-
ers all frequent subgraphs with required support.
Consider the set of all subgraphs in the collec-
tion of graphs, which corresponds to the set of all
potential features. Note that there may be overlap-
ping (subgraphs sharing nodes/edges) and redun-
dant (subgraphs included in others) features. Be-
cause its size is exponential in the number of edges
(just like the number of n-grams is exponential in
n), it is common to only retain/mine the most fre-
quent subgraphs (again just like for n-grams with a
minimum document frequency (Fürnkranz, 1998;
Joachims, 1998)). This is controlled via a param-
eter known as the support, which sets the mini-
mum number of graphs in which a given subgraph
has to appear to be considered as a feature, i. e.
the number of subgraph matches in the collection.
Here, since node labels are unique inside a graph,
we do not have to consider multiple occurrences
of the same subgraph in a given graph. The lower
the support, the more features selected/considered
but the more expensive the mining and the training
(not only in time spent for the learning but also for
the feature vector generation).
</bodyText>
<subsectionHeader confidence="0.998143">
4.2 Unsupervised support selection
</subsectionHeader>
<bodyText confidence="0.999997333333333">
The optimal value for the support can be learned
through cross-validation so as to maximize the
prediction accuracy of the subsequent classifier,
making the whole feature mining process super-
vised. But if we consider that the classifier can
only improve its goodness of fit with more fea-
tures (the sets of features being nested as the sup-
port varies), it is likely that the lowest support will
lead to the best test accuracy; assuming subse-
quent regularization to prevent overfitting. How-
ever, this will come at the cost of an exponential
number of features as observed in practice. In-
deed, as the support decreases, the number of fea-
tures increases slightly up until a point where it
increases exponentially, which makes both the fea-
ture vector generation and the learning expensive,
especially with multiple classes. Moreover, we
observed that the prediction performances did not
benefit that much from using all the possible fea-
tures (support of 1) as opposed to a more manage-
able number of features corresponding to a higher
support. Therefore, we propose to select the sup-
port using the so-called elbow method. This is an
unsupervised empirical method initially developed
for selecting the number of clusters in k-means
(Thorndike, 1953). Figure 3 (upper plots) in Sec-
tion 5 illustrates this process.
</bodyText>
<page confidence="0.947924">
1705
</page>
<subsectionHeader confidence="0.9928">
4.3 Considered classifiers
</subsectionHeader>
<bodyText confidence="0.999992454545454">
In text categorization, standard baseline classifiers
include k-nearest neighbors (kNN) (Larkey and
Croft, 1996), Naive Bayes (NB) (McCallum and
Nigam, 1998) and linear Support Vector Machines
(SVM) (Joachims, 1998) with the latter perform-
ing the best on n-gram features as verified in our
experiments. Since our subgraph features corre-
spond to “long-distance n-grams”, we used linear
SVMs as our classifiers in all our experiments –
the goal of our work being to explore and propose
better features rather than a different classifier.
</bodyText>
<subsectionHeader confidence="0.998419">
4.4 Multiclass scenario
</subsectionHeader>
<bodyText confidence="0.9999794">
In standard binary graph classification (e. g., pre-
dicting chemical compounds’ carcinogenicity as
either positive or negative (Helma et al., 2001)),
feature mining is performed on the whole graph
collection as we expect the mined features to be
able to discriminate between the two classes (thus
producing a good classifier). However, for the
task of text categorization, there are usually more
than two classes (e.g., 118 categories of news ar-
ticles for the Reuters-21578 dataset) and with a
skewed class distribution (e. g., a lot more news
related to “acquisition” than to “grain”). There-
fore, a single support value might lead to some
classes generating a tremendous number of fea-
tures (e. g., hundreds of thousands of frequent sub-
graphs) and some others only a few (e. g., a few
hundreds subgraphs) resulting in a skewed and
non-discriminative feature set. To include dis-
criminative features for these minority classes, we
would need an extremely low support resulting
in an exponential number of features because of
the majority classes. For these reasons, we de-
cided to mine frequent subgraphs per class using
the same relative support (%) and then aggregat-
ing each feature set into a global one at the cost of
a supervised process (but which still avoids cross-
validated parameter tuning). This was not needed
for the tasks of spam detection and opinion mining
since the corresponding datasets consist of only
two balanced classes.
</bodyText>
<subsectionHeader confidence="0.977605">
4.5 Main core mining using gSpan
</subsectionHeader>
<bodyText confidence="0.999980464285714">
Since the main drawback of mining frequent sub-
graphs for text categorization rather than chemical
compound classification is the very high number
of possible subgraphs because of the size of the
graphs and the total number of graphs (more than
10x in both cases), we thought of ways to reduce
the graphs’ sizes while retaining as much classifi-
cation information as possible.
The graph-of-words representation is designed
to capture dependency between words, i. e. de-
pendency between features in the context of ma-
chine learning but at the document-level. Ini-
tially, we wanted to capture recurring sets of words
(i. e. take into account word inversion and sub-
set matching) and not just sequences of words like
with n-grams. In terms of subgraphs, this means
words that co-occur with each other and form a
dense subgraph as opposed to a path like for a n-
gram. Therefore, when reducing the graphs, we
need to keep their densest part(s) and that is why
we considered extracting their main cores. Com-
pared to other density-based algorithms, retaining
the main core of a graph has the advantage of be-
ing linear in the number of edges, i. e. in the num-
ber of unique terms in a document in our case (the
number of edges is at most the number of nodes
times the fixed size of the sliding window, a small
constant in practice).
</bodyText>
<subsectionHeader confidence="0.967556">
4.6 Unsupervised n-gram feature selection
</subsectionHeader>
<bodyText confidence="0.999901821428571">
Similarly to (Hassan et al., 2007) that used graph-
of-words to propose alternative weights for the n-
gram features, we can capitalize on main core re-
tention to still extract binary n-gram features for
classification but considering only the terms be-
longing to the main core of each document. Be-
cause some terms never belong to any main core
of any document, the dimension of the overall fea-
ture space decreases. Additionally, since a docu-
ment is only represented by a subset of its original
terms, the number of non-zero feature values per
document also decreases, which matters for SVM,
even for the linear kernel, when considering the
dual formulation or in the primal with more recent
optimization techniques (Joachims, 2006).
Compared to most existing feature selection
techniques in the field (Yang and Pedersen, 1997),
it is unsupervised and corpus-independent as it
does not rely on any labeled data like IG, MI
or χ2 nor any collection-wide statistics like IDF,
which can be of interest for large-scale text cate-
gorization in order to process documents in paral-
lel, independently of each other. In some sense,
it is similar to what Özgür et al. (2005) proposed
with corpus-based and class-based keyword selec-
tion for text classification except that we use here
document-based keyword selection following the
approach from Rousseau and Vazirgiannis (2015).
</bodyText>
<page confidence="0.984665">
1706
</page>
<sectionHeader confidence="0.999097" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999874">
In this section we present the experiments we con-
ducted to validate our approaches.
</bodyText>
<subsectionHeader confidence="0.918108">
5.1 Datasets
</subsectionHeader>
<bodyText confidence="0.9892174">
We used four standard text datasets: two for multi-
class document categorization (WebKB and R8),
one for spam detection (LingSpam) and one for
opinion mining (Amazon) so as to cover all the
main subtasks of text categorization:
</bodyText>
<listItem confidence="0.961481333333333">
• WebKB: 4 most frequent categories among
labeled webpages from various CS depart-
ments – split into 2,803 for training and 1,396
for test (Cardoso-Cachopo, 2007, p. 39–41).
• R8: 8 most frequent categories of Reuters-
21578, a set of labeled news articles from the
1987 Reuters newswire – split into 5,485 for
training and 2,189 for test (Debole and Se-
bastiani, 2005).
• LingSpam: 2,893 emails classified as spam
or legitimate messages – split into 10 sets for
10-fold cross validation (Androutsopoulos et
al., 2000).
• Amazon: 8,000 product reviews over four
different sub-collections (books, DVDs, elec-
</listItem>
<bodyText confidence="0.808425">
tronics and kitchen appliances) classified as
positive or negative – split into 1,600 for
training and 400 for test each (Blitzer et al.,
2007).
</bodyText>
<subsectionHeader confidence="0.993943">
5.2 Implementation
</subsectionHeader>
<bodyText confidence="0.999984555555555">
We developed our approaches mostly in Python
using the igraph library (Csardi and Nepusz,
2006) for the graph representation and main core
extraction. For unsupervised subgraph feature
mining, we used the C++ implementation of
gSpan from gBoost (Saigo et al., 2009). Finally
for classification and standard n-gram text catego-
rization we used scikit (Pedregosa et al., 2011),
a standard Python machine learning library.
</bodyText>
<subsectionHeader confidence="0.989182">
5.3 Evaluation metrics
</subsectionHeader>
<bodyText confidence="0.999886142857143">
To evaluate the performance of our proposed ap-
proaches over standard baselines, we computed on
the test set both the micro- and macro-average F1-
score. Because we are dealing with single-label
classification, the micro-average F1-score corre-
sponds to the accuracy and is a measure of the
overall prediction effectiveness (Manning et al.,
</bodyText>
<table confidence="0.999264">
Dataset # subgraphs before # subgraphs after reduction
WebKB 30,868 10,113 67 %
R8 39,428 11,373 71 %
LingSpam 54,779 15,514 72 %
Amazon 16,415 8,745 47 %
Dataset # n-grams before # n-grams after reduction
WebKB 1,849,848 735,447 60 %
R8 1,604,280 788,465 51 %
LingSpam 2,733,043 1,016,061 63 %
Amazon 583,457 376,664 35 %
</table>
<tableCaption confidence="0.998092">
Table 1: Total number of features (n-grams or sub-
</tableCaption>
<bodyText confidence="0.970436909090909">
graphs) vs. number of features present only in
main cores along with the reduction of the dimen-
sion of the feature space on all four datasets.
2008, p. 281). Conversely, the macro-average F1-
score takes into account the skewed class label dis-
tributions by weighting each class uniformly. The
statistical significance of improvement in accuracy
over the n-gram SVM baseline was assessed us-
ing the micro sign test (p &lt; 0.05) (Yang and Liu,
1999). For the Amazon dataset, we report the av-
erage of each metric over the four sub-collections.
</bodyText>
<sectionHeader confidence="0.592832" genericHeader="evaluation">
5.4 Results
</sectionHeader>
<bodyText confidence="0.999987851851852">
Table 2 shows the results on the four considered
datasets. The first three rows correspond to the
baselines: unsupervised n-gram feature extrac-
tion and then supervised learning using kNN, NB
(Multinomial but Bernoulli yields similar results)
and linear SVM. The last three rows correspond to
our approaches.
In our first approach, denoted as “gSpan +
SVM”, we mine frequent subgraphs (gSpan) as
features and then train a linear SVM. These fea-
tures correspond to long-distance n-grams. This
leads to the best results in text categorization on
almost all datasets (all if we compare to baseline
methods), in particular on multiclass document
categorization (R8 and WebKB).
In our second approach, denoted as “MC +
gSpan + SVM”, we repeat the same procedure
except that we mine frequent subgraphs (gSpan)
from the main core (MC) of each graph-of-words
and then train an SVM on the resulting features.
Main cores can vary from 1-core to 12-core de-
pending on the graph structure, 5-core and 6-core
being the most frequent (more than 60%). This
yields results similar to the SVM baseline for a
faster mining and training compared to gSpan +
SVM. Table 1 (upper table) shows the reduction
in the dimension of the feature space and we see
</bodyText>
<page confidence="0.998254">
1707
</page>
<tableCaption confidence="0.8657726">
Table 2: Test accuracy and macro-average F1-score on four standard datasets. Bold font marks the best
performance in a column. * indicates statistical significance at p &lt; 0.05 using micro sign test with regards
to the SVM baseline of the same column. MC corresponds to unsupervised feature selection using the
main core of each graph-of-words to extract n-gram and subgraph features. gSpan mining support values
are 1.6% (WebKB), 7% (R8), 4% (LingSpam) and 0.5% (Amazon).
</tableCaption>
<table confidence="0.966067">
Dataset WebKB R8 LingSpam Amazon
Method
Accuracy F1-score Accuracy F1-score Accuracy F1-score Accuracy F1-score
kNN (k=5) 0.679 0.617 0.894 0.705 0.910 0.774 0.512 0.644
NB (Multinomial) 0.866 0.861 0.934 0.839 0.990 0.971 0.768 0.767
linear SVM 0.889 0.871 0.947 0.858 0.991 0.973 0.792 0.790
gSpan + SVM 0.912* 0.882 0.955* 0.864 0.991 0.972 0.798* 0.795
MC + gSpan + SVM 0.901* 0.871 0.949* 0.858 0.990 0.973 0.800* 0.798
MC + SVM 0.872 0.863 0.937 0.849 0.990 0.972 0.786 0.774
</table>
<figure confidence="0.989815">
# features
5 6 7 8 9 10 11 12 13
support (%)
accuracy
1 2 3 4
support (%)
250
200
# documents
150
100
50
0
# non-zero n-gram feature values before unsupervised feature selection
# documents
250
200
150
100
50
0
0 1000 2000 3000 4000 5000
# non-zero n-gram feature values after unsupervised feature selection
</figure>
<figureCaption confidence="0.997944">
Figure 2: Distribution of non-zero n-gram feature
values before and after unsupervised feature selec-
tion (main core retention) on R8 dataset.
</figureCaption>
<figure confidence="0.997499142857143">
250k
200k
150k
100k
50k
0
1 2 3 4
support (%)
1.00
0.95
0.90
0.85
5 6 7 8 9 10 11 12 13
support (%)
</figure>
<bodyText confidence="0.975377">
that on average less than 60% of the subgraphs are
kept for little to no cost in prediction effectiveness.
In our final approach, denoted as “MC + SVM”,
we performed unsupervised feature selection by
keeping the terms appearing in the main core (MC)
of each document’s graph-of-words representation
and then extracted standard n-gram features. Ta-
ble 1 (lower table) shows the reduction in the di-
mension of the feature space and we see that on av-
erage less than half the n-grams remain. Figure 2
shows the distribution of non-zero features before
and after the feature selection on the R8 dataset.
Similar changes in distribution can be observed on
the other datasets, from a right-tail Gaussian to a
power law distribution as expected from the main
core retention. Table 2 shows that the main core
retention has little to no cost in accuracy and F1-
score but can reduce drastically the feature space
and the number of non-zero values per document.
Figure 3: Number of subgraph features/accuracy
in test per support (%) on WebKB (left) and R8
(right) datasets: in black, the selected support
value chosen via the elbow method and in red, the
accuracy in test for the SVM baseline.
</bodyText>
<subsectionHeader confidence="0.998247">
5.5 Unsupervised support selection
</subsectionHeader>
<bodyText confidence="0.992404416666667">
Figure 3 above illustrates the unsupervised heuris-
tic (elbow method) we used to select the support
value, which corresponds to the minimum number
of graphs in which a subgraph has to appear to be
considered frequent. We noticed that as the sup-
port decreases, the number of features increases
slightly up until a point where it increases expo-
nentially. This support value, highlighted in black
on the figure and chosen before taking into ac-
count the class label, is the value we used in our
experiments and for which we report the results in
Table 1 and 2. The lower plots provide evidence
</bodyText>
<page confidence="0.994251">
1708
</page>
<figureCaption confidence="0.989418">
Figure 4: Distribution of n-grams (standard and
long-distance ones) among all the features on We-
bKB dataset.
Figure 5: Distribution of n-grams (standard and
long-distance ones) among the top 5% most dis-
criminative features for SVM on WebKB dataset.
</figureCaption>
<figure confidence="0.9973243">
1-grams 2-grams 3-grams 4-grams 5-grams 6-grams
# features (%) 100
80
60
40
20
0
baseline
gSpan
MC + gSpan
1-grams 2-grams 3-grams 4-grams 5-grams 6-grams
# features (%) 100
80
60
40
20
0
baseline SVM
gSpan + SVM
MC + gSpan + SVM
</figure>
<bodyText confidence="0.956603666666667">
that the elbow method helps selecting in an unsu-
pervised manner a support that leads to the best or
close to the best accuracy.
</bodyText>
<subsectionHeader confidence="0.999404">
5.6 Distribution of mined n-grams
</subsectionHeader>
<bodyText confidence="0.999977351351351">
In order to gain more insights on why the long-
distance n-grams mined with gSpan result in bet-
ter classification performances than the baseline n-
grams, we computed the distribution of the num-
ber of unigrams, bigrams, etc. up to 6-grams in the
traditional feature set and ours (Figure 4) as well
as in the top 5% features that contribute the most
to the classification decision of the trained SVM
(Figure 5). Again, a long-distance n-gram corre-
sponds to a subgraph of size n in a graph-of-words
and can be seen as a relaxed definition of the tra-
ditional n-gram, one that takes into account word
inversion for instance. To obtain comparable re-
sults, we considered for the baseline n-grams with
a minimum document frequency equal to the sup-
port. Otherwise, by definition, there are at least as
many bigrams as there are unigrams and so forth.
Figure 4 shows that our approaches mine way
more n-grams than unigrams compared to the
baseline. This happens because with graph-of-
words a subgraph of size n corresponds to a set
of n terms while with bag-of-words a n-gram cor-
responds to a sequence of n terms. Note that even
when restricting the subgraphs to the main cores,
there are still more higher order n-grams mined.
Figure 5 shows that the higher order n-grams
still contribute indeed to the classification deci-
sion and in higher proportion than with the base-
line, even when restricting to the main cores. For
instance, on the R8 dataset, {bank, base, rate}
was a discriminative (top 5% SVM features) long-
distance 3-gram for the category “interest” and
occurred in documents in the form of “barclays
bank cut its base lending rate”, “midland bank
matches its base rate” and “base rate of natwest
bank dropped”, pattern that would be hard to cap-
ture with traditional n-gram bag-of-words.
</bodyText>
<subsectionHeader confidence="0.99602">
5.7 Timing
</subsectionHeader>
<bodyText confidence="0.9999806">
With an Intel Core i5-3317U clocking at 2.6GHz
and 8GB of RAM, mining the subgraph features
with gSpan takes on average 30s for the selected
support. It can take several hours with lower sup-
port and goes down to 5s using the main cores.
</bodyText>
<sectionHeader confidence="0.996156" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999926058823529">
In this paper, we tackled the task of text cate-
gorization by representing documents as graph-
of-words and then considering the problem as a
graph classification one. We were able to extract
more discriminative features that correspond to
long-distance n-grams through frequent subgraph
mining. Experiments on four standard datasets
show statistically significant higher accuracy and
macro-averaged F1-score compared to baselines.
To the best of our knowledge, graph classifi-
cation has never been tested at that scale – thou-
sands of graphs and tens of thousands of unique
node labels – and also in the multiclass scenario.
For these reasons, we could not capitalize on all
standard methods. In particular, we believe new
kernels that support a very high number of unique
node labels could yield even better performances.
</bodyText>
<page confidence="0.990064">
1709
</page>
<table confidence="0.8130775">
Mukund Deshpande, Michihiro Kuramochi, Nikil
Wale, and George Karypis. 2005. Fre-
quent Substructure-Based Approaches for Classi-
fying Chemical Compounds. IEEE Transactions
on Knowledge and Data Engineering, 17(8):1036–
1050.
References
Charu C. Aggarwal and ChengXiang Zhai. 2012. A
Survey of Text Classification Algorithms. In Mining
Text Data, pages 163–222.
</table>
<reference confidence="0.998368545454546">
Ion Androutsopoulos, John Koutsias, Konstantinos V.
Chandrinos, George Paliouras, and Constantine D.
Spyropoulos. 2000. An Evaluation of Naive
Bayesian Anti-Spam Filtering. In Proceedings of
the Workshop on Machine Learning in the New In-
formation Age, 11th European Conference on Ma-
chine Learning, pages 9–17.
Shilpa Arora, Elijah Mayfield, Carolyn Penstein-Rosé,
and Eric Nyberg. 2010. Sentiment Classification
Using Automatically Extracted Subgraph Features.
In Proceedings of the NAACL HLT 2010 Workshop
on Computational Approaches to Analysis and Gen-
eration of Emotion in Text, CAAGET ’10, pages
131–139.
Nikoletta Bassiou and Constantine Kotropoulos. 2010.
Word Clustering Using PLSA Enhanced with Long
Distance Bigrams. In Proceedings of the 20th Inter-
national Conference on Pattern Recognition, ICPR
’10, pages 4226–4229.
Vladimir Batagelj and Matjaž Zaversnik. 2003.
An O(m) Algorithm for Cores Decomposition of
Networks. The Computing Research Repository
(CoRR), cs.DS/0310049.
Roi Blanco and Christina Lioma. 2012. Graph-based
term weighting for information retrieval. Informa-
tion Retrieval, 15(1):54–92.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boomboxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
ACL ’07, pages 440–447.
Ana Cardoso-Cachopo. 2007. Improving Methods for
Single-label Text Categorization. Ph.D. thesis, Insti-
tuto Superior Técnico, Universidade de Lisboa, Lis-
bon, Portugal.
Luigi Pietro Cordella, Pasquale Foggia, Carlo Sansone,
and Mario Vento. 2001. An improved algorithm for
matching large graphs. In Proceedings of the 3rd
IAPR-TC15 Workshop on Graph-based Representa-
tions in Pattern Recognition, pages 149–159.
Gabor Csardi and Tamas Nepusz. 2006. The igraph
software package for complex network research. In-
terJournal, Complex Systems, 1695(5):1–9.
Franca Debole and Fabrizio Sebastiani. 2005. An
Analysis of the Relative Hardness of Reuters-21578
Subsets: Research Articles. Journal of the Ameri-
can Society for Information Science and Technology,
56(6):584–596.
Katja Filippova. 2010. Multi-sentence Compression:
Finding Shortest Paths in Word Graphs. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, COLING ’10, pages 322–
330.
Johannes Fürnkranz. 1998. A study using n-gram
features for text categorization. Technical Report
OEFAI-TR-98-30, Austrian Research Institute for
Artificial Intelligence.
Michael R. Garey and David S. Johnson. 1990. Com-
puters and Intractability; A Guide to the Theory of
NP-Completeness. W. H. Freeman &amp; Co.
Thomas Gärtner, Peter Flach, and Stefan Wrobel.
2003. On graph kernels: Hardness results and
efficient alternatives. In Proceedings of the An-
nual Conference on Computational Learning The-
ory, COLT ’03, pages 129–143.
Samer Hassan, Rada Mihalcea, and Carmen Banea.
2007. Random-Walk Term Weighting for Improved
Text Classification. In Proceedings of the Interna-
tional Conference on Semantic Computing, ICSC
’07, pages 242–249.
Christoph Helma, Ross D. King, Stefan Kramer,
and Ashwin Srinivasan. 2001. The predictive
toxicology challenge 2000–2001. Bioinformatics,
17(1):107–108.
Jun Huan, Wei Wang, and Jan Prins. 2003. Efficient
Mining of Frequent Subgraphs in the Presence of
Isomorphism. In Proceedings of the 3rd IEEE In-
ternational Conference on Data Mining, ICDM ’03,
pages 549–552.
Chuntao Jiang, Frans Coenen, Robert Sanderson, and
Michele Zito. 2010. Text classification using graph
mining-based feature extraction. Knowledge-Based
Systems, 23(4):302–308.
Ning Jin, Calvin Young, and Wei Wang. 2010. GAIA:
graph classification using evolutionary computation.
In Proceedings of the 2010 ACM SIGMOD interna-
tional conference on Management of data, SIGMOD
’10, pages 879–890.
Thorsten Joachims. 1998. Text categorization with
Support Vector Machines: Learning with many rel-
evant features. In Proceedings of the 10th European
Conference on Machine Learning, ECML ’98, pages
137–142.
Thorsten Joachims. 2006. Training Linear SVMs
in Linear Time. In Proceedings of the 12th ACM
SIGKDD international conference on Knowledge
Discovery and Data mining, KDD ’06, pages 217–
226.
</reference>
<page confidence="0.753886">
1710
</page>
<reference confidence="0.999827342592592">
Hisashi Kashima, Koji Tsuda, and Akihiro Inokuchi.
2003. Marginalized kernels between labeled graphs.
In Proceedings of the 20th International Conference
on Machine Learning, volume 3 of ICML ’03, pages
321–328.
Taku Kudo and Yuji Matsumoto. 2004. A Boosting Al-
gorithm for Classification of Semi-Structured Text.
In Proceedings of the 9th Conference on Empirical
Methods in Natural Language Processing, volume 4
of EMNLP ’04, pages 301–308.
Taku Kudo, Eisaku Maeda, and Yuji Matsumoto. 2004.
An application of boosting to graph classification.
In Advances in Neural Information Processing Sys-
tems 17, NIPS ’04, pages 729–736.
Leah S. Larkey and W. Bruce Croft. 1996. Combining
Classifiers in Text Categorization. In Proceedings
of the 19th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ’96, pages 289–297.
Pierre Mahé, Nobuhisa Ueda, Tatsuya Akutsu, Jean-
Luc Perret, and Jean-Philippe Vert. 2004. Exten-
sions of marginalized graph kernels. In Proceed-
ings of the 21st International Conference on Ma-
chine Learning, ICML ’04, pages 70–78.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schütze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.
Alex Markov, Mark Last, and Abraham Kandel. 2007.
Fast Categorization of Web Documents Represented
by Graphs. In Advances in Web Mining and Web
Usage Analysis, number 4811 in Lecture Notes in
Artificial Intelligence, pages 56–71.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. 2005. Sentiment Classification Using
Word Sub-sequences and Dependency Sub-trees. In
Proceedings of the 9th Pacific-Asia Conference on
Advances in Knowledge Discovery and Data Min-
ing, PAKDD ’05, pages 301–311.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for Naive Bayes text classi-
fication. In Proceedings of the AAAI workshop on
learning for text categorization, AAAI ’98, pages
41–48.
Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing Order into Texts. In Proceedings of the 9th
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ’04, pages 404–411.
Siegfried Nijssen and Joost N. Kok. 2004. A Quick-
start in Frequent Structure Mining Can Make a Dif-
ference. In Proceedings of the 10th ACM SIGKDD
international conference on Knowledge Discovery
and Data mining, KDD ’04, pages 647–652.
Yukio Ohsawa, Nels E. Benson, and Masahiko
Yachida. 1998. KeyGraph: Automatic Indexing by
Co-occurrence Graph Based on Building Construc-
tion Metaphor. In Proceedings of the Advances in
Digital Libraries Conference, ADL ’98, pages 12–
18.
Arzucan Özgür, Levent Özgür, and Tunga Güngör.
2005. Text Categorization with Class-based and
Corpus-based Keyword Selection. In Proceedings
of the 20th International Conference on Computer
and Information Sciences, ISCIS ’05, pages 606–
615.
Fabian Pedregosa, Gaël Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. The Journal of Machine Learning
Research, 12:2825–2830.
François Rousseau and Michalis Vazirgiannis. 2013.
Graph-of-word and TW-IDF: New Approach to Ad
Hoc IR. In Proceedings of the 22nd ACM inter-
national conference on Information and knowledge
management, CIKM ’13, pages 59–68.
François Rousseau and Michalis Vazirgiannis. 2015.
Main Core Retention on Graph-of-words for Single-
Document Keyword Extraction. In Proceedings of
the 37th European Conference on Information Re-
trieval, ECIR ’15, pages 382–393.
Hiroto Saigo, Sebastian Nowozin, Tadashi Kadowaki,
Taku Kudo, and Koji Tsuda. 2009. gBoost: a math-
ematical programming approach to graph classifica-
tion and regression. Machine Learning, 75(1):69–
89.
Fabrizio Sebastiani. 2002. Machine Learning in Au-
tomated Text Categorization. ACM Computing Sur-
veys, 34(1):1–47.
Stephen B. Seidman. 1983. Network structure and
minimum degree. Social Networks, 5:269–287.
Nino Shervashidze. Visited on 30/05/2015. Graph ker-
nels. http://www.di.ens.fr/~shervashidze/code.html.
Marisa Thoma, Hong Cheng, Arthur Gretton, Ji-
awei Han, Hans-Peter Kriegel, Alexander J. Smola,
Le Song, Philip S. Yu, Xifeng Yan, and Karsten M.
Borgwardt. 2009. Near-optimal Supervised Feature
Selection among Frequent Subgraphs. In Proceed-
ings of the SIAM International Conference on Data
Mining, SDM ’09, pages 1076–1087.
Robert Thorndike. 1953. Who belongs in the family?
Psychometrika, 18(4):267–276.
S. V. N. Vishwanathan, Nicol N. Schraudolph, Risi
Kondor, and Karsten M. Borgwardt. 2010. Graph
kernels. Journal of Machine Learning Research,
11:1201–1242.
</reference>
<page confidence="0.810354">
1711
</page>
<reference confidence="0.998833785714286">
Xifeng Yan and Jiawei Han. 2002. gspan: Graph-
based substructure pattern mining. In Proceedings
of the 2nd IEEE International Conference on Data
Mining, ICDM ’02, pages 721–724.
Yiming Yang and Xin Liu. 1999. A Re-examination of
Text Categorization Methods. In Proceedings of the
22nd annual international ACM SIGIR conference
on Research and development in information re-
trieval, SIGIR ’99, pages 42–49.
Yiming Yang and J. O. Pedersen. 1997. A Compar-
ative Study on Feature Selection in Text Catego-
rization. In Proceedings of the 14th International
Conference on Machine Learning, ICML ’97, pages
412–420.
</reference>
<page confidence="0.994368">
1712
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.128285">
<title confidence="0.734177">Text Categorization as a Graph Classification Problem François Rousseau Emmanouil Kiagias Michalis Vazirgiannis</title>
<author confidence="0.239988">École Polytechnique LIX</author>
<author confidence="0.239988">France</author>
<abstract confidence="0.985324888888889">In this paper, we consider the task of text categorization as a graph classification problem. By representing textual documents as graph-of-words instead of historical n-gram bag-of-words, we extract more discriminative features that correspond to long-distance n-grams through frequent subgraph mining. Moreover, by capitalizing on the concept of k-core, we reduce the graph representation to its densest part – its main core – speeding up the feature extraction step for little to no cost in prediction performances. Experiments on four standard text classification datasets show statistically significant higher accuracy and macro-averaged F1-score compared to baseline approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ion Androutsopoulos</author>
<author>John Koutsias</author>
<author>Konstantinos V Chandrinos</author>
<author>George Paliouras</author>
<author>Constantine D Spyropoulos</author>
</authors>
<title>An Evaluation of Naive Bayesian Anti-Spam Filtering.</title>
<date>2000</date>
<booktitle>In Proceedings of the Workshop on Machine Learning in the New Information Age, 11th European Conference on Machine Learning,</booktitle>
<pages>9--17</pages>
<contexts>
<context position="5066" citStr="Androutsopoulos et al., 2000" startWordPosition="785" endWordPosition="788">the field and (Aggarwal and Zhai, 2012) for a survey of the more recent works that capitalize on additional metainformation. We note in particular the seminal work of Joachims (1998) who was the first to propose the use of a linear SVM with TF×IDF term features for the task. This approach is one of the standard baselines because of its simplicity yet effectiveness (unsupervised n-gram feature mining followed by standard supervised learning). Another popular approach is the use of Naive Bayes and its multiple variants (McCallum and Nigam, 1998), in particular for the subtask of spam detection (Androutsopoulos et al., 2000). Finally, there are a couple of works such as (Hassan et al., 2007) that used the graph-of-words representation to propose alternative weights for the n-gram features but still without considering the task as a graph classification problem. 2.2 Graph classification Graph classification corresponds to the task of automatically predicting the class label of a given graph. The learning part in itself does not differ from other supervised learning problems and most proposed methods deal with the feature extraction part. They fall into two main categories: approaches that consider subgraphs as fea</context>
<context position="23182" citStr="Androutsopoulos et al., 2000" startWordPosition="3776" endWordPosition="3779">ection (LingSpam) and one for opinion mining (Amazon) so as to cover all the main subtasks of text categorization: • WebKB: 4 most frequent categories among labeled webpages from various CS departments – split into 2,803 for training and 1,396 for test (Cardoso-Cachopo, 2007, p. 39–41). • R8: 8 most frequent categories of Reuters21578, a set of labeled news articles from the 1987 Reuters newswire – split into 5,485 for training and 2,189 for test (Debole and Sebastiani, 2005). • LingSpam: 2,893 emails classified as spam or legitimate messages – split into 10 sets for 10-fold cross validation (Androutsopoulos et al., 2000). • Amazon: 8,000 product reviews over four different sub-collections (books, DVDs, electronics and kitchen appliances) classified as positive or negative – split into 1,600 for training and 400 for test each (Blitzer et al., 2007). 5.2 Implementation We developed our approaches mostly in Python using the igraph library (Csardi and Nepusz, 2006) for the graph representation and main core extraction. For unsupervised subgraph feature mining, we used the C++ implementation of gSpan from gBoost (Saigo et al., 2009). Finally for classification and standard n-gram text categorization we used scikit</context>
</contexts>
<marker>Androutsopoulos, Koutsias, Chandrinos, Paliouras, Spyropoulos, 2000</marker>
<rawString>Ion Androutsopoulos, John Koutsias, Konstantinos V. Chandrinos, George Paliouras, and Constantine D. Spyropoulos. 2000. An Evaluation of Naive Bayesian Anti-Spam Filtering. In Proceedings of the Workshop on Machine Learning in the New Information Age, 11th European Conference on Machine Learning, pages 9–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shilpa Arora</author>
<author>Elijah Mayfield</author>
<author>Carolyn Penstein-Rosé</author>
<author>Eric Nyberg</author>
</authors>
<title>Sentiment Classification Using Automatically Extracted Subgraph Features.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, CAAGET ’10,</booktitle>
<pages>131--139</pages>
<contexts>
<context position="8155" citStr="Arora et al. (2010)" startWordPosition="1291" endWordPosition="1294">s by Kashima et al. (2003) and by Mahé et al. (2004). We refer to (Vishwanathan et al., 2010) for an indepth review of the topic and in particular its limitations in terms of number of unique node labels, which make them unsuitable for our problem as tested in practice (limited to a few tens of unique labels compared to hundreds of thousands for us). 2.3 Similar works The work of Markov et al. (2007) is perhaps the closest to ours since they also perform subgraph feature mining on graph-of-words representations but with non-standard datasets and baselines. The works of Jiang et al. (2010) and Arora et al. (2010) are also related but their representations are different and closer to parse and dependency trees used as base features for text categorization by Kudo and Matsumoto (2004) and Matsumoto et al. (2005). Moreover, they do not discuss the choice of the support value, which controls the total number of features and can potentially lead to millions of subgraphs on standard datasets. 1703 3 Preliminary concepts In this section, we introduce the preliminary concepts upon which our work is built. 3.1 Graph-of-words We model a textual document as a graph-of-words, which corresponds to a graph whose ve</context>
<context position="14546" citStr="Arora et al., 2010" startWordPosition="2350" endWordPosition="2353">atterns between graphs are intuitively good candidates for classification since, at least for chemical compounds, shared subparts of molecules are good indicators of belonging to one particular class. We assumed it would the same for text. Indeed, subgraphs of graph-of-words correspond to sets of words co-occurring together, just not necessarily always as the same sequence like for n-grams – it can be seen as a relaxed definition of a n-gram to capture additional variants. We used gSpan (graph-based Substructure pattern (Yan and Han, 2002)) as frequent subgraph miner like (Jiang et al., 2010; Arora et al., 2010) mostly because of its fast available C++ implementation from gBoost (Saigo et al., 2009). Briefly, the key idea behind gSpan is that instead of enumerating all the subgraphs and testing for isomorphism throughout the collection, it first builds for each graph a lexicographic order of all the edges using depth-first-search (DFS) traversal and assigns to it a unique minimum DFS code. Based on all these DFS codes, a hierarchical search tree is constructed at the collection-level. By pre-order traversal of this tree, gSpan discovers all frequent subgraphs with required support. Consider the set o</context>
</contexts>
<marker>Arora, Mayfield, Penstein-Rosé, Nyberg, 2010</marker>
<rawString>Shilpa Arora, Elijah Mayfield, Carolyn Penstein-Rosé, and Eric Nyberg. 2010. Sentiment Classification Using Automatically Extracted Subgraph Features. In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, CAAGET ’10, pages 131–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikoletta Bassiou</author>
<author>Constantine Kotropoulos</author>
</authors>
<title>Word Clustering Using PLSA Enhanced with Long Distance Bigrams.</title>
<date>2010</date>
<booktitle>In Proceedings of the 20th International Conference on Pattern Recognition, ICPR ’10,</booktitle>
<pages>4226--4229</pages>
<contexts>
<context position="10491" citStr="Bassiou and Kotropoulos, 2010" startWordPosition="1668" endWordPosition="1671">preprocessing steps have been applied (tokenization, stop word removal and stemming). The undirected edges were drawn between terms co-occurring within a sliding window over the processed text of size 4, value consistently reported as working well in the references aforementioned and validated in our experiments. Edge direction was used by Filippova (2010) so as to extract valid sentences but not here in order to capture some word inversion. Note that for small-enough window sizes (which is typically the case in practice), we can consider that two terms linked represent a longdistance bigram (Bassiou and Kotropoulos, 2010), if not a bigram. Furthermore, by extending the denomination, we can consider that a subgraph of size n is a long-distance n-gram, if not an ngram. Indeed, the nodes belonging to a subgraph do not necessarily appear in a sequence in the document like for a n-gram. Moreover, this enables us to “merge” together n-grams that share the same terms but maybe not in the same order. In the experiments, by abusing the terminology, we will refer to them as n-grams to adopt a common terminology with the baseline approaches. 3.2 Node/edge labels and subgraph matching In graph classification, it is common</context>
</contexts>
<marker>Bassiou, Kotropoulos, 2010</marker>
<rawString>Nikoletta Bassiou and Constantine Kotropoulos. 2010. Word Clustering Using PLSA Enhanced with Long Distance Bigrams. In Proceedings of the 20th International Conference on Pattern Recognition, ICPR ’10, pages 4226–4229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Batagelj</author>
<author>Matjaž Zaversnik</author>
</authors>
<title>An O(m) Algorithm for Cores Decomposition of Networks. The Computing Research Repository (CoRR),</title>
<date>2003</date>
<marker>Batagelj, Zaversnik, 2003</marker>
<rawString>Vladimir Batagelj and Matjaž Zaversnik. 2003. An O(m) Algorithm for Cores Decomposition of Networks. The Computing Research Repository (CoRR), cs.DS/0310049.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Blanco</author>
<author>Christina Lioma</author>
</authors>
<title>Graph-based term weighting for information retrieval.</title>
<date>2012</date>
<journal>Information Retrieval,</journal>
<volume>15</volume>
<issue>1</issue>
<contexts>
<context position="9269" citStr="Blanco and Lioma, 2012" startWordPosition="1473" endWordPosition="1476">t. 3.1 Graph-of-words We model a textual document as a graph-of-words, which corresponds to a graph whose vertices represent unique terms of the document and whose edges represent co-occurrences between the terms within a fixed-size sliding window. The underlying assumption is that all the words present in a document have some undirected relationships with the others, modulo a window size outside of which the relationship is not considered. This representation was first used in keyword extraction and summarization (Ohsawa et al., 1998; Mihalcea and Tarau, 2004) and more recently in ad hoc IR (Blanco and Lioma, 2012; Rousseau and Vazirgiannis, 2013). We refer to (Blanco and Lioma, 2012) for an in-depth review of the graph representations of text in NLP. As a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software. Figure 1: Graph-of-words representation of a texhde me tual document – in bold font, its main core. Figure 1 illustrates the graph-of-words representation of a textual document. The vertices correspond to the remaining terms after standard preproce</context>
</contexts>
<marker>Blanco, Lioma, 2012</marker>
<rawString>Roi Blanco and Christina Lioma. 2012. Graph-based term weighting for information retrieval. Information Retrieval, 15(1):54–92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, bollywood, boomboxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, ACL ’07,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="23413" citStr="Blitzer et al., 2007" startWordPosition="3812" endWordPosition="3815">,396 for test (Cardoso-Cachopo, 2007, p. 39–41). • R8: 8 most frequent categories of Reuters21578, a set of labeled news articles from the 1987 Reuters newswire – split into 5,485 for training and 2,189 for test (Debole and Sebastiani, 2005). • LingSpam: 2,893 emails classified as spam or legitimate messages – split into 10 sets for 10-fold cross validation (Androutsopoulos et al., 2000). • Amazon: 8,000 product reviews over four different sub-collections (books, DVDs, electronics and kitchen appliances) classified as positive or negative – split into 1,600 for training and 400 for test each (Blitzer et al., 2007). 5.2 Implementation We developed our approaches mostly in Python using the igraph library (Csardi and Nepusz, 2006) for the graph representation and main core extraction. For unsupervised subgraph feature mining, we used the C++ implementation of gSpan from gBoost (Saigo et al., 2009). Finally for classification and standard n-gram text categorization we used scikit (Pedregosa et al., 2011), a standard Python machine learning library. 5.3 Evaluation metrics To evaluate the performance of our proposed approaches over standard baselines, we computed on the test set both the micro- and macro-ave</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, bollywood, boomboxes and blenders: Domain adaptation for sentiment classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, ACL ’07, pages 440–447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana Cardoso-Cachopo</author>
</authors>
<title>Improving Methods for Single-label Text Categorization.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Instituto Superior Técnico, Universidade de Lisboa,</institution>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="22828" citStr="Cardoso-Cachopo, 2007" startWordPosition="3718" endWordPosition="3719">xcept that we use here document-based keyword selection following the approach from Rousseau and Vazirgiannis (2015). 1706 5 Experiments In this section we present the experiments we conducted to validate our approaches. 5.1 Datasets We used four standard text datasets: two for multiclass document categorization (WebKB and R8), one for spam detection (LingSpam) and one for opinion mining (Amazon) so as to cover all the main subtasks of text categorization: • WebKB: 4 most frequent categories among labeled webpages from various CS departments – split into 2,803 for training and 1,396 for test (Cardoso-Cachopo, 2007, p. 39–41). • R8: 8 most frequent categories of Reuters21578, a set of labeled news articles from the 1987 Reuters newswire – split into 5,485 for training and 2,189 for test (Debole and Sebastiani, 2005). • LingSpam: 2,893 emails classified as spam or legitimate messages – split into 10 sets for 10-fold cross validation (Androutsopoulos et al., 2000). • Amazon: 8,000 product reviews over four different sub-collections (books, DVDs, electronics and kitchen appliances) classified as positive or negative – split into 1,600 for training and 400 for test each (Blitzer et al., 2007). 5.2 Implement</context>
</contexts>
<marker>Cardoso-Cachopo, 2007</marker>
<rawString>Ana Cardoso-Cachopo. 2007. Improving Methods for Single-label Text Categorization. Ph.D. thesis, Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luigi Pietro Cordella</author>
<author>Pasquale Foggia</author>
<author>Carlo Sansone</author>
<author>Mario Vento</author>
</authors>
<title>An improved algorithm for matching large graphs.</title>
<date>2001</date>
<booktitle>In Proceedings of the 3rd IAPR-TC15 Workshop on Graph-based Representations in Pattern Recognition,</booktitle>
<pages>149--159</pages>
<contexts>
<context position="11964" citStr="Cordella et al. (2001)" startWordPosition="1927" endWordPosition="1930">n the multiple nodes labeled the same (e. g., C or H). In the case of graph-of-words, node labels are unique inside a graph since they represent unique terms of the document and we can therefore omit these functions since they are injective in our case and we can substitute node ids for node labels. In particular, the general problem of subgraph matching, which defines an isomorphism between a graph and a subgraph and is NP-complete (Garey and Johnson, 1990), can be reduced to a polynomial problem when node labels are unique. In our experiments, we used the standard algorithm VF2 developed by Cordella et al. (2001). 3.3 K-core and main core Seidman (1983) defined the k-core of a graph as the maximal connected subgraph whose vertices are at least of degree k within the subgraph. The non-empty k-core of largest k is called the main core and corresponds to the most cohesive set(s) of vertices. The corresponding value of k may differ from one graph to another. Batagelj and Zaveršnik (2003) proposed an algorithm to extract the main core of an unweighted graph in time linear in the number of edges, complexity similar in our case to the other NLP preprocessing steps. Bold font on Figure 1 indicates that a vert</context>
</contexts>
<marker>Cordella, Foggia, Sansone, Vento, 2001</marker>
<rawString>Luigi Pietro Cordella, Pasquale Foggia, Carlo Sansone, and Mario Vento. 2001. An improved algorithm for matching large graphs. In Proceedings of the 3rd IAPR-TC15 Workshop on Graph-based Representations in Pattern Recognition, pages 149–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabor Csardi</author>
<author>Tamas Nepusz</author>
</authors>
<title>The igraph software package for complex network research.</title>
<date>2006</date>
<journal>InterJournal, Complex Systems,</journal>
<volume>1695</volume>
<issue>5</issue>
<contexts>
<context position="23529" citStr="Csardi and Nepusz, 2006" startWordPosition="3829" endWordPosition="3832">ed news articles from the 1987 Reuters newswire – split into 5,485 for training and 2,189 for test (Debole and Sebastiani, 2005). • LingSpam: 2,893 emails classified as spam or legitimate messages – split into 10 sets for 10-fold cross validation (Androutsopoulos et al., 2000). • Amazon: 8,000 product reviews over four different sub-collections (books, DVDs, electronics and kitchen appliances) classified as positive or negative – split into 1,600 for training and 400 for test each (Blitzer et al., 2007). 5.2 Implementation We developed our approaches mostly in Python using the igraph library (Csardi and Nepusz, 2006) for the graph representation and main core extraction. For unsupervised subgraph feature mining, we used the C++ implementation of gSpan from gBoost (Saigo et al., 2009). Finally for classification and standard n-gram text categorization we used scikit (Pedregosa et al., 2011), a standard Python machine learning library. 5.3 Evaluation metrics To evaluate the performance of our proposed approaches over standard baselines, we computed on the test set both the micro- and macro-average F1- score. Because we are dealing with single-label classification, the micro-average F1-score corresponds to t</context>
</contexts>
<marker>Csardi, Nepusz, 2006</marker>
<rawString>Gabor Csardi and Tamas Nepusz. 2006. The igraph software package for complex network research. InterJournal, Complex Systems, 1695(5):1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franca Debole</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>An Analysis of the Relative Hardness of Reuters-21578 Subsets: Research Articles.</title>
<date>2005</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>56</volume>
<issue>6</issue>
<contexts>
<context position="23033" citStr="Debole and Sebastiani, 2005" startWordPosition="3752" endWordPosition="3756">lidate our approaches. 5.1 Datasets We used four standard text datasets: two for multiclass document categorization (WebKB and R8), one for spam detection (LingSpam) and one for opinion mining (Amazon) so as to cover all the main subtasks of text categorization: • WebKB: 4 most frequent categories among labeled webpages from various CS departments – split into 2,803 for training and 1,396 for test (Cardoso-Cachopo, 2007, p. 39–41). • R8: 8 most frequent categories of Reuters21578, a set of labeled news articles from the 1987 Reuters newswire – split into 5,485 for training and 2,189 for test (Debole and Sebastiani, 2005). • LingSpam: 2,893 emails classified as spam or legitimate messages – split into 10 sets for 10-fold cross validation (Androutsopoulos et al., 2000). • Amazon: 8,000 product reviews over four different sub-collections (books, DVDs, electronics and kitchen appliances) classified as positive or negative – split into 1,600 for training and 400 for test each (Blitzer et al., 2007). 5.2 Implementation We developed our approaches mostly in Python using the igraph library (Csardi and Nepusz, 2006) for the graph representation and main core extraction. For unsupervised subgraph feature mining, we use</context>
</contexts>
<marker>Debole, Sebastiani, 2005</marker>
<rawString>Franca Debole and Fabrizio Sebastiani. 2005. An Analysis of the Relative Hardness of Reuters-21578 Subsets: Research Articles. Journal of the American Society for Information Science and Technology, 56(6):584–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
</authors>
<title>Multi-sentence Compression: Finding Shortest Paths in Word Graphs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10,</booktitle>
<pages>322--330</pages>
<contexts>
<context position="10219" citStr="Filippova (2010)" startWordPosition="1624" endWordPosition="1626">are and software. Figure 1: Graph-of-words representation of a texhde me tual document – in bold font, its main core. Figure 1 illustrates the graph-of-words representation of a textual document. The vertices correspond to the remaining terms after standard preprocessing steps have been applied (tokenization, stop word removal and stemming). The undirected edges were drawn between terms co-occurring within a sliding window over the processed text of size 4, value consistently reported as working well in the references aforementioned and validated in our experiments. Edge direction was used by Filippova (2010) so as to extract valid sentences but not here in order to capture some word inversion. Note that for small-enough window sizes (which is typically the case in practice), we can consider that two terms linked represent a longdistance bigram (Bassiou and Kotropoulos, 2010), if not a bigram. Furthermore, by extending the denomination, we can consider that a subgraph of size n is a long-distance n-gram, if not an ngram. Indeed, the nodes belonging to a subgraph do not necessarily appear in a sequence in the document like for a n-gram. Moreover, this enables us to “merge” together n-grams that sha</context>
</contexts>
<marker>Filippova, 2010</marker>
<rawString>Katja Filippova. 2010. Multi-sentence Compression: Finding Shortest Paths in Word Graphs. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10, pages 322– 330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Fürnkranz</author>
</authors>
<title>A study using n-gram features for text categorization.</title>
<date>1998</date>
<tech>Technical Report OEFAI-TR-98-30,</tech>
<institution>Austrian Research Institute for Artificial Intelligence.</institution>
<contexts>
<context position="15619" citStr="Fürnkranz, 1998" startWordPosition="2528" endWordPosition="2529">at the collection-level. By pre-order traversal of this tree, gSpan discovers all frequent subgraphs with required support. Consider the set of all subgraphs in the collection of graphs, which corresponds to the set of all potential features. Note that there may be overlapping (subgraphs sharing nodes/edges) and redundant (subgraphs included in others) features. Because its size is exponential in the number of edges (just like the number of n-grams is exponential in n), it is common to only retain/mine the most frequent subgraphs (again just like for n-grams with a minimum document frequency (Fürnkranz, 1998; Joachims, 1998)). This is controlled via a parameter known as the support, which sets the minimum number of graphs in which a given subgraph has to appear to be considered as a feature, i. e. the number of subgraph matches in the collection. Here, since node labels are unique inside a graph, we do not have to consider multiple occurrences of the same subgraph in a given graph. The lower the support, the more features selected/considered but the more expensive the mining and the training (not only in time spent for the learning but also for the feature vector generation). 4.2 Unsupervised sup</context>
</contexts>
<marker>Fürnkranz, 1998</marker>
<rawString>Johannes Fürnkranz. 1998. A study using n-gram features for text categorization. Technical Report OEFAI-TR-98-30, Austrian Research Institute for Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael R Garey</author>
<author>David S Johnson</author>
</authors>
<title>Computers and Intractability; A Guide to the Theory of NP-Completeness.</title>
<date>1990</date>
<contexts>
<context position="11804" citStr="Garey and Johnson, 1990" startWordPosition="1899" endWordPosition="1902">sider the case of chemical compounds (e. g., the benzene C6H6). Then in its graph representation (its “structural formula”), it is crucial to differentiate between the multiple nodes labeled the same (e. g., C or H). In the case of graph-of-words, node labels are unique inside a graph since they represent unique terms of the document and we can therefore omit these functions since they are injective in our case and we can substitute node ids for node labels. In particular, the general problem of subgraph matching, which defines an isomorphism between a graph and a subgraph and is NP-complete (Garey and Johnson, 1990), can be reduced to a polynomial problem when node labels are unique. In our experiments, we used the standard algorithm VF2 developed by Cordella et al. (2001). 3.3 K-core and main core Seidman (1983) defined the k-core of a graph as the maximal connected subgraph whose vertices are at least of degree k within the subgraph. The non-empty k-core of largest k is called the main core and corresponds to the most cohesive set(s) of vertices. The corresponding value of k may differ from one graph to another. Batagelj and Zaveršnik (2003) proposed an algorithm to extract the main core of an unweight</context>
</contexts>
<marker>Garey, Johnson, 1990</marker>
<rawString>Michael R. Garey and David S. Johnson. 1990. Computers and Intractability; A Guide to the Theory of NP-Completeness. W. H. Freeman &amp; Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Gärtner</author>
<author>Peter Flach</author>
<author>Stefan Wrobel</author>
</authors>
<title>On graph kernels: Hardness results and efficient alternatives.</title>
<date>2003</date>
<booktitle>In Proceedings of the Annual Conference on Computational Learning Theory, COLT ’03,</booktitle>
<pages>129--143</pages>
<contexts>
<context position="7267" citStr="Gärtner et al. (2003)" startWordPosition="1135" endWordPosition="1138"> when experimenting, gBoost did not converge on our larger datasets while GAIA and CORK consider subgraphs of node size at least 2, which exclude unigrams, resulting in poorer performances. Moreover, all these approaches have been developed for binary classification, which meant mining features as many times as the number of classes instead of just once (one-vs-all learning strategy). In this paper, we tackle the scalability issue differently through an unsupervised feature selection approach to reduce the size of the graphs and a fortiori the number of frequent subgraphs. 2.2.2 Graph kernels Gärtner et al. (2003) proposed the first kernels between graphs (as opposed to previous kernels on graphs, i. e. between nodes) based on either random walks or cycles to tackle the problem of classification between graphs. In parallel, the idea of marginalized kernels was extended to graphs by Kashima et al. (2003) and by Mahé et al. (2004). We refer to (Vishwanathan et al., 2010) for an indepth review of the topic and in particular its limitations in terms of number of unique node labels, which make them unsuitable for our problem as tested in practice (limited to a few tens of unique labels compared to hundreds </context>
</contexts>
<marker>Gärtner, Flach, Wrobel, 2003</marker>
<rawString>Thomas Gärtner, Peter Flach, and Stefan Wrobel. 2003. On graph kernels: Hardness results and efficient alternatives. In Proceedings of the Annual Conference on Computational Learning Theory, COLT ’03, pages 129–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samer Hassan</author>
<author>Rada Mihalcea</author>
<author>Carmen Banea</author>
</authors>
<title>Random-Walk Term Weighting for Improved Text Classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Semantic Computing, ICSC ’07,</booktitle>
<pages>242--249</pages>
<contexts>
<context position="5134" citStr="Hassan et al., 2007" startWordPosition="798" endWordPosition="801"> that capitalize on additional metainformation. We note in particular the seminal work of Joachims (1998) who was the first to propose the use of a linear SVM with TF×IDF term features for the task. This approach is one of the standard baselines because of its simplicity yet effectiveness (unsupervised n-gram feature mining followed by standard supervised learning). Another popular approach is the use of Naive Bayes and its multiple variants (McCallum and Nigam, 1998), in particular for the subtask of spam detection (Androutsopoulos et al., 2000). Finally, there are a couple of works such as (Hassan et al., 2007) that used the graph-of-words representation to propose alternative weights for the n-gram features but still without considering the task as a graph classification problem. 2.2 Graph classification Graph classification corresponds to the task of automatically predicting the class label of a given graph. The learning part in itself does not differ from other supervised learning problems and most proposed methods deal with the feature extraction part. They fall into two main categories: approaches that consider subgraphs as features and graph kernels. 2.2.1 Subgraphs as features The main idea i</context>
<context position="20997" citStr="Hassan et al., 2007" startWordPosition="3419" endWordPosition="3422">ur with each other and form a dense subgraph as opposed to a path like for a ngram. Therefore, when reducing the graphs, we need to keep their densest part(s) and that is why we considered extracting their main cores. Compared to other density-based algorithms, retaining the main core of a graph has the advantage of being linear in the number of edges, i. e. in the number of unique terms in a document in our case (the number of edges is at most the number of nodes times the fixed size of the sliding window, a small constant in practice). 4.6 Unsupervised n-gram feature selection Similarly to (Hassan et al., 2007) that used graphof-words to propose alternative weights for the ngram features, we can capitalize on main core retention to still extract binary n-gram features for classification but considering only the terms belonging to the main core of each document. Because some terms never belong to any main core of any document, the dimension of the overall feature space decreases. Additionally, since a document is only represented by a subset of its original terms, the number of non-zero feature values per document also decreases, which matters for SVM, even for the linear kernel, when considering the</context>
</contexts>
<marker>Hassan, Mihalcea, Banea, 2007</marker>
<rawString>Samer Hassan, Rada Mihalcea, and Carmen Banea. 2007. Random-Walk Term Weighting for Improved Text Classification. In Proceedings of the International Conference on Semantic Computing, ICSC ’07, pages 242–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Helma</author>
<author>Ross D King</author>
<author>Stefan Kramer</author>
<author>Ashwin Srinivasan</author>
</authors>
<title>The predictive toxicology challenge 2000–2001.</title>
<date>2001</date>
<journal>Bioinformatics,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="2608" citStr="Helma et al., 2001" startWordPosition="395" endWordPosition="398">istorically, following the traditional bag-of-words representation, unigrams have been considered as the natural features and later extended to n-grams to capture some word dependency and word order. However, ngrams correspond to sequences of words and thus fail to capture word inversion and subset matching (e. g., “article about news” vs. “news article”). We believe graphs can help solve these issues like they did for instance with chemical compounds where repeating substructure patterns are good indicators of belonging to one particular class, e. g., predicting carcinogenicity in molecules (Helma et al., 2001). Graph classification has received a lot of attention this past decade and various techniques have been developed to deal with the task but rarely applied on textual data and at its scale. In our work, we explored a graph representation of text, namely graph-of-words, to challenge the traditional bag-of-words representation and help better classify textual documents into categories. We first trained a classifier using frequent subgraphs as features for increased effectiveness. We then reduced each graph-of-words to its main core before mining the features for increased efficiency. Finally, we</context>
<context position="18278" citStr="Helma et al., 2001" startWordPosition="2956" endWordPosition="2959"> (Larkey and Croft, 1996), Naive Bayes (NB) (McCallum and Nigam, 1998) and linear Support Vector Machines (SVM) (Joachims, 1998) with the latter performing the best on n-gram features as verified in our experiments. Since our subgraph features correspond to “long-distance n-grams”, we used linear SVMs as our classifiers in all our experiments – the goal of our work being to explore and propose better features rather than a different classifier. 4.4 Multiclass scenario In standard binary graph classification (e. g., predicting chemical compounds’ carcinogenicity as either positive or negative (Helma et al., 2001)), feature mining is performed on the whole graph collection as we expect the mined features to be able to discriminate between the two classes (thus producing a good classifier). However, for the task of text categorization, there are usually more than two classes (e.g., 118 categories of news articles for the Reuters-21578 dataset) and with a skewed class distribution (e. g., a lot more news related to “acquisition” than to “grain”). Therefore, a single support value might lead to some classes generating a tremendous number of features (e. g., hundreds of thousands of frequent subgraphs) and</context>
</contexts>
<marker>Helma, King, Kramer, Srinivasan, 2001</marker>
<rawString>Christoph Helma, Ross D. King, Stefan Kramer, and Ashwin Srinivasan. 2001. The predictive toxicology challenge 2000–2001. Bioinformatics, 17(1):107–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Huan</author>
<author>Wei Wang</author>
<author>Jan Prins</author>
</authors>
<title>Efficient Mining of Frequent Subgraphs in the Presence of Isomorphism.</title>
<date>2003</date>
<booktitle>In Proceedings of the 3rd IEEE International Conference on Data Mining, ICDM ’03,</booktitle>
<pages>549--552</pages>
<contexts>
<context position="6186" citStr="Huan et al., 2003" startWordPosition="964" endWordPosition="967">ure extraction part. They fall into two main categories: approaches that consider subgraphs as features and graph kernels. 2.2.1 Subgraphs as features The main idea is to mine frequent subgraphs and use them as features for classification, be it with Adaboost (Kudo et al., 2004) or a linear SVM (Deshpande et al., 2005). Indeed, most datasets that were used in the associated experiments correspond to chemical compounds where repeating substructure patterns are good indicators of belonging to one particular class. Some popular graph pattern mining algorithms are gSpan (Yan and Han, 2002), FFSM (Huan et al., 2003) and Gaston (Nijssen and Kok, 2004). The number of frequent subgraphs can be enormous, especially for large graph collections, and handling such a feature set can be very expensive. To overcome this issue, recent works have proposed to retain or even only mine the discriminative subgraphs, i. e. features that contribute to the classification decision, in particular gBoost (Saigo et al., 2009), CORK (Thoma et al., 2009) and GAIA (Jin et al., 2010). However, when experimenting, gBoost did not converge on our larger datasets while GAIA and CORK consider subgraphs of node size at least 2, which ex</context>
</contexts>
<marker>Huan, Wang, Prins, 2003</marker>
<rawString>Jun Huan, Wei Wang, and Jan Prins. 2003. Efficient Mining of Frequent Subgraphs in the Presence of Isomorphism. In Proceedings of the 3rd IEEE International Conference on Data Mining, ICDM ’03, pages 549–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chuntao Jiang</author>
<author>Frans Coenen</author>
<author>Robert Sanderson</author>
<author>Michele Zito</author>
</authors>
<title>Text classification using graph mining-based feature extraction.</title>
<date>2010</date>
<journal>Knowledge-Based Systems,</journal>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="8131" citStr="Jiang et al. (2010)" startWordPosition="1286" endWordPosition="1289">ls was extended to graphs by Kashima et al. (2003) and by Mahé et al. (2004). We refer to (Vishwanathan et al., 2010) for an indepth review of the topic and in particular its limitations in terms of number of unique node labels, which make them unsuitable for our problem as tested in practice (limited to a few tens of unique labels compared to hundreds of thousands for us). 2.3 Similar works The work of Markov et al. (2007) is perhaps the closest to ours since they also perform subgraph feature mining on graph-of-words representations but with non-standard datasets and baselines. The works of Jiang et al. (2010) and Arora et al. (2010) are also related but their representations are different and closer to parse and dependency trees used as base features for text categorization by Kudo and Matsumoto (2004) and Matsumoto et al. (2005). Moreover, they do not discuss the choice of the support value, which controls the total number of features and can potentially lead to millions of subgraphs on standard datasets. 1703 3 Preliminary concepts In this section, we introduce the preliminary concepts upon which our work is built. 3.1 Graph-of-words We model a textual document as a graph-of-words, which corresp</context>
<context position="14525" citStr="Jiang et al., 2010" startWordPosition="2346" endWordPosition="2349">ating substructure patterns between graphs are intuitively good candidates for classification since, at least for chemical compounds, shared subparts of molecules are good indicators of belonging to one particular class. We assumed it would the same for text. Indeed, subgraphs of graph-of-words correspond to sets of words co-occurring together, just not necessarily always as the same sequence like for n-grams – it can be seen as a relaxed definition of a n-gram to capture additional variants. We used gSpan (graph-based Substructure pattern (Yan and Han, 2002)) as frequent subgraph miner like (Jiang et al., 2010; Arora et al., 2010) mostly because of its fast available C++ implementation from gBoost (Saigo et al., 2009). Briefly, the key idea behind gSpan is that instead of enumerating all the subgraphs and testing for isomorphism throughout the collection, it first builds for each graph a lexicographic order of all the edges using depth-first-search (DFS) traversal and assigns to it a unique minimum DFS code. Based on all these DFS codes, a hierarchical search tree is constructed at the collection-level. By pre-order traversal of this tree, gSpan discovers all frequent subgraphs with required suppor</context>
</contexts>
<marker>Jiang, Coenen, Sanderson, Zito, 2010</marker>
<rawString>Chuntao Jiang, Frans Coenen, Robert Sanderson, and Michele Zito. 2010. Text classification using graph mining-based feature extraction. Knowledge-Based Systems, 23(4):302–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ning Jin</author>
<author>Calvin Young</author>
<author>Wei Wang</author>
</authors>
<title>GAIA: graph classification using evolutionary computation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 ACM SIGMOD international conference on Management of data, SIGMOD ’10,</booktitle>
<pages>879--890</pages>
<contexts>
<context position="6636" citStr="Jin et al., 2010" startWordPosition="1037" endWordPosition="1040">patterns are good indicators of belonging to one particular class. Some popular graph pattern mining algorithms are gSpan (Yan and Han, 2002), FFSM (Huan et al., 2003) and Gaston (Nijssen and Kok, 2004). The number of frequent subgraphs can be enormous, especially for large graph collections, and handling such a feature set can be very expensive. To overcome this issue, recent works have proposed to retain or even only mine the discriminative subgraphs, i. e. features that contribute to the classification decision, in particular gBoost (Saigo et al., 2009), CORK (Thoma et al., 2009) and GAIA (Jin et al., 2010). However, when experimenting, gBoost did not converge on our larger datasets while GAIA and CORK consider subgraphs of node size at least 2, which exclude unigrams, resulting in poorer performances. Moreover, all these approaches have been developed for binary classification, which meant mining features as many times as the number of classes instead of just once (one-vs-all learning strategy). In this paper, we tackle the scalability issue differently through an unsupervised feature selection approach to reduce the size of the graphs and a fortiori the number of frequent subgraphs. 2.2.2 Grap</context>
</contexts>
<marker>Jin, Young, Wang, 2010</marker>
<rawString>Ning Jin, Calvin Young, and Wei Wang. 2010. GAIA: graph classification using evolutionary computation. In Proceedings of the 2010 ACM SIGMOD international conference on Management of data, SIGMOD ’10, pages 879–890.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text categorization with Support Vector Machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>In Proceedings of the 10th European Conference on Machine Learning, ECML ’98,</booktitle>
<pages>137--142</pages>
<contexts>
<context position="4619" citStr="Joachims (1998)" startWordPosition="712" endWordPosition="713">putational Linguistics 2 Related work In this section, we present the related work in text categorization, graph classification and the combination of the two fields like in our case. 2.1 Text categorization Text categorization, a.k.a. text classification, corresponds to the task of automatically predicting the class label of a given textual document. We refer to (Sebastiani, 2002) for an in-depth review of the earliest works in the field and (Aggarwal and Zhai, 2012) for a survey of the more recent works that capitalize on additional metainformation. We note in particular the seminal work of Joachims (1998) who was the first to propose the use of a linear SVM with TF×IDF term features for the task. This approach is one of the standard baselines because of its simplicity yet effectiveness (unsupervised n-gram feature mining followed by standard supervised learning). Another popular approach is the use of Naive Bayes and its multiple variants (McCallum and Nigam, 1998), in particular for the subtask of spam detection (Androutsopoulos et al., 2000). Finally, there are a couple of works such as (Hassan et al., 2007) that used the graph-of-words representation to propose alternative weights for the n</context>
<context position="15636" citStr="Joachims, 1998" startWordPosition="2530" endWordPosition="2531">-level. By pre-order traversal of this tree, gSpan discovers all frequent subgraphs with required support. Consider the set of all subgraphs in the collection of graphs, which corresponds to the set of all potential features. Note that there may be overlapping (subgraphs sharing nodes/edges) and redundant (subgraphs included in others) features. Because its size is exponential in the number of edges (just like the number of n-grams is exponential in n), it is common to only retain/mine the most frequent subgraphs (again just like for n-grams with a minimum document frequency (Fürnkranz, 1998; Joachims, 1998)). This is controlled via a parameter known as the support, which sets the minimum number of graphs in which a given subgraph has to appear to be considered as a feature, i. e. the number of subgraph matches in the collection. Here, since node labels are unique inside a graph, we do not have to consider multiple occurrences of the same subgraph in a given graph. The lower the support, the more features selected/considered but the more expensive the mining and the training (not only in time spent for the learning but also for the feature vector generation). 4.2 Unsupervised support selection Th</context>
<context position="17787" citStr="Joachims, 1998" startWordPosition="2881" endWordPosition="2882">support of 1) as opposed to a more manageable number of features corresponding to a higher support. Therefore, we propose to select the support using the so-called elbow method. This is an unsupervised empirical method initially developed for selecting the number of clusters in k-means (Thorndike, 1953). Figure 3 (upper plots) in Section 5 illustrates this process. 1705 4.3 Considered classifiers In text categorization, standard baseline classifiers include k-nearest neighbors (kNN) (Larkey and Croft, 1996), Naive Bayes (NB) (McCallum and Nigam, 1998) and linear Support Vector Machines (SVM) (Joachims, 1998) with the latter performing the best on n-gram features as verified in our experiments. Since our subgraph features correspond to “long-distance n-grams”, we used linear SVMs as our classifiers in all our experiments – the goal of our work being to explore and propose better features rather than a different classifier. 4.4 Multiclass scenario In standard binary graph classification (e. g., predicting chemical compounds’ carcinogenicity as either positive or negative (Helma et al., 2001)), feature mining is performed on the whole graph collection as we expect the mined features to be able to di</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Text categorization with Support Vector Machines: Learning with many relevant features. In Proceedings of the 10th European Conference on Machine Learning, ECML ’98, pages 137–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Training Linear SVMs in Linear Time.</title>
<date>2006</date>
<booktitle>In Proceedings of the 12th ACM SIGKDD international conference on Knowledge Discovery and Data mining, KDD ’06,</booktitle>
<pages>217--226</pages>
<contexts>
<context position="21689" citStr="Joachims, 2006" startWordPosition="3536" endWordPosition="3537">s, we can capitalize on main core retention to still extract binary n-gram features for classification but considering only the terms belonging to the main core of each document. Because some terms never belong to any main core of any document, the dimension of the overall feature space decreases. Additionally, since a document is only represented by a subset of its original terms, the number of non-zero feature values per document also decreases, which matters for SVM, even for the linear kernel, when considering the dual formulation or in the primal with more recent optimization techniques (Joachims, 2006). Compared to most existing feature selection techniques in the field (Yang and Pedersen, 1997), it is unsupervised and corpus-independent as it does not rely on any labeled data like IG, MI or χ2 nor any collection-wide statistics like IDF, which can be of interest for large-scale text categorization in order to process documents in parallel, independently of each other. In some sense, it is similar to what Özgür et al. (2005) proposed with corpus-based and class-based keyword selection for text classification except that we use here document-based keyword selection following the approach fro</context>
</contexts>
<marker>Joachims, 2006</marker>
<rawString>Thorsten Joachims. 2006. Training Linear SVMs in Linear Time. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge Discovery and Data mining, KDD ’06, pages 217– 226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hisashi Kashima</author>
<author>Koji Tsuda</author>
<author>Akihiro Inokuchi</author>
</authors>
<title>Marginalized kernels between labeled graphs.</title>
<date>2003</date>
<booktitle>In Proceedings of the 20th International Conference on Machine Learning,</booktitle>
<volume>3</volume>
<pages>321--328</pages>
<contexts>
<context position="7562" citStr="Kashima et al. (2003)" startWordPosition="1186" endWordPosition="1189"> as many times as the number of classes instead of just once (one-vs-all learning strategy). In this paper, we tackle the scalability issue differently through an unsupervised feature selection approach to reduce the size of the graphs and a fortiori the number of frequent subgraphs. 2.2.2 Graph kernels Gärtner et al. (2003) proposed the first kernels between graphs (as opposed to previous kernels on graphs, i. e. between nodes) based on either random walks or cycles to tackle the problem of classification between graphs. In parallel, the idea of marginalized kernels was extended to graphs by Kashima et al. (2003) and by Mahé et al. (2004). We refer to (Vishwanathan et al., 2010) for an indepth review of the topic and in particular its limitations in terms of number of unique node labels, which make them unsuitable for our problem as tested in practice (limited to a few tens of unique labels compared to hundreds of thousands for us). 2.3 Similar works The work of Markov et al. (2007) is perhaps the closest to ours since they also perform subgraph feature mining on graph-of-words representations but with non-standard datasets and baselines. The works of Jiang et al. (2010) and Arora et al. (2010) are al</context>
</contexts>
<marker>Kashima, Tsuda, Inokuchi, 2003</marker>
<rawString>Hisashi Kashima, Koji Tsuda, and Akihiro Inokuchi. 2003. Marginalized kernels between labeled graphs. In Proceedings of the 20th International Conference on Machine Learning, volume 3 of ICML ’03, pages 321–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>A Boosting Algorithm for Classification of Semi-Structured Text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 9th Conference on Empirical Methods in Natural Language Processing,</booktitle>
<volume>4</volume>
<pages>301--308</pages>
<contexts>
<context position="8328" citStr="Kudo and Matsumoto (2004)" startWordPosition="1319" endWordPosition="1322">erms of number of unique node labels, which make them unsuitable for our problem as tested in practice (limited to a few tens of unique labels compared to hundreds of thousands for us). 2.3 Similar works The work of Markov et al. (2007) is perhaps the closest to ours since they also perform subgraph feature mining on graph-of-words representations but with non-standard datasets and baselines. The works of Jiang et al. (2010) and Arora et al. (2010) are also related but their representations are different and closer to parse and dependency trees used as base features for text categorization by Kudo and Matsumoto (2004) and Matsumoto et al. (2005). Moreover, they do not discuss the choice of the support value, which controls the total number of features and can potentially lead to millions of subgraphs on standard datasets. 1703 3 Preliminary concepts In this section, we introduce the preliminary concepts upon which our work is built. 3.1 Graph-of-words We model a textual document as a graph-of-words, which corresponds to a graph whose vertices represent unique terms of the document and whose edges represent co-occurrences between the terms within a fixed-size sliding window. The underlying assumption is tha</context>
</contexts>
<marker>Kudo, Matsumoto, 2004</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2004. A Boosting Algorithm for Classification of Semi-Structured Text. In Proceedings of the 9th Conference on Empirical Methods in Natural Language Processing, volume 4 of EMNLP ’04, pages 301–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Eisaku Maeda</author>
<author>Yuji Matsumoto</author>
</authors>
<title>An application of boosting to graph classification.</title>
<date>2004</date>
<booktitle>In Advances in Neural Information Processing Systems 17, NIPS ’04,</booktitle>
<pages>729--736</pages>
<contexts>
<context position="5847" citStr="Kudo et al., 2004" startWordPosition="910" endWordPosition="913">ures but still without considering the task as a graph classification problem. 2.2 Graph classification Graph classification corresponds to the task of automatically predicting the class label of a given graph. The learning part in itself does not differ from other supervised learning problems and most proposed methods deal with the feature extraction part. They fall into two main categories: approaches that consider subgraphs as features and graph kernels. 2.2.1 Subgraphs as features The main idea is to mine frequent subgraphs and use them as features for classification, be it with Adaboost (Kudo et al., 2004) or a linear SVM (Deshpande et al., 2005). Indeed, most datasets that were used in the associated experiments correspond to chemical compounds where repeating substructure patterns are good indicators of belonging to one particular class. Some popular graph pattern mining algorithms are gSpan (Yan and Han, 2002), FFSM (Huan et al., 2003) and Gaston (Nijssen and Kok, 2004). The number of frequent subgraphs can be enormous, especially for large graph collections, and handling such a feature set can be very expensive. To overcome this issue, recent works have proposed to retain or even only mine </context>
</contexts>
<marker>Kudo, Maeda, Matsumoto, 2004</marker>
<rawString>Taku Kudo, Eisaku Maeda, and Yuji Matsumoto. 2004. An application of boosting to graph classification. In Advances in Neural Information Processing Systems 17, NIPS ’04, pages 729–736.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leah S Larkey</author>
<author>W Bruce Croft</author>
</authors>
<title>Combining Classifiers in Text Categorization.</title>
<date>1996</date>
<booktitle>In Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’96,</booktitle>
<pages>289--297</pages>
<contexts>
<context position="17684" citStr="Larkey and Croft, 1996" startWordPosition="2864" endWordPosition="2867">, we observed that the prediction performances did not benefit that much from using all the possible features (support of 1) as opposed to a more manageable number of features corresponding to a higher support. Therefore, we propose to select the support using the so-called elbow method. This is an unsupervised empirical method initially developed for selecting the number of clusters in k-means (Thorndike, 1953). Figure 3 (upper plots) in Section 5 illustrates this process. 1705 4.3 Considered classifiers In text categorization, standard baseline classifiers include k-nearest neighbors (kNN) (Larkey and Croft, 1996), Naive Bayes (NB) (McCallum and Nigam, 1998) and linear Support Vector Machines (SVM) (Joachims, 1998) with the latter performing the best on n-gram features as verified in our experiments. Since our subgraph features correspond to “long-distance n-grams”, we used linear SVMs as our classifiers in all our experiments – the goal of our work being to explore and propose better features rather than a different classifier. 4.4 Multiclass scenario In standard binary graph classification (e. g., predicting chemical compounds’ carcinogenicity as either positive or negative (Helma et al., 2001)), fea</context>
</contexts>
<marker>Larkey, Croft, 1996</marker>
<rawString>Leah S. Larkey and W. Bruce Croft. 1996. Combining Classifiers in Text Categorization. In Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’96, pages 289–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Mahé</author>
<author>Nobuhisa Ueda</author>
<author>Tatsuya Akutsu</author>
<author>JeanLuc Perret</author>
<author>Jean-Philippe Vert</author>
</authors>
<title>Extensions of marginalized graph kernels.</title>
<date>2004</date>
<booktitle>In Proceedings of the 21st International Conference on Machine Learning, ICML ’04,</booktitle>
<pages>70--78</pages>
<contexts>
<context position="7588" citStr="Mahé et al. (2004)" startWordPosition="1192" endWordPosition="1195">of classes instead of just once (one-vs-all learning strategy). In this paper, we tackle the scalability issue differently through an unsupervised feature selection approach to reduce the size of the graphs and a fortiori the number of frequent subgraphs. 2.2.2 Graph kernels Gärtner et al. (2003) proposed the first kernels between graphs (as opposed to previous kernels on graphs, i. e. between nodes) based on either random walks or cycles to tackle the problem of classification between graphs. In parallel, the idea of marginalized kernels was extended to graphs by Kashima et al. (2003) and by Mahé et al. (2004). We refer to (Vishwanathan et al., 2010) for an indepth review of the topic and in particular its limitations in terms of number of unique node labels, which make them unsuitable for our problem as tested in practice (limited to a few tens of unique labels compared to hundreds of thousands for us). 2.3 Similar works The work of Markov et al. (2007) is perhaps the closest to ours since they also perform subgraph feature mining on graph-of-words representations but with non-standard datasets and baselines. The works of Jiang et al. (2010) and Arora et al. (2010) are also related but their repre</context>
</contexts>
<marker>Mahé, Ueda, Akutsu, Perret, Vert, 2004</marker>
<rawString>Pierre Mahé, Nobuhisa Ueda, Tatsuya Akutsu, JeanLuc Perret, and Jean-Philippe Vert. 2004. Extensions of marginalized graph kernels. In Proceedings of the 21st International Conference on Machine Learning, ICML ’04, pages 70–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Schütze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<marker>Manning, Raghavan, Schütze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008. Introduction to Information Retrieval. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Markov</author>
<author>Mark Last</author>
<author>Abraham Kandel</author>
</authors>
<title>Fast Categorization of Web Documents Represented by Graphs.</title>
<date>2007</date>
<booktitle>In Advances in Web Mining and Web Usage Analysis, number 4811 in Lecture Notes in Artificial Intelligence,</booktitle>
<pages>56--71</pages>
<contexts>
<context position="7939" citStr="Markov et al. (2007)" startWordPosition="1257" endWordPosition="1260">ed to previous kernels on graphs, i. e. between nodes) based on either random walks or cycles to tackle the problem of classification between graphs. In parallel, the idea of marginalized kernels was extended to graphs by Kashima et al. (2003) and by Mahé et al. (2004). We refer to (Vishwanathan et al., 2010) for an indepth review of the topic and in particular its limitations in terms of number of unique node labels, which make them unsuitable for our problem as tested in practice (limited to a few tens of unique labels compared to hundreds of thousands for us). 2.3 Similar works The work of Markov et al. (2007) is perhaps the closest to ours since they also perform subgraph feature mining on graph-of-words representations but with non-standard datasets and baselines. The works of Jiang et al. (2010) and Arora et al. (2010) are also related but their representations are different and closer to parse and dependency trees used as base features for text categorization by Kudo and Matsumoto (2004) and Matsumoto et al. (2005). Moreover, they do not discuss the choice of the support value, which controls the total number of features and can potentially lead to millions of subgraphs on standard datasets. 17</context>
</contexts>
<marker>Markov, Last, Kandel, 2007</marker>
<rawString>Alex Markov, Mark Last, and Abraham Kandel. 2007. Fast Categorization of Web Documents Represented by Graphs. In Advances in Web Mining and Web Usage Analysis, number 4811 in Lecture Notes in Artificial Intelligence, pages 56–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shotaro Matsumoto</author>
<author>Hiroya Takamura</author>
<author>Manabu Okumura</author>
</authors>
<title>Sentiment Classification Using Word Sub-sequences and Dependency Sub-trees.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining, PAKDD ’05,</booktitle>
<pages>301--311</pages>
<contexts>
<context position="8356" citStr="Matsumoto et al. (2005)" startWordPosition="1324" endWordPosition="1327">labels, which make them unsuitable for our problem as tested in practice (limited to a few tens of unique labels compared to hundreds of thousands for us). 2.3 Similar works The work of Markov et al. (2007) is perhaps the closest to ours since they also perform subgraph feature mining on graph-of-words representations but with non-standard datasets and baselines. The works of Jiang et al. (2010) and Arora et al. (2010) are also related but their representations are different and closer to parse and dependency trees used as base features for text categorization by Kudo and Matsumoto (2004) and Matsumoto et al. (2005). Moreover, they do not discuss the choice of the support value, which controls the total number of features and can potentially lead to millions of subgraphs on standard datasets. 1703 3 Preliminary concepts In this section, we introduce the preliminary concepts upon which our work is built. 3.1 Graph-of-words We model a textual document as a graph-of-words, which corresponds to a graph whose vertices represent unique terms of the document and whose edges represent co-occurrences between the terms within a fixed-size sliding window. The underlying assumption is that all the words present in a</context>
</contexts>
<marker>Matsumoto, Takamura, Okumura, 2005</marker>
<rawString>Shotaro Matsumoto, Hiroya Takamura, and Manabu Okumura. 2005. Sentiment Classification Using Word Sub-sequences and Dependency Sub-trees. In Proceedings of the 9th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining, PAKDD ’05, pages 301–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Kamal Nigam</author>
</authors>
<title>A comparison of event models for Naive Bayes text classification.</title>
<date>1998</date>
<booktitle>In Proceedings of the AAAI workshop on learning for text categorization, AAAI ’98,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="4986" citStr="McCallum and Nigam, 1998" startWordPosition="772" endWordPosition="775">refer to (Sebastiani, 2002) for an in-depth review of the earliest works in the field and (Aggarwal and Zhai, 2012) for a survey of the more recent works that capitalize on additional metainformation. We note in particular the seminal work of Joachims (1998) who was the first to propose the use of a linear SVM with TF×IDF term features for the task. This approach is one of the standard baselines because of its simplicity yet effectiveness (unsupervised n-gram feature mining followed by standard supervised learning). Another popular approach is the use of Naive Bayes and its multiple variants (McCallum and Nigam, 1998), in particular for the subtask of spam detection (Androutsopoulos et al., 2000). Finally, there are a couple of works such as (Hassan et al., 2007) that used the graph-of-words representation to propose alternative weights for the n-gram features but still without considering the task as a graph classification problem. 2.2 Graph classification Graph classification corresponds to the task of automatically predicting the class label of a given graph. The learning part in itself does not differ from other supervised learning problems and most proposed methods deal with the feature extraction par</context>
<context position="17729" citStr="McCallum and Nigam, 1998" startWordPosition="2871" endWordPosition="2874">ces did not benefit that much from using all the possible features (support of 1) as opposed to a more manageable number of features corresponding to a higher support. Therefore, we propose to select the support using the so-called elbow method. This is an unsupervised empirical method initially developed for selecting the number of clusters in k-means (Thorndike, 1953). Figure 3 (upper plots) in Section 5 illustrates this process. 1705 4.3 Considered classifiers In text categorization, standard baseline classifiers include k-nearest neighbors (kNN) (Larkey and Croft, 1996), Naive Bayes (NB) (McCallum and Nigam, 1998) and linear Support Vector Machines (SVM) (Joachims, 1998) with the latter performing the best on n-gram features as verified in our experiments. Since our subgraph features correspond to “long-distance n-grams”, we used linear SVMs as our classifiers in all our experiments – the goal of our work being to explore and propose better features rather than a different classifier. 4.4 Multiclass scenario In standard binary graph classification (e. g., predicting chemical compounds’ carcinogenicity as either positive or negative (Helma et al., 2001)), feature mining is performed on the whole graph c</context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>Andrew McCallum and Kamal Nigam. 1998. A comparison of event models for Naive Bayes text classification. In Proceedings of the AAAI workshop on learning for text categorization, AAAI ’98, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>TextRank: Bringing Order into Texts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 9th Conference on Empirical Methods in Natural Language Processing, EMNLP ’04,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="9214" citStr="Mihalcea and Tarau, 2004" startWordPosition="1461" endWordPosition="1465">oduce the preliminary concepts upon which our work is built. 3.1 Graph-of-words We model a textual document as a graph-of-words, which corresponds to a graph whose vertices represent unique terms of the document and whose edges represent co-occurrences between the terms within a fixed-size sliding window. The underlying assumption is that all the words present in a document have some undirected relationships with the others, modulo a window size outside of which the relationship is not considered. This representation was first used in keyword extraction and summarization (Ohsawa et al., 1998; Mihalcea and Tarau, 2004) and more recently in ad hoc IR (Blanco and Lioma, 2012; Rousseau and Vazirgiannis, 2013). We refer to (Blanco and Lioma, 2012) for an in-depth review of the graph representations of text in NLP. As a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software. Figure 1: Graph-of-words representation of a texhde me tual document – in bold font, its main core. Figure 1 illustrates the graph-of-words representation of a textual document. The vertices co</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing Order into Texts. In Proceedings of the 9th Conference on Empirical Methods in Natural Language Processing, EMNLP ’04, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siegfried Nijssen</author>
<author>Joost N Kok</author>
</authors>
<title>A Quickstart in Frequent Structure Mining Can Make a Difference.</title>
<date>2004</date>
<booktitle>In Proceedings of the 10th ACM SIGKDD international conference on Knowledge Discovery and Data mining, KDD ’04,</booktitle>
<pages>647--652</pages>
<contexts>
<context position="6221" citStr="Nijssen and Kok, 2004" startWordPosition="970" endWordPosition="973">into two main categories: approaches that consider subgraphs as features and graph kernels. 2.2.1 Subgraphs as features The main idea is to mine frequent subgraphs and use them as features for classification, be it with Adaboost (Kudo et al., 2004) or a linear SVM (Deshpande et al., 2005). Indeed, most datasets that were used in the associated experiments correspond to chemical compounds where repeating substructure patterns are good indicators of belonging to one particular class. Some popular graph pattern mining algorithms are gSpan (Yan and Han, 2002), FFSM (Huan et al., 2003) and Gaston (Nijssen and Kok, 2004). The number of frequent subgraphs can be enormous, especially for large graph collections, and handling such a feature set can be very expensive. To overcome this issue, recent works have proposed to retain or even only mine the discriminative subgraphs, i. e. features that contribute to the classification decision, in particular gBoost (Saigo et al., 2009), CORK (Thoma et al., 2009) and GAIA (Jin et al., 2010). However, when experimenting, gBoost did not converge on our larger datasets while GAIA and CORK consider subgraphs of node size at least 2, which exclude unigrams, resulting in poorer</context>
</contexts>
<marker>Nijssen, Kok, 2004</marker>
<rawString>Siegfried Nijssen and Joost N. Kok. 2004. A Quickstart in Frequent Structure Mining Can Make a Difference. In Proceedings of the 10th ACM SIGKDD international conference on Knowledge Discovery and Data mining, KDD ’04, pages 647–652.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yukio Ohsawa</author>
<author>Nels E Benson</author>
<author>Masahiko Yachida</author>
</authors>
<title>KeyGraph: Automatic Indexing by Co-occurrence Graph Based on Building Construction Metaphor.</title>
<date>1998</date>
<booktitle>In Proceedings of the Advances in Digital Libraries Conference, ADL ’98,</booktitle>
<pages>12--18</pages>
<contexts>
<context position="9187" citStr="Ohsawa et al., 1998" startWordPosition="1457" endWordPosition="1460">this section, we introduce the preliminary concepts upon which our work is built. 3.1 Graph-of-words We model a textual document as a graph-of-words, which corresponds to a graph whose vertices represent unique terms of the document and whose edges represent co-occurrences between the terms within a fixed-size sliding window. The underlying assumption is that all the words present in a document have some undirected relationships with the others, modulo a window size outside of which the relationship is not considered. This representation was first used in keyword extraction and summarization (Ohsawa et al., 1998; Mihalcea and Tarau, 2004) and more recently in ad hoc IR (Blanco and Lioma, 2012; Rousseau and Vazirgiannis, 2013). We refer to (Blanco and Lioma, 2012) for an in-depth review of the graph representations of text in NLP. As a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software. Figure 1: Graph-of-words representation of a texhde me tual document – in bold font, its main core. Figure 1 illustrates the graph-of-words representation of a textua</context>
</contexts>
<marker>Ohsawa, Benson, Yachida, 1998</marker>
<rawString>Yukio Ohsawa, Nels E. Benson, and Masahiko Yachida. 1998. KeyGraph: Automatic Indexing by Co-occurrence Graph Based on Building Construction Metaphor. In Proceedings of the Advances in Digital Libraries Conference, ADL ’98, pages 12– 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arzucan Özgür</author>
<author>Levent Özgür</author>
<author>Tunga Güngör</author>
</authors>
<title>Text Categorization with Class-based and Corpus-based Keyword Selection.</title>
<date>2005</date>
<booktitle>In Proceedings of the 20th International Conference on Computer and Information Sciences, ISCIS ’05,</booktitle>
<pages>606--615</pages>
<contexts>
<context position="22120" citStr="Özgür et al. (2005)" startWordPosition="3607" endWordPosition="3610">document also decreases, which matters for SVM, even for the linear kernel, when considering the dual formulation or in the primal with more recent optimization techniques (Joachims, 2006). Compared to most existing feature selection techniques in the field (Yang and Pedersen, 1997), it is unsupervised and corpus-independent as it does not rely on any labeled data like IG, MI or χ2 nor any collection-wide statistics like IDF, which can be of interest for large-scale text categorization in order to process documents in parallel, independently of each other. In some sense, it is similar to what Özgür et al. (2005) proposed with corpus-based and class-based keyword selection for text classification except that we use here document-based keyword selection following the approach from Rousseau and Vazirgiannis (2015). 1706 5 Experiments In this section we present the experiments we conducted to validate our approaches. 5.1 Datasets We used four standard text datasets: two for multiclass document categorization (WebKB and R8), one for spam detection (LingSpam) and one for opinion mining (Amazon) so as to cover all the main subtasks of text categorization: • WebKB: 4 most frequent categories among labeled we</context>
</contexts>
<marker>Özgür, Özgür, Güngör, 2005</marker>
<rawString>Arzucan Özgür, Levent Özgür, and Tunga Güngör. 2005. Text Categorization with Class-based and Corpus-based Keyword Selection. In Proceedings of the 20th International Conference on Computer and Information Sciences, ISCIS ’05, pages 606– 615.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Gaël Varoquaux</author>
<author>Alexandre Gramfort</author>
<author>Vincent Michel</author>
<author>Bertrand Thirion</author>
<author>Olivier Grisel</author>
<author>Mathieu Blondel</author>
<author>Peter Prettenhofer</author>
<author>Ron Weiss</author>
<author>Vincent Dubourg</author>
<author>J Vanderplas</author>
<author>A Passos</author>
<author>D Cournapeau</author>
<author>M Brucher</author>
<author>M Perrot</author>
<author>E Duchesnay</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<contexts>
<context position="23807" citStr="Pedregosa et al., 2011" startWordPosition="3871" endWordPosition="3874">• Amazon: 8,000 product reviews over four different sub-collections (books, DVDs, electronics and kitchen appliances) classified as positive or negative – split into 1,600 for training and 400 for test each (Blitzer et al., 2007). 5.2 Implementation We developed our approaches mostly in Python using the igraph library (Csardi and Nepusz, 2006) for the graph representation and main core extraction. For unsupervised subgraph feature mining, we used the C++ implementation of gSpan from gBoost (Saigo et al., 2009). Finally for classification and standard n-gram text categorization we used scikit (Pedregosa et al., 2011), a standard Python machine learning library. 5.3 Evaluation metrics To evaluate the performance of our proposed approaches over standard baselines, we computed on the test set both the micro- and macro-average F1- score. Because we are dealing with single-label classification, the micro-average F1-score corresponds to the accuracy and is a measure of the overall prediction effectiveness (Manning et al., Dataset # subgraphs before # subgraphs after reduction WebKB 30,868 10,113 67 % R8 39,428 11,373 71 % LingSpam 54,779 15,514 72 % Amazon 16,415 8,745 47 % Dataset # n-grams before # n-grams af</context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, Duchesnay, 2011</marker>
<rawString>Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. The Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>François Rousseau</author>
<author>Michalis Vazirgiannis</author>
</authors>
<title>Graph-of-word and TW-IDF: New Approach to Ad Hoc IR.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd ACM international conference on Information and knowledge management, CIKM ’13,</booktitle>
<pages>59--68</pages>
<contexts>
<context position="9303" citStr="Rousseau and Vazirgiannis, 2013" startWordPosition="1477" endWordPosition="1481"> model a textual document as a graph-of-words, which corresponds to a graph whose vertices represent unique terms of the document and whose edges represent co-occurrences between the terms within a fixed-size sliding window. The underlying assumption is that all the words present in a document have some undirected relationships with the others, modulo a window size outside of which the relationship is not considered. This representation was first used in keyword extraction and summarization (Ohsawa et al., 1998; Mihalcea and Tarau, 2004) and more recently in ad hoc IR (Blanco and Lioma, 2012; Rousseau and Vazirgiannis, 2013). We refer to (Blanco and Lioma, 2012) for an in-depth review of the graph representations of text in NLP. As a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software. Figure 1: Graph-of-words representation of a texhde me tual document – in bold font, its main core. Figure 1 illustrates the graph-of-words representation of a textual document. The vertices correspond to the remaining terms after standard preprocessing steps have been applied (tok</context>
</contexts>
<marker>Rousseau, Vazirgiannis, 2013</marker>
<rawString>François Rousseau and Michalis Vazirgiannis. 2013. Graph-of-word and TW-IDF: New Approach to Ad Hoc IR. In Proceedings of the 22nd ACM international conference on Information and knowledge management, CIKM ’13, pages 59–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>François Rousseau</author>
<author>Michalis Vazirgiannis</author>
</authors>
<title>Main Core Retention on Graph-of-words for SingleDocument Keyword Extraction.</title>
<date>2015</date>
<booktitle>In Proceedings of the 37th European Conference on Information Retrieval, ECIR ’15,</booktitle>
<pages>382--393</pages>
<contexts>
<context position="22323" citStr="Rousseau and Vazirgiannis (2015)" startWordPosition="3634" endWordPosition="3637">Compared to most existing feature selection techniques in the field (Yang and Pedersen, 1997), it is unsupervised and corpus-independent as it does not rely on any labeled data like IG, MI or χ2 nor any collection-wide statistics like IDF, which can be of interest for large-scale text categorization in order to process documents in parallel, independently of each other. In some sense, it is similar to what Özgür et al. (2005) proposed with corpus-based and class-based keyword selection for text classification except that we use here document-based keyword selection following the approach from Rousseau and Vazirgiannis (2015). 1706 5 Experiments In this section we present the experiments we conducted to validate our approaches. 5.1 Datasets We used four standard text datasets: two for multiclass document categorization (WebKB and R8), one for spam detection (LingSpam) and one for opinion mining (Amazon) so as to cover all the main subtasks of text categorization: • WebKB: 4 most frequent categories among labeled webpages from various CS departments – split into 2,803 for training and 1,396 for test (Cardoso-Cachopo, 2007, p. 39–41). • R8: 8 most frequent categories of Reuters21578, a set of labeled news articles f</context>
</contexts>
<marker>Rousseau, Vazirgiannis, 2015</marker>
<rawString>François Rousseau and Michalis Vazirgiannis. 2015. Main Core Retention on Graph-of-words for SingleDocument Keyword Extraction. In Proceedings of the 37th European Conference on Information Retrieval, ECIR ’15, pages 382–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroto Saigo</author>
<author>Sebastian Nowozin</author>
<author>Tadashi Kadowaki</author>
<author>Taku Kudo</author>
<author>Koji Tsuda</author>
</authors>
<title>gBoost: a mathematical programming approach to graph classification and regression.</title>
<date>2009</date>
<booktitle>Machine Learning,</booktitle>
<volume>75</volume>
<issue>1</issue>
<pages>89</pages>
<contexts>
<context position="6581" citStr="Saigo et al., 2009" startWordPosition="1026" endWordPosition="1029">spond to chemical compounds where repeating substructure patterns are good indicators of belonging to one particular class. Some popular graph pattern mining algorithms are gSpan (Yan and Han, 2002), FFSM (Huan et al., 2003) and Gaston (Nijssen and Kok, 2004). The number of frequent subgraphs can be enormous, especially for large graph collections, and handling such a feature set can be very expensive. To overcome this issue, recent works have proposed to retain or even only mine the discriminative subgraphs, i. e. features that contribute to the classification decision, in particular gBoost (Saigo et al., 2009), CORK (Thoma et al., 2009) and GAIA (Jin et al., 2010). However, when experimenting, gBoost did not converge on our larger datasets while GAIA and CORK consider subgraphs of node size at least 2, which exclude unigrams, resulting in poorer performances. Moreover, all these approaches have been developed for binary classification, which meant mining features as many times as the number of classes instead of just once (one-vs-all learning strategy). In this paper, we tackle the scalability issue differently through an unsupervised feature selection approach to reduce the size of the graphs and </context>
<context position="14635" citStr="Saigo et al., 2009" startWordPosition="2364" endWordPosition="2367"> for chemical compounds, shared subparts of molecules are good indicators of belonging to one particular class. We assumed it would the same for text. Indeed, subgraphs of graph-of-words correspond to sets of words co-occurring together, just not necessarily always as the same sequence like for n-grams – it can be seen as a relaxed definition of a n-gram to capture additional variants. We used gSpan (graph-based Substructure pattern (Yan and Han, 2002)) as frequent subgraph miner like (Jiang et al., 2010; Arora et al., 2010) mostly because of its fast available C++ implementation from gBoost (Saigo et al., 2009). Briefly, the key idea behind gSpan is that instead of enumerating all the subgraphs and testing for isomorphism throughout the collection, it first builds for each graph a lexicographic order of all the edges using depth-first-search (DFS) traversal and assigns to it a unique minimum DFS code. Based on all these DFS codes, a hierarchical search tree is constructed at the collection-level. By pre-order traversal of this tree, gSpan discovers all frequent subgraphs with required support. Consider the set of all subgraphs in the collection of graphs, which corresponds to the set of all potentia</context>
<context position="23699" citStr="Saigo et al., 2009" startWordPosition="3855" endWordPosition="3858">r legitimate messages – split into 10 sets for 10-fold cross validation (Androutsopoulos et al., 2000). • Amazon: 8,000 product reviews over four different sub-collections (books, DVDs, electronics and kitchen appliances) classified as positive or negative – split into 1,600 for training and 400 for test each (Blitzer et al., 2007). 5.2 Implementation We developed our approaches mostly in Python using the igraph library (Csardi and Nepusz, 2006) for the graph representation and main core extraction. For unsupervised subgraph feature mining, we used the C++ implementation of gSpan from gBoost (Saigo et al., 2009). Finally for classification and standard n-gram text categorization we used scikit (Pedregosa et al., 2011), a standard Python machine learning library. 5.3 Evaluation metrics To evaluate the performance of our proposed approaches over standard baselines, we computed on the test set both the micro- and macro-average F1- score. Because we are dealing with single-label classification, the micro-average F1-score corresponds to the accuracy and is a measure of the overall prediction effectiveness (Manning et al., Dataset # subgraphs before # subgraphs after reduction WebKB 30,868 10,113 67 % R8 3</context>
</contexts>
<marker>Saigo, Nowozin, Kadowaki, Kudo, Tsuda, 2009</marker>
<rawString>Hiroto Saigo, Sebastian Nowozin, Tadashi Kadowaki, Taku Kudo, and Koji Tsuda. 2009. gBoost: a mathematical programming approach to graph classification and regression. Machine Learning, 75(1):69– 89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Machine Learning in Automated Text Categorization.</title>
<date>2002</date>
<journal>ACM Computing Surveys,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="4388" citStr="Sebastiani, 2002" startWordPosition="670" endWordPosition="671">ings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1702–1712, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Related work In this section, we present the related work in text categorization, graph classification and the combination of the two fields like in our case. 2.1 Text categorization Text categorization, a.k.a. text classification, corresponds to the task of automatically predicting the class label of a given textual document. We refer to (Sebastiani, 2002) for an in-depth review of the earliest works in the field and (Aggarwal and Zhai, 2012) for a survey of the more recent works that capitalize on additional metainformation. We note in particular the seminal work of Joachims (1998) who was the first to propose the use of a linear SVM with TF×IDF term features for the task. This approach is one of the standard baselines because of its simplicity yet effectiveness (unsupervised n-gram feature mining followed by standard supervised learning). Another popular approach is the use of Naive Bayes and its multiple variants (McCallum and Nigam, 1998), </context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>Fabrizio Sebastiani. 2002. Machine Learning in Automated Text Categorization. ACM Computing Surveys, 34(1):1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen B Seidman</author>
</authors>
<title>Network structure and minimum degree.</title>
<date>1983</date>
<journal>Social Networks,</journal>
<pages>5--269</pages>
<contexts>
<context position="12005" citStr="Seidman (1983)" startWordPosition="1936" endWordPosition="1937">r H). In the case of graph-of-words, node labels are unique inside a graph since they represent unique terms of the document and we can therefore omit these functions since they are injective in our case and we can substitute node ids for node labels. In particular, the general problem of subgraph matching, which defines an isomorphism between a graph and a subgraph and is NP-complete (Garey and Johnson, 1990), can be reduced to a polynomial problem when node labels are unique. In our experiments, we used the standard algorithm VF2 developed by Cordella et al. (2001). 3.3 K-core and main core Seidman (1983) defined the k-core of a graph as the maximal connected subgraph whose vertices are at least of degree k within the subgraph. The non-empty k-core of largest k is called the main core and corresponds to the most cohesive set(s) of vertices. The corresponding value of k may differ from one graph to another. Batagelj and Zaveršnik (2003) proposed an algorithm to extract the main core of an unweighted graph in time linear in the number of edges, complexity similar in our case to the other NLP preprocessing steps. Bold font on Figure 1 indicates that a vertex belongs to the main core of the graph.</context>
</contexts>
<marker>Seidman, 1983</marker>
<rawString>Stephen B. Seidman. 1983. Network structure and minimum degree. Social Networks, 5:269–287.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Nino Shervashidze</author>
</authors>
<title>Visited on 30/05/2015. Graph kernels.</title>
<note>http://www.di.ens.fr/~shervashidze/code.html.</note>
<marker>Shervashidze, </marker>
<rawString>Nino Shervashidze. Visited on 30/05/2015. Graph kernels. http://www.di.ens.fr/~shervashidze/code.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marisa Thoma</author>
<author>Hong Cheng</author>
<author>Arthur Gretton</author>
<author>Jiawei Han</author>
<author>Hans-Peter Kriegel</author>
<author>Alexander J Smola</author>
<author>Philip S Yu Le Song</author>
<author>Xifeng Yan</author>
<author>Karsten M Borgwardt</author>
</authors>
<title>Near-optimal Supervised Feature Selection among Frequent Subgraphs.</title>
<date>2009</date>
<booktitle>In Proceedings of the SIAM International Conference on Data Mining, SDM ’09,</booktitle>
<pages>1076--1087</pages>
<marker>Thoma, Cheng, Gretton, Han, Kriegel, Smola, Le Song, Yan, Borgwardt, 2009</marker>
<rawString>Marisa Thoma, Hong Cheng, Arthur Gretton, Jiawei Han, Hans-Peter Kriegel, Alexander J. Smola, Le Song, Philip S. Yu, Xifeng Yan, and Karsten M. Borgwardt. 2009. Near-optimal Supervised Feature Selection among Frequent Subgraphs. In Proceedings of the SIAM International Conference on Data Mining, SDM ’09, pages 1076–1087.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Thorndike</author>
</authors>
<title>Who belongs in the family?</title>
<date>1953</date>
<journal>Psychometrika,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="17476" citStr="Thorndike, 1953" startWordPosition="2837" endWordPosition="2838">r of features increases slightly up until a point where it increases exponentially, which makes both the feature vector generation and the learning expensive, especially with multiple classes. Moreover, we observed that the prediction performances did not benefit that much from using all the possible features (support of 1) as opposed to a more manageable number of features corresponding to a higher support. Therefore, we propose to select the support using the so-called elbow method. This is an unsupervised empirical method initially developed for selecting the number of clusters in k-means (Thorndike, 1953). Figure 3 (upper plots) in Section 5 illustrates this process. 1705 4.3 Considered classifiers In text categorization, standard baseline classifiers include k-nearest neighbors (kNN) (Larkey and Croft, 1996), Naive Bayes (NB) (McCallum and Nigam, 1998) and linear Support Vector Machines (SVM) (Joachims, 1998) with the latter performing the best on n-gram features as verified in our experiments. Since our subgraph features correspond to “long-distance n-grams”, we used linear SVMs as our classifiers in all our experiments – the goal of our work being to explore and propose better features rath</context>
</contexts>
<marker>Thorndike, 1953</marker>
<rawString>Robert Thorndike. 1953. Who belongs in the family? Psychometrika, 18(4):267–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S V N Vishwanathan</author>
<author>Nicol N Schraudolph</author>
<author>Risi Kondor</author>
<author>Karsten M Borgwardt</author>
</authors>
<title>Graph kernels.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>11--1201</pages>
<contexts>
<context position="7629" citStr="Vishwanathan et al., 2010" startWordPosition="1199" endWordPosition="1202">one-vs-all learning strategy). In this paper, we tackle the scalability issue differently through an unsupervised feature selection approach to reduce the size of the graphs and a fortiori the number of frequent subgraphs. 2.2.2 Graph kernels Gärtner et al. (2003) proposed the first kernels between graphs (as opposed to previous kernels on graphs, i. e. between nodes) based on either random walks or cycles to tackle the problem of classification between graphs. In parallel, the idea of marginalized kernels was extended to graphs by Kashima et al. (2003) and by Mahé et al. (2004). We refer to (Vishwanathan et al., 2010) for an indepth review of the topic and in particular its limitations in terms of number of unique node labels, which make them unsuitable for our problem as tested in practice (limited to a few tens of unique labels compared to hundreds of thousands for us). 2.3 Similar works The work of Markov et al. (2007) is perhaps the closest to ours since they also perform subgraph feature mining on graph-of-words representations but with non-standard datasets and baselines. The works of Jiang et al. (2010) and Arora et al. (2010) are also related but their representations are different and closer to pa</context>
</contexts>
<marker>Vishwanathan, Schraudolph, Kondor, Borgwardt, 2010</marker>
<rawString>S. V. N. Vishwanathan, Nicol N. Schraudolph, Risi Kondor, and Karsten M. Borgwardt. 2010. Graph kernels. Journal of Machine Learning Research, 11:1201–1242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xifeng Yan</author>
<author>Jiawei Han</author>
</authors>
<title>gspan: Graphbased substructure pattern mining.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2nd IEEE International Conference on Data Mining, ICDM ’02,</booktitle>
<pages>721--724</pages>
<contexts>
<context position="6160" citStr="Yan and Han, 2002" startWordPosition="959" endWordPosition="962">methods deal with the feature extraction part. They fall into two main categories: approaches that consider subgraphs as features and graph kernels. 2.2.1 Subgraphs as features The main idea is to mine frequent subgraphs and use them as features for classification, be it with Adaboost (Kudo et al., 2004) or a linear SVM (Deshpande et al., 2005). Indeed, most datasets that were used in the associated experiments correspond to chemical compounds where repeating substructure patterns are good indicators of belonging to one particular class. Some popular graph pattern mining algorithms are gSpan (Yan and Han, 2002), FFSM (Huan et al., 2003) and Gaston (Nijssen and Kok, 2004). The number of frequent subgraphs can be enormous, especially for large graph collections, and handling such a feature set can be very expensive. To overcome this issue, recent works have proposed to retain or even only mine the discriminative subgraphs, i. e. features that contribute to the classification decision, in particular gBoost (Saigo et al., 2009), CORK (Thoma et al., 2009) and GAIA (Jin et al., 2010). However, when experimenting, gBoost did not converge on our larger datasets while GAIA and CORK consider subgraphs of node</context>
<context position="14472" citStr="Yan and Han, 2002" startWordPosition="2336" endWordPosition="2339">the methods that consider subgraphs as features. Repeating substructure patterns between graphs are intuitively good candidates for classification since, at least for chemical compounds, shared subparts of molecules are good indicators of belonging to one particular class. We assumed it would the same for text. Indeed, subgraphs of graph-of-words correspond to sets of words co-occurring together, just not necessarily always as the same sequence like for n-grams – it can be seen as a relaxed definition of a n-gram to capture additional variants. We used gSpan (graph-based Substructure pattern (Yan and Han, 2002)) as frequent subgraph miner like (Jiang et al., 2010; Arora et al., 2010) mostly because of its fast available C++ implementation from gBoost (Saigo et al., 2009). Briefly, the key idea behind gSpan is that instead of enumerating all the subgraphs and testing for isomorphism throughout the collection, it first builds for each graph a lexicographic order of all the edges using depth-first-search (DFS) traversal and assigns to it a unique minimum DFS code. Based on all these DFS codes, a hierarchical search tree is constructed at the collection-level. By pre-order traversal of this tree, gSpan </context>
</contexts>
<marker>Yan, Han, 2002</marker>
<rawString>Xifeng Yan and Jiawei Han. 2002. gspan: Graphbased substructure pattern mining. In Proceedings of the 2nd IEEE International Conference on Data Mining, ICDM ’02, pages 721–724.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Xin Liu</author>
</authors>
<title>A Re-examination of Text Categorization Methods.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’99,</booktitle>
<pages>42--49</pages>
<contexts>
<context position="25031" citStr="Yang and Liu, 1999" startWordPosition="4072" endWordPosition="4075">eduction WebKB 1,849,848 735,447 60 % R8 1,604,280 788,465 51 % LingSpam 2,733,043 1,016,061 63 % Amazon 583,457 376,664 35 % Table 1: Total number of features (n-grams or subgraphs) vs. number of features present only in main cores along with the reduction of the dimension of the feature space on all four datasets. 2008, p. 281). Conversely, the macro-average F1- score takes into account the skewed class label distributions by weighting each class uniformly. The statistical significance of improvement in accuracy over the n-gram SVM baseline was assessed using the micro sign test (p &lt; 0.05) (Yang and Liu, 1999). For the Amazon dataset, we report the average of each metric over the four sub-collections. 5.4 Results Table 2 shows the results on the four considered datasets. The first three rows correspond to the baselines: unsupervised n-gram feature extraction and then supervised learning using kNN, NB (Multinomial but Bernoulli yields similar results) and linear SVM. The last three rows correspond to our approaches. In our first approach, denoted as “gSpan + SVM”, we mine frequent subgraphs (gSpan) as features and then train a linear SVM. These features correspond to long-distance n-grams. This lead</context>
</contexts>
<marker>Yang, Liu, 1999</marker>
<rawString>Yiming Yang and Xin Liu. 1999. A Re-examination of Text Categorization Methods. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’99, pages 42–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>J O Pedersen</author>
</authors>
<title>A Comparative Study on Feature Selection in Text Categorization.</title>
<date>1997</date>
<booktitle>In Proceedings of the 14th International Conference on Machine Learning, ICML ’97,</booktitle>
<pages>412--420</pages>
<contexts>
<context position="21784" citStr="Yang and Pedersen, 1997" startWordPosition="3548" endWordPosition="3551">r classification but considering only the terms belonging to the main core of each document. Because some terms never belong to any main core of any document, the dimension of the overall feature space decreases. Additionally, since a document is only represented by a subset of its original terms, the number of non-zero feature values per document also decreases, which matters for SVM, even for the linear kernel, when considering the dual formulation or in the primal with more recent optimization techniques (Joachims, 2006). Compared to most existing feature selection techniques in the field (Yang and Pedersen, 1997), it is unsupervised and corpus-independent as it does not rely on any labeled data like IG, MI or χ2 nor any collection-wide statistics like IDF, which can be of interest for large-scale text categorization in order to process documents in parallel, independently of each other. In some sense, it is similar to what Özgür et al. (2005) proposed with corpus-based and class-based keyword selection for text classification except that we use here document-based keyword selection following the approach from Rousseau and Vazirgiannis (2015). 1706 5 Experiments In this section we present the experimen</context>
</contexts>
<marker>Yang, Pedersen, 1997</marker>
<rawString>Yiming Yang and J. O. Pedersen. 1997. A Comparative Study on Feature Selection in Text Categorization. In Proceedings of the 14th International Conference on Machine Learning, ICML ’97, pages 412–420.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>