<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000034">
<title confidence="0.987446">
Entity Hierarchy Embedding
</title>
<author confidence="0.999834">
Zhiting Hu, Poyao Huang, Yuntian Deng, Yingkai Gao, Eric P. Xing
</author>
<affiliation confidence="0.886852666666667">
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.998763">
{zhitingh,poyaoh,yuntiand,yingkaig,epxing}@cs.cmu.edu
</email>
<sectionHeader confidence="0.993881" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999746444444445">
Existing distributed representations are
limited in utilizing structured knowledge
to improve semantic relatedness modeling.
We propose a principled framework of em-
bedding entities that integrates hierarchi-
cal information from large-scale knowl-
edge bases. The novel embedding model
associates each category node of the hi-
erarchy with a distance metric. To cap-
ture structured semantics, the entity sim-
ilarity of context prediction are measured
under the aggregated metrics of relevant
categories along all inter-entity paths. We
show that both the entity vectors and cat-
egory distance metrics encode meaningful
semantics. Experiments in entity linking
and entity search show superiority of the
proposed method.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999978116666667">
There has been a growing interest in distributed
representation that learns compact vectors (a.k.a
embedding) for words (Mikolov et al., 2013a),
phrases (Passos et al., 2014), and concepts (Hill
and Korhonen, 2014), etc. The induced vectors
are expected to capture semantic relatedness of
the linguistic items, and are widely used in senti-
ment analysis (Tang et al., 2014), machine transla-
tion (Zhang et al., 2014), and information retrieval
(Clinchant and Perronnin, 2013), to name a few.
Despite the impressive success, existing work
is still limited in utilizing structured knowledge
to enhance the representation. For instance, word
and phrase embeddings are largely induced from
plain text. Though recent knowledge graph em-
beddings (Lin et al., 2015; Wang et al., 2014) inte-
grate the relational structure among entities, they
primarily target at link prediction and lack an ex-
plicit relatedness measure.
In this paper, we propose to improve the dis-
tributed representations of entities by integrating
hierarchical information from large-scale knowl-
edge bases (KBs). An entity hierarchy groups en-
tities into categories which are further organized
to form a taxonomy. It provides rich structured
knowledge on entity relatedness (Resnik, 1995).
Our work goes beyond the previous heuristic use
of entity hierarchy which relies on hand-crafted
features (Kaptein and Kamps, 2013; Ponzetto
and Strube, 2007), and develops a principled
optimization-based framework. We learn a dis-
tance metric for each category node, and mea-
sure entity-context similarity under the aggregat-
ed metrics of all relevant categories. The met-
ric aggregation encodes the hierarchical property
that nearby entities tend to share common seman-
tic features. We further provide a highly-efficient
implementation in order to handle large complex
hierarchies.
We train a distributed representation for the w-
hole entity hierarchy of Wikipedia. Both the entity
vectors and the category distance metrics capture
meaningful semantics. We deploy the embedding
in both entity linking (Han and Sun, 2012) and
entity search (Demartini et al., 2010) tasks. Hi-
erarchy embedding significantly outperforms that
without structural knowledge. Our methods also
show superiority over existing competitors.
To the best of our knowledge, this is the first
work to learn distributed representations that in-
corporates hierarchical knowledge in a principled
framework. Our model that encodes hierarchy by
distance metric learning and aggregation provides
a potentially important and general scheme for u-
tilizing hierarchical knowledge.
The rest of the paper is organized as follows:
¬ß2 describes the proposed embedding model; ¬ß3
presents the application of the learned embed-
ding; ¬ß4 evaluates the approach; ¬ß5 reviews related
work; and finally, ¬ß6 concludes the paper.
</bodyText>
<page confidence="0.939484">
1292
</page>
<note confidence="0.984788">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1292‚Äì1300,
Beijing, China, July 26-31, 2015. cÔøΩ2015 Association for Computational Linguistics
</note>
<figure confidence="0.9723275625">
e1
...e2....e3...
(ùëí1, ùëí3)
Entity pairs
ùëí1, ùëí2
ùëÄùëí1.ùëí3 = ùúã‚Ñé1ùëÄ‚Ñé1 + ùúã‚Ñé2ùëÄ‚Ñé2 ùë£ùëí1 ùë£ùëí2
Dist. metrics
ùëÄùëí1,ùëí2 = ùëÄ‚Ñé2
ùëÄ‚Ñé2
e1 e2
ùëÄ‚Ñé1
h2
h1
ùë£ùëí3
e3
Text Context Entity Hierarchy
</figure>
<figureCaption confidence="0.9272565">
Figure 1: The model architecture. The text context of an entity is based on its KB encyclopedia article.
The entity hierarchical structure is incorporated through distance metric learning and aggregation.
</figureCaption>
<sectionHeader confidence="0.903736" genericHeader="introduction">
2 Entity Hierarchy Embedding
</sectionHeader>
<bodyText confidence="0.999974076923077">
The objective of the embedding model is to find
a representation for each entity that is useful for
predicting other entities occurring in its contex-
t. We build entity‚Äôs context upon KB encyclope-
dia articles, where entity annotations are readily
available. We further incorporate the entity hierar-
chical structure in the context prediction through
distance metric learning and aggregation, which
encodes the rich structured knowledge in the in-
duced representations. Our method is flexible and
efficient to model large complex DAG-structured
hierarchies. Figure 1 shows an overview of the
model architecture.
</bodyText>
<subsectionHeader confidence="0.998616">
2.1 Model Architecture
</subsectionHeader>
<bodyText confidence="0.999003730769231">
Our architecture builds on the skip-gram word
embedding framework (Mikolov et al., 2013b).
In the skip-gram model, a set of (target, con-
text) word pairs are extracted by sliding a fixed-
length context window over a text corpus, and the
word vectors are learned such that the similarity
of the target- and context-word vectors is maxi-
mized. We generalize both the context definition
and the similarity measure for entity hierarchy em-
bedding.
Unlike words that can be directly extracted from
plain text, entities are hidden semantics underly-
ing their surface forms. In order to avoid manual
annotation cost, we exploit the text corpora from
KBs where the referent entities of surface text are
readily annotated. Moreover, since a KB encyclo-
pedia article typically focuses on describing one
entity, we naturally extend the entity‚Äôs context as
its whole article, and obtain a set of entity pairs
D = {(eT, eC)1, where eT denotes the target-
entity and eC denotes the context-entity occurring
in entity eT‚Äôs context.
Let ¬£ be the set of entities. For each entity e E
¬£, the model learns both a ‚Äútarget vector‚Äù ve E R&apos;
and ‚Äúcontext vector‚Äù ¬Øve E R&apos;, by maximizing the
training objective
</bodyText>
<equation confidence="0.994591">
_ 1
L ID |ÔøΩ(eT eC)ED log p(eC |eT), (1)
</equation>
<bodyText confidence="0.993457">
where the prediction probability is defined as a
softmax:
</bodyText>
<equation confidence="0.9961815">
p {
p(eC  |eT) = EeEÔøΩexd{ e d,(eT, e )}.(2)
</equation>
<bodyText confidence="0.999198">
Here d(e, e&apos;) is the distance between the target
vector of e (i.e., ve) and the context vector of e&apos;
(i.e., ¬Øve,). We present the design in the following.
</bodyText>
<subsectionHeader confidence="0.999436">
2.2 Hierarchical Extension
</subsectionHeader>
<bodyText confidence="0.999209458333333">
An entity hierarchy takes entities as leaf nodes and
categories as internal nodes, which provides key
knowledge sources on semantic relatedness that 1)
far-away entities in the hierarchy tend to be se-
mantically distant, and 2) nearby entities tend to
share common semantic features. We aim to en-
code this knowledge in our representations. As K-
B hierarchies are large complex DAG structures,
we develop a highly-efficient scheme to enable
practical training.
Specifically, we associate a separate distance
metric Mh E R&apos;√ó&apos; with each category h in the
hierarchy. A distance metric is a positive semidef-
inite (PSD) matrix. We then measure the distance
between two entities under some aggregated dis-
tance metric as detailed below. The local metrics
thus not only serve to capture the characteristics
of individual categories, but also make it possible
to share the representation across entities through
metric aggregation of relevant categories.
Metric aggregation Given two entities e and e&apos;,
let Pe,e, be the path between them. One obvious
way to define the aggregated metric Me,e, E R&apos;√ó&apos;
is through a combination of the metrics on the
</bodyText>
<page confidence="0.989805">
1293
</page>
<figureCaption confidence="0.816144666666667">
Figure 2: Paths in a DAG-structured hierarchy. A
path P is defined as a sequence of non-duplicated
nodes with the property that there exists a turning
</figureCaption>
<bodyText confidence="0.969508666666667">
node t ‚àà P such that any two consecutive nodes
before t are (child, parent) pairs, while consecu-
tive nodes after t are (parent, child) pairs. Thus a
turning node is necessarily a common ancestor.
path: F-hEPe,e&apos; Mh, leading to a nice property that
the more nodes a path has, the more distant the en-
tities tend to be (as Mh is PSD). This simple strat-
egy, however, can be problematic when the hier-
archy has a complex DAG structure, in that there
can be multiple paths between two entities (Fig-
ure 2). Though the shortest path can be selected, it
ignores other related category nodes and loses rich
information. In contrast, an ideal scheme should
not only mirror the distance in the hierarchy, but
also take into account all possible paths in order to
capture the full aspects of relatedness.
However, hierarchies in large KBs can be com-
plex and contains combinationally many paths be-
tween any two entities. We propose an efficien-
t approach that avoids enumerating paths and in-
stead models the underlying nodes directly. In par-
ticular, we extend Pe,e&apos; as the set of all category
nodes included in any of the e ‚Üí e&apos; paths, and
define the aggregate metric as
</bodyText>
<equation confidence="0.946365">
ÔøΩMe,e&apos; = Œ≥e,e&apos; hEPe,e&apos; œÄee&apos;,hMh, (3)
</equation>
<bodyText confidence="0.988402657142857">
being the average #steps going down from node
h to node e in the hierarchy (infinite if h is not
an ancestor of e). This implements the intuition
that an entity (e.g., ‚ÄúIphone‚Äù) is more relevant to
its immediate categories (e.g., ‚ÄúMobile phones‚Äù)
than to farther and more generic ancestors (e.g.,
‚ÄúTechnology‚Äù). The scaling factor Œ≥e,e&apos; encodes
the distance of the entities in the hierarchy and can
be of various choices. We set Œ≥e,e&apos; = minh{shie+
shie&apos;} to mirror the least common ancestor.
In Figure 2, Pe,e&apos; = {h2, h3, h4}, and the rel-
ative weights of the categories are œÄee&apos;,h2 ‚àù 3/2
and œÄee&apos;,h3 = œÄee&apos;,h4 ‚àù 1. Category h2 is the least
common ancestor and Œ≥e,e&apos; = 3.
Based on the aggregated metric, the distance be-
tween a target entity eT and a context entity eC can
then be measured as
d (eT, eC) = (veT ‚àí ¬ØveC )TMeT ,eC (veT ‚àí ¬ØveC). (4)
Note that nearby entities in the hierarchy tend to
share a large proportion of local metrics in Eq 3,
and hence can exhibit common semantic features
when measuring distance with others.
Complexity of aggregation As computing dis-
tance is a frequent operation in both training and
application stages, a highly efficient aggregation
algorithm is necessary in order to handle complex
large entity hierarchies (with millions of nodes).
Our formulation (Eq 3) avoids exhaustive enumer-
ation over all paths by modeling the relevant nodes
directly. We show that this allows linear complex-
ity in the number of children of two entities‚Äô com-
mon ancestors, which is efficient in practice.
The most costly operation is to find Pe,e&apos;, i.e.,
the set of all category nodes that can occur in any
of e ‚Üí e&apos; paths. We use a two-step procedure that
</bodyText>
<listItem confidence="0.977649">
(1) finds all common ancestors of entity e and e&apos;
that are turning nodes of any e ‚Üí e&apos; paths (e.g.,
h2 in Figure 2), denoted as Qe,e&apos;; (2) expands from
Qe,e&apos; to construct the full Pe,e&apos;. For the first step,
the following theorem shows each common ances-
tor can be efficiently assessed by testing only its
children nodes. For the second step, it is straight-
forward to see that Pe,e&apos; can be constructed by ex-
panding Qe,e&apos; with its descendants that are ances-
tors of either e or e&apos;. Other parameters (œÄee&apos; and
Œ≥e,e&apos;) of aggregation can be computed during the
above process.
</listItem>
<bodyText confidence="0.9925355">
We next provide the theorem for the first step.
Let Ae be the ancestor nodes of entity e (including
e itself). For a node h ‚àà Ae ‚à™ Ae&apos;, we define
its critical node th as the nearest (w.r.t the length
of the shortest path) descendant of h (including h
itself) that is in Qe,e&apos; ‚à™ {e, e&apos;}. E.g., in Figure 2,
th1 = h2; th2 = h2; th3 = e. Let Ch be the set of
immediate child nodes of h.
Theorem 1. ‚àÄh ‚àà Ae ‚à© Ae&apos;, h ‚àà Qe,e&apos; iff it satis-
fies the two conditions: (1) |Ch‚à©(Ae ‚à™ Ae&apos;)  |‚â• 2;
</bodyText>
<listItem confidence="0.680675">
(2) ‚àÉa, b ‚àà Ch s.t. to =6 tb.
</listItem>
<figure confidence="0.732650944444444">
turning node
h3
e
h2
h4
h1
e-) e‚Ä≤path 2
e -) e‚Ä≤path 1
e&apos;
where {œÄee&apos;,h} are the relative weights of the cat-
egories such that F-hEP &apos; œÄee&apos;,h = 1. This serves
e,e
to balance the size of P across different entity
pairs. We set œÄee&apos;,h ‚àù ( 1
shÔøΩe + shie
)
&apos;
with shie
</figure>
<page confidence="0.982094">
1294
</page>
<bodyText confidence="0.998930791666667">
Proof. We outline the proof here, and provide the
details in the appendix.
Sufficiency: Note that e, e&apos; ‚àà/ Qe,e0. We prove
the sufficiency by enumerating possible situation-
s: (i) ta = e, tb = e&apos;; (ii) ta = e, tb ‚àà Qe,e0;
(iii) ta, tb ‚àà Qe,e0. For (i): as ta = e, there exists
a path e ‚Üí ¬∑¬∑ ¬∑ ‚Üí a ‚Üí h where any two con-
secutive nodes is a (child, parent) pair. Similarly,
there is a path h ‚Üí b ‚Üí ¬∑ ¬∑ ¬∑ ‚Üí e&apos; where any t-
wo consecutive nodes is a (parent, child) pair. It
is provable that the two paths intersect only at h,
and thus can be combined to form an e ‚Üí e&apos; path:
e ‚Üí ¬∑¬∑¬∑ ‚Üí a ‚Üí h ‚Üí b ‚Üí ¬∑¬∑¬∑ ‚Üí e&apos;, yielding h
as a turning node. The cases (ii) and (iii) can be
proved similarly.
Necessity: We prove by contradiction. Suppose
that ‚àÄa, b ‚àà Ch ‚à© (Ae ‚à™ Ae0) we have ta = tb.
W.l.o.g. we consider two cases: (i) ta = tb = e,
and (ii) ta = tb ‚àà Qe,e0. It is provable that both
cases will lead to contradiction.
Therefore, by checking common ancestors from
the bottom up, we can construct Qe,e0 with time
complexity linear to the number of all ancestors‚Äô
children.
</bodyText>
<subsectionHeader confidence="0.998961">
2.3 Learning
</subsectionHeader>
<bodyText confidence="0.996761333333333">
For efficiency, we use negative sampling to refor-
mulate the training objective, which is then opti-
mized through coordinate gradient ascent.
Specifically, given the training data {(eT, eC)}
extracted from KB corpora, the representation
learning is formulated as maximizing the objec-
tive in Eq 1, subject to PSD constraints on distance
metrics Mh ÔøΩ: 0, and kvek2 = k¬Øvek2 = 1 to avoid
scale ambiguity.
The likelihood of each data sample is defined
as a softmax in Eq 2, which iterates over all en-
tities in the denominator and is thus computation-
ally prohibitive. We apply the negative sampling
technique as in conventional skip-gram model, by
replacing each log probability log p(eC|eT) with
</bodyText>
<equation confidence="0.769155">
ÔøΩk
log œÉ(‚àíd (eT, eC)) + i=1 Eei‚àºP(e) [log œÉ(‚àíd (eT, ei))] ,
</equation>
<bodyText confidence="0.998519083333333">
where œÉ(x) = 1/(1 + exp(‚àíx)) is the sigmoid
function; and for each data sample we draw k neg-
ative samples from the noise distribution P(e) ‚àù
U(e)3/4 with U(e) being the unigram distribution
(Mikolov et al., 2013b).
The negative sampling objective is optimized
using coordinate gradient ascent, as shown in Al-
gorithm 1. To avoid overfitting and improve effi-
ciency, in practice we restrict the distance metrics
Mh to be diagonal (Xing et al., 2002). Thus the
PSD project of Mh (line 17) is simply taking the
positive part for each diagonal elements.
</bodyText>
<figure confidence="0.670002142857143">
Algorithm 1 Entity Hierarchy Embedding
Input: The training data D = {(eT, eC)},
Entity hierarchy,
Parameters: n ‚Äì dimension of the embedding
k ‚Äì number of negative samples
77 ‚Äì gradient learning rate
B ‚Äì minibatch size
</figure>
<listItem confidence="0.721451444444445">
1: Initialize v, ¬Øv, M randomly such that v112 = ÔøΩ¬ØvÔøΩ2 = 1
andM&gt;- 0.
2: repeat
3: Sample a batch B = {(eT, eC)i}Bi=1 from D
4: for all (eT, eC) ‚àà B do
5: Compute {P, œÄ, 7}eT ,eC for metric aggregation
6: Sample negative pairs {(eT, ei)}ki=1
7: Compute {{P, œÄ, 7}eT ,ei}ki=1 for metric aggrega-
tion
8: end for
9: repeat
10: for all e ‚àà E included in B do
11: ve = ve + 77 ‚àÇL
‚àÇve
12: ¬Øve = ¬Øve + 77 ‚àÇL
‚àÇ¬Øve
13: ve, ¬Øve = Project to unit sphere(ve, ¬Øve)
14: end for
15: until convergence
16: repeat
17: for all h included in B do
18: Mh = Mh + 77 ‚àÇL
‚àÇMh
19: Mh = Project to PSD(Mh)
20: end for
21: until convergence
22: until convergence
</listItem>
<bodyText confidence="0.345355">
Output: Entity vectors v, ¬Øv, and category dist. metrics M
</bodyText>
<sectionHeader confidence="0.989499" genericHeader="method">
3 Applications
</sectionHeader>
<bodyText confidence="0.999953578947368">
One primary goal of learning semantic embedding
is to improve NLP tasks. The compact represen-
tations are easy to work with because they en-
able efficient computation of semantic relatedness.
Compared to word embedding, entity embedding
is particularly suitable for various language under-
standing applications that extract underlying se-
mantics of surface text. Incorporating entity hier-
archies further enriches the embedding with struc-
tured knowledge.
In this section, we demonstrate how the learned
entity hierarchy embedding can be utilized in t-
wo important tasks, i.e., entity linking and enti-
ty search. In both tasks, we measure the seman-
tic relatedness between entities as the reciprocal
distance defined in Eq 4. This greatly simpli-
fies previous methods which have used various
hand-crafted features, and leads to improved per-
formance as shown in our experiments.
</bodyText>
<page confidence="0.946341">
1295
</page>
<subsectionHeader confidence="0.991151">
3.1 Entity Linking
</subsectionHeader>
<bodyText confidence="0.988500607142857">
The entity linking task is to link surface form-
s (mentions) of entities in a document to entities
in a reference KB. It is an essential first step for
downstream tasks such as semantic search and K-
B construction. The quality of entity relatedness
measure is critical for entity linking performance,
because of the key observation that entities in a
document tend to be semantically coherent. For
example, in sentence ‚ÄúApple released an operating
system Lion‚Äù, The mentions ‚ÄúApple‚Äù and ‚ÄúLion‚Äù
refer to Apple Inc. and Mac OS X Lion, respec-
tively, as is more coherent than other configura-
tions like (fruit apple, animal lion).
Our algorithm finds the optimal configuration
for the mentions of a document by maximizing
the overall relatedness among assigned entities, to-
gether with the local mention-to-entity compatibil-
ity. Specifically, we first construct a mention-to-
entity dictionary based on Wikipedia annotations.
For each mention m, the dictionary contains a set
of candidate entities and for each candidate entity
e a compatibility score P(e|m) which is propor-
tional to the frequency that m refers to e. For effi-
ciency we only consider the top-5 candidate enti-
ties according to P(e|m). Given a set of mentions
M = {mi}Mi=1 in a document, let A = {emi}Mi=1
be a configuration of its entity assignments. The
score of A is formulated as probability
</bodyText>
<equation confidence="0.9967465">
P(A|M) a rMP(emi|mi) EM=
i=1 #z
</equation>
<bodyText confidence="0.999944428571428">
where for each entity assignment we define its
global relatedness to other entity assignments as
the sum of the reciprocal distances (E = 0.01 is
a constant used to avoid divide-by-zero). Direct
enumeration of all potential configurations is com-
putationally prohibitive, we therefore use simulat-
ed annealing to search for an optimal solution.
</bodyText>
<subsectionHeader confidence="0.991294">
3.2 Entity Search
</subsectionHeader>
<bodyText confidence="0.99997734375">
Entity search has attracted a growing interest
(Chen et al., 2014b; Balog et al., 2011). Unlike
conventional web search that finds unorganized
web pages, entity search retrieves knowledge di-
rectly by generating a list of relevant entities in re-
sponse to a search request. The input of the entity
search task is a natural language question Q along
with one or more desired entity categories C. For
example, a query can be Q =‚Äúfilms directed by
Akira Kurosawa‚Äù and C ={Japanese films}.
Previous methods typically score candidate en-
tities by measuring both the similarity between en-
tity content and the query question Q (text match-
ing), and the similarity between categories of en-
tities and the query categories C (category match-
ing).
We apply a similar category matching strate-
gy as in previous work (Chen et al., 2014b) that
assesses lexical (e.g., head words) similarity be-
tween category names, while replacing the text
matching with entity relatedness measure. Specif-
ically, we first extract the underlying entities men-
tioned in Q through entity linking, then score each
candidate entity by its average relatedness to the
entities in Q. For instance, the entity Rashomon
will obtain a high score in the above example as
it is highly related with the entity Akira Kurosawa
in the query. This scheme not only avoids complex
document processing (e.g., topic modeling) in tex-
t matching, but also implicitly augments the short
query text with background knowledge, and thus
improves the accuracy and robustness.
</bodyText>
<sectionHeader confidence="0.999586" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999989148148148">
We validate the quality of our entity representa-
tion by evaluating its applications of entity linking
and entity search on public benchmarks. In the
entity linking task, our approach improves the F1
score by 10% over state-of-the-art results. We al-
so validate the advantage of incorporating hierar-
chical structure. In the entity search task, our sim-
ple algorithm shows competitive performance. We
further qualitatively analyze the entity vectors and
category metrics, both of which capture meaning-
ful semantics, and can potentially open up a wide
rage of other applications.
Knowledge base We use the Wikipedia snap-
shot from Jan 12nd, 2015 as our training data and
KB. After pruning administrative information we
obtain an entity hierarchy including about 4.1M
entities and 0.8M categories organized into 12
layers. Loops in the original hierarchy are re-
moved by deleting bottom-up edges, yielding a
DAG structure. We extract a set of 87.6M entity
pairs from the wiki links on Wikipedia articles.
We train 100-dimensional vector represen-
tations for the entities and distance metrics
(100√ó100 diagonal matrixes) for the categories
(we would study the impact of dimensionality in
the future). We set the batch size B = 500, the
initial learning rate q = 0.1 and decrease it by a
</bodyText>
<equation confidence="0.6451985">
1
d emj + ÔøΩ,
</equation>
<page confidence="0.956513">
1296
</page>
<bodyText confidence="0.999878">
factor of 5 whenever the objective value does not
increase, and the negative sample size k = 5. The
model is trained on a Linux machine with 128G
RAM and 16 cores. It takes 5 days to converge.
</bodyText>
<subsectionHeader confidence="0.9870055">
4.1 Entity Linking
4.1.1 Setup
</subsectionHeader>
<bodyText confidence="0.999977714285714">
Dataset As our entities based on English
Wikipedia include not only named entities (e.g.,
persons, organizations) but also general concepts
(e.g., ‚Äúcomputer‚Äù and ‚Äúhuman‚Äù), we use a stan-
dard entity linking dataset IITB1 where mentions
of Wikipedia entities are manually annotated ex-
haustively. The dataset contains about 100 docu-
ments and 17K mentions in total. As in the base-
line work, we use only the mentions whose refer-
ent entities are contained in Wikipedia.
Criteria We adopt the common criteria, preci-
sion, recall, and F1. Let A‚àó be the golden stan-
dard entity annotations, and A be the annotations
by entity linking model, then
</bodyText>
<equation confidence="0.8623435">
precision = |A‚àó ‚à© A |recall = A Al
JA |IS I
</equation>
<bodyText confidence="0.99959245">
The F1 score is then computed based on the aver-
age precision and recall across all documents.
Baselines We compare our algorithm with the
following approaches. All the competitors are de-
signed to be able to link general concept mentions
to Wikipedia.
CSAW (Kulkarni et al., 2009) has a similar
framework as our algorithm. It measures entity
relatedness using a variation of Jaccard similarity
on Wikipedia page incoming links.
Entity-TM (Han and Sun, 2012) models an en-
tity as a distribution over mentions and words, and
sets up a probabilistic generative process for the
observed text.
Ours-NoH. To validate the advantage of incor-
porating hierarchical structure, we design a base-
line that relies on entity embedding without enti-
ty hierarchy. That is, we obtain entity vectors by
fixing the distance metric in Eq 4 as an identity
matrix.
</bodyText>
<sectionHeader confidence="0.924492" genericHeader="method">
4.1.2 Results
</sectionHeader>
<bodyText confidence="0.999019666666667">
Table 1 shows the performance of the competitors.
Our algorithm using the entity hierarchy embed-
ding gets 21% to 10% improvement in F1, and
</bodyText>
<footnote confidence="0.971269">
1http://www.cse.iitb.ac.in/soumen/doc/CSAW/Annot
</footnote>
<table confidence="0.9989614">
Methods Precision Recall F1
CSAW 0.65 0.74 0.69
Entity-TM 0.81 0.80 0.80
Ours-NoH 0.78 0.85 0.81
Ours 0.87 0.94 0.90
</table>
<tableCaption confidence="0.999735">
Table 1: Entity linking performance
</tableCaption>
<bodyText confidence="0.995804730769231">
over 6% and 14% improvements in Precision and
Recall, respectively. The CSAW model devises
a set of entity features based on text content and
link structures of Wikipedia pages, and combines
them to measure relatedness. Compared to these
hand-crafted features which are essentially heuris-
tic and hard to verify, our embedding model in-
duces semantic representations by optimizing a s-
ingle well-defined objective. Note that the em-
bedding actually also encodes the Wikipedia inter-
page network, as we train on the entity-context
pairs which are extracted from wiki links.
The Entity-TM model learns a representation
for each entity as a word distribution. However, as
noted in (Baroni et al., 2014), the counting-based
distributional model usually shows inferior perfor-
mance than context-predicting methods as ours.
Moreover, in addition to the text context, our mod-
el integrates the entity hierarchical structure which
provides rich knowledge of semantic relatedness.
The comparison between Ours and Ours-NoH fur-
ther reveals the effect of integrating the hierarchy
in learning entity vectors. With entity hierarchy,
we obtain more semantically meaningful represen-
tations that achieve 9% F1 improvement over en-
tity vectors without hierarchical knowledge.
</bodyText>
<subsectionHeader confidence="0.9905295">
4.2 Entity Search
4.2.1 Setup
</subsectionHeader>
<bodyText confidence="0.9999809">
Dataset We use the dataset from INEX 2009 en-
tity ranking track2, which contains 55 queries. The
golden standard results of each query contains a
set of relevant entities each of which corresponds
to a Wikipedia page.
Criteria We use the common criteria of
precision@k, i.e., the percentage of relevant
entities in the top-k results (we set k = 10), as
well as precision@R where R is the number of
golden standard entities for a query.
</bodyText>
<footnote confidence="0.926684">
2http://www.inex.otago.ac.nz/tracks/entity-
ranking/entity-ranking.asp
</footnote>
<page confidence="0.993168">
1297
</page>
<bodyText confidence="0.999721045454546">
Baselines We compare our algorithm with the
following recent competitors.
Balog (Balog et al., 2011) develops a proba-
bilistic generative model which represents entities,
as well as the query, as distributions over both
words and categories. Entities are then ranked
based on the KL-divergence between the distribu-
tions.
K&amp;K (Kaptein and Kamps, 2013) exploits
Wikipedia entity hierarchy to derive the content of
each category, which is in turn used to measure
relatedness with the query categories. It further
incorporates inter-entity links for relevance propa-
gation.
Chen (Chen et al., 2014b) creates for each en-
tity a context profile leveraging both the whole
document (long-range) and sentences around en-
tity (short-range) context, and models query text
by a generative model. Categories are weighted
based on the head words and other features. Our
algorithm exploits a similar method for category
matching.
</bodyText>
<table confidence="0.997218">
Methods Precision@10 Precision@R
Balog 0.18 0.16
K&amp;K 0.31 0.28
Chen 0.55 0.42
Ours 0.57 0.46
</table>
<tableCaption confidence="0.998727">
Table 2: Entity search performance.
</tableCaption>
<sectionHeader confidence="0.591747" genericHeader="evaluation">
4.2.2 Results
</sectionHeader>
<bodyText confidence="0.9993469">
Table 2 lists the entity search results of the com-
petitors. Our algorithm shows superiority over the
previous best performing methods. Balog con-
structs representations for each entity merely by
counting (and smoothing) its co-occurrence be-
tween words and categories, which is inadequate
to capture relatedness accurately. K&amp;K leverages
the rich resources in Wikipedia such as text, hi-
erarchy, and link structures. However, the hand-
crafted features are still suboptimal compared with
our learned representations.
Chen performs well by combining both long-
and short-context of entities, as well as catego-
ry lexical similarity. Our algorithm replaces it-
s text matching component with a semantic en-
richment step, i.e., grounding entity mentions in
the query text onto KB entities. This augments
the short query with rich background knowledge,
facilitating accurate relatedness measure based on
our high-quality entity embedding.
</bodyText>
<subsectionHeader confidence="0.999808">
4.3 Qualitative Analysis
</subsectionHeader>
<bodyText confidence="0.999943852941176">
We qualitatively inspect the learned representa-
tions of the entity hierarchy. The results show that
both the entity vectors and the category distance
metrics capture meaningful semantics, and can po-
tentially boost a wide range of applications such as
recommendation and knowledge base completion.
Entity vectors Table 3 shows a list of target en-
tities, and their top-4 nearest entities in the whole
entity set or subsets belonging to given categories.
Measuring under the whole set (column 2) results
in nearest neighbors that are strongly related with
the target entity. For instance, the nearest enti-
ties for ‚Äúblack hole‚Äù are ‚Äúfaster-than-light‚Äù, ‚Äúevent
horizon‚Äù, ‚Äúwhite hole‚Äù, and ‚Äútime dilation‚Äù, all of
which are concepts from physical cosmology and
the theory of relativity. Similar results can be ob-
served from other 3 examples.
Even more interesting is to specify a category
and search for the most related entities under the
category. The third column of Table 3 shows sev-
eral examples. E.g., our model found that the most
related Chinese websites to Youtube are ‚ÄúTudou‚Äù,
‚Äú56.com‚Äù, ‚ÄúYouku‚Äù (three top video hosting ser-
vices in China), and ‚ÄúYinYueTai‚Äù (a major MV
sharing site in China). The high-quality results
show that our embedding model is able to discov-
er meaningful relationships between entities from
the complex entity hierarchy and plain text. This
can be a useful feature in a wide range of appli-
cations such as semantic search (e.g., looking for
movies about black hole), recommendation (e.g.,
suggesting TV series of specific genre for kids),
and knowledge base completion (e.g., extracting
relations between persons), to name a few.
</bodyText>
<table confidence="0.991178285714286">
Target entity Most related entities
black hole overall: American films:
faster-than-light Hidden Universe 3D
eventhorizon Hubble (film)
white hole Quantum Quest
time dilation Particle Fever
Youtube overall: Chinese websites:
Instagram Tudou
Twitter 56.com
Facebook Youku
Dipdive YinYueTai
Harvard overall: businesspeople in software:
University Yale University Jack Dangermond
University of Pennsylvania Bill Gates
Princeton University Scott McNealy
Swarthmore College Marc Chardon
X-Men: Days of overall: children‚Äôs television series:
Future Past (film) Marvel Studios Ben 10: Race Against Time
X-Men: The Last Stand Kim Possible: A Sitch in Time
X2 (film) Ben 10: Alien Force
Man of Steel (film) Star Wars: The Clone Wars
</table>
<tableCaption confidence="0.824785666666667">
Table 3: Most related entities under specific cate-
gories. ‚ÄúOverall‚Äù represents the most general cat-
egory that includes all the entities.
</tableCaption>
<page confidence="0.986317">
1298
</page>
<figureCaption confidence="0.967192">
Figure 3: Distance metric visualization for the
</figureCaption>
<bodyText confidence="0.978409576923077">
subcategories of the category ‚ÄúMicrosoft‚Äù. The t-
SNE (Van der Maaten and Hinton, 2008) algorith-
m is used to map the high-dimensional (diagonal)
matrixes into the 2D space.
Category distance metrics In addition to learn-
ing vector representations of entities, we also as-
sociate with each category a local distance metric
to capture the features of individual category. As
we restrict the distance metrics to be diagonal ma-
trixes, the magnitude of each diagonal value can
be viewed as how much a category is character-
ized by the corresponding dimension. Categories
with close semantic meanings are expected to have
similar metrics.
Figure 3 visualizes the metrics of all subcate-
gories under the category ‚ÄúMicrosoft‚Äù, where we
amplify some parts of the figure to showcase the
clustering of semantically relevant categories. For
instance, the categories of Microsoft Windows op-
erating systems, and those of the Xbox games,
are embedded close to each other, respectively.
The results validate that our hierarchy embedding
model can not only encode relatedness between
leaf entities, but also capture semantic similarity
of the internal categories. This can be helpful in
taxonomy refinement and relation discovery.
</bodyText>
<sectionHeader confidence="0.999971" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999980690476191">
Distributed representation There has been a
growing interest in distributed representation of
words. Skip-gram model (Mikolov et al., 2013a)
is one of the most popular methods to learn word
representations. The model aims to find a rep-
resentation for each word that is useful for pre-
dicting its context words. Word-context simi-
larity is measured by simple inner product. A
set of recent works generalizing the basic skip-
gram to incorporate dependency context (Levy
and Goldberg, 2014), word senses (Chen et al.,
2014a), and multi-modal data (Hill and Korho-
nen, 2014). However, these work leverages lim-
ited structured knowledge. Our proposed method
goes beyond skip-gram significantly such that we
measures entity-context similarity under aggregat-
ed distance metrics of hierarchical category n-
odes. This effectively captures the structured
knowledge. Another research line learn knowl-
edge graph embedding (Lin et al., 2015; Wang et
al., 2014; Bordes et al., 2013), which models enti-
ties as vectors and relations as some operations on
the vector space (e.g., translation). These work-
s aim at relation prediction for knowledge graph
completion, and can be viewed as a supplement to
the above that extracts semantics from plain text.
Utilizing hierarchical knowledge Semantic hi-
erarchies are key sources of knowledge. Previ-
ous works (Ponzetto and Strube, 2007; Leacock
and Chodorow, 1998) use KB hierarchies to de-
fine relatedness between concepts, typically based
on path-length measure. Recent works (Yogatama
et al., 2015; Zhao et al., 2011) learn representa-
tions through hierarchical sparse coding that en-
forces similar sparse patterns between nearby n-
odes. Category hierarchies have also been wide-
ly used in classification (Xiao et al., 2011; Wein-
berger and Chapelle, 2009). E.g., in (Verma et al.,
2012) category nodes are endowed with discrim-
inative power by learning distance metrics. Our
approach differs in terms of entity vector learning
and metric aggregation on DAG hierarchy.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999983705882353">
In this paper, we proposed to learn entity hierarchy
embedding to boost semantic NLP tasks. A princi-
pled framework was developed to incorporate both
text context and entity hierarchical structure from
large-scale knowledge bases. We learn a distance
metric for each category node, and measure enti-
ty vector similarity under aggregated metrics. A
flexible and efficient metric aggregation scheme
was also developed to model large-scale hierar-
chies. Experiments in both entity linking and enti-
ty search tasks show superiority of our approach.
The qualitative analysis indicates that our model
can be potentially useful in a wide range of other
applications such as knowledge base completion
and ontology refinement. Another interesting as-
pect of future work is to incorporate other sources
of knowledge to further enrich the semantics.
</bodyText>
<sectionHeader confidence="0.998379" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.792429">
This research is supported by NSF IIS-1218282,
IIS-12511827, and IIS-1450545.
</bodyText>
<figure confidence="0.995443214285714">
Xbox
Xbox 360 games
Xbox live arcade games
Xbox games
Windows me
Windows server 2008 r2
Windows 98
snapshots of Windows 2000
snapshots of Microsoft Windows
snapshots of Windows softwares
snapshots of Windows Vista
snapshots of Microsoft Office
Windows 2000
Windows 7
</figure>
<page confidence="0.991982">
1299
</page>
<sectionHeader confidence="0.995218" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999952446601941">
Krisztian Balog, Marc Bron, and Maarten De Rijke.
2011. Query modeling for entity search based on
terms, categories, and examples. TOIS, 29(4):22.
Marco Baroni, Georgiana Dinu, and Germ¬¥an
Kruszewski. 2014. Dont count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proc. of
ACL, volume 1, pages 238‚Äì247.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Proc. of NIPS, pages 2787‚Äì2795.
Xinxiong Chen, Zhiyuan Liu, and Maosong Sun.
2014a. A unified model for word sense representa-
tion and disambiguation. In Proc. of EMNLP, pages
1025‚Äì1035.
Yueguo Chen, Lexi Gao, Shuming Shi, Xiaoyong Du,
and Ji-Rong Wen. 2014b. Improving context and
category matching for entity search. In Proc. of
AAAI.
St¬¥ephane Clinchant and Florent Perronnin. 2013. Ag-
gregating continuous word embeddings for informa-
tion retrieval. page 100.
Gianluca Demartini, Tereza Iofciu, and Arjen P
De Vries. 2010. Overview of the inex 2009 entity
ranking track. In Focused Retrieval and Evaluation,
pages 254‚Äì264. Springer.
Xianpei Han and Le Sun. 2012. An entity-topic model
for entity linking. In Proc. of EMNLP, pages 105‚Äì
115. Association for Computational Linguistics.
Felix Hill and Anna Korhonen. 2014. Learning ab-
stract concept embeddings from multi-modal data:
Since you probably can‚Äôt see what i mean. In Proc.
of EMNLP, pages 255‚Äì265.
Rianne Kaptein and Jaap Kamps. 2013. Exploiting the
category structure of wikipedia for entity ranking.
Artificial Intelligence, 194:111‚Äì129.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,
and Soumen Chakrabarti. 2009. Collective anno-
tation of wikipedia entities in web text. In Proc. of
KDD, pages 457‚Äì466. ACM.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and wordnet similarity for word
sense identification. WordNet: An electronic lexical
database, 49(2):265‚Äì283.
Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proc. ofACL, volume 2,
pages 302‚Äì308.
Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and
Xuan Zhu. 2015. Learning entity and relation em-
beddings for knowledge graph completion. In Proc.
of AAAI.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proc. of NIPS, pages 3111‚Äì3119.
Alexandre Passos, Vineet Kumar, and Andrew McCal-
lum, 2014. Lexicon Infused Phrase Embeddings for
Named Entity Resolution, pages 78‚Äì86. Association
for Computational Linguistics.
Simone Paolo Ponzetto and Michael Strube. 2007.
Knowledge derived from wikipedia for computing
semantic relatedness. JAIR, 30(1):181‚Äì212.
Philip Resnik. 1995. Using information content to e-
valuate semantic similarity in a taxonomy. In Proc.
of IJCAI, pages 448‚Äì453.
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Li-
u, and Bing Qin. 2014. Learning sentiment-specific
word embedding for twitter sentiment classification.
In Proc. of ACL, pages 1555‚Äì1565.
Laurens Van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. JMLR, 9(2579-
2605):85.
Nakul Verma, Dhruv Mahajan, Sundararajan Sella-
manickam, and Vinod Nair. 2012. Learning hier-
archical similarity metrics. In Proc. of CVPR, pages
2280‚Äì2287. IEEE.
Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014. Knowledge graph and text jointly em-
bedding. In Proc. of EMNLP.
Kilian Q Weinberger and Olivier Chapelle. 2009.
Large margin taxonomy embedding for document
categorization. In Proc. of NIPS, pages 1737‚Äì1744.
Lin Xiao, Dengyong Zhou, and Mingrui Wu. 2011.
Hierarchical classification via orthogonal transfer.
In Proc. of ICML, pages 801‚Äì808.
Eric P Xing, Michael I Jordan, Stuart Russell, and An-
drew Y Ng. 2002. Distance metric learning with
application to clustering with side-information. In
Proc. of NIPS, pages 505‚Äì512.
Dani Yogatama, Manaal Faruqui, Chris Dyer, and Noah
Smith. 2015. Learning word representations with
hierarchical sparse coding. Proc. of ICML.
Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, and
Chengqing Zong. 2014. Bilingually-constrained
phrase embeddings for machine translation. In Proc.
ofACL.
Bin Zhao, Fei Li, and Eric P Xing. 2011. Large-scale
category structure aware image categorization. In
Proc. of NIPS, pages 1251‚Äì1259.
</reference>
<page confidence="0.987866">
1300
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.966801">
<title confidence="0.998213">Entity Hierarchy Embedding</title>
<author confidence="0.999753">Zhiting Hu</author>
<author confidence="0.999753">Poyao Huang</author>
<author confidence="0.999753">Yuntian Deng</author>
<author confidence="0.999753">Yingkai Gao</author>
<author confidence="0.999753">Eric P Xing</author>
<affiliation confidence="0.99995">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.999497">Pittsburgh, PA 15213, USA</address>
<abstract confidence="0.998366526315789">Existing distributed representations are limited in utilizing structured knowledge to improve semantic relatedness modeling. We propose a principled framework of embedding entities that integrates hierarchical information from large-scale knowledge bases. The novel embedding model associates each category node of the hierarchy with a distance metric. To capture structured semantics, the entity similarity of context prediction are measured under the aggregated metrics of relevant categories along all inter-entity paths. We show that both the entity vectors and category distance metrics encode meaningful semantics. Experiments in entity linking and entity search show superiority of the proposed method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Krisztian Balog</author>
<author>Marc Bron</author>
<author>Maarten De Rijke</author>
</authors>
<title>Query modeling for entity search based on terms, categories, and examples.</title>
<date>2011</date>
<journal>TOIS,</journal>
<volume>29</volume>
<issue>4</issue>
<marker>Balog, Bron, De Rijke, 2011</marker>
<rawString>Krisztian Balog, Marc Bron, and Maarten De Rijke. 2011. Query modeling for entity search based on terms, categories, and examples. TOIS, 29(4):22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ¬¥an Kruszewski</author>
</authors>
<title>Dont count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In Proc. of ACL,</booktitle>
<volume>1</volume>
<pages>238--247</pages>
<contexts>
<context position="23614" citStr="Baroni et al., 2014" startWordPosition="3972" endWordPosition="3975">AW model devises a set of entity features based on text content and link structures of Wikipedia pages, and combines them to measure relatedness. Compared to these hand-crafted features which are essentially heuristic and hard to verify, our embedding model induces semantic representations by optimizing a single well-defined objective. Note that the embedding actually also encodes the Wikipedia interpage network, as we train on the entity-context pairs which are extracted from wiki links. The Entity-TM model learns a representation for each entity as a word distribution. However, as noted in (Baroni et al., 2014), the counting-based distributional model usually shows inferior performance than context-predicting methods as ours. Moreover, in addition to the text context, our model integrates the entity hierarchical structure which provides rich knowledge of semantic relatedness. The comparison between Ours and Ours-NoH further reveals the effect of integrating the hierarchy in learning entity vectors. With entity hierarchy, we obtain more semantically meaningful representations that achieve 9% F1 improvement over entity vectors without hierarchical knowledge. 4.2 Entity Search 4.2.1 Setup Dataset We us</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ¬¥an Kruszewski. 2014. Dont count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proc. of ACL, volume 1, pages 238‚Äì247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Nicolas Usunier</author>
<author>Alberto GarciaDuran</author>
<author>Jason Weston</author>
<author>Oksana Yakhnenko</author>
</authors>
<title>Translating embeddings for modeling multirelational data.</title>
<date>2013</date>
<booktitle>In Proc. of NIPS,</booktitle>
<pages>2787--2795</pages>
<contexts>
<context position="31468" citStr="Bordes et al., 2013" startWordPosition="5168" endWordPosition="5171">red by simple inner product. A set of recent works generalizing the basic skipgram to incorporate dependency context (Levy and Goldberg, 2014), word senses (Chen et al., 2014a), and multi-modal data (Hill and Korhonen, 2014). However, these work leverages limited structured knowledge. Our proposed method goes beyond skip-gram significantly such that we measures entity-context similarity under aggregated distance metrics of hierarchical category nodes. This effectively captures the structured knowledge. Another research line learn knowledge graph embedding (Lin et al., 2015; Wang et al., 2014; Bordes et al., 2013), which models entities as vectors and relations as some operations on the vector space (e.g., translation). These works aim at relation prediction for knowledge graph completion, and can be viewed as a supplement to the above that extracts semantics from plain text. Utilizing hierarchical knowledge Semantic hierarchies are key sources of knowledge. Previous works (Ponzetto and Strube, 2007; Leacock and Chodorow, 1998) use KB hierarchies to define relatedness between concepts, typically based on path-length measure. Recent works (Yogatama et al., 2015; Zhao et al., 2011) learn representations </context>
</contexts>
<marker>Bordes, Usunier, GarciaDuran, Weston, Yakhnenko, 2013</marker>
<rawString>Antoine Bordes, Nicolas Usunier, Alberto GarciaDuran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multirelational data. In Proc. of NIPS, pages 2787‚Äì2795.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinxiong Chen</author>
<author>Zhiyuan Liu</author>
<author>Maosong Sun</author>
</authors>
<title>A unified model for word sense representation and disambiguation.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>1025--1035</pages>
<contexts>
<context position="18072" citStr="Chen et al., 2014" startWordPosition="3063" endWordPosition="3066">iven a set of mentions M = {mi}Mi=1 in a document, let A = {emi}Mi=1 be a configuration of its entity assignments. The score of A is formulated as probability P(A|M) a rMP(emi|mi) EM= i=1 #z where for each entity assignment we define its global relatedness to other entity assignments as the sum of the reciprocal distances (E = 0.01 is a constant used to avoid divide-by-zero). Direct enumeration of all potential configurations is computationally prohibitive, we therefore use simulated annealing to search for an optimal solution. 3.2 Entity Search Entity search has attracted a growing interest (Chen et al., 2014b; Balog et al., 2011). Unlike conventional web search that finds unorganized web pages, entity search retrieves knowledge directly by generating a list of relevant entities in response to a search request. The input of the entity search task is a natural language question Q along with one or more desired entity categories C. For example, a query can be Q =‚Äúfilms directed by Akira Kurosawa‚Äù and C ={Japanese films}. Previous methods typically score candidate entities by measuring both the similarity between entity content and the query question Q (text matching), and the similarity between cate</context>
<context position="25301" citStr="Chen et al., 2014" startWordPosition="4223" endWordPosition="4226">g/entity-ranking.asp 1297 Baselines We compare our algorithm with the following recent competitors. Balog (Balog et al., 2011) develops a probabilistic generative model which represents entities, as well as the query, as distributions over both words and categories. Entities are then ranked based on the KL-divergence between the distributions. K&amp;K (Kaptein and Kamps, 2013) exploits Wikipedia entity hierarchy to derive the content of each category, which is in turn used to measure relatedness with the query categories. It further incorporates inter-entity links for relevance propagation. Chen (Chen et al., 2014b) creates for each entity a context profile leveraging both the whole document (long-range) and sentences around entity (short-range) context, and models query text by a generative model. Categories are weighted based on the head words and other features. Our algorithm exploits a similar method for category matching. Methods Precision@10 Precision@R Balog 0.18 0.16 K&amp;K 0.31 0.28 Chen 0.55 0.42 Ours 0.57 0.46 Table 2: Entity search performance. 4.2.2 Results Table 2 lists the entity search results of the competitors. Our algorithm shows superiority over the previous best performing methods. Ba</context>
<context position="31022" citStr="Chen et al., 2014" startWordPosition="5103" endWordPosition="5106">ternal categories. This can be helpful in taxonomy refinement and relation discovery. 5 Related Work Distributed representation There has been a growing interest in distributed representation of words. Skip-gram model (Mikolov et al., 2013a) is one of the most popular methods to learn word representations. The model aims to find a representation for each word that is useful for predicting its context words. Word-context similarity is measured by simple inner product. A set of recent works generalizing the basic skipgram to incorporate dependency context (Levy and Goldberg, 2014), word senses (Chen et al., 2014a), and multi-modal data (Hill and Korhonen, 2014). However, these work leverages limited structured knowledge. Our proposed method goes beyond skip-gram significantly such that we measures entity-context similarity under aggregated distance metrics of hierarchical category nodes. This effectively captures the structured knowledge. Another research line learn knowledge graph embedding (Lin et al., 2015; Wang et al., 2014; Bordes et al., 2013), which models entities as vectors and relations as some operations on the vector space (e.g., translation). These works aim at relation prediction for kn</context>
</contexts>
<marker>Chen, Liu, Sun, 2014</marker>
<rawString>Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014a. A unified model for word sense representation and disambiguation. In Proc. of EMNLP, pages 1025‚Äì1035.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yueguo Chen</author>
<author>Lexi Gao</author>
<author>Shuming Shi</author>
<author>Xiaoyong Du</author>
<author>Ji-Rong Wen</author>
</authors>
<title>Improving context and category matching for entity search.</title>
<date>2014</date>
<booktitle>In Proc. of AAAI.</booktitle>
<contexts>
<context position="18072" citStr="Chen et al., 2014" startWordPosition="3063" endWordPosition="3066">iven a set of mentions M = {mi}Mi=1 in a document, let A = {emi}Mi=1 be a configuration of its entity assignments. The score of A is formulated as probability P(A|M) a rMP(emi|mi) EM= i=1 #z where for each entity assignment we define its global relatedness to other entity assignments as the sum of the reciprocal distances (E = 0.01 is a constant used to avoid divide-by-zero). Direct enumeration of all potential configurations is computationally prohibitive, we therefore use simulated annealing to search for an optimal solution. 3.2 Entity Search Entity search has attracted a growing interest (Chen et al., 2014b; Balog et al., 2011). Unlike conventional web search that finds unorganized web pages, entity search retrieves knowledge directly by generating a list of relevant entities in response to a search request. The input of the entity search task is a natural language question Q along with one or more desired entity categories C. For example, a query can be Q =‚Äúfilms directed by Akira Kurosawa‚Äù and C ={Japanese films}. Previous methods typically score candidate entities by measuring both the similarity between entity content and the query question Q (text matching), and the similarity between cate</context>
<context position="25301" citStr="Chen et al., 2014" startWordPosition="4223" endWordPosition="4226">g/entity-ranking.asp 1297 Baselines We compare our algorithm with the following recent competitors. Balog (Balog et al., 2011) develops a probabilistic generative model which represents entities, as well as the query, as distributions over both words and categories. Entities are then ranked based on the KL-divergence between the distributions. K&amp;K (Kaptein and Kamps, 2013) exploits Wikipedia entity hierarchy to derive the content of each category, which is in turn used to measure relatedness with the query categories. It further incorporates inter-entity links for relevance propagation. Chen (Chen et al., 2014b) creates for each entity a context profile leveraging both the whole document (long-range) and sentences around entity (short-range) context, and models query text by a generative model. Categories are weighted based on the head words and other features. Our algorithm exploits a similar method for category matching. Methods Precision@10 Precision@R Balog 0.18 0.16 K&amp;K 0.31 0.28 Chen 0.55 0.42 Ours 0.57 0.46 Table 2: Entity search performance. 4.2.2 Results Table 2 lists the entity search results of the competitors. Our algorithm shows superiority over the previous best performing methods. Ba</context>
<context position="31022" citStr="Chen et al., 2014" startWordPosition="5103" endWordPosition="5106">ternal categories. This can be helpful in taxonomy refinement and relation discovery. 5 Related Work Distributed representation There has been a growing interest in distributed representation of words. Skip-gram model (Mikolov et al., 2013a) is one of the most popular methods to learn word representations. The model aims to find a representation for each word that is useful for predicting its context words. Word-context similarity is measured by simple inner product. A set of recent works generalizing the basic skipgram to incorporate dependency context (Levy and Goldberg, 2014), word senses (Chen et al., 2014a), and multi-modal data (Hill and Korhonen, 2014). However, these work leverages limited structured knowledge. Our proposed method goes beyond skip-gram significantly such that we measures entity-context similarity under aggregated distance metrics of hierarchical category nodes. This effectively captures the structured knowledge. Another research line learn knowledge graph embedding (Lin et al., 2015; Wang et al., 2014; Bordes et al., 2013), which models entities as vectors and relations as some operations on the vector space (e.g., translation). These works aim at relation prediction for kn</context>
</contexts>
<marker>Chen, Gao, Shi, Du, Wen, 2014</marker>
<rawString>Yueguo Chen, Lexi Gao, Shuming Shi, Xiaoyong Du, and Ji-Rong Wen. 2014b. Improving context and category matching for entity search. In Proc. of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>St¬¥ephane Clinchant</author>
<author>Florent Perronnin</author>
</authors>
<title>Aggregating continuous word embeddings for information retrieval.</title>
<date>2013</date>
<pages>100</pages>
<contexts>
<context position="1433" citStr="Clinchant and Perronnin, 2013" startWordPosition="199" endWordPosition="202">tegory distance metrics encode meaningful semantics. Experiments in entity linking and entity search show superiority of the proposed method. 1 Introduction There has been a growing interest in distributed representation that learns compact vectors (a.k.a embedding) for words (Mikolov et al., 2013a), phrases (Passos et al., 2014), and concepts (Hill and Korhonen, 2014), etc. The induced vectors are expected to capture semantic relatedness of the linguistic items, and are widely used in sentiment analysis (Tang et al., 2014), machine translation (Zhang et al., 2014), and information retrieval (Clinchant and Perronnin, 2013), to name a few. Despite the impressive success, existing work is still limited in utilizing structured knowledge to enhance the representation. For instance, word and phrase embeddings are largely induced from plain text. Though recent knowledge graph embeddings (Lin et al., 2015; Wang et al., 2014) integrate the relational structure among entities, they primarily target at link prediction and lack an explicit relatedness measure. In this paper, we propose to improve the distributed representations of entities by integrating hierarchical information from large-scale knowledge bases (KBs). An </context>
</contexts>
<marker>Clinchant, Perronnin, 2013</marker>
<rawString>St¬¥ephane Clinchant and Florent Perronnin. 2013. Aggregating continuous word embeddings for information retrieval. page 100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gianluca Demartini</author>
<author>Tereza Iofciu</author>
<author>Arjen P De Vries</author>
</authors>
<title>Overview of the inex 2009 entity ranking track.</title>
<date>2010</date>
<booktitle>In Focused Retrieval and Evaluation,</booktitle>
<pages>254--264</pages>
<publisher>Springer.</publisher>
<marker>Demartini, Iofciu, De Vries, 2010</marker>
<rawString>Gianluca Demartini, Tereza Iofciu, and Arjen P De Vries. 2010. Overview of the inex 2009 entity ranking track. In Focused Retrieval and Evaluation, pages 254‚Äì264. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianpei Han</author>
<author>Le Sun</author>
</authors>
<title>An entity-topic model for entity linking.</title>
<date>2012</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>105--115</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Han, Le Sun, 2012</marker>
<rawString>Xianpei Han and Le Sun. 2012. An entity-topic model for entity linking. In Proc. of EMNLP, pages 105‚Äì 115. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>Anna Korhonen</author>
</authors>
<title>Learning abstract concept embeddings from multi-modal data: Since you probably can‚Äôt see what i mean.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>255--265</pages>
<contexts>
<context position="1174" citStr="Hill and Korhonen, 2014" startWordPosition="159" endWordPosition="162">the hierarchy with a distance metric. To capture structured semantics, the entity similarity of context prediction are measured under the aggregated metrics of relevant categories along all inter-entity paths. We show that both the entity vectors and category distance metrics encode meaningful semantics. Experiments in entity linking and entity search show superiority of the proposed method. 1 Introduction There has been a growing interest in distributed representation that learns compact vectors (a.k.a embedding) for words (Mikolov et al., 2013a), phrases (Passos et al., 2014), and concepts (Hill and Korhonen, 2014), etc. The induced vectors are expected to capture semantic relatedness of the linguistic items, and are widely used in sentiment analysis (Tang et al., 2014), machine translation (Zhang et al., 2014), and information retrieval (Clinchant and Perronnin, 2013), to name a few. Despite the impressive success, existing work is still limited in utilizing structured knowledge to enhance the representation. For instance, word and phrase embeddings are largely induced from plain text. Though recent knowledge graph embeddings (Lin et al., 2015; Wang et al., 2014) integrate the relational structure amon</context>
<context position="31072" citStr="Hill and Korhonen, 2014" startWordPosition="5110" endWordPosition="5114">axonomy refinement and relation discovery. 5 Related Work Distributed representation There has been a growing interest in distributed representation of words. Skip-gram model (Mikolov et al., 2013a) is one of the most popular methods to learn word representations. The model aims to find a representation for each word that is useful for predicting its context words. Word-context similarity is measured by simple inner product. A set of recent works generalizing the basic skipgram to incorporate dependency context (Levy and Goldberg, 2014), word senses (Chen et al., 2014a), and multi-modal data (Hill and Korhonen, 2014). However, these work leverages limited structured knowledge. Our proposed method goes beyond skip-gram significantly such that we measures entity-context similarity under aggregated distance metrics of hierarchical category nodes. This effectively captures the structured knowledge. Another research line learn knowledge graph embedding (Lin et al., 2015; Wang et al., 2014; Bordes et al., 2013), which models entities as vectors and relations as some operations on the vector space (e.g., translation). These works aim at relation prediction for knowledge graph completion, and can be viewed as a s</context>
</contexts>
<marker>Hill, Korhonen, 2014</marker>
<rawString>Felix Hill and Anna Korhonen. 2014. Learning abstract concept embeddings from multi-modal data: Since you probably can‚Äôt see what i mean. In Proc. of EMNLP, pages 255‚Äì265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rianne Kaptein</author>
<author>Jaap Kamps</author>
</authors>
<title>Exploiting the category structure of wikipedia for entity ranking.</title>
<date>2013</date>
<journal>Artificial Intelligence,</journal>
<pages>194--111</pages>
<contexts>
<context position="2336" citStr="Kaptein and Kamps, 2013" startWordPosition="335" endWordPosition="338">5; Wang et al., 2014) integrate the relational structure among entities, they primarily target at link prediction and lack an explicit relatedness measure. In this paper, we propose to improve the distributed representations of entities by integrating hierarchical information from large-scale knowledge bases (KBs). An entity hierarchy groups entities into categories which are further organized to form a taxonomy. It provides rich structured knowledge on entity relatedness (Resnik, 1995). Our work goes beyond the previous heuristic use of entity hierarchy which relies on hand-crafted features (Kaptein and Kamps, 2013; Ponzetto and Strube, 2007), and develops a principled optimization-based framework. We learn a distance metric for each category node, and measure entity-context similarity under the aggregated metrics of all relevant categories. The metric aggregation encodes the hierarchical property that nearby entities tend to share common semantic features. We further provide a highly-efficient implementation in order to handle large complex hierarchies. We train a distributed representation for the whole entity hierarchy of Wikipedia. Both the entity vectors and the category distance metrics capture me</context>
<context position="25059" citStr="Kaptein and Kamps, 2013" startWordPosition="4186" endWordPosition="4189">he common criteria of precision@k, i.e., the percentage of relevant entities in the top-k results (we set k = 10), as well as precision@R where R is the number of golden standard entities for a query. 2http://www.inex.otago.ac.nz/tracks/entityranking/entity-ranking.asp 1297 Baselines We compare our algorithm with the following recent competitors. Balog (Balog et al., 2011) develops a probabilistic generative model which represents entities, as well as the query, as distributions over both words and categories. Entities are then ranked based on the KL-divergence between the distributions. K&amp;K (Kaptein and Kamps, 2013) exploits Wikipedia entity hierarchy to derive the content of each category, which is in turn used to measure relatedness with the query categories. It further incorporates inter-entity links for relevance propagation. Chen (Chen et al., 2014b) creates for each entity a context profile leveraging both the whole document (long-range) and sentences around entity (short-range) context, and models query text by a generative model. Categories are weighted based on the head words and other features. Our algorithm exploits a similar method for category matching. Methods Precision@10 Precision@R Balog</context>
</contexts>
<marker>Kaptein, Kamps, 2013</marker>
<rawString>Rianne Kaptein and Jaap Kamps. 2013. Exploiting the category structure of wikipedia for entity ranking. Artificial Intelligence, 194:111‚Äì129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sayali Kulkarni</author>
<author>Amit Singh</author>
<author>Ganesh Ramakrishnan</author>
<author>Soumen Chakrabarti</author>
</authors>
<title>Collective annotation of wikipedia entities in web text.</title>
<date>2009</date>
<booktitle>In Proc. of KDD,</booktitle>
<pages>457--466</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="22005" citStr="Kulkarni et al., 2009" startWordPosition="3719" endWordPosition="3722">nd 17K mentions in total. As in the baseline work, we use only the mentions whose referent entities are contained in Wikipedia. Criteria We adopt the common criteria, precision, recall, and F1. Let A‚àó be the golden standard entity annotations, and A be the annotations by entity linking model, then precision = |A‚àó ‚à© A |recall = A Al JA |IS I The F1 score is then computed based on the average precision and recall across all documents. Baselines We compare our algorithm with the following approaches. All the competitors are designed to be able to link general concept mentions to Wikipedia. CSAW (Kulkarni et al., 2009) has a similar framework as our algorithm. It measures entity relatedness using a variation of Jaccard similarity on Wikipedia page incoming links. Entity-TM (Han and Sun, 2012) models an entity as a distribution over mentions and words, and sets up a probabilistic generative process for the observed text. Ours-NoH. To validate the advantage of incorporating hierarchical structure, we design a baseline that relies on entity embedding without entity hierarchy. That is, we obtain entity vectors by fixing the distance metric in Eq 4 as an identity matrix. 4.1.2 Results Table 1 shows the performan</context>
</contexts>
<marker>Kulkarni, Singh, Ramakrishnan, Chakrabarti, 2009</marker>
<rawString>Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and Soumen Chakrabarti. 2009. Collective annotation of wikipedia entities in web text. In Proc. of KDD, pages 457‚Äì466. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Combining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database,</title>
<date>1998</date>
<pages>49--2</pages>
<contexts>
<context position="31890" citStr="Leacock and Chodorow, 1998" startWordPosition="5234" endWordPosition="5237"> metrics of hierarchical category nodes. This effectively captures the structured knowledge. Another research line learn knowledge graph embedding (Lin et al., 2015; Wang et al., 2014; Bordes et al., 2013), which models entities as vectors and relations as some operations on the vector space (e.g., translation). These works aim at relation prediction for knowledge graph completion, and can be viewed as a supplement to the above that extracts semantics from plain text. Utilizing hierarchical knowledge Semantic hierarchies are key sources of knowledge. Previous works (Ponzetto and Strube, 2007; Leacock and Chodorow, 1998) use KB hierarchies to define relatedness between concepts, typically based on path-length measure. Recent works (Yogatama et al., 2015; Zhao et al., 2011) learn representations through hierarchical sparse coding that enforces similar sparse patterns between nearby nodes. Category hierarchies have also been widely used in classification (Xiao et al., 2011; Weinberger and Chapelle, 2009). E.g., in (Verma et al., 2012) category nodes are endowed with discriminative power by learning distance metrics. Our approach differs in terms of entity vector learning and metric aggregation on DAG hierarchy.</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Claudia Leacock and Martin Chodorow. 1998. Combining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database, 49(2):265‚Äì283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Dependencybased word embeddings.</title>
<date>2014</date>
<booktitle>In Proc. ofACL,</booktitle>
<volume>2</volume>
<pages>302--308</pages>
<contexts>
<context position="30990" citStr="Levy and Goldberg, 2014" startWordPosition="5097" endWordPosition="5100">o capture semantic similarity of the internal categories. This can be helpful in taxonomy refinement and relation discovery. 5 Related Work Distributed representation There has been a growing interest in distributed representation of words. Skip-gram model (Mikolov et al., 2013a) is one of the most popular methods to learn word representations. The model aims to find a representation for each word that is useful for predicting its context words. Word-context similarity is measured by simple inner product. A set of recent works generalizing the basic skipgram to incorporate dependency context (Levy and Goldberg, 2014), word senses (Chen et al., 2014a), and multi-modal data (Hill and Korhonen, 2014). However, these work leverages limited structured knowledge. Our proposed method goes beyond skip-gram significantly such that we measures entity-context similarity under aggregated distance metrics of hierarchical category nodes. This effectively captures the structured knowledge. Another research line learn knowledge graph embedding (Lin et al., 2015; Wang et al., 2014; Bordes et al., 2013), which models entities as vectors and relations as some operations on the vector space (e.g., translation). These works a</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. Dependencybased word embeddings. In Proc. ofACL, volume 2, pages 302‚Äì308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yankai Lin</author>
<author>Zhiyuan Liu</author>
<author>Maosong Sun</author>
<author>Yang Liu</author>
<author>Xuan Zhu</author>
</authors>
<title>Learning entity and relation embeddings for knowledge graph completion.</title>
<date>2015</date>
<booktitle>In Proc. of AAAI.</booktitle>
<contexts>
<context position="1714" citStr="Lin et al., 2015" startWordPosition="242" endWordPosition="245">013a), phrases (Passos et al., 2014), and concepts (Hill and Korhonen, 2014), etc. The induced vectors are expected to capture semantic relatedness of the linguistic items, and are widely used in sentiment analysis (Tang et al., 2014), machine translation (Zhang et al., 2014), and information retrieval (Clinchant and Perronnin, 2013), to name a few. Despite the impressive success, existing work is still limited in utilizing structured knowledge to enhance the representation. For instance, word and phrase embeddings are largely induced from plain text. Though recent knowledge graph embeddings (Lin et al., 2015; Wang et al., 2014) integrate the relational structure among entities, they primarily target at link prediction and lack an explicit relatedness measure. In this paper, we propose to improve the distributed representations of entities by integrating hierarchical information from large-scale knowledge bases (KBs). An entity hierarchy groups entities into categories which are further organized to form a taxonomy. It provides rich structured knowledge on entity relatedness (Resnik, 1995). Our work goes beyond the previous heuristic use of entity hierarchy which relies on hand-crafted features (K</context>
<context position="31427" citStr="Lin et al., 2015" startWordPosition="5160" endWordPosition="5163">rds. Word-context similarity is measured by simple inner product. A set of recent works generalizing the basic skipgram to incorporate dependency context (Levy and Goldberg, 2014), word senses (Chen et al., 2014a), and multi-modal data (Hill and Korhonen, 2014). However, these work leverages limited structured knowledge. Our proposed method goes beyond skip-gram significantly such that we measures entity-context similarity under aggregated distance metrics of hierarchical category nodes. This effectively captures the structured knowledge. Another research line learn knowledge graph embedding (Lin et al., 2015; Wang et al., 2014; Bordes et al., 2013), which models entities as vectors and relations as some operations on the vector space (e.g., translation). These works aim at relation prediction for knowledge graph completion, and can be viewed as a supplement to the above that extracts semantics from plain text. Utilizing hierarchical knowledge Semantic hierarchies are key sources of knowledge. Previous works (Ponzetto and Strube, 2007; Leacock and Chodorow, 1998) use KB hierarchies to define relatedness between concepts, typically based on path-length measure. Recent works (Yogatama et al., 2015; </context>
</contexts>
<marker>Lin, Liu, Sun, Liu, Zhu, 2015</marker>
<rawString>Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning entity and relation embeddings for knowledge graph completion. In Proc. of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<contexts>
<context position="1101" citStr="Mikolov et al., 2013" startWordPosition="148" endWordPosition="151">ge bases. The novel embedding model associates each category node of the hierarchy with a distance metric. To capture structured semantics, the entity similarity of context prediction are measured under the aggregated metrics of relevant categories along all inter-entity paths. We show that both the entity vectors and category distance metrics encode meaningful semantics. Experiments in entity linking and entity search show superiority of the proposed method. 1 Introduction There has been a growing interest in distributed representation that learns compact vectors (a.k.a embedding) for words (Mikolov et al., 2013a), phrases (Passos et al., 2014), and concepts (Hill and Korhonen, 2014), etc. The induced vectors are expected to capture semantic relatedness of the linguistic items, and are widely used in sentiment analysis (Tang et al., 2014), machine translation (Zhang et al., 2014), and information retrieval (Clinchant and Perronnin, 2013), to name a few. Despite the impressive success, existing work is still limited in utilizing structured knowledge to enhance the representation. For instance, word and phrase embeddings are largely induced from plain text. Though recent knowledge graph embeddings (Lin</context>
<context position="5157" citStr="Mikolov et al., 2013" startWordPosition="755" endWordPosition="758">r predicting other entities occurring in its context. We build entity‚Äôs context upon KB encyclopedia articles, where entity annotations are readily available. We further incorporate the entity hierarchical structure in the context prediction through distance metric learning and aggregation, which encodes the rich structured knowledge in the induced representations. Our method is flexible and efficient to model large complex DAG-structured hierarchies. Figure 1 shows an overview of the model architecture. 2.1 Model Architecture Our architecture builds on the skip-gram word embedding framework (Mikolov et al., 2013b). In the skip-gram model, a set of (target, context) word pairs are extracted by sliding a fixedlength context window over a text corpus, and the word vectors are learned such that the similarity of the target- and context-word vectors is maximized. We generalize both the context definition and the similarity measure for entity hierarchy embedding. Unlike words that can be directly extracted from plain text, entities are hidden semantics underlying their surface forms. In order to avoid manual annotation cost, we exploit the text corpora from KBs where the referent entities of surface text a</context>
<context position="14108" citStr="Mikolov et al., 2013" startWordPosition="2386" endWordPosition="2389"> Mh ÔøΩ: 0, and kvek2 = k¬Øvek2 = 1 to avoid scale ambiguity. The likelihood of each data sample is defined as a softmax in Eq 2, which iterates over all entities in the denominator and is thus computationally prohibitive. We apply the negative sampling technique as in conventional skip-gram model, by replacing each log probability log p(eC|eT) with ÔøΩk log œÉ(‚àíd (eT, eC)) + i=1 Eei‚àºP(e) [log œÉ(‚àíd (eT, ei))] , where œÉ(x) = 1/(1 + exp(‚àíx)) is the sigmoid function; and for each data sample we draw k negative samples from the noise distribution P(e) ‚àù U(e)3/4 with U(e) being the unigram distribution (Mikolov et al., 2013b). The negative sampling objective is optimized using coordinate gradient ascent, as shown in Algorithm 1. To avoid overfitting and improve efficiency, in practice we restrict the distance metrics Mh to be diagonal (Xing et al., 2002). Thus the PSD project of Mh (line 17) is simply taking the positive part for each diagonal elements. Algorithm 1 Entity Hierarchy Embedding Input: The training data D = {(eT, eC)}, Entity hierarchy, Parameters: n ‚Äì dimension of the embedding k ‚Äì number of negative samples 77 ‚Äì gradient learning rate B ‚Äì minibatch size 1: Initialize v, ¬Øv, M randomly such that v1</context>
<context position="30644" citStr="Mikolov et al., 2013" startWordPosition="5039" endWordPosition="5042">of the figure to showcase the clustering of semantically relevant categories. For instance, the categories of Microsoft Windows operating systems, and those of the Xbox games, are embedded close to each other, respectively. The results validate that our hierarchy embedding model can not only encode relatedness between leaf entities, but also capture semantic similarity of the internal categories. This can be helpful in taxonomy refinement and relation discovery. 5 Related Work Distributed representation There has been a growing interest in distributed representation of words. Skip-gram model (Mikolov et al., 2013a) is one of the most popular methods to learn word representations. The model aims to find a representation for each word that is useful for predicting its context words. Word-context similarity is measured by simple inner product. A set of recent works generalizing the basic skipgram to incorporate dependency context (Levy and Goldberg, 2014), word senses (Chen et al., 2014a), and multi-modal data (Hill and Korhonen, 2014). However, these work leverages limited structured knowledge. Our proposed method goes beyond skip-gram significantly such that we measures entity-context similarity under </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Proc. of NIPS,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="1101" citStr="Mikolov et al., 2013" startWordPosition="148" endWordPosition="151">ge bases. The novel embedding model associates each category node of the hierarchy with a distance metric. To capture structured semantics, the entity similarity of context prediction are measured under the aggregated metrics of relevant categories along all inter-entity paths. We show that both the entity vectors and category distance metrics encode meaningful semantics. Experiments in entity linking and entity search show superiority of the proposed method. 1 Introduction There has been a growing interest in distributed representation that learns compact vectors (a.k.a embedding) for words (Mikolov et al., 2013a), phrases (Passos et al., 2014), and concepts (Hill and Korhonen, 2014), etc. The induced vectors are expected to capture semantic relatedness of the linguistic items, and are widely used in sentiment analysis (Tang et al., 2014), machine translation (Zhang et al., 2014), and information retrieval (Clinchant and Perronnin, 2013), to name a few. Despite the impressive success, existing work is still limited in utilizing structured knowledge to enhance the representation. For instance, word and phrase embeddings are largely induced from plain text. Though recent knowledge graph embeddings (Lin</context>
<context position="5157" citStr="Mikolov et al., 2013" startWordPosition="755" endWordPosition="758">r predicting other entities occurring in its context. We build entity‚Äôs context upon KB encyclopedia articles, where entity annotations are readily available. We further incorporate the entity hierarchical structure in the context prediction through distance metric learning and aggregation, which encodes the rich structured knowledge in the induced representations. Our method is flexible and efficient to model large complex DAG-structured hierarchies. Figure 1 shows an overview of the model architecture. 2.1 Model Architecture Our architecture builds on the skip-gram word embedding framework (Mikolov et al., 2013b). In the skip-gram model, a set of (target, context) word pairs are extracted by sliding a fixedlength context window over a text corpus, and the word vectors are learned such that the similarity of the target- and context-word vectors is maximized. We generalize both the context definition and the similarity measure for entity hierarchy embedding. Unlike words that can be directly extracted from plain text, entities are hidden semantics underlying their surface forms. In order to avoid manual annotation cost, we exploit the text corpora from KBs where the referent entities of surface text a</context>
<context position="14108" citStr="Mikolov et al., 2013" startWordPosition="2386" endWordPosition="2389"> Mh ÔøΩ: 0, and kvek2 = k¬Øvek2 = 1 to avoid scale ambiguity. The likelihood of each data sample is defined as a softmax in Eq 2, which iterates over all entities in the denominator and is thus computationally prohibitive. We apply the negative sampling technique as in conventional skip-gram model, by replacing each log probability log p(eC|eT) with ÔøΩk log œÉ(‚àíd (eT, eC)) + i=1 Eei‚àºP(e) [log œÉ(‚àíd (eT, ei))] , where œÉ(x) = 1/(1 + exp(‚àíx)) is the sigmoid function; and for each data sample we draw k negative samples from the noise distribution P(e) ‚àù U(e)3/4 with U(e) being the unigram distribution (Mikolov et al., 2013b). The negative sampling objective is optimized using coordinate gradient ascent, as shown in Algorithm 1. To avoid overfitting and improve efficiency, in practice we restrict the distance metrics Mh to be diagonal (Xing et al., 2002). Thus the PSD project of Mh (line 17) is simply taking the positive part for each diagonal elements. Algorithm 1 Entity Hierarchy Embedding Input: The training data D = {(eT, eC)}, Entity hierarchy, Parameters: n ‚Äì dimension of the embedding k ‚Äì number of negative samples 77 ‚Äì gradient learning rate B ‚Äì minibatch size 1: Initialize v, ¬Øv, M randomly such that v1</context>
<context position="30644" citStr="Mikolov et al., 2013" startWordPosition="5039" endWordPosition="5042">of the figure to showcase the clustering of semantically relevant categories. For instance, the categories of Microsoft Windows operating systems, and those of the Xbox games, are embedded close to each other, respectively. The results validate that our hierarchy embedding model can not only encode relatedness between leaf entities, but also capture semantic similarity of the internal categories. This can be helpful in taxonomy refinement and relation discovery. 5 Related Work Distributed representation There has been a growing interest in distributed representation of words. Skip-gram model (Mikolov et al., 2013a) is one of the most popular methods to learn word representations. The model aims to find a representation for each word that is useful for predicting its context words. Word-context similarity is measured by simple inner product. A set of recent works generalizing the basic skipgram to incorporate dependency context (Levy and Goldberg, 2014), word senses (Chen et al., 2014a), and multi-modal data (Hill and Korhonen, 2014). However, these work leverages limited structured knowledge. Our proposed method goes beyond skip-gram significantly such that we measures entity-context similarity under </context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Proc. of NIPS, pages 3111‚Äì3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Passos</author>
<author>Vineet Kumar</author>
<author>Andrew McCallum</author>
</authors>
<title>Lexicon Infused Phrase Embeddings for Named Entity Resolution,</title>
<date>2014</date>
<pages>78--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1134" citStr="Passos et al., 2014" startWordPosition="153" endWordPosition="156">el associates each category node of the hierarchy with a distance metric. To capture structured semantics, the entity similarity of context prediction are measured under the aggregated metrics of relevant categories along all inter-entity paths. We show that both the entity vectors and category distance metrics encode meaningful semantics. Experiments in entity linking and entity search show superiority of the proposed method. 1 Introduction There has been a growing interest in distributed representation that learns compact vectors (a.k.a embedding) for words (Mikolov et al., 2013a), phrases (Passos et al., 2014), and concepts (Hill and Korhonen, 2014), etc. The induced vectors are expected to capture semantic relatedness of the linguistic items, and are widely used in sentiment analysis (Tang et al., 2014), machine translation (Zhang et al., 2014), and information retrieval (Clinchant and Perronnin, 2013), to name a few. Despite the impressive success, existing work is still limited in utilizing structured knowledge to enhance the representation. For instance, word and phrase embeddings are largely induced from plain text. Though recent knowledge graph embeddings (Lin et al., 2015; Wang et al., 2014)</context>
</contexts>
<marker>Passos, Kumar, McCallum, 2014</marker>
<rawString>Alexandre Passos, Vineet Kumar, and Andrew McCallum, 2014. Lexicon Infused Phrase Embeddings for Named Entity Resolution, pages 78‚Äì86. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Knowledge derived from wikipedia for computing semantic relatedness.</title>
<date>2007</date>
<journal>JAIR,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="2364" citStr="Ponzetto and Strube, 2007" startWordPosition="339" endWordPosition="342">egrate the relational structure among entities, they primarily target at link prediction and lack an explicit relatedness measure. In this paper, we propose to improve the distributed representations of entities by integrating hierarchical information from large-scale knowledge bases (KBs). An entity hierarchy groups entities into categories which are further organized to form a taxonomy. It provides rich structured knowledge on entity relatedness (Resnik, 1995). Our work goes beyond the previous heuristic use of entity hierarchy which relies on hand-crafted features (Kaptein and Kamps, 2013; Ponzetto and Strube, 2007), and develops a principled optimization-based framework. We learn a distance metric for each category node, and measure entity-context similarity under the aggregated metrics of all relevant categories. The metric aggregation encodes the hierarchical property that nearby entities tend to share common semantic features. We further provide a highly-efficient implementation in order to handle large complex hierarchies. We train a distributed representation for the whole entity hierarchy of Wikipedia. Both the entity vectors and the category distance metrics capture meaningful semantics. We deplo</context>
<context position="31861" citStr="Ponzetto and Strube, 2007" startWordPosition="5230" endWordPosition="5233">y under aggregated distance metrics of hierarchical category nodes. This effectively captures the structured knowledge. Another research line learn knowledge graph embedding (Lin et al., 2015; Wang et al., 2014; Bordes et al., 2013), which models entities as vectors and relations as some operations on the vector space (e.g., translation). These works aim at relation prediction for knowledge graph completion, and can be viewed as a supplement to the above that extracts semantics from plain text. Utilizing hierarchical knowledge Semantic hierarchies are key sources of knowledge. Previous works (Ponzetto and Strube, 2007; Leacock and Chodorow, 1998) use KB hierarchies to define relatedness between concepts, typically based on path-length measure. Recent works (Yogatama et al., 2015; Zhao et al., 2011) learn representations through hierarchical sparse coding that enforces similar sparse patterns between nearby nodes. Category hierarchies have also been widely used in classification (Xiao et al., 2011; Weinberger and Chapelle, 2009). E.g., in (Verma et al., 2012) category nodes are endowed with discriminative power by learning distance metrics. Our approach differs in terms of entity vector learning and metric </context>
</contexts>
<marker>Ponzetto, Strube, 2007</marker>
<rawString>Simone Paolo Ponzetto and Michael Strube. 2007. Knowledge derived from wikipedia for computing semantic relatedness. JAIR, 30(1):181‚Äì212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proc. of IJCAI,</booktitle>
<pages>448--453</pages>
<contexts>
<context position="2204" citStr="Resnik, 1995" startWordPosition="317" endWordPosition="318"> word and phrase embeddings are largely induced from plain text. Though recent knowledge graph embeddings (Lin et al., 2015; Wang et al., 2014) integrate the relational structure among entities, they primarily target at link prediction and lack an explicit relatedness measure. In this paper, we propose to improve the distributed representations of entities by integrating hierarchical information from large-scale knowledge bases (KBs). An entity hierarchy groups entities into categories which are further organized to form a taxonomy. It provides rich structured knowledge on entity relatedness (Resnik, 1995). Our work goes beyond the previous heuristic use of entity hierarchy which relies on hand-crafted features (Kaptein and Kamps, 2013; Ponzetto and Strube, 2007), and develops a principled optimization-based framework. We learn a distance metric for each category node, and measure entity-context similarity under the aggregated metrics of all relevant categories. The metric aggregation encodes the hierarchical property that nearby entities tend to share common semantic features. We further provide a highly-efficient implementation in order to handle large complex hierarchies. We train a distribu</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proc. of IJCAI, pages 448‚Äì453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duyu Tang</author>
<author>Furu Wei</author>
<author>Nan Yang</author>
<author>Ming Zhou</author>
<author>Ting Liu</author>
<author>Bing Qin</author>
</authors>
<title>Learning sentiment-specific word embedding for twitter sentiment classification.</title>
<date>2014</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>1555--1565</pages>
<contexts>
<context position="1332" citStr="Tang et al., 2014" startWordPosition="185" endWordPosition="188">vant categories along all inter-entity paths. We show that both the entity vectors and category distance metrics encode meaningful semantics. Experiments in entity linking and entity search show superiority of the proposed method. 1 Introduction There has been a growing interest in distributed representation that learns compact vectors (a.k.a embedding) for words (Mikolov et al., 2013a), phrases (Passos et al., 2014), and concepts (Hill and Korhonen, 2014), etc. The induced vectors are expected to capture semantic relatedness of the linguistic items, and are widely used in sentiment analysis (Tang et al., 2014), machine translation (Zhang et al., 2014), and information retrieval (Clinchant and Perronnin, 2013), to name a few. Despite the impressive success, existing work is still limited in utilizing structured knowledge to enhance the representation. For instance, word and phrase embeddings are largely induced from plain text. Though recent knowledge graph embeddings (Lin et al., 2015; Wang et al., 2014) integrate the relational structure among entities, they primarily target at link prediction and lack an explicit relatedness measure. In this paper, we propose to improve the distributed representa</context>
</contexts>
<marker>Tang, Wei, Yang, Zhou, Liu, Qin, 2014</marker>
<rawString>Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu, and Bing Qin. 2014. Learning sentiment-specific word embedding for twitter sentiment classification. In Proc. of ACL, pages 1555‚Äì1565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurens Van der Maaten</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Visualizing data using t-sne.</title>
<date>2008</date>
<journal>JMLR,</journal>
<pages>9--2579</pages>
<marker>Van der Maaten, Hinton, 2008</marker>
<rawString>Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. JMLR, 9(2579-2605):85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nakul Verma</author>
<author>Dhruv Mahajan</author>
<author>Sundararajan Sellamanickam</author>
<author>Vinod Nair</author>
</authors>
<title>Learning hierarchical similarity metrics.</title>
<date>2012</date>
<booktitle>In Proc. of CVPR,</booktitle>
<pages>2280--2287</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="32310" citStr="Verma et al., 2012" startWordPosition="5300" endWordPosition="5303">ove that extracts semantics from plain text. Utilizing hierarchical knowledge Semantic hierarchies are key sources of knowledge. Previous works (Ponzetto and Strube, 2007; Leacock and Chodorow, 1998) use KB hierarchies to define relatedness between concepts, typically based on path-length measure. Recent works (Yogatama et al., 2015; Zhao et al., 2011) learn representations through hierarchical sparse coding that enforces similar sparse patterns between nearby nodes. Category hierarchies have also been widely used in classification (Xiao et al., 2011; Weinberger and Chapelle, 2009). E.g., in (Verma et al., 2012) category nodes are endowed with discriminative power by learning distance metrics. Our approach differs in terms of entity vector learning and metric aggregation on DAG hierarchy. 6 Conclusion In this paper, we proposed to learn entity hierarchy embedding to boost semantic NLP tasks. A principled framework was developed to incorporate both text context and entity hierarchical structure from large-scale knowledge bases. We learn a distance metric for each category node, and measure entity vector similarity under aggregated metrics. A flexible and efficient metric aggregation scheme was also de</context>
</contexts>
<marker>Verma, Mahajan, Sellamanickam, Nair, 2012</marker>
<rawString>Nakul Verma, Dhruv Mahajan, Sundararajan Sellamanickam, and Vinod Nair. 2012. Learning hierarchical similarity metrics. In Proc. of CVPR, pages 2280‚Äì2287. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhen Wang</author>
<author>Jianwen Zhang</author>
<author>Jianlin Feng</author>
<author>Zheng Chen</author>
</authors>
<title>Knowledge graph and text jointly embedding.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1734" citStr="Wang et al., 2014" startWordPosition="246" endWordPosition="249">ssos et al., 2014), and concepts (Hill and Korhonen, 2014), etc. The induced vectors are expected to capture semantic relatedness of the linguistic items, and are widely used in sentiment analysis (Tang et al., 2014), machine translation (Zhang et al., 2014), and information retrieval (Clinchant and Perronnin, 2013), to name a few. Despite the impressive success, existing work is still limited in utilizing structured knowledge to enhance the representation. For instance, word and phrase embeddings are largely induced from plain text. Though recent knowledge graph embeddings (Lin et al., 2015; Wang et al., 2014) integrate the relational structure among entities, they primarily target at link prediction and lack an explicit relatedness measure. In this paper, we propose to improve the distributed representations of entities by integrating hierarchical information from large-scale knowledge bases (KBs). An entity hierarchy groups entities into categories which are further organized to form a taxonomy. It provides rich structured knowledge on entity relatedness (Resnik, 1995). Our work goes beyond the previous heuristic use of entity hierarchy which relies on hand-crafted features (Kaptein and Kamps, 20</context>
<context position="31446" citStr="Wang et al., 2014" startWordPosition="5164" endWordPosition="5167">similarity is measured by simple inner product. A set of recent works generalizing the basic skipgram to incorporate dependency context (Levy and Goldberg, 2014), word senses (Chen et al., 2014a), and multi-modal data (Hill and Korhonen, 2014). However, these work leverages limited structured knowledge. Our proposed method goes beyond skip-gram significantly such that we measures entity-context similarity under aggregated distance metrics of hierarchical category nodes. This effectively captures the structured knowledge. Another research line learn knowledge graph embedding (Lin et al., 2015; Wang et al., 2014; Bordes et al., 2013), which models entities as vectors and relations as some operations on the vector space (e.g., translation). These works aim at relation prediction for knowledge graph completion, and can be viewed as a supplement to the above that extracts semantics from plain text. Utilizing hierarchical knowledge Semantic hierarchies are key sources of knowledge. Previous works (Ponzetto and Strube, 2007; Leacock and Chodorow, 1998) use KB hierarchies to define relatedness between concepts, typically based on path-length measure. Recent works (Yogatama et al., 2015; Zhao et al., 2011) </context>
</contexts>
<marker>Wang, Zhang, Feng, Chen, 2014</marker>
<rawString>Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge graph and text jointly embedding. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kilian Q Weinberger</author>
<author>Olivier Chapelle</author>
</authors>
<title>Large margin taxonomy embedding for document categorization.</title>
<date>2009</date>
<booktitle>In Proc. of NIPS,</booktitle>
<pages>1737--1744</pages>
<contexts>
<context position="32279" citStr="Weinberger and Chapelle, 2009" startWordPosition="5293" endWordPosition="5297">nd can be viewed as a supplement to the above that extracts semantics from plain text. Utilizing hierarchical knowledge Semantic hierarchies are key sources of knowledge. Previous works (Ponzetto and Strube, 2007; Leacock and Chodorow, 1998) use KB hierarchies to define relatedness between concepts, typically based on path-length measure. Recent works (Yogatama et al., 2015; Zhao et al., 2011) learn representations through hierarchical sparse coding that enforces similar sparse patterns between nearby nodes. Category hierarchies have also been widely used in classification (Xiao et al., 2011; Weinberger and Chapelle, 2009). E.g., in (Verma et al., 2012) category nodes are endowed with discriminative power by learning distance metrics. Our approach differs in terms of entity vector learning and metric aggregation on DAG hierarchy. 6 Conclusion In this paper, we proposed to learn entity hierarchy embedding to boost semantic NLP tasks. A principled framework was developed to incorporate both text context and entity hierarchical structure from large-scale knowledge bases. We learn a distance metric for each category node, and measure entity vector similarity under aggregated metrics. A flexible and efficient metric</context>
</contexts>
<marker>Weinberger, Chapelle, 2009</marker>
<rawString>Kilian Q Weinberger and Olivier Chapelle. 2009. Large margin taxonomy embedding for document categorization. In Proc. of NIPS, pages 1737‚Äì1744.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Xiao</author>
<author>Dengyong Zhou</author>
<author>Mingrui Wu</author>
</authors>
<title>Hierarchical classification via orthogonal transfer.</title>
<date>2011</date>
<booktitle>In Proc. of ICML,</booktitle>
<pages>801--808</pages>
<contexts>
<context position="32247" citStr="Xiao et al., 2011" startWordPosition="5289" endWordPosition="5292">graph completion, and can be viewed as a supplement to the above that extracts semantics from plain text. Utilizing hierarchical knowledge Semantic hierarchies are key sources of knowledge. Previous works (Ponzetto and Strube, 2007; Leacock and Chodorow, 1998) use KB hierarchies to define relatedness between concepts, typically based on path-length measure. Recent works (Yogatama et al., 2015; Zhao et al., 2011) learn representations through hierarchical sparse coding that enforces similar sparse patterns between nearby nodes. Category hierarchies have also been widely used in classification (Xiao et al., 2011; Weinberger and Chapelle, 2009). E.g., in (Verma et al., 2012) category nodes are endowed with discriminative power by learning distance metrics. Our approach differs in terms of entity vector learning and metric aggregation on DAG hierarchy. 6 Conclusion In this paper, we proposed to learn entity hierarchy embedding to boost semantic NLP tasks. A principled framework was developed to incorporate both text context and entity hierarchical structure from large-scale knowledge bases. We learn a distance metric for each category node, and measure entity vector similarity under aggregated metrics.</context>
</contexts>
<marker>Xiao, Zhou, Wu, 2011</marker>
<rawString>Lin Xiao, Dengyong Zhou, and Mingrui Wu. 2011. Hierarchical classification via orthogonal transfer. In Proc. of ICML, pages 801‚Äì808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric P Xing</author>
<author>Michael I Jordan</author>
<author>Stuart Russell</author>
<author>Andrew Y Ng</author>
</authors>
<title>Distance metric learning with application to clustering with side-information.</title>
<date>2002</date>
<booktitle>In Proc. of NIPS,</booktitle>
<pages>505--512</pages>
<contexts>
<context position="14343" citStr="Xing et al., 2002" startWordPosition="2424" endWordPosition="2427">tive sampling technique as in conventional skip-gram model, by replacing each log probability log p(eC|eT) with ÔøΩk log œÉ(‚àíd (eT, eC)) + i=1 Eei‚àºP(e) [log œÉ(‚àíd (eT, ei))] , where œÉ(x) = 1/(1 + exp(‚àíx)) is the sigmoid function; and for each data sample we draw k negative samples from the noise distribution P(e) ‚àù U(e)3/4 with U(e) being the unigram distribution (Mikolov et al., 2013b). The negative sampling objective is optimized using coordinate gradient ascent, as shown in Algorithm 1. To avoid overfitting and improve efficiency, in practice we restrict the distance metrics Mh to be diagonal (Xing et al., 2002). Thus the PSD project of Mh (line 17) is simply taking the positive part for each diagonal elements. Algorithm 1 Entity Hierarchy Embedding Input: The training data D = {(eT, eC)}, Entity hierarchy, Parameters: n ‚Äì dimension of the embedding k ‚Äì number of negative samples 77 ‚Äì gradient learning rate B ‚Äì minibatch size 1: Initialize v, ¬Øv, M randomly such that v112 = ÔøΩ¬ØvÔøΩ2 = 1 andM&gt;- 0. 2: repeat 3: Sample a batch B = {(eT, eC)i}Bi=1 from D 4: for all (eT, eC) ‚àà B do 5: Compute {P, œÄ, 7}eT ,eC for metric aggregation 6: Sample negative pairs {(eT, ei)}ki=1 7: Compute {{P, œÄ, 7}eT ,ei}ki=1 for m</context>
</contexts>
<marker>Xing, Jordan, Russell, Ng, 2002</marker>
<rawString>Eric P Xing, Michael I Jordan, Stuart Russell, and Andrew Y Ng. 2002. Distance metric learning with application to clustering with side-information. In Proc. of NIPS, pages 505‚Äì512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dani Yogatama</author>
<author>Manaal Faruqui</author>
<author>Chris Dyer</author>
<author>Noah Smith</author>
</authors>
<title>Learning word representations with hierarchical sparse coding.</title>
<date>2015</date>
<booktitle>Proc. of ICML.</booktitle>
<contexts>
<context position="32025" citStr="Yogatama et al., 2015" startWordPosition="5254" endWordPosition="5257">dding (Lin et al., 2015; Wang et al., 2014; Bordes et al., 2013), which models entities as vectors and relations as some operations on the vector space (e.g., translation). These works aim at relation prediction for knowledge graph completion, and can be viewed as a supplement to the above that extracts semantics from plain text. Utilizing hierarchical knowledge Semantic hierarchies are key sources of knowledge. Previous works (Ponzetto and Strube, 2007; Leacock and Chodorow, 1998) use KB hierarchies to define relatedness between concepts, typically based on path-length measure. Recent works (Yogatama et al., 2015; Zhao et al., 2011) learn representations through hierarchical sparse coding that enforces similar sparse patterns between nearby nodes. Category hierarchies have also been widely used in classification (Xiao et al., 2011; Weinberger and Chapelle, 2009). E.g., in (Verma et al., 2012) category nodes are endowed with discriminative power by learning distance metrics. Our approach differs in terms of entity vector learning and metric aggregation on DAG hierarchy. 6 Conclusion In this paper, we proposed to learn entity hierarchy embedding to boost semantic NLP tasks. A principled framework was de</context>
</contexts>
<marker>Yogatama, Faruqui, Dyer, Smith, 2015</marker>
<rawString>Dani Yogatama, Manaal Faruqui, Chris Dyer, and Noah Smith. 2015. Learning word representations with hierarchical sparse coding. Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiajun Zhang</author>
<author>Shujie Liu</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Chengqing Zong</author>
</authors>
<title>Bilingually-constrained phrase embeddings for machine translation. In</title>
<date>2014</date>
<booktitle>Proc. ofACL.</booktitle>
<contexts>
<context position="1374" citStr="Zhang et al., 2014" startWordPosition="192" endWordPosition="195">ths. We show that both the entity vectors and category distance metrics encode meaningful semantics. Experiments in entity linking and entity search show superiority of the proposed method. 1 Introduction There has been a growing interest in distributed representation that learns compact vectors (a.k.a embedding) for words (Mikolov et al., 2013a), phrases (Passos et al., 2014), and concepts (Hill and Korhonen, 2014), etc. The induced vectors are expected to capture semantic relatedness of the linguistic items, and are widely used in sentiment analysis (Tang et al., 2014), machine translation (Zhang et al., 2014), and information retrieval (Clinchant and Perronnin, 2013), to name a few. Despite the impressive success, existing work is still limited in utilizing structured knowledge to enhance the representation. For instance, word and phrase embeddings are largely induced from plain text. Though recent knowledge graph embeddings (Lin et al., 2015; Wang et al., 2014) integrate the relational structure among entities, they primarily target at link prediction and lack an explicit relatedness measure. In this paper, we propose to improve the distributed representations of entities by integrating hierarchi</context>
</contexts>
<marker>Zhang, Liu, Li, Zhou, Zong, 2014</marker>
<rawString>Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, and Chengqing Zong. 2014. Bilingually-constrained phrase embeddings for machine translation. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Zhao</author>
<author>Fei Li</author>
<author>Eric P Xing</author>
</authors>
<title>Large-scale category structure aware image categorization.</title>
<date>2011</date>
<booktitle>In Proc. of NIPS,</booktitle>
<pages>1251--1259</pages>
<contexts>
<context position="32045" citStr="Zhao et al., 2011" startWordPosition="5258" endWordPosition="5261">; Wang et al., 2014; Bordes et al., 2013), which models entities as vectors and relations as some operations on the vector space (e.g., translation). These works aim at relation prediction for knowledge graph completion, and can be viewed as a supplement to the above that extracts semantics from plain text. Utilizing hierarchical knowledge Semantic hierarchies are key sources of knowledge. Previous works (Ponzetto and Strube, 2007; Leacock and Chodorow, 1998) use KB hierarchies to define relatedness between concepts, typically based on path-length measure. Recent works (Yogatama et al., 2015; Zhao et al., 2011) learn representations through hierarchical sparse coding that enforces similar sparse patterns between nearby nodes. Category hierarchies have also been widely used in classification (Xiao et al., 2011; Weinberger and Chapelle, 2009). E.g., in (Verma et al., 2012) category nodes are endowed with discriminative power by learning distance metrics. Our approach differs in terms of entity vector learning and metric aggregation on DAG hierarchy. 6 Conclusion In this paper, we proposed to learn entity hierarchy embedding to boost semantic NLP tasks. A principled framework was developed to incorpora</context>
</contexts>
<marker>Zhao, Li, Xing, 2011</marker>
<rawString>Bin Zhao, Fei Li, and Eric P Xing. 2011. Large-scale category structure aware image categorization. In Proc. of NIPS, pages 1251‚Äì1259.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>