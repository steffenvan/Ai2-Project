<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.065816">
<title confidence="0.9724485">
Disambiguating Noun and Verb Senses Using
Automatically Acquired Selectional Preferences*
</title>
<author confidence="0.911077">
Diana McCarthy and John Carroll
</author>
<affiliation confidence="0.891995">
Cognitive k Computing Sciences
University of Sussex
</affiliation>
<address confidence="0.982739">
Brighton BN1 9QH, UK
</address>
<email confidence="0.911454">
fdianam,johncalOcogs.susx.ac.uk
</email>
<author confidence="0.684107">
Judita Preiss
</author>
<affiliation confidence="0.959115">
Computer Laboratory
University of Cambridge, JJ Thomson Avenue
</affiliation>
<address confidence="0.980007">
Cambridge CB3 OFD, UK
</address>
<email confidence="0.929183">
Judita.PreissOcl.cam.ac.uk
</email>
<sectionHeader confidence="0.981119" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999925833333333">
Our system for the SENSEVAL-2 all words task
uses automatically acquired selectional prefer-
ences to sense tag subject and object head
nouns, along with the associated verbal pred-
icates. The selectional preferences comprise
probability distributions over WordNet nouns,
and these distributions are conditioned on
WordNet verb classes. The conditional distri-
butions are used directly to disambiguate the
head nouns. We use prior distributions and
Bayes rule to compute the highest probability
verb class, given a noun class. We also use
anaphora resolution and the &apos;one sense per dis-
course&apos; heuristic to cover nouns and verbs not
occurring in these relationships in the target
text. The selectional preferences are acquired
without recourse to sense tagged data so our
system is unsupervised.
</bodyText>
<sectionHeader confidence="0.995596" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999917846153846">
In the first SENSEVAL, we used automati-
cally acquired selectional preferences to disam-
biguate head nouns occurring in specific gram-
matical relationships (Carroll and McCarthy,
2000). The selectional preference models pro-
vided co-occurrence behaviour between Word-
Net synsetsl in the noun hyponym hierarchy
and verbal predicates. Preference scores, based
on mutual information, were attached to the
classes in the models. These scores were condi-
tioned on the verbal context and the grammat-
ical relationship in which the nouns for training
had occurred. The system performed compara-
</bodyText>
<footnote confidence="0.9051616">
* This work was supported by UK EPSRC projects
GR/L53175 `PSET: Practical Simplification of English
Text&apos; and GR/N36462/93 &apos;Robust Accurate Statistical
Parsing (RASP)&apos;.
1We will hereafter refer to WordNet synsets as classes.
</footnote>
<bodyText confidence="0.998874675">
bly to the other system using selectional prefer-
ences alone.
The work here is an extension of this earlier
work, this time applied to the English all words
task. We use probability distributions rather
than mutual information to quantify the prefer-
ences. The preference models are modifications
of the Tree Cut Models (Tcms) originally pro-
posed by Li and Abe (1995; 1998). A TCM is a
set of classes cutting across the WordNet noun
hypernym hierarchy which covers all the nouns
of WordNet disjointly, i.e. the classes in the set
are not hyponyms of one another. The set of
classes is associated with a probability distri-
bution. In our work, we acquire Tcms condi-
tioned on a verb class, rather than a verb form.
We then use Bayes rule to obtain probability
estimates for verb classes conditioned on co-
occurring noun classes.
Using selectional preferences alone for disam-
biguation enables us to investigate the situa-
tions when they are useful, as well as cases when
they are not. However, this means we loose out
in cases where preferences do not provide the
necessary information and other complemen-
tary information would help. Another disad-
vantage of using selectional preferences alone for
disambiguation is that the preferences only ap-
ply to the grammatical slots for which they have
been acquired. In addition, selectional prefer-
ences only help disambiguation for slots where
there is a strong enough tie between predicate
and argument. In this work, we use subject and
object relationships, since these appear to work
better than other relationships (Resnik, 1997;
McCarthy, 2001), and we use argument heads,
rather than the entire argument phrase.
Our basic system is restricted to using only
selectional information, and no other source of
disambiguating information. However, we ex-
</bodyText>
<page confidence="0.998475">
119
</page>
<bodyText confidence="0.999974416666667">
perimented with two methods of extending the
coverage to include other grammatical contexts.
The first of these methods is the &apos;one sense per
discourse&apos; heuristic (Gale et al., 1992). With
this method a sense tag for a given word is
applied to other occurrences of the same word
within the discourse. The second method uses
anaphora resolution to link pronouns to their
antecedents. Using the anaphoric links we
are able to use the preferences for a verb co-
occurring with a pronoun with the antecedent
of that pronoun.
</bodyText>
<sectionHeader confidence="0.953037" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.999985">
There is a training phrase and a run-time dis-
ambiguation phase for our system. In the train-
ing phase a preprocessor and parser are used
to obtain training data for selectional prefer-
ence acquisition. At run-time the preproces-
sor and parser are used for identifying predi-
cates and argument heads for application of the
acquired selectional preferences for disambigua-
tion. Anaphora resolution is used at run-time to
make links between antecedents of nouns, where
the antecedents or the predicates may be in sub-
ject or object relationships.
</bodyText>
<subsectionHeader confidence="0.959001">
2.1 Preprocessor and Parser
</subsectionHeader>
<bodyText confidence="0.999031411764706">
The preprocessor consists of three modules ap-
plied in sequence: a tokeniser, a part-of-speech
(PoS) tagger, and a lemmatiser. The tokeniser
comprises a small set of manually-developed
finite-state rules for identifying word and sen-
tence boundaries. The tagger (Elworthy, 1994)
uses a bigram HMM augmented with a statisti-
cal unknown word guesser. When applied to the
training data for selectional preference acquisi-
tion it produces the single highest-ranked tag
for each word; at run-time it returns multiple
tags whose associated forward-backward proba-
bilities are incorporated into parse probabilities.
The lemmatiser (IVIinnen et al., 2001) reduces
inflected verbs and nouns to their base forms.
The parser uses a &apos;shallow&apos; unification-based
grammar of English PoS tags, performs disam-
biguation using a context-sensitive probabilistic
model (Carroll and Briscoe, 1996), and recovers
from extra-grammaticality by returning partial
parses. The output of the parser is a set of
grammatical relations specifying the syntactic
dependency between each head and its depen-
dent(s), read off from the phrase structure tree
that is returned from the disambiguation phase.
For selectional preference acquisition we applied
the analysis system to the 90 million words of
the written portion of the British National Cor-
pus (BNC); both in the acquisition phase and at
run-time we extracted from the analyser output
only subject-verb and verb-direct object depen-
dencies2. Thus we did not use the SENSEVAL-
2 Penn Treebank-style bracketings supplied for
the test data.
</bodyText>
<subsectionHeader confidence="0.993424">
2.2 Selectional Preferences
</subsectionHeader>
<bodyText confidence="0.999946457142857">
A TCM provides a probability distribution over
the noun hyponym hierarchy of WordNet. We
acquire Tcms conditioned on WordNet verb
classes to represent the selectional preferences
of the verbs in that verb class. The noun fre-
quency data used for acquiring a TCM is that
occurring with verbs from the target verb class.
The verb members for training are taken from
the class directly and all hyponym classes. How-
ever not all verbs in a verb class are used for
training. We use verbs which have a frequency
at or above 20 in the BNC, and belong to no
more than 10 WordNet classes.
The noun data is used to populate the hy-
pernym hierarchy with frequencies, where the
frequency count for any noun is divided by the
number of noun classes it is a member of. A
hyperonym class includes the frequency credit
attributed to all its hyponyms.
A portion of two Tcms is shown in figure 1.
The TCMS are similar as they both contain di-
rect objects occurring with the verb seize; the
TCM for the class which includes clutch has a
higher probability for the entity noun class
compared to the class which also includes as-
sume and usurp. This example includes only
classes at WordNet roots, although it is quite
possible for the TCM to use more specific noun
classes. The method for determining the gen-
eralisation level uses the minimum description
length principle and is a modification of that
proposed by Li and Abe (1995; 1998). In
our modification, all internal nodes of WordNet
have their synonyms placed at newly created
leaves. Doing this ensures that all nouns are
</bodyText>
<footnote confidence="0.70802125">
2In a previous evaluation of grammatical relation ac-
curacy, the analyser returned subject-verb and verb-
direct object dependencies with 84-88% recall and pre-
cision (Carroll et al., 1999).
</footnote>
<page confidence="0.982745">
120
</page>
<figureCaption confidence="0.99964">
Figure 1: Tcms for the direct object slot of two verb classes which include the verb seize.
</figureCaption>
<figure confidence="0.999261117647059">
I.
money
monarchy
control
throne
0.53
M.14 0.0
0.07 0.02
possession
0.26 0.0.1 TCM for seize assume usurp
0.01 TCM for seize clutch
party
straw
handle
0.06
0.05
group
</figure>
<bodyText confidence="0.896372">
covered by the probability distribution specified
by the TCM.
</bodyText>
<subsectionHeader confidence="0.971829">
2.3 Disambiguation
</subsectionHeader>
<bodyText confidence="0.999992826086957">
The probability distributions enable us to get
estimates for p(noun classIverb class) for dis-
ambiguation. To disambiguate a noun occur-
ring with a given verb, the noun class (n1) out
of all those to which the noun belongs that
gives the largest estimate for p(n1iv1) is taken,
where the verb class (v1) is the one for the co-
occurring verb which maximises this estimate.
The selectional preferences provide an estimate
for p(nlIv1). The probability estimate of the
hyperonym noun class (n2) occurring above n1
on the TCM for v1 is multiplied by the ratio of
the prior probability estimate for the hyponym
divided by that for the hyperonym on the TCM,
i.e. byP(&apos;1) These prior estimates are taken
from populating the noun hypernym hierarchy
with the prior frequency data.
To disambiguate a verb occurring with a
given noun, the verbclass (v2) which gives the
largest estimate for p(v2I n3) is taken. The noun
class (n3) for the co-occurring noun is taken as
the one that maximises this estimate. Bayes
rule is used to obtain this estimate:
</bodyText>
<equation confidence="0.9985085">
P( t&apos;2In3) = p(n31v2)
P(713)
</equation>
<bodyText confidence="0.999974625">
The Tcms for the candidate verb classes are
used for the estimate of p(n3I v2). The estimate
for p(n3) is taken from a frequency distribution
stored over the entire noun hyponym hierarchy
for the prior noun data for the target grammat-
ical slot. The estimate p(v2) is taken from a
frequency distribution over the entire verb hy-
ponym hierarchy for the given grammatical slot.
</bodyText>
<subsectionHeader confidence="0.9600545">
2.4 Increasing Coverage — OSPD and
anaphora resolution
</subsectionHeader>
<bodyText confidence="0.999941481481481">
When applying the one sense per discourse
(OSPD) heuristic, we simply used a tag for a
noun, or verb to apply to all the other nouns (or
verbs) in the discourse, provided that there was
not more than one possible tagging provided by
the selectional preferences for that discourse.
In order to increase coverage of the selectional
preferences we used anaphoric links to allow
preferences of verbs occurring with pronouns to
apply to antecedents.
The anaphora resolution algorithm imple-
mented is due to Kennedy and Boguraev (1996).
The algorithm resolves third person pronouns,
reciprocals and reflexives, and its cited accuracy
is 75% when evaluated on various texts taken
from the World Wide Web.
The algorithm places each discourse referent
into a coreference class, where discourse refer-
ents in the same class are believed to refer to the
same object. The classes have a salience value
associated with them, and an antecedent for a
pronoun is chosen from the class with the high-
est salience value. The salience value of a class
is computed by assigning weights to the gram-
matical features of its discourse referents, and
these grammatical features are obtained from
the Briscoe and Carroll (1996) parser.
</bodyText>
<sectionHeader confidence="0.996673" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.9932655">
We entered three systems for the SENSEVAL-2
English all words task:
sussex-se! Selectional preferences were used
alone. Preferences at the subject slot were
applied first, if these were not applicable
then the direct object slot was tried.
</bodyText>
<equation confidence="0.680181">
p(v2)
</equation>
<page confidence="0.974097">
121
</page>
<table confidence="0.9996134">
System Precision Recall Attempted
(Sussex-) (%) (%) (%)
sel 59.8 14.0 23
sel-ospd 56.6 16.9 30
sel-ospd-ana 54.5 16.9 31
</table>
<tableCaption confidence="0.998519">
Table 1: English all words fine-grained results
</tableCaption>
<table confidence="0.99120475">
Slot Nouns (%) Verbs (%)
subject 34 36
direct object 28 45
random baseline 24 25
</table>
<tableCaption confidence="0.7907015">
Table 2: Analysis of sussex-sel precision for pol-
ysemous nouns and verbs
</tableCaption>
<bodyText confidence="0.999969235294118">
sussex-sel-ospd The selectional preferences
were applied first, followed by the one sense
per discourse heuristic. In the English all
words task a discourse was demarcated by
a unique text identifier.
sussex-sel-ospd-ana The selectional prefer-
ences were used, then the anaphoric links
were applied to extend coverage, and finally
the one sense per discourse was applied.
The results are shown in table 1. We only
attempted disambiguation for head nouns and
verbs in subject and direct object relation-
ships, those tagged using anaphoric links to
antecedents in these relationships and those
tagged using the one sense per discourse heuris-
tic. We do not include the coarse-grained re-
sults which are just slightly better than the fine-
grained results, and this seems to be typical of
other systems. We did not take advantage of
the coarse grained classification as this was not
available at the time of acquiring the selectional
preferences.
From analysis of the fine-grained results of
the selectional preference results for system
sussex-sel, we see that nouns performed better
than verbs because there were more monose-
mous nouns than verbs. However, if we re-
move the monosemous cases, and rely on the
preferences, the verbs were disambiguated more
accurately than the nouns, having only a 1%
higher random baseline. Also, the direct object
slot outperformed the subject slot. In future it
would be better to use the preferences from this
slot first.
</bodyText>
<sectionHeader confidence="0.998421" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999982">
Given that this method is unsupervised, we feel
our results are promising. The one sense per dis-
course heuristic works well and increases cover-
age. However, we feel that anaphora resolution
information has not reached its full potential.
There is plenty of scope for combining evidence
from several anaphoric links, especially once we
have covered more grammatical relationships.
We hope that precision can also be improved
by combining or comparing several pieces of evi-
dence for a single test item. We are currently ac-
quiring preferences for adjective-noun relation-
ships.
</bodyText>
<sectionHeader confidence="0.997922" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999895348837209">
John Carroll and Ted Briscoe. 1996. Apportioning de-
velopment effort in a probabilistic LR parsing system
through evaluation. In ACL/SIGDAT Conference on
Empirical Methods in Natural Language Processing,
pages 92-100, University of Pennsylvania, PA.
John Carroll and Diana McCarthy. 2000. Word sense
disambiguation using automatically acquired verbal
preferences. Computers and the Humanities. Senseval
Special Issue, 34(1-2):109-114.
John Carroll, Guido Minnen, and Ted Briscoe. 1999.
Corpus annotation for parser evaluation. In EACL-
99 Workshop on Linguistically Interpreted Corpora,
pages 35-41, Bergen, Norway.
David Elworthy. 1994. Does Baum-Welch re-estimation
help taggers? In 4th ACL Conference on Applied
Natural Language Processing, pages 53-58, Stuttgart,
Germany.
William Gale, Kenneth Church, arid David Yarowsky.
1992. A method for disambiguating word senses in a
large corpus. Computers and the Humanities, 26:415-
439.
Chris Kennedy and Bran Boguraev. 1996. Anaphora for
everyone: Pronominal anaphora resolution without a
parser. In 16th International Conference of Compu-
tational Linguistics, COLING-96, pages 113-118.
Hang Li and Naoki Abe. 1995. Generalizing case frames
using a thesaurus and the MDL principle. In Inter-
national Conference on Recent Advances in Natural
Language Processing, pages 239-248, Bulgaria.
Hang Li and Naoki Abe. 1998. Generalizing case frames
using a thesaurus and the MDL principle. Computa-
tional Linguistics, 24(2):217-244.
Diana McCarthy. 2001. Lexical acquisition at the
syntax-semantics interface: diathesis alternations,
subcategorization frames and selectional preferences.
Ph.D. thesis, University of Sussex.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Natural
Language Engineering, 7(3):207-223.
Philip Resnik. 1997. Selectional preference and sense
disambiguation. In SIGLEX Workshop on Tagging Text
with Lexical Semantics: Why What and How?, pages
52-57, Washington, DC.
</reference>
<page confidence="0.997486">
122
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.342979">
<title confidence="0.990336">Disambiguating Noun and Verb Senses Using Automatically Acquired Selectional Preferences*</title>
<author confidence="0.990886">Diana McCarthy</author>
<author confidence="0.990886">John</author>
<affiliation confidence="0.998673">University of</affiliation>
<address confidence="0.791937">Brighton BN1 9QH,</address>
<email confidence="0.998541">fdianam,johncalOcogs.susx.ac.uk</email>
<author confidence="0.505434">Judita</author>
<affiliation confidence="0.948043">Computer of Cambridge,</affiliation>
<address confidence="0.976824">Cambridge CB3 OFD,</address>
<email confidence="0.965991">Judita.PreissOcl.cam.ac.uk</email>
<abstract confidence="0.999760473684211">for the words task uses automatically acquired selectional preferences to sense tag subject and object head nouns, along with the associated verbal predicates. The selectional preferences comprise probability distributions over WordNet nouns, and these distributions are conditioned on WordNet verb classes. The conditional distributions are used directly to disambiguate the head nouns. We use prior distributions and Bayes rule to compute the highest probability verb class, given a noun class. We also use anaphora resolution and the &apos;one sense per discourse&apos; heuristic to cover nouns and verbs not occurring in these relationships in the target text. The selectional preferences are acquired without recourse to sense tagged data so our system is unsupervised.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Ted Briscoe</author>
</authors>
<title>Apportioning development effort in a probabilistic LR parsing system through evaluation.</title>
<date>1996</date>
<booktitle>In ACL/SIGDAT Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>92--100</pages>
<location>University of Pennsylvania, PA.</location>
<contexts>
<context position="5739" citStr="Carroll and Briscoe, 1996" startWordPosition="880" endWordPosition="883">ies. The tagger (Elworthy, 1994) uses a bigram HMM augmented with a statistical unknown word guesser. When applied to the training data for selectional preference acquisition it produces the single highest-ranked tag for each word; at run-time it returns multiple tags whose associated forward-backward probabilities are incorporated into parse probabilities. The lemmatiser (IVIinnen et al., 2001) reduces inflected verbs and nouns to their base forms. The parser uses a &apos;shallow&apos; unification-based grammar of English PoS tags, performs disambiguation using a context-sensitive probabilistic model (Carroll and Briscoe, 1996), and recovers from extra-grammaticality by returning partial parses. The output of the parser is a set of grammatical relations specifying the syntactic dependency between each head and its dependent(s), read off from the phrase structure tree that is returned from the disambiguation phase. For selectional preference acquisition we applied the analysis system to the 90 million words of the written portion of the British National Corpus (BNC); both in the acquisition phase and at run-time we extracted from the analyser output only subject-verb and verb-direct object dependencies2. Thus we did </context>
</contexts>
<marker>Carroll, Briscoe, 1996</marker>
<rawString>John Carroll and Ted Briscoe. 1996. Apportioning development effort in a probabilistic LR parsing system through evaluation. In ACL/SIGDAT Conference on Empirical Methods in Natural Language Processing, pages 92-100, University of Pennsylvania, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Diana McCarthy</author>
</authors>
<title>Word sense disambiguation using automatically acquired verbal preferences. Computers and the Humanities. Senseval Special Issue,</title>
<date>2000</date>
<pages>34--1</pages>
<contexts>
<context position="1349" citStr="Carroll and McCarthy, 2000" startWordPosition="187" endWordPosition="190">ons are used directly to disambiguate the head nouns. We use prior distributions and Bayes rule to compute the highest probability verb class, given a noun class. We also use anaphora resolution and the &apos;one sense per discourse&apos; heuristic to cover nouns and verbs not occurring in these relationships in the target text. The selectional preferences are acquired without recourse to sense tagged data so our system is unsupervised. 1 Introduction In the first SENSEVAL, we used automatically acquired selectional preferences to disambiguate head nouns occurring in specific grammatical relationships (Carroll and McCarthy, 2000). The selectional preference models provided co-occurrence behaviour between WordNet synsetsl in the noun hyponym hierarchy and verbal predicates. Preference scores, based on mutual information, were attached to the classes in the models. These scores were conditioned on the verbal context and the grammatical relationship in which the nouns for training had occurred. The system performed compara* This work was supported by UK EPSRC projects GR/L53175 `PSET: Practical Simplification of English Text&apos; and GR/N36462/93 &apos;Robust Accurate Statistical Parsing (RASP)&apos;. 1We will hereafter refer to WordN</context>
</contexts>
<marker>Carroll, McCarthy, 2000</marker>
<rawString>John Carroll and Diana McCarthy. 2000. Word sense disambiguation using automatically acquired verbal preferences. Computers and the Humanities. Senseval Special Issue, 34(1-2):109-114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Guido Minnen</author>
<author>Ted Briscoe</author>
</authors>
<title>Corpus annotation for parser evaluation.</title>
<date>1999</date>
<booktitle>In EACL99 Workshop on Linguistically Interpreted Corpora,</booktitle>
<pages>35--41</pages>
<location>Bergen,</location>
<contexts>
<context position="8189" citStr="Carroll et al., 1999" startWordPosition="1292" endWordPosition="1295">This example includes only classes at WordNet roots, although it is quite possible for the TCM to use more specific noun classes. The method for determining the generalisation level uses the minimum description length principle and is a modification of that proposed by Li and Abe (1995; 1998). In our modification, all internal nodes of WordNet have their synonyms placed at newly created leaves. Doing this ensures that all nouns are 2In a previous evaluation of grammatical relation accuracy, the analyser returned subject-verb and verbdirect object dependencies with 84-88% recall and precision (Carroll et al., 1999). 120 Figure 1: Tcms for the direct object slot of two verb classes which include the verb seize. I. money monarchy control throne 0.53 M.14 0.0 0.07 0.02 possession 0.26 0.0.1 TCM for seize assume usurp 0.01 TCM for seize clutch party straw handle 0.06 0.05 group covered by the probability distribution specified by the TCM. 2.3 Disambiguation The probability distributions enable us to get estimates for p(noun classIverb class) for disambiguation. To disambiguate a noun occurring with a given verb, the noun class (n1) out of all those to which the noun belongs that gives the largest estimate f</context>
</contexts>
<marker>Carroll, Minnen, Briscoe, 1999</marker>
<rawString>John Carroll, Guido Minnen, and Ted Briscoe. 1999. Corpus annotation for parser evaluation. In EACL99 Workshop on Linguistically Interpreted Corpora, pages 35-41, Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Elworthy</author>
</authors>
<title>Does Baum-Welch re-estimation help taggers?</title>
<date>1994</date>
<booktitle>In 4th ACL Conference on Applied Natural Language Processing,</booktitle>
<pages>53--58</pages>
<location>Stuttgart, Germany.</location>
<contexts>
<context position="5145" citStr="Elworthy, 1994" startWordPosition="796" endWordPosition="797">r and parser are used for identifying predicates and argument heads for application of the acquired selectional preferences for disambiguation. Anaphora resolution is used at run-time to make links between antecedents of nouns, where the antecedents or the predicates may be in subject or object relationships. 2.1 Preprocessor and Parser The preprocessor consists of three modules applied in sequence: a tokeniser, a part-of-speech (PoS) tagger, and a lemmatiser. The tokeniser comprises a small set of manually-developed finite-state rules for identifying word and sentence boundaries. The tagger (Elworthy, 1994) uses a bigram HMM augmented with a statistical unknown word guesser. When applied to the training data for selectional preference acquisition it produces the single highest-ranked tag for each word; at run-time it returns multiple tags whose associated forward-backward probabilities are incorporated into parse probabilities. The lemmatiser (IVIinnen et al., 2001) reduces inflected verbs and nouns to their base forms. The parser uses a &apos;shallow&apos; unification-based grammar of English PoS tags, performs disambiguation using a context-sensitive probabilistic model (Carroll and Briscoe, 1996), and </context>
</contexts>
<marker>Elworthy, 1994</marker>
<rawString>David Elworthy. 1994. Does Baum-Welch re-estimation help taggers? In 4th ACL Conference on Applied Natural Language Processing, pages 53-58, Stuttgart, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Gale</author>
<author>Kenneth Church</author>
<author>arid David Yarowsky</author>
</authors>
<title>A method for disambiguating word senses in a large corpus. Computers and the Humanities,</title>
<date>1992</date>
<pages>26--415</pages>
<contexts>
<context position="3945" citStr="Gale et al., 1992" startWordPosition="601" endWordPosition="604">uation for slots where there is a strong enough tie between predicate and argument. In this work, we use subject and object relationships, since these appear to work better than other relationships (Resnik, 1997; McCarthy, 2001), and we use argument heads, rather than the entire argument phrase. Our basic system is restricted to using only selectional information, and no other source of disambiguating information. However, we ex119 perimented with two methods of extending the coverage to include other grammatical contexts. The first of these methods is the &apos;one sense per discourse&apos; heuristic (Gale et al., 1992). With this method a sense tag for a given word is applied to other occurrences of the same word within the discourse. The second method uses anaphora resolution to link pronouns to their antecedents. Using the anaphoric links we are able to use the preferences for a verb cooccurring with a pronoun with the antecedent of that pronoun. 2 System Description There is a training phrase and a run-time disambiguation phase for our system. In the training phase a preprocessor and parser are used to obtain training data for selectional preference acquisition. At run-time the preprocessor and parser ar</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>William Gale, Kenneth Church, arid David Yarowsky. 1992. A method for disambiguating word senses in a large corpus. Computers and the Humanities, 26:415-439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Kennedy</author>
<author>Bran Boguraev</author>
</authors>
<title>Anaphora for everyone: Pronominal anaphora resolution without a parser.</title>
<date>1996</date>
<booktitle>In 16th International Conference of Computational Linguistics, COLING-96,</booktitle>
<pages>113--118</pages>
<contexts>
<context position="10558" citStr="Kennedy and Boguraev (1996)" startWordPosition="1690" endWordPosition="1693">m hierarchy for the given grammatical slot. 2.4 Increasing Coverage — OSPD and anaphora resolution When applying the one sense per discourse (OSPD) heuristic, we simply used a tag for a noun, or verb to apply to all the other nouns (or verbs) in the discourse, provided that there was not more than one possible tagging provided by the selectional preferences for that discourse. In order to increase coverage of the selectional preferences we used anaphoric links to allow preferences of verbs occurring with pronouns to apply to antecedents. The anaphora resolution algorithm implemented is due to Kennedy and Boguraev (1996). The algorithm resolves third person pronouns, reciprocals and reflexives, and its cited accuracy is 75% when evaluated on various texts taken from the World Wide Web. The algorithm places each discourse referent into a coreference class, where discourse referents in the same class are believed to refer to the same object. The classes have a salience value associated with them, and an antecedent for a pronoun is chosen from the class with the highest salience value. The salience value of a class is computed by assigning weights to the grammatical features of its discourse referents, and these</context>
</contexts>
<marker>Kennedy, Boguraev, 1996</marker>
<rawString>Chris Kennedy and Bran Boguraev. 1996. Anaphora for everyone: Pronominal anaphora resolution without a parser. In 16th International Conference of Computational Linguistics, COLING-96, pages 113-118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
<author>Naoki Abe</author>
</authors>
<title>Generalizing case frames using a thesaurus and the MDL principle.</title>
<date>1995</date>
<booktitle>In International Conference on Recent Advances in Natural Language Processing,</booktitle>
<pages>239--248</pages>
<contexts>
<context position="2336" citStr="Li and Abe (1995" startWordPosition="340" endWordPosition="343">he system performed compara* This work was supported by UK EPSRC projects GR/L53175 `PSET: Practical Simplification of English Text&apos; and GR/N36462/93 &apos;Robust Accurate Statistical Parsing (RASP)&apos;. 1We will hereafter refer to WordNet synsets as classes. bly to the other system using selectional preferences alone. The work here is an extension of this earlier work, this time applied to the English all words task. We use probability distributions rather than mutual information to quantify the preferences. The preference models are modifications of the Tree Cut Models (Tcms) originally proposed by Li and Abe (1995; 1998). A TCM is a set of classes cutting across the WordNet noun hypernym hierarchy which covers all the nouns of WordNet disjointly, i.e. the classes in the set are not hyponyms of one another. The set of classes is associated with a probability distribution. In our work, we acquire Tcms conditioned on a verb class, rather than a verb form. We then use Bayes rule to obtain probability estimates for verb classes conditioned on cooccurring noun classes. Using selectional preferences alone for disambiguation enables us to investigate the situations when they are useful, as well as cases when t</context>
<context position="7854" citStr="Li and Abe (1995" startWordPosition="1240" endWordPosition="1243">ncy credit attributed to all its hyponyms. A portion of two Tcms is shown in figure 1. The TCMS are similar as they both contain direct objects occurring with the verb seize; the TCM for the class which includes clutch has a higher probability for the entity noun class compared to the class which also includes assume and usurp. This example includes only classes at WordNet roots, although it is quite possible for the TCM to use more specific noun classes. The method for determining the generalisation level uses the minimum description length principle and is a modification of that proposed by Li and Abe (1995; 1998). In our modification, all internal nodes of WordNet have their synonyms placed at newly created leaves. Doing this ensures that all nouns are 2In a previous evaluation of grammatical relation accuracy, the analyser returned subject-verb and verbdirect object dependencies with 84-88% recall and precision (Carroll et al., 1999). 120 Figure 1: Tcms for the direct object slot of two verb classes which include the verb seize. I. money monarchy control throne 0.53 M.14 0.0 0.07 0.02 possession 0.26 0.0.1 TCM for seize assume usurp 0.01 TCM for seize clutch party straw handle 0.06 0.05 group </context>
</contexts>
<marker>Li, Abe, 1995</marker>
<rawString>Hang Li and Naoki Abe. 1995. Generalizing case frames using a thesaurus and the MDL principle. In International Conference on Recent Advances in Natural Language Processing, pages 239-248, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
<author>Naoki Abe</author>
</authors>
<title>Generalizing case frames using a thesaurus and the MDL principle.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--2</pages>
<marker>Li, Abe, 1998</marker>
<rawString>Hang Li and Naoki Abe. 1998. Generalizing case frames using a thesaurus and the MDL principle. Computational Linguistics, 24(2):217-244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
</authors>
<title>Lexical acquisition at the syntax-semantics interface: diathesis alternations, subcategorization frames and selectional preferences.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Sussex.</institution>
<contexts>
<context position="3555" citStr="McCarthy, 2001" startWordPosition="542" endWordPosition="543">are not. However, this means we loose out in cases where preferences do not provide the necessary information and other complementary information would help. Another disadvantage of using selectional preferences alone for disambiguation is that the preferences only apply to the grammatical slots for which they have been acquired. In addition, selectional preferences only help disambiguation for slots where there is a strong enough tie between predicate and argument. In this work, we use subject and object relationships, since these appear to work better than other relationships (Resnik, 1997; McCarthy, 2001), and we use argument heads, rather than the entire argument phrase. Our basic system is restricted to using only selectional information, and no other source of disambiguating information. However, we ex119 perimented with two methods of extending the coverage to include other grammatical contexts. The first of these methods is the &apos;one sense per discourse&apos; heuristic (Gale et al., 1992). With this method a sense tag for a given word is applied to other occurrences of the same word within the discourse. The second method uses anaphora resolution to link pronouns to their antecedents. Using the</context>
</contexts>
<marker>McCarthy, 2001</marker>
<rawString>Diana McCarthy. 2001. Lexical acquisition at the syntax-semantics interface: diathesis alternations, subcategorization frames and selectional preferences. Ph.D. thesis, University of Sussex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>John Carroll</author>
<author>Darren Pearce</author>
</authors>
<title>Applied morphological processing of English.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<pages>7--3</pages>
<marker>Minnen, Carroll, Pearce, 2001</marker>
<rawString>Guido Minnen, John Carroll, and Darren Pearce. 2001. Applied morphological processing of English. Natural Language Engineering, 7(3):207-223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selectional preference and sense disambiguation.</title>
<date>1997</date>
<booktitle>In SIGLEX Workshop on Tagging Text with Lexical Semantics: Why What and How?,</booktitle>
<pages>52--57</pages>
<location>Washington, DC.</location>
<contexts>
<context position="3538" citStr="Resnik, 1997" startWordPosition="540" endWordPosition="541">ses when they are not. However, this means we loose out in cases where preferences do not provide the necessary information and other complementary information would help. Another disadvantage of using selectional preferences alone for disambiguation is that the preferences only apply to the grammatical slots for which they have been acquired. In addition, selectional preferences only help disambiguation for slots where there is a strong enough tie between predicate and argument. In this work, we use subject and object relationships, since these appear to work better than other relationships (Resnik, 1997; McCarthy, 2001), and we use argument heads, rather than the entire argument phrase. Our basic system is restricted to using only selectional information, and no other source of disambiguating information. However, we ex119 perimented with two methods of extending the coverage to include other grammatical contexts. The first of these methods is the &apos;one sense per discourse&apos; heuristic (Gale et al., 1992). With this method a sense tag for a given word is applied to other occurrences of the same word within the discourse. The second method uses anaphora resolution to link pronouns to their antec</context>
</contexts>
<marker>Resnik, 1997</marker>
<rawString>Philip Resnik. 1997. Selectional preference and sense disambiguation. In SIGLEX Workshop on Tagging Text with Lexical Semantics: Why What and How?, pages 52-57, Washington, DC.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>