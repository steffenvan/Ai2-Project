<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001676">
<title confidence="0.992172">
Unsupervised word segmentation for Sesotho using Adaptor Grammars
</title>
<author confidence="0.991822">
Mark Johnson
</author>
<affiliation confidence="0.971659">
Brown University
</affiliation>
<email confidence="0.968841">
Mark Johnson@Brown.edu
</email>
<sectionHeader confidence="0.994557" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994756">
This paper describes a variety of non-
parametric Bayesian models of word segmen-
tation based on Adaptor Grammars that model
different aspects of the input and incorporate
different kinds of prior knowledge, and ap-
plies them to the Bantu language Sesotho.
While we find overall word segmentation ac-
curacies lower than these models achieve on
English, we also find some interesting dif-
ferences in which factors contribute to better
word segmentation. Specifically, we found lit-
tle improvement to word segmentation accu-
racy when we modeled contextual dependen-
cies, while modeling morphological structure
did improve segmentation accuracy.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999864176470588">
A Bayesian approach to learning (Bishop, 2006) is
especially useful for computational models of lan-
guage acquisition because we can use it to study
the effect of different kinds and amounts of prior
knowledge on the learning process. The Bayesian
approach is agnostic as to what this prior knowl-
edge might consist of; the prior could encode the
kinds of rich universal grammar hypothesised by
e.g., Chomsky (1986), or it could express a vague
non-linguistic preference for simpler as opposed to
more complex models, as in some of the grammars
discussed below. Clearly there’s a wide range of
possible priors, and one of the exciting possibilities
raised by Bayesian methods is that we may soon be
able to empirically evaluate the potential contribu-
tion of different kinds of prior knowledge to lan-
guage learning.
</bodyText>
<page confidence="0.917589">
20
</page>
<bodyText confidence="0.998127882352941">
The Bayesian framework is surprisingly flexible.
The bulk of the work on Bayesian inference is on
parametric models, where the goal is to learn the
value of a set of parameters (much as in Chomsky’s
Principles and Parameters conception of learning).
However, recently Bayesian methods for nonpara-
metric inference have been developed, in which the
parameters themselves, as well as their values, are
learned from data. (The term “nonparametric” is
perhaps misleading here: it does not mean that the
models have no parameters, rather it means that the
learning process considers models with different sets
of parameters). One can think of the prior as pro-
viding an infinite set of possible parameters, from
which a learner selects a subset with which to model
their language.
If one pairs each of these infinitely-many pa-
rameters with possible structures (or equivalently,
rules that generate such structures) then these non-
parametric Bayesian learning methods can learn
the structures relevant to a language. Determining
whether methods such as these can in fact learn lin-
guistic structure bears on the nature vs. nurture de-
bates in language acquisition, since one of the argu-
ments for the nativist position is that there doesn’t
seem to be a way to learn structure from the input
that children receive.
While there’s no reason why these methods can’t
be used to learn the syntax and semantics of human
languages, much of the work to date has focused on
lower-level learning problems such as morphologi-
cal structure learning (Goldwater et al., 2006b) and
word segmentation, where the learner is given un-
segmented broad-phonemic utterance transcriptions
</bodyText>
<note confidence="0.8007895">
Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, pages 20–27,
Columbus, Ohio, USA June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999960772727273">
and has to identify the word boundaries (Goldwater
et al., 2006a; Goldwater et al., 2007). One reason for
this is that these problems seem simpler than learn-
ing syntax, where the non-linguistic context plausi-
bly supplies important information to human learn-
ers. Virtually everyone agrees that the set of possible
morphemes and words, if not infinite, is astronom-
ically large, so it seems plausible that humans use
some kind of nonparametric procedure to learn the
lexicon.
Johnson et al. (2007) introduced Adaptor Gram-
mars as a framework in which a wide variety
of linguistically-interesting nonparametric inference
problems can be formulated and evaluated, includ-
ing a number of variants of the models described by
Goldwater (2007). Johnson (2008) presented a vari-
ety of different adaptor grammar word segmentation
models and applied them to the problem of segment-
ing Brent’s phonemicized version of the Bernstein-
Ratner corpus of child-directed English (Bernstein-
Ratner, 1987; Brent, 1999). The main results of that
paper were the following:
</bodyText>
<listItem confidence="0.992360727272727">
1. it confirmed the importance of modeling con-
textual dependencies above the word level for
word segmentation (Goldwater et al., 2006a),
2. it showed a small but significant improvement
to segmentation accuracy by learning the possi-
ble syllable structures of the language together
with the lexicon, and
3. it found no significant advantage to learning
morphological structure together with the lex-
icon (indeed, that model confused morphologi-
cal and lexical structure).
</listItem>
<bodyText confidence="0.99964584375">
Of course the last result is a null result, and it’s pos-
sible that a different model would be able to usefuly
combine morphological learning with word segmen-
tation.
This paper continues that research by applying
the same kinds of models to Sesotho, a Bantu lan-
guage spoken in Southern Africa. Bantu languages
are especially interesting for this kind of study, as
they have rich productive agglutinative morpholo-
gies and relatively transparent phonologies, as com-
pared to languages such as Finnish or Turkish which
have complex harmony processes and other phono-
logical complexities. The relative clarity of Bantu
has inspired previous computational work, such as
the algorithm for learning Swahili morphology by
Hu et al. (2005). The Hu et al. algorithm uses
a Minimum Description Length procedure (Rissa-
nen, 1989) that is conceptually related to the non-
parametric Bayesian procedure used here. However,
the work here is focused on determining whether the
word segmentation methods that work well for En-
glish generalize to Sesotho and whether modeling
morphological and/or syllable structure improves
Sesotho word segmentation, rather than learning
Sesotho morphological structure per se.
The rest of this paper is structured as follows.
Section 2 informally reviews adaptor grammars and
describes how they are used to specify different
Bayesian models. Section 3 describes the Sesotho
corpus we used and the specific adaptor grammars
we used for word segmentation, and section 5 sum-
marizes and concludes the paper.
</bodyText>
<sectionHeader confidence="0.932816" genericHeader="method">
2 Adaptor grammars
</sectionHeader>
<bodyText confidence="0.999866615384615">
One reason why Probabilistic Context-Free Gram-
mars (PCFGs) are interesting is because they are
very simple and natural models of hierarchical struc-
ture. They are parametric models because each
PCFG has a fixed number of rules, each of which
has a numerical parameter associated with it. One
way to construct nonparametric Bayesian models is
to take a parametric model class and let one or more
of their components grow unboundedly.
There are two obvious ways to construct nonpara-
metric models from PCFGs. First, we can let the
number of nonterminals grow unboundedly, as in the
Infinite PCFG, where the nonterminals of the gram-
mar can be indefinitely refined versions of a base
PCFG (Liang et al., 2007). Second, we can fix the
set of nonterminals but permit the number of rules
or productions to grow unboundedly, which leads to
Adaptor Grammars (Johnson et al., 2007).
At any point in learning, an Adaptor Grammar has
a finite set of rules, but these can grow unbound-
edly (typically logarithmically) with the size of the
training data. In a word-segmentation application
these rules typically generate words or morphemes,
so the learner is effectively learning the morphemes
and words of its language.
The new rules learnt by an Adaptor Grammar are
</bodyText>
<page confidence="0.998464">
21
</page>
<bodyText confidence="0.992106078947368">
compositions of old ones (that can themselves be
compositions of other rules), so it’s natural to think
of these new rules as tree fragments, where each
entire fragment is associated with its own proba-
bility. Viewed this way, an adaptor grammar can
be viewed as learning the tree fragments or con-
structions involved in a language, much as in Bod
(1998). For computational reasons adaptor gram-
mars require these fragments to consist of subtrees
(i.e., their yields are terminals).
We now provide an informal description of Adap-
tor Grammars (for a more formal description see
Johnson et al. (2007)). An adaptor grammar con-
sists of terminals V , nonterminals N (including a
start symbol 5), initial rules R and rule probabilities
p, just as in a PCFG. In addition, it also has a vec-
tor of concentration parameters α, where αA &gt; 0 is
called the (Dirichlet) concentration parameter asso-
ciated with nonterminal A.
The nonterminals A for which αA &gt; 0 are
adapted, which means that each subtree for A that
can be generated using the initial rules R is consid-
ered as a potential rule in the adaptor grammar. If
αA = 0 then A is unadapted, which means it ex-
pands just as in an ordinary PCFG.
Adaptor grammars are so-called because they
adapt both the subtrees and their probabilities to the
corpus they are generating. Formally, they are Hi-
erarchical Dirichlet Processes that generate a distri-
bution over distributions over trees that can be de-
fined in terms of stick-breaking processes (Teh et al.,
2006). It’s probably easiest to understand them in
terms of their conditional or sampling distribution,
which is the probability of generating a new tree T
given the trees (Ti, ... , T,) that the adaptor gram-
mar has already generated.
An adaptor grammar can be viewed as generating
a tree top-down, just like a PCFG. Suppose we have
</bodyText>
<listItem confidence="0.851998333333333">
a node A to expand. If A is unadapted (i.e., αA = 0)
then A expands just as in a PCFG, i.e., we pick a
rule A —* Q E R with probability pA,O and recur-
sively expand Q. If A is adapted and has expanded
nA times before, then:
1. A expands to a subtree a with probability
nQ/(nA+αA), where nQ is the number of times
A has expanded to subtree a before, and
2. A expands to Q where A—* Q E R with prob-
</listItem>
<subsubsectionHeader confidence="0.3892">
ability αA pA,O/(nA + αA).
</subsubsectionHeader>
<bodyText confidence="0.999838413043479">
Thus an adapted nonterminal A expands to a previ-
ously expanded subtree a with probability propor-
tional to the number nQ of times it was used before,
and expands just as in a PCFG (i.e., using R) with
probability proportional to the concentration param-
eter αA. This parameter specifies how likely A is to
expand into a potentially new subtree; as nA and nQ
grow this becomes increasingly unlikely.
We used the publically available adaptor gram-
mar inference software described in Johnson et al.
(2007), which we modified slightly as described be-
low. The basic algorithm is a Metropolis-within-
Gibbs or Hybrid MCMC sampler (Robert and
Casella, 2004), which resamples the parse tree for
each sentence in the training data conditioned on the
parses for the other sentences. In order to produce
sample parses efficiently the algorithm constructs a
PCFG approximation to the adaptor grammar which
contains one rule for each adapted subtree a, and
uses a Metropolis accept/reject step to correct for the
difference between the true adaptor grammar dis-
tribution and the PCFG approximation. With the
datasets described below less than 0.1% of proposal
parses from this PCFG approximation are rejected,
so it is quite a good approximation to the adaptor
grammar distribution.
On the other hand, at convergence this algorithm
produces a sequence of samples from the posterior
distribution over adaptor grammars, and this poste-
rior distribution seems quite broad. For example,
at convergence with the most stable of our models,
each time a sentence’s parse is resampled there is
an approximately 25% chance of the parse chang-
ing. Perhaps this is not surprising given the com-
paratively small amount of training data and the fact
that the models only use fairly crude distributional
information.
As just described, adaptor grammars require the
user to specify a concentration parameter αA for
each adapted nonterminal A. It’s not obvious how
this should be done. Previous work has treated αA
as an adjustable parameter, usually tying all of the
αA to some shared value which is adjusted to opti-
mize task performance (say, word segmentation ac-
curacy). Clearly, this is undesirable.
Teh et al. (2006) describes how to learn the con-
</bodyText>
<page confidence="0.976998">
22
</page>
<bodyText confidence="0.99975075">
centration parameters α, and we modified their pro-
cedure for adaptor grammars. Specifically, we put
a vague Gamma(10, 0.1) prior on each αA, and af-
ter each iteration through the training data we per-
formed 100 Metropolis-Hastings resampling steps
for each αA from an increasingly narrow Gamma
proposal distribution. We found that the perfor-
mance of the models with automatically learned
concentration parameters α was generally as good
as the models where α was tuned by hand (although
admittedly we only tried three or four different val-
ues for α).
</bodyText>
<table confidence="0.9994732">
word f-score morpheme f-score
0.431 0.352
0.478 0.387
0.467 0.389
0.502 0.349
0.476 0.372
0.490 0.393
0.529 0.321
0.556 0.378
0.537 0.352
</table>
<tableCaption confidence="0.9801925">
Table 1: Summary of word and morpheme f-scores for
the different models discussed in this paper.
</tableCaption>
<bodyText confidence="0.6902841">
Model
word
colloc
colloc2
word − syll
colloc − syll
colloc2 − syll
word − morph
word − smorph
colloc − smorph
</bodyText>
<sectionHeader confidence="0.838573" genericHeader="method">
3 Models of Sesotho word segmentation
</sectionHeader>
<bodyText confidence="0.999889034482759">
We wanted to make our Sesotho corpus as similar
as possible to one used in previous work on word
segmentation. We extracted all of the non-child
utterances from the LI–LV files from the Sesotho
corpus of child speech (Demuth, 1992), and used
the Sesotho gloss as our gold-standard corpus (we
did not phonemicize them as Sesotho orthography
is very close to phonemic). This produced 8,503
utterances containing 21,037 word tokens, 30,200
morpheme tokens and 100,113 phonemes. By com-
parison, the Brent corpus contains 9,790 utterances,
33,399 word tokens and 95,809 phonemes. Thus
the Sesotho corpus contains approximately the same
number of utterances and phonemes as the Brent
corpus, but far fewer (and hence far longer) words.
This is not surprising as the Sesotho corpus involves
an older child and Sesotho, being an agglutinative
language, tends to have morphologically complex
words.
In the subsections that follow we describe a vari-
ety of adaptor grammar models for word segmenta-
tion. All of these models were given same Sesotho
data, which consisted of the Sesotho gold-standard
corpus described above with all word boundaries
(spaces) and morpheme boundaries (hyphens) re-
moved. We computed the f-score (geometric aver-
age of precision and recall) with which the models
recovered the words or the morphemes annotated in
the gold-standard corpus.
</bodyText>
<subsectionHeader confidence="0.997205">
3.1 Unigram grammar
</subsectionHeader>
<bodyText confidence="0.999978125">
We begin by describing an adaptor grammar that
simulates the unigram word segmentation model
proposed by Goldwater et al. (2006a). In this model
each utterance is generated as a sequence of words,
and each word is a sequence of phonemes. This
grammar contains three kinds of rules, including
rules that expand the nonterminal Phoneme to all of
the phonemes seen in the training data.
</bodyText>
<equation confidence="0.9386345">
Sentence —* Word+
Word —* Phoneme+
</equation>
<bodyText confidence="0.99945525">
Adapted non-terminals are indicated by underlin-
ing, so in the word grammar only the Word nonter-
minal is adapted. Our software doesn’t permit reg-
ular expressions in rules, so we expand all Kleene
stars in rules into right-recursive structures over new
unadapted nonterminals. Figure 1 shows a sample
parse tree generated by this grammar for the sen-
tence:
kae
where
“You took it from where?”
This sentence shows a typical inflected verb, with a
subject marker (glossed SM), an object marker (OM),
perfect tense marker (PERF) and mood marker (IN).
In order to keep the trees a managable size, we only
display the root node, leaf nodes and nodes labeled
with adapted nonterminals.
The word grammar has a word segmentation f-
score of 43%, which is considerably below the 56%
f-score the same grammar achieves on the Brent cor-
pus. This difference presumably reflects the fact that
Sesotho words are longer and more complex, and so
segmentation is a harder task.
We actually ran the adaptor grammar sampler for
</bodyText>
<table confidence="0.4398595">
u- e- nk- il- e
SM- OM- take- PERF- IN
</table>
<page confidence="0.927695">
23
</page>
<figure confidence="0.95318">
Sentence Sentence
Word Word
u e n k i l e k a e
</figure>
<figureCaption confidence="0.995514">
Figure 1: A sample (correct) parse tree generated by the
word adaptor grammar for a Sesotho utterance.
</figureCaption>
<bodyText confidence="0.999957736842105">
the word grammar four times (as we did for all gram-
mars discussed in this paper). Because the sampler
is non-deterministic, each run produced a different
series of sample segmentations. However, the av-
erage segmentation f-score seems to be very stable.
The accuracies of the final sample of the four runs
ranges between 42.8% and 43.7%. Similarly, one
can compute the average f-score over the last 100
samples for each run; the average f-score ranges be-
tween 42.6% and 43.7%. Thus while there may
be considerable uncertainty as to where the word
boundaries are in any given sentence (which is re-
flected in fact that the word boundaries are very
likely to change from sample to sample), the aver-
age accuracy of such boundaries seems very stable.
The final sample grammars contained the initial
rules R, together with between 1,772 and 1,827 ad-
ditional expansions for Word, corresponding to the
cached subtrees for the adapted Word nonterminal.
</bodyText>
<subsectionHeader confidence="0.999741">
3.2 Collocation grammar
</subsectionHeader>
<bodyText confidence="0.997213555555556">
Goldwater et al. (2006a) showed that incorporating a
bigram model of word-to-word dependencies signif-
icantly improves word segmentation accuracy in En-
glish. While it is not possible to formulate such a bi-
gram model as an adaptor grammar, Johnson (2008)
showed that a similar improvement can be achieved
in an adaptor grammar by explicitly modeling col-
locations or sequences of words. The colloc adaptor
grammar is:
</bodyText>
<equation confidence="0.945602666666667">
Sentence —* Colloc+
Colloc —* Word+
Word —* Phoneme+
</equation>
<bodyText confidence="0.9995888">
This grammar generates a Sentence as a sequence
of Colloc(ations), where each Colloc(ation) is a se-
quence of Words. Figure 2 shows a sample parse tree
generated by the colloc grammar. In terms of word
segmentation, this grammar performs much worse
</bodyText>
<figureCaption confidence="0.99317725">
Figure 2: A sample parse tree generated by the colloc
grammar. The substrings generated by Word in fact tend
to be morphemes and Colloc tend to be words, which is
how they are evaluated in Table 1.
</figureCaption>
<bodyText confidence="0.999002545454546">
than the word grammar, with an f-score of 27%.
In fact, it seems that the Word nonterminals typ-
ically expand to morphemes and the Colloc nonter-
minals typically expand to words. It makes sense
that for a language like Sesotho, when given a gram-
mar with a hierarchy of units, the learner would use
the lower-level units as morphemes and the higher-
level units as words. If we simply interpret the Word
trees as morphemes and the Colloc trees as words
we get a better word segmentation accuracy of 48%
f-score.
</bodyText>
<subsectionHeader confidence="0.999629">
3.3 Adding more levels
</subsectionHeader>
<bodyText confidence="0.999956571428571">
If two levels are better than one, perhaps three levels
would be better than two? More specifically, per-
haps adding another level of adaptation would per-
mit the model to capture the kind of interword con-
text dependencies that improved English word seg-
mentation. Our colloc2 adaptor grammar includes
the following rules:
</bodyText>
<equation confidence="0.997061">
Sentence —* Colloc+
Colloc —* Word+
Word —* Morph+
Morph —* Phoneme+
</equation>
<bodyText confidence="0.999792">
This grammar generates sequences of Words
grouped together in collocations, as in the previous
grammar, but each Word now consists of a sequence
of Morph(emes). Figure 3 shows a sample parse tree
generated by the colloc2 grammar.
Interestingly, word segmentation f-score is
46.7%, which is slightly lower than that obtained
by the simpler colloc grammar. Informally, it seems
that when given an extra level of structure the
colloc2 model uses it to describe structure internal
</bodyText>
<figure confidence="0.966745142857143">
Colloc
Word
e
Word
Colloc
Word
Colloc
Word Word
i l e k a
u e
n k
24
Sentence
Colloc
</figure>
<figureCaption confidence="0.9735185">
Figure 3: A sample parse tree generated by the colloc2
grammar.
</figureCaption>
<figure confidence="0.996696">
Sentence
Word
Syll
l e
</figure>
<figureCaption confidence="0.974032333333333">
Figure 4: A sample parse tree generated by the
word − syll grammar, in which Words consist of se-
quences of Syll(ables).
</figureCaption>
<figure confidence="0.983703958333334">
Morph
u
Morph
e
Morph
n k i
Morph
l e
Morph
k a
Morph
e
Word
Word
Word
Syll
u
Syll
Syll
e n k i
Word
Syll
k a e
Sentence
</figure>
<bodyText confidence="0.99926625">
to the word, rather than to capture interword depen-
dencies. Perhaps this shouldn’t be surprising, since
Sesotho words in this corpus are considerably more
complex than the English words in the Brent corpus.
</bodyText>
<sectionHeader confidence="0.938582" genericHeader="method">
4 Adding syllable structure
</sectionHeader>
<bodyText confidence="0.999914916666667">
Johnson (2008) found a small but significant im-
provement in word segmentation accuracy by using
an adaptor grammar that models English words as
a sequence of syllables. The word − syll grammar
builds in knowledge that syllables consist of an op-
tional Onset, a Nuc(leus) and an optional Coda, and
knows that Onsets and Codas are composes of con-
sonants and that Nucleii are vocalic (and that syl-
labic consonsants are possible Nucleii), and learns
the possible syllables of the language. The rules in
the adaptor grammars that expand Word are changed
to the following:
</bodyText>
<equation confidence="0.992228">
Word → Syll+
Syll → (Onset) Nuc (Coda)
Syll → SC
Onset → C+
Nuc → V+
Coda → C+
</equation>
<bodyText confidence="0.9998358">
In this grammar C expands to any consonant and V
expands to any vowel, SC expands to the syllablic
consonants ‘l’, ‘m’ ‘n’ and ‘r’, and parentheses indi-
cate optionality. Figure 4 shows a sample parse tree
produced by the word − syll adaptor grammar (i.e.,
where Words are generated by a unigram model),
while Figure 5 shows a sample parse tree generated
by the corresponding colloc − syll adaptor grammar
(where Words are generated as a part of a Colloca-
tion).
</bodyText>
<figureCaption confidence="0.99634">
Figure 5: A sample parse tree generated by the
colloc − syll grammar, in which Colloc(ations) consist of
sequences of Words, which in turn consist of sequences
of Syll(ables).
</figureCaption>
<bodyText confidence="0.9999162">
Building in this knowledge of syllable struc-
ture does improve word segmentation accuracy,
but the best performance comes from the simplest
word − syll grammar (with a word segmentation f-
score of 50%).
</bodyText>
<subsectionHeader confidence="0.998434">
4.1 Tracking morphological position
</subsectionHeader>
<bodyText confidence="0.9999817">
As we noted earlier, the various Colloc grammars
wound up capturing a certain amount of morpholog-
ical structure, even though they only implement a
relatively simple unigram model of morpheme word
order. Here we investigate whether we can im-
prove word segmentation accuracy with more so-
phisticated models of morphological structure.
The word − morph grammar generates a word as
a sequence of one to five morphemes. The relevant
productions are the following:
</bodyText>
<equation confidence="0.951383166666667">
Word → T1 (T2 (T3 (T4 (T5))))
T1 → Phoneme+
T2 → Phoneme+
T3 → Phoneme+
T4 → Phoneme+
T5 → Phoneme+
</equation>
<figure confidence="0.99275515">
Colloc
Word
Syll
k a e
Colloc
Word
Syll
u
Word
Syll
e
Syll
n k i
Word
Syll
l e
25
Word
Word
Sentence
Word
u
e
n k
i l e
k a
e
u e
n k i l e
k a e
T2
T1
T3
T
T
P2
S
P1
S
Sentence
</figure>
<figureCaption confidence="0.999274">
Figure 6: A sample parse tree generated by the
word − morph grammar, in which Words consist of mor-
phemes T1–T5, each of which is associated with specific
lexical items.
</figureCaption>
<bodyText confidence="0.999942055555556">
While each morpheme is generated by a unigram
character model, because each of these five mor-
pheme positions is independently adapted, the gram-
mar can learn which morphemes prefer to appear in
which position. Figure 6 contains a sample parse
generated by this grammar. Modifying the gram-
mar in this way significantly improves word seg-
mentation accuracy, achieving a word segmentation
f-score of 53%.
Inspired by this, we decided to see what would
happen if we built-in some specific knowledge of
Sesotho morphology, namely that a word consists of
a stem plus an optional suffix and zero to three op-
tional prefixes. (This kind of information is often
built into morphology learning models, either ex-
plicitly or implicitly via restrictions on the search
procedure). The resulting grammar, which we call
word − smorph, generates words as follows:
</bodyText>
<equation confidence="0.974008166666667">
Word → (P1 (P2 (P3))) T (S)
P1 → Phoneme+
P2 → Phoneme+
P3 → Phoneme+
T → Phoneme+
S → Phoneme+
</equation>
<bodyText confidence="0.975513272727273">
Figure 7 contains a sample parse tree generated
by this grammar. Perhaps not surprisingly, with this
modification the grammar achieves the highest word
segmentation f-score of any of the models examined
in this paper, namely 55.6%.
Of course, this morphological structure is per-
fectly compatible with models which posit higher-
level structure than Words. We can replace the Word
expansion in the colloc grammar with one just given;
the resulting grammar is called colloc − smorph,
and a sample parse tree is given in Figure 8. Interest-
</bodyText>
<figureCaption confidence="0.988004">
Figure 7: A sample parse tree generated by the
word − smorph grammar, in which Words consist of up
to five morphemes that satisfy prespecified ordering con-
straints.
</figureCaption>
<figure confidence="0.972539">
Sentence
Colloc
</figure>
<figureCaption confidence="0.9917126">
Figure 8: A sample parse tree generated by the
colloc − smorph grammar, in which Colloc(ations) gen-
erate a sequence of Words, which in turn consist of up
to five morphemes that satisfy prespecified ordering con-
straints.
</figureCaption>
<bodyText confidence="0.999829">
ingly, this grammar achieves a lower accuracy than
either of the two word-based morphology grammars
we considered above.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999985705882353">
Perhaps the most important conclusion to be drawn
from this paper is that the methods developed for
unsupervised word segmentation for English also
work for Sesotho, despite its having radically dif-
ferent morphological structures to English. Just as
with English, more structured adaptor grammars can
achieve better word-segmentation accuracies than
simpler ones. While we find overall word segmen-
tation accuracies lower than these models achieve
on English, we also found some interesting differ-
ences in which factors contribute to better word seg-
mentation. Perhaps surprisingly, we found little
improvement to word segmentation accuracy when
we modeled contextual dependencies, even though
these are most important in English. But includ-
ing either morphological structure or syllable struc-
ture in the model improved word segmentation accu-
</bodyText>
<figure confidence="0.9991452">
Word Word
T
S
S
n k i l e
e
P1
u e
T
k a
</figure>
<page confidence="0.98615">
26
</page>
<bodyText confidence="0.993843">
racy markedly, with morphological structure making
a larger impact. Given how important morphology is
in Sesotho, perhaps this is no surprise after all.
</bodyText>
<sectionHeader confidence="0.996551" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999953">
I’d like to thank Katherine Demuth for the Sesotho
data and help with Sesotho morphology, my collabo-
rators Sharon Goldwater and Tom Griffiths for their
comments and suggestions about adaptor grammars,
and the anonymous SIGMORPHON reviewers for
their careful reading and insightful comments on the
original abstract. This research was funded by NSF
awards 0544127 and 0631667.
</bodyText>
<sectionHeader confidence="0.999438" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999969616438356">
N. Bernstein-Ratner. 1987. The phonology of parent-
child speech. In K. Nelson and A. van Kleeck, editors,
Children’s Language, volume 6. Erlbaum, Hillsdale,
NJ.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
Rens Bod. 1998. Beyond grammar: an experience-based
theory of language. CSLI Publications, Stanford, Cal-
ifornia.
M. Brent. 1999. An efficient, probabilistically sound
algorithm for segmentation and word discovery. Ma-
chine Learning, 34:71–105.
Noam Chomsky. 1986. Knowledge of Language: Its
Nature, Origin and Use. Praeger, New York.
Katherine Demuth. 1992. Acquisition of Sesotho.
In Dan Slobin, editor, The Cross-Linguistic Study
of Language Acquisition, volume 3, pages 557–638.
Lawrence Erlbaum Associates, Hillsdale, N.J.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006a. Contextual dependencies in unsupervised
word segmentation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 673–680, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Sharon Goldwater, Tom Griffiths, and Mark Johnson.
2006b. Interpolating between types and tokens
by estimating power-law generators. In Y. Weiss,
B. Sch¨olkopf, and J. Platt, editors, Advances in Neural
Information Processing Systems 18, pages 459–466,
Cambridge, MA. MIT Press.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2007. Distributional cues to word boundaries:
Context is important. In David Bamman, Tatiana
Magnitskaia, and Colleen Zaller, editors, Proceedings
of the 31st Annual Boston University Conference on
Language Development, pages 239–250, Somerville,
MA. Cascadilla Press.
Sharon Goldwater. 2007. Nonparametric Bayesian Mod-
els of Lexical Acquisition. Ph.D. thesis, Brown Uni-
versity.
Yu Hu, Irina Matveeva, John Goldsmith, and Colin
Sprague. 2005. Refining the SED heuristic for mor-
pheme discovery: Another look at Swahili. In Pro-
ceedings of the Workshop on Psychocomputational
Models ofHuman Language Acquisition, pages 28–35,
Ann Arbor, Michigan, June. Association for Computa-
tional Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Adaptor Grammars: A framework for spec-
ifying compositional nonparametric Bayesian models.
In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Ad-
vances in Neural Information Processing Systems 19,
pages 641–648. MIT Press, Cambridge, MA.
Mark Johnson. 2008. Using adaptor grammars to identi-
fying synergies in the unsupervised acquisition of lin-
guistic structure. In Proceedings of the 46th Annual
Meeting of the Association of Computational Linguis-
tics, Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.
2007. The infinite PCFG using hierarchical Dirichlet
processes. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 688–697.
Rissanen. 1989. Stochastic Complexity in Statistical In-
quiry. World Scientific Company, Singapore.
Christian P. Robert and George Casella. 2004. Monte
Carlo Statistical Methods. Springer.
Y. W. Teh, M. Jordan, M. Beal, and D. Blei. 2006. Hier-
archical Dirichlet processes. Journal of the American
Statistical Association, 101:1566–1581.
</reference>
<page confidence="0.998809">
27
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.989847">
<title confidence="0.998421">Unsupervised word segmentation for Sesotho using Adaptor Grammars</title>
<author confidence="0.999998">Mark Johnson</author>
<affiliation confidence="0.999906">Brown University</affiliation>
<author confidence="0.997902">Mark JohnsonBrown edu</author>
<abstract confidence="0.9995825625">This paper describes a variety of nonparametric Bayesian models of word segmenbased on Grammars model different aspects of the input and incorporate different kinds of prior knowledge, and applies them to the Bantu language Sesotho. While we find overall word segmentation accuracies lower than these models achieve on English, we also find some interesting differences in which factors contribute to better word segmentation. Specifically, we found little improvement to word segmentation accuracy when we modeled contextual dependencies, while modeling morphological structure did improve segmentation accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Bernstein-Ratner</author>
</authors>
<title>The phonology of parentchild speech.</title>
<date>1987</date>
<booktitle>Children’s Language,</booktitle>
<volume>6</volume>
<editor>In K. Nelson and A. van Kleeck, editors,</editor>
<publisher>Erlbaum,</publisher>
<location>Hillsdale, NJ.</location>
<marker>Bernstein-Ratner, 1987</marker>
<rawString>N. Bernstein-Ratner. 1987. The phonology of parentchild speech. In K. Nelson and A. van Kleeck, editors, Children’s Language, volume 6. Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<date>2006</date>
<booktitle>Pattern Recognition and Machine Learning.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="823" citStr="Bishop, 2006" startWordPosition="118" endWordPosition="119">on based on Adaptor Grammars that model different aspects of the input and incorporate different kinds of prior knowledge, and applies them to the Bantu language Sesotho. While we find overall word segmentation accuracies lower than these models achieve on English, we also find some interesting differences in which factors contribute to better word segmentation. Specifically, we found little improvement to word segmentation accuracy when we modeled contextual dependencies, while modeling morphological structure did improve segmentation accuracy. 1 Introduction A Bayesian approach to learning (Bishop, 2006) is especially useful for computational models of language acquisition because we can use it to study the effect of different kinds and amounts of prior knowledge on the learning process. The Bayesian approach is agnostic as to what this prior knowledge might consist of; the prior could encode the kinds of rich universal grammar hypothesised by e.g., Chomsky (1986), or it could express a vague non-linguistic preference for simpler as opposed to more complex models, as in some of the grammars discussed below. Clearly there’s a wide range of possible priors, and one of the exciting possibilities</context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>Beyond grammar: an experience-based theory of language.</title>
<date>1998</date>
<publisher>CSLI Publications,</publisher>
<location>Stanford, California.</location>
<contexts>
<context position="8100" citStr="Bod (1998)" startWordPosition="1280" endWordPosition="1281">ally) with the size of the training data. In a word-segmentation application these rules typically generate words or morphemes, so the learner is effectively learning the morphemes and words of its language. The new rules learnt by an Adaptor Grammar are 21 compositions of old ones (that can themselves be compositions of other rules), so it’s natural to think of these new rules as tree fragments, where each entire fragment is associated with its own probability. Viewed this way, an adaptor grammar can be viewed as learning the tree fragments or constructions involved in a language, much as in Bod (1998). For computational reasons adaptor grammars require these fragments to consist of subtrees (i.e., their yields are terminals). We now provide an informal description of Adaptor Grammars (for a more formal description see Johnson et al. (2007)). An adaptor grammar consists of terminals V , nonterminals N (including a start symbol 5), initial rules R and rule probabilities p, just as in a PCFG. In addition, it also has a vector of concentration parameters α, where αA &gt; 0 is called the (Dirichlet) concentration parameter associated with nonterminal A. The nonterminals A for which αA &gt; 0 are adap</context>
</contexts>
<marker>Bod, 1998</marker>
<rawString>Rens Bod. 1998. Beyond grammar: an experience-based theory of language. CSLI Publications, Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Brent</author>
</authors>
<title>An efficient, probabilistically sound algorithm for segmentation and word discovery.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--71</pages>
<contexts>
<context position="4438" citStr="Brent, 1999" startWordPosition="692" endWordPosition="693"> it seems plausible that humans use some kind of nonparametric procedure to learn the lexicon. Johnson et al. (2007) introduced Adaptor Grammars as a framework in which a wide variety of linguistically-interesting nonparametric inference problems can be formulated and evaluated, including a number of variants of the models described by Goldwater (2007). Johnson (2008) presented a variety of different adaptor grammar word segmentation models and applied them to the problem of segmenting Brent’s phonemicized version of the BernsteinRatner corpus of child-directed English (BernsteinRatner, 1987; Brent, 1999). The main results of that paper were the following: 1. it confirmed the importance of modeling contextual dependencies above the word level for word segmentation (Goldwater et al., 2006a), 2. it showed a small but significant improvement to segmentation accuracy by learning the possible syllable structures of the language together with the lexicon, and 3. it found no significant advantage to learning morphological structure together with the lexicon (indeed, that model confused morphological and lexical structure). Of course the last result is a null result, and it’s possible that a different</context>
</contexts>
<marker>Brent, 1999</marker>
<rawString>M. Brent. 1999. An efficient, probabilistically sound algorithm for segmentation and word discovery. Machine Learning, 34:71–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Knowledge of Language: Its Nature, Origin and Use.</title>
<date>1986</date>
<publisher>Praeger,</publisher>
<location>New York.</location>
<contexts>
<context position="1190" citStr="Chomsky (1986)" startWordPosition="179" endWordPosition="180">pecifically, we found little improvement to word segmentation accuracy when we modeled contextual dependencies, while modeling morphological structure did improve segmentation accuracy. 1 Introduction A Bayesian approach to learning (Bishop, 2006) is especially useful for computational models of language acquisition because we can use it to study the effect of different kinds and amounts of prior knowledge on the learning process. The Bayesian approach is agnostic as to what this prior knowledge might consist of; the prior could encode the kinds of rich universal grammar hypothesised by e.g., Chomsky (1986), or it could express a vague non-linguistic preference for simpler as opposed to more complex models, as in some of the grammars discussed below. Clearly there’s a wide range of possible priors, and one of the exciting possibilities raised by Bayesian methods is that we may soon be able to empirically evaluate the potential contribution of different kinds of prior knowledge to language learning. 20 The Bayesian framework is surprisingly flexible. The bulk of the work on Bayesian inference is on parametric models, where the goal is to learn the value of a set of parameters (much as in Chomsky’</context>
</contexts>
<marker>Chomsky, 1986</marker>
<rawString>Noam Chomsky. 1986. Knowledge of Language: Its Nature, Origin and Use. Praeger, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katherine Demuth</author>
</authors>
<title>Acquisition of Sesotho.</title>
<date>1992</date>
<booktitle>The Cross-Linguistic Study of Language Acquisition,</booktitle>
<volume>3</volume>
<pages>557--638</pages>
<editor>In Dan Slobin, editor,</editor>
<location>Hillsdale, N.J.</location>
<contexts>
<context position="13372" citStr="Demuth, 1992" startWordPosition="2185" endWordPosition="2186">ord f-score morpheme f-score 0.431 0.352 0.478 0.387 0.467 0.389 0.502 0.349 0.476 0.372 0.490 0.393 0.529 0.321 0.556 0.378 0.537 0.352 Table 1: Summary of word and morpheme f-scores for the different models discussed in this paper. Model word colloc colloc2 word − syll colloc − syll colloc2 − syll word − morph word − smorph colloc − smorph 3 Models of Sesotho word segmentation We wanted to make our Sesotho corpus as similar as possible to one used in previous work on word segmentation. We extracted all of the non-child utterances from the LI–LV files from the Sesotho corpus of child speech (Demuth, 1992), and used the Sesotho gloss as our gold-standard corpus (we did not phonemicize them as Sesotho orthography is very close to phonemic). This produced 8,503 utterances containing 21,037 word tokens, 30,200 morpheme tokens and 100,113 phonemes. By comparison, the Brent corpus contains 9,790 utterances, 33,399 word tokens and 95,809 phonemes. Thus the Sesotho corpus contains approximately the same number of utterances and phonemes as the Brent corpus, but far fewer (and hence far longer) words. This is not surprising as the Sesotho corpus involves an older child and Sesotho, being an agglutinati</context>
</contexts>
<marker>Demuth, 1992</marker>
<rawString>Katherine Demuth. 1992. Acquisition of Sesotho. In Dan Slobin, editor, The Cross-Linguistic Study of Language Acquisition, volume 3, pages 557–638. Lawrence Erlbaum Associates, Hillsdale, N.J.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Contextual dependencies in unsupervised word segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>673--680</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="3140" citStr="Goldwater et al., 2006" startWordPosition="496" endWordPosition="499">ametric Bayesian learning methods can learn the structures relevant to a language. Determining whether methods such as these can in fact learn linguistic structure bears on the nature vs. nurture debates in language acquisition, since one of the arguments for the nativist position is that there doesn’t seem to be a way to learn structure from the input that children receive. While there’s no reason why these methods can’t be used to learn the syntax and semantics of human languages, much of the work to date has focused on lower-level learning problems such as morphological structure learning (Goldwater et al., 2006b) and word segmentation, where the learner is given unsegmented broad-phonemic utterance transcriptions Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, pages 20–27, Columbus, Ohio, USA June 2008. c�2008 Association for Computational Linguistics and has to identify the word boundaries (Goldwater et al., 2006a; Goldwater et al., 2007). One reason for this is that these problems seem simpler than learning syntax, where the non-linguistic context plausibly supplies important information to human learners. Virtually everyone agrees that</context>
<context position="4624" citStr="Goldwater et al., 2006" startWordPosition="720" endWordPosition="723">variety of linguistically-interesting nonparametric inference problems can be formulated and evaluated, including a number of variants of the models described by Goldwater (2007). Johnson (2008) presented a variety of different adaptor grammar word segmentation models and applied them to the problem of segmenting Brent’s phonemicized version of the BernsteinRatner corpus of child-directed English (BernsteinRatner, 1987; Brent, 1999). The main results of that paper were the following: 1. it confirmed the importance of modeling contextual dependencies above the word level for word segmentation (Goldwater et al., 2006a), 2. it showed a small but significant improvement to segmentation accuracy by learning the possible syllable structures of the language together with the lexicon, and 3. it found no significant advantage to learning morphological structure together with the lexicon (indeed, that model confused morphological and lexical structure). Of course the last result is a null result, and it’s possible that a different model would be able to usefuly combine morphological learning with word segmentation. This paper continues that research by applying the same kinds of models to Sesotho, a Bantu languag</context>
<context position="14640" citStr="Goldwater et al. (2006" startWordPosition="2379" endWordPosition="2382">plex words. In the subsections that follow we describe a variety of adaptor grammar models for word segmentation. All of these models were given same Sesotho data, which consisted of the Sesotho gold-standard corpus described above with all word boundaries (spaces) and morpheme boundaries (hyphens) removed. We computed the f-score (geometric average of precision and recall) with which the models recovered the words or the morphemes annotated in the gold-standard corpus. 3.1 Unigram grammar We begin by describing an adaptor grammar that simulates the unigram word segmentation model proposed by Goldwater et al. (2006a). In this model each utterance is generated as a sequence of words, and each word is a sequence of phonemes. This grammar contains three kinds of rules, including rules that expand the nonterminal Phoneme to all of the phonemes seen in the training data. Sentence —* Word+ Word —* Phoneme+ Adapted non-terminals are indicated by underlining, so in the word grammar only the Word nonterminal is adapted. Our software doesn’t permit regular expressions in rules, so we expand all Kleene stars in rules into right-recursive structures over new unadapted nonterminals. Figure 1 shows a sample parse tre</context>
<context position="17117" citStr="Goldwater et al. (2006" startWordPosition="2804" endWordPosition="2807"> average f-score over the last 100 samples for each run; the average f-score ranges between 42.6% and 43.7%. Thus while there may be considerable uncertainty as to where the word boundaries are in any given sentence (which is reflected in fact that the word boundaries are very likely to change from sample to sample), the average accuracy of such boundaries seems very stable. The final sample grammars contained the initial rules R, together with between 1,772 and 1,827 additional expansions for Word, corresponding to the cached subtrees for the adapted Word nonterminal. 3.2 Collocation grammar Goldwater et al. (2006a) showed that incorporating a bigram model of word-to-word dependencies significantly improves word segmentation accuracy in English. While it is not possible to formulate such a bigram model as an adaptor grammar, Johnson (2008) showed that a similar improvement can be achieved in an adaptor grammar by explicitly modeling collocations or sequences of words. The colloc adaptor grammar is: Sentence —* Colloc+ Colloc —* Word+ Word —* Phoneme+ This grammar generates a Sentence as a sequence of Colloc(ations), where each Colloc(ation) is a sequence of Words. Figure 2 shows a sample parse tree gen</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2006a. Contextual dependencies in unsupervised word segmentation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 673–680, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Interpolating between types and tokens by estimating power-law generators. In</title>
<date>2006</date>
<booktitle>Advances in Neural Information Processing Systems 18,</booktitle>
<pages>459--466</pages>
<editor>Y. Weiss, B. Sch¨olkopf, and J. Platt, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3140" citStr="Goldwater et al., 2006" startWordPosition="496" endWordPosition="499">ametric Bayesian learning methods can learn the structures relevant to a language. Determining whether methods such as these can in fact learn linguistic structure bears on the nature vs. nurture debates in language acquisition, since one of the arguments for the nativist position is that there doesn’t seem to be a way to learn structure from the input that children receive. While there’s no reason why these methods can’t be used to learn the syntax and semantics of human languages, much of the work to date has focused on lower-level learning problems such as morphological structure learning (Goldwater et al., 2006b) and word segmentation, where the learner is given unsegmented broad-phonemic utterance transcriptions Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, pages 20–27, Columbus, Ohio, USA June 2008. c�2008 Association for Computational Linguistics and has to identify the word boundaries (Goldwater et al., 2006a; Goldwater et al., 2007). One reason for this is that these problems seem simpler than learning syntax, where the non-linguistic context plausibly supplies important information to human learners. Virtually everyone agrees that</context>
<context position="4624" citStr="Goldwater et al., 2006" startWordPosition="720" endWordPosition="723">variety of linguistically-interesting nonparametric inference problems can be formulated and evaluated, including a number of variants of the models described by Goldwater (2007). Johnson (2008) presented a variety of different adaptor grammar word segmentation models and applied them to the problem of segmenting Brent’s phonemicized version of the BernsteinRatner corpus of child-directed English (BernsteinRatner, 1987; Brent, 1999). The main results of that paper were the following: 1. it confirmed the importance of modeling contextual dependencies above the word level for word segmentation (Goldwater et al., 2006a), 2. it showed a small but significant improvement to segmentation accuracy by learning the possible syllable structures of the language together with the lexicon, and 3. it found no significant advantage to learning morphological structure together with the lexicon (indeed, that model confused morphological and lexical structure). Of course the last result is a null result, and it’s possible that a different model would be able to usefuly combine morphological learning with word segmentation. This paper continues that research by applying the same kinds of models to Sesotho, a Bantu languag</context>
<context position="14640" citStr="Goldwater et al. (2006" startWordPosition="2379" endWordPosition="2382">plex words. In the subsections that follow we describe a variety of adaptor grammar models for word segmentation. All of these models were given same Sesotho data, which consisted of the Sesotho gold-standard corpus described above with all word boundaries (spaces) and morpheme boundaries (hyphens) removed. We computed the f-score (geometric average of precision and recall) with which the models recovered the words or the morphemes annotated in the gold-standard corpus. 3.1 Unigram grammar We begin by describing an adaptor grammar that simulates the unigram word segmentation model proposed by Goldwater et al. (2006a). In this model each utterance is generated as a sequence of words, and each word is a sequence of phonemes. This grammar contains three kinds of rules, including rules that expand the nonterminal Phoneme to all of the phonemes seen in the training data. Sentence —* Word+ Word —* Phoneme+ Adapted non-terminals are indicated by underlining, so in the word grammar only the Word nonterminal is adapted. Our software doesn’t permit regular expressions in rules, so we expand all Kleene stars in rules into right-recursive structures over new unadapted nonterminals. Figure 1 shows a sample parse tre</context>
<context position="17117" citStr="Goldwater et al. (2006" startWordPosition="2804" endWordPosition="2807"> average f-score over the last 100 samples for each run; the average f-score ranges between 42.6% and 43.7%. Thus while there may be considerable uncertainty as to where the word boundaries are in any given sentence (which is reflected in fact that the word boundaries are very likely to change from sample to sample), the average accuracy of such boundaries seems very stable. The final sample grammars contained the initial rules R, together with between 1,772 and 1,827 additional expansions for Word, corresponding to the cached subtrees for the adapted Word nonterminal. 3.2 Collocation grammar Goldwater et al. (2006a) showed that incorporating a bigram model of word-to-word dependencies significantly improves word segmentation accuracy in English. While it is not possible to formulate such a bigram model as an adaptor grammar, Johnson (2008) showed that a similar improvement can be achieved in an adaptor grammar by explicitly modeling collocations or sequences of words. The colloc adaptor grammar is: Sentence —* Colloc+ Colloc —* Word+ Word —* Phoneme+ This grammar generates a Sentence as a sequence of Colloc(ations), where each Colloc(ation) is a sequence of Words. Figure 2 shows a sample parse tree gen</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Tom Griffiths, and Mark Johnson. 2006b. Interpolating between types and tokens by estimating power-law generators. In Y. Weiss, B. Sch¨olkopf, and J. Platt, editors, Advances in Neural Information Processing Systems 18, pages 459–466, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Distributional cues to word boundaries: Context is important.</title>
<date>2007</date>
<booktitle>Proceedings of the 31st Annual Boston University Conference on Language Development,</booktitle>
<pages>239--250</pages>
<editor>In David Bamman, Tatiana Magnitskaia, and Colleen Zaller, editors,</editor>
<publisher>Cascadilla Press.</publisher>
<location>Somerville, MA.</location>
<contexts>
<context position="3537" citStr="Goldwater et al., 2007" startWordPosition="552" endWordPosition="555">eason why these methods can’t be used to learn the syntax and semantics of human languages, much of the work to date has focused on lower-level learning problems such as morphological structure learning (Goldwater et al., 2006b) and word segmentation, where the learner is given unsegmented broad-phonemic utterance transcriptions Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, pages 20–27, Columbus, Ohio, USA June 2008. c�2008 Association for Computational Linguistics and has to identify the word boundaries (Goldwater et al., 2006a; Goldwater et al., 2007). One reason for this is that these problems seem simpler than learning syntax, where the non-linguistic context plausibly supplies important information to human learners. Virtually everyone agrees that the set of possible morphemes and words, if not infinite, is astronomically large, so it seems plausible that humans use some kind of nonparametric procedure to learn the lexicon. Johnson et al. (2007) introduced Adaptor Grammars as a framework in which a wide variety of linguistically-interesting nonparametric inference problems can be formulated and evaluated, including a number of variants </context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2007</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2007. Distributional cues to word boundaries: Context is important. In David Bamman, Tatiana Magnitskaia, and Colleen Zaller, editors, Proceedings of the 31st Annual Boston University Conference on Language Development, pages 239–250, Somerville, MA. Cascadilla Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
</authors>
<title>Nonparametric Bayesian Models of Lexical Acquisition.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Brown University.</institution>
<contexts>
<context position="4180" citStr="Goldwater (2007)" startWordPosition="654" endWordPosition="655">at these problems seem simpler than learning syntax, where the non-linguistic context plausibly supplies important information to human learners. Virtually everyone agrees that the set of possible morphemes and words, if not infinite, is astronomically large, so it seems plausible that humans use some kind of nonparametric procedure to learn the lexicon. Johnson et al. (2007) introduced Adaptor Grammars as a framework in which a wide variety of linguistically-interesting nonparametric inference problems can be formulated and evaluated, including a number of variants of the models described by Goldwater (2007). Johnson (2008) presented a variety of different adaptor grammar word segmentation models and applied them to the problem of segmenting Brent’s phonemicized version of the BernsteinRatner corpus of child-directed English (BernsteinRatner, 1987; Brent, 1999). The main results of that paper were the following: 1. it confirmed the importance of modeling contextual dependencies above the word level for word segmentation (Goldwater et al., 2006a), 2. it showed a small but significant improvement to segmentation accuracy by learning the possible syllable structures of the language together with the</context>
</contexts>
<marker>Goldwater, 2007</marker>
<rawString>Sharon Goldwater. 2007. Nonparametric Bayesian Models of Lexical Acquisition. Ph.D. thesis, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu Hu</author>
<author>Irina Matveeva</author>
<author>John Goldsmith</author>
<author>Colin Sprague</author>
</authors>
<title>Refining the SED heuristic for morpheme discovery: Another look at Swahili.</title>
<date>2005</date>
<booktitle>In Proceedings of the Workshop on Psychocomputational Models ofHuman Language Acquisition,</booktitle>
<pages>28--35</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="5687" citStr="Hu et al. (2005)" startWordPosition="886" endWordPosition="889">ombine morphological learning with word segmentation. This paper continues that research by applying the same kinds of models to Sesotho, a Bantu language spoken in Southern Africa. Bantu languages are especially interesting for this kind of study, as they have rich productive agglutinative morphologies and relatively transparent phonologies, as compared to languages such as Finnish or Turkish which have complex harmony processes and other phonological complexities. The relative clarity of Bantu has inspired previous computational work, such as the algorithm for learning Swahili morphology by Hu et al. (2005). The Hu et al. algorithm uses a Minimum Description Length procedure (Rissanen, 1989) that is conceptually related to the nonparametric Bayesian procedure used here. However, the work here is focused on determining whether the word segmentation methods that work well for English generalize to Sesotho and whether modeling morphological and/or syllable structure improves Sesotho word segmentation, rather than learning Sesotho morphological structure per se. The rest of this paper is structured as follows. Section 2 informally reviews adaptor grammars and describes how they are used to specify d</context>
</contexts>
<marker>Hu, Matveeva, Goldsmith, Sprague, 2005</marker>
<rawString>Yu Hu, Irina Matveeva, John Goldsmith, and Colin Sprague. 2005. Refining the SED heuristic for morpheme discovery: Another look at Swahili. In Proceedings of the Workshop on Psychocomputational Models ofHuman Language Acquisition, pages 28–35, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Adaptor Grammars: A framework for specifying compositional nonparametric Bayesian models.</title>
<date>2007</date>
<booktitle>Advances in Neural Information Processing Systems 19,</booktitle>
<pages>641--648</pages>
<editor>In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3942" citStr="Johnson et al. (2007)" startWordPosition="617" endWordPosition="620">ational Morphology and Phonology, pages 20–27, Columbus, Ohio, USA June 2008. c�2008 Association for Computational Linguistics and has to identify the word boundaries (Goldwater et al., 2006a; Goldwater et al., 2007). One reason for this is that these problems seem simpler than learning syntax, where the non-linguistic context plausibly supplies important information to human learners. Virtually everyone agrees that the set of possible morphemes and words, if not infinite, is astronomically large, so it seems plausible that humans use some kind of nonparametric procedure to learn the lexicon. Johnson et al. (2007) introduced Adaptor Grammars as a framework in which a wide variety of linguistically-interesting nonparametric inference problems can be formulated and evaluated, including a number of variants of the models described by Goldwater (2007). Johnson (2008) presented a variety of different adaptor grammar word segmentation models and applied them to the problem of segmenting Brent’s phonemicized version of the BernsteinRatner corpus of child-directed English (BernsteinRatner, 1987; Brent, 1999). The main results of that paper were the following: 1. it confirmed the importance of modeling contextu</context>
<context position="7363" citStr="Johnson et al., 2007" startWordPosition="1154" endWordPosition="1157">umerical parameter associated with it. One way to construct nonparametric Bayesian models is to take a parametric model class and let one or more of their components grow unboundedly. There are two obvious ways to construct nonparametric models from PCFGs. First, we can let the number of nonterminals grow unboundedly, as in the Infinite PCFG, where the nonterminals of the grammar can be indefinitely refined versions of a base PCFG (Liang et al., 2007). Second, we can fix the set of nonterminals but permit the number of rules or productions to grow unboundedly, which leads to Adaptor Grammars (Johnson et al., 2007). At any point in learning, an Adaptor Grammar has a finite set of rules, but these can grow unboundedly (typically logarithmically) with the size of the training data. In a word-segmentation application these rules typically generate words or morphemes, so the learner is effectively learning the morphemes and words of its language. The new rules learnt by an Adaptor Grammar are 21 compositions of old ones (that can themselves be compositions of other rules), so it’s natural to think of these new rules as tree fragments, where each entire fragment is associated with its own probability. Viewed</context>
<context position="10492" citStr="Johnson et al. (2007)" startWordPosition="1710" endWordPosition="1713">er of times A has expanded to subtree a before, and 2. A expands to Q where A—* Q E R with probability αA pA,O/(nA + αA). Thus an adapted nonterminal A expands to a previously expanded subtree a with probability proportional to the number nQ of times it was used before, and expands just as in a PCFG (i.e., using R) with probability proportional to the concentration parameter αA. This parameter specifies how likely A is to expand into a potentially new subtree; as nA and nQ grow this becomes increasingly unlikely. We used the publically available adaptor grammar inference software described in Johnson et al. (2007), which we modified slightly as described below. The basic algorithm is a Metropolis-withinGibbs or Hybrid MCMC sampler (Robert and Casella, 2004), which resamples the parse tree for each sentence in the training data conditioned on the parses for the other sentences. In order to produce sample parses efficiently the algorithm constructs a PCFG approximation to the adaptor grammar which contains one rule for each adapted subtree a, and uses a Metropolis accept/reject step to correct for the difference between the true adaptor grammar distribution and the PCFG approximation. With the datasets d</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. 2007. Adaptor Grammars: A framework for specifying compositional nonparametric Bayesian models. In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 641–648. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Using adaptor grammars to identifying synergies in the unsupervised acquisition of linguistic structure.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="4196" citStr="Johnson (2008)" startWordPosition="656" endWordPosition="657">seem simpler than learning syntax, where the non-linguistic context plausibly supplies important information to human learners. Virtually everyone agrees that the set of possible morphemes and words, if not infinite, is astronomically large, so it seems plausible that humans use some kind of nonparametric procedure to learn the lexicon. Johnson et al. (2007) introduced Adaptor Grammars as a framework in which a wide variety of linguistically-interesting nonparametric inference problems can be formulated and evaluated, including a number of variants of the models described by Goldwater (2007). Johnson (2008) presented a variety of different adaptor grammar word segmentation models and applied them to the problem of segmenting Brent’s phonemicized version of the BernsteinRatner corpus of child-directed English (BernsteinRatner, 1987; Brent, 1999). The main results of that paper were the following: 1. it confirmed the importance of modeling contextual dependencies above the word level for word segmentation (Goldwater et al., 2006a), 2. it showed a small but significant improvement to segmentation accuracy by learning the possible syllable structures of the language together with the lexicon, and 3.</context>
<context position="17347" citStr="Johnson (2008)" startWordPosition="2842" endWordPosition="2843"> fact that the word boundaries are very likely to change from sample to sample), the average accuracy of such boundaries seems very stable. The final sample grammars contained the initial rules R, together with between 1,772 and 1,827 additional expansions for Word, corresponding to the cached subtrees for the adapted Word nonterminal. 3.2 Collocation grammar Goldwater et al. (2006a) showed that incorporating a bigram model of word-to-word dependencies significantly improves word segmentation accuracy in English. While it is not possible to formulate such a bigram model as an adaptor grammar, Johnson (2008) showed that a similar improvement can be achieved in an adaptor grammar by explicitly modeling collocations or sequences of words. The colloc adaptor grammar is: Sentence —* Colloc+ Colloc —* Word+ Word —* Phoneme+ This grammar generates a Sentence as a sequence of Colloc(ations), where each Colloc(ation) is a sequence of Words. Figure 2 shows a sample parse tree generated by the colloc grammar. In terms of word segmentation, this grammar performs much worse Figure 2: A sample parse tree generated by the colloc grammar. The substrings generated by Word in fact tend to be morphemes and Colloc </context>
<context position="20067" citStr="Johnson (2008)" startWordPosition="3320" endWordPosition="3321">i l e k a u e n k 24 Sentence Colloc Figure 3: A sample parse tree generated by the colloc2 grammar. Sentence Word Syll l e Figure 4: A sample parse tree generated by the word − syll grammar, in which Words consist of sequences of Syll(ables). Morph u Morph e Morph n k i Morph l e Morph k a Morph e Word Word Word Syll u Syll Syll e n k i Word Syll k a e Sentence to the word, rather than to capture interword dependencies. Perhaps this shouldn’t be surprising, since Sesotho words in this corpus are considerably more complex than the English words in the Brent corpus. 4 Adding syllable structure Johnson (2008) found a small but significant improvement in word segmentation accuracy by using an adaptor grammar that models English words as a sequence of syllables. The word − syll grammar builds in knowledge that syllables consist of an optional Onset, a Nuc(leus) and an optional Coda, and knows that Onsets and Codas are composes of consonants and that Nucleii are vocalic (and that syllabic consonsants are possible Nucleii), and learns the possible syllables of the language. The rules in the adaptor grammars that expand Word are changed to the following: Word → Syll+ Syll → (Onset) Nuc (Coda) Syll → SC</context>
</contexts>
<marker>Johnson, 2008</marker>
<rawString>Mark Johnson. 2008. Using adaptor grammars to identifying synergies in the unsupervised acquisition of linguistic structure. In Proceedings of the 46th Annual Meeting of the Association of Computational Linguistics, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Slav Petrov</author>
<author>Michael Jordan</author>
<author>Dan Klein</author>
</authors>
<title>The infinite PCFG using hierarchical Dirichlet processes.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>688--697</pages>
<contexts>
<context position="7197" citStr="Liang et al., 2007" startWordPosition="1126" endWordPosition="1129">e they are very simple and natural models of hierarchical structure. They are parametric models because each PCFG has a fixed number of rules, each of which has a numerical parameter associated with it. One way to construct nonparametric Bayesian models is to take a parametric model class and let one or more of their components grow unboundedly. There are two obvious ways to construct nonparametric models from PCFGs. First, we can let the number of nonterminals grow unboundedly, as in the Infinite PCFG, where the nonterminals of the grammar can be indefinitely refined versions of a base PCFG (Liang et al., 2007). Second, we can fix the set of nonterminals but permit the number of rules or productions to grow unboundedly, which leads to Adaptor Grammars (Johnson et al., 2007). At any point in learning, an Adaptor Grammar has a finite set of rules, but these can grow unboundedly (typically logarithmically) with the size of the training data. In a word-segmentation application these rules typically generate words or morphemes, so the learner is effectively learning the morphemes and words of its language. The new rules learnt by an Adaptor Grammar are 21 compositions of old ones (that can themselves be </context>
</contexts>
<marker>Liang, Petrov, Jordan, Klein, 2007</marker>
<rawString>Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein. 2007. The infinite PCFG using hierarchical Dirichlet processes. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 688–697.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rissanen</author>
</authors>
<title>Stochastic Complexity in Statistical Inquiry.</title>
<date>1989</date>
<publisher>World Scientific Company,</publisher>
<contexts>
<context position="5773" citStr="Rissanen, 1989" startWordPosition="901" endWordPosition="903">h by applying the same kinds of models to Sesotho, a Bantu language spoken in Southern Africa. Bantu languages are especially interesting for this kind of study, as they have rich productive agglutinative morphologies and relatively transparent phonologies, as compared to languages such as Finnish or Turkish which have complex harmony processes and other phonological complexities. The relative clarity of Bantu has inspired previous computational work, such as the algorithm for learning Swahili morphology by Hu et al. (2005). The Hu et al. algorithm uses a Minimum Description Length procedure (Rissanen, 1989) that is conceptually related to the nonparametric Bayesian procedure used here. However, the work here is focused on determining whether the word segmentation methods that work well for English generalize to Sesotho and whether modeling morphological and/or syllable structure improves Sesotho word segmentation, rather than learning Sesotho morphological structure per se. The rest of this paper is structured as follows. Section 2 informally reviews adaptor grammars and describes how they are used to specify different Bayesian models. Section 3 describes the Sesotho corpus we used and the speci</context>
</contexts>
<marker>Rissanen, 1989</marker>
<rawString>Rissanen. 1989. Stochastic Complexity in Statistical Inquiry. World Scientific Company, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian P Robert</author>
<author>George Casella</author>
</authors>
<title>Monte Carlo Statistical Methods.</title>
<date>2004</date>
<publisher>Springer.</publisher>
<contexts>
<context position="10638" citStr="Robert and Casella, 2004" startWordPosition="1733" endWordPosition="1736">erminal A expands to a previously expanded subtree a with probability proportional to the number nQ of times it was used before, and expands just as in a PCFG (i.e., using R) with probability proportional to the concentration parameter αA. This parameter specifies how likely A is to expand into a potentially new subtree; as nA and nQ grow this becomes increasingly unlikely. We used the publically available adaptor grammar inference software described in Johnson et al. (2007), which we modified slightly as described below. The basic algorithm is a Metropolis-withinGibbs or Hybrid MCMC sampler (Robert and Casella, 2004), which resamples the parse tree for each sentence in the training data conditioned on the parses for the other sentences. In order to produce sample parses efficiently the algorithm constructs a PCFG approximation to the adaptor grammar which contains one rule for each adapted subtree a, and uses a Metropolis accept/reject step to correct for the difference between the true adaptor grammar distribution and the PCFG approximation. With the datasets described below less than 0.1% of proposal parses from this PCFG approximation are rejected, so it is quite a good approximation to the adaptor gra</context>
</contexts>
<marker>Robert, Casella, 2004</marker>
<rawString>Christian P. Robert and George Casella. 2004. Monte Carlo Statistical Methods. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
<author>M Jordan</author>
<author>M Beal</author>
<author>D Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<pages>101--1566</pages>
<contexts>
<context position="9243" citStr="Teh et al., 2006" startWordPosition="1476" endWordPosition="1479">ssociated with nonterminal A. The nonterminals A for which αA &gt; 0 are adapted, which means that each subtree for A that can be generated using the initial rules R is considered as a potential rule in the adaptor grammar. If αA = 0 then A is unadapted, which means it expands just as in an ordinary PCFG. Adaptor grammars are so-called because they adapt both the subtrees and their probabilities to the corpus they are generating. Formally, they are Hierarchical Dirichlet Processes that generate a distribution over distributions over trees that can be defined in terms of stick-breaking processes (Teh et al., 2006). It’s probably easiest to understand them in terms of their conditional or sampling distribution, which is the probability of generating a new tree T given the trees (Ti, ... , T,) that the adaptor grammar has already generated. An adaptor grammar can be viewed as generating a tree top-down, just like a PCFG. Suppose we have a node A to expand. If A is unadapted (i.e., αA = 0) then A expands just as in a PCFG, i.e., we pick a rule A —* Q E R with probability pA,O and recursively expand Q. If A is adapted and has expanded nA times before, then: 1. A expands to a subtree a with probability nQ/(</context>
<context position="12175" citStr="Teh et al. (2006)" startWordPosition="1980" endWordPosition="1983">an approximately 25% chance of the parse changing. Perhaps this is not surprising given the comparatively small amount of training data and the fact that the models only use fairly crude distributional information. As just described, adaptor grammars require the user to specify a concentration parameter αA for each adapted nonterminal A. It’s not obvious how this should be done. Previous work has treated αA as an adjustable parameter, usually tying all of the αA to some shared value which is adjusted to optimize task performance (say, word segmentation accuracy). Clearly, this is undesirable. Teh et al. (2006) describes how to learn the con22 centration parameters α, and we modified their procedure for adaptor grammars. Specifically, we put a vague Gamma(10, 0.1) prior on each αA, and after each iteration through the training data we performed 100 Metropolis-Hastings resampling steps for each αA from an increasingly narrow Gamma proposal distribution. We found that the performance of the models with automatically learned concentration parameters α was generally as good as the models where α was tuned by hand (although admittedly we only tried three or four different values for α). word f-score morp</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Y. W. Teh, M. Jordan, M. Beal, and D. Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101:1566–1581.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>