<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001526">
<title confidence="0.994131">
Learning Entity Representation for Entity Disambiguation
</title>
<author confidence="0.967806">
Zhengyan Het Shujie Liut Mu Lit Ming Zhou$ Longkai Zhangt Houfeng Wangt*
</author>
<affiliation confidence="0.9696315">
t Key Laboratory of Computational Linguistics (Peking University) Ministry of Education,China
t Microsoft Research Asia
</affiliation>
<email confidence="0.9288845">
hezhengyan.hit@gmail.com {shujliu,muli,mingzhou}@microsoft.com
zhlongk@qq.com wanghf@pku.edu.cn
</email>
<sectionHeader confidence="0.997237" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999951705882353">
We propose a novel entity disambigua-
tion model, based on Deep Neural Net-
work (DNN). Instead of utilizing simple
similarity measures and their disjoint com-
binations, our method directly optimizes
document and entity representations for a
given similarity measure. Stacked Denois-
ing Auto-encoders are first employed to
learn an initial document representation in
an unsupervised pre-training stage. A su-
pervised fine-tuning stage follows to opti-
mize the representation towards the simi-
larity measure. Experiment results show
that our method achieves state-of-the-art
performance on two public datasets with-
out any manually designed features, even
beating complex collective approaches.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991683639344262">
Entity linking or disambiguation has recently re-
ceived much attention in natural language process-
ing community (Bunescu and Pasca, 2006; Han
et al., 2011; Kataria et al., 2011; Sen, 2012). It is
an essential first step for succeeding sub-tasks in
knowledge base construction (Ji and Grishman,
2011) like populating attribute to entities. Given
a sentence with four mentions, “The [[Python]] of
[[Delphi]] was a creature with the body of a snake.
This creature dwelled on [[Mount Parnassus]], in
central [[Greece]].” How can we determine that
Python is an earth-dragon in Greece mythology
and not the popular programming language, Del-
phi is not the auto parts supplier, and Mount Par-
nassus is in Greece, not in Colorado?
A most straightforward method is to compare
the context of the mention and the definition of
candidate entities. Previous work has explored
many ways of measuring the relatedness of context
∗Corresponding author
d and entity e, such as dot product, cosine similar-
ity, Kullback-Leibler divergence, Jaccard distance,
or more complicated ones (Zheng et al., 2010;
Kulkarni et al., 2009; Hoffart et al., 2011; Bunescu
and Pasca, 2006; Cucerzan, 2007; Zhang et al.,
2011). However, these measures are often dupli-
cate or over-specified, because they are disjointly
combined and their atomic nature determines that
they have no internal structure.
Another line of work focuses on collective dis-
ambiguation (Kulkarni et al., 2009; Han et al.,
2011; Ratinov et al., 2011; Hoffart et al., 2011).
Ambiguous mentions within the same context are
resolved simultaneously based on the coherence
among decisions. Collective approaches often un-
dergo a non-trivial decision process. In fact, (Rati-
nov et al., 2011) show that even though global ap-
proaches can be improved, local methods based on
only similarity sim(d, e) of context d and entity e
are hard to beat. This somehow reveals the impor-
tance of a good modeling of sim(d, e).
Rather than learning context entity associa-
tion at word level, topic model based approaches
(Kataria et al., 2011; Sen, 2012) can learn it in
the semantic space. However, the one-topic-per-
entity assumption makes it impossible to scale to
large knowledge base, as every entity has a sepa-
rate word distribution P(w|e); besides, the train-
ing objective does not directly correspond with
disambiguation performances.
To overcome disadvantages of previous ap-
proaches, we propose a novel method to learn con-
text entity association enriched with deep architec-
ture. Deep neural networks (Hinton et al., 2006;
Bengio et al., 2007) are built in a hierarchical man-
ner, and allow us to compare context and entity
at some higher level abstraction; while at lower
levels, general concepts are shared across entities,
resulting in compact models. Moreover, to make
our model highly correlated with disambiguation
performance, our method directly optimizes doc-
</bodyText>
<page confidence="0.981562">
30
</page>
<bodyText confidence="0.9131320625">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 30–34,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
ument and entity representations for a fixed simi-
larity measure. In fact, the underlying representa-
tions for computing similarity measure add inter-
nal structure to the given similarity measure. Fea-
tures are learned leveraging large scale annotation
of Wikipedia, without any manual design efforts.
Furthermore, the learned model is compact com-
pared with topic model based approaches, and can
be trained discriminatively without relying on ex-
pensive sampling strategy. Despite its simplicity,
it beats all complex collective approaches in our
experiments. The learned similarity measure can
be readily incorporated into any existing collective
approaches, which further boosts performance.
</bodyText>
<sectionHeader confidence="0.779339" genericHeader="method">
2 Learning Representation for
Contextual Document
</sectionHeader>
<bodyText confidence="0.995149692307692">
Given a mention string m with its context docu-
ment d, a list of candidate entities C(m) are gen-
erated for m, for each candidate entity ei E C(m),
we compute a ranking score sim(dm, ei) indicat-
ing how likely m refers to ei. The linking result is
e = arg maxez sim(dm, ei).
Our algorithm consists of two stages. In the pre-
training stage, Stacked Denoising Auto-encoders
are built in an unsupervised layer-wise fashion to
discover general concepts encoding d and e. In the
supervised fine-tuning stage, the entire network
weights are fine-tuned to optimize the similarity
score sim(d, e).
</bodyText>
<subsectionHeader confidence="0.98904">
2.1 Greedy Layer-wise Pre-training
</subsectionHeader>
<bodyText confidence="0.999507172413793">
Stacked Auto-encoders (Bengio et al., 2007) is
one of the building blocks of deep learning. As-
sume the input is a vector x, an auto-encoder con-
sists of an encoding process h(x) and a decod-
ing process g(h(x)). The goal is to minimize the
reconstruction error L(x, g(h(x))), thus retaining
maximum information. By repeatedly stacking
new auto-encoder on top of previously learned
h(x), stacked auto-encoders are obtained. This
way we learn multiple levels of representation of
input x.
One problem of auto-encoder is that it treats all
words equally, no matter it is a function word or
a content word. Denoising Auto-encoder (DA)
(Vincent et al., 2008) seeks to reconstruct x given
a random corruption x˜ of x. DA can capture global
structure while ignoring noise as the author shows
in image processing. In our case, we input each
document as a binary bag-of-words vector (Fig.
1). DA will capture general concepts and ignore
noise like function words. By applying masking
noise (randomly mask 1 with 0), the model also
exhibits a fill-in-the-blank property (Vincent et
al., 2010): the missing components must be re-
covered from partial input. Take “greece” for ex-
ample, the model must learn to predict it with
“python” “mount”, through some hidden unit. The
hidden unit may somehow express the concept of
Greece mythology.
</bodyText>
<figureCaption confidence="0.9991">
Figure 1: DA and reconstruction sampling.
</figureCaption>
<bodyText confidence="0.999983">
In order to distinguish between a large num-
ber of entities, the vocabulary size must be large
enough. This adds considerable computational
overhead because the reconstruction process in-
volves expensive dense matrix multiplication. Re-
construction sampling keeps the sparse property
of matrix multiplication by reconstructing a small
subset of original input, with no loss of quality of
the learned representation (Dauphin et al., 2011).
</bodyText>
<subsectionHeader confidence="0.996269">
2.2 Supervised Fine-tuning
</subsectionHeader>
<bodyText confidence="0.999981833333333">
This stage we optimize the learned representation
(“hidden layer n” in Fig. 2) towards the ranking
score sim(d, e), with large scale Wikipedia an-
notation as supervision. We collect hyperlinks in
Wikipedia as our training set {(di, ei, mi)}, where
mi is the mention string for candidate generation.
The network weights below “hidden layer n” are
initialized with the pre-training stage.
Next, we stack another layer on top of the
learned representation. The whole network is
tuned by the final supervised objective. The reason
to stack another layer on top of the learned rep-
resentation, is to capture problem specific struc-
tures. Denote the encoding of d and e as dˆ and
eˆ respectively, after stacking the problem-specific
layer, the representation for d is given as f(d) =
sigmoid(W x dˆ + b), where W and b are weight
and bias term respectively. f(e) follows the same
</bodyText>
<figure confidence="0.998434066666667">
reconstruct input
reconstruct random
zero node
not reconstruct
active
active, but
mask out
inactive
coding
dragon delphi
snake phd
mount greece
python
g(h(x))
h(x)
</figure>
<page confidence="0.999655">
31
</page>
<bodyText confidence="0.982922333333333">
encoding process.
The similarity score of (d, e) pair is defined as
the dot product of f(d) and f(e) (Fig. 2):
</bodyText>
<equation confidence="0.878871">
sim(d, e) = Dot(f(d), f(e)) (1)
</equation>
<figureCaption confidence="0.992465">
Figure 2: Network structure of fine-tuning stage.
</figureCaption>
<bodyText confidence="0.931694823529412">
Our goal is to rank the correct entity higher
than the rest candidates relative to the context of
the mention. For each training instance (d, e), we
contrast it with one of its negative candidate pair
(d, e&apos;). This gives the pairwise ranking criterion:
G(d, e) = max{0,1 − sim(d, e) + sim(d, e&apos;)1
(2)
Alternatively, we can contrast with all its candi-
date pairs (d, ei). That is, we raise the similarity
score of true pair sim(d, e) and penalize all the
rest sim(d, ei). The loss function is defined as
negative log of softmax function:
d e = to exp sim(d, e)
G 3
(� ) g ( EeiEC(m) exp sim(d, ei) )
Finally, we seek to minimize the following train-
ing objective across all training instances:
</bodyText>
<equation confidence="0.8656695">
G =1: G(d, e) (4)
d,e
</equation>
<bodyText confidence="0.99992032">
The loss function is closely related to con-
trastive estimation (Smith and Eisner, 2005),
which defines where the positive example takes
probability mass from. We find that by penaliz-
ing more negative examples, convergence speed
can be greatly accelerated. In our experiments, the
softmax loss function consistently outperforms
pairwise ranking loss function, which is taken as
our default setting.
However, the softmax training criterion adds
additional computational overhead when per-
forming mini-batch Stochastic Gradient Descent
(SGD). Although we can use a plain SGD (i.e.
mini-batch size is 1), mini-batch SGD is faster to
converge and more stable. Assume the mini-batch
size is m and the number of candidates is n, a total
of m x n forward-backward passes over the net-
work are performed to compute a similarity ma-
trix (Fig. 3), while pairwise ranking criterion only
needs 2xm. We address this problem by grouping
training pairs with same mention m into one mini-
batch {(d, ei)|ei E C(m)1. Observe that if candi-
date entities overlap, they share the same forward-
backward path. Only m + n forward-backward
passes are needed for each mini-batch now.
</bodyText>
<equation confidence="0.789959375">
Python (programming language)
Pythonidae
Python (mythology)
d0
d1
...
=sim(d,e)
e0 e1 e2 en
</equation>
<figureCaption confidence="0.998154">
Figure 3: Sharing path within mini-batch.
</figureCaption>
<bodyText confidence="0.997935">
The re-organization of mini-batch is similar
in spirit to Backpropagation Through Structure
(BTS) (Goller and Kuchler, 1996). BTS is a vari-
ant of the general backpropagation algorithm for
structured neural network. In BTS, parent node
is computed with its child nodes at the forward
pass stage; child node receives gradient as the sum
of derivatives from all its parents. Here (Fig. 2),
parent node is the score node sim(d, e) and child
nodes are f(d) and f(e). In Figure 3, each row
shares forward path of f(d) while each column
shares forward path of f(e). At backpropagation
stage, gradient is summed over each row of score
nodes for f(d) and over each column for f(e).
Till now, our input simply consists of bag-of-
words binary vector. We can incorporate any
handcrafted feature f(d, e) as:
</bodyText>
<equation confidence="0.859461">
sim(d, e) = Dot(f(d), f(e)) + λ��f(d, e) (5)
</equation>
<bodyText confidence="0.998965333333333">
In fact, we find that with only Dot(f(d), f(e))
as ranking score, the performance is sufficiently
good. So we leave this as our future work.
</bodyText>
<figure confidence="0.997241111111111">
sim(d,e)
f(d) f(e)
hidden layer n
stacked auto-encoder
dm
...
... ...
... ...
... ...
</figure>
<page confidence="0.996249">
32
</page>
<sectionHeader confidence="0.995719" genericHeader="evaluation">
3 Experiments and Analysis
</sectionHeader>
<bodyText confidence="0.999844936170213">
Training settings: In pre-training stage, input
layer has 100,000 units, all hidden layers have
1,000 units with rectifier function max(0, x). Fol-
lowing (Glorot et al., 2011), for the first recon-
struction layer, we use sigmoid activation func-
tion and cross-entropy error function. For higher
reconstruction layers, we use softplus (log(1 +
exp(x))) as activation function and squared loss
as error function. For corruption process, we use a
masking noise probability in {0.1,0.4,0.7} for the
first layer, a Gaussian noise with standard devi-
ation of 0.1 for higher layers. For reconstruction
sampling, we set the reconstruction rate to 0.01. In
fine-tuning stage, the final layer has 200 units with
sigmoid activation function. The learning rate is
set to 1e-3. The mini-batch size is set to 20.
We run all our experiments on a Linux ma-
chine with 72GB memory 6 core Xeon CPU. The
model is implemented in Python with C exten-
sions, numpy configured with Openblas library.
Thanks to reconstruction sampling and refined
mini-batch arrangement, it takes about 1 day to
converge for pre-training and 3 days for fine-
tuning, which is fast given our training set size.
Datasets: We use half of Wikipedia 1 plain text
(˜1.5M articles split into sections) for pre-training.
We collect a total of 40M hyperlinks grouped by
name string m for fine-tuning stage. We holdout
a subset of hyperlinks for model selection, and we
find that 3 layers network with a higher masking
noise rate (0.7) always gives best performance.
We select TAC-KBP 2010 (Ji and Grishman,
2011) dataset for non-collective approaches, and
AIDA 2 dataset for collective approaches. For both
datasets, we evaluate the non-NIL queries. The
TAC-KBP and AIDA testb dataset contains 1020
and 4485 non-NIL queries respectively.
For candidate generation, mention-to-entity dic-
tionary is built by mining Wikipedia structures,
following (Cucerzan, 2007). We keep top 30 can-
didates by prominence P(e|m) for speed consid-
eration. The candidate generation recall are 94.0%
and 98.5% for TAC and AIDA respectively.
Analysis: Table 1 shows evaluation results
across several best performing systems. (Han et
al., 2011) is a collective approach, using Person-
alized PageRank to propagate evidence between
</bodyText>
<footnote confidence="0.999955">
1available at http://dumps.wikimedia.org/enwiki/, we use
the 20110405 xml dump.
2available at http://www.mpi-inf.mpg.de/yago-naga/aida/
</footnote>
<bodyText confidence="0.996368809523809">
different decisions. To our surprise, our method
with only local evidence even beats several com-
plex collective methods with simple word similar-
ity. This reveals the importance of context model-
ing in semantic space. Collective approaches can
improve performance only when local evidence is
not confident enough. When embedding our sim-
ilarity measure sim(d, e) into (Han et al., 2011),
we achieve the best results on AIDA.
A close error analysis shows some typical er-
rors due to the lack of prominence feature and
name matching feature. Some queries acciden-
tally link to rare candidates and some link to en-
tities with completely different names. We will
add these features as mentioned in Eq. 5 in future.
We will also add NIL-detection module, which is
required by more realistic application scenarios.
A first thought is to construct pseudo-NIL with
Wikipedia annotations and automatically learn the
threshold and feature weight as in (Bunescu and
Pasca, 2006; Kulkarni et al., 2009).
</bodyText>
<table confidence="0.958436285714286">
Methods micro macro
P@1 P@1
TAC 2010 eval
Lcc (2010) (top1, noweb) 79.22 -
Siel 2010 (top2, noweb) 71.57 -
our best 80.97 -
AIDA dataset (collective approaches)
AIDA (2011) 82.29 82.02
Shirakawa et al. (2011) 81.40 83.57
Kulkarni et al. (2009) 72.87 76.74
wordsim (cosine) 48.38 37.30
Han (2011) +wordsim 78.97 75.77
our best (non-collective) 84.82 83.37
Han (2011) + our best 85.62 83.95
</table>
<tableCaption confidence="0.999957">
Table 1: Evaluation on TAC and AIDA dataset.
</tableCaption>
<sectionHeader confidence="0.989301" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999992">
We propose a deep learning approach that auto-
matically learns context-entity similarity measure
for entity disambiguation. The intermediate rep-
resentations are learned leveraging large scale an-
notations of Wikipedia, without any manual effort
of designing features. The learned representation
of entity is compact and can scale to very large
knowledge base. Furthermore, experiment reveals
the importance of context modeling in this field.
By incorporating our learned measure into collec-
tive approach, performance is further improved.
</bodyText>
<page confidence="0.998744">
33
</page>
<sectionHeader confidence="0.998391" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.7681115">
We thank Nan Yang, Jie Liu and Fei Wang for helpful discus-
sions. This research was partly supported by National High
</bodyText>
<note confidence="0.79123125">
Technology Research and Development Program of China
(863 Program) (No. 2012AA011101), National Natural Sci-
ence Foundation of China (No.91024009) and Major Na-
tional Social Science Fund of China(No. 12&amp;ZD227).
</note>
<sectionHeader confidence="0.998576" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999786145631068">
Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle.
2007. Greedy layer-wise training of deep networks.
Advances in neural information processing systems,
19:153.
R. Bunescu and M. Pasca. 2006. Using encyclope-
dic knowledge for named entity disambiguation. In
Proceedings of EACL, volume 6, pages 9–16.
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on wikipedia data. In Proceedings
of EMNLP-CoNLL, volume 6, pages 708–716.
Y. Dauphin, X. Glorot, and Y. Bengio. 2011.
Large-scale learning of embeddings with recon-
struction sampling. In Proceedings of the Twenty-
eighth International Conference on Machine Learn-
ing (ICML11).
X. Glorot, A. Bordes, and Y. Bengio. 2011. Domain
adaptation for large-scale sentiment classification: A
deep learning approach. In Proceedings of the 28th
International Conference on Machine Learning.
Christoph Goller and Andreas Kuchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In Neural Net-
works, 1996., IEEE International Conference on,
volume 1, pages 347–352. IEEE.
X. Han, L. Sun, and J. Zhao. 2011. Collective en-
tity linking in web text: a graph-based method. In
Proceedings of the 34th international ACM SIGIR
conference on Research and development in Infor-
mation Retrieval, pages 765–774. ACM.
G.E. Hinton, S. Osindero, and Y.W. Teh. 2006. A fast
learning algorithm for deep belief nets. Neural com-
putation, 18(7):1527–1554.
J. Hoffart, M.A. Yosef, I. Bordino, H. F¨urstenau,
M. Pinkal, M. Spaniol, B. Taneva, S. Thater, and
G. Weikum. 2011. Robust disambiguation of
named entities in text. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 782–792. Association for Com-
putational Linguistics.
Heng Ji and Ralph Grishman. 2011. Knowledge
base population: Successful approaches and chal-
lenges. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1148–
1158, Portland, Oregon, USA, June. Association for
Computational Linguistics.
S.S. Kataria, K.S. Kumar, R. Rastogi, P. Sen, and S.H.
Sengamedu. 2011. Entity disambiguation with hier-
archical topic models. In Proceedings of KDD.
S. Kulkarni, A. Singh, G. Ramakrishnan, and
S. Chakrabarti. 2009. Collective annotation of
wikipedia entities in web text. In Proceedings of
the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 457–
466. ACM.
J. Lehmann, S. Monahan, L. Nezda, A. Jung, and
Y. Shi. 2010. Lcc approaches to knowledge base
population at tac 2010. In Proc. TAC 2010 Work-
shop.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambigua-
tion to wikipedia. In Proceedings of the Annual
Meeting of the Association of Computational Lin-
guistics (ACL).
P. Sen. 2012. Collective context-aware topic mod-
els for entity disambiguation. In Proceedings of the
21st international conference on World Wide Web,
pages 729–738. ACM.
M. Shirakawa, H. Wang, Y. Song, Z. Wang,
K. Nakayama, T. Hara, and S. Nishio. 2011. Entity
disambiguation based on a probabilistic taxonomy.
Technical report, Technical Report MSR-TR-2011-
125, Microsoft Research.
N.A. Smith and J. Eisner. 2005. Contrastive estima-
tion: Training log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics, pages 354–
362. Association for Computational Linguistics.
P. Vincent, H. Larochelle, Y. Bengio, and P.A. Man-
zagol. 2008. Extracting and composing robust
features with denoising autoencoders. In Proceed-
ings of the 25th international conference on Ma-
chine learning, pages 1096–1103. ACM.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie,
Yoshua Bengio, and Pierre-Antoine Manzagol.
2010. Stacked denoising autoencoders: Learning
useful representations in a deep network with a local
denoising criterion. The Journal of Machine Learn-
ing Research, 11:3371–3408.
W. Zhang, Y.C. Sim, J. Su, and C.L. Tan. 2011. Entity
linking with effective acronym expansion, instance
selection and topic modeling. In Proceedings of
the Twenty-Second international joint conference on
Artificial Intelligence-Volume Volume Three, pages
1909–1914. AAAI Press.
Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xi-
aoyan Zhu. 2010. Learning to link entities with
knowledge base. In Human Language Technolo-
gies: The 2010 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 483–491, Los Ange-
les, California, June. Association for Computational
Linguistics.
</reference>
<page confidence="0.999319">
34
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.262258">
<title confidence="0.999984">Learning Entity Representation for Entity Disambiguation</title>
<author confidence="0.688673">Shujie Mu Ming Longkai Houfeng</author>
<abstract confidence="0.92805780952381">tKey Laboratory of Computational Linguistics (Peking University) Ministry of tMicrosoft Research zhlongk@qq.com wanghf@pku.edu.cn Abstract We propose a novel entity disambiguation model, based on Deep Neural Network (DNN). Instead of utilizing simple similarity measures and their disjoint combinations, our method directly optimizes document and entity representations for a given similarity measure. Stacked Denoising Auto-encoders are first employed to learn an initial document representation in an unsupervised pre-training stage. A supervised fine-tuning stage follows to optimize the representation towards the similarity measure. Experiment results show that our method achieves state-of-the-art performance on two public datasets without any manually designed features, even beating complex collective approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>P Lamblin</author>
<author>D Popovici</author>
<author>H Larochelle</author>
</authors>
<title>Greedy layer-wise training of deep networks. Advances in neural information processing systems,</title>
<date>2007</date>
<pages>19--153</pages>
<contexts>
<context position="3610" citStr="Bengio et al., 2007" startWordPosition="541" endWordPosition="544">Rather than learning context entity association at word level, topic model based approaches (Kataria et al., 2011; Sen, 2012) can learn it in the semantic space. However, the one-topic-perentity assumption makes it impossible to scale to large knowledge base, as every entity has a separate word distribution P(w|e); besides, the training objective does not directly correspond with disambiguation performances. To overcome disadvantages of previous approaches, we propose a novel method to learn context entity association enriched with deep architecture. Deep neural networks (Hinton et al., 2006; Bengio et al., 2007) are built in a hierarchical manner, and allow us to compare context and entity at some higher level abstraction; while at lower levels, general concepts are shared across entities, resulting in compact models. Moreover, to make our model highly correlated with disambiguation performance, our method directly optimizes doc30 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 30–34, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics ument and entity representations for a fixed similarity measure. In fact, the underlying </context>
<context position="5524" citStr="Bengio et al., 2007" startWordPosition="830" endWordPosition="833">cument d, a list of candidate entities C(m) are generated for m, for each candidate entity ei E C(m), we compute a ranking score sim(dm, ei) indicating how likely m refers to ei. The linking result is e = arg maxez sim(dm, ei). Our algorithm consists of two stages. In the pretraining stage, Stacked Denoising Auto-encoders are built in an unsupervised layer-wise fashion to discover general concepts encoding d and e. In the supervised fine-tuning stage, the entire network weights are fine-tuned to optimize the similarity score sim(d, e). 2.1 Greedy Layer-wise Pre-training Stacked Auto-encoders (Bengio et al., 2007) is one of the building blocks of deep learning. Assume the input is a vector x, an auto-encoder consists of an encoding process h(x) and a decoding process g(h(x)). The goal is to minimize the reconstruction error L(x, g(h(x))), thus retaining maximum information. By repeatedly stacking new auto-encoder on top of previously learned h(x), stacked auto-encoders are obtained. This way we learn multiple levels of representation of input x. One problem of auto-encoder is that it treats all words equally, no matter it is a function word or a content word. Denoising Auto-encoder (DA) (Vincent et al.</context>
</contexts>
<marker>Bengio, Lamblin, Popovici, Larochelle, 2007</marker>
<rawString>Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. 2007. Greedy layer-wise training of deep networks. Advances in neural information processing systems, 19:153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bunescu</author>
<author>M Pasca</author>
</authors>
<title>Using encyclopedic knowledge for named entity disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL,</booktitle>
<volume>6</volume>
<pages>9--16</pages>
<contexts>
<context position="1189" citStr="Bunescu and Pasca, 2006" startWordPosition="153" endWordPosition="156">ity representations for a given similarity measure. Stacked Denoising Auto-encoders are first employed to learn an initial document representation in an unsupervised pre-training stage. A supervised fine-tuning stage follows to optimize the representation towards the similarity measure. Experiment results show that our method achieves state-of-the-art performance on two public datasets without any manually designed features, even beating complex collective approaches. 1 Introduction Entity linking or disambiguation has recently received much attention in natural language processing community (Bunescu and Pasca, 2006; Han et al., 2011; Kataria et al., 2011; Sen, 2012). It is an essential first step for succeeding sub-tasks in knowledge base construction (Ji and Grishman, 2011) like populating attribute to entities. Given a sentence with four mentions, “The [[Python]] of [[Delphi]] was a creature with the body of a snake. This creature dwelled on [[Mount Parnassus]], in central [[Greece]].” How can we determine that Python is an earth-dragon in Greece mythology and not the popular programming language, Delphi is not the auto parts supplier, and Mount Parnassus is in Greece, not in Colorado? A most straight</context>
<context position="14972" citStr="Bunescu and Pasca, 2006" startWordPosition="2373" endWordPosition="2376">rity measure sim(d, e) into (Han et al., 2011), we achieve the best results on AIDA. A close error analysis shows some typical errors due to the lack of prominence feature and name matching feature. Some queries accidentally link to rare candidates and some link to entities with completely different names. We will add these features as mentioned in Eq. 5 in future. We will also add NIL-detection module, which is required by more realistic application scenarios. A first thought is to construct pseudo-NIL with Wikipedia annotations and automatically learn the threshold and feature weight as in (Bunescu and Pasca, 2006; Kulkarni et al., 2009). Methods micro macro P@1 P@1 TAC 2010 eval Lcc (2010) (top1, noweb) 79.22 - Siel 2010 (top2, noweb) 71.57 - our best 80.97 - AIDA dataset (collective approaches) AIDA (2011) 82.29 82.02 Shirakawa et al. (2011) 81.40 83.57 Kulkarni et al. (2009) 72.87 76.74 wordsim (cosine) 48.38 37.30 Han (2011) +wordsim 78.97 75.77 our best (non-collective) 84.82 83.37 Han (2011) + our best 85.62 83.95 Table 1: Evaluation on TAC and AIDA dataset. 4 Conclusion We propose a deep learning approach that automatically learns context-entity similarity measure for entity disambiguation. The </context>
</contexts>
<marker>Bunescu, Pasca, 2006</marker>
<rawString>R. Bunescu and M. Pasca. 2006. Using encyclopedic knowledge for named entity disambiguation. In Proceedings of EACL, volume 6, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
</authors>
<title>Large-scale named entity disambiguation based on wikipedia data.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<volume>6</volume>
<pages>708--716</pages>
<contexts>
<context position="2219" citStr="Cucerzan, 2007" startWordPosition="320" endWordPosition="321"> an earth-dragon in Greece mythology and not the popular programming language, Delphi is not the auto parts supplier, and Mount Parnassus is in Greece, not in Colorado? A most straightforward method is to compare the context of the mention and the definition of candidate entities. Previous work has explored many ways of measuring the relatedness of context ∗Corresponding author d and entity e, such as dot product, cosine similarity, Kullback-Leibler divergence, Jaccard distance, or more complicated ones (Zheng et al., 2010; Kulkarni et al., 2009; Hoffart et al., 2011; Bunescu and Pasca, 2006; Cucerzan, 2007; Zhang et al., 2011). However, these measures are often duplicate or over-specified, because they are disjointly combined and their atomic nature determines that they have no internal structure. Another line of work focuses on collective disambiguation (Kulkarni et al., 2009; Han et al., 2011; Ratinov et al., 2011; Hoffart et al., 2011). Ambiguous mentions within the same context are resolved simultaneously based on the coherence among decisions. Collective approaches often undergo a non-trivial decision process. In fact, (Ratinov et al., 2011) show that even though global approaches can be i</context>
<context position="13533" citStr="Cucerzan, 2007" startWordPosition="2155" endWordPosition="2156">0M hyperlinks grouped by name string m for fine-tuning stage. We holdout a subset of hyperlinks for model selection, and we find that 3 layers network with a higher masking noise rate (0.7) always gives best performance. We select TAC-KBP 2010 (Ji and Grishman, 2011) dataset for non-collective approaches, and AIDA 2 dataset for collective approaches. For both datasets, we evaluate the non-NIL queries. The TAC-KBP and AIDA testb dataset contains 1020 and 4485 non-NIL queries respectively. For candidate generation, mention-to-entity dictionary is built by mining Wikipedia structures, following (Cucerzan, 2007). We keep top 30 candidates by prominence P(e|m) for speed consideration. The candidate generation recall are 94.0% and 98.5% for TAC and AIDA respectively. Analysis: Table 1 shows evaluation results across several best performing systems. (Han et al., 2011) is a collective approach, using Personalized PageRank to propagate evidence between 1available at http://dumps.wikimedia.org/enwiki/, we use the 20110405 xml dump. 2available at http://www.mpi-inf.mpg.de/yago-naga/aida/ different decisions. To our surprise, our method with only local evidence even beats several complex collective methods w</context>
</contexts>
<marker>Cucerzan, 2007</marker>
<rawString>S. Cucerzan. 2007. Large-scale named entity disambiguation based on wikipedia data. In Proceedings of EMNLP-CoNLL, volume 6, pages 708–716.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Dauphin</author>
<author>X Glorot</author>
<author>Y Bengio</author>
</authors>
<title>Large-scale learning of embeddings with reconstruction sampling.</title>
<date>2011</date>
<booktitle>In Proceedings of the Twentyeighth International Conference on Machine Learning (ICML11).</booktitle>
<contexts>
<context position="7279" citStr="Dauphin et al., 2011" startWordPosition="1112" endWordPosition="1115">l must learn to predict it with “python” “mount”, through some hidden unit. The hidden unit may somehow express the concept of Greece mythology. Figure 1: DA and reconstruction sampling. In order to distinguish between a large number of entities, the vocabulary size must be large enough. This adds considerable computational overhead because the reconstruction process involves expensive dense matrix multiplication. Reconstruction sampling keeps the sparse property of matrix multiplication by reconstructing a small subset of original input, with no loss of quality of the learned representation (Dauphin et al., 2011). 2.2 Supervised Fine-tuning This stage we optimize the learned representation (“hidden layer n” in Fig. 2) towards the ranking score sim(d, e), with large scale Wikipedia annotation as supervision. We collect hyperlinks in Wikipedia as our training set {(di, ei, mi)}, where mi is the mention string for candidate generation. The network weights below “hidden layer n” are initialized with the pre-training stage. Next, we stack another layer on top of the learned representation. The whole network is tuned by the final supervised objective. The reason to stack another layer on top of the learned </context>
</contexts>
<marker>Dauphin, Glorot, Bengio, 2011</marker>
<rawString>Y. Dauphin, X. Glorot, and Y. Bengio. 2011. Large-scale learning of embeddings with reconstruction sampling. In Proceedings of the Twentyeighth International Conference on Machine Learning (ICML11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Glorot</author>
<author>A Bordes</author>
<author>Y Bengio</author>
</authors>
<title>Domain adaptation for large-scale sentiment classification: A deep learning approach.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="11807" citStr="Glorot et al., 2011" startWordPosition="1878" endWordPosition="1881"> each column for f(e). Till now, our input simply consists of bag-ofwords binary vector. We can incorporate any handcrafted feature f(d, e) as: sim(d, e) = Dot(f(d), f(e)) + λ��f(d, e) (5) In fact, we find that with only Dot(f(d), f(e)) as ranking score, the performance is sufficiently good. So we leave this as our future work. sim(d,e) f(d) f(e) hidden layer n stacked auto-encoder dm ... ... ... ... ... ... ... 32 3 Experiments and Analysis Training settings: In pre-training stage, input layer has 100,000 units, all hidden layers have 1,000 units with rectifier function max(0, x). Following (Glorot et al., 2011), for the first reconstruction layer, we use sigmoid activation function and cross-entropy error function. For higher reconstruction layers, we use softplus (log(1 + exp(x))) as activation function and squared loss as error function. For corruption process, we use a masking noise probability in {0.1,0.4,0.7} for the first layer, a Gaussian noise with standard deviation of 0.1 for higher layers. For reconstruction sampling, we set the reconstruction rate to 0.01. In fine-tuning stage, the final layer has 200 units with sigmoid activation function. The learning rate is set to 1e-3. The mini-batc</context>
</contexts>
<marker>Glorot, Bordes, Bengio, 2011</marker>
<rawString>X. Glorot, A. Bordes, and Y. Bengio. 2011. Domain adaptation for large-scale sentiment classification: A deep learning approach. In Proceedings of the 28th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Goller</author>
<author>Andreas Kuchler</author>
</authors>
<title>Learning task-dependent distributed representations by backpropagation through structure.</title>
<date>1996</date>
<booktitle>In Neural Networks, 1996., IEEE International Conference on,</booktitle>
<volume>1</volume>
<pages>347--352</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="10661" citStr="Goller and Kuchler, 1996" startWordPosition="1678" endWordPosition="1681">erformed to compute a similarity matrix (Fig. 3), while pairwise ranking criterion only needs 2xm. We address this problem by grouping training pairs with same mention m into one minibatch {(d, ei)|ei E C(m)1. Observe that if candidate entities overlap, they share the same forwardbackward path. Only m + n forward-backward passes are needed for each mini-batch now. Python (programming language) Pythonidae Python (mythology) d0 d1 ... =sim(d,e) e0 e1 e2 en Figure 3: Sharing path within mini-batch. The re-organization of mini-batch is similar in spirit to Backpropagation Through Structure (BTS) (Goller and Kuchler, 1996). BTS is a variant of the general backpropagation algorithm for structured neural network. In BTS, parent node is computed with its child nodes at the forward pass stage; child node receives gradient as the sum of derivatives from all its parents. Here (Fig. 2), parent node is the score node sim(d, e) and child nodes are f(d) and f(e). In Figure 3, each row shares forward path of f(d) while each column shares forward path of f(e). At backpropagation stage, gradient is summed over each row of score nodes for f(d) and over each column for f(e). Till now, our input simply consists of bag-ofwords </context>
</contexts>
<marker>Goller, Kuchler, 1996</marker>
<rawString>Christoph Goller and Andreas Kuchler. 1996. Learning task-dependent distributed representations by backpropagation through structure. In Neural Networks, 1996., IEEE International Conference on, volume 1, pages 347–352. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Han</author>
<author>L Sun</author>
<author>J Zhao</author>
</authors>
<title>Collective entity linking in web text: a graph-based method.</title>
<date>2011</date>
<booktitle>In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval,</booktitle>
<pages>765--774</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1207" citStr="Han et al., 2011" startWordPosition="157" endWordPosition="160"> given similarity measure. Stacked Denoising Auto-encoders are first employed to learn an initial document representation in an unsupervised pre-training stage. A supervised fine-tuning stage follows to optimize the representation towards the similarity measure. Experiment results show that our method achieves state-of-the-art performance on two public datasets without any manually designed features, even beating complex collective approaches. 1 Introduction Entity linking or disambiguation has recently received much attention in natural language processing community (Bunescu and Pasca, 2006; Han et al., 2011; Kataria et al., 2011; Sen, 2012). It is an essential first step for succeeding sub-tasks in knowledge base construction (Ji and Grishman, 2011) like populating attribute to entities. Given a sentence with four mentions, “The [[Python]] of [[Delphi]] was a creature with the body of a snake. This creature dwelled on [[Mount Parnassus]], in central [[Greece]].” How can we determine that Python is an earth-dragon in Greece mythology and not the popular programming language, Delphi is not the auto parts supplier, and Mount Parnassus is in Greece, not in Colorado? A most straightforward method is </context>
<context position="2513" citStr="Han et al., 2011" startWordPosition="364" endWordPosition="367">ork has explored many ways of measuring the relatedness of context ∗Corresponding author d and entity e, such as dot product, cosine similarity, Kullback-Leibler divergence, Jaccard distance, or more complicated ones (Zheng et al., 2010; Kulkarni et al., 2009; Hoffart et al., 2011; Bunescu and Pasca, 2006; Cucerzan, 2007; Zhang et al., 2011). However, these measures are often duplicate or over-specified, because they are disjointly combined and their atomic nature determines that they have no internal structure. Another line of work focuses on collective disambiguation (Kulkarni et al., 2009; Han et al., 2011; Ratinov et al., 2011; Hoffart et al., 2011). Ambiguous mentions within the same context are resolved simultaneously based on the coherence among decisions. Collective approaches often undergo a non-trivial decision process. In fact, (Ratinov et al., 2011) show that even though global approaches can be improved, local methods based on only similarity sim(d, e) of context d and entity e are hard to beat. This somehow reveals the importance of a good modeling of sim(d, e). Rather than learning context entity association at word level, topic model based approaches (Kataria et al., 2011; Sen, 201</context>
<context position="13791" citStr="Han et al., 2011" startWordPosition="2194" endWordPosition="2197">hman, 2011) dataset for non-collective approaches, and AIDA 2 dataset for collective approaches. For both datasets, we evaluate the non-NIL queries. The TAC-KBP and AIDA testb dataset contains 1020 and 4485 non-NIL queries respectively. For candidate generation, mention-to-entity dictionary is built by mining Wikipedia structures, following (Cucerzan, 2007). We keep top 30 candidates by prominence P(e|m) for speed consideration. The candidate generation recall are 94.0% and 98.5% for TAC and AIDA respectively. Analysis: Table 1 shows evaluation results across several best performing systems. (Han et al., 2011) is a collective approach, using Personalized PageRank to propagate evidence between 1available at http://dumps.wikimedia.org/enwiki/, we use the 20110405 xml dump. 2available at http://www.mpi-inf.mpg.de/yago-naga/aida/ different decisions. To our surprise, our method with only local evidence even beats several complex collective methods with simple word similarity. This reveals the importance of context modeling in semantic space. Collective approaches can improve performance only when local evidence is not confident enough. When embedding our similarity measure sim(d, e) into (Han et al., 2</context>
</contexts>
<marker>Han, Sun, Zhao, 2011</marker>
<rawString>X. Han, L. Sun, and J. Zhao. 2011. Collective entity linking in web text: a graph-based method. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages 765–774. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Hinton</author>
<author>S Osindero</author>
<author>Y W Teh</author>
</authors>
<title>A fast learning algorithm for deep belief nets.</title>
<date>2006</date>
<booktitle>Neural computation,</booktitle>
<pages>18--7</pages>
<contexts>
<context position="3588" citStr="Hinton et al., 2006" startWordPosition="537" endWordPosition="540">deling of sim(d, e). Rather than learning context entity association at word level, topic model based approaches (Kataria et al., 2011; Sen, 2012) can learn it in the semantic space. However, the one-topic-perentity assumption makes it impossible to scale to large knowledge base, as every entity has a separate word distribution P(w|e); besides, the training objective does not directly correspond with disambiguation performances. To overcome disadvantages of previous approaches, we propose a novel method to learn context entity association enriched with deep architecture. Deep neural networks (Hinton et al., 2006; Bengio et al., 2007) are built in a hierarchical manner, and allow us to compare context and entity at some higher level abstraction; while at lower levels, general concepts are shared across entities, resulting in compact models. Moreover, to make our model highly correlated with disambiguation performance, our method directly optimizes doc30 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 30–34, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics ument and entity representations for a fixed similarity measure. In</context>
</contexts>
<marker>Hinton, Osindero, Teh, 2006</marker>
<rawString>G.E. Hinton, S. Osindero, and Y.W. Teh. 2006. A fast learning algorithm for deep belief nets. Neural computation, 18(7):1527–1554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hoffart</author>
<author>M A Yosef</author>
<author>I Bordino</author>
<author>H F¨urstenau</author>
<author>M Pinkal</author>
<author>M Spaniol</author>
<author>B Taneva</author>
<author>S Thater</author>
<author>G Weikum</author>
</authors>
<title>Robust disambiguation of named entities in text.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>782--792</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Hoffart, Yosef, Bordino, F¨urstenau, Pinkal, Spaniol, Taneva, Thater, Weikum, 2011</marker>
<rawString>J. Hoffart, M.A. Yosef, I. Bordino, H. F¨urstenau, M. Pinkal, M. Spaniol, B. Taneva, S. Thater, and G. Weikum. 2011. Robust disambiguation of named entities in text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 782–792. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
</authors>
<title>Knowledge base population: Successful approaches and challenges.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1148--1158</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="1352" citStr="Ji and Grishman, 2011" startWordPosition="180" endWordPosition="183">d pre-training stage. A supervised fine-tuning stage follows to optimize the representation towards the similarity measure. Experiment results show that our method achieves state-of-the-art performance on two public datasets without any manually designed features, even beating complex collective approaches. 1 Introduction Entity linking or disambiguation has recently received much attention in natural language processing community (Bunescu and Pasca, 2006; Han et al., 2011; Kataria et al., 2011; Sen, 2012). It is an essential first step for succeeding sub-tasks in knowledge base construction (Ji and Grishman, 2011) like populating attribute to entities. Given a sentence with four mentions, “The [[Python]] of [[Delphi]] was a creature with the body of a snake. This creature dwelled on [[Mount Parnassus]], in central [[Greece]].” How can we determine that Python is an earth-dragon in Greece mythology and not the popular programming language, Delphi is not the auto parts supplier, and Mount Parnassus is in Greece, not in Colorado? A most straightforward method is to compare the context of the mention and the definition of candidate entities. Previous work has explored many ways of measuring the relatedness</context>
<context position="13185" citStr="Ji and Grishman, 2011" startWordPosition="2106" endWordPosition="2109">umpy configured with Openblas library. Thanks to reconstruction sampling and refined mini-batch arrangement, it takes about 1 day to converge for pre-training and 3 days for finetuning, which is fast given our training set size. Datasets: We use half of Wikipedia 1 plain text (˜1.5M articles split into sections) for pre-training. We collect a total of 40M hyperlinks grouped by name string m for fine-tuning stage. We holdout a subset of hyperlinks for model selection, and we find that 3 layers network with a higher masking noise rate (0.7) always gives best performance. We select TAC-KBP 2010 (Ji and Grishman, 2011) dataset for non-collective approaches, and AIDA 2 dataset for collective approaches. For both datasets, we evaluate the non-NIL queries. The TAC-KBP and AIDA testb dataset contains 1020 and 4485 non-NIL queries respectively. For candidate generation, mention-to-entity dictionary is built by mining Wikipedia structures, following (Cucerzan, 2007). We keep top 30 candidates by prominence P(e|m) for speed consideration. The candidate generation recall are 94.0% and 98.5% for TAC and AIDA respectively. Analysis: Table 1 shows evaluation results across several best performing systems. (Han et al.,</context>
</contexts>
<marker>Ji, Grishman, 2011</marker>
<rawString>Heng Ji and Ralph Grishman. 2011. Knowledge base population: Successful approaches and challenges. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1148– 1158, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S S Kataria</author>
<author>K S Kumar</author>
<author>R Rastogi</author>
<author>P Sen</author>
<author>S H Sengamedu</author>
</authors>
<title>Entity disambiguation with hierarchical topic models.</title>
<date>2011</date>
<booktitle>In Proceedings of KDD.</booktitle>
<contexts>
<context position="1229" citStr="Kataria et al., 2011" startWordPosition="161" endWordPosition="164">measure. Stacked Denoising Auto-encoders are first employed to learn an initial document representation in an unsupervised pre-training stage. A supervised fine-tuning stage follows to optimize the representation towards the similarity measure. Experiment results show that our method achieves state-of-the-art performance on two public datasets without any manually designed features, even beating complex collective approaches. 1 Introduction Entity linking or disambiguation has recently received much attention in natural language processing community (Bunescu and Pasca, 2006; Han et al., 2011; Kataria et al., 2011; Sen, 2012). It is an essential first step for succeeding sub-tasks in knowledge base construction (Ji and Grishman, 2011) like populating attribute to entities. Given a sentence with four mentions, “The [[Python]] of [[Delphi]] was a creature with the body of a snake. This creature dwelled on [[Mount Parnassus]], in central [[Greece]].” How can we determine that Python is an earth-dragon in Greece mythology and not the popular programming language, Delphi is not the auto parts supplier, and Mount Parnassus is in Greece, not in Colorado? A most straightforward method is to compare the context</context>
<context position="3103" citStr="Kataria et al., 2011" startWordPosition="462" endWordPosition="465">et al., 2009; Han et al., 2011; Ratinov et al., 2011; Hoffart et al., 2011). Ambiguous mentions within the same context are resolved simultaneously based on the coherence among decisions. Collective approaches often undergo a non-trivial decision process. In fact, (Ratinov et al., 2011) show that even though global approaches can be improved, local methods based on only similarity sim(d, e) of context d and entity e are hard to beat. This somehow reveals the importance of a good modeling of sim(d, e). Rather than learning context entity association at word level, topic model based approaches (Kataria et al., 2011; Sen, 2012) can learn it in the semantic space. However, the one-topic-perentity assumption makes it impossible to scale to large knowledge base, as every entity has a separate word distribution P(w|e); besides, the training objective does not directly correspond with disambiguation performances. To overcome disadvantages of previous approaches, we propose a novel method to learn context entity association enriched with deep architecture. Deep neural networks (Hinton et al., 2006; Bengio et al., 2007) are built in a hierarchical manner, and allow us to compare context and entity at some highe</context>
</contexts>
<marker>Kataria, Kumar, Rastogi, Sen, Sengamedu, 2011</marker>
<rawString>S.S. Kataria, K.S. Kumar, R. Rastogi, P. Sen, and S.H. Sengamedu. 2011. Entity disambiguation with hierarchical topic models. In Proceedings of KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kulkarni</author>
<author>A Singh</author>
<author>G Ramakrishnan</author>
<author>S Chakrabarti</author>
</authors>
<title>Collective annotation of wikipedia entities in web text.</title>
<date>2009</date>
<booktitle>In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>457--466</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2156" citStr="Kulkarni et al., 2009" startWordPosition="308" endWordPosition="311">rnassus]], in central [[Greece]].” How can we determine that Python is an earth-dragon in Greece mythology and not the popular programming language, Delphi is not the auto parts supplier, and Mount Parnassus is in Greece, not in Colorado? A most straightforward method is to compare the context of the mention and the definition of candidate entities. Previous work has explored many ways of measuring the relatedness of context ∗Corresponding author d and entity e, such as dot product, cosine similarity, Kullback-Leibler divergence, Jaccard distance, or more complicated ones (Zheng et al., 2010; Kulkarni et al., 2009; Hoffart et al., 2011; Bunescu and Pasca, 2006; Cucerzan, 2007; Zhang et al., 2011). However, these measures are often duplicate or over-specified, because they are disjointly combined and their atomic nature determines that they have no internal structure. Another line of work focuses on collective disambiguation (Kulkarni et al., 2009; Han et al., 2011; Ratinov et al., 2011; Hoffart et al., 2011). Ambiguous mentions within the same context are resolved simultaneously based on the coherence among decisions. Collective approaches often undergo a non-trivial decision process. In fact, (Ratinov</context>
<context position="14996" citStr="Kulkarni et al., 2009" startWordPosition="2377" endWordPosition="2380">to (Han et al., 2011), we achieve the best results on AIDA. A close error analysis shows some typical errors due to the lack of prominence feature and name matching feature. Some queries accidentally link to rare candidates and some link to entities with completely different names. We will add these features as mentioned in Eq. 5 in future. We will also add NIL-detection module, which is required by more realistic application scenarios. A first thought is to construct pseudo-NIL with Wikipedia annotations and automatically learn the threshold and feature weight as in (Bunescu and Pasca, 2006; Kulkarni et al., 2009). Methods micro macro P@1 P@1 TAC 2010 eval Lcc (2010) (top1, noweb) 79.22 - Siel 2010 (top2, noweb) 71.57 - our best 80.97 - AIDA dataset (collective approaches) AIDA (2011) 82.29 82.02 Shirakawa et al. (2011) 81.40 83.57 Kulkarni et al. (2009) 72.87 76.74 wordsim (cosine) 48.38 37.30 Han (2011) +wordsim 78.97 75.77 our best (non-collective) 84.82 83.37 Han (2011) + our best 85.62 83.95 Table 1: Evaluation on TAC and AIDA dataset. 4 Conclusion We propose a deep learning approach that automatically learns context-entity similarity measure for entity disambiguation. The intermediate representat</context>
</contexts>
<marker>Kulkarni, Singh, Ramakrishnan, Chakrabarti, 2009</marker>
<rawString>S. Kulkarni, A. Singh, G. Ramakrishnan, and S. Chakrabarti. 2009. Collective annotation of wikipedia entities in web text. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 457– 466. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lehmann</author>
<author>S Monahan</author>
<author>L Nezda</author>
<author>A Jung</author>
<author>Y Shi</author>
</authors>
<title>Lcc approaches to knowledge base population at tac 2010. In</title>
<date>2010</date>
<booktitle>Proc. TAC 2010 Workshop.</booktitle>
<marker>Lehmann, Monahan, Nezda, Jung, Shi, 2010</marker>
<rawString>J. Lehmann, S. Monahan, L. Nezda, A. Jung, and Y. Shi. 2010. Lcc approaches to knowledge base population at tac 2010. In Proc. TAC 2010 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ratinov</author>
<author>D Roth</author>
<author>D Downey</author>
<author>M Anderson</author>
</authors>
<title>Local and global algorithms for disambiguation to wikipedia.</title>
<date>2011</date>
<booktitle>In Proceedings of the Annual Meeting of the Association of Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="2535" citStr="Ratinov et al., 2011" startWordPosition="368" endWordPosition="371">any ways of measuring the relatedness of context ∗Corresponding author d and entity e, such as dot product, cosine similarity, Kullback-Leibler divergence, Jaccard distance, or more complicated ones (Zheng et al., 2010; Kulkarni et al., 2009; Hoffart et al., 2011; Bunescu and Pasca, 2006; Cucerzan, 2007; Zhang et al., 2011). However, these measures are often duplicate or over-specified, because they are disjointly combined and their atomic nature determines that they have no internal structure. Another line of work focuses on collective disambiguation (Kulkarni et al., 2009; Han et al., 2011; Ratinov et al., 2011; Hoffart et al., 2011). Ambiguous mentions within the same context are resolved simultaneously based on the coherence among decisions. Collective approaches often undergo a non-trivial decision process. In fact, (Ratinov et al., 2011) show that even though global approaches can be improved, local methods based on only similarity sim(d, e) of context d and entity e are hard to beat. This somehow reveals the importance of a good modeling of sim(d, e). Rather than learning context entity association at word level, topic model based approaches (Kataria et al., 2011; Sen, 2012) can learn it in the</context>
</contexts>
<marker>Ratinov, Roth, Downey, Anderson, 2011</marker>
<rawString>L. Ratinov, D. Roth, D. Downey, and M. Anderson. 2011. Local and global algorithms for disambiguation to wikipedia. In Proceedings of the Annual Meeting of the Association of Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Sen</author>
</authors>
<title>Collective context-aware topic models for entity disambiguation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st international conference on World Wide Web,</booktitle>
<pages>729--738</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1241" citStr="Sen, 2012" startWordPosition="165" endWordPosition="166">sing Auto-encoders are first employed to learn an initial document representation in an unsupervised pre-training stage. A supervised fine-tuning stage follows to optimize the representation towards the similarity measure. Experiment results show that our method achieves state-of-the-art performance on two public datasets without any manually designed features, even beating complex collective approaches. 1 Introduction Entity linking or disambiguation has recently received much attention in natural language processing community (Bunescu and Pasca, 2006; Han et al., 2011; Kataria et al., 2011; Sen, 2012). It is an essential first step for succeeding sub-tasks in knowledge base construction (Ji and Grishman, 2011) like populating attribute to entities. Given a sentence with four mentions, “The [[Python]] of [[Delphi]] was a creature with the body of a snake. This creature dwelled on [[Mount Parnassus]], in central [[Greece]].” How can we determine that Python is an earth-dragon in Greece mythology and not the popular programming language, Delphi is not the auto parts supplier, and Mount Parnassus is in Greece, not in Colorado? A most straightforward method is to compare the context of the ment</context>
<context position="3115" citStr="Sen, 2012" startWordPosition="466" endWordPosition="467">l., 2011; Ratinov et al., 2011; Hoffart et al., 2011). Ambiguous mentions within the same context are resolved simultaneously based on the coherence among decisions. Collective approaches often undergo a non-trivial decision process. In fact, (Ratinov et al., 2011) show that even though global approaches can be improved, local methods based on only similarity sim(d, e) of context d and entity e are hard to beat. This somehow reveals the importance of a good modeling of sim(d, e). Rather than learning context entity association at word level, topic model based approaches (Kataria et al., 2011; Sen, 2012) can learn it in the semantic space. However, the one-topic-perentity assumption makes it impossible to scale to large knowledge base, as every entity has a separate word distribution P(w|e); besides, the training objective does not directly correspond with disambiguation performances. To overcome disadvantages of previous approaches, we propose a novel method to learn context entity association enriched with deep architecture. Deep neural networks (Hinton et al., 2006; Bengio et al., 2007) are built in a hierarchical manner, and allow us to compare context and entity at some higher level abst</context>
</contexts>
<marker>Sen, 2012</marker>
<rawString>P. Sen. 2012. Collective context-aware topic models for entity disambiguation. In Proceedings of the 21st international conference on World Wide Web, pages 729–738. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Shirakawa</author>
<author>H Wang</author>
<author>Y Song</author>
<author>Z Wang</author>
<author>K Nakayama</author>
<author>T Hara</author>
<author>S Nishio</author>
</authors>
<title>Entity disambiguation based on a probabilistic taxonomy.</title>
<date>2011</date>
<tech>Technical report, Technical Report MSR-TR-2011-125, Microsoft Research.</tech>
<contexts>
<context position="15206" citStr="Shirakawa et al. (2011)" startWordPosition="2413" endWordPosition="2416">are candidates and some link to entities with completely different names. We will add these features as mentioned in Eq. 5 in future. We will also add NIL-detection module, which is required by more realistic application scenarios. A first thought is to construct pseudo-NIL with Wikipedia annotations and automatically learn the threshold and feature weight as in (Bunescu and Pasca, 2006; Kulkarni et al., 2009). Methods micro macro P@1 P@1 TAC 2010 eval Lcc (2010) (top1, noweb) 79.22 - Siel 2010 (top2, noweb) 71.57 - our best 80.97 - AIDA dataset (collective approaches) AIDA (2011) 82.29 82.02 Shirakawa et al. (2011) 81.40 83.57 Kulkarni et al. (2009) 72.87 76.74 wordsim (cosine) 48.38 37.30 Han (2011) +wordsim 78.97 75.77 our best (non-collective) 84.82 83.37 Han (2011) + our best 85.62 83.95 Table 1: Evaluation on TAC and AIDA dataset. 4 Conclusion We propose a deep learning approach that automatically learns context-entity similarity measure for entity disambiguation. The intermediate representations are learned leveraging large scale annotations of Wikipedia, without any manual effort of designing features. The learned representation of entity is compact and can scale to very large knowledge base. Fur</context>
</contexts>
<marker>Shirakawa, Wang, Song, Wang, Nakayama, Hara, Nishio, 2011</marker>
<rawString>M. Shirakawa, H. Wang, Y. Song, Z. Wang, K. Nakayama, T. Hara, and S. Nishio. 2011. Entity disambiguation based on a probabilistic taxonomy. Technical report, Technical Report MSR-TR-2011-125, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
<author>J Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>354--362</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9339" citStr="Smith and Eisner, 2005" startWordPosition="1470" endWordPosition="1473">negative candidate pair (d, e&apos;). This gives the pairwise ranking criterion: G(d, e) = max{0,1 − sim(d, e) + sim(d, e&apos;)1 (2) Alternatively, we can contrast with all its candidate pairs (d, ei). That is, we raise the similarity score of true pair sim(d, e) and penalize all the rest sim(d, ei). The loss function is defined as negative log of softmax function: d e = to exp sim(d, e) G 3 (� ) g ( EeiEC(m) exp sim(d, ei) ) Finally, we seek to minimize the following training objective across all training instances: G =1: G(d, e) (4) d,e The loss function is closely related to contrastive estimation (Smith and Eisner, 2005), which defines where the positive example takes probability mass from. We find that by penalizing more negative examples, convergence speed can be greatly accelerated. In our experiments, the softmax loss function consistently outperforms pairwise ranking loss function, which is taken as our default setting. However, the softmax training criterion adds additional computational overhead when performing mini-batch Stochastic Gradient Descent (SGD). Although we can use a plain SGD (i.e. mini-batch size is 1), mini-batch SGD is faster to converge and more stable. Assume the mini-batch size is m a</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>N.A. Smith and J. Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 354– 362. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Vincent</author>
<author>H Larochelle</author>
<author>Y Bengio</author>
<author>P A Manzagol</author>
</authors>
<title>Extracting and composing robust features with denoising autoencoders.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>1096--1103</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6131" citStr="Vincent et al., 2008" startWordPosition="931" endWordPosition="934"> et al., 2007) is one of the building blocks of deep learning. Assume the input is a vector x, an auto-encoder consists of an encoding process h(x) and a decoding process g(h(x)). The goal is to minimize the reconstruction error L(x, g(h(x))), thus retaining maximum information. By repeatedly stacking new auto-encoder on top of previously learned h(x), stacked auto-encoders are obtained. This way we learn multiple levels of representation of input x. One problem of auto-encoder is that it treats all words equally, no matter it is a function word or a content word. Denoising Auto-encoder (DA) (Vincent et al., 2008) seeks to reconstruct x given a random corruption x˜ of x. DA can capture global structure while ignoring noise as the author shows in image processing. In our case, we input each document as a binary bag-of-words vector (Fig. 1). DA will capture general concepts and ignore noise like function words. By applying masking noise (randomly mask 1 with 0), the model also exhibits a fill-in-the-blank property (Vincent et al., 2010): the missing components must be recovered from partial input. Take “greece” for example, the model must learn to predict it with “python” “mount”, through some hidden uni</context>
</contexts>
<marker>Vincent, Larochelle, Bengio, Manzagol, 2008</marker>
<rawString>P. Vincent, H. Larochelle, Y. Bengio, and P.A. Manzagol. 2008. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pages 1096–1103. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Vincent</author>
<author>Hugo Larochelle</author>
<author>Isabelle Lajoie</author>
<author>Yoshua Bengio</author>
<author>Pierre-Antoine Manzagol</author>
</authors>
<title>Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.</title>
<date>2010</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>11--3371</pages>
<contexts>
<context position="6560" citStr="Vincent et al., 2010" startWordPosition="1002" endWordPosition="1005">epresentation of input x. One problem of auto-encoder is that it treats all words equally, no matter it is a function word or a content word. Denoising Auto-encoder (DA) (Vincent et al., 2008) seeks to reconstruct x given a random corruption x˜ of x. DA can capture global structure while ignoring noise as the author shows in image processing. In our case, we input each document as a binary bag-of-words vector (Fig. 1). DA will capture general concepts and ignore noise like function words. By applying masking noise (randomly mask 1 with 0), the model also exhibits a fill-in-the-blank property (Vincent et al., 2010): the missing components must be recovered from partial input. Take “greece” for example, the model must learn to predict it with “python” “mount”, through some hidden unit. The hidden unit may somehow express the concept of Greece mythology. Figure 1: DA and reconstruction sampling. In order to distinguish between a large number of entities, the vocabulary size must be large enough. This adds considerable computational overhead because the reconstruction process involves expensive dense matrix multiplication. Reconstruction sampling keeps the sparse property of matrix multiplication by recons</context>
</contexts>
<marker>Vincent, Larochelle, Lajoie, Bengio, Manzagol, 2010</marker>
<rawString>Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. 2010. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. The Journal of Machine Learning Research, 11:3371–3408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Zhang</author>
<author>Y C Sim</author>
<author>J Su</author>
<author>C L Tan</author>
</authors>
<title>Entity linking with effective acronym expansion, instance selection and topic modeling.</title>
<date>2011</date>
<booktitle>In Proceedings of the Twenty-Second international joint conference on Artificial Intelligence-Volume Volume Three,</booktitle>
<pages>1909--1914</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="2240" citStr="Zhang et al., 2011" startWordPosition="322" endWordPosition="325"> in Greece mythology and not the popular programming language, Delphi is not the auto parts supplier, and Mount Parnassus is in Greece, not in Colorado? A most straightforward method is to compare the context of the mention and the definition of candidate entities. Previous work has explored many ways of measuring the relatedness of context ∗Corresponding author d and entity e, such as dot product, cosine similarity, Kullback-Leibler divergence, Jaccard distance, or more complicated ones (Zheng et al., 2010; Kulkarni et al., 2009; Hoffart et al., 2011; Bunescu and Pasca, 2006; Cucerzan, 2007; Zhang et al., 2011). However, these measures are often duplicate or over-specified, because they are disjointly combined and their atomic nature determines that they have no internal structure. Another line of work focuses on collective disambiguation (Kulkarni et al., 2009; Han et al., 2011; Ratinov et al., 2011; Hoffart et al., 2011). Ambiguous mentions within the same context are resolved simultaneously based on the coherence among decisions. Collective approaches often undergo a non-trivial decision process. In fact, (Ratinov et al., 2011) show that even though global approaches can be improved, local method</context>
</contexts>
<marker>Zhang, Sim, Su, Tan, 2011</marker>
<rawString>W. Zhang, Y.C. Sim, J. Su, and C.L. Tan. 2011. Entity linking with effective acronym expansion, instance selection and topic modeling. In Proceedings of the Twenty-Second international joint conference on Artificial Intelligence-Volume Volume Three, pages 1909–1914. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhicheng Zheng</author>
<author>Fangtao Li</author>
<author>Minlie Huang</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Learning to link entities with knowledge base.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>483--491</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="2133" citStr="Zheng et al., 2010" startWordPosition="304" endWordPosition="307">welled on [[Mount Parnassus]], in central [[Greece]].” How can we determine that Python is an earth-dragon in Greece mythology and not the popular programming language, Delphi is not the auto parts supplier, and Mount Parnassus is in Greece, not in Colorado? A most straightforward method is to compare the context of the mention and the definition of candidate entities. Previous work has explored many ways of measuring the relatedness of context ∗Corresponding author d and entity e, such as dot product, cosine similarity, Kullback-Leibler divergence, Jaccard distance, or more complicated ones (Zheng et al., 2010; Kulkarni et al., 2009; Hoffart et al., 2011; Bunescu and Pasca, 2006; Cucerzan, 2007; Zhang et al., 2011). However, these measures are often duplicate or over-specified, because they are disjointly combined and their atomic nature determines that they have no internal structure. Another line of work focuses on collective disambiguation (Kulkarni et al., 2009; Han et al., 2011; Ratinov et al., 2011; Hoffart et al., 2011). Ambiguous mentions within the same context are resolved simultaneously based on the coherence among decisions. Collective approaches often undergo a non-trivial decision pro</context>
</contexts>
<marker>Zheng, Li, Huang, Zhu, 2010</marker>
<rawString>Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xiaoyan Zhu. 2010. Learning to link entities with knowledge base. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 483–491, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>