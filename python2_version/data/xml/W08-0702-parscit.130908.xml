<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.998347">
Bayesian Learning Over Conflicting Data:
Predictions for language change
</title>
<author confidence="0.991444">
Rebecca Morley
</author>
<affiliation confidence="0.898947">
Cognitive Science Department
Johns Hopkins University
3400 N. Charles St.
</affiliation>
<address confidence="0.91549">
Baltimore, MD 21218
</address>
<email confidence="0.995364">
morley@cogsci.jhu.edu
</email>
<sectionHeader confidence="0.998572" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9993168">
This paper is an analysis of the claim that a
universal ban on certain (‘anti-markedness’)
grammars is necessary in order to explain their
non-occurrence in the languages of the world.
To assess the validity of this hypothesis I ex-
amine the implications of one sound change (a
&gt; o) for learning in a specific phonological
domain (stress assignment), making explicit
assumptions about the type of data that results,
and the learning function that computes over
that data. The preliminary conclusion is that
restrictions on possible end-point languages
are unneeded, and that the most likely outcome
of change is a lexicon that is inconsistent with
respect to a single generating rule.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975907407408">
The basic tenet of Evolutionary Phonology is that
the observed universal commonalities in
phonological systems of the world arise from the
universal commonality of the way listeners and
speakers produce and perceive sound structures
(Blevins, 2004). Diachronic processes operating
via the transmission of the speech signal act with-
out regard for the subsequent system they create.
Alternate theories in the tradition of Chomsky ar-
gue for universal prohibitions which would serve
to ban or repair certain changes just in case they
would result in a ‘disallowed’ system (Kiparsky
2004, 2006). In Optimality Theoretic terms, this
would be a grammar that violates the canonical set
of universal markedness constraints. I will call this
claim the Universal-Grammar-Delimited Hypothe-
sis Space (UG-Delimited H) Principle.
Without this check, Kiparsky argues, common
and natural sound changes (‘blind’ Evolutionary
Phonology) would frequently produce unnatural
and in fact unobserved ‘anti-markedness’ lan-
guages (such as a system in which lower sonority
vowels were stressed in preference to higher so-
nority vowels).
An analysis of the properties of possible
grammars is an analysis that involves explicitly
characterizing the properties of the learner, as well
as of the data to which the learner is exposed. The
work in this paper is, to my knowledge, the first
attempt to do exactly this kind of analysis, for ex-
actly the type of scenario in which a dispreferred,
but hypothetically learnable, grammar might arise.
Diachronic changes that are caused by factors
outside of the grammar have the capability of dis-
rupting a categorical rule system, introducing ir-
regularities into a previously regular pattern. These
irregularities may have an ‘unnatural’ or anti-
markedness character, but typically, they will co-
exist alongside remnants of the prior natural pat-
tern. That is the first observation. The second is
that if learners are allowed to adopt mixed-
grammar hypotheses (‘co-phonologies’ (Inkelas
1997), ‘stratal faithfulness’ (Ito and Mester 2001),
‘lexical indexation’ (Pater 2000)), then under a
posterior-maximizing learning model, these hybrid
systems are the most likely outcome (rather than a
categorical ‘anti-markedness’ grammar).
I will work through a case study of sonority
sensitive stress, paying special attention to the
lexicon that would be produced after a hypothetical
sound change of the type Kiparsky proposes. By
examining the output of Bayesian hypothesis test-
ing in this domain I will conclude that for the pure
anti-markedness grammar to arise, not only is a
</bodyText>
<page confidence="0.951465">
2
</page>
<note confidence="0.834738">
Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, pages 2–11,
Columbus, Ohio, USA June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999868428571429">
certain type of diachronic change necessary, but
also a certain type of non-uniform lexical distribu-
tion. To first approximation, this confluence of
circumstances appears rather rare, leading me to
tentatively reject the hypothesis that categorical
bans on allowed grammars are necessary to explain
the distribution of the world’s languages.
</bodyText>
<sectionHeader confidence="0.987812" genericHeader="method">
2 Gujarati Phonology
</sectionHeader>
<bodyText confidence="0.999329222222222">
Kiparsky uses Gujarati to provide a concrete illus-
tration of the relevant phonological paradigm: a
sonority-sensitive stress system that respects the
posited universal implicational hierarchy. There
are eight vowels in Gujarati, corresponding to three
sonority tiers: low: (), mid: (i,e,,o,,u), and high:
(a). The stress system is described as conforming
to the following position- and sonority- dependent
rules.
</bodyText>
<listItem confidence="0.9835059">
[1] GUJARATI: Sonority &amp; Position -to-Stress
• stress penultimate [a] (the most sonorous vowel)
• otherwise stress ante-penultimate [a]
• otherwise stress final [a]
• otherwise stress penultimate mid-sonority vowel
(any of [i,e,,o,,u])
• otherwise stress ante-penultimate mid-sonority
vowel
• otherwise stress the penultimate position (which
must be [] (the lowest sonority vowel))
</listItem>
<bodyText confidence="0.999846222222222">
This type of system is easily describable within
a standard OT framework (Prince and Smolensky
1993/2004) that utilizes a universally ordered so-
nority scale with respect to the markedness of (or
dispreference for) stressing a particular vowel.
Crucially, however, the reverse type of system, in
which lower sonority vowels are the ones that at-
tract stress, is so far unattested, and predicted,
within the same framework, to be impossible.
</bodyText>
<subsectionHeader confidence="0.782776">
2.1 Gujarati′
</subsectionHeader>
<bodyText confidence="0.969043">
In stating his claim about the necessity of intrinsic
bans on possible grammars, Kiparsky makes the
following assumption: A common and natural type
of sound change is one in which all a’s of a lan-
guage change to ’s 1. I will adopt this assumption
</bodyText>
<footnote confidence="0.795404">
1 In fact, it is not clear how likely an internally motivated la n-
guage change of a completely general nature is. What might
</footnote>
<bodyText confidence="0.999450047619048">
as well for the sake of argument, leaving aside a
discussion of the evidence for how plausible it may
be. It should be kept in mind that this particular
change is being considered only as a stand-in for a
class of possible sound changes that could produce
similar outcomes with respect to markedness im-
plications.
A change in vowel quality (with unchanged
stress placement) will alter the make-up of the
Gujarati lexicon, and raise the possibility of a sys-
tem in which stress preferentially falls on the low-
est-sonority vowel, [] (formerly [a], the most so-
norous vowel)2. This new lexicon will, in turn, act
as the input to the learner of Gujarati′. To deter-
mine the outcome of learning over this data set,
some sort of characterization of the learner’s hy-
pothesis space is necessary. The list in [2] repre-
sents the full hypothesis set considered in this pa-
per3. To begin, I will consider only hypotheses 1)-
3), leaving aside the discussion of hypotheses 4)
and 5) until Section 3.3.
</bodyText>
<listItem confidence="0.982343555555556">
[2] H :Hypothesis Space
1) PENULT: Stress Penultimate Vowel
2) GUJARATI: Sonority &amp; Position -to-Stress
3) GUJARATI*: Reversed-Sonority &amp; Position -to-
Stress4
4) NULL(G*/G): GUJARATI* and G UJARATI equally
likely generators of data
5) MAX(G*/G): mixed-grammar of GUJARATI* and
GUJARATI with variable weights
</listItem>
<bodyText confidence="0.996930928571429">
be more plausible is that such changes would depend very
heavily on context, with tokens that were less fully realized
(e.g., shorter) being more likely to undergo the change than
more fully /a/-like tokens. This, of course, would be corre-
lated with their stress status.
2 An alternative traditional generativist account, rather than
admitting an anti-markedness hypothesis, might propose a
difference between stress-attracting ’s and non-stress-
attracting ’s based on differences in their underlying repre-
sentations, effectively encoding the diachronic change within
the synchronic grammar. This type of analytic bias will im-
pede or prevent changes from affecting the rule system
(grammar) of a language, and thus it is not pursued in the
present work.
</bodyText>
<footnote confidence="0.90157625">
3 This is clearly far from the only way in which the learning
problem can be formulated. Given that this is, to my knowl-
edge, the first study of its kind, a number of somewhat arbi-
trary representational decisions had to be made. For the pur-
poses of this work the given H-space is the result of what I
view as a minimal departure from the standard formalisms
both of linguistic theory and Bayesian learning.
4 As in [1], but with the sonority classes reversed.
</footnote>
<page confidence="0.993946">
3
</page>
<subsectionHeader confidence="0.970183">
2.2 Evidence to the Learner: Gujarati Lexicon
</subsectionHeader>
<bodyText confidence="0.999749826086957">
The hypothetical lexicon of Gujarati′ (L′) depends
on the inventory of the old Gujarati (L). For a
given possible Gujarati, L is mapped to L′ via the
sound change a &gt; . To construct the space of L I
start by making a list of all possible word types,
where the type depends on features that are rele-
vant to the hypotheses under consideration, namely
the vowel identities. This listing also corresponds
to a particular lexicon LmU ∈ L; this is the word
inventory under what I will call the Minimal Lexi-
con Uniformity assumption: that all types are rep-
resented in equal numbers, and each type occurs
exactly once. For 3-syllable words and an 8 vowel
inventory, there are 83, or 512 distinct types. For
2-syllable words, there are 82, or 64 types.
Table 1 lists the word types for 3-syllable
words. ‘Case’ refers to the type (vowel make-up)
of the word before the hypothetical sound change
(where M indicates any of the mid-sonority vowel
class {i,e,,o,,u}). We will restrict ourselves for
the moment to considering only the first three hy-
potheses in the space: PENULT(P), GUJARATI(G),
and GUJARATI*(G*)).
</bodyText>
<figure confidence="0.735409941176471">
Case Gujarati Example # types
L &gt; L′ H
1 (,(,M),a) [pika]&gt;[prik] 21
(M,,a)
(a,,M)
(a,,(,a))
2 (M,M,a) [hoija]&gt;[hoijr] 84
(a,M,(M,a,)) G*
3 (M,a,(,M,a)) [mubak]&gt;[mubk] 48
G*,P
4 ((,M), M,) [tumot]&gt;[tumot] 78
(,M,M) G,P
5 (M,,M) [kojldi]&gt;[kojldi] 42
(M,,) G
6 (a,a,(a,M)) [awana]&gt;[wn] 239
(,(a,),(a,,M)) G,G*,P
(M,M,M)
</figure>
<tableCaption confidence="0.9924035">
Table 1. Uniform Gujarati Lexicon: three-syllable
words (words taken from de Lacy (2006))
</tableCaption>
<bodyText confidence="0.998939">
Each row represents positive evidence for
some subset of the three hypotheses under consid-
eration; the hypotheses consistent with a given
case are specified in the last column below the type
counts. For example, in Row 3, the word
[mubak] in Gujarati, with stress determined by
the markedness-abiding grammar described in [1]
has become [mubk] in Gujarati′. This form
now exhibits stress on the lowest (rather than the
highest) sonority vowel in the word. This pattern
is consistent with the anti-markedness grammar
GUJARATI*. However, the stress placement in this
word is also consistent with the simple positional
grammar PENULT. If we indicate the number of
types that support none of the hypotheses as A
(=arbitrary), and the number that support all hy-
potheses as N (= neutral), then we can calculate the
total type counts in support of each hypothesis
(A=21; G*=371; G=359; N=239; P=365;
T=512). Note that G* exceeds P by six word
types.
</bodyText>
<sectionHeader confidence="0.987904" genericHeader="method">
3 The Bayesian Learner
</sectionHeader>
<bodyText confidence="0.999988857142857">
The numbers in Table 1 represent the make-up of a
possible lexicon of Gujarati′, namely, LmU′. This
will act as the initial input to our Bayesian learner
(for simplicity, all calculations in this section will
be performed only for 3-syllable words).
The Bayesian model has been extensively ap-
plied to learning scenarios in a number of cognitive
domains (e.g., Chater et al., 2006; Kemp et al.,
2007; Kording and Wolpert, 2006; Tenenbaum et
al., 2007), and involves a fairly minimal and intui-
tive apparatus. Bayes theorem, which provides a
formula for computing the posterior probability of
a hypothesis given the data, and thus a method for
evaluating competing grammars, is given in (1).
</bodyText>
<equation confidence="0.9980905">
p(h  |d) = p(d  |h)p(h) (1)
p(d)
</equation>
<bodyText confidence="0.990396">
For the problem at hand, the members of d are
stress assignments corresponding to each of the n
words of the lexicon. The conditional probability
€
of a stress assignment di under hypothesis h is
more properly written as p(di|h,yi), where stress
assignment (as can be seen from Table 1) depends
on the particular word type yi. I will assume that
the conditional probability of each surface stressed
form is independent of any other. The probability
of the set d given h and y (where h = GUJARATI*,
PENULT, or GUJARATI) can then be expanded as the
product of the probability of each member of d
</bodyText>
<page confidence="0.989402">
4
</page>
<bodyText confidence="0.903049">
given h and each member of y (see Equation (2)). spect to GUJARATI* is approximately .14. Using
this value in Equation (2) we find that GUJARATI*α
</bodyText>
<subsectionHeader confidence="0.613601">
3.1 ‘Non-Deterministic’ Hypothesis Space wins out over both GUJARATIα and PENULTα by
</subsectionHeader>
<bodyText confidence="0.9925628125">
several orders of magnitude:
Applying Bayes Theorem to the first three hy-
potheses of [2] returns a value of p(h|d)=0 for each
grammar. To avoid this collapse (due to the exis-
tence of contradictory data), let us assign a small
probability of error (2α) under each hypothesis.
For a given 3-syllable word type, y, there are three
€
stress possibilities: C = {1,2,3}, and the stress class
assigned by a given hypothesis Hi is written as a
function of the input word type: Hi(y) ∈ C. For the
Non-Deterministic version of the same hypothesis,
written as Hiα, stress will be assigned to the con-
sistent position (c=Hi(y)) with probability 1-2α,
and to either of the two inconsistent positions with
probability α. See [3].
</bodyText>
<equation confidence="0.989647">
1−2α c=Hi(y)
α c ; eHi(y)
</equation>
<bodyText confidence="0.998312307692308">
We are assessing the consequences of learning
with no markedness biases, so we will let the prior
�
probability in Equation (1) be uniform over the
hypothesis space. Since we are concerned with the
winner in any two-hypothesis competition, we will
work with the ratio of their posteriors. Here the
hypotheses GUJARATI*α, GUJARATIα and PENULTα
are the Non-Deterministic counterparts of the pre-
viously introduced hypotheses of the same names,
and the numerical values of G*, P and T are ex-
tracted from Table 1, under LmU′ (and given at the
end of Section 2.1).
</bodyText>
<equation confidence="0.9390265">
∏ p(d, |GUJARATI*α , y,)
,
∏ p(d,  |PENULTα ,y,)
,
</equation>
<bodyText confidence="0.999976609756098">
As we can see from Equation (2), the relative
probability advantage is highly dependent on the
magnitude of α. Since α is an error term, it should
remain relatively small. Within this constraint, we
could allow the learner to fit this parameter based
on maximizing hypothesis likelihood. For the 3-
syllable uniform lexicon, αmL computed with re-
This initial result seems to provide strong sup-
port for The UG-Delimited H Principle: the
GUJARATI* grammar seems overwhelmingly likely
€to arise, and yet is unattested. However, it is in-
structive to consider the inherent sensitivity of the
Bayesian learner to quite small differences be-
tween the linguistic hypotheses in question. A dis-
crepancy between data coverage of a mere 6
words, as seen in the above case, can lead to a hy-
pothesis advantage of four orders of magnitude.
And, in fact, a discrepancy of even 1 word can give
a posterior advantage on the order of a factor of 5
or greater (depending on the value of α). This re-
sult is the consequence of the extreme probability
distribution over only two types of data (consistent
and inconsistent -- with values close to 1 in the
first case, and close to 0 in the second). Since the
probability of an independent collection of out-
comes (a particular input lexicon) is computed via
multiplication, each additional difference in data
coverage compounds the single point case, such
that the ratio grows exponentially.
If this behavior is indeed a problem for our
linguistic domain (where different sub-regions of
phonological regularity are often observed to co-
exist stably in natural language (Inkelas 1997))
then there are various means at our disposal to
modify the learning model. In the following sec-
tion I will consider an alternative weighted deci-
sion metric; in Section 3.3 I will expand the hy-
pothesis space to include mixed-grammar com-
petitors; and in Section 4 I will alter the parameters
of the learning rule to provide a more stringent
threshold for success in hypothesis competition.
</bodyText>
<subsectionHeader confidence="0.998285">
3.2 Optimal Bayes Classifier
</subsectionHeader>
<bodyText confidence="0.999444">
So far, we have been implicitly assuming a winner-
take-all classification strategy whereby the hy-
pothesis with the highest likelihood given the data
is the one selected by the learner, and all others
discarded. Let us now consider, instead, the Opti-
mal Bayes Classifier which categorizes new in-
stances of data by taking a weighted sum of the
</bodyText>
<equation confidence="0.944764947368421">
[3] Hiα : Non-Deterministic Version of Hi
�
�
�
p(c  |Hiα,y) =
p(GUJARATI*α  |d)
p(PENULTα  |d)
αT−c* (1− 2α)c*
=(2)
∏ (1−2α)
α ∏
[di ≠c* (yi )][di =c* (yi )]
∏ (1−2α)
α ∏
[di ≠P (yi )]
αT−P(1− 2α)P
[di-P(yi )]
p(G* |d) ≈1.85×104; p(G* |d) ≈ 3.4 ×108 .
p(P  |d) p(G  |d)
</equation>
<page confidence="0.897517">
5
</page>
<bodyText confidence="0.986155333333333">
predictions of all hypotheses in the space.
As expressed in Equation (3), the probability
that a new word y will be assigned to category cm
(stress syllable m), given the body of training data
d — p(cm|d,y) — is the weighted sum of the prob-
ability each hypothesis gives of cm classification —
p(cm|Hs,y) — where each of these terms is
weighted by the a posteriori probability of the par-
ticular hypothesis given the training data, p(Hs  |d).
</bodyText>
<equation confidence="0.978671">
p(cm  |d,y) = p(cm
∑  |Hs,y) p(Hs  |d) (3)
H
s
</equation>
<bodyText confidence="0.9997624">
Consider now the situation where there are
three hypotheses in the space: Hiα, Hjα, and Hkα.
The formulation of the selector function in Equa-
tion (3) allows for the possibility of a ganging-up
effect whereby Hjα and Hkα, even if they individu-
ally have lower posterior probability over d than
does Hiα, can act together to influence the classifi-
cation of a new data point y. We can choose the
lexicon in this example so as to showcase the larg-
est possible effect these two subordinate rules
could have by making the difference in consistent
data between the (deterministic) hypotheses as
small as possible, such that Hi has a coverage ad-
vantage of only one data point over both Hj and Hk.
We will also consider those words for which Hj
and Hk differ from the classification predicted by
Hi (Hi(y)=c1), but agree with one another in se-
lecting c2 with the highest probability (Hj(y)=
Hk(y)= c2).
From Equation (2), with G*-P=1,
</bodyText>
<equation confidence="0.9705815">
p(H&apos;k  |d) = α p(Hiα  |d) (4a)
1−2α
</equation>
<bodyText confidence="0.966481333333333">
Substituting (4a) into Equation (3) gives the prob-
ability that classification will occur in line with the
€ dominant hypothesis Hi:
</bodyText>
<equation confidence="0.98344725">
α
p(c1  |d,y) = (1− 2α)P(Hi α  |d)+α
1− 2α
P(Hiα  |d) (4b)
</equation>
<bodyText confidence="0.993265375">
And the probability that classification will occur in
line with the subordinate, but mutually reinforcing,
Hj and Hk can be calculated similarly.
€
The ratio of the probability of categorizing the
new item consistently with Hi to that of categoriz-
ing consistently with Hj and Hk can then be shown
to be
</bodyText>
<equation confidence="0.94262625">
6α2 − 4α +1 (5)
3α(1− 2α)
Now take Hi = GUJARATI*, Hj= GUJARATI, and Hk
= PENULT; y is a new word of the type in Row 4 of
</equation>
<tableCaption confidence="0.950684">
Table 1. The gang-up phenomenon, where
</tableCaption>
<bodyText confidence="0.9444545">
�
GUJARATI and PENULT collude to move stress away
from the position preferred by GUJARATI*, may be
seen to have any kind of appreciable effect (where
</bodyText>
<equation confidence="0.9995355">
p(c1  |d,y)
p(c2  |d,y)
</equation>
<bodyText confidence="0.99997245">
(relatively large values for α). Outside of this re-
gion GUJARATI* dominates. And keep in mind, the
advantage to GUJARATI* only gets higher for larger
differences in coverage (in Equation (5) only a sin-
gle data point separates the three hypotheses), and
for instances of lexical items where GUJARATI and
PENULT disagree (Row 5 of Table 1).
So far we have seen that the Bayesian frame-
work exhibits a potential over-sensitivity when
applied to problems of the type formulated in this
paper: learning over a space of quasi-categorical,
contradictory hypotheses. This is true whether we
consider learning to result in a single winner-take-
all hypothesis, or instead opt for the weighted deci-
sion metric of the Optimal Bayes Classifier. We
will return to this issue in Section 4. First, how-
ever, I will expand the hypothesis space under con-
sideration, in Section 3.3, and introduce, in Section
3.4, a non-uniform prior, adding principled biases
on the selection of those different hypotheses.
</bodyText>
<subsectionHeader confidence="0.97386">
3.3 Mixed-Grammar Hypotheses
</subsectionHeader>
<bodyText confidence="0.9999355">
Before we can assess the performance of the Baye-
sian learner with respect to the UG-Delimited H
Principle we must make sure we consider all po-
tential competitor hypotheses that might be better
predictors of the data than those examined so far.
In particular, it is instructive to introduce some-
thing like a class of null hypotheses: hybrid gram-
mars which explicitly encode equality between any
pair of competing alternatives’ ability to explain
the data5.
</bodyText>
<footnote confidence="0.766128833333333">
5 The effect of mixed-grammar hypotheses can also be
realized by allowing a selection procedure over a set of simple
grammars, as described in Section 3.2, but, crucially, with the
weights calculated under the assumption that data are
generated by a combination of grammars (see, for example,
the variational model proposed by Yang (1999), or the
</footnote>
<equation confidence="0.998652285714286">
P(Hiα  |d)
α
+ α
1− 2α
p(c1  |d,y)
p(c2  |d,y)
≤1.5) only in the region .17 &lt; α &lt; .4
</equation>
<page confidence="0.977805">
6
</page>
<bodyText confidence="0.9999532">
I define this class as follows: the posterior
probability that the hypothesis NULL(i/j)α assigns to
a stress class c is calculated by allotting equal
probability to selecting the Hiα or the Hjα rule to
produce an output of that class:
</bodyText>
<equation confidence="0.70061">
p(c  |NULL(i / j)&amp;quot; ,y) = wi p(c  |Hi&amp;quot; ,y) + wj p(c  |H j&amp;quot; ,y) (6)
</equation>
<bodyText confidence="0.99997025">
where wi = wj = .5. From Equation (6) and the
definition in [3], we can compute the probability
distribution of stress assignment c given the appli-
cation of NULL(i/j)α to a particular word, y
</bodyText>
<equation confidence="0.9710684">
[4] NULL(i/j)α: ‘Null Hypothesis’
1− 2α c = Hi (y) = H j (y)
1−α
2
α c ;d Hi (y) &amp;c ;d H j (y)
</equation>
<bodyText confidence="0.9998243125">
It can be shown that, for LMU′ (the Gujarati′
lexicon generated from the Gujarati minimum uni-
form lexicon), the null hypothesis, NULL(G*/G)α,
is the decisive winner over GUJARATI*α (by ap-
proximately 30 orders of magnitude). With this
broader consideration of the hypothesis space, the
anti-markedness grammar is no longer the outcome
of learning. And it turns out that we can specify
another hypothesis that gives an even higher likeli-
hood over the data.
The ‘maximum likelihood’ hypotheses are
specified by allowing all three parameters (wi, wj,
and α (now σ)) in Equation (6) to be estimated
from the data. MAX(i/j)σ is defined explicitly below
in [5] for any given weighted combination of Hiσ
and Hjσ.
</bodyText>
<equation confidence="0.989356">
[5] MAX(i/j)σ: ‘Maximum Likelihood’
p(c  |MAX(i / j)&amp;quot; ,y) =
{ (w, + wj)(1− 2σ) c = H, (y) = H j (y)
(1− 2σ)w, +σwj c = H, (y) &amp;c ≠ H j (y)
(1− 2σ)wj +σw, c = H j (y) &amp;c ≠ H, (y)
(w, + wj)σ c ≠ H, (y) &amp;c ≠ H j (y)
</equation>
<bodyText confidence="0.963886666666667">
When Hi = GUJARATI* and Hj = GUJARATI,
MAX(G*/G)σ assigns the highest posterior of any
we have seen so far (approximately 56 orders of
magnitude larger than G*). This is because, within
the space of candidates, it gives the highest likeli-
hood to the observed data, and the prior probability
</bodyText>
<note confidence="0.415657">
probabilistic version of Optimality Theory over rankings
utilized by Jarosz (2006)).
</note>
<bodyText confidence="0.998982">
(assumed so far to be uniform) plays no role in this
calculation. As the hypotheses we are considering
become more complicated, however, we are led to
consider an alternative to this assumption, one in
which hypotheses with longer description lengths,
or greater complexity, are penalized (Rissanen
1989).
</bodyText>
<subsectionHeader confidence="0.5451935">
3.4 Non-Uniform Prior: Hypothesis Description
Length
</subsectionHeader>
<bodyText confidence="0.999709583333333">
Under the uniform prior assumption, only with a
lexicon in which GUJARATI* accounts for at least
44 times as much data as does GUJARATI will
MAX(G*/G)σ be defeated. In this section I will
show how that result would be altered by consid-
ering a better approximation to the prior probabil-
ity distribution over those hypotheses. MAX(G*/G)σ
and GUJARATI*α can be seen to differ in a basic
way related to the number of parameters and rules
they must each keep track of. A domain-
independent means of determining a prior prob-
ability based on this difference in size, or com-
plexity, can be found in the information theoretic
notion of coding cost, or description length.
Each hypothesis uses a particular labeling
strategy to encode the input data (which can be
quantified by the number of binary pieces of in-
formation, or bits needed to transmit that informa-
tion to a waiting decoder). In addition, a certain
number of bits is needed to encode the hypothesis
itself. The total description length for a string (or
set of data) d and a particular hypothesis H is given
by the following general formula for two-part
coding.
</bodyText>
<equation confidence="0.998222">
L(d,H) = L(d  |H)+ L(H) (7)
</equation>
<bodyText confidence="0.999273">
The relation of (7) to Bayes Theorem becomes
clear when we introduce the fundamental trans-
formation from probability to optimal code length
given by
</bodyText>
<equation confidence="0.998802">
L(x) = −log P(x) (8)
</equation>
<bodyText confidence="0.993941555555556">
Intuitively, Equation (8) calls for assigning shorter
length codes to higher probability symbols x
which, on average, will minimize the code length
€
for a string, d, of symbols drawn from distribution
P(x). The ability to transform between length and
probability allows for the conceptualization of the
prior probabilities over the hypothesis space as
biases against complexity.
</bodyText>
<equation confidence="0.993997">
p(c  |NULL(i / j)α ,y) = {
c = Hi (y) XOR c = H j (y)
</equation>
<page confidence="0.986202">
7
</page>
<bodyText confidence="0.9999454">
We can think of the hypotheses in H as deci-
sion trees which produce stressed outputs from
input words. In order to encode such decision trees
we need something like the binary coding scheme
given in Rissanen (1989, section 7.2).
</bodyText>
<equation confidence="0.9970465">
L(T) = log( kT +m T — 2) (9)
T JI
</equation>
<bodyText confidence="0.970864272727273">
Here kT is the number of internal (non-terminal)
nodes in the tree and mT is the number of leaf (ter-
minal) nodes. Equation (9) provides a measure of
�
how much the grammar compresses its input – or
how many classes it must keep track of to produce
the correct output. For a series of decisions, based
on querying for a series of features at a series of
internal nodes, there will be a particular outcome at
a particular leaf node. For the GUJARATI* gram-
mar, kT=5 (corresponding to the relevant questions
about vowel identity listed in definition [1] above),
and mT=6 (corresponding to the possible stress
decisions resulting from the answers to each of
those questions).
Additionally, all Non-Deterministic hypothe-
ses require the estimation of at least one error term.
I will approximate the coding length for a set of k
free parameters ( θ ), estimated over a string of
ö
length n, by Equation (10) (Rissanen 1989, section
3.1).
</bodyText>
<equation confidence="0.448739">
log n (10)
</equation>
<bodyText confidence="0.985342235294118">
Since I am only interested here in computing
the length associated with the hypotheses them-
selves (the negative log of their prior probability),
€
we will focus on the second term of Equation (7),
which can be written as the sum of (9) and (10).
MAX(G*/G)σ consists of a decision tree that is
twice as large as that of GUJARATI*α (since it keeps
track of both GUJARATI*α and GUJARATIα). Addi-
tionally, the combination hypothesis makes use of
one more estimated parameter (wG*).
Under LMU′, where n=512 words, the prior
probability ratio6 of MAX(G*/G)σ to GUJARATI*α is
1.7x104. From this result we can calculate that the
type of lexicon in which the mixed-grammar hy-
pothesis would be rejected is one in which the
GUJARATI* hypothesis accounts for at least eight
</bodyText>
<footnote confidence="0.6581355">
6 the contribution of the hypotheses lengths, converted back to
probability via Equation (8)
</footnote>
<bodyText confidence="0.99898265">
times more data than does GUJARATI (G*/G = 8).
This value must be regarded as an approxima-
tion due to its dependence on the particular coding
scheme used7. It is, however, likely the best and
most principled estimate of the linguistic-bias-free
prior we can achieve8.
Under the information theoretic treatment, its
lower probability prior is still not enough to pre-
vent MAX(G*/G)σ from winning under LMU′ (by 52
orders of magnitude over GUJARATI*α). The pro-
ductions of a learner who has converged on this
grammar would not be obviously consistent with a
reversed sonority-to-stress output (since many
words would show a stress pattern that is incom-
patible with that hypothesis), but neither would
those productions be inconsistent with such a
grammar (since a (slim) majority of words provide
positive evidence for such a hypothesis). The ty-
pological status of such languages will be dis-
cussed in the following section.
</bodyText>
<sectionHeader confidence="0.990079" genericHeader="method">
4 Discussion &amp; Conclusion
</sectionHeader>
<bodyText confidence="0.94492740625">
The foregoing analysis has served to address the
question of whether the observed frequency of oc-
currence (approximately never) of anti-markedness
systems (such as a grammar with a preference for
stressing low sonority vowels over high) requires
an active constraint that removes those grammars
from the learner’s hypothesis space. The central
claim within this paper has been that attempts to
answer this question must involve a careful exami-
nation and specification of the learning process, as
well as the inputs to the learner.
Given that systems, at any particular time, tend
7 In practice, a code length exactly equal to the negative log of
the probability of a particular symbol may be unattainable, and
the relationship in Equation (8) becomes an approximation
which may be better in some cases than others. Due to this
limitation, it is not clear how much the exact magnitude of a
result obtained with this method can be relied upon (for a brief
discussion of this issue see, for example, Brent (1999).)
8 An alternative to this approach is to imagine all grammars as
potential mixtures, and to stipulate a prior probability distri-
bution over the possible weight values. Each grammar in this
view is equally complex, but certain weight combinations may
be more likely than others (such as the ‘simple’ 0/100% distri-
bution over weights). Conceptually this seems at least as rea-
sonable as the current approach. We are still left, however,
with the problem of determining the prior probability distribu-
tion over the weights, in a manner which, ideally, would be
independent of the problem at hand.
L(ö
θ ) = k
2
</bodyText>
<page confidence="0.988876">
8
</page>
<bodyText confidence="0.999830523076923">
to be in a state in which higher sonority vowels
attract stress (due to assumed perceptual factors),
the hypothetical sound change that disrupts the
natural order must act over forms that are origi-
nally markedness-abiding. Thus, there will be a
residue of those forms in the language even after
the change has occurred (those in which /a/’s not
derived from /a/’s fail to attract stress in the pres-
ence of mid-sonority vowels). If this residue is
small enough then the anti-markedness hypothesis
might emerge as the winner. In turn, for this resi-
due to be small, the lexicon before the change must
exhibit a certain make-up, such that some word
types either fail to appear or occur with much
lower frequency than others.
In order to approximate these conditions I cre-
ated 1000 (x5) simulated lexicons by sampling
(without replacement) from the uniform word in-
ventory (LMU) at five different rates; for 3-syllable
words: 1% (=5 types), 3% (=15 types), 5% (=26
types), 7% (=36 types), and 10% (=51 types).
Higher sampling rates meant a greater likelihood
of reproducing the underlying uniform type distri-
bution over the 1000 trials, while lower sampling
rates (under-sampling) allowed for a higher likeli-
hood of departure from uniformity, and a greater
chance for skewed, or outlier, lexicons to emerge.
These simulations were done for the full set of
both 3-syllable and 2-syllable words (a more real-
istic distribution of input to the learner). To com-
bine the two word lengths, with differing numbers
of types, I scaled selection from the two classes. A
cursory examination of the online English database
CELEX (1993) gives a count of 45,652 for 3-
syllable words, and 61,738 for 2-syllable words, a
1:1.4 relationship. Using this as a rough guide, and
since the ratio of total types between 3-syllable and
2-syllable words is 512:64, a 1:10 scale was used
(giving a proportion of 512:640=1:1.25). Each of
the five sampling rates maintained this 1:10 scaling
factor, such that the lexicon containing 3-syllable
word types sampled at 7%, also contained 2-
syllable word types sampled at 70%; this is the
lexicon of 36 3-syllable word types (out of a possi-
ble total of 512) and 45 2-syllable word types (out
of a possible total of 64) (Row 5: [36,45] in Table
2).
Each lexicon, L, at a particular sampling rate,
was transformed to its L′ counterpart (via the
change a&gt;o), and the coverage ratio between hy-
potheses GUJARATI* and GUJARATI over L′ was
computed. As given at the end of Section 3.4 for
the description-length prior, a value greater than
G*/G = 8 is needed for a GUJARATI* outcome.
Here, due to concerns about the sensitivity of the
Bayesian learner, and the degree of uncertainty in
the calculation of the prior, I relax this criterion.
The last four columns of Table 2 correspond to
four (largely arbitrary) values for the G*/G ratio
which were stipulated as thresholds (or possible
prior probability ratios) that would allow GU-
JARATI* to beat MAX(G*/G)σ. Each cell contains
the percentage of anti-markedness outcomes (cal-
culated from 1000 runs) for a given threshold, at a
given sampling rate.
</bodyText>
<table confidence="0.998332428571429">
Sampling [3,2]-syllable G*/G
Rate word types 5 2.5 1.7 1.25
1%,10% [5,6] 0 0 .4% 6.4%
3%,30% [15,19] 0 0 0 .9%
5%,50% [26,32] 0 0 0 .1%
7%,70% [36,45] 0 0 0 0
10%,100% [51,64] 0 0 0 0
</table>
<tableCaption confidence="0.991096">
Table 2: Estimated probabilities of learned anti-
</tableCaption>
<bodyText confidence="0.970596136363636">
markedness grammar: under 5 different sampling rates
(given as [number of 3-syllable,2-syllable word types]),
for four different threshold coverage ratios.
The very low occurrence rates of Table 2 show
that changing our assumptions about the make-up
of the lexicon (departing from uniformity) do not
qualitatively alter the results of the previous sec-
tions. A pure anti-markedness grammar (GU-
JARATI*) seems to be a relatively rare outcome as
compared to a mixed-grammar competitor
(MAX(G*/G)σ), even under relaxed acceptance cri-
teria.
The above work relies heavily on the existence
of a residue of natural patterns in a post-sound
change language. Under circumstances in which
sound change is non-neutralizing (that is, o is ab-
sent from the inventory of Gujarati before the
sound change), there will be no contradictory evi-
dence to the learner of Gujarati′: all data is consis-
tent with the GUJARATI* hypothesis. Furthermore,
there is a long-standing intuition in the literature
that the most likely sound changes might actually
</bodyText>
<page confidence="0.992841">
9
</page>
<bodyText confidence="0.999975609756098">
be of this type (Martinet 1955)9.
Under these circumstances we might expect
GUJARATI* to emerge as the clear winner. This
will depend critically on whether or not we con-
sider the lack of conflicting data to be an over-
whelming factor in hypothesis selection. If, in-
stead, we maintain our space of non-deterministic
hypotheses, then there is still competition from the
mixed-grammar alternatives. Under the non-
neutralizing scenario, Gujarati has 7 vowels (rather
than 8); for 3-syllable words, all 343 types support
the GUJARATI*α hypothesis, while 265 are also
consistent with PENULTα. And G*/P = 1.3. 2-
syllable words will provide somewhat less of an
advantage to the anti-markedness grammar
(49:46~1.13), and with a larger weight (10 times
greater frequency to approximate the CELEX ra-
tios), giving an adjusted ratio of roughly 1.15.
Whether this is enough of an advantage to cause
GUJARATI* to be selected will depend on the pa-
rameters of our learner, as well as the prior prob-
ability ratio between the two hypotheses: the dif-
ference in complexity between the GUJARATI* rule,
which computes stress location based on both po-
sition and sonority, and the PENULT rule, which
only computes over position.
What the above discussion illustrates is that the
actual form of common or likely sound changes
can significantly alter the outcome of analysis. If
non-neutralizing sound changes are the norm, then
the dispreferred grammar might have a higher pre-
dicted likelihood than that calculated here. Alter-
natively, if chain shifts predominate, whereby all
the vowels in the system undergo related incre-
mental changes in quality, the outcome might be
different again. And if realistic sound changes op-
erate on a word by word basis, as predicted by
Evolutionary Phonology, such that results are even
less consistent in terms of sonority class, an even
lower likelihood for a true anti-markedness gram-
mar might be the result10.
</bodyText>
<sectionHeader confidence="0.439766" genericHeader="method">
9 Thanks to Adam Albright for bringing this to my attention.
</sectionHeader>
<bodyText confidence="0.9762445">
10 Another issue so far undiscussed is the aptness of describing
the GUJARATI* hypothesis as a reversed sonority-to-stress
scale. In either instantiation of Gujarati′ (deriving either from
the 7- or 8-vowel system) there are only two operable sonority
categories {MID,a}. Stressing a preferentially over a higher-
sonority mid vowel is already dispreferred behavior from a
universalist perspective, but it is qualitatively different than a
hypothesis that targets sonority as the deciding factor (rather
than vowel identity). This second hypothesis, for example,
This work has been a preliminary attempt to
accurately lay out the methodological requirements
for addressing questions of how grammars arise.
Further research ought to be concerned with ex-
actly the complications to the question just raised.
For present purposes, however, there are two gen-
eral points to be made. The first is that, in order to
determine what any theory predicts in this domain,
one has to make assumptions about what consti-
tutes a realistic language learner, as well as estab-
lish estimates of the normal state of lexical statis-
tics. The second point is that determining those
predictions tells us what the relevant typological
facts are. The work here suggests that it is the oc-
currence, not so much of pure anti-markedness
systems, but of partial anti-markedness (mixed-
grammar) systems that is the critical issue. It may
turn out to be the case that these systems are also
very rare, and the over-prediction claim holds in its
revised form. However, the true distribution of
these types of languages seems far from clear at
the present time, and work will have to be done to
establish the fact of the matter11.
</bodyText>
<sectionHeader confidence="0.999075" genericHeader="method">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.990632076923077">
This work was supported by an NSF IGERT grant
and a Department of Education Javits Fellowship.
I would like to thank Paul Smolensky, Colin Wil-
son, and Simon Fischer-Baum for their invaluable
assistance. Thanks also go to the three reviewers
of this paper, especially Adam Albright for his ex-
tensive and extremely helpful comments.
would avoid stressing newly encountered a’s, precisely be-
cause of the high sonority of the vowel. The likelihood of
achieving a true sonority scale reversal seems even lower than
that of learning the ‘stress-a’ rule. This is because the strong-
est evidence for a sonority-sensitive scale involves multiple
tiers or classes of sonority (probably at least three). However,
the more different classes of vowels (the more complications
to the calculation of stress) the less likely it seems that an
indirect sound change (one that does not target sonority itself)
will produce a clean reversal of the pattern. Again, disorder,
or proliferating ‘co-phonologies’ seem more likely to carry the
day.
11 In the first place, it is not a given that pure anti-markedness
systems are completely non-occurring (see, for example,
Poppe (1960); McLendon (1975); Breen and Pensalfini
(1999)). As for potential mixed-grammar languages, these
might include systems that have been analyzed as exhibiting
high degrees of lexical exceptionality, or gone largely un-
analyzed due to what is perceived as patternless behavior.
</bodyText>
<page confidence="0.996989">
10
</page>
<note confidence="0.850852">
References McLendon, S. (1975). A Grammar of Eastern Pomo,
</note>
<reference confidence="0.991323220338983">
University of California Press.
Blevins, J. (2004). Evolutionary Phonology: the emer-
gence of sound patterns. New York, Cambridge Univer-
sity Press.
Breen, G. and R. Pensalfini (1999). &amp;quot;Arrernte: a lan-
guage with no syllable onsets.&amp;quot; Linguistic Inquiry 30(1):
1-25.
Chater, N., J. B. Tenenbaum, et al. (2006). &amp;quot;Probabilis-
tic models of cognition: conceptual foundations.&amp;quot;
Trends in Cognitive Science 10(7): 287-291.
Court, C. (1970). Nasal harmony and some indonesian
sound laws. Pacific Linguistics Series C No.13. S. A.
Wurm and C. Laycock.
de Lacy, P. (2006). Markedness: Reduction and Preser-
vation in Phonology, Cambridge University Press.
Inkelas, S. (1997). The theoretical status of morphologi-
cally conditioned phonology: a case study of dominance
effects. Yearbook of Morphology. G. Booij and J. van
Marle, Kluwer Academic Publishers: 121-155.
Ito, J. and A. Mester (2001). &amp;quot;Covert generalizations in
Optimality Theory: the role of stratal faithfulness con-
straints.&amp;quot; Studies in Phonetics, Phonology and Mor-
phology 7: 273-299.
Jarosz, G. (2006). Richness of the base and probabilistic
unsupervised learning in Optimality Theory. Proceed-
ings of the Eighth Meeting of the ACL Special Interest
Group in Computational Phonology and Morphology,
New York City.
Kemp, C., A. Perfors, et al. (2007). &amp;quot;Learning overhy-
potheses with hierarchical Bayesian models.&amp;quot; Develop-
mental Science 10(3): 307-321.
Kiparsky, P. (2004). &amp;quot;Universals constrain change;
change results in typological generalizations.&amp;quot; ms.
Kiparsky, P. (2006). &amp;quot;The Amphichronic Program vs.
Evolutionary Phonology.&amp;quot; Theoretical Linguistics 32:
217-236.
Kording, K. P. and D. M. Wolper (2006). &amp;quot;Bayesian
decision theory in sensorimotor control.&amp;quot; Trends in
Cognitive Science 10(7): 319-326.
Martinet, A. (1955). Economie des changements pho-
netiques. Bern, Francke.
Mitchell, T. M. (1997). Machine Learning, McGraw-
Hill.
Pater, J. (2000). &amp;quot;Non-uniformity in English seconday
stress: the role of ranked and lexically specific con-
straints.&amp;quot; Phonology 17: 237-274.
Poppe, N. N. (1960). Buriat Grammar, Indiana Univer-
sity Publications.
Prince, A. and P. Smolenksy (1993/2004). Optimality
Theory, Blackwell Publishing.
Rissanen, J. (1989). Stochastic Complexity in Statistical
Enquiry, World Scientific Publishing Co.
Tenenbaum, J. B., C. Kemp, et al. (2007). Theory-based
Bayesian models of inductive reasoning. Inductive Rea-
soning. A. Feeney and E. Heit, Cambridge University
Press.
Yang, C. (1999). A Selectionist Theory of Language
Acquisition. 27th Annual Meeting of the Association for
Computational Linguistics, College Park, MD.
</reference>
<page confidence="0.999486">
11
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.450244">
<title confidence="0.992462">Bayesian Learning Over Conflicting Predictions for language change</title>
<author confidence="0.884828">Rebecca</author>
<affiliation confidence="0.926706">Cognitive Science</affiliation>
<address confidence="0.800721333333333">Johns Hopkins 3400 N. Charles Baltimore, MD</address>
<email confidence="0.999209">morley@cogsci.jhu.edu</email>
<abstract confidence="0.995583">This paper is an analysis of the claim that a universal ban on certain (‘anti-markedness’) grammars is necessary in order to explain their non-occurrence in the languages of the world. To assess the validity of this hypothesis I exthe implications of one sound change for learning in a specific phonological domain (stress assignment), making explicit assumptions about the type of data that results, and the learning function that computes over that data. The preliminary conclusion is that restrictions on possible end-point languages are unneeded, and that the most likely outcome of change is a lexicon that is inconsistent with respect to a single generating rule.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<publisher>Press.</publisher>
<institution>University of California</institution>
<marker></marker>
<rawString>University of California Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blevins</author>
</authors>
<title>Evolutionary Phonology: the emergence of sound patterns.</title>
<date>2004</date>
<publisher>University Press.</publisher>
<location>New York, Cambridge</location>
<contexts>
<context position="1160" citStr="Blevins, 2004" startWordPosition="172" endWordPosition="173">ess assignment), making explicit assumptions about the type of data that results, and the learning function that computes over that data. The preliminary conclusion is that restrictions on possible end-point languages are unneeded, and that the most likely outcome of change is a lexicon that is inconsistent with respect to a single generating rule. 1 Introduction The basic tenet of Evolutionary Phonology is that the observed universal commonalities in phonological systems of the world arise from the universal commonality of the way listeners and speakers produce and perceive sound structures (Blevins, 2004). Diachronic processes operating via the transmission of the speech signal act without regard for the subsequent system they create. Alternate theories in the tradition of Chomsky argue for universal prohibitions which would serve to ban or repair certain changes just in case they would result in a ‘disallowed’ system (Kiparsky 2004, 2006). In Optimality Theoretic terms, this would be a grammar that violates the canonical set of universal markedness constraints. I will call this claim the Universal-Grammar-Delimited Hypothesis Space (UG-Delimited H) Principle. Without this check, Kiparsky argu</context>
</contexts>
<marker>Blevins, 2004</marker>
<rawString>Blevins, J. (2004). Evolutionary Phonology: the emergence of sound patterns. New York, Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Breen</author>
<author>R Pensalfini</author>
</authors>
<title>Arrernte: a language with no syllable onsets.&amp;quot;</title>
<date>1999</date>
<journal>Linguistic Inquiry</journal>
<volume>30</volume>
<issue>1</issue>
<pages>1--25</pages>
<marker>Breen, Pensalfini, 1999</marker>
<rawString>Breen, G. and R. Pensalfini (1999). &amp;quot;Arrernte: a language with no syllable onsets.&amp;quot; Linguistic Inquiry 30(1): 1-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chater</author>
<author>J B Tenenbaum</author>
</authors>
<title>Probabilistic models of cognition: conceptual foundations.&amp;quot;</title>
<date>2006</date>
<journal>Trends in Cognitive Science</journal>
<volume>10</volume>
<issue>7</issue>
<pages>287--291</pages>
<marker>Chater, Tenenbaum, 2006</marker>
<rawString>Chater, N., J. B. Tenenbaum, et al. (2006). &amp;quot;Probabilistic models of cognition: conceptual foundations.&amp;quot; Trends in Cognitive Science 10(7): 287-291.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Court</author>
</authors>
<title>Nasal harmony and some indonesian sound laws. Pacific Linguistics Series</title>
<date>1970</date>
<marker>Court, 1970</marker>
<rawString>Court, C. (1970). Nasal harmony and some indonesian sound laws. Pacific Linguistics Series C No.13. S. A. Wurm and C. Laycock.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P de Lacy</author>
</authors>
<title>Markedness: Reduction and Preservation in Phonology,</title>
<date>2006</date>
<publisher>Cambridge University Press.</publisher>
<marker>de Lacy, 2006</marker>
<rawString>de Lacy, P. (2006). Markedness: Reduction and Preservation in Phonology, Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Inkelas</author>
</authors>
<title>The theoretical status of morphologically conditioned phonology: a case study of dominance effects.</title>
<date>1997</date>
<journal>Yearbook of Morphology. G. Booij</journal>
<pages>121--155</pages>
<publisher>Kluwer Academic Publishers:</publisher>
<contexts>
<context position="2909" citStr="Inkelas 1997" startWordPosition="441" endWordPosition="442">ind of analysis, for exactly the type of scenario in which a dispreferred, but hypothetically learnable, grammar might arise. Diachronic changes that are caused by factors outside of the grammar have the capability of disrupting a categorical rule system, introducing irregularities into a previously regular pattern. These irregularities may have an ‘unnatural’ or antimarkedness character, but typically, they will coexist alongside remnants of the prior natural pattern. That is the first observation. The second is that if learners are allowed to adopt mixedgrammar hypotheses (‘co-phonologies’ (Inkelas 1997), ‘stratal faithfulness’ (Ito and Mester 2001), ‘lexical indexation’ (Pater 2000)), then under a posterior-maximizing learning model, these hybrid systems are the most likely outcome (rather than a categorical ‘anti-markedness’ grammar). I will work through a case study of sonority sensitive stress, paying special attention to the lexicon that would be produced after a hypothetical sound change of the type Kiparsky proposes. By examining the output of Bayesian hypothesis testing in this domain I will conclude that for the pure anti-markedness grammar to arise, not only is a 2 Proceedings of th</context>
<context position="15289" citStr="Inkelas 1997" startWordPosition="2460" endWordPosition="2461">equence of the extreme probability distribution over only two types of data (consistent and inconsistent -- with values close to 1 in the first case, and close to 0 in the second). Since the probability of an independent collection of outcomes (a particular input lexicon) is computed via multiplication, each additional difference in data coverage compounds the single point case, such that the ratio grows exponentially. If this behavior is indeed a problem for our linguistic domain (where different sub-regions of phonological regularity are often observed to coexist stably in natural language (Inkelas 1997)) then there are various means at our disposal to modify the learning model. In the following section I will consider an alternative weighted decision metric; in Section 3.3 I will expand the hypothesis space to include mixed-grammar competitors; and in Section 4 I will alter the parameters of the learning rule to provide a more stringent threshold for success in hypothesis competition. 3.2 Optimal Bayes Classifier So far, we have been implicitly assuming a winnertake-all classification strategy whereby the hypothesis with the highest likelihood given the data is the one selected by the learne</context>
</contexts>
<marker>Inkelas, 1997</marker>
<rawString>Inkelas, S. (1997). The theoretical status of morphologically conditioned phonology: a case study of dominance effects. Yearbook of Morphology. G. Booij and J. van Marle, Kluwer Academic Publishers: 121-155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ito</author>
<author>A Mester</author>
</authors>
<title>Covert generalizations in Optimality Theory: the role of stratal faithfulness constraints.&amp;quot;</title>
<date>2001</date>
<journal>Studies in Phonetics, Phonology and Morphology</journal>
<volume>7</volume>
<pages>273--299</pages>
<contexts>
<context position="2955" citStr="Ito and Mester 2001" startWordPosition="445" endWordPosition="448">f scenario in which a dispreferred, but hypothetically learnable, grammar might arise. Diachronic changes that are caused by factors outside of the grammar have the capability of disrupting a categorical rule system, introducing irregularities into a previously regular pattern. These irregularities may have an ‘unnatural’ or antimarkedness character, but typically, they will coexist alongside remnants of the prior natural pattern. That is the first observation. The second is that if learners are allowed to adopt mixedgrammar hypotheses (‘co-phonologies’ (Inkelas 1997), ‘stratal faithfulness’ (Ito and Mester 2001), ‘lexical indexation’ (Pater 2000)), then under a posterior-maximizing learning model, these hybrid systems are the most likely outcome (rather than a categorical ‘anti-markedness’ grammar). I will work through a case study of sonority sensitive stress, paying special attention to the lexicon that would be produced after a hypothetical sound change of the type Kiparsky proposes. By examining the output of Bayesian hypothesis testing in this domain I will conclude that for the pure anti-markedness grammar to arise, not only is a 2 Proceedings of the Tenth Meeting of the ACL Special Interest Gr</context>
</contexts>
<marker>Ito, Mester, 2001</marker>
<rawString>Ito, J. and A. Mester (2001). &amp;quot;Covert generalizations in Optimality Theory: the role of stratal faithfulness constraints.&amp;quot; Studies in Phonetics, Phonology and Morphology 7: 273-299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Jarosz</author>
</authors>
<title>Richness of the base and probabilistic unsupervised learning in Optimality Theory.</title>
<date>2006</date>
<booktitle>Proceedings of the Eighth Meeting of the ACL Special Interest Group in Computational Phonology and Morphology,</booktitle>
<location>New York City.</location>
<contexts>
<context position="22361" citStr="Jarosz (2006)" startWordPosition="3727" endWordPosition="3728">combination of Hiσ and Hjσ. [5] MAX(i/j)σ: ‘Maximum Likelihood’ p(c |MAX(i / j)&amp;quot; ,y) = { (w, + wj)(1− 2σ) c = H, (y) = H j (y) (1− 2σ)w, +σwj c = H, (y) &amp;c ≠ H j (y) (1− 2σ)wj +σw, c = H j (y) &amp;c ≠ H, (y) (w, + wj)σ c ≠ H, (y) &amp;c ≠ H j (y) When Hi = GUJARATI* and Hj = GUJARATI, MAX(G*/G)σ assigns the highest posterior of any we have seen so far (approximately 56 orders of magnitude larger than G*). This is because, within the space of candidates, it gives the highest likelihood to the observed data, and the prior probability probabilistic version of Optimality Theory over rankings utilized by Jarosz (2006)). (assumed so far to be uniform) plays no role in this calculation. As the hypotheses we are considering become more complicated, however, we are led to consider an alternative to this assumption, one in which hypotheses with longer description lengths, or greater complexity, are penalized (Rissanen 1989). 3.4 Non-Uniform Prior: Hypothesis Description Length Under the uniform prior assumption, only with a lexicon in which GUJARATI* accounts for at least 44 times as much data as does GUJARATI will MAX(G*/G)σ be defeated. In this section I will show how that result would be altered by consideri</context>
</contexts>
<marker>Jarosz, 2006</marker>
<rawString>Jarosz, G. (2006). Richness of the base and probabilistic unsupervised learning in Optimality Theory. Proceedings of the Eighth Meeting of the ACL Special Interest Group in Computational Phonology and Morphology, New York City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Kemp</author>
<author>A Perfors</author>
</authors>
<title>Learning overhypotheses with hierarchical Bayesian models.&amp;quot;</title>
<date>2007</date>
<journal>Developmental Science</journal>
<volume>10</volume>
<issue>3</issue>
<pages>307--321</pages>
<marker>Kemp, Perfors, 2007</marker>
<rawString>Kemp, C., A. Perfors, et al. (2007). &amp;quot;Learning overhypotheses with hierarchical Bayesian models.&amp;quot; Developmental Science 10(3): 307-321.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kiparsky</author>
</authors>
<title>Universals constrain change; change results in typological generalizations.&amp;quot; ms.</title>
<date>2004</date>
<contexts>
<context position="1494" citStr="Kiparsky 2004" startWordPosition="225" endWordPosition="226">enerating rule. 1 Introduction The basic tenet of Evolutionary Phonology is that the observed universal commonalities in phonological systems of the world arise from the universal commonality of the way listeners and speakers produce and perceive sound structures (Blevins, 2004). Diachronic processes operating via the transmission of the speech signal act without regard for the subsequent system they create. Alternate theories in the tradition of Chomsky argue for universal prohibitions which would serve to ban or repair certain changes just in case they would result in a ‘disallowed’ system (Kiparsky 2004, 2006). In Optimality Theoretic terms, this would be a grammar that violates the canonical set of universal markedness constraints. I will call this claim the Universal-Grammar-Delimited Hypothesis Space (UG-Delimited H) Principle. Without this check, Kiparsky argues, common and natural sound changes (‘blind’ Evolutionary Phonology) would frequently produce unnatural and in fact unobserved ‘anti-markedness’ languages (such as a system in which lower sonority vowels were stressed in preference to higher sonority vowels). An analysis of the properties of possible grammars is an analysis that in</context>
</contexts>
<marker>Kiparsky, 2004</marker>
<rawString>Kiparsky, P. (2004). &amp;quot;Universals constrain change; change results in typological generalizations.&amp;quot; ms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kiparsky</author>
</authors>
<title>The Amphichronic Program vs. Evolutionary Phonology.&amp;quot;</title>
<date>2006</date>
<journal>Theoretical Linguistics</journal>
<volume>32</volume>
<pages>217--236</pages>
<marker>Kiparsky, 2006</marker>
<rawString>Kiparsky, P. (2006). &amp;quot;The Amphichronic Program vs. Evolutionary Phonology.&amp;quot; Theoretical Linguistics 32: 217-236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K P Kording</author>
<author>D M Wolper</author>
</authors>
<title>Bayesian decision theory in sensorimotor control.&amp;quot;</title>
<date>2006</date>
<journal>Trends in Cognitive Science</journal>
<volume>10</volume>
<issue>7</issue>
<pages>319--326</pages>
<marker>Kording, Wolper, 2006</marker>
<rawString>Kording, K. P. and D. M. Wolper (2006). &amp;quot;Bayesian decision theory in sensorimotor control.&amp;quot; Trends in Cognitive Science 10(7): 319-326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Martinet</author>
</authors>
<title>Economie des changements phonetiques.</title>
<date>1955</date>
<location>Bern, Francke.</location>
<contexts>
<context position="33489" citStr="Martinet 1955" startWordPosition="5599" endWordPosition="5600">red to a mixed-grammar competitor (MAX(G*/G)σ), even under relaxed acceptance criteria. The above work relies heavily on the existence of a residue of natural patterns in a post-sound change language. Under circumstances in which sound change is non-neutralizing (that is, o is absent from the inventory of Gujarati before the sound change), there will be no contradictory evidence to the learner of Gujarati′: all data is consistent with the GUJARATI* hypothesis. Furthermore, there is a long-standing intuition in the literature that the most likely sound changes might actually 9 be of this type (Martinet 1955)9. Under these circumstances we might expect GUJARATI* to emerge as the clear winner. This will depend critically on whether or not we consider the lack of conflicting data to be an overwhelming factor in hypothesis selection. If, instead, we maintain our space of non-deterministic hypotheses, then there is still competition from the mixed-grammar alternatives. Under the nonneutralizing scenario, Gujarati has 7 vowels (rather than 8); for 3-syllable words, all 343 types support the GUJARATI*α hypothesis, while 265 are also consistent with PENULTα. And G*/P = 1.3. 2- syllable words will provide</context>
</contexts>
<marker>Martinet, 1955</marker>
<rawString>Martinet, A. (1955). Economie des changements phonetiques. Bern, Francke.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Mitchell</author>
</authors>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<location>McGrawHill.</location>
<marker>Mitchell, 1997</marker>
<rawString>Mitchell, T. M. (1997). Machine Learning, McGrawHill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pater</author>
</authors>
<title>Non-uniformity in English seconday stress: the role of ranked and lexically specific constraints.&amp;quot;</title>
<date>2000</date>
<journal>Phonology</journal>
<volume>17</volume>
<pages>237--274</pages>
<contexts>
<context position="2990" citStr="Pater 2000" startWordPosition="451" endWordPosition="452">thetically learnable, grammar might arise. Diachronic changes that are caused by factors outside of the grammar have the capability of disrupting a categorical rule system, introducing irregularities into a previously regular pattern. These irregularities may have an ‘unnatural’ or antimarkedness character, but typically, they will coexist alongside remnants of the prior natural pattern. That is the first observation. The second is that if learners are allowed to adopt mixedgrammar hypotheses (‘co-phonologies’ (Inkelas 1997), ‘stratal faithfulness’ (Ito and Mester 2001), ‘lexical indexation’ (Pater 2000)), then under a posterior-maximizing learning model, these hybrid systems are the most likely outcome (rather than a categorical ‘anti-markedness’ grammar). I will work through a case study of sonority sensitive stress, paying special attention to the lexicon that would be produced after a hypothetical sound change of the type Kiparsky proposes. By examining the output of Bayesian hypothesis testing in this domain I will conclude that for the pure anti-markedness grammar to arise, not only is a 2 Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and</context>
</contexts>
<marker>Pater, 2000</marker>
<rawString>Pater, J. (2000). &amp;quot;Non-uniformity in English seconday stress: the role of ranked and lexically specific constraints.&amp;quot; Phonology 17: 237-274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N N Poppe</author>
</authors>
<date>1960</date>
<institution>Buriat Grammar, Indiana University Publications.</institution>
<marker>Poppe, 1960</marker>
<rawString>Poppe, N. N. (1960). Buriat Grammar, Indiana University Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Prince</author>
<author>P Smolenksy</author>
</authors>
<date>1993</date>
<publisher>Optimality Theory, Blackwell Publishing.</publisher>
<marker>Prince, Smolenksy, 1993</marker>
<rawString>Prince, A. and P. Smolenksy (1993/2004). Optimality Theory, Blackwell Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rissanen</author>
</authors>
<title>Stochastic Complexity in Statistical Enquiry,</title>
<date>1989</date>
<publisher>World Scientific Publishing Co.</publisher>
<contexts>
<context position="22668" citStr="Rissanen 1989" startWordPosition="3774" endWordPosition="3775">est posterior of any we have seen so far (approximately 56 orders of magnitude larger than G*). This is because, within the space of candidates, it gives the highest likelihood to the observed data, and the prior probability probabilistic version of Optimality Theory over rankings utilized by Jarosz (2006)). (assumed so far to be uniform) plays no role in this calculation. As the hypotheses we are considering become more complicated, however, we are led to consider an alternative to this assumption, one in which hypotheses with longer description lengths, or greater complexity, are penalized (Rissanen 1989). 3.4 Non-Uniform Prior: Hypothesis Description Length Under the uniform prior assumption, only with a lexicon in which GUJARATI* accounts for at least 44 times as much data as does GUJARATI will MAX(G*/G)σ be defeated. In this section I will show how that result would be altered by considering a better approximation to the prior probability distribution over those hypotheses. MAX(G*/G)σ and GUJARATI*α can be seen to differ in a basic way related to the number of parameters and rules they must each keep track of. A domainindependent means of determining a prior probability based on this differ</context>
<context position="24681" citStr="Rissanen (1989" startWordPosition="4120" endWordPosition="4121">r assigning shorter length codes to higher probability symbols x which, on average, will minimize the code length € for a string, d, of symbols drawn from distribution P(x). The ability to transform between length and probability allows for the conceptualization of the prior probabilities over the hypothesis space as biases against complexity. p(c |NULL(i / j)α ,y) = { c = Hi (y) XOR c = H j (y) 7 We can think of the hypotheses in H as decision trees which produce stressed outputs from input words. In order to encode such decision trees we need something like the binary coding scheme given in Rissanen (1989, section 7.2). L(T) = log( kT +m T — 2) (9) T JI Here kT is the number of internal (non-terminal) nodes in the tree and mT is the number of leaf (terminal) nodes. Equation (9) provides a measure of � how much the grammar compresses its input – or how many classes it must keep track of to produce the correct output. For a series of decisions, based on querying for a series of features at a series of internal nodes, there will be a particular outcome at a particular leaf node. For the GUJARATI* grammar, kT=5 (corresponding to the relevant questions about vowel identity listed in definition [1] </context>
</contexts>
<marker>Rissanen, 1989</marker>
<rawString>Rissanen, J. (1989). Stochastic Complexity in Statistical Enquiry, World Scientific Publishing Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Tenenbaum</author>
<author>C Kemp</author>
</authors>
<title>Theory-based Bayesian models of inductive reasoning.</title>
<date>2007</date>
<journal>Inductive Reasoning. A. Feeney</journal>
<publisher>Cambridge University Press.</publisher>
<marker>Tenenbaum, Kemp, 2007</marker>
<rawString>Tenenbaum, J. B., C. Kemp, et al. (2007). Theory-based Bayesian models of inductive reasoning. Inductive Reasoning. A. Feeney and E. Heit, Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Yang</author>
</authors>
<title>A Selectionist Theory of Language Acquisition.</title>
<date>1999</date>
<booktitle>27th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>College Park, MD.</location>
<contexts>
<context position="20393" citStr="Yang (1999)" startWordPosition="3350" endWordPosition="3351">be better predictors of the data than those examined so far. In particular, it is instructive to introduce something like a class of null hypotheses: hybrid grammars which explicitly encode equality between any pair of competing alternatives’ ability to explain the data5. 5 The effect of mixed-grammar hypotheses can also be realized by allowing a selection procedure over a set of simple grammars, as described in Section 3.2, but, crucially, with the weights calculated under the assumption that data are generated by a combination of grammars (see, for example, the variational model proposed by Yang (1999), or the P(Hiα |d) α + α 1− 2α p(c1 |d,y) p(c2 |d,y) ≤1.5) only in the region .17 &lt; α &lt; .4 6 I define this class as follows: the posterior probability that the hypothesis NULL(i/j)α assigns to a stress class c is calculated by allotting equal probability to selecting the Hiα or the Hjα rule to produce an output of that class: p(c |NULL(i / j)&amp;quot; ,y) = wi p(c |Hi&amp;quot; ,y) + wj p(c |H j&amp;quot; ,y) (6) where wi = wj = .5. From Equation (6) and the definition in [3], we can compute the probability distribution of stress assignment c given the application of NULL(i/j)α to a particular word, y [4] NULL(i/j)α: ‘</context>
</contexts>
<marker>Yang, 1999</marker>
<rawString>Yang, C. (1999). A Selectionist Theory of Language Acquisition. 27th Annual Meeting of the Association for Computational Linguistics, College Park, MD.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>