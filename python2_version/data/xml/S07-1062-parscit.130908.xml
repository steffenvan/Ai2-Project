<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013158">
<title confidence="0.982442">
RTV: Tree Kernels for Thematic Role Classification
</title>
<author confidence="0.995568">
Daniele Pighin Alessandro Moschitti Roberto Basili
</author>
<affiliation confidence="0.996042">
FBK-irst; University of Trento, DIT University of Trento, DIT University of Rome Tor Vergata, DISP
</affiliation>
<email confidence="0.960402">
pighin@itc.it moschitti@dit.unitn.it basili@info.uniroma2.it
</email>
<sectionHeader confidence="0.99312" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999732181818182">
We present a simple, two-steps supervised
strategy for the identification and classifica-
tion of thematic roles in natural language
texts. We employ no external source of in-
formation but automatic parse trees of the in-
put sentences. We use a few attribute-value
features and tree kernel functions applied to
specialized structured features. The result-
ing system has an F1 of 75.44 on the Se-
mEval2007 closed task on semantic role la-
beling.
</bodyText>
<sectionHeader confidence="0.998803" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999660966666667">
In this paper we present a system for the labeling
of semantic roles that produces VerbNet (Kipper et
al., 2000) like annotations of free text sentences us-
ing only full syntactic parses of the input sentences.
The labeling process is modeled as a cascade of two
distinct classification steps: (1) boundary detection
(BD), in which the word sequences that encode a
thematic role for a given predicate are recognized,
and (2) role classification (RC), in which the type
of thematic role with respect to the predicate is as-
signed. After role classification, a set of simple
heuristics are applied in order to ensure that only
well formed annotations are output.
We designed our system on a per-predicate basis,
training one boundary classifier and a battery of role
classifiers for each predicate word. We clustered all
the senses of the same verb together and ended up
with 50 distinct boundary classifiers (one for each
target predicate word) and 619 role classifiers to rec-
ognize the 47 distinct role labels that appear in the
training set.
The remainder of this paper is structured as fol-
lows: Section 2 describes in some detail the archi-
tecture of our labeling system; Section 3 describes
the features that we use to represent the classifier
examples; Section 4 describes the experimental set-
ting and reports the accuracy of the system on the
SemEval2007 semantic role labeling closed task; fi-
nally, Section 5 discusses the results and presents
our conclusions.
</bodyText>
<sectionHeader confidence="0.991504" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.999817454545454">
Given a target predicate word in a natural language
sentence, a SRL system is meant to correctly iden-
tify all the arguments of the predicate. This problem
is usually divided in two sub-tasks: (a) the detection
of the boundaries (i. e. the word span) of each argu-
ment and (b) the classification of the argument type,
e.g. Arg0 or ArgM in PropBank or Agent and Goal
in FrameNet or VerbNet.
The standard approach to learn both the detection
and the classification of predicate arguments is sum-
marized by the following steps:
</bodyText>
<listItem confidence="0.918121333333333">
1 Given a sentence from the training-set, gener-
ate a full syntactic parse-tree;
2 let P and A be the set of predicates and the
set of parse-tree nodes (i.e. the potential argu-
ments), respectively;
3 for each pair (p, a) E P x A:
</listItem>
<subsectionHeader confidence="0.853463">
3.1 extract the feature representation set, Fp,a;
</subsectionHeader>
<bodyText confidence="0.9161145">
3.2 if the sub-tree rooted in a covers exactly the
words of one argument of p, put Fp,a in T+
(positive examples), otherwise put it in T−
(negative examples).
For instance, in Figure 1.a, for each combination
of the predicate approve with any other tree node a
</bodyText>
<page confidence="0.958977">
288
</page>
<bodyText confidence="0.979434769230769">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 288–291,
Prague, June 2007. c�2007 Association for Computational Linguistics
that does not overlap with the predicate, a classifier
example Fapprove,a is generated. If a exactly covers
one of the predicate arguments (in this case: ”The
charter”, ”by the EC Commission” or ”on Sept. 21”)
it is regarded as a positive instance, otherwise it will
be a negative one, e. g. Fapprove,(NN charter).
The T+ and T− sets are used to train the bound-
ary classifier. To train the role multi-class classifier,
T+ can be reorganized as positive T+argi and nega-
tive T−argi examples for each argument i. In this way,
an individual ONE-vs-ALL classifier for each argu-
ment i can be trained. We adopted this solution, ac-
cording to (Pradhan et al., 2005), since it is simple
and effective. In the classification phase, given an
unseen sentence, all its Fp,a are generated and clas-
sified by each individual role classifier. The role la-
bel associated with the maximum among the scores
provided by the individual classifiers is eventually
selected.
To make the annotations consistent with the un-
derlying linguistic model, we employ a few simple
heuristics to resolve the overlap situations that may
occur, e. g. both “charter” and “the charter” in Figure
1 may be assigned a role:
</bodyText>
<listItem confidence="0.9946344">
• if more than two nodes are involved, i. e. a node
d and two or more of its descendants ni are
classified as arguments, then assume that d is
not an argument. This choice is justified by pre-
vious studies (Moschitti et al., 2006b) showing
that the accuracy of classification is higher for
lower nodes;
• if only two nodes are involved, i. e. they dom-
inate each other, then keep the one with the
highest classification score.
</listItem>
<sectionHeader confidence="0.996816" genericHeader="method">
3 Features for Semantic Role Labeling
</sectionHeader>
<bodyText confidence="0.9999845">
We explicitly represent as attribute-value pairs the
following features of each Fp,a pair:
</bodyText>
<listItem confidence="0.966988285714286">
• Phrase Type, Predicate Word, Head Word, Po-
sition and Voice as defined in (Gildea and Juras-
fky, 2002);
• Partial Path, No Direction Path, Head Word
POS, First and Last Word/POS in Constituent
and SubCategorization as proposed in (Pradhan
et al., 2005);
</listItem>
<subsectionHeader confidence="0.69514">
Experiencer ARGM-TMP
</subsectionHeader>
<figureCaption confidence="0.9989285">
Figure 1: A sentence parse tree (a) and two example AST&apos;`
structures relative to the predicate approve (b).
</figureCaption>
<table confidence="0.9929335">
Set Props T T+ T−
Train 15,838 793,104 45,157 747,947
Dev 1,606 75,302 4,291 71,011
Train- Dev 14,232 717,802 40,866 676,936
</table>
<tableCaption confidence="0.714723666666667">
Table 1: Composition of the dataset in terms of: number of
annotations (Props); number of candidate argument nodes (T);
positive (T+) and negative (T−) boundary classifier examples.
</tableCaption>
<listItem confidence="0.9655505">
• Syntactic Frame as designed in (Xue and
Palmer, 2004).
</listItem>
<bodyText confidence="0.999208">
We also employ structured features derived by the
full parses in an attempt to capture relevant aspects
that may not be emphasized by the explicit feature
representation. (Moschitti et al., 2006a) and (Mos-
chitti et al., 2006b) defined several classes of struc-
tured features that were successfully employed with
tree kernels for the different stages of an SRL pro-
cess. Figure 1 shows an example of the AST&apos; struc-
tures that we used for both the boundary detection
and the role classification stages.
</bodyText>
<sectionHeader confidence="0.99951" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.997702333333333">
In this section we discuss the setup and the results
of the experiments carried out on the dataset of the
SemEval2007 closed task on SRL.
</bodyText>
<figure confidence="0.997667642857143">
NP
IN
NP
IN
the
EC
DT
NN
.
VP
AUX
was
PP
VBN
The
charter
a) S
NP
VP
.
PP
approved
NNP
NNP
Commission
NNP
Sept.
CD
21
by
on
DT
Cause
b) S
NP-B
The
charter
Commission
EC
DT
VP
VBN-P
approved
approved
by
DT
the
NP
NNP
NN
IN
NNP
VP
VBN-P
VP
PP-B
</figure>
<page confidence="0.95742">
289
</page>
<table confidence="0.9996792">
Task Kernel(s) Precision Recall F0=1
BD poly 94.34% 71.26% 81.19
poly + TK 92.89% 76.09% 83.65
BD + RC poly 88.72% 68.76% 77.47
poly + TK 86.60% 72.40% 78.86
</table>
<tableCaption confidence="0.930925">
Table 2: SRL accuracy on the development test for the bound-
ary detection (BD) and the complete SRL task (BD+RC) using
the polynomial kernel alone (poly) or combined with a tree ker-
nel function (poly + TK).
</tableCaption>
<subsectionHeader confidence="0.981989">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.99987225">
The training set comprises 15,8381 training annota-
tions organized on a per-verb basis. In order to build
a development set (Dev), we sampled about one
tenth, i. e. 1,606 annotations, of the original train-
ing set. For the final evaluation on the test set (Test),
consisting of 3,094 annotations, we trained our clas-
sifiers on the whole training data. Statistics on the
dataset composition are shown in Table 1.
The evaluations were carried out with the SVM-
Light-TK2 software (Moschitti, 2004) which ex-
tends the SVM-Light package (Joachims, 1999)
with tree kernel functions. We used the default
polynomial kernel (degree=3) for the linear features
and a SubSet Tree (SST) kernel (Collins and Duffy,
2002) for the comparison of AST&apos; structured fea-
tures. The kernels are normalized and summed by
assigning a weight of 0.3 to the TK contribution.
Training all the 50 boundary classifiers and the
619 role classifiers on the whole dataset took about
4 hours on a 64 bits machine (2.2GHz, 1GB RAM)3.
</bodyText>
<subsectionHeader confidence="0.92098">
4.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999720111111111">
All the evaluations were carried out using
the CoNLL2005 evaluator tool available at
http://www.lsi.upc.es/∼srlconll/soft.html.
Table 2 shows the aggregate results on boundary
detection (BD) and the complete SRL task (BD+RC)
on the development set using the polynomial kernel
alone (poly) or in conjunction with the tree kernels
and structured features (poly+TK). For both tasks,
tree kernel functions do trigger automatic feature se-
</bodyText>
<footnote confidence="0.976908125">
1A bunch of unaligned annotations were removed from the
dataset.
2http://ai-nlp.info.uniroma2.it/moschitti/
3In order to have a faster development cycle, we only used
60k training examples to train the boundary classifier of the verb
say. The accuracy on this relation is still very high, as we mea-
sured an overall F1 of 87.18 on the development set and of 85.13
on the test set.
</footnote>
<table confidence="0.99996602">
Role #TI Precision Recall Fp=1
Ov(BD) 87.09% 72.96% 79.40
Ov(BD+RC) 6931 81.58% 70.16% 75.44
ARG2 4 100.00% 25.00% 40.00
ARG3 17 61.11% 64.71% 62.86
ARG4 4 0.00% 0.00% 0.00
ARGM-ADV 188 55.14% 31.38% 40.00
ARGM-CAU 13 50.00% 23.08% 31.58
ARGM-DIR 4 100.00% 25.00% 40.00
ARGM-EXT 3 0.00% 0.00% 0.00
ARGM-LOC 151 51.66% 51.66% 51.66
ARGM-MNR 85 41.94% 15.29% 22.41
ARGM-PNC 28 38.46% 17.86% 24.39
ARGM-PRD 9 83.33% 55.56% 66.67
ARGM-REC 1 0.00% 0.00% 0.00
ARGM-TMP 386 55.65% 35.75% 43.53
Actor1 12 85.71% 50.00% 63.16
Actor2 1 100.00% 100.00% 100.00
Agent 2551 91.38% 77.34% 83.78
Asset 21 42.42% 66.67% 51.85
Attribute 17 60.00% 70.59% 64.86
Beneficiary 24 65.00% 54.17% 59.09
Cause 48 75.56% 70.83% 73.12
Experiencer 132 86.49% 72.73% 79.01
Location 12 83.33% 41.67% 55.56
Material 7 100.00% 14.29% 25.00
Patient 37 76.67% 62.16% 68.66
Patient1 20 72.73% 40.00% 51.61
Predicate 181 63.75% 56.35% 59.82
Product 106 70.79% 59.43% 64.62
R-ARGM-LOC 2 0.00% 0.00% 0.00
R-ARGM-MNR 2 0.00% 0.00% 0.00
R-ARGM-TMP 4 0.00% 0.00% 0.00
R-Agent 74 70.15% 63.51% 66.67
R-Experiencer 5 100.00% 20.00% 33.33
R-Patient 2 0.00% 0.00% 0.00
R-Predicate 1 0.00% 0.00% 0.00
R-Product 2 0.00% 0.00% 0.00
R-Recipient 8 100.00% 87.50% 93.33
R-Theme 7 75.00% 42.86% 54.55
R-Theme1 7 100.00% 85.71% 92.31
R-Theme2 1 50.00% 100.00% 66.67
R-Topic 14 66.67% 42.86% 52.17
Recipient 48 75.51% 77.08% 76.29
Source 25 65.22% 60.00% 62.50
Stimulus 21 33.33% 19.05% 24.24
Theme 650 79.22% 68.62% 73.54
Theme1 69 77.42% 69.57% 73.28
Theme2 60 74.55% 68.33% 71.30
Topic 1867 84.26% 82.27% 83.25
</table>
<tableCaption confidence="0.999248">
Table 3: Evaluation of the semantic role labeling accuracy on
</tableCaption>
<bodyText confidence="0.866630333333333">
the SemEval2007 - Task 17 test set using the poly + TK kernel.
Column #TI reports the number of instances of each role label
in the test set. Rows Ov(BD) and Ov(BD + RC) show the overall
accuracy on the boundary detection and the complete SRL task,
respectively.
lection and improve the polynomial kernel by 2.46
and 1.39 F1 points, respectively.
The SRL accuracy for each one of the 47 dis-
tinct role labels is shown in Table 3. Column 2 lists
</bodyText>
<page confidence="0.979123">
290
</page>
<bodyText confidence="0.999907142857143">
the number of instances of each role in the test set.
Many roles have very few positive examples both in
the training and the test sets, and therefore have little
or no impact on the overall accuracy which is domi-
nated by the few roles which are very frequent, such
as Theme, Agent, Topic and ARGM-TMP which ac-
count for almost 80% of all the test roles.
</bodyText>
<sectionHeader confidence="0.997784" genericHeader="method">
5 Final Remarks
</sectionHeader>
<bodyText confidence="0.999717173913044">
In this paper we presented a system that employs
tree kernels and a basic set of flat features for the
classification of thematic roles.
We adopted a very simple approach that is meant
to be as general and fast as possible. The issue
of generality is addressed by training the bound-
ary and role classifiers on a per-predicate basis and
by employing tree kernel and structured features in
the learning algorithm. The resulting architecture
can indeed be used to learn the classification of
roles of non-verbal predicates as well, and the au-
tomatic feature selection triggered by the tree kernel
should compensate for the lack of ad-hoc, well es-
tablished explicit features for some classes of non-
verbal predicates, e. g. adverbs or prepositions.
Splitting the learning problem also has the clear
advantage of noticeably improving the efficiency of
the classifiers, thus reducing training and classifica-
tion time. On the other hand, this split results in
some classifiers having too few training instances
and therefore being very inaccurate. This is espe-
cially true for the boundary classifiers, which con-
versely need to be very accurate in order to posi-
tively support the following stages of the SRL pro-
cess. The solution of a monolithic boundary classi-
fier that we previously employed (Moschitti et al.,
2006b) is noticeably more accurate though much
less efficient, especially for training. Indeed, after
the SemEval2007 evaluation period was over, we
ran another experiment using a monolithic boundary
classifier. On the test set, we measured F1 values of
82.09 vs 79.40 and 77.17 vs 75.44 for the boundary
detection and the complete SRL tasks, respectively.
Although it was provided as part of both the train-
ing and test data, we chose not to use the verb sense
information. This choice is motivated by our in-
tention to depend on as less external resources as
possible in order to be able to port our SRL system
to other linguistic models and languages, for which
such resources may not exist. Still, identifying the
predicate sense is a key issue especially for role clas-
sification, as the argument structure of a predicate is
largely determined by its sense. In the near feature
we plan to use larger structured features, i. e. span-
ning all the potential arguments of a predicate, to
improve the accuracy of our role classifiers.
</bodyText>
<sectionHeader confidence="0.998278" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999793666666667">
The development of the SRL system was carried out
at the University of Rome Tor Vergata and financed
by the EU project PrestoSpace4 (FP6-507336).
</bodyText>
<sectionHeader confidence="0.999429" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999211303030303">
Michael Collins and Nigel Duffy. 2002. New ranking algo-
rithms for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In ACL02.
Daniel Gildea and Daniel Jurasfky. 2002. Automatic label-
ing of semantic roles. Computational Linguistic, 28(3):496–
530.
T. Joachims. 1999. Making large-scale SVM learning practical.
In B. Scholkopf, C. Burges, and A. Smola, editors, Advances
in Kernel Methods - Support Vector Learning.
Karin Kipper, Hoa Trang Dang, and Martha Palmer. 2000.
Class-based construction of a verb lexicon. In Proceedings
ofAAAI-2000 Seventeenth National Conference on Artificial
Intelligence, Austin, TX.
Alessandro Moschitti, Daniele Pighin, and Roberto Basili.
2006a. Semantic role labeling via tree kernel joint inference.
In Proceedings of the Tenth Conference on Computational
Natural Language Learning, CoNLL-X.
Alessandro Moschitti, Daniele Pighin, and Roberto Basili.
2006b. Tree kernel engineering in semantic role labeling
systems. In Proceedings of the Workshop on Learning Struc-
tured Information in Natural Language Applications, EACL
2006, pages 49–56, Trento, Italy, April. European Chapter
of the Association for Computational Linguistics.
Alessandro Moschitti. 2004. A study on convolution kernel
for shallow semantic parsing. In proceedings of ACL-2004,
Barcelona, Spain.
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne Ward,
James H. Martin, and Daniel Jurafsky. 2005. Support vector
learning for semantic argument classification. to appear in
Machine Learning Journal.
Nianwen Xue and Martha Palmer. 2004. Calibrating features
for semantic role labeling. In Proceedings of EMNLP 2004,
pages 88–94, Barcelona, Spain, July.
</reference>
<footnote confidence="0.951517">
4http://www.prestospace.org
</footnote>
<page confidence="0.992269">
291
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.559031">
<title confidence="0.99987">RTV: Tree Kernels for Thematic Role Classification</title>
<author confidence="0.999951">Daniele Pighin Alessandro Moschitti Roberto Basili</author>
<affiliation confidence="0.992987">University of Trento, DIT University of Trento, DIT University of Rome DISP</affiliation>
<email confidence="0.675298">pighin@itc.itmoschitti@dit.unitn.itbasili@info.uniroma2.it</email>
<abstract confidence="0.984653166666667">We present a simple, two-steps supervised strategy for the identification and classification of thematic roles in natural language texts. We employ no external source of information but automatic parse trees of the input sentences. We use a few attribute-value features and tree kernel functions applied to specialized structured features. The resultsystem has an 75.44 on the SemEval2007 closed task on semantic role labeling.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In ACL02.</booktitle>
<contexts>
<context position="7897" citStr="Collins and Duffy, 2002" startWordPosition="1329" endWordPosition="1332">b basis. In order to build a development set (Dev), we sampled about one tenth, i. e. 1,606 annotations, of the original training set. For the final evaluation on the test set (Test), consisting of 3,094 annotations, we trained our classifiers on the whole training data. Statistics on the dataset composition are shown in Table 1. The evaluations were carried out with the SVMLight-TK2 software (Moschitti, 2004) which extends the SVM-Light package (Joachims, 1999) with tree kernel functions. We used the default polynomial kernel (degree=3) for the linear features and a SubSet Tree (SST) kernel (Collins and Duffy, 2002) for the comparison of AST&apos; structured features. The kernels are normalized and summed by assigning a weight of 0.3 to the TK contribution. Training all the 50 boundary classifiers and the 619 role classifiers on the whole dataset took about 4 hours on a 64 bits machine (2.2GHz, 1GB RAM)3. 4.2 Evaluation All the evaluations were carried out using the CoNLL2005 evaluator tool available at http://www.lsi.upc.es/∼srlconll/soft.html. Table 2 shows the aggregate results on boundary detection (BD) and the complete SRL task (BD+RC) on the development set using the polynomial kernel alone (poly) or in</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In ACL02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurasfky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistic,</journal>
<volume>28</volume>
<issue>3</issue>
<pages>530</pages>
<contexts>
<context position="5280" citStr="Gildea and Jurasfky, 2002" startWordPosition="875" endWordPosition="879">nvolved, i. e. a node d and two or more of its descendants ni are classified as arguments, then assume that d is not an argument. This choice is justified by previous studies (Moschitti et al., 2006b) showing that the accuracy of classification is higher for lower nodes; • if only two nodes are involved, i. e. they dominate each other, then keep the one with the highest classification score. 3 Features for Semantic Role Labeling We explicitly represent as attribute-value pairs the following features of each Fp,a pair: • Phrase Type, Predicate Word, Head Word, Position and Voice as defined in (Gildea and Jurasfky, 2002); • Partial Path, No Direction Path, Head Word POS, First and Last Word/POS in Constituent and SubCategorization as proposed in (Pradhan et al., 2005); Experiencer ARGM-TMP Figure 1: A sentence parse tree (a) and two example AST&apos;` structures relative to the predicate approve (b). Set Props T T+ T− Train 15,838 793,104 45,157 747,947 Dev 1,606 75,302 4,291 71,011 Train- Dev 14,232 717,802 40,866 676,936 Table 1: Composition of the dataset in terms of: number of annotations (Props); number of candidate argument nodes (T); positive (T+) and negative (T−) boundary classifier examples. • Syntactic </context>
</contexts>
<marker>Gildea, Jurasfky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurasfky. 2002. Automatic labeling of semantic roles. Computational Linguistic, 28(3):496– 530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning.</booktitle>
<editor>In B. Scholkopf, C. Burges, and A. Smola, editors,</editor>
<contexts>
<context position="7739" citStr="Joachims, 1999" startWordPosition="1306" endWordPosition="1307"> (poly) or combined with a tree kernel function (poly + TK). 4.1 Setup The training set comprises 15,8381 training annotations organized on a per-verb basis. In order to build a development set (Dev), we sampled about one tenth, i. e. 1,606 annotations, of the original training set. For the final evaluation on the test set (Test), consisting of 3,094 annotations, we trained our classifiers on the whole training data. Statistics on the dataset composition are shown in Table 1. The evaluations were carried out with the SVMLight-TK2 software (Moschitti, 2004) which extends the SVM-Light package (Joachims, 1999) with tree kernel functions. We used the default polynomial kernel (degree=3) for the linear features and a SubSet Tree (SST) kernel (Collins and Duffy, 2002) for the comparison of AST&apos; structured features. The kernels are normalized and summed by assigning a weight of 0.3 to the TK contribution. Training all the 50 boundary classifiers and the 619 role classifiers on the whole dataset took about 4 hours on a 64 bits machine (2.2GHz, 1GB RAM)3. 4.2 Evaluation All the evaluations were carried out using the CoNLL2005 evaluator tool available at http://www.lsi.upc.es/∼srlconll/soft.html. Table 2 </context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making large-scale SVM learning practical. In B. Scholkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper</author>
<author>Hoa Trang Dang</author>
<author>Martha Palmer</author>
</authors>
<title>Class-based construction of a verb lexicon.</title>
<date>2000</date>
<booktitle>In Proceedings ofAAAI-2000 Seventeenth National Conference on Artificial Intelligence,</booktitle>
<location>Austin, TX.</location>
<contexts>
<context position="836" citStr="Kipper et al., 2000" startWordPosition="122" endWordPosition="125">oschitti@dit.unitn.it basili@info.uniroma2.it Abstract We present a simple, two-steps supervised strategy for the identification and classification of thematic roles in natural language texts. We employ no external source of information but automatic parse trees of the input sentences. We use a few attribute-value features and tree kernel functions applied to specialized structured features. The resulting system has an F1 of 75.44 on the SemEval2007 closed task on semantic role labeling. 1 Introduction In this paper we present a system for the labeling of semantic roles that produces VerbNet (Kipper et al., 2000) like annotations of free text sentences using only full syntactic parses of the input sentences. The labeling process is modeled as a cascade of two distinct classification steps: (1) boundary detection (BD), in which the word sequences that encode a thematic role for a given predicate are recognized, and (2) role classification (RC), in which the type of thematic role with respect to the predicate is assigned. After role classification, a set of simple heuristics are applied in order to ensure that only well formed annotations are output. We designed our system on a per-predicate basis, trai</context>
</contexts>
<marker>Kipper, Dang, Palmer, 2000</marker>
<rawString>Karin Kipper, Hoa Trang Dang, and Martha Palmer. 2000. Class-based construction of a verb lexicon. In Proceedings ofAAAI-2000 Seventeenth National Conference on Artificial Intelligence, Austin, TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Daniele Pighin</author>
<author>Roberto Basili</author>
</authors>
<title>Semantic role labeling via tree kernel joint inference.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X.</booktitle>
<contexts>
<context position="4852" citStr="Moschitti et al., 2006" startWordPosition="804" endWordPosition="807">ed by each individual role classifier. The role label associated with the maximum among the scores provided by the individual classifiers is eventually selected. To make the annotations consistent with the underlying linguistic model, we employ a few simple heuristics to resolve the overlap situations that may occur, e. g. both “charter” and “the charter” in Figure 1 may be assigned a role: • if more than two nodes are involved, i. e. a node d and two or more of its descendants ni are classified as arguments, then assume that d is not an argument. This choice is justified by previous studies (Moschitti et al., 2006b) showing that the accuracy of classification is higher for lower nodes; • if only two nodes are involved, i. e. they dominate each other, then keep the one with the highest classification score. 3 Features for Semantic Role Labeling We explicitly represent as attribute-value pairs the following features of each Fp,a pair: • Phrase Type, Predicate Word, Head Word, Position and Voice as defined in (Gildea and Jurasfky, 2002); • Partial Path, No Direction Path, Head Word POS, First and Last Word/POS in Constituent and SubCategorization as proposed in (Pradhan et al., 2005); Experiencer ARGM-TMP</context>
<context position="6119" citStr="Moschitti et al., 2006" startWordPosition="1009" endWordPosition="1012">example AST&apos;` structures relative to the predicate approve (b). Set Props T T+ T− Train 15,838 793,104 45,157 747,947 Dev 1,606 75,302 4,291 71,011 Train- Dev 14,232 717,802 40,866 676,936 Table 1: Composition of the dataset in terms of: number of annotations (Props); number of candidate argument nodes (T); positive (T+) and negative (T−) boundary classifier examples. • Syntactic Frame as designed in (Xue and Palmer, 2004). We also employ structured features derived by the full parses in an attempt to capture relevant aspects that may not be emphasized by the explicit feature representation. (Moschitti et al., 2006a) and (Moschitti et al., 2006b) defined several classes of structured features that were successfully employed with tree kernels for the different stages of an SRL process. Figure 1 shows an example of the AST&apos; structures that we used for both the boundary detection and the role classification stages. 4 Experiments In this section we discuss the setup and the results of the experiments carried out on the dataset of the SemEval2007 closed task on SRL. NP IN NP IN the EC DT NN . VP AUX was PP VBN The charter a) S NP VP . PP approved NNP NNP Commission NNP Sept. CD 21 by on DT Cause b) S NP-B Th</context>
<context position="12762" citStr="Moschitti et al., 2006" startWordPosition="2134" endWordPosition="2137">s of nonverbal predicates, e. g. adverbs or prepositions. Splitting the learning problem also has the clear advantage of noticeably improving the efficiency of the classifiers, thus reducing training and classification time. On the other hand, this split results in some classifiers having too few training instances and therefore being very inaccurate. This is especially true for the boundary classifiers, which conversely need to be very accurate in order to positively support the following stages of the SRL process. The solution of a monolithic boundary classifier that we previously employed (Moschitti et al., 2006b) is noticeably more accurate though much less efficient, especially for training. Indeed, after the SemEval2007 evaluation period was over, we ran another experiment using a monolithic boundary classifier. On the test set, we measured F1 values of 82.09 vs 79.40 and 77.17 vs 75.44 for the boundary detection and the complete SRL tasks, respectively. Although it was provided as part of both the training and test data, we chose not to use the verb sense information. This choice is motivated by our intention to depend on as less external resources as possible in order to be able to port our SRL </context>
</contexts>
<marker>Moschitti, Pighin, Basili, 2006</marker>
<rawString>Alessandro Moschitti, Daniele Pighin, and Roberto Basili. 2006a. Semantic role labeling via tree kernel joint inference. In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Daniele Pighin</author>
<author>Roberto Basili</author>
</authors>
<title>Tree kernel engineering in semantic role labeling systems.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Learning Structured Information in Natural Language Applications, EACL</booktitle>
<pages>49--56</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="4852" citStr="Moschitti et al., 2006" startWordPosition="804" endWordPosition="807">ed by each individual role classifier. The role label associated with the maximum among the scores provided by the individual classifiers is eventually selected. To make the annotations consistent with the underlying linguistic model, we employ a few simple heuristics to resolve the overlap situations that may occur, e. g. both “charter” and “the charter” in Figure 1 may be assigned a role: • if more than two nodes are involved, i. e. a node d and two or more of its descendants ni are classified as arguments, then assume that d is not an argument. This choice is justified by previous studies (Moschitti et al., 2006b) showing that the accuracy of classification is higher for lower nodes; • if only two nodes are involved, i. e. they dominate each other, then keep the one with the highest classification score. 3 Features for Semantic Role Labeling We explicitly represent as attribute-value pairs the following features of each Fp,a pair: • Phrase Type, Predicate Word, Head Word, Position and Voice as defined in (Gildea and Jurasfky, 2002); • Partial Path, No Direction Path, Head Word POS, First and Last Word/POS in Constituent and SubCategorization as proposed in (Pradhan et al., 2005); Experiencer ARGM-TMP</context>
<context position="6119" citStr="Moschitti et al., 2006" startWordPosition="1009" endWordPosition="1012">example AST&apos;` structures relative to the predicate approve (b). Set Props T T+ T− Train 15,838 793,104 45,157 747,947 Dev 1,606 75,302 4,291 71,011 Train- Dev 14,232 717,802 40,866 676,936 Table 1: Composition of the dataset in terms of: number of annotations (Props); number of candidate argument nodes (T); positive (T+) and negative (T−) boundary classifier examples. • Syntactic Frame as designed in (Xue and Palmer, 2004). We also employ structured features derived by the full parses in an attempt to capture relevant aspects that may not be emphasized by the explicit feature representation. (Moschitti et al., 2006a) and (Moschitti et al., 2006b) defined several classes of structured features that were successfully employed with tree kernels for the different stages of an SRL process. Figure 1 shows an example of the AST&apos; structures that we used for both the boundary detection and the role classification stages. 4 Experiments In this section we discuss the setup and the results of the experiments carried out on the dataset of the SemEval2007 closed task on SRL. NP IN NP IN the EC DT NN . VP AUX was PP VBN The charter a) S NP VP . PP approved NNP NNP Commission NNP Sept. CD 21 by on DT Cause b) S NP-B Th</context>
<context position="12762" citStr="Moschitti et al., 2006" startWordPosition="2134" endWordPosition="2137">s of nonverbal predicates, e. g. adverbs or prepositions. Splitting the learning problem also has the clear advantage of noticeably improving the efficiency of the classifiers, thus reducing training and classification time. On the other hand, this split results in some classifiers having too few training instances and therefore being very inaccurate. This is especially true for the boundary classifiers, which conversely need to be very accurate in order to positively support the following stages of the SRL process. The solution of a monolithic boundary classifier that we previously employed (Moschitti et al., 2006b) is noticeably more accurate though much less efficient, especially for training. Indeed, after the SemEval2007 evaluation period was over, we ran another experiment using a monolithic boundary classifier. On the test set, we measured F1 values of 82.09 vs 79.40 and 77.17 vs 75.44 for the boundary detection and the complete SRL tasks, respectively. Although it was provided as part of both the training and test data, we chose not to use the verb sense information. This choice is motivated by our intention to depend on as less external resources as possible in order to be able to port our SRL </context>
</contexts>
<marker>Moschitti, Pighin, Basili, 2006</marker>
<rawString>Alessandro Moschitti, Daniele Pighin, and Roberto Basili. 2006b. Tree kernel engineering in semantic role labeling systems. In Proceedings of the Workshop on Learning Structured Information in Natural Language Applications, EACL 2006, pages 49–56, Trento, Italy, April. European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>A study on convolution kernel for shallow semantic parsing.</title>
<date>2004</date>
<booktitle>In proceedings of ACL-2004,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="7686" citStr="Moschitti, 2004" startWordPosition="1298" endWordPosition="1299">ete SRL task (BD+RC) using the polynomial kernel alone (poly) or combined with a tree kernel function (poly + TK). 4.1 Setup The training set comprises 15,8381 training annotations organized on a per-verb basis. In order to build a development set (Dev), we sampled about one tenth, i. e. 1,606 annotations, of the original training set. For the final evaluation on the test set (Test), consisting of 3,094 annotations, we trained our classifiers on the whole training data. Statistics on the dataset composition are shown in Table 1. The evaluations were carried out with the SVMLight-TK2 software (Moschitti, 2004) which extends the SVM-Light package (Joachims, 1999) with tree kernel functions. We used the default polynomial kernel (degree=3) for the linear features and a SubSet Tree (SST) kernel (Collins and Duffy, 2002) for the comparison of AST&apos; structured features. The kernels are normalized and summed by assigning a weight of 0.3 to the TK contribution. Training all the 50 boundary classifiers and the 619 role classifiers on the whole dataset took about 4 hours on a 64 bits machine (2.2GHz, 1GB RAM)3. 4.2 Evaluation All the evaluations were carried out using the CoNLL2005 evaluator tool available a</context>
</contexts>
<marker>Moschitti, 2004</marker>
<rawString>Alessandro Moschitti. 2004. A study on convolution kernel for shallow semantic parsing. In proceedings of ACL-2004, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Kadri Hacioglu</author>
<author>Valeri Krugler</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Support vector learning for semantic argument classification. to appear in</title>
<date>2005</date>
<journal>Machine Learning Journal.</journal>
<contexts>
<context position="4100" citStr="Pradhan et al., 2005" startWordPosition="673" endWordPosition="676">icate, a classifier example Fapprove,a is generated. If a exactly covers one of the predicate arguments (in this case: ”The charter”, ”by the EC Commission” or ”on Sept. 21”) it is regarded as a positive instance, otherwise it will be a negative one, e. g. Fapprove,(NN charter). The T+ and T− sets are used to train the boundary classifier. To train the role multi-class classifier, T+ can be reorganized as positive T+argi and negative T−argi examples for each argument i. In this way, an individual ONE-vs-ALL classifier for each argument i can be trained. We adopted this solution, according to (Pradhan et al., 2005), since it is simple and effective. In the classification phase, given an unseen sentence, all its Fp,a are generated and classified by each individual role classifier. The role label associated with the maximum among the scores provided by the individual classifiers is eventually selected. To make the annotations consistent with the underlying linguistic model, we employ a few simple heuristics to resolve the overlap situations that may occur, e. g. both “charter” and “the charter” in Figure 1 may be assigned a role: • if more than two nodes are involved, i. e. a node d and two or more of its</context>
<context position="5430" citStr="Pradhan et al., 2005" startWordPosition="900" endWordPosition="903">by previous studies (Moschitti et al., 2006b) showing that the accuracy of classification is higher for lower nodes; • if only two nodes are involved, i. e. they dominate each other, then keep the one with the highest classification score. 3 Features for Semantic Role Labeling We explicitly represent as attribute-value pairs the following features of each Fp,a pair: • Phrase Type, Predicate Word, Head Word, Position and Voice as defined in (Gildea and Jurasfky, 2002); • Partial Path, No Direction Path, Head Word POS, First and Last Word/POS in Constituent and SubCategorization as proposed in (Pradhan et al., 2005); Experiencer ARGM-TMP Figure 1: A sentence parse tree (a) and two example AST&apos;` structures relative to the predicate approve (b). Set Props T T+ T− Train 15,838 793,104 45,157 747,947 Dev 1,606 75,302 4,291 71,011 Train- Dev 14,232 717,802 40,866 676,936 Table 1: Composition of the dataset in terms of: number of annotations (Props); number of candidate argument nodes (T); positive (T+) and negative (T−) boundary classifier examples. • Syntactic Frame as designed in (Xue and Palmer, 2004). We also employ structured features derived by the full parses in an attempt to capture relevant aspects t</context>
</contexts>
<marker>Pradhan, Hacioglu, Krugler, Ward, Martin, Jurafsky, 2005</marker>
<rawString>Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne Ward, James H. Martin, and Daniel Jurafsky. 2005. Support vector learning for semantic argument classification. to appear in Machine Learning Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP 2004,</booktitle>
<pages>88--94</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="5923" citStr="Xue and Palmer, 2004" startWordPosition="978" endWordPosition="981">irection Path, Head Word POS, First and Last Word/POS in Constituent and SubCategorization as proposed in (Pradhan et al., 2005); Experiencer ARGM-TMP Figure 1: A sentence parse tree (a) and two example AST&apos;` structures relative to the predicate approve (b). Set Props T T+ T− Train 15,838 793,104 45,157 747,947 Dev 1,606 75,302 4,291 71,011 Train- Dev 14,232 717,802 40,866 676,936 Table 1: Composition of the dataset in terms of: number of annotations (Props); number of candidate argument nodes (T); positive (T+) and negative (T−) boundary classifier examples. • Syntactic Frame as designed in (Xue and Palmer, 2004). We also employ structured features derived by the full parses in an attempt to capture relevant aspects that may not be emphasized by the explicit feature representation. (Moschitti et al., 2006a) and (Moschitti et al., 2006b) defined several classes of structured features that were successfully employed with tree kernels for the different stages of an SRL process. Figure 1 shows an example of the AST&apos; structures that we used for both the boundary detection and the role classification stages. 4 Experiments In this section we discuss the setup and the results of the experiments carried out on</context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role labeling. In Proceedings of EMNLP 2004, pages 88–94, Barcelona, Spain, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>