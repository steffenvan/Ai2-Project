<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000523">
<title confidence="0.9996745">
A Method for Forming Mutual Beliefs for Communication through
Human-robot Multi-modal Interaction
</title>
<author confidence="0.886912">
Naoto Iwahashi
</author>
<affiliation confidence="0.819181">
Sony Computer Science Labs.
Tokyo, Japan
</affiliation>
<email confidence="0.997423">
iwahashi@csl.sony.co.jp
</email>
<sectionHeader confidence="0.996641" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999981090909091">
This paper describes a method of multi-
modal language processing that reflects
experiences shared by people and robots.
Through incremental online optimization
in the process of interaction, the user and
the robot form mutual beliefs represented
by a stochastic model. Based on these mu-
tual beliefs, the robot can interpret even
fragmental and ambiguous utterances, and
can act and generate utterances appropri-
ate for a given situation.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999890563636364">
The process of human communication is based on
certain beliefs shared by those who are communi-
cating. Language is one such mutual belief, and it
is used to convey meaning based on its relevance
to other mutual beliefs (Sperber and Wilson, 1995).
These mutual beliefs are formed through interac-
tion with the environment and other people, and the
meaning of utterances is embedded in such shared
experiences.
If those communicating want to logically con-
vince each other that proposition p is a mutual belief,
they must prove that the infinitely nested proposition
“They have information that they have information
that ... that they have information that p” also holds.
However, in reality, all we can do is assume, based
on a few clues, that our beliefs are identical to those
of other people we are talking to. That is, it can
never be guaranteed that our beliefs are identical to
those of other people.
The processes of utterance generation and un-
derstanding rely on a system of mutual beliefs as-
sumed by each person, and this system changes au-
tonomously and recursively through these processes.
The listener interprets utterances based on their rel-
evance to their system of assumed mutual beliefs.
The listener also receives information for updating
their system of assumed mutual beliefs through this
process. In addition, the speaker can receive simi-
lar information through the response of the listener.
Through utterances, people simultaneously send and
receive information about one another’s system of
assumed mutual beliefs. In this sense, we can say
that a mutual belief system assumed by one per-
son couples with mutual belief systems assumed by
other people they are communicating with.
To enable humans and robots to communicate
with one another in a physical environment the
way people do, spoken-language processing meth-
ods must emphasize mutual understanding, and they
must have a mechanism that would enable the mu-
tual belief systems couple with one another. More-
over, language, perception, and behavior have to
be processed integratively in order for humans and
robots to physically share their environment as the
basis for the formation of common experiences and
in order for linguistic and nonlinguistic beliefs to
combine in the process of human-robot interaction.
Previous language processing methods, which are
characterized by fixed language knowledge, do not
satisfy these requirements because they cannot dy-
namically reflect experiences in the communication
process in a real environment.
I have been working on methods for forming lin-
guistic beliefs, such as beliefs about phonemes, lexi-
con, and grammar, based on common perceptual ex-
</bodyText>
<figureCaption confidence="0.997896">
Figure 1: Interaction between a user and a robot
</figureCaption>
<bodyText confidence="0.999888619047619">
periences between people and robots (for further de-
tail, see (Iwahashi, 2001; Iwahashi, 2003)). This pa-
per describes a method that enables robots to learn
a system of mutual beliefs including nonlinguistic
ones through multi-modal language interaction with
people. The learning is based on incremental on-
line optimization, and it uses information from raw
speech and visual observations as well as behavioral
reinforcement, which is integrated in a probabilistic
framework.
Theoretical research (Clark, 1996) and its com-
putational modelling (Traum, 1994) focused on the
formation of mutual beliefs that is a direct target of
communication, and aimed at representing the for-
mation of mutual beliefs as a procedure- and rule-
driven process. In contrast, this study focuses on a
system of mutual beliefs that is used in the process of
utterance generation and understanding in a physical
environment, and aims at representing the formation
of this system by a mathematical model of coupling
systems.
</bodyText>
<sectionHeader confidence="0.977913" genericHeader="method">
2 Task for Forming Mutual Beliefs
</sectionHeader>
<bodyText confidence="0.999909727272727">
The task for forming mutual beliefs was set up as
follows. A robot was sat at a table so that the robot
and the user sitting at the table could see and move
the objects on the table (Fig. 1) The user and the
robot initially shared certain basic linguistic beliefs,
including a lexicon with a small number of items and
a simple grammar, and the robot could understand
some utterances 1. The user asked the robot to move
an object by making an utterance and a gesture, and
the robot acted in response. If the robot responded
incorrectly, the user slapped the robot’s hand. The
</bodyText>
<footnote confidence="0.844878">
1No function words were included in the lexicon.
</footnote>
<figureCaption confidence="0.995887">
Figure 2: A scene during which utterances were
made and understood
</figureCaption>
<bodyText confidence="0.999013578947369">
robot also asked the user to move an object, and the
user acted in response. Mutual beliefs were formed
incrementally, online, through such interaction.
Figure 2 shows an example of utterance genera-
tion and understanding using mutual beliefs. In the
scene shown in Fig. 2, the object on the left, Kermit,
has just been put on the table.
If the user in the figure wants to ask the robot to
move Kermit onto the box, he may say “Kermit box
move-onto”. In this situation, if the user assumes
that the robot shares the belief that the object moved
in the previous action is likely to be the next target
for movement and the belief that the box is likely
to be something for the object to be moved onto,
he might just say “move-onto”. To understand this
fragmental utterance, the robot has to have similar
beliefs. Inversely, when the robot wants to ask the
user to do something, mutual beliefs are used in the
same way.
</bodyText>
<sectionHeader confidence="0.832428" genericHeader="method">
3 Algorithm for Learning a System of
Mutual Beliefs
</sectionHeader>
<subsectionHeader confidence="0.999915">
3.1 Setting
</subsectionHeader>
<bodyText confidence="0.999824727272727">
The robot has an arm with a hand, and a stereo
camera unit. A close-talk microphone is used for
speech input, and the speech is represented by a
time-sequence of Mel-scale cepstrum coefficients.
Visual observation of of object i is represented
by using such features as color (three-dimensional:
L*a*b* parameters), size (one-dimensional), and
shape (two-dimensional). Trajectory u of the ob-
ject’s motion is represented by a time-sequence of its
positions. A touch sensor is attached to the robot’s
hand.
</bodyText>
<subsectionHeader confidence="0.9818715">
3.2 Representation of a system of mutual
beliefs
</subsectionHeader>
<bodyText confidence="0.979891">
In the algorithm I developed, the system of mutual
beliefs consists of two parts: 1) a decision func-
tion, composed of a set of beliefs with values rep-
resenting the degree of confidence that each belief
is shared by the robot and the user, and 2) a global
confidence function, which represents the degree of
confidence for the decision function. The beliefs I
used are those concerning lexicon, grammar, behav-
ioral context, and motion-object relationship. The
degree of confidence for each belief is represented
by a scalar value. The beliefs are represented by
stochastic models as follows:
Lexicon L
The lexicon is represented by a set of pairs, each
with probability density function (pdf) p(s|ci) of
feature s of a spoken word and a pdf for representing
the image concept of lexical item ci, i = 1, ..., N.
Two types of image concepts are used. One is a con-
cept for the static observation of an object. Con-
ditional pdf p(o|c) of feature o of an object given
lexical item c is represented by a Gaussian pdf. The
other is a concept for the movement of an object.
The concept is viewed as the process of change in
the relationship between the trajector and the land-
mark. Here, given lexical item c, position ot,p of
trajector object t, and position ol,p of landmark ob-
ject l, conditional pdf p(u|ot,p, ol,p, c) of trajectory u
is represented by a hidden Markov Model (HMM).
Pdf p(s|c) of the features of a spoken word is also
represented by an HMM.
Grammar G
I assume that an utterance can be understood based
on its conceptual structure z, which has three at-
tributes: landmark, trajector, and motion, each of
which contains certain elements of the utterance.
Grammar G is represented by a set of occurrence
probabilities for the orders of these attributes in an
utterance and a statistical bigram model of the lexi-
cal items for each of the three attributes.
</bodyText>
<subsectionHeader confidence="0.664294">
Effect of behavioral context B1(i, q; H)
</subsectionHeader>
<bodyText confidence="0.973232055555556">
Effect ofbehavioral context represents the belief that
the current utterance refers to object i, given behav-
ioral context q. q includes information on whether
object i was a trajector or a landmark in the previ-
ous action and whether the user’s current gesture is
referring to object i. This belief is represented by a
parameter set H = {hc, hg, hp}, and it takes hc as
its value if object i was involved in the previous ac-
tion, hg if the object is being held, hp if it is being
pointed to, and 0 in all other cases.
Motion-object relationship B2(ot,f, ol,f, WM; R)
Motion-object relationship represents the belief that
in the motion corresponing to lexical item WM, fea-
ture ot,f of object t and feature ol,f of object l are
typical for a trajector and a landmark, respectively.
This belief is represented by a conditional multivari-
ate Gaussian pdf, p(ot,f, ol,f|WM; R), where R is
its parameter set.
</bodyText>
<subsectionHeader confidence="0.995478">
3.3 Decision function
</subsectionHeader>
<bodyText confidence="0.9999195">
The beliefs described above are organized and as-
signed confidence values to obtain the decision func-
tion used in the process of utterance generation and
understanding. This decision function is written as
</bodyText>
<equation confidence="0.946489833333333">
Ψ(s, a, O, q, L, G, R, H, Γ)
C
= max γ1 log p(s|z; L, G) [Speech]
l,z
+γ2 log p(u|ot,p, ol,p, WM; L) [Motion]
( )
+γ2 log p(ot,f|WT; L) + log p(ol,f|WL; L)
[Object]
+γ3 log p(ot,f, ol,f, |WM; R)
[Motion-Object Relationship]
+γ4 (B1 (t, q; H) + B1 (l, q; H)) /
[Behavioral Context]
</equation>
<bodyText confidence="0.961868714285714">
where Γ = {γ1, ..., γ4} is a set of confidence values
for beliefs corresponding to the speech, motion, ob-
ject, motion-object relationship, and behavioral con-
text; a denotes the action, and it is represented by a
pair (t, u) of trajector object t and trajectory u of its
movement; O denotes the scene, which includes the
positions and features of all the objects in the scene;
and WT and WL denotes the sequences of the lexi-
cal items in the utterances for the trajector and land-
mark, respectively. Given O, q, L, G, R, H, and
Γ, the corresponding action, a˜ = (˜u, ˜t), understood
to be the meaning of utterance s is determined by
maximizing the decision function as
Ψ(s, a, O, q, L, G, R, H, Γ).
</bodyText>
<subsectionHeader confidence="0.930279">
3.4 Global confidence function
</subsectionHeader>
<bodyText confidence="0.99984">
Global confidence function f outputs an estimate of
the probability that the robot’s utterance s will be
correctly understood by the user, and it is written as
</bodyText>
<equation confidence="0.856226">
f(x) = 1 arctan Cx λ2λ1 / + 0.5,
</equation>
<bodyText confidence="0.999706285714286">
where λ1 and λ2 are the parameters of this function.
A margin in the value of the output of the decision
function in the process of generating an utterance is
used for input x of this function. Margin d obtained
in the process of generating utterance s that means
action a in scene O under behavioral context q is
defined as
</bodyText>
<equation confidence="0.942521">
d(s, a, O, q, L, G, R, H, Γ)
(
= min Ψ(s, a, O, q, L, G, R, H, Γ)
Ada
)−Ψ(s,A, O, q, L, G,R, H,Γ) .
</equation>
<bodyText confidence="0.999860846153846">
We can easily see that a large margin increases the
probability of the robot being understood correctly
by the user. If there is a high probability of the
robot’s utterances being understood correctly even
when the margin is small, we can say that the robot’s
beliefs are consistent with those of the user. When
the robot asks for action a in scene O under behav-
ioral context q, the robot generates utterance s˜ so
as to make the value of the output of f as close as
possible to value of parameter ξ, which represents
the taget probability of the robot’s utterance being
understood correctly. This utterance can be repre-
sented as
</bodyText>
<equation confidence="0.555116333333333">
( )
s˜ = arg min f(d(s, a, O, q, L, G, R, H, Γ)) − ξ .
s
</equation>
<bodyText confidence="0.999969333333333">
The robot can increase the chance of being under-
stood correctly by using more words. On the other
hand, if the robot can predict correct understanding
with a sufficiently high probability, the robot can
manage with a fragmental utterance using a small
number of words.
</bodyText>
<subsectionHeader confidence="0.937139">
3.5 Learning
</subsectionHeader>
<bodyText confidence="0.999978548387097">
The decision function and the global confidence
function are learned separately in the utterance un-
derstanding and utterance generation processes, re-
spectively.
The decision function is learned incrementally,
online, through a sequence of episodes each of
which consists of the following steps. 1) Through
an utterance and a gesture, the user asks the robot to
move an object. 2) The robot acts on its understand-
ing of the utterance. 3) If the robot acts correctly, the
process is terminated. Otherwise, the user slaps the
robot’s hand. 4) The robot acts in a different way. 5)
If the robot acts incorrectly, the user slaps the robot’s
hand.
When the robot acts correctly in the first or sec-
ond trial in an episode, the robot associates utterance
s, action a, scene O, and behavioral context q with
each other, and makes these associations a learning
sample. Then the robot adapts the values of parame-
ter set R for the belief about the motion-object rela-
tionship, parameter set H for the belief about the ef-
fect of the behavioral context, and a set of weighting
parameters, Γ. R is learned by using the Bayesian
learning method. H and Γ are learned based on the
minimum error criterion (Juang and Katagiri, 1992).
Lexicon L and grammar G are given beforehand and
are fixed. When the ith sample (si, ai, Oi, qi) is ob-
tained based on this process of association, Hi and
Γi are adapted to minimize the probability of mis-
understanding based on the minimum error criterion
as
</bodyText>
<equation confidence="0.96643025">
i
wi−j g(d (sj, aj, Oj, qj, L, G, Ri, Hi, Γi))
j=i−K
→ min,
</equation>
<bodyText confidence="0.999973272727273">
where g(x) is −x if x &lt; 0 and 0 otherwise, and
K and wi−j represent the number of latest samples
used in the learning process and the weights for each
sample, respectively.
Global confidence function f is learned incremen-
tally, online, through a sequence of episodes which
consist of the following steps. 1) The robot gener-
ates an utterance to ask the user to move an object.
2) The user acts according to their understanding
of the robot’s utterance. 3) The robot determines
whether the user’s action is correct or not.
</bodyText>
<equation confidence="0.819039">
a˜ = arg max
a
</equation>
<bodyText confidence="0.967471642857143">
In each episode, the robot generates an utterance
that makes the value of the output of global confi-
dence function f as close to ξ as possible. After each
episode, the value of margin d in the utterance gen-
eration process is associated with information about
whether the utterance was understood correctly or
not, and this sample of associations is used for learn-
ing. The learning is done so as to approximate the
probability that an utterance will be understood cor-
rectly by minimizing the weighted sum of squared
errors in the latest episodes. After the ith episode,
parameters λ1 and λ2 are adapted as
[λ1,i, λ2,i] ← (1 − δ)[λ1,i−1, λ2,i−1] + δ[ ˜λ1,i, ˜λ2,i],
where
</bodyText>
<equation confidence="0.9981465">
(˜λ1,i, ˜λ2,i)
wi−j(f(dj; λ1, λ2) − ej)2,
</equation>
<bodyText confidence="0.999851333333333">
where ei is 1 if the user’s understanding is correct
and 0 if it is not, and δ is the value that determines
the learning speed.
</bodyText>
<sectionHeader confidence="0.999437" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.965885">
4.1 Conditions
</subsectionHeader>
<bodyText confidence="0.9999945">
The lexicon used in the experiments included eleven
items for the static image and six items to describe
the motions. Eleven stuffed toys and four boxes
were used as objects in the interaction between a
user and a robot. In the learning process, the inter-
action was simulated by using software.
</bodyText>
<subsectionHeader confidence="0.999512">
4.2 Learning of decision function
</subsectionHeader>
<bodyText confidence="0.982648769230769">
Sequence X of quadruplets (si, ai, Oi, qi) consisting
of the user’s utterance si, scene Oi, behavioral con-
text qi, and action ai that the user wanted to ask the
robot to perform (i = 1, ..., nd), was used for the
interaction. At the beginning of the sequence, the
sentences were relatively complete (e.g., “green ker-
mit red box move-onto”). Then the length of the sen-
tences was gradually reduced (e.g., “move-onto”).
R could be estimated with high accuracy during
the episodes in which relatively complete utterances
were understood correctly. H and F could be effec-
tively estimated based on the estimation of R when
fragmental utterances were given. Figure 3 shows
</bodyText>
<figure confidence="0.942014">
Episode
Episode
</figure>
<figureCaption confidence="0.999712">
Figure 4: The change in decision error rate
</figureCaption>
<bodyText confidence="0.9999252">
changes in the values of γ1, γ2, γ3, and γ4hc. We
can see that each value was adapted according to
the ambiguity of a given sentence. Figure 4 shows
the decision error (misunderstanding) rates obtained
during the course of the interaction, along with the
error rates obtained in the same data, X, by keeping
the values of the parameters of the decision function
fixed to their initial values.
Examples of actions generated as a result of cor-
rect understanding are shown together with the out-
put log probabilities from the weighted beliefs in
Figs. 5 (a) and (b), along with the second and third
action candidates, which led to incorrect actions. We
can see that each nonlinguistic belief was used ap-
propriately in understanding the utterances. Beliefs
</bodyText>
<figure confidence="0.988363785714286">
1.6
1.4
1.2
1.0
0.8
0.6
0.4
0.2
0
Y 4hc
Y2
Y3
Y1
32 64 128
</figure>
<figureCaption confidence="0.929418">
Figure 3: Changes in the confidence values
</figureCaption>
<figure confidence="0.998503428571428">
w/o learning
with learning
32 64 128
80
Error rate[%]
60
40
20
0
= arg min i
a1,a2 j=i−K
Confidence weights
&amp;quot; Move-onto &amp;quot;
(b)
</figure>
<figureCaption confidence="0.998182">
Figure 5: Examples of fragmental utterances under-
</figureCaption>
<bodyText confidence="0.967967">
stood correctly by the robot
about the behavioral context were more effective in
Fig. 5 (a), while in Fig. 5 (b), beliefs about the object
concepts were more effective than other nonlinguis-
tic beliefs in leading to the correct understanding.
This learning process is described in greater detail
in (Miyata et al., 2001)
</bodyText>
<subsectionHeader confidence="0.996643">
4.3 Learning of the global confidence function
</subsectionHeader>
<bodyText confidence="0.999988742857143">
In the experiments with the learning of the global
confidence function, The robot’s utterances were ex-
pressed through text on a display instead of oral
speech, and they included one word describing the
motion and either no words or one to several words
describing the trajector and landmark objects or just
the trajector object.
A sequence of triplets (a, O, q) consisting of
scene O, behavioral context q, and action a that the
robot needed to ask the user to perform, was used for
the interaction. In each episode, the robot generated
an utterance to bring the global confidence function
as close to 0.75 as possible.
The changes in f(d) are shown in Fig. 5 (a),
where three lines are drawn for d0.5 = f−1(0.5),
d0.75 = f−1(0.75), and d0.9 = f−1(0.9) in order
to make the shape of f(d) easily recognizable. The
changes in the number of words used to describe the
objects in each utterance are shown in Fig. 5 (b),
along with the changes obtained in the case when
f(d) was not learned, which are shown for com-
parison. The initial values were set at d0.9 = 161,
d0.75 = 120, and d0.5 = 100, which means that a
large margin was necessary for an utterance to be
understood correctly. Note that when all the values
are close to 0, the slope in the middle of f is steep,
and the robot makes a decision that a small margin
is enough for its utterances to be understood cor-
rectly. After the learning began, these values rapidly
approached 0, and the number of words decreased.
The slope became temporarily smooth at around the
15th episode. Then, the number of words became
too small, which sometimes lead to misunderstand-
ing. Finally, the slope became steep again at around
the 35th episode.
</bodyText>
<sectionHeader confidence="0.99764" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999926066666667">
The above experiments illustrate the importance of
misunderstanding and clarification, i.e. error and re-
pair, in the formation of mutual beliefs between peo-
ple and machines. In the learning period for utter-
ance understanding by the robot, the values of the
parameters of the decision function changed signif-
icantly when the robot acted incorrectly in the first
trial, and correctly in the second trial. In the learn-
ing period for utterance generation by the robot,
in the experiment in which the target value of the
global confidence function was set to 0.95, which
was larger than 0.75 and closer to 1, the global con-
fidence function was not properly estimated because
almost all utterances were understood correctly (The
results of this experiment are not presented in de-
</bodyText>
<figure confidence="0.999886">
(a)
correct incorrect
previous
Speech Motion Behavioral
Context
1st
(Correct)
3rd
(Incorrect)
Speech Object Motion Behavioral
context
1st
(Correct)
2nd
(Incorrect)
&amp;quot; Grover Small Kermit Jump-over &amp;quot;
correct
incorrect
previous
(a)
0 10 20 30 40 50 60 70
Episode
</figure>
<figureCaption confidence="0.954191">
Figure 6: Changes in the global confidence function
(a) and the number of words needed to describe the
objects in each utterance (b)
</figureCaption>
<bodyText confidence="0.9998922">
tail in this paper). These results show that occa-
sional errors enhance the formation of mutual be-
liefs in both the utterance generation and utterance
understanding processes. This implies that in or-
der to obtain information about mutual beliefs, both
the robot and the user must face the risk of not be-
ing understood correctly. The importance of error
and repair to learning in general has been seen as an
exploration-exploitation trade-off in the area of re-
inforcement learning by machines (e.g. (Dayan and
Sejnowski, 1996)).
The experimental results showed that the robot
could learn its system of the beliefs the robot as-
sumed the user had. Because the user came to un-
derstand the robot’s fragmental and ambiguous ut-
terances, the user and the robot must have shared
similar beliefs, and must have been aware of that.
It would be interesting to investigate by experiment
the dynamics of sharing beliefs between a user and
a robot.
</bodyText>
<sectionHeader confidence="0.999719" genericHeader="method">
6 Related Works
</sectionHeader>
<bodyText confidence="0.999967809523809">
(Winograd, 1972) and (Shapiro et al., 2000) ex-
plored the grounding of the meanings of utterances
in conversation onto the physical world by using
logic, but the researchers did not investigate the pro-
cessing of information from the real physical world.
(Matsui et al., 2000) focused on enabling robots to
work in the real world, and integrated language with
information from robot’s sensors by using pattern
recognition. (Inamura et al., 2000) investigated an
autonomous mobile robot that controlled its actions
and conversations with a user based on a Bayesian
network. The use of Bayesian networks in the in-
terpretation and generation of dialogue was also in-
vestigated by (Lemon et al., 2002). In (Singh et
al., 2000), the learning of dialogue strategies us-
ing reinforcement learning was investigated. Some
of these works looked at beliefs ”held by” the ma-
chines themselves, but none focused on the for-
mation of mutual beliefs between humans and ma-
chines through interaction, based on common expe-
riences.
</bodyText>
<sectionHeader confidence="0.998205" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999930285714286">
The presented method enables the formation of mu-
tual beliefs between people and robots through in-
teraction in physical environments, and it facilitates
the process of human-machine communication. In
the future, I want to focus on the generalization of
learning of mutual beliefs and the learning of dia-
logue control.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996947928571429">
H. Clark. 1996. Using Language. Cambridge University
Press.
P. Dayan and T. J. Sejnowski. 1996. Exploration
Bonuses and Dual Control. Machine Learning, 25:5–
22.
T. Inamura, M. Inaba and H. Inoue. 2000. Integration
Model of Learning Mechanism and Dialogue Strategy
based on Stochastic Experience Representation us-
ing Bayesian Network. Proceedings of International
Workshop on Robot and Human Interactive Communi-
cation, 27–29.
N. Iwahashi. 2001. Language Acquisition by Robots.
The Institute ofElectronics, Information, and Commu-
nication Engineers Technical Report SP2001-96.
</reference>
<figure confidence="0.9992305">
(b)
Number of words
4
3
2
1
w/o learning
with learning
80
90%
75%
50%
0
-20
-400 10 20 30 40 50 60 70
Episode
Margin d
60
40
20
</figure>
<reference confidence="0.999681722222223">
N. Iwahashi. 2003. Language Acquisition by Robots:
Towards New Paradigm of Language Processing.
Journal ofJapanese Society for Artificial Intelligence,
18(1):49-58.
B.-H. Juang and S. Katagiri. 1992. Discriminative
Learning for Minimum Error Classification. IEEE
Transactions on Signal Processing, 40(12):3043–
3054.
O. Lemon, P. Parikh and S. Peters. 2002. Probabilistic
Dialogue Management. Proceedings of Third SIGdial
Workshop on Discourse and Dialogue, 125–128.
T. Matsui, H. Asoh, J. Fry, Y. Motomura, F. Asano, T
Kurita and N. Otsu. 1999. Integrated Natural Spo-
ken Dialogue System of Jijo-2 Mobile Robot for Office
Services. Proceedings of 15th National Conference on
Artificial Intelligence.
A. Miyata, N. Iwahashi and A. Kurematsu. 2001. Mutual
belief forming by robots based on the process of utter-
ance comprehension. Technical Report of The Insti-
tute of Electronics, Information, and Communication
Engineers, SP2001-98.
C. S. Shapiro, H. O. Ismail, and J. F. Santore. 2000. Our
Dinner with Cassie. Working Notes for AAAI 2000
Spring Symposium on Natural Dialogues with Prac-
tical Robotic Devices, 57-61.
S. Singh, M. Kearns, D. J. Litman and M. A. Malker.
2000. Empirical Evaluation of a Reinforce Learning
Spoken Dialogue System. Proc. 16th National Con-
ference on Artificial Intelligence, 645–651.
D. Sperber and D. Wilson. 1995. Relevance (2nd Edi-
tion). Blackwell.
D. R. Traum. 1994. A computationaltheory ofgrounding
in natural language conversation. Unpublished doc-
toral dissertation, University of Rochester.
T. Winograd. 1972. Understanding Natural Language.
Academic Press New York.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.421778">
<title confidence="0.9977705">A Method for Forming Mutual Beliefs for Communication Human-robot Multi-modal Interaction</title>
<author confidence="0.6755">Naoto</author>
<affiliation confidence="0.961948">Sony Computer Science</affiliation>
<address confidence="0.638373">Tokyo,</address>
<email confidence="0.991745">iwahashi@csl.sony.co.jp</email>
<abstract confidence="0.997528333333333">This paper describes a method of multimodal language processing that reflects experiences shared by people and robots. Through incremental online optimization in the process of interaction, the user and the robot form mutual beliefs represented by a stochastic model. Based on these mutual beliefs, the robot can interpret even fragmental and ambiguous utterances, and can act and generate utterances appropriate for a given situation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Clark</author>
</authors>
<title>Using Language.</title>
<date>1996</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="3854" citStr="Clark, 1996" startWordPosition="596" endWordPosition="597">es, lexicon, and grammar, based on common perceptual exFigure 1: Interaction between a user and a robot periences between people and robots (for further detail, see (Iwahashi, 2001; Iwahashi, 2003)). This paper describes a method that enables robots to learn a system of mutual beliefs including nonlinguistic ones through multi-modal language interaction with people. The learning is based on incremental online optimization, and it uses information from raw speech and visual observations as well as behavioral reinforcement, which is integrated in a probabilistic framework. Theoretical research (Clark, 1996) and its computational modelling (Traum, 1994) focused on the formation of mutual beliefs that is a direct target of communication, and aimed at representing the formation of mutual beliefs as a procedure- and ruledriven process. In contrast, this study focuses on a system of mutual beliefs that is used in the process of utterance generation and understanding in a physical environment, and aims at representing the formation of this system by a mathematical model of coupling systems. 2 Task for Forming Mutual Beliefs The task for forming mutual beliefs was set up as follows. A robot was sat at </context>
</contexts>
<marker>Clark, 1996</marker>
<rawString>H. Clark. 1996. Using Language. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Dayan</author>
<author>T J Sejnowski</author>
</authors>
<title>Exploration Bonuses and Dual Control.</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<volume>25</volume>
<pages>22</pages>
<contexts>
<context position="21000" citStr="Dayan and Sejnowski, 1996" startWordPosition="3617" endWordPosition="3620"> confidence function (a) and the number of words needed to describe the objects in each utterance (b) tail in this paper). These results show that occasional errors enhance the formation of mutual beliefs in both the utterance generation and utterance understanding processes. This implies that in order to obtain information about mutual beliefs, both the robot and the user must face the risk of not being understood correctly. The importance of error and repair to learning in general has been seen as an exploration-exploitation trade-off in the area of reinforcement learning by machines (e.g. (Dayan and Sejnowski, 1996)). The experimental results showed that the robot could learn its system of the beliefs the robot assumed the user had. Because the user came to understand the robot’s fragmental and ambiguous utterances, the user and the robot must have shared similar beliefs, and must have been aware of that. It would be interesting to investigate by experiment the dynamics of sharing beliefs between a user and a robot. 6 Related Works (Winograd, 1972) and (Shapiro et al., 2000) explored the grounding of the meanings of utterances in conversation onto the physical world by using logic, but the researchers di</context>
</contexts>
<marker>Dayan, Sejnowski, 1996</marker>
<rawString>P. Dayan and T. J. Sejnowski. 1996. Exploration Bonuses and Dual Control. Machine Learning, 25:5– 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Inamura</author>
<author>M Inaba</author>
<author>H Inoue</author>
</authors>
<title>Integration Model of Learning Mechanism and Dialogue Strategy based on Stochastic Experience Representation using Bayesian Network.</title>
<date>2000</date>
<booktitle>Proceedings of International Workshop on Robot and Human Interactive Communication,</booktitle>
<pages>27--29</pages>
<contexts>
<context position="21868" citStr="Inamura et al., 2000" startWordPosition="3762" endWordPosition="3765">milar beliefs, and must have been aware of that. It would be interesting to investigate by experiment the dynamics of sharing beliefs between a user and a robot. 6 Related Works (Winograd, 1972) and (Shapiro et al., 2000) explored the grounding of the meanings of utterances in conversation onto the physical world by using logic, but the researchers did not investigate the processing of information from the real physical world. (Matsui et al., 2000) focused on enabling robots to work in the real world, and integrated language with information from robot’s sensors by using pattern recognition. (Inamura et al., 2000) investigated an autonomous mobile robot that controlled its actions and conversations with a user based on a Bayesian network. The use of Bayesian networks in the interpretation and generation of dialogue was also investigated by (Lemon et al., 2002). In (Singh et al., 2000), the learning of dialogue strategies using reinforcement learning was investigated. Some of these works looked at beliefs ”held by” the machines themselves, but none focused on the formation of mutual beliefs between humans and machines through interaction, based on common experiences. 7 Conclusion The presented method en</context>
</contexts>
<marker>Inamura, Inaba, Inoue, 2000</marker>
<rawString>T. Inamura, M. Inaba and H. Inoue. 2000. Integration Model of Learning Mechanism and Dialogue Strategy based on Stochastic Experience Representation using Bayesian Network. Proceedings of International Workshop on Robot and Human Interactive Communication, 27–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Iwahashi</author>
</authors>
<title>Language Acquisition by Robots.</title>
<date>2001</date>
<booktitle>The Institute ofElectronics, Information, and Communication Engineers</booktitle>
<tech>Technical Report SP2001-96.</tech>
<contexts>
<context position="3422" citStr="Iwahashi, 2001" startWordPosition="534" endWordPosition="535">ences and in order for linguistic and nonlinguistic beliefs to combine in the process of human-robot interaction. Previous language processing methods, which are characterized by fixed language knowledge, do not satisfy these requirements because they cannot dynamically reflect experiences in the communication process in a real environment. I have been working on methods for forming linguistic beliefs, such as beliefs about phonemes, lexicon, and grammar, based on common perceptual exFigure 1: Interaction between a user and a robot periences between people and robots (for further detail, see (Iwahashi, 2001; Iwahashi, 2003)). This paper describes a method that enables robots to learn a system of mutual beliefs including nonlinguistic ones through multi-modal language interaction with people. The learning is based on incremental online optimization, and it uses information from raw speech and visual observations as well as behavioral reinforcement, which is integrated in a probabilistic framework. Theoretical research (Clark, 1996) and its computational modelling (Traum, 1994) focused on the formation of mutual beliefs that is a direct target of communication, and aimed at representing the format</context>
</contexts>
<marker>Iwahashi, 2001</marker>
<rawString>N. Iwahashi. 2001. Language Acquisition by Robots. The Institute ofElectronics, Information, and Communication Engineers Technical Report SP2001-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Iwahashi</author>
</authors>
<title>Language Acquisition by Robots: Towards New Paradigm of Language Processing.</title>
<date>2003</date>
<journal>Journal ofJapanese Society for Artificial Intelligence,</journal>
<pages>18--1</pages>
<contexts>
<context position="3439" citStr="Iwahashi, 2003" startWordPosition="536" endWordPosition="537">er for linguistic and nonlinguistic beliefs to combine in the process of human-robot interaction. Previous language processing methods, which are characterized by fixed language knowledge, do not satisfy these requirements because they cannot dynamically reflect experiences in the communication process in a real environment. I have been working on methods for forming linguistic beliefs, such as beliefs about phonemes, lexicon, and grammar, based on common perceptual exFigure 1: Interaction between a user and a robot periences between people and robots (for further detail, see (Iwahashi, 2001; Iwahashi, 2003)). This paper describes a method that enables robots to learn a system of mutual beliefs including nonlinguistic ones through multi-modal language interaction with people. The learning is based on incremental online optimization, and it uses information from raw speech and visual observations as well as behavioral reinforcement, which is integrated in a probabilistic framework. Theoretical research (Clark, 1996) and its computational modelling (Traum, 1994) focused on the formation of mutual beliefs that is a direct target of communication, and aimed at representing the formation of mutual bel</context>
</contexts>
<marker>Iwahashi, 2003</marker>
<rawString>N. Iwahashi. 2003. Language Acquisition by Robots: Towards New Paradigm of Language Processing. Journal ofJapanese Society for Artificial Intelligence, 18(1):49-58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B-H Juang</author>
<author>S Katagiri</author>
</authors>
<title>Discriminative Learning for Minimum Error Classification.</title>
<date>1992</date>
<journal>IEEE Transactions on Signal Processing,</journal>
<volume>40</volume>
<issue>12</issue>
<pages>3054</pages>
<contexts>
<context position="13435" citStr="Juang and Katagiri, 1992" startWordPosition="2301" endWordPosition="2304">bot acts incorrectly, the user slaps the robot’s hand. When the robot acts correctly in the first or second trial in an episode, the robot associates utterance s, action a, scene O, and behavioral context q with each other, and makes these associations a learning sample. Then the robot adapts the values of parameter set R for the belief about the motion-object relationship, parameter set H for the belief about the effect of the behavioral context, and a set of weighting parameters, Γ. R is learned by using the Bayesian learning method. H and Γ are learned based on the minimum error criterion (Juang and Katagiri, 1992). Lexicon L and grammar G are given beforehand and are fixed. When the ith sample (si, ai, Oi, qi) is obtained based on this process of association, Hi and Γi are adapted to minimize the probability of misunderstanding based on the minimum error criterion as i wi−j g(d (sj, aj, Oj, qj, L, G, Ri, Hi, Γi)) j=i−K → min, where g(x) is −x if x &lt; 0 and 0 otherwise, and K and wi−j represent the number of latest samples used in the learning process and the weights for each sample, respectively. Global confidence function f is learned incrementally, online, through a sequence of episodes which consist </context>
</contexts>
<marker>Juang, Katagiri, 1992</marker>
<rawString>B.-H. Juang and S. Katagiri. 1992. Discriminative Learning for Minimum Error Classification. IEEE Transactions on Signal Processing, 40(12):3043– 3054.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Lemon</author>
<author>P Parikh</author>
<author>S Peters</author>
</authors>
<title>Probabilistic Dialogue Management.</title>
<date>2002</date>
<booktitle>Proceedings of Third SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>125--128</pages>
<contexts>
<context position="22119" citStr="Lemon et al., 2002" startWordPosition="3803" endWordPosition="3806">eanings of utterances in conversation onto the physical world by using logic, but the researchers did not investigate the processing of information from the real physical world. (Matsui et al., 2000) focused on enabling robots to work in the real world, and integrated language with information from robot’s sensors by using pattern recognition. (Inamura et al., 2000) investigated an autonomous mobile robot that controlled its actions and conversations with a user based on a Bayesian network. The use of Bayesian networks in the interpretation and generation of dialogue was also investigated by (Lemon et al., 2002). In (Singh et al., 2000), the learning of dialogue strategies using reinforcement learning was investigated. Some of these works looked at beliefs ”held by” the machines themselves, but none focused on the formation of mutual beliefs between humans and machines through interaction, based on common experiences. 7 Conclusion The presented method enables the formation of mutual beliefs between people and robots through interaction in physical environments, and it facilitates the process of human-machine communication. In the future, I want to focus on the generalization of learning of mutual bel</context>
</contexts>
<marker>Lemon, Parikh, Peters, 2002</marker>
<rawString>O. Lemon, P. Parikh and S. Peters. 2002. Probabilistic Dialogue Management. Proceedings of Third SIGdial Workshop on Discourse and Dialogue, 125–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Matsui</author>
<author>H Asoh</author>
<author>J Fry</author>
<author>Y Motomura</author>
<author>F Asano</author>
<author>T Kurita</author>
<author>N Otsu</author>
</authors>
<date>1999</date>
<booktitle>Integrated Natural Spoken Dialogue System of Jijo-2 Mobile Robot for Office Services. Proceedings of 15th National Conference on Artificial Intelligence.</booktitle>
<marker>Matsui, Asoh, Fry, Motomura, Asano, Kurita, Otsu, 1999</marker>
<rawString>T. Matsui, H. Asoh, J. Fry, Y. Motomura, F. Asano, T Kurita and N. Otsu. 1999. Integrated Natural Spoken Dialogue System of Jijo-2 Mobile Robot for Office Services. Proceedings of 15th National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Miyata</author>
<author>N Iwahashi</author>
<author>A Kurematsu</author>
</authors>
<title>Mutual belief forming by robots based on the process of utterance comprehension.</title>
<date>2001</date>
<booktitle>Technical Report of The Institute of Electronics, Information, and Communication Engineers,</booktitle>
<pages>2001--98</pages>
<contexts>
<context position="17562" citStr="Miyata et al., 2001" startWordPosition="3030" endWordPosition="3033">the utterances. Beliefs 1.6 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0 Y 4hc Y2 Y3 Y1 32 64 128 Figure 3: Changes in the confidence values w/o learning with learning 32 64 128 80 Error rate[%] 60 40 20 0 = arg min i a1,a2 j=i−K Confidence weights &amp;quot; Move-onto &amp;quot; (b) Figure 5: Examples of fragmental utterances understood correctly by the robot about the behavioral context were more effective in Fig. 5 (a), while in Fig. 5 (b), beliefs about the object concepts were more effective than other nonlinguistic beliefs in leading to the correct understanding. This learning process is described in greater detail in (Miyata et al., 2001) 4.3 Learning of the global confidence function In the experiments with the learning of the global confidence function, The robot’s utterances were expressed through text on a display instead of oral speech, and they included one word describing the motion and either no words or one to several words describing the trajector and landmark objects or just the trajector object. A sequence of triplets (a, O, q) consisting of scene O, behavioral context q, and action a that the robot needed to ask the user to perform, was used for the interaction. In each episode, the robot generated an utterance to</context>
</contexts>
<marker>Miyata, Iwahashi, Kurematsu, 2001</marker>
<rawString>A. Miyata, N. Iwahashi and A. Kurematsu. 2001. Mutual belief forming by robots based on the process of utterance comprehension. Technical Report of The Institute of Electronics, Information, and Communication Engineers, SP2001-98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C S Shapiro</author>
<author>H O Ismail</author>
<author>J F Santore</author>
</authors>
<date>2000</date>
<booktitle>Our Dinner with Cassie. Working Notes for AAAI 2000 Spring Symposium on Natural Dialogues with Practical Robotic Devices,</booktitle>
<pages>57--61</pages>
<contexts>
<context position="21468" citStr="Shapiro et al., 2000" startWordPosition="3698" endWordPosition="3701">ing in general has been seen as an exploration-exploitation trade-off in the area of reinforcement learning by machines (e.g. (Dayan and Sejnowski, 1996)). The experimental results showed that the robot could learn its system of the beliefs the robot assumed the user had. Because the user came to understand the robot’s fragmental and ambiguous utterances, the user and the robot must have shared similar beliefs, and must have been aware of that. It would be interesting to investigate by experiment the dynamics of sharing beliefs between a user and a robot. 6 Related Works (Winograd, 1972) and (Shapiro et al., 2000) explored the grounding of the meanings of utterances in conversation onto the physical world by using logic, but the researchers did not investigate the processing of information from the real physical world. (Matsui et al., 2000) focused on enabling robots to work in the real world, and integrated language with information from robot’s sensors by using pattern recognition. (Inamura et al., 2000) investigated an autonomous mobile robot that controlled its actions and conversations with a user based on a Bayesian network. The use of Bayesian networks in the interpretation and generation of dia</context>
</contexts>
<marker>Shapiro, Ismail, Santore, 2000</marker>
<rawString>C. S. Shapiro, H. O. Ismail, and J. F. Santore. 2000. Our Dinner with Cassie. Working Notes for AAAI 2000 Spring Symposium on Natural Dialogues with Practical Robotic Devices, 57-61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Singh</author>
<author>M Kearns</author>
<author>D J Litman</author>
<author>M A Malker</author>
</authors>
<title>Empirical Evaluation of a Reinforce Learning Spoken Dialogue System.</title>
<date>2000</date>
<booktitle>Proc. 16th National Conference on Artificial Intelligence,</booktitle>
<pages>645--651</pages>
<contexts>
<context position="22144" citStr="Singh et al., 2000" startWordPosition="3808" endWordPosition="3811">conversation onto the physical world by using logic, but the researchers did not investigate the processing of information from the real physical world. (Matsui et al., 2000) focused on enabling robots to work in the real world, and integrated language with information from robot’s sensors by using pattern recognition. (Inamura et al., 2000) investigated an autonomous mobile robot that controlled its actions and conversations with a user based on a Bayesian network. The use of Bayesian networks in the interpretation and generation of dialogue was also investigated by (Lemon et al., 2002). In (Singh et al., 2000), the learning of dialogue strategies using reinforcement learning was investigated. Some of these works looked at beliefs ”held by” the machines themselves, but none focused on the formation of mutual beliefs between humans and machines through interaction, based on common experiences. 7 Conclusion The presented method enables the formation of mutual beliefs between people and robots through interaction in physical environments, and it facilitates the process of human-machine communication. In the future, I want to focus on the generalization of learning of mutual beliefs and the learning of </context>
</contexts>
<marker>Singh, Kearns, Litman, Malker, 2000</marker>
<rawString>S. Singh, M. Kearns, D. J. Litman and M. A. Malker. 2000. Empirical Evaluation of a Reinforce Learning Spoken Dialogue System. Proc. 16th National Conference on Artificial Intelligence, 645–651.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Sperber</author>
<author>D Wilson</author>
</authors>
<date>1995</date>
<booktitle>Relevance (2nd Edition).</booktitle>
<publisher>Blackwell.</publisher>
<contexts>
<context position="882" citStr="Sperber and Wilson, 1995" startWordPosition="129" endWordPosition="132">eflects experiences shared by people and robots. Through incremental online optimization in the process of interaction, the user and the robot form mutual beliefs represented by a stochastic model. Based on these mutual beliefs, the robot can interpret even fragmental and ambiguous utterances, and can act and generate utterances appropriate for a given situation. 1 Introduction The process of human communication is based on certain beliefs shared by those who are communicating. Language is one such mutual belief, and it is used to convey meaning based on its relevance to other mutual beliefs (Sperber and Wilson, 1995). These mutual beliefs are formed through interaction with the environment and other people, and the meaning of utterances is embedded in such shared experiences. If those communicating want to logically convince each other that proposition p is a mutual belief, they must prove that the infinitely nested proposition “They have information that they have information that ... that they have information that p” also holds. However, in reality, all we can do is assume, based on a few clues, that our beliefs are identical to those of other people we are talking to. That is, it can never be guarante</context>
</contexts>
<marker>Sperber, Wilson, 1995</marker>
<rawString>D. Sperber and D. Wilson. 1995. Relevance (2nd Edition). Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Traum</author>
</authors>
<title>A computationaltheory ofgrounding in natural language conversation. Unpublished doctoral dissertation,</title>
<date>1994</date>
<institution>University of Rochester.</institution>
<contexts>
<context position="3900" citStr="Traum, 1994" startWordPosition="603" endWordPosition="604">eptual exFigure 1: Interaction between a user and a robot periences between people and robots (for further detail, see (Iwahashi, 2001; Iwahashi, 2003)). This paper describes a method that enables robots to learn a system of mutual beliefs including nonlinguistic ones through multi-modal language interaction with people. The learning is based on incremental online optimization, and it uses information from raw speech and visual observations as well as behavioral reinforcement, which is integrated in a probabilistic framework. Theoretical research (Clark, 1996) and its computational modelling (Traum, 1994) focused on the formation of mutual beliefs that is a direct target of communication, and aimed at representing the formation of mutual beliefs as a procedure- and ruledriven process. In contrast, this study focuses on a system of mutual beliefs that is used in the process of utterance generation and understanding in a physical environment, and aims at representing the formation of this system by a mathematical model of coupling systems. 2 Task for Forming Mutual Beliefs The task for forming mutual beliefs was set up as follows. A robot was sat at a table so that the robot and the user sitting</context>
</contexts>
<marker>Traum, 1994</marker>
<rawString>D. R. Traum. 1994. A computationaltheory ofgrounding in natural language conversation. Unpublished doctoral dissertation, University of Rochester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Winograd</author>
</authors>
<title>Understanding Natural Language.</title>
<date>1972</date>
<publisher>Academic Press</publisher>
<location>New York.</location>
<contexts>
<context position="21441" citStr="Winograd, 1972" startWordPosition="3695" endWordPosition="3696">r and repair to learning in general has been seen as an exploration-exploitation trade-off in the area of reinforcement learning by machines (e.g. (Dayan and Sejnowski, 1996)). The experimental results showed that the robot could learn its system of the beliefs the robot assumed the user had. Because the user came to understand the robot’s fragmental and ambiguous utterances, the user and the robot must have shared similar beliefs, and must have been aware of that. It would be interesting to investigate by experiment the dynamics of sharing beliefs between a user and a robot. 6 Related Works (Winograd, 1972) and (Shapiro et al., 2000) explored the grounding of the meanings of utterances in conversation onto the physical world by using logic, but the researchers did not investigate the processing of information from the real physical world. (Matsui et al., 2000) focused on enabling robots to work in the real world, and integrated language with information from robot’s sensors by using pattern recognition. (Inamura et al., 2000) investigated an autonomous mobile robot that controlled its actions and conversations with a user based on a Bayesian network. The use of Bayesian networks in the interpret</context>
</contexts>
<marker>Winograd, 1972</marker>
<rawString>T. Winograd. 1972. Understanding Natural Language. Academic Press New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>