<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001325">
<title confidence="0.9987645">
Transliteration using a Phrase-based Statistical Machine Translation
System to Re-score the Output of a Joint Multigram Model
</title>
<author confidence="0.93489">
Andrew Finch
</author>
<affiliation confidence="0.515674">
NICT
3-5 Hikaridai
Keihanna Science City
</affiliation>
<address confidence="0.657779">
619-0289 JAPAN
</address>
<email confidence="0.980327">
andrew.finch@nict.go.jp
</email>
<sectionHeader confidence="0.993485" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999388730769231">
The system presented in this paper uses a
combination of two techniques to directly
transliterate from grapheme to grapheme. The
technique makes no language specific as-
sumptions, uses no dictionaries or explicit
phonetic information; the process transforms
sequences of tokens in the source language
directly into to sequences of tokens in the
target. All the language pairs in our experi-
ments were transliterated by applying this
technique in a single unified manner. The
approach we take is that of hypothesis re-
scoring to integrate the models of two state-
of-the-art techniques: phrase-based statistical
machine translation (SMT), and a joint multi-
gram model. The joint multigram model was
used to generate an n-best list of translitera-
tion hypotheses that were re-scored using the
models of the phrase-based SMT system. The
both of the models’ scores for each hypothesis
were linearly interpolated to produce a final
hypothesis score that was used to re-rank the
hypotheses. In our experiments on develop-
ment data, the combined system was able to
outperform both of its component systems
substantially.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999718875">
In statistical machine translation the re-scoring
of hypotheses produced by a system with addi-
tional models that incorporate information not
available to the original system has been shown
to be an effective technique to improve system
performance (Paul et al., 2006). Our approach
uses a re-scoring technique to integrate the
models of two transliteration systems that are
each capable in their own right: a phrase-based
statistical machine translation system (Koehn et
al., 2003), and a joint multigram model (Deligne
and Bimbot, 1995; Bisani and Ney, 2008).
In this work we treat the process of translit-
eration as a process of direct transduction from
sequences of tokens in the source language to
sequences of tokens in the target language with
</bodyText>
<note confidence="0.57278875">
Eiichiro Sumita
NICT
3-5 Hikaridai
Keihanna Science City
</note>
<sectionHeader confidence="0.783186" genericHeader="introduction">
619-0289 JAPAN
</sectionHeader>
<email confidence="0.705921">
eiichiro.sumita@nict.go.jp
</email>
<bodyText confidence="0.999918555555556">
no modeling of the phonetics of either source or
target language (Knight and Graehl, 1997). Tak-
ing this approach allows for a very general
transliteration system to be built that does not
require any language specific knowledge to be
incorporated into the system (for some language
pairs this may not be the best strategy since lin-
guistic information can be used to overcome
issues of data sparseness on smaller datasets).
</bodyText>
<sectionHeader confidence="0.933951" genericHeader="method">
2 Component Systems
</sectionHeader>
<bodyText confidence="0.999929571428572">
For this shared task we chose to combine two
systems through a process of re-scoring. The
systems were selected because of their expected
strong level of performance (SMT systems have
been used successfully in the field, and joint
multigram models have performed well both in
grapheme to phoneme conversion and Arabic-
English transliteration). Secondly, the joint mul-
tigram model relies on key features not present
in the SMT system, that is the history of bilin-
gual phrase pairs used to derive the target. For
this reason we felt the systems would comple-
ment each other well. We now briefly describe
the component systems.
</bodyText>
<subsectionHeader confidence="0.966923">
2.1 Joint Multigram Model
</subsectionHeader>
<bodyText confidence="0.985631666666667">
The joint multigram approach proposed by (De-
ligne and Bimbot, 1995) has arisen as an exten-
sion of the use of variable-length n-grams (mul-
tigrams) in language modeling. In a joint multi-
gram, the units in the model consist of multiple
input and output symbols. (Bisani and Ney,
2008) refined the approach and applied to it
grapheme-to-phoneme conversion, where its
performance was shown to be comparable to
state-of-the-art systems. The approach was later
applied to Arabic-English transliteration (Dese-
laers et al., 2009) again with promising results.
</bodyText>
<listItem confidence="0.759938">
Joint multigram models have the following
characteristics:
• The symbols in the source and target are
co-segmented
</listItem>
<page confidence="0.990616">
48
</page>
<note confidence="0.8145045">
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 48–52,
Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics
</note>
<figure confidence="0.995173">
0.9
0.7
F-Score 0.6
0.4
0.3
Joint Multigram Size
</figure>
<figureCaption confidence="0.9888845">
Figure 1: The effect on F-score by tuning with
respect to joint multigram size
</figureCaption>
<listItem confidence="0.967318833333333">
- Maximum likelihood training using an
EM algorithm (Deligne and Bimbot,
1995)
• The probability of sequences of joint mul-
tigrams is modeled using an n-gram
model
</listItem>
<bodyText confidence="0.99955">
In these respects the model can be viewed as
a close relative of the joint source channel
model proposed by (Li et al., 2004) for translit-
eration.
</bodyText>
<subsectionHeader confidence="0.987984">
2.2 Phrase-based SMT
</subsectionHeader>
<bodyText confidence="0.999161">
It is possible to view the process of translitera-
tion as a process of translation at the character
level, without re-ordering. From this perspective
it is possible to directly employ a phrase-based
SMT system in the task of transliteration (Finch
and Sumita, 2008; Rama and Gali, 2009). A
phrase-based SMT system has the following
characteristics:
</bodyText>
<listItem confidence="0.929053454545455">
• The symbols in the source and target are
aligned one to many in both directions.
Joint sequences of source and target sym-
bols are heuristically extracted given
these alignments
• Transliteration is performed using a log-
linear model with weights tuned on de-
velopment data
• The models include: a translation model
(with 5 sub-models), and a target lan-
guage model
</listItem>
<bodyText confidence="0.9999194">
The bilingual phrase-pairs are analogous to
the joint multigrams, however the translation
model of the SMT system doesn’t use the con-
text of previously translated phrase-pairs, in-
stead relying on a target language model.
</bodyText>
<sectionHeader confidence="0.996319" genericHeader="method">
3 Experimental Conditions
</sectionHeader>
<subsectionHeader confidence="0.998709">
3.1 SMT Decoder
</subsectionHeader>
<bodyText confidence="0.9999907">
In our experiments we used an in-house phrase-
based statistical machine translation decoder
called CleopATRa. This decoder operates on
exactly the same principles as the publicly
available MOSES decoder (Koehn et al., 2003).
Our decoder was modified to be able to decode
source sequences with reference to a target se-
quence; the decoding process being forced to
generate the target. The decoder was also con-
figured to combine scores of multiple deriva-
tions yielding the same target sequence. In this
way the models in the decoder were used to de-
rive scores used to re-score the n-best (we used
n=20 for our experiments) hypotheses generated
by the joint multigram model. The phrase-
extraction process was symmetrized with re-
spect to token order using the technique pro-
posed in (Finch and Sumita, 2010). In order to
adapt the SMT system to the task of translitera-
tion, the decoder was constrained decode in a
monotone manner, and furthermore during train-
ing, the phrase extraction process was con-
strained such that only phrases with monotone
order were extracted in order to minimize the
effects of errors in the word alignment process.
In a final step the scores from both systems
were linearly interpolated to produce a single
integrated hypothesis score. The hypotheses
were then re-ranked according to this integrated
score for the final submission.
</bodyText>
<subsectionHeader confidence="0.993239">
3.2 Joint Multigram model
</subsectionHeader>
<bodyText confidence="0.999986266666667">
For the joint multigram system we used the pub-
licly available Sequitur G2P grapheme-to-
phoneme converter (Bisani and Ney, 2008). The
system was used with its default settings, and
pilot experiments were run on development data
to determine appropriate settings for the maxi-
mum size of the multigrams. The results for the
English-to-Japanese task are shown in Figure 1.
As can be seen in the figure, the system rapidly
improves to a near-optimal value with a maxi-
mum multigram size of 4. No improvement at
all was observed for sizes over 7. We therefore
chose a maximum multigram size of 8 for the
experiments presented in this paper, and for the
systems entered in the shared task.
</bodyText>
<subsectionHeader confidence="0.999562">
3.3 Pre-processing
</subsectionHeader>
<bodyText confidence="0.9995038">
In order to reduce data sparseness we took the
decision to work with data in only its lowercase
form.
We chose not to perform any tokenization or
phonetic mapping for any of the language pairs
</bodyText>
<equation confidence="0.606422">
1 2 3 4 5 6 7 8 9 10
</equation>
<page confidence="0.836248">
49
</page>
<figure confidence="0.995991666666667">
Word 1 Word 2 Word m
Segment into individual words and transliterate each word independently
Search for the best path
hypothesis 1
hypothesis 2
hypothesis n
n-best
Transliterate
...
hypothesis 1
hypothesis 2
hypothesis n
n-best
Transliterate
...
hypothesis 1
hypothesis 2
hypothesis n
n-best
Transliterate
...
</figure>
<figureCaption confidence="0.990946">
Figure 2: The transliteration process for multi-word sequences
</figureCaption>
<figure confidence="0.997939">
0.85
F-Score 0.84
0.83
0 0.2 0.4 0.6 0.8 1.0
SMT System Interpolation Weight
</figure>
<figureCaption confidence="0.987942333333333">
Figure 3: The effect on the F-score of the integrated
system by tuning with respect to the SMT system’s
interpolation weight
</figureCaption>
<bodyText confidence="0.8557585">
in the shared task. We adopted this approach
because:
</bodyText>
<listItem confidence="0.9565076">
• It allowed us to have a single unified
approach for all language pairs
• It was in the spirit of the shared task, as
it did not require extra knowledge out-
side of the supplied corpora
</listItem>
<subsectionHeader confidence="0.998562">
3.4 Handling Multi-Word Sequences
</subsectionHeader>
<bodyText confidence="0.998991">
The data for some languages contained some
multi-word sequences. To handle these we had
to consider the following strategies:
</bodyText>
<listItem confidence="0.99488925">
• Introduce a &lt;space&gt; token into the se-
quence, and treat it as one long charac-
ter sequence to transliterate; or
• Segment the word sequences into indi-
</listItem>
<bodyText confidence="0.91493155">
vidual words and transliterate these in-
dependently, combining the n-best hy-
pothesis lists for all the individual words
in the sequence into a single output se-
quence.
We adopted both approaches: for those multi-
word sequences where the number of words in
the source and target matched, the latter ap-
proach was taken; for those where the numbers
of source and target words differed, the former
approach was taken. The decoding process for
multi-word sequences is shown in Figure 2.
During recombination, the score for the target
word sequence was calculated as the product of
the scores of each hypothesis for each word.
Therefore a search over all combinations of hy-
potheses is required. In almost all cases we were
able to perform a full search. For the rare long
word sequences in the data, a beam search strat-
egy was adopted.
</bodyText>
<subsectionHeader confidence="0.98469">
3.5 Building the Models
</subsectionHeader>
<bodyText confidence="0.999997428571429">
For the final submissions, all systems were
trained on the union of the training data and de-
velopment data. It was felt that the training set
was sufficiently small that the inclusion of the
development data into the training set would
yield a reasonable boost in performance by in-
creasing the coverage of the systems. All tunable
parameters were tuned on development data us-
ing systems built using only the training data.
Under the assumption that these parameters
would perform well in the systems trained on
the combined development/training corpora,
these tuned parameters were transferred directly
to the systems trained on all available data.
</bodyText>
<subsectionHeader confidence="0.995552">
3.6 Parameter Tuning
</subsectionHeader>
<bodyText confidence="0.999843666666667">
The SMT systems were tuned using the mini-
mum error rate training procedure introduced in
(Och, 2003). For convenience, we used BLEU
as a proxy for the various metrics used in the
shared task evaluation. The BLEU score is
commonly used to evaluate the performance of
</bodyText>
<page confidence="0.983083">
50
</page>
<table confidence="0.999766909090909">
Language Pair Mean MRR MAPref
Accuracy in F-score
top-1
English ➝ Thai 0.412 0.883 0.550 0.412
Thai ➝ English 0.397 0.873 0.525 0.397
English ➝ Hindi 0.445 0.884 0.574 0.445
English ➝ Tamil 0.390 0.887 0.522 0.390
English ➝ Kannada 0.371 0.871 0.506 0.371
English ➝ Japanese 0.378 0.783 0.510 0.378
Arabic ➝ English 0.403 0.891 0.512 0.327
English ➝ Bangla 0.412 0.883 0.550 0.412
</table>
<tableCaption confidence="0.999948">
Table 1: The results of our system in the official evaluation on the test data on all performance metrics.
</tableCaption>
<bodyText confidence="0.999933333333333">
machine translation systems and is a function of
the geometric mean of n-gram precision. The
use of BLEU score as a proxy has been shown
to be a reasonable strategy for the metrics used
in these experiments (Finch and Sumita, 2009).
Nonetheless, it is reasonable to assume that one
would be able to improve the performance in a
particular evaluation metric by doing minimum
error rate training specifically for that metric.
The interpolation weight was tuned by a grid
search to find the value that gave the maximal f-
score (according to the official f-score evalua-
tion metric for the shared task) on the develop-
ment data, the process for English-Japanese is
shown in Figure 3.
</bodyText>
<sectionHeader confidence="0.999867" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.99997240625">
The results of our experiments are shown in Ta-
ble 1. These results are the official shared task
evaluation results on the test data, and the scores
for all of the evaluation metrics are shown in the
table. The reader is referred to the workshop
white paper (Li et al., 2010) for details of the
evaluation metrics. The system achieved a high
level of performance on most of the language
pairs. Comparing the individual systems to each
other, and to the integrated system, the joint
multigram system outperformed the phrase-
based SMT system. In experiments run on the
English-to-Japanese katakana task, the joint
multigram system in isolation achieved an F-
score of 0.837 on development data, whereas the
SMT system in isolation achieved an F-score of
0.824. When integrated the models of the sys-
tems complemented each other well, and on the
same English-Japanese task the integrated sys-
tem achieved an F-score of 0.843.
We feel that for some language pairs, most
notably Arabic-English where a large difference
existed between our system and the top-ranked
system, there is much room for improvement.
One of the strengths in terms of the utility of our
approach is that it is free from dependence on
the linguistic characteristics of the languages
being processed. This property makes it gener-
ally applicable, but due to the limited amounts
of data available for the shared task, we believe
that in order to progress, a language-dependent
approach will be required.
</bodyText>
<sectionHeader confidence="0.998548" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99999025">
We applied a system that integrated two state-of-
the-art systems through a process of re-scoring,
to the NEWS 2010 Workshop shared task on
transliteration generation. Our systems gave a
strong performance on the shared task test set,
and our experiments show the integrated system
was able to outperform both of its component
systems. In future work we would like to depart
from the direct grapheme-to-grapheme approach
taken here and address the problem of how best
to represent the source and target sequences by
either analyzing their symbols further, or ag-
glomerating them. We would also like to inves-
tigate the use of co-segmentation schemes that
do not rely on maximum likelihood training to
overcome the issues inherent in this technique.
</bodyText>
<sectionHeader confidence="0.996307" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999814571428571">
The results presented in this paper draw on the
following data sets. For English-Japanese and
Arabic-English, the reader is referred to the CJK
website: http://www.cjk.org. For English-Hindi,
English-Tamil, and English-Kannada, and
English-Bangla the data sets originated from the
work of Kumaran and Kellner, 2007.
</bodyText>
<page confidence="0.997992">
51
</page>
<sectionHeader confidence="0.990078" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999295661764706">
Peter Brown, Stephen Della Pietra, Vincent Della
Pietra, and Robert Mercer, 1991. The mathematics
of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2), 263-
311.
Sabine Deligne, and Fred6ric Bimbot, 1995. Lan-
guage modeling by variable length sequences:
theoretical formulation and evaluation of multi-
grams. In: Proc. IEEE Internat. Conf. on Acous-
tics, Speech and Signal Processing, Vol. 1, Detroit,
MI, USA, pp. 169–172.
Maximilian Bisani and Hermann Ney, 2008. Joint-
Sequence Models for Grapheme-to-Phoneme
Conversion. Speech Communication, Volume 50,
Issue 5, Pages 434-451.
Thomas Deselaers, Sasa Hasan, Oliver Bender, and
Hermann Ney, 2009. A Deep Learning Approach
to Machine Transliteration. In Proceedings of the
EACL 2009 Workshop on Statistical Machine
Translation (WMT09), Athens, Greece.
Andrew Finch and Eiichiro Sumita, 2008. Phrase-
based machine transliteration. In Proceedings of
WTCAST&apos;08, pages 13-18.
Andrew Finch and Eiichiro Sumita, 2009. Translit-
eration by Bidirectional Statistical Machine Trans-
lation, Proceedings of the 2009 Named Entities
Workshop: Shared Task on Transliteration, Singa-
pore.
Andrew Finch and Eiichiro Sumita, 2010. Exploiting
Directional Asymmetry in Phrase-table Generation
for Statistical Machine Translation, In Proceed-
ings of NLP2010, Tokyo, Japan.
Kevin Knight and Jonathan Graehl, 1997. Machine
Transliteration. Proceedings of the Thirty-Fifth
Annual Meeting of the Association for Computa-
tional Linguistics and Eighth Conference of the
European Chapter of the Association for Compu-
tational Linguistics, pp. 128-135, Somerset, New
Jersey.
Philipp Koehn, Franz Josef Och, and Daniel Marcu,
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the Human Language Technology
Conference 2003 (HLT-NAACL 2003), Edmonton,
Canada.
Franz Josef Och, 2003. Minimum error rate training
for statistical machine translation, Proceedings of
the ACL.
A Kumaran and Tobias Kellner, 2007. A generic
framework for machine transliteration, Proc. of
the 30th SIGIR.
Haizhou Li, Min Zhang, Jian Su, 2004. A joint source
channel model for machine transliteration, Proc.
of the 42nd ACL.
Haizhou Li, A Kumaran, Min Zhang and Vladimir
Pervouchine, 2010. Whitepaper of NEWS 2010
Shared Task on Transliteration Generation. In
Proc. of ACL2010 Named Entities Workshop.
Michael Paul, Eiichiro Sumita and Seiichi Yama-
moto, 2006, Multiple Translation-Engine-based
Hypotheses and Edit-Distance-based Rescoring
for a Greedy Decoder for Statistical Machine
Translation, Information and Media Technologies,
Vol. 1, No. 1, pp.446-460 .
Taraka Rama and Karthik Gali, 2009. Modeling ma-
chine transliteration as a phrase based statistical
machine translation problem, Proceedings of the
2009 Named Entities Workshop: Shared Task on
Transliteration, Singapore.
</reference>
<page confidence="0.998841">
52
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9847315">Transliteration using a Phrase-based Statistical Machine System to Re-score the Output of a Joint Multigram Model</title>
<author confidence="0.914567">Andrew</author>
<pubnum confidence="0.694367">3-5</pubnum>
<affiliation confidence="0.917111">Keihanna Science</affiliation>
<address confidence="0.73738">619-0289</address>
<email confidence="0.954197">andrew.finch@nict.go.jp</email>
<abstract confidence="0.961965529577466">The system presented in this paper uses a combination of two techniques to directly transliterate from grapheme to grapheme. The technique makes no language specific assumptions, uses no dictionaries or explicit phonetic information; the process transforms sequences of tokens in the source language directly into to sequences of tokens in the target. All the language pairs in our experiments were transliterated by applying this technique in a single unified manner. The approach we take is that of hypothesis rescoring to integrate the models of two stateof-the-art techniques: phrase-based statistical machine translation (SMT), and a joint multigram model. The joint multigram model was to generate an list of transliteration hypotheses that were re-scored using the models of the phrase-based SMT system. The both of the models’ scores for each hypothesis were linearly interpolated to produce a final hypothesis score that was used to re-rank the hypotheses. In our experiments on development data, the combined system was able to outperform both of its component systems substantially. In statistical machine translation the re-scoring of hypotheses produced by a system with additional models that incorporate information not available to the original system has been shown to be an effective technique to improve system performance (Paul et al., 2006). Our approach uses a re-scoring technique to integrate the models of two transliteration systems that are each capable in their own right: a phrase-based statistical machine translation system (Koehn et al., 2003), and a joint multigram model (Deligne and Bimbot, 1995; Bisani and Ney, 2008). In this work we treat the process of transliteration as a process of direct transduction from sequences of tokens in the source language to sequences of tokens in the target language with Eiichiro 3-5 Keihanna Science 619-0289 eiichiro.sumita@nict.go.jp no modeling of the phonetics of either source or target language (Knight and Graehl, 1997). Taking this approach allows for a very general transliteration system to be built that does not require any language specific knowledge to be incorporated into the system (for some language pairs this may not be the best strategy since linguistic information can be used to overcome issues of data sparseness on smaller datasets). Systems For this shared task we chose to combine two systems through a process of re-scoring. The systems were selected because of their expected strong level of performance (SMT systems have been used successfully in the field, and joint multigram models have performed well both in grapheme to phoneme conversion and Arabic- English transliteration). Secondly, the joint multigram model relies on key features not present in the SMT system, that is the history of bilingual phrase pairs used to derive the target. For this reason we felt the systems would complement each other well. We now briefly describe the component systems. Multigram Model The joint multigram approach proposed by (Deligne and Bimbot, 1995) has arisen as an extenof the use of variable-length (multigrams) in language modeling. In a joint multigram, the units in the model consist of multiple input and output symbols. (Bisani and Ney, 2008) refined the approach and applied to it grapheme-to-phoneme conversion, where its performance was shown to be comparable to state-of-the-art systems. The approach was later applied to Arabic-English transliteration (Deselaers et al., 2009) again with promising results. Joint multigram models have the following characteristics: • The symbols in the source and target are co-segmented 48 of the 2010 Named Entities Workshop, ACL pages Sweden, 16 July 2010. Association for Computational Linguistics 0.9 0.7 F-Score 0.6 0.4 0.3 Joint Multigram Size Figure 1: The effect on F-score by tuning with respect to joint multigram size likelihood training using an EM algorithm (Deligne and Bimbot, 1995) • The probability of sequences of joint mulis modeled using an model In these respects the model can be viewed as a close relative of the joint source channel model proposed by (Li et al., 2004) for transliteration. SMT It is possible to view the process of transliteration as a process of translation at the character level, without re-ordering. From this perspective it is possible to directly employ a phrase-based SMT system in the task of transliteration (Finch and Sumita, 2008; Rama and Gali, 2009). A phrase-based SMT system has the following characteristics: • The symbols in the source and target are aligned one to many in both directions. Joint sequences of source and target symbols are heuristically extracted given these alignments • Transliteration is performed using a loglinear model with weights tuned on development data • The models include: a translation model (with 5 sub-models), and a target language model The bilingual phrase-pairs are analogous to the joint multigrams, however the translation model of the SMT system doesn’t use the context of previously translated phrase-pairs, instead relying on a target language model. Conditions Decoder In our experiments we used an in-house phrasebased statistical machine translation decoder called CleopATRa. This decoder operates on exactly the same principles as the publicly available MOSES decoder (Koehn et al., 2003). Our decoder was modified to be able to decode source sequences with reference to a target sequence; the decoding process being forced to generate the target. The decoder was also configured to combine scores of multiple derivations yielding the same target sequence. In this way the models in the decoder were used to descores used to re-score the (we used for our experiments) hypotheses generated by the joint multigram model. The phraseextraction process was symmetrized with respect to token order using the technique proposed in (Finch and Sumita, 2010). In order to adapt the SMT system to the task of transliteration, the decoder was constrained decode in a monotone manner, and furthermore during training, the phrase extraction process was constrained such that only phrases with monotone order were extracted in order to minimize the effects of errors in the word alignment process. In a final step the scores from both systems were linearly interpolated to produce a single integrated hypothesis score. The hypotheses were then re-ranked according to this integrated score for the final submission. Multigram model For the joint multigram system we used the publicly available Sequitur G2P grapheme-tophoneme converter (Bisani and Ney, 2008). The system was used with its default settings, and pilot experiments were run on development data to determine appropriate settings for the maximum size of the multigrams. The results for the English-to-Japanese task are shown in Figure 1. As can be seen in the figure, the system rapidly improves to a near-optimal value with a maximum multigram size of 4. No improvement at all was observed for sizes over 7. We therefore chose a maximum multigram size of 8 for the experiments presented in this paper, and for the systems entered in the shared task. In order to reduce data sparseness we took the decision to work with data in only its lowercase form. We chose not to perform any tokenization or phonetic mapping for any of the language pairs 1 2 3 4 5 6 7 8 9 10 49 1 Word 2 Word Segment into individual words and transliterate each word independently Search for the best path hypothesis 1 hypothesis 2 Transliterate ... hypothesis 1 hypothesis 2 Transliterate ... hypothesis 1 hypothesis 2 Transliterate ... 2: The for multi-word sequences 0.85 F-Score 0.84 0.83 0 0.2 0.4 0.6 0.8 1.0 SMT System Interpolation Weight Figure 3: The effect on the F-score of the integrated system by tuning with respect to the SMT system’s interpolation weight in the shared task. We adopted this approach because: • It allowed us to have a single unified approach for all language pairs • It was in the spirit of the shared task, as it did not require extra knowledge outside of the supplied corpora Multi-Word Sequences The data for some languages contained some multi-word sequences. To handle these we had to consider the following strategies: • Introduce a &lt;space&gt; token into the sequence, and treat it as one long character sequence to transliterate; or • Segment the word sequences into individual words and transliterate these incombining the hypothesis lists for all the individual words in the sequence into a single output sequence. We adopted both approaches: for those multiword sequences where the number of words in the source and target matched, the latter approach was taken; for those where the numbers of source and target words differed, the former approach was taken. The decoding process for multi-word sequences is shown in Figure 2. During recombination, the score for the target word sequence was calculated as the product of the scores of each hypothesis for each word. Therefore a search over all combinations of hypotheses is required. In almost all cases we were able to perform a full search. For the rare long word sequences in the data, a beam search strategy was adopted. the Models For the final submissions, all systems were trained on the union of the training data and development data. It was felt that the training set was sufficiently small that the inclusion of the development data into the training set would yield a reasonable boost in performance by increasing the coverage of the systems. All tunable parameters were tuned on development data using systems built using only the training data. Under the assumption that these parameters would perform well in the systems trained on the combined development/training corpora, these tuned parameters were transferred directly to the systems trained on all available data. Tuning The SMT systems were tuned using the minimum error rate training procedure introduced in (Och, 2003). For convenience, we used BLEU as a proxy for the various metrics used in the shared task evaluation. The BLEU score is commonly used to evaluate the performance of 50 Language Pair F-score MRR Accuracy in top-1 0.412 0.883 0.550 0.412 0.397 0.873 0.525 0.397 0.445 0.884 0.574 0.445 0.390 0.887 0.522 0.390 0.371 0.871 0.506 0.371 0.378 0.783 0.510 0.378 0.403 0.891 0.512 0.327 0.412 0.883 0.550 0.412 Table 1: The results of our system in the official evaluation on the test data on all performance metrics. machine translation systems and is a function of geometric mean of precision. The use of BLEU score as a proxy has been shown to be a reasonable strategy for the metrics used in these experiments (Finch and Sumita, 2009). Nonetheless, it is reasonable to assume that one would be able to improve the performance in a particular evaluation metric by doing minimum error rate training specifically for that metric. The interpolation weight was tuned by a grid search to find the value that gave the maximal fscore (according to the official f-score evaluation metric for the shared task) on the development data, the process for English-Japanese is shown in Figure 3. The results of our experiments are shown in Table 1. These results are the official shared task evaluation results on the test data, and the scores for all of the evaluation metrics are shown in the table. The reader is referred to the workshop white paper (Li et al., 2010) for details of the evaluation metrics. The system achieved a high level of performance on most of the language pairs. Comparing the individual systems to each other, and to the integrated system, the joint multigram system outperformed the phrasebased SMT system. In experiments run on the English-to-Japanese katakana task, the joint multigram system in isolation achieved an Fscore of 0.837 on development data, whereas the SMT system in isolation achieved an F-score of 0.824. When integrated the models of the systems complemented each other well, and on the same English-Japanese task the integrated system achieved an F-score of 0.843. We feel that for some language pairs, most notably Arabic-English where a large difference existed between our system and the top-ranked system, there is much room for improvement. One of the strengths in terms of the utility of our approach is that it is free from dependence on the linguistic characteristics of the languages being processed. This property makes it generally applicable, but due to the limited amounts of data available for the shared task, we believe that in order to progress, a language-dependent approach will be required. We applied a system that integrated two state-ofthe-art systems through a process of re-scoring, to the NEWS 2010 Workshop shared task on transliteration generation. Our systems gave a strong performance on the shared task test set, and our experiments show the integrated system was able to outperform both of its component systems. In future work we would like to depart from the direct grapheme-to-grapheme approach taken here and address the problem of how best to represent the source and target sequences by either analyzing their symbols further, or agglomerating them. We would also like to investigate the use of co-segmentation schemes that do not rely on maximum likelihood training to overcome the issues inherent in this technique. Acknowledgements The results presented in this paper draw on the following data sets. For English-Japanese and Arabic-English, the reader is referred to the CJK website: http://www.cjk.org. For English-Hindi, English-Tamil, and English-Kannada, and English-Bangla the data sets originated from the work of Kumaran and Kellner, 2007. 51</abstract>
<title confidence="0.770466">References</title>
<author confidence="0.772373">Peter Brown</author>
<author confidence="0.772373">Stephen Della Pietra</author>
<author confidence="0.772373">Vincent Della</author>
<note confidence="0.873325636363636">Pietra, and Robert Mercer, 1991. The mathematics of statistical machine translation: parameter esti- 19(2), 263- 311. Sabine Deligne, and Fred6ric Bimbot, 1995. Language modeling by variable length sequences: theoretical formulation and evaluation of multi- Proc. IEEE Internat. Conf. on Acous- Speech and Signal Vol. 1, Detroit, MI, USA, pp. 169–172. Maximilian Bisani and Hermann Ney, 2008. Joint-</note>
<title confidence="0.689658">Sequence Models for Grapheme-to-Phoneme</title>
<note confidence="0.902147153846154">Conversion. Speech Communication, Volume 50, Issue 5, Pages 434-451. Thomas Deselaers, Sasa Hasan, Oliver Bender, and Hermann Ney, 2009. A Deep Learning Approach Machine Transliteration. Proceedings of the EACL 2009 Workshop on Statistical Machine Athens, Greece. Andrew Finch and Eiichiro Sumita, 2008. Phrasemachine transliteration. Proceedings of pages 13-18. Andrew Finch and Eiichiro Sumita, 2009. Transliteration by Bidirectional Statistical Machine Transof the 2009 Named Entities Shared Task on Singapore. Andrew Finch and Eiichiro Sumita, 2010. Exploiting Directional Asymmetry in Phrase-table Generation Statistical Machine Translation, Proceedof Tokyo, Japan. Kevin Knight and Jonathan Graehl, 1997. Machine of the Thirty-Fifth Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Compupp. 128-135, Somerset, New Jersey.</note>
<author confidence="0.6872045">Pro-</author>
<affiliation confidence="0.997718">ceedings of the Human Language Technology</affiliation>
<address confidence="0.9755805">2003 (HLT-NAACL Edmonton, Canada.</address>
<abstract confidence="0.91315875">Franz Josef Och, 2003. Minimum error rate training statistical machine translation, of A Kumaran and Tobias Kellner, 2007. A generic for machine transliteration, of 30th Haizhou Li, Min Zhang, Jian Su, 2004. A joint source model for machine transliteration, the 42nd</abstract>
<affiliation confidence="0.345513">Haizhou Li, A Kumaran, Min Zhang and Vladimir</affiliation>
<address confidence="0.918283">Pervouchine, 2010. Whitepaper of NEWS 2010</address>
<author confidence="0.731263">In</author>
<title confidence="0.874994">of ACL2010 Named Entities</title>
<author confidence="0.959415">Michael Paul</author>
<author confidence="0.959415">Eiichiro Sumita</author>
<author confidence="0.959415">Seiichi Yama-</author>
<title confidence="0.5580515">moto, 2006, Multiple Translation-Engine-based Hypotheses and Edit-Distance-based Rescoring for a Greedy Decoder for Statistical Machine and Media</title>
<abstract confidence="0.9306635">Vol. 1, No. 1, pp.446-460 . Taraka Rama and Karthik Gali, 2009. Modeling machine transliteration as a phrase based statistical translation problem, of the 2009 Named Entities Workshop: Shared Task on Singapore.</abstract>
<intro confidence="0.791117">52</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter Brown</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>Robert Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<marker>Brown, Pietra, Pietra, Mercer, 1991</marker>
<rawString>Peter Brown, Stephen Della Pietra, Vincent Della Pietra, and Robert Mercer, 1991. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2), 263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Deligne</author>
<author>Fred6ric Bimbot</author>
</authors>
<title>Language modeling by variable length sequences: theoretical formulation and evaluation of multigrams. In:</title>
<date>1995</date>
<booktitle>Proc. IEEE Internat. Conf. on Acoustics, Speech and Signal Processing,</booktitle>
<volume>1</volume>
<pages>169--172</pages>
<location>Detroit, MI, USA,</location>
<contexts>
<context position="1885" citStr="Deligne and Bimbot, 1995" startWordPosition="282" endWordPosition="285">stem was able to outperform both of its component systems substantially. 1 Introduction In statistical machine translation the re-scoring of hypotheses produced by a system with additional models that incorporate information not available to the original system has been shown to be an effective technique to improve system performance (Paul et al., 2006). Our approach uses a re-scoring technique to integrate the models of two transliteration systems that are each capable in their own right: a phrase-based statistical machine translation system (Koehn et al., 2003), and a joint multigram model (Deligne and Bimbot, 1995; Bisani and Ney, 2008). In this work we treat the process of transliteration as a process of direct transduction from sequences of tokens in the source language to sequences of tokens in the target language with Eiichiro Sumita NICT 3-5 Hikaridai Keihanna Science City 619-0289 JAPAN eiichiro.sumita@nict.go.jp no modeling of the phonetics of either source or target language (Knight and Graehl, 1997). Taking this approach allows for a very general transliteration system to be built that does not require any language specific knowledge to be incorporated into the system (for some language pairs </context>
<context position="3356" citStr="Deligne and Bimbot, 1995" startWordPosition="520" endWordPosition="524">ystems were selected because of their expected strong level of performance (SMT systems have been used successfully in the field, and joint multigram models have performed well both in grapheme to phoneme conversion and ArabicEnglish transliteration). Secondly, the joint multigram model relies on key features not present in the SMT system, that is the history of bilingual phrase pairs used to derive the target. For this reason we felt the systems would complement each other well. We now briefly describe the component systems. 2.1 Joint Multigram Model The joint multigram approach proposed by (Deligne and Bimbot, 1995) has arisen as an extension of the use of variable-length n-grams (multigrams) in language modeling. In a joint multigram, the units in the model consist of multiple input and output symbols. (Bisani and Ney, 2008) refined the approach and applied to it grapheme-to-phoneme conversion, where its performance was shown to be comparable to state-of-the-art systems. The approach was later applied to Arabic-English transliteration (Deselaers et al., 2009) again with promising results. Joint multigram models have the following characteristics: • The symbols in the source and target are co-segmented 4</context>
</contexts>
<marker>Deligne, Bimbot, 1995</marker>
<rawString>Sabine Deligne, and Fred6ric Bimbot, 1995. Language modeling by variable length sequences: theoretical formulation and evaluation of multigrams. In: Proc. IEEE Internat. Conf. on Acoustics, Speech and Signal Processing, Vol. 1, Detroit, MI, USA, pp. 169–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maximilian Bisani</author>
<author>Hermann Ney</author>
</authors>
<title>JointSequence Models for Grapheme-to-Phoneme Conversion.</title>
<date>2008</date>
<journal>Speech Communication,</journal>
<volume>50</volume>
<pages>434--451</pages>
<contexts>
<context position="1908" citStr="Bisani and Ney, 2008" startWordPosition="286" endWordPosition="289">m both of its component systems substantially. 1 Introduction In statistical machine translation the re-scoring of hypotheses produced by a system with additional models that incorporate information not available to the original system has been shown to be an effective technique to improve system performance (Paul et al., 2006). Our approach uses a re-scoring technique to integrate the models of two transliteration systems that are each capable in their own right: a phrase-based statistical machine translation system (Koehn et al., 2003), and a joint multigram model (Deligne and Bimbot, 1995; Bisani and Ney, 2008). In this work we treat the process of transliteration as a process of direct transduction from sequences of tokens in the source language to sequences of tokens in the target language with Eiichiro Sumita NICT 3-5 Hikaridai Keihanna Science City 619-0289 JAPAN eiichiro.sumita@nict.go.jp no modeling of the phonetics of either source or target language (Knight and Graehl, 1997). Taking this approach allows for a very general transliteration system to be built that does not require any language specific knowledge to be incorporated into the system (for some language pairs this may not be the bes</context>
<context position="3570" citStr="Bisani and Ney, 2008" startWordPosition="559" endWordPosition="562">d ArabicEnglish transliteration). Secondly, the joint multigram model relies on key features not present in the SMT system, that is the history of bilingual phrase pairs used to derive the target. For this reason we felt the systems would complement each other well. We now briefly describe the component systems. 2.1 Joint Multigram Model The joint multigram approach proposed by (Deligne and Bimbot, 1995) has arisen as an extension of the use of variable-length n-grams (multigrams) in language modeling. In a joint multigram, the units in the model consist of multiple input and output symbols. (Bisani and Ney, 2008) refined the approach and applied to it grapheme-to-phoneme conversion, where its performance was shown to be comparable to state-of-the-art systems. The approach was later applied to Arabic-English transliteration (Deselaers et al., 2009) again with promising results. Joint multigram models have the following characteristics: • The symbols in the source and target are co-segmented 48 Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 48–52, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics 0.9 0.7 F-Score 0.6 0.4 0.3 Joint Multigram Size Figure 1: T</context>
<context position="7047" citStr="Bisani and Ney, 2008" startWordPosition="1117" endWordPosition="1120">constrained decode in a monotone manner, and furthermore during training, the phrase extraction process was constrained such that only phrases with monotone order were extracted in order to minimize the effects of errors in the word alignment process. In a final step the scores from both systems were linearly interpolated to produce a single integrated hypothesis score. The hypotheses were then re-ranked according to this integrated score for the final submission. 3.2 Joint Multigram model For the joint multigram system we used the publicly available Sequitur G2P grapheme-tophoneme converter (Bisani and Ney, 2008). The system was used with its default settings, and pilot experiments were run on development data to determine appropriate settings for the maximum size of the multigrams. The results for the English-to-Japanese task are shown in Figure 1. As can be seen in the figure, the system rapidly improves to a near-optimal value with a maximum multigram size of 4. No improvement at all was observed for sizes over 7. We therefore chose a maximum multigram size of 8 for the experiments presented in this paper, and for the systems entered in the shared task. 3.3 Pre-processing In order to reduce data sp</context>
</contexts>
<marker>Bisani, Ney, 2008</marker>
<rawString>Maximilian Bisani and Hermann Ney, 2008. JointSequence Models for Grapheme-to-Phoneme Conversion. Speech Communication, Volume 50, Issue 5, Pages 434-451.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Deselaers</author>
<author>Sasa Hasan</author>
<author>Oliver Bender</author>
<author>Hermann Ney</author>
</authors>
<title>A Deep Learning Approach to Machine Transliteration.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL 2009 Workshop on Statistical Machine Translation (WMT09),</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="3809" citStr="Deselaers et al., 2009" startWordPosition="591" endWordPosition="595">d complement each other well. We now briefly describe the component systems. 2.1 Joint Multigram Model The joint multigram approach proposed by (Deligne and Bimbot, 1995) has arisen as an extension of the use of variable-length n-grams (multigrams) in language modeling. In a joint multigram, the units in the model consist of multiple input and output symbols. (Bisani and Ney, 2008) refined the approach and applied to it grapheme-to-phoneme conversion, where its performance was shown to be comparable to state-of-the-art systems. The approach was later applied to Arabic-English transliteration (Deselaers et al., 2009) again with promising results. Joint multigram models have the following characteristics: • The symbols in the source and target are co-segmented 48 Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 48–52, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics 0.9 0.7 F-Score 0.6 0.4 0.3 Joint Multigram Size Figure 1: The effect on F-score by tuning with respect to joint multigram size - Maximum likelihood training using an EM algorithm (Deligne and Bimbot, 1995) • The probability of sequences of joint multigrams is modeled using an n-gram model In these</context>
</contexts>
<marker>Deselaers, Hasan, Bender, Ney, 2009</marker>
<rawString>Thomas Deselaers, Sasa Hasan, Oliver Bender, and Hermann Ney, 2009. A Deep Learning Approach to Machine Transliteration. In Proceedings of the EACL 2009 Workshop on Statistical Machine Translation (WMT09), Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Finch</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Phrasebased machine transliteration.</title>
<date>2008</date>
<booktitle>In Proceedings of WTCAST&apos;08,</booktitle>
<pages>13--18</pages>
<contexts>
<context position="4832" citStr="Finch and Sumita, 2008" startWordPosition="758" endWordPosition="761">ect to joint multigram size - Maximum likelihood training using an EM algorithm (Deligne and Bimbot, 1995) • The probability of sequences of joint multigrams is modeled using an n-gram model In these respects the model can be viewed as a close relative of the joint source channel model proposed by (Li et al., 2004) for transliteration. 2.2 Phrase-based SMT It is possible to view the process of transliteration as a process of translation at the character level, without re-ordering. From this perspective it is possible to directly employ a phrase-based SMT system in the task of transliteration (Finch and Sumita, 2008; Rama and Gali, 2009). A phrase-based SMT system has the following characteristics: • The symbols in the source and target are aligned one to many in both directions. Joint sequences of source and target symbols are heuristically extracted given these alignments • Transliteration is performed using a loglinear model with weights tuned on development data • The models include: a translation model (with 5 sub-models), and a target language model The bilingual phrase-pairs are analogous to the joint multigrams, however the translation model of the SMT system doesn’t use the context of previously</context>
</contexts>
<marker>Finch, Sumita, 2008</marker>
<rawString>Andrew Finch and Eiichiro Sumita, 2008. Phrasebased machine transliteration. In Proceedings of WTCAST&apos;08, pages 13-18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Finch</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Transliteration by Bidirectional Statistical Machine Translation,</title>
<date>2009</date>
<booktitle>Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration,</booktitle>
<contexts>
<context position="11472" citStr="Finch and Sumita, 2009" startWordPosition="1874" endWordPosition="1877">hai ➝ English 0.397 0.873 0.525 0.397 English ➝ Hindi 0.445 0.884 0.574 0.445 English ➝ Tamil 0.390 0.887 0.522 0.390 English ➝ Kannada 0.371 0.871 0.506 0.371 English ➝ Japanese 0.378 0.783 0.510 0.378 Arabic ➝ English 0.403 0.891 0.512 0.327 English ➝ Bangla 0.412 0.883 0.550 0.412 Table 1: The results of our system in the official evaluation on the test data on all performance metrics. machine translation systems and is a function of the geometric mean of n-gram precision. The use of BLEU score as a proxy has been shown to be a reasonable strategy for the metrics used in these experiments (Finch and Sumita, 2009). Nonetheless, it is reasonable to assume that one would be able to improve the performance in a particular evaluation metric by doing minimum error rate training specifically for that metric. The interpolation weight was tuned by a grid search to find the value that gave the maximal fscore (according to the official f-score evaluation metric for the shared task) on the development data, the process for English-Japanese is shown in Figure 3. 4 Results The results of our experiments are shown in Table 1. These results are the official shared task evaluation results on the test data, and the sco</context>
</contexts>
<marker>Finch, Sumita, 2009</marker>
<rawString>Andrew Finch and Eiichiro Sumita, 2009. Transliteration by Bidirectional Statistical Machine Translation, Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Finch</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Exploiting Directional Asymmetry in Phrase-table Generation for Statistical Machine Translation,</title>
<date>2010</date>
<booktitle>In Proceedings of NLP2010,</booktitle>
<location>Tokyo, Japan.</location>
<contexts>
<context position="6343" citStr="Finch and Sumita, 2010" startWordPosition="1004" endWordPosition="1007">y available MOSES decoder (Koehn et al., 2003). Our decoder was modified to be able to decode source sequences with reference to a target sequence; the decoding process being forced to generate the target. The decoder was also configured to combine scores of multiple derivations yielding the same target sequence. In this way the models in the decoder were used to derive scores used to re-score the n-best (we used n=20 for our experiments) hypotheses generated by the joint multigram model. The phraseextraction process was symmetrized with respect to token order using the technique proposed in (Finch and Sumita, 2010). In order to adapt the SMT system to the task of transliteration, the decoder was constrained decode in a monotone manner, and furthermore during training, the phrase extraction process was constrained such that only phrases with monotone order were extracted in order to minimize the effects of errors in the word alignment process. In a final step the scores from both systems were linearly interpolated to produce a single integrated hypothesis score. The hypotheses were then re-ranked according to this integrated score for the final submission. 3.2 Joint Multigram model For the joint multigra</context>
</contexts>
<marker>Finch, Sumita, 2010</marker>
<rawString>Andrew Finch and Eiichiro Sumita, 2010. Exploiting Directional Asymmetry in Phrase-table Generation for Statistical Machine Translation, In Proceedings of NLP2010, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<date>1997</date>
<booktitle>Machine Transliteration. Proceedings of the Thirty-Fifth Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>128--135</pages>
<location>Somerset, New Jersey.</location>
<contexts>
<context position="2287" citStr="Knight and Graehl, 1997" startWordPosition="345" endWordPosition="348">ntegrate the models of two transliteration systems that are each capable in their own right: a phrase-based statistical machine translation system (Koehn et al., 2003), and a joint multigram model (Deligne and Bimbot, 1995; Bisani and Ney, 2008). In this work we treat the process of transliteration as a process of direct transduction from sequences of tokens in the source language to sequences of tokens in the target language with Eiichiro Sumita NICT 3-5 Hikaridai Keihanna Science City 619-0289 JAPAN eiichiro.sumita@nict.go.jp no modeling of the phonetics of either source or target language (Knight and Graehl, 1997). Taking this approach allows for a very general transliteration system to be built that does not require any language specific knowledge to be incorporated into the system (for some language pairs this may not be the best strategy since linguistic information can be used to overcome issues of data sparseness on smaller datasets). 2 Component Systems For this shared task we chose to combine two systems through a process of re-scoring. The systems were selected because of their expected strong level of performance (SMT systems have been used successfully in the field, and joint multigram models</context>
</contexts>
<marker>Knight, Graehl, 1997</marker>
<rawString>Kevin Knight and Jonathan Graehl, 1997. Machine Transliteration. Proceedings of the Thirty-Fifth Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics, pp. 128-135, Somerset, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1830" citStr="Koehn et al., 2003" startWordPosition="273" endWordPosition="276">r experiments on development data, the combined system was able to outperform both of its component systems substantially. 1 Introduction In statistical machine translation the re-scoring of hypotheses produced by a system with additional models that incorporate information not available to the original system has been shown to be an effective technique to improve system performance (Paul et al., 2006). Our approach uses a re-scoring technique to integrate the models of two transliteration systems that are each capable in their own right: a phrase-based statistical machine translation system (Koehn et al., 2003), and a joint multigram model (Deligne and Bimbot, 1995; Bisani and Ney, 2008). In this work we treat the process of transliteration as a process of direct transduction from sequences of tokens in the source language to sequences of tokens in the target language with Eiichiro Sumita NICT 3-5 Hikaridai Keihanna Science City 619-0289 JAPAN eiichiro.sumita@nict.go.jp no modeling of the phonetics of either source or target language (Knight and Graehl, 1997). Taking this approach allows for a very general transliteration system to be built that does not require any language specific knowledge to be</context>
<context position="5766" citStr="Koehn et al., 2003" startWordPosition="905" endWordPosition="908">del with weights tuned on development data • The models include: a translation model (with 5 sub-models), and a target language model The bilingual phrase-pairs are analogous to the joint multigrams, however the translation model of the SMT system doesn’t use the context of previously translated phrase-pairs, instead relying on a target language model. 3 Experimental Conditions 3.1 SMT Decoder In our experiments we used an in-house phrasebased statistical machine translation decoder called CleopATRa. This decoder operates on exactly the same principles as the publicly available MOSES decoder (Koehn et al., 2003). Our decoder was modified to be able to decode source sequences with reference to a target sequence; the decoding process being forced to generate the target. The decoder was also configured to combine scores of multiple derivations yielding the same target sequence. In this way the models in the decoder were used to derive scores used to re-score the n-best (we used n=20 for our experiments) hypotheses generated by the joint multigram model. The phraseextraction process was symmetrized with respect to token order using the technique proposed in (Finch and Sumita, 2010). In order to adapt the</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu, 2003. Statistical Phrase-Based Translation. In Proceedings of the Human Language Technology Conference 2003 (HLT-NAACL 2003), Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation,</title>
<date>2003</date>
<booktitle>Proceedings of the ACL.</booktitle>
<contexts>
<context position="10584" citStr="Och, 2003" startWordPosition="1721" endWordPosition="1722">mall that the inclusion of the development data into the training set would yield a reasonable boost in performance by increasing the coverage of the systems. All tunable parameters were tuned on development data using systems built using only the training data. Under the assumption that these parameters would perform well in the systems trained on the combined development/training corpora, these tuned parameters were transferred directly to the systems trained on all available data. 3.6 Parameter Tuning The SMT systems were tuned using the minimum error rate training procedure introduced in (Och, 2003). For convenience, we used BLEU as a proxy for the various metrics used in the shared task evaluation. The BLEU score is commonly used to evaluate the performance of 50 Language Pair Mean MRR MAPref Accuracy in F-score top-1 English ➝ Thai 0.412 0.883 0.550 0.412 Thai ➝ English 0.397 0.873 0.525 0.397 English ➝ Hindi 0.445 0.884 0.574 0.445 English ➝ Tamil 0.390 0.887 0.522 0.390 English ➝ Kannada 0.371 0.871 0.506 0.371 English ➝ Japanese 0.378 0.783 0.510 0.378 Arabic ➝ English 0.403 0.891 0.512 0.327 English ➝ Bangla 0.412 0.883 0.550 0.412 Table 1: The results of our system in the official</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och, 2003. Minimum error rate training for statistical machine translation, Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kumaran</author>
<author>Tobias Kellner</author>
</authors>
<title>A generic framework for machine transliteration,</title>
<date>2007</date>
<booktitle>Proc. of the 30th SIGIR.</booktitle>
<marker>Kumaran, Kellner, 2007</marker>
<rawString>A Kumaran and Tobias Kellner, 2007. A generic framework for machine transliteration, Proc. of the 30th SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>Min Zhang</author>
<author>Jian Su</author>
</authors>
<title>A joint source channel model for machine transliteration,</title>
<date>2004</date>
<booktitle>Proc. of the 42nd ACL.</booktitle>
<contexts>
<context position="4526" citStr="Li et al., 2004" startWordPosition="709" endWordPosition="712">ls in the source and target are co-segmented 48 Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 48–52, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics 0.9 0.7 F-Score 0.6 0.4 0.3 Joint Multigram Size Figure 1: The effect on F-score by tuning with respect to joint multigram size - Maximum likelihood training using an EM algorithm (Deligne and Bimbot, 1995) • The probability of sequences of joint multigrams is modeled using an n-gram model In these respects the model can be viewed as a close relative of the joint source channel model proposed by (Li et al., 2004) for transliteration. 2.2 Phrase-based SMT It is possible to view the process of transliteration as a process of translation at the character level, without re-ordering. From this perspective it is possible to directly employ a phrase-based SMT system in the task of transliteration (Finch and Sumita, 2008; Rama and Gali, 2009). A phrase-based SMT system has the following characteristics: • The symbols in the source and target are aligned one to many in both directions. Joint sequences of source and target symbols are heuristically extracted given these alignments • Transliteration is performed</context>
</contexts>
<marker>Li, Zhang, Su, 2004</marker>
<rawString>Haizhou Li, Min Zhang, Jian Su, 2004. A joint source channel model for machine transliteration, Proc. of the 42nd ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>A Kumaran</author>
<author>Min Zhang</author>
<author>Vladimir Pervouchine</author>
</authors>
<date>2010</date>
<booktitle>Whitepaper of NEWS 2010 Shared Task on Transliteration Generation. In Proc. of ACL2010 Named Entities Workshop.</booktitle>
<contexts>
<context position="12202" citStr="Li et al., 2010" startWordPosition="2002" endWordPosition="2005">tion metric by doing minimum error rate training specifically for that metric. The interpolation weight was tuned by a grid search to find the value that gave the maximal fscore (according to the official f-score evaluation metric for the shared task) on the development data, the process for English-Japanese is shown in Figure 3. 4 Results The results of our experiments are shown in Table 1. These results are the official shared task evaluation results on the test data, and the scores for all of the evaluation metrics are shown in the table. The reader is referred to the workshop white paper (Li et al., 2010) for details of the evaluation metrics. The system achieved a high level of performance on most of the language pairs. Comparing the individual systems to each other, and to the integrated system, the joint multigram system outperformed the phrasebased SMT system. In experiments run on the English-to-Japanese katakana task, the joint multigram system in isolation achieved an Fscore of 0.837 on development data, whereas the SMT system in isolation achieved an F-score of 0.824. When integrated the models of the systems complemented each other well, and on the same English-Japanese task the integ</context>
</contexts>
<marker>Li, Kumaran, Zhang, Pervouchine, 2010</marker>
<rawString>Haizhou Li, A Kumaran, Min Zhang and Vladimir Pervouchine, 2010. Whitepaper of NEWS 2010 Shared Task on Transliteration Generation. In Proc. of ACL2010 Named Entities Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
</authors>
<title>Eiichiro Sumita and Seiichi Yamamoto,</title>
<date>2006</date>
<journal>Information and Media Technologies,</journal>
<volume>1</volume>
<pages>446--460</pages>
<marker>Paul, 2006</marker>
<rawString>Michael Paul, Eiichiro Sumita and Seiichi Yamamoto, 2006, Multiple Translation-Engine-based Hypotheses and Edit-Distance-based Rescoring for a Greedy Decoder for Statistical Machine Translation, Information and Media Technologies, Vol. 1, No. 1, pp.446-460 .</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taraka Rama</author>
<author>Karthik Gali</author>
</authors>
<title>Modeling machine transliteration as a phrase based statistical machine translation problem,</title>
<date>2009</date>
<booktitle>Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration,</booktitle>
<contexts>
<context position="4854" citStr="Rama and Gali, 2009" startWordPosition="762" endWordPosition="765">ize - Maximum likelihood training using an EM algorithm (Deligne and Bimbot, 1995) • The probability of sequences of joint multigrams is modeled using an n-gram model In these respects the model can be viewed as a close relative of the joint source channel model proposed by (Li et al., 2004) for transliteration. 2.2 Phrase-based SMT It is possible to view the process of transliteration as a process of translation at the character level, without re-ordering. From this perspective it is possible to directly employ a phrase-based SMT system in the task of transliteration (Finch and Sumita, 2008; Rama and Gali, 2009). A phrase-based SMT system has the following characteristics: • The symbols in the source and target are aligned one to many in both directions. Joint sequences of source and target symbols are heuristically extracted given these alignments • Transliteration is performed using a loglinear model with weights tuned on development data • The models include: a translation model (with 5 sub-models), and a target language model The bilingual phrase-pairs are analogous to the joint multigrams, however the translation model of the SMT system doesn’t use the context of previously translated phrase-pai</context>
</contexts>
<marker>Rama, Gali, 2009</marker>
<rawString>Taraka Rama and Karthik Gali, 2009. Modeling machine transliteration as a phrase based statistical machine translation problem, Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration, Singapore.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>