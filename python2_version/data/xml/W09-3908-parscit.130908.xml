<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002228">
<title confidence="0.997239">
Participant Subjectivity and Involvement as a Basis for Discourse
Segmentation
</title>
<author confidence="0.996495">
John Niekrasz and Johanna Moore
</author>
<affiliation confidence="0.998616666666667">
Human Communication Research Centre
School of Informatics
University of Edinburgh
</affiliation>
<email confidence="0.997556">
{jniekras,jmoore}@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993884" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999572333333333">
We propose a framework for analyzing
episodic conversational activities in terms
of expressed relationships between the
participants and utterance content. We
test the hypothesis that linguistic features
which express such properties, e.g. tense,
aspect, and person deixis, are a useful ba-
sis for automatic intentional discourse seg-
mentation. We present a novel algorithm
and test our hypothesis on a set of inten-
tionally segmented conversational mono-
logues. Our algorithm performs better
than a simple baseline and as well as or
better than well-known lexical-semantic
segmentation methods.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996934375">
This paper concerns the analysis of conversations
in terms of communicative activities. Examples of
the kinds of activities we are interested in include
relating a personal experience, making a group de-
cision, committing to future action, and giving in-
structions. The reason we are interested in these
kinds of events is that they are part of partici-
pants’ common-sense notion of the goals and ac-
complishments of a dialogue. They are part of par-
ticipants’ subjective experience of what happened
and show up in summaries of conversations such
as meeting minutes. We therefore consider them
an ideal target for the practical, common-sense de-
scription of conversations.
Activities like these commonly occur as cohe-
sive episodes of multiple turns within a conver-
sation (Korolija, 1998). They represent an inter-
mediate level of dialogue structure – greater than
a single speech act but still small enough to have
a potentially well-defined singular purpose. They
have a temporal granularity of anywhere from a
few seconds to several minutes.
Ultimately, it would be useful to use descrip-
tions of such activities in automatic summariza-
tion technologies for conversational genres. This
would provide an activity-oriented summary de-
scribing what ’happened’ that would complement
one based on information content or what the con-
versation was ’about’. Part of our research goal is
thus to identify a set of discourse features for seg-
menting, classifying, and describing conversations
in this way.
</bodyText>
<subsectionHeader confidence="0.999606">
1.1 Participant subjectivity and involvement
</subsectionHeader>
<bodyText confidence="0.99992652173913">
The approach we take to this problem is founded
upon two basic ideas. The first is that the activities
we are interested in represent a coarse level of the
intentional structure of dialogue (Grosz and Sid-
ner, 1986). In other words, each activity is unified
by a common purpose that is shared between the
participants. This suggests there may be linguis-
tic properties which are shared amongst the utter-
ances of a given activity episode.
The second idea concerns the properties which
distinguish different activity types. We propose
that activity types may be usefully distinguished
according to two complex properties of utterances,
both of which concern relationships between the
participants and the utterance: participant sub-
jectivity and participant involvement. Participant
subjectivity concerns attitudinal and perspectival
relationships toward the dialogue content. This
includes properties such as whether the utterance
expresses the private mental state of the speaker,
or the participants’ temporal relationship to a de-
scribed event. Participant involvement concerns
the roles participants play within the dialogue con-
</bodyText>
<note confidence="0.709953">
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 54–61,
</note>
<affiliation confidence="0.662708">
Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics
</affiliation>
<page confidence="0.998919">
54
</page>
<bodyText confidence="0.930976">
tent, e.g., as the agent of a described event.
</bodyText>
<subsectionHeader confidence="0.764766">
1.2 Intentional segmentation
</subsectionHeader>
<bodyText confidence="0.999917555555556">
The hypothesis we test in this paper is that the
linguistic phenomena which express participant-
relational properties may be used as an effective
means of intentional discourse segmentation. This
is based on the idea that if adjacent discourse seg-
ments have different activity types, then they are
distinguishable by participant-relational features.
If we can reliably extract such features, then this
would allow segmentation of the dialogue accord-
ingly.
We test our hypothesis by constructing an algo-
rithm and examining its performance on an exist-
ing set of intentionally segmented conversational
monologues (i.e., one person speaks while another
listens) (Passonneau and Litman, 1997, henceforth
P&amp;L). While our long term goal is to apply our
techniques to multi-party conversations (and to
a somewhat coarser-grained analysis), using this
dataset is a stepping-stone toward that end which
allows us to compare our results with existing in-
tentional segmentation algorithms.
An example dialogue extract from the dataset
is shown in Dialogue 1. Two horizontal lines in-
dicate a segment boundary which was identified
by at least 3 of 7 annotators. A single horizon-
tal line indicates a segment boundary which was
identified by 2 or fewer annotators. In the exam-
</bodyText>
<footnote confidence="0.917744333333334">
PearStories-09 (Chafe, 1980)
21.2 okay.
22.1 Meanwhile,
22.2 there are three little boys,
22.3 up on the road a little bit,
22.4 and they see this little accident.
23.1 And u-h they come over,
23.2 and they help him,
23.3 and you know,
23.4 help him pick up the pears and everything.
24.1 A-nd the one thing that struck me about the- three
little boys that were there,
24.2 is that one had ay uh I don’t know what you call
them,
24.3 but it’s a paddle,
24.4 and a ball-,
24.5 is attached to the paddle,
24.6 and you know you bounce it?
25.1 And that sound was really prominent.
26.1 Well anyway,
26.2 so- u-m tsk all the pears are picked up,
26.3 and he’s on his way again,
Dialogue 1: An example dialogue extract showing
intentional segment boundaries.
</footnote>
<bodyText confidence="0.999949272727273">
ple, there are three basic types of discourse activity
distinguishable according to the properties of par-
ticipant subjectivity and participant involvement.
The segments beginning at 22.1 and 26.2 share the
use of the historical present tense – a type of par-
ticipant subjectivity – in a narrative activity type.
Utterances 24.1 and 25.1, on the other hand, are
about the prior perceptions of the speaker, a type
of participant involvement in a past event. The
segment beginning at 24.2 is a type of generic de-
scription activity, exhibiting its own distinct con-
figuration of participant relational features, such
as the generic you and present tense.
We structure the rest of the paper as follows.
First, we begin by describing related and support-
ing theoretical work. This is followed by a test of
our main hypothesis. We then follow this with a
similar experiment which contextualizes our work
both theoretically and in practical terms with re-
spect to the most commonly studied segmentation
task: topic segmentation. We finish with a general
discussion of the implications of our experiments.
</bodyText>
<sectionHeader confidence="0.811404" genericHeader="introduction">
2 Background and Related Work
</sectionHeader>
<bodyText confidence="0.99985137037037">
The influential work of Grosz and Sidner (1986)
provides a helpful starting point for understand-
ing our approach. Their theory suggests that in-
tentions (which equate to the goals and purposes
of a dialogue) are a foundation for the structure of
discourse. The individual discourse purposes that
emerge in a dialogue relate directly to the natural
aggregation of utterances into discourse segments.
The attentional state of the dialogue, which con-
tains salient objects and relations and allows for
the efficient generation and interpretation of utter-
ances, is then dependent upon this interrelated in-
tentional and linguistic structure in the emerging
dialogue.
Grosz and Sidner’s theory suggests that atten-
tional state is parasitic upon the underlying inten-
tional structure. This implication has informed
many approaches which relate referring expres-
sions (an attentional phenomenon) to discourse
structure. One example is Centering theory (Grosz
et al., 1995), which concerns the relationship of
referring expressions to discourse coherence. An-
other is P&amp;L, who demonstrated that co-reference
and inferred relations between noun phrases are
a useful basis for automatic intentional segmen-
tation.
Our approach expands on this by highlighting
</bodyText>
<page confidence="0.997493">
55
</page>
<bodyText confidence="0.999989048387097">
the fact that objects that are in focus within the
attentional state have an important quality which
may be exploited: they are focused upon by the
participants from particular points of view. In ad-
dition, the objects may in fact be the participants
themselves. We would expect the linguistic fea-
tures which express such relationships (e.g., as-
pect, subjectivity, modality, and person deixis) to
therefore correlate with intentional structure, and
to do so in a way which is important to partici-
pants’ subjective experience of the dialogue.
This approach is supported by a theory put forth
by Chafe (1994), who describes how speakers can
express ideas from alternative perspectives. For
example, a subject who is recounting the events of
a movie of a man picking pears might say “the man
was picking pears”, “the man picks some pears”,
or “you see a man picking pears.” Each variant is
an expression of the same idea but reflects a dif-
ferent perspective toward, or manner of participa-
tion in, the described event. The linguistic vari-
ation one sees in this example is in the proper-
ties of tense and aspect in the main clause (and in
the last variant, a perspectival superordinate clause
which uses the generic you). We have observed
that discourse coheres in these perspectival terms,
with shifts of perspective usually occurring at in-
tentional boundaries.
Wiebe (1994; 1995) has investigated a phe-
nomenon closely related to this: point-of-view
and subjectivity in fictional narrative. She notes
that paragraph-level blocks of text often share a
common objective or subjective context. That
is, sentences may or may not be conveyed from
the point-of-view of individuals, e.g., the author
or the characters within the narrative. Sentences
continue, resume, or initiate such contexts, and
she develops automatic methods for determining
when the contexts shift and whose point-of-view
is being taken. Her algorithm provides a de-
tailed method for analyzing written fiction, but
has not been developed for conversational or non-
narrative genres.
Smith’s (2003) analysis of texts, however,
draws a more general set of connections between
the content of sentences and types of discourse
segments. She does this by analyzing texts at
the level of short passages and determines a non-
exhaustive list of five basic “discourse modes” oc-
curring at that level: narrative, description, report,
information, and argument. The mode of a pas-
sage is determined by the type of situations de-
scribed in the text (e.g., event, state, general sta-
tive, etc.) and the temporal progression of the sit-
uations in the discourse. Situation types are in
turn organized according to the perspectival prop-
erties of aspect and temporal location. A narrative
passage, for example, relates principally specific
events and states, with dynamic temporal advance-
ment of narrative time between sentences. On the
other hand, an information passage relates primar-
ily general statives with atemporal progression.
</bodyText>
<sectionHeader confidence="0.996288" genericHeader="method">
3 Automatic Segmentation Experiment
</sectionHeader>
<bodyText confidence="0.999951875">
The analysis described in the previous sections
suggests that participant-relational features corre-
late with the intentional structure of discourse. In
this section we describe an experiment which tests
the hypothesis that a small set of such features, i.e.,
tense, aspect, and first- and second-person pro-
nouns, are a useful basis for intentional segmen-
tation.
</bodyText>
<subsectionHeader confidence="0.997442">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999969227272727">
Our experiment uses the same dataset as P&amp;L, a
corpus of 20 spoken narrative monologues known
as the Pear Stories (Chafe, 1980). Chafe asked
subjects to view a silent movie and then sum-
marize it for a second person. Their speech
was then manually transcribed and segmented into
prosodic phrases. This resulted in a mean 100
phrases per narrative and a mean 6.7 words per
phrase. P&amp;L later had each narrative segmented
by seven annotators according to an informal defi-
nition of communicative intention. Each prosodic
phrase boundary was a possible discourse segment
boundary. Using Cochran’s Q test, they concluded
that an appropriate gold standard could be pro-
duced by using the set of boundaries assigned by
at least three of the seven annotators. This is the
gold standard we use in this paper. It assigns a
boundary at a mean 16.9% (Q = 4.5%) of the pos-
sible boundary sites in each narrative. The result is
a mean discourse segment length of 5.9 prosodic
phrases, (Q = 1.4 across the means of each narra-
tive).
</bodyText>
<subsectionHeader confidence="0.99541">
3.2 Algorithm
</subsectionHeader>
<bodyText confidence="0.99989875">
The basic idea behind our algorithm is to distin-
guish utterances according to the type of activ-
ity in which they occur. To do this, we iden-
tify a set of utterance properties relating to par-
</bodyText>
<page confidence="0.984967">
56
</page>
<bodyText confidence="0.999786777777778">
ticipant subjectivity and participant involvement,
according to which activity types may be distin-
guished. We then develop a routine for automati-
cally extracting the linguistic features which indi-
cate such properties. Finally, the dialogue is seg-
mented at locations of high discontinuity in that
feature space. The algorithm works in four phases:
pre-processing, feature extraction, similarity mea-
surement, and boundary assignment.
</bodyText>
<subsectionHeader confidence="0.984436">
3.2.1 Pre-processing
</subsectionHeader>
<bodyText confidence="0.99991475">
For pre-processing, disfluencies are removed by
deleting repeated strings of words and incomplete
words. The transcript is then parsed (Klein and
Manning, 2002), and a collection of typed gram-
matical dependencies are generated (de Marneffe
et al., 2006). The TTT2 chunker (Grover and To-
bin, 2006) is then used to perform tense and aspect
tagging.
</bodyText>
<subsectionHeader confidence="0.499052">
3.2.2 Feature extraction
</subsectionHeader>
<bodyText confidence="0.999774346938776">
Feature extraction is the most important and
novel part of our algorithm. Each prosodic phrase
(the corpus uses prosodic phrases as sentence-like
units, see Data section) is assigned values for five
binary features. The extracted features correspond
to a set of utterance properties which were iden-
tified manually through corpus analysis. The first
four relate directly to individual activity types and
are therefore mutually exclusive properties.
first-person participation [1P] – helps to distin-
guish meta-discussion between the speaker
and hearer (e.g., “Did I tell you that?”)
generic second-person [2P-GEN] – helps to dis-
tinguish narration told from the perspective
of a generic participant (e.g., “You see a man
picking pears”)
third-person stative/progressive [3P-STAT]
– helps to distinguish narrative activities
related to “setting the scene” (e.g., “[There is
a man  |a man is] picking pears”)
third-person event [3P-EVENT] – helps to dis-
tinguish event-driven third-person narrative
activities (e.g. “The man drops the pears”)
past/non-past [PAST] – helps to distinguish nar-
rative activities by temporal orientation (e.g.
“The man drops the pears” vs. “The man
dropped the pears”)
Feature extraction works by identifying the lin-
guistic elements that indicate each utterance prop-
erty. First, prosodic phrases containing a first- or
second-person pronoun in grammatical subject or
object relation to any clause are identified (com-
mon fillers like you know, I think, and I don’t know
are ignored). Of the identified phrases, those with
first-person pronouns are marked for 1P, while the
others are marked for 2P-GEN. For the remain-
ing prosodic phrases, those with a matrix clause
are identified. Of those identified, if either its
head verb is be or have, it is tagged by TTT2 as
having progressive aspect, or the prosodic phrase
contains an existential there, then it is marked for
3P-STAT. The others are marked for 3P-EVENT.
Finally, if the matrix clause was tagged as past
tense, the phrase is marked for PAST. In cases
where no participant-relational features are iden-
tified (e.g., no matrix clause, no pronouns), the
prosodic phrase is assigned the same features as
the preceding one, effectively marking a continua-
tion of the current activity type.
</bodyText>
<subsectionHeader confidence="0.813322">
3.2.3 Similarity measurement
</subsectionHeader>
<bodyText confidence="0.999937083333334">
Similarity measurement is calculated according
to the cosine similarity cos(vi, ci) between the fea-
ture vector vi of each prosodic phrase i and a
weighted sum ci of the feature vectors in the pre-
ceding context. The algorithm requires a parame-
ter l to be set for the desired mean segment length.
This determines the window w = floor(l/2) of
preceding utterances to be used. The weighted
sum representing the preceding context is com-
puted as ci = Ew j�1((1 + w − j)/w)vi−j, which
gives increasingly greater weight to more recent
phrases.
</bodyText>
<subsectionHeader confidence="0.862507">
3.2.4 Boundary assignment
</subsectionHeader>
<bodyText confidence="0.999984">
In the final step, the algorithm assigns bound-
aries where the similarity score is lowest, namely
prior to prosodic phrases where cos is less than the
first 1/l quantile for that discourse.
</bodyText>
<subsectionHeader confidence="0.999219">
3.3 Experimental Method and Evaluation
</subsectionHeader>
<bodyText confidence="0.9999385">
Our experiment compares the performance of
our novel algorithm (which we call NM09) with
a naive baseline and a well-known alternative
method – P&amp;L’s co-reference based NP algorithm.
To our knowledge, P&amp;L is the only existing publi-
cation describing algorithms designed specifically
for intentional segmentation of dialogue. Their
NP algorithm exploits annotations of direct and
</bodyText>
<page confidence="0.996666">
57
</page>
<bodyText confidence="0.999985941176471">
inferred relations between noun phrases in adja-
cent units. Inspired by Centering theory (Grosz
et al., 1995), these annotations are used in a com-
putational account of discourse focus to measure
coherence. Although adding pause-based features
improved results slightly, the NP method was the
clear winner amongst those using a single feature
type and produced very good results.
The NP algorithm requires co-reference anno-
tations as input, so to create a fully-automatic
version (NP-AUTO) we have employed a state-of-
the-art co-reference resolution system (Poesio and
Kabadjov, 2004) to generate the required input.
We also include results based on P&amp;L’s original
human co-reference annotations (NP-HUMAN).
For reference, we include a baseline that ran-
domly assigns boundaries at the same mean fre-
quency as the gold-standard annotations, i.e., a se-
quence drawn from the Bernoulli distribution with
success probability p = 0.169 (this probability de-
termines the value of the target segment length pa-
rameter l in our own algorithm). As a top-line ref-
erence, we calculate the mean of the seven anno-
tators’ scores with respect to the three-annotator
gold standard.
For evaluation we employ two types of mea-
sure. On one hand, we use P(k) (Beeferman et al.,
1999) as an error measure designed to accommo-
date near-miss boundary assignments. It is useful
because it estimates the probability that two ran-
domly drawn points will be assigned incorrectly
to either the same or different segments. On the
other hand, we use Cohen’s Kappa (K) to evalu-
ate the precise placement of boundaries such that
each potential boundary site is considered a binary
classification. While K is typically used to evalu-
ate inter-annotator agreement, it is a useful mea-
sure of classification accuracy in our experiment
for two reasons. First, it accounts for the strong
class bias in our data. Second, it allows a direct
and intuitive comparison with our inter-annotator
top-line reference. We also provide results for the
commonly-used IR measures F1, recall, and pre-
cision. These are useful for comparing with pre-
vious results in the literature and provide a more
widely-understood measure of the accuracy of the
results. Precision and recall are also helpful in re-
vealing the effects of any classification bias the al-
gorithms may have.
The results are calculated for 18 of the 20 narra-
tives, as manual feature development involved the
</bodyText>
<tableCaption confidence="0.998349">
Table 1: Mean results for the 18 test narratives.
</tableCaption>
<table confidence="0.9982525">
P(k) K F1 Rec. Prec.
Human .21 .58 .65 .64 .69
NP-HUMAN .35 .38 .40 .52 .46
NM09 .44 .11 .24 .23 .28
NP-AUTO .52 .03 .27 .71 .17
Random .50 .00 .15 .14 .17
</table>
<bodyText confidence="0.5865715">
use of two randomly selected narratives as devel-
opment data. The one exception is NP-HUMAN,
which is evaluated on the 10 narratives for which
there are manual co-reference annotations.
</bodyText>
<sectionHeader confidence="0.843318" genericHeader="method">
3.4 Results
</sectionHeader>
<bodyText confidence="0.999836">
The mean results for the 18 narratives, calculated
in comparison to the three-annotator gold stan-
dard, are shown in Table 1. NP-HUMAN and NM09
are both superior to the random baseline for all
measures (p&lt;0.05). NP-AUTO, however, is only
superior in terms of recall and F1 (p&lt;0.05).
</bodyText>
<subsectionHeader confidence="0.584723">
3.5 Discussion
</subsectionHeader>
<bodyText confidence="0.999981296296296">
The results indicate that the simple set of features
we have chosen can be used for intentional seg-
mentation. While the results are not near human
performance, it is encouraging that such a simple
set of easily extractable features achieves results
that are 19% (K), 24% (P(k)), and 18% (F1) of
human performance, relative to the random base-
line.
The other notable result is the very high recall
score of NP-AUTO, which helps to produce a re-
spectable F1 score. However, a low K reveals that
when accounting for class bias, this system is ac-
tually not far from the performance of a high recall
random classifier.
Error analysis showed that the reason for the
problems with NP-AUTO was the lack of reference
chains produced by the automatic co-reference
system. While the system seems to have per-
formed well for direct co-reference, it did not do
well with bridging reference. Inferred relations
were an important part of the reference chains pro-
duced by P&amp;L, and it is now clear that these play
a significant role in the performance of the NP al-
gorithm. Our algorithm is not dependent on this
difficult processing problem, which typically re-
quires world knowledge in the form of training on
large datasets or the use of large lexical resources.
</bodyText>
<page confidence="0.998256">
58
</page>
<sectionHeader confidence="0.946451" genericHeader="method">
4 Topic vs. Intentional Segmentation
</sectionHeader>
<bodyText confidence="0.999987909090909">
It is important to place our experiment on inten-
tional segmentation in context with the most com-
monly studied automatic segmentation task: topic-
based segmentation. While the two tasks are dis-
tinct, the literature has drawn connections between
them which can at times be confusing. In this sec-
tion, we attempt to clarify those connections by
pointing out some of their differences and similar-
ities. We also conduct an experiment comparing
our algorithm to well-known topic-segmentation
algorithms and discuss the results.
</bodyText>
<subsectionHeader confidence="0.999813">
4.1 Automatic segmentation in the literature
</subsectionHeader>
<bodyText confidence="0.999991815384615">
One of the most widely-cited discourse segmen-
tation algorithms is TextTiling (Hearst, 1997).
Designed to segment texts into multi-paragraph
subtopics, it works by operationalizing the notion
of lexical cohesion (Halliday and Hasan, 1976).
TextTiling and related algorithms exploit the col-
location of semantically related lexemes to mea-
sure coherence. Recent improvements to this
method include the use of alternative lexical sim-
ilarity metrics like LSA (Choi et al., 2001) and
alternative segmentation methods like the mini-
mum cut model (Malioutov and Barzilay, 2006)
and ranking and clustering (Choi, 2000). Re-
cently, Bayesian approaches which model top-
ics as a lexical generative process have been em-
ployed (Purver et al., 2006; Eisenstein and Barzi-
lay, 2008). What these algorithms all share is a
focus on the semantic content of the discourse.
Passonneau and Litman (1997) is another of the
most widely-cited articles on discourse segmenta-
tion. Their overall approach combines an investi-
gation of prosodic features, cue words, and entity
reference. As described above, their approach to
using entity reference is motivated by Centering
theory (Grosz et al., 1995) and the hypothesis that
intentional structure is exhibited in the attentional
relationships between discourse referents.
Hearst and P&amp;L try to achieve different goals,
but their tasks are nonetheless related. One might
reasonably hypothesize, for example, that either
lexical similarity or co-reference could be use-
ful to either type of segmentation on the grounds
that the two phenomena are clearly related. How-
ever, there are also clear differences of intent be-
tween the two studies. While there is an ob-
vious difference in the dataset (written exposi-
tory text vs. spoken narrative monologue), the an-
notation instructions reflect the difference most
clearly. Hearst instructed naive annotators to mark
paragraph boundaries “where the topics seem to
change,” whereas P&amp;L asked naive annotators to
mark prosodic phrases where the speaker had be-
gun a new communicative task.
The results indicate that there is a difference
in granularity between the two tasks, with inten-
tional segmentation relating to finer-grained struc-
ture. Hearst’s segments have a mean of about 200
words to P&amp;L’s 40. Also, two hierarchical topic
segmentations of meetings (Hsueh, 2008; Gruen-
stein et al., 2008) have averages above 400 words
for the smallest level of segment.
To our knowledge, P&amp;L is the only existing
study of automatic intention-based segmentation.
However, their work has been frequently cited as a
study of topic-oriented segmentation, e.g., (Galley
et al., 2003; Eisenstein and Barzilay, 2008). Also,
recent research in conversational genres (Galley et
al., 2003; Hsueh and Moore, 2007) analyze events
like discussing an agenda or giving a presentation,
which resemble more intentional categories. Inter-
estingly, these algorithms demonstrate the bene-
fit of including non-lexical, non-semantic features.
The results imply that further analysis is needed to
understand the links between different types of co-
herence and different types of segmentation.
</bodyText>
<subsectionHeader confidence="0.950202">
4.2 Experiment 2
</subsectionHeader>
<bodyText confidence="0.99995780952381">
We have extended the above experiment to com-
pare the results of our novel algorithm with ex-
isting topic segmentation methods. We employ
Choi’s implementations of C99 (Choi, 2000) and
TEXTTILING (Hearst, 1997) as examples of well-
known topic-oriented methods. While we ac-
knowledge that there are newer algorithms which
improve upon this work, these were selected for
being well studied and easy to apply out-of-the-
box. Our method and evaluation is the same as in
the previous experiment.
The mean results for the 18 narratives are shown
in Table 2, with the human and baseline score re-
produced from the previous table. All three auto-
matic algorithms are superior to the random base-
line in terms of P(k), n, and F1 (p&lt;0.05). The
only statistically significant difference (p&lt;0.05)
between the three automatic methods is between
NM09 and TEXTTILING in terms of F1. The ob-
served difference between NM09 and TEXTTIL-
ING in terms of n is only moderately significant
</bodyText>
<page confidence="0.999406">
59
</page>
<tableCaption confidence="0.981088">
Table 2: Results comparing our method to topic-
oriented segmentation methods.
</tableCaption>
<table confidence="0.998756333333333">
NP-auto P(k) r. F1 Rec. Prec.
Human .21 .58 .65 .64 .69
NM09 .44 .11 .24 .24 .28
C99 .44 .08 .22 .20 .24
TEXTTILING .41 .05 .18 .16 .21
Random .50 .00 .15 .14 .17
</table>
<bodyText confidence="0.979519333333333">
(p&lt;0.08). The observed differences between be-
tween NM09 and C99 are minimally significant
(p&lt;0.24) .
</bodyText>
<subsectionHeader confidence="0.940928">
4.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999996470588235">
The comparable performance achieved by our
simple perspective-based approach in comparison
to lexical-semantic approaches suggests two main
points. First, it validates our novel approach in
practical applied terms. It shows that perspective-
oriented features, being simple to extract and ap-
plicable to a variety of genres, are potentially very
useful for automatic discourse segmentation sys-
tems.
Second, the results show that the teasing apart
of topic-oriented and intentional structure may be
quite difficult. Studies of coherence at the level of
short passages or episodes (Korolija, 1998) sug-
gest that coherence is established through a com-
plex interaction of topical, intentional, and other
contextual factors. In this experiment, the major
portion of the dialogues are oriented toward the
basic narrative activity which is the premise of the
Pear Stories dataset. This means that there are
many times when the activity type does not change
at intentional boundaries. At other times, the ac-
tivity type changes but neither the topic nor the set
of referents is significantly changed. The differ-
ent types of algorithms we have tried (i.e., topical,
referential, and perspectival) seem to be operating
on somewhat orthogonal bases, though it is dif-
ficult to say quantitatively how this relates to the
types of “communicative task” transitions occur-
ring at the boundaries. In a sense, we have pro-
posed an algorithm for performing “activity type
cohesion” which mimics the methods of lexical
cohesion but is based upon a different dimension
of the discourse. The results indicate that these are
both related to intentional structure.
</bodyText>
<sectionHeader confidence="0.965798" genericHeader="conclusions">
5 General Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999989769230769">
Future work in intentional segmentation is needed.
Our ultimate goal is to extend this work to more
conversational domains (e.g., multi-party planning
meetings) and to define the richer set of perspec-
tives and related deictic features that would be
needed for them. For example, we hypothesize
that the different uses of second-person pronouns
in conversations (Gupta et al., 2007) are likely to
reflect alternative activity types. Our feature set
and extraction methods will therefore need to be
further developed to capture this complexity.
The other question we would like to address is
the relationship between various types of coher-
ence (e.g., topical, referential, perspectival, etc.)
and different types (and levels) of discourse struc-
ture. Our current approach uses a feature space
that is orthogonal to most existing segmentation
methods. This has allowed us to gain a deeper
understanding of the relationship between certain
linguistic features and the underlying intentional
structure, but more work is needed.
In terms of practical motivations, we also plan
to address the open question of how to effectively
combine our feature set with other feature sets
which have also been demonstrated to contribute
to discourse structuring and segmentation.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999894454545455">
Doug Beeferman, Adam Berger, and John D. Lafferty.
1999. Statistical models for text segmentation. Ma-
chine Learning, 34(1-3):177–210.
Wallace L. Chafe, editor. 1980. The Pear Stories:
Cognitive, Cultural, and Linguistic Aspects of Nar-
rative Production, volume 3 of Advances in Dis-
course Processes. Ablex, Norwood, NJ.
Wallace L. Chafe. 1994. Discourse, Consciousness,
and Time: The Flow and Displacement of Conscious
Experience in Speaking and Writing. University of
Chicago Press, Chicago.
Freddy Y. Y. Choi, Peter Wiemer-Hastings, and Jo-
hanna Moore. 2001. Latent semantic analysis for
text segmentation. In Proc. EMNLP, pages 109–
117.
Freddy Y. Y. Choi. 2000. Advances in domain inde-
pendent linear text segmentation. In Proc. NAACL,
pages 26–33.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proc. LREC, pages 562–569.
</reference>
<page confidence="0.999041">
60
</page>
<bodyText confidence="0.965735">
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proc. EMNLP,
pages 334–343.
Massimo Poesio and Mijail A. Kabadjov. 2004. A
general-purpose, off-the-shelf anaphora resolution
module: Implementation and preliminary evalua-
tion. In Proc. LREC.
</bodyText>
<reference confidence="0.999793169491526">
Michel Galley, Kathleen McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. In Proc.
ACL, pages 562–569.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational Linguistics, 12(3):175–204.
Barbara J. Grosz, Aravind Joshi, and Scott Weinstein.
1995. Centering: A framework for modelling the
local coherence of discourse. Computational Lin-
guistics, 21(2):203–225.
Claire Grover and Richard Tobin. 2006. Rule-based
chunking and reusability. In Proc. LREC.
Alexander Gruenstein, John Niekrasz, and Matthew
Purver. 2008. Meeting structure annotation: Anno-
tations collected with a general purpose toolkit. In
L. Dybkjaer and W. Minker, editors, Recent Trends
in Discourse and Dialogue, pages 247–274.
Surabhi Gupta, John Niekrasz, Matthew Purver, and
Daniel Jurafsky. 2007. Resolving “you” in multi-
party dialog. In Proc. SIGdial, pages 227–230.
M. A. K. Halliday and Ruqayia Hasan. 1976. Cohe-
sion in English. Longman, New York.
Marti Hearst. 1997. TextTiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33–64.
Pei-Yun Hsueh and Johanna D. Moore. 2007. Com-
bining multiple knowledge sources for dialogue seg-
mentation in multimedia archives. In Proc. ACL,
pages 1016–1023.
Pei-Yun Hsueh. 2008. Meeting Decision Detection:
Multimodal Information Fusion for Multi-Party Di-
alogue Understanding. Ph.D. thesis, School of In-
formatics, University of Edinburgh.
Dan Klein and Christopher D. Manning. 2002. Fast
exact inference with a factored model for natural
language parsing. In NIPS 15.
Natascha Korolija. 1998. Episodes in talk: Construct-
ing coherence in multiparty conversation. Ph.D. the-
sis, Linköping University, The Tema Institute, De-
partment of Communications Studies.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Proc.
COLING-ACL, pages 25–32.
Rebecca J. Passonneau and Diane J. Litman. 1997.
Discourse segmentation by human and automated
means. Computational Linguistics, 23(1):103–139.
Matthew Purver, Konrad Körding, Thomas Griffiths,
and Joshua Tenenbaum. 2006. Unsupervised topic
modelling for multi-party spoken discourse. In
Proc. COLING-ACL, pages 17–24.
Carlota S. Smith. 2003. Modes of Discourse. Camb-
drige University Press, Cambridge.
Janyce M. Wiebe. 1994. Tracking point of view in nar-
rative. Computational Linguistics, 20(2):233–287.
Janyce M. Wiebe. 1995. References in narrative text.
In Judy Duchan, Gail Bruder, and Lynne Hewitt, ed-
itors, Deixis in Narrative: A Cognitive Science Per-
spective, pages 263–286.
</reference>
<page confidence="0.999272">
61
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.892388">
<title confidence="0.999017">Participant Subjectivity and Involvement as a Basis for Segmentation</title>
<author confidence="0.97266">John Niekrasz</author>
<author confidence="0.97266">Johanna</author>
<affiliation confidence="0.981555333333333">Human Communication Research School of University of</affiliation>
<email confidence="0.989456">jniekras@inf.ed.ac.uk</email>
<email confidence="0.989456">jmoore@inf.ed.ac.uk</email>
<abstract confidence="0.9982798125">We propose a framework for analyzing episodic conversational activities in terms of expressed relationships between the participants and utterance content. We test the hypothesis that linguistic features which express such properties, e.g. tense, aspect, and person deixis, are a useful basis for automatic intentional discourse segmentation. We present a novel algorithm and test our hypothesis on a set of intentionally segmented conversational monologues. Our algorithm performs better than a simple baseline and as well as or better than well-known lexical-semantic segmentation methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Doug Beeferman</author>
<author>Adam Berger</author>
<author>John D Lafferty</author>
</authors>
<title>Statistical models for text segmentation.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="18313" citStr="Beeferman et al., 1999" startWordPosition="2876" endWordPosition="2879">sults based on P&amp;L’s original human co-reference annotations (NP-HUMAN). For reference, we include a baseline that randomly assigns boundaries at the same mean frequency as the gold-standard annotations, i.e., a sequence drawn from the Bernoulli distribution with success probability p = 0.169 (this probability determines the value of the target segment length parameter l in our own algorithm). As a top-line reference, we calculate the mean of the seven annotators’ scores with respect to the three-annotator gold standard. For evaluation we employ two types of measure. On one hand, we use P(k) (Beeferman et al., 1999) as an error measure designed to accommodate near-miss boundary assignments. It is useful because it estimates the probability that two randomly drawn points will be assigned incorrectly to either the same or different segments. On the other hand, we use Cohen’s Kappa (K) to evaluate the precise placement of boundaries such that each potential boundary site is considered a binary classification. While K is typically used to evaluate inter-annotator agreement, it is a useful measure of classification accuracy in our experiment for two reasons. First, it accounts for the strong class bias in our</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1999</marker>
<rawString>Doug Beeferman, Adam Berger, and John D. Lafferty. 1999. Statistical models for text segmentation. Machine Learning, 34(1-3):177–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wallace L Chafe</author>
<author>editor</author>
</authors>
<date>1980</date>
<booktitle>The Pear Stories: Cognitive, Cultural, and Linguistic Aspects of Narrative Production,</booktitle>
<volume>3</volume>
<location>Norwood, NJ.</location>
<marker>Chafe, editor, 1980</marker>
<rawString>Wallace L. Chafe, editor. 1980. The Pear Stories: Cognitive, Cultural, and Linguistic Aspects of Narrative Production, volume 3 of Advances in Discourse Processes. Ablex, Norwood, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wallace L Chafe</author>
</authors>
<title>Discourse, Consciousness, and Time: The Flow and Displacement of Conscious Experience in Speaking and Writing.</title>
<date>1994</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="8783" citStr="Chafe (1994)" startWordPosition="1365" endWordPosition="1366">ing 55 the fact that objects that are in focus within the attentional state have an important quality which may be exploited: they are focused upon by the participants from particular points of view. In addition, the objects may in fact be the participants themselves. We would expect the linguistic features which express such relationships (e.g., aspect, subjectivity, modality, and person deixis) to therefore correlate with intentional structure, and to do so in a way which is important to participants’ subjective experience of the dialogue. This approach is supported by a theory put forth by Chafe (1994), who describes how speakers can express ideas from alternative perspectives. For example, a subject who is recounting the events of a movie of a man picking pears might say “the man was picking pears”, “the man picks some pears”, or “you see a man picking pears.” Each variant is an expression of the same idea but reflects a different perspective toward, or manner of participation in, the described event. The linguistic variation one sees in this example is in the properties of tense and aspect in the main clause (and in the last variant, a perspectival superordinate clause which uses the gene</context>
</contexts>
<marker>Chafe, 1994</marker>
<rawString>Wallace L. Chafe. 1994. Discourse, Consciousness, and Time: The Flow and Displacement of Conscious Experience in Speaking and Writing. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Freddy Y Y Choi</author>
<author>Peter Wiemer-Hastings</author>
<author>Johanna Moore</author>
</authors>
<title>Latent semantic analysis for text segmentation.</title>
<date>2001</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>109--117</pages>
<contexts>
<context position="22486" citStr="Choi et al., 2001" startWordPosition="3564" endWordPosition="3567">eriment comparing our algorithm to well-known topic-segmentation algorithms and discuss the results. 4.1 Automatic segmentation in the literature One of the most widely-cited discourse segmentation algorithms is TextTiling (Hearst, 1997). Designed to segment texts into multi-paragraph subtopics, it works by operationalizing the notion of lexical cohesion (Halliday and Hasan, 1976). TextTiling and related algorithms exploit the collocation of semantically related lexemes to measure coherence. Recent improvements to this method include the use of alternative lexical similarity metrics like LSA (Choi et al., 2001) and alternative segmentation methods like the minimum cut model (Malioutov and Barzilay, 2006) and ranking and clustering (Choi, 2000). Recently, Bayesian approaches which model topics as a lexical generative process have been employed (Purver et al., 2006; Eisenstein and Barzilay, 2008). What these algorithms all share is a focus on the semantic content of the discourse. Passonneau and Litman (1997) is another of the most widely-cited articles on discourse segmentation. Their overall approach combines an investigation of prosodic features, cue words, and entity reference. As described above,</context>
</contexts>
<marker>Choi, Wiemer-Hastings, Moore, 2001</marker>
<rawString>Freddy Y. Y. Choi, Peter Wiemer-Hastings, and Johanna Moore. 2001. Latent semantic analysis for text segmentation. In Proc. EMNLP, pages 109– 117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Freddy Y Y Choi</author>
</authors>
<title>Advances in domain independent linear text segmentation.</title>
<date>2000</date>
<booktitle>In Proc. NAACL,</booktitle>
<pages>26--33</pages>
<contexts>
<context position="22621" citStr="Choi, 2000" startWordPosition="3586" endWordPosition="3587">ure One of the most widely-cited discourse segmentation algorithms is TextTiling (Hearst, 1997). Designed to segment texts into multi-paragraph subtopics, it works by operationalizing the notion of lexical cohesion (Halliday and Hasan, 1976). TextTiling and related algorithms exploit the collocation of semantically related lexemes to measure coherence. Recent improvements to this method include the use of alternative lexical similarity metrics like LSA (Choi et al., 2001) and alternative segmentation methods like the minimum cut model (Malioutov and Barzilay, 2006) and ranking and clustering (Choi, 2000). Recently, Bayesian approaches which model topics as a lexical generative process have been employed (Purver et al., 2006; Eisenstein and Barzilay, 2008). What these algorithms all share is a focus on the semantic content of the discourse. Passonneau and Litman (1997) is another of the most widely-cited articles on discourse segmentation. Their overall approach combines an investigation of prosodic features, cue words, and entity reference. As described above, their approach to using entity reference is motivated by Centering theory (Grosz et al., 1995) and the hypothesis that intentional str</context>
<context position="25322" citStr="Choi, 2000" startWordPosition="4001" endWordPosition="4002">genres (Galley et al., 2003; Hsueh and Moore, 2007) analyze events like discussing an agenda or giving a presentation, which resemble more intentional categories. Interestingly, these algorithms demonstrate the benefit of including non-lexical, non-semantic features. The results imply that further analysis is needed to understand the links between different types of coherence and different types of segmentation. 4.2 Experiment 2 We have extended the above experiment to compare the results of our novel algorithm with existing topic segmentation methods. We employ Choi’s implementations of C99 (Choi, 2000) and TEXTTILING (Hearst, 1997) as examples of wellknown topic-oriented methods. While we acknowledge that there are newer algorithms which improve upon this work, these were selected for being well studied and easy to apply out-of-thebox. Our method and evaluation is the same as in the previous experiment. The mean results for the 18 narratives are shown in Table 2, with the human and baseline score reproduced from the previous table. All three automatic algorithms are superior to the random baseline in terms of P(k), n, and F1 (p&lt;0.05). The only statistically significant difference (p&lt;0.05) b</context>
</contexts>
<marker>Choi, 2000</marker>
<rawString>Freddy Y. Y. Choi. 2000. Advances in domain independent linear text segmentation. In Proc. NAACL, pages 26–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proc. LREC,</booktitle>
<pages>562--569</pages>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proc. LREC, pages 562–569.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
</authors>
<title>Eric FoslerLussier, and Hongyan Jing.</title>
<date>2003</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>562--569</pages>
<marker>Galley, McKeown, 2003</marker>
<rawString>Michel Galley, Kathleen McKeown, Eric FoslerLussier, and Hongyan Jing. 2003. Discourse segmentation of multi-party conversation. In Proc. ACL, pages 562–569.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="2586" citStr="Grosz and Sidner, 1986" startWordPosition="387" endWordPosition="391">marization technologies for conversational genres. This would provide an activity-oriented summary describing what ’happened’ that would complement one based on information content or what the conversation was ’about’. Part of our research goal is thus to identify a set of discourse features for segmenting, classifying, and describing conversations in this way. 1.1 Participant subjectivity and involvement The approach we take to this problem is founded upon two basic ideas. The first is that the activities we are interested in represent a coarse level of the intentional structure of dialogue (Grosz and Sidner, 1986). In other words, each activity is unified by a common purpose that is shared between the participants. This suggests there may be linguistic properties which are shared amongst the utterances of a given activity episode. The second idea concerns the properties which distinguish different activity types. We propose that activity types may be usefully distinguished according to two complex properties of utterances, both of which concern relationships between the participants and the utterance: participant subjectivity and participant involvement. Participant subjectivity concerns attitudinal an</context>
<context position="6981" citStr="Grosz and Sidner (1986)" startWordPosition="1087" endWordPosition="1090">distinct configuration of participant relational features, such as the generic you and present tense. We structure the rest of the paper as follows. First, we begin by describing related and supporting theoretical work. This is followed by a test of our main hypothesis. We then follow this with a similar experiment which contextualizes our work both theoretically and in practical terms with respect to the most commonly studied segmentation task: topic segmentation. We finish with a general discussion of the implications of our experiments. 2 Background and Related Work The influential work of Grosz and Sidner (1986) provides a helpful starting point for understanding our approach. Their theory suggests that intentions (which equate to the goals and purposes of a dialogue) are a foundation for the structure of discourse. The individual discourse purposes that emerge in a dialogue relate directly to the natural aggregation of utterances into discourse segments. The attentional state of the dialogue, which contains salient objects and relations and allows for the efficient generation and interpretation of utterances, is then dependent upon this interrelated intentional and linguistic structure in the emergi</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara J. Grosz and Candace L. Sidner. 1986. Attention, intentions, and the structure of discourse. Computational Linguistics, 12(3):175–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Aravind Joshi</author>
<author>Scott Weinstein</author>
</authors>
<title>Centering: A framework for modelling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="7893" citStr="Grosz et al., 1995" startWordPosition="1223" endWordPosition="1226">atural aggregation of utterances into discourse segments. The attentional state of the dialogue, which contains salient objects and relations and allows for the efficient generation and interpretation of utterances, is then dependent upon this interrelated intentional and linguistic structure in the emerging dialogue. Grosz and Sidner’s theory suggests that attentional state is parasitic upon the underlying intentional structure. This implication has informed many approaches which relate referring expressions (an attentional phenomenon) to discourse structure. One example is Centering theory (Grosz et al., 1995), which concerns the relationship of referring expressions to discourse coherence. Another is P&amp;L, who demonstrated that co-reference and inferred relations between noun phrases are a useful basis for automatic intentional segmentation. Our approach expands on this by highlighting 55 the fact that objects that are in focus within the attentional state have an important quality which may be exploited: they are focused upon by the participants from particular points of view. In addition, the objects may in fact be the participants themselves. We would expect the linguistic features which express</context>
<context position="17166" citStr="Grosz et al., 1995" startWordPosition="2694" endWordPosition="2697">rior to prosodic phrases where cos is less than the first 1/l quantile for that discourse. 3.3 Experimental Method and Evaluation Our experiment compares the performance of our novel algorithm (which we call NM09) with a naive baseline and a well-known alternative method – P&amp;L’s co-reference based NP algorithm. To our knowledge, P&amp;L is the only existing publication describing algorithms designed specifically for intentional segmentation of dialogue. Their NP algorithm exploits annotations of direct and 57 inferred relations between noun phrases in adjacent units. Inspired by Centering theory (Grosz et al., 1995), these annotations are used in a computational account of discourse focus to measure coherence. Although adding pause-based features improved results slightly, the NP method was the clear winner amongst those using a single feature type and produced very good results. The NP algorithm requires co-reference annotations as input, so to create a fully-automatic version (NP-AUTO) we have employed a state-ofthe-art co-reference resolution system (Poesio and Kabadjov, 2004) to generate the required input. We also include results based on P&amp;L’s original human co-reference annotations (NP-HUMAN). For</context>
<context position="23181" citStr="Grosz et al., 1995" startWordPosition="3673" endWordPosition="3676">and Barzilay, 2006) and ranking and clustering (Choi, 2000). Recently, Bayesian approaches which model topics as a lexical generative process have been employed (Purver et al., 2006; Eisenstein and Barzilay, 2008). What these algorithms all share is a focus on the semantic content of the discourse. Passonneau and Litman (1997) is another of the most widely-cited articles on discourse segmentation. Their overall approach combines an investigation of prosodic features, cue words, and entity reference. As described above, their approach to using entity reference is motivated by Centering theory (Grosz et al., 1995) and the hypothesis that intentional structure is exhibited in the attentional relationships between discourse referents. Hearst and P&amp;L try to achieve different goals, but their tasks are nonetheless related. One might reasonably hypothesize, for example, that either lexical similarity or co-reference could be useful to either type of segmentation on the grounds that the two phenomena are clearly related. However, there are also clear differences of intent between the two studies. While there is an obvious difference in the dataset (written expository text vs. spoken narrative monologue), the</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>Barbara J. Grosz, Aravind Joshi, and Scott Weinstein. 1995. Centering: A framework for modelling the local coherence of discourse. Computational Linguistics, 21(2):203–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Grover</author>
<author>Richard Tobin</author>
</authors>
<title>Rule-based chunking and reusability.</title>
<date>2006</date>
<booktitle>In Proc. LREC.</booktitle>
<contexts>
<context position="13520" citStr="Grover and Tobin, 2006" startWordPosition="2121" endWordPosition="2125">outine for automatically extracting the linguistic features which indicate such properties. Finally, the dialogue is segmented at locations of high discontinuity in that feature space. The algorithm works in four phases: pre-processing, feature extraction, similarity measurement, and boundary assignment. 3.2.1 Pre-processing For pre-processing, disfluencies are removed by deleting repeated strings of words and incomplete words. The transcript is then parsed (Klein and Manning, 2002), and a collection of typed grammatical dependencies are generated (de Marneffe et al., 2006). The TTT2 chunker (Grover and Tobin, 2006) is then used to perform tense and aspect tagging. 3.2.2 Feature extraction Feature extraction is the most important and novel part of our algorithm. Each prosodic phrase (the corpus uses prosodic phrases as sentence-like units, see Data section) is assigned values for five binary features. The extracted features correspond to a set of utterance properties which were identified manually through corpus analysis. The first four relate directly to individual activity types and are therefore mutually exclusive properties. first-person participation [1P] – helps to distinguish meta-discussion betwe</context>
</contexts>
<marker>Grover, Tobin, 2006</marker>
<rawString>Claire Grover and Richard Tobin. 2006. Rule-based chunking and reusability. In Proc. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Gruenstein</author>
<author>John Niekrasz</author>
<author>Matthew Purver</author>
</authors>
<title>Meeting structure annotation: Annotations collected with a general purpose toolkit.</title>
<date>2008</date>
<booktitle>Recent Trends in Discourse and Dialogue,</booktitle>
<pages>247--274</pages>
<editor>In L. Dybkjaer and W. Minker, editors,</editor>
<contexts>
<context position="24364" citStr="Gruenstein et al., 2008" startWordPosition="3855" endWordPosition="3859">text vs. spoken narrative monologue), the annotation instructions reflect the difference most clearly. Hearst instructed naive annotators to mark paragraph boundaries “where the topics seem to change,” whereas P&amp;L asked naive annotators to mark prosodic phrases where the speaker had begun a new communicative task. The results indicate that there is a difference in granularity between the two tasks, with intentional segmentation relating to finer-grained structure. Hearst’s segments have a mean of about 200 words to P&amp;L’s 40. Also, two hierarchical topic segmentations of meetings (Hsueh, 2008; Gruenstein et al., 2008) have averages above 400 words for the smallest level of segment. To our knowledge, P&amp;L is the only existing study of automatic intention-based segmentation. However, their work has been frequently cited as a study of topic-oriented segmentation, e.g., (Galley et al., 2003; Eisenstein and Barzilay, 2008). Also, recent research in conversational genres (Galley et al., 2003; Hsueh and Moore, 2007) analyze events like discussing an agenda or giving a presentation, which resemble more intentional categories. Interestingly, these algorithms demonstrate the benefit of including non-lexical, non-sema</context>
</contexts>
<marker>Gruenstein, Niekrasz, Purver, 2008</marker>
<rawString>Alexander Gruenstein, John Niekrasz, and Matthew Purver. 2008. Meeting structure annotation: Annotations collected with a general purpose toolkit. In L. Dybkjaer and W. Minker, editors, Recent Trends in Discourse and Dialogue, pages 247–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Surabhi Gupta</author>
<author>John Niekrasz</author>
<author>Matthew Purver</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Resolving “you” in multiparty dialog.</title>
<date>2007</date>
<booktitle>In Proc. SIGdial,</booktitle>
<pages>227--230</pages>
<contexts>
<context position="28512" citStr="Gupta et al., 2007" startWordPosition="4513" endWordPosition="4516">ctivity type cohesion” which mimics the methods of lexical cohesion but is based upon a different dimension of the discourse. The results indicate that these are both related to intentional structure. 5 General Discussion and Future Work Future work in intentional segmentation is needed. Our ultimate goal is to extend this work to more conversational domains (e.g., multi-party planning meetings) and to define the richer set of perspectives and related deictic features that would be needed for them. For example, we hypothesize that the different uses of second-person pronouns in conversations (Gupta et al., 2007) are likely to reflect alternative activity types. Our feature set and extraction methods will therefore need to be further developed to capture this complexity. The other question we would like to address is the relationship between various types of coherence (e.g., topical, referential, perspectival, etc.) and different types (and levels) of discourse structure. Our current approach uses a feature space that is orthogonal to most existing segmentation methods. This has allowed us to gain a deeper understanding of the relationship between certain linguistic features and the underlying intenti</context>
</contexts>
<marker>Gupta, Niekrasz, Purver, Jurafsky, 2007</marker>
<rawString>Surabhi Gupta, John Niekrasz, Matthew Purver, and Daniel Jurafsky. 2007. Resolving “you” in multiparty dialog. In Proc. SIGdial, pages 227–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
<author>Ruqayia Hasan</author>
</authors>
<date>1976</date>
<booktitle>Cohesion in English.</booktitle>
<publisher>Longman,</publisher>
<location>New York.</location>
<contexts>
<context position="22251" citStr="Halliday and Hasan, 1976" startWordPosition="3528" endWordPosition="3531">tasks are distinct, the literature has drawn connections between them which can at times be confusing. In this section, we attempt to clarify those connections by pointing out some of their differences and similarities. We also conduct an experiment comparing our algorithm to well-known topic-segmentation algorithms and discuss the results. 4.1 Automatic segmentation in the literature One of the most widely-cited discourse segmentation algorithms is TextTiling (Hearst, 1997). Designed to segment texts into multi-paragraph subtopics, it works by operationalizing the notion of lexical cohesion (Halliday and Hasan, 1976). TextTiling and related algorithms exploit the collocation of semantically related lexemes to measure coherence. Recent improvements to this method include the use of alternative lexical similarity metrics like LSA (Choi et al., 2001) and alternative segmentation methods like the minimum cut model (Malioutov and Barzilay, 2006) and ranking and clustering (Choi, 2000). Recently, Bayesian approaches which model topics as a lexical generative process have been employed (Purver et al., 2006; Eisenstein and Barzilay, 2008). What these algorithms all share is a focus on the semantic content of the </context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>M. A. K. Halliday and Ruqayia Hasan. 1976. Cohesion in English. Longman, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>TextTiling: Segmenting text into multi-paragraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="22105" citStr="Hearst, 1997" startWordPosition="3510" endWordPosition="3511">ntentional segmentation in context with the most commonly studied automatic segmentation task: topicbased segmentation. While the two tasks are distinct, the literature has drawn connections between them which can at times be confusing. In this section, we attempt to clarify those connections by pointing out some of their differences and similarities. We also conduct an experiment comparing our algorithm to well-known topic-segmentation algorithms and discuss the results. 4.1 Automatic segmentation in the literature One of the most widely-cited discourse segmentation algorithms is TextTiling (Hearst, 1997). Designed to segment texts into multi-paragraph subtopics, it works by operationalizing the notion of lexical cohesion (Halliday and Hasan, 1976). TextTiling and related algorithms exploit the collocation of semantically related lexemes to measure coherence. Recent improvements to this method include the use of alternative lexical similarity metrics like LSA (Choi et al., 2001) and alternative segmentation methods like the minimum cut model (Malioutov and Barzilay, 2006) and ranking and clustering (Choi, 2000). Recently, Bayesian approaches which model topics as a lexical generative process h</context>
<context position="25352" citStr="Hearst, 1997" startWordPosition="4005" endWordPosition="4006"> Hsueh and Moore, 2007) analyze events like discussing an agenda or giving a presentation, which resemble more intentional categories. Interestingly, these algorithms demonstrate the benefit of including non-lexical, non-semantic features. The results imply that further analysis is needed to understand the links between different types of coherence and different types of segmentation. 4.2 Experiment 2 We have extended the above experiment to compare the results of our novel algorithm with existing topic segmentation methods. We employ Choi’s implementations of C99 (Choi, 2000) and TEXTTILING (Hearst, 1997) as examples of wellknown topic-oriented methods. While we acknowledge that there are newer algorithms which improve upon this work, these were selected for being well studied and easy to apply out-of-thebox. Our method and evaluation is the same as in the previous experiment. The mean results for the 18 narratives are shown in Table 2, with the human and baseline score reproduced from the previous table. All three automatic algorithms are superior to the random baseline in terms of P(k), n, and F1 (p&lt;0.05). The only statistically significant difference (p&lt;0.05) between the three automatic met</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Marti Hearst. 1997. TextTiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics, 23(1):33–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pei-Yun Hsueh</author>
<author>Johanna D Moore</author>
</authors>
<title>Combining multiple knowledge sources for dialogue segmentation in multimedia archives.</title>
<date>2007</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>1016--1023</pages>
<contexts>
<context position="24762" citStr="Hsueh and Moore, 2007" startWordPosition="3916" endWordPosition="3919">, with intentional segmentation relating to finer-grained structure. Hearst’s segments have a mean of about 200 words to P&amp;L’s 40. Also, two hierarchical topic segmentations of meetings (Hsueh, 2008; Gruenstein et al., 2008) have averages above 400 words for the smallest level of segment. To our knowledge, P&amp;L is the only existing study of automatic intention-based segmentation. However, their work has been frequently cited as a study of topic-oriented segmentation, e.g., (Galley et al., 2003; Eisenstein and Barzilay, 2008). Also, recent research in conversational genres (Galley et al., 2003; Hsueh and Moore, 2007) analyze events like discussing an agenda or giving a presentation, which resemble more intentional categories. Interestingly, these algorithms demonstrate the benefit of including non-lexical, non-semantic features. The results imply that further analysis is needed to understand the links between different types of coherence and different types of segmentation. 4.2 Experiment 2 We have extended the above experiment to compare the results of our novel algorithm with existing topic segmentation methods. We employ Choi’s implementations of C99 (Choi, 2000) and TEXTTILING (Hearst, 1997) as exampl</context>
</contexts>
<marker>Hsueh, Moore, 2007</marker>
<rawString>Pei-Yun Hsueh and Johanna D. Moore. 2007. Combining multiple knowledge sources for dialogue segmentation in multimedia archives. In Proc. ACL, pages 1016–1023.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pei-Yun Hsueh</author>
</authors>
<title>Meeting Decision Detection: Multimodal Information Fusion for Multi-Party Dialogue Understanding.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>School of Informatics, University of Edinburgh.</institution>
<contexts>
<context position="24338" citStr="Hsueh, 2008" startWordPosition="3853" endWordPosition="3854">n expository text vs. spoken narrative monologue), the annotation instructions reflect the difference most clearly. Hearst instructed naive annotators to mark paragraph boundaries “where the topics seem to change,” whereas P&amp;L asked naive annotators to mark prosodic phrases where the speaker had begun a new communicative task. The results indicate that there is a difference in granularity between the two tasks, with intentional segmentation relating to finer-grained structure. Hearst’s segments have a mean of about 200 words to P&amp;L’s 40. Also, two hierarchical topic segmentations of meetings (Hsueh, 2008; Gruenstein et al., 2008) have averages above 400 words for the smallest level of segment. To our knowledge, P&amp;L is the only existing study of automatic intention-based segmentation. However, their work has been frequently cited as a study of topic-oriented segmentation, e.g., (Galley et al., 2003; Eisenstein and Barzilay, 2008). Also, recent research in conversational genres (Galley et al., 2003; Hsueh and Moore, 2007) analyze events like discussing an agenda or giving a presentation, which resemble more intentional categories. Interestingly, these algorithms demonstrate the benefit of inclu</context>
</contexts>
<marker>Hsueh, 2008</marker>
<rawString>Pei-Yun Hsueh. 2008. Meeting Decision Detection: Multimodal Information Fusion for Multi-Party Dialogue Understanding. Ph.D. thesis, School of Informatics, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2002</date>
<booktitle>In NIPS 15.</booktitle>
<contexts>
<context position="13384" citStr="Klein and Manning, 2002" startWordPosition="2099" endWordPosition="2102">g to par56 ticipant subjectivity and participant involvement, according to which activity types may be distinguished. We then develop a routine for automatically extracting the linguistic features which indicate such properties. Finally, the dialogue is segmented at locations of high discontinuity in that feature space. The algorithm works in four phases: pre-processing, feature extraction, similarity measurement, and boundary assignment. 3.2.1 Pre-processing For pre-processing, disfluencies are removed by deleting repeated strings of words and incomplete words. The transcript is then parsed (Klein and Manning, 2002), and a collection of typed grammatical dependencies are generated (de Marneffe et al., 2006). The TTT2 chunker (Grover and Tobin, 2006) is then used to perform tense and aspect tagging. 3.2.2 Feature extraction Feature extraction is the most important and novel part of our algorithm. Each prosodic phrase (the corpus uses prosodic phrases as sentence-like units, see Data section) is assigned values for five binary features. The extracted features correspond to a set of utterance properties which were identified manually through corpus analysis. The first four relate directly to individual acti</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Christopher D. Manning. 2002. Fast exact inference with a factored model for natural language parsing. In NIPS 15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Natascha Korolija</author>
</authors>
<title>Episodes in talk: Constructing coherence in multiparty conversation.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Linköping University, The Tema Institute, Department of Communications Studies.</institution>
<contexts>
<context position="1621" citStr="Korolija, 1998" startWordPosition="237" endWordPosition="238">rsonal experience, making a group decision, committing to future action, and giving instructions. The reason we are interested in these kinds of events is that they are part of participants’ common-sense notion of the goals and accomplishments of a dialogue. They are part of participants’ subjective experience of what happened and show up in summaries of conversations such as meeting minutes. We therefore consider them an ideal target for the practical, common-sense description of conversations. Activities like these commonly occur as cohesive episodes of multiple turns within a conversation (Korolija, 1998). They represent an intermediate level of dialogue structure – greater than a single speech act but still small enough to have a potentially well-defined singular purpose. They have a temporal granularity of anywhere from a few seconds to several minutes. Ultimately, it would be useful to use descriptions of such activities in automatic summarization technologies for conversational genres. This would provide an activity-oriented summary describing what ’happened’ that would complement one based on information content or what the conversation was ’about’. Part of our research goal is thus to id</context>
<context position="27052" citStr="Korolija, 1998" startWordPosition="4283" endWordPosition="4284">0.24) . 4.3 Discussion The comparable performance achieved by our simple perspective-based approach in comparison to lexical-semantic approaches suggests two main points. First, it validates our novel approach in practical applied terms. It shows that perspectiveoriented features, being simple to extract and applicable to a variety of genres, are potentially very useful for automatic discourse segmentation systems. Second, the results show that the teasing apart of topic-oriented and intentional structure may be quite difficult. Studies of coherence at the level of short passages or episodes (Korolija, 1998) suggest that coherence is established through a complex interaction of topical, intentional, and other contextual factors. In this experiment, the major portion of the dialogues are oriented toward the basic narrative activity which is the premise of the Pear Stories dataset. This means that there are many times when the activity type does not change at intentional boundaries. At other times, the activity type changes but neither the topic nor the set of referents is significantly changed. The different types of algorithms we have tried (i.e., topical, referential, and perspectival) seem to b</context>
</contexts>
<marker>Korolija, 1998</marker>
<rawString>Natascha Korolija. 1998. Episodes in talk: Constructing coherence in multiparty conversation. Ph.D. thesis, Linköping University, The Tema Institute, Department of Communications Studies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Malioutov</author>
<author>Regina Barzilay</author>
</authors>
<title>Minimum cut model for spoken lecture segmentation.</title>
<date>2006</date>
<booktitle>In Proc. COLING-ACL,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="22581" citStr="Malioutov and Barzilay, 2006" startWordPosition="3578" endWordPosition="3581">uss the results. 4.1 Automatic segmentation in the literature One of the most widely-cited discourse segmentation algorithms is TextTiling (Hearst, 1997). Designed to segment texts into multi-paragraph subtopics, it works by operationalizing the notion of lexical cohesion (Halliday and Hasan, 1976). TextTiling and related algorithms exploit the collocation of semantically related lexemes to measure coherence. Recent improvements to this method include the use of alternative lexical similarity metrics like LSA (Choi et al., 2001) and alternative segmentation methods like the minimum cut model (Malioutov and Barzilay, 2006) and ranking and clustering (Choi, 2000). Recently, Bayesian approaches which model topics as a lexical generative process have been employed (Purver et al., 2006; Eisenstein and Barzilay, 2008). What these algorithms all share is a focus on the semantic content of the discourse. Passonneau and Litman (1997) is another of the most widely-cited articles on discourse segmentation. Their overall approach combines an investigation of prosodic features, cue words, and entity reference. As described above, their approach to using entity reference is motivated by Centering theory (Grosz et al., 1995)</context>
</contexts>
<marker>Malioutov, Barzilay, 2006</marker>
<rawString>Igor Malioutov and Regina Barzilay. 2006. Minimum cut model for spoken lecture segmentation. In Proc. COLING-ACL, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Diane J Litman</author>
</authors>
<title>Discourse segmentation by human and automated means.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="4484" citStr="Passonneau and Litman, 1997" startWordPosition="663" endWordPosition="666">c phenomena which express participantrelational properties may be used as an effective means of intentional discourse segmentation. This is based on the idea that if adjacent discourse segments have different activity types, then they are distinguishable by participant-relational features. If we can reliably extract such features, then this would allow segmentation of the dialogue accordingly. We test our hypothesis by constructing an algorithm and examining its performance on an existing set of intentionally segmented conversational monologues (i.e., one person speaks while another listens) (Passonneau and Litman, 1997, henceforth P&amp;L). While our long term goal is to apply our techniques to multi-party conversations (and to a somewhat coarser-grained analysis), using this dataset is a stepping-stone toward that end which allows us to compare our results with existing intentional segmentation algorithms. An example dialogue extract from the dataset is shown in Dialogue 1. Two horizontal lines indicate a segment boundary which was identified by at least 3 of 7 annotators. A single horizontal line indicates a segment boundary which was identified by 2 or fewer annotators. In the examPearStories-09 (Chafe, 1980</context>
<context position="22890" citStr="Passonneau and Litman (1997)" startWordPosition="3629" endWordPosition="3632">g and related algorithms exploit the collocation of semantically related lexemes to measure coherence. Recent improvements to this method include the use of alternative lexical similarity metrics like LSA (Choi et al., 2001) and alternative segmentation methods like the minimum cut model (Malioutov and Barzilay, 2006) and ranking and clustering (Choi, 2000). Recently, Bayesian approaches which model topics as a lexical generative process have been employed (Purver et al., 2006; Eisenstein and Barzilay, 2008). What these algorithms all share is a focus on the semantic content of the discourse. Passonneau and Litman (1997) is another of the most widely-cited articles on discourse segmentation. Their overall approach combines an investigation of prosodic features, cue words, and entity reference. As described above, their approach to using entity reference is motivated by Centering theory (Grosz et al., 1995) and the hypothesis that intentional structure is exhibited in the attentional relationships between discourse referents. Hearst and P&amp;L try to achieve different goals, but their tasks are nonetheless related. One might reasonably hypothesize, for example, that either lexical similarity or co-reference could</context>
</contexts>
<marker>Passonneau, Litman, 1997</marker>
<rawString>Rebecca J. Passonneau and Diane J. Litman. 1997. Discourse segmentation by human and automated means. Computational Linguistics, 23(1):103–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Purver</author>
<author>Konrad Körding</author>
<author>Thomas Griffiths</author>
<author>Joshua Tenenbaum</author>
</authors>
<title>Unsupervised topic modelling for multi-party spoken discourse.</title>
<date>2006</date>
<booktitle>In Proc. COLING-ACL,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="22743" citStr="Purver et al., 2006" startWordPosition="3605" endWordPosition="3608">nt texts into multi-paragraph subtopics, it works by operationalizing the notion of lexical cohesion (Halliday and Hasan, 1976). TextTiling and related algorithms exploit the collocation of semantically related lexemes to measure coherence. Recent improvements to this method include the use of alternative lexical similarity metrics like LSA (Choi et al., 2001) and alternative segmentation methods like the minimum cut model (Malioutov and Barzilay, 2006) and ranking and clustering (Choi, 2000). Recently, Bayesian approaches which model topics as a lexical generative process have been employed (Purver et al., 2006; Eisenstein and Barzilay, 2008). What these algorithms all share is a focus on the semantic content of the discourse. Passonneau and Litman (1997) is another of the most widely-cited articles on discourse segmentation. Their overall approach combines an investigation of prosodic features, cue words, and entity reference. As described above, their approach to using entity reference is motivated by Centering theory (Grosz et al., 1995) and the hypothesis that intentional structure is exhibited in the attentional relationships between discourse referents. Hearst and P&amp;L try to achieve different </context>
</contexts>
<marker>Purver, Körding, Griffiths, Tenenbaum, 2006</marker>
<rawString>Matthew Purver, Konrad Körding, Thomas Griffiths, and Joshua Tenenbaum. 2006. Unsupervised topic modelling for multi-party spoken discourse. In Proc. COLING-ACL, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlota S Smith</author>
</authors>
<title>Modes of Discourse.</title>
<date>2003</date>
<publisher>Cambdrige University Press,</publisher>
<location>Cambridge.</location>
<marker>Smith, 2003</marker>
<rawString>Carlota S. Smith. 2003. Modes of Discourse. Cambdrige University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce M Wiebe</author>
</authors>
<title>Tracking point of view in narrative.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="9545" citStr="Wiebe (1994" startWordPosition="1494" endWordPosition="1495">icking pears might say “the man was picking pears”, “the man picks some pears”, or “you see a man picking pears.” Each variant is an expression of the same idea but reflects a different perspective toward, or manner of participation in, the described event. The linguistic variation one sees in this example is in the properties of tense and aspect in the main clause (and in the last variant, a perspectival superordinate clause which uses the generic you). We have observed that discourse coheres in these perspectival terms, with shifts of perspective usually occurring at intentional boundaries. Wiebe (1994; 1995) has investigated a phenomenon closely related to this: point-of-view and subjectivity in fictional narrative. She notes that paragraph-level blocks of text often share a common objective or subjective context. That is, sentences may or may not be conveyed from the point-of-view of individuals, e.g., the author or the characters within the narrative. Sentences continue, resume, or initiate such contexts, and she develops automatic methods for determining when the contexts shift and whose point-of-view is being taken. Her algorithm provides a detailed method for analyzing written fiction</context>
</contexts>
<marker>Wiebe, 1994</marker>
<rawString>Janyce M. Wiebe. 1994. Tracking point of view in narrative. Computational Linguistics, 20(2):233–287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce M Wiebe</author>
</authors>
<title>References in narrative text.</title>
<date>1995</date>
<booktitle>Deixis in Narrative: A Cognitive Science Perspective,</booktitle>
<pages>263--286</pages>
<editor>In Judy Duchan, Gail Bruder, and Lynne Hewitt, editors,</editor>
<marker>Wiebe, 1995</marker>
<rawString>Janyce M. Wiebe. 1995. References in narrative text. In Judy Duchan, Gail Bruder, and Lynne Hewitt, editors, Deixis in Narrative: A Cognitive Science Perspective, pages 263–286.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>