<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001975">
<title confidence="0.996019">
Is a Companion a distinctive kind of relationship with a machine?
</title>
<author confidence="0.995828">
Yorick Wilks
</author>
<affiliation confidence="0.990083">
Florida Institute of Human and Machine Cognition
</affiliation>
<email confidence="0.987034">
ywilks@ihmc.us
</email>
<sectionHeader confidence="0.982571" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999973052631579">
I start from a perspective close to that of the EC
COMPANIONS project, and set out its aim to
model a new kind of human-computer relation-
ship based on long-term interaction, with some
tasks involved although a Companion should not
be inherently task-based, since there need be no
stopping point to its conversation. Some demon-
stration of its functionality will be given but the
main purpose here is an analysis of what it is
people might want from such a relationship and
what evidence we have for whatever we con-
clude. Is politeness important? Is an attempt at
emotional sympathy important or achievable?
Does a user want a consistent personality in a
Companion or a variety of personalities? Should
we be talking more in terms of a &amp;quot;cognitive pros-
thesis (or orthosis)?&amp;quot; ---something to extract, or-
ganize, and locate the user&apos;s knowledge or per-
sonal information---rather than attitudes?
</bodyText>
<sectionHeader confidence="0.997818" genericHeader="keywords">
1. Introduction
</sectionHeader>
<bodyText confidence="0.998749222222222">
It is convenient to distinguish Companions from
both (a) conversational internet agents that carry
out specific tasks, such as the train and plane
scheduling and ticket ordering speech dialogue
applications back to the MIT ATIS systems (Zue
et al., 1992), and also from (b) descendants of the
early chatbots PARRY and ELIZA, the best of
which compete annually in the Loebner competi-
tion (Loebner). These have essentially no mem-
ory or knowledge but are simple finite state re-
sponse sets, although ELIZA had primitive
“scripts” giving some context, and PARRY
(Colby, 1971) had parameters like FEAR and
ANGER that changed with the conversation and
determined which reply was selected at a given
point.
I take plausible distinguishing features of a
Companion agent to be:
</bodyText>
<listItem confidence="0.895806181818182">
1) that it has no central or over-riding task
and there is no point at which its conver-
sation is complete or has to stop, al-
though it may have some tasks it carries
out in the course of conversation;
2) That it should be capable of a sustained
discourse over a long-period, possibly
ideally the whole life-time of its princi-
pal user;
3) It is essentially the Companion of a par-
ticular individual, its principal user,
about whom it knows a great deal of per-
sonal knowledge, and whose interests it
serves—it could, in principle, contain all
the information associated with a whole
life;
4) It establishes some form of relationship
with that user, if that is appropriate,
which would have aspects associated
with the term “emotion”, and shared ini-
tiative is essential;
5) It is not essentially an internet agent or
</listItem>
<bodyText confidence="0.989681787878788">
interface, but since it will have to have
access to the internet for information (in-
cluding the whole-life information about
its user—which could be public data like
Facebook, or life information built up by
the Companion over long periods of in-
teraction with the user) and to act in the
world, e.g. to reserve at a restaurant or
call a doctor. But a Companion need not
be a robot to act in the world in this way,
and we may as well assume its internet
agent status, with access to open internet
knowledge sources.
Given this narrowing of focus in this paper, what
questions then arise and what choices does that
leave open? We now discuss some obvious ques-
tions that have arisen in the literature:
i) Emotion, politeness and affection
Cheepen and Monaghan (1997) presented results
some thirteen years ago that customers of some
automata, such as ATMs, are repelled by exces-
sive politeness and endless repetitions of ”thank
you for using our service”, because they know
they are dealing with a machine and such feigned
sincerity is inappropriate. This suggests that po-
liteness is very much a matter of judgment in
certain situations, just as it is with humans,
where inappropriate politeness is often encoun-
tered. Wallis (Wallis et al., 2001) has reported
results that many find computer conversational-
ists “chippy” or “cocky” and suggests that this
should be avoided as it breeds hostility on the
part of users; he believes this is always a major
</bodyText>
<page confidence="0.991056">
13
</page>
<note confidence="0.6365185">
Proceedings of the 2010 Workshop on Companionable Dialogue Systems, ACL 2010, pages 13–18,
Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999667121495328">
risk in human-machine interactions.
We know, since the original work of Nass
(Reeves and Nass, 1996) and colleagues that
people will display some level of feeling for the
simplest machines, even PCs in his original ex-
periments, and Levy (2007) has argued persua-
sively that the trend seems to be towards high
levels of “affectionate” relationships with ma-
chines in the next decades, as realistic hardware
and sophisticated speech generation make ma-
chine interlocutors increasingly lifelike. How-
ever, much of this work is about human psychol-
ogy, faced with entities known to be artificial,
and does not bear directly on the issue of
whether Companions should attempt to detect
emotion in what they hear from us, or attempt to
generate it in what they say back.
The AI area of “emotion and machines” is con-
fused and contradictory: it has established itself
as more than an eccentric minority taste, but as
yet has nothing concrete to show beyond some
better than random algorithms for detecting “sen-
timent” in incoming text (e.g. Wiebe et al.,
2005), but even there its success is dependent on
effective content extraction techniques. This
work began as “content analysis” (Krippendorff,
2004) at the Harvard psychology department
many years ago and, while prose texts may offer
enough length to enable a measure of sentiment
to be assessed, this is not always the case with
short dialogue turns. That technology rested al-
most entirely on the supposed sentiment value of
individual words, which ignores the fact that
their value is content dependent. “Cancer” may
be marked as negative word but the utterance “I
have found a cure for cancer” is presumably
positive and detecting the appropriate response to
that utterance rests on the ability to do informa-
tion extraction beyond single terms. Failure to
observe this has led to many of the classic fool-
ishnesses of chatbots such as congratulating peo-
ple on the death of their relatives, and so on.
At deeper levels, there are conflicting theories of
emotion for automata, not all of which are con-
sistent and which apply only in limited ranges of
discourse. So, for example, the classic theory that
emotion is a response to the failure and success
of the machine’s plans (e.g. Marsella and Gratch,
2003) covers only those situations that are
clearly plan driven and, as we noted, Compan-
ionship dialogue is not always closely related to
plans and tasks. “Dimensional” theories (Cowie
et al., 2001, following Wundt, 1913), display
emotions along dimensions marked with opposed
qualities (such as positive-negative) and nor-
mally distribute across the space emotion “primi-
tives”, such as FEAR, and these normally as-
signed by manual tagging. All such assignments
of tags rest, like the text-sentiment theories
above, on human pre-tagging. The problem with
this is that tagging for “COMPANY” or “TEM-
PERATURE” (in classic NLP) is a quite differ-
ent task from tagging for “FEAR” and “AN-
GER”. These latter terms are not, and probably
cannot be, analyzed but rest on the commonsense
intuitions of the tagger, which may vary very
much from person to person—they have very
low consilience between taggers.
All this makes many emotion theories look
primitive in terms of developments in AI and
NLP elsewhere. Appraisal Theory (Scherer et al,
2008) seeks to explain why individuals can have
quite different emotional reactions to similar
situations because they have appraised them dif-
ferently, e.g. a death welcomed or regretted. Ap-
praisal can also be of the performance of planned
activities, in which case this theory approximates
to the plan-based one mentioned above. The the-
ory itself, like all such theories, has a large-
commonsense component, and the issue for
computational implementation is how, in assess-
ing the emotional state of the Companion’s user
to make such concepts quantitatively evaluable.
If the Companion conducts long conversations
with a user about his or her life, then one might
expect there to be ample opportunity to assess
the user’s appraisal of, say, a funeral or wedding
by means of the application of the sentiment ex-
traction techniques to what is said in the presence
of the relevant image. In so far as a Companion
can be said to have over-arching goals, such as
keeping the user happy then, to that degree, it is
not difficult to envisage methods (again based on
estimates of the happiness, or otherwise, of the
user’s utterances) for self-appraisal by the Com-
panion of its own performance and some conse-
quent causal link to generated demonstrations of
its own emotions of satisfaction or guilt.
In speaking of “language” and Companions, we
have so far ignored speech, although that is a
communication mode in which a great deal has
been done to identify and, more recently, gener-
ate, emotion-bearing components (Luneski et al.,
2008). Elements of the above approaches can be
found in the work of Worgan and Moore (see
figure below, from REFERENCE REMOVED),
where there is the same commitment to the cen-
</bodyText>
<page confidence="0.994377">
14
</page>
<bodyText confidence="0.992433738095239">
trality of emotion in the communication process,
but in a form focusing on an integration of
speech and language (rather than visual and de-
sign) technologies. Their argument is for a layer
in a dialogue manager over and above local re-
sponse management, but one which would seek
to navigate the whole conversation across a two-
dimensional space onto which Companion and
user are mapped using continuous values (rather
than discrete values corresponding to primitive
but unexplained emotional terms) but in such a
way as to both respond to the a user’s demon-
strated emotion appropriately, but also----again,
if appropriate or chosen by the user----to draw
the user back to other more positive emotional
areas of the two-dimensional space. It is not yet
clear what the right mechanism should be for the
integration of this “landscape” global emotion-
based dialogue manager should be with the local
dialogue management that generates responses
and alters the world context: in the Senior Com-
panion this last was sophisticated stack of net-
works (see Wilks et al., in press). In some sense,
we are just looking for a modern and defensible
interface to replace what PARRY had in simple
form in 1971 when the sum of two emotion pa-
rameters determined which response to select
from a stack of alternatives.
This last is a high level issue to be settled in a
Companion’s architecture and also, perhaps, to
be under the control of the user, namely: should a
Companion invariably try to cheer a user up if
miserable which is trying to “move” the user
to the most naturally desirable (i.e. the top-right)
quadrant of the space----or, rather, to track to the
part of the space where the user is deemed to be
and stay there in roughly the same emotional lo-
cation—i.e. be sad with a sad user and happy
with a happy one? There is no general answer to
this question and, indeed, in an ideal Companion,
which tracking method should be used would
itself be a conversation topic e.g. “Do you want
me to cheer you up or would you rather stay mis-
erable?”.
ii) What should a Companion look like?
A faceless Companion is a plausible candidate
for Companionhood: the proverbial furry hand-
bag, warm and light to carry, chatty but with full
internet access. Such a Companion could always
take control of a nearby screen or a phone if it
needed to show anything. If there is to be a face,
the question of the “uncanny valley effect” al-
ways comes up, where it is argued that users are
more uneasy the more something is very like
ourselves (Mori, 1970). But many observers do
not feel this, and, indeed it cannot in principle
apply to an avatar so good that one cannot be
sure it is artificial, as many feel about the Emily
from Manchester (Emily 2009).
On the other hand, if the quality is not good, and
in particular if the lip synch is not perfect, it may
be better to go for an abstract avatar ---the Com-
panions logo was chosen with that in mind, and
without a mouth at all. Non-human avatars seem
to avoid some of the problems that arise with
valleys and mixed feelings generally, and the
best REMOVED demonstration video so far fea-
tures REMOVED.
iii) Voice or Typing to communicate with a
Companion?
At the moment the limitation on the use of voice
is two-fold: first, although trained ASR for a sin-
gle user—such as a Companion’s user—is now
very good and up in the high 90%, it still intro-
duces uncertainty into understanding an utter-
ance that is far greater than that of spelling er-
rors. Secondly, it is currently not possible to
store sufficient ASR software locally on a mobile
phone to recognize a large vocabulary in real
time; access to a remote server takes additional
time and can be subject to fluctuations and de-
lays. All of which suggests that a web-based
Companion may have to use typed input in the
immediate future—though using TTS output—
</bodyText>
<page confidence="0.98974">
15
</page>
<bodyText confidence="0.999890555555556">
which is no problem for most mobile phone us-
ers, who have come to find typed chat perfectly
natural. However, this is almost certainly only a
transitory delay as mobile RAM increases rap-
idly and the problem should not determine re-
search decisions---there is no doubt that voice
will move back to the centre of communication
once storage and access size have grown by an-
other order of magnitude.
</bodyText>
<listItem confidence="0.560451">
iv) One Companion personality or several?
</listItem>
<bodyText confidence="0.991053395604397">
Some (e.g. Pulman, in Wilks, 2010) have argued
that having a consistent personality is a condition
on Companionhood, but one could differ and
argue that, although that is true of people—
multiple personalities being a classic psycho-
sis—there is no reason why we should expect
this of a Companion. Perhaps a Companion
should have a personality adapted to its particu-
lar relationship to a user at a given moment:
Lowe (in Wilks, 2010) has pointed out that one
might want a Companion to function as, say, a
gym trainer, in which case a rather harsh attitude
on the part of the Companion might well be the
best one. If a Companion’s emotional attitude
were to (figuratively) move across a two dimen-
sional emotion space (see diagram above) imitat-
ing or correcting what it perceived to be the
user’s state over time (as Worgan, see above, has
proposed), then that shift in attitude might well
seem to be the product of different personalities,
as it sometimes can with humans.
It might be better, pace Pulman, to give a user
access to, and some control over, the display of a
multiple-personality Companion, something one
could think of as an “agency” of Companions,
rather than a single “agent”, all of which shared
access to the same knowledge of the world and
of the state and history of the user.
v) Ethics and goals in the Companion
The issue is very close to the question of what
goals a Companion can plausibly have, beyond
something very general, such as “keep the user
happy and do what they ask if you can”, which
are goals and constraints that directly relate to
the standard discussions of the ethics a robot
could be considered to have, a discussion started
long ago by Asimov (1975). Clearly, there will
be need for a Companion to have goals to carry
out specific tasks: if it is to place a restaurant
table booking on the phone for a user who has
just said to it “Get me a table for two tonight at
Branca around 8.30”---a phone request well
within the bounds of the currently achievable
technology and the Companion will first have
to find the restaurant’s phone number before it
phones and ask about availability before choos-
ing a reservation time. This is the standard con-
tent of goal-driven behavior, with alternatives at
every stage if unexpected replies are encountered
(such as the restaurant being fully booked to-
night). But one does not need to consider such
goals as “goals of its own” since they are in-
ferred from what it was told and are simply as-
sumed, as an agent or slave of the user. But a
Companion that finds its user not responding
after some minutes of conversation might well
have to take an independent decision to call a
doctor urgently, based on a stored permanent
goal about danger to a user who is unable to an-
swer but is not asleep etc.
vi) Safeguards for the information content of a
Companion
Data protection, privacy, or whatever term one
prefers, now captures a crucial concept in the
new information society. A Companion that had
learned intimate details of a user’s life over
months or years would certainly have contents
needing protection, and many forces
commercial, security, governmental, research---
might well want access to it, or even to those of
all the Companions in a given society. If socie-
ties move to a clear legal state where one’s per-
sonal data is one’s own, with the owner or origi-
nator having rights over sale and distribution of
their data---which is not at all the case at the
moment in most countries----then the issue of the
personal data elicited by a Companion would
automatically be covered.
If we ignore the issues of governments and na-
tional security---and a Companion would clearly
be useful to the police when wanting to know as
much as possible about a murder suspect, so that
it might then be an issue of whether talking to
one’s Companion constituted any kind of self-
incrimination, in countries where that form of
communication is protected. Some might well
want one’s relationship to a Companion put on
some basis like that of a relationship to a priest
or doctor, or even to a spouse, who cannot al-
ways be forced to give evidence in common-law
countries.
</bodyText>
<page confidence="0.99352">
16
</page>
<bodyText confidence="0.982083136363637">
More realistically, a user might well want to pro-
tect parts of his or her Companion’s information,
or even an organized life-story based on that,
from particular individuals: e.g. “this must never
be told to my children, even when I am gone”. It
is not hard to imagine a Companion deciding
whom to divulge certain things to, selecting be-
tween classes of offspring, relations, friends, col-
leagues etc. There will almost certainly need to
be a new set of laws covering the ownership, in-
heritance and destruction of Companion-objects
in the future.
vii) What must a Companion know?
There is no clear answer to this question: dogs
make excellent Companions and know nothing.
More relevantly, Colby’s PARRY program, the
best conversationalist of its day (Colby, 1971)
and possibly since, famously “knew’ nothing:
John McCarthy at Stanford dismissed PARRY’s
performance by saying:”It doesn’t even know
who the US President is”, forgetting as he said it
that most of world’s population did not know
that, at least at the time. On the other hand, it is
hard to relate over a long term to an interlocutor
who knows little or nothing and has no memory
of what it or you have said in the past. It is hard
to attribute personality to an entity with no mem-
ory and little or no knowledge.
Much of what a Companion knows that is per-
sonal it should elicit in conversation from its
user; yet much could also be gained from pub-
licly available sources, just as the current Senior
Companion demo goes off to Facebook, inde-
pendently of a conversation, to find out who its
user’s friends are. Current information extraction
technology (e.g. Ciravegna et al., 2004) allows a
reasonable job to be made of going to Wikipedia
for general information when, say, a world city is
mentioned; the Companion can then glean some-
thing about that city from Wikipedia and ask a
relevant question such as “Did you see the Eiffel
Tower when you were in Paris?” which again
gives a plausible illusion of general knowledge.
A concrete Companion paradigm: the
Victorian Companion
The subsections above are mini-discussions of
some of the constraints on what it is to be a
Companion, the subject of a recent book collec-
tion (Wilks, 2010). The upshot of those discus-
sions is that there are many dimensions of
choice, even within an agreed definition of what
a Companion is to be, and they will depend on
the user’s tastes and needs above all. In the sec-
tion that follows, I cut though the choices and
make a semi-serious proposal for a model Com-
panion, one based on a once well-known social
stereotype.
More seriously, and in the spirit of a priori
thoughts (and what else can we have at this tech-
nological stage of development?) about what a
Companion should be, I would suggest we could
profitably spend a few moments reminding our-
selves of the role of the Victorian lady’s Com-
panion. One could, and in no scientific manner,
risk a listing of features of the ideal Victorian
Companion:
</bodyText>
<listItem confidence="0.9901224">
1. Politeness
2. Discretion
3. Knowing their place
4. Dependence
5. Emotions firmly under control
6. Modesty
7. Wit
8. Cheerfulness
9. Well-informed
10. Diverting
11. Looks are irrelevant
12. Long-term relationship if possible
13. Trustworthy
14. Limited socialization between Com-
panions permitted off-duty.
</listItem>
<bodyText confidence="0.94936605">
The Victorian virtue of discretion here brings to
mind the “confidant” concept that Boden (in
Wilks, 2010) explicitly rejected as being a plau-
sible one for automated Companions:
Most secrets are secret from some HBs [Human
Beings] but not others. If two CCs [Computer
Companions] were to share their HB-users’ se-
crets with each other, how would they know
which other CCs (i.e. potentially, users) to ’trust’
in this way? The HB could of course say &amp;quot;This is
not to be told to Tommy&amp;quot; but usually we re-
gard it as obvious that our confidant (sic) knows
what should not be told to Tommy -- either to
avoid upsetting Tommy, or to avoid upsetting the
original HB. How is a CC to emulate that?
The HB could certainly say &amp;quot;Tell this to no-one&amp;quot;
-- where &amp;quot;no-one&amp;quot; includes other CCs. But would
the HB always remember to do that?
How could a secret-sharing CC deal with family
feuds? Some family websites have special func-
</bodyText>
<page confidence="0.997846">
17
</page>
<bodyText confidence="0.999989673469388">
tionalities to deal with this. E.g Robbie is never
shown input posted by Billie. Could similar, or
more subtle, functionalities be given to CCs?”
Boden brings up real difficulties in extending
this notion to a computer Companion, but the
problems are not all where she thinks. I see no
difficulty in programming the notion of explicit
secrets for a Companion, or even things to be
kept from specific individuals (“Never tell this to
Tommy”). Companions will have less problems
remembering to be discrete than people do, and I
suspect people have less instinctive discretion
than Boden believes: they have to be told explic-
itly who to say what to, or not, in most cases,
unless they are told to tell no one. In any case,
much of this will be moot because Companions
will normally deal only with one person except
when, say, making phone calls to an official,
friend or restaurant, where they can try to keep
the conversation to limited replies that they can
be sure to understand. The notion of a stored fact
that must not be disclosed is relatively simple to
code. Nonetheless, the Lady’s Companion anal-
ogy foresees that Companions will, in time, gos-
sip among themselves behind their owners’
backs.
I would argue that the “Lady’s Companion” list
above an attractive and plausible one: it assumes
emotion will be largely linguistic in expression,
it implies care for the mental and emotional state
of the user, and I would personally find it hard to
abuse any computer with the characteristics
listed above. Many of the situations discussed
above are, at the moment, wildly speculative:
that of a Companion acting as its owner’s agent,
on the phone or World Wide Web, perhaps hold-
ing power of attorney in case of an owner’s inca-
pacity and, with the owner’s advance permission,
perhaps even being a source of conversational
comfort for relatives after the owner’s death.
Companions may not all be nice or even friendly:
Companions to stop us falling asleep while driv-
ing may tell us jokes but will probably shout at
us and make us do stretching exercises. Long-
voyage Companions in space will be indispensa-
ble cognitive prostheses (or, more correctly, or-
thoses) for running a huge vessel and experi-
ments above any beyond any personal services---
Hollywood already knows all that.
</bodyText>
<sectionHeader confidence="0.996829" genericHeader="introduction">
Acknowledgement:
</sectionHeader>
<footnote confidence="0.56755">
This work was funded by the Companions project
sponsored by the European Commission as part of the
Information Society Technologies (IST) programme
under EC grant number IST-FP6-034434.
</footnote>
<sectionHeader confidence="0.976513" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999903559322034">
Colby, K.M. &amp;quot;Artificial Paranoia.&amp;quot; Artif. Intell. 2(1)
(1971), pp. 1-2
Cheepen, C. and Monaghan, J. 1997, ‘Designing
Naturalness in Automated Dialogues - some problems
and solutions’. In Proceedings ‘First International
Workshop on Human- Computer Conversation’, Bel-
lagio, Italy.
Cowie, R., Douglas-Cowie, E., Tsapatsoulis, N., Vot-
sis, G., Kollias, S., Fellenz, W. and Taylor, JG. 2001.
Emotion recognition in human-computer interaction,
Signal Processing Magazine, IEEE, 18(1), pp. 32–80.
Emily,2009.http://www.youtube.com/watch?v=UYgL
Ft5wfP4&amp;feature=player_embedded#
http://www.surrealaward.com/avatar/3ddigital12.shtm
l
Krippendorff, K. 2004. Content Analysis: An Intro-
duction to Its Methodology. 2nd edition, Thousand
Oaks, CA: Sage.
Levy, D. 2007. Love and Sex with Robots: The Evo-
lution of Human–Robot Relationships. London:
Duckworth.
Luneski, A., Moore, R. K., &amp; Bamidis, P. D. (2008).
Affective computing and collaborative networks: to-
wards emotion-aware interaction. In L. M. Cama-
rinha-Matos &amp; W. Picard (Eds.), Pervasive Collabora-
tive Networks (Vol. 283, pp. 315-322). Boston:
Springer.
Marsella, S. and Gratch, J. (2003) Modeling Coping
Behavior in Virtual Humans: Don&apos;t Worry, Be Happy.
2nd Int Conf on Autonomous Agents and Multiagent
Systems (AAMAS), Melbourne, Australia, July 2003.
Reeves, B., Nass, C. 1996, The media equation: how
people treat computers, television, and new media like
real people and places, Cambridge: Cambridge Uni-
versity Press, 1996.
Scherer, S., Schwenker, F. and Palm, G. 2008. Emo-
tion recognition from speech using multi-classifier
systems and rbf-ensembles, in Speech, Audio, Image
and Biomedical Signal Processing using Neural Net-
works, pp. 49–70, Springer: Berlin.
Wallis, P., Mitchard, H., O’Dea, D., and Das J. 2001,
Dialogue modelling for a conversational agent. In
‘AI-2001: Advances in Artificial Intelligence’,
Stumptner, Corbett, and Brooks, (eds.), In Proceed-
ings 14th Australian Joint Conference on Artificial
Intelligence, Adelaide, Australia.
Wiebe, J., Wilson , T., and Cardie, C. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, vol-
ume 39, issue 2-3, pp. 165-210.
Wilks, Y. (ed.) (2010) Artificial Companions in Soci-
ety: scientific, economic, psychological and philoso-
phical perspectives. John Benjamins: Amsterdam.
Wundt, W., 1913. Grundriss der Psychologie, A.
Kroner: Berlin.
Zue, V., Glass, J., Goddeau, D., Goodine, D.,
Hirschman, L. 1992. The MIT ATIS system, In Proc.
Workshop on speech and natural language, Harriman,
New York.
</reference>
<page confidence="0.999291">
18
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.279979">
<title confidence="0.762626">Is a Companion a distinctive kind of relationship with a machine? Yorick Wilks</title>
<author confidence="0.428351">Florida Institute of Human</author>
<author confidence="0.428351">Machine</author>
<email confidence="0.887705">ywilks@ihmc.us</email>
<abstract confidence="0.9928208">I start from a perspective close to that of the EC COMPANIONS project, and set out its aim to model a new kind of human-computer relationship based on long-term interaction, with some tasks involved although a Companion should not since there need be no stopping point to its conversation. Some demonstration of its functionality will be given but the main purpose here is an analysis of what it is people might want from such a relationship and what evidence we have for whatever we conclude. Is politeness important? Is an attempt at emotional sympathy important or achievable? Does a user want a consistent personality in a Companion or a variety of personalities? Should we be talking more in terms of a &amp;quot;cognitive prosthesis (or orthosis)?&amp;quot; ---something to extract, organize, and locate the user&apos;s knowledge or personal information---rather than attitudes?</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K M Colby</author>
</authors>
<title>Artificial Paranoia.&amp;quot;</title>
<date>1971</date>
<journal>Artif. Intell.</journal>
<volume>2</volume>
<issue>1</issue>
<pages>1--2</pages>
<contexts>
<context position="1626" citStr="Colby, 1971" startWordPosition="262" endWordPosition="263">---rather than attitudes? 1. Introduction It is convenient to distinguish Companions from both (a) conversational internet agents that carry out specific tasks, such as the train and plane scheduling and ticket ordering speech dialogue applications back to the MIT ATIS systems (Zue et al., 1992), and also from (b) descendants of the early chatbots PARRY and ELIZA, the best of which compete annually in the Loebner competition (Loebner). These have essentially no memory or knowledge but are simple finite state response sets, although ELIZA had primitive “scripts” giving some context, and PARRY (Colby, 1971) had parameters like FEAR and ANGER that changed with the conversation and determined which reply was selected at a given point. I take plausible distinguishing features of a Companion agent to be: 1) that it has no central or over-riding task and there is no point at which its conversation is complete or has to stop, although it may have some tasks it carries out in the course of conversation; 2) That it should be capable of a sustained discourse over a long-period, possibly ideally the whole life-time of its principal user; 3) It is essentially the Companion of a particular individual, its p</context>
<context position="18394" citStr="Colby, 1971" startWordPosition="3139" endWordPosition="3140"> particular individuals: e.g. “this must never be told to my children, even when I am gone”. It is not hard to imagine a Companion deciding whom to divulge certain things to, selecting between classes of offspring, relations, friends, colleagues etc. There will almost certainly need to be a new set of laws covering the ownership, inheritance and destruction of Companion-objects in the future. vii) What must a Companion know? There is no clear answer to this question: dogs make excellent Companions and know nothing. More relevantly, Colby’s PARRY program, the best conversationalist of its day (Colby, 1971) and possibly since, famously “knew’ nothing: John McCarthy at Stanford dismissed PARRY’s performance by saying:”It doesn’t even know who the US President is”, forgetting as he said it that most of world’s population did not know that, at least at the time. On the other hand, it is hard to relate over a long term to an interlocutor who knows little or nothing and has no memory of what it or you have said in the past. It is hard to attribute personality to an entity with no memory and little or no knowledge. Much of what a Companion knows that is personal it should elicit in conversation from i</context>
</contexts>
<marker>Colby, 1971</marker>
<rawString>Colby, K.M. &amp;quot;Artificial Paranoia.&amp;quot; Artif. Intell. 2(1) (1971), pp. 1-2</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cheepen</author>
<author>J Monaghan</author>
</authors>
<title>Designing Naturalness in Automated Dialogues - some problems and solutions’.</title>
<date>1997</date>
<booktitle>In Proceedings ‘First International Workshop on Human- Computer Conversation’,</booktitle>
<location>Bellagio, Italy.</location>
<contexts>
<context position="3399" citStr="Cheepen and Monaghan (1997)" startWordPosition="570" endWordPosition="573">ich could be public data like Facebook, or life information built up by the Companion over long periods of interaction with the user) and to act in the world, e.g. to reserve at a restaurant or call a doctor. But a Companion need not be a robot to act in the world in this way, and we may as well assume its internet agent status, with access to open internet knowledge sources. Given this narrowing of focus in this paper, what questions then arise and what choices does that leave open? We now discuss some obvious questions that have arisen in the literature: i) Emotion, politeness and affection Cheepen and Monaghan (1997) presented results some thirteen years ago that customers of some automata, such as ATMs, are repelled by excessive politeness and endless repetitions of ”thank you for using our service”, because they know they are dealing with a machine and such feigned sincerity is inappropriate. This suggests that politeness is very much a matter of judgment in certain situations, just as it is with humans, where inappropriate politeness is often encountered. Wallis (Wallis et al., 2001) has reported results that many find computer conversationalists “chippy” or “cocky” and suggests that this should be avo</context>
</contexts>
<marker>Cheepen, Monaghan, 1997</marker>
<rawString>Cheepen, C. and Monaghan, J. 1997, ‘Designing Naturalness in Automated Dialogues - some problems and solutions’. In Proceedings ‘First International Workshop on Human- Computer Conversation’, Bellagio, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Cowie</author>
<author>E Douglas-Cowie</author>
<author>N Tsapatsoulis</author>
<author>G Votsis</author>
<author>S Kollias</author>
<author>W Fellenz</author>
<author>JG Taylor</author>
</authors>
<date>2001</date>
<booktitle>Emotion recognition in human-computer interaction, Signal Processing Magazine, IEEE,</booktitle>
<volume>18</volume>
<issue>1</issue>
<pages>32--80</pages>
<note>Emily,2009.http://www.youtube.com/watch?v=UYgL Ft5wfP4&amp;feature=player_embedded#</note>
<contexts>
<context position="6699" citStr="Cowie et al., 2001" startWordPosition="1113" endWordPosition="1116">o many of the classic foolishnesses of chatbots such as congratulating people on the death of their relatives, and so on. At deeper levels, there are conflicting theories of emotion for automata, not all of which are consistent and which apply only in limited ranges of discourse. So, for example, the classic theory that emotion is a response to the failure and success of the machine’s plans (e.g. Marsella and Gratch, 2003) covers only those situations that are clearly plan driven and, as we noted, Companionship dialogue is not always closely related to plans and tasks. “Dimensional” theories (Cowie et al., 2001, following Wundt, 1913), display emotions along dimensions marked with opposed qualities (such as positive-negative) and normally distribute across the space emotion “primitives”, such as FEAR, and these normally assigned by manual tagging. All such assignments of tags rest, like the text-sentiment theories above, on human pre-tagging. The problem with this is that tagging for “COMPANY” or “TEMPERATURE” (in classic NLP) is a quite different task from tagging for “FEAR” and “ANGER”. These latter terms are not, and probably cannot be, analyzed but rest on the commonsense intuitions of the tagge</context>
</contexts>
<marker>Cowie, Douglas-Cowie, Tsapatsoulis, Votsis, Kollias, Fellenz, Taylor, 2001</marker>
<rawString>Cowie, R., Douglas-Cowie, E., Tsapatsoulis, N., Votsis, G., Kollias, S., Fellenz, W. and Taylor, JG. 2001. Emotion recognition in human-computer interaction, Signal Processing Magazine, IEEE, 18(1), pp. 32–80. Emily,2009.http://www.youtube.com/watch?v=UYgL Ft5wfP4&amp;feature=player_embedded#</rawString>
</citation>
<citation valid="false">
<note>http://www.surrealaward.com/avatar/3ddigital12.shtm l</note>
<marker></marker>
<rawString>http://www.surrealaward.com/avatar/3ddigital12.shtm l</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology. 2nd edition,</title>
<date>2004</date>
<publisher>Sage.</publisher>
<location>Thousand Oaks, CA:</location>
<contexts>
<context position="5448" citStr="Krippendorff, 2004" startWordPosition="904" endWordPosition="905">o be artificial, and does not bear directly on the issue of whether Companions should attempt to detect emotion in what they hear from us, or attempt to generate it in what they say back. The AI area of “emotion and machines” is confused and contradictory: it has established itself as more than an eccentric minority taste, but as yet has nothing concrete to show beyond some better than random algorithms for detecting “sentiment” in incoming text (e.g. Wiebe et al., 2005), but even there its success is dependent on effective content extraction techniques. This work began as “content analysis” (Krippendorff, 2004) at the Harvard psychology department many years ago and, while prose texts may offer enough length to enable a measure of sentiment to be assessed, this is not always the case with short dialogue turns. That technology rested almost entirely on the supposed sentiment value of individual words, which ignores the fact that their value is content dependent. “Cancer” may be marked as negative word but the utterance “I have found a cure for cancer” is presumably positive and detecting the appropriate response to that utterance rests on the ability to do information extraction beyond single terms. </context>
</contexts>
<marker>Krippendorff, 2004</marker>
<rawString>Krippendorff, K. 2004. Content Analysis: An Introduction to Its Methodology. 2nd edition, Thousand Oaks, CA: Sage.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Levy</author>
</authors>
<title>Love and Sex with Robots: The Evolution of Human–Robot Relationships.</title>
<date>2007</date>
<location>London: Duckworth.</location>
<contexts>
<context position="4501" citStr="Levy (2007)" startWordPosition="749" endWordPosition="750">d results that many find computer conversationalists “chippy” or “cocky” and suggests that this should be avoided as it breeds hostility on the part of users; he believes this is always a major 13 Proceedings of the 2010 Workshop on Companionable Dialogue Systems, ACL 2010, pages 13–18, Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics risk in human-machine interactions. We know, since the original work of Nass (Reeves and Nass, 1996) and colleagues that people will display some level of feeling for the simplest machines, even PCs in his original experiments, and Levy (2007) has argued persuasively that the trend seems to be towards high levels of “affectionate” relationships with machines in the next decades, as realistic hardware and sophisticated speech generation make machine interlocutors increasingly lifelike. However, much of this work is about human psychology, faced with entities known to be artificial, and does not bear directly on the issue of whether Companions should attempt to detect emotion in what they hear from us, or attempt to generate it in what they say back. The AI area of “emotion and machines” is confused and contradictory: it has establis</context>
</contexts>
<marker>Levy, 2007</marker>
<rawString>Levy, D. 2007. Love and Sex with Robots: The Evolution of Human–Robot Relationships. London: Duckworth.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Luneski</author>
<author>R K Moore</author>
<author>P D Bamidis</author>
</authors>
<title>Affective computing and collaborative networks: towards emotion-aware interaction.</title>
<date>2008</date>
<journal>In L. M. Camarinha-Matos &amp; W. Picard (Eds.), Pervasive Collaborative Networks</journal>
<volume>283</volume>
<pages>315--322</pages>
<publisher>Springer.</publisher>
<location>Boston:</location>
<contexts>
<context position="9059" citStr="Luneski et al., 2008" startWordPosition="1499" endWordPosition="1502"> can be said to have over-arching goals, such as keeping the user happy then, to that degree, it is not difficult to envisage methods (again based on estimates of the happiness, or otherwise, of the user’s utterances) for self-appraisal by the Companion of its own performance and some consequent causal link to generated demonstrations of its own emotions of satisfaction or guilt. In speaking of “language” and Companions, we have so far ignored speech, although that is a communication mode in which a great deal has been done to identify and, more recently, generate, emotion-bearing components (Luneski et al., 2008). Elements of the above approaches can be found in the work of Worgan and Moore (see figure below, from REFERENCE REMOVED), where there is the same commitment to the cen14 trality of emotion in the communication process, but in a form focusing on an integration of speech and language (rather than visual and design) technologies. Their argument is for a layer in a dialogue manager over and above local response management, but one which would seek to navigate the whole conversation across a twodimensional space onto which Companion and user are mapped using continuous values (rather than discret</context>
</contexts>
<marker>Luneski, Moore, Bamidis, 2008</marker>
<rawString>Luneski, A., Moore, R. K., &amp; Bamidis, P. D. (2008). Affective computing and collaborative networks: towards emotion-aware interaction. In L. M. Camarinha-Matos &amp; W. Picard (Eds.), Pervasive Collaborative Networks (Vol. 283, pp. 315-322). Boston: Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Marsella</author>
<author>J Gratch</author>
</authors>
<title>Modeling Coping Behavior in Virtual Humans: Don&apos;t Worry, Be Happy.</title>
<date>2003</date>
<booktitle>2nd Int Conf on Autonomous Agents and Multiagent Systems (AAMAS),</booktitle>
<location>Melbourne, Australia,</location>
<contexts>
<context position="6507" citStr="Marsella and Gratch, 2003" startWordPosition="1082" endWordPosition="1085">ure for cancer” is presumably positive and detecting the appropriate response to that utterance rests on the ability to do information extraction beyond single terms. Failure to observe this has led to many of the classic foolishnesses of chatbots such as congratulating people on the death of their relatives, and so on. At deeper levels, there are conflicting theories of emotion for automata, not all of which are consistent and which apply only in limited ranges of discourse. So, for example, the classic theory that emotion is a response to the failure and success of the machine’s plans (e.g. Marsella and Gratch, 2003) covers only those situations that are clearly plan driven and, as we noted, Companionship dialogue is not always closely related to plans and tasks. “Dimensional” theories (Cowie et al., 2001, following Wundt, 1913), display emotions along dimensions marked with opposed qualities (such as positive-negative) and normally distribute across the space emotion “primitives”, such as FEAR, and these normally assigned by manual tagging. All such assignments of tags rest, like the text-sentiment theories above, on human pre-tagging. The problem with this is that tagging for “COMPANY” or “TEMPERATURE” </context>
</contexts>
<marker>Marsella, Gratch, 2003</marker>
<rawString>Marsella, S. and Gratch, J. (2003) Modeling Coping Behavior in Virtual Humans: Don&apos;t Worry, Be Happy. 2nd Int Conf on Autonomous Agents and Multiagent Systems (AAMAS), Melbourne, Australia, July 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Reeves</author>
<author>C Nass</author>
</authors>
<title>The media equation: how people treat computers, television, and new media like real people and places, Cambridge:</title>
<date>1996</date>
<publisher>Cambridge University Press,</publisher>
<contexts>
<context position="4358" citStr="Reeves and Nass, 1996" startWordPosition="723" endWordPosition="726">f judgment in certain situations, just as it is with humans, where inappropriate politeness is often encountered. Wallis (Wallis et al., 2001) has reported results that many find computer conversationalists “chippy” or “cocky” and suggests that this should be avoided as it breeds hostility on the part of users; he believes this is always a major 13 Proceedings of the 2010 Workshop on Companionable Dialogue Systems, ACL 2010, pages 13–18, Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics risk in human-machine interactions. We know, since the original work of Nass (Reeves and Nass, 1996) and colleagues that people will display some level of feeling for the simplest machines, even PCs in his original experiments, and Levy (2007) has argued persuasively that the trend seems to be towards high levels of “affectionate” relationships with machines in the next decades, as realistic hardware and sophisticated speech generation make machine interlocutors increasingly lifelike. However, much of this work is about human psychology, faced with entities known to be artificial, and does not bear directly on the issue of whether Companions should attempt to detect emotion in what they hear</context>
</contexts>
<marker>Reeves, Nass, 1996</marker>
<rawString>Reeves, B., Nass, C. 1996, The media equation: how people treat computers, television, and new media like real people and places, Cambridge: Cambridge University Press, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Scherer</author>
<author>F Schwenker</author>
<author>G Palm</author>
</authors>
<title>Emotion recognition from speech using multi-classifier systems and rbf-ensembles,</title>
<date>2008</date>
<booktitle>in Speech, Audio, Image and Biomedical Signal Processing using Neural Networks,</booktitle>
<pages>49--70</pages>
<publisher>Springer: Berlin.</publisher>
<contexts>
<context position="7537" citStr="Scherer et al, 2008" startWordPosition="1247" endWordPosition="1250">gned by manual tagging. All such assignments of tags rest, like the text-sentiment theories above, on human pre-tagging. The problem with this is that tagging for “COMPANY” or “TEMPERATURE” (in classic NLP) is a quite different task from tagging for “FEAR” and “ANGER”. These latter terms are not, and probably cannot be, analyzed but rest on the commonsense intuitions of the tagger, which may vary very much from person to person—they have very low consilience between taggers. All this makes many emotion theories look primitive in terms of developments in AI and NLP elsewhere. Appraisal Theory (Scherer et al, 2008) seeks to explain why individuals can have quite different emotional reactions to similar situations because they have appraised them differently, e.g. a death welcomed or regretted. Appraisal can also be of the performance of planned activities, in which case this theory approximates to the plan-based one mentioned above. The theory itself, like all such theories, has a largecommonsense component, and the issue for computational implementation is how, in assessing the emotional state of the Companion’s user to make such concepts quantitatively evaluable. If the Companion conducts long convers</context>
</contexts>
<marker>Scherer, Schwenker, Palm, 2008</marker>
<rawString>Scherer, S., Schwenker, F. and Palm, G. 2008. Emotion recognition from speech using multi-classifier systems and rbf-ensembles, in Speech, Audio, Image and Biomedical Signal Processing using Neural Networks, pp. 49–70, Springer: Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Wallis</author>
<author>H Mitchard</author>
<author>D O’Dea</author>
<author>J Das</author>
</authors>
<title>Dialogue modelling for a conversational agent.</title>
<date>2001</date>
<booktitle>In ‘AI-2001: Advances in Artificial Intelligence’, Stumptner,</booktitle>
<editor>Corbett, and Brooks, (eds.),</editor>
<location>Adelaide, Australia.</location>
<marker>Wallis, Mitchard, O’Dea, Das, 2001</marker>
<rawString>Wallis, P., Mitchard, H., O’Dea, D., and Das J. 2001, Dialogue modelling for a conversational agent. In ‘AI-2001: Advances in Artificial Intelligence’, Stumptner, Corbett, and Brooks, (eds.), In Proceedings 14th Australian Joint Conference on Artificial Intelligence, Adelaide, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T</author>
<author>C Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language.</title>
<date>2005</date>
<journal>Language Resources and Evaluation,</journal>
<volume>39</volume>
<pages>2--3</pages>
<marker>T, Cardie, 2005</marker>
<rawString>Wiebe, J., Wilson , T., and Cardie, C. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, volume 39, issue 2-3, pp. 165-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wilks</author>
</authors>
<title>Artificial Companions in Society: scientific, economic, psychological and philosophical perspectives.</title>
<date>2010</date>
<booktitle>Grundriss der Psychologie,</booktitle>
<publisher>John Benjamins: Amsterdam. Wundt, W.,</publisher>
<location>A. Kroner: Berlin.</location>
<contexts>
<context position="13532" citStr="Wilks, 2010" startWordPosition="2291" endWordPosition="2292">and delays. All of which suggests that a web-based Companion may have to use typed input in the immediate future—though using TTS output— 15 which is no problem for most mobile phone users, who have come to find typed chat perfectly natural. However, this is almost certainly only a transitory delay as mobile RAM increases rapidly and the problem should not determine research decisions---there is no doubt that voice will move back to the centre of communication once storage and access size have grown by another order of magnitude. iv) One Companion personality or several? Some (e.g. Pulman, in Wilks, 2010) have argued that having a consistent personality is a condition on Companionhood, but one could differ and argue that, although that is true of people— multiple personalities being a classic psychosis—there is no reason why we should expect this of a Companion. Perhaps a Companion should have a personality adapted to its particular relationship to a user at a given moment: Lowe (in Wilks, 2010) has pointed out that one might want a Companion to function as, say, a gym trainer, in which case a rather harsh attitude on the part of the Companion might well be the best one. If a Companion’s emoti</context>
<context position="19827" citStr="Wilks, 2010" startWordPosition="3391" endWordPosition="3392">rmation extraction technology (e.g. Ciravegna et al., 2004) allows a reasonable job to be made of going to Wikipedia for general information when, say, a world city is mentioned; the Companion can then glean something about that city from Wikipedia and ask a relevant question such as “Did you see the Eiffel Tower when you were in Paris?” which again gives a plausible illusion of general knowledge. A concrete Companion paradigm: the Victorian Companion The subsections above are mini-discussions of some of the constraints on what it is to be a Companion, the subject of a recent book collection (Wilks, 2010). The upshot of those discussions is that there are many dimensions of choice, even within an agreed definition of what a Companion is to be, and they will depend on the user’s tastes and needs above all. In the section that follows, I cut though the choices and make a semi-serious proposal for a model Companion, one based on a once well-known social stereotype. More seriously, and in the spirit of a priori thoughts (and what else can we have at this technological stage of development?) about what a Companion should be, I would suggest we could profitably spend a few moments reminding ourselve</context>
</contexts>
<marker>Wilks, 2010</marker>
<rawString>Wilks, Y. (ed.) (2010) Artificial Companions in Society: scientific, economic, psychological and philosophical perspectives. John Benjamins: Amsterdam. Wundt, W., 1913. Grundriss der Psychologie, A. Kroner: Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Zue</author>
<author>J Glass</author>
<author>D Goddeau</author>
<author>D Goodine</author>
<author>L Hirschman</author>
</authors>
<title>The MIT ATIS system,</title>
<date>1992</date>
<booktitle>In Proc. Workshop on speech and natural language,</booktitle>
<location>Harriman, New York.</location>
<contexts>
<context position="1310" citStr="Zue et al., 1992" startWordPosition="208" endWordPosition="211">nt? Is an attempt at emotional sympathy important or achievable? Does a user want a consistent personality in a Companion or a variety of personalities? Should we be talking more in terms of a &amp;quot;cognitive prosthesis (or orthosis)?&amp;quot; ---something to extract, organize, and locate the user&apos;s knowledge or personal information---rather than attitudes? 1. Introduction It is convenient to distinguish Companions from both (a) conversational internet agents that carry out specific tasks, such as the train and plane scheduling and ticket ordering speech dialogue applications back to the MIT ATIS systems (Zue et al., 1992), and also from (b) descendants of the early chatbots PARRY and ELIZA, the best of which compete annually in the Loebner competition (Loebner). These have essentially no memory or knowledge but are simple finite state response sets, although ELIZA had primitive “scripts” giving some context, and PARRY (Colby, 1971) had parameters like FEAR and ANGER that changed with the conversation and determined which reply was selected at a given point. I take plausible distinguishing features of a Companion agent to be: 1) that it has no central or over-riding task and there is no point at which its conve</context>
</contexts>
<marker>Zue, Glass, Goddeau, Goodine, Hirschman, 1992</marker>
<rawString>Zue, V., Glass, J., Goddeau, D., Goodine, D., Hirschman, L. 1992. The MIT ATIS system, In Proc. Workshop on speech and natural language, Harriman, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>