<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.995003">
Group-based Generation of Referring Expressions
</title>
<author confidence="0.998705">
Funakoshi Kotaro ∗ Watanabe Satoru † Tokunaga Takenobu
</author>
<affiliation confidence="0.7732385">
Department of Computer Science, Tokyo Institute of Technology
Tokyo Meguro ˆOokayama 2-12-1, 152-8552, Japan
</affiliation>
<email confidence="0.998704">
take@cl.cs.titech.ac.jp
</email>
<sectionHeader confidence="0.997385" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999873642857143">
Past work of generating referring expres-
sions mainly utilized attributes of objects
and binary relations between objects in or-
der to distinguish the target object from
others. However, such an approach does
not work well when there is no distinc-
tive attribute among objects. To over-
come this limitation, this paper proposes
a novel generation method utilizing per-
ceptual groups of objects and n-ary re-
lations among them. The evaluation us-
ing 18 subjects showed that the proposed
method could effectively generate proper
referring expressions.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997203764705882">
In the last two decades, many researchers have
studied the generation of referring expressions to
enable computers to communicate with humans
about objects in the world.
In order to refer to an intended object (the tar-
get) among others (distractors), most past work
(Appelt, 1985; Dale and Haddock, 1991; Dale,
1992; Dale and Reiter, 1995; Heeman and Hirst,
1995; Horacek,1997; Krahmer and Theune, 2002;
van Deemter, 2002; Krahmer et al., 2003) utilized
attributes of the target and binary relations be-
tween the target and distractors. Therefore, these
methods cannot generate proper referring expres-
sions in situations where there is no signiÞcant
surface difference between the target and distrac-
tors, and no binary relation is useful to distinguish
the target. Here, a proper referring expression
</bodyText>
<footnote confidence="0.463879">
∗Currently at Honda Research Institute Japan Co., Ltd.
†Currently at Hitachi, Ltd.
</footnote>
<bodyText confidence="0.9994786">
means a concise and natural linguistic expression
enabling hearers to identify the target.
For example, consider indicating object b to per-
son P in the situation of Figure 1. Note that la-
bels a, b and c are assigned for explanation to the
readers, and person P does not share these labels
with the speaker. Because object b is not distin-
guishable from objects a or c by means of their
appearance, one would try to use a binary relation
between object b and the table, i.e., “a ball to the
right of the table”. However, “to the right of” is
not a discriminatory relation, for objects a and c
are also located to the right of the table. Using a
and c as a reference object instead of the table does
not make sense, since a and c cannot be uniquely
identiÞed because of the same reason that b cannot
be identiÞed. Such situations have drawn less at-
tention (Stone, 2000), but can frequently occur in
some domains such as object arrangement (Tanaka
et al., 2004).
</bodyText>
<figureCaption confidence="0.997241">
Figure 1: An example of problematic situations
</figureCaption>
<bodyText confidence="0.999619166666667">
In the situation of Figure 1, the speaker can indi-
cate object b to person P with a simple expression
“the front ball”. In order to generate such an ex-
pression, one must be able to recognize the salient
perceptual group of the objects and use the n-ary
relative relations in the group.
</bodyText>
<figure confidence="0.99668975">
a
c
b
P
</figure>
<page confidence="0.991487">
73
</page>
<note confidence="0.6886085">
Proceedings of the Fourth International Natural Language Generation Conference, pages 73–80,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999132">
To overcome the problem described above, Fu-
nakoshi et al. (2004) proposed a method of gen-
erating Japanese referring expressions that utilizes
n-ary relations among members of a group. They,
however, dealt with the limited situations where
only homogeneous objects are randomly arranged
(see Figure 2). Thus, their method could han-
dle only spatial n-ary relation, and could not han-
dle attributes and binary relations between objects
which have been the main concern of the past re-
search.
In this paper, we extend the generation method
proposed by (Funakoshi et al., 2004) so as to han-
dle object attributes and binary relations between
objects as well. In what follows, Section 2 shows
an extension of the SOG representation that was
proposed in (Funakoshi et al., 2004). Our new
method will be described in Section 3 and eval-
uated in Section 4. Finally we conclude the paper
in Section 5.
</bodyText>
<sectionHeader confidence="0.992188" genericHeader="method">
2 SOG representation
</sectionHeader>
<bodyText confidence="0.999893428571429">
Funakoshi et al. (2004) proposed an intermedi-
ate representation between a referring expression
and the situation that is referred to by the expres-
sion. The intermediate representation represents
a course of narrowing down to the target as a se-
quence of groups from the group of all objects to
the singleton group of the target object. Thus it is
called SOG (Sequence Of Groups).
The following example shows an expression de-
scribing the target x in Figure 2 with the cor-
responding SOG representation below it. Since
Japanese is a head-Þnal language, the order of
groups in the SOG representation can be retained
in the linguistic expression.
</bodyText>
<equation confidence="0.9812122">
hidari oku ni aru(1) mittu no tama no uti no(2)
itiban migi no tama(3)
(the rightmost ball(3) among the three balls(2)
at the back left(1))
SOG:[{a, b, c, d, e, f, x}, {a, b, x}, {x}],
</equation>
<bodyText confidence="0.908313666666667">
where {a, b, c, d, e, f, x} denotes all objects in
the situation, {a, b, x} denotes the three objects
at the back left, and {x} denotes the target.
</bodyText>
<subsectionHeader confidence="0.956514">
2.1 Extended SOG
</subsectionHeader>
<bodyText confidence="0.9987704">
As mentioned above, (Funakoshi et al., 2004) sup-
posed the limited situations where only homoge-
neous objects are randomly arranged, and consid-
ered only spatial subsumption relations between
consecutive groups. Therefore, relations between
</bodyText>
<figureCaption confidence="0.632944">
Figure 2: An example from (Funakoshi et al.,
2004)
</figureCaption>
<bodyText confidence="0.9289721">
groups are not explicitly denoted in the original
SOGs as shown below.
SOG: [G0, G1, ... , Gn]
Gi: a group
In this paper, however, other types of relations
between groups are also considered. We propose
an extended SOG representation where types of
relations are explicitly denoted as shown below. In
the rest of this paper, we will refer to this extended
SOG representation by simply saying “SOG”.
</bodyText>
<listItem confidence="0.497435">
SOG: [G0R0G1R1 ... GiRi ... Gn]
Gi: a group
Ri: a relation between Gi and Gi+1
</listItem>
<subsectionHeader confidence="0.998834">
2.2 Relations between groups
</subsectionHeader>
<bodyText confidence="0.999953666666667">
Ri, a relation between groups Gi and Gi+1, de-
notes a shift of attention from Gi to Gi+1 with
a certain focused feature. The feature can be an
attribute of objects or a relation between objects.
There are two types of relations between groups:
intra-group relation and inter-group relation.
Intra-group relation When Ri is an intra-group
relation, Gi subsumes Gi+1, that is, Gi D Gi+1.
Intra-group relations are further classiÞed into the
following subcategories according to the feature
used to narrow down Gi to Gi+1. We denote these
subcategories with the following symbols.
</bodyText>
<equation confidence="0.8635404">
space
−→ : spatial subsumption
type
−→ : the object type
shape
</equation>
<listItem confidence="0.534708">
−→ : the shape of objects
color−→ : the color of objects
−→ : the size of objects
</listItem>
<subsubsectionHeader confidence="0.409737">
size
</subsubsectionHeader>
<bodyText confidence="0.846119333333333">
With respect to this classiÞcation, (Funakoshi et
al., 2004) dealt with only the space
−→ relation.
</bodyText>
<figure confidence="0.983218571428571">
P
x
c d
a
b
f
e
</figure>
<page confidence="0.996833">
74
</page>
<bodyText confidence="0.9650608">
Inter-group relation When Ri is an inter-group
relation, Gi and Gi+1 are mutually exclusive, that
is, Gi n Gi+1 = φ. An inter-group relation is a
spatial relation and denoted by symbol ⇒.
space type shape
</bodyText>
<equation confidence="0.7128458">
Example Ri can be one of
−→ ,
color
−→, size
−→ and ⇒. We show a referring expres-
</equation>
<bodyText confidence="0.999771714285714">
sion indicating object b1 and the corresponding
SOG in the situation of Figure 3. In the SOG,
{all} denotes the total set of objects in the situ-
ation. The indexed underlines denote correspon-
dence between SOG and linguistic expressions.
As shown in the figure, we allow objects being on
the other objects.
</bodyText>
<equation confidence="0.9535175">
marui(1) futatu no tukue no uti no(2)
hidari no(3) tukue no(4) ue no(5) tama(6)
(the ball(6) on(5) the left(3) table(4)
among the two(2) round(1) tables(2))
SOG: [{all} type
−→ {t1, t2, t3} shape
−→(1)
{t1, t2}(2)
</equation>
<figureCaption confidence="0.989552">
Figure 3: An example situation
</figureCaption>
<sectionHeader confidence="0.997807" genericHeader="method">
3 Generation
</sectionHeader>
<bodyText confidence="0.999794">
Our generation algorithm proposed in this section
consists of four steps: perceptual grouping, SOG
generation, surface realization and scoring. In the
rest of this section, we describe these four steps by
using Figure 3 as an example.
</bodyText>
<subsectionHeader confidence="0.998509">
3.1 Step 1: Perceptual grouping
</subsectionHeader>
<bodyText confidence="0.99965708">
Our algorithm starts with identifying groups of
objects that are naturally recognized by humans.
We adopt Th´orisson’s perceptual grouping algo-
rithm (Th´orisson, 1994) for this purpose. Per-
ceptual grouping is performed with objects in the
situation with respect to each of the following
features: type, shape, color, size, and proxim-
ity. Three special features, total, singleton, and
closure are respectively used to recognize the to-
tal set of objects, groups containing each single
object, and objects bounded in perceptually sig-
nificant regions (table tops in the domain of this
paper). These three features are handled not by
Th`orisson’s algorithm but by individual proce-
dures.
Type is the most dominant feature because hu-
mans rarely recognize objects of different types as
a group. Thus, first we group objects with respect
to types, and then group objects of the same type
with respect to other features (except for total).
Although we adopt Th´orisson’s grouping algo-
rithm, we use different grouping strategies from
the original. Th´orisson (1994) lists the following
three combinations of features as possible strate-
gies of perceptual grouping.
</bodyText>
<listItem confidence="0.999948">
• shape and proximity
• color and proximity
• size and proximity
</listItem>
<bodyText confidence="0.808401103448276">
However, these strategies are inappropriate to gen-
erate referring expressions. For example, because
two blue balls b1 and b� in Figure 3 are too
much distant from each other, Th´orisson’s algo-
rithm cannot recognize the group consisting of b1
and b� with the original strategies. However, the
expression like “the left blue ball” can naturally
refer to b1. When using such an expression, we
assume an implicit group consisting of b1 and b�.
Hence, we do not combine features but use them
separately.
The results of perceptual grouping of the situa-
tion in Figure 3 are shown below. Relation labels
are assigned to recognized groups with respect to
features used in perceptual grouping. We define
six labels: all, type, shape, color, size, and
space. Features singleton, proximity and closure
share the same label space. A group may have
several labels.
feature label recognized groups
total all {t1, t2, t3, p1, b1, b2, b3, b4, b5}
singleton space {t1}, {t2}, {t3}, {p1}, {b1}, {b2},
{b3}, {b4}, {b5}
type type {t1, t2, t3}, {p1}, {b1, b2, b3, b4, b5}
shape shape {t1, t2}, {t3}
color color {b1, b2}, {b3}, {b4, b5}
size size {b1, b3, b4}, {b2, b5}
proximity space {t2, t3}, {b1, b3,b4,b5}, {b3, b4,b5}
closure space {b1}, {b3, b4}
</bodyText>
<figure confidence="0.964122166666667">
blue
black
red
b4
p
space f f
−→(3) {t1}(4) ⇒(5) {b1}(6)]
75
Target # target object
AllGroups # all generated groups
SOGList # list of generated SOGs
01:makeSOG()
</figure>
<listItem confidence="0.91546075">
2: SOG = []; # list of groups and symbols
3: All = getAll(); # total set
4: add(All, SOG); # add All to SOG
5: TypeList = getAllTypes(All);
# list of all object types
6: TargetType = getType(Target);
# type of the target
7: TargetSailency = saliency(TargetType);
# saliency of the target type
8: for each Type in TypeList do
# {Table, Plant, Ball}
9: if saliency(Type) ≥
TargetSaliency then
# saliency: Table &gt; Plant &gt; Ball
10: Group = getTypeGroup(Type);
# get the type group of Type
11: extend(SOG, Group);
12: end if
13: end for
14:return
</listItem>
<figureCaption confidence="0.997575">
Figure 4: Function makeSOG
</figureCaption>
<subsectionHeader confidence="0.969473">
3.2 Step 2: SOG generation
</subsectionHeader>
<bodyText confidence="0.980922407407408">
The next step is generating SOGs. This is so-
called content planning in natural language gen-
eration. Figure 4, Figure 5 and Figure 6 show the
algorithm of making SOGs.
Three variables Target, AllGroups, and
SOGList defined in Figure 4 are global variables.
Target holds the target object which the refer-
ring expression refers to. AllGroups holds the
set of all groups recognized in Step 1. Given
Target and AllGroups, function makeSOG
enumerates possible SOGs in the depth-first man-
ner, and stores them in SOGList.
makeSOG (Figure 4) makeSOG starts with a list
(SOG) that contains the total set of objects in the
domain. It chooses groups of objects that are more
salient than or equal to the target object and calls
function extend for each of the groups.
extend (Figure 5) Given an SOG and a group
to be added to the SOG, function extend extends
the SOG with the group for each label attached to
the group. This extension is done by creating a
copy of the given SOG and adding to its end an
intra-group relation symbol defined in Section 2.2
corresponding to the given label and group. Fi-
nally it calls search with the copy.
search (Figure 6) This function takes an SOG
as its argument. According to the last group in
</bodyText>
<listItem confidence="0.8690441">
01:extend(SOG, Group)
2: Labels = getLabels(Group);
3: for each Label in Labels do
4: SOGcopy = copy(SOG);
5: add(L����
−→, SOGcopy);
6: add(Group, SOGcopy);
7: search(SOGcopy);
8: end for
09:return
</listItem>
<figureCaption confidence="0.939327">
Figure 5: Function extend
</figureCaption>
<listItem confidence="0.820709045454545">
the SOG (LastGroup), it extends the SOG as
described below.
1. If LastGroup is a singleton of the target
object, append SOG to SOGList and return.
2. If LastGroup is a singleton of a non-target
object, find the groups that contain the target
object and satisfy the following three condi-
tions: (a), (b) and (c).
(a) All objects in the group locate in
the same direction from the object of
LastGroup (the reference). Possi-
ble directions are one of “back”, “back
right”, “right”, “front right”, “front”,
“front left”, “left”, “left back” and “on”.
The direction is determined on the basis
of coordinate values of the objects, and
is assigned to the group for the use of
surface realization.
(b) There is no same type object located be-
tween the group and the reference.
(c) The group is not a total set of a certain
type of object.
</listItem>
<bodyText confidence="0.991779090909091">
Then, for each of the groups, make a copy
of the SOG, and concatenate “⇒” and the
group to the copy, and call search recur-
sively with the new SOG.
3. If LastGroup contains the target object
together with other objects, let the inter-
section of LastGroup and each group in
AllGroups be NewG, and copy the label
from each group to NewG. If NewG contains
the target object, call function extend un-
less Checked contains NewG.
</bodyText>
<listItem confidence="0.9773915">
4. If LastGroup contains only non-target ob-
jects, call function extend for each group
(Group) in AllGroups which is subsumed
by LastGroup.
</listItem>
<figureCaption confidence="0.9913025">
Figure 7 shows the SOGs generated to refer to
object bl in Figure 3.
</figureCaption>
<figure confidence="0.5262065">
76
1. [{all} type −→{t1, t2, t3} space
−→ {t1} ⇒{b1}]
2. [{all} type
</figure>
<equation confidence="0.96048971875">
−→ {t1, t2, t3} shape
−→ {t1, t2} space
−→ {t1} ⇒{b1}]
3. [{all} type −→{b1, b2, b3, b4, b5} space −→{b1}]
4. [{all} type
−→ {b1, b2, b3, b4, b5} color
−→ {b1, b2} space
−→ {b1}]
5. [{all} type
−→ {b1, b2, b3, b4, b5} color
−→ {b1, b2} size
−→ {b1}]
6. [{all} type
−→ {b1, b2, b3, b4, b5}size
−→ {b1, b4, b3} space −→{b1}]
7. [{all} type −→{b1, b2, b3, b4, b5}size
−→ {b1, b4, b3} color
−→ {b1}]
8. [{all} type −→{b1, b2, b3, b4, b5} space
−→ {b1, b3, b4, b5} space
−→ {b1}]
9. [{all} type
−→ {b1, b2, b3, b4, b5} space
−→ {b1, b3, b4, b5} color
−→ {b1}]
10. [{all} type −→{b1, b2, b3, b4, b5} space
−→ {b1, b3, b4, b5} size
−→ {b1, b3, b4} space −→{b1}]
11. [{all} type −→{b1, b2, b3, b4, b5} space
−→ {b1, b3, b4, b5}size
−→ {b1, b3, b4} color
−→ {b1}]
</equation>
<figureCaption confidence="0.9678">
Figure 7: Generated SOGs from the situation in Figure 3
</figureCaption>
<equation confidence="0.427819">
01:search(SOG)
</equation>
<listItem confidence="0.978529463414634">
2: LastGroup = getLastElement(SOG);
# get the rightmost group in SOG
3: Card = getCardinality(LastGroup);
4: if Card == 1 then
5: if containsTarget(LastGroup) then
# check if LastGroup contains
# the target
6: add(SOG, SOGList);
7: else
8: GroupList =
searchTargetGroups(LastGroup);
# find groups containing the target
9: for each Group in GroupList do
10: SOGcopy = copy(SOG);
11: add(⇒, SOGcopy);
12: add(Group, SOGcopy);
13: search(SOGcopy);
14: end for
15: end if
16: elsif containsTarget(LastGroup) then
17: Checked = [ ];
18: for each Group in AllGroups do
19: NewG = Intersect(Group, LastGroup);
# make intersection
20: Labels = getLabels(Group);
21: setLabels(Labels, NewG);
# copy labels from Group to NewG
22: if containsTarget(NewG) &amp;
!contains(Checked, NewG) then
23: add(NewG, Checked);
24: extend(SOG, Group);
25: end if
26: end for
27: else
28: for each Group of AllGroups do
29: if contains(LastGroup, Group) then
30: extend(SOG, Group);
31: end if
32: end for
33: end if
34:return
</listItem>
<figureCaption confidence="0.992781">
Figure 6: Function search
</figureCaption>
<subsectionHeader confidence="0.995025">
3.3 Step 3: Surface realization
</subsectionHeader>
<bodyText confidence="0.999489444444445">
A referring expression is generated by determin-
istically assigning a linguistic expression to each
element in an SOG according to Rule 1 and 2.
As Japanese is a head-Þnal language, simple con-
catenation of element expressions makes a well-
formed noun phrase1. Rule 1 generates expres-
sions for groups and Rule 2 does for relations.
Each rule consists of several subrules which are
applied in this order.
</bodyText>
<subsectionHeader confidence="0.215228">
[Rule 1]: Realization of groups
</subsectionHeader>
<bodyText confidence="0.750373">
Rule 1.1 The total set ({all}) is not realized.
(Funakoshi et al., 2004) collected referring
expressions from human subjects through ex-
periments and found that humans rarely men-
tioned the total set. According to their obser-
vation, we do not realize the total set.
</bodyText>
<subsubsectionHeader confidence="0.55355">
Rule 1.2 Realize the type name for a singleton.
</subsubsectionHeader>
<bodyText confidence="0.759355866666667">
Type is realized as a noun and only for a sin-
gleton because the type feature is used Þrst to
narrow down the group, and the succeeding
groups consist of the same type objects until
reaching the singleton. When the singleton is
not the last element of SOG, particle “no” is
added.
Rule 1.3 The total set of the same type objects is
not realized.
This is because the same reason as Rule 1.1.
Rule 1.4 The group followed by the relation space
−→
is realized as “[cardinality] [type] no-uti
(among)”, e.g., “futatu-no (two) tukue (desk)
no-uti (among)”. The group followed by
</bodyText>
<footnote confidence="0.70880875">
1Although different languages require different surface
realization rules, we presume perceptual grouping and SOG
generation (Step 1 and 2) are applicable to other languages as
well.
</footnote>
<page confidence="0.996495">
77
</page>
<figure confidence="0.670766">
the relation ⇒ is realized as “[cardinality]
[type] no”.
When consecutive groups are connected by
space
</figure>
<bodyText confidence="0.9996342">
other than spatial relations ( −→ and ⇒),
they can be realized as a sequence of relations
ahead of the noun (type name). For example,
expression “the red ball among big balls” can
be simplified to “the big red ball”.
</bodyText>
<figure confidence="0.9562665">
Rule 1.5 Other groups are not realized.
[Rule 2]: Realization of relations
Rule 2.1 Relation −→is not realized.
type
See Rule 1.2. hape lor
s co
ule 2.2 Relations −→ , −→ and size −→ are real-
R
</figure>
<bodyText confidence="0.507152">
ized as the expressions corresponding to their
</bodyText>
<listItem confidence="0.6232875">
attribute values. Spatial relations (space
−→ and
⇒) are realized as follows, where |Gi |de-
notes the cardinality of Gi.
</listItem>
<subsubsectionHeader confidence="0.442748">
space
</subsubsectionHeader>
<bodyText confidence="0.994157763157895">
Intra-group relation (Gi −→ Gi+1)
If |Gi |= 2 (i.e., |Gi+1 |= 1), based on the
geometric relations among objects, generate
one of four directional expressions “{migi,
hidari, temae, oku} no ({right, left, front,
back})”.
If |Gi |≥ 3 and |Gi+1 |= 1, based on the
geometric relations among objects, generate
one of eight directional expressions “itiban
{migi, hidari, temae, oku, migi temae, hi-
dari temae, migi oku, hidari oku} no ({right,
left, front, back, front right, front left, back
right, back left}-most)” if applicable. If none
of these expressions is applicable, generate
expression “mannaka no (middle)” if appli-
cable. Otherwise, generate one of four ex-
pressions “{hidari, migi, temae, oku} kara
j-banme no (j-th from {left, right, front,
back})”.
If |Gi+1 |≥ 2, based on the geometric rela-
tions among objects, generate one of eight di-
rectional expressions “{migi, hidari, temae,
oku, migi temae, hidari temae, migi oku, hi-
dari oku} no ({right, left, front, back, front
right, front left, back right, back left})”.
Inter-group relation (Gi ⇒Gi+1)
|Gi |= 1 should hold because of search
in Step 2. According to the direction as-
signed by search, generate one of nine ex-
pressions : “{migi, hidari, temae, oku, migi
temae, hidari temaen, migi oku, hidari oku,
ue} no ({right, left, front, back, front right,
front left, back right, back left, on})”.
Figure 8 shows the expressions generated from
the first three SOGs shown in Figure 7. The num-
bers in the parentheses denote coindexes of frag-
ments between the SOGs and the realized expres-
sions.
</bodyText>
<subsectionHeader confidence="0.989851">
3.4 Step 4: Scoring
</subsectionHeader>
<bodyText confidence="0.99954125">
We assign a score to each expression by taking into
account the relations used in the expression, and
the length of the expression.
First we assign a cost ranging over [0, 1] to each
relation in the given SOG. Costs of relations are
decided as below. These costs conform to the pri-
orities of features described in (Dale and Reiter,
1995).
</bodyText>
<equation confidence="0.978989555555556">
type
−→ : No cost (to be neglected)
shape
−→ : 0.2
color
−→ : 0.4
size
−→ :big(est): 0.6, small(est): 0.8, middle: 1.0
space
</equation>
<bodyText confidence="0.899366076923077">
−→,⇒ : Cost functions are defined according to the
potential functions proposed in (Tokunaga
et al., 2005). The cost for relation “on” is
fixed to 0.
Then, the average cost of the relations is calcu-
lated to obtain the relation cost, Crel. The cost of
surface length (Clen) is calculated by
Clen =maxi length(expressioni),
length(expression)
where the length of an expression is measured by
the number of characters.
Using these costs, the score of an expression is
calculated by
</bodyText>
<equation confidence="0.484763">
1
score = α × Crel + (1 − α) × Clen .
</equation>
<bodyText confidence="0.979514">
α was set to 0.5 in the following experiments.
</bodyText>
<sectionHeader confidence="0.999563" genericHeader="method">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.640033">
4.1 Experiments
</subsectionHeader>
<bodyText confidence="0.999994083333333">
We conducted two experiments to evaluate expres-
sions generated by the proposed method.
Both experiments used the same 18 subjects and
the same 20 object arrangements which were gen-
erated automatically. For each arrangement, all
factors (number of objects, positions of objects, at-
tributes of objects, and the target object) were ran-
domly decided in advance to conform to the fol-
lowing conditions: (1) the proposed method can
generate more than five expressions for the given
target and (2) more than two other objects exist
which are the same type as the target.
</bodyText>
<page confidence="0.971868">
78
</page>
<listItem confidence="0.3693">
1. SOG: [{all} type
</listItem>
<equation confidence="0.97100025">
−→ {t1, t2,t3} space
−→(1) {t1}(2) ⇒(3) {b1}(4)]
itiban hidari no(1) tukue no(2) ue no(3) tama(4) (the ball(4) on(3) the leftmost(1) table(2))
2. SOG: [{all} type
−→ {t1, t2, t3} shy (1) {t1, t2 9 tl bl
}c2� c3� { }c4� c5� { }(6)]
marui(1) futatu no tukue no uti(2) hidari no(3) tukue no(4) ue no(5) tama(6)
(the ball(6) on(5) the left(3) table(4) among(2) the round(1) two tables(2))
3. SOG: [{all} type
−→ {b1, b2, b3, b4, b5} space
−→(1) {b1}(2)]
itiban hidari no(1) tama(2) (the leftmost(1) ball(2))
</equation>
<figureCaption confidence="0.999382">
Figure 8: Realized expressions
</figureCaption>
<figure confidence="0.9992765">
b3
b1
b2 t1 p1
「一番手前の玉」
第20問
b4
t2
20/20
</figure>
<figureCaption confidence="0.999877">
Figure 9: An example stimulus of Experiment 1
</figureCaption>
<bodyText confidence="0.9972506">
Experiment 1 Experiment 1 was designed to
evaluate the ability of expressions to identify the
targets. The subjects were presented an arrange-
ment with a generated referring expression which
gained the highest score at a time, and were in-
structed to choose the object referred to by the ex-
pression. Figure 9 is an example of visual stimuli
used in Experiment 1. Each subject responded to
all 20 arrangements.
Experiment 2 Experiment 2 was designed to
evaluate validity of the scoring function described
in Section 3.4. The subjects were presented an
arrangement with a marked target together with
the best five generated expressions referring to the
target at a time. Then the subjects were asked
to choose the best one from the five expressions.
Figure 10 is an example of visual stimuli used in
Experiment 2. Each subject responded to the all
20 arrangements. The expressions used in Experi-
ment 2 include those used in Experiment 1.
</bodyText>
<sectionHeader confidence="0.91543" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.981026739130435">
Table 1 shows the results of Experiment 1. The
average accuracy of target identification is 95%.
Figure 10: An example stimulus of Experiment 2
This shows a good performance of the generation
algorithm proposed in this paper.
The expression generated for arrangement
No. 20 (shown in Figure 9) resulted in the excep-
tionally poor accuracy. To refer to object bl, our
algorithm generated expression “itiban temae no
tama (the most front ball)” because bl is the most
close object to person P in terms of the vertical
axis. Humans, however, chose the object that is the
closest to P in terms of Euclidean distance. Some
psychological investigation is necessary to build
a more precise geometrical calculation model to
solve this problem (Landragin et al., 2001).
Table 2 shows the results of Experiment 2. The
first row shows the rank of expressions based on
their score. The second row shows the count of hu-
man votes for the expression. The third row shows
the ratio of the votes. The top two expressions oc-
cupy 72% of the total. This concludes that our
scoring function works well.
</bodyText>
<sectionHeader confidence="0.99954" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9953595">
This paper extended the SOG representation pro-
posed in (Funakoshi et al., 2004) to generate refer-
</bodyText>
<page confidence="0.999111">
79
</page>
<tableCaption confidence="0.999424">
Table 1: Result of Experiment 1
</tableCaption>
<table confidence="0.999922">
Arrangement No. 1 2 3 4 5 6 7 8 9 10
Accuracy 0.89 1.0 1.0 1.0 1.0 1.0 1.0 0.94 1.0 1.0
11 12 13 14 15 16 17 18 19 20 Ave.
1.0 0.94 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.17 0.95
</table>
<tableCaption confidence="0.972198">
Table 2: Result of Experiment 2
</tableCaption>
<table confidence="0.646127333333333">
Rank 1 2 3 4 5 Total
Vote 134 125 59 22 20 360
Share 0.37 0.35 0.16 0.06 0.06 1
</table>
<bodyText confidence="0.998571483870968">
ring expressions in more general situations.
The proposed method was implemented and
evaluated through two psychological experiments
using 18 subjects. The experiments showed that
generated expressions had enough discrimination
ability and that the scoring function conforms to
human preference well.
The proposed method would be able to handle
other attributes and relations as far as they can be
represented in terms of features as described in
section 3. Corresponding surface realization rules
might be added in that case.
In the implementation, we introduced rather ad
hoc parameters, particularly in the scoring func-
tion. Although this worked well in our experi-
ments, further psychological validation is indis-
pensable.
This paper assumed a Þxed reference frame is
shared by all participants in a situation. How-
ever, when we apply our method to conversational
agent systems, e.g., (Tanaka et al., 2004), refer-
ence frames change dynamically and they must
be properly determined each time when generat-
ing referring expressions.
In this paper, we focused on two dimensional
situations. To apply our method to three dimen-
sional worlds, more investigation on human per-
ception of spatial relations are required. We ac-
knowledge that a simple application of the current
method does not work well enough in three dimen-
sional worlds.
</bodyText>
<sectionHeader confidence="0.999646" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999943169491526">
Douglas E. Appelt. 1985. Planning English referring expres-
sions. ArtiÞcial Intelligence, 26:1–33.
Robert Dale and Nicholas Haddock. 1991. Generating re-
ferring expressions involving relations. In Proceedings of
the Fifth Conference of the European Chapter of the As-
sociation for Computational Linguistics(EACL’91), pages
161–166.
Robert Dale and Ehud Reiter. 1995. Computational interpre-
tations of the Gricean maxims in the generation of refer-
ring expressions. Cognitive Science, 19(2):233–263.
Robert Dale. 1992. Generating referring expressions: Con-
structing descriptions in a domain of objects and pro-
cesses. MIT Press, Cambridge.
Kotaro Funakoshi, Satoru Watanabe, Naoko Kuriyama, and
Takenobu Tokunaga. 2004. Generating referring expres-
sions using perceptual groups. In Proceedings of the 3rd
International Conference on Natural Language Genera-
tion: INLG04, pages 51–60.
Peter Heeman and Graeme Hirst. 1995. Collaborating refer-
ring expressions. Computational Linguistics, 21(3):351–
382.
Helmut Horacek. 1997. An algorithm for generating refer-
ential descriptions with ßexible interfaces. In Proceedings
of the 35th Annual Meeting of the Association for Compu-
tational Linguistics, pages 206–213.
Emiel Krahmer and Mari¨et Theune. 2002. EfÞcient context-
sensitive generation of descriptions. In Kees van Deemter
and Rodger Kibble, editors, Information Sharing: Given-
ness and Newness in Language Processing. CSLI Publica-
tions, Stanford, California.
Emiel Krahmer, Sebastiaan van Erk, and Andr´e Verleg. 2003.
Graph-based generation of referring expressions. Compu-
tational Linguistics, 29(1):53–72.
Fr´ed´eric Landragin, Nadia Bellalem, and Laurent Romary.
2001. Visual salience and perceptual grouping in mul-
timodal interactivity. In Proceedings of International
Workshop on Information Presentation and Natural Mul-
timodal Dialogue (IPNMD), pages 151–155.
Matthew Stone. 2000. On identifying sets. In Proceedings
of the 1st International Conference on Natural Language
Generation: INLG00, pages 116–123.
Hozumi Tanaka, Takenobu Tokunaga, and Yusuke Shinyama.
2004. Animated agents capable of understanding natural
language and performing actions. In Helmut Prendinger
and Mituru Ishizuka, editors, Life-Like Characters, pages
429–444. Springer.
Kristinn R. Th´orisson. 1994. Simulated perceptual grouping:
An application to human-computer interaction. In Pro-
ceedings of the Sixteenth Annual Conference of the Cog-
nitive Science Society, pages 876–881.
Takenobu Tokunaga, Tomofumi Koyama, and Suguru Saito.
2005. Meaning of Japanese spatial nouns. In Proceedings
of the Second ACL-SIGSEM Workshop on the Linguistic
Dimentions of Prepositions and their Use in Computa-
tional Linguistics: Formalisms and Applications, pages
93–100.
Kees van Deemter. 2002. Generating referring expressions:
Boolean extensions of the incremental algorithm. Compu-
tational Linguistics, 28(1):37–52.
</reference>
<page confidence="0.998248">
80
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.690815">
<title confidence="0.999847">Group-based Generation of Referring Expressions</title>
<author confidence="0.996538">Kotaro Satoru Takenobu</author>
<affiliation confidence="0.990357">Department of Computer Science, Tokyo Institute of</affiliation>
<address confidence="0.722767">Meguro 2-12-1, 152-8552,</address>
<email confidence="0.852796">take@cl.cs.titech.ac.jp</email>
<abstract confidence="0.999273133333333">Past work of generating referring expressions mainly utilized attributes of objects and binary relations between objects in order to distinguish the target object from others. However, such an approach does not work well when there is no distinctive attribute among objects. To overcome this limitation, this paper proposes a novel generation method utilizing pergroups of objects and relations among them. The evaluation using 18 subjects showed that the proposed method could effectively generate proper referring expressions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Douglas E Appelt</author>
</authors>
<title>Planning English referring expressions.</title>
<date>1985</date>
<journal>ArtiÞcial Intelligence,</journal>
<pages>26--1</pages>
<contexts>
<context position="1082" citStr="Appelt, 1985" startWordPosition="159" endWordPosition="160"> there is no distinctive attribute among objects. To overcome this limitation, this paper proposes a novel generation method utilizing perceptual groups of objects and n-ary relations among them. The evaluation using 18 subjects showed that the proposed method could effectively generate proper referring expressions. 1 Introduction In the last two decades, many researchers have studied the generation of referring expressions to enable computers to communicate with humans about objects in the world. In order to refer to an intended object (the target) among others (distractors), most past work (Appelt, 1985; Dale and Haddock, 1991; Dale, 1992; Dale and Reiter, 1995; Heeman and Hirst, 1995; Horacek,1997; Krahmer and Theune, 2002; van Deemter, 2002; Krahmer et al., 2003) utilized attributes of the target and binary relations between the target and distractors. Therefore, these methods cannot generate proper referring expressions in situations where there is no signiÞcant surface difference between the target and distractors, and no binary relation is useful to distinguish the target. Here, a proper referring expression ∗Currently at Honda Research Institute Japan Co., Ltd. †Currently at Hitachi, L</context>
</contexts>
<marker>Appelt, 1985</marker>
<rawString>Douglas E. Appelt. 1985. Planning English referring expressions. ArtiÞcial Intelligence, 26:1–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Nicholas Haddock</author>
</authors>
<title>Generating referring expressions involving relations.</title>
<date>1991</date>
<booktitle>In Proceedings of the Fifth Conference of the European Chapter of the Association for Computational Linguistics(EACL’91),</booktitle>
<pages>161--166</pages>
<contexts>
<context position="1106" citStr="Dale and Haddock, 1991" startWordPosition="161" endWordPosition="164">istinctive attribute among objects. To overcome this limitation, this paper proposes a novel generation method utilizing perceptual groups of objects and n-ary relations among them. The evaluation using 18 subjects showed that the proposed method could effectively generate proper referring expressions. 1 Introduction In the last two decades, many researchers have studied the generation of referring expressions to enable computers to communicate with humans about objects in the world. In order to refer to an intended object (the target) among others (distractors), most past work (Appelt, 1985; Dale and Haddock, 1991; Dale, 1992; Dale and Reiter, 1995; Heeman and Hirst, 1995; Horacek,1997; Krahmer and Theune, 2002; van Deemter, 2002; Krahmer et al., 2003) utilized attributes of the target and binary relations between the target and distractors. Therefore, these methods cannot generate proper referring expressions in situations where there is no signiÞcant surface difference between the target and distractors, and no binary relation is useful to distinguish the target. Here, a proper referring expression ∗Currently at Honda Research Institute Japan Co., Ltd. †Currently at Hitachi, Ltd. means a concise and </context>
</contexts>
<marker>Dale, Haddock, 1991</marker>
<rawString>Robert Dale and Nicholas Haddock. 1991. Generating referring expressions involving relations. In Proceedings of the Fifth Conference of the European Chapter of the Association for Computational Linguistics(EACL’91), pages 161–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ehud Reiter</author>
</authors>
<title>Computational interpretations of the Gricean maxims in the generation of referring expressions.</title>
<date>1995</date>
<journal>Cognitive Science,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1141" citStr="Dale and Reiter, 1995" startWordPosition="167" endWordPosition="170">To overcome this limitation, this paper proposes a novel generation method utilizing perceptual groups of objects and n-ary relations among them. The evaluation using 18 subjects showed that the proposed method could effectively generate proper referring expressions. 1 Introduction In the last two decades, many researchers have studied the generation of referring expressions to enable computers to communicate with humans about objects in the world. In order to refer to an intended object (the target) among others (distractors), most past work (Appelt, 1985; Dale and Haddock, 1991; Dale, 1992; Dale and Reiter, 1995; Heeman and Hirst, 1995; Horacek,1997; Krahmer and Theune, 2002; van Deemter, 2002; Krahmer et al., 2003) utilized attributes of the target and binary relations between the target and distractors. Therefore, these methods cannot generate proper referring expressions in situations where there is no signiÞcant surface difference between the target and distractors, and no binary relation is useful to distinguish the target. Here, a proper referring expression ∗Currently at Honda Research Institute Japan Co., Ltd. †Currently at Hitachi, Ltd. means a concise and natural linguistic expression enabl</context>
<context position="19962" citStr="Dale and Reiter, 1995" startWordPosition="3403" endWordPosition="3406">t, front, back, front right, front left, back right, back left, on})”. Figure 8 shows the expressions generated from the first three SOGs shown in Figure 7. The numbers in the parentheses denote coindexes of fragments between the SOGs and the realized expressions. 3.4 Step 4: Scoring We assign a score to each expression by taking into account the relations used in the expression, and the length of the expression. First we assign a cost ranging over [0, 1] to each relation in the given SOG. Costs of relations are decided as below. These costs conform to the priorities of features described in (Dale and Reiter, 1995). type −→ : No cost (to be neglected) shape −→ : 0.2 color −→ : 0.4 size −→ :big(est): 0.6, small(est): 0.8, middle: 1.0 space −→,⇒ : Cost functions are defined according to the potential functions proposed in (Tokunaga et al., 2005). The cost for relation “on” is fixed to 0. Then, the average cost of the relations is calculated to obtain the relation cost, Crel. The cost of surface length (Clen) is calculated by Clen =maxi length(expressioni), length(expression) where the length of an expression is measured by the number of characters. Using these costs, the score of an expression is calculat</context>
</contexts>
<marker>Dale, Reiter, 1995</marker>
<rawString>Robert Dale and Ehud Reiter. 1995. Computational interpretations of the Gricean maxims in the generation of referring expressions. Cognitive Science, 19(2):233–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
</authors>
<title>Generating referring expressions: Constructing descriptions in a domain of objects and processes.</title>
<date>1992</date>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="1118" citStr="Dale, 1992" startWordPosition="165" endWordPosition="166">ng objects. To overcome this limitation, this paper proposes a novel generation method utilizing perceptual groups of objects and n-ary relations among them. The evaluation using 18 subjects showed that the proposed method could effectively generate proper referring expressions. 1 Introduction In the last two decades, many researchers have studied the generation of referring expressions to enable computers to communicate with humans about objects in the world. In order to refer to an intended object (the target) among others (distractors), most past work (Appelt, 1985; Dale and Haddock, 1991; Dale, 1992; Dale and Reiter, 1995; Heeman and Hirst, 1995; Horacek,1997; Krahmer and Theune, 2002; van Deemter, 2002; Krahmer et al., 2003) utilized attributes of the target and binary relations between the target and distractors. Therefore, these methods cannot generate proper referring expressions in situations where there is no signiÞcant surface difference between the target and distractors, and no binary relation is useful to distinguish the target. Here, a proper referring expression ∗Currently at Honda Research Institute Japan Co., Ltd. †Currently at Hitachi, Ltd. means a concise and natural ling</context>
</contexts>
<marker>Dale, 1992</marker>
<rawString>Robert Dale. 1992. Generating referring expressions: Constructing descriptions in a domain of objects and processes. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kotaro Funakoshi</author>
<author>Satoru Watanabe</author>
<author>Naoko Kuriyama</author>
<author>Takenobu Tokunaga</author>
</authors>
<title>Generating referring expressions using perceptual groups.</title>
<date>2004</date>
<booktitle>In Proceedings of the 3rd International Conference on Natural Language Generation: INLG04,</booktitle>
<pages>51--60</pages>
<contexts>
<context position="3213" citStr="Funakoshi et al. (2004)" startWordPosition="519" endWordPosition="523">ains such as object arrangement (Tanaka et al., 2004). Figure 1: An example of problematic situations In the situation of Figure 1, the speaker can indicate object b to person P with a simple expression “the front ball”. In order to generate such an expression, one must be able to recognize the salient perceptual group of the objects and use the n-ary relative relations in the group. a c b P 73 Proceedings of the Fourth International Natural Language Generation Conference, pages 73–80, Sydney, July 2006. c�2006 Association for Computational Linguistics To overcome the problem described above, Funakoshi et al. (2004) proposed a method of generating Japanese referring expressions that utilizes n-ary relations among members of a group. They, however, dealt with the limited situations where only homogeneous objects are randomly arranged (see Figure 2). Thus, their method could handle only spatial n-ary relation, and could not handle attributes and binary relations between objects which have been the main concern of the past research. In this paper, we extend the generation method proposed by (Funakoshi et al., 2004) so as to handle object attributes and binary relations between objects as well. In what follo</context>
<context position="5093" citStr="Funakoshi et al., 2004" startWordPosition="844" endWordPosition="847">pression describing the target x in Figure 2 with the corresponding SOG representation below it. Since Japanese is a head-Þnal language, the order of groups in the SOG representation can be retained in the linguistic expression. hidari oku ni aru(1) mittu no tama no uti no(2) itiban migi no tama(3) (the rightmost ball(3) among the three balls(2) at the back left(1)) SOG:[{a, b, c, d, e, f, x}, {a, b, x}, {x}], where {a, b, c, d, e, f, x} denotes all objects in the situation, {a, b, x} denotes the three objects at the back left, and {x} denotes the target. 2.1 Extended SOG As mentioned above, (Funakoshi et al., 2004) supposed the limited situations where only homogeneous objects are randomly arranged, and considered only spatial subsumption relations between consecutive groups. Therefore, relations between Figure 2: An example from (Funakoshi et al., 2004) groups are not explicitly denoted in the original SOGs as shown below. SOG: [G0, G1, ... , Gn] Gi: a group In this paper, however, other types of relations between groups are also considered. We propose an extended SOG representation where types of relations are explicitly denoted as shown below. In the rest of this paper, we will refer to this extended</context>
<context position="6632" citStr="Funakoshi et al., 2004" startWordPosition="1100" endWordPosition="1103">r a relation between objects. There are two types of relations between groups: intra-group relation and inter-group relation. Intra-group relation When Ri is an intra-group relation, Gi subsumes Gi+1, that is, Gi D Gi+1. Intra-group relations are further classiÞed into the following subcategories according to the feature used to narrow down Gi to Gi+1. We denote these subcategories with the following symbols. space −→ : spatial subsumption type −→ : the object type shape −→ : the shape of objects color−→ : the color of objects −→ : the size of objects size With respect to this classiÞcation, (Funakoshi et al., 2004) dealt with only the space −→ relation. P x c d a b f e 74 Inter-group relation When Ri is an inter-group relation, Gi and Gi+1 are mutually exclusive, that is, Gi n Gi+1 = φ. An inter-group relation is a spatial relation and denoted by symbol ⇒. space type shape Example Ri can be one of −→ , color −→, size −→ and ⇒. We show a referring expression indicating object b1 and the corresponding SOG in the situation of Figure 3. In the SOG, {all} denotes the total set of objects in the situation. The indexed underlines denote correspondence between SOG and linguistic expressions. As shown in the fig</context>
<context position="16371" citStr="Funakoshi et al., 2004" startWordPosition="2791" endWordPosition="2794">nd(SOG, Group); 31: end if 32: end for 33: end if 34:return Figure 6: Function search 3.3 Step 3: Surface realization A referring expression is generated by deterministically assigning a linguistic expression to each element in an SOG according to Rule 1 and 2. As Japanese is a head-Þnal language, simple concatenation of element expressions makes a wellformed noun phrase1. Rule 1 generates expressions for groups and Rule 2 does for relations. Each rule consists of several subrules which are applied in this order. [Rule 1]: Realization of groups Rule 1.1 The total set ({all}) is not realized. (Funakoshi et al., 2004) collected referring expressions from human subjects through experiments and found that humans rarely mentioned the total set. According to their observation, we do not realize the total set. Rule 1.2 Realize the type name for a singleton. Type is realized as a noun and only for a singleton because the type feature is used Þrst to narrow down the group, and the succeeding groups consist of the same type objects until reaching the singleton. When the singleton is not the last element of SOG, particle “no” is added. Rule 1.3 The total set of the same type objects is not realized. This is because</context>
<context position="24009" citStr="Funakoshi et al., 2004" startWordPosition="4100" endWordPosition="4103">ject that is the closest to P in terms of Euclidean distance. Some psychological investigation is necessary to build a more precise geometrical calculation model to solve this problem (Landragin et al., 2001). Table 2 shows the results of Experiment 2. The first row shows the rank of expressions based on their score. The second row shows the count of human votes for the expression. The third row shows the ratio of the votes. The top two expressions occupy 72% of the total. This concludes that our scoring function works well. 5 Conclusion This paper extended the SOG representation proposed in (Funakoshi et al., 2004) to generate refer79 Table 1: Result of Experiment 1 Arrangement No. 1 2 3 4 5 6 7 8 9 10 Accuracy 0.89 1.0 1.0 1.0 1.0 1.0 1.0 0.94 1.0 1.0 11 12 13 14 15 16 17 18 19 20 Ave. 1.0 0.94 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.17 0.95 Table 2: Result of Experiment 2 Rank 1 2 3 4 5 Total Vote 134 125 59 22 20 360 Share 0.37 0.35 0.16 0.06 0.06 1 ring expressions in more general situations. The proposed method was implemented and evaluated through two psychological experiments using 18 subjects. The experiments showed that generated expressions had enough discrimination ability and that the scoring function</context>
</contexts>
<marker>Funakoshi, Watanabe, Kuriyama, Tokunaga, 2004</marker>
<rawString>Kotaro Funakoshi, Satoru Watanabe, Naoko Kuriyama, and Takenobu Tokunaga. 2004. Generating referring expressions using perceptual groups. In Proceedings of the 3rd International Conference on Natural Language Generation: INLG04, pages 51–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Heeman</author>
<author>Graeme Hirst</author>
</authors>
<title>Collaborating referring expressions.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>3</issue>
<pages>382</pages>
<contexts>
<context position="1165" citStr="Heeman and Hirst, 1995" startWordPosition="171" endWordPosition="174">tion, this paper proposes a novel generation method utilizing perceptual groups of objects and n-ary relations among them. The evaluation using 18 subjects showed that the proposed method could effectively generate proper referring expressions. 1 Introduction In the last two decades, many researchers have studied the generation of referring expressions to enable computers to communicate with humans about objects in the world. In order to refer to an intended object (the target) among others (distractors), most past work (Appelt, 1985; Dale and Haddock, 1991; Dale, 1992; Dale and Reiter, 1995; Heeman and Hirst, 1995; Horacek,1997; Krahmer and Theune, 2002; van Deemter, 2002; Krahmer et al., 2003) utilized attributes of the target and binary relations between the target and distractors. Therefore, these methods cannot generate proper referring expressions in situations where there is no signiÞcant surface difference between the target and distractors, and no binary relation is useful to distinguish the target. Here, a proper referring expression ∗Currently at Honda Research Institute Japan Co., Ltd. †Currently at Hitachi, Ltd. means a concise and natural linguistic expression enabling hearers to identify </context>
</contexts>
<marker>Heeman, Hirst, 1995</marker>
<rawString>Peter Heeman and Graeme Hirst. 1995. Collaborating referring expressions. Computational Linguistics, 21(3):351– 382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Horacek</author>
</authors>
<title>An algorithm for generating referential descriptions with ßexible interfaces.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>206--213</pages>
<marker>Horacek, 1997</marker>
<rawString>Helmut Horacek. 1997. An algorithm for generating referential descriptions with ßexible interfaces. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 206–213.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>Mari¨et Theune</author>
</authors>
<title>EfÞcient contextsensitive generation of descriptions.</title>
<date>2002</date>
<booktitle>Information Sharing: Givenness and Newness in Language Processing.</booktitle>
<editor>In Kees van Deemter and Rodger Kibble, editors,</editor>
<publisher>CSLI Publications,</publisher>
<location>Stanford, California.</location>
<contexts>
<context position="1205" citStr="Krahmer and Theune, 2002" startWordPosition="176" endWordPosition="179">ration method utilizing perceptual groups of objects and n-ary relations among them. The evaluation using 18 subjects showed that the proposed method could effectively generate proper referring expressions. 1 Introduction In the last two decades, many researchers have studied the generation of referring expressions to enable computers to communicate with humans about objects in the world. In order to refer to an intended object (the target) among others (distractors), most past work (Appelt, 1985; Dale and Haddock, 1991; Dale, 1992; Dale and Reiter, 1995; Heeman and Hirst, 1995; Horacek,1997; Krahmer and Theune, 2002; van Deemter, 2002; Krahmer et al., 2003) utilized attributes of the target and binary relations between the target and distractors. Therefore, these methods cannot generate proper referring expressions in situations where there is no signiÞcant surface difference between the target and distractors, and no binary relation is useful to distinguish the target. Here, a proper referring expression ∗Currently at Honda Research Institute Japan Co., Ltd. †Currently at Hitachi, Ltd. means a concise and natural linguistic expression enabling hearers to identify the target. For example, consider indica</context>
</contexts>
<marker>Krahmer, Theune, 2002</marker>
<rawString>Emiel Krahmer and Mari¨et Theune. 2002. EfÞcient contextsensitive generation of descriptions. In Kees van Deemter and Rodger Kibble, editors, Information Sharing: Givenness and Newness in Language Processing. CSLI Publications, Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>Sebastiaan van Erk</author>
<author>Andr´e Verleg</author>
</authors>
<title>Graph-based generation of referring expressions.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<marker>Krahmer, van Erk, Verleg, 2003</marker>
<rawString>Emiel Krahmer, Sebastiaan van Erk, and Andr´e Verleg. 2003. Graph-based generation of referring expressions. Computational Linguistics, 29(1):53–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fr´ed´eric Landragin</author>
<author>Nadia Bellalem</author>
<author>Laurent Romary</author>
</authors>
<title>Visual salience and perceptual grouping in multimodal interactivity.</title>
<date>2001</date>
<booktitle>In Proceedings of International Workshop on Information Presentation and Natural Multimodal Dialogue (IPNMD),</booktitle>
<pages>151--155</pages>
<contexts>
<context position="23594" citStr="Landragin et al., 2001" startWordPosition="4025" endWordPosition="4028">riment 2 This shows a good performance of the generation algorithm proposed in this paper. The expression generated for arrangement No. 20 (shown in Figure 9) resulted in the exceptionally poor accuracy. To refer to object bl, our algorithm generated expression “itiban temae no tama (the most front ball)” because bl is the most close object to person P in terms of the vertical axis. Humans, however, chose the object that is the closest to P in terms of Euclidean distance. Some psychological investigation is necessary to build a more precise geometrical calculation model to solve this problem (Landragin et al., 2001). Table 2 shows the results of Experiment 2. The first row shows the rank of expressions based on their score. The second row shows the count of human votes for the expression. The third row shows the ratio of the votes. The top two expressions occupy 72% of the total. This concludes that our scoring function works well. 5 Conclusion This paper extended the SOG representation proposed in (Funakoshi et al., 2004) to generate refer79 Table 1: Result of Experiment 1 Arrangement No. 1 2 3 4 5 6 7 8 9 10 Accuracy 0.89 1.0 1.0 1.0 1.0 1.0 1.0 0.94 1.0 1.0 11 12 13 14 15 16 17 18 19 20 Ave. 1.0 0.94 </context>
</contexts>
<marker>Landragin, Bellalem, Romary, 2001</marker>
<rawString>Fr´ed´eric Landragin, Nadia Bellalem, and Laurent Romary. 2001. Visual salience and perceptual grouping in multimodal interactivity. In Proceedings of International Workshop on Information Presentation and Natural Multimodal Dialogue (IPNMD), pages 151–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Stone</author>
</authors>
<title>On identifying sets.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st International Conference on Natural Language Generation: INLG00,</booktitle>
<pages>116--123</pages>
<contexts>
<context position="2552" citStr="Stone, 2000" startWordPosition="412" endWordPosition="413">rson P does not share these labels with the speaker. Because object b is not distinguishable from objects a or c by means of their appearance, one would try to use a binary relation between object b and the table, i.e., “a ball to the right of the table”. However, “to the right of” is not a discriminatory relation, for objects a and c are also located to the right of the table. Using a and c as a reference object instead of the table does not make sense, since a and c cannot be uniquely identiÞed because of the same reason that b cannot be identiÞed. Such situations have drawn less attention (Stone, 2000), but can frequently occur in some domains such as object arrangement (Tanaka et al., 2004). Figure 1: An example of problematic situations In the situation of Figure 1, the speaker can indicate object b to person P with a simple expression “the front ball”. In order to generate such an expression, one must be able to recognize the salient perceptual group of the objects and use the n-ary relative relations in the group. a c b P 73 Proceedings of the Fourth International Natural Language Generation Conference, pages 73–80, Sydney, July 2006. c�2006 Association for Computational Linguistics To </context>
</contexts>
<marker>Stone, 2000</marker>
<rawString>Matthew Stone. 2000. On identifying sets. In Proceedings of the 1st International Conference on Natural Language Generation: INLG00, pages 116–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hozumi Tanaka</author>
<author>Takenobu Tokunaga</author>
<author>Yusuke Shinyama</author>
</authors>
<title>Animated agents capable of understanding natural language and performing actions.</title>
<date>2004</date>
<booktitle>In Helmut Prendinger and Mituru Ishizuka, editors, Life-Like Characters,</booktitle>
<pages>429--444</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2643" citStr="Tanaka et al., 2004" startWordPosition="425" endWordPosition="428">guishable from objects a or c by means of their appearance, one would try to use a binary relation between object b and the table, i.e., “a ball to the right of the table”. However, “to the right of” is not a discriminatory relation, for objects a and c are also located to the right of the table. Using a and c as a reference object instead of the table does not make sense, since a and c cannot be uniquely identiÞed because of the same reason that b cannot be identiÞed. Such situations have drawn less attention (Stone, 2000), but can frequently occur in some domains such as object arrangement (Tanaka et al., 2004). Figure 1: An example of problematic situations In the situation of Figure 1, the speaker can indicate object b to person P with a simple expression “the front ball”. In order to generate such an expression, one must be able to recognize the salient perceptual group of the objects and use the n-ary relative relations in the group. a c b P 73 Proceedings of the Fourth International Natural Language Generation Conference, pages 73–80, Sydney, July 2006. c�2006 Association for Computational Linguistics To overcome the problem described above, Funakoshi et al. (2004) proposed a method of generati</context>
</contexts>
<marker>Tanaka, Tokunaga, Shinyama, 2004</marker>
<rawString>Hozumi Tanaka, Takenobu Tokunaga, and Yusuke Shinyama. 2004. Animated agents capable of understanding natural language and performing actions. In Helmut Prendinger and Mituru Ishizuka, editors, Life-Like Characters, pages 429–444. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristinn R Th´orisson</author>
</authors>
<title>Simulated perceptual grouping: An application to human-computer interaction.</title>
<date>1994</date>
<booktitle>In Proceedings of the Sixteenth Annual Conference of the Cognitive Science Society,</booktitle>
<pages>876--881</pages>
<marker>Th´orisson, 1994</marker>
<rawString>Kristinn R. Th´orisson. 1994. Simulated perceptual grouping: An application to human-computer interaction. In Proceedings of the Sixteenth Annual Conference of the Cognitive Science Society, pages 876–881.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takenobu Tokunaga</author>
<author>Tomofumi Koyama</author>
<author>Suguru Saito</author>
</authors>
<title>Meaning of Japanese spatial nouns.</title>
<date>2005</date>
<booktitle>In Proceedings of the Second ACL-SIGSEM Workshop on the Linguistic Dimentions of Prepositions and their Use in Computational Linguistics: Formalisms and Applications,</booktitle>
<pages>93--100</pages>
<contexts>
<context position="20195" citStr="Tokunaga et al., 2005" startWordPosition="3445" endWordPosition="3448">s and the realized expressions. 3.4 Step 4: Scoring We assign a score to each expression by taking into account the relations used in the expression, and the length of the expression. First we assign a cost ranging over [0, 1] to each relation in the given SOG. Costs of relations are decided as below. These costs conform to the priorities of features described in (Dale and Reiter, 1995). type −→ : No cost (to be neglected) shape −→ : 0.2 color −→ : 0.4 size −→ :big(est): 0.6, small(est): 0.8, middle: 1.0 space −→,⇒ : Cost functions are defined according to the potential functions proposed in (Tokunaga et al., 2005). The cost for relation “on” is fixed to 0. Then, the average cost of the relations is calculated to obtain the relation cost, Crel. The cost of surface length (Clen) is calculated by Clen =maxi length(expressioni), length(expression) where the length of an expression is measured by the number of characters. Using these costs, the score of an expression is calculated by 1 score = α × Crel + (1 − α) × Clen . α was set to 0.5 in the following experiments. 4 Evaluation 4.1 Experiments We conducted two experiments to evaluate expressions generated by the proposed method. Both experiments used the </context>
</contexts>
<marker>Tokunaga, Koyama, Saito, 2005</marker>
<rawString>Takenobu Tokunaga, Tomofumi Koyama, and Suguru Saito. 2005. Meaning of Japanese spatial nouns. In Proceedings of the Second ACL-SIGSEM Workshop on the Linguistic Dimentions of Prepositions and their Use in Computational Linguistics: Formalisms and Applications, pages 93–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kees van Deemter</author>
</authors>
<title>Generating referring expressions: Boolean extensions of the incremental algorithm.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>1</issue>
<marker>van Deemter, 2002</marker>
<rawString>Kees van Deemter. 2002. Generating referring expressions: Boolean extensions of the incremental algorithm. Computational Linguistics, 28(1):37–52.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>