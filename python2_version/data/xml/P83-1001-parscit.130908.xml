<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000013">
<sectionHeader confidence="0.493517" genericHeader="abstract">
CONTEXT-FREENESS AND THE COMPUTER PROCESSING OF HUMAN LANGUAGES
</sectionHeader>
<author confidence="0.618344">
Geoffrey K. Pullum
</author>
<affiliation confidence="0.8421435">
Cowell College
University of California, Santa Cruz
</affiliation>
<address confidence="0.71033">
Santa Cruz, California 95064
</address>
<email confidence="0.479759">
ABSTRACT
</email>
<bodyText confidence="0.9966968">
Context-free grammars, far from having insufficient
expressive power for the description of human lang-
uages, may be overly powerful, along three dimen-
sions; (1) weak generative capacity: there exists
an interesting proper subset of the CFL&apos;s, the
profligate CFL&apos;s, within which no human language
appears to fall; (2) strong generative capacity:
human languages can be appropriately described in
terms of a proper subset of the CF-PSG&apos;s, namely
those with the ECPO property; (3) time complexity:
the recent controversy about the importance of a
low deterministic polynomial time bound on the
recognition problem for human languages is mis-
directed, since an appropriately restrictive theory
would guarantee even more, namely a linear bound.
</bodyText>
<sectionHeader confidence="0.822864" genericHeader="introduction">
0. INTRODUCTION
</sectionHeader>
<bodyText confidence="0.997088105263158">
Many computationally inclined linguists appear
to think that in order to achieve adequate grammars
for human languages we need a bit more power than
is offered by context-free phrase structure gram-
mars (CF-PSG&apos;s), though not a whole lot more. In
this paper, I am concerned with the defense of a
more conservative view: that even CF-PSG&apos;s should
be regarded as too powerful, in three computation-
ally relevant respects: weak generative capacity,
strong generative capacity, and time complexity of
recognition. All three of these matters should be
of concern to theoretical linguists; the study of
what mathematically definable classes human
languages fall into does not exhaust scientific
linguistics, but it can hardly be claimed to be
irrelevant to it. And it should be obvious that
all three issues also have some payoff in terms of
certain computationally interesting, if rather
indirect, implications.
</bodyText>
<sectionHeader confidence="0.837638" genericHeader="method">
1. WEAK GENERATIVE CAPACITY
</sectionHeader>
<bodyText confidence="0.996325595238095">
Weak generative capacity (WGC) results are held
by some linguists (e.g. Chomsky (1981)) to be unim-
portant. Nonetheless, they cannot be ignored by
linguists who are interested in setting their work
in a context of (even potential) computational
implementation (which, of course, some linguists
are not). To paraphrase Montague, we might say
that linguistically (as opposed to psycholinguisti-
cally) there is no important theoretical difference
between natural languages and high-level program-
ming languages. Mediating programs (e.g. a com-
piler or interpreter), of considerable complexity,
will be needed for the interpretation of computer
input in either Prolog or Japanese. In the latter
case the level of complexity will be much higher,
but the assumption is that we are talking quantita-
tively, not qualitatively. And if we are seriously
interested in the computational properties of
either kind of language, we will be interested in
their language-theoretic properties, as well as
properties of the grammars that define them and the
parsers that accept them.
The most important language-theoretic class con-
sidered by designers of programming languages, com-
pilers, etc. is the context-free languages
(CFL&apos;s). Ginsburg (1980, 7) goes so far as to say
on behalf of formal language theorists, &amp;quot;We live or
die on the context-free languages.&amp;quot;) The class of
CFL&apos;s is very rich. Although there are simply
definable languages well known to be non-CF,
linguists often take CFL&apos;s to be non-CF in error.
Several examples are cited in Pullum and Gazdar
(1982). For another example, see Dowty, Wall and
Peters (1980; p.81), where exercise 3 invites the
reader to prove a certain artificial language non-
CF. The exercise is impossible, for the language
is a CFL, as noted by William H. Baxter (personal
communication to Gerald Gazdar).
From this point on, it will be useful to be able
to refer to certain types of formal language by
names. I shall use the terms defined in (1) thru
(3), among others.
</bodyText>
<listItem confidence="0.454293">
(1) Triple Counting Languages:
</listItem>
<bodyText confidence="0.591881833333333">
languages that can be mapped by a homomorphism
onto some language of the form
aa bn cnI n &gt; 1}
(2) String Matching Languages:
languages that can be mapped by a homomorphism
onto some language of the form
</bodyText>
<listItem confidence="0.577362">
(xxix is in some infinite language A)
(3) String Contrastibt Languages:
</listItem>
<bodyText confidence="0.884713769230769">
languages that can be mapped by a homomorphism
onto some language of the form
{xcylx and y are in some infinite language A
and x A 0
Programming languages are virtually always
designed to be CF, except that there is a moot
point concerning the implications of obligatory
initial declaration of variables as in ALGOL or
Pascal, since if variables (identifiers) can be
alphanumeric strings of arbitrary length, a syntac-
tic guarantee that each variable has been declared
is tantamount to a syntax for a string matching
language. The following view seems a sensible one
</bodyText>
<page confidence="0.968145">
1
</page>
<bodyText confidence="0.994461444444444">
to take about such cases: languages like ALGOL or
Pascal are CF, but not all ALGOL or Pascal programs
compile or run. Programs using undeclared vari-
ables make no sense either to the compiler or to
the CPU. But they are still programs, provided
they conform in all other ways to the syntax of the
language in question, just as a program which
always goes into an infinite loop and thus never
gives any output is a program. Aho and Ullmann
(1977, 140) take such a view:
the syntax of ALGOL...does not get down to
the level of characters in a name. Instead,
all names are represented by a token such as
id, and it is left to the bookkeeping phase of
the compiler to keep track of declarations and
uses of particular names.
The bookkeeping has to be done, of course, even in
the case of languages like LISP whose syntax does
not demand a list of declarations at the start of
each program.
Various efforts have been made in the linguistic
literature to show that some human language has an
infinite, appropriately extractable subset that is
a triple counting language or a string matching
language. (By appropriately extractable I mean
isolable via either homomorphism or intersection
with a regular set.) But all the published claims
of this sort are fallacious (Pullum and Gazdar
1982). This lends plausibility to the hypothesis
that human languages are all CF. Stronger claims
than this (e.g. that human languages are regular, or
finite cardinality) have seldom seriously defended.
I now want to propose one, however.
I propose that human languages are never profli-
gate CFL&apos;s in the sense given by the following
definition.
</bodyText>
<listItem confidence="0.983618625">
(i) A CFL is profligate if all CF-PSG&apos;s
generating it have nonterminal vocabularies
strictly larger than their terminal
vocabularies.
(ii) A CFL is profligate if it is the image of a
profligate language under some homomorphism.
[OPEN PROBLEM: Is profligacy decidable for an
arbitrary CFL? I conjecture that it is not, but I
</listItem>
<bodyText confidence="0.972429647058823">
have not been able to prove ,this.]
Clearly, only an infinite CFL can be profligate,
and clearly the most commonly cited infinite CFL&apos;s
are not profligate. For instance, {anbnin &gt; 0} is
not profligate, because it has two terminal symbols
but there is a grammar for it that has only one
nonterminal symbol, namely S. (The rules are: (S
--&gt; aSb, S --&gt; e).) However, profligate CFL&apos;s do
exist. There are even regular languages that are
profligate: a simple example (due to Christopher
Culy) is (a* + b*).
More interesting is the fact that some string
contrasting languages as defined above are profli-
gate. Consider the string contrasting language over
the vocabulary (a, b, c) where A = (a + b)*. A
string in in (a + b)*c(a + b)* will be in this
language if any one of the following is met:
</bodyText>
<listItem confidence="0.6412472">
(a) x is longer than /;
(b) x is shorter than /;
(c) x is the same length as / but there is an i
such that the ith symbol of x is distinct
from the ith symbol of /.
</listItem>
<bodyText confidence="0.991937827586207">
The interesting condition here is (c). The grammar
has to generate, for all i and for all pairs &lt;u, v&gt;
of symbols in the terminal vocabulary, all those
strings in (a + b)*c(a + b)* such that the ith sym-
bol is u and the ith symbol after c is v. There is
no bound on i, so recursion has to be involved.
But it must be recursion through a category that
preserves a record of which symbol is crucially
going to be deposited at the ith position in the
terminal string and mismatched with a distinct sym-
bol in the second half. A CF-PSG that does this
can be constructed (see Pullum and Gazdar 1982,
478, for a grammar for a very similar language).
But such a grammar has to use recursive nontermi-
nals, one for each terminal, to carry down informa-
tion about the symbol to be deposited at a certain
point in the string. In the language just given
there are only two relevant terminal symbols, but
if there were a thousand symbols that could appear
in the x and / strings, then the vocabulary of
recursive nonterminals would have to be increased
in proportion. (The second clause in the defini-
tion of profligacy makes it irrelevant whether
there are other terminals in the language, like c
in the language cited, that do not have to partici-
pate in the recursive mechanisms just referred to.)
For a profligate CFL, the argument that a CF-PSG
is a cumbersome and inelegant form of grammar might
well have to be accepted. A CF-PSG offers, in some
cases at least, an appallingly inelegant hypothesis
as to the proper description of such a language,
and would be rejected by any linguist or program-
mer. The discovery that some human language is
profligate would therefore provide (for the first
time, I claim) real grounds for a rejection of CF-
PSG&apos;s on the basis of strong generative capacity
(considerations of what structural descriptions are
assigned to strings) as opposed to weak (what
language is generated).
However, no human language has been shown to be
a profligate CFL. There is one relevant argument
in the literature, found in Chomsky (1963). The
argument is based on the nonidentity of consti-
tuents allegedly required in comparative clause
constructions like (4).
(4) She is more competent as [a designer of
programming languages] than he is as
[a designer of microchips].
Chomsky took sentences like (5) to be ungrammati-
cal, and thus assumed that the nonidentity between
the bracketed phrases in the previous example had
to be guaranteed by the grammar.
(5) She is more competent as [a designer of
programming languages] than he is as
[a designer of programming languages].
Chomsky took this as an argument for non-CF-ness in
English, since he thought all string contrasting
languages were non-CF (see Chomsky 1963, 378-379),
</bodyText>
<page confidence="0.962556">
2
</page>
<bodyText confidence="0.990477564705883">
but it can be reinterpreted as an attempt to show
that English is (at least) profligate. (It could
even be reconstituted as a formally valid argument
that English was non-CF if supplemented by a
demonstration that the class of phrases from which
the bracketed sequences are drawn is not only.
infinite but non-regular; cf. Zwicky and Sadodk.)
However, the argument clearly collapses on empir-
ical grounds. As pointed out by Pullum and Gazdar
(1982, 476-477), even Chomsky now agrees that
strings like (5) are grammatical (though they need
a contrastive context and the appropriate intona-
tion to make them readily acceptable to infor-
mants). Hence these examples do not show that
there is a homomorphism mapping English onto some
profligate string contrasting language.
The interesting thing about this, if it is
correct, is that it suggests that human languages
not only never demand the syntactic string com-
parison required by string matching languages, they
never call for syntactic string camparision over
infinite sets of strings at all, whether for
symbol-by-symbol checking of identity (which typi-
cally makes the language non-CF) or for specifying
a mismatch between symbols (which may not make the
language non-CF, but typically makes it profli-
gate).
There is an important point about profligacy
that I should make at this point. My claim that
human languages are non-profligate entails that
each human language has at least one CF-PSG in
which the nonterminal vocabulary has cardinality
strictly less than the terminal vocabulary, but not
that the best grammar to implement for it will
necessarily meet this condition. The point is
important, because the phrase structure grammars
employed in natural language processing generally
have complex nonterminals consisting of sizeable
feature bundles. It is not uncommon for a large
natural language processing system to employ thirty
or forty binary features (or a rough equivalent in
terms of multi-valued features), i.e. about as many
features as are employed for phonological descript-
ion by Chomsky and Halle (1968). The GPSG system
described in Gawron et al. (1982) has employed
features on this sort of scale at all points in its
development, for example. Thirty or forty binary
features yields between a billion and a trillion
logically distinguishable nonterminals (if all
values for each feature are compatible with all
combinations of values for all other features).
Because economical techniques for rapid checking of
relevant feature values are built into the parsers
normally used for such grammars, the size of the
potentially available nonterminal vocabulary is not
a practical concern. In principle, if the goal of
capturing generalizations and reducing the size of
the grammar formulation were put aside, the nonter-
minal vocabulary could be vastly reduced by replac-
ing rule schemata by long lists of distinct rules
expanding the same nonterminal.
Naturally, no claim has been made here that pro-
fligate CFL&apos;s are computationally intractable. No
CFL&apos;s are intractable in the theoretical sense, and
intractability in practice is so closely tied to
details of particular machines and programming
environments as to be pointless to talk about in
terms divorced from actual measurements of size for
grammars, vocabularies, and address spaces. I have
been concerned only to point out that there is an
interesting proper subset of the infinite CFL&apos;s
within which the human languages seem to fall.
One further thing may be worth pointing out.
The kind of string contrasting languages I have
been concerned with above are strictly nondeter-
ministic. The deterministic CFL&apos;s (DCFL&apos;s) are
closed under complementation. But the cor71- _at
of
(6) (malx and m are in (a + b)* and x m)
in (a + b)*c(a + b)* is (7a), identical to (7b), a
string matching language.
(7)a. (malx and m are in (a + b)* and x = .1)
b. (xcxix is in (a + b)*)
If (7a) [=(7b)] is non-CF and is the complement of
(6), then (6) is not a DCFL.
</bodyText>
<sectionHeader confidence="0.543231666666667" genericHeader="method">
[OPEN PROBLEM: Are there any nonregular profligate
DCFL&apos;s?]
2. STRONG GENERATIVE CAPACITY
</sectionHeader>
<bodyText confidence="0.999530357142857">
I now turn to a claim involving strong genera-
tive capacity (SGC). In addition to claiming that
human languages are non-profligate CFL&apos;s, I want to
suggest that every human language has a linguisti-
cally adequate grammar possessing the Exhaustive
Constant Partial Ordering (ECPO) property of Gazdar
and Pullum (1981). A grammar has this property if
there is a single partial ordering of the nontermi-
nal vocabulary which no right hand side of any rule
violates. The ECPO CF-PSG&apos;s are a nonempty proper
subset of the CF-PSG&apos;s. The claim that human
languages always have ECPO CF-PSG&apos;s is a claim
about the strong generative capacity that an
appropriate theory of human language should have---
one of the first such claims to have been seriously
advanced, in fact. It does not affect weak
generative capacity; Shieber (1983a) proves that
every CFL has an ECPO grammar. It is always poss-
ible to construct an ECPO grammar for any CFL if
one is willing to pay the price of inventing new
nonterminals ad hoc to construct it. The content
of the claim lies in the fact that linguists demand
independent motivation for the nonterminals they
postulate, so that the possibility of creating new
ones just to guarantee ECPO-ness is not always a
reasonable one.
[OPEN PROBLEM: Could there be a non-profligate CFL
which had #(N) &lt; #T (i.e. nonterminal vocabulary
strictly mailer than terminal vocabulary) for at
least one of its non-ECPO grammars, but whose ECPO
grammars always had #(N) &gt; #(T)?]
When the linguist&apos;s criteria of evaluation are
kept in mind, it is fairly clear what sort of facts
in a human language would convince linguists to
abandon the ECPO claim. For example, if English
had PP - S order in verb phrases (explain to him
that he&apos;ll have to leave) but had S&apos; - PP order in
adjectives (so that lucky for us we found you had
the form lucky we found you for us), the grammar of
English would not have the ECPO property. But such
facts appear not to turn up in the languages we know
about.
</bodyText>
<page confidence="0.993548">
3
</page>
<bodyText confidence="0.989617588235294">
The ECPO claim has interesting consequences
relating to patterns of constituent order and how
these can be described in a fully general way. If
a grammar has the ECPO property, it can be stated
in what Gazdar and Pullum call ID/LP format, and
this renders numerous significant generalizations
elegantly capturable. There are also some poten-
tially interesting implications for parsing, stu-
died by Shieber (1983a), who shows that a modified
Earley algorithm can be used to parse ID/LP format
grammars directly.
One putative challenge to any claim that CF-
PSG&apos;s can be strongly adequate descriptions for
human languages comes from Dutch and has been dis-
cussed recently by Bresnan, Kaplan, Peters, and
Zaenen (1982). Dutch has constructions like
(7) dat Jan Piet Marie zag leren zwemmen
that Jan Piet Marie saw teach swim
&apos;that Jan saw Piet teach Marie to swim&apos;
These seem to involve crossing dependencies over a
domain of potentially arbitrary length, a confi-
guration that is syntactically not expressible by a
CF-PSG. In the special case where the dependency
involves stringvise identity, a language with this
sort of structure reduces to something like (xxlx
is in A*), a string matching language. However,
analysis reveals that, as Bresnan et al. accept,
the actual dependencies in Dutch are not syntactic.
Grammaticality of a string like (7) is not in gen-
eral affected by interchanging the NP&apos;s with one
another, since it does not matter to the â€¢ith verb
what the ith NP might be. What is crucial is that
(in cases with simple transitive verbs, as above)
the ith predicate (verb) takes the interpretation
of the i-lth noun phrase as its argument.
Strictly, this does not bear on the issue of SGC in
any way that can be explicated without making
reference to semantics. What is really at issue is
whether a CF-PSG can assign syntactic qtructures to
sentences of Dutch in a way that supports semantic
interpretation.
Certain recent work within the framework of gen-
eralized phrase structure grammar suggests to me
that there is a very strong probability of the
answer being yes. One interesting development is
to be found in Culy (forthcoming), where it is
shown that it is possible for a CFL-inducing syntax
in ID/LP format to assign a &amp;quot;flat&amp;quot; constituent
structure to strings like Piet Marie ;AL Ieren
zwemmen (&amp;quot;saw Piet teach Marie to swim&apos;), and
assign them the correct semantics.
Ivan Sag, in unpublished work, has developed a
different account, in which strings like gag leren
zwemmen Cum teach to swim&apos;) are treated as com-
pound verbs whose semantics is only satisfied if
they are provided with the appropriate number of NP
sisters. Whereas Culy has the syntax determine the
relative numbers of NP&apos;s and verbs, Sag is explor-
ing the assumption that this is unnecessary, since
the semantic interpretation procedure can carry
this descriptive burden. Under this view too,
there is nothing about the syntax of Dutch that
makes it non-CF, and there is not necessarily any-
thing in the grammar that makes it non-ECPO.
Henry Thompson also discusses the Dutch problem
from the GPSG standpoint (in this volume).
One other interesting line of work being pursued
(at Stanford, like the work of Culy and of Sag) is
due to Carl Pollard (Pollard, forthcoming, provides
an introduction). Pollard has developed a general-
ization of context-free grammar which is defined
not on trees but on &amp;quot;headed strings&amp;quot;, i.e. strings
with a mark indicating that one distinguished ele-
ment of the string is the &amp;quot;head&amp;quot;, and which com-
bines constituents not only by concatenation but
also by &amp;quot;head wrap&amp;quot;. This operation is analogous
to Emmon Bach&apos;s notion &amp;quot;right (or left) wrap&amp;quot; but
not equivalent to it. It involves wrapping a con-
stituent A around a constituent B so that the head
is to the left (or right) of B and the rest of A is
to the right (or left) of B. Pollard has shown
that this provides for an elegant syntactic treat-
ment of the Dutch facts. I mention his work
because I want to return to make a point about it
in the immediately following section.
</bodyText>
<sectionHeader confidence="0.773724" genericHeader="method">
3. TIME COMPLEXITY OF RECOGNITION
</sectionHeader>
<bodyText confidence="0.999984039215687">
The time complexity of the recognition problem
(TCR) for human languages is like WGC questions in
being decried as irrelevant by some linguists, but
again, it is hardly one that serious computational
approaches can legitimately ignore. Gazdar (1981)
has recently reminded the linguistic community of
this, and has been answered at great length by
Berwick and Weinberg (1982). Gazdar noted that if
transformational grammars (TG&apos;s) were stripped of
all their transformations, they became CFL-
inducing, which meant that the series of works
showing CFL&apos;s to have sub-cubic recognition times
became relevant to them. Berwick and Weinberg&apos;s
paper represents a concerted effOrt to discredit
any such suggestion by insisting that (a) it isn&apos;t
only the CFL&apos;s that have low polynomial recognition
time results, and (b) it isn&apos;t clear that any
asymptotic recognition time results have practical
implications for human language use (or for com-
puter modelling of it).
Both points should be quite uncontroversial, of
course, and it is only by dint of inaccurate attri-
bution that Berwick and Weinberg manage to suggest
that Gazdar denies them. However, the two points
simply do not add up to a reason for not being con-
cerned with TCR results. Perfectly straightforward
considerations of theoretical restrictiveness dic-
tate that if the languages recognizable in polyno-
mial time are a proper subset of those recognizable
in exponential time (or whatever), it is desirable
to explore the hypothesis that the human languages
fall within the former class rather than just the
latter.
Certainly, it is not just CFL&apos;s that have been
shown to be efficiently recognizable in determinis-
tic time on a Turing machine. Not only every
context-free grammar but also every context-
sensitive grammar that can actually be exhibited
generates a language that can be recognized in
deterministic linear time on a two-tape Turing
machine. It is certainly not the case that all the
context-sensitive languages are linearly recogniz-
able; it can be shown (in a highly indirect way)
that there must be some that are not. But all the
examples ever constructed generate linearly recog-
nizable languages. And it is still unknown whether
there are CFL&apos;s not linearly recognizable.
It is therefore not at all necessary that a
human language should be a CFL in order to be effi-
ciently recognizable. But the claims about recog-
nizability of CFL&apos;s do not stop at saying that by
</bodyText>
<page confidence="0.995646">
4
</page>
<bodyText confidence="0.994400642335766">
good fortune there happens to be a fast recognition
algorithm for each member of the class of CFL&apos;s.
The claim, rather, is that there is 4 single,
universal algorithm that works for every member of
the class and has a low deterministic polynomial
time complexity. That is what cannot be said of
the context-sensitive languages.
Nonetheless, there are well-understood classes
of grammars and automata for which it can be said.
For example, Pollard, in the course of the work
mentioned above, has shown that if one or other of
left head wrap and right head wrap is permitted in
the theory of generalized context-free grammar,
recognizability in deterministic time n5 is
guaranteed, and if both left head wrap and right
head wrap are allowed in grammars (with individual
grammars free to have either or both), then in the
general case the upper bound for recognition time
is n7. These are, while not sub-cubic, still low
deterministic polynomial time bounds. Pollard&apos;s
system contrasts in this regard with the lexical-
functional grammar advocated by Bresnan et al.,
which is currently conjectured to have an NP-
complete recognition problem.
I remain cautious about welcoming the move that
Pollard makes because as yet his non-CFL-inducing
syntactic theory does not provide an explanation
for the fact that human languages always seem to
turn out to be CFL&apos;s. It should be pointed out,
however, that it is true of every grammatical
theory that not every grammar defined as possible
is held to be likely to turn up in practice, so it
is not inconceivable that the grammars of human
languages might fall within the CFL-inducing proper
subset of Pollard-style head grammars.
Of course, another possibility is that it might
turn out that some human language ultimately pro-
vides evidence of non-CF-ness, and thus of a need
for mechanisms at least as powerful as Pollard&apos;s.
Bresnan et al. mention at the end of their paper
on Dutch a set of potential candidates: the so
called &amp;quot;free word order&amp;quot; or &amp;quot;nonconfigurational&amp;quot;
languages, particularly Australian languages like
Dyirbal and Walbiri, which can allegedly distribute
elements of a phrase at random throughout a sen-
tence in almost any order. I have certain doubts
about the interpretation of the empirical material
on these languages, but I shall not pursue that
here. I want instead to show that, counter to the
naive intuition that wild word order would neces-
sarily lead to gross parsing complexity, even ram-
pantly free word order in a language does not
necessarily indicate a parsing problem that exhi-
bits itself in TCR terms.
Let us call transposition of adjacent terminal
symbols scrambling, and let us refer to the closure
of a language L under scrambling as the scramble of
L. The scramble of a CFL (even a regular one) can
be non-CF. For example, the scramble of the regu-
lar language (abc)* is non-CF, although (abc)*
itself is regular. (Of course, the scramble of a
CFL is not always non-CF. The scramble of a*b*c*
is (a, b, c)*, and both are regular, hence CF.)
Suppose for the sake of discussion that there is a
human language that is closed under scrambling (or
has an appropriately extractable infinite subset
that is). The example just cited, the scramble of
(abc)*, is a fairly clear case of the sort of thing
that might be modeled in a human language that was
closed under scrambling. Imagine, for example, the
case of a language in which each transitive clause
had a verb (a), a nominative noun phrase (b), and
an accusative noun phrase (c), and free word order
permitted the a, b, and c from any number of
clauses to occur interspersed in any order
throughout the sentence. If we denote the number
of x&apos;s in a string Z by Nx(2), we can say tnat the
scramble of (abc)* is (8).
(8)(i is in (, b, c)* and Na(x) = Nb(x) = Nc(x))
xx a
Attention was first drawn to this sort of language
by Bach (1981), and I shall therefore call it a
Bach language. What TCR properties does a Bach
language have? The one in (8), at least, can be
shown to be recognizable in linear time. The proof
is rather trivial, since it is just a corollary of
a previously known result. Cook (1971) shows that
any language that is recognized by a two-way deter-
ministic pushdown stack automaton (2DPDA) is recog-
nizable in linear time on a Turing machine. In the
Appendix, I give an informal description of a 2DPDA
that will recognize the language in (8). Given
this, the proof that (8) is linearly recognizable
is trivial.
Thus even if my WGC and SGC conjectures were
falsified by discoveries about free word order
languages (which I consider that they have not
been), there would still be no ground for tolerat-
ing theories of grammar and parsing that fail to
impose a linear time bound on recognition. And
recent work of Shieber (1983b) shows that there are
interesting avenues in natural language parsing to
be explored using deterministic context-free
parsers that do work in linear time.
In the light of the above remarks, some of the
points made by Berwick and Weinberg look rather
peculiar. For example, Berwick and Weinberg argue
at length that things are really so complicated in
practical implementations that a cubic bound on
recognition time might not make much difference;
for short sentences a theory that only guarantees
an exponential time bound might do just as well.
This is, to begin with, a very odd response to be
made by defenders of TG when confronted by a
theoretically restrictive claim. If someone made
the theoretical claim that some problem had the
time complexity of the Travelling Salesman problem,
and was met by the response that real-life travel-
ling salesmen do not visit very many cities before
returning to head office, I think theoretical com-
puter scientists would have a right to be amused.
Likewise, it is funny to see practical implementa-
tion considerations brought to bear in defending TG
against the phrase structure backlash, when (a) no
formalized version of modern TG exists, let alone
being available for implementation, and (b) large
phrase structure grammars. are being implemented on
computers and shown to run very fast (see e.g. Slo-
cum 1983, who reports an all-paths, bottom-up
parser actually running in linear time using a CF-
PSG with 400 rules and 10,000 lexical entries).
Berwick and Weinberg seem to imply that data
permitting a comparison of CF-PSG with TG are
available. This is quite untrue, as far as I know.
I therefore find it nothing short of astonishing to
find Chomsky (1981, 234), taking a very similar
position, affirming that because the size of the
</bodyText>
<page confidence="0.969387">
5
</page>
<bodyText confidence="0.999965818181818">
grammar is a constant factor in TCR calculations,
and possibly a large one,
The real empirical content of existing
results... may well-be that grammars are
preferred if they are not too complex in
their rule structure. If parsability is a
factor in language evolution, we would
expect it to prefer &apos;short grammars&apos;---such
as transformational grammars based on the
projection principle or the binding
theory...
TG&apos;s based on the &amp;quot;projection principle&amp;quot; and the
&amp;quot;binding theory&amp;quot; have yet to be formulated with
sufficient explicitness for it to be determined
whether they have a rule structure at all, let
alone a simple one, and the existence of parsing
algorithms for them, of any sort whatever, has not
been demonstrated.
The real reason to reject a cubic recognition-
time guarantee as a goal to be attained by syntac-
tic theory construction is not that the quest is
pointless, but rather that it is not nearly ambi-
tious enough a goal. Anyone who settles for a
cubic TCR bound may be settling for a theory a lot
laser than it could be. (This accusation would be
levellable equally at TG, lexical-functional gram-
mar, Pollards generalized context-free grammar,
and generalized phrase structure grammar as
currently conceived.) Closer to what is called for
would be a theory that defines human grammars as
some proper subset of the ECP0 CF-PSG&apos;s that gen-
erate infinite, nonprof ligate, linear-time recog-
nizable languages. Just as the description of
ALGOL-60 in BNF formalism had a galvanizing effect
on theoretical computer science (Ginsburg 1980, 6-
7), precise specification of a theory of this sort
might sharpen quite considerably our view of the
computational issues involved in natural language
processing. And it would simultaneously be of con-
siderable linguistic interest, at least for those
who accept that we need a sharper theory of natural
language than the vaguely-outlined decorative nota-
tions for Turing machines that are so often taken
for theories in linguistics.
</bodyText>
<sectionHeader confidence="0.830008" genericHeader="conclusions">
ACKNOWLEDGEMENT
</sectionHeader>
<bodyText confidence="0.94553875">
I thank Chris Culy, Carl Pollard, Stuart Shieber,
Tom Wasow, and Arnold Zwicky for useful conversa-
tions and helpful comments. The research reported
here was in no way supported by Hewlett-Packard.
</bodyText>
<sectionHeader confidence="0.97086" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995541372881356">
Aho, A. V. and J. D. Ullmann (1977). Principles of
Compiler Design. Addison-Wesley.
Bach, E. (1981). Discontinuous constituents in
generalized categorial grammars. NELS 11, 1-12.
Berwick, R., and A. Weinberg (1982). Parsing effi-
ciency and the evaluation of grammatical
theories. LI 13.165-191.
Bresnan, J. W.; R. M. Kaplan; S. Peters; and A.
Zaenen (1982). Cross-serial dependencies in
Dutch. LI 13.613-635.
Chomsky, N. (1963). Formal properties of grammars.
In R. D. Luce, R. R. Bush, and E. Galanter,
eds., Handbook of Mathematical Psychology II.
John Wiley.
Chomsky, N. (1981). Knowledge of language: its
elements and origins. Phil. Trans. of the Royal
Soc. of Lond. B 295, 223-234.
Cook, S. A. (1971). Linear time simulation of
deterministic two-way pushdown automata.
Proceedings of the 1971 IFIP Conference, 75-80.
North-Holland.
Culy, C. D. (forthcoming). An extension of phrase
structure rules and an application to natural
language. Stanford MA thesis. of Linguistics,
Stanford University.
Dovty, D. R.; R. Wall; and P. S. Peters (1980).
Introduction to Montague Semantics. D. Reidel.
Gawron, J. M., et al. (1982). The GPSG linguistic
system. Proc. 20th Ann. Meeting of ACL 74-81.
Gazdar, G. (1981). Unbounded dependencies and
coordinate structure. LI 12.155-184.
Gazdar, G. and G. K. Pullum (1981). Subcategorize-
tion, constituent order, and the notion &apos;head&apos;.
In M. Moortgat, H. v. d. Hulst, and T. Hoekstra
(eds.), The Scope 91 Lexical Rules, 107-123.
For is
Ginsburg, S. (1980). Methods for specifying formal
languages---past-present-future. In R. V. Book,
ed., Formal Language Theory: Perspectives and
Open Problems, 1-47. Academic Press.
Pollard, C. J. (forthcoming). Generalized
context-free grammars, head grammars, and
natural language.
Pullum, G. K. (1982). Free word order and phrase
structure rules. NELS 12, 209-220.
Pullum, G. K. and Gazdar, G. (1982). Natural
languages and context-free languages. Lim. and
Phil. 4.471-504.
Shieber, S. M. (1983a). Direct parsing of ID/LP
grammars. Unpublished, SRI, Menlo Park, CA.
Shieber, S. M. (1983b). Sentence disambiguation by
a shift-reduce parsing technique. In this
volume.
Slocum, J. (1983). A status report on the LRC
machine translation system. Conf. on Applied
Nat. Lang. Proc. 166-173. ACL, Menlo Park, CA.
Zwicky, A. M. and J. M. Sadock (forthcoming). A
note on Ay languages. Submitted to Ling. and
Phil.
</reference>
<bodyText confidence="0.9690055">
Appendix: a 2DPDA that recognizes a Bach language
The language (xlx is in (a + b + c)* and Na(x) =
Nb(x) = Nc(x)) is accepted by a 2DPDA with a single
symbol z in its stack vocabulary, (a, b, c) as
input vocabulary, four states, and the following
instruction set. State 1: move rightward, reading
as, b&apos;s, and c&apos;s, and adding a z to the stack each
time a appears on the input tape. On encountering
right end marker in state 1, go to state 2. State
2: move left, popping a z each time a b appears.
On reaching left end marker in state 2 with empty
stack (which will mean Na(x) = Nb(x)), go to state
3. State 3: move right, pushing a z on the stack
for every a encountered. On reaching right end
marker in state 3, go to state 4. State 4: move
left, popping a z for each c encountered. On
reaching left end marker in state 4 with empty
stack (which will mean Na(w) = Nc(w)), accept.
</bodyText>
<page confidence="0.997292">
6
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.970713">
<title confidence="0.998111">CONTEXT-FREENESS AND THE COMPUTER PROCESSING OF HUMAN LANGUAGES</title>
<author confidence="0.999995">Geoffrey K Pullum</author>
<affiliation confidence="0.9993925">Cowell College University of California, Santa Cruz</affiliation>
<address confidence="0.999914">Santa Cruz, California 95064</address>
<abstract confidence="0.998264625">Context-free grammars, far from having insufficient expressive power for the description of human languages, may be overly powerful, along three dimensions; (1) weak generative capacity: there exists an interesting proper subset of the CFL&apos;s, the profligate CFL&apos;s, within which no human language appears to fall; (2) strong generative capacity: languages can described in terms of a proper subset of the CF-PSG&apos;s, namely those with the ECPO property; (3) time complexity: the recent controversy about the importance of a low deterministic polynomial time bound on the recognition problem for human languages is misdirected, since an appropriately restrictive theory would guarantee even more, namely a linear bound.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>J D Ullmann</author>
</authors>
<date>1977</date>
<booktitle>Principles of Compiler Design.</booktitle>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="5165" citStr="Aho and Ullmann (1977" startWordPosition="826" endWordPosition="829"> arbitrary length, a syntactic guarantee that each variable has been declared is tantamount to a syntax for a string matching language. The following view seems a sensible one 1 to take about such cases: languages like ALGOL or Pascal are CF, but not all ALGOL or Pascal programs compile or run. Programs using undeclared variables make no sense either to the compiler or to the CPU. But they are still programs, provided they conform in all other ways to the syntax of the language in question, just as a program which always goes into an infinite loop and thus never gives any output is a program. Aho and Ullmann (1977, 140) take such a view: the syntax of ALGOL...does not get down to the level of characters in a name. Instead, all names are represented by a token such as id, and it is left to the bookkeeping phase of the compiler to keep track of declarations and uses of particular names. The bookkeeping has to be done, of course, even in the case of languages like LISP whose syntax does not demand a list of declarations at the start of each program. Various efforts have been made in the linguistic literature to show that some human language has an infinite, appropriately extractable subset that is a tripl</context>
</contexts>
<marker>Aho, Ullmann, 1977</marker>
<rawString>Aho, A. V. and J. D. Ullmann (1977). Principles of Compiler Design. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bach</author>
</authors>
<title>Discontinuous constituents in generalized categorial grammars.</title>
<date>1981</date>
<journal>NELS</journal>
<volume>11</volume>
<pages>1--12</pages>
<contexts>
<context position="26737" citStr="Bach (1981)" startWordPosition="4489" endWordPosition="4490">e of the sort of thing that might be modeled in a human language that was closed under scrambling. Imagine, for example, the case of a language in which each transitive clause had a verb (a), a nominative noun phrase (b), and an accusative noun phrase (c), and free word order permitted the a, b, and c from any number of clauses to occur interspersed in any order throughout the sentence. If we denote the number of x&apos;s in a string Z by Nx(2), we can say tnat the scramble of (abc)* is (8). (8)(i is in (, b, c)* and Na(x) = Nb(x) = Nc(x)) xx a Attention was first drawn to this sort of language by Bach (1981), and I shall therefore call it a Bach language. What TCR properties does a Bach language have? The one in (8), at least, can be shown to be recognizable in linear time. The proof is rather trivial, since it is just a corollary of a previously known result. Cook (1971) shows that any language that is recognized by a two-way deterministic pushdown stack automaton (2DPDA) is recognizable in linear time on a Turing machine. In the Appendix, I give an informal description of a 2DPDA that will recognize the language in (8). Given this, the proof that (8) is linearly recognizable is trivial. Thus ev</context>
</contexts>
<marker>Bach, 1981</marker>
<rawString>Bach, E. (1981). Discontinuous constituents in generalized categorial grammars. NELS 11, 1-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Berwick</author>
<author>A Weinberg</author>
</authors>
<title>Parsing efficiency and the evaluation of grammatical theories.</title>
<date>1982</date>
<journal>LI</journal>
<pages>13--165</pages>
<contexts>
<context position="20830" citStr="Berwick and Weinberg (1982)" startWordPosition="3487" endWordPosition="3490">ght (or left) of B. Pollard has shown that this provides for an elegant syntactic treatment of the Dutch facts. I mention his work because I want to return to make a point about it in the immediately following section. 3. TIME COMPLEXITY OF RECOGNITION The time complexity of the recognition problem (TCR) for human languages is like WGC questions in being decried as irrelevant by some linguists, but again, it is hardly one that serious computational approaches can legitimately ignore. Gazdar (1981) has recently reminded the linguistic community of this, and has been answered at great length by Berwick and Weinberg (1982). Gazdar noted that if transformational grammars (TG&apos;s) were stripped of all their transformations, they became CFLinducing, which meant that the series of works showing CFL&apos;s to have sub-cubic recognition times became relevant to them. Berwick and Weinberg&apos;s paper represents a concerted effOrt to discredit any such suggestion by insisting that (a) it isn&apos;t only the CFL&apos;s that have low polynomial recognition time results, and (b) it isn&apos;t clear that any asymptotic recognition time results have practical implications for human language use (or for computer modelling of it). Both points should b</context>
</contexts>
<marker>Berwick, Weinberg, 1982</marker>
<rawString>Berwick, R., and A. Weinberg (1982). Parsing efficiency and the evaluation of grammatical theories. LI 13.165-191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Bresnan</author>
<author>R M Kaplan</author>
<author>S Peters</author>
<author>A Zaenen</author>
</authors>
<date>1982</date>
<booktitle>Cross-serial dependencies in Dutch. LI</booktitle>
<pages>13--613</pages>
<marker>Bresnan, Kaplan, Peters, Zaenen, 1982</marker>
<rawString>Bresnan, J. W.; R. M. Kaplan; S. Peters; and A. Zaenen (1982). Cross-serial dependencies in Dutch. LI 13.613-635.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Formal properties of grammars.</title>
<date>1963</date>
<booktitle>Handbook of Mathematical Psychology II.</booktitle>
<editor>In R. D. Luce, R. R. Bush, and E. Galanter, eds.,</editor>
<publisher>John Wiley.</publisher>
<contexts>
<context position="9661" citStr="Chomsky (1963)" startWordPosition="1627" endWordPosition="1628"> in some cases at least, an appallingly inelegant hypothesis as to the proper description of such a language, and would be rejected by any linguist or programmer. The discovery that some human language is profligate would therefore provide (for the first time, I claim) real grounds for a rejection of CFPSG&apos;s on the basis of strong generative capacity (considerations of what structural descriptions are assigned to strings) as opposed to weak (what language is generated). However, no human language has been shown to be a profligate CFL. There is one relevant argument in the literature, found in Chomsky (1963). The argument is based on the nonidentity of constituents allegedly required in comparative clause constructions like (4). (4) She is more competent as [a designer of programming languages] than he is as [a designer of microchips]. Chomsky took sentences like (5) to be ungrammatical, and thus assumed that the nonidentity between the bracketed phrases in the previous example had to be guaranteed by the grammar. (5) She is more competent as [a designer of programming languages] than he is as [a designer of programming languages]. Chomsky took this as an argument for non-CF-ness in English, sinc</context>
</contexts>
<marker>Chomsky, 1963</marker>
<rawString>Chomsky, N. (1963). Formal properties of grammars. In R. D. Luce, R. R. Bush, and E. Galanter, eds., Handbook of Mathematical Psychology II. John Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Knowledge of language: its elements and origins.</title>
<date>1981</date>
<journal>Phil. Trans. of the Royal Soc. of Lond. B</journal>
<volume>295</volume>
<pages>223--234</pages>
<contexts>
<context position="1947" citStr="Chomsky (1981)" startWordPosition="292" endWordPosition="293">nt respects: weak generative capacity, strong generative capacity, and time complexity of recognition. All three of these matters should be of concern to theoretical linguists; the study of what mathematically definable classes human languages fall into does not exhaust scientific linguistics, but it can hardly be claimed to be irrelevant to it. And it should be obvious that all three issues also have some payoff in terms of certain computationally interesting, if rather indirect, implications. 1. WEAK GENERATIVE CAPACITY Weak generative capacity (WGC) results are held by some linguists (e.g. Chomsky (1981)) to be unimportant. Nonetheless, they cannot be ignored by linguists who are interested in setting their work in a context of (even potential) computational implementation (which, of course, some linguists are not). To paraphrase Montague, we might say that linguistically (as opposed to psycholinguistically) there is no important theoretical difference between natural languages and high-level programming languages. Mediating programs (e.g. a compiler or interpreter), of considerable complexity, will be needed for the interpretation of computer input in either Prolog or Japanese. In the latter</context>
<context position="29343" citStr="Chomsky (1981" startWordPosition="4931" endWordPosition="4932">g TG against the phrase structure backlash, when (a) no formalized version of modern TG exists, let alone being available for implementation, and (b) large phrase structure grammars. are being implemented on computers and shown to run very fast (see e.g. Slocum 1983, who reports an all-paths, bottom-up parser actually running in linear time using a CFPSG with 400 rules and 10,000 lexical entries). Berwick and Weinberg seem to imply that data permitting a comparison of CF-PSG with TG are available. This is quite untrue, as far as I know. I therefore find it nothing short of astonishing to find Chomsky (1981, 234), taking a very similar position, affirming that because the size of the 5 grammar is a constant factor in TCR calculations, and possibly a large one, The real empirical content of existing results... may well-be that grammars are preferred if they are not too complex in their rule structure. If parsability is a factor in language evolution, we would expect it to prefer &apos;short grammars&apos;---such as transformational grammars based on the projection principle or the binding theory... TG&apos;s based on the &amp;quot;projection principle&amp;quot; and the &amp;quot;binding theory&amp;quot; have yet to be formulated with sufficient e</context>
</contexts>
<marker>Chomsky, 1981</marker>
<rawString>Chomsky, N. (1981). Knowledge of language: its elements and origins. Phil. Trans. of the Royal Soc. of Lond. B 295, 223-234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A Cook</author>
</authors>
<title>Linear time simulation of deterministic two-way pushdown automata.</title>
<date>1971</date>
<booktitle>Proceedings of the 1971 IFIP Conference,</booktitle>
<pages>75--80</pages>
<publisher>North-Holland.</publisher>
<contexts>
<context position="27006" citStr="Cook (1971)" startWordPosition="4539" endWordPosition="4540">d order permitted the a, b, and c from any number of clauses to occur interspersed in any order throughout the sentence. If we denote the number of x&apos;s in a string Z by Nx(2), we can say tnat the scramble of (abc)* is (8). (8)(i is in (, b, c)* and Na(x) = Nb(x) = Nc(x)) xx a Attention was first drawn to this sort of language by Bach (1981), and I shall therefore call it a Bach language. What TCR properties does a Bach language have? The one in (8), at least, can be shown to be recognizable in linear time. The proof is rather trivial, since it is just a corollary of a previously known result. Cook (1971) shows that any language that is recognized by a two-way deterministic pushdown stack automaton (2DPDA) is recognizable in linear time on a Turing machine. In the Appendix, I give an informal description of a 2DPDA that will recognize the language in (8). Given this, the proof that (8) is linearly recognizable is trivial. Thus even if my WGC and SGC conjectures were falsified by discoveries about free word order languages (which I consider that they have not been), there would still be no ground for tolerating theories of grammar and parsing that fail to impose a linear time bound on recogniti</context>
</contexts>
<marker>Cook, 1971</marker>
<rawString>Cook, S. A. (1971). Linear time simulation of deterministic two-way pushdown automata. Proceedings of the 1971 IFIP Conference, 75-80. North-Holland.</rawString>
</citation>
<citation valid="false">
<authors>
<author>C D Culy</author>
</authors>
<title>An extension of phrase structure rules and an application to natural language. Stanford MA thesis. of Linguistics,</title>
<institution>Stanford University.</institution>
<marker>Culy, </marker>
<rawString>Culy, C. D. (forthcoming). An extension of phrase structure rules and an application to natural language. Stanford MA thesis. of Linguistics, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Dovty</author>
<author>R Wall</author>
<author>P S Peters</author>
</authors>
<title>Introduction to Montague Semantics.</title>
<date>1980</date>
<tech>D. Reidel.</tech>
<marker>Dovty, Wall, Peters, 1980</marker>
<rawString>Dovty, D. R.; R. Wall; and P. S. Peters (1980). Introduction to Montague Semantics. D. Reidel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Gawron</author>
</authors>
<title>The GPSG linguistic system.</title>
<date>1982</date>
<booktitle>Proc. 20th Ann. Meeting of ACL</booktitle>
<pages>74--81</pages>
<marker>Gawron, 1982</marker>
<rawString>Gawron, J. M., et al. (1982). The GPSG linguistic system. Proc. 20th Ann. Meeting of ACL 74-81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
</authors>
<title>Unbounded dependencies and coordinate structure.</title>
<date>1981</date>
<journal>LI</journal>
<pages>12--155</pages>
<contexts>
<context position="20705" citStr="Gazdar (1981)" startWordPosition="3469" endWordPosition="3470">tituent A around a constituent B so that the head is to the left (or right) of B and the rest of A is to the right (or left) of B. Pollard has shown that this provides for an elegant syntactic treatment of the Dutch facts. I mention his work because I want to return to make a point about it in the immediately following section. 3. TIME COMPLEXITY OF RECOGNITION The time complexity of the recognition problem (TCR) for human languages is like WGC questions in being decried as irrelevant by some linguists, but again, it is hardly one that serious computational approaches can legitimately ignore. Gazdar (1981) has recently reminded the linguistic community of this, and has been answered at great length by Berwick and Weinberg (1982). Gazdar noted that if transformational grammars (TG&apos;s) were stripped of all their transformations, they became CFLinducing, which meant that the series of works showing CFL&apos;s to have sub-cubic recognition times became relevant to them. Berwick and Weinberg&apos;s paper represents a concerted effOrt to discredit any such suggestion by insisting that (a) it isn&apos;t only the CFL&apos;s that have low polynomial recognition time results, and (b) it isn&apos;t clear that any asymptotic recogn</context>
</contexts>
<marker>Gazdar, 1981</marker>
<rawString>Gazdar, G. (1981). Unbounded dependencies and coordinate structure. LI 12.155-184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
<author>G K Pullum</author>
</authors>
<title>Subcategorizetion, constituent order, and the notion &apos;head&apos;.</title>
<date>1981</date>
<booktitle>The Scope 91 Lexical Rules,</booktitle>
<pages>107--123</pages>
<editor>In M. Moortgat, H. v. d. Hulst, and T. Hoekstra (eds.),</editor>
<publisher>For is</publisher>
<contexts>
<context position="14720" citStr="Gazdar and Pullum (1981)" startWordPosition="2442" endWordPosition="2445">)*c(a + b)* is (7a), identical to (7b), a string matching language. (7)a. (malx and m are in (a + b)* and x = .1) b. (xcxix is in (a + b)*) If (7a) [=(7b)] is non-CF and is the complement of (6), then (6) is not a DCFL. [OPEN PROBLEM: Are there any nonregular profligate DCFL&apos;s?] 2. STRONG GENERATIVE CAPACITY I now turn to a claim involving strong generative capacity (SGC). In addition to claiming that human languages are non-profligate CFL&apos;s, I want to suggest that every human language has a linguistically adequate grammar possessing the Exhaustive Constant Partial Ordering (ECPO) property of Gazdar and Pullum (1981). A grammar has this property if there is a single partial ordering of the nonterminal vocabulary which no right hand side of any rule violates. The ECPO CF-PSG&apos;s are a nonempty proper subset of the CF-PSG&apos;s. The claim that human languages always have ECPO CF-PSG&apos;s is a claim about the strong generative capacity that an appropriate theory of human language should have--- one of the first such claims to have been seriously advanced, in fact. It does not affect weak generative capacity; Shieber (1983a) proves that every CFL has an ECPO grammar. It is always possible to construct an ECPO grammar </context>
</contexts>
<marker>Gazdar, Pullum, 1981</marker>
<rawString>Gazdar, G. and G. K. Pullum (1981). Subcategorizetion, constituent order, and the notion &apos;head&apos;. In M. Moortgat, H. v. d. Hulst, and T. Hoekstra (eds.), The Scope 91 Lexical Rules, 107-123. For is</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ginsburg</author>
</authors>
<title>Methods for specifying formal languages---past-present-future. In</title>
<date>1980</date>
<booktitle>Formal Language Theory: Perspectives and Open Problems,</booktitle>
<pages>1--47</pages>
<editor>R. V. Book, ed.,</editor>
<publisher>Academic Press.</publisher>
<contexts>
<context position="3084" citStr="Ginsburg (1980" startWordPosition="462" endWordPosition="463"> interpretation of computer input in either Prolog or Japanese. In the latter case the level of complexity will be much higher, but the assumption is that we are talking quantitatively, not qualitatively. And if we are seriously interested in the computational properties of either kind of language, we will be interested in their language-theoretic properties, as well as properties of the grammars that define them and the parsers that accept them. The most important language-theoretic class considered by designers of programming languages, compilers, etc. is the context-free languages (CFL&apos;s). Ginsburg (1980, 7) goes so far as to say on behalf of formal language theorists, &amp;quot;We live or die on the context-free languages.&amp;quot;) The class of CFL&apos;s is very rich. Although there are simply definable languages well known to be non-CF, linguists often take CFL&apos;s to be non-CF in error. Several examples are cited in Pullum and Gazdar (1982). For another example, see Dowty, Wall and Peters (1980; p.81), where exercise 3 invites the reader to prove a certain artificial language nonCF. The exercise is impossible, for the language is a CFL, as noted by William H. Baxter (personal communication to Gerald Gazdar). Fr</context>
<context position="30964" citStr="Ginsburg 1980" startWordPosition="5196" endWordPosition="5197">goal. Anyone who settles for a cubic TCR bound may be settling for a theory a lot laser than it could be. (This accusation would be levellable equally at TG, lexical-functional grammar, Pollards generalized context-free grammar, and generalized phrase structure grammar as currently conceived.) Closer to what is called for would be a theory that defines human grammars as some proper subset of the ECP0 CF-PSG&apos;s that generate infinite, nonprof ligate, linear-time recognizable languages. Just as the description of ALGOL-60 in BNF formalism had a galvanizing effect on theoretical computer science (Ginsburg 1980, 6- 7), precise specification of a theory of this sort might sharpen quite considerably our view of the computational issues involved in natural language processing. And it would simultaneously be of considerable linguistic interest, at least for those who accept that we need a sharper theory of natural language than the vaguely-outlined decorative notations for Turing machines that are so often taken for theories in linguistics. ACKNOWLEDGEMENT I thank Chris Culy, Carl Pollard, Stuart Shieber, Tom Wasow, and Arnold Zwicky for useful conversations and helpful comments. The research reported h</context>
</contexts>
<marker>Ginsburg, 1980</marker>
<rawString>Ginsburg, S. (1980). Methods for specifying formal languages---past-present-future. In R. V. Book, ed., Formal Language Theory: Perspectives and Open Problems, 1-47. Academic Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>C J Pollard</author>
</authors>
<title>Generalized context-free grammars, head grammars, and natural language.</title>
<marker>Pollard, </marker>
<rawString>Pollard, C. J. (forthcoming). Generalized context-free grammars, head grammars, and natural language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G K Pullum</author>
</authors>
<title>Free word order and phrase structure rules.</title>
<date>1982</date>
<journal>NELS</journal>
<volume>12</volume>
<pages>209--220</pages>
<marker>Pullum, 1982</marker>
<rawString>Pullum, G. K. (1982). Free word order and phrase structure rules. NELS 12, 209-220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G K Pullum</author>
<author>G Gazdar</author>
</authors>
<title>Natural languages and context-free</title>
<date>1982</date>
<pages>4--471</pages>
<contexts>
<context position="3408" citStr="Pullum and Gazdar (1982)" startWordPosition="517" endWordPosition="520">erested in their language-theoretic properties, as well as properties of the grammars that define them and the parsers that accept them. The most important language-theoretic class considered by designers of programming languages, compilers, etc. is the context-free languages (CFL&apos;s). Ginsburg (1980, 7) goes so far as to say on behalf of formal language theorists, &amp;quot;We live or die on the context-free languages.&amp;quot;) The class of CFL&apos;s is very rich. Although there are simply definable languages well known to be non-CF, linguists often take CFL&apos;s to be non-CF in error. Several examples are cited in Pullum and Gazdar (1982). For another example, see Dowty, Wall and Peters (1980; p.81), where exercise 3 invites the reader to prove a certain artificial language nonCF. The exercise is impossible, for the language is a CFL, as noted by William H. Baxter (personal communication to Gerald Gazdar). From this point on, it will be useful to be able to refer to certain types of formal language by names. I shall use the terms defined in (1) thru (3), among others. (1) Triple Counting Languages: languages that can be mapped by a homomorphism onto some language of the form aa bn cnI n &gt; 1} (2) String Matching Languages: lang</context>
<context position="6004" citStr="Pullum and Gazdar 1982" startWordPosition="970" endWordPosition="973"> to keep track of declarations and uses of particular names. The bookkeeping has to be done, of course, even in the case of languages like LISP whose syntax does not demand a list of declarations at the start of each program. Various efforts have been made in the linguistic literature to show that some human language has an infinite, appropriately extractable subset that is a triple counting language or a string matching language. (By appropriately extractable I mean isolable via either homomorphism or intersection with a regular set.) But all the published claims of this sort are fallacious (Pullum and Gazdar 1982). This lends plausibility to the hypothesis that human languages are all CF. Stronger claims than this (e.g. that human languages are regular, or finite cardinality) have seldom seriously defended. I now want to propose one, however. I propose that human languages are never profligate CFL&apos;s in the sense given by the following definition. (i) A CFL is profligate if all CF-PSG&apos;s generating it have nonterminal vocabularies strictly larger than their terminal vocabularies. (ii) A CFL is profligate if it is the image of a profligate language under some homomorphism. [OPEN PROBLEM: Is profligacy dec</context>
<context position="8212" citStr="Pullum and Gazdar 1982" startWordPosition="1376" endWordPosition="1379">inct from the ith symbol of /. The interesting condition here is (c). The grammar has to generate, for all i and for all pairs &lt;u, v&gt; of symbols in the terminal vocabulary, all those strings in (a + b)*c(a + b)* such that the ith symbol is u and the ith symbol after c is v. There is no bound on i, so recursion has to be involved. But it must be recursion through a category that preserves a record of which symbol is crucially going to be deposited at the ith position in the terminal string and mismatched with a distinct symbol in the second half. A CF-PSG that does this can be constructed (see Pullum and Gazdar 1982, 478, for a grammar for a very similar language). But such a grammar has to use recursive nonterminals, one for each terminal, to carry down information about the symbol to be deposited at a certain point in the string. In the language just given there are only two relevant terminal symbols, but if there were a thousand symbols that could appear in the x and / strings, then the vocabulary of recursive nonterminals would have to be increased in proportion. (The second clause in the definition of profligacy makes it irrelevant whether there are other terminals in the language, like c in the lan</context>
<context position="10796" citStr="Pullum and Gazdar (1982" startWordPosition="1810" endWordPosition="1813">rogramming languages]. Chomsky took this as an argument for non-CF-ness in English, since he thought all string contrasting languages were non-CF (see Chomsky 1963, 378-379), 2 but it can be reinterpreted as an attempt to show that English is (at least) profligate. (It could even be reconstituted as a formally valid argument that English was non-CF if supplemented by a demonstration that the class of phrases from which the bracketed sequences are drawn is not only. infinite but non-regular; cf. Zwicky and Sadodk.) However, the argument clearly collapses on empirical grounds. As pointed out by Pullum and Gazdar (1982, 476-477), even Chomsky now agrees that strings like (5) are grammatical (though they need a contrastive context and the appropriate intonation to make them readily acceptable to informants). Hence these examples do not show that there is a homomorphism mapping English onto some profligate string contrasting language. The interesting thing about this, if it is correct, is that it suggests that human languages not only never demand the syntactic string comparison required by string matching languages, they never call for syntactic string camparision over infinite sets of strings at all, whethe</context>
</contexts>
<marker>Pullum, Gazdar, 1982</marker>
<rawString>Pullum, G. K. and Gazdar, G. (1982). Natural languages and context-free languages. Lim. and Phil. 4.471-504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Shieber</author>
</authors>
<title>Direct parsing of ID/LP grammars.</title>
<date>1983</date>
<location>Unpublished, SRI, Menlo Park, CA.</location>
<contexts>
<context position="15223" citStr="Shieber (1983" startWordPosition="2529" endWordPosition="2530">equate grammar possessing the Exhaustive Constant Partial Ordering (ECPO) property of Gazdar and Pullum (1981). A grammar has this property if there is a single partial ordering of the nonterminal vocabulary which no right hand side of any rule violates. The ECPO CF-PSG&apos;s are a nonempty proper subset of the CF-PSG&apos;s. The claim that human languages always have ECPO CF-PSG&apos;s is a claim about the strong generative capacity that an appropriate theory of human language should have--- one of the first such claims to have been seriously advanced, in fact. It does not affect weak generative capacity; Shieber (1983a) proves that every CFL has an ECPO grammar. It is always possible to construct an ECPO grammar for any CFL if one is willing to pay the price of inventing new nonterminals ad hoc to construct it. The content of the claim lies in the fact that linguists demand independent motivation for the nonterminals they postulate, so that the possibility of creating new ones just to guarantee ECPO-ness is not always a reasonable one. [OPEN PROBLEM: Could there be a non-profligate CFL which had #(N) &lt; #T (i.e. nonterminal vocabulary strictly mailer than terminal vocabulary) for at least one of its non-ECP</context>
<context position="16803" citStr="Shieber (1983" startWordPosition="2807" endWordPosition="2808">(so that lucky for us we found you had the form lucky we found you for us), the grammar of English would not have the ECPO property. But such facts appear not to turn up in the languages we know about. 3 The ECPO claim has interesting consequences relating to patterns of constituent order and how these can be described in a fully general way. If a grammar has the ECPO property, it can be stated in what Gazdar and Pullum call ID/LP format, and this renders numerous significant generalizations elegantly capturable. There are also some potentially interesting implications for parsing, studied by Shieber (1983a), who shows that a modified Earley algorithm can be used to parse ID/LP format grammars directly. One putative challenge to any claim that CFPSG&apos;s can be strongly adequate descriptions for human languages comes from Dutch and has been discussed recently by Bresnan, Kaplan, Peters, and Zaenen (1982). Dutch has constructions like (7) dat Jan Piet Marie zag leren zwemmen that Jan Piet Marie saw teach swim &apos;that Jan saw Piet teach Marie to swim&apos; These seem to involve crossing dependencies over a domain of potentially arbitrary length, a configuration that is syntactically not expressible by a CF</context>
<context position="27642" citStr="Shieber (1983" startWordPosition="4649" endWordPosition="4650">e that is recognized by a two-way deterministic pushdown stack automaton (2DPDA) is recognizable in linear time on a Turing machine. In the Appendix, I give an informal description of a 2DPDA that will recognize the language in (8). Given this, the proof that (8) is linearly recognizable is trivial. Thus even if my WGC and SGC conjectures were falsified by discoveries about free word order languages (which I consider that they have not been), there would still be no ground for tolerating theories of grammar and parsing that fail to impose a linear time bound on recognition. And recent work of Shieber (1983b) shows that there are interesting avenues in natural language parsing to be explored using deterministic context-free parsers that do work in linear time. In the light of the above remarks, some of the points made by Berwick and Weinberg look rather peculiar. For example, Berwick and Weinberg argue at length that things are really so complicated in practical implementations that a cubic bound on recognition time might not make much difference; for short sentences a theory that only guarantees an exponential time bound might do just as well. This is, to begin with, a very odd response to be m</context>
</contexts>
<marker>Shieber, 1983</marker>
<rawString>Shieber, S. M. (1983a). Direct parsing of ID/LP grammars. Unpublished, SRI, Menlo Park, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Shieber</author>
</authors>
<title>Sentence disambiguation by a shift-reduce parsing technique. In this volume.</title>
<date>1983</date>
<contexts>
<context position="15223" citStr="Shieber (1983" startWordPosition="2529" endWordPosition="2530">equate grammar possessing the Exhaustive Constant Partial Ordering (ECPO) property of Gazdar and Pullum (1981). A grammar has this property if there is a single partial ordering of the nonterminal vocabulary which no right hand side of any rule violates. The ECPO CF-PSG&apos;s are a nonempty proper subset of the CF-PSG&apos;s. The claim that human languages always have ECPO CF-PSG&apos;s is a claim about the strong generative capacity that an appropriate theory of human language should have--- one of the first such claims to have been seriously advanced, in fact. It does not affect weak generative capacity; Shieber (1983a) proves that every CFL has an ECPO grammar. It is always possible to construct an ECPO grammar for any CFL if one is willing to pay the price of inventing new nonterminals ad hoc to construct it. The content of the claim lies in the fact that linguists demand independent motivation for the nonterminals they postulate, so that the possibility of creating new ones just to guarantee ECPO-ness is not always a reasonable one. [OPEN PROBLEM: Could there be a non-profligate CFL which had #(N) &lt; #T (i.e. nonterminal vocabulary strictly mailer than terminal vocabulary) for at least one of its non-ECP</context>
<context position="16803" citStr="Shieber (1983" startWordPosition="2807" endWordPosition="2808">(so that lucky for us we found you had the form lucky we found you for us), the grammar of English would not have the ECPO property. But such facts appear not to turn up in the languages we know about. 3 The ECPO claim has interesting consequences relating to patterns of constituent order and how these can be described in a fully general way. If a grammar has the ECPO property, it can be stated in what Gazdar and Pullum call ID/LP format, and this renders numerous significant generalizations elegantly capturable. There are also some potentially interesting implications for parsing, studied by Shieber (1983a), who shows that a modified Earley algorithm can be used to parse ID/LP format grammars directly. One putative challenge to any claim that CFPSG&apos;s can be strongly adequate descriptions for human languages comes from Dutch and has been discussed recently by Bresnan, Kaplan, Peters, and Zaenen (1982). Dutch has constructions like (7) dat Jan Piet Marie zag leren zwemmen that Jan Piet Marie saw teach swim &apos;that Jan saw Piet teach Marie to swim&apos; These seem to involve crossing dependencies over a domain of potentially arbitrary length, a configuration that is syntactically not expressible by a CF</context>
<context position="27642" citStr="Shieber (1983" startWordPosition="4649" endWordPosition="4650">e that is recognized by a two-way deterministic pushdown stack automaton (2DPDA) is recognizable in linear time on a Turing machine. In the Appendix, I give an informal description of a 2DPDA that will recognize the language in (8). Given this, the proof that (8) is linearly recognizable is trivial. Thus even if my WGC and SGC conjectures were falsified by discoveries about free word order languages (which I consider that they have not been), there would still be no ground for tolerating theories of grammar and parsing that fail to impose a linear time bound on recognition. And recent work of Shieber (1983b) shows that there are interesting avenues in natural language parsing to be explored using deterministic context-free parsers that do work in linear time. In the light of the above remarks, some of the points made by Berwick and Weinberg look rather peculiar. For example, Berwick and Weinberg argue at length that things are really so complicated in practical implementations that a cubic bound on recognition time might not make much difference; for short sentences a theory that only guarantees an exponential time bound might do just as well. This is, to begin with, a very odd response to be m</context>
</contexts>
<marker>Shieber, 1983</marker>
<rawString>Shieber, S. M. (1983b). Sentence disambiguation by a shift-reduce parsing technique. In this volume.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Slocum</author>
</authors>
<title>A status report on the LRC machine translation system.</title>
<date>1983</date>
<booktitle>Conf. on Applied Nat. Lang. Proc. 166-173. ACL,</booktitle>
<location>Menlo Park, CA.</location>
<contexts>
<context position="28996" citStr="Slocum 1983" startWordPosition="4870" endWordPosition="4872">me complexity of the Travelling Salesman problem, and was met by the response that real-life travelling salesmen do not visit very many cities before returning to head office, I think theoretical computer scientists would have a right to be amused. Likewise, it is funny to see practical implementation considerations brought to bear in defending TG against the phrase structure backlash, when (a) no formalized version of modern TG exists, let alone being available for implementation, and (b) large phrase structure grammars. are being implemented on computers and shown to run very fast (see e.g. Slocum 1983, who reports an all-paths, bottom-up parser actually running in linear time using a CFPSG with 400 rules and 10,000 lexical entries). Berwick and Weinberg seem to imply that data permitting a comparison of CF-PSG with TG are available. This is quite untrue, as far as I know. I therefore find it nothing short of astonishing to find Chomsky (1981, 234), taking a very similar position, affirming that because the size of the 5 grammar is a constant factor in TCR calculations, and possibly a large one, The real empirical content of existing results... may well-be that grammars are preferred if the</context>
</contexts>
<marker>Slocum, 1983</marker>
<rawString>Slocum, J. (1983). A status report on the LRC machine translation system. Conf. on Applied Nat. Lang. Proc. 166-173. ACL, Menlo Park, CA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A M Zwicky</author>
<author>J M</author>
</authors>
<title>Sadock (forthcoming). A note on Ay languages.</title>
<note>Submitted to Ling. and Phil.</note>
<marker>Zwicky, M, </marker>
<rawString>Zwicky, A. M. and J. M. Sadock (forthcoming). A note on Ay languages. Submitted to Ling. and Phil.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>