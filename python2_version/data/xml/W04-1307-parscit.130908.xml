<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.294345">
<title confidence="0.99672">
Statistics Learning and Universal Grammar:
Modeling Word Segmentation
</title>
<author confidence="0.927235">
Timothy Gambell
</author>
<address confidence="0.895645333333333">
59 Bishop Street
New Haven, CT 06511
USA
</address>
<email confidence="0.999201">
timothy.gambell@aya.yale.edu
</email>
<author confidence="0.994675">
Charles Yang
</author>
<affiliation confidence="0.996718">
Department of Linguistics, Yale University
</affiliation>
<address confidence="0.958763">
New Haven, CT 06511
USA
</address>
<email confidence="0.998841">
charles.yale.edu@yale.edu
</email>
<sectionHeader confidence="0.993884" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999786833333333">
This paper describes a computational model of word
segmentation and presents simulation results on re-
alistic acquisition In particular, we explore the ca-
pacity and limitations of statistical learning mecha-
nisms that have recently gained prominence in cog-
nitive psychology and linguistics.
</bodyText>
<sectionHeader confidence="0.999267" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999987171052632">
Two facts about language learning are indisputable.
First, only a human baby, but not her pet kitten, can
learn a language. It is clear, then, that there must
be some element in our biology that accounts for
this unique ability. Chomsky’s Universal Grammar
(UG), an innate form of knowledge specific to lan-
guage, is an account of what this ability is. This po-
sition gains support from formal learning theory [1-
3], which sharpens the logical conclusion [4,5] that
no (realistically efficient) learning is possible with-
out priori restrictions on the learning space. Sec-
ond, it is also clear that no matter how much of a
head start the child has through UG, language is
learned. Phonology, lexicon, and grammar, while
governed by universal principles and constraints, do
vary from language to language, and they must be
learned on the basis of linguistic experience. In
other words–indeed a truism–both endowment and
learning contribute to language acquisition, the re-
sult of which is extremely sophisticated body of
linguistic knowledge. Consequently, both must be
taken in account, explicitly, in a theory of language
acquisition [6,7].
Controversies arise when it comes to the relative
contributions by innate knowledge and experience-
based learning. Some researchers, in particular lin-
guists, approach language acquisition by charac-
terizing the scope and limits of innate principles
of Universal Grammar that govern the world’s lan-
guage. Others, in particular psychologists, tend to
emphasize the role of experience and the child’s
domain-general learning ability. Such division of
research agenda understandably stems from the di-
vision of labor between endowment and learning–
plainly, things that are built in needn’t be learned,
and things that can be garnered from experience
needn’t be built in.
The important paper of Saffran, Aslin, &amp; New-
port [8] on statistical learning (SL), suggests that
children may be powerful learners after all. Very
young infants can exploit transitional probabilities
between syllables for the task of word segmenta-
tion, with only minimum exposure to an artificial
language. Subsequent work has demonstrated SL
in other domains including artificial grammar learn-
ing [9], music [10], vision [11], as well as in other
species [12]. This then raises the possibility of
learning as an alternative to the innate endowment
of linguistic knowledge [13].
We believe that the computational modeling of
psychological processes, with special attention to
concrete mechanisms and quantitative evaluations,
can play an important role in the endowment vs.
learning debate. Linguists’ investigations of UG are
rarely developmental, even less so corpus-oriented.
Developmental psychologists, by contrast, often
stop at identifying components in a cognitive task
[14], without an account of how such components
work together in an algorithmic manner. On the
other hand, if computation is to be of relevance
to linguistics, psychology, and cognitive science in
general, being merely computational will not suf-
fice. A model must be psychological plausible, and
ready to face its implications in the broad empirical
contexts [7]. For example, how does it generalize
to typologically different languages? How does the
model’s behavior compare with that of human lan-
guage learners and processors?
In this article, we will present a simple compu-
tational model of word segmentation and some of
its formal and developmental issues in child lan-
guage acquisition. Specifically we show that SL
using transitional probabilities cannot reliably seg-
ment words when scaled to a realistic setting (e.g.,
child-directed English). To be successful, it must
be constrained by the knowledge of phonological
</bodyText>
<page confidence="0.996742">
49
</page>
<bodyText confidence="0.999985094339623">
structure. Indeed, the model reveals that SL may 50 CV syllables, many languages, including English,
well be an artifact–an impressive one, nonetheless– make use of a far more diverse range of syllabic
that plays no role in actual word segmentation in types. And then, syllabification of speech is far
human children. from trivial, which (most likely) involve both in-
2 Statistics does not Refute UG nate knowledge of phonological structures as well
It has been suggested [15, 8] that word segmenta- as discovering language-specific instantiations [14].
tion from continuous speech may be achieved by All these problems have to be solved before SL for
using transitional probabilities (TP) between ad- word segmentation can take place.
jacent syllables A and B, where , TP(A—*B) = 3 The Model
P(AB)/P(A), with P(AB) being the frequency of B To give a precise evaluation of SL in a realis-
following A, and P(A) the total frequency of A. tic setting, we constructed a series of (embarrass-
Word boundaries are postulated at local minima, ingly simple) computational models tested on child-
where the TP is lower than its neighbors. For ex- directed English.
ample, given sufficient amount of exposure to En- The learning data consists of a random sam-
glish, the learner may establish that, in the four- ple of child-directed English sentences from the
syllable sequence “prettybaby”, TP(pre—*tty) and CHILDES database [19] The words were then pho-
TP(ba—*by) are both higher than TP(tty—*ba): a netically transcribed using the Carnegie Mellon Pro-
word boundary can be (correctly) postulated. It nunciation Dictionary, and were then grouped into
is remarkable that 8-month-old infants can extract syllables. Spaces between words are removed; how-
three-syllable words in the continuous speech of an ever, utterance breaks are available to the modeled
artificial language from only two minutes of expo- learner. Altogether, there are 226,178 words, con-
sure [8]. sisting of 263,660 syllables.
To be effective, a learning algorithm–indeed any Implementing SL-based segmentation is straight-
algorithm–must have an appropriate representation forward. One first gathers pair-wise TPs from the
of the relevant learning data. We thus need to be training data, which are used to identify local min-
cautious about the interpretation of the success of ima and postulate word boundaries in the on-line
SL, as the authors themselves note [16]. If any- processing of syllable sequences. Scoring is done
thing, it seems that the findings strengthen, rather for each utterance and then averaged. Viewed as an
than weaken, the case for (innate) linguistic knowl- information retrieval problem, it is customary [20]
edge. A classic argument for innateness [4, 5, to report both precision and recall of the perfor-
17] comes from the fact that syntactic operations mance.
are defined over specific types of data structures– The segmentation results using TP local minima
constituents and phrases–but not over, say, linear are remarkably poor, even under the assumption
strings of words, or numerous other logical possibil- that the learner has already syllabified the input per-
ities. While infants seem to keep track of statistical fectly. Precision is 41.6%, and recall is 23.3%; over
information, any conclusion drawn from such find- half of the words extracted by the model are not ac-
ings must presuppose children knowing what kind tual English words, while close to 80% of actual
of statistical information to keep track of. After all, words fail to be extracted. And it is straightfor-
an infinite range of statistical correlations exists in ward why this is the case. In order for SL to be
the acoustic input: e.g., What is the probability of a effective, a TP at an actual word boundary must
syllable rhyming with the next? What is the proba- be lower than its neighbors. Obviously, this con-
bility of two adjacent vowels being both nasal? The dition cannot be met if the input is a sequence of
fact that infants can use SL to segment syllable se- monosyllabic words, for which a space must be pos-
quences at all entails that, at the minimum, they tulated for every syllable; there are no local min-
know the relevant unit of information over which ima to speak of. While the pseudowords in [8]
correlative statistics is gathered: in this case, it is are uniformly three-syllables long, much of child-
the syllables, rather than segments, or front vowels. directed English consists of sequences of monosyl-
A host of questions then arises. First, How do labic words: corpus statistics reveals that on aver-
they know so? It is quite possible that the primacy age, a monosyllabic word is followed by another
of syllables as the basic unit of speech is innately monosyllabic word 85% of time. As long as this
available, as suggested in neonate speech perception is the case, SL cannot, in principle, work.
studies [18]? Second, where do the syllables come
from? While the experiments in [8] used uniformly
</bodyText>
<sectionHeader confidence="0.991118" genericHeader="method">
4 Statistics Needs UG
</sectionHeader>
<bodyText confidence="0.999985927272728">
This is not to say that SL cannot be effective
for word segmentation. Its application, must be
constrained–like that of any learning algorithm
however powerful–as suggested by formal learning
theories [1-3]. The performance improves dramat-
ically, in fact, if the learner is equipped with even
a small amount of prior knowledge about phono-
logical structures. Specifically, we assume, uncon-
troversially, that each word can have only one pri-
mary stress. (This would not work for functional
words, however.) If the learner knows this, then
it may limit the search for local minima only in
the window between two syllables that both bear
primary stress, e.g., between the two a’s in the
sequence “languageacquisition”. This assumption
is plausible given that 7.5-month-old infants are
sensitive to strong/weak prosodic distinctions [14].
When stress information suffices, no SL is em-
ployed, so “bigbadwolf” breaks into three words
for free. Once this simple principle is built in, the
stress-delimited SL algorithm can achieve the pre-
cision of 73.5% and 71.2%, which compare favor-
ably to the best performance reported in the litera-
ture [20]. (That work, however, uses an computa-
tionally prohibitive and psychological implausible
algorithm that iteratively optimizes the entire lexi-
con.)
The computational models complement the ex-
perimental study that prosodic information takes
priority over statistical information when both are
available [21]. Yet again one needs to be cautious
about the improved performance, and a number of
unresolved issues need to be addressed by future
work. It remains possible that SL is not used at
all in actual word segmentation. Once the one-
word-one-stress principle is built in, we may con-
sider a model that does not use any statistics, hence
avoiding the computational cost that is likely to
be considerable. (While we don’t know how in-
fants keep track of TPs, there are clearly quite some
work to do. Syllables in English number in the
thousands; now take the quadratic for the potential
number of pair-wise TPs.) It simply stores previ-
ously extracted words in the memory to bootstrap
new words. Young children’s familiar segmenta-
tion errors–”I was have” from be-have, “hiccing up”
from hicc-up, “two dults”, from a-dult–suggest that
this process does take place. Moreover, there is ev-
idence that 8-month-old infants can store familiar
sounds in the memory [22]. And finally, there are
plenty of single-word utterances–up to 10% [23]–
that give many words for free. The implementation
of a purely symbolic learner that recycles known
words yields even better performance: a precision
of 81.5% and recall of 90.1%.
</bodyText>
<sectionHeader confidence="0.989403" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9985155">
Further work, both experimental and computational,
will need to address a few pressing questions, in or-
der to gain a better assessment of the relative contri-
bution of SL and UG to language acquisition. These
include, more pertinent to the problem of word seg-
mentation:
</bodyText>
<listItem confidence="0.9554154375">
• Can statistical learning be used in the acquisi-
tion of language-specific phonotactics, a pre-
requisite to syllabification and a prelude to
word segmentation?
• Given that prosodic constraints are critical for
the success of SL in word segmentation, future
work needs to quantify the availability of stress
information in spoken corpora.
• Can further experiments, carried over realistic
linguistic input, further tease apart the multi-
ple strategies used in word segmentation [14]?
What are the psychological mechanisms (algo-
rithms) that integrate these strategies?
• How does word segmentation, statistical or
otherwise, work for agglutinative (e.g., Turk-
ish) and polysynthetic languages (e.g. Mo-
</listItem>
<bodyText confidence="0.98514752631579">
hawk), where the division between words,
morphology, and syntax is quite different from
more clear-cut cases like English?
Computational modeling can make explicit the
balance between statistics and UG, and are in the
same vein as the recent findings [24] on when/where
SL is effective/possible. UG can help SL by
providing specific constraints on its application,
and modeling may raise new questions for fur-
ther experimental studies. In related work [6,7],
we have augmented traditional theories of UG–
derivational phonology, and the Principles and Pa-
rameters framework–with a component of statisti-
cal learning, with novel and desirable consequences.
Yet in all cases, statistical learning, while perhaps
domain-general, is constrained by what appears to
be innate and domain-specific knowledge of linguis-
tic structures, such that learning can operate on spe-
cific aspects of the input evidence
</bodyText>
<sectionHeader confidence="0.9984" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99915625">
1. Gold, E. M. (1967). Language identification in
the limit. Information and Control, 10:447-74.
2. Valiant, L. (1984). A theory of the learnable.
Communication of the ACM. 1134-1142.
</reference>
<page confidence="0.991674">
51
</page>
<reference confidence="0.999932757142857">
3. Vapnik, V. (1995). The Nature of Statistical
Learning Theory. Berlin: Springer.
4. Chomsky, N. (1959). Review of Verbal Behav-
ior by B.F. Skinner. Language, 35, 26-57.
5. Chomsky, N. (1975). Reflections on Language.
New York: Pantheon.
6. Yang, C. D. (1999). A selectionist theory of
language development. In Proceedings of 37th
Meeting of the Association for Computational
Linguistics. Maryland, MD. 431-5.
7. Yang, C. D. (2002). Knowledge and Learning
in Natural Language. Oxford: Oxford Univer-
sity Press.
8. Saffran, J.R., Aslin, R.N., &amp; Newport, E.L.
(1996). Statistical learning by 8-month old in-
fants. Science, 274, 1926-1928.
9. Gomez, R.L., &amp; Gerken, L.A. (1999). Artifi-
cial grammar learning by one-year-olds leads
to specific and abstract knowledge. Cognition,
70, 109-135.
10. Saffran, J.R., Johnson, E.K., Aslin R.N. &amp;
Newport, E.L. (1999). Statistical learning of
tone sequences by human infants and adults.
Cognition, 70, 27-52.
11. Fiser, J., &amp; Aslin, R.N. (2002). Statistical
learning of new visual feature combinations by
infants. PNAS, 99, 15822-6.
12. Hauser, M., Newport, E.L., &amp; Aslin, R.N.
(2001). Segmentation of the speech stream in
a non-human primate: Statistical learning in
cotton-top tamarins. Cognition, 78, B41-B52.
13. Bates, E., &amp; Elman, J. (1996). Learning redis-
covered. Science, 274, 1849-50.
14. Jusczyk, P.W. (1999). How infants begin to ex-
tract words from speech. Trends in Cognitive
Sciences, 3, 323-8.
15. Chomsky, N. (1955/1975). The Logical Struc-
ture of Linguistic Theory. Manuscript, Har-
vard University and Massachusetts Institute of
Technology. Published in 1975 by New York:
Plenum.
16. Saffran, J.R., Aslin, R.N., &amp; Newport, E.L.
(1997). Letters. Science, 276, 1177-1181
17. Crain, S., &amp; Nakayama, M. (1987). Structure
dependency in grammar formation. Language,
63:522-543.
18. Bijeljiac-Babic, R., Bertoncini, J., &amp; Mehler,
J. (1993). How do four-day-old infants cate-
gorize multisyllabic utterances. Developmen-
tal psychology, 29, 711-21.
19. MacWhinney, B. (1995). The CHILDES
Project: Tools for Analyzing Talk. Hillsdale:
Lawrence Erlbaum.
20. Brent, M. (1999). Speech segmentation and
word discovery: a computational perspective.
Trends in Cognitive Science, 3, 294-301.
21. Johnson, E.K. &amp; Jusczyk, P.W. (2001) Word
segmentation by 8-month-olds: When speech
cues count more than statistics. Journal of
Memory and Language, 44, 1-20.
22. Jusczyk, P. W., &amp; Hohne, E. A. (1997). In-
fants’ memory for spoken words. Science, 277,
1984-6.
23. Brent, M.R., &amp; Siskind, J.M. (2001). The role
of exposure to isolated words in early vocabu-
lary development. Cognition, 81, B33-44.
24. Newport, E.L., &amp; Aslin, R.N. (2004). Learn-
ing at a distance: I. Statistical learning of non-
adjacent dependencies. Cognitive Psychology,
48, 127-62.
</reference>
<page confidence="0.998858">
52
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.229302">
<title confidence="0.9955115">Statistics Learning and Universal Modeling Word Segmentation</title>
<author confidence="0.681642333333333">Timothy Bishop New Haven</author>
<author confidence="0.681642333333333">CT</author>
<email confidence="0.996704">timothy.gambell@aya.yale.edu</email>
<author confidence="0.823283">Charles</author>
<affiliation confidence="0.901473">Department of Linguistics, Yale</affiliation>
<address confidence="0.639027">New Haven, CT</address>
<email confidence="0.996217">charles.yale.edu@yale.edu</email>
<abstract confidence="0.994657">This paper describes a computational model of word segmentation and presents simulation results on realistic acquisition In particular, we explore the capacity and limitations of statistical learning mechanisms that have recently gained prominence in cognitive psychology and linguistics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E M Gold</author>
</authors>
<title>Language identification in the limit.</title>
<date>1967</date>
<journal>Information and Control,</journal>
<pages>10--447</pages>
<contexts>
<context position="9476" citStr="[1, 2, 3]" startWordPosition="1489" endWordPosition="1489">quite possible that the primacy age, a monosyllabic word is followed by another of syllables as the basic unit of speech is innately monosyllabic word 85% of time. As long as this available, as suggested in neonate speech perception is the case, SL cannot, in principle, work. studies [18]? Second, where do the syllables come from? While the experiments in [8] used uniformly 4 Statistics Needs UG This is not to say that SL cannot be effective for word segmentation. Its application, must be constrained–like that of any learning algorithm however powerful–as suggested by formal learning theories [1, 2, 3]. The performance improves dramatically, in fact, if the learner is equipped with even a small amount of prior knowledge about phonological structures. Specifically, we assume, uncontroversially, that each word can have only one primary stress. (This would not work for functional words, however.) If the learner knows this, then it may limit the search for local minima only in the window between two syllables that both bear primary stress, e.g., between the two a’s in the sequence “languageacquisition”. This assumption is plausible given that 7.5-month-old infants are sensitive to strong/weak p</context>
</contexts>
<marker>1.</marker>
<rawString>Gold, E. M. (1967). Language identification in the limit. Information and Control, 10:447-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Valiant</author>
</authors>
<title>A theory of the learnable.</title>
<date>1984</date>
<journal>Communication of the ACM.</journal>
<pages>1134--1142</pages>
<contexts>
<context position="9476" citStr="[1, 2, 3]" startWordPosition="1489" endWordPosition="1489">quite possible that the primacy age, a monosyllabic word is followed by another of syllables as the basic unit of speech is innately monosyllabic word 85% of time. As long as this available, as suggested in neonate speech perception is the case, SL cannot, in principle, work. studies [18]? Second, where do the syllables come from? While the experiments in [8] used uniformly 4 Statistics Needs UG This is not to say that SL cannot be effective for word segmentation. Its application, must be constrained–like that of any learning algorithm however powerful–as suggested by formal learning theories [1, 2, 3]. The performance improves dramatically, in fact, if the learner is equipped with even a small amount of prior knowledge about phonological structures. Specifically, we assume, uncontroversially, that each word can have only one primary stress. (This would not work for functional words, however.) If the learner knows this, then it may limit the search for local minima only in the window between two syllables that both bear primary stress, e.g., between the two a’s in the sequence “languageacquisition”. This assumption is plausible given that 7.5-month-old infants are sensitive to strong/weak p</context>
</contexts>
<marker>2.</marker>
<rawString>Valiant, L. (1984). A theory of the learnable. Communication of the ACM. 1134-1142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer.</publisher>
<location>Berlin:</location>
<contexts>
<context position="9476" citStr="[1, 2, 3]" startWordPosition="1489" endWordPosition="1489">quite possible that the primacy age, a monosyllabic word is followed by another of syllables as the basic unit of speech is innately monosyllabic word 85% of time. As long as this available, as suggested in neonate speech perception is the case, SL cannot, in principle, work. studies [18]? Second, where do the syllables come from? While the experiments in [8] used uniformly 4 Statistics Needs UG This is not to say that SL cannot be effective for word segmentation. Its application, must be constrained–like that of any learning algorithm however powerful–as suggested by formal learning theories [1, 2, 3]. The performance improves dramatically, in fact, if the learner is equipped with even a small amount of prior knowledge about phonological structures. Specifically, we assume, uncontroversially, that each word can have only one primary stress. (This would not work for functional words, however.) If the learner knows this, then it may limit the search for local minima only in the window between two syllables that both bear primary stress, e.g., between the two a’s in the sequence “languageacquisition”. This assumption is plausible given that 7.5-month-old infants are sensitive to strong/weak p</context>
</contexts>
<marker>3.</marker>
<rawString>Vapnik, V. (1995). The Nature of Statistical Learning Theory. Berlin: Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Review of Verbal Behavior by B.F.</title>
<date>1959</date>
<journal>Skinner. Language,</journal>
<volume>35</volume>
<pages>26--57</pages>
<contexts>
<context position="1034" citStr="[4,5]" startWordPosition="154" endWordPosition="154">y and limitations of statistical learning mechanisms that have recently gained prominence in cognitive psychology and linguistics. 1 Introduction Two facts about language learning are indisputable. First, only a human baby, but not her pet kitten, can learn a language. It is clear, then, that there must be some element in our biology that accounts for this unique ability. Chomsky’s Universal Grammar (UG), an innate form of knowledge specific to language, is an account of what this ability is. This position gains support from formal learning theory [1- 3], which sharpens the logical conclusion [4,5] that no (realistically efficient) learning is possible without priori restrictions on the learning space. Second, it is also clear that no matter how much of a head start the child has through UG, language is learned. Phonology, lexicon, and grammar, while governed by universal principles and constraints, do vary from language to language, and they must be learned on the basis of linguistic experience. In other words–indeed a truism–both endowment and learning contribute to language acquisition, the result of which is extremely sophisticated body of linguistic knowledge. Consequently, both mu</context>
</contexts>
<marker>4.</marker>
<rawString>Chomsky, N. (1959). Review of Verbal Behavior by B.F. Skinner. Language, 35, 26-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Reflections on Language.</title>
<date>1975</date>
<publisher>Pantheon.</publisher>
<location>New York:</location>
<contexts>
<context position="1034" citStr="[4,5]" startWordPosition="154" endWordPosition="154">y and limitations of statistical learning mechanisms that have recently gained prominence in cognitive psychology and linguistics. 1 Introduction Two facts about language learning are indisputable. First, only a human baby, but not her pet kitten, can learn a language. It is clear, then, that there must be some element in our biology that accounts for this unique ability. Chomsky’s Universal Grammar (UG), an innate form of knowledge specific to language, is an account of what this ability is. This position gains support from formal learning theory [1- 3], which sharpens the logical conclusion [4,5] that no (realistically efficient) learning is possible without priori restrictions on the learning space. Second, it is also clear that no matter how much of a head start the child has through UG, language is learned. Phonology, lexicon, and grammar, while governed by universal principles and constraints, do vary from language to language, and they must be learned on the basis of linguistic experience. In other words–indeed a truism–both endowment and learning contribute to language acquisition, the result of which is extremely sophisticated body of linguistic knowledge. Consequently, both mu</context>
</contexts>
<marker>5.</marker>
<rawString>Chomsky, N. (1975). Reflections on Language. New York: Pantheon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Yang</author>
</authors>
<title>A selectionist theory of language development.</title>
<date>1999</date>
<booktitle>In Proceedings of 37th Meeting of the Association for Computational Linguistics.</booktitle>
<pages>431--5</pages>
<location>Maryland, MD.</location>
<contexts>
<context position="1711" citStr="[6,7]" startWordPosition="259" endWordPosition="259">estrictions on the learning space. Second, it is also clear that no matter how much of a head start the child has through UG, language is learned. Phonology, lexicon, and grammar, while governed by universal principles and constraints, do vary from language to language, and they must be learned on the basis of linguistic experience. In other words–indeed a truism–both endowment and learning contribute to language acquisition, the result of which is extremely sophisticated body of linguistic knowledge. Consequently, both must be taken in account, explicitly, in a theory of language acquisition [6,7]. Controversies arise when it comes to the relative contributions by innate knowledge and experiencebased learning. Some researchers, in particular linguists, approach language acquisition by characterizing the scope and limits of innate principles of Universal Grammar that govern the world’s language. Others, in particular psychologists, tend to emphasize the role of experience and the child’s domain-general learning ability. Such division of research agenda understandably stems from the division of labor between endowment and learning– plainly, things that are built in needn’t be learned, an</context>
</contexts>
<marker>6.</marker>
<rawString>Yang, C. D. (1999). A selectionist theory of language development. In Proceedings of 37th Meeting of the Association for Computational Linguistics. Maryland, MD. 431-5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Yang</author>
</authors>
<date>2002</date>
<booktitle>Knowledge and Learning in Natural Language.</booktitle>
<publisher>University Press.</publisher>
<location>Oxford: Oxford</location>
<contexts>
<context position="1711" citStr="[6,7]" startWordPosition="259" endWordPosition="259">estrictions on the learning space. Second, it is also clear that no matter how much of a head start the child has through UG, language is learned. Phonology, lexicon, and grammar, while governed by universal principles and constraints, do vary from language to language, and they must be learned on the basis of linguistic experience. In other words–indeed a truism–both endowment and learning contribute to language acquisition, the result of which is extremely sophisticated body of linguistic knowledge. Consequently, both must be taken in account, explicitly, in a theory of language acquisition [6,7]. Controversies arise when it comes to the relative contributions by innate knowledge and experiencebased learning. Some researchers, in particular linguists, approach language acquisition by characterizing the scope and limits of innate principles of Universal Grammar that govern the world’s language. Others, in particular psychologists, tend to emphasize the role of experience and the child’s domain-general learning ability. Such division of research agenda understandably stems from the division of labor between endowment and learning– plainly, things that are built in needn’t be learned, an</context>
<context position="3718" citStr="[7]" startWordPosition="559" endWordPosition="559"> important role in the endowment vs. learning debate. Linguists’ investigations of UG are rarely developmental, even less so corpus-oriented. Developmental psychologists, by contrast, often stop at identifying components in a cognitive task [14], without an account of how such components work together in an algorithmic manner. On the other hand, if computation is to be of relevance to linguistics, psychology, and cognitive science in general, being merely computational will not suffice. A model must be psychological plausible, and ready to face its implications in the broad empirical contexts [7]. For example, how does it generalize to typologically different languages? How does the model’s behavior compare with that of human language learners and processors? In this article, we will present a simple computational model of word segmentation and some of its formal and developmental issues in child language acquisition. Specifically we show that SL using transitional probabilities cannot reliably segment words when scaled to a realistic setting (e.g., child-directed English). To be successful, it must be constrained by the knowledge of phonological 49 structure. Indeed, the model reveal</context>
</contexts>
<marker>7.</marker>
<rawString>Yang, C. D. (2002). Knowledge and Learning in Natural Language. Oxford: Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Saffran</author>
<author>R N Aslin</author>
<author>E L Newport</author>
</authors>
<title>Statistical learning by 8-month old infants.</title>
<date>1996</date>
<journal>Science,</journal>
<volume>274</volume>
<pages>1926--1928</pages>
<contexts>
<context position="2430" citStr="[8]" startWordPosition="368" endWordPosition="368">Some researchers, in particular linguists, approach language acquisition by characterizing the scope and limits of innate principles of Universal Grammar that govern the world’s language. Others, in particular psychologists, tend to emphasize the role of experience and the child’s domain-general learning ability. Such division of research agenda understandably stems from the division of labor between endowment and learning– plainly, things that are built in needn’t be learned, and things that can be garnered from experience needn’t be built in. The important paper of Saffran, Aslin, &amp; Newport [8] on statistical learning (SL), suggests that children may be powerful learners after all. Very young infants can exploit transitional probabilities between syllables for the task of word segmentation, with only minimum exposure to an artificial language. Subsequent work has demonstrated SL in other domains including artificial grammar learning [9], music [10], vision [11], as well as in other species [12]. This then raises the possibility of learning as an alternative to the innate endowment of linguistic knowledge [13]. We believe that the computational modeling of psychological processes, wi</context>
<context position="4761" citStr="[15, 8]" startWordPosition="723" endWordPosition="724">a realistic setting (e.g., child-directed English). To be successful, it must be constrained by the knowledge of phonological 49 structure. Indeed, the model reveals that SL may 50 CV syllables, many languages, including English, well be an artifact–an impressive one, nonetheless– make use of a far more diverse range of syllabic that plays no role in actual word segmentation in types. And then, syllabification of speech is far human children. from trivial, which (most likely) involve both in2 Statistics does not Refute UG nate knowledge of phonological structures as well It has been suggested [15, 8] that word segmenta- as discovering language-specific instantiations [14]. tion from continuous speech may be achieved by All these problems have to be solved before SL for using transitional probabilities (TP) between ad- word segmentation can take place. jacent syllables A and B, where , TP(A—*B) = 3 The Model P(AB)/P(A), with P(AB) being the frequency of B To give a precise evaluation of SL in a realisfollowing A, and P(A) the total frequency of A. tic setting, we constructed a series of (embarrassWord boundaries are postulated at local minima, ingly simple) computational models tested on c</context>
<context position="6227" citStr="[8]" startWordPosition="954" endWordPosition="954">e syllable sequence “prettybaby”, TP(pre—*tty) and CHILDES database [19] The words were then phoTP(ba—*by) are both higher than TP(tty—*ba): a netically transcribed using the Carnegie Mellon Proword boundary can be (correctly) postulated. It nunciation Dictionary, and were then grouped into is remarkable that 8-month-old infants can extract syllables. Spaces between words are removed; howthree-syllable words in the continuous speech of an ever, utterance breaks are available to the modeled artificial language from only two minutes of expo- learner. Altogether, there are 226,178 words, consure [8]. sisting of 263,660 syllables. To be effective, a learning algorithm–indeed any Implementing SL-based segmentation is straightalgorithm–must have an appropriate representation forward. One first gathers pair-wise TPs from the of the relevant learning data. We thus need to be training data, which are used to identify local mincautious about the interpretation of the success of ima and postulate word boundaries in the on-line SL, as the authors themselves note [16]. If any- processing of syllable sequences. Scoring is done thing, it seems that the findings strengthen, rather for each utterance </context>
<context position="8540" citStr="[8]" startWordPosition="1338" endWordPosition="1338">e the acoustic input: e.g., What is the probability of a effective, a TP at an actual word boundary must syllable rhyming with the next? What is the proba- be lower than its neighbors. Obviously, this conbility of two adjacent vowels being both nasal? The dition cannot be met if the input is a sequence of fact that infants can use SL to segment syllable se- monosyllabic words, for which a space must be posquences at all entails that, at the minimum, they tulated for every syllable; there are no local minknow the relevant unit of information over which ima to speak of. While the pseudowords in [8] correlative statistics is gathered: in this case, it is are uniformly three-syllables long, much of childthe syllables, rather than segments, or front vowels. directed English consists of sequences of monosylA host of questions then arises. First, How do labic words: corpus statistics reveals that on averthey know so? It is quite possible that the primacy age, a monosyllabic word is followed by another of syllables as the basic unit of speech is innately monosyllabic word 85% of time. As long as this available, as suggested in neonate speech perception is the case, SL cannot, in principle, wo</context>
</contexts>
<marker>8.</marker>
<rawString>Saffran, J.R., Aslin, R.N., &amp; Newport, E.L. (1996). Statistical learning by 8-month old infants. Science, 274, 1926-1928.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R L Gomez</author>
<author>L A Gerken</author>
</authors>
<title>Artificial grammar learning by one-year-olds leads to specific and abstract knowledge.</title>
<date>1999</date>
<journal>Cognition,</journal>
<volume>70</volume>
<pages>109--135</pages>
<contexts>
<context position="2779" citStr="[9]" startWordPosition="419" endWordPosition="419">nderstandably stems from the division of labor between endowment and learning– plainly, things that are built in needn’t be learned, and things that can be garnered from experience needn’t be built in. The important paper of Saffran, Aslin, &amp; Newport [8] on statistical learning (SL), suggests that children may be powerful learners after all. Very young infants can exploit transitional probabilities between syllables for the task of word segmentation, with only minimum exposure to an artificial language. Subsequent work has demonstrated SL in other domains including artificial grammar learning [9], music [10], vision [11], as well as in other species [12]. This then raises the possibility of learning as an alternative to the innate endowment of linguistic knowledge [13]. We believe that the computational modeling of psychological processes, with special attention to concrete mechanisms and quantitative evaluations, can play an important role in the endowment vs. learning debate. Linguists’ investigations of UG are rarely developmental, even less so corpus-oriented. Developmental psychologists, by contrast, often stop at identifying components in a cognitive task [14], without an accoun</context>
</contexts>
<marker>9.</marker>
<rawString>Gomez, R.L., &amp; Gerken, L.A. (1999). Artificial grammar learning by one-year-olds leads to specific and abstract knowledge. Cognition, 70, 109-135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Saffran</author>
<author>E K Johnson</author>
<author>R N Aslin</author>
<author>E L Newport</author>
</authors>
<title>Statistical learning of tone sequences by human infants and adults.</title>
<date>1999</date>
<journal>Cognition,</journal>
<volume>70</volume>
<pages>27--52</pages>
<contexts>
<context position="2791" citStr="[10]" startWordPosition="421" endWordPosition="421">ly stems from the division of labor between endowment and learning– plainly, things that are built in needn’t be learned, and things that can be garnered from experience needn’t be built in. The important paper of Saffran, Aslin, &amp; Newport [8] on statistical learning (SL), suggests that children may be powerful learners after all. Very young infants can exploit transitional probabilities between syllables for the task of word segmentation, with only minimum exposure to an artificial language. Subsequent work has demonstrated SL in other domains including artificial grammar learning [9], music [10], vision [11], as well as in other species [12]. This then raises the possibility of learning as an alternative to the innate endowment of linguistic knowledge [13]. We believe that the computational modeling of psychological processes, with special attention to concrete mechanisms and quantitative evaluations, can play an important role in the endowment vs. learning debate. Linguists’ investigations of UG are rarely developmental, even less so corpus-oriented. Developmental psychologists, by contrast, often stop at identifying components in a cognitive task [14], without an account of how suc</context>
</contexts>
<marker>10.</marker>
<rawString>Saffran, J.R., Johnson, E.K., Aslin R.N. &amp; Newport, E.L. (1999). Statistical learning of tone sequences by human infants and adults. Cognition, 70, 27-52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Fiser</author>
<author>R N Aslin</author>
</authors>
<title>Statistical learning of new visual feature combinations by infants.</title>
<date>2002</date>
<journal>PNAS,</journal>
<volume>99</volume>
<pages>15822--6</pages>
<contexts>
<context position="2804" citStr="[11]" startWordPosition="423" endWordPosition="423"> the division of labor between endowment and learning– plainly, things that are built in needn’t be learned, and things that can be garnered from experience needn’t be built in. The important paper of Saffran, Aslin, &amp; Newport [8] on statistical learning (SL), suggests that children may be powerful learners after all. Very young infants can exploit transitional probabilities between syllables for the task of word segmentation, with only minimum exposure to an artificial language. Subsequent work has demonstrated SL in other domains including artificial grammar learning [9], music [10], vision [11], as well as in other species [12]. This then raises the possibility of learning as an alternative to the innate endowment of linguistic knowledge [13]. We believe that the computational modeling of psychological processes, with special attention to concrete mechanisms and quantitative evaluations, can play an important role in the endowment vs. learning debate. Linguists’ investigations of UG are rarely developmental, even less so corpus-oriented. Developmental psychologists, by contrast, often stop at identifying components in a cognitive task [14], without an account of how such components </context>
</contexts>
<marker>11.</marker>
<rawString>Fiser, J., &amp; Aslin, R.N. (2002). Statistical learning of new visual feature combinations by infants. PNAS, 99, 15822-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hauser</author>
<author>E L Newport</author>
<author>R N Aslin</author>
</authors>
<title>Segmentation of the speech stream in a non-human primate: Statistical learning in cotton-top tamarins.</title>
<date>2001</date>
<journal>Cognition,</journal>
<volume>78</volume>
<pages>41--52</pages>
<contexts>
<context position="2838" citStr="[12]" startWordPosition="430" endWordPosition="430">owment and learning– plainly, things that are built in needn’t be learned, and things that can be garnered from experience needn’t be built in. The important paper of Saffran, Aslin, &amp; Newport [8] on statistical learning (SL), suggests that children may be powerful learners after all. Very young infants can exploit transitional probabilities between syllables for the task of word segmentation, with only minimum exposure to an artificial language. Subsequent work has demonstrated SL in other domains including artificial grammar learning [9], music [10], vision [11], as well as in other species [12]. This then raises the possibility of learning as an alternative to the innate endowment of linguistic knowledge [13]. We believe that the computational modeling of psychological processes, with special attention to concrete mechanisms and quantitative evaluations, can play an important role in the endowment vs. learning debate. Linguists’ investigations of UG are rarely developmental, even less so corpus-oriented. Developmental psychologists, by contrast, often stop at identifying components in a cognitive task [14], without an account of how such components work together in an algorithmic ma</context>
</contexts>
<marker>12.</marker>
<rawString>Hauser, M., Newport, E.L., &amp; Aslin, R.N. (2001). Segmentation of the speech stream in a non-human primate: Statistical learning in cotton-top tamarins. Cognition, 78, B41-B52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bates</author>
<author>J Elman</author>
</authors>
<date>1996</date>
<journal>Learning rediscovered. Science,</journal>
<volume>274</volume>
<pages>1849--50</pages>
<contexts>
<context position="2955" citStr="[13]" startWordPosition="448" endWordPosition="448">ience needn’t be built in. The important paper of Saffran, Aslin, &amp; Newport [8] on statistical learning (SL), suggests that children may be powerful learners after all. Very young infants can exploit transitional probabilities between syllables for the task of word segmentation, with only minimum exposure to an artificial language. Subsequent work has demonstrated SL in other domains including artificial grammar learning [9], music [10], vision [11], as well as in other species [12]. This then raises the possibility of learning as an alternative to the innate endowment of linguistic knowledge [13]. We believe that the computational modeling of psychological processes, with special attention to concrete mechanisms and quantitative evaluations, can play an important role in the endowment vs. learning debate. Linguists’ investigations of UG are rarely developmental, even less so corpus-oriented. Developmental psychologists, by contrast, often stop at identifying components in a cognitive task [14], without an account of how such components work together in an algorithmic manner. On the other hand, if computation is to be of relevance to linguistics, psychology, and cognitive science in ge</context>
</contexts>
<marker>13.</marker>
<rawString>Bates, E., &amp; Elman, J. (1996). Learning rediscovered. Science, 274, 1849-50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P W Jusczyk</author>
</authors>
<title>How infants begin to extract words from speech.</title>
<date>1999</date>
<booktitle>Trends in Cognitive Sciences,</booktitle>
<volume>3</volume>
<pages>323--8</pages>
<contexts>
<context position="3360" citStr="[14]" startWordPosition="502" endWordPosition="502">al grammar learning [9], music [10], vision [11], as well as in other species [12]. This then raises the possibility of learning as an alternative to the innate endowment of linguistic knowledge [13]. We believe that the computational modeling of psychological processes, with special attention to concrete mechanisms and quantitative evaluations, can play an important role in the endowment vs. learning debate. Linguists’ investigations of UG are rarely developmental, even less so corpus-oriented. Developmental psychologists, by contrast, often stop at identifying components in a cognitive task [14], without an account of how such components work together in an algorithmic manner. On the other hand, if computation is to be of relevance to linguistics, psychology, and cognitive science in general, being merely computational will not suffice. A model must be psychological plausible, and ready to face its implications in the broad empirical contexts [7]. For example, how does it generalize to typologically different languages? How does the model’s behavior compare with that of human language learners and processors? In this article, we will present a simple computational model of word segme</context>
<context position="4834" citStr="[14]" startWordPosition="732" endWordPosition="732">t be constrained by the knowledge of phonological 49 structure. Indeed, the model reveals that SL may 50 CV syllables, many languages, including English, well be an artifact–an impressive one, nonetheless– make use of a far more diverse range of syllabic that plays no role in actual word segmentation in types. And then, syllabification of speech is far human children. from trivial, which (most likely) involve both in2 Statistics does not Refute UG nate knowledge of phonological structures as well It has been suggested [15, 8] that word segmenta- as discovering language-specific instantiations [14]. tion from continuous speech may be achieved by All these problems have to be solved before SL for using transitional probabilities (TP) between ad- word segmentation can take place. jacent syllables A and B, where , TP(A—*B) = 3 The Model P(AB)/P(A), with P(AB) being the frequency of B To give a precise evaluation of SL in a realisfollowing A, and P(A) the total frequency of A. tic setting, we constructed a series of (embarrassWord boundaries are postulated at local minima, ingly simple) computational models tested on childwhere the TP is lower than its neighbors. For ex- directed English. a</context>
<context position="10101" citStr="[14]" startWordPosition="1586" endWordPosition="1586">roves dramatically, in fact, if the learner is equipped with even a small amount of prior knowledge about phonological structures. Specifically, we assume, uncontroversially, that each word can have only one primary stress. (This would not work for functional words, however.) If the learner knows this, then it may limit the search for local minima only in the window between two syllables that both bear primary stress, e.g., between the two a’s in the sequence “languageacquisition”. This assumption is plausible given that 7.5-month-old infants are sensitive to strong/weak prosodic distinctions [14]. When stress information suffices, no SL is employed, so “bigbadwolf” breaks into three words for free. Once this simple principle is built in, the stress-delimited SL algorithm can achieve the precision of 73.5% and 71.2%, which compare favorably to the best performance reported in the literature [20]. (That work, however, uses an computationally prohibitive and psychological implausible algorithm that iteratively optimizes the entire lexicon.) The computational models complement the experimental study that prosodic information takes priority over statistical information when both are availa</context>
<context position="12675" citStr="[14]" startWordPosition="1997" endWordPosition="1997">ative contribution of SL and UG to language acquisition. These include, more pertinent to the problem of word segmentation: • Can statistical learning be used in the acquisition of language-specific phonotactics, a prerequisite to syllabification and a prelude to word segmentation? • Given that prosodic constraints are critical for the success of SL in word segmentation, future work needs to quantify the availability of stress information in spoken corpora. • Can further experiments, carried over realistic linguistic input, further tease apart the multiple strategies used in word segmentation [14]? What are the psychological mechanisms (algorithms) that integrate these strategies? • How does word segmentation, statistical or otherwise, work for agglutinative (e.g., Turkish) and polysynthetic languages (e.g. Mohawk), where the division between words, morphology, and syntax is quite different from more clear-cut cases like English? Computational modeling can make explicit the balance between statistics and UG, and are in the same vein as the recent findings [24] on when/where SL is effective/possible. UG can help SL by providing specific constraints on its application, and modeling may r</context>
</contexts>
<marker>14.</marker>
<rawString>Jusczyk, P.W. (1999). How infants begin to extract words from speech. Trends in Cognitive Sciences, 3, 323-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>The Logical Structure of Linguistic Theory.</title>
<date>1955</date>
<tech>Manuscript,</tech>
<publisher>Plenum.</publisher>
<institution>Harvard University and Massachusetts Institute of Technology.</institution>
<location>New York:</location>
<note>Published in</note>
<contexts>
<context position="4761" citStr="[15, 8]" startWordPosition="723" endWordPosition="724">a realistic setting (e.g., child-directed English). To be successful, it must be constrained by the knowledge of phonological 49 structure. Indeed, the model reveals that SL may 50 CV syllables, many languages, including English, well be an artifact–an impressive one, nonetheless– make use of a far more diverse range of syllabic that plays no role in actual word segmentation in types. And then, syllabification of speech is far human children. from trivial, which (most likely) involve both in2 Statistics does not Refute UG nate knowledge of phonological structures as well It has been suggested [15, 8] that word segmenta- as discovering language-specific instantiations [14]. tion from continuous speech may be achieved by All these problems have to be solved before SL for using transitional probabilities (TP) between ad- word segmentation can take place. jacent syllables A and B, where , TP(A—*B) = 3 The Model P(AB)/P(A), with P(AB) being the frequency of B To give a precise evaluation of SL in a realisfollowing A, and P(A) the total frequency of A. tic setting, we constructed a series of (embarrassWord boundaries are postulated at local minima, ingly simple) computational models tested on c</context>
</contexts>
<marker>15.</marker>
<rawString>Chomsky, N. (1955/1975). The Logical Structure of Linguistic Theory. Manuscript, Harvard University and Massachusetts Institute of Technology. Published in 1975 by New York: Plenum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Saffran</author>
<author>R N Aslin</author>
<author>E L Newport</author>
</authors>
<date>1997</date>
<journal>Letters. Science,</journal>
<volume>276</volume>
<pages>1177--1181</pages>
<contexts>
<context position="6695" citStr="[16]" startWordPosition="1025" endWordPosition="1025">re available to the modeled artificial language from only two minutes of expo- learner. Altogether, there are 226,178 words, consure [8]. sisting of 263,660 syllables. To be effective, a learning algorithm–indeed any Implementing SL-based segmentation is straightalgorithm–must have an appropriate representation forward. One first gathers pair-wise TPs from the of the relevant learning data. We thus need to be training data, which are used to identify local mincautious about the interpretation of the success of ima and postulate word boundaries in the on-line SL, as the authors themselves note [16]. If any- processing of syllable sequences. Scoring is done thing, it seems that the findings strengthen, rather for each utterance and then averaged. Viewed as an than weaken, the case for (innate) linguistic knowl- information retrieval problem, it is customary [20] edge. A classic argument for innateness [4, 5, to report both precision and recall of the perfor17] comes from the fact that syntactic operations mance. are defined over specific types of data structures– The segmentation results using TP local minima constituents and phrases–but not over, say, linear are remarkably poor, even un</context>
</contexts>
<marker>16.</marker>
<rawString>Saffran, J.R., Aslin, R.N., &amp; Newport, E.L. (1997). Letters. Science, 276, 1177-1181</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Crain</author>
<author>M Nakayama</author>
</authors>
<title>Structure dependency in grammar formation.</title>
<date>1987</date>
<journal>Language,</journal>
<pages>63--522</pages>
<marker>17.</marker>
<rawString>Crain, S., &amp; Nakayama, M. (1987). Structure dependency in grammar formation. Language, 63:522-543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bijeljiac-Babic</author>
<author>J Bertoncini</author>
<author>J Mehler</author>
</authors>
<title>How do four-day-old infants categorize multisyllabic utterances.</title>
<date>1993</date>
<journal>Developmental psychology,</journal>
<volume>29</volume>
<pages>711--21</pages>
<contexts>
<context position="9156" citStr="[18]" startWordPosition="1440" endWordPosition="1440"> statistics is gathered: in this case, it is are uniformly three-syllables long, much of childthe syllables, rather than segments, or front vowels. directed English consists of sequences of monosylA host of questions then arises. First, How do labic words: corpus statistics reveals that on averthey know so? It is quite possible that the primacy age, a monosyllabic word is followed by another of syllables as the basic unit of speech is innately monosyllabic word 85% of time. As long as this available, as suggested in neonate speech perception is the case, SL cannot, in principle, work. studies [18]? Second, where do the syllables come from? While the experiments in [8] used uniformly 4 Statistics Needs UG This is not to say that SL cannot be effective for word segmentation. Its application, must be constrained–like that of any learning algorithm however powerful–as suggested by formal learning theories [1, 2, 3]. The performance improves dramatically, in fact, if the learner is equipped with even a small amount of prior knowledge about phonological structures. Specifically, we assume, uncontroversially, that each word can have only one primary stress. (This would not work for functional</context>
</contexts>
<marker>18.</marker>
<rawString>Bijeljiac-Babic, R., Bertoncini, J., &amp; Mehler, J. (1993). How do four-day-old infants categorize multisyllabic utterances. Developmental psychology, 29, 711-21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacWhinney</author>
</authors>
<title>The CHILDES Project: Tools for Analyzing Talk.</title>
<date>1995</date>
<location>Hillsdale: Lawrence Erlbaum.</location>
<contexts>
<context position="5696" citStr="[19]" startWordPosition="874" endWordPosition="874">P(AB) being the frequency of B To give a precise evaluation of SL in a realisfollowing A, and P(A) the total frequency of A. tic setting, we constructed a series of (embarrassWord boundaries are postulated at local minima, ingly simple) computational models tested on childwhere the TP is lower than its neighbors. For ex- directed English. ample, given sufficient amount of exposure to En- The learning data consists of a random samglish, the learner may establish that, in the four- ple of child-directed English sentences from the syllable sequence “prettybaby”, TP(pre—*tty) and CHILDES database [19] The words were then phoTP(ba—*by) are both higher than TP(tty—*ba): a netically transcribed using the Carnegie Mellon Proword boundary can be (correctly) postulated. It nunciation Dictionary, and were then grouped into is remarkable that 8-month-old infants can extract syllables. Spaces between words are removed; howthree-syllable words in the continuous speech of an ever, utterance breaks are available to the modeled artificial language from only two minutes of expo- learner. Altogether, there are 226,178 words, consure [8]. sisting of 263,660 syllables. To be effective, a learning algorithm</context>
</contexts>
<marker>19.</marker>
<rawString>MacWhinney, B. (1995). The CHILDES Project: Tools for Analyzing Talk. Hillsdale: Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Brent</author>
</authors>
<title>Speech segmentation and word discovery: a computational perspective.</title>
<date>1999</date>
<journal>Trends in Cognitive Science,</journal>
<volume>3</volume>
<pages>294--301</pages>
<contexts>
<context position="6963" citStr="[20]" startWordPosition="1066" endWordPosition="1066">ithm–must have an appropriate representation forward. One first gathers pair-wise TPs from the of the relevant learning data. We thus need to be training data, which are used to identify local mincautious about the interpretation of the success of ima and postulate word boundaries in the on-line SL, as the authors themselves note [16]. If any- processing of syllable sequences. Scoring is done thing, it seems that the findings strengthen, rather for each utterance and then averaged. Viewed as an than weaken, the case for (innate) linguistic knowl- information retrieval problem, it is customary [20] edge. A classic argument for innateness [4, 5, to report both precision and recall of the perfor17] comes from the fact that syntactic operations mance. are defined over specific types of data structures– The segmentation results using TP local minima constituents and phrases–but not over, say, linear are remarkably poor, even under the assumption strings of words, or numerous other logical possibil- that the learner has already syllabified the input perities. While infants seem to keep track of statistical fectly. Precision is 41.6%, and recall is 23.3%; over information, any conclusion draw</context>
<context position="10405" citStr="[20]" startWordPosition="1637" endWordPosition="1637">then it may limit the search for local minima only in the window between two syllables that both bear primary stress, e.g., between the two a’s in the sequence “languageacquisition”. This assumption is plausible given that 7.5-month-old infants are sensitive to strong/weak prosodic distinctions [14]. When stress information suffices, no SL is employed, so “bigbadwolf” breaks into three words for free. Once this simple principle is built in, the stress-delimited SL algorithm can achieve the precision of 73.5% and 71.2%, which compare favorably to the best performance reported in the literature [20]. (That work, however, uses an computationally prohibitive and psychological implausible algorithm that iteratively optimizes the entire lexicon.) The computational models complement the experimental study that prosodic information takes priority over statistical information when both are available [21]. Yet again one needs to be cautious about the improved performance, and a number of unresolved issues need to be addressed by future work. It remains possible that SL is not used at all in actual word segmentation. Once the oneword-one-stress principle is built in, we may consider a model that </context>
</contexts>
<marker>20.</marker>
<rawString>Brent, M. (1999). Speech segmentation and word discovery: a computational perspective. Trends in Cognitive Science, 3, 294-301.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E K Johnson</author>
<author>P W Jusczyk</author>
</authors>
<title>Word segmentation by 8-month-olds: When speech cues count more than statistics.</title>
<date>2001</date>
<journal>Journal of Memory and Language,</journal>
<volume>44</volume>
<pages>1--20</pages>
<contexts>
<context position="10709" citStr="[21]" startWordPosition="1677" endWordPosition="1677">en stress information suffices, no SL is employed, so “bigbadwolf” breaks into three words for free. Once this simple principle is built in, the stress-delimited SL algorithm can achieve the precision of 73.5% and 71.2%, which compare favorably to the best performance reported in the literature [20]. (That work, however, uses an computationally prohibitive and psychological implausible algorithm that iteratively optimizes the entire lexicon.) The computational models complement the experimental study that prosodic information takes priority over statistical information when both are available [21]. Yet again one needs to be cautious about the improved performance, and a number of unresolved issues need to be addressed by future work. It remains possible that SL is not used at all in actual word segmentation. Once the oneword-one-stress principle is built in, we may consider a model that does not use any statistics, hence avoiding the computational cost that is likely to be considerable. (While we don’t know how infants keep track of TPs, there are clearly quite some work to do. Syllables in English number in the thousands; now take the quadratic for the potential number of pair-wise TP</context>
</contexts>
<marker>21.</marker>
<rawString>Johnson, E.K. &amp; Jusczyk, P.W. (2001) Word segmentation by 8-month-olds: When speech cues count more than statistics. Journal of Memory and Language, 44, 1-20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P W Jusczyk</author>
<author>E A Hohne</author>
</authors>
<title>Infants’ memory for spoken words.</title>
<date>1997</date>
<journal>Science,</journal>
<volume>277</volume>
<pages>1984--6</pages>
<contexts>
<context position="11660" citStr="[22]" startWordPosition="1837" endWordPosition="1837">putational cost that is likely to be considerable. (While we don’t know how infants keep track of TPs, there are clearly quite some work to do. Syllables in English number in the thousands; now take the quadratic for the potential number of pair-wise TPs.) It simply stores previously extracted words in the memory to bootstrap new words. Young children’s familiar segmentation errors–”I was have” from be-have, “hiccing up” from hicc-up, “two dults”, from a-dult–suggest that this process does take place. Moreover, there is evidence that 8-month-old infants can store familiar sounds in the memory [22]. And finally, there are plenty of single-word utterances–up to 10% [23]– that give many words for free. The implementation of a purely symbolic learner that recycles known words yields even better performance: a precision of 81.5% and recall of 90.1%. 5 Conclusion Further work, both experimental and computational, will need to address a few pressing questions, in order to gain a better assessment of the relative contribution of SL and UG to language acquisition. These include, more pertinent to the problem of word segmentation: • Can statistical learning be used in the acquisition of language</context>
</contexts>
<marker>22.</marker>
<rawString>Jusczyk, P. W., &amp; Hohne, E. A. (1997). Infants’ memory for spoken words. Science, 277, 1984-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Brent</author>
<author>J M Siskind</author>
</authors>
<title>The role of exposure to isolated words in early vocabulary development.</title>
<date>2001</date>
<journal>Cognition,</journal>
<volume>81</volume>
<pages>33--44</pages>
<contexts>
<context position="11732" citStr="[23]" startWordPosition="1848" endWordPosition="1848">how infants keep track of TPs, there are clearly quite some work to do. Syllables in English number in the thousands; now take the quadratic for the potential number of pair-wise TPs.) It simply stores previously extracted words in the memory to bootstrap new words. Young children’s familiar segmentation errors–”I was have” from be-have, “hiccing up” from hicc-up, “two dults”, from a-dult–suggest that this process does take place. Moreover, there is evidence that 8-month-old infants can store familiar sounds in the memory [22]. And finally, there are plenty of single-word utterances–up to 10% [23]– that give many words for free. The implementation of a purely symbolic learner that recycles known words yields even better performance: a precision of 81.5% and recall of 90.1%. 5 Conclusion Further work, both experimental and computational, will need to address a few pressing questions, in order to gain a better assessment of the relative contribution of SL and UG to language acquisition. These include, more pertinent to the problem of word segmentation: • Can statistical learning be used in the acquisition of language-specific phonotactics, a prerequisite to syllabification and a prelude </context>
</contexts>
<marker>23.</marker>
<rawString>Brent, M.R., &amp; Siskind, J.M. (2001). The role of exposure to isolated words in early vocabulary development. Cognition, 81, B33-44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E L Newport</author>
<author>R N Aslin</author>
</authors>
<title>Learning at a distance: I. Statistical learning of nonadjacent dependencies.</title>
<date>2004</date>
<journal>Cognitive Psychology,</journal>
<volume>48</volume>
<pages>127--62</pages>
<contexts>
<context position="13147" citStr="[24]" startWordPosition="2067" endWordPosition="2067">her experiments, carried over realistic linguistic input, further tease apart the multiple strategies used in word segmentation [14]? What are the psychological mechanisms (algorithms) that integrate these strategies? • How does word segmentation, statistical or otherwise, work for agglutinative (e.g., Turkish) and polysynthetic languages (e.g. Mohawk), where the division between words, morphology, and syntax is quite different from more clear-cut cases like English? Computational modeling can make explicit the balance between statistics and UG, and are in the same vein as the recent findings [24] on when/where SL is effective/possible. UG can help SL by providing specific constraints on its application, and modeling may raise new questions for further experimental studies. In related work [6,7], we have augmented traditional theories of UG– derivational phonology, and the Principles and Parameters framework–with a component of statistical learning, with novel and desirable consequences. Yet in all cases, statistical learning, while perhaps domain-general, is constrained by what appears to be innate and domain-specific knowledge of linguistic structures, such that learning can operate </context>
</contexts>
<marker>24.</marker>
<rawString>Newport, E.L., &amp; Aslin, R.N. (2004). Learning at a distance: I. Statistical learning of nonadjacent dependencies. Cognitive Psychology, 48, 127-62.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>