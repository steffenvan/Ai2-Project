<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000054">
<title confidence="0.9793655">
Chinese Word Segmentation and Named Entity Recognition by
Character Tagging
</title>
<note confidence="0.6724924">
1
Kun Yu Sadao Kurohashi2 Hao 1
Liu Toshiaki a zawa1
Nak
Graduate School of Information Science and Technology, The University of
</note>
<author confidence="0.400322">
Tokyo, Tokyo, Japan, 113-86561
</author>
<affiliation confidence="0.774207">
Graduate School of Informatics, Kyoto University, Kyoto, Japan, 606-85012
</affiliation>
<email confidence="0.9635015">
{kunyu, liuhao, nakazawa}@kc.t.u-tokyo.ac.jp1
kuro@i.kyoto-u.ac.jp2
</email>
<sectionHeader confidence="0.996305" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999225">
This paper describes our word segmenta-
tion system and named entity recognition
(NER) system for participating in the
third SIGHAN Bakeoff. Both of them are
based on character tagging, but use dif-
ferent tag sets and different features.
Evaluation results show that our word
segmentation system achieved 93.3% and
94.7% F-score in UPUC and MSRA open
tests, and our NER system got 70.84%
and 81.32% F-score in LDC and MSRA
open tests.
</bodyText>
<sectionHeader confidence="0.998781" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999780269230769">
Dealing with word segmentation as character
tagging showed good results in last SIGHAN
Bakeoff (J.K.Low et al.,2005). It is good at un-
known word identification, but only using char-
acter-level features sometimes makes mistakes
when identifying known words (T.Nakagawa,
2004). Researchers use word-level features
(J.K.Low et al.,2005) to solve this problem.
Based on this idea, we develop a word segmenta-
tion system based on character-tagging, which
also combine character-level and word-level fea-
tures. In addition, a character-based NER module
and a rule-based factoid identification module
are developed for post-processing.
Named entity recognition based on character-
tagging has shown better accuracy than word-
based methods (H.Jing et al.,2003). But the small
window of text makes it difficult to recognize the
named entities with many characters, such as
organization names (H.Jing et al.,2003). Consid-
ering about this, we developed a NER system
based on character-tagging, which combines
word-level and character-level features together.
In addition, in-NE probability is defined in this
system to remove incorrect named entities and
create new named entities as post-processing.
</bodyText>
<sectionHeader confidence="0.9988975" genericHeader="introduction">
2 Character Tagging for Word
Segmentation and NER
</sectionHeader>
<subsectionHeader confidence="0.979241">
2.1 Basic Model
</subsectionHeader>
<bodyText confidence="0.99992125">
We look both word segmentation and NER as
character tagging, which is to find the tag se-
quence T* with the highest probability given a
sequence of characters S=c1c2...cn.
</bodyText>
<equation confidence="0.994844666666667">
T* = arg max (  |)
P T S
T
</equation>
<bodyText confidence="0.999368666666667">
Then we assume that the tagging of one char-
acter is independent of each other, and modify
formula 1 as
</bodyText>
<equation confidence="0.9469385">
(2)
= arg max
T t t
= 1 2
</equation>
<bodyText confidence="0.999516875">
Beam search (n=3) (Ratnaparkhi,1996) is ap-
plied for tag sequence searching, but we only
search the valid sequences to ensure the validity
of searching result. SVM is selected as the basic
classification model for tagging because of its
robustness to over-fitting and high performance
(Sebastiani, 2002). To simplify the calculation,
the output of SVM is regarded as P(ti|ci).
</bodyText>
<subsectionHeader confidence="0.999426">
2.2 Tag Definition
</subsectionHeader>
<bodyText confidence="0.99975525">
Four tags ‘B, I, E, S’ are defined for the word
segmentation system, in which ‘B’ means the
character is the beginning of one word, ‘I’ means
the character is inside one word, ‘E’ means the
character is at the end of one word and ‘S’ means
the character is one word by itself.
For the NER system, different tag sets are de-
fined for different corpuses. Table 1 shows the
</bodyText>
<equation confidence="0.9982211875">
(1)
T t t t
= 1 2 ... n
n
max P(t
T=arg
1t2...tn  |c1c2 ... cn
*
)
tn
...
)
P
∏
i = 1
(ti  |ci
</equation>
<page confidence="0.942925">
146
</page>
<bodyText confidence="0.968308181818182">
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 146–149,
Sydney, July 2006. c�2006 Association for Computational Linguistics
tag set defined for MSRA corpus. It is the prod-
uct of Segment-Tag set and NE-Tag set, because
not only named entities but also words are seg-
mented in this corpus. Here NE-Tag ‘O’ means
the character does not belong to any named enti-
ties. For LDC corpus, because there is no seg-
mentation information, we delete NE-Tag ‘O’
but add tag ‘NONE’ to indicate the character
does not belong to any named entities (Table 2).
</bodyText>
<subsectionHeader confidence="0.994087">
2.3 Feature Definition
</subsectionHeader>
<bodyText confidence="0.9970395">
First, some features based on characters are
defined for the two tasks, which are:
</bodyText>
<figure confidence="0.886443">
(a) Cn (n=-2,-1,0,1,2)
(b) Pu(C0)
</figure>
<bodyText confidence="0.999712571428571">
Feature Cn (n=-2,-1,0,1,2) mean the Chinese
characters appearing in different positions (the
current character and two characters to its left
and right), and they are binary features. A char-
acter list, which contains all the characters in the
lexicon introduced later, is used to identify them.
Besides of that, feature Pu(C0) means whether C0
is in a punctuation character list. It is also binary
feature and all the punctuations in the punctua-
tion character list come from Penn Chinese Tree-
bank 5.1 (N.Xue et al.,2002).
In addition, we define some word-level fea-
tures based on a lexicon to enlarge the window
size of text in the two tasks, which are:
</bodyText>
<listItem confidence="0.73577">
(c) Wn (n=-1,0,1)
</listItem>
<bodyText confidence="0.999919">
Feature Wn (n=-1,0,1) mean the lexicon words
in different positions (the word containing C0
and one word to its left and right) and they are
also binary features. We select all the possible
words in the lexicon that satisfy the requirements,
not like only selecting the longest one in
(J.K.Low et al.,2005). To create the lexicon, we
use following steps. First, a lexicon from NICT
(National Institute of Information and Communi-
cations Technology, Japan) is used as the basic
lexicon, which is extracted from Peking Univer-
sity Corpus of the second SIGHAN Bakeoff
(T.Emerson, 2005), Penn Chinese Treebank 4.0
(N.Xue et al.,2002), a Chinese-to-English Word-
list1 and part of NICT corpus (K.Uchimoto et
al.,2004; Y.J.Zhang et al.,2005). Then, all the
words containing digits and letters are removed
</bodyText>
<footnote confidence="0.670798">
1 http://projects.ldc.upenn.edu/Chinese/
</footnote>
<bodyText confidence="0.987472045454545">
from this lexicon. At last, all the punctuations in
Penn Chinese Treebank 5.1 (N.Xue et al.,2002)
and all the words in the training data of UPUC
and MSRA corpuses are added into the lexicon.
Besides of above features, some extra features
are defined only for NER task.
First, we add some character-based features to
improve the accuracy of person name recognition,
which are CNn (n=-2,-1,0,1,2). They mean
whether C n (n=-2,-1,0,1,2)belong to a Chinese
surname list. All of them are binary features. The
Chinese surname list contains the most famous
100 Chinese surnames, such as, , ,
(Zhao, Qian, Sun, Li).
Then, we add some word-based features to
help identify the organization name, which are
WORGn (n=-1,0,1). They mean whether W n (n=
-1,0,1) belong to an organization suffix list. All
of them are also binary features. The organiza-
tion suffix list is created by extracting the last
word from all the organization names in the
training data of both MSRA and LDC corpuses.
</bodyText>
<sectionHeader confidence="0.957916" genericHeader="method">
3 Post-processing
</sectionHeader>
<bodyText confidence="0.998836285714286">
Besides of the basic model, a NER module
and a factoid identification module are developed
in our word segmentation system for post-
processing. In addition, we define in-NE prob-
ability to delete the incorrect named entities and
identify new named entities in the post-
processing phrase of our NER system.
</bodyText>
<subsectionHeader confidence="0.634995">
3.1 Named Entity Recognition for Word
Segmentation
</subsectionHeader>
<bodyText confidence="0.798524941176471">
In this module, if two or more segments in the
outputs of basic model are recognized as one
named entity, we combine them as one segment.
This module uses the same basic NER model
as what we introduced in the previous section.
But it only identifies person and location names,
because organization names often contain more
than one word. In addition, to keep the high ac-
curacy of person name recognition, the features
about organization suffixes are not used here.
3.2 Factoid Identification for Word Seg-
mentation
Rules are used to identify the following fac-
toids among the segments from the basic word
segmentation model:
NUMBER: Integer, decimal, Chinese number
PERCENT: Percentage and fraction
</bodyText>
<sectionHeader confidence="0.5253755" genericHeader="method">
DATE: Date
FOREIGN: English words
</sectionHeader>
<tableCaption confidence="0.945248">
Table 1 Tags of NER for MSRA corpus
</tableCaption>
<table confidence="0.979415">
Segment-Tag NE-Tag
B, I, E, S PER, LOC, ORG, O
</table>
<tableCaption confidence="0.778798">
Table 2 Tags of NER for LDC corpus
</tableCaption>
<table confidence="0.960777">
Segment Tag NE Tag + NONE
B, I, E, S PER, LOC, ORG, GPE
</table>
<page confidence="0.879516">
147
</page>
<tableCaption confidence="0.9982475">
Table 3 shows some rules defined here.
Table 3 Some Rules for Factoid Identification
</tableCaption>
<table confidence="0.9676896">
Factoid Rule
NUMBER If previous segment ends with DIGIT and current
segment starts with DIGIT, then combine them.
PERCENT If previous segment is composed of DIGIT and
current segment equals ‘%’, then combine them.
DATE If previous segment is composed of DIGIT and
current segment is in the list of ‘, , ,
(Year, Month, Day, Day)’, then combine them.
FOREIGN Combine the consequent letters as one segment.
(DIGIT means both Arabic and Chinese numerals)
</table>
<subsectionHeader confidence="0.997305">
3.3 NER Deletion and Creation
</subsectionHeader>
<bodyText confidence="0.966789666666667">
In-word probability has been used in unknown
word identification successfully (H.Q.Li et al.,
2004). Accordingly, we define in-NE probability
to help delete and create named entities (NE).
Formula 3 shows the definition of in-NE prob-
ability for character sequence cici+1...ci+n. Here ‘#
of cici+1...ci+n as NE’ is defined as TimeInNE and
the occurrence of cici+1...ci+n in different type of
NE is treated differently.
Then, we use some criteria to delete the incor-
rect NE and create new possible NE, in which
different thresholds are set for different tasks.
Criterion 1: If PInNE(cici+1...ci+n) of one NE
type is lower than TDel, and TimeInNE(cici+1...ci+n)
of the same NE type is also lower than TTime, then
delete this type of NE composed of cici+1...ci+n.
Criterion 2: If PInNE(cici+1...ci+n) of one NE
type is higher than TCre, and in other places the
character sequence cici+1...ci+n does not belong to
any NE, then create a new NE containing
cici+1...ci+n with this NE type.
</bodyText>
<sectionHeader confidence="0.889248" genericHeader="evaluation">
4 Evaluation Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.995561">
4.1 Evaluation Setting
</subsectionHeader>
<bodyText confidence="0.99995125">
SVMlight (T.Joachims, 1999) was used as
SVM tool. In addition, we used the MSRA train-
ing corpus of NER task in this Bakeoff to train
our NER post-processing module.
</bodyText>
<subsectionHeader confidence="0.983824">
4.2 Results of Word Segmentation
</subsectionHeader>
<bodyText confidence="0.982974666666667">
We attended the open track of word segmenta-
tion task for two corpuses: UPUC and MSRA.
Table 4 shows the evaluation results.
</bodyText>
<tableCaption confidence="0.993112">
Table 4 Results of Word Segmentation Task (in percentage %)
</tableCaption>
<table confidence="0.966967333333333">
Corpus Pre. Rec. F-score Roov Riv
UPUC 94.4 92.2 93.3 68.0 97.0
MSRA 94.0 95.3 94.7 50.3 96.9
</table>
<bodyText confidence="0.968954659574468">
The F-score of our word segmentation system
in UPUC corpus ranked 4th (same as that of the
3rd group) among all the 8 participants. And it
was only 1.1% lower than the highest one and
0.2% lower than the second one. It showed that
our character-tagging approach was feasible. But
the F-score of MSRA corpus was only higher
than one participant in all the 10 groups (the
highest one was 97.9%). Error analysis shows
that there are two main reasons.
First, in MSRA corpus, they tend to segment
one organization name as one word, such as
(China Chamber of Commerce in
USA). But our basic segmentation model seg-
mented such word into several words, e.g. /
/(USA/China/Chamber of Commerce),
and our post-processing NER module does not
consider about organization names.
Second, our factoid identification rule did not
combine the consequent DATE factoids into one
word, but they are combined in MSRA corpus.
For example, our system segmented the word
9 (9 o’clock in the evening) into three
parts/9/ (Evening/9 o’clock/Exact).
This error can be solved by revising the rules for
factoid identification.
Besides of that, we also found although our
large lexicon helped identify the known word
successfully, it also decreased the recall of OOV
words (our Riv of UPUC corpus ranked 2nd, with
only 0.6% decrease than the highest one, but
Roov ranked 4th, with 8.8% decrease than the
highest one). The large size of this lexicon is
looked as the main reason.
Our lexicon contains 221,407 words, in which
6,400 words are single-character words. It made
our system easy to segment one word into sev-
eral words, for example word (Economy
Group) in UPUC corpus was segmented into
(Economy) and (Group). Moreover, the
large size of this lexicon also brought errors of
combining two words into one word if the word
was in the lexicon. For example, words (Only)
and (Have) in MSRA corpus were identified
as one word because there existed the word
(Only) in our lexicon. We will reduce our lexi-
con to a reasonable size to solve these problems.
</bodyText>
<subsectionHeader confidence="0.999065">
4.3 Results of NER
</subsectionHeader>
<bodyText confidence="0.999915428571429">
We also attended the open track of NER task
for both LDC corpus and MSRA corpus. Table 5
and Table 6 give the evaluation results.
There were only 3 participants in the open
track of LDC corpus and our group got the best
F-score. In addition, among all the 11 partici-
pants for MSRA corpus, our system ranked 6th
</bodyText>
<equation confidence="0.949884466666667">
# of ... as NE
c c c
i i +1 i n
+
# of ... in testing data
c c c
i i+1 i n
+
PInNE
(... )
c c c
i i + 1 i n
+
=
(3)
</equation>
<page confidence="0.993756">
148
</page>
<bodyText confidence="0.997829625">
by F-score. It showed the validity of our charac-
ter-tagging method for NER. But for location
name (LOC) in LDC corpus, both the precision
and recall of our NER system were very low. It
was because there were too few location names
in the training data (there were only 476 LOC in
the training data, but 5648 PER, 5190 ORG and
9545 GPE in the same data set).
</bodyText>
<tableCaption confidence="0.996864">
Table 5 Results of NER Task for LDC corpus (in percentage %)
</tableCaption>
<table confidence="0.99974025">
PER LOC ORG GPE Overall
Pre. 83.29 58.52 61.48 78.66 76.16
Rec. 66.93 18.87 45.19 79.94 66.21
F-score 74.22 28.57 52.09 79.30 70.84
</table>
<tableCaption confidence="0.99426">
Table 6 Results of NER Task for MSRA corpus (in percentage %)
</tableCaption>
<table confidence="0.99233675">
PER LOC ORG Overall
Pre. 90.76 85.62 73.90 84.68
Rec. 76.13 85.41 65.74 78.22
F-score 82.80 85.52 69.58 81.32
</table>
<bodyText confidence="0.987601195121951">
Besides of that, error analysis shows there are
four types of main errors in the NER results.
First, some organization names were very long
and can be divided into several words, in which
parts of them can also be looked as named enti-
ties. In such case, our system only recognized the
small parts as named entities. For example,
(Fei Zhengqing
Eastern Asia Research Center of Harvard Univ.)
was an organization name. But our system rec-
ognized it as(Harvard Univ.)/ORG+
(Fei Zheng Qing)/PER+(Eastern
Asia)/LOC+ (Research Center)/ORG.
Adding more context features may be useful to
resolve this issue.
In addition, our system was not good at recog-
nizing foreign person names, such as
(Riordan), and abbreviations, such as (Los
Angeles), if they seldom or never appeared in
training corpus. It is because the use of the large
lexicon decreased the unknown word identifica-
tion ability of our NER system simultaneously.
Third, the in-NE probability used in post-
processing is helpful to identify named entities
which cannot be recognized by the basic model.
But it also recognized some words which can
only be regarded as named entities in the local
context incorrectly. For example, our system
recognized (Najing) as GPE in
(Send to Najing for remedy) in LDC corpus.
We will consider about adding the in-NE prob-
ability as one feature into the basic model to
solve this problem.
At last, in LDC corpus, they combine the at-
tributive of one named entity (especially person
and organization names) with the named entity
together. But our system only recognized the
named entity by itself. For example, our system
only recognized (Liu Gui Fang) as PER
in the reference person name
(Liu Gui Fang who does not know the inside).
</bodyText>
<sectionHeader confidence="0.994436" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999966529411765">
Through the participation of the third
SIGHAN Bakeoff, we found that tagging charac-
ters with both character-level and word-level fea-
tures was effective for both word segmentation
and NER. While, this work is only our
preliminary attempt and there are still many
works needed to do in the future, such as the
control of lexicon size, the use of extra
knowledge (e.g. pos-tag), the feature definition,
and so on. In addition, our word segmentation
system only combined the NER module as post-
processing, which resulted in that lots of infor-
mation from NER module cannot be used by the
basic model. We will consider about combining
the NER and factoid identification modules into
the basic word segmentation model by defining
new tag sets in our future work.
</bodyText>
<sectionHeader confidence="0.980393" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999975">
We would like to thank Dr. Kiyotaka Uchi-
moto for providing the NICT lexicon.
</bodyText>
<sectionHeader confidence="0.995878" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.999638192307692">
T.Emerson. 2005. The Second International Chinese Word Seg-
mentation Bakeoff. In the 4th SIGHAN Workshop. pp. 123-133.
H.Jing et al. 2003. HowtogetaChineseName(Entity): Segmentation
and Combination Issues. In EMNLP 2003. pp. 200-207.
T.Joachims. 1999. Making large-scale SVM learning practical.
Advances in Kernel Methods - Support Vector Learning. MIT-
Press.
H.Q.Li et al. 2004. The Use of SVM for Chinese New Word Identi-
fication. In IJCNLP 2004. pp. 723-732.
J.K.Low et al. 2005. A Maximum Entropy Approach to Chinese
Word Segmentation. In the 4th SIGHAN Workshop. pp. 161-164.
T.Nakagawa. 2004. Chinese and Japanese Word Segmentation
Using Word-level and Character-level Information. In COLING
2004. pp. 466-472.
A.Ratnaparkhi. 1996. A Maximum Entropy Model for Part-of-
Speech Tagging. In EMNLP 1996.
F.Sebastiani. 2002. Machine learning in automated text categoriza-
tion. ACM Computing Surveys. 34(1): 1-47.
K.Uchimoto et al. 2004. Multilingual Aligned Parallel Treebank
Corpus Reflecting Contextual Information and its Applications.
In Proceedings of the MLR 2004. pp. 63-70.
N.Xue et al. 2002. Building a Large-Scale Annotated Chinese Cor-
pus. In COLING 2002.
Y.J.Zhang et al. 2005. Building an Annotated Japanese-Chinese
Parallel Corpus – A part of NICT Multilingual Corpora. In Pro-
ceedings of the MT SummitX. pp. 71-78.
</reference>
<page confidence="0.998974">
149
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.034963">
<title confidence="0.991465">Chinese Word Segmentation and Named Entity Recognition</title>
<author confidence="0.591774">Character Tagging</author>
<date confidence="0.234522">1</date>
<author confidence="0.744456">Yu Sadao Hao</author>
<affiliation confidence="0.7105524">Toshiaki Nak Graduate School of Information Science and Technology, The University of Tokyo, Japan, School of Informatics, Kyoto University, Kyoto, Japan,</affiliation>
<email confidence="0.989134">liuhao,</email>
<abstract confidence="0.999088076923077">This paper describes our word segmentation system and named entity recognition (NER) system for participating in the third SIGHAN Bakeoff. Both of them are based on character tagging, but use different tag sets and different features. Evaluation results show that our word segmentation system achieved 93.3% and 94.7% F-score in UPUC and MSRA open tests, and our NER system got 70.84% and 81.32% F-score in LDC and MSRA open tests.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Emerson</author>
</authors>
<title>The Second International Chinese Word Segmentation Bakeoff.</title>
<date>2005</date>
<booktitle>In the 4th SIGHAN Workshop.</booktitle>
<pages>123--133</pages>
<contexts>
<context position="5235" citStr="Emerson, 2005" startWordPosition="862" endWordPosition="863">sks, which are: (c) Wn (n=-1,0,1) Feature Wn (n=-1,0,1) mean the lexicon words in different positions (the word containing C0 and one word to its left and right) and they are also binary features. We select all the possible words in the lexicon that satisfy the requirements, not like only selecting the longest one in (J.K.Low et al.,2005). To create the lexicon, we use following steps. First, a lexicon from NICT (National Institute of Information and Communications Technology, Japan) is used as the basic lexicon, which is extracted from Peking University Corpus of the second SIGHAN Bakeoff (T.Emerson, 2005), Penn Chinese Treebank 4.0 (N.Xue et al.,2002), a Chinese-to-English Wordlist1 and part of NICT corpus (K.Uchimoto et al.,2004; Y.J.Zhang et al.,2005). Then, all the words containing digits and letters are removed 1 http://projects.ldc.upenn.edu/Chinese/ from this lexicon. At last, all the punctuations in Penn Chinese Treebank 5.1 (N.Xue et al.,2002) and all the words in the training data of UPUC and MSRA corpuses are added into the lexicon. Besides of above features, some extra features are defined only for NER task. First, we add some character-based features to improve the accuracy of pers</context>
</contexts>
<marker>Emerson, 2005</marker>
<rawString>T.Emerson. 2005. The Second International Chinese Word Segmentation Bakeoff. In the 4th SIGHAN Workshop. pp. 123-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jing</author>
</authors>
<title>HowtogetaChineseName(Entity): Segmentation and Combination Issues.</title>
<date>2003</date>
<booktitle>In EMNLP</booktitle>
<pages>200--207</pages>
<marker>Jing, 2003</marker>
<rawString>H.Jing et al. 2003. HowtogetaChineseName(Entity): Segmentation and Combination Issues. In EMNLP 2003. pp. 200-207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning. MITPress.</booktitle>
<contexts>
<context position="9384" citStr="Joachims, 1999" startWordPosition="1542" endWordPosition="1543">e the incorrect NE and create new possible NE, in which different thresholds are set for different tasks. Criterion 1: If PInNE(cici+1...ci+n) of one NE type is lower than TDel, and TimeInNE(cici+1...ci+n) of the same NE type is also lower than TTime, then delete this type of NE composed of cici+1...ci+n. Criterion 2: If PInNE(cici+1...ci+n) of one NE type is higher than TCre, and in other places the character sequence cici+1...ci+n does not belong to any NE, then create a new NE containing cici+1...ci+n with this NE type. 4 Evaluation Results and Discussion 4.1 Evaluation Setting SVMlight (T.Joachims, 1999) was used as SVM tool. In addition, we used the MSRA training corpus of NER task in this Bakeoff to train our NER post-processing module. 4.2 Results of Word Segmentation We attended the open track of word segmentation task for two corpuses: UPUC and MSRA. Table 4 shows the evaluation results. Table 4 Results of Word Segmentation Task (in percentage %) Corpus Pre. Rec. F-score Roov Riv UPUC 94.4 92.2 93.3 68.0 97.0 MSRA 94.0 95.3 94.7 50.3 96.9 The F-score of our word segmentation system in UPUC corpus ranked 4th (same as that of the 3rd group) among all the 8 participants. And it was only 1.1</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T.Joachims. 1999. Making large-scale SVM learning practical. Advances in Kernel Methods - Support Vector Learning. MITPress.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Q Li</author>
</authors>
<title>The Use of SVM for Chinese New Word Identification.</title>
<date>2004</date>
<booktitle>In IJCNLP</booktitle>
<pages>723--732</pages>
<marker>Li, 2004</marker>
<rawString>H.Q.Li et al. 2004. The Use of SVM for Chinese New Word Identification. In IJCNLP 2004. pp. 723-732.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J K Low</author>
</authors>
<title>A Maximum Entropy Approach to Chinese Word Segmentation.</title>
<date>2005</date>
<booktitle>In the 4th SIGHAN Workshop.</booktitle>
<pages>161--164</pages>
<marker>Low, 2005</marker>
<rawString>J.K.Low et al. 2005. A Maximum Entropy Approach to Chinese Word Segmentation. In the 4th SIGHAN Workshop. pp. 161-164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Nakagawa</author>
</authors>
<title>Chinese and Japanese Word Segmentation Using Word-level and Character-level Information.</title>
<date>2004</date>
<booktitle>In COLING</booktitle>
<pages>466--472</pages>
<contexts>
<context position="1108" citStr="Nakagawa, 2004" startWordPosition="162" endWordPosition="163">ting in the third SIGHAN Bakeoff. Both of them are based on character tagging, but use different tag sets and different features. Evaluation results show that our word segmentation system achieved 93.3% and 94.7% F-score in UPUC and MSRA open tests, and our NER system got 70.84% and 81.32% F-score in LDC and MSRA open tests. 1 Introduction Dealing with word segmentation as character tagging showed good results in last SIGHAN Bakeoff (J.K.Low et al.,2005). It is good at unknown word identification, but only using character-level features sometimes makes mistakes when identifying known words (T.Nakagawa, 2004). Researchers use word-level features (J.K.Low et al.,2005) to solve this problem. Based on this idea, we develop a word segmentation system based on character-tagging, which also combine character-level and word-level features. In addition, a character-based NER module and a rule-based factoid identification module are developed for post-processing. Named entity recognition based on charactertagging has shown better accuracy than wordbased methods (H.Jing et al.,2003). But the small window of text makes it difficult to recognize the named entities with many characters, such as organization na</context>
</contexts>
<marker>Nakagawa, 2004</marker>
<rawString>T.Nakagawa. 2004. Chinese and Japanese Word Segmentation Using Word-level and Character-level Information. In COLING 2004. pp. 466-472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Model for Part-ofSpeech Tagging.</title>
<date>1996</date>
<booktitle>In EMNLP</booktitle>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A.Ratnaparkhi. 1996. A Maximum Entropy Model for Part-ofSpeech Tagging. In EMNLP 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization.</title>
<date>2002</date>
<journal>ACM Computing Surveys.</journal>
<volume>34</volume>
<issue>1</issue>
<pages>1--47</pages>
<contexts>
<context position="2713" citStr="Sebastiani, 2002" startWordPosition="419" endWordPosition="420">h word segmentation and NER as character tagging, which is to find the tag sequence T* with the highest probability given a sequence of characters S=c1c2...cn. T* = arg max ( |) P T S T Then we assume that the tagging of one character is independent of each other, and modify formula 1 as (2) = arg max T t t = 1 2 Beam search (n=3) (Ratnaparkhi,1996) is applied for tag sequence searching, but we only search the valid sequences to ensure the validity of searching result. SVM is selected as the basic classification model for tagging because of its robustness to over-fitting and high performance (Sebastiani, 2002). To simplify the calculation, the output of SVM is regarded as P(ti|ci). 2.2 Tag Definition Four tags ‘B, I, E, S’ are defined for the word segmentation system, in which ‘B’ means the character is the beginning of one word, ‘I’ means the character is inside one word, ‘E’ means the character is at the end of one word and ‘S’ means the character is one word by itself. For the NER system, different tag sets are defined for different corpuses. Table 1 shows the (1) T t t t = 1 2 ... n n max P(t T=arg 1t2...tn |c1c2 ... cn * ) tn ... ) P ∏ i = 1 (ti |ci 146 Proceedings of the Fifth SIGHAN Workshop</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>F.Sebastiani. 2002. Machine learning in automated text categorization. ACM Computing Surveys. 34(1): 1-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Uchimoto</author>
</authors>
<title>Multilingual Aligned Parallel Treebank Corpus Reflecting Contextual Information and its Applications.</title>
<date>2004</date>
<booktitle>In Proceedings of the MLR</booktitle>
<pages>63--70</pages>
<marker>Uchimoto, 2004</marker>
<rawString>K.Uchimoto et al. 2004. Multilingual Aligned Parallel Treebank Corpus Reflecting Contextual Information and its Applications. In Proceedings of the MLR 2004. pp. 63-70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
</authors>
<title>Building a Large-Scale Annotated Chinese Corpus.</title>
<date>2002</date>
<booktitle>In COLING</booktitle>
<marker>Xue, 2002</marker>
<rawString>N.Xue et al. 2002. Building a Large-Scale Annotated Chinese Corpus. In COLING 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y J Zhang</author>
</authors>
<title>Building an Annotated Japanese-Chinese Parallel Corpus – A part of NICT Multilingual Corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the MT SummitX.</booktitle>
<pages>71--78</pages>
<marker>Zhang, 2005</marker>
<rawString>Y.J.Zhang et al. 2005. Building an Annotated Japanese-Chinese Parallel Corpus – A part of NICT Multilingual Corpora. In Proceedings of the MT SummitX. pp. 71-78.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>