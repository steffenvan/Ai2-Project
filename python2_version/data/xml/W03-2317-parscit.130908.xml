<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.981492">
Experiments with discourse-level choices and readability
</title>
<author confidence="0.982154">
tSandra Williams, tEhud Reiter and tLies1 Osman
</author>
<affiliation confidence="0.9975475">
1-Department of Computing Science, tDepartment of Medicine and Therapeutics
University of Aberdeen
</affiliation>
<email confidence="0.99511">
iswilliam,ereiterl@csd.abdn.ac.uk med078@abdn.ac.uk
</email>
<sectionHeader confidence="0.996594" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998512">
This paper reports on pilot experiments
that are being used, together with corpus
analysis, in the development of a Natural
Language Generation (NLG) system,
GIRL (Generator for Individual Reading
Levels). GIRL generates reports for indi-
viduals after a literacy assessment.
We tested GIRL&apos; s output on adult learner
readers and good readers. Our aim was to
find out if choices the system makes at
the discourse-level have an impact on
readability. Our preliminary results indi-
cate that such choices do indeed appear to
be important for learner readers. These
will be investigated further in future
larger-scale experiments. Ultimately we
intend to use the results to develop a
mechanism that makes discourse-level
choices that are appropriate for individu-
als&apos; reading skills.
</bodyText>
<sectionHeader confidence="0.997778" genericHeader="keywords">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999858157894737">
The Generator for Individual Reading Levels
(GIRL) project is developing a Natural Language
Generation (NLG) system that generates feed-
back reports for adults after a web-based literacy
assessment (Williams 2002).
The literacy assessment was designed by
NFER-Nelson for the Target Skills application
(2002) and it is aimed at adults with poor basic
literacy. It produces a multi-level appraisal of a
candidate&apos;s literacy skills It tests eight skills:
letter recognition, sentence completion, word
ordering, form filling, punctuation and capitals,
spelling, skimming and scanning and listening.
The entire assessment consists of ninety ques-
tions, but the more difficult tests are only given
to stronger candidates who have scored well on
earlier tests. All questions are multiple choice.
Figure 1 shows a screenshot of a typical question
in the sentence completion test.
</bodyText>
<figure confidence="0.7227205">
ApeaerIce_CompleMoriQue.ork=1-1.1icrosoftlrfternetEXWOrer
D°&apos; 1-1-140mme,,
</figure>
<figureCaption confidence="0.999766">
Figure 1. A screen shot of a literacy test question.
</figureCaption>
<bodyText confidence="0.999826583333333">
In our implementation, each question is as-
sembled on-the-fly by a web server program
which retrieves question data (question text,
graphics, audio file and multiple-choice answers)
from a database. As each question page is
downloaded, an audio file of spoken instructions
plays automatically. A candidate can play the
instructions again by clicking on the audio player
graphic.
The inputs to the NLG system, GIRL, are the
answers a candidate gives to questions in the lit-
eracy assessment. GIRL currently generates a
</bodyText>
<figure confidence="0.9559545">
He it .,,ew 5,,rites Dols He
rr
C
MEM Eel
Click on the word that fits the
sentence.
The film starts at seven
o&apos;clock
r ticket
r minute
r cinema
r hours
</figure>
<page confidence="0.986655">
127
</page>
<bodyText confidence="0.9892595">
feedback report after every test in the assessment.
An example is shown in Figure 2.
</bodyText>
<keyword confidence="0.346764">
Fred Bloggs,
</keyword>
<sectionHeader confidence="0.796072" genericHeader="introduction">
ALPHABET LETTERS
</sectionHeader>
<bodyText confidence="0.906230666666667">
You finished the ALPHABET LETTERS TEST. Well done.
You got eight out of ten, so you did very well.
Sometimes you did not pick the right letter. For example, you
did not click on: d.
Many people find learning letters hard, but you can do it.
If you practise reading, then your skills will improve.
</bodyText>
<figureCaption confidence="0.999432">
Figure 2. A type A &amp;quot;easy&amp;quot; report generated by GIRL.
</figureCaption>
<bodyText confidence="0.999983142857143">
GIRL is being developed with the goal of tai-
loring output texts to the individual reading skills
of users (readers). In working towards this goal,
we hope to find out more about generating
documents for readers at different reading levels,
how to test a system on real users, and how to
implement reading level decision-making
mechanisms as part of the generation process,
and which decisions produce the most marked
impact on the readability of the output texts.
It is very important to base the development of
this system on solid empirical evidence rather
than on our own intuitions. There has been very
little empirical work on what kinds of texts are
most appropriate for people with good reading
skills and even less on what is appropriate for
people with poor literacy. We were therefore
motivated to do our own empirical studies. We
are attacking the problem on two fronts: corpus
analysis (Williams and Reiter 2003) and experi-
ments with real readers, the subject of this paper.
</bodyText>
<subsectionHeader confidence="0.991031">
1.1 Readability
</subsectionHeader>
<bodyText confidence="0.995893454545455">
Following Kintsch and Vipond (1979), we relate
readability directly to readers&apos; performance on
the reading task (i.e. reading speed, ability to an-
swer comprehension questions and ability to re-
call content). In these experiments, we measured
reading speed and comprehension. We also
analysed errors made in reading aloud, but that is
not described here. The measures of readability
we use are thus quantitative and are based on the
hypotheses that readability increases or decreases
with:
</bodyText>
<listItem confidence="0.9827118">
• an increase or decrease in the average
reading rate for a particular reader;
• an increase or decrease in the number of
correct answers given to comprehension
questions.
</listItem>
<sectionHeader confidence="0.907082" genericHeader="method">
1.2 Related work
</sectionHeader>
<bodyText confidence="0.9999898">
We decided to investigate the impact of dis-
course-level choices as a novel approach to the
problem of how to modify reports for different
reading levels. Related work can be found in the
PSET project (Devlin et al. 2000). PSET investi-
gated how lexical-level choices and syntactic-
level choices affect readability for aphasic read-
ers, but it did not consider discourse choices.
As we mentioned above, there is very little
empirical work to date on the impact of NLG
system choices on readability and this is why it is
so important for this project to carry out empiri-
cal work. One exception is the SPOT project
(Walker et al. 2002). SPOT investigated which
types of system outputs readers prefer. Readers
were asked to rate the system&apos;s output utterances
on understandability, well-formedness and ap-
propriateness for the dialogue context. Apart
from understandability, these do not relate to
readability and we cannot assume that readers
always choose the most readable utterances.
They could be influenced by many other factors
such as style. Also, all the judges were good
readers and their preferences may not in any case
be appropriate for learner readers.
</bodyText>
<sectionHeader confidence="0.976555" genericHeader="method">
2. The NLG system
</sectionHeader>
<bodyText confidence="0.999811">
A data-to-text NLG system like GIRL is able to
linguistically express its output in a variety of
ways that might affect readability. Here we look
at features the microplanner can vary when lin-
guistically realising discourse relations.
</bodyText>
<subsectionHeader confidence="0.944609">
2.1 Deriving microplanner rules from a
corpus analysis
</subsectionHeader>
<bodyText confidence="0.9998785">
Decisions about realising discourse relations are
made in a module called the microplanner. The
input is a tree of discourse relations joining
pieces of information. It plans how the informa-
tion from the tree will be ordered, whether it will
be marked with discourse cue phrases (e.g. &apos;but&apos;,
</bodyText>
<page confidence="0.996878">
128
</page>
<bodyText confidence="0.9990555">
&apos;if&apos; and &apos;for example&apos;), and how it will be packed
into sentences and punctuated.
To determine how human writers make these de-
cisions for good readers, we carried out a corpus
analysis (Williams and Reiter 2003). We used the
RST Discourse Treebank Corpus (Carlson et al.
2002). The discourse relations analysed were
concession, condition, elaboration-additional,
evaluation, example, reason and restatement. The
features we analysed are listed below.
</bodyText>
<listItem confidence="0.98908859375">
• Text span order. The order of text spans
in discourse relations. For instance &amp;quot;be-
cause you got four out of ten, you need to
practise&amp;quot; or &amp;quot;you need to practise because
you got four out of ten&amp;quot;.
• Cue phrase existence and selection.
Whether cue phrases are present in a rela-
tion, or not, and which ones are used. For
instance, cue phrases if and then are both
present in &amp;quot;if you practise, then you will
improve&amp;quot;, but not in &amp;quot;if you practise, you
will improve&amp;quot;.
• Cue phrase position. The positions where
cue phrases are located. For instance, for
example is before the text span in &amp;quot;for ex-
ample, you did not click on the letter D&amp;quot;,
mid-span in &amp;quot;you did not click, for exam-
ple, on the letter D&amp;quot;, and after it in &amp;quot;you
did not click on the letter D, for example&amp;quot;.
At present, GIRL can only handle posi-
tions before and after.
• Existence and selection of between-span
punctuation. Sometimes there is punctua-
tion between texts spans, e.g. the comma
in &amp;quot;many people find learning letters hard,
but you can do it&amp;quot; and the full stop in &amp;quot;you
finished the Alphabet Letters test. Well
done&amp;quot;, sometimes there is none e.g. &amp;quot;many
people find learning letters hard but you
can do it&amp;quot;.
• First text span length. The length of the
first text span in words.
</listItem>
<bodyText confidence="0.999976807692308">
The inspiration for choosing the first four fea-
tures was Moser and Moore&apos;s analysis (1996).
The features are interdependent. For instance,
choosing a particular ordering of text spans can
constrain the choice of cue phrase. For instance,
the first span in a relation can never have cue
phrase but (Williams and Reiter 2003). The cor-
pus analysis was therefore extremely useful in
deriving a set of rules for choosing legal combi-
nations of features for each relation.
We hypothesised that certain values for fea-
tures were more likely to increase readability.
Commas between segments make the discourse
structure more explicit. Sentence-breaking
punctuation gives shorter sentences and selection
of short, common cue phrases can help learner
readers. Sentence length and word length are
both believed to have a major impact on read-
ability (Flesch 1949).
The corpus analysis results were input to ma-
chine learning algorithms to derive decision trees
and rules for GIRL&apos; s microplanner. But the
analysis they are based on was a corpus written
for good readers and we need data to adapt them
for learner readers, so we carried out the experi-
ments described here.
</bodyText>
<subsectionHeader confidence="0.96429">
2.2 Modifications for the experiments
</subsectionHeader>
<bodyText confidence="0.999773444444444">
For the experiments, the system was modified to
generate much shorter, more restricted, reports
than those of the original GIRL system (Williams
2002). It was also modified to produce eight re-
ports, one after each section of the literacy test,
rather than a single long report after the entire
test. These modifications increased our chances
of collecting some reading data from each stu-
dent, even if the student did not complete the en-
tire literacy assessment (see section 4). Each
short report consists of a salutation, a heading
and exactly five paragraphs. Each paragraph
consists of exactly one discourse relation. The
system can produce two versions of each report,
A and B (see Table 1).
In text types A and B, discourse relations were
generated by varying only one discourse feature
per paragraph. Table 1 shows which features
were varied. Based on our corpus analysis results
and on psycholinguistic evidence, we hypothe-
sised that type A reports would be more readable
(&amp;quot;easier&amp;quot;) than type B reports (&amp;quot;harder&amp;quot;).
Figure 2, shows an example of a type A report
and Figure 3, a type B report. The text spans in
the first paragraph are in statement:evaluation
order in A and evaluation:statement in B. The
second paragraph includes the cue phrase so in
</bodyText>
<page confidence="0.993229">
129
</page>
<bodyText confidence="0.9999075">
A, and therefore in B. The third paragraph has
for example before the second span in A and after
it in B. The fourth paragraph has a comma pres-
ent between text spans in A and no comma in B.
Finally, the fifth paragraph has cue phrase then
present in A, but not in B.
</bodyText>
<subsectionHeader confidence="0.40559">
Fred Bloggs,
MISSING WORDS
</subsectionHeader>
<bodyText confidence="0.760998333333333">
Well done. You finished the MISSING WORDS TEST.
You got four out of fifteen, therefore you need to practise.
Sometimes you did not click on the right word. You did not
pick: recycle, for example.
Many people find learning words hard but you can do it.
If you practise reading, your skills will improve.
</bodyText>
<figureCaption confidence="0.969469">
Figure 3, A type B &amp;quot;harder&amp;quot; report generated by GIRL
</figureCaption>
<bodyText confidence="0.999759846153846">
Varying options in the manner shown in Table
1 ignores any crossover effects since there are
actually 32 text types which would be possible
from a 2x2x2x2x2 matrix of features. Also, any
cumulative effects from having more than one
discourse relation per paragraph are ignored in
this design. We chose a simplified experimental
design, since this was a pilot experiment. We
wanted to test a large number of options and
were aiming only to get indications of which op-
tions would have the greatest effects on readabil-
ity. Future, more detailed experiments should
look at crossover and cumulative effects.
</bodyText>
<sectionHeader confidence="0.7774075" genericHeader="method">
3. Experiments
3.1 Participants
</sectionHeader>
<bodyText confidence="0.999996190476191">
There were twenty-seven participants over the
entire series of pilot experiments: twenty-one
adults on basic skills literacy programmes
(learner readers), four Ph.D. students and on
other good readers. Because the design of the
experiment was evolving, the conditions changed
and we only use results from the final version.
That is, nine learners and five good readers for
reading speed and eleven learners for compre-
hension.
People who register for literacy courses are
poor readers for a variety of reasons such as:
missed school, learning difficulties, dyslexia,
poor eyesight, poor hearing, short-term memory
problems, or a combination of these. Personal
data was recorded for each participant including
age range, gender, first language, eyesight prob-
lems, hearing problems and any known reading
problems (e.g. dyslexia). This data could be used
to sub-classify readers, but the number of partici-
pants was too small to do this.
</bodyText>
<subsectionHeader confidence="0.996809">
3.2 Method
</subsectionHeader>
<bodyText confidence="0.999961125">
Each participant underwent a web-based literacy
assessment as described in the Introduction. After
completion of each test in the assessment, GIRL
generated feedback on how well the participant
had done. The report is one of the two types de-
scribed above and chosen by the system at ran-
dom. In total, each participant was presented with
between three and five reports of type A and
three to five of type B.
Each participant was recorded reading his/her
reports aloud, rather than recording silent reading
times. This is because we discovered in an ear-
lier pilot that following the more usual procedure
of asking participants to read silently and then
click a button led to erroneous reading times for
learners (Williams 2002). We could not be cer-
tain whether they had actually &apos;read&apos; the reports
or not. Recordings provide evidence that reading
has, in fact, occurred.
The recordings were made digitally and were
annotated by hand by the first author using
CSLU&apos; s SpeechViewer software (Hosom et al.
1998). The speech waveforms were annotated
with beginnings and ends of words and pauses
</bodyText>
<table confidence="0.998581571428572">
Paragraph Discourse Feature Varied Report type A Report type B
relation &amp;quot;easier&amp;quot; &amp;quot;harder&amp;quot;
1 evaluation text span order statement:evaluation evaluation:statement
2 reason/result cue phrase choice &amp;quot;so&amp;quot; &amp;quot;therefore&amp;quot;
3 example cue phrase position before segment after segment
4 concession comma between spans comma no comma
5 condition existence of cue phrase &amp;quot;if&amp;quot; and &amp;quot;then&amp;quot; &amp;quot;if&amp;quot; only
</table>
<tableCaption confidence="0.999962">
Table 1. The discourse features varied in each paragraph in reports type A and B.
</tableCaption>
<page confidence="0.994142">
130
</page>
<bodyText confidence="0.999248">
and with reading errors. Using the resulting an-
notation files, timings for each word, pause and
paragraph could be calculated accurately (to
within, say, 10ms). Only paragraph time and
some pause times are used here
</bodyText>
<figureCaption confidence="0.9968365">
Figure 4. Comprehension questions are presented
alongside a second view of the first report
</figureCaption>
<bodyText confidence="0.999783034482759">
After a participant had seen a screen showing
his/her first report, had read aloud from that re-
port and had been recorded, comprehension
questions were presented (see Figure 4). The
questions are displayed alongside a second view
of the report and thus involved only comprehen-
sion, not recall. The experimenter read the ques-
tions aloud to learners, if necessary. Questions
asked the meaning of information items in the
report and of certain discourse relations. For ex-
ample, question three is a &apos;why&apos; question to de-
termine if the reader has understood the relation
in the second paragraph. Comprehension ques-
tions were administered only once, because an
earlier pilot demonstrated that the meanings of
each report are similar enough to prime readers.
On completion of the entire literacy assess-
ment, an overall literacy level was calculated.
This is subdivided into overall reading, overall
writing and overall listening. Overall reading
and overall writing are further subdivided into
word focus, sentence focus and text focus scores.
If there was time after the experiment, infor-
mal chats with each participant provided useful
information about readers&apos; attitudes to the as-
sessment and the reports. Participants offered
ideas and suggestions for improvements. Basic
skills tutors who were present during the pilots
offered valuable suggestions.
</bodyText>
<sectionHeader confidence="0.999357" genericHeader="evaluation">
4. Results
</sectionHeader>
<bodyText confidence="0.999834928571429">
Not all learners managed to complete the literacy
assessment because of time limits Some learners
and all good readers completed the test within an
hour. Other learners took much longer, with one
taking four hours! Two learners did not wish to
be recorded reading aloud (although the majority
of people were willing, sometimes even eager, to
be recorded). Also, some recordings turned out
to be too noisy. So we do not have a complete set
of recordings for every person. For fourteen peo-
ple, a maximum of 154 full text recordings were
possible (720 paragraphs). We have good re-
cordings of 297 paragraphs. All eleven learners
completed the comprehension test.
</bodyText>
<subsectionHeader confidence="0.998942">
4.1 Reading speed
</subsectionHeader>
<bodyText confidence="0.999714235294118">
Reading speeds were calculated in milliseconds
per word (ms/word). Individuals, and particularly
learner readers, vary a great deal in their reading
aloud rates, so we calculated adjusted reading
times for each person. The adjusted time is an
individual&apos;s raw time per word for a single para-
graph less that same person&apos;s average time per
word over all of his/her recordings. In other
words, adjusted times are a person&apos;s deviations
from his/her average time. If the adjusted time is
zero, then it is the same as that person&apos;s average
time. A negative adjusted time means the person
read faster and a positive adjusted time means
they read slower. We were thus able to compare
reading times for both versions of a paragraph,
for all readers and calculate which version was
read faster.
</bodyText>
<table confidence="0.940785142857143">
4.1.1 Paragraph 1: order of text spans
statement evaluation
:evaluation :statement
# adj.time # adj.time
Learners 26 -111.4 12 -47.1
Good 23 -37.3 16 -30.6
readers
</table>
<tableCaption confidence="0.953555">
Table 2. Mean adjusted times in ms/word on two or-
derings, where # = number of samples.
</tableCaption>
<bodyText confidence="0.9738558">
Table 2 shows that learner readers read state-
ment:evaluation order on average 64.3ms/word
faster than evaluation:statement order. This re-
sult agrees with our RST Discourse Treebank
corpus analysis (Williams and Reiter 2003)
</bodyText>
<figure confidence="0.894975035714286">
Ily Molly Mandy,
ALPHABET
LETTERS
Well done. You&apos; finished the
ALPHABET LETTERS test.
Youigot seven out often,
therefore you need to practise.
Sometimes you did not pick the
right letter. You did not click on:
N, for example!
Many people find learning letters
hard hut you can do it.
6, What most you do to improve?
QUESTIONS
1. What was the test about?
C dont know r words r letters
2. How many did you get right?
3. Why do you need to practise?
C I got some wrong, Cl got
them all right. Cl dont know
4. Type one letter you got wrong
here: I
5. What can you do, even if it is
hard?
C learn to swim r don&apos;t know
C learn letters
If you practise reading, your
skills will improve.
</figure>
<page confidence="0.99275">
131
</page>
<bodyText confidence="0.9997022">
where we found that the statement:evaluation
order is far more common In fact we found this
ordering present in 86% of evaluation relations.
There is little difference in times for good readers
(6.7 ms/word).
</bodyText>
<subsubsectionHeader confidence="0.502035">
4.1.2 Paragraph 2: cue phrase selection
</subsubsectionHeader>
<bodyText confidence="0.999619777777778">
Table 3 shows that learner readers read relations
with the cue phrase so on average 90.8ms/word
faster than those containing therefore. Good
readers&apos; times showed a small difference of only
6.4ms/word. It could be argued that these differ-
ences are due to the fact that therefore has more,
and longer, syllables than so. However, if this
were the reason, then the differences would be
the same for both tvoes of reader.
</bodyText>
<table confidence="0.9976796">
&amp;quot;so&amp;quot; &amp;quot;therefore&amp;quot;
# adj.time # adj.time
Learners 20 -126.3 12 -35.5
Good 13 -60.8 11 -54.4
readers
</table>
<tableCaption confidence="0.9901325">
Table 3, Average adjusted times in ms/word for so and
therefore, where # = number of samples
</tableCaption>
<table confidence="0.993735142857143">
4.1.3 Paragraph 3: cue phrase position
before 2-&apos;span after 2-. span
# adj. # adj. time
time
Learners 20 7.5 12 -25.0
Good 13 17.0 15 13.8
readers
</table>
<tableCaption confidence="0.95058">
Table 4, Average adjusted times in ms/word with for
example before or after the 2lld span, where # = num-
ber of samples
</tableCaption>
<bodyText confidence="0.9880663">
Table 4 shows that learner readers were on aver-
age 32.5ms/word faster when for example was
positioned after the second span, compared to
before it. Again, there is little difference in times
for good readers (3.2ms/word). The result for
learners was unexpected because we thought
people would read faster when they were told in
advance that the information they were about to
read was going to be an example (i.e. when for
example is before the second span). If it is after
the span, they have to re-evaluate the information
they have just read. Also, we found very few ex-
amples of the after position in our RST Discourse
Treebank analysis (Williams and Reiter 2003).
Scarcity of the easier-to-read version in the cor-
pus may provide further evidence for Ober-
lander&apos;s theory (Oberlander 1998) that writers do
not always &apos;do the right thing&apos; for readers. This
result will be investigated further in future ex-
periments.
</bodyText>
<subsubsectionHeader confidence="0.538523">
4.1.4 Paragraph 4: between-span comma
</subsubsectionHeader>
<bodyText confidence="0.998257727272727">
Table 5 shows that learner readers read this para-
graph on average 26.8ms/word faster when a
comma was present. This is what we expected.
The comma between text spans indicates the dis-
course structure more explicitly and we would
expect it to help learner readers. Once again
there is little difference in times for good readers
(5.6ms/word). Since the cue phrase is present
before the second span, the comma may be re-
dundant for good readers. Future experiments
will investigate this.
</bodyText>
<tableCaption confidence="0.926153333333333">
Table 5, Average adjusted times in ms/word for a
between-span comma or no comma, where # = num-
ber of samples
</tableCaption>
<table confidence="0.983584833333333">
4.1.5 Paragraph 5: presence of second cue
then no then
adj. adj.
time time
Learners 18 -8.3 12 -37.3
Good readers 7 -57.8 9 -51.0
</table>
<tableCaption confidence="0.886898666666667">
Table 6, Average adjusted times in ms/word with and
without cue phrase then where # = number of samples
Table 6 shows that learner readers read relations
</tableCaption>
<bodyText confidence="0.967971909090909">
with no second cue phrase 29.0ms/word faster.
Good readers showed little difference in times
(6.8ms/word). This is not what we expected. We
expected the second cue phrase to help learners
because it makes the condition relation more ex-
plicit when both if and then are present. This re-
sult ties in with our corpus analysis (Williams
and Reiter 2003) where few cases with both cues
present were found. Writers do not often use both
cue phrases and learner readers seem to find an
extra phrase adds difficulty rather than helping.
</bodyText>
<subsectionHeader confidence="0.967814">
4.1.6 Sentence length
</subsectionHeader>
<bodyText confidence="0.9999802">
The figures for reading times vs. sentence
length show that all readers are slower on sen-
tences above 23 words in length, and some learn-
ers are slower above 18 words. We require more
data to verify this.
</bodyText>
<subsectionHeader confidence="0.988185">
4.2 Comprehension
</subsectionHeader>
<bodyText confidence="0.999945666666667">
We found that learner readers can have problems
with answering comprehension questions, even
when the questions are administered verbally.
Some learner readers are unfamiliar with rea-
soning about textual meanings. Some find it very
hard to create answers using different words from
</bodyText>
<figure confidence="0.998041923076923">
comma
no comma
12
Learners
-32.8
-59.6
20
-69.9
9
-64.3
Good readers
adj.time
adj.time
</figure>
<page confidence="0.985638">
132
</page>
<bodyText confidence="0.99925375">
those that are present in the text they have just
read. We therefore implemented a version of the
system that generated more explanation for each
paragraph (discourse relation), see Figure 5.
</bodyText>
<figureCaption confidence="0.782731875">
Figure 5. &apos;Explanation&apos; text
Figure 6 shows the comprehension scores for
eleven learner readers. Scores, shown on the x-
axis, are number of questions answered correctly
out of six. Learner readers&apos; scores are shown as
gray bars and their mean scores are black bars.
Figure 6, Grey bars = Learner readers&apos; scores, Black
bars = Mean learner readers&apos; scores
</figureCaption>
<bodyText confidence="0.999920428571429">
Learners&apos; highest mean comprehension score
was on the explanation text type, but there is lit-
tle difference between this and other mean
scores. We simplified comprehension questions
at the same time as introducing the explanation
text, so we need to do further experiments to de-
termine which has the most impact.
</bodyText>
<sectionHeader confidence="0.998553" genericHeader="conclusions">
5. Conclusions
</sectionHeader>
<bodyText confidence="0.97431925">
We previously analysed a corpus to determine
how writers linguistically realise a number of
discourse relations (Williams and Reiter 2003).
Since the features analysed for each relation are
interdependent (see section 2.2). Interdependen-
cies can be conceptualised as a matrix like that
shown in Table 7, where each cell (shown blank)
actually contains rules (e.g. length vs. punctua-
tion rules might be 1-10 words -&gt; comma and
&gt;10 words -&gt; full stop).
cue choice
cue position
</bodyText>
<tableCaption confidence="0.918341">
Table 7. Interdependencies of features for one dis-
course relation
</tableCaption>
<bodyText confidence="0.9999555625">
The corpus analysis results were input to ma-
chine learning algorithms to derive decision trees
and sets of rules for GIRL&apos; s microplanner, so that
given input text spans of fixed lengths linked in a
discourse relation tree, it can determine ordering,
between-span punctuation, cue choice and cue
position. Since the corpus analysis was based on
a corpus written for good readers, we required
data from experiments like this to adapt the rules
for learner readers. To find out in detail how to
adapt each cell of the matrix for each relation, we
need more extensive experiments than these.
Nevertheless, our pilot experiments are a good
start. They enabled us to develop and refine our
experimental method. Our preliminary reading
speed results show:
</bodyText>
<listItem confidence="0.892161">
• Text span order. Learners were slightly
faster reading statement:evaluation order.
Good readers&apos; speeds showed only small
differences.
• Cue phrase choice. Learners were faster
reading relations containing so than those
containing therefore. Good readers were
also slightly faster reading so. This result
was the only statistically significant one.
• Cue phrase position Learner readers were
slightly faster when for example was posi-
tioned after the second segment. The po-
sition made very little difference to good
readers.
</listItem>
<figure confidence="0.9734181">
a.)
punctuation
cue choice
w
I length
order
punctuation
Fred Bloggs,
MISSING WORDS
Well done. You finished the MISSING WORDS TEST.
</figure>
<figureCaption confidence="0.504157">
You got four out of fifteen. You made eleven mistakes. That
means you need to practise.
Here is one you got wrong. You did not pick: recycle.
Many people find learning words hard. Perhaps you find it
hard? You can do it.
</figureCaption>
<bodyText confidence="0.968169">
The more you practise reading, the more your skills will
improve.
</bodyText>
<page confidence="0.995634">
133
</page>
<listItem confidence="0.9355328">
• Presence of punctuation. Learner readers
were slightly faster when there was a
comma between discourse segments. This
made very little difference to good readers.
• Cue phrase existence. Learner readers
</listItem>
<bodyText confidence="0.962828303030303">
were slightly faster when then is not pres-
ent. This made very little difference to
good readers.
Sentence length and comprehension results re-
quire further investigation. The reading speed
results indicate that discourse realisation choices
make a greater impact on the reading speeds of
learner readers than on those of good readers.
This is an important first step in acquiring em-
pirical evidence from real readers who have poor
literacy skills. Discourse-level choices do indeed
make a difference for these readers. This infor-
mation is very valuable for the development of
the GIRL NLG system.
We require more extensive, larger-scale ex-
periment to derive rules appropriate for adapting
our existing corpus-analysis-based models to in-
dividuals&apos; reading skills We need to know the
impact of each feature on all the others. For in-
stance (a) and (b), below, are almost equally
likely according to our previous corpus analysis
of the condition relation:
a) If you need help, ask your tutor.
b) Ask your tutor if you need help.
These experiments have shown that learners
find a easier than b, since it has a comma. How-
ever, the order of text spans in b could be easier.
We do not yet know how each feature affects the
others and which has the most impact. Nor do we
yet know if readability changes for a particular
feature as the discourse relation changes. The
choices seem deceptively simple, but their impact
on people with poor literacy can be considerable.
</bodyText>
<sectionHeader confidence="0.996524" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999436111111111">
This research is supported by an EPSRC student-
ship. We are grateful to: the tutors and students
who took part in experiments at: Moray College,
South Shields College, Southampton City Col-
lege, Banff and Buchan College and the Univer-
sity of Aberdeen; CTAD Ltd. for their help and
advice and for permission to use the Target Skills
Literacy Assessment; Somayajulu Sripada and
the EWNLG reviewers for comments and advice.
</bodyText>
<sectionHeader confidence="0.994224" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999609195652174">
Lynn Carlson, Daniel Marcu, and Mary Ellen Oku-
rowski. 2002. Building a Discourse-Tagged Corpus
in the Framework of Rhetorical Structure Theory.
In Current Directions in Discourse and Dialogue,
Jan van Kuppevelt and Ronnie Smith (eds.), Klu-
wer Academic Publishers.
Siobhan Devlin, Yvonne Canning, John Tait, John
Carroll, Guido Minnen and Darren Pearce. 2000.
An AAC aid for aphasic people with reading diffi-
culties. In Proceeding of the 9th Biennial Confer-
ence of the International Society for Augmentative
and Alternative Communication (ISADC 2000).
August 2000, Washington D.C.
Rudolph Flesch. 1949. The Art of Readable Writing.
Harper. USA.
John-Paul Hosom, Mark Fanty, Pieter Vermeulen,
Ben Serridge and Tim Carmel. 1998. The Center
for Spoken Language Understanding, Oregan
Graduate Institute for Science and Technology.
Walter Kintsch and Douglas Vipond. 1979. Reading
Comprehension and Readability in Educational
Practice and Psychological Theory. In L.G. Nils-
son (ed.) Perspectives on Memory Research. Law-
rence Erlbaum.
Megan Moser and Johanna Moore 1996 On the corre-
lation of cues with discourse structure: results from
a corpus study. Unpublished manuscript.
Jon Oberlander. 1998. Do the Right Thing ...but Ex-
pect the Unexpected. Computational Linguistics.
24, 3. 501-507
Target Skills Literacy Assessment. 2002. Published by
the Basic Skills Agency, in association with Cam-
bridge Training and Development Ltd. and NFER-
Nelson Ltd.
Marilyn Walker, Owen Rambow and Monica Rogati.
2002. Training a sentence planner for spoken dia-
logue using boosting. In Computer Speech and
Language. 16, 409-433.
Sandra Williams. 2002. Natural Language Generation
of discourse connectives for different reading lev-
els. In Proceedings of the 5th Annual CLUK Re-
search Colloquium.
Sandra Williams and Ehud Reiter. 2003. A corpus
analysis of discourse relations for Natural Lan-
guage Generation. To appear in Proceedings of
Corpus Linguistics 2003.
</reference>
<page confidence="0.998618">
134
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.601067">
<title confidence="0.741777">Experiments with discourse-level choices and readability</title>
<author confidence="0.662586">tSandra Williams</author>
<author confidence="0.662586">tEhud Reiter</author>
<author confidence="0.662586">tLies</author>
<affiliation confidence="0.961561">1-Department of Computing Science, tDepartment of Medicine and University of Aberdeen</affiliation>
<email confidence="0.941972">iswilliam,ereiterl@csd.abdn.ac.ukmed078@abdn.ac.uk</email>
<abstract confidence="0.998189952380952">This paper reports on pilot experiments that are being used, together with corpus analysis, in the development of a Natural Language Generation (NLG) system, for Individual Reading Levels). GIRL generates reports for individuals after a literacy assessment. tested output on adult learner readers and good readers. Our aim was to find out if choices the system makes at the discourse-level have an impact on results indicate that such choices do indeed appear to be important for learner readers. These will be investigated further in future larger-scale experiments. Ultimately we intend to use the results to develop a mechanism that makes discourse-level choices that are appropriate for individuals&apos; reading skills.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lynn Carlson</author>
<author>Daniel Marcu</author>
<author>Mary Ellen Okurowski</author>
</authors>
<date>2002</date>
<booktitle>Building a Discourse-Tagged Corpus in the Framework of Rhetorical Structure Theory. In Current Directions in Discourse and Dialogue, Jan</booktitle>
<editor>van Kuppevelt and Ronnie Smith (eds.),</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="6910" citStr="Carlson et al. 2002" startWordPosition="1096" endWordPosition="1099">iving microplanner rules from a corpus analysis Decisions about realising discourse relations are made in a module called the microplanner. The input is a tree of discourse relations joining pieces of information. It plans how the information from the tree will be ordered, whether it will be marked with discourse cue phrases (e.g. &apos;but&apos;, 128 &apos;if&apos; and &apos;for example&apos;), and how it will be packed into sentences and punctuated. To determine how human writers make these decisions for good readers, we carried out a corpus analysis (Williams and Reiter 2003). We used the RST Discourse Treebank Corpus (Carlson et al. 2002). The discourse relations analysed were concession, condition, elaboration-additional, evaluation, example, reason and restatement. The features we analysed are listed below. • Text span order. The order of text spans in discourse relations. For instance &amp;quot;because you got four out of ten, you need to practise&amp;quot; or &amp;quot;you need to practise because you got four out of ten&amp;quot;. • Cue phrase existence and selection. Whether cue phrases are present in a relation, or not, and which ones are used. For instance, cue phrases if and then are both present in &amp;quot;if you practise, then you will improve&amp;quot;, but not in &amp;quot;</context>
</contexts>
<marker>Carlson, Marcu, Okurowski, 2002</marker>
<rawString>Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski. 2002. Building a Discourse-Tagged Corpus in the Framework of Rhetorical Structure Theory. In Current Directions in Discourse and Dialogue, Jan van Kuppevelt and Ronnie Smith (eds.), Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siobhan Devlin</author>
<author>Yvonne Canning</author>
<author>John Tait</author>
<author>John Carroll</author>
<author>Guido Minnen</author>
<author>Darren Pearce</author>
</authors>
<title>An AAC aid for aphasic people with reading difficulties.</title>
<date>2000</date>
<booktitle>In Proceeding of the 9th Biennial Conference of the International Society for Augmentative and Alternative Communication (ISADC</booktitle>
<location>Washington D.C.</location>
<contexts>
<context position="5091" citStr="Devlin et al. 2000" startWordPosition="800" endWordPosition="803">lso analysed errors made in reading aloud, but that is not described here. The measures of readability we use are thus quantitative and are based on the hypotheses that readability increases or decreases with: • an increase or decrease in the average reading rate for a particular reader; • an increase or decrease in the number of correct answers given to comprehension questions. 1.2 Related work We decided to investigate the impact of discourse-level choices as a novel approach to the problem of how to modify reports for different reading levels. Related work can be found in the PSET project (Devlin et al. 2000). PSET investigated how lexical-level choices and syntacticlevel choices affect readability for aphasic readers, but it did not consider discourse choices. As we mentioned above, there is very little empirical work to date on the impact of NLG system choices on readability and this is why it is so important for this project to carry out empirical work. One exception is the SPOT project (Walker et al. 2002). SPOT investigated which types of system outputs readers prefer. Readers were asked to rate the system&apos;s output utterances on understandability, well-formedness and appropriateness for the d</context>
</contexts>
<marker>Devlin, Canning, Tait, Carroll, Minnen, Pearce, 2000</marker>
<rawString>Siobhan Devlin, Yvonne Canning, John Tait, John Carroll, Guido Minnen and Darren Pearce. 2000. An AAC aid for aphasic people with reading difficulties. In Proceeding of the 9th Biennial Conference of the International Society for Augmentative and Alternative Communication (ISADC 2000). August 2000, Washington D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rudolph Flesch</author>
</authors>
<title>The Art of Readable Writing.</title>
<date>1949</date>
<publisher>Harper. USA.</publisher>
<contexts>
<context position="9174" citStr="Flesch 1949" startWordPosition="1482" endWordPosition="1483"> the first span in a relation can never have cue phrase but (Williams and Reiter 2003). The corpus analysis was therefore extremely useful in deriving a set of rules for choosing legal combinations of features for each relation. We hypothesised that certain values for features were more likely to increase readability. Commas between segments make the discourse structure more explicit. Sentence-breaking punctuation gives shorter sentences and selection of short, common cue phrases can help learner readers. Sentence length and word length are both believed to have a major impact on readability (Flesch 1949). The corpus analysis results were input to machine learning algorithms to derive decision trees and rules for GIRL&apos; s microplanner. But the analysis they are based on was a corpus written for good readers and we need data to adapt them for learner readers, so we carried out the experiments described here. 2.2 Modifications for the experiments For the experiments, the system was modified to generate much shorter, more restricted, reports than those of the original GIRL system (Williams 2002). It was also modified to produce eight reports, one after each section of the literacy test, rather tha</context>
</contexts>
<marker>Flesch, 1949</marker>
<rawString>Rudolph Flesch. 1949. The Art of Readable Writing. Harper. USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John-Paul Hosom</author>
<author>Mark Fanty</author>
<author>Pieter Vermeulen</author>
<author>Ben Serridge</author>
<author>Tim Carmel</author>
</authors>
<date>1998</date>
<institution>The Center for Spoken Language Understanding, Oregan Graduate Institute for Science and Technology.</institution>
<contexts>
<context position="14016" citStr="Hosom et al. 1998" startWordPosition="2289" endWordPosition="2292">to five of type B. Each participant was recorded reading his/her reports aloud, rather than recording silent reading times. This is because we discovered in an earlier pilot that following the more usual procedure of asking participants to read silently and then click a button led to erroneous reading times for learners (Williams 2002). We could not be certain whether they had actually &apos;read&apos; the reports or not. Recordings provide evidence that reading has, in fact, occurred. The recordings were made digitally and were annotated by hand by the first author using CSLU&apos; s SpeechViewer software (Hosom et al. 1998). The speech waveforms were annotated with beginnings and ends of words and pauses Paragraph Discourse Feature Varied Report type A Report type B relation &amp;quot;easier&amp;quot; &amp;quot;harder&amp;quot; 1 evaluation text span order statement:evaluation evaluation:statement 2 reason/result cue phrase choice &amp;quot;so&amp;quot; &amp;quot;therefore&amp;quot; 3 example cue phrase position before segment after segment 4 concession comma between spans comma no comma 5 condition existence of cue phrase &amp;quot;if&amp;quot; and &amp;quot;then&amp;quot; &amp;quot;if&amp;quot; only Table 1. The discourse features varied in each paragraph in reports type A and B. 130 and with reading errors. Using the resulting annot</context>
</contexts>
<marker>Hosom, Fanty, Vermeulen, Serridge, Carmel, 1998</marker>
<rawString>John-Paul Hosom, Mark Fanty, Pieter Vermeulen, Ben Serridge and Tim Carmel. 1998. The Center for Spoken Language Understanding, Oregan Graduate Institute for Science and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Kintsch</author>
<author>Douglas Vipond</author>
</authors>
<date>1979</date>
<booktitle>Reading Comprehension and Readability in Educational Practice and Psychological Theory. In L.G. Nilsson (ed.) Perspectives on Memory Research. Lawrence Erlbaum.</booktitle>
<contexts>
<context position="4229" citStr="Kintsch and Vipond (1979)" startWordPosition="659" endWordPosition="662">ked impact on the readability of the output texts. It is very important to base the development of this system on solid empirical evidence rather than on our own intuitions. There has been very little empirical work on what kinds of texts are most appropriate for people with good reading skills and even less on what is appropriate for people with poor literacy. We were therefore motivated to do our own empirical studies. We are attacking the problem on two fronts: corpus analysis (Williams and Reiter 2003) and experiments with real readers, the subject of this paper. 1.1 Readability Following Kintsch and Vipond (1979), we relate readability directly to readers&apos; performance on the reading task (i.e. reading speed, ability to answer comprehension questions and ability to recall content). In these experiments, we measured reading speed and comprehension. We also analysed errors made in reading aloud, but that is not described here. The measures of readability we use are thus quantitative and are based on the hypotheses that readability increases or decreases with: • an increase or decrease in the average reading rate for a particular reader; • an increase or decrease in the number of correct answers given to </context>
</contexts>
<marker>Kintsch, Vipond, 1979</marker>
<rawString>Walter Kintsch and Douglas Vipond. 1979. Reading Comprehension and Readability in Educational Practice and Psychological Theory. In L.G. Nilsson (ed.) Perspectives on Memory Research. Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Megan Moser</author>
<author>Johanna Moore</author>
</authors>
<title>On the correlation of cues with discourse structure: results from a corpus study.</title>
<date>1996</date>
<note>Unpublished manuscript.</note>
<marker>Moser, Moore, 1996</marker>
<rawString>Megan Moser and Johanna Moore 1996 On the correlation of cues with discourse structure: results from a corpus study. Unpublished manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Oberlander</author>
</authors>
<title>Do the Right Thing ...but Expect the Unexpected.</title>
<date>1998</date>
<journal>Computational Linguistics.</journal>
<volume>24</volume>
<pages>501--507</pages>
<contexts>
<context position="20769" citStr="Oberlander 1998" startWordPosition="3418" endWordPosition="3419">erence in times for good readers (3.2ms/word). The result for learners was unexpected because we thought people would read faster when they were told in advance that the information they were about to read was going to be an example (i.e. when for example is before the second span). If it is after the span, they have to re-evaluate the information they have just read. Also, we found very few examples of the after position in our RST Discourse Treebank analysis (Williams and Reiter 2003). Scarcity of the easier-to-read version in the corpus may provide further evidence for Oberlander&apos;s theory (Oberlander 1998) that writers do not always &apos;do the right thing&apos; for readers. This result will be investigated further in future experiments. 4.1.4 Paragraph 4: between-span comma Table 5 shows that learner readers read this paragraph on average 26.8ms/word faster when a comma was present. This is what we expected. The comma between text spans indicates the discourse structure more explicitly and we would expect it to help learner readers. Once again there is little difference in times for good readers (5.6ms/word). Since the cue phrase is present before the second span, the comma may be redundant for good re</context>
</contexts>
<marker>Oberlander, 1998</marker>
<rawString>Jon Oberlander. 1998. Do the Right Thing ...but Expect the Unexpected. Computational Linguistics. 24, 3. 501-507</rawString>
</citation>
<citation valid="true">
<title>Target Skills Literacy Assessment.</title>
<date>2002</date>
<booktitle>in association with Cambridge Training and Development Ltd. and NFERNelson Ltd.</booktitle>
<contexts>
<context position="1363" citStr="(2002)" startWordPosition="192" endWordPosition="192">hoices do indeed appear to be important for learner readers. These will be investigated further in future larger-scale experiments. Ultimately we intend to use the results to develop a mechanism that makes discourse-level choices that are appropriate for individuals&apos; reading skills. 1. Introduction The Generator for Individual Reading Levels (GIRL) project is developing a Natural Language Generation (NLG) system that generates feedback reports for adults after a web-based literacy assessment (Williams 2002). The literacy assessment was designed by NFER-Nelson for the Target Skills application (2002) and it is aimed at adults with poor basic literacy. It produces a multi-level appraisal of a candidate&apos;s literacy skills It tests eight skills: letter recognition, sentence completion, word ordering, form filling, punctuation and capitals, spelling, skimming and scanning and listening. The entire assessment consists of ninety questions, but the more difficult tests are only given to stronger candidates who have scored well on earlier tests. All questions are multiple choice. Figure 1 shows a screenshot of a typical question in the sentence completion test. ApeaerIce_CompleMoriQue.ork=1-1.1icr</context>
</contexts>
<marker>2002</marker>
<rawString>Target Skills Literacy Assessment. 2002. Published by the Basic Skills Agency, in association with Cambridge Training and Development Ltd. and NFERNelson Ltd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Owen Rambow</author>
<author>Monica Rogati</author>
</authors>
<title>Training a sentence planner for spoken dialogue using boosting.</title>
<date>2002</date>
<journal>In Computer Speech and Language.</journal>
<volume>16</volume>
<pages>409--433</pages>
<contexts>
<context position="5500" citStr="Walker et al. 2002" startWordPosition="871" endWordPosition="874"> to investigate the impact of discourse-level choices as a novel approach to the problem of how to modify reports for different reading levels. Related work can be found in the PSET project (Devlin et al. 2000). PSET investigated how lexical-level choices and syntacticlevel choices affect readability for aphasic readers, but it did not consider discourse choices. As we mentioned above, there is very little empirical work to date on the impact of NLG system choices on readability and this is why it is so important for this project to carry out empirical work. One exception is the SPOT project (Walker et al. 2002). SPOT investigated which types of system outputs readers prefer. Readers were asked to rate the system&apos;s output utterances on understandability, well-formedness and appropriateness for the dialogue context. Apart from understandability, these do not relate to readability and we cannot assume that readers always choose the most readable utterances. They could be influenced by many other factors such as style. Also, all the judges were good readers and their preferences may not in any case be appropriate for learner readers. 2. The NLG system A data-to-text NLG system like GIRL is able to lingu</context>
</contexts>
<marker>Walker, Rambow, Rogati, 2002</marker>
<rawString>Marilyn Walker, Owen Rambow and Monica Rogati. 2002. Training a sentence planner for spoken dialogue using boosting. In Computer Speech and Language. 16, 409-433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Williams</author>
</authors>
<title>Natural Language Generation of discourse connectives for different reading levels.</title>
<date>2002</date>
<booktitle>In Proceedings of the 5th Annual CLUK Research Colloquium.</booktitle>
<contexts>
<context position="1269" citStr="Williams 2002" startWordPosition="178" endWordPosition="179">kes at the discourse-level have an impact on readability. Our preliminary results indicate that such choices do indeed appear to be important for learner readers. These will be investigated further in future larger-scale experiments. Ultimately we intend to use the results to develop a mechanism that makes discourse-level choices that are appropriate for individuals&apos; reading skills. 1. Introduction The Generator for Individual Reading Levels (GIRL) project is developing a Natural Language Generation (NLG) system that generates feedback reports for adults after a web-based literacy assessment (Williams 2002). The literacy assessment was designed by NFER-Nelson for the Target Skills application (2002) and it is aimed at adults with poor basic literacy. It produces a multi-level appraisal of a candidate&apos;s literacy skills It tests eight skills: letter recognition, sentence completion, word ordering, form filling, punctuation and capitals, spelling, skimming and scanning and listening. The entire assessment consists of ninety questions, but the more difficult tests are only given to stronger candidates who have scored well on earlier tests. All questions are multiple choice. Figure 1 shows a screensh</context>
<context position="9670" citStr="Williams 2002" startWordPosition="1564" endWordPosition="1565">learner readers. Sentence length and word length are both believed to have a major impact on readability (Flesch 1949). The corpus analysis results were input to machine learning algorithms to derive decision trees and rules for GIRL&apos; s microplanner. But the analysis they are based on was a corpus written for good readers and we need data to adapt them for learner readers, so we carried out the experiments described here. 2.2 Modifications for the experiments For the experiments, the system was modified to generate much shorter, more restricted, reports than those of the original GIRL system (Williams 2002). It was also modified to produce eight reports, one after each section of the literacy test, rather than a single long report after the entire test. These modifications increased our chances of collecting some reading data from each student, even if the student did not complete the entire literacy assessment (see section 4). Each short report consists of a salutation, a heading and exactly five paragraphs. Each paragraph consists of exactly one discourse relation. The system can produce two versions of each report, A and B (see Table 1). In text types A and B, discourse relations were generat</context>
<context position="13735" citStr="Williams 2002" startWordPosition="2244" endWordPosition="2245">f each test in the assessment, GIRL generated feedback on how well the participant had done. The report is one of the two types described above and chosen by the system at random. In total, each participant was presented with between three and five reports of type A and three to five of type B. Each participant was recorded reading his/her reports aloud, rather than recording silent reading times. This is because we discovered in an earlier pilot that following the more usual procedure of asking participants to read silently and then click a button led to erroneous reading times for learners (Williams 2002). We could not be certain whether they had actually &apos;read&apos; the reports or not. Recordings provide evidence that reading has, in fact, occurred. The recordings were made digitally and were annotated by hand by the first author using CSLU&apos; s SpeechViewer software (Hosom et al. 1998). The speech waveforms were annotated with beginnings and ends of words and pauses Paragraph Discourse Feature Varied Report type A Report type B relation &amp;quot;easier&amp;quot; &amp;quot;harder&amp;quot; 1 evaluation text span order statement:evaluation evaluation:statement 2 reason/result cue phrase choice &amp;quot;so&amp;quot; &amp;quot;therefore&amp;quot; 3 example cue phrase pos</context>
</contexts>
<marker>Williams, 2002</marker>
<rawString>Sandra Williams. 2002. Natural Language Generation of discourse connectives for different reading levels. In Proceedings of the 5th Annual CLUK Research Colloquium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Williams</author>
<author>Ehud Reiter</author>
</authors>
<title>A corpus analysis of discourse relations for Natural Language Generation. To appear in</title>
<date>2003</date>
<booktitle>Proceedings of Corpus Linguistics</booktitle>
<contexts>
<context position="4115" citStr="Williams and Reiter 2003" startWordPosition="641" endWordPosition="644">ading level decision-making mechanisms as part of the generation process, and which decisions produce the most marked impact on the readability of the output texts. It is very important to base the development of this system on solid empirical evidence rather than on our own intuitions. There has been very little empirical work on what kinds of texts are most appropriate for people with good reading skills and even less on what is appropriate for people with poor literacy. We were therefore motivated to do our own empirical studies. We are attacking the problem on two fronts: corpus analysis (Williams and Reiter 2003) and experiments with real readers, the subject of this paper. 1.1 Readability Following Kintsch and Vipond (1979), we relate readability directly to readers&apos; performance on the reading task (i.e. reading speed, ability to answer comprehension questions and ability to recall content). In these experiments, we measured reading speed and comprehension. We also analysed errors made in reading aloud, but that is not described here. The measures of readability we use are thus quantitative and are based on the hypotheses that readability increases or decreases with: • an increase or decrease in the </context>
<context position="6845" citStr="Williams and Reiter 2003" startWordPosition="1085" endWordPosition="1088">er can vary when linguistically realising discourse relations. 2.1 Deriving microplanner rules from a corpus analysis Decisions about realising discourse relations are made in a module called the microplanner. The input is a tree of discourse relations joining pieces of information. It plans how the information from the tree will be ordered, whether it will be marked with discourse cue phrases (e.g. &apos;but&apos;, 128 &apos;if&apos; and &apos;for example&apos;), and how it will be packed into sentences and punctuated. To determine how human writers make these decisions for good readers, we carried out a corpus analysis (Williams and Reiter 2003). We used the RST Discourse Treebank Corpus (Carlson et al. 2002). The discourse relations analysed were concession, condition, elaboration-additional, evaluation, example, reason and restatement. The features we analysed are listed below. • Text span order. The order of text spans in discourse relations. For instance &amp;quot;because you got four out of ten, you need to practise&amp;quot; or &amp;quot;you need to practise because you got four out of ten&amp;quot;. • Cue phrase existence and selection. Whether cue phrases are present in a relation, or not, and which ones are used. For instance, cue phrases if and then are both </context>
<context position="8648" citStr="Williams and Reiter 2003" startWordPosition="1399" endWordPosition="1402">comma in &amp;quot;many people find learning letters hard, but you can do it&amp;quot; and the full stop in &amp;quot;you finished the Alphabet Letters test. Well done&amp;quot;, sometimes there is none e.g. &amp;quot;many people find learning letters hard but you can do it&amp;quot;. • First text span length. The length of the first text span in words. The inspiration for choosing the first four features was Moser and Moore&apos;s analysis (1996). The features are interdependent. For instance, choosing a particular ordering of text spans can constrain the choice of cue phrase. For instance, the first span in a relation can never have cue phrase but (Williams and Reiter 2003). The corpus analysis was therefore extremely useful in deriving a set of rules for choosing legal combinations of features for each relation. We hypothesised that certain values for features were more likely to increase readability. Commas between segments make the discourse structure more explicit. Sentence-breaking punctuation gives shorter sentences and selection of short, common cue phrases can help learner readers. Sentence length and word length are both believed to have a major impact on readability (Flesch 1949). The corpus analysis results were input to machine learning algorithms to</context>
<context position="18168" citStr="Williams and Reiter 2003" startWordPosition="2951" endWordPosition="2954">read slower. We were thus able to compare reading times for both versions of a paragraph, for all readers and calculate which version was read faster. 4.1.1 Paragraph 1: order of text spans statement evaluation :evaluation :statement # adj.time # adj.time Learners 26 -111.4 12 -47.1 Good 23 -37.3 16 -30.6 readers Table 2. Mean adjusted times in ms/word on two orderings, where # = number of samples. Table 2 shows that learner readers read statement:evaluation order on average 64.3ms/word faster than evaluation:statement order. This result agrees with our RST Discourse Treebank corpus analysis (Williams and Reiter 2003) Ily Molly Mandy, ALPHABET LETTERS Well done. You&apos; finished the ALPHABET LETTERS test. Youigot seven out often, therefore you need to practise. Sometimes you did not pick the right letter. You did not click on: N, for example! Many people find learning letters hard hut you can do it. 6, What most you do to improve? QUESTIONS 1. What was the test about? C dont know r words r letters 2. How many did you get right? 3. Why do you need to practise? C I got some wrong, Cl got them all right. Cl dont know 4. Type one letter you got wrong here: I 5. What can you do, even if it is hard? C learn to swim</context>
<context position="20644" citStr="Williams and Reiter 2003" startWordPosition="3397" endWordPosition="3400">n average 32.5ms/word faster when for example was positioned after the second span, compared to before it. Again, there is little difference in times for good readers (3.2ms/word). The result for learners was unexpected because we thought people would read faster when they were told in advance that the information they were about to read was going to be an example (i.e. when for example is before the second span). If it is after the span, they have to re-evaluate the information they have just read. Also, we found very few examples of the after position in our RST Discourse Treebank analysis (Williams and Reiter 2003). Scarcity of the easier-to-read version in the corpus may provide further evidence for Oberlander&apos;s theory (Oberlander 1998) that writers do not always &apos;do the right thing&apos; for readers. This result will be investigated further in future experiments. 4.1.4 Paragraph 4: between-span comma Table 5 shows that learner readers read this paragraph on average 26.8ms/word faster when a comma was present. This is what we expected. The comma between text spans indicates the discourse structure more explicitly and we would expect it to help learner readers. Once again there is little difference in times </context>
<context position="22159" citStr="Williams and Reiter 2003" startWordPosition="3655" endWordPosition="3658">aragraph 5: presence of second cue then no then adj. adj. time time Learners 18 -8.3 12 -37.3 Good readers 7 -57.8 9 -51.0 Table 6, Average adjusted times in ms/word with and without cue phrase then where # = number of samples Table 6 shows that learner readers read relations with no second cue phrase 29.0ms/word faster. Good readers showed little difference in times (6.8ms/word). This is not what we expected. We expected the second cue phrase to help learners because it makes the condition relation more explicit when both if and then are present. This result ties in with our corpus analysis (Williams and Reiter 2003) where few cases with both cues present were found. Writers do not often use both cue phrases and learner readers seem to find an extra phrase adds difficulty rather than helping. 4.1.6 Sentence length The figures for reading times vs. sentence length show that all readers are slower on sentences above 23 words in length, and some learners are slower above 18 words. We require more data to verify this. 4.2 Comprehension We found that learner readers can have problems with answering comprehension questions, even when the questions are administered verbally. Some learner readers are unfamiliar w</context>
<context position="23968" citStr="Williams and Reiter 2003" startWordPosition="3950" endWordPosition="3953">&apos; scores are shown as gray bars and their mean scores are black bars. Figure 6, Grey bars = Learner readers&apos; scores, Black bars = Mean learner readers&apos; scores Learners&apos; highest mean comprehension score was on the explanation text type, but there is little difference between this and other mean scores. We simplified comprehension questions at the same time as introducing the explanation text, so we need to do further experiments to determine which has the most impact. 5. Conclusions We previously analysed a corpus to determine how writers linguistically realise a number of discourse relations (Williams and Reiter 2003). Since the features analysed for each relation are interdependent (see section 2.2). Interdependencies can be conceptualised as a matrix like that shown in Table 7, where each cell (shown blank) actually contains rules (e.g. length vs. punctuation rules might be 1-10 words -&gt; comma and &gt;10 words -&gt; full stop). cue choice cue position Table 7. Interdependencies of features for one discourse relation The corpus analysis results were input to machine learning algorithms to derive decision trees and sets of rules for GIRL&apos; s microplanner, so that given input text spans of fixed lengths linked in </context>
</contexts>
<marker>Williams, Reiter, 2003</marker>
<rawString>Sandra Williams and Ehud Reiter. 2003. A corpus analysis of discourse relations for Natural Language Generation. To appear in Proceedings of Corpus Linguistics 2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>