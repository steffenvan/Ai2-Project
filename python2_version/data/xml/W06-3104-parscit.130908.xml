<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005656">
<title confidence="0.8782025">
Quasi-Synchronous Grammars:
Alignment by Soft Projection of Syntactic Dependencies
</title>
<author confidence="0.942521">
David A. Smith and Jason Eisner
</author>
<affiliation confidence="0.917826">
Department of Computer Science
Center for Language and Speech Processing
Johns Hopkins University
</affiliation>
<address confidence="0.857304">
Baltimore, MD 21218, USA
</address>
<email confidence="0.999614">
{dasmith,eisner}@jhu.edu
</email>
<sectionHeader confidence="0.99482" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999768666666667">
Many syntactic models in machine trans-
lation are channels that transform one
tree into another, or synchronous gram-
mars that generate trees in parallel. We
present a new model of the translation pro-
cess: quasi-synchronous grammar (QG).
Given a source-language parse tree T1, a
QG defines a monolingual grammar that
generates translations of T1. The trees
T2 allowed by this monolingual gram-
mar are inspired by pieces of substruc-
ture in T1 and aligned to T1 at those
points. We describe experiments learning
quasi-synchronous context-free grammars
from bitext. As with other monolingual
language models, we evaluate the cross-
entropy of QGs on unseen text and show
that a better fit to bilingual data is achieved
by allowing greater syntactic divergence.
When evaluated on a word alignment task,
QG matches standard baselines.
</bodyText>
<sectionHeader confidence="0.819251" genericHeader="categories and subject descriptors">
1 Motivation and Related Work
</sectionHeader>
<subsectionHeader confidence="0.987353">
1.1 Sloppy Syntactic Alignment
</subsectionHeader>
<bodyText confidence="0.999964142857143">
This paper proposes a new type of syntax-based
model for machine translation and alignment. The
goal is to make use of syntactic formalisms, such as
context-free grammar or tree-substitution grammar,
without being overly constrained by them.
Let S1 and S2 denote the source and target sen-
tences. We seek to model the conditional probability
</bodyText>
<equation confidence="0.724082">
p(T2, A  |T1) (1)
</equation>
<bodyText confidence="0.999836">
where T1 is a parse tree for S1, T2 is a parse tree
for S2, and A is a node-to-node alignment between
them. This model allows one to carry out a variety
of alignment and decoding tasks. Given T1, one can
translate it by finding the T2 and A that maximize
(1). Given T1 and T2, one can align them by finding
the A that maximizes (1) (equivalent to maximizing
p(A  |T2, T1)). Similarly, one can align S1 and S2
by finding the parses T1 and T2, and alignment A,
that maximize p(T2, A  |T1) · p(T1  |S1), where
p(T1  |S1) is given by a monolingual parser. We
usually accomplish such maximizations by dynamic
programming.
Equation (1) does not assume that T1 and T2 are
isomorphic. For example, a model might judge T2
and A to be likely, given T1, provided that many—
but not necessarily all—of the syntactic dependen-
cies in T1 are aligned with corresponding depen-
dencies in T2. Hwa et al. (2002) found that hu-
man translations from Chinese to English preserved
only 39–42% of the unlabeled Chinese dependen-
cies. They increased this figure to 67% by using
more involved heuristics for aligning dependencies
across these two languages. That suggests that (1)
should be defined to consider more than one depen-
dency at a time.
This inspires the key novel feature of our models:
A does not have to be a “well-behaved” syntactic
alignment. Any portion of T2 can align to any por-
tion of T1, or to NULL. Nodes that are syntactically
related in T1 do not have to translate into nodes that
are syntactically related in T2—although (1) is usu-
ally higher if they do.
This property makes our approach especially
promising for aligning freely, or erroneously, trans-
lated sentences, and for coping with syntactic diver-
</bodyText>
<page confidence="0.983706">
23
</page>
<note confidence="0.88296">
Proceedings of the Workshop on Statistical Machine Translation, pages 23–30,
New York City, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.9997055625">
gences observed between even closely related lan-
guages (Dorr, 1994; Fox, 2002). We can patch to-
gether an alignment without accounting for all the
details of the translation process. For instance, per-
haps a source NP (figure 1) or PP (figure 2) appears
“out of place” in the target sentence. A linguist
might account for the position of the PP auf diese
Frage either syntactically (by invoking scrambling)
or semantically (by describing a deep analysis-
transfer-synthesis process in the translator’s head).
But an MT researcher may not have the wherewithal
to design, adequately train, and efficiently compute
with “deep” accounts of this sort. Under our ap-
proach, it is possible to use a simple, tractable syn-
tactic model, but with some contextual probability
of “sloppy” transfer.
</bodyText>
<subsectionHeader confidence="0.8166435">
1.2 From Synchronous to Quasi-Synchronous
Grammars
</subsectionHeader>
<bodyText confidence="0.999836333333333">
Because our approach will let anything align to
anything, it is reminiscent of IBM Models 1–5
(Brown et al., 1993). It differs from the many ap-
proaches where (1) is defined by a stochastic syn-
chronous grammar (Wu, 1997; Alshawi et al., 2000;
Yamada and Knight, 2001; Eisner, 2003; Gildea,
2003; Melamed, 2004) and from transfer-based sys-
tems defined by context-free grammars (Lavie et al.,
2003).
The synchronous grammar approach, originally
due to Shieber and Schabes (1990), supposes that T2
is generated in lockstep to T1.1 When choosing how
to expand a certain VP node in T2, a synchronous
CFG process would observe that this node is aligned
to a node VP0 in T1, which had been expanded in T1
by VP0 —* NP0 V0. This might bias it toward choos-
ing to expand the VP in T2 as VP —* V NP, with the
new children V aligned to V0 and NP aligned to NP0.
The process then continues recursively by choosing
moves to expand these children.
One can regard this stochastic process as an in-
stance of analysis-transfer-synthesis MT. Analysis
chooses a parse T1 given S1. Transfer maps the
context-free rules in T1 to rules of T2. Synthesis
</bodyText>
<footnote confidence="0.9890156">
1The usual presentation describes a process that generates
T1 and T2 jointly, leading to a joint model p(T2, A, T1). Divid-
ing by the marginal p(T1) gives a conditional model p(T2, A
T1) as in (1). In the text, we directly describe an equivalent
conditional process for generating T2, A given T1.
</footnote>
<bodyText confidence="0.999873633333333">
deterministically assembles the latter rules into an
actual tree T2 and reads off its yield S2.
What is worrisome about the synchronous pro-
cess is that it can only produce trees T2 that are
perfectly isomorphic to T1. It is possible to relax
this requirement by using synchronous grammar for-
malisms more sophisticated than CFG:2 one can per-
mit unaligned nodes (Yamada and Knight, 2001),
duplicated children (Gildea, 2003)3, or alignment
between elementary trees of differing sizes rather
than between single rules (Eisner, 2003; Ding and
Palmer, 2005; Quirk et al., 2005). However, one
would need rather powerful and slow grammar for-
malisms (Shieber and Schabes, 1990; Melamed et
al., 2004), often with discontiguous constituents, to
account for all the linguistic divergences that could
arise from different movement patterns (scrambling,
wh-in situ) or free translation. In particular, a syn-
chronous grammar cannot practically allow S2 to be
any permutation of S1, as IBM Models 1–5 do.
Our alternative is to define a “quasi-synchronous”
stochastic process. It generates T2 in a way that is
not in thrall to T1 but is “inspired by it.” (A human
translator might be imagined to behave similarly.)
When choosing how to expand nodes of T2, we are
influenced both by the structure of T1 and by mono-
lingual preferences about the structure of T2. Just as
conditional Markov models can more easily incor-
porate global features than HMMs, we can look at
the entire tree T1 at every stage in generating T2.
</bodyText>
<sectionHeader confidence="0.97152" genericHeader="method">
2 Quasi-Synchronous Grammar
</sectionHeader>
<bodyText confidence="0.999970857142857">
Given an input S1 or its parse T1, a quasi-
synchronous grammar (QG) constructs a monolin-
gual grammar for parsing, or generating, the possi-
ble translations S2—that is, a grammar for finding
appropriate trees T2. What ties this target-language
grammar to the source-language input? The gram-
mar provides for target-language words to take on
</bodyText>
<footnote confidence="0.7350378">
2When one moves beyond CFG, the derived trees T1 and
T2 are still produced from a single derivation tree, but may be
shaped differently from the derivation tree and from each other.
3For tree-to-tree alignment, Gildea proposed a clone opera-
tion that allowed subtrees of the source tree to be reused in gen-
erating a target tree. In order to preserve dynamic programming
constraints, the identity of the cloned subtree is chosen indepen-
dently of its insertion point. This breakage of monotonic tree
alignment moves Gildea’s alignment model from synchronous
to quasi-synchronous.
</footnote>
<page confidence="0.985112">
24
</page>
<figure confidence="0.999173363636364">
koennte/VVFIN:3
Tschernobyl/NE:6 dann/ADV:1 etwas/ADV:0 spaeter/ADJ:1 an/PREP:0 kommen/VVINF:0 ./S-SYMBOL:10
Reihe/NN:0
die/ART:0
Then:1 we:2
Chernobyl:6
with:5 later:9
could:3
deal:4 .:10
some:7
time:8
</figure>
<figureCaption confidence="0.996019">
Figure 1: German and English dependency parses and their alignments from our system where German
is the target language. Tschernobyl depends on k¨onnte even though their English analogues are not in a
dependency relationship. Note the parser’s error in not attaching etwas to sp¨ater.
</figureCaption>
<figure confidence="0.942186333333333">
German: Tschernobyl k¨onnte dann etwas sp¨ater an die Reihe kommen .
Literally: Chernobyl could then somewhat later on the queue come.
English: Then we could deal with Chernobyl some time later.
</figure>
<figureCaption confidence="0.8682925">
Figure 2: Here the German sentence exhibits scrambling of the phrase auf diese Frage and negates the object
of bekommen instead of the verb itself.
</figureCaption>
<figure confidence="0.977571">
German: Auf diese Frage habe ich leider keine Antwort bekommen .
Literally: To this question have I unfortunately no answer received.
English: I did not unfortunately receive an answer to this question.
Auf/PREP:8
Frage/NN:10
diese/DEM:9
habe/VHFIN:2 ich/PPRO:1 leider/ADV:4
bekommen/VVpast:5
keine/INDEF:3
Antwort/NN:7
./S-SYMBOL:11
I:1
not:3 unfortunately:4 receive:5 .:11
did:2
an:6 to:8
question:10
answer:7
this:9
</figure>
<page confidence="0.98746">
25
</page>
<bodyText confidence="0.999968041666667">
multiple hidden “senses,” which correspond to (pos-
sibly empty sets of) word tokens in 51 or nodes in
T1. To take a familiar example, when parsing the
English side of a French-English bitext, the word
bank might have the sense banque (financial) in one
sentence and rive (littoral) in another.
The QG4 considers the “sense” of the former bank
token to be a pointer to the particular banque token
to which it aligns. Thus, a particular assignment of
51 “senses” to word tokens in 52 encodes a word
alignment.
Now, selectional preferences in the monolingual
grammar can be influenced by these T1-specific
senses. So they can encode preferences for how T2
ought to copy the syntactic structure of T1. For ex-
ample, if T1 contains the phrase banque nationale,
then the QG for generating a corresponding T2 may
encourage any T2 English noun whose sense is
banque (more precisely, T1’s token of banque) to
generate an adjectival English modifier with sense
nationale. The exact probability of this, as well as
the likely identity and position of that English mod-
ifier (e.g., national bank), may also be influenced by
monolingual facts about English.
</bodyText>
<subsectionHeader confidence="0.955413">
2.1 Definition
</subsectionHeader>
<bodyText confidence="0.999057153846154">
A quasi-synchronous grammar is a monolingual
grammar that generates translations of a source-
language sentence. Each state of this monolingual
grammar is annotated with a “sense”—a set of zero
or more nodes from the source tree or forest.
For example, consider a quasi-synchronous
context-free grammar (QCFG) for generating trans-
lations of a source tree T1. The QCFG generates the
target sentence using nonterminals from the cross
product U x 2V1, where U is the set of monolingual
target-language nonterminals such as NP, and V1 is
the set of nodes in T1.
Thus, a binarized QCFG has rules of the form
</bodyText>
<equation confidence="0.984246">
(A, α) (B, 0)(C, Y) (2)
(A, α) w (3)
</equation>
<bodyText confidence="0.9993905">
where A, B, C E U are ordinary target-language
nonterminals, α, 0, -y E 2V1 are sets of source tree
</bodyText>
<footnote confidence="0.719227666666667">
4By abuse of terminology, we often use “QG” to refer to the
T1-specific monolingual grammar, although the QG is properly
a recipe for constructing such a grammar from any input T1.
</footnote>
<bodyText confidence="0.977919666666667">
nodes to which A, B, C respectively align, and w is
a target-language terminal.
Similarly, a quasi-synchronous tree-substitution
grammar (QTSG) annotates the root and frontier
nodes of its elementary trees with sets of source
nodes from 2V1.
</bodyText>
<subsectionHeader confidence="0.999462">
2.2 Taming Source Nodes
</subsectionHeader>
<bodyText confidence="0.999895475">
This simple proposal, however, presents two main
difficulties. First, the number of possible senses for
each target node is exponential in the number of
source nodes. Second, note that the senses are sets
of source tree nodes, not word types or absolute sen-
tence positions as in some other translation models.
Except in the case of identical source trees, source
tree nodes will not recur between training and test.
To overcome the first problem, we want further re-
strictions on the set α in a QG state such as (A, α). It
should not be an arbitrary set of source nodes. In the
experiments of this paper, we adopt the simplest op-
tion of requiring |α |&lt; 1. Thus each node in the tar-
get tree is aligned to a single node in the source tree,
or to 0 (the traditional NULL alignment). This allows
one-to-many but not many-to-one alignments.
To allow many-to-many alignments, one could
limit |α |to at most 2 or 3 source nodes, perhaps fur-
ther requiring the 2 or 3 source nodes to fall in a par-
ticular configuration within the source tree, such as
child-parent or child-parent-grandparent. With that
configurational requirement, the number of possi-
ble senses α remains small—at most three times the
number of source nodes.
We must also deal with the menagerie of differ-
ent source tree nodes in different sentences. In other
words, how can we tie the parameters of the different
QGs that are used to generate translations of differ-
ent source sentences? The answer is that the proba-
bility or weight of a rule such as (2) should depend
on the specific nodes in α, 0, and -y only through
their properties—e.g., their nonterminal labels, their
head words, and their grammatical relationship in
the source tree. Such properties do recur between
training and test.
For example, suppose for simplicity that |α |=
|0 |= |-y |= 1. Then the rewrite probabilities of (2)
and (3) could be log-linearly modeled using features
that ask whether the single node in α has two chil-
dren in the source tree; whether its children in the
</bodyText>
<page confidence="0.978908">
26
</page>
<bodyText confidence="0.9993794">
source are the nodes in Q and &apos;y; whether its non-
terminal label in the source is A; whether its fringe
in the source translates as w; and so on. The model
should also consider monolingual features of (2) and
(3), evaluating in particular whether A —* BC is
likely in the target language.
Whether rule weights are given by factored gener-
ative models or by naive Bayes or log-linear models,
we want to score QG productions with a small set of
monolingual and bilingual features.
</bodyText>
<subsectionHeader confidence="0.99901">
2.3 Synchronous Grammars Again
</subsectionHeader>
<bodyText confidence="0.999616">
Finally, note that synchronous grammar is a special
case of quasi-synchronous grammar. In the context-
free case, a synchronous grammar restricts senses to
single nodes in the source tree and the NULL node.
Further, for any k-ary production
</bodyText>
<equation confidence="0.938067">
(X0, α0) — (X1, α1) ... (Xk, αk)
</equation>
<bodyText confidence="0.828121">
a synchronous context-free grammar requires that
</bodyText>
<listItem confidence="0.991877333333333">
1. (Vi =� j) αi =� αj unless αi = NULL,
2. (Vi &gt; 0) αi is a child of α0 in the source tree,
unless αi = NULL.
</listItem>
<bodyText confidence="0.995949">
Since NULL has no children in the source tree, these
rules imply that the children of any node aligned to
NULL are themselves aligned to NULL. The con-
struction for synchronous tree-substitution and tree-
adjoining grammars goes through similarly but op-
erates on the derivation trees.
</bodyText>
<sectionHeader confidence="0.933699" genericHeader="method">
3 Parameterizing a QCFG
</sectionHeader>
<bodyText confidence="0.999904945945946">
Recall that our goal is a conditional model of
p(T2, A  |T1). For the remainder of this paper, we
adopt a dependency-tree representation of T1 and
T2. Each tree node represents a word of the sentence
together with a part-of-speech tag. Syntactic depen-
dencies in each tree are represented directly by the
parent-child relationships.
Why this representation? First, it helps us con-
cisely formulate a QG translation model where the
source dependencies influence the generation of tar-
get dependencies (see figure 3). Second, for evalu-
ation, it is trivial to obtain the word-to-word align-
ments from the node-to-node alignments. Third, the
part-of-speech tags are useful backoff features, and
in fact play a special role in our model below.
When stochastically generating a translation T2,
our quasi-synchronous generative process will be in-
fluenced by both fluency and adequacy. That is, it
considers both the local well-formedness of T2 (a
monolingual criterion) and T2’s local faithfulness
to T1 (a bilingual criterion). We combine these in
a simple generative model rather than a log-linear
model. When generating the children of a node in
T2, the process first generates their tags using mono-
lingual parameters (fluency), and then fills in in the
words using bilingual parameters (adequacy) that se-
lect and translate words from T1.5
Concretely, each node in T2 is labeled by a triple
(tag, word, aligned word). Given a parent node
(p, h, h&apos;) in T2, we wish to generate sequences of
left and right child nodes, of the form (c, a, a&apos;).
Our monolingual parameters come from a simple
generative model of syntax used for grammar induc-
tion: the Dependency Model with Valence (DMV) of
Klein and Manning (2004). In scoring dependency
attachments, DMV uses tags rather than words. The
parameters of the model are:
</bodyText>
<listItem confidence="0.992456">
1. pehoose(c  |p, dir): the probability of generat-
ing c as the next child tag in the sequence of
dir children, where dir E {left, right}.
2. pstop(s  |h, dir, adj): the probability of gener-
</listItem>
<bodyText confidence="0.966755222222222">
ating no more child tags in the sequence of dir
children. This is conditioned in part on the “ad-
jacency” adj E {true, false}, which indicates
whether the sequence of dir children is empty
so far.
Our bilingual parameters score word-to-word
translation and aligned dependency configurations.
We thus use the conditional probability ptrans(a |
a&apos;) that source word a&apos;, which may be NULL, trans-
lates as target word a. Finally, when a parent word
h aligned to h&apos; generates a child, we stochastically
decide to align the child to a node a&apos; in T1 with
one several possible relations to h&apos;. A “monotonic”
dependency alignment, for example, would have
h&apos; and a&apos; in a parent-child relationship like their
target-tree analogues. In different versions of the
model, we allowed various dependency alignment
configurations (figure 3). These configurations rep-
</bodyText>
<footnote confidence="0.953152">
5This division of labor is somewhat artificial, and could be
remedied in a log-linear model, Naive Bayes model, or defi-
cient generative model that generates both tags and words con-
ditioned on both monolingual and bilingual context.
</footnote>
<page confidence="0.999011">
27
</page>
<bodyText confidence="0.999967166666667">
resent cases where the parent-child dependency be-
ing generated by the QG in the target language maps
onto source-language child-parent, for head swap-
ping; the same source node, for two-to-one align-
ment; nodes that are siblings or in a c-command re-
lationship, for scrambling and extraposition; or in
a grandparent-grandchild relationship, e.g. when a
preposition is inserted in the source language. We
also allowed a “none-of-the-above” configuration, to
account for extremely mismatched sentences.
The probability of the target-language depen-
dency treelet rooted at h is thus:
</bodyText>
<equation confidence="0.998977285714286">
P(D(h)  |h, h&apos;, p) _
H H
dirE{l,r} cEdepsD(p,dir)
P(D(c)  |a, a&apos;, c) × pstop(nostop  |p, dir, adj)
×pchoose(c  |p, dir)
×pconfig(config) × ptrans(a  |a&apos;)
pstop(stop  |p, dir, adj)
</equation>
<sectionHeader confidence="0.99509" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999985">
We claim that for modeling human-translated bitext,
it is better to project syntax only loosely. To evaluate
this claim, we train quasi-synchronous dependency
grammars that allow progressively more divergence
from monotonic tree alignment. We evaluate these
models on cross-entropy over held-out data and on
error rate in a word-alignment task.
One might doubt the use of dependency trees
for alignment, since Gildea (2004) found that con-
stituency trees aligned better. That experiment, how-
ever, aligned only the 1-best parse trees. We too will
consider only the 1-best source tree T1, but in con-
strast to Gildea, we will search for the target tree T2
that aligns best with T1. Finding T2 and the align-
ment is simply a matter of parsing 52 with the QG
derived from T1.
</bodyText>
<subsectionHeader confidence="0.995468">
4.1 Data and Training
</subsectionHeader>
<bodyText confidence="0.999977625">
We performed our modeling experiments with the
German-English portion of the Europarl European
Parliament transcripts (Koehn, 2002). We obtained
monolingual parse trees from the Stanford German
and English parsers (Klein and Manning, 2003).
Initial estimates of lexical translation probabilities
came from the IBM Model 4 translation tables pro-
duced by GIZA++ (Brown et al., 1993; Och and
Ney, 2003).
All text was lowercased and numbers of two or
more digits were converted to an equal number of
hash signs. The bitext was divided into training
sets of 1K, 10K, and 100K sentence pairs. We held
out one thousand sentences for evaluating the cross-
entropy of the various models and hand-aligned
100 sentence pairs to evaluate alignment error rate
(AER).
We trained the model parameters on bitext using
the Expectation-Maximization (EM) algorithm. The
T1 tree is fully observed, but we parse the target lan-
guage. As noted, the initial lexical translation proba-
bilities came from IBM Model 4. We initialized the
monolingual DMV parameters in one of two ways:
using either simple tag co-occurrences as in (Klein
and Manning, 2004) or “supervised” counts from the
monolingual target-language parser. This latter ini-
tialization simulates the condition when one has a
small amount of bitext but a larger amount of tar-
get data for language modeling. As with any mono-
lingual grammar, we perform EM training with the
Inside-Outside algorithm, computing inside prob-
abilities with dynamic programming and outside
probabilities through backpropagation.
Searching the full space of target-language depen-
dency trees and alignments to the source tree con-
sumed several seconds per sentence. During train-
ing, therefore, we constrained alignments to come
from the union of GIZA++ Model 4 alignments.
These constraints were applied only during training
and not during evaluation of cross-entropy or AER.
</bodyText>
<subsectionHeader confidence="0.997134">
4.2 Conditional Cross-Entropy of the Model
</subsectionHeader>
<bodyText confidence="0.999984416666667">
To test the explanatory power of our QCFG, we eval-
uated its conditional cross-entropy on held-out data
(table 1). In other words, we measured how well a
trained QCFG could predict the true translation of
novel source sentences by summing over all parses
of the target given the source. We trained QCFG
models under different conditions of bitext size and
parameter initialization. However, the principal in-
dependent variable was the set of dependency align-
ment configurations allowed.
From these cross-entropy results, it is clear that
strictly synchronous grammar is unwise. We ob-
</bodyText>
<page confidence="0.996852">
28
</page>
<figure confidence="0.999184230769231">
(a) parent-child (b) child-parent (c) same node
see
sehe
ich
I
likes Voelkerrecht law
schwimmt
gern swimming
international
(d) siblings (e) grandparent-grandchild (f) c-command
bekommen answer Wahlkampf campaign sagte bought
auf Antwort to von 2003 Was dass what
2003 kaufte
</figure>
<figureCaption confidence="0.9589258">
Figure 3: When a head h aligned to h&apos; generates a new child a aligned to a&apos; under the QCFG, h&apos; and a&apos; may be related in the
source tree as, among other things, (a) parent–child, (b) child–parent, (c) identical nodes, (d) siblings, (e) grandparent–grandchild,
(f) c-commander–c-commandee, (g) none of the above. Here German is the source and English is the target. Case (g), not pictured
above, can be seen in figure 1, in English-German order, where the child-parent pair Tschernobyl k¨onnte correspond to the words
Chernobyl and could, respectively. Since could dominates Chernobyl, they are not in a c-command relationship.
</figureCaption>
<table confidence="0.9989509">
Permitted configurations CE CE CE
at 1k 10k 100k
0 or parent-child (a) 43.82 22.40 13.44
+ child-parent (b) 41.27 21.73 12.62
+ same node (c) 41.01 21.50 12.38
+ all breakages (g) 35.63 18.72 11.27
+ siblings (d) 34.59 18.59 11.21
+ grandparent-grandchild (e) 34.52 18.55 11.17
+ c-command (f) 34.46 18.59 11.27
No alignments allowed 60.86 53.28 46.94
</table>
<tableCaption confidence="0.654934714285714">
Table 1: Cross-entropy on held-out data with different depen-
dency configurations (figure 3) allowed, for 1k, 10k, and 100k
training sentences. The big error reductions arrive when we
allow arbitrary non-local alignments in condition (g). Distin-
guishing some common cases of non-local alignments improves
performance further. For comparison, we show cross-entropy
when every target language node is unaligned.
</tableCaption>
<bodyText confidence="0.9999644">
tain comparatively poor performance if we require
parent-child pairs in the target tree to align to parent-
child pairs in the source (or to parent-NULL or
NULL-NULL). Performance improves as we allow
and distinguish more alignment configurations.
</bodyText>
<subsectionHeader confidence="0.996462">
4.3 Word Alignment
</subsectionHeader>
<bodyText confidence="0.999969764705883">
We computed standard measures of alignment preci-
sion, recall, and error rate on a test set of 100 hand-
aligned German sentence pairs with 1300 alignment
links. As with many word-alignment evaluations,
we do not score links to NULL. Just as for cross-
entropy, we see that more permissive alignments
lead to better performance (table 2).
Having selected the best system using the cross-
entropy measurement, we compare its alignment er-
ror rate against the standard GIZA++ Model 4 base-
lines. As Figure 4 shows, our QCFG for German —*
English consistently produces better alignments than
the Model 4 channel model for the same direction,
German —* English. This comparison is the appro-
priate one because both of these models are forced
to align each English word to at most one German
word. 6
</bodyText>
<sectionHeader confidence="0.999237" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999442">
With quasi-synchronous grammars, we have pre-
sented a new approach to syntactic MT: construct-
ing a monolingual target-language grammar that de-
scribes the aligned translations of a source-language
sentence. We described a simple parameterization
</bodyText>
<footnote confidence="0.992053">
6For German --+ English MT, one would use a German --+
English QCFG as above, but an English --+ German channel
model. In this arguably inappropriate comparison, Figure 4
shows, the Model 4 channel model produces slightly better
word alignments than the QG.
</footnote>
<page confidence="0.990654">
29
</page>
<table confidence="0.998866555555555">
Permitted configurations AER AER AER
at 1k 10k 100k
0 or parent-child (a) 40.69 39.03 33.62
+ child-parent (b) 43.17 39.78 33.79
+ same node (c) 43.22 40.86 34.38
+ all breakages (g) 37.63 30.51 25.99
+ siblings (d) 37.87 33.36 29.27
+ grandparent-grandchild (e) 36.78 32.73 28.84
+ c-command (f) 37.04 33.51 27.45
</table>
<tableCaption confidence="0.9966465">
Table 2: Alignment error rate (%) with different dependency
configurations allowed.
</tableCaption>
<figure confidence="0.919226">
1000 10000 100000 1e+06
training sentence pairs
</figure>
<figureCaption confidence="0.997208">
Figure 4: Alignment error rate with best model (all break-
ages). The QCFG consistently beat one GIZA++ model and
was close to the other.
</figureCaption>
<bodyText confidence="0.99986572">
with gradually increasing syntactic domains of lo-
cality, and estimated those parameters on German-
English bitext.
The QG formalism admits many more nuanced
options for features than we have exploited. In par-
ticular, we now are exploring log-linear QGs that
score overlapping elementary trees of T2 while con-
sidering the syntactic configuration and lexical con-
tent of the T1 nodes to which each elementary tree
aligns.
Even simple QGs, however, turned out to do quite
well. Our evaluation on a German-English word-
alignment task showed them to be competitive with
IBM model 4—consistently beating the German-
English direction by several percentage points of
alignment error rate and within 1% AER of the
English-German direction. In particular, alignment
accuracy benefited from allowing syntactic break-
ages between the two dependency structures.
We are also working on a translation decoding us-
ing QG. Our first system uses the QG to find optimal
T2 aligned to T1 and then extracts a synchronous
tree-substitution grammar from the aligned trees.
Our second system searches a target-language vo-
cabulary for the optimal T2 given the input T1.
</bodyText>
<sectionHeader confidence="0.993966" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999801666666667">
This work was supported by a National Science
Foundation Graduate Research Fellowship for the
first author and by NSF Grant No. 0313193.
</bodyText>
<sectionHeader confidence="0.998853" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999844115384615">
H. Alshawi, S. Bangalore, and S. Douglas. 2000. Learning
dependency translation models as collections of finite state
head transducers. CL, 26(1):45–60.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine trans-
lation: Parameter estimation. CL, 19(2):263–311.
Y. Ding and M. Palmer. 2005. Machine translation using prob-
abilistic synchronous dependency insertion grammars. In
ACL, pages 541–548.
B. J. Dorr. 1994. Machine translation divergences: A formal
description and proposed solution. Computational Linguis-
tics, 20(4):597–633.
J. Eisner. 2003. Learning non-isomorphic tree mappings for
machine translation. In ACL Companion Vol.
H. J. Fox. 2002. Phrasal cohesion and statistical machine trans-
lation. In EMNLP, pages 392–399.
D. Gildea. 2003. Loosely tree-based alignment for machine
translation. In ACL, pages 80–87.
D. Gildea. 2004. Dependencies vs. constituents for tree-based
alignment. In EMNLP, pages 214–221.
R. Hwa, P. Resnik, A. Weinberg, and O. Kolak. 2002. Evalu-
ating translational correspondence using annotation projec-
tion. In ACL.
D. Klein and C. D. Manning. 2003. Accurate unlexicalized
parsing. In ACL, pages 423–430.
D. Klein and C. D. Manning. 2004. Corpus-based induction of
syntactic structure: Models of dependency and constituency.
In ACL, pages 479–486.
P. Koehn. 2002. Europarl: A multilingual
corpus for evaluation of machine translation.
http://www.iccs.informatics.ed.ac.uk/˜pkoehn/-
publications/europarl.ps.
A. Lavie, S. Vogel, L. Levin, E. Peterson, K. Probst, A. F.
Llitj´os, R. Reynolds, J. Carbonell, and R. Cohen. 2003. Ex-
periments with a Hindi-to-English transfer-based MT system
under a miserly data scenario. ACM Transactions on Asian
Language Information Processing, 2(2):143 – 163.
I. D. Melamed, G. Satta, and B. Wellington. 2004. Generalized
multitext grammars. In ACL, pages 661–668.
I. D. Melamed. 2004. Statistical machine translation by pars-
ing. In ACL, pages 653–660.
F. J. Och and H. Ney. 2003. A systematic comparison of various
statistical alignment models. CL, 29(1):19–51.
C. Quirk, A. Menezes, and C. Cherry. 2005. Dependency
treelet translation: Syntactically informed phrasal SMT. In
ACL, pages 271–279.
S. M. Shieber and Y. Schabes. 1990. Synchronous tree-
adjoining grammars. In ACL, pages 253–258.
D. Wu. 1997. Stochastic inversion transduction grammars and
bilingual parsing of parallel corpora. CL, 23(3):377–403.
K. Yamada and K. Knight. 2001. A syntax-based statistical
translation model. In ACL.
</reference>
<figure confidence="0.996807909090909">
alignment error rate
0.45
0.35
0.25
0.5
0.4
0.3
0.2
GCFG
za4
Giza4 bk
</figure>
<page confidence="0.947967">
30
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.998937">Quasi-Synchronous Grammars: Alignment by Soft Projection of Syntactic Dependencies</title>
<author confidence="0.999877">A Smith</author>
<affiliation confidence="0.876101333333333">Department of Computer Center for Language and Speech Johns Hopkins</affiliation>
<address confidence="0.997861">Baltimore, MD 21218,</address>
<abstract confidence="0.998597018072289">Many syntactic models in machine translation are channels that transform one tree into another, or synchronous grammars that generate trees in parallel. We present a new model of the translation process: quasi-synchronous grammar (QG). a source-language parse tree a defines a that translations of The trees by this monolingual grammar are inspired by pieces of substrucin aligned to those points. We describe experiments learning quasi-synchronous context-free grammars from bitext. As with other monolingual language models, we evaluate the crossentropy of QGs on unseen text and show that a better fit to bilingual data is achieved by allowing greater syntactic divergence. When evaluated on a word alignment task, QG matches standard baselines. 1 Motivation and Related Work 1.1 Sloppy Syntactic Alignment This paper proposes a new type of syntax-based model for machine translation and alignment. The goal is to make use of syntactic formalisms, such as context-free grammar or tree-substitution grammar, without being overly constrained by them. the source and target sentences. We seek to model the conditional probability A a parse tree for a parse tree and a node-to-node alignment between them. This model allows one to carry out a variety alignment and decoding tasks. Given one can it by finding the maximize Given one can align them by finding maximizes (1) (equivalent to maximizing Similarly, one can align finding the parses and alignment maximize A where given by a monolingual parser. We usually accomplish such maximizations by dynamic programming. (1) does not assume that For example, a model might judge be likely, given provided that but not necessarily all—of the syntactic dependenin aligned with corresponding depenin Hwa et al. (2002) found that human translations from Chinese to English preserved only 39–42% of the unlabeled Chinese dependencies. They increased this figure to 67% by using more involved heuristics for aligning dependencies across these two languages. That suggests that (1) should be defined to consider more than one dependency at a time. This inspires the key novel feature of our models: not have to be a “well-behaved” syntactic Any portion of align to any porof or to Nodes that are syntactically in to translate into nodes that syntactically related in (1) is usually higher if they do. This property makes our approach especially promising for aligning freely, or erroneously, transsentences, and for coping with syntactic diver- 23 of the Workshop on Statistical Machine pages 23–30, York City, June 2006. Association for Computational Linguistics between even closely related languages (Dorr, 1994; Fox, 2002). We can patch together an alignment without accounting for all the details of the translation process. For instance, perhaps a source NP (figure 1) or PP (figure 2) appears “out of place” in the target sentence. A linguist account for the position of the PP diese syntactically (by invoking scrambling) or semantically (by describing a deep analysistransfer-synthesis process in the translator’s head). But an MT researcher may not have the wherewithal to design, adequately train, and efficiently compute with “deep” accounts of this sort. Under our approach, it is possible to use a simple, tractable syntactic model, but with some contextual probability of “sloppy” transfer. 1.2 From Synchronous to Quasi-Synchronous Grammars Because our approach will let anything align to anything, it is reminiscent of IBM Models 1–5 (Brown et al., 1993). It differs from the many approaches where (1) is defined by a stochastic synchronous grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Melamed, 2004) and from transfer-based systems defined by context-free grammars (Lavie et al., 2003). The synchronous grammar approach, originally to Shieber and Schabes (1990), supposes that generated in lockstep to When choosing how expand a certain VP node in a synchronous CFG process would observe that this node is aligned a node in which had been expanded in —* This might bias it toward choosto expand the VP in VP NP, with the children V aligned to and NP aligned to The process then continues recursively by choosing moves to expand these children. One can regard this stochastic process as an instance of analysis-transfer-synthesis MT. Analysis a parse Transfer maps the rules in rules of Synthesis usual presentation describes a process that generates leading to a joint model A, Dividby the marginal a conditional model A in (1). In the text, we directly describe an equivalent process for generating A deterministically assembles the latter rules into an tree reads off its yield What is worrisome about the synchronous prois that it can only produce trees are isomorphic to It is possible to relax this requirement by using synchronous grammar formore sophisticated than one can permit unaligned nodes (Yamada and Knight, 2001), children (Gildea, or alignment between elementary trees of differing sizes rather than between single rules (Eisner, 2003; Ding and Palmer, 2005; Quirk et al., 2005). However, one would need rather powerful and slow grammar formalisms (Shieber and Schabes, 1990; Melamed et al., 2004), often with discontiguous constituents, to account for all the linguistic divergences that could arise from different movement patterns (scrambling, or free translation. In particular, a syngrammar cannot practically allow be permutation of as IBM Models 1–5 do. Our alternative is to define a “quasi-synchronous” process. It generates a way that is in thrall to is “inspired by it.” (A human translator might be imagined to behave similarly.) choosing how to expand nodes of we are both by the structure of by monopreferences about the structure of Just as conditional Markov models can more easily incorporate global features than HMMs, we can look at entire tree every stage in generating 2 Quasi-Synchronous Grammar an input its parse a quasisynchronous grammar (QG) constructs a monolingual grammar for parsing, or generating, the possitranslations is, a grammar for finding trees What ties this target-language grammar to the source-language input? The grammar provides for target-language words to take on one moves beyond CFG, the derived trees still produced from a single derivation tree, but may be shaped differently from the derivation tree and from each other. tree-to-tree alignment, Gildea proposed a operation that allowed subtrees of the source tree to be reused in generating a target tree. In order to preserve dynamic programming constraints, the identity of the cloned subtree is chosen independently of its insertion point. This breakage of monotonic tree alignment moves Gildea’s alignment model from synchronous to quasi-synchronous.</abstract>
<note confidence="0.829851769230769">24 koennte/VVFIN:3 Tschernobyl/NE:6 dann/ADV:1 etwas/ADV:0 spaeter/ADJ:1 an/PREP:0 kommen/VVINF:0 ./S-SYMBOL:10 Reihe/NN:0 die/ART:0 Then:1 we:2 Chernobyl:6 with:5 later:9 could:3 deal:4 .:10 some:7 time:8 Figure 1: German and English dependency parses and their alignments from our system where German</note>
<abstract confidence="0.9417561">the target language. on though their English analogues are not in a relationship. Note the parser’s error in not attaching k¨onnte dann etwas sp¨ater an die Reihe kommen . Literally: Chernobyl could then somewhat later on the queue come. we could deal with Chernobyl some time later. 2: Here the German sentence exhibits scrambling of the phrase diese Frage negates the object of the verb itself. diese Frage habe ich leider keine Antwort bekommen . Literally: To this question have I unfortunately no answer received. did not unfortunately receive an answer to this question.</abstract>
<note confidence="0.944290733333333">Auf/PREP:8 Frage/NN:10 diese/DEM:9 habe/VHFIN:2 ich/PPRO:1 leider/ADV:4 bekommen/VVpast:5 keine/INDEF:3 Antwort/NN:7 ./S-SYMBOL:11 I:1 not:3 unfortunately:4 receive:5 .:11 did:2 an:6 to:8 question:10 answer:7 this:9</note>
<abstract confidence="0.997178243986254">25 multiple hidden “senses,” which correspond to (posempty sets of) word tokens in nodes in To take a familiar example, when parsing the English side of a French-English bitext, the word have the sense in one and in another. considers the “sense” of the former to be a pointer to the particular to which it aligns. Thus, a particular assignment of to word tokens in a word alignment. Now, selectional preferences in the monolingual can be influenced by these So they can encode preferences for how to copy the syntactic structure of For exif the phrase the QG for generating a corresponding any noun whose sense is precisely, token of to generate an adjectival English modifier with sense The exact probability of this, as well as the likely identity and position of that English mod- (e.g., may also be influenced by monolingual facts about English. 2.1 Definition A quasi-synchronous grammar is a monolingual grammar that generates translations of a sourcelanguage sentence. Each state of this monolingual grammar is annotated with a “sense”—a set of zero or more nodes from the source tree or forest. For example, consider a quasi-synchronous (QCFG) for generating transof a source tree The QCFG generates the target sentence using nonterminals from the cross x where the set of monolingual nonterminals such as NP, and set of nodes in Thus, a binarized QCFG has rules of the form α) (B, 0)(C, Y) α) w B, C E U ordinary target-language 0, -y E are sets of source tree abuse of terminology, we often use “QG” to refer to the monolingual grammar, although the QG is properly recipe for constructing such a grammar from any input to which B, C align, and a target-language terminal. Similarly, a quasi-synchronous tree-substitution grammar (QTSG) annotates the root and frontier nodes of its elementary trees with sets of source from 2.2 Taming Source Nodes This simple proposal, however, presents two main difficulties. First, the number of possible senses for each target node is exponential in the number of source nodes. Second, note that the senses are sets of source tree nodes, not word types or absolute sentence positions as in some other translation models. Except in the case of identical source trees, source tree nodes will not recur between training and test. To overcome the first problem, we want further reon the set a QG state such as It not be an of source nodes. In the experiments of this paper, we adopt the simplest opof requiring &lt; Thus each node in the tartree is aligned to a in the source tree, to traditional This allows one-to-many but not many-to-one alignments. To allow many-to-many alignments, one could at most 2 or 3 source nodes, perhaps further requiring the 2 or 3 source nodes to fall in a particular configuration within the source tree, such as child-parent or child-parent-grandparent. With that configurational requirement, the number of possisenses small—at most three times the number of source nodes. We must also deal with the menagerie of different source tree nodes in different sentences. In other words, how can we tie the parameters of the different QGs that are used to generate translations of different source sentences? The answer is that the probability or weight of a rule such as (2) should depend the specific nodes in and through their properties—e.g., their nonterminal labels, their head words, and their grammatical relationship in the source tree. Such properties do recur between training and test. example, suppose for simplicity that Then the rewrite probabilities of (2) and (3) could be log-linearly modeled using features ask whether the single node in two children in the source tree; whether its children in the 26 are the nodes in whether its nonlabel in the source is whether its fringe the source translates as and so on. The model should also consider monolingual features of (2) and evaluating in particular whether —* BC likely in the target language. Whether rule weights are given by factored generative models or by naive Bayes or log-linear models, we want to score QG productions with a small set of monolingual and bilingual features. 2.3 Synchronous Grammars Again Finally, note that synchronous grammar is a special case of quasi-synchronous grammar. In the contextfree case, a synchronous grammar restricts senses to nodes in the source tree and the for any production — ... a synchronous context-free grammar requires that 1. 2. &gt; a child of the source tree, no children in the source tree, these rules imply that the children of any node aligned to themselves aligned to The construction for synchronous tree-substitution and treeadjoining grammars goes through similarly but operates on the derivation trees. 3 Parameterizing a QCFG Recall that our goal is a conditional model of A  |For the remainder of this paper, we a dependency-tree representation of Each tree node represents a word of the sentence together with a part-of-speech tag. Syntactic dependencies in each tree are represented directly by the parent-child relationships. Why this representation? First, it helps us concisely formulate a QG translation model where the source dependencies influence the generation of target dependencies (see figure 3). Second, for evaluation, it is trivial to obtain the word-to-word alignments from the node-to-node alignments. Third, the part-of-speech tags are useful backoff features, and in fact play a special role in our model below. stochastically generating a translation our quasi-synchronous generative process will be influenced by both fluency and adequacy. That is, it both the local well-formedness of criterion) and local faithfulness bilingual criterion). We combine these in a simple generative model rather than a log-linear model. When generating the children of a node in the process first generates their tags using monolingual parameters (fluency), and then fills in in the words using bilingual parameters (adequacy) that seand translate words from each node in labeled by a triple (tag, word, aligned word). Given a parent node h, we wish to generate sequences of and right child nodes, of the form a, parameters from a simple generative model of syntax used for grammar inducthe Dependency Model with Valence of Klein and Manning (2004). In scoring dependency tags rather than words. The parameters of the model are: 1.  |p, the probability of generatthe next child tag in the sequence of where E {left, 2.  |h, dir, the probability of generno more child tags in the sequence of children. This is conditioned in part on the “ad- E {true, which indicates the sequence of is empty so far. parameters word-to-word translation and aligned dependency configurations. thus use the conditional probability | source word which may be transas target word Finally, when a parent word to generates a child, we stochastically to align the child to a node in several possible relations to A “monotonic” dependency alignment, for example, would have andin a parent-child relationship like their target-tree analogues. In different versions of the model, we allowed various dependency alignment (figure 3). These configurations repdivision of labor is somewhat artificial, and could be remedied in a log-linear model, Naive Bayes model, or deficient generative model that generates both tags and words conditioned on both monolingual and bilingual context. 27 resent cases where the parent-child dependency being generated by the QG in the target language maps onto source-language child-parent, for head swapping; the same source node, for two-to-one alignment; nodes that are siblings or in a c-command relationship, for scrambling and extraposition; or in a grandparent-grandchild relationship, e.g. when a preposition is inserted in the source language. We also allowed a “none-of-the-above” configuration, to account for extremely mismatched sentences. The probability of the target-language depentreelet rooted at thus: _ H H dir, dir, 4 Experiments We claim that for modeling human-translated bitext, it is better to project syntax only loosely. To evaluate this claim, we train quasi-synchronous dependency grammars that allow progressively more divergence from monotonic tree alignment. We evaluate these models on cross-entropy over held-out data and on error rate in a word-alignment task. One might doubt the use of dependency trees for alignment, since Gildea (2004) found that constituency trees aligned better. That experiment, however, aligned only the 1-best parse trees. We too will only the 1-best source tree but in conto Gildea, we will search for the target tree aligns best with Finding the alignis simply a matter of parsing the QG from 4.1 Data and Training We performed our modeling experiments with the German-English portion of the Europarl European Parliament transcripts (Koehn, 2002). We obtained monolingual parse trees from the Stanford German and English parsers (Klein and Manning, 2003). Initial estimates of lexical translation probabilities came from the IBM Model 4 translation tables proby (Brown et al., 1993; Och and Ney, 2003). All text was lowercased and numbers of two or more digits were converted to an equal number of hash signs. The bitext was divided into training sets of 1K, 10K, and 100K sentence pairs. We held out one thousand sentences for evaluating the crossentropy of the various models and hand-aligned 100 sentence pairs to evaluate alignment error rate (AER). We trained the model parameters on bitext using the Expectation-Maximization (EM) algorithm. The is fully observed, but we parse the target language. As noted, the initial lexical translation probabilities came from IBM Model 4. We initialized the in one of two ways: using either simple tag co-occurrences as in (Klein and Manning, 2004) or “supervised” counts from the monolingual target-language parser. This latter initialization simulates the condition when one has a small amount of bitext but a larger amount of target data for language modeling. As with any monolingual grammar, we perform EM training with the Inside-Outside algorithm, computing inside probabilities with dynamic programming and outside probabilities through backpropagation. Searching the full space of target-language dependency trees and alignments to the source tree consumed several seconds per sentence. During training, therefore, we constrained alignments to come the union of Model 4 alignments. These constraints were applied only during training and not during evaluation of cross-entropy or AER. 4.2 Conditional Cross-Entropy of the Model To test the explanatory power of our QCFG, we evaluated its conditional cross-entropy on held-out data (table 1). In other words, we measured how well a trained QCFG could predict the true translation of novel source sentences by summing over all parses of the target given the source. We trained QCFG models under different conditions of bitext size and parameter initialization. However, the principal independent variable was the set of dependency alignment configurations allowed. From these cross-entropy results, it is clear that synchronous grammar is unwise. We ob- 28 (a) parent-child (b) child-parent (c) same node see sehe ich I likes Voelkerrecht law schwimmt gern swimming international (d) siblings (e) grandparent-grandchild (f) c-command bekommen answer Wahlkampf campaign sagte bought auf Antwort to von 2003 Was dass what 2003 kaufte 3: a head to a new child to the QCFG, be related in the source tree as, among other things, (a) parent–child, (b) child–parent, (c) identical nodes, (d) siblings, (e) grandparent–grandchild, (f) c-commander–c-commandee, (g) none of the above. Here German is the source and English is the target. Case (g), not pictured can be seen in figure 1, in English-German order, where the child-parent pair k¨onnte to the words respectively. Since they are not in a c-command relationship.</abstract>
<note confidence="0.987608222222222">Permitted configurations CE at 1k 10k CE 100k parent-child (a) 43.82 22.40 13.44 + child-parent (b) 41.27 21.73 12.62 + same node (c) 41.01 21.50 12.38 + all breakages (g) 35.63 18.72 11.27 + siblings (d) 34.59 18.59 11.21 + grandparent-grandchild (e) 34.52 18.55 11.17 + c-command (f) 34.46 18.59 11.27 No alignments allowed 60.86 53.28 46.94</note>
<abstract confidence="0.97791965">1: on held-out data with different dependency configurations (figure 3) allowed, for 1k, 10k, and 100k training sentences. The big error reductions arrive when we allow arbitrary non-local alignments in condition (g). Distinguishing some common cases of non-local alignments improves performance further. For comparison, we show cross-entropy when every target language node is unaligned. tain comparatively poor performance if we require parent-child pairs in the target tree to align to parentpairs in the source (or to Performance improves as we allow and distinguish more alignment configurations. 4.3 Word Alignment We computed standard measures of alignment precision, recall, and error rate on a test set of 100 handaligned German sentence pairs with 1300 alignment links. As with many word-alignment evaluations, do not score links to Just as for crossentropy, we see that more permissive alignments lead to better performance (table 2). Having selected the best system using the crossentropy measurement, we compare its alignment errate against the standard Model 4 base- As Figure 4 shows, our QCFG for German English consistently produces better alignments than the Model 4 channel model for the same direction, This comparison is the appropriate one because both of these models are forced to align each English word to at most one German 5 Conclusions With quasi-synchronous grammars, we have presented a new approach to syntactic MT: constructing a monolingual target-language grammar that describes the aligned translations of a source-language sentence. We described a simple parameterization German MT, one would use a German QCFG as above, but an English channel model. In this arguably inappropriate comparison, Figure 4 shows, the Model 4 channel model produces slightly better word alignments than the QG.</abstract>
<note confidence="0.892167333333333">29 Permitted configurations at 1k AER 10k 100k parent-child (a) 40.69 39.03 33.62 + child-parent (b) 43.17 39.78 33.79 + same node (c) 43.22 40.86 34.38 + all breakages (g) 37.63 30.51 25.99 + siblings (d) 37.87 33.36 29.27 + grandparent-grandchild (e) 36.78 32.73 28.84 + c-command (f) 37.04 33.51 27.45</note>
<abstract confidence="0.976696515151515">2: error rate (%) with different dependency configurations allowed. 1000 10000 100000 1e+06 training sentence pairs 4: error rate with best model (all break- The QCFG consistently beat one model and was close to the other. with gradually increasing syntactic domains of locality, and estimated those parameters on German- English bitext. The QG formalism admits many more nuanced options for features than we have exploited. In particular, we now are exploring log-linear QGs that overlapping elementary trees of considering the syntactic configuration and lexical conof the to which each elementary tree aligns. Even simple QGs, however, turned out to do quite well. Our evaluation on a German-English wordalignment task showed them to be competitive with IBM model 4—consistently beating the German- English direction by several percentage points of alignment error rate and within 1% AER of the English-German direction. In particular, alignment accuracy benefited from allowing syntactic breakages between the two dependency structures. We are also working on a translation decoding using QG. Our first system uses the QG to find optimal to then extracts a synchronous tree-substitution grammar from the aligned trees. Our second system searches a target-language vofor the optimal the input Acknowledgements</abstract>
<note confidence="0.9119505">This work was supported by a National Science Foundation Graduate Research Fellowship for the first author and by NSF Grant No. 0313193. References</note>
<abstract confidence="0.922605733333333">H. Alshawi, S. Bangalore, and S. Douglas. 2000. Learning dependency translation models as collections of finite state transducers. 26(1):45–60. P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine trans- Parameter estimation. 19(2):263–311. Y. Ding and M. Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion grammars. In pages 541–548. B. J. Dorr. 1994. Machine translation divergences: A formal and proposed solution. Linguis- 20(4):597–633. J. Eisner. 2003. Learning non-isomorphic tree mappings for translation. In Companion Vol. H. J. Fox. 2002. Phrasal cohesion and statistical machine trans-</abstract>
<note confidence="0.837889848484848">In pages 392–399. D. Gildea. 2003. Loosely tree-based alignment for machine In pages 80–87. D. Gildea. 2004. Dependencies vs. constituents for tree-based In pages 214–221. R. Hwa, P. Resnik, A. Weinberg, and O. Kolak. 2002. Evaluating translational correspondence using annotation projec- In D. Klein and C. D. Manning. 2003. Accurate unlexicalized In pages 423–430. D. Klein and C. D. Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. pages 479–486. P. Koehn. 2002. Europarl: A corpus for evaluation of machine translation. publications/europarl.ps. A. Lavie, S. Vogel, L. Levin, E. Peterson, K. Probst, A. F. Llitj´os, R. Reynolds, J. Carbonell, and R. Cohen. 2003. Experiments with a Hindi-to-English transfer-based MT system a miserly data scenario. Transactions on Asian Information 2(2):143 – 163. I. D. Melamed, G. Satta, and B. Wellington. 2004. Generalized grammars. In pages 661–668. I. D. Melamed. 2004. Statistical machine translation by pars- In pages 653–660. F. J. Och and H. Ney. 2003. A systematic comparison of various alignment models. 29(1):19–51. C. Quirk, A. Menezes, and C. Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal SMT. In pages 271–279. S. M. Shieber and Y. Schabes. 1990. Synchronous treegrammars. In pages 253–258. D. Wu. 1997. Stochastic inversion transduction grammars and</note>
<abstract confidence="0.760603090909091">parsing of parallel corpora. 23(3):377–403. K. Yamada and K. Knight. 2001. A syntax-based statistical model. In alignment error rate 0.45 0.35 0.25 0.5 0.4 0.3 0.2</abstract>
<note confidence="0.710739">za4 Giza4 bk 30</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Alshawi</author>
<author>S Bangalore</author>
<author>S Douglas</author>
</authors>
<title>Learning dependency translation models as collections of finite state head transducers.</title>
<date>2000</date>
<journal>CL,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="4437" citStr="Alshawi et al., 2000" startWordPosition="725" endWordPosition="728">ransfer-synthesis process in the translator’s head). But an MT researcher may not have the wherewithal to design, adequately train, and efficiently compute with “deep” accounts of this sort. Under our approach, it is possible to use a simple, tractable syntactic model, but with some contextual probability of “sloppy” transfer. 1.2 From Synchronous to Quasi-Synchronous Grammars Because our approach will let anything align to anything, it is reminiscent of IBM Models 1–5 (Brown et al., 1993). It differs from the many approaches where (1) is defined by a stochastic synchronous grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Melamed, 2004) and from transfer-based systems defined by context-free grammars (Lavie et al., 2003). The synchronous grammar approach, originally due to Shieber and Schabes (1990), supposes that T2 is generated in lockstep to T1.1 When choosing how to expand a certain VP node in T2, a synchronous CFG process would observe that this node is aligned to a node VP0 in T1, which had been expanded in T1 by VP0 —* NP0 V0. This might bias it toward choosing to expand the VP in T2 as VP —* V NP, with the new children V aligned to V0 and NP aligned</context>
</contexts>
<marker>Alshawi, Bangalore, Douglas, 2000</marker>
<rawString>H. Alshawi, S. Bangalore, and S. Douglas. 2000. Learning dependency translation models as collections of finite state head transducers. CL, 26(1):45–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>CL,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="4311" citStr="Brown et al., 1993" startWordPosition="702" endWordPosition="705">ition of the PP auf diese Frage either syntactically (by invoking scrambling) or semantically (by describing a deep analysistransfer-synthesis process in the translator’s head). But an MT researcher may not have the wherewithal to design, adequately train, and efficiently compute with “deep” accounts of this sort. Under our approach, it is possible to use a simple, tractable syntactic model, but with some contextual probability of “sloppy” transfer. 1.2 From Synchronous to Quasi-Synchronous Grammars Because our approach will let anything align to anything, it is reminiscent of IBM Models 1–5 (Brown et al., 1993). It differs from the many approaches where (1) is defined by a stochastic synchronous grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Melamed, 2004) and from transfer-based systems defined by context-free grammars (Lavie et al., 2003). The synchronous grammar approach, originally due to Shieber and Schabes (1990), supposes that T2 is generated in lockstep to T1.1 When choosing how to expand a certain VP node in T2, a synchronous CFG process would observe that this node is aligned to a node VP0 in T1, which had been expanded in T1 by VP0 —* NP0 V0.</context>
<context position="19922" citStr="Brown et al., 1993" startWordPosition="3284" endWordPosition="3287">r only the 1-best source tree T1, but in constrast to Gildea, we will search for the target tree T2 that aligns best with T1. Finding T2 and the alignment is simply a matter of parsing 52 with the QG derived from T1. 4.1 Data and Training We performed our modeling experiments with the German-English portion of the Europarl European Parliament transcripts (Koehn, 2002). We obtained monolingual parse trees from the Stanford German and English parsers (Klein and Manning, 2003). Initial estimates of lexical translation probabilities came from the IBM Model 4 translation tables produced by GIZA++ (Brown et al., 1993; Och and Ney, 2003). All text was lowercased and numbers of two or more digits were converted to an equal number of hash signs. The bitext was divided into training sets of 1K, 10K, and 100K sentence pairs. We held out one thousand sentences for evaluating the crossentropy of the various models and hand-aligned 100 sentence pairs to evaluate alignment error rate (AER). We trained the model parameters on bitext using the Expectation-Maximization (EM) algorithm. The T1 tree is fully observed, but we parse the target language. As noted, the initial lexical translation probabilities came from IBM</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. CL, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ding</author>
<author>M Palmer</author>
</authors>
<title>Machine translation using probabilistic synchronous dependency insertion grammars.</title>
<date>2005</date>
<booktitle>In ACL,</booktitle>
<pages>541--548</pages>
<contexts>
<context position="6171" citStr="Ding and Palmer, 2005" startWordPosition="1022" endWordPosition="1025">cribe an equivalent conditional process for generating T2, A given T1. deterministically assembles the latter rules into an actual tree T2 and reads off its yield S2. What is worrisome about the synchronous process is that it can only produce trees T2 that are perfectly isomorphic to T1. It is possible to relax this requirement by using synchronous grammar formalisms more sophisticated than CFG:2 one can permit unaligned nodes (Yamada and Knight, 2001), duplicated children (Gildea, 2003)3, or alignment between elementary trees of differing sizes rather than between single rules (Eisner, 2003; Ding and Palmer, 2005; Quirk et al., 2005). However, one would need rather powerful and slow grammar formalisms (Shieber and Schabes, 1990; Melamed et al., 2004), often with discontiguous constituents, to account for all the linguistic divergences that could arise from different movement patterns (scrambling, wh-in situ) or free translation. In particular, a synchronous grammar cannot practically allow S2 to be any permutation of S1, as IBM Models 1–5 do. Our alternative is to define a “quasi-synchronous” stochastic process. It generates T2 in a way that is not in thrall to T1 but is “inspired by it.” (A human tra</context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Y. Ding and M. Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion grammars. In ACL, pages 541–548.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Dorr</author>
</authors>
<title>Machine translation divergences: A formal description and proposed solution.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="3431" citStr="Dorr, 1994" startWordPosition="562" endWordPosition="563">yntactic alignment. Any portion of T2 can align to any portion of T1, or to NULL. Nodes that are syntactically related in T1 do not have to translate into nodes that are syntactically related in T2—although (1) is usually higher if they do. This property makes our approach especially promising for aligning freely, or erroneously, translated sentences, and for coping with syntactic diver23 Proceedings of the Workshop on Statistical Machine Translation, pages 23–30, New York City, June 2006. c�2006 Association for Computational Linguistics gences observed between even closely related languages (Dorr, 1994; Fox, 2002). We can patch together an alignment without accounting for all the details of the translation process. For instance, perhaps a source NP (figure 1) or PP (figure 2) appears “out of place” in the target sentence. A linguist might account for the position of the PP auf diese Frage either syntactically (by invoking scrambling) or semantically (by describing a deep analysistransfer-synthesis process in the translator’s head). But an MT researcher may not have the wherewithal to design, adequately train, and efficiently compute with “deep” accounts of this sort. Under our approach, it </context>
</contexts>
<marker>Dorr, 1994</marker>
<rawString>B. J. Dorr. 1994. Machine translation divergences: A formal description and proposed solution. Computational Linguistics, 20(4):597–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for machine translation.</title>
<date>2003</date>
<journal>In ACL Companion</journal>
<volume>Vol.</volume>
<contexts>
<context position="4476" citStr="Eisner, 2003" startWordPosition="733" endWordPosition="734">ead). But an MT researcher may not have the wherewithal to design, adequately train, and efficiently compute with “deep” accounts of this sort. Under our approach, it is possible to use a simple, tractable syntactic model, but with some contextual probability of “sloppy” transfer. 1.2 From Synchronous to Quasi-Synchronous Grammars Because our approach will let anything align to anything, it is reminiscent of IBM Models 1–5 (Brown et al., 1993). It differs from the many approaches where (1) is defined by a stochastic synchronous grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Melamed, 2004) and from transfer-based systems defined by context-free grammars (Lavie et al., 2003). The synchronous grammar approach, originally due to Shieber and Schabes (1990), supposes that T2 is generated in lockstep to T1.1 When choosing how to expand a certain VP node in T2, a synchronous CFG process would observe that this node is aligned to a node VP0 in T1, which had been expanded in T1 by VP0 —* NP0 V0. This might bias it toward choosing to expand the VP in T2 as VP —* V NP, with the new children V aligned to V0 and NP aligned to NP0. The process then continues rec</context>
<context position="6148" citStr="Eisner, 2003" startWordPosition="1020" endWordPosition="1021">e directly describe an equivalent conditional process for generating T2, A given T1. deterministically assembles the latter rules into an actual tree T2 and reads off its yield S2. What is worrisome about the synchronous process is that it can only produce trees T2 that are perfectly isomorphic to T1. It is possible to relax this requirement by using synchronous grammar formalisms more sophisticated than CFG:2 one can permit unaligned nodes (Yamada and Knight, 2001), duplicated children (Gildea, 2003)3, or alignment between elementary trees of differing sizes rather than between single rules (Eisner, 2003; Ding and Palmer, 2005; Quirk et al., 2005). However, one would need rather powerful and slow grammar formalisms (Shieber and Schabes, 1990; Melamed et al., 2004), often with discontiguous constituents, to account for all the linguistic divergences that could arise from different movement patterns (scrambling, wh-in situ) or free translation. In particular, a synchronous grammar cannot practically allow S2 to be any permutation of S1, as IBM Models 1–5 do. Our alternative is to define a “quasi-synchronous” stochastic process. It generates T2 in a way that is not in thrall to T1 but is “inspir</context>
</contexts>
<marker>Eisner, 2003</marker>
<rawString>J. Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In ACL Companion Vol.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H J Fox</author>
</authors>
<title>Phrasal cohesion and statistical machine translation.</title>
<date>2002</date>
<booktitle>In EMNLP,</booktitle>
<pages>392--399</pages>
<contexts>
<context position="3443" citStr="Fox, 2002" startWordPosition="564" endWordPosition="565">gnment. Any portion of T2 can align to any portion of T1, or to NULL. Nodes that are syntactically related in T1 do not have to translate into nodes that are syntactically related in T2—although (1) is usually higher if they do. This property makes our approach especially promising for aligning freely, or erroneously, translated sentences, and for coping with syntactic diver23 Proceedings of the Workshop on Statistical Machine Translation, pages 23–30, New York City, June 2006. c�2006 Association for Computational Linguistics gences observed between even closely related languages (Dorr, 1994; Fox, 2002). We can patch together an alignment without accounting for all the details of the translation process. For instance, perhaps a source NP (figure 1) or PP (figure 2) appears “out of place” in the target sentence. A linguist might account for the position of the PP auf diese Frage either syntactically (by invoking scrambling) or semantically (by describing a deep analysistransfer-synthesis process in the translator’s head). But an MT researcher may not have the wherewithal to design, adequately train, and efficiently compute with “deep” accounts of this sort. Under our approach, it is possible </context>
</contexts>
<marker>Fox, 2002</marker>
<rawString>H. J. Fox. 2002. Phrasal cohesion and statistical machine translation. In EMNLP, pages 392–399.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
</authors>
<title>Loosely tree-based alignment for machine translation. In</title>
<date>2003</date>
<booktitle>ACL,</booktitle>
<pages>80--87</pages>
<contexts>
<context position="4490" citStr="Gildea, 2003" startWordPosition="735" endWordPosition="736">T researcher may not have the wherewithal to design, adequately train, and efficiently compute with “deep” accounts of this sort. Under our approach, it is possible to use a simple, tractable syntactic model, but with some contextual probability of “sloppy” transfer. 1.2 From Synchronous to Quasi-Synchronous Grammars Because our approach will let anything align to anything, it is reminiscent of IBM Models 1–5 (Brown et al., 1993). It differs from the many approaches where (1) is defined by a stochastic synchronous grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Melamed, 2004) and from transfer-based systems defined by context-free grammars (Lavie et al., 2003). The synchronous grammar approach, originally due to Shieber and Schabes (1990), supposes that T2 is generated in lockstep to T1.1 When choosing how to expand a certain VP node in T2, a synchronous CFG process would observe that this node is aligned to a node VP0 in T1, which had been expanded in T1 by VP0 —* NP0 V0. This might bias it toward choosing to expand the VP in T2 as VP —* V NP, with the new children V aligned to V0 and NP aligned to NP0. The process then continues recursively by ch</context>
<context position="6042" citStr="Gildea, 2003" startWordPosition="1005" endWordPosition="1006">(T2, A, T1). Dividing by the marginal p(T1) gives a conditional model p(T2, A T1) as in (1). In the text, we directly describe an equivalent conditional process for generating T2, A given T1. deterministically assembles the latter rules into an actual tree T2 and reads off its yield S2. What is worrisome about the synchronous process is that it can only produce trees T2 that are perfectly isomorphic to T1. It is possible to relax this requirement by using synchronous grammar formalisms more sophisticated than CFG:2 one can permit unaligned nodes (Yamada and Knight, 2001), duplicated children (Gildea, 2003)3, or alignment between elementary trees of differing sizes rather than between single rules (Eisner, 2003; Ding and Palmer, 2005; Quirk et al., 2005). However, one would need rather powerful and slow grammar formalisms (Shieber and Schabes, 1990; Melamed et al., 2004), often with discontiguous constituents, to account for all the linguistic divergences that could arise from different movement patterns (scrambling, wh-in situ) or free translation. In particular, a synchronous grammar cannot practically allow S2 to be any permutation of S1, as IBM Models 1–5 do. Our alternative is to define a “</context>
</contexts>
<marker>Gildea, 2003</marker>
<rawString>D. Gildea. 2003. Loosely tree-based alignment for machine translation. In ACL, pages 80–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
</authors>
<title>Dependencies vs. constituents for tree-based alignment. In</title>
<date>2004</date>
<booktitle>EMNLP,</booktitle>
<pages>214--221</pages>
<contexts>
<context position="19175" citStr="Gildea (2004)" startWordPosition="3162" endWordPosition="3163">: P(D(h) |h, h&apos;, p) _ H H dirE{l,r} cEdepsD(p,dir) P(D(c) |a, a&apos;, c) × pstop(nostop |p, dir, adj) ×pchoose(c |p, dir) ×pconfig(config) × ptrans(a |a&apos;) pstop(stop |p, dir, adj) 4 Experiments We claim that for modeling human-translated bitext, it is better to project syntax only loosely. To evaluate this claim, we train quasi-synchronous dependency grammars that allow progressively more divergence from monotonic tree alignment. We evaluate these models on cross-entropy over held-out data and on error rate in a word-alignment task. One might doubt the use of dependency trees for alignment, since Gildea (2004) found that constituency trees aligned better. That experiment, however, aligned only the 1-best parse trees. We too will consider only the 1-best source tree T1, but in constrast to Gildea, we will search for the target tree T2 that aligns best with T1. Finding T2 and the alignment is simply a matter of parsing 52 with the QG derived from T1. 4.1 Data and Training We performed our modeling experiments with the German-English portion of the Europarl European Parliament transcripts (Koehn, 2002). We obtained monolingual parse trees from the Stanford German and English parsers (Klein and Manning</context>
</contexts>
<marker>Gildea, 2004</marker>
<rawString>D. Gildea. 2004. Dependencies vs. constituents for tree-based alignment. In EMNLP, pages 214–221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hwa</author>
<author>P Resnik</author>
<author>A Weinberg</author>
<author>O Kolak</author>
</authors>
<title>Evaluating translational correspondence using annotation projection.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2403" citStr="Hwa et al. (2002)" startWordPosition="393" endWordPosition="396">nd T2, one can align them by finding the A that maximizes (1) (equivalent to maximizing p(A |T2, T1)). Similarly, one can align S1 and S2 by finding the parses T1 and T2, and alignment A, that maximize p(T2, A |T1) · p(T1 |S1), where p(T1 |S1) is given by a monolingual parser. We usually accomplish such maximizations by dynamic programming. Equation (1) does not assume that T1 and T2 are isomorphic. For example, a model might judge T2 and A to be likely, given T1, provided that many— but not necessarily all—of the syntactic dependencies in T1 are aligned with corresponding dependencies in T2. Hwa et al. (2002) found that human translations from Chinese to English preserved only 39–42% of the unlabeled Chinese dependencies. They increased this figure to 67% by using more involved heuristics for aligning dependencies across these two languages. That suggests that (1) should be defined to consider more than one dependency at a time. This inspires the key novel feature of our models: A does not have to be a “well-behaved” syntactic alignment. Any portion of T2 can align to any portion of T1, or to NULL. Nodes that are syntactically related in T1 do not have to translate into nodes that are syntacticall</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Kolak, 2002</marker>
<rawString>R. Hwa, P. Resnik, A. Weinberg, and O. Kolak. 2002. Evaluating translational correspondence using annotation projection. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In ACL,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="19782" citStr="Klein and Manning, 2003" startWordPosition="3262" endWordPosition="3265">nce Gildea (2004) found that constituency trees aligned better. That experiment, however, aligned only the 1-best parse trees. We too will consider only the 1-best source tree T1, but in constrast to Gildea, we will search for the target tree T2 that aligns best with T1. Finding T2 and the alignment is simply a matter of parsing 52 with the QG derived from T1. 4.1 Data and Training We performed our modeling experiments with the German-English portion of the Europarl European Parliament transcripts (Koehn, 2002). We obtained monolingual parse trees from the Stanford German and English parsers (Klein and Manning, 2003). Initial estimates of lexical translation probabilities came from the IBM Model 4 translation tables produced by GIZA++ (Brown et al., 1993; Och and Ney, 2003). All text was lowercased and numbers of two or more digits were converted to an equal number of hash signs. The bitext was divided into training sets of 1K, 10K, and 100K sentence pairs. We held out one thousand sentences for evaluating the crossentropy of the various models and hand-aligned 100 sentence pairs to evaluate alignment error rate (AER). We trained the model parameters on bitext using the Expectation-Maximization (EM) algor</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. D. Manning. 2003. Accurate unlexicalized parsing. In ACL, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In ACL,</booktitle>
<pages>479--486</pages>
<contexts>
<context position="16616" citStr="Klein and Manning (2004)" startWordPosition="2754" endWordPosition="2757"> log-linear model. When generating the children of a node in T2, the process first generates their tags using monolingual parameters (fluency), and then fills in in the words using bilingual parameters (adequacy) that select and translate words from T1.5 Concretely, each node in T2 is labeled by a triple (tag, word, aligned word). Given a parent node (p, h, h&apos;) in T2, we wish to generate sequences of left and right child nodes, of the form (c, a, a&apos;). Our monolingual parameters come from a simple generative model of syntax used for grammar induction: the Dependency Model with Valence (DMV) of Klein and Manning (2004). In scoring dependency attachments, DMV uses tags rather than words. The parameters of the model are: 1. pehoose(c |p, dir): the probability of generating c as the next child tag in the sequence of dir children, where dir E {left, right}. 2. pstop(s |h, dir, adj): the probability of generating no more child tags in the sequence of dir children. This is conditioned in part on the “adjacency” adj E {true, false}, which indicates whether the sequence of dir children is empty so far. Our bilingual parameters score word-to-word translation and aligned dependency configurations. We thus use the con</context>
<context position="20668" citStr="Klein and Manning, 2004" startWordPosition="3408" endWordPosition="3411">signs. The bitext was divided into training sets of 1K, 10K, and 100K sentence pairs. We held out one thousand sentences for evaluating the crossentropy of the various models and hand-aligned 100 sentence pairs to evaluate alignment error rate (AER). We trained the model parameters on bitext using the Expectation-Maximization (EM) algorithm. The T1 tree is fully observed, but we parse the target language. As noted, the initial lexical translation probabilities came from IBM Model 4. We initialized the monolingual DMV parameters in one of two ways: using either simple tag co-occurrences as in (Klein and Manning, 2004) or “supervised” counts from the monolingual target-language parser. This latter initialization simulates the condition when one has a small amount of bitext but a larger amount of target data for language modeling. As with any monolingual grammar, we perform EM training with the Inside-Outside algorithm, computing inside probabilities with dynamic programming and outside probabilities through backpropagation. Searching the full space of target-language dependency trees and alignments to the source tree consumed several seconds per sentence. During training, therefore, we constrained alignment</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>D. Klein and C. D. Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In ACL, pages 479–486.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A multilingual corpus for evaluation of machine translation.</title>
<date>2002</date>
<note>http://www.iccs.informatics.ed.ac.uk/˜pkoehn/-publications/europarl.ps.</note>
<contexts>
<context position="19674" citStr="Koehn, 2002" startWordPosition="3248" endWordPosition="3249">ror rate in a word-alignment task. One might doubt the use of dependency trees for alignment, since Gildea (2004) found that constituency trees aligned better. That experiment, however, aligned only the 1-best parse trees. We too will consider only the 1-best source tree T1, but in constrast to Gildea, we will search for the target tree T2 that aligns best with T1. Finding T2 and the alignment is simply a matter of parsing 52 with the QG derived from T1. 4.1 Data and Training We performed our modeling experiments with the German-English portion of the Europarl European Parliament transcripts (Koehn, 2002). We obtained monolingual parse trees from the Stanford German and English parsers (Klein and Manning, 2003). Initial estimates of lexical translation probabilities came from the IBM Model 4 translation tables produced by GIZA++ (Brown et al., 1993; Och and Ney, 2003). All text was lowercased and numbers of two or more digits were converted to an equal number of hash signs. The bitext was divided into training sets of 1K, 10K, and 100K sentence pairs. We held out one thousand sentences for evaluating the crossentropy of the various models and hand-aligned 100 sentence pairs to evaluate alignme</context>
</contexts>
<marker>Koehn, 2002</marker>
<rawString>P. Koehn. 2002. Europarl: A multilingual corpus for evaluation of machine translation. http://www.iccs.informatics.ed.ac.uk/˜pkoehn/-publications/europarl.ps.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie</author>
<author>S Vogel</author>
<author>L Levin</author>
<author>E Peterson</author>
<author>K Probst</author>
<author>A F Llitj´os</author>
<author>R Reynolds</author>
<author>J Carbonell</author>
<author>R Cohen</author>
</authors>
<title>Experiments with a Hindi-to-English transfer-based MT system under a miserly data scenario.</title>
<date>2003</date>
<journal>ACM Transactions on Asian Language Information Processing,</journal>
<volume>2</volume>
<issue>2</issue>
<pages>163</pages>
<marker>Lavie, Vogel, Levin, Peterson, Probst, Llitj´os, Reynolds, Carbonell, Cohen, 2003</marker>
<rawString>A. Lavie, S. Vogel, L. Levin, E. Peterson, K. Probst, A. F. Llitj´os, R. Reynolds, J. Carbonell, and R. Cohen. 2003. Experiments with a Hindi-to-English transfer-based MT system under a miserly data scenario. ACM Transactions on Asian Language Information Processing, 2(2):143 – 163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
<author>G Satta</author>
<author>B Wellington</author>
</authors>
<title>Generalized multitext grammars.</title>
<date>2004</date>
<booktitle>In ACL,</booktitle>
<pages>661--668</pages>
<contexts>
<context position="6311" citStr="Melamed et al., 2004" startWordPosition="1045" endWordPosition="1048">nd reads off its yield S2. What is worrisome about the synchronous process is that it can only produce trees T2 that are perfectly isomorphic to T1. It is possible to relax this requirement by using synchronous grammar formalisms more sophisticated than CFG:2 one can permit unaligned nodes (Yamada and Knight, 2001), duplicated children (Gildea, 2003)3, or alignment between elementary trees of differing sizes rather than between single rules (Eisner, 2003; Ding and Palmer, 2005; Quirk et al., 2005). However, one would need rather powerful and slow grammar formalisms (Shieber and Schabes, 1990; Melamed et al., 2004), often with discontiguous constituents, to account for all the linguistic divergences that could arise from different movement patterns (scrambling, wh-in situ) or free translation. In particular, a synchronous grammar cannot practically allow S2 to be any permutation of S1, as IBM Models 1–5 do. Our alternative is to define a “quasi-synchronous” stochastic process. It generates T2 in a way that is not in thrall to T1 but is “inspired by it.” (A human translator might be imagined to behave similarly.) When choosing how to expand nodes of T2, we are influenced both by the structure of T1 and b</context>
</contexts>
<marker>Melamed, Satta, Wellington, 2004</marker>
<rawString>I. D. Melamed, G. Satta, and B. Wellington. 2004. Generalized multitext grammars. In ACL, pages 661–668.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
</authors>
<title>Statistical machine translation by parsing.</title>
<date>2004</date>
<booktitle>In ACL,</booktitle>
<pages>653--660</pages>
<contexts>
<context position="4506" citStr="Melamed, 2004" startWordPosition="737" endWordPosition="738">ay not have the wherewithal to design, adequately train, and efficiently compute with “deep” accounts of this sort. Under our approach, it is possible to use a simple, tractable syntactic model, but with some contextual probability of “sloppy” transfer. 1.2 From Synchronous to Quasi-Synchronous Grammars Because our approach will let anything align to anything, it is reminiscent of IBM Models 1–5 (Brown et al., 1993). It differs from the many approaches where (1) is defined by a stochastic synchronous grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Melamed, 2004) and from transfer-based systems defined by context-free grammars (Lavie et al., 2003). The synchronous grammar approach, originally due to Shieber and Schabes (1990), supposes that T2 is generated in lockstep to T1.1 When choosing how to expand a certain VP node in T2, a synchronous CFG process would observe that this node is aligned to a node VP0 in T1, which had been expanded in T1 by VP0 —* NP0 V0. This might bias it toward choosing to expand the VP in T2 as VP —* V NP, with the new children V aligned to V0 and NP aligned to NP0. The process then continues recursively by choosing moves to </context>
</contexts>
<marker>Melamed, 2004</marker>
<rawString>I. D. Melamed. 2004. Statistical machine translation by parsing. In ACL, pages 653–660.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>CL,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="19942" citStr="Och and Ney, 2003" startWordPosition="3288" endWordPosition="3291">urce tree T1, but in constrast to Gildea, we will search for the target tree T2 that aligns best with T1. Finding T2 and the alignment is simply a matter of parsing 52 with the QG derived from T1. 4.1 Data and Training We performed our modeling experiments with the German-English portion of the Europarl European Parliament transcripts (Koehn, 2002). We obtained monolingual parse trees from the Stanford German and English parsers (Klein and Manning, 2003). Initial estimates of lexical translation probabilities came from the IBM Model 4 translation tables produced by GIZA++ (Brown et al., 1993; Och and Ney, 2003). All text was lowercased and numbers of two or more digits were converted to an equal number of hash signs. The bitext was divided into training sets of 1K, 10K, and 100K sentence pairs. We held out one thousand sentences for evaluating the crossentropy of the various models and hand-aligned 100 sentence pairs to evaluate alignment error rate (AER). We trained the model parameters on bitext using the Expectation-Maximization (EM) algorithm. The T1 tree is fully observed, but we parse the target language. As noted, the initial lexical translation probabilities came from IBM Model 4. We initial</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. CL, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Quirk</author>
<author>A Menezes</author>
<author>C Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal SMT. In</title>
<date>2005</date>
<booktitle>ACL,</booktitle>
<pages>271--279</pages>
<contexts>
<context position="6192" citStr="Quirk et al., 2005" startWordPosition="1026" endWordPosition="1029">ditional process for generating T2, A given T1. deterministically assembles the latter rules into an actual tree T2 and reads off its yield S2. What is worrisome about the synchronous process is that it can only produce trees T2 that are perfectly isomorphic to T1. It is possible to relax this requirement by using synchronous grammar formalisms more sophisticated than CFG:2 one can permit unaligned nodes (Yamada and Knight, 2001), duplicated children (Gildea, 2003)3, or alignment between elementary trees of differing sizes rather than between single rules (Eisner, 2003; Ding and Palmer, 2005; Quirk et al., 2005). However, one would need rather powerful and slow grammar formalisms (Shieber and Schabes, 1990; Melamed et al., 2004), often with discontiguous constituents, to account for all the linguistic divergences that could arise from different movement patterns (scrambling, wh-in situ) or free translation. In particular, a synchronous grammar cannot practically allow S2 to be any permutation of S1, as IBM Models 1–5 do. Our alternative is to define a “quasi-synchronous” stochastic process. It generates T2 in a way that is not in thrall to T1 but is “inspired by it.” (A human translator might be imag</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>C. Quirk, A. Menezes, and C. Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal SMT. In ACL, pages 271–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Shieber</author>
<author>Y Schabes</author>
</authors>
<title>Synchronous treeadjoining grammars.</title>
<date>1990</date>
<booktitle>In ACL,</booktitle>
<pages>253--258</pages>
<contexts>
<context position="4672" citStr="Shieber and Schabes (1990)" startWordPosition="759" endWordPosition="762">use a simple, tractable syntactic model, but with some contextual probability of “sloppy” transfer. 1.2 From Synchronous to Quasi-Synchronous Grammars Because our approach will let anything align to anything, it is reminiscent of IBM Models 1–5 (Brown et al., 1993). It differs from the many approaches where (1) is defined by a stochastic synchronous grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Melamed, 2004) and from transfer-based systems defined by context-free grammars (Lavie et al., 2003). The synchronous grammar approach, originally due to Shieber and Schabes (1990), supposes that T2 is generated in lockstep to T1.1 When choosing how to expand a certain VP node in T2, a synchronous CFG process would observe that this node is aligned to a node VP0 in T1, which had been expanded in T1 by VP0 —* NP0 V0. This might bias it toward choosing to expand the VP in T2 as VP —* V NP, with the new children V aligned to V0 and NP aligned to NP0. The process then continues recursively by choosing moves to expand these children. One can regard this stochastic process as an instance of analysis-transfer-synthesis MT. Analysis chooses a parse T1 given S1. Transfer maps th</context>
<context position="6288" citStr="Shieber and Schabes, 1990" startWordPosition="1041" endWordPosition="1044">es into an actual tree T2 and reads off its yield S2. What is worrisome about the synchronous process is that it can only produce trees T2 that are perfectly isomorphic to T1. It is possible to relax this requirement by using synchronous grammar formalisms more sophisticated than CFG:2 one can permit unaligned nodes (Yamada and Knight, 2001), duplicated children (Gildea, 2003)3, or alignment between elementary trees of differing sizes rather than between single rules (Eisner, 2003; Ding and Palmer, 2005; Quirk et al., 2005). However, one would need rather powerful and slow grammar formalisms (Shieber and Schabes, 1990; Melamed et al., 2004), often with discontiguous constituents, to account for all the linguistic divergences that could arise from different movement patterns (scrambling, wh-in situ) or free translation. In particular, a synchronous grammar cannot practically allow S2 to be any permutation of S1, as IBM Models 1–5 do. Our alternative is to define a “quasi-synchronous” stochastic process. It generates T2 in a way that is not in thrall to T1 but is “inspired by it.” (A human translator might be imagined to behave similarly.) When choosing how to expand nodes of T2, we are influenced both by th</context>
</contexts>
<marker>Shieber, Schabes, 1990</marker>
<rawString>S. M. Shieber and Y. Schabes. 1990. Synchronous treeadjoining grammars. In ACL, pages 253–258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>CL,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="4415" citStr="Wu, 1997" startWordPosition="723" endWordPosition="724"> analysistransfer-synthesis process in the translator’s head). But an MT researcher may not have the wherewithal to design, adequately train, and efficiently compute with “deep” accounts of this sort. Under our approach, it is possible to use a simple, tractable syntactic model, but with some contextual probability of “sloppy” transfer. 1.2 From Synchronous to Quasi-Synchronous Grammars Because our approach will let anything align to anything, it is reminiscent of IBM Models 1–5 (Brown et al., 1993). It differs from the many approaches where (1) is defined by a stochastic synchronous grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Melamed, 2004) and from transfer-based systems defined by context-free grammars (Lavie et al., 2003). The synchronous grammar approach, originally due to Shieber and Schabes (1990), supposes that T2 is generated in lockstep to T1.1 When choosing how to expand a certain VP node in T2, a synchronous CFG process would observe that this node is aligned to a node VP0 in T1, which had been expanded in T1 by VP0 —* NP0 V0. This might bias it toward choosing to expand the VP in T2 as VP —* V NP, with the new children V aligne</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>D. Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. CL, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamada</author>
<author>K Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="4462" citStr="Yamada and Knight, 2001" startWordPosition="729" endWordPosition="732">ess in the translator’s head). But an MT researcher may not have the wherewithal to design, adequately train, and efficiently compute with “deep” accounts of this sort. Under our approach, it is possible to use a simple, tractable syntactic model, but with some contextual probability of “sloppy” transfer. 1.2 From Synchronous to Quasi-Synchronous Grammars Because our approach will let anything align to anything, it is reminiscent of IBM Models 1–5 (Brown et al., 1993). It differs from the many approaches where (1) is defined by a stochastic synchronous grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Melamed, 2004) and from transfer-based systems defined by context-free grammars (Lavie et al., 2003). The synchronous grammar approach, originally due to Shieber and Schabes (1990), supposes that T2 is generated in lockstep to T1.1 When choosing how to expand a certain VP node in T2, a synchronous CFG process would observe that this node is aligned to a node VP0 in T1, which had been expanded in T1 by VP0 —* NP0 V0. This might bias it toward choosing to expand the VP in T2 as VP —* V NP, with the new children V aligned to V0 and NP aligned to NP0. The process then</context>
<context position="6006" citStr="Yamada and Knight, 2001" startWordPosition="999" endWordPosition="1002">s T1 and T2 jointly, leading to a joint model p(T2, A, T1). Dividing by the marginal p(T1) gives a conditional model p(T2, A T1) as in (1). In the text, we directly describe an equivalent conditional process for generating T2, A given T1. deterministically assembles the latter rules into an actual tree T2 and reads off its yield S2. What is worrisome about the synchronous process is that it can only produce trees T2 that are perfectly isomorphic to T1. It is possible to relax this requirement by using synchronous grammar formalisms more sophisticated than CFG:2 one can permit unaligned nodes (Yamada and Knight, 2001), duplicated children (Gildea, 2003)3, or alignment between elementary trees of differing sizes rather than between single rules (Eisner, 2003; Ding and Palmer, 2005; Quirk et al., 2005). However, one would need rather powerful and slow grammar formalisms (Shieber and Schabes, 1990; Melamed et al., 2004), often with discontiguous constituents, to account for all the linguistic divergences that could arise from different movement patterns (scrambling, wh-in situ) or free translation. In particular, a synchronous grammar cannot practically allow S2 to be any permutation of S1, as IBM Models 1–5 </context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>K. Yamada and K. Knight. 2001. A syntax-based statistical translation model. In ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>