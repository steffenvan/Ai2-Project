<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000378">
<title confidence="0.995704">
Non-linear Learning for Statistical Machine Translation
</title>
<author confidence="0.996132">
Shujian Huang, Huadong Chen, Xinyu Dai and Jiajun Chen
</author>
<affiliation confidence="0.889021666666667">
State Key Laboratory for Novel Software Technology
Nanjing University
Nanjing 210023, China
</affiliation>
<email confidence="0.979522">
{huangsj, chenhd, daixy, chenjj}@nlp.nju.edu.cn
</email>
<sectionHeader confidence="0.997287" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999852">
Modern statistical machine translation
(SMT) systems usually use a linear com-
bination of features to model the quality
of each translation hypothesis. The linear
combination assumes that all the features
are in a linear relationship and constrains
that each feature interacts with the rest fea-
tures in an linear manner, which might
limit the expressive power of the model
and lead to a under-fit model on the cur-
rent data. In this paper, we propose a non-
linear modeling for the quality of transla-
tion hypotheses based on neural networks,
which allows more complex interaction
between features. A learning framework is
presented for training the non-linear mod-
els. We also discuss possible heuristics
in designing the network structure which
may improve the non-linear learning per-
formance. Experimental results show that
with the basic features of a hierarchical
phrase-based machine translation system,
our method produce translations that are
better than a linear model.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999962909090909">
One of the core problems in the research of statis-
tical machine translation is the modeling of trans-
lation hypotheses. Each modeling method defines
a score of a target sentence e = e1e2...ei...eI,
given a source sentence f = f1f2...fj...fJ, where
each ei is the ith target word and fj is the jth
source word. The well-known modeling method
starts from the Source-Channel model (Brown et
al., 1993)(Equation 1). The scoring of e decom-
poses to the calculation of a translation model and
a language model.
</bodyText>
<equation confidence="0.99917">
Pr(e|f) = Pr(e)Pr(f|e)/Pr(f) (1)
</equation>
<bodyText confidence="0.99787525">
The modeling method is extended to log-linear
models by Och and Ney (2002), as shown in Equa-
tion 2, where hm(e|f) is the mth feature function
and Am is the corresponding weight.
</bodyText>
<equation confidence="0.994934">
Pr(e|f) = pxm1 (e|f)
exp[EMm=1 Amhm(e|f)] (2)
Ee′ exp[EMm=1 Amhm(e′|f)]
</equation>
<bodyText confidence="0.9997492">
Because the normalization term in Equation 2 is
the same for all translation hypotheses of the same
source sentence, the score of each hypothesis, de-
noted by sL, is actually a linear combination of all
features, as shown in Equation 3.
</bodyText>
<equation confidence="0.996874666666667">
M
sL(e) = Amhm(e|f) (3)
m=1
</equation>
<bodyText confidence="0.999878">
The log-linear models are flexible to incorpo-
rate new features and show significant advantage
over the traditional source-channel models, thus
become the state-of-the-art modeling method and
are applied in various translation settings (Yamada
and Knight, 2001; Koehn et al., 2003; Chiang,
2005; Liu et al., 2006).
It is worth noticing that log-linear models try to
separate good and bad translation hypotheses us-
ing a linear hyper-plane. However, complex inter-
actions between features make it difficult to lin-
early separate good translation hypotheses from
bad ones (Clark et al., 2014).
Taking common features in a typical phrase-
based (Koehn et al., 2003) or hierarchical phrase-
based (Chiang, 2005) machine translation system
as an example, the language model feature favors
shorter hypotheses; the word penalty feature en-
courages longer hypotheses. The phrase trans-
lation probability feature selects phrases that oc-
curs more frequently in the training corpus, which
sometimes is long with a lower translation proba-
bility, as in translating named entities or idioms;
</bodyText>
<page confidence="0.980718">
825
</page>
<note confidence="0.978050666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 825–835,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999959203703704">
sometimes is short but with a high translation
probability, as in translating verbs or pronouns.
These three features jointly decide the choice of
translations. Simply use the weighted sum of their
values may not be the best choice for modeling
translations. As a result, log-linear models may
under-fit the data. This under-fitting may prevents
the further improvement of translation quality.
In this paper, we propose a non-linear model-
ing of translation hypotheses based on neural net-
works. The traditional features of a machine trans-
lation system are used as the input to the net-
work. By feeding input features to nodes in a hid-
den layer, complex interactions among features are
modeled, resulting in much stronger expressive
power than traditional log-linear models. (Sec-
tion 3)
Employing a neural network for SMT model-
ing has two issues to be tackled. The first is-
sue is the parameter learning. Log-linear models
rely on minimum error rate training (MERT) (Och,
2003) to achieve best performance. When the
scoring function become non-linear, the intersec-
tion points of these non-linear functions could not
be effectively calculated and enumerated. Thus
MERT is no longer suitable for learning the pa-
rameters. To solve the problem, we present a
framework for effective training including several
criteria to transform the training problem into a bi-
nary classification task, a unified objective func-
tion and an iterative training algorithm. (Sec-
tion 4)
The second issue is the structure of neural net-
work. Single layer neural networks are equivalent
to linear models; two-layer networks with suffi-
cient nodes are capable of learning any continuous
function (Bishop, 1995). Adding more layers into
the network could model complex functions with
less nodes, but also brings the problem of van-
ishing gradient (Erhan et al., 2009). We adapt a
two-layer feed-forward neural network to keep the
training process efficient. We notice that one ma-
jor problem that prevents a neural network training
reaching a good solution is that there are too many
local minimums in the parameter space. Thus we
discuss how to constrain the learning of neural net-
works with our intuitions and observations of the
features. (Section 5)
Experiments are conducted to compare vari-
ous settings and verify the effectiveness of our
proposed learning framework. Experimental re-
sults show that our framework could achieve better
translation quality even with the same traditional
features as previous linear models. (Section 6)
</bodyText>
<sectionHeader confidence="0.999699" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.99993152173913">
Many research has been attempting to bring non-
linearity into the training of SMT. These efforts
could be roughly divided into the following three
categories.
The first line of research attempted to re-
interpret original features via feature transforma-
tion or additional learning. For example, Maskey
and Zhou (2012) use a deep belief network to
learn representations of the phrase translation and
lexical translation probability features. Clark et
al. (2014) used discretization to transform real-
valued dense features into a set of binary indica-
tor features. Lu et al. (2014) learned new fea-
tures using a semi-supervised deep auto encoder.
These work focus on the explicit representation
of the features and usually employ extra learning
procedure. Our proposed method only takes the
original features, with no transformation, as the
input. Feature transformation or combination are
performed implicitly during the training of the net-
work and integrated with the optimization of trans-
lation quality.
The second line of research attempted to use
non-linear models instead of log-linear models,
which is most similar in spirit with our work. Duh
and Kirchhoff (2008) used the boosting method
to combine several results of MERT and achieved
improvement in a re-ranking setting. Liu et
al. (2013) proposed an additive neural network
which employed a two-layer neural network for
embedding-based features. To avoid local min-
imum, they still rely on a pre-training and post-
training from MERT or PRO. Comparing to these
efforts, our proposed method takes a further step
that it is integrated with iterative training, instead
of re-ranking, and works without the help of any
pre-trained linear models.
The third line of research attempted to add
non-linear features/components into the log-linear
learning framework. Neural network based mod-
els are trained as language models (Vaswani et
al., 2013; Auli and Gao, 2014), translation mod-
els (Gao et al., 2014) or joint language and transla-
tion models (Auli et al., 2013; Devlin et al., 2014).
Liu et al. (2013) also introduced word embed-
ding for source and target sides of the translation
</bodyText>
<page confidence="0.986233">
826
</page>
<figure confidence="0.783238">
output
layer
Mo
hidden
layer
Mh
input
</figure>
<figureCaption confidence="0.9921705">
Figure 1: A two-layer feed-forward neural net-
work.
</figureCaption>
<bodyText confidence="0.999844571428572">
rules as local features. In this paper, we focus on
enhancing the expressive power of the modeling,
which is independent of the research of enhanc-
ing translation systems with new designed fea-
tures. We believe additional improvement could
be achieved by incorporating more features into
our framework.
</bodyText>
<sectionHeader confidence="0.999522" genericHeader="method">
3 Non-linear Translation
</sectionHeader>
<bodyText confidence="0.999988333333333">
The non-linear modeling of translation hypothe-
ses could be used in both phrase-based system and
syntax-based systems. In this paper, we take the
hierarchical phrase based machine translation sys-
tem (Chiang, 2005) as an example and introduce
how we fit the non-linearity into the system.
</bodyText>
<subsectionHeader confidence="0.999203">
3.1 Two-layer Neural Networks
</subsectionHeader>
<bodyText confidence="0.999616916666667">
We employ a two-layer neural network as the non-
linear model for scoring translation hypotheses.
The structure of a typical two-layer feed-forward
neural network includes an input layer, a hidden
layer, and a output layer (as shown in Figure 1).
We use the input layer to accept input features,
the hidden layer to combine different input fea-
tures, the output layer with only one node to out-
put the model score for each translation hypothesis
based on the value of hidden nodes. More specifi-
cally, the score of hypothesis e, denoted as sN, is
defined as:
</bodyText>
<equation confidence="0.981925">
sN(e) _ ao(Mo·ah(Mh·hnt1 (e|f)+bh)+bo) (4)
</equation>
<bodyText confidence="0.999988333333333">
where M, b is the weight matrix, bias vector of
the neural nodes, respectively; a is the activation
function, which is often set to non-linear functions
such as the tanh function or sigmoid function; sub-
script h and o indicates the parameters of hidden
layer and output layer, respectively.
</bodyText>
<subsectionHeader confidence="0.988008">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.9997506">
We use the standard features of a typical hier-
archical phrase based translation system(Chiang,
2005). Adding new features into the framework is
left as a future direction. The features as listed as
following:
</bodyText>
<listItem confidence="0.998451952380952">
• p(α|-y) and p(-y|α): conditional probability
of translating α as -y and translating α as -y,
where α and -y is the left and right hand side
of a initial phrase or hierarchical translation
rule, respectively;
• pw(α|-y) and pw(-y|α): lexical probability of
translating words in α as words in -y and
translating words in -y as words in α;
• plnt: language model probability;
• wc: accumulated count of individual words
generated during translation;
• pc: accumulated count of initial phrases used;
• rc: accumulated count of hierarchical rule
phrases used;
• gc: accumulated count of glue rule used in
this hypothesis;
• uc: accumulated count of unknown source
word. which has no entry in the translation
table;
• nc: accumulated count of source phrases that
translate into null;
</listItem>
<subsectionHeader confidence="0.997735">
3.3 Decoding
</subsectionHeader>
<bodyText confidence="0.999981470588235">
The basic decoding algorithm could be kept al-
most the same as traditional phrase-based or
syntax-based translation systems (Yamada and
Knight, 2001; Koehn et al., 2003; Chiang, 2005;
Liu et al., 2006). For example, in the experiments
of this paper, we use a CKY style decoding algo-
rithm following Chiang (2005).
Our non-linear translation system is different
from traditional systems in the way to calculate
the score for each hypothesis. Instead of calculat-
ing the score as a linear combination, we use neu-
ral networks (Section 3.1) to perform a non-linear
combination of feature values.
We also use the cube-pruning algorithm (Chi-
ang, 2005) to keep the decoding efficient. Al-
though the non-linearity in model scores may
cause more search errors (Huang and Chiang,
</bodyText>
<page confidence="0.984281">
827
</page>
<bodyText confidence="0.9969195">
2007) finding the highest scoring hypothesis, in
practice it still achieves reasonable results.
</bodyText>
<sectionHeader confidence="0.987969" genericHeader="method">
4 Non-linear Learning Framework
</sectionHeader>
<bodyText confidence="0.999799727272727">
Traditional machine translation systems rely on
MERT to tune the weights of different features.
MERT performs efficient search by enumerating
the score function of all the hypotheses and us-
ing intersections of these linear functions to form
the ”upper-envelope” of the model score func-
tion (Och, 2003). When the scoring function is
non-linear, it is not feasible to find the intersec-
tions of these functions. In this section, we discuss
alternatives to train the parameters for non-linear
models.
</bodyText>
<subsectionHeader confidence="0.996107">
4.1 Training Criteria
</subsectionHeader>
<bodyText confidence="0.991831227272727">
The task of machine translation is a complex prob-
lem with structural output space. Decoding algo-
rithms search for the translation hypothesis with
the highest score, according to a given scoring
function, from an exponentially large set of candi-
date hypotheses. The purpose of training is to se-
lect the scoring function, so that the function score
the hypotheses ”correctly”. The correctness is of-
ten introduced by some extrinsic metrics, such as
BLEU (Papineni et al., 2002).
We denote the scoring function as s(f, e;
⃗
simply s, which is parameterized by 0; denote the
set of all translation hypotheses as C; denote the
extrinsic metric as eval(·) 1. Note that, in linear
cases, s is a linear function as in Equation 3, while
in the non-linear case described in this paper, s is
the scoring function in Equation 4.
Ideally, the training objective is to select a scor-
ing function s, from all functions S, that scores the
correct translation (or references) 6, higher than
any other hypotheses (Equation 5).
</bodyText>
<equation confidence="0.995734">
s� = {s ∈ S|s(6) &gt; s(e) ∀e ∈ C} (5)
</equation>
<bodyText confidence="0.991901048780488">
In practice, the candidate set C is exponentially
large and hard to enumerate; the correct translation
e� may not even exist in the current search space for
various reasons, e.g. unknown source word. As a
result, we use the n-best set Cnbest to approximate
C, use the extrinsic metric eval(·) to evaluate the
quality of hypotheses in Cnbest and use the fol-
lowing three alternatives as approximations to the
ideal objective.
1In our experiments, we use sentence level BLEU with +1
smoothing as the evaluation metric.
Best v.s. Rest (BR) To score the best hypothesis
in the n-best set e� higher than the rest hy-
potheses. This objective is very similar to
MERT in that it tries to optimize the score
of e� and doesn’t concern about the ranking of
rest hypotheses. In this case, e� is an approxi-
mation of 6.
Best v.s. Worst (BW) To score the best hypoth-
esis higher than the worst hypothesis in the
n-best set. This objective is motivated by the
practice of separating the ”hope” and ”fear”
translation hypotheses (Chiang, 2012). We
take a simpler strategy which uses the best
and worst hypothesis in Cnbest as the ”hope”
and ”fear” hypothesis, respectively, in order
to avoid multi-pass decoding.
Pairwise (PW) To score the better hypothesis in
sampled hypothesis pairs higher than the
worse one in the same pair. This objective
is adapted from the Pairwise Ranking Opti-
mization (PRO) (Hopkins and May, 2011),
which tries to ranking all the hypotheses in-
stead of selecting the best one. We use the
same sampling strategy as their original pa-
per.
Note that each of the above criteria transforms
the original problem of selecting best hypothe-
ses from an exponential space to a certain pair-
wise comparison problem, which could be easily
trained using binary classifiers.
</bodyText>
<subsectionHeader confidence="0.99715">
4.2 Training Objective
</subsectionHeader>
<bodyText confidence="0.999956285714286">
For the binary classification task, we use a hinge
loss following Watanabe (2012). Because the net-
work has a lot of parameters compared with the
linear model, we use a L1 norm instead of L2
norm as the regularization term, to favor sparse so-
lutions. We define our training objective function
in Equation 6.
</bodyText>
<equation confidence="0.873857833333333">
arg min
0
+ a · ||0||1
with
S(·) = max{s(f, e1; 0) − s(f, e2; 0) + 1, 0}
(6)
</equation>
<bodyText confidence="0.989033">
where D is the given training data; (e1, e2) is a
training hypothesis-pair, with e1 to be the one with
</bodyText>
<figure confidence="0.65405175">
⃗0), or
1 � E S(f, e1, e2; 0)
N (e1^)ET(f)
fED
</figure>
<page confidence="0.984485">
828
</page>
<bodyText confidence="0.998809363636364">
higher eval(·) score; N is the total number of
hypothesis-pairs in D; T (f), or simply T, is the
set of hypothesis-pairs for each source sentence f.
The set T is decided by the criterion used for
training. For the BR setting, the best hypothesis is
paired with every other hypothesis in the n-best list
(Equation 7); while for the BW setting, it is only
paired with the worst hypothesis (Equation 8). The
generation of T in PW setting is the same with
PRO sampling, we refer the readers to the original
paper of Hopkins and May (2011).
</bodyText>
<table confidence="0.524833666666667">
TBR = I(e1,e2)|e1 =arg max
eECnbest
e2 E Cnbest and e1 =� e2}
Algorithm 1 Iterative Training Algorithm
Input: the set of training sentences D, max num-
ber of iteration I
</table>
<listItem confidence="0.975385888888889">
1: 00 +– RandomInit(),
2: for i = 0 to I do
3: Ti +– 0;
4: for each f E D do
5: Cnbest +– NbestDecode(f ; 0i)
6: T +– GeneratePair(Cnbest)
7: Ti +– Ti U T
8: end for
9: Tall +– WeightedCombine(Ui�1
</listItem>
<figure confidence="0.594860363636364">
k=0Tk, Ti)
10: 0i+1 �Optimize(Tall, 0i)
11: end for
eval(e),
(7)
TBW = I(e1,e2)|e1
= arg max
eECnbest
eval(e),
e2 = arg min eval(e)}
eECnbest (8)
</figure>
<subsectionHeader confidence="0.98563">
4.3 Training Procedure
</subsectionHeader>
<bodyText confidence="0.999828428571429">
In standard training algorithm for classification,
the training instances stays the same in each itera-
tion. In machine translation, decoding algorithms
usually return a very different n-best set with dif-
ferent parameters. This is due to the exponentially
large size of search space. MERT and PRO extend
the current n-best set by merging the n-best set
of all previous iterations into a pool (Och, 2003;
Hopkins and May, 2011). In this way, the enlarged
n-best set may give a better approximation of the
true hypothesis set C and may lead to better and
more stable training results.
We argue that the training should still focus on
hypotheses obtained in current round, because in
each iteration the searching for the n-best set is in-
dependent of previous iterations. To compromise
the above two goals, in our practice, training hy-
pothesis pairs are first generated from the current
n-best set, then merged with the pairs generated
from all previous iterations. In order to make the
model focus more on pairs from current iteration,
we assign pairs in previous iterations a small con-
stant weight and assign pairs in current iteration a
relatively large constant weight 2. This is inspired
by the AdaBoost algorithm (Schapire, 1999) in
weighting instances.
Following the spirit of MERT, we propose a
iterative training procedure (Algorithm 1). The
</bodyText>
<footnote confidence="0.8730115">
2In our experiments, we empirically set the constants to
be 0.1 and 0.9, respectively.
</footnote>
<bodyText confidence="0.999912">
training starts by randomly initialized model pa-
rameters 00 (line 1). In ith iteration, the decod-
ing algorithm decodes each sentence f to get the
n-best set Cnbest (line 5). Training hypothesis
pairs T are extracted from Cnbest according to the
training criterion described in Section 4.2 (line 6).
Newly collected pairs Ti are combined with pairs
from previous iterations before used for training
(line 9). 0i+1 is obtained by solving Equation 6
using the Conjugate Sub-Gradient method (Le et
al., 2011) (line 10).
</bodyText>
<sectionHeader confidence="0.888066" genericHeader="method">
5 Structure of the Network
</sectionHeader>
<bodyText confidence="0.99991135">
Although neural networks bring strong expressive
power to the modeling of translation hypothesis,
training a neural network is prone to resulting in
local minimum which may affect the training re-
sults. We speculate that one reason for these local
minimums is that the structure of a well-connected
network has too many parameters. Take a neu-
ral network with k nodes in the input layer and m
nodes in the hidden layer as an example. Every
node in the hidden layer is connected to each of
the k input nodes. This simple structure resulting
in at least k x m parameters.
In Section 4.2, we use L1 norm in the objec-
tive function in order to get sparser solutions. In
this section, we propose some constrained network
structures according to our prior knowledge of the
features. These structures have much less param-
eters or simpler structures comparing to original
neural networks, thus reduce the possibility of get-
ting stuck in local minimums.
</bodyText>
<page confidence="0.992663">
829
</page>
<subsectionHeader confidence="0.945275">
5.1 Network with two-degree Hidden Layer
</subsectionHeader>
<bodyText confidence="0.999933028571429">
We find the first pitfall of the standard two-layer
neural network is that each node in the hidden
layer receives input from every input layer node.
Features used in SMT are usually manually de-
signed, which has their concrete meanings. For a
network of several hidden nodes, combining every
features into every hidden node may be redundant
and not necessary to represent the quality of a hy-
pothesis.
As a result, we take a harsh step and constrain
the nodes in hidden layer to have a in-degree of
two, which means each hidden node only accepts
inputs from two input nodes. We do not use any
other prior knowledge about features in this set-
ting. So for a network with k nodes in the in-
put layer, the hidden layer should contain C2k =
k(k − 1)/2 nodes to accept all combinations from
the input layer. We name this network structure as
Two-Degree Hidden Layer Network (TDN).
It is easy to see that a TDN has C2k x 2 =
k(k − 1) parameters for the hidden layer because
of the constrained degree. This is one order of
magnitude less than a standard two-layer network
with the same number of hidden nodes, which has
C2k x k = k2(k − 1)/2 parameters.
Note that we perform a 2-degree combination
that looks similar in spirit with those combina-
tion of atomic features in large scale discrimina-
tive learning for other NLP tasks, such as POS tag-
ging and parsing. However, unlike the practice in
these tasks that directly combines values of differ-
ent features to generate a new feature type, we first
linearly combine the value of these features and
perform non-linear transformation on these values
via an activation function.
</bodyText>
<subsectionHeader confidence="0.998377">
5.2 Network with Grouped Features
</subsectionHeader>
<bodyText confidence="0.999962628571428">
It might be a too strong constraint to require the
hidden node have in-degree of 2. In order to re-
lax this constraint, we need more prior knowledge
from the features.
Our first observation is that there are different
types of features. These types are different from
each other in terms of value ranges, sources, im-
portance, etc. For example, language model fea-
tures usually take a very small value of probability,
and word count feature takes a integer value and
usually has a much higher weight in linear case
than other count features.
The second observation is that features of the
same type may not have complex interaction with
each other. For example, it is reasonable to com-
bine language model features with word count fea-
tures in a hidden node. But it may not be neces-
sary to combine the count of initial phrases and the
count of unknown words into a hidden node.
Based on the above two intuitions, we design
a new structure of network that has the following
constraints: given a disjoint partition of features:
G1, G2,..., Gk, every hidden node takes input from
a set of input nodes, where any two nodes in this
set come from two different feature groups. Un-
der this constraint, the in-degree of a hidden node
is at most k. We name this network structure as
Grouped Network (GN).
In practice, we divide the basic features in Sec-
tion 3.2 into five groups: language model features,
translation probability features, lexical probability
features, the word count feature, and the rest of
count features. This division considers not only
the value ranges, but also types of features and the
possibility of them interact with each other.
</bodyText>
<sectionHeader confidence="0.997637" genericHeader="evaluation">
6 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.987075">
6.1 General Settings
</subsectionHeader>
<bodyText confidence="0.999831333333333">
We conduct experiments on a large scale machine
translation tasks. The parallel data comes from
LDC, including LDC2002E18, LDC2003E14,
LDC2004E12, LDC2004T08, LDC2005T10,
LDC2007T09, which consists of 8.2 million
of sentence pairs. Monolingual data includes
Xinhua portion of Gigaword corpus. We use
multi-references data MT03 as training data,
MT02 as development data, and MT04, MT05
as test data. These data are mainly in the same
genre, avoiding the extra consideration of domain
adaptation.
</bodyText>
<table confidence="0.998865428571429">
Data Usage Sents.
LDC TM train 8,260,093
Gigaword LM train 14,684,074
MT03 train 919
MT02 dev 878
MT04 test 1,789
MT05 test 1,083
</table>
<tableCaption confidence="0.999959">
Table 1: Experimental data and statistics.
</tableCaption>
<bodyText confidence="0.9911475">
The Chinese side of the corpora is word seg-
mented using ICTCLAS3. Our translation sys-
</bodyText>
<footnote confidence="0.955173">
3http://ictclas.nlpir.org/
</footnote>
<page confidence="0.987145">
830
</page>
<table confidence="0.9997482">
Criteria MT03(train) MT02(dev) MT04 MT05
BR, 35.02 36.63 34.96 34.15
BR 38.66 40.04 38.73 37.50
BW 39.55 39.36 38.72 37.81
PW 38.61 38.85 38.73 37.98
</table>
<tableCaption confidence="0.988033">
Table 2: BLEU4 in percentage on different training criteria (”BR”, ”BW” and ”PW” refer to experiments
</tableCaption>
<bodyText confidence="0.98179775">
with ”Best v.s. Rest”, ”Best v.s. Worst” and ”Pairwise” training criteria, respectively. ”BR,” indicates
generate hypothesis pairs from n-best set of current iteration only presented in Section 4.3.
tem is an in-house implementation of the hier-
archical phrase-based translation system(Chiang,
2005). We set the beam size to 20. We train a
5-gram language model on the monolingual data
with MKN smoothing(Chen and Goodman, 1998).
For each parameter tuning experiments, we ran the
same training procedure 3 times and present the
average results. The translation quality is evalu-
ated use 4-gram case-insensitive BLEU (Papineni
et al., 2002). Significant test is performed using
bootstrap re-sampling implemented by Clark et
al. (2011). We employ a two-layer neural network
with 11 input layer nodes, corresponding to fea-
tures listed in Section 3.2 and 1 output layer node.
The number of nodes in the hidden layer varies in
different settings. The sigmoid function is used as
the activation function for each node in the hidden
layer. For the output layer we use a linear activa-
tion function. We try different A for the Li norm
from 0.01 to 0.00001 and use the one with best
performance on the development set. We solve the
optimization problem with ALGLIB package4.
</bodyText>
<subsectionHeader confidence="0.998862">
6.2 Experiments of Training Criteria
</subsectionHeader>
<bodyText confidence="0.99993375">
This set experiments evaluates different training
criteria discussed in Section 4.1. We generate
hypothesis-pair according to BW, BR and PW cri-
teria, respectively, and perform training with these
pairs. In the PW criterion, we use the sampling
method of PRO (Hopkins and May, 2011) and get
the 50 hypothesis pairs for each sentence. We use
20 hidden nodes for all three settings to make a
fair comparison.
The results are presented in Table 2. The
first two rows compare training with and with-
out the weighted combination of hypothesis pairs
we discussed in Section 4.3. As the result sug-
gested, with the weighted combination of hypothe-
sis pairs from previous iterations, the performance
improves significantly on both test sets.
</bodyText>
<footnote confidence="0.887593">
4http://www.alglib.net/
</footnote>
<bodyText confidence="0.953757333333333">
Although the system performance on the dev
set varies, the performance on test sets are al-
most comparable. This suggest that although the
three training criteria are based on different as-
sumptions, their are basically equivalent for train-
ing translation systems.
</bodyText>
<table confidence="0.89408525">
Criteria Pairs/iteration Accuracy(%)
BR 19 70.7
BW 1 79.5
PW 100 67.3
</table>
<tableCaption confidence="0.994259">
Table 3: Comparison of different training criteria
</tableCaption>
<bodyText confidence="0.972394772727273">
in number of new instances per iteration and train-
ing accuracy.
We also compares the three training criteria in
their number of new instances per iteration and
final training accuracy (Table 3). Compared to
BR which tries to separate the best hypothesis
from the rest hypotheses in the n-best set, and PW
which tries to obtain a correct ranking of all hy-
potheses, BW only aims at separating the best and
worst hypothesis of each iteration, which is a eas-
ier task for learning a classifiers. It requires the
least training instances and achieves the best per-
formance in training. Note that, the accuracy for
each system in Table 3 are the accuracy each sys-
tem achieves after training stops. They are not cal-
culated on the same set of instances, thus not di-
rectly comparable. We use the differences in accu-
racy as an indicator for the difficulties of the cor-
responding learning task.
For the rest of this paper, we use the BW crite-
rion because it is much simpler compared to sam-
pling method of PRO (Hopkins and May, 2011).
</bodyText>
<subsectionHeader confidence="0.999688">
6.3 Experiments of Network Structures
</subsectionHeader>
<bodyText confidence="0.99270175">
We make several comparisons of the network
structures and compare them with a baseline hi-
erarchical phrase-based translation system (HPB).
Table 4 shows the translation performance of
</bodyText>
<page confidence="0.993592">
831
</page>
<table confidence="0.999836">
Systems MT03(train) MT02(dev) MT04 MT05 Test Average
HPB 39.25 39.07 38.81 38.01 38.41
TLayer20 39.55* 39.36* 38.72 37.81 38.27(-0.14)
TLayer30 39.70+ 39.71* 38.89 37.90 38.40(-0.01)
TLayer50 39.26 38.97 38.72 38.79+ 38.76(+0.35)
TLayer100 39.42 38.77 38.65 38.65+ 38.69(+0.28)
TLayer200 39.69 38.68 38.72 38.80+ 38.74(+0.32)
TDN 39.60+ 38.94 38.99* 38.13 38.56(+0.15)
GN 39.73+ 39.41+ 39.45+ 38.51+ 38.98(+0.57)
</table>
<tableCaption confidence="0.9986092">
Table 4: BLEU4 in percentage for comparing of systems using different network structures (HPB refers
to the baseline hierarchical phrase-based system. TLayer, TDN, GN refer to the standard 2-layer network,
Two-Degree Hidden Layer Network, Grouped Network, respectively. Subscript of TLayer indicates the
number of nodes in the hidden layer.) +, * marks results that are significant better than the baseline
system with p &lt; 0.01 and p &lt; 0.05.
</tableCaption>
<table confidence="0.999890666666667">
Systems # Hidden Nodes # Parameters Training Time per iter.(s)
HPB - 11 1041
TLayer20 20 261 671
TLayer30 30 391 729
TLayer50 50 651 952
TLayer100 100 1,301 1,256
TLayer200 200 2,601 2,065
TDN 55 221 808
GN 214 1,111 1,440
</table>
<tableCaption confidence="0.695674">
Table 5: Comparison of network scales and training time of different systems, including the number of
nodes in the hidden layer, the number of parameters, the average training time per iteration (15 iterations).
The notations of systems are the same as in Table4.
</tableCaption>
<bodyText confidence="0.999798333333333">
different systems5. All 5 two-layer feed forward
neural networks models could achieve compara-
ble or better performance comparing to the base-
line system. We can see that training a larger net-
work may lead to better translation quality (from
TLayer20 and TLayer30 to TLayer50). However,
increasing the number of hidden node to 100 and
200 does not bring further improvement. One pos-
sible reason is that training a larger network with
arbitrary connections brings in too many param-
eters which may be difficult to train with limited
training data.
TDN and GN are the two network structures
proposed in Section 5. With the constraint that
all input to the hidden node should be of degree
2, TDN performs comparable to the baseline sys-
tem. With the grouped feature, we could design
networks such as GN, which shows significant im-
provement over the baseline systems (+0.57) and
achieves the best performance among all neural
systems.
</bodyText>
<footnote confidence="0.789202">
5TLayer20 is the same system as BW in Table 2
</footnote>
<bodyText confidence="0.99217025">
Table 4 shows statistics related to the efficiency
issue of different systems. The baseline system
(HPB) uses MERT for training. HPB has a very
small number of parameters and searches for the
best parameters exhaustively in each iteration. The
non-linear systems with few nodes (TLayer20 and
TLayer30) train faster than HPB in each iteration
because they perform back-propagation instead of
exhaustive search. We iterate 15 iterations for each
non-linear system, while MERT takes about 10
rounds to reach its best performance.
When the number of nodes in the hidden layer
increases (from 20 to 200), the number of param-
eters in the system also increases, which requires
longer time to compute the score for each hypoth-
esis and to update the parameters through back-
propagation. The network with 200 hidden nodes
takes about twice the time to train for each itera-
tion, compared to the linear system6.
TDN and GN have larger numbers of hidden
</bodyText>
<footnote confidence="0.9924405">
6Matrix operation is CPU intensive. The cost will in-
crease when multiple tasks are running.
</footnote>
<page confidence="0.993759">
832
</page>
<bodyText confidence="0.999948636363636">
nodes. However, because of our intuitions in de-
signing the structure of the networks, the degree
of the hidden node is constrained. So these two
networks are sparser in parameters and take sig-
nificant less training time than standard neural net-
works. For example, GN has a comparable num-
ber of hidden nodes with TLayer200, but only has
half of its parameters and takes about 70% time to
train in each iteration. In other words, our pro-
posed network structure provides more efficient
training in these cases and achieve better results.
</bodyText>
<sectionHeader confidence="0.998866" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999775">
In this paper, we discuss a non-linear framework
for modeling translation hypothesis for statisti-
cal machine translation system. We also present
a learning framework including training criterion
and algorithms to integrate our modeling into a
state of the art hierarchical phrase based machine
translation system. Compared to previous effort
in bringing in non-linearity into machine transla-
tion, our method uses a single two-layer neural
networks and performs training independent with
any previous linear training methods (e.g. MERT).
Our method also trains its parameters without any
pre-training or post-training procedure. Experi-
ment shows that our method could improve the
baseline system even with the same feature as
input, in a large scale Chinese-English machine
translation task.
In training neural networks with hidden nodes,
we use heuristics to reduce the complexity of net-
work structures and obtain extra advantages over
standard networks. It shows that heuristics and in-
tuitions of the data and features are still important
to a machine translation system.
Neural networks are able to perform feature
learning by using hidden nodes to model the in-
teraction among a large vector of raw features,
as in image and speech processing (Krizhevsky et
al., 2012; Hinton et al., 2012). We are trying to
model the interaction between hand-crafted fea-
tures, which is indeed similar in spirit with learn-
ing features from raw features. Although our fea-
tures already have concrete meaning, e.g. the
probability of translation, the fluency of target sen-
tence, etc. Combining these features may have ex-
tra advantage in modeling the translation process.
As future work, it is necessary to integrate more
features into our learning framework. It is also in-
teresting to see how the non-linear modeling fits
in to more complex learning tasks which involves
domain specific learning techniques.
</bodyText>
<sectionHeader confidence="0.997713" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999746428571428">
The authors would like to thank Yue Zhang
and the anonymous reviewers for their valu-
able comments. This work is supported by the
National Natural Science Foundation of China
(No. 61300158, 61223003), the Jiangsu Provin-
cial Research Foundation for Basic Research (No.
BK20130580).
</bodyText>
<sectionHeader confidence="0.998682" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998819684210527">
Michael Auli and Jianfeng Gao. 2014. Decoder in-
tegration and expected BLEU training for recurrent
neural network language models. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics, ACL 2014, June 22-27,
2014, Baltimore, MD, USA, Volume 2: Short Papers,
pages 136–142.
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint language and translation
modeling with recurrent neural networks. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2013,
18-21 October 2013, Grand Hyatt Seattle, Seattle,
Washington, USA, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 1044–1054.
Christopher M. Bishop. 1995. Neural Networks for
Pattern Recognition. Oxford University Press, Inc.,
New York, NY, USA.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathe-
matic of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263–
311.
S. F. Chen and J. T. Goodman. 1998. An empirical
study of smoothing techniques for language mod-
eling. Technical report, Computer Science Group,
Harvard University, Technical Report TR-10-98.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In annual
meeting of the Association for Computational Lin-
guistics.
David Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. J. Mach.
Learn. Res., 13(1):1159–1187, April.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for opti-
mizer instability. In Proceedings of the 49th Annual
</reference>
<page confidence="0.995723">
833
</page>
<reference confidence="0.998853008928572">
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: Short Pa-
pers - Volume 2, HLT ’11, pages 176–181, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Jonathan Clark, Chris Dyer, and Alon Lavie. 2014.
Locally non-linear learning for statistical machine
translation via discretization and structured regular-
ization. Transactions of the Association for Compu-
tational Linguistics, 2:393–404.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard M. Schwartz, and John Makhoul.
2014. Fast and robust neural network joint models
for statistical machine translation. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics, ACL 2014, June 22-27,
2014, Baltimore, MD, USA, Volume 1: Long Papers,
pages 1370–1380.
Kevin Duh and Katrin Kirchhoff. 2008. Beyond log-
linear models: Boosted minimum error rate train-
ing for n-best re-ranking. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics on Human Language Technolo-
gies: Short Papers, HLT-Short ’08, pages 37–40,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Dumitru Erhan, Pierre antoine Manzagol, Yoshua Ben-
gio, Samy Bengio, and Pascal Vincent. 2009. The
difficulty of training deep architectures and the ef-
fect of unsupervised pre-training. In David V.
Dyk and Max Welling, editors, Proceedings of the
Twelfth International Conference on Artificial In-
telligence and Statistics (AISTATS-09), volume 5,
pages 153–160. Journal of Machine Learning Re-
search - Proceedings Track.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2014. Learning continuous phrase repre-
sentations for translation modeling. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics, ACL 2014, June 22-27,
2014, Baltimore, MD, USA, Volume 1: Long Papers,
pages 699–709.
Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl,
Abdel-rahman Mohamed, Navdeep Jaitly, Andrew
Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N
Sainath, et al. 2012. Deep neural networks for
acoustic modeling in speech recognition: The shared
views of four research groups. Signal Processing
Magazine, IEEE, 29(6):82–97.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’11, pages 1352–1362, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
144–151, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hin-
ton. 2012. Imagenet classification with deep con-
volutional neural networks. In F. Pereira, C.J.C.
Burges, L. Bottou, and K.Q. Weinberger, editors,
Advances in Neural Information Processing Systems
25, pages 1097–1105. Curran Associates, Inc.
Quoc V. Le, Jiquan Ngiam, Adam Coates, Ahbik
Lahiri, Bobby Prochnow, and Andrew Y. Ng. 2011.
On optimization methods for deep learning. In Pro-
ceedings of the 28th International Conference on
Machine Learning, ICML 2011, Bellevue, Washing-
ton, USA, June 28 - July 2, 2011, pages 265–272.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the 44th Annual Meet-
ing of the Association of Computational Linguistics.
The Association for Computer Linguistics.
Lemao Liu, Taro Watanabe, Eiichiro Sumita, and
Tiejun Zhao. 2013. Additive neural networks for
statistical machine translation. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics, ACL 2013, 4-9 August 2013,
Sofia, Bulgaria, Volume 1: Long Papers, pages 791–
801.
Shixiang Lu, Zhenbiao Chen, and Bo Xu. 2014.
Learning new semi-supervised deep auto-encoder
features for statistical machine translation. In Pro-
ceedings of the 52nd Annual Meeting of the Asso-
ciation for Computational Linguistics (Volume 1:
Long Papers), pages 122–132, Baltimore, Maryland,
June. Association for Computational Linguistics.
Sameer Maskey and Bowen Zhou. 2012. Unsuper-
vised deep belief features for speech translation. In
INTERSPEECH 2012, 13th Annual Conference of
the International Speech Communication Associa-
tion, Portland, Oregon, USA, September 9-13, 2012.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. pages 295–302.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ’03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 160–
167, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL ’02: Proceed-
ings of the 40th Annual Meeting on Association for
</reference>
<page confidence="0.987027">
834
</page>
<reference confidence="0.999606821428571">
Computational Linguistics, pages 311–318, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Robert E. Schapire. 1999. A brief introduction to
boosting. In Proceedings of the 16th International
Joint Conference on Artificial Intelligence - Volume
2, IJCAI’99, pages 1401–1406, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and
David Chiang. 2013. Decoding with large-scale
neural language models improves translation. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2013, 18-21 October 2013, Grand Hyatt Seattle,
Seattle, Washington, USA, A meeting of SIGDAT,
a Special Interest Group of the ACL, pages 1387–
1392.
Taro Watanabe. 2012. Optimized online rank learning
for machine translation. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, NAACL HLT ’12, pages
253–262, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of the 39th Annual Meeting of the Association of
Computational Linguistics, pages 523–530.
</reference>
<page confidence="0.99886">
835
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.354717">
<title confidence="0.999959">Non-linear Learning for Statistical Machine Translation</title>
<author confidence="0.90735">Shujian Huang</author>
<author confidence="0.90735">Huadong Chen</author>
<author confidence="0.90735">Xinyu Dai</author>
<author confidence="0.90735">Jiajun</author>
<affiliation confidence="0.732308">State Key Laboratory for Novel Software</affiliation>
<address confidence="0.709196">Nanjing Nanjing 210023,</address>
<email confidence="0.964265">chenhd,daixy,</email>
<abstract confidence="0.9998206">Modern statistical machine translation (SMT) systems usually use a linear combination of features to model the quality of each translation hypothesis. The linear combination assumes that all the features are in a linear relationship and constrains that each feature interacts with the rest features in an linear manner, which might limit the expressive power of the model and lead to a under-fit model on the current data. In this paper, we propose a nonlinear modeling for the quality of translation hypotheses based on neural networks, which allows more complex interaction between features. A learning framework is presented for training the non-linear models. We also discuss possible heuristics in designing the network structure which may improve the non-linear learning performance. Experimental results show that with the basic features of a hierarchical phrase-based machine translation system, our method produce translations that are better than a linear model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Jianfeng Gao</author>
</authors>
<title>Decoder integration and expected BLEU training for recurrent neural network language models.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL</booktitle>
<volume>Volume</volume>
<pages>136--142</pages>
<location>Baltimore, MD, USA,</location>
<contexts>
<context position="8055" citStr="Auli and Gao, 2014" startWordPosition="1255" endWordPosition="1258">) proposed an additive neural network which employed a two-layer neural network for embedding-based features. To avoid local minimum, they still rely on a pre-training and posttraining from MERT or PRO. Comparing to these efforts, our proposed method takes a further step that it is integrated with iterative training, instead of re-ranking, and works without the help of any pre-trained linear models. The third line of research attempted to add non-linear features/components into the log-linear learning framework. Neural network based models are trained as language models (Vaswani et al., 2013; Auli and Gao, 2014), translation models (Gao et al., 2014) or joint language and translation models (Auli et al., 2013; Devlin et al., 2014). Liu et al. (2013) also introduced word embedding for source and target sides of the translation 826 output layer Mo hidden layer Mh input Figure 1: A two-layer feed-forward neural network. rules as local features. In this paper, we focus on enhancing the expressive power of the modeling, which is independent of the research of enhancing translation systems with new designed features. We believe additional improvement could be achieved by incorporating more features into ou</context>
</contexts>
<marker>Auli, Gao, 2014</marker>
<rawString>Michael Auli and Jianfeng Gao. 2014. Decoder integration and expected BLEU training for recurrent neural network language models. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014, June 22-27, 2014, Baltimore, MD, USA, Volume 2: Short Papers, pages 136–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Michel Galley</author>
<author>Chris Quirk</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Joint language and translation modeling with recurrent neural networks.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP</booktitle>
<pages>18--21</pages>
<location>Grand Hyatt Seattle, Seattle, Washington, USA,</location>
<contexts>
<context position="8154" citStr="Auli et al., 2013" startWordPosition="1273" endWordPosition="1276">eatures. To avoid local minimum, they still rely on a pre-training and posttraining from MERT or PRO. Comparing to these efforts, our proposed method takes a further step that it is integrated with iterative training, instead of re-ranking, and works without the help of any pre-trained linear models. The third line of research attempted to add non-linear features/components into the log-linear learning framework. Neural network based models are trained as language models (Vaswani et al., 2013; Auli and Gao, 2014), translation models (Gao et al., 2014) or joint language and translation models (Auli et al., 2013; Devlin et al., 2014). Liu et al. (2013) also introduced word embedding for source and target sides of the translation 826 output layer Mo hidden layer Mh input Figure 1: A two-layer feed-forward neural network. rules as local features. In this paper, we focus on enhancing the expressive power of the modeling, which is independent of the research of enhancing translation systems with new designed features. We believe additional improvement could be achieved by incorporating more features into our framework. 3 Non-linear Translation The non-linear modeling of translation hypotheses could be us</context>
</contexts>
<marker>Auli, Galley, Quirk, Zweig, 2013</marker>
<rawString>Michael Auli, Michel Galley, Chris Quirk, and Geoffrey Zweig. 2013. Joint language and translation modeling with recurrent neural networks. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1044–1054.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<title>Neural Networks for Pattern Recognition.</title>
<date>1995</date>
<publisher>Oxford University Press, Inc.,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5306" citStr="Bishop, 1995" startWordPosition="826" endWordPosition="827">on points of these non-linear functions could not be effectively calculated and enumerated. Thus MERT is no longer suitable for learning the parameters. To solve the problem, we present a framework for effective training including several criteria to transform the training problem into a binary classification task, a unified objective function and an iterative training algorithm. (Section 4) The second issue is the structure of neural network. Single layer neural networks are equivalent to linear models; two-layer networks with sufficient nodes are capable of learning any continuous function (Bishop, 1995). Adding more layers into the network could model complex functions with less nodes, but also brings the problem of vanishing gradient (Erhan et al., 2009). We adapt a two-layer feed-forward neural network to keep the training process efficient. We notice that one major problem that prevents a neural network training reaching a good solution is that there are too many local minimums in the parameter space. Thus we discuss how to constrain the learning of neural networks with our intuitions and observations of the features. (Section 5) Experiments are conducted to compare various settings and v</context>
</contexts>
<marker>Bishop, 1995</marker>
<rawString>Christopher M. Bishop. 1995. Neural Networks for Pattern Recognition. Oxford University Press, Inc., New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematic of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>311</pages>
<contexts>
<context position="1645" citStr="Brown et al., 1993" startWordPosition="250" endWordPosition="253">r learning performance. Experimental results show that with the basic features of a hierarchical phrase-based machine translation system, our method produce translations that are better than a linear model. 1 Introduction One of the core problems in the research of statistical machine translation is the modeling of translation hypotheses. Each modeling method defines a score of a target sentence e = e1e2...ei...eI, given a source sentence f = f1f2...fj...fJ, where each ei is the ith target word and fj is the jth source word. The well-known modeling method starts from the Source-Channel model (Brown et al., 1993)(Equation 1). The scoring of e decomposes to the calculation of a translation model and a language model. Pr(e|f) = Pr(e)Pr(f|e)/Pr(f) (1) The modeling method is extended to log-linear models by Och and Ney (2002), as shown in Equation 2, where hm(e|f) is the mth feature function and Am is the corresponding weight. Pr(e|f) = pxm1 (e|f) exp[EMm=1 Amhm(e|f)] (2) Ee′ exp[EMm=1 Amhm(e′|f)] Because the normalization term in Equation 2 is the same for all translation hypotheses of the same source sentence, the score of each hypothesis, denoted by sL, is actually a linear combination of all features,</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematic of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263– 311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>J T Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical report,</tech>
<institution>Computer Science Group, Harvard University,</institution>
<contexts>
<context position="24535" citStr="Chen and Goodman, 1998" startWordPosition="4046" endWordPosition="4049"> 34.96 34.15 BR 38.66 40.04 38.73 37.50 BW 39.55 39.36 38.72 37.81 PW 38.61 38.85 38.73 37.98 Table 2: BLEU4 in percentage on different training criteria (”BR”, ”BW” and ”PW” refer to experiments with ”Best v.s. Rest”, ”Best v.s. Worst” and ”Pairwise” training criteria, respectively. ”BR,” indicates generate hypothesis pairs from n-best set of current iteration only presented in Section 4.3. tem is an in-house implementation of the hierarchical phrase-based translation system(Chiang, 2005). We set the beam size to 20. We train a 5-gram language model on the monolingual data with MKN smoothing(Chen and Goodman, 1998). For each parameter tuning experiments, we ran the same training procedure 3 times and present the average results. The translation quality is evaluated use 4-gram case-insensitive BLEU (Papineni et al., 2002). Significant test is performed using bootstrap re-sampling implemented by Clark et al. (2011). We employ a two-layer neural network with 11 input layer nodes, corresponding to features listed in Section 3.2 and 1 output layer node. The number of nodes in the hidden layer varies in different settings. The sigmoid function is used as the activation function for each node in the hidden lay</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>S. F. Chen and J. T. Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical report, Computer Science Group, Harvard University, Technical Report TR-10-98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In annual meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2591" citStr="Chiang, 2005" startWordPosition="405" endWordPosition="406">|f) exp[EMm=1 Amhm(e|f)] (2) Ee′ exp[EMm=1 Amhm(e′|f)] Because the normalization term in Equation 2 is the same for all translation hypotheses of the same source sentence, the score of each hypothesis, denoted by sL, is actually a linear combination of all features, as shown in Equation 3. M sL(e) = Amhm(e|f) (3) m=1 The log-linear models are flexible to incorporate new features and show significant advantage over the traditional source-channel models, thus become the state-of-the-art modeling method and are applied in various translation settings (Yamada and Knight, 2001; Koehn et al., 2003; Chiang, 2005; Liu et al., 2006). It is worth noticing that log-linear models try to separate good and bad translation hypotheses using a linear hyper-plane. However, complex interactions between features make it difficult to linearly separate good translation hypotheses from bad ones (Clark et al., 2014). Taking common features in a typical phrasebased (Koehn et al., 2003) or hierarchical phrasebased (Chiang, 2005) machine translation system as an example, the language model feature favors shorter hypotheses; the word penalty feature encourages longer hypotheses. The phrase translation probability feature</context>
<context position="8905" citStr="Chiang, 2005" startWordPosition="1394" endWordPosition="1395">o hidden layer Mh input Figure 1: A two-layer feed-forward neural network. rules as local features. In this paper, we focus on enhancing the expressive power of the modeling, which is independent of the research of enhancing translation systems with new designed features. We believe additional improvement could be achieved by incorporating more features into our framework. 3 Non-linear Translation The non-linear modeling of translation hypotheses could be used in both phrase-based system and syntax-based systems. In this paper, we take the hierarchical phrase based machine translation system (Chiang, 2005) as an example and introduce how we fit the non-linearity into the system. 3.1 Two-layer Neural Networks We employ a two-layer neural network as the nonlinear model for scoring translation hypotheses. The structure of a typical two-layer feed-forward neural network includes an input layer, a hidden layer, and a output layer (as shown in Figure 1). We use the input layer to accept input features, the hidden layer to combine different input features, the output layer with only one node to output the model score for each translation hypothesis based on the value of hidden nodes. More specifically</context>
<context position="11094" citStr="Chiang, 2005" startWordPosition="1756" endWordPosition="1757">probability; • wc: accumulated count of individual words generated during translation; • pc: accumulated count of initial phrases used; • rc: accumulated count of hierarchical rule phrases used; • gc: accumulated count of glue rule used in this hypothesis; • uc: accumulated count of unknown source word. which has no entry in the translation table; • nc: accumulated count of source phrases that translate into null; 3.3 Decoding The basic decoding algorithm could be kept almost the same as traditional phrase-based or syntax-based translation systems (Yamada and Knight, 2001; Koehn et al., 2003; Chiang, 2005; Liu et al., 2006). For example, in the experiments of this paper, we use a CKY style decoding algorithm following Chiang (2005). Our non-linear translation system is different from traditional systems in the way to calculate the score for each hypothesis. Instead of calculating the score as a linear combination, we use neural networks (Section 3.1) to perform a non-linear combination of feature values. We also use the cube-pruning algorithm (Chiang, 2005) to keep the decoding efficient. Although the non-linearity in model scores may cause more search errors (Huang and Chiang, 827 2007) findi</context>
<context position="24406" citStr="Chiang, 2005" startWordPosition="4025" endWordPosition="4026">ng ICTCLAS3. Our translation sys3http://ictclas.nlpir.org/ 830 Criteria MT03(train) MT02(dev) MT04 MT05 BR, 35.02 36.63 34.96 34.15 BR 38.66 40.04 38.73 37.50 BW 39.55 39.36 38.72 37.81 PW 38.61 38.85 38.73 37.98 Table 2: BLEU4 in percentage on different training criteria (”BR”, ”BW” and ”PW” refer to experiments with ”Best v.s. Rest”, ”Best v.s. Worst” and ”Pairwise” training criteria, respectively. ”BR,” indicates generate hypothesis pairs from n-best set of current iteration only presented in Section 4.3. tem is an in-house implementation of the hierarchical phrase-based translation system(Chiang, 2005). We set the beam size to 20. We train a 5-gram language model on the monolingual data with MKN smoothing(Chen and Goodman, 1998). For each parameter tuning experiments, we ran the same training procedure 3 times and present the average results. The translation quality is evaluated use 4-gram case-insensitive BLEU (Papineni et al., 2002). Significant test is performed using bootstrap re-sampling implemented by Clark et al. (2011). We employ a two-layer neural network with 11 input layer nodes, corresponding to features listed in Section 3.2 and 1 output layer node. The number of nodes in the h</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In annual meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hope and fear for discriminative training of statistical translation models.</title>
<date>2012</date>
<journal>J. Mach. Learn. Res.,</journal>
<volume>13</volume>
<issue>1</issue>
<contexts>
<context position="14396" citStr="Chiang, 2012" startWordPosition="2311" endWordPosition="2312">al objective. 1In our experiments, we use sentence level BLEU with +1 smoothing as the evaluation metric. Best v.s. Rest (BR) To score the best hypothesis in the n-best set e� higher than the rest hypotheses. This objective is very similar to MERT in that it tries to optimize the score of e� and doesn’t concern about the ranking of rest hypotheses. In this case, e� is an approximation of 6. Best v.s. Worst (BW) To score the best hypothesis higher than the worst hypothesis in the n-best set. This objective is motivated by the practice of separating the ”hope” and ”fear” translation hypotheses (Chiang, 2012). We take a simpler strategy which uses the best and worst hypothesis in Cnbest as the ”hope” and ”fear” hypothesis, respectively, in order to avoid multi-pass decoding. Pairwise (PW) To score the better hypothesis in sampled hypothesis pairs higher than the worse one in the same pair. This objective is adapted from the Pairwise Ranking Optimization (PRO) (Hopkins and May, 2011), which tries to ranking all the hypotheses instead of selecting the best one. We use the same sampling strategy as their original paper. Note that each of the above criteria transforms the original problem of selecting</context>
</contexts>
<marker>Chiang, 2012</marker>
<rawString>David Chiang. 2012. Hope and fear for discriminative training of statistical translation models. J. Mach. Learn. Res., 13(1):1159–1187, April.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT ’11,</booktitle>
<pages>176--181</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="24839" citStr="Clark et al. (2011)" startWordPosition="4091" endWordPosition="4094">e hypothesis pairs from n-best set of current iteration only presented in Section 4.3. tem is an in-house implementation of the hierarchical phrase-based translation system(Chiang, 2005). We set the beam size to 20. We train a 5-gram language model on the monolingual data with MKN smoothing(Chen and Goodman, 1998). For each parameter tuning experiments, we ran the same training procedure 3 times and present the average results. The translation quality is evaluated use 4-gram case-insensitive BLEU (Papineni et al., 2002). Significant test is performed using bootstrap re-sampling implemented by Clark et al. (2011). We employ a two-layer neural network with 11 input layer nodes, corresponding to features listed in Section 3.2 and 1 output layer node. The number of nodes in the hidden layer varies in different settings. The sigmoid function is used as the activation function for each node in the hidden layer. For the output layer we use a linear activation function. We try different A for the Li norm from 0.01 to 0.00001 and use the one with best performance on the development set. We solve the optimization problem with ALGLIB package4. 6.2 Experiments of Training Criteria This set experiments evaluates </context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT ’11, pages 176–181, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
</authors>
<title>Locally non-linear learning for statistical machine translation via discretization and structured regularization.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>2--393</pages>
<contexts>
<context position="2884" citStr="Clark et al., 2014" startWordPosition="450" endWordPosition="453">n 3. M sL(e) = Amhm(e|f) (3) m=1 The log-linear models are flexible to incorporate new features and show significant advantage over the traditional source-channel models, thus become the state-of-the-art modeling method and are applied in various translation settings (Yamada and Knight, 2001; Koehn et al., 2003; Chiang, 2005; Liu et al., 2006). It is worth noticing that log-linear models try to separate good and bad translation hypotheses using a linear hyper-plane. However, complex interactions between features make it difficult to linearly separate good translation hypotheses from bad ones (Clark et al., 2014). Taking common features in a typical phrasebased (Koehn et al., 2003) or hierarchical phrasebased (Chiang, 2005) machine translation system as an example, the language model feature favors shorter hypotheses; the word penalty feature encourages longer hypotheses. The phrase translation probability feature selects phrases that occurs more frequently in the training corpus, which sometimes is long with a lower translation probability, as in translating named entities or idioms; 825 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International </context>
<context position="6603" citStr="Clark et al. (2014)" startWordPosition="1028" endWordPosition="1031">ts show that our framework could achieve better translation quality even with the same traditional features as previous linear models. (Section 6) 2 Related work Many research has been attempting to bring nonlinearity into the training of SMT. These efforts could be roughly divided into the following three categories. The first line of research attempted to reinterpret original features via feature transformation or additional learning. For example, Maskey and Zhou (2012) use a deep belief network to learn representations of the phrase translation and lexical translation probability features. Clark et al. (2014) used discretization to transform realvalued dense features into a set of binary indicator features. Lu et al. (2014) learned new features using a semi-supervised deep auto encoder. These work focus on the explicit representation of the features and usually employ extra learning procedure. Our proposed method only takes the original features, with no transformation, as the input. Feature transformation or combination are performed implicitly during the training of the network and integrated with the optimization of translation quality. The second line of research attempted to use non-linear mo</context>
</contexts>
<marker>Clark, Dyer, Lavie, 2014</marker>
<rawString>Jonathan Clark, Chris Dyer, and Alon Lavie. 2014. Locally non-linear learning for statistical machine translation via discretization and structured regularization. Transactions of the Association for Computational Linguistics, 2:393–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
<author>Zhongqiang Huang</author>
<author>Thomas Lamar</author>
<author>Richard M Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Fast and robust neural network joint models for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL</booktitle>
<volume>Volume</volume>
<pages>1370--1380</pages>
<location>Baltimore, MD, USA,</location>
<contexts>
<context position="8176" citStr="Devlin et al., 2014" startWordPosition="1277" endWordPosition="1280">ocal minimum, they still rely on a pre-training and posttraining from MERT or PRO. Comparing to these efforts, our proposed method takes a further step that it is integrated with iterative training, instead of re-ranking, and works without the help of any pre-trained linear models. The third line of research attempted to add non-linear features/components into the log-linear learning framework. Neural network based models are trained as language models (Vaswani et al., 2013; Auli and Gao, 2014), translation models (Gao et al., 2014) or joint language and translation models (Auli et al., 2013; Devlin et al., 2014). Liu et al. (2013) also introduced word embedding for source and target sides of the translation 826 output layer Mo hidden layer Mh input Figure 1: A two-layer feed-forward neural network. rules as local features. In this paper, we focus on enhancing the expressive power of the modeling, which is independent of the research of enhancing translation systems with new designed features. We believe additional improvement could be achieved by incorporating more features into our framework. 3 Non-linear Translation The non-linear modeling of translation hypotheses could be used in both phrase-base</context>
</contexts>
<marker>Devlin, Zbib, Huang, Lamar, Schwartz, Makhoul, 2014</marker>
<rawString>Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard M. Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014, June 22-27, 2014, Baltimore, MD, USA, Volume 1: Long Papers, pages 1370–1380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Duh</author>
<author>Katrin Kirchhoff</author>
</authors>
<title>Beyond loglinear models: Boosted minimum error rate training for n-best re-ranking.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, HLT-Short ’08,</booktitle>
<pages>37--40</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7309" citStr="Duh and Kirchhoff (2008)" startWordPosition="1138" endWordPosition="1141">ndicator features. Lu et al. (2014) learned new features using a semi-supervised deep auto encoder. These work focus on the explicit representation of the features and usually employ extra learning procedure. Our proposed method only takes the original features, with no transformation, as the input. Feature transformation or combination are performed implicitly during the training of the network and integrated with the optimization of translation quality. The second line of research attempted to use non-linear models instead of log-linear models, which is most similar in spirit with our work. Duh and Kirchhoff (2008) used the boosting method to combine several results of MERT and achieved improvement in a re-ranking setting. Liu et al. (2013) proposed an additive neural network which employed a two-layer neural network for embedding-based features. To avoid local minimum, they still rely on a pre-training and posttraining from MERT or PRO. Comparing to these efforts, our proposed method takes a further step that it is integrated with iterative training, instead of re-ranking, and works without the help of any pre-trained linear models. The third line of research attempted to add non-linear features/compon</context>
</contexts>
<marker>Duh, Kirchhoff, 2008</marker>
<rawString>Kevin Duh and Katrin Kirchhoff. 2008. Beyond loglinear models: Boosted minimum error rate training for n-best re-ranking. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, HLT-Short ’08, pages 37–40, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Dumitru Erhan</author>
<author>Pierre antoine Manzagol</author>
<author>Yoshua Bengio</author>
<author>Samy Bengio</author>
<author>Pascal Vincent</author>
</authors>
<title>The difficulty of training deep architectures and the effect of unsupervised pre-training.</title>
<date>2009</date>
<booktitle>Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics (AISTATS-09),</booktitle>
<volume>5</volume>
<pages>153--160</pages>
<editor>In David V. Dyk and Max Welling, editors,</editor>
<contexts>
<context position="5461" citStr="Erhan et al., 2009" startWordPosition="850" endWordPosition="853">rs. To solve the problem, we present a framework for effective training including several criteria to transform the training problem into a binary classification task, a unified objective function and an iterative training algorithm. (Section 4) The second issue is the structure of neural network. Single layer neural networks are equivalent to linear models; two-layer networks with sufficient nodes are capable of learning any continuous function (Bishop, 1995). Adding more layers into the network could model complex functions with less nodes, but also brings the problem of vanishing gradient (Erhan et al., 2009). We adapt a two-layer feed-forward neural network to keep the training process efficient. We notice that one major problem that prevents a neural network training reaching a good solution is that there are too many local minimums in the parameter space. Thus we discuss how to constrain the learning of neural networks with our intuitions and observations of the features. (Section 5) Experiments are conducted to compare various settings and verify the effectiveness of our proposed learning framework. Experimental results show that our framework could achieve better translation quality even with</context>
</contexts>
<marker>Erhan, Manzagol, Bengio, Bengio, Vincent, 2009</marker>
<rawString>Dumitru Erhan, Pierre antoine Manzagol, Yoshua Bengio, Samy Bengio, and Pascal Vincent. 2009. The difficulty of training deep architectures and the effect of unsupervised pre-training. In David V. Dyk and Max Welling, editors, Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics (AISTATS-09), volume 5, pages 153–160. Journal of Machine Learning Research - Proceedings Track.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Xiaodong He</author>
<author>Wen-tau Yih</author>
<author>Li Deng</author>
</authors>
<title>Learning continuous phrase representations for translation modeling.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL</booktitle>
<volume>Volume</volume>
<pages>699--709</pages>
<location>Baltimore, MD, USA,</location>
<contexts>
<context position="8094" citStr="Gao et al., 2014" startWordPosition="1262" endWordPosition="1265">ch employed a two-layer neural network for embedding-based features. To avoid local minimum, they still rely on a pre-training and posttraining from MERT or PRO. Comparing to these efforts, our proposed method takes a further step that it is integrated with iterative training, instead of re-ranking, and works without the help of any pre-trained linear models. The third line of research attempted to add non-linear features/components into the log-linear learning framework. Neural network based models are trained as language models (Vaswani et al., 2013; Auli and Gao, 2014), translation models (Gao et al., 2014) or joint language and translation models (Auli et al., 2013; Devlin et al., 2014). Liu et al. (2013) also introduced word embedding for source and target sides of the translation 826 output layer Mo hidden layer Mh input Figure 1: A two-layer feed-forward neural network. rules as local features. In this paper, we focus on enhancing the expressive power of the modeling, which is independent of the research of enhancing translation systems with new designed features. We believe additional improvement could be achieved by incorporating more features into our framework. 3 Non-linear Translation T</context>
</contexts>
<marker>Gao, He, Yih, Deng, 2014</marker>
<rawString>Jianfeng Gao, Xiaodong He, Wen-tau Yih, and Li Deng. 2014. Learning continuous phrase representations for translation modeling. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014, June 22-27, 2014, Baltimore, MD, USA, Volume 1: Long Papers, pages 699–709.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Hinton</author>
<author>Li Deng</author>
<author>Dong Yu</author>
<author>George E Dahl</author>
<author>Abdel-rahman Mohamed</author>
<author>Navdeep Jaitly</author>
<author>Andrew Senior</author>
<author>Vincent Vanhoucke</author>
<author>Patrick Nguyen</author>
<author>Tara N Sainath</author>
</authors>
<title>Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. Signal Processing Magazine,</title>
<date>2012</date>
<pages>29--6</pages>
<publisher>IEEE,</publisher>
<contexts>
<context position="32983" citStr="Hinton et al., 2012" startWordPosition="5421" endWordPosition="5424">ove the baseline system even with the same feature as input, in a large scale Chinese-English machine translation task. In training neural networks with hidden nodes, we use heuristics to reduce the complexity of network structures and obtain extra advantages over standard networks. It shows that heuristics and intuitions of the data and features are still important to a machine translation system. Neural networks are able to perform feature learning by using hidden nodes to model the interaction among a large vector of raw features, as in image and speech processing (Krizhevsky et al., 2012; Hinton et al., 2012). We are trying to model the interaction between hand-crafted features, which is indeed similar in spirit with learning features from raw features. Although our features already have concrete meaning, e.g. the probability of translation, the fluency of target sentence, etc. Combining these features may have extra advantage in modeling the translation process. As future work, it is necessary to integrate more features into our learning framework. It is also interesting to see how the non-linear modeling fits in to more complex learning tasks which involves domain specific learning techniques. A</context>
</contexts>
<marker>Hinton, Deng, Yu, Dahl, Mohamed, Jaitly, Senior, Vanhoucke, Nguyen, Sainath, 2012</marker>
<rawString>Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. 2012. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. Signal Processing Magazine, IEEE, 29(6):82–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>1352--1362</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="14777" citStr="Hopkins and May, 2011" startWordPosition="2371" endWordPosition="2374">n approximation of 6. Best v.s. Worst (BW) To score the best hypothesis higher than the worst hypothesis in the n-best set. This objective is motivated by the practice of separating the ”hope” and ”fear” translation hypotheses (Chiang, 2012). We take a simpler strategy which uses the best and worst hypothesis in Cnbest as the ”hope” and ”fear” hypothesis, respectively, in order to avoid multi-pass decoding. Pairwise (PW) To score the better hypothesis in sampled hypothesis pairs higher than the worse one in the same pair. This objective is adapted from the Pairwise Ranking Optimization (PRO) (Hopkins and May, 2011), which tries to ranking all the hypotheses instead of selecting the best one. We use the same sampling strategy as their original paper. Note that each of the above criteria transforms the original problem of selecting best hypotheses from an exponential space to a certain pairwise comparison problem, which could be easily trained using binary classifiers. 4.2 Training Objective For the binary classification task, we use a hinge loss following Watanabe (2012). Because the network has a lot of parameters compared with the linear model, we use a L1 norm instead of L2 norm as the regularization </context>
<context position="16232" citStr="Hopkins and May (2011)" startWordPosition="2640" endWordPosition="2643">sis-pair, with e1 to be the one with ⃗0), or 1 � E S(f, e1, e2; 0) N (e1^)ET(f) fED 828 higher eval(·) score; N is the total number of hypothesis-pairs in D; T (f), or simply T, is the set of hypothesis-pairs for each source sentence f. The set T is decided by the criterion used for training. For the BR setting, the best hypothesis is paired with every other hypothesis in the n-best list (Equation 7); while for the BW setting, it is only paired with the worst hypothesis (Equation 8). The generation of T in PW setting is the same with PRO sampling, we refer the readers to the original paper of Hopkins and May (2011). TBR = I(e1,e2)|e1 =arg max eECnbest e2 E Cnbest and e1 =� e2} Algorithm 1 Iterative Training Algorithm Input: the set of training sentences D, max number of iteration I 1: 00 +– RandomInit(), 2: for i = 0 to I do 3: Ti +– 0; 4: for each f E D do 5: Cnbest +– NbestDecode(f ; 0i) 6: T +– GeneratePair(Cnbest) 7: Ti +– Ti U T 8: end for 9: Tall +– WeightedCombine(Ui�1 k=0Tk, Ti) 10: 0i+1 �Optimize(Tall, 0i) 11: end for eval(e), (7) TBW = I(e1,e2)|e1 = arg max eECnbest eval(e), e2 = arg min eval(e)} eECnbest (8) 4.3 Training Procedure In standard training algorithm for classification, the trainin</context>
<context position="25689" citStr="Hopkins and May, 2011" startWordPosition="4234" endWordPosition="4237">n is used as the activation function for each node in the hidden layer. For the output layer we use a linear activation function. We try different A for the Li norm from 0.01 to 0.00001 and use the one with best performance on the development set. We solve the optimization problem with ALGLIB package4. 6.2 Experiments of Training Criteria This set experiments evaluates different training criteria discussed in Section 4.1. We generate hypothesis-pair according to BW, BR and PW criteria, respectively, and perform training with these pairs. In the PW criterion, we use the sampling method of PRO (Hopkins and May, 2011) and get the 50 hypothesis pairs for each sentence. We use 20 hidden nodes for all three settings to make a fair comparison. The results are presented in Table 2. The first two rows compare training with and without the weighted combination of hypothesis pairs we discussed in Section 4.3. As the result suggested, with the weighted combination of hypothesis pairs from previous iterations, the performance improves significantly on both test sets. 4http://www.alglib.net/ Although the system performance on the dev set varies, the performance on test sets are almost comparable. This suggest that al</context>
<context position="27565" citStr="Hopkins and May, 2011" startWordPosition="4553" endWordPosition="4556">st and worst hypothesis of each iteration, which is a easier task for learning a classifiers. It requires the least training instances and achieves the best performance in training. Note that, the accuracy for each system in Table 3 are the accuracy each system achieves after training stops. They are not calculated on the same set of instances, thus not directly comparable. We use the differences in accuracy as an indicator for the difficulties of the corresponding learning task. For the rest of this paper, we use the BW criterion because it is much simpler compared to sampling method of PRO (Hopkins and May, 2011). 6.3 Experiments of Network Structures We make several comparisons of the network structures and compare them with a baseline hierarchical phrase-based translation system (HPB). Table 4 shows the translation performance of 831 Systems MT03(train) MT02(dev) MT04 MT05 Test Average HPB 39.25 39.07 38.81 38.01 38.41 TLayer20 39.55* 39.36* 38.72 37.81 38.27(-0.14) TLayer30 39.70+ 39.71* 38.89 37.90 38.40(-0.01) TLayer50 39.26 38.97 38.72 38.79+ 38.76(+0.35) TLayer100 39.42 38.77 38.65 38.65+ 38.69(+0.28) TLayer200 39.69 38.68 38.72 38.80+ 38.74(+0.32) TDN 39.60+ 38.94 38.99* 38.13 38.56(+0.15) GN </context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 1352–1362, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>144--151</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144–151, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In HLTNAACL.</booktitle>
<contexts>
<context position="2577" citStr="Koehn et al., 2003" startWordPosition="401" endWordPosition="404">t. Pr(e|f) = pxm1 (e|f) exp[EMm=1 Amhm(e|f)] (2) Ee′ exp[EMm=1 Amhm(e′|f)] Because the normalization term in Equation 2 is the same for all translation hypotheses of the same source sentence, the score of each hypothesis, denoted by sL, is actually a linear combination of all features, as shown in Equation 3. M sL(e) = Amhm(e|f) (3) m=1 The log-linear models are flexible to incorporate new features and show significant advantage over the traditional source-channel models, thus become the state-of-the-art modeling method and are applied in various translation settings (Yamada and Knight, 2001; Koehn et al., 2003; Chiang, 2005; Liu et al., 2006). It is worth noticing that log-linear models try to separate good and bad translation hypotheses using a linear hyper-plane. However, complex interactions between features make it difficult to linearly separate good translation hypotheses from bad ones (Clark et al., 2014). Taking common features in a typical phrasebased (Koehn et al., 2003) or hierarchical phrasebased (Chiang, 2005) machine translation system as an example, the language model feature favors shorter hypotheses; the word penalty feature encourages longer hypotheses. The phrase translation proba</context>
<context position="11080" citStr="Koehn et al., 2003" startWordPosition="1752" endWordPosition="1755">lnt: language model probability; • wc: accumulated count of individual words generated during translation; • pc: accumulated count of initial phrases used; • rc: accumulated count of hierarchical rule phrases used; • gc: accumulated count of glue rule used in this hypothesis; • uc: accumulated count of unknown source word. which has no entry in the translation table; • nc: accumulated count of source phrases that translate into null; 3.3 Decoding The basic decoding algorithm could be kept almost the same as traditional phrase-based or syntax-based translation systems (Yamada and Knight, 2001; Koehn et al., 2003; Chiang, 2005; Liu et al., 2006). For example, in the experiments of this paper, we use a CKY style decoding algorithm following Chiang (2005). Our non-linear translation system is different from traditional systems in the way to calculate the score for each hypothesis. Instead of calculating the score as a linear combination, we use neural networks (Section 3.1) to perform a non-linear combination of feature values. We also use the cube-pruning algorithm (Chiang, 2005) to keep the decoding efficient. Although the non-linearity in model scores may cause more search errors (Huang and Chiang, 8</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Imagenet classification with deep convolutional neural networks.</title>
<date>2012</date>
<booktitle>Advances in Neural Information Processing Systems 25,</booktitle>
<pages>1097--1105</pages>
<editor>In F. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors,</editor>
<publisher>Curran Associates, Inc.</publisher>
<contexts>
<context position="32961" citStr="Krizhevsky et al., 2012" startWordPosition="5417" endWordPosition="5420">hat our method could improve the baseline system even with the same feature as input, in a large scale Chinese-English machine translation task. In training neural networks with hidden nodes, we use heuristics to reduce the complexity of network structures and obtain extra advantages over standard networks. It shows that heuristics and intuitions of the data and features are still important to a machine translation system. Neural networks are able to perform feature learning by using hidden nodes to model the interaction among a large vector of raw features, as in image and speech processing (Krizhevsky et al., 2012; Hinton et al., 2012). We are trying to model the interaction between hand-crafted features, which is indeed similar in spirit with learning features from raw features. Although our features already have concrete meaning, e.g. the probability of translation, the fluency of target sentence, etc. Combining these features may have extra advantage in modeling the translation process. As future work, it is necessary to integrate more features into our learning framework. It is also interesting to see how the non-linear modeling fits in to more complex learning tasks which involves domain specific </context>
</contexts>
<marker>Krizhevsky, Sutskever, Hinton, 2012</marker>
<rawString>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2012. Imagenet classification with deep convolutional neural networks. In F. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1097–1105. Curran Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc V Le</author>
<author>Jiquan Ngiam</author>
<author>Adam Coates</author>
<author>Ahbik Lahiri</author>
<author>Bobby Prochnow</author>
<author>Andrew Y Ng</author>
</authors>
<title>On optimization methods for deep learning.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning, ICML 2011,</booktitle>
<pages>265--272</pages>
<location>Bellevue, Washington, USA,</location>
<contexts>
<context position="18707" citStr="Le et al., 2011" startWordPosition="3062" endWordPosition="3065">edure (Algorithm 1). The 2In our experiments, we empirically set the constants to be 0.1 and 0.9, respectively. training starts by randomly initialized model parameters 00 (line 1). In ith iteration, the decoding algorithm decodes each sentence f to get the n-best set Cnbest (line 5). Training hypothesis pairs T are extracted from Cnbest according to the training criterion described in Section 4.2 (line 6). Newly collected pairs Ti are combined with pairs from previous iterations before used for training (line 9). 0i+1 is obtained by solving Equation 6 using the Conjugate Sub-Gradient method (Le et al., 2011) (line 10). 5 Structure of the Network Although neural networks bring strong expressive power to the modeling of translation hypothesis, training a neural network is prone to resulting in local minimum which may affect the training results. We speculate that one reason for these local minimums is that the structure of a well-connected network has too many parameters. Take a neural network with k nodes in the input layer and m nodes in the hidden layer as an example. Every node in the hidden layer is connected to each of the k input nodes. This simple structure resulting in at least k x m param</context>
</contexts>
<marker>Le, Ngiam, Coates, Lahiri, Prochnow, Ng, 2011</marker>
<rawString>Quoc V. Le, Jiquan Ngiam, Adam Coates, Ahbik Lahiri, Bobby Prochnow, and Andrew Y. Ng. 2011. On optimization methods for deep learning. In Proceedings of the 28th International Conference on Machine Learning, ICML 2011, Bellevue, Washington, USA, June 28 - July 2, 2011, pages 265–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-string alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the Association of Computational Linguistics. The Association</booktitle>
<institution>for Computer Linguistics.</institution>
<contexts>
<context position="2610" citStr="Liu et al., 2006" startWordPosition="407" endWordPosition="410">Amhm(e|f)] (2) Ee′ exp[EMm=1 Amhm(e′|f)] Because the normalization term in Equation 2 is the same for all translation hypotheses of the same source sentence, the score of each hypothesis, denoted by sL, is actually a linear combination of all features, as shown in Equation 3. M sL(e) = Amhm(e|f) (3) m=1 The log-linear models are flexible to incorporate new features and show significant advantage over the traditional source-channel models, thus become the state-of-the-art modeling method and are applied in various translation settings (Yamada and Knight, 2001; Koehn et al., 2003; Chiang, 2005; Liu et al., 2006). It is worth noticing that log-linear models try to separate good and bad translation hypotheses using a linear hyper-plane. However, complex interactions between features make it difficult to linearly separate good translation hypotheses from bad ones (Clark et al., 2014). Taking common features in a typical phrasebased (Koehn et al., 2003) or hierarchical phrasebased (Chiang, 2005) machine translation system as an example, the language model feature favors shorter hypotheses; the word penalty feature encourages longer hypotheses. The phrase translation probability feature selects phrases th</context>
<context position="11113" citStr="Liu et al., 2006" startWordPosition="1758" endWordPosition="1761"> wc: accumulated count of individual words generated during translation; • pc: accumulated count of initial phrases used; • rc: accumulated count of hierarchical rule phrases used; • gc: accumulated count of glue rule used in this hypothesis; • uc: accumulated count of unknown source word. which has no entry in the translation table; • nc: accumulated count of source phrases that translate into null; 3.3 Decoding The basic decoding algorithm could be kept almost the same as traditional phrase-based or syntax-based translation systems (Yamada and Knight, 2001; Koehn et al., 2003; Chiang, 2005; Liu et al., 2006). For example, in the experiments of this paper, we use a CKY style decoding algorithm following Chiang (2005). Our non-linear translation system is different from traditional systems in the way to calculate the score for each hypothesis. Instead of calculating the score as a linear combination, we use neural networks (Section 3.1) to perform a non-linear combination of feature values. We also use the cube-pruning algorithm (Chiang, 2005) to keep the decoding efficient. Although the non-linearity in model scores may cause more search errors (Huang and Chiang, 827 2007) finding the highest scor</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Treeto-string alignment template for statistical machine translation. In Proceedings of the 44th Annual Meeting of the Association of Computational Linguistics. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lemao Liu</author>
<author>Taro Watanabe</author>
<author>Eiichiro Sumita</author>
<author>Tiejun Zhao</author>
</authors>
<title>Additive neural networks for statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013,</booktitle>
<pages>4--9</pages>
<location>Sofia, Bulgaria, Volume</location>
<contexts>
<context position="7437" citStr="Liu et al. (2013)" startWordPosition="1159" endWordPosition="1162">presentation of the features and usually employ extra learning procedure. Our proposed method only takes the original features, with no transformation, as the input. Feature transformation or combination are performed implicitly during the training of the network and integrated with the optimization of translation quality. The second line of research attempted to use non-linear models instead of log-linear models, which is most similar in spirit with our work. Duh and Kirchhoff (2008) used the boosting method to combine several results of MERT and achieved improvement in a re-ranking setting. Liu et al. (2013) proposed an additive neural network which employed a two-layer neural network for embedding-based features. To avoid local minimum, they still rely on a pre-training and posttraining from MERT or PRO. Comparing to these efforts, our proposed method takes a further step that it is integrated with iterative training, instead of re-ranking, and works without the help of any pre-trained linear models. The third line of research attempted to add non-linear features/components into the log-linear learning framework. Neural network based models are trained as language models (Vaswani et al., 2013; A</context>
</contexts>
<marker>Liu, Watanabe, Sumita, Zhao, 2013</marker>
<rawString>Lemao Liu, Taro Watanabe, Eiichiro Sumita, and Tiejun Zhao. 2013. Additive neural networks for statistical machine translation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013, 4-9 August 2013, Sofia, Bulgaria, Volume 1: Long Papers, pages 791– 801.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shixiang Lu</author>
<author>Zhenbiao Chen</author>
<author>Bo Xu</author>
</authors>
<title>Learning new semi-supervised deep auto-encoder features for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>122--132</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="6720" citStr="Lu et al. (2014)" startWordPosition="1048" endWordPosition="1051">linear models. (Section 6) 2 Related work Many research has been attempting to bring nonlinearity into the training of SMT. These efforts could be roughly divided into the following three categories. The first line of research attempted to reinterpret original features via feature transformation or additional learning. For example, Maskey and Zhou (2012) use a deep belief network to learn representations of the phrase translation and lexical translation probability features. Clark et al. (2014) used discretization to transform realvalued dense features into a set of binary indicator features. Lu et al. (2014) learned new features using a semi-supervised deep auto encoder. These work focus on the explicit representation of the features and usually employ extra learning procedure. Our proposed method only takes the original features, with no transformation, as the input. Feature transformation or combination are performed implicitly during the training of the network and integrated with the optimization of translation quality. The second line of research attempted to use non-linear models instead of log-linear models, which is most similar in spirit with our work. Duh and Kirchhoff (2008) used the b</context>
</contexts>
<marker>Lu, Chen, Xu, 2014</marker>
<rawString>Shixiang Lu, Zhenbiao Chen, and Bo Xu. 2014. Learning new semi-supervised deep auto-encoder features for statistical machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 122–132, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Maskey</author>
<author>Bowen Zhou</author>
</authors>
<title>Unsupervised deep belief features for speech translation.</title>
<date>2012</date>
<booktitle>In INTERSPEECH 2012, 13th Annual Conference of the International Speech Communication Association,</booktitle>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="6460" citStr="Maskey and Zhou (2012)" startWordPosition="1007" endWordPosition="1010">ction 5) Experiments are conducted to compare various settings and verify the effectiveness of our proposed learning framework. Experimental results show that our framework could achieve better translation quality even with the same traditional features as previous linear models. (Section 6) 2 Related work Many research has been attempting to bring nonlinearity into the training of SMT. These efforts could be roughly divided into the following three categories. The first line of research attempted to reinterpret original features via feature transformation or additional learning. For example, Maskey and Zhou (2012) use a deep belief network to learn representations of the phrase translation and lexical translation probability features. Clark et al. (2014) used discretization to transform realvalued dense features into a set of binary indicator features. Lu et al. (2014) learned new features using a semi-supervised deep auto encoder. These work focus on the explicit representation of the features and usually employ extra learning procedure. Our proposed method only takes the original features, with no transformation, as the input. Feature transformation or combination are performed implicitly during the </context>
</contexts>
<marker>Maskey, Zhou, 2012</marker>
<rawString>Sameer Maskey and Bowen Zhou. 2012. Unsupervised deep belief features for speech translation. In INTERSPEECH 2012, 13th Annual Conference of the International Speech Communication Association, Portland, Oregon, USA, September 9-13, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<pages>295--302</pages>
<contexts>
<context position="1858" citStr="Och and Ney (2002)" startWordPosition="285" endWordPosition="288">duction One of the core problems in the research of statistical machine translation is the modeling of translation hypotheses. Each modeling method defines a score of a target sentence e = e1e2...ei...eI, given a source sentence f = f1f2...fj...fJ, where each ei is the ith target word and fj is the jth source word. The well-known modeling method starts from the Source-Channel model (Brown et al., 1993)(Equation 1). The scoring of e decomposes to the calculation of a translation model and a language model. Pr(e|f) = Pr(e)Pr(f|e)/Pr(f) (1) The modeling method is extended to log-linear models by Och and Ney (2002), as shown in Equation 2, where hm(e|f) is the mth feature function and Am is the corresponding weight. Pr(e|f) = pxm1 (e|f) exp[EMm=1 Amhm(e|f)] (2) Ee′ exp[EMm=1 Amhm(e′|f)] Because the normalization term in Equation 2 is the same for all translation hypotheses of the same source sentence, the score of each hypothesis, denoted by sL, is actually a linear combination of all features, as shown in Equation 3. M sL(e) = Amhm(e|f) (3) m=1 The log-linear models are flexible to incorporate new features and show significant advantage over the traditional source-channel models, thus become the state-</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4604" citStr="Och, 2003" startWordPosition="718" endWordPosition="719">vement of translation quality. In this paper, we propose a non-linear modeling of translation hypotheses based on neural networks. The traditional features of a machine translation system are used as the input to the network. By feeding input features to nodes in a hidden layer, complex interactions among features are modeled, resulting in much stronger expressive power than traditional log-linear models. (Section 3) Employing a neural network for SMT modeling has two issues to be tackled. The first issue is the parameter learning. Log-linear models rely on minimum error rate training (MERT) (Och, 2003) to achieve best performance. When the scoring function become non-linear, the intersection points of these non-linear functions could not be effectively calculated and enumerated. Thus MERT is no longer suitable for learning the parameters. To solve the problem, we present a framework for effective training including several criteria to transform the training problem into a binary classification task, a unified objective function and an iterative training algorithm. (Section 4) The second issue is the structure of neural network. Single layer neural networks are equivalent to linear models; t</context>
<context position="12112" citStr="Och, 2003" startWordPosition="1917" endWordPosition="1918">We also use the cube-pruning algorithm (Chiang, 2005) to keep the decoding efficient. Although the non-linearity in model scores may cause more search errors (Huang and Chiang, 827 2007) finding the highest scoring hypothesis, in practice it still achieves reasonable results. 4 Non-linear Learning Framework Traditional machine translation systems rely on MERT to tune the weights of different features. MERT performs efficient search by enumerating the score function of all the hypotheses and using intersections of these linear functions to form the ”upper-envelope” of the model score function (Och, 2003). When the scoring function is non-linear, it is not feasible to find the intersections of these functions. In this section, we discuss alternatives to train the parameters for non-linear models. 4.1 Training Criteria The task of machine translation is a complex problem with structural output space. Decoding algorithms search for the translation hypothesis with the highest score, according to a given scoring function, from an exponentially large set of candidate hypotheses. The purpose of training is to select the scoring function, so that the function score the hypotheses ”correctly”. The cor</context>
<context position="17171" citStr="Och, 2003" startWordPosition="2813" endWordPosition="2814">nd for 9: Tall +– WeightedCombine(Ui�1 k=0Tk, Ti) 10: 0i+1 �Optimize(Tall, 0i) 11: end for eval(e), (7) TBW = I(e1,e2)|e1 = arg max eECnbest eval(e), e2 = arg min eval(e)} eECnbest (8) 4.3 Training Procedure In standard training algorithm for classification, the training instances stays the same in each iteration. In machine translation, decoding algorithms usually return a very different n-best set with different parameters. This is due to the exponentially large size of search space. MERT and PRO extend the current n-best set by merging the n-best set of all previous iterations into a pool (Och, 2003; Hopkins and May, 2011). In this way, the enlarged n-best set may give a better approximation of the true hypothesis set C and may lead to better and more stable training results. We argue that the training should still focus on hypotheses obtained in current round, because in each iteration the searching for the n-best set is independent of previous iterations. To compromise the above two goals, in our practice, training hypothesis pairs are first generated from the current n-best set, then merged with the pairs generated from all previous iterations. In order to make the model focus more on</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 160– 167, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="12804" citStr="Papineni et al., 2002" startWordPosition="2026" endWordPosition="2029">the intersections of these functions. In this section, we discuss alternatives to train the parameters for non-linear models. 4.1 Training Criteria The task of machine translation is a complex problem with structural output space. Decoding algorithms search for the translation hypothesis with the highest score, according to a given scoring function, from an exponentially large set of candidate hypotheses. The purpose of training is to select the scoring function, so that the function score the hypotheses ”correctly”. The correctness is often introduced by some extrinsic metrics, such as BLEU (Papineni et al., 2002). We denote the scoring function as s(f, e; ⃗ simply s, which is parameterized by 0; denote the set of all translation hypotheses as C; denote the extrinsic metric as eval(·) 1. Note that, in linear cases, s is a linear function as in Equation 3, while in the non-linear case described in this paper, s is the scoring function in Equation 4. Ideally, the training objective is to select a scoring function s, from all functions S, that scores the correct translation (or references) 6, higher than any other hypotheses (Equation 5). s� = {s ∈ S|s(6) &gt; s(e) ∀e ∈ C} (5) In practice, the candidate set </context>
<context position="24745" citStr="Papineni et al., 2002" startWordPosition="4078" endWordPosition="4081"> Rest”, ”Best v.s. Worst” and ”Pairwise” training criteria, respectively. ”BR,” indicates generate hypothesis pairs from n-best set of current iteration only presented in Section 4.3. tem is an in-house implementation of the hierarchical phrase-based translation system(Chiang, 2005). We set the beam size to 20. We train a 5-gram language model on the monolingual data with MKN smoothing(Chen and Goodman, 1998). For each parameter tuning experiments, we ran the same training procedure 3 times and present the average results. The translation quality is evaluated use 4-gram case-insensitive BLEU (Papineni et al., 2002). Significant test is performed using bootstrap re-sampling implemented by Clark et al. (2011). We employ a two-layer neural network with 11 input layer nodes, corresponding to features listed in Section 3.2 and 1 output layer node. The number of nodes in the hidden layer varies in different settings. The sigmoid function is used as the activation function for each node in the hidden layer. For the output layer we use a linear activation function. We try different A for the Li norm from 0.01 to 0.00001 and use the one with best performance on the development set. We solve the optimization prob</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311–318, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
</authors>
<title>A brief introduction to boosting.</title>
<date>1999</date>
<booktitle>In Proceedings of the 16th International Joint Conference on Artificial Intelligence - Volume 2, IJCAI’99,</booktitle>
<pages>1401--1406</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="18000" citStr="Schapire, 1999" startWordPosition="2951" endWordPosition="2952"> still focus on hypotheses obtained in current round, because in each iteration the searching for the n-best set is independent of previous iterations. To compromise the above two goals, in our practice, training hypothesis pairs are first generated from the current n-best set, then merged with the pairs generated from all previous iterations. In order to make the model focus more on pairs from current iteration, we assign pairs in previous iterations a small constant weight and assign pairs in current iteration a relatively large constant weight 2. This is inspired by the AdaBoost algorithm (Schapire, 1999) in weighting instances. Following the spirit of MERT, we propose a iterative training procedure (Algorithm 1). The 2In our experiments, we empirically set the constants to be 0.1 and 0.9, respectively. training starts by randomly initialized model parameters 00 (line 1). In ith iteration, the decoding algorithm decodes each sentence f to get the n-best set Cnbest (line 5). Training hypothesis pairs T are extracted from Cnbest according to the training criterion described in Section 4.2 (line 6). Newly collected pairs Ti are combined with pairs from previous iterations before used for training</context>
</contexts>
<marker>Schapire, 1999</marker>
<rawString>Robert E. Schapire. 1999. A brief introduction to boosting. In Proceedings of the 16th International Joint Conference on Artificial Intelligence - Volume 2, IJCAI’99, pages 1401–1406, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Yinggong Zhao</author>
<author>Victoria Fossum</author>
<author>David Chiang</author>
</authors>
<title>Decoding with large-scale neural language models improves translation.</title>
<date>2013</date>
<journal>A meeting of SIGDAT, a Special Interest Group of the ACL,</journal>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP</booktitle>
<pages>18--21</pages>
<location>Grand Hyatt Seattle, Seattle, Washington, USA,</location>
<contexts>
<context position="8034" citStr="Vaswani et al., 2013" startWordPosition="1251" endWordPosition="1254">ting. Liu et al. (2013) proposed an additive neural network which employed a two-layer neural network for embedding-based features. To avoid local minimum, they still rely on a pre-training and posttraining from MERT or PRO. Comparing to these efforts, our proposed method takes a further step that it is integrated with iterative training, instead of re-ranking, and works without the help of any pre-trained linear models. The third line of research attempted to add non-linear features/components into the log-linear learning framework. Neural network based models are trained as language models (Vaswani et al., 2013; Auli and Gao, 2014), translation models (Gao et al., 2014) or joint language and translation models (Auli et al., 2013; Devlin et al., 2014). Liu et al. (2013) also introduced word embedding for source and target sides of the translation 826 output layer Mo hidden layer Mh input Figure 1: A two-layer feed-forward neural network. rules as local features. In this paper, we focus on enhancing the expressive power of the modeling, which is independent of the research of enhancing translation systems with new designed features. We believe additional improvement could be achieved by incorporating </context>
</contexts>
<marker>Vaswani, Zhao, Fossum, Chiang, 2013</marker>
<rawString>Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. 2013. Decoding with large-scale neural language models improves translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1387– 1392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
</authors>
<title>Optimized online rank learning for machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT ’12,</booktitle>
<pages>253--262</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="15241" citStr="Watanabe (2012)" startWordPosition="2449" endWordPosition="2450">thesis pairs higher than the worse one in the same pair. This objective is adapted from the Pairwise Ranking Optimization (PRO) (Hopkins and May, 2011), which tries to ranking all the hypotheses instead of selecting the best one. We use the same sampling strategy as their original paper. Note that each of the above criteria transforms the original problem of selecting best hypotheses from an exponential space to a certain pairwise comparison problem, which could be easily trained using binary classifiers. 4.2 Training Objective For the binary classification task, we use a hinge loss following Watanabe (2012). Because the network has a lot of parameters compared with the linear model, we use a L1 norm instead of L2 norm as the regularization term, to favor sparse solutions. We define our training objective function in Equation 6. arg min 0 + a · ||0||1 with S(·) = max{s(f, e1; 0) − s(f, e2; 0) + 1, 0} (6) where D is the given training data; (e1, e2) is a training hypothesis-pair, with e1 to be the one with ⃗0), or 1 � E S(f, e1, e2; 0) N (e1^)ET(f) fED 828 higher eval(·) score; N is the total number of hypothesis-pairs in D; T (f), or simply T, is the set of hypothesis-pairs for each source senten</context>
</contexts>
<marker>Watanabe, 2012</marker>
<rawString>Taro Watanabe. 2012. Optimized online rank learning for machine translation. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT ’12, pages 253–262, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntaxbased statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="2557" citStr="Yamada and Knight, 2001" startWordPosition="397" endWordPosition="400">s the corresponding weight. Pr(e|f) = pxm1 (e|f) exp[EMm=1 Amhm(e|f)] (2) Ee′ exp[EMm=1 Amhm(e′|f)] Because the normalization term in Equation 2 is the same for all translation hypotheses of the same source sentence, the score of each hypothesis, denoted by sL, is actually a linear combination of all features, as shown in Equation 3. M sL(e) = Amhm(e|f) (3) m=1 The log-linear models are flexible to incorporate new features and show significant advantage over the traditional source-channel models, thus become the state-of-the-art modeling method and are applied in various translation settings (Yamada and Knight, 2001; Koehn et al., 2003; Chiang, 2005; Liu et al., 2006). It is worth noticing that log-linear models try to separate good and bad translation hypotheses using a linear hyper-plane. However, complex interactions between features make it difficult to linearly separate good translation hypotheses from bad ones (Clark et al., 2014). Taking common features in a typical phrasebased (Koehn et al., 2003) or hierarchical phrasebased (Chiang, 2005) machine translation system as an example, the language model feature favors shorter hypotheses; the word penalty feature encourages longer hypotheses. The phra</context>
<context position="11060" citStr="Yamada and Knight, 2001" startWordPosition="1748" endWordPosition="1751"> in -y as words in α; • plnt: language model probability; • wc: accumulated count of individual words generated during translation; • pc: accumulated count of initial phrases used; • rc: accumulated count of hierarchical rule phrases used; • gc: accumulated count of glue rule used in this hypothesis; • uc: accumulated count of unknown source word. which has no entry in the translation table; • nc: accumulated count of source phrases that translate into null; 3.3 Decoding The basic decoding algorithm could be kept almost the same as traditional phrase-based or syntax-based translation systems (Yamada and Knight, 2001; Koehn et al., 2003; Chiang, 2005; Liu et al., 2006). For example, in the experiments of this paper, we use a CKY style decoding algorithm following Chiang (2005). Our non-linear translation system is different from traditional systems in the way to calculate the score for each hypothesis. Instead of calculating the score as a linear combination, we use neural networks (Section 3.1) to perform a non-linear combination of feature values. We also use the cube-pruning algorithm (Chiang, 2005) to keep the decoding efficient. Although the non-linearity in model scores may cause more search errors </context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntaxbased statistical translation model. In Proceedings of the 39th Annual Meeting of the Association of Computational Linguistics, pages 523–530.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>