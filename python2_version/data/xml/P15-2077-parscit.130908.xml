<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.054569">
<title confidence="0.7698785">
Early and Late Combinations of Criteria for Reranking Distributional
Thesauri
</title>
<author confidence="0.586793">
Olivier Ferret
</author>
<note confidence="0.932456">
CEA, LIST, Vision and Content Engineering Laboratory,
Gif-sur-Yvette, F-91191 France.
</note>
<email confidence="0.988848">
olivier.ferret@cea.fr
</email>
<sectionHeader confidence="0.993652" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999819076923077">
In this article, we first propose to ex-
ploit a new criterion for improving distri-
butional thesauri. Following a bootstrap-
ping perspective, we select relations be-
tween the terms of similar nominal com-
pounds for building in an unsupervised
way the training set of a classifier perform-
ing the reranking of a thesaurus. Then, we
evaluate several ways to combine thesauri
reranked according to different criteria and
show that exploiting the complementary
information brought by these criteria leads
to significant improvements.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999955382352941">
The work presented in this article aims at im-
proving thesauri built following the distributional
approach as implemented by (Grefenstette, 1994;
Lin, 1998; Curran and Moens, 2002). A part of
the work for improving such thesauri focuses on
the filtering of the components of the distribu-
tional contexts of words (Padr´o et al., 2014; Po-
lajnar and Clark, 2014) or their reweighting, ei-
ther by turning the weights of these components
into ranks (Broda et al., 2009) or by adapting
them through a bootstrapping method from the
thesaurus to improve (Zhitomirsky-Geffet and Da-
gan, 2009; Yamamoto and Asakura, 2010). The
other part implies more radical changes, includ-
ing dimensionality reduction methods such as La-
tent Semantic Analysis (Pad´o and Lapata, 2007),
multi-prototype (Reisinger and Mooney, 2010) or
exemplar-based models (Erk and Pado, 2010),
neural approaches (Huang et al., 2012; Mikolov et
al., 2013) or the adoption of a Bayesian viewpoint
(Kazama et al., 2010; Dinu and Lapata, 2010).
Our work follows (Ferret, 2012), which pro-
posed a different way from (Zhitomirsky-Geffet
and Dagan, 2009) to exploit bootstrapping by se-
lecting in an unsupervised way a set of semanti-
cally similar words from an initial thesaurus and
training from them a classifier to rerank the se-
mantic neighbors of the initial thesaurus entries.
More precisely, we propose a new criterion for this
selection, based on the similarity relations of the
components of similar compounds, and we show
two modes – early and late – of combination of
thesauri reranked from different criteria, including
ours, leading to significant further improvements.
</bodyText>
<sectionHeader confidence="0.907733" genericHeader="method">
2 Reranking a distributional thesaurus
</sectionHeader>
<bodyText confidence="0.999959034482759">
Distributional thesauri are characterized by het-
erogeneous performance in their entries, even for
high frequency entries. This is a favorable situa-
tion for implementing a bootstrapping approach in
which the results for “good” entries are exploited
for improving the results of the other ones. How-
ever, such idea faces two problems: first, detect-
ing “good” entries; second, learning a model from
them for improving the performance of the other
entries.
The first issue consists in selecting without su-
pervision a set of positive and negative examples
of similar words that represents a good compro-
mise between its error rate and its size. Straight-
forward solutions such as using the similarity
value between an entry and its neighbors or relying
on the frequency of entries are not satisfactory in
terms of error rate. Hence, we propose in Section 3
a new method, based on the semantic composi-
tionality hypothesis of compounds, for achieving
this selection in a more indirect way and show the
interest to combine it with the criterion of (Ferret,
2012) for building a large training set with a rea-
sonable error rate.
We address the second issue by following
(Hagiwara et al., 2009), which defined a Sup-
port Vector Machine (SVM) model for deciding
whether two words are similar or not. In our con-
text, a positive example is a pair of nouns that are
</bodyText>
<page confidence="0.90227">
470
</page>
<bodyText confidence="0.929944473684211">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 470–476,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
semantically similar while a negative example is
a pair of non similar nouns. The features of each
pair of nouns are built by summing the weights of
the elements shared by their distributional repre-
sentations, which are vectors of weighted cooccur-
rents. Cooccurrents not shared by the two nouns
are given a null weight.
This SVM model is used for improving a the-
saurus by reranking its semantic neighbors as fol-
lows: for each entry E of the thesaurus, the rep-
resentation as an example of the word pair (E,
neighbor) is built for each of the neighbors of E
and submitted to the SVM model in classification
mode. Finally, all the neighbors of E are reranked
according to the value of the decision function
computed for each neighbor by the SVM model.
</bodyText>
<sectionHeader confidence="0.976167" genericHeader="method">
3 Unsupervised example selection
</sectionHeader>
<bodyText confidence="0.997480323529412">
The evaluation of distributional thesauri shows
that a true semantic neighbor is more likely to be
found when the thesaurus entry is a high frequency
noun and the neighbor has a low rank. However,
relying only on these two criteria doesn’t lead to
a good enough set of positive examples. For in-
stance, taking as positive examples from the ini-
tial thesaurus of Section 4 the first neighbor of its
2,148 most frequent entries, the number of pos-
itive examples of (Hagiwara et al., 2009), only
leads to 44.3% of correct examples. Moreover,
this percentage exceeds 50% only when the num-
ber of examples is less than 654, which represents
a very small training set for this kind of task.
Hence, we propose a more selective approach
for choosing positive examples among high fre-
quency nouns to get a more balanced solution be-
tween the number of examples and their error rate.
This approach exploits a form of semantic compo-
sitionality hypothesis of compounds. While much
work has been done recently for defining the dis-
tributional representation of compounds by com-
posing the distributional representations of their
components (Mitchell and Lapata, 2010; Paperno
et al., 2014), we adopt a kind of reverse viewpoint
by exploiting the possibility to link the meaning
of a compound to the meaning of its components.
More precisely, we assume that the mono-terms
of two semantically related compounds with the
same syntactic role in their compound are likely
to be semantically linked themselves.
In this work, we only consider compounds hav-
ing one of these three term structures (with their
percentage of the vocabulary of compounds):
</bodyText>
<figure confidence="0.991652666666667">
(a) &lt;noun&gt;mod &lt;noun&gt;head (30)
(b) &lt;adjective&gt;mod &lt;noun&gt;head (58)
(c) &lt;noun&gt;head &lt;preposition&gt;&lt;noun&gt;mod (12)
</figure>
<bodyText confidence="0.998571157894737">
Each compound CZ is represented as a pair
(HZ, MZ), where HZ stands for the head of the
compound whereas MZ represents its modifier
(mod). According to the assumption underlying
our selection procedure, if a compound (H2, M2)
is a semantic neighbor of a compound (H1, M1)
(i.e. at most its cth neighbor in a distributional the-
saurus of compounds), we can expect H1 and H2
on one hand and M1 and M2 on the other hand to
be semantically similar. Since distributional the-
sauri of compounds are far from being perfect, we
added constraints on the matching of the compo-
nents of two compounds. More precisely, the posi-
tive examples of semantically similar nouns (noun
pairs after →) are selected by the three following
rules, where H1 = H2 means that H1 is the same
word as H2 and H1 ≡ H2 means that H2 is at most
the mth neighbor of H1 in the initial thesaurus of
mono-terms (but is different from H1):
</bodyText>
<equation confidence="0.947240666666667">
(1) H1 ≡ H2 &amp; M1 = M2 → (H1, H2)
(2) M1 ≡ M2 &amp; H1 = H2 → (M1, M2)
(3) M1 ≡ M2 &amp; H1 ≡ H2 → (H1, H2), (M1, M2)
</equation>
<bodyText confidence="0.999984285714286">
The selection of negative examples is also an
important issue but benefits from the fact that the
number of semantic neighbors of an entry that
are actually semantically linked to this entry in a
distributional thesaurus quickly decreases as their
rank increase. In the experiments of Section 4,
we built negative examples from positive exam-
ples by turning each positive example (A,B) into
two negative examples: (A, rank 10 A neighbor)
and (B, rank 10 B neighbor). Choosing neighbors
with a higher rank would have guaranteed fewer
false negative examples but taking neighbors with
a rather small rank for building negative examples
is more useful in terms of discrimination.
</bodyText>
<sectionHeader confidence="0.997144" genericHeader="evaluation">
4 Experiments and evaluation
</sectionHeader>
<subsectionHeader confidence="0.99998">
4.1 Building of distributional thesauri
</subsectionHeader>
<bodyText confidence="0.999864428571429">
The first step of the work we present is the build-
ing of two distributional thesauri: the thesaurus of
mono-terms to improve (A2ST) and a thesaurus
of compounds (A2ST-comp). Similarly to (Ferret,
2012), they were both built from the AQUAINT-2
corpus, a 380 million-word corpus of news arti-
cles in English. The building procedure, defined
</bodyText>
<page confidence="0.994013">
471
</page>
<bodyText confidence="0.999958227272727">
by (Ferret, 2010), was also identical to (Ferret,
2012), with distributional contexts compared with
the Cosine measure and made of window-based
lemmatized cooccurrents (1 word before and af-
ter) weighted by Positive Pointwise Mutual Infor-
mation (PPMI). For the thesaurus of compounds,
a preprocessing step was added to identify nom-
inal compounds in texts. This identification was
done in two steps: first, a set of compounds were
extracted from the AQUAINT-2 corpus by rely-
ing on a restricted set of morpho-syntactic pat-
terns applied by the Multiword Expression Toolkit
(mwetoolkit) (Ramisch et al., 2010); then, the
most frequent compounds in this set (frequency
&gt; 100) were selected as reference and their oc-
currences in the AQUAINT-2 corpus were iden-
tified by applying the longest-match strategy to
the output of the TreeTagger part-of-speech tagger
(Schmid, 1994)1. Finally, distributional contexts
made of mono-terms and compounds were built as
stated above and neighbors were found for 29,174
compounds.
</bodyText>
<subsectionHeader confidence="0.99377">
4.2 Example selection
</subsectionHeader>
<bodyText confidence="0.999987208333333">
We applied the three rules of Section 3 with all
the entries of our thesaurus of compounds and the
upper half in frequency of our mono-term entries.
For mono-terms, we only took the first neighbor
(m = 1) of each entry because of the rather low
performance of the initial thesaurus while for com-
pounds, a larger value (c = 3) was chosen for
enlarging the number of selected examples since
neighbors were globally more reliable (see results
of Table 2). As the selection method makes the
definition of a development set quite difficult, the
values of these two parameters were chosen in a
conservative way.
Table 1 gives for each rule and two combina-
tions of them the number of selected positive ex-
amples (#pos. ex.) and the percentage of positive
(%good pos.) and negative examples (%bad neg.)
found in our Gold Standard resource for thesaurus
evaluation. This resource results from the union of
the synonyms of WordNet 3.0 and the associated
words of the Moby thesaurus. Table 1 also gives
the same data for examples selected by the method
of (Ferret, 2012) (symmetry row, sym. for short),
based on the fact that as similarity relations are
</bodyText>
<footnote confidence="0.690094">
1Longest-match strategy: if C1 is a reference compound
that is part of a reference compound C2, the identification
of an occurrence of C2 blocks out the identification of the
associated occurrence of C1.
</footnote>
<table confidence="0.999416666666667">
method %good pos. %bad neg. #pos. ex.
symmetry 59.7 12.4 796
56.9 16.1 921
44.7 14.7 308
46.2 16.9 40
rules (1,2) 53.0 16.1 1,115
rules (1,2,3) 52.4 15.9 1,131
sym. + (1,2) 54.3 15.0 1,710
sym. + (1,2,3) 53.9 14.5 1,725
</table>
<tableCaption confidence="0.999805">
Table 1: Selection of examples.
</tableCaption>
<bodyText confidence="0.999763423076923">
symmetric, a pair of words (A,B) are more likely
to be similar if the first neighbor of A is B and the
first neighbor of B is A. The data for the union of
the examples produced by the two methods also
appear in Table 1.
Concerning the method we propose, Table 1
shows that rule (3), which is a priori the least reli-
able of the three rules as it only requires similarity
and not equality for both heads and modifiers, ac-
tually produces a very small set of examples that
tends to degrade global results. As a consequence,
only the combination of rules (1) and (2) is used
thereafter (row in bold). Table 1 also suggests that
the heads of two semantically linked compounds
are more likely to be actually linked themselves
if they have the same modifier than the modifiers
of two semantically linked compounds having the
same head. This confirms our expectation that the
head of a compound is more related to the mean-
ing of the compound than its modifier. More glob-
ally, Table 1 shows that the symmetry method has
higher results than the second one but their associ-
ation produces an interesting compromise between
the number of examples, 1,710, and its error rate,
45.7. The fact that the two methods only share 201
noun pairs also illustrates their complementarity.
</bodyText>
<subsectionHeader confidence="0.999611">
4.3 Reranking evaluation
</subsectionHeader>
<bodyText confidence="0.99988725">
For our SVM models, we adopted the RBF kernel,
as (Hagiwara et al., 2009), and a grid search strat-
egy for optimizing both the γ and C parameters
by applying a 5-fold cross validation procedure to
our training set and adopting the precision mea-
sure as the evaluation function to optimize. The
models were built with LIBSVM (Chang and Lin,
2001) and then applied to the neighbors of our ini-
tial thesaurus.
Table 2 gives the results of the reranking for
both the method we propose, compound (comp.
for short), with examples selected by rules (1) and
</bodyText>
<page confidence="0.99406">
472
</page>
<bodyText confidence="0.999981326530612">
(2), and the one of (Ferret, 2012), symmetry. In
either case, they correspond to an intrinsic evalu-
ation achieved by comparing the semantic neigh-
bors of each thesaurus entry with the synonyms
and related words of our Gold Standard resource
for that entry. 12,243 entries with frequency &gt; 10
were present in this resource and evaluated in such
a way. As the neighbors are ranked according to
their similarity value with their entry, we adopted
the classical evaluation measures of Information
Retrieval by replacing documents with synonyms
and queries with entries: R-precision (R-prec.),
Mean Average Precision (MAP) and precision at
different cut-offs (1, 5 and 10).
More precisely, the initial row of Table 2 gives
the values of these measures for our initial the-
saurus of mono-terms while its A2ST-comp row
corresponds to the measures for our thesaurus of
compounds. It should be note that in the case of
the A2ST-comp thesaurus, the number of evalu-
ated entries is very small, restricted to 813 entries,
with also a very small number of reference syn-
onyms by entry. Hence, the results of the evalu-
ation of A2ST-comp have to be considered with
caution even if their high level for the very first se-
mantic neighbors tends to confirm the positive im-
pact of the low level of ambiguity of compounds
compared to mono-terms.
The two following rows gives the results of the
thesauri built from the best models of (Baroni et
al., 2014), B14-count for the count model, whose
main parameters are close or identical to ours,
and B14-predict for the predict model, built from
(Mikolov et al., 2013). These results first illus-
trate the known importance of corpus size, as the
(Baroni et al., 2014)’s corpus is more than 7 times
larger than ours, and the fact that for building the-
sauri, the count model is superior to the predict
model. This last observation is confirmed by the
results of the skip-gram model of (Mikolov et al.,
2013) with its best parameters2 for our corpus (5th
row), which clearly exhibits worst results than ini-
tial. For this Mikolov thesaurus and the follow-
ing reranked ones, each value corresponds to the
difference between the measure for the considered
thesaurus and the measure for the initial thesaurus.
All these differences were found statistically sig-
nificant according to a paired Wilcoxon test with
p-value &lt; 0.05.
</bodyText>
<footnote confidence="0.9761925">
2word2vec -cbow 0 -size 600 -window 10 -negative 0 -hs
0 -sample 1e-5
</footnote>
<table confidence="0.999886">
Thesaurus R-prec. MAP P@1 P@5 P@10
initial (A2ST) 7.7 5.6 22.5 14.1 10.8
A2ST-comp 32.7 39.5 34.9 12.3 7.1
B14-count 12.5 9.8 31.9 19.6 15.2
B14-pred 10.9 8.5 30.3 18.4 13.8
Mikolov -2.2 -1.4 -6.2 -4.6 -3.8
symmetry +0.3 +0.1 +2.1 +0.8 +0.6
compound +0.1 +0.0 +2.0 +0.9 +0.6
sym.+comp. +0.3 +0.2 +2.8 +1.2 +0.9
RRF +0.7 +0.6 +3.7 +1.9 +1.4
borda +0.7 +0.5 +3.6 +1.7 +1.3
condorcet +0.5 +0.4 +3.4 +1.6 +1.2
CombSum +0.9 +0.8 +4.7 +2.2 +1.5
CS-w-Mik +1.2 +1.4 +4.2 +2.0 +1.5
</table>
<tableCaption confidence="0.957045">
Table 2: Evaluation of our initial thesaurus and its
reranked versions (values = percentages).
</tableCaption>
<bodyText confidence="0.9999305">
The analysis of the next two rows of Table 2
first shows that each criterion used for reranking
our initial thesaurus leads to a global increase of
results. The extent of this increase is quite sim-
ilar for the two criteria: symmetry slightly out-
performs compound but the difference is not sig-
nificant. This increase is higher for P@{1,5,10}
than for R-precision and MAP, which can be ex-
plained by the high number of synonyms and re-
lated words, 38.7 on average, that an entry of our
initial thesaurus has in our reference. Hence, even
a significant increase of P@{1,5,10} may have a
modest impact on R-precision and MAP as the
overall recall, equal to 9.8%, is low.
</bodyText>
<subsectionHeader confidence="0.994135">
4.4 Thesaurus fusion
</subsectionHeader>
<bodyText confidence="0.999966111111111">
Having several thesauri reranked according to dif-
ferent criteria offers the opportunity to apply en-
semble methods. Such idea was already experi-
mented in (Curran, 2002) for thesauri built with
different parameters (window or syntactic based
cooccurrents, etc). We tested more particularly
two general strategies for data fusion (Atrey et al.,
2010): early and late fusions. The first one con-
sists in our case in fusing the training sets built
from our two criteria. As for each criterion, a clas-
sifier is then built from the fused training set and
applied for reranking the initial thesaurus (see the
sym.+comp. row of Table 2).
Table 3 illustrates qualitatively the impact of
this first strategy for the entry esteem. Its Word-
Net row gives all the synonyms for this entry in
WordNet while its Moby row gives the first re-
lated words for this entry in Moby. In our initial
</bodyText>
<page confidence="0.998569">
473
</page>
<table confidence="0.9999524">
WordNet respect, admiration, regard
Moby admiration, appreciation, accep-
tance, dignity, regard, respect, ac-
count, adherence, consideration,
estimate, estimation, fame, great-
ness, homage + 79 words more
initial cordiality, gratitude, admiration,
comradeship, back-scratching,
perplexity, respect, ruination,
appreciation, neighbourliness ...
sym.+comp. respect, admiration, trust, recog-
nition, gratitude, confidence, af-
fection, understanding, solidarity,
dignity, appreciation, regard, sym-
pathy, acceptance ...
</table>
<tableCaption confidence="0.930282">
Table 3: Reranking for the entry esteem with the
early fusion strategy.
</tableCaption>
<bodyText confidence="0.999958104166667">
thesaurus, the first two neighbors of esteem that
are present in our reference resources are admira-
tion (rank 3) and respect (rank 7). The reranking
produces a thesaurus in which these two words ap-
pear as the first two neighbors of the entry while
its third synonym in WordNet raises from rank 22
to rank 12. Moreover, the number of neighbors
among the first 14 ones that are present in Moby
increases from 3 to 6.
The late fusion strategy relies on the methods
used in Information Retrieval for merging ranked
lists of retrieved documents. More precisely, we
experimented the Borda, Condorcet (Nuray and
Can, 2006) and Reciprocal Rank (RRF) (Cormack
et al., 2009) fusions based on ranks and the Comb-
Sum fusion based on similarity values, normalized
in our case with the Zero-one method (Wu et al.,
2006). The corresponding thesauri were built by
fusing, entry by entry, the lists of neighbors com-
ing from the initial, symmetry and compound the-
sauri.
Table 2 first shows that all the thesauri pro-
duced by our ensemble methods outperform our
first three thesauri, which confirms that initial,
symmetry and compound can bring complemen-
tary information, exploited by the fusion. It also
shows that our late fusion methods are more ef-
fective than our early fusion method. However,
no specific element advocates at this stage for a
generalization of this observation. The evaluation
reported by Table 2 also suggests that for fusing
distributional thesauri, the similarity of a neighbor
with its entry is a more relevant criterion than its
rank. Among the rank based methods, we observe
that RRF is clearly superior to condorcet but only
weakly superior to borda. Finally, the last row
of Table 2 – CS-w-Mik – illustrates one step fur-
ther the interest of ensemble methods for distribu-
tional thesauri: whereas the “Mikolov thesaurus”
gets the worst results among all the thesauri of Ta-
ble 2, adding it to the initial, symmetry and com-
pound thesauri in the CombSum method leads to
improve both R-precision and MAP, with a only
small decrease of P@1 and P@5. From a more
global perspective, it is interesting to note that our
best method, CombSum, clearly outperforms the
reranking method of (Ferret, 2013) with the same
initial starting point.
</bodyText>
<sectionHeader confidence="0.916209" genericHeader="conclusions">
5 Conclusion and perspectives
</sectionHeader>
<bodyText confidence="0.999810666666667">
In this article, we have presented a method based
on bootstrapping for improving distributional the-
sauri. More precisely, we have proposed a new
criterion, based on the relations of mono-terms in
similar compounds, for the unsupervised selection
of training examples used for reranking the seman-
tic neighbors of a thesaurus. We have evaluated
two different strategies for combining this crite-
rion with an already existing one and showed that a
late fusion approach based on the merging of lists
of neighbors is particularly effective compared to
an early fusion approach based on the merging of
training sets.
We plan to extend this work by studying how
the combination of the unsupervised selection of
examples and their use for training supervised
classifiers can be exploited for improving distribu-
tional thesauri through feature selection. We will
also investigated the interest of taking into account
word senses in this framework, as in (Huang et al.,
2012) or (Reisinger and Mooney, 2010).
</bodyText>
<sectionHeader confidence="0.998616" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998197923076923">
Pradeep K. Atrey, M. Anwar Hossain, Abdulmotaleb
El Saddik, and Mohan S. Kankanhalli. 2010. Mul-
timodal fusion for multimedia analysis: a survey.
Multimedia Systems, 16(6):345–379.
Marco Baroni, Georgiana Dinu, and Germ´an
Kruszewski. 2014. Don’t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In 52nd An-
nual Meeting of the Association for Computational
Linguistics (ACL 2014), pages 238–247, Baltimore,
Maryland.
Bartosz Broda, Maciej Piasecki, and Stan Szpakow-
icz. 2009. Rank-Based Transformation in Measur-
</reference>
<page confidence="0.99536">
474
</page>
<reference confidence="0.996169477064221">
ing Semantic Relatedness. In 22,d Canadian Con-
ference on Artificial Intelligence, pages 187–190.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIB-
SVM: a library for support vector machines.
http://www.csie.ntu.edu.tw/˜cjlin/
libsvm.
Gordon V. Cormack, Charles L. A. Clarke, and Ste-
fan Buettcher. 2009. Reciprocal rank fusion outper-
forms condorcet and individual rank learning meth-
ods. In 32,d International ACM SIGIR Conference
on Research and Development in Information Re-
trieval (SIGIR’09), pages 758–759.
James R. Curran and Marc Moens. 2002. Improve-
ments in automatic thesaurus extraction. In Work-
shop of the ACL Special Interest Group on the Lexi-
con (SIGLEX), pages 59–66, Philadelphia, USA.
James Curran. 2002. Ensemble methods for auto-
matic thesaurus extraction. In 2002 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2002), pages 222–229.
Georgiana Dinu and Mirella Lapata. 2010. Measur-
ing distributional similarity in context. In 2010 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2010), pages 1162–1172, MIT,
Massachusetts, USA.
Katrin Erk and Sebastian Pado. 2010. Exemplar-based
models for word meaning in context. In 481hth An-
nual Meeting of the Association for Computational
Linguistics (ACL 2010), short paper, pages 92–97,
Uppsala, Sweden, July.
Olivier Ferret. 2010. Testing semantic similarity mea-
sures for extracting synonyms from a corpus. In
Seventh conference on International Language Re-
sources and Evaluation (LREC’10), Valletta, Malta.
Olivier Ferret. 2012. Combining bootstrapping and
feature selection for improving a distributional the-
saurus. In 201h European Conference on Artificial
Intelligence (ECAI2012), pages 336–341, Montpel-
lier, France.
Olivier Ferret. 2013. Identifying bad semantic
neighbors for improving distributional thesauri. In
51s1 Annual Meeting of the Association for Com-
putational Linguistics (ACL 2013), pages 561–571,
Sofia, Bulgaria.
Gregory Grefenstette. 1994. Explorations in auto-
matic thesaurus discovery. Kluwer Academic Pub-
lishers.
Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko
Toyama. 2009. Supervised synonym acquisition
using distributional features and syntactic patterns.
Information and Media Technologies, 4(2):59–83.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In 50th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL’12), pages
873–882.
Jun’ichi Kazama, Stijn De Saeger, Kow Kuroda,
Masaki Murata, and Kentaro Torisawa. 2010. A
bayesian method for robust estimation of distribu-
tional similarities. In 481h Annual Meeting of the
Association for Computational Linguistics, pages
247–256, Uppsala, Sweden.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In 171h International Confer-
ence on Computational Linguistics and 361h Annual
Meeting of the Association for Computational Lin-
guistics (ACL-COLING’98), pages 768–774, Mon-
tral, Canada.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In 2013 Conference of the
North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies (NAACL HLT 2013), pages 746–751, At-
lanta, Georgia.
Jeffrey Mitchell and Mirella Lapata. 2010. Composi-
tion in distributional models of semantics. Cognitive
Science, 34(8):1388–1439.
Rabia Nuray and Fazli Can. 2006. Automatic rank-
ing of information retrieval systems using data fu-
sion. Information Processing and Management,
42(3):595–614.
Sebastian Pad´o and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161–199.
Muntsa Padr´o, Marco Idiart, Aline Villavicencio, and
Carlos Ramisch. 2014. Nothing like good old fre-
quency: Studying context filters for distributional
thesauri. In 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP 2014),
pages 419–424, Doha, Qatar.
Denis Paperno, Nghia The Pham, and Marco Baroni.
2014. A practical and linguistically-motivated ap-
proach to compositional distributional semantics. In
52,d Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2014), pages 90–99, Bal-
timore, Maryland.
Tamara Polajnar and Stephen Clark. 2014. Improv-
ing distributional semantic vectors through context
selection and normalisation. In 141h Conference of
the European Chapter of the Association for Compu-
tational Linguistics (EACL 2014), pages 230–238,
Gothenburg, Sweden.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010. mwetoolkit: a Framework for Multi-
word Expression Identification. In Seventh Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2010), Valetta, Malta, May.
</reference>
<page confidence="0.987634">
475
</page>
<reference confidence="0.999742652173913">
Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL 2010), pages 109–117, Los Angeles,
California, June.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In International Con-
ference on New Methods in Language Processing.
Shengli Wu, Fabio Crestani, and Yaxin Bi. 2006. Eval-
uating score normalization methods in data fusion.
In Third Asia Conference on Information Retrieval
Technology (AIRS’06), pages 642–648. Springer-
Verlag.
Kazuhide Yamamoto and Takeshi Asakura. 2010.
Even unassociated features can improve lexical dis-
tributional similarity. In Second Workshop on
NLP Challenges in the Information Explosion Era
(NLPIX 2010), pages 32–39, Beijing, China.
Maayan Zhitomirsky-Geffet and Ido Dagan. 2009.
Bootstrapping Distributional Feature Vector Quality.
Computational Linguistics, 35(3):435–461.
</reference>
<page confidence="0.999108">
476
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.788034">
<title confidence="0.953473">Early and Late Combinations of Criteria for Reranking Distributional Thesauri</title>
<author confidence="0.955587">Olivier</author>
<affiliation confidence="0.879749">CEA, LIST, Vision and Content Engineering</affiliation>
<address confidence="0.989358">Gif-sur-Yvette, F-91191</address>
<email confidence="0.994934">olivier.ferret@cea.fr</email>
<abstract confidence="0.999495785714286">In this article, we first propose to exploit a new criterion for improving distributional thesauri. Following a bootstrapping perspective, we select relations between the terms of similar nominal compounds for building in an unsupervised way the training set of a classifier performing the reranking of a thesaurus. Then, we evaluate several ways to combine thesauri reranked according to different criteria and show that exploiting the complementary information brought by these criteria leads to significant improvements.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Pradeep K Atrey</author>
<author>M Anwar Hossain</author>
<author>Abdulmotaleb El Saddik</author>
<author>Mohan S Kankanhalli</author>
</authors>
<title>Multimodal fusion for multimedia analysis: a survey. Multimedia Systems,</title>
<date>2010</date>
<pages>16--6</pages>
<marker>Atrey, Hossain, El Saddik, Kankanhalli, 2010</marker>
<rawString>Pradeep K. Atrey, M. Anwar Hossain, Abdulmotaleb El Saddik, and Mohan S. Kankanhalli. 2010. Multimodal fusion for multimedia analysis: a survey. Multimedia Systems, 16(6):345–379.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014),</booktitle>
<pages>238--247</pages>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="14548" citStr="Baroni et al., 2014" startWordPosition="2428" endWordPosition="2431"> corresponds to the measures for our thesaurus of compounds. It should be note that in the case of the A2ST-comp thesaurus, the number of evaluated entries is very small, restricted to 813 entries, with also a very small number of reference synonyms by entry. Hence, the results of the evaluation of A2ST-comp have to be considered with caution even if their high level for the very first semantic neighbors tends to confirm the positive impact of the low level of ambiguity of compounds compared to mono-terms. The two following rows gives the results of the thesauri built from the best models of (Baroni et al., 2014), B14-count for the count model, whose main parameters are close or identical to ours, and B14-predict for the predict model, built from (Mikolov et al., 2013). These results first illustrate the known importance of corpus size, as the (Baroni et al., 2014)’s corpus is more than 7 times larger than ours, and the fact that for building thesauri, the count model is superior to the predict model. This last observation is confirmed by the results of the skip-gram model of (Mikolov et al., 2013) with its best parameters2 for our corpus (5th row), which clearly exhibits worst results than initial. F</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014. Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014), pages 238–247, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bartosz Broda</author>
<author>Maciej Piasecki</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Rank-Based Transformation in Measuring Semantic Relatedness.</title>
<date>2009</date>
<booktitle>In 22,d Canadian Conference on Artificial Intelligence,</booktitle>
<pages>187--190</pages>
<contexts>
<context position="1211" citStr="Broda et al., 2009" startWordPosition="180" endWordPosition="183">ing to different criteria and show that exploiting the complementary information brought by these criteria leads to significant improvements. 1 Introduction The work presented in this article aims at improving thesauri built following the distributional approach as implemented by (Grefenstette, 1994; Lin, 1998; Curran and Moens, 2002). A part of the work for improving such thesauri focuses on the filtering of the components of the distributional contexts of words (Padr´o et al., 2014; Polajnar and Clark, 2014) or their reweighting, either by turning the weights of these components into ranks (Broda et al., 2009) or by adapting them through a bootstrapping method from the thesaurus to improve (Zhitomirsky-Geffet and Dagan, 2009; Yamamoto and Asakura, 2010). The other part implies more radical changes, including dimensionality reduction methods such as Latent Semantic Analysis (Pad´o and Lapata, 2007), multi-prototype (Reisinger and Mooney, 2010) or exemplar-based models (Erk and Pado, 2010), neural approaches (Huang et al., 2012; Mikolov et al., 2013) or the adoption of a Bayesian viewpoint (Kazama et al., 2010; Dinu and Lapata, 2010). Our work follows (Ferret, 2012), which proposed a different way fr</context>
</contexts>
<marker>Broda, Piasecki, Szpakowicz, 2009</marker>
<rawString>Bartosz Broda, Maciej Piasecki, and Stan Szpakowicz. 2009. Rank-Based Transformation in Measuring Semantic Relatedness. In 22,d Canadian Conference on Artificial Intelligence, pages 187–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines.</title>
<date>2001</date>
<note>http://www.csie.ntu.edu.tw/˜cjlin/ libsvm.</note>
<contexts>
<context position="12911" citStr="Chang and Lin, 2001" startWordPosition="2149" endWordPosition="2152">igher results than the second one but their association produces an interesting compromise between the number of examples, 1,710, and its error rate, 45.7. The fact that the two methods only share 201 noun pairs also illustrates their complementarity. 4.3 Reranking evaluation For our SVM models, we adopted the RBF kernel, as (Hagiwara et al., 2009), and a grid search strategy for optimizing both the γ and C parameters by applying a 5-fold cross validation procedure to our training set and adopting the precision measure as the evaluation function to optimize. The models were built with LIBSVM (Chang and Lin, 2001) and then applied to the neighbors of our initial thesaurus. Table 2 gives the results of the reranking for both the method we propose, compound (comp. for short), with examples selected by rules (1) and 472 (2), and the one of (Ferret, 2012), symmetry. In either case, they correspond to an intrinsic evaluation achieved by comparing the semantic neighbors of each thesaurus entry with the synonyms and related words of our Gold Standard resource for that entry. 12,243 entries with frequency &gt; 10 were present in this resource and evaluated in such a way. As the neighbors are ranked according to t</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM: a library for support vector machines. http://www.csie.ntu.edu.tw/˜cjlin/ libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gordon V Cormack</author>
<author>Charles L A Clarke</author>
<author>Stefan Buettcher</author>
</authors>
<title>Reciprocal rank fusion outperforms condorcet and individual rank learning methods.</title>
<date>2009</date>
<booktitle>In 32,d International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’09),</booktitle>
<pages>758--759</pages>
<contexts>
<context position="18901" citStr="Cormack et al., 2009" startWordPosition="3144" endWordPosition="3147">m that are present in our reference resources are admiration (rank 3) and respect (rank 7). The reranking produces a thesaurus in which these two words appear as the first two neighbors of the entry while its third synonym in WordNet raises from rank 22 to rank 12. Moreover, the number of neighbors among the first 14 ones that are present in Moby increases from 3 to 6. The late fusion strategy relies on the methods used in Information Retrieval for merging ranked lists of retrieved documents. More precisely, we experimented the Borda, Condorcet (Nuray and Can, 2006) and Reciprocal Rank (RRF) (Cormack et al., 2009) fusions based on ranks and the CombSum fusion based on similarity values, normalized in our case with the Zero-one method (Wu et al., 2006). The corresponding thesauri were built by fusing, entry by entry, the lists of neighbors coming from the initial, symmetry and compound thesauri. Table 2 first shows that all the thesauri produced by our ensemble methods outperform our first three thesauri, which confirms that initial, symmetry and compound can bring complementary information, exploited by the fusion. It also shows that our late fusion methods are more effective than our early fusion meth</context>
</contexts>
<marker>Cormack, Clarke, Buettcher, 2009</marker>
<rawString>Gordon V. Cormack, Charles L. A. Clarke, and Stefan Buettcher. 2009. Reciprocal rank fusion outperforms condorcet and individual rank learning methods. In 32,d International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’09), pages 758–759.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Marc Moens</author>
</authors>
<title>Improvements in automatic thesaurus extraction.</title>
<date>2002</date>
<booktitle>In Workshop of the ACL Special Interest Group on the Lexicon (SIGLEX),</booktitle>
<pages>59--66</pages>
<location>Philadelphia, USA.</location>
<contexts>
<context position="928" citStr="Curran and Moens, 2002" startWordPosition="130" endWordPosition="133">uri. Following a bootstrapping perspective, we select relations between the terms of similar nominal compounds for building in an unsupervised way the training set of a classifier performing the reranking of a thesaurus. Then, we evaluate several ways to combine thesauri reranked according to different criteria and show that exploiting the complementary information brought by these criteria leads to significant improvements. 1 Introduction The work presented in this article aims at improving thesauri built following the distributional approach as implemented by (Grefenstette, 1994; Lin, 1998; Curran and Moens, 2002). A part of the work for improving such thesauri focuses on the filtering of the components of the distributional contexts of words (Padr´o et al., 2014; Polajnar and Clark, 2014) or their reweighting, either by turning the weights of these components into ranks (Broda et al., 2009) or by adapting them through a bootstrapping method from the thesaurus to improve (Zhitomirsky-Geffet and Dagan, 2009; Yamamoto and Asakura, 2010). The other part implies more radical changes, including dimensionality reduction methods such as Latent Semantic Analysis (Pad´o and Lapata, 2007), multi-prototype (Reisi</context>
</contexts>
<marker>Curran, Moens, 2002</marker>
<rawString>James R. Curran and Marc Moens. 2002. Improvements in automatic thesaurus extraction. In Workshop of the ACL Special Interest Group on the Lexicon (SIGLEX), pages 59–66, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Curran</author>
</authors>
<title>Ensemble methods for automatic thesaurus extraction.</title>
<date>2002</date>
<booktitle>In 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>222--229</pages>
<contexts>
<context position="16947" citStr="Curran, 2002" startWordPosition="2840" endWordPosition="2841">y outperforms compound but the difference is not significant. This increase is higher for P@{1,5,10} than for R-precision and MAP, which can be explained by the high number of synonyms and related words, 38.7 on average, that an entry of our initial thesaurus has in our reference. Hence, even a significant increase of P@{1,5,10} may have a modest impact on R-precision and MAP as the overall recall, equal to 9.8%, is low. 4.4 Thesaurus fusion Having several thesauri reranked according to different criteria offers the opportunity to apply ensemble methods. Such idea was already experimented in (Curran, 2002) for thesauri built with different parameters (window or syntactic based cooccurrents, etc). We tested more particularly two general strategies for data fusion (Atrey et al., 2010): early and late fusions. The first one consists in our case in fusing the training sets built from our two criteria. As for each criterion, a classifier is then built from the fused training set and applied for reranking the initial thesaurus (see the sym.+comp. row of Table 2). Table 3 illustrates qualitatively the impact of this first strategy for the entry esteem. Its WordNet row gives all the synonyms for this e</context>
</contexts>
<marker>Curran, 2002</marker>
<rawString>James Curran. 2002. Ensemble methods for automatic thesaurus extraction. In 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP 2002), pages 222–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Mirella Lapata</author>
</authors>
<title>Measuring distributional similarity in context.</title>
<date>2010</date>
<booktitle>In 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP 2010),</booktitle>
<pages>1162--1172</pages>
<location>MIT, Massachusetts, USA.</location>
<contexts>
<context position="1743" citStr="Dinu and Lapata, 2010" startWordPosition="261" endWordPosition="264">ighting, either by turning the weights of these components into ranks (Broda et al., 2009) or by adapting them through a bootstrapping method from the thesaurus to improve (Zhitomirsky-Geffet and Dagan, 2009; Yamamoto and Asakura, 2010). The other part implies more radical changes, including dimensionality reduction methods such as Latent Semantic Analysis (Pad´o and Lapata, 2007), multi-prototype (Reisinger and Mooney, 2010) or exemplar-based models (Erk and Pado, 2010), neural approaches (Huang et al., 2012; Mikolov et al., 2013) or the adoption of a Bayesian viewpoint (Kazama et al., 2010; Dinu and Lapata, 2010). Our work follows (Ferret, 2012), which proposed a different way from (Zhitomirsky-Geffet and Dagan, 2009) to exploit bootstrapping by selecting in an unsupervised way a set of semantically similar words from an initial thesaurus and training from them a classifier to rerank the semantic neighbors of the initial thesaurus entries. More precisely, we propose a new criterion for this selection, based on the similarity relations of the components of similar compounds, and we show two modes – early and late – of combination of thesauri reranked from different criteria, including ours, leading to </context>
</contexts>
<marker>Dinu, Lapata, 2010</marker>
<rawString>Georgiana Dinu and Mirella Lapata. 2010. Measuring distributional similarity in context. In 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP 2010), pages 1162–1172, MIT, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pado</author>
</authors>
<title>Exemplar-based models for word meaning in context.</title>
<date>2010</date>
<booktitle>In 481hth Annual Meeting of the Association for Computational Linguistics (ACL 2010), short paper,</booktitle>
<pages>92--97</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="1596" citStr="Erk and Pado, 2010" startWordPosition="236" endWordPosition="239">focuses on the filtering of the components of the distributional contexts of words (Padr´o et al., 2014; Polajnar and Clark, 2014) or their reweighting, either by turning the weights of these components into ranks (Broda et al., 2009) or by adapting them through a bootstrapping method from the thesaurus to improve (Zhitomirsky-Geffet and Dagan, 2009; Yamamoto and Asakura, 2010). The other part implies more radical changes, including dimensionality reduction methods such as Latent Semantic Analysis (Pad´o and Lapata, 2007), multi-prototype (Reisinger and Mooney, 2010) or exemplar-based models (Erk and Pado, 2010), neural approaches (Huang et al., 2012; Mikolov et al., 2013) or the adoption of a Bayesian viewpoint (Kazama et al., 2010; Dinu and Lapata, 2010). Our work follows (Ferret, 2012), which proposed a different way from (Zhitomirsky-Geffet and Dagan, 2009) to exploit bootstrapping by selecting in an unsupervised way a set of semantically similar words from an initial thesaurus and training from them a classifier to rerank the semantic neighbors of the initial thesaurus entries. More precisely, we propose a new criterion for this selection, based on the similarity relations of the components of s</context>
</contexts>
<marker>Erk, Pado, 2010</marker>
<rawString>Katrin Erk and Sebastian Pado. 2010. Exemplar-based models for word meaning in context. In 481hth Annual Meeting of the Association for Computational Linguistics (ACL 2010), short paper, pages 92–97, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Ferret</author>
</authors>
<title>Testing semantic similarity measures for extracting synonyms from a corpus.</title>
<date>2010</date>
<booktitle>In Seventh conference on International Language Resources and Evaluation (LREC’10),</booktitle>
<location>Valletta,</location>
<contexts>
<context position="8668" citStr="Ferret, 2010" startWordPosition="1428" endWordPosition="1429"> higher rank would have guaranteed fewer false negative examples but taking neighbors with a rather small rank for building negative examples is more useful in terms of discrimination. 4 Experiments and evaluation 4.1 Building of distributional thesauri The first step of the work we present is the building of two distributional thesauri: the thesaurus of mono-terms to improve (A2ST) and a thesaurus of compounds (A2ST-comp). Similarly to (Ferret, 2012), they were both built from the AQUAINT-2 corpus, a 380 million-word corpus of news articles in English. The building procedure, defined 471 by (Ferret, 2010), was also identical to (Ferret, 2012), with distributional contexts compared with the Cosine measure and made of window-based lemmatized cooccurrents (1 word before and after) weighted by Positive Pointwise Mutual Information (PPMI). For the thesaurus of compounds, a preprocessing step was added to identify nominal compounds in texts. This identification was done in two steps: first, a set of compounds were extracted from the AQUAINT-2 corpus by relying on a restricted set of morpho-syntactic patterns applied by the Multiword Expression Toolkit (mwetoolkit) (Ramisch et al., 2010); then, the m</context>
</contexts>
<marker>Ferret, 2010</marker>
<rawString>Olivier Ferret. 2010. Testing semantic similarity measures for extracting synonyms from a corpus. In Seventh conference on International Language Resources and Evaluation (LREC’10), Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Ferret</author>
</authors>
<title>Combining bootstrapping and feature selection for improving a distributional thesaurus. In</title>
<date>2012</date>
<booktitle>201h European Conference on Artificial Intelligence (ECAI2012),</booktitle>
<pages>336--341</pages>
<location>Montpellier, France.</location>
<contexts>
<context position="1776" citStr="Ferret, 2012" startWordPosition="268" endWordPosition="269">these components into ranks (Broda et al., 2009) or by adapting them through a bootstrapping method from the thesaurus to improve (Zhitomirsky-Geffet and Dagan, 2009; Yamamoto and Asakura, 2010). The other part implies more radical changes, including dimensionality reduction methods such as Latent Semantic Analysis (Pad´o and Lapata, 2007), multi-prototype (Reisinger and Mooney, 2010) or exemplar-based models (Erk and Pado, 2010), neural approaches (Huang et al., 2012; Mikolov et al., 2013) or the adoption of a Bayesian viewpoint (Kazama et al., 2010; Dinu and Lapata, 2010). Our work follows (Ferret, 2012), which proposed a different way from (Zhitomirsky-Geffet and Dagan, 2009) to exploit bootstrapping by selecting in an unsupervised way a set of semantically similar words from an initial thesaurus and training from them a classifier to rerank the semantic neighbors of the initial thesaurus entries. More precisely, we propose a new criterion for this selection, based on the similarity relations of the components of similar compounds, and we show two modes – early and late – of combination of thesauri reranked from different criteria, including ours, leading to significant further improvements.</context>
<context position="3467" citStr="Ferret, 2012" startWordPosition="540" endWordPosition="541">ther entries. The first issue consists in selecting without supervision a set of positive and negative examples of similar words that represents a good compromise between its error rate and its size. Straightforward solutions such as using the similarity value between an entry and its neighbors or relying on the frequency of entries are not satisfactory in terms of error rate. Hence, we propose in Section 3 a new method, based on the semantic compositionality hypothesis of compounds, for achieving this selection in a more indirect way and show the interest to combine it with the criterion of (Ferret, 2012) for building a large training set with a reasonable error rate. We address the second issue by following (Hagiwara et al., 2009), which defined a Support Vector Machine (SVM) model for deciding whether two words are similar or not. In our context, a positive example is a pair of nouns that are 470 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 470–476, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics semantically similar whi</context>
<context position="8510" citStr="Ferret, 2012" startWordPosition="1402" endWordPosition="1403">ive examples by turning each positive example (A,B) into two negative examples: (A, rank 10 A neighbor) and (B, rank 10 B neighbor). Choosing neighbors with a higher rank would have guaranteed fewer false negative examples but taking neighbors with a rather small rank for building negative examples is more useful in terms of discrimination. 4 Experiments and evaluation 4.1 Building of distributional thesauri The first step of the work we present is the building of two distributional thesauri: the thesaurus of mono-terms to improve (A2ST) and a thesaurus of compounds (A2ST-comp). Similarly to (Ferret, 2012), they were both built from the AQUAINT-2 corpus, a 380 million-word corpus of news articles in English. The building procedure, defined 471 by (Ferret, 2010), was also identical to (Ferret, 2012), with distributional contexts compared with the Cosine measure and made of window-based lemmatized cooccurrents (1 word before and after) weighted by Positive Pointwise Mutual Information (PPMI). For the thesaurus of compounds, a preprocessing step was added to identify nominal compounds in texts. This identification was done in two steps: first, a set of compounds were extracted from the AQUAINT-2 c</context>
<context position="10740" citStr="Ferret, 2012" startWordPosition="1767" endWordPosition="1768">ble 2). As the selection method makes the definition of a development set quite difficult, the values of these two parameters were chosen in a conservative way. Table 1 gives for each rule and two combinations of them the number of selected positive examples (#pos. ex.) and the percentage of positive (%good pos.) and negative examples (%bad neg.) found in our Gold Standard resource for thesaurus evaluation. This resource results from the union of the synonyms of WordNet 3.0 and the associated words of the Moby thesaurus. Table 1 also gives the same data for examples selected by the method of (Ferret, 2012) (symmetry row, sym. for short), based on the fact that as similarity relations are 1Longest-match strategy: if C1 is a reference compound that is part of a reference compound C2, the identification of an occurrence of C2 blocks out the identification of the associated occurrence of C1. method %good pos. %bad neg. #pos. ex. symmetry 59.7 12.4 796 56.9 16.1 921 44.7 14.7 308 46.2 16.9 40 rules (1,2) 53.0 16.1 1,115 rules (1,2,3) 52.4 15.9 1,131 sym. + (1,2) 54.3 15.0 1,710 sym. + (1,2,3) 53.9 14.5 1,725 Table 1: Selection of examples. symmetric, a pair of words (A,B) are more likely to be simil</context>
<context position="13153" citStr="Ferret, 2012" startWordPosition="2195" endWordPosition="2196">y. 4.3 Reranking evaluation For our SVM models, we adopted the RBF kernel, as (Hagiwara et al., 2009), and a grid search strategy for optimizing both the γ and C parameters by applying a 5-fold cross validation procedure to our training set and adopting the precision measure as the evaluation function to optimize. The models were built with LIBSVM (Chang and Lin, 2001) and then applied to the neighbors of our initial thesaurus. Table 2 gives the results of the reranking for both the method we propose, compound (comp. for short), with examples selected by rules (1) and 472 (2), and the one of (Ferret, 2012), symmetry. In either case, they correspond to an intrinsic evaluation achieved by comparing the semantic neighbors of each thesaurus entry with the synonyms and related words of our Gold Standard resource for that entry. 12,243 entries with frequency &gt; 10 were present in this resource and evaluated in such a way. As the neighbors are ranked according to their similarity value with their entry, we adopted the classical evaluation measures of Information Retrieval by replacing documents with synonyms and queries with entries: R-precision (R-prec.), Mean Average Precision (MAP) and precision at </context>
</contexts>
<marker>Ferret, 2012</marker>
<rawString>Olivier Ferret. 2012. Combining bootstrapping and feature selection for improving a distributional thesaurus. In 201h European Conference on Artificial Intelligence (ECAI2012), pages 336–341, Montpellier, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Ferret</author>
</authors>
<title>Identifying bad semantic neighbors for improving distributional thesauri.</title>
<date>2013</date>
<booktitle>In 51s1 Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>561--571</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="20435" citStr="Ferret, 2013" startWordPosition="3403" endWordPosition="3404">t RRF is clearly superior to condorcet but only weakly superior to borda. Finally, the last row of Table 2 – CS-w-Mik – illustrates one step further the interest of ensemble methods for distributional thesauri: whereas the “Mikolov thesaurus” gets the worst results among all the thesauri of Table 2, adding it to the initial, symmetry and compound thesauri in the CombSum method leads to improve both R-precision and MAP, with a only small decrease of P@1 and P@5. From a more global perspective, it is interesting to note that our best method, CombSum, clearly outperforms the reranking method of (Ferret, 2013) with the same initial starting point. 5 Conclusion and perspectives In this article, we have presented a method based on bootstrapping for improving distributional thesauri. More precisely, we have proposed a new criterion, based on the relations of mono-terms in similar compounds, for the unsupervised selection of training examples used for reranking the semantic neighbors of a thesaurus. We have evaluated two different strategies for combining this criterion with an already existing one and showed that a late fusion approach based on the merging of lists of neighbors is particularly effecti</context>
</contexts>
<marker>Ferret, 2013</marker>
<rawString>Olivier Ferret. 2013. Identifying bad semantic neighbors for improving distributional thesauri. In 51s1 Annual Meeting of the Association for Computational Linguistics (ACL 2013), pages 561–571, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in automatic thesaurus discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="892" citStr="Grefenstette, 1994" startWordPosition="126" endWordPosition="127"> improving distributional thesauri. Following a bootstrapping perspective, we select relations between the terms of similar nominal compounds for building in an unsupervised way the training set of a classifier performing the reranking of a thesaurus. Then, we evaluate several ways to combine thesauri reranked according to different criteria and show that exploiting the complementary information brought by these criteria leads to significant improvements. 1 Introduction The work presented in this article aims at improving thesauri built following the distributional approach as implemented by (Grefenstette, 1994; Lin, 1998; Curran and Moens, 2002). A part of the work for improving such thesauri focuses on the filtering of the components of the distributional contexts of words (Padr´o et al., 2014; Polajnar and Clark, 2014) or their reweighting, either by turning the weights of these components into ranks (Broda et al., 2009) or by adapting them through a bootstrapping method from the thesaurus to improve (Zhitomirsky-Geffet and Dagan, 2009; Yamamoto and Asakura, 2010). The other part implies more radical changes, including dimensionality reduction methods such as Latent Semantic Analysis (Pad´o and L</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Explorations in automatic thesaurus discovery. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masato Hagiwara</author>
<author>Yasuhiro Ogawa</author>
<author>Katsuhiko Toyama</author>
</authors>
<title>Supervised synonym acquisition using distributional features and syntactic patterns.</title>
<date>2009</date>
<journal>Information and Media Technologies,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="3596" citStr="Hagiwara et al., 2009" startWordPosition="561" endWordPosition="564">ar words that represents a good compromise between its error rate and its size. Straightforward solutions such as using the similarity value between an entry and its neighbors or relying on the frequency of entries are not satisfactory in terms of error rate. Hence, we propose in Section 3 a new method, based on the semantic compositionality hypothesis of compounds, for achieving this selection in a more indirect way and show the interest to combine it with the criterion of (Ferret, 2012) for building a large training set with a reasonable error rate. We address the second issue by following (Hagiwara et al., 2009), which defined a Support Vector Machine (SVM) model for deciding whether two words are similar or not. In our context, a positive example is a pair of nouns that are 470 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 470–476, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics semantically similar while a negative example is a pair of non similar nouns. The features of each pair of nouns are built by summing the weights of the </context>
<context position="5308" citStr="Hagiwara et al., 2009" startWordPosition="852" endWordPosition="855">are reranked according to the value of the decision function computed for each neighbor by the SVM model. 3 Unsupervised example selection The evaluation of distributional thesauri shows that a true semantic neighbor is more likely to be found when the thesaurus entry is a high frequency noun and the neighbor has a low rank. However, relying only on these two criteria doesn’t lead to a good enough set of positive examples. For instance, taking as positive examples from the initial thesaurus of Section 4 the first neighbor of its 2,148 most frequent entries, the number of positive examples of (Hagiwara et al., 2009), only leads to 44.3% of correct examples. Moreover, this percentage exceeds 50% only when the number of examples is less than 654, which represents a very small training set for this kind of task. Hence, we propose a more selective approach for choosing positive examples among high frequency nouns to get a more balanced solution between the number of examples and their error rate. This approach exploits a form of semantic compositionality hypothesis of compounds. While much work has been done recently for defining the distributional representation of compounds by composing the distributional </context>
<context position="12641" citStr="Hagiwara et al., 2009" startWordPosition="2102" endWordPosition="2105">e modifier than the modifiers of two semantically linked compounds having the same head. This confirms our expectation that the head of a compound is more related to the meaning of the compound than its modifier. More globally, Table 1 shows that the symmetry method has higher results than the second one but their association produces an interesting compromise between the number of examples, 1,710, and its error rate, 45.7. The fact that the two methods only share 201 noun pairs also illustrates their complementarity. 4.3 Reranking evaluation For our SVM models, we adopted the RBF kernel, as (Hagiwara et al., 2009), and a grid search strategy for optimizing both the γ and C parameters by applying a 5-fold cross validation procedure to our training set and adopting the precision measure as the evaluation function to optimize. The models were built with LIBSVM (Chang and Lin, 2001) and then applied to the neighbors of our initial thesaurus. Table 2 gives the results of the reranking for both the method we propose, compound (comp. for short), with examples selected by rules (1) and 472 (2), and the one of (Ferret, 2012), symmetry. In either case, they correspond to an intrinsic evaluation achieved by compa</context>
</contexts>
<marker>Hagiwara, Ogawa, Toyama, 2009</marker>
<rawString>Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko Toyama. 2009. Supervised synonym acquisition using distributional features and syntactic patterns. Information and Media Technologies, 4(2):59–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In 50th Annual Meeting of the Association for Computational Linguistics (ACL’12),</booktitle>
<pages>873--882</pages>
<contexts>
<context position="1635" citStr="Huang et al., 2012" startWordPosition="242" endWordPosition="245">ts of the distributional contexts of words (Padr´o et al., 2014; Polajnar and Clark, 2014) or their reweighting, either by turning the weights of these components into ranks (Broda et al., 2009) or by adapting them through a bootstrapping method from the thesaurus to improve (Zhitomirsky-Geffet and Dagan, 2009; Yamamoto and Asakura, 2010). The other part implies more radical changes, including dimensionality reduction methods such as Latent Semantic Analysis (Pad´o and Lapata, 2007), multi-prototype (Reisinger and Mooney, 2010) or exemplar-based models (Erk and Pado, 2010), neural approaches (Huang et al., 2012; Mikolov et al., 2013) or the adoption of a Bayesian viewpoint (Kazama et al., 2010; Dinu and Lapata, 2010). Our work follows (Ferret, 2012), which proposed a different way from (Zhitomirsky-Geffet and Dagan, 2009) to exploit bootstrapping by selecting in an unsupervised way a set of semantically similar words from an initial thesaurus and training from them a classifier to rerank the semantic neighbors of the initial thesaurus entries. More precisely, we propose a new criterion for this selection, based on the similarity relations of the components of similar compounds, and we show two modes</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. Improving word representations via global context and multiple word prototypes. In 50th Annual Meeting of the Association for Computational Linguistics (ACL’12), pages 873–882.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Stijn De Saeger</author>
<author>Kow Kuroda</author>
<author>Masaki Murata</author>
<author>Kentaro Torisawa</author>
</authors>
<title>A bayesian method for robust estimation of distributional similarities.</title>
<date>2010</date>
<booktitle>In 481h Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>247--256</pages>
<location>Uppsala,</location>
<marker>Kazama, De Saeger, Kuroda, Murata, Torisawa, 2010</marker>
<rawString>Jun’ichi Kazama, Stijn De Saeger, Kow Kuroda, Masaki Murata, and Kentaro Torisawa. 2010. A bayesian method for robust estimation of distributional similarities. In 481h Annual Meeting of the Association for Computational Linguistics, pages 247–256, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In 171h International Conference on Computational Linguistics and 361h Annual Meeting of the Association for Computational Linguistics (ACL-COLING’98),</booktitle>
<pages>768--774</pages>
<location>Montral, Canada.</location>
<contexts>
<context position="903" citStr="Lin, 1998" startWordPosition="128" endWordPosition="129">ional thesauri. Following a bootstrapping perspective, we select relations between the terms of similar nominal compounds for building in an unsupervised way the training set of a classifier performing the reranking of a thesaurus. Then, we evaluate several ways to combine thesauri reranked according to different criteria and show that exploiting the complementary information brought by these criteria leads to significant improvements. 1 Introduction The work presented in this article aims at improving thesauri built following the distributional approach as implemented by (Grefenstette, 1994; Lin, 1998; Curran and Moens, 2002). A part of the work for improving such thesauri focuses on the filtering of the components of the distributional contexts of words (Padr´o et al., 2014; Polajnar and Clark, 2014) or their reweighting, either by turning the weights of these components into ranks (Broda et al., 2009) or by adapting them through a bootstrapping method from the thesaurus to improve (Zhitomirsky-Geffet and Dagan, 2009; Yamamoto and Asakura, 2010). The other part implies more radical changes, including dimensionality reduction methods such as Latent Semantic Analysis (Pad´o and Lapata, 2007</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In 171h International Conference on Computational Linguistics and 361h Annual Meeting of the Association for Computational Linguistics (ACL-COLING’98), pages 768–774, Montral, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT 2013),</booktitle>
<pages>746--751</pages>
<location>Atlanta,</location>
<contexts>
<context position="1658" citStr="Mikolov et al., 2013" startWordPosition="246" endWordPosition="249">onal contexts of words (Padr´o et al., 2014; Polajnar and Clark, 2014) or their reweighting, either by turning the weights of these components into ranks (Broda et al., 2009) or by adapting them through a bootstrapping method from the thesaurus to improve (Zhitomirsky-Geffet and Dagan, 2009; Yamamoto and Asakura, 2010). The other part implies more radical changes, including dimensionality reduction methods such as Latent Semantic Analysis (Pad´o and Lapata, 2007), multi-prototype (Reisinger and Mooney, 2010) or exemplar-based models (Erk and Pado, 2010), neural approaches (Huang et al., 2012; Mikolov et al., 2013) or the adoption of a Bayesian viewpoint (Kazama et al., 2010; Dinu and Lapata, 2010). Our work follows (Ferret, 2012), which proposed a different way from (Zhitomirsky-Geffet and Dagan, 2009) to exploit bootstrapping by selecting in an unsupervised way a set of semantically similar words from an initial thesaurus and training from them a classifier to rerank the semantic neighbors of the initial thesaurus entries. More precisely, we propose a new criterion for this selection, based on the similarity relations of the components of similar compounds, and we show two modes – early and late – of </context>
<context position="14707" citStr="Mikolov et al., 2013" startWordPosition="2454" endWordPosition="2457"> very small, restricted to 813 entries, with also a very small number of reference synonyms by entry. Hence, the results of the evaluation of A2ST-comp have to be considered with caution even if their high level for the very first semantic neighbors tends to confirm the positive impact of the low level of ambiguity of compounds compared to mono-terms. The two following rows gives the results of the thesauri built from the best models of (Baroni et al., 2014), B14-count for the count model, whose main parameters are close or identical to ours, and B14-predict for the predict model, built from (Mikolov et al., 2013). These results first illustrate the known importance of corpus size, as the (Baroni et al., 2014)’s corpus is more than 7 times larger than ours, and the fact that for building thesauri, the count model is superior to the predict model. This last observation is confirmed by the results of the skip-gram model of (Mikolov et al., 2013) with its best parameters2 for our corpus (5th row), which clearly exhibits worst results than initial. For this Mikolov thesaurus and the following reranked ones, each value corresponds to the difference between the measure for the considered thesaurus and the me</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word representations. In 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT 2013), pages 746–751, Atlanta, Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="5970" citStr="Mitchell and Lapata, 2010" startWordPosition="959" endWordPosition="962">ples. Moreover, this percentage exceeds 50% only when the number of examples is less than 654, which represents a very small training set for this kind of task. Hence, we propose a more selective approach for choosing positive examples among high frequency nouns to get a more balanced solution between the number of examples and their error rate. This approach exploits a form of semantic compositionality hypothesis of compounds. While much work has been done recently for defining the distributional representation of compounds by composing the distributional representations of their components (Mitchell and Lapata, 2010; Paperno et al., 2014), we adopt a kind of reverse viewpoint by exploiting the possibility to link the meaning of a compound to the meaning of its components. More precisely, we assume that the mono-terms of two semantically related compounds with the same syntactic role in their compound are likely to be semantically linked themselves. In this work, we only consider compounds having one of these three term structures (with their percentage of the vocabulary of compounds): (a) &lt;noun&gt;mod &lt;noun&gt;head (30) (b) &lt;adjective&gt;mod &lt;noun&gt;head (58) (c) &lt;noun&gt;head &lt;preposition&gt;&lt;noun&gt;mod (12) Each compound</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeffrey Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rabia Nuray</author>
<author>Fazli Can</author>
</authors>
<title>Automatic ranking of information retrieval systems using data fusion.</title>
<date>2006</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>42--3</pages>
<contexts>
<context position="18852" citStr="Nuray and Can, 2006" startWordPosition="3136" endWordPosition="3139">egy. thesaurus, the first two neighbors of esteem that are present in our reference resources are admiration (rank 3) and respect (rank 7). The reranking produces a thesaurus in which these two words appear as the first two neighbors of the entry while its third synonym in WordNet raises from rank 22 to rank 12. Moreover, the number of neighbors among the first 14 ones that are present in Moby increases from 3 to 6. The late fusion strategy relies on the methods used in Information Retrieval for merging ranked lists of retrieved documents. More precisely, we experimented the Borda, Condorcet (Nuray and Can, 2006) and Reciprocal Rank (RRF) (Cormack et al., 2009) fusions based on ranks and the CombSum fusion based on similarity values, normalized in our case with the Zero-one method (Wu et al., 2006). The corresponding thesauri were built by fusing, entry by entry, the lists of neighbors coming from the initial, symmetry and compound thesauri. Table 2 first shows that all the thesauri produced by our ensemble methods outperform our first three thesauri, which confirms that initial, symmetry and compound can bring complementary information, exploited by the fusion. It also shows that our late fusion meth</context>
</contexts>
<marker>Nuray, Can, 2006</marker>
<rawString>Rabia Nuray and Fazli Can. 2006. Automatic ranking of information retrieval systems using data fusion. Information Processing and Management, 42(3):595–614.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Muntsa Padr´o</author>
<author>Marco Idiart</author>
<author>Aline Villavicencio</author>
<author>Carlos Ramisch</author>
</authors>
<title>Nothing like good old frequency: Studying context filters for distributional thesauri.</title>
<date>2014</date>
<booktitle>In 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014),</booktitle>
<pages>419--424</pages>
<location>Doha, Qatar.</location>
<marker>Padr´o, Idiart, Villavicencio, Ramisch, 2014</marker>
<rawString>Muntsa Padr´o, Marco Idiart, Aline Villavicencio, and Carlos Ramisch. 2014. Nothing like good old frequency: Studying context filters for distributional thesauri. In 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014), pages 419–424, Doha, Qatar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denis Paperno</author>
<author>Nghia The Pham</author>
<author>Marco Baroni</author>
</authors>
<title>A practical and linguistically-motivated approach to compositional distributional semantics.</title>
<date>2014</date>
<booktitle>In 52,d Annual Meeting of the Association for Computational Linguistics (ACL 2014),</booktitle>
<pages>90--99</pages>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="5993" citStr="Paperno et al., 2014" startWordPosition="963" endWordPosition="966">tage exceeds 50% only when the number of examples is less than 654, which represents a very small training set for this kind of task. Hence, we propose a more selective approach for choosing positive examples among high frequency nouns to get a more balanced solution between the number of examples and their error rate. This approach exploits a form of semantic compositionality hypothesis of compounds. While much work has been done recently for defining the distributional representation of compounds by composing the distributional representations of their components (Mitchell and Lapata, 2010; Paperno et al., 2014), we adopt a kind of reverse viewpoint by exploiting the possibility to link the meaning of a compound to the meaning of its components. More precisely, we assume that the mono-terms of two semantically related compounds with the same syntactic role in their compound are likely to be semantically linked themselves. In this work, we only consider compounds having one of these three term structures (with their percentage of the vocabulary of compounds): (a) &lt;noun&gt;mod &lt;noun&gt;head (30) (b) &lt;adjective&gt;mod &lt;noun&gt;head (58) (c) &lt;noun&gt;head &lt;preposition&gt;&lt;noun&gt;mod (12) Each compound CZ is represented as a</context>
</contexts>
<marker>Paperno, Pham, Baroni, 2014</marker>
<rawString>Denis Paperno, Nghia The Pham, and Marco Baroni. 2014. A practical and linguistically-motivated approach to compositional distributional semantics. In 52,d Annual Meeting of the Association for Computational Linguistics (ACL 2014), pages 90–99, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamara Polajnar</author>
<author>Stephen Clark</author>
</authors>
<title>Improving distributional semantic vectors through context selection and normalisation.</title>
<date>2014</date>
<booktitle>In 141h Conference of the European Chapter of the Association for Computational Linguistics (EACL 2014),</booktitle>
<pages>230--238</pages>
<location>Gothenburg,</location>
<contexts>
<context position="1107" citStr="Polajnar and Clark, 2014" startWordPosition="161" endWordPosition="165">er performing the reranking of a thesaurus. Then, we evaluate several ways to combine thesauri reranked according to different criteria and show that exploiting the complementary information brought by these criteria leads to significant improvements. 1 Introduction The work presented in this article aims at improving thesauri built following the distributional approach as implemented by (Grefenstette, 1994; Lin, 1998; Curran and Moens, 2002). A part of the work for improving such thesauri focuses on the filtering of the components of the distributional contexts of words (Padr´o et al., 2014; Polajnar and Clark, 2014) or their reweighting, either by turning the weights of these components into ranks (Broda et al., 2009) or by adapting them through a bootstrapping method from the thesaurus to improve (Zhitomirsky-Geffet and Dagan, 2009; Yamamoto and Asakura, 2010). The other part implies more radical changes, including dimensionality reduction methods such as Latent Semantic Analysis (Pad´o and Lapata, 2007), multi-prototype (Reisinger and Mooney, 2010) or exemplar-based models (Erk and Pado, 2010), neural approaches (Huang et al., 2012; Mikolov et al., 2013) or the adoption of a Bayesian viewpoint (Kazama </context>
</contexts>
<marker>Polajnar, Clark, 2014</marker>
<rawString>Tamara Polajnar and Stephen Clark. 2014. Improving distributional semantic vectors through context selection and normalisation. In 141h Conference of the European Chapter of the Association for Computational Linguistics (EACL 2014), pages 230–238, Gothenburg, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Ramisch</author>
<author>Aline Villavicencio</author>
<author>Christian Boitet</author>
</authors>
<title>mwetoolkit: a Framework for Multiword Expression Identification.</title>
<date>2010</date>
<booktitle>In Seventh International Conference on Language Resources and Evaluation (LREC 2010),</booktitle>
<location>Valetta, Malta,</location>
<contexts>
<context position="9255" citStr="Ramisch et al., 2010" startWordPosition="1517" endWordPosition="1520">ure, defined 471 by (Ferret, 2010), was also identical to (Ferret, 2012), with distributional contexts compared with the Cosine measure and made of window-based lemmatized cooccurrents (1 word before and after) weighted by Positive Pointwise Mutual Information (PPMI). For the thesaurus of compounds, a preprocessing step was added to identify nominal compounds in texts. This identification was done in two steps: first, a set of compounds were extracted from the AQUAINT-2 corpus by relying on a restricted set of morpho-syntactic patterns applied by the Multiword Expression Toolkit (mwetoolkit) (Ramisch et al., 2010); then, the most frequent compounds in this set (frequency &gt; 100) were selected as reference and their occurrences in the AQUAINT-2 corpus were identified by applying the longest-match strategy to the output of the TreeTagger part-of-speech tagger (Schmid, 1994)1. Finally, distributional contexts made of mono-terms and compounds were built as stated above and neighbors were found for 29,174 compounds. 4.2 Example selection We applied the three rules of Section 3 with all the entries of our thesaurus of compounds and the upper half in frequency of our mono-term entries. For mono-terms, we only </context>
</contexts>
<marker>Ramisch, Villavicencio, Boitet, 2010</marker>
<rawString>Carlos Ramisch, Aline Villavicencio, and Christian Boitet. 2010. mwetoolkit: a Framework for Multiword Expression Identification. In Seventh International Conference on Language Resources and Evaluation (LREC 2010), Valetta, Malta, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Raymond J Mooney</author>
</authors>
<title>Multi-prototype vector-space models of word meaning.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2010),</booktitle>
<pages>109--117</pages>
<location>Los Angeles, California,</location>
<contexts>
<context position="1550" citStr="Reisinger and Mooney, 2010" startWordPosition="229" endWordPosition="232">2002). A part of the work for improving such thesauri focuses on the filtering of the components of the distributional contexts of words (Padr´o et al., 2014; Polajnar and Clark, 2014) or their reweighting, either by turning the weights of these components into ranks (Broda et al., 2009) or by adapting them through a bootstrapping method from the thesaurus to improve (Zhitomirsky-Geffet and Dagan, 2009; Yamamoto and Asakura, 2010). The other part implies more radical changes, including dimensionality reduction methods such as Latent Semantic Analysis (Pad´o and Lapata, 2007), multi-prototype (Reisinger and Mooney, 2010) or exemplar-based models (Erk and Pado, 2010), neural approaches (Huang et al., 2012; Mikolov et al., 2013) or the adoption of a Bayesian viewpoint (Kazama et al., 2010; Dinu and Lapata, 2010). Our work follows (Ferret, 2012), which proposed a different way from (Zhitomirsky-Geffet and Dagan, 2009) to exploit bootstrapping by selecting in an unsupervised way a set of semantically similar words from an initial thesaurus and training from them a classifier to rerank the semantic neighbors of the initial thesaurus entries. More precisely, we propose a new criterion for this selection, based on t</context>
</contexts>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>Joseph Reisinger and Raymond J. Mooney. 2010. Multi-prototype vector-space models of word meaning. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2010), pages 109–117, Los Angeles, California, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In International Conference on New Methods in Language Processing.</booktitle>
<contexts>
<context position="9517" citStr="Schmid, 1994" startWordPosition="1560" endWordPosition="1561">or the thesaurus of compounds, a preprocessing step was added to identify nominal compounds in texts. This identification was done in two steps: first, a set of compounds were extracted from the AQUAINT-2 corpus by relying on a restricted set of morpho-syntactic patterns applied by the Multiword Expression Toolkit (mwetoolkit) (Ramisch et al., 2010); then, the most frequent compounds in this set (frequency &gt; 100) were selected as reference and their occurrences in the AQUAINT-2 corpus were identified by applying the longest-match strategy to the output of the TreeTagger part-of-speech tagger (Schmid, 1994)1. Finally, distributional contexts made of mono-terms and compounds were built as stated above and neighbors were found for 29,174 compounds. 4.2 Example selection We applied the three rules of Section 3 with all the entries of our thesaurus of compounds and the upper half in frequency of our mono-term entries. For mono-terms, we only took the first neighbor (m = 1) of each entry because of the rather low performance of the initial thesaurus while for compounds, a larger value (c = 3) was chosen for enlarging the number of selected examples since neighbors were globally more reliable (see res</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In International Conference on New Methods in Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shengli Wu</author>
<author>Fabio Crestani</author>
<author>Yaxin Bi</author>
</authors>
<title>Evaluating score normalization methods in data fusion.</title>
<date>2006</date>
<booktitle>In Third Asia Conference on Information Retrieval Technology (AIRS’06),</booktitle>
<pages>642--648</pages>
<publisher>SpringerVerlag.</publisher>
<contexts>
<context position="19041" citStr="Wu et al., 2006" startWordPosition="3169" endWordPosition="3172"> words appear as the first two neighbors of the entry while its third synonym in WordNet raises from rank 22 to rank 12. Moreover, the number of neighbors among the first 14 ones that are present in Moby increases from 3 to 6. The late fusion strategy relies on the methods used in Information Retrieval for merging ranked lists of retrieved documents. More precisely, we experimented the Borda, Condorcet (Nuray and Can, 2006) and Reciprocal Rank (RRF) (Cormack et al., 2009) fusions based on ranks and the CombSum fusion based on similarity values, normalized in our case with the Zero-one method (Wu et al., 2006). The corresponding thesauri were built by fusing, entry by entry, the lists of neighbors coming from the initial, symmetry and compound thesauri. Table 2 first shows that all the thesauri produced by our ensemble methods outperform our first three thesauri, which confirms that initial, symmetry and compound can bring complementary information, exploited by the fusion. It also shows that our late fusion methods are more effective than our early fusion method. However, no specific element advocates at this stage for a generalization of this observation. The evaluation reported by Table 2 also s</context>
</contexts>
<marker>Wu, Crestani, Bi, 2006</marker>
<rawString>Shengli Wu, Fabio Crestani, and Yaxin Bi. 2006. Evaluating score normalization methods in data fusion. In Third Asia Conference on Information Retrieval Technology (AIRS’06), pages 642–648. SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuhide Yamamoto</author>
<author>Takeshi Asakura</author>
</authors>
<title>Even unassociated features can improve lexical distributional similarity.</title>
<date>2010</date>
<booktitle>In Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010),</booktitle>
<pages>32--39</pages>
<location>Beijing, China.</location>
<contexts>
<context position="1357" citStr="Yamamoto and Asakura, 2010" startWordPosition="202" endWordPosition="205">ts. 1 Introduction The work presented in this article aims at improving thesauri built following the distributional approach as implemented by (Grefenstette, 1994; Lin, 1998; Curran and Moens, 2002). A part of the work for improving such thesauri focuses on the filtering of the components of the distributional contexts of words (Padr´o et al., 2014; Polajnar and Clark, 2014) or their reweighting, either by turning the weights of these components into ranks (Broda et al., 2009) or by adapting them through a bootstrapping method from the thesaurus to improve (Zhitomirsky-Geffet and Dagan, 2009; Yamamoto and Asakura, 2010). The other part implies more radical changes, including dimensionality reduction methods such as Latent Semantic Analysis (Pad´o and Lapata, 2007), multi-prototype (Reisinger and Mooney, 2010) or exemplar-based models (Erk and Pado, 2010), neural approaches (Huang et al., 2012; Mikolov et al., 2013) or the adoption of a Bayesian viewpoint (Kazama et al., 2010; Dinu and Lapata, 2010). Our work follows (Ferret, 2012), which proposed a different way from (Zhitomirsky-Geffet and Dagan, 2009) to exploit bootstrapping by selecting in an unsupervised way a set of semantically similar words from an i</context>
</contexts>
<marker>Yamamoto, Asakura, 2010</marker>
<rawString>Kazuhide Yamamoto and Takeshi Asakura. 2010. Even unassociated features can improve lexical distributional similarity. In Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 32–39, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maayan Zhitomirsky-Geffet</author>
<author>Ido Dagan</author>
</authors>
<title>Bootstrapping Distributional Feature Vector Quality.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="1328" citStr="Zhitomirsky-Geffet and Dagan, 2009" startWordPosition="197" endWordPosition="201">eria leads to significant improvements. 1 Introduction The work presented in this article aims at improving thesauri built following the distributional approach as implemented by (Grefenstette, 1994; Lin, 1998; Curran and Moens, 2002). A part of the work for improving such thesauri focuses on the filtering of the components of the distributional contexts of words (Padr´o et al., 2014; Polajnar and Clark, 2014) or their reweighting, either by turning the weights of these components into ranks (Broda et al., 2009) or by adapting them through a bootstrapping method from the thesaurus to improve (Zhitomirsky-Geffet and Dagan, 2009; Yamamoto and Asakura, 2010). The other part implies more radical changes, including dimensionality reduction methods such as Latent Semantic Analysis (Pad´o and Lapata, 2007), multi-prototype (Reisinger and Mooney, 2010) or exemplar-based models (Erk and Pado, 2010), neural approaches (Huang et al., 2012; Mikolov et al., 2013) or the adoption of a Bayesian viewpoint (Kazama et al., 2010; Dinu and Lapata, 2010). Our work follows (Ferret, 2012), which proposed a different way from (Zhitomirsky-Geffet and Dagan, 2009) to exploit bootstrapping by selecting in an unsupervised way a set of semanti</context>
</contexts>
<marker>Zhitomirsky-Geffet, Dagan, 2009</marker>
<rawString>Maayan Zhitomirsky-Geffet and Ido Dagan. 2009. Bootstrapping Distributional Feature Vector Quality. Computational Linguistics, 35(3):435–461.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>