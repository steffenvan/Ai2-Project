<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.8438825">
SPEECH DIALOGUE WITH FACIAL DISPLAYS:
MULTIMODAL HUMAN-COMPUTER CONVERSATION
</title>
<author confidence="0.880818">
Katashi Nagao and Akikazu Takeuchi
</author>
<affiliation confidence="0.841403">
Sony Computer Science Laboratory Inc.
</affiliation>
<address confidence="0.902276">
3-14-13 Higashi-got anda, Shinagawa-ku, Tokyo 141, Japan
</address>
<email confidence="0.988202">
E-mail: {nagao,takeuchi}Ocsl.sony.co.jp
</email>
<sectionHeader confidence="0.997156" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998565">
Human face-to-face conversation is an ideal model
for human-computer dialogue. One of the major
features of face-to-face communication is its multi-
plicity of communication channels that act on mul-
tiple modalities. To realize a natural multimodal
dialogue, it is necessary to study how humans per-
ceive information and determine the information
to which humans are sensitive. A face is an in-
dependent communication channel that conveys
emotional and conversational signals, encoded as
facial expressions. We have developed an experi-
mental system that integrates speech dialogue and
facial animation, to investigate the effect of intro-
ducing communicative facial expressions as a new
modality in human-computer conversation. Our
experiments have showen that facial expressions
are helpful, especially upon first contact with the
system. We have also discovered that featuring
facial expressions at an early stage improves sub-
sequent interaction.
</bodyText>
<sectionHeader confidence="0.979022" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999950517857143">
Human face-to-face conversation is an ideal model
for human-computer dialogue. One of the major
features of face-to-face communication is its mul-
tiplicity of communication channels that act on
multiple modalities. A channel is a communica-
tion medium associated with a particular encod-
ing method. Examples are the auditory channel
(carrying speech) and the visual channel (carry-
ing facial expressions). A modality is the sense
used to perceive signals from the outside world.
Many researchers have been developing mul-
timodal dialogue systems. In some cases, re-
searchers have shown that information in one
channel complements or modifies information in
another. As a simple example, the phrase &amp;quot;delete
it&amp;quot; involves the coordination of voice with ges-
ture. Neither makes sense without the other. Re-
searchers have also noticed that nonverbal (ges-
ture or gaze) information plays a role in set-
ting the situational context which is useful in re-
stricting the hypothesis space constructed dur-
ing language processing. Anthropomorphic inter-
faces present another approach to multimodal di-
alogues. An anthropomorphic interface, such as
Guides [Don et al., 1991], provides a means to
realize a new style of interaction. Such research
attempts to computationally capture the commu-
nicative power of the human face and apply it to
human-computer dialogue.
Our research is closely related to the last ap-
proach. The aim of this research is to improve
human-computer dialogue by introducing human-
like behavior into a speech dialogue system. Such
behavior will include factors such as facial expres-
sions and head and eye movement. It will help to
reduce any stress experienced by users of comput-
ing systems, lowering the complexity associated
with understanding system status.
Like most dialogue systems developed by nat-
ural language researchers, our current system can
handle domain-dependent, information-seeking di-
alogues. Of course, the system encounters prob-
lems with ambiguity and missing information (i.e.,
anaphora and ellipsis). The system tries to re-
solve them using techniques from natural language
understanding (e.g., constraint-based, case-based.
and plan-based methods). We are also studying
the use of synergic multimodality to resolve lin-
guistic problems, as in conventional multimodal
systems. This work will be reported in a separate
publication.
In this paper, we concentrate on the role
of â€¢nonverbal modality for increasing flexibility of
human-computer dialogue and reducing the men-
tal barriers that many users associate with com-
puter systems.
</bodyText>
<sectionHeader confidence="0.768118" genericHeader="method">
Research Overview of Multimodal
Dialogues
</sectionHeader>
<bodyText confidence="0.972315">
Multimodal dialogues that combine verbal and
nonverbal communication have been pursued
</bodyText>
<page confidence="0.997368">
102
</page>
<bodyText confidence="0.880062">
mainly from the following three viewpoints.
</bodyText>
<listItem confidence="0.9863255">
1. Combining direct manipulation with natural lan-
guage (deictic) expressions
</listItem>
<bodyText confidence="0.999382">
&amp;quot;Direct manipulation (DM)&amp;quot; was suggested by
Shneiderman [1983]. The user can interact di-
rectly with graphical objects displayed on the
computer screen with rapid, incremental, re-
versible operations whose effects on the objects
of interest are immediately visible.
The semantics of natural language (NL) ex-
pressions is anchored to real-world objects and
events by means of pointing and demonstrating
actions and deictic expressions such as &amp;quot;this,&amp;quot;
&amp;quot;that,&amp;quot; &amp;quot;here,&amp;quot; &amp;quot;there,&amp;quot; &amp;quot;then,&amp;quot; and &amp;quot;now.&amp;quot;
Some research on dialogue systems has com-
bined deictic gestures and natural language such
as Put-That-There [Bolt, 1980], CUBRICON
[Neal et al., 1988], and ALFRESCO [Stock, 1991].
One of the advantages of combined NL/DM in-
teraction is that it can easily resolve the miss-
ing information in NL expressions. For exam-
ple, when the system receives a user request in
speech like &amp;quot;delete that object,&amp;quot; it can fill in the
missing information by looking for a pointing
gesture from the user or objects on the screen
at the time the request is made.
</bodyText>
<listItem confidence="0.6205365">
2. Using nonverbal inputs to specify the -7ontext
and filter out unrelated information
</listItem>
<bodyText confidence="0.999892958333334">
The focus of attention or the focal point plays
a very important role in processing applications
with a broad hypothesis space such as speech
recognition. One example of focusing modality
is following the user&apos;s looking behavior. Fixa-
tion or gaze is useful for the dialogue system
to determine the context of the user&apos;s inter-
est. For example, when a user is looking at
a car, that the user says at that time may be
related to the car. Prosodic information (e.g.,
voice tones) in the user&apos;s utterance also helps
to determine focus. In this case, the system
uses prosodic information to infer the user&apos;s be-
liefs or intentions. Combining gestural informa-
tion with spoken language comprehension shows
another example of how context may be deter-
mined by the user&apos;s nonverbal behavior [Ovi-
att et al., 19931. This research uses multimodal
forms that prompt a user to speak or write into
labeled fields. The forms are capable of guiding
and segmenting inputs, of conveying the kind of
information the system is expecting, and of re-
ducing ambiguities in utterances by restricting
syntactic and semantic complexities.
</bodyText>
<listItem confidence="0.958727">
3. Incorporating human-like behavior into dialogue
systems to reduce operation complexity and
stress often associated with computer systems
</listItem>
<bodyText confidence="0.99941456097561">
Designing human-computer dialogue requires
that the computer makes appropriate backchan-
nel feedbacks like nodding or expressions such
as &amp;quot;aha&amp;quot; and &amp;quot;I see.&amp;quot; One of the major ad-
vantages of using such nonverbal behavior in
human-computer conversation is that reactions
are quicker than those from voice-based re-
sponses. For example, the facial backchannel
plays an important role in human face-to-face
conversation. We consider such quick reac-
tions as being situated actions [Suchman, 1987]
which are necessary for resource-bounded dia-
logue participants. Timely responses are crucial
to successful conversation, since some delay in
reactions can imply specific meanings or make
messages unnecessarily ambiguous.
Generally, visual channels contribute to quick
user recognition of system status. For example,
the system&apos;s gaze behavior (head and eye move-
ment) gives a strong impression of whether it
is paying attention or not. If the system&apos;s eyes
wander around aimlessly, the user easily recog-
nizes the system&apos;s attention elsewhere, perhaps
even unaware that he or she is speaking to it.
Thus, gaze is an important indicator of system
(in this case, speech recognition) status.
By using human-like nonverbal behavior, the
system can more flexibly respond to the user
than is possible by using verbal modality alone.
We focused on the third viewpoint and devel-
oped a system that acts like a human. We em-
ployed communicative facial expressions as a new
modality in human-computer conversation. We
have already discussed this, however, in another
paper [Takeuchi and Nagao, 1993]. Here, we con-
sider our implemented system as a testbed for in-
corporating human-like (nonverbal) behavior into
dialogue systems.
The following sections give a system overview,
an example dialogue along with a brief explanation
of the process, and our experimental results.
</bodyText>
<subsectionHeader confidence="0.766544333333333">
Incorporating Facial Displays into a
Speech Dialogue System
Facial Displays as a New Modality
</subsectionHeader>
<bodyText confidence="0.9731175">
The study of facial expressions has attracted the
interest of a number of different disciplines, in-
cluding psychology, ethology, and interpersonal
communications. Currently, there are two basic
schools of thought. One regards facial expres-
sions as being expressions of emotion [Ekman and
Friesen, 1984]. The other views facial expressions
in a social context, regarding them as being com-
municative signals [Chovil, 1991]. The term &amp;quot;fa-
cial displays&amp;quot; is essentially the same as &amp;quot;facial ex-
pressions,&amp;quot; but is less reminiscent of emotion. In
this paper, therefore, we use &amp;quot;facial displays.&amp;quot;
</bodyText>
<page confidence="0.998983">
103
</page>
<bodyText confidence="0.904538615384615">
A face is an independent communication chan-
nel that conveys emotional and conversational sig-
nals, encoded as facial displays. Facial displays
can be also regarded as being a modality because
the human brain has a special circuit dedicated to
their processing.
Table 1 lists all the communicative facial dis-
plays used in the experiments described in a later
section. The categorization framework, terminol-
ogy, and individual displays are based on the work
of Chovil [1991], with the exception of the em-
phasizer, underliner, and facial shrug. These were
coined by Ekman [1969].
</bodyText>
<tableCaption confidence="0.733955">
Table 1: Communicative Facial Displays Used in
the Experiments. (Categorization based mostly
on Chovil [1991])
</tableCaption>
<listItem confidence="0.866996904761905">
Syntactic Display
1. Exclamation mark Eyebrow raising
2. Question mark Eyebrow raising or lowering
3. Emphasizer Eyebrow raising or lowering
4. Underliner Longer eyebrow raising
5. Punctuation Eyebrow movement
6. End of an utterance Eyebrow raising
7. Beginning of a story Eyebrow raising
8. Story continuation Avoid eye contact
9. End of a story Eye contact
Speaker Display
10. Thinking/Remembering Eyebrow raising or lowering,
11. Facial shrug: closing the eyes,
&amp;quot;I don&apos;t know&amp;quot; pulling back one mouth side
12. Interactive: &amp;quot;You know?&amp;quot; Eyebrow flashes,
13. Metacommunicative: mouth corners pulled down,
Indication of sarcasm or joke mouth corners pulled back
14. &amp;quot;Yes&amp;quot; Eyebrow raising
15. &amp;quot;No&amp;quot; Eyebrow raising and
15. &amp;quot;Not&amp;quot; looking up and off
17. &amp;quot;But&amp;quot; Eyebrow actions
</listItem>
<table confidence="0.9094516875">
Eyebrow actions
Eyebrow actions
Eyebrow actions
Listener Comment Display
18. Backchannel: Eyebrow raising,
Indication of attendance mouth corners turned down
19. Indication of loudness Eyebrows drawn to center
Understanding levels Eyebrow raising, head nod
20. Confident Eyebrow raising
21. Moderately confident Eyebrow lowering
22. Not confident Eyebrow raising
23. &amp;quot;Yes&amp;quot;
Evaluation of utterances Eyebrow raising
24. Agreement Eyebrow raising
25. Request for more information Longer eyebrow raising
26. Incredulity
</table>
<bodyText confidence="0.984595375">
Three major categories are defined as follows.
Syntactic displays. These are facial displays
that (1) place stress on particular words or clauses,
(2) are connected with the syntactic aspects of an
utterance, or (3) are connected with the organiza-
tion of the talk.
Speaker displays. Speaker displays are facial
displays that (1) illustrate the idea being verbally
conveyed, or (2) add additional information to the
ongoing verbal content.
Listener comment displays. These are facial
displays made by the person who is not speaking,
in response to the utterances of the speaker.
An Integrated System of Speech
Dialogue and Facial Animation
We have developed an experimental system that
integrates speech dialogue and facial animation to
investigate the effects of human-like behavior in
human-computer dialogue.
The system consists of two subsystems, a fa-
cial animation subsystem that generates a three-
dimensional face capable of a range of facial dis-
plays, and a speech dialogue subsystem that rec-
ognizes and interprets speech, and generates voice
outputs. Currently, the animation subsystem runs
on an SGI 320VGX and the speech dialogue sub-
system on a Sony NEWS workstation. These two
subsystems communicate with each other via an
Ethernet network.
Figure 1 shows the configuration of the inte-
grated system. Figure 2 illustrates the interaction
of a user with the system.
</bodyText>
<figureCaption confidence="0.99838">
Figure 1: System Configuration
</figureCaption>
<subsectionHeader confidence="0.969914">
Facial Animation Subsystem
</subsectionHeader>
<bodyText confidence="0.999977833333333">
The face is modeled three-dimensionally. Our cur-
rent version is composed of approximately 500
polygons. The face can be rendered with a skin-
like surface material, by applying a texture map
taken from a photograph or a video frame.
In 3D computer graphics, a facial display is
realized by local deformation of the polygons rep-
resenting the face. Waters showed that deforma-
tion that simulates the action of muscles under-
lying the face looks more natural [Waters, 1987].
We therefore use numerical equations to simulate
muscle actions, as defined by Waters. Currently,
</bodyText>
<figure confidence="0.994988052631579">
ISpoken utterance ...â€ž.
Speech recognition
Word sequence
Syntactic &amp; semantic analysis
*Semantic representation
Plan recognition
Speaker&apos;s intention
Muscle control
ype of fai di
Muscle parameters
Response generation
System&apos;s response
Facial animation
Info. on its sync
Voice synthesis
Facial display
Facial animation subsystem
Voice
Speech dialogue subsystem
</figure>
<page confidence="0.727517">
104
</page>
<figureCaption confidence="0.999735">
Figure 2: Dialogue Snapshot
</figureCaption>
<bodyText confidence="0.999988424242424">
the system incorporates 16 muscles and 10 pa-
rameters, controlling mouth opening, jaw rotation,
eye movement, eyelid opening, and head orienta-
tion. These 16 muscles were determined by Wa-
ters, considering the correspondence with action
units in the Facial Action Coding System (FACS)
[Ekman and Friesen. 1978]. For details of the fa-
cial modeling and animation system, see [Takeuchi
and Franks, 19921.
We use 26 synthesized facial displays, corre-
sponding to those listed in Table 1, and two ad-
ditional displays. All facial displays are generated
by the above method, and rendered with a texture
map of a young boy&apos;s face. The added displays
are &amp;quot;Smile&amp;quot; and &amp;quot;Neutral.&amp;quot; The &amp;quot;Neutral&amp;quot; display
features no muscle contraction whatsoever, and is
used when no conversational signal is needed.
At run-time, the animation subsystem awaits
a request from the speech subsystem. When the
animation subsystem receives a request that spec-
ifies values for the 26 parameters, it starts to de-
form the face, on the basis of the received values.
The deformation process is controlled by the dif-
ferential equation f&apos; = a â€” f, where f is a param-
eter value at time t and f&apos; is its time derivative
at time t. a is the target value specified in the
request. A feature of this equation is that defor-
mation is fast in the early phase but soon slows,
corresponding closely to the real dynamics of fa-
cial displays. Currently, the base performance of
the animation subsystem is around 20-25 frames
per second when running on an SGI Power Series.
This is sufficient to enable real-time animation.
</bodyText>
<subsectionHeader confidence="0.473806">
Speech Dialogue Subsystem
</subsectionHeader>
<bodyText confidence="0.999860593220339">
Our speech dialogue subsystem works as follows.
First, a voice input is acoustically analyzed by a
built-in sound processing board. Then, a speech
recognition module is invoked to output word se-
quences that have been assigned higher scores by
a probabilistic phoneme model. These word se-
quences are syntactically and semantically ana-
lyzed and disambiguated by applying a relatively
loose grammar and a restricted domain knowledge.
Using a semantic representation of the input ut-
terance, a plan recognition module extracts the
speaker&apos;s intention. For example, from the ut-
terance &amp;quot;I am interested in Sony&apos;s workstation.&amp;quot;
the module interprets the speaker&apos;s intention as
&amp;quot;he wants to get precise information about Sony&apos;s
workstation.&amp;quot; Once the system determines the
speaker&apos;s intention, a response generation module
is invoked. This generates a response to satisfy the
speaker&apos;s request. Finally, the system&apos;s response is
output as voice by a voice synthesis module. This
module also sends the information about lip syn-
chronization that describes phonemes (including
silence) in the response and their time durations
to the facial animation subsystem.
With the exception of the voice synthesis mod-
ule, each module can send messages to the facial
animation subsystem to request the generation of
a facial display. The relation between the speech
dialogues and facial displays is discussed later.
In this case, the specific task of the system
is to provide information about Sony&apos;s computer-
related products. For example, the system can an-
swer questions about price, size, weight. and spec-
ifications of Sony&apos;s workstations and PCs.
Below, we describe the modules of the speech
dialogue subsystem.
Speech recognition. This module was jointly
developed with the Electrotechnical Laboratory
and Tokyo Institute of Technology. Speaker-
independent continuous speech inputs are ac-
cepted without special hardware. To obtain a
high level of accuracy, context-dependent pho-
netic hidden Markov models are used to construct
phoneme-level hypotheses [Itou et ad_ 19921. This
module can generate N-best word-level hypothe-
ses.
Syntactic and semantic analysis. This mod-
ule consists of a parsing mechanism, a semantic
analyzer, a relatively loose grammar consisting of
24 rules, a lexicon that includes 34 nouns. 8 verbs.
4 adjectives and 22 particles, and a frame-based
knowledge base consisting of 61 conceptual frames.
Our semantic analyzer can handle ambiguities in
syntactic structures and generates a semantic rep-
resentation of the speaker&apos;s utterance. We ap-
plied a preferential constraint satisfaction tech-
nique [Nagao, 1992] for performing disambigua-
tion and semantic analysis. By allowing the prefer-
ences to control the application of the constraints.
</bodyText>
<page confidence="0.996742">
105
</page>
<bodyText confidence="0.999399">
ambiguities can be efficiently resolved, thus avoid-
ing combinatorial explosions.
Plan recognition. This module determines the
speaker&apos;s intention by constructing a model of
his/her beliefs, dynamically adjusting and expand-
ing the model as the dialogue progresses [Nagao,
1993]. The model deals with the dynamic nature
of dialogues by applying the following two mech-
anisms. First, preferences among the contexts are
dynamically computed based on the facts and as-
sumptions within each context. The preference
provides a measure of the plausibility of a context.
The currently most preferable context contains a
currently recognized plan. Secondly, changing the
most plausible context among mutually exclusive
contexts within a dialogue is formally treated as
belief revision of a plan-recognizing agent. How-
ever, in some dialogues, many alternatives may
have very similar preference values. In this situ-
ation, one may wish to obtain additional infor-
mation, allowing one to be more certain about
committing to the preferable context. A crite-
rion for detecting such a critical situation based
on the preference measures for mutually exclusive
contexts is being explored. The module also main-
tains the topic of the current dialogue and can han-
dle anaphora (reference of pronouns) and ellipsis
(omission of subjects).
Response generation. This module generates a
response by using domain knowledge (database)
and text templates (typical patterns of utter-
ances). It selects appropriate templates and com-
bines them to construct a response that satisfies
the speaker&apos;s request.
In our prototype system, the method used to
comprehend speech is a specific combination of
specific types of knowledge sources with a rather
fixed information flow, preventing flexible inter-
action between them. A new method that en-
ables flexible control of omni-directional informa-
tion flow in a very context-sensitive fashion has
been announced [Nagao et at., 19931. Its archi-
tecture is based on dynamical constraint [Hasida
et at., 19931 which defines a fine classification
based on the dimensions of satisfaction and the vi-
olation of constraints. A constraint is represented
in terms of a clausal logic program. A fine-grained
declarative semantics is defined for this constraint
by measuring the degree of violation in terms of
real-valued potential energy. A field of force arises
along the gradient of this energy, inferences be-
ing controlled on the basis of the dynamics. This
allows us to design combinatorial behaviors un-
der declarative semantics within tractable com-
putational complexity. Our forthcoming system
can, therefore, concentrate on its computational
resources according to a dynamic focal point that
is important to speech processing with broad hy-
pothesis space, and apply every kind of constraint,
from phonetic to pragmatic, at the same time.
</bodyText>
<note confidence="0.558053333333333">
Correspondence between
Conversational Situations and Facial
Displays
</note>
<bodyText confidence="0.999936692307692">
The speech dialogue subsystem recognizes a num-
ber of typical conversational situations that are
important to dialogues. We associate these situ-
ations with an appropriate facial display(s). For
example, in situations where speech input is not
recognized or where it is syntactically invalid, the
listener comment display &amp;quot;Not confident&amp;quot; is dis-
played. If the speaker&apos;s request exceeds the range
of the system&apos;s domain knowledge, then the sys-
tem displays a facial shrug and replies &amp;quot;I cannot
answer such a question.&amp;quot; The relationships be-
tween conversational situations and facial displays
are listed in Table 2.
</bodyText>
<subsectionHeader confidence="0.552129">
Example Dialogue
</subsectionHeader>
<bodyText confidence="0.84142575">
Here is an example of a possible dialogue with the
system&apos;. Un: indicates user speech inputs, Sn:
indicates voice responses delivered by the system
to the user. Terms enclosed within square brackets
illustrate the types of facial displays introduced
in previous section. &amp;quot;Continuing&amp;quot; means that the
system facial expression remains the same.
Ul: Hello.
[Attend and BOStory] Hi. This is Sony Com-
puter Science Laboratory.
S2: [Continuing] I can answer any question about
computer-related products.
</bodyText>
<listItem confidence="0.932035611111111">
U2: uh
S3: [NotConfident] I beg your pardon.
U3: I want to know about a personal computer.
S4: [Question] Do you want to know about a Sony
personal computer?
U4: No, I don&apos;t.
55: [Shrug] I cannot answer such a question.
U5: Please tell me about a Sony personal com-
puter.
S6: [BOStory] Sony personal computer &amp;quot;Quar-
terL&amp;quot; is a standard IBM compatible notebook-
style personal computer.
U6: What can I do with it?
S7: [BOStory] You can use all IBM PC software.
S8: [Continuing] For example, you can use a word
processor, and a spreadsheet.
U7: Can I use UNIX with it?
59: [BOStory] If you want to use UNIX,
</listItem>
<footnote confidence="0.890304">
1 Actually, the system accepts and speaks only
Japanese. The example is a translation.
</footnote>
<page confidence="0.997502">
106
</page>
<tableCaption confidence="0.999381">
Table 2: Relation between Conversational Situations and Facial Displays
</tableCaption>
<table confidence="0.999360388888889">
CONVERSATIONAL SITUATION FACIAL DISPLAY(S)
Recognition failure NotConfident (Listener comment display &amp;quot;Not confident&amp;quot;)
Syntactically invalid utterance NotConfident
Many recognition candidates ModConfident (Listener comment display &amp;quot;Moderately confident&amp;quot;)
with close scores
Beginning of a dialogue Attend (Listener comment display &amp;quot;Indication of attendance&amp;quot;)
Introduction to a topic BOStory (Syntactic display &amp;quot;Beginning of a story&amp;quot;)
Shift to another topic EOStory (Syntactic display &amp;quot;End of a story&amp;quot;) and Â°Story
Clarification dialogue Question (Syntactic display &amp;quot;Question mark&amp;quot;)
Underline a remark Underliner (Syntactic display &amp;quot;Underliner&amp;quot;)
Answer &amp;quot;Yes&amp;quot; SpeakerYes (Speaker display &amp;quot;Yes&amp;quot;)
Answer &amp;quot;No&amp;quot; SpeakerNo (Speaker display &amp;quot;No&amp;quot;)
Out of the domain Shrug (Speaker display &amp;quot;Facial shrug&amp;quot;)
Answer &amp;quot;Yes&amp;quot; with emphasis SpeakerYes and Emphasizer (Syntactic display &amp;quot;Emphasizer&amp;quot;)
Violation of pragmatic Incredulity (Listener comment display &amp;quot;Incredulity&amp;quot;)
constraints
Reply to &amp;quot;Thanks&amp;quot; ListenerYes (Listener comment display &amp;quot;Yes&amp;quot;) and
Smile (Complementary display &amp;quot;Smile&amp;quot;)
</table>
<listItem confidence="0.898007083333333">
S10: [Underlined I recommend you get a work-
station.
U8: Tell inc about a workstation.
S11: [EOStory and BOStory] Sony workstation
&amp;quot;NEWS&amp;quot; is a high-performance laptop worksta-
tion.
S12: [Continuing] Its CPU is an R3081 RISC pro-
cessor, and its processing speed of 37 MIPS is
the fastest in this class.
U9: Is it large?
S10: [SpeakerNo and Emphasizer] No, it isn&apos;t.
S11: [BOStory] &amp;quot;NEWS&amp;quot; is 32.4 cm in width,
36.4 cm in depth, and 6.9 cm in height.
U10: Is it light?
S12: [SpeakerYes and Emphasizer] Yes, it is.
S13: [BOStory] The weight of &amp;quot;NEWS&amp;quot; is 4.5 kg.
U11: How much?
S14: [BOStory] &amp;quot;NEWS&amp;quot; costs 700,000 yen.
U12: What does the personal computer cost?
S15: [EOStory and BOStory] &amp;quot;QuarterL&amp;quot; costs
398,000 yen.
U13: Thank you.
S16: [ListenerYes and Smile] You are welcome.
S17: [Continuing] It&apos;s my pleasure.
</listItem>
<bodyText confidence="0.99857325">
U2 is an example of noisy inputs. The system
could not recognize the expression and displayed
the facial backchannel NotConfident and replied &amp;quot;I
beg your pardon.&amp;quot; In U3, there is missing infor-
mation about personal computer maker. So, the
system enters a clarification dialogue S4, showing
the Question display. In this case, the system tried
to drive the user into the domain with which the
system is familiar. However, the user refused to
agree with the system by utterance U4, then the
system revealed its discouragement by showing a
facial shrug. In U8, the user changes the topic, by
asking for workstation information. The system
recognizes this by comparison with the prior topic
(i.e., personal computers). Therefore, in response
to question S11, the system displays EOStory and
subsequently BOStory to indicate the shift to a
different topic. The system also manages the topic
structure so that it can handle anaphora and el-
lipsis in utterances such as U9, U10, and U11.
</bodyText>
<subsectionHeader confidence="0.747498">
Experimental Results
</subsectionHeader>
<bodyText confidence="0.999956714285714">
To examine the effect of facial displays on the in-
teraction between humans and computers, exper-
iments were performed using the prototype sys-
tem. The system was tested on 32 volunteer sub-
jects. Two experiments were prepared. In one
experiment, called F, the subjects held a conver-
sation with the system, which used facial displays
to reinforce its response. In the other experiment,
called N, the subjects held a conversation with
the system, which answered using short phrases
instead of facial displays. The short phrases were
two- or three-word sentences that described the
corresponding facial displays. For example, in-
stead of the &amp;quot;Not confident&amp;quot; display, it simply
displayed the words &amp;quot;I am not confident.&amp;quot; The
subjects were divided into two groups, FN and
NF. As the names indicate, the subjects in the
FN group were first subjected to experiment F
and then N. The subjects in the NF group were
first subjected to N and then F. In both experi-
ments, the subjects were assigned the goal of en-
</bodyText>
<page confidence="0.997503">
107
</page>
<bodyText confidence="0.999517017857143">
quiring about the functions and prices of Sony&apos;s
computer products. In each experiment, the sub-
jects were requested to complete the conversation
within 10 minutes. During the experiments, the
number of occurrences of each facial display was
counted. The conversation content was also evalu-
ated based on how many topics a subject covered
intentionally. The degree of task achievement re-
flects how it is preferable to obtain a greater num-
ber of visit more topics, and take the least amount
of time possible. According to the frequencies
of appeared facial displays and the conversational
scores, the conversations that occurred during the
experiments can be classified into two types. The
first is &amp;quot;smooth conversation&amp;quot; in which the score is
relatively high and the displays &amp;quot;Moderately con-
fident,&amp;quot; &amp;quot;Beginning of a story,&amp;quot; and &amp;quot;Indication
of attendance&amp;quot; appear most often. The second is
&amp;quot;dull conversation,&amp;quot; characterized by a lower score
and in which the displays &amp;quot;Neutral&amp;quot; and &amp;quot;Not con-
fident&amp;quot; appear more frequently.
The results are summarized as follows. The
details of the experiments were presented in an-
other paper [Takeuchi and Nagao, 19931.
1. The first experiments of the two groups are
compared. Conversation using facial displays
is clearly more successful (classified as smooth
conversation) than that using short phrases. We
can therefore conclude that facial displays help
conversation in the case of initial contact.
2. The overall results for both groups are com-
pared. Considering that the only difference be-
tween the two groups is the order in which the
experiments were conducted, we can conclude
that early interaction with facial displays con-
tributes to success in the later interaction.
3. The experiments using facial displays F and
those using short phrases N are compared. Con-
trary to our expectations, the result indicates
that facial displays have little influence on suc-
cessful conversation. This means that the learn-
ing effect, occurring over the duration of the ex-
periments, is equal in effect to the facial dis-
plays. However, we believe that the effect of
the facial displays will overtake the learning ef-
fect once the qualities of speech recognition and
facial animation have been improved.
The premature settings of the prototype sys-
tem, and the strict restrictions imposed on the
conversation inevitably detract from the poten-
tial advantages available from systems using com-
municative facial displays. We believe that fur-
ther elaboration of the system will greatly im-
prove the results. The subjects were relatively
well-experienced in using computers. Experiments
with computer novices should also be done.
</bodyText>
<subsectionHeader confidence="0.384285">
Concluding Remarks and Further
</subsectionHeader>
<bodyText confidence="0.992791785714286">
Work
Our experiments showed that facial displays are
helpful, especially upon first contact with the sys-
tem. It was also shown that early interaction
with facial displays improves subsequent interac-
tion, even though the subsequent interaction does
not use facial displays. These results prove quan-
titatively that interfaces with facial displays help
to break down the mental barrier that many users
have toward computing systems.
As a future research direction, we plan to in-
tegrate more communication channels and modal-
ities. Among these, the prosodic information pro-
cessing in speech recognition and speech synthe-
sis are of special interest, as well as the recogni-
tion of users&apos; gestures and facial displays. Also,
further work needs to be done on the design
and implementation of the coordination of mul-
tiple communication modalities. We believe that
such coordination is an emergent phenomenon
from the tight interaction between the system and
its ever-changing environments (including humans
and other interactive systems) by means of situ-
ated actions and (more deliberate) cooperative ac-
tions. Precise control of multiple coordinated ac-
tivities is not, therefore, directly implementable.
Only constraints or relationships among percep-
tion, conversational situations, and action will be
implementable.
To date, conversation with computing sys-
tems has been over-regulated conversation. This
has been made necessary by communication be-
ing done through limited channels, making it nec-
essary to avoid information collision in the nar-
row channels. Multiple channels reduce the ne-
cessity for conversational regulation, allowing new
styles of conversation to appear. A new style of
conversation has smaller granularity, is highly in-
terruptible, and invokes more spontaneous utter-
ances. Such conversation is closer to our daily con-
versation with families and friends, and this will
further increase familiarity with computers.
Co-constructive conversation, that is less con-
strained by domains or tasks, is one of our fu-
ture goals. We are extending our conversational
model to deal with a new style of human-computer
interaction called social interaction [Nagao and
Takeuchi, 1994] which includes co-constructive
conversation. This style of conversation features
a group of individuals where, say, those individ-
uals talk about the food they ate together in a
restraurant a month ago. There are no special
roles (like the chairperson) for the participants to
play. They all have the same role. The conversa-
tion terminates only once all the participants are
satisfied with the conclusion.
</bodyText>
<page confidence="0.997461">
108
</page>
<bodyText confidence="0.999979333333333">
We are also interested in developing interac-
tive characters and stories as an application for
interactive entertainment. We are now building a
conversational, anthropomorphic computer char-
acter that we hope will entertain us with some
pleasant stories.
</bodyText>
<sectionHeader confidence="0.988654" genericHeader="acknowledgments">
ACKNOWLEDGMENTS
</sectionHeader>
<bodyText confidence="0.999945888888889">
The authors would like to thank Mario Tokoro and
colleagues at Sony CSL for their encouragement
and helpful advice. We also extend our thanks to
Nicole Chovil for her useful comments on a draft
of this paper, and Satoru Hayamizu, Katunobu
Itou, and Steve Franks for their contributions to
the implementation of the prototype system. Spe-
cial thanks go to Keith Waters for granting per-
mission to access his original animation system.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999954105263158">
[Bolt, 1980) Richard A. Bolt. 1980. Put-That-There:
Voice and gesture at the graphics interface. Com-
puter Graphics, 14(3):262-270.
[Chovil, 19911 Nicole Chovil. 1991. Discourse-oriented
facial displays in conversation. Research on Lan-
guage and Social Interaction, 25:163-194.
[Don et al., 19911 Abbe Don, Tim Oren, and Brenda
Laurel. 1991. Guides 3.0. In Proceedings of ACM
CHI&apos;91: Conference on Human Factors in Comput-
ing Systems, pages 447-448. ACM Press.
[Ekman and Friesen, 19691 Paul Ekman and Wal-
lace V. Friesen. 1969. The repertoire of nonverbal
behavior: Categories, origins, usages, and coding.
Semiotica, 1:49-98.
[Ekman and Friesen, 1978] Paul Ekman and Wal-
lace V. Friesen. 1978. Facial Action Coding System..
Consulting Psychologists Press, Palo Alto, Califor-
nia.
[Ekman and Friesen, 1984) Paul Ekman and Wal-
lace V. Friesen. 1984. Unmasking the Face. Con-
sulting Psychologists Press, Palo Alto, California.
[Hasida et al., 1993) Koiti Hasida, Katashi Nagao,
and Takashi Miyata. 1993. Joint utterance: In-
trasentential speaker/hearer switch as an emergent
phenomenon. In Proceedings of the Thirteenth In-
ternational Joint Conference on Artificial Intelli-
gence (IJCAI-93), pages 1193-1199. Morgan Kauf-
mann Publishers, Inc.
[Itou et al., 1992) Katunobu Itou, Satoru Hayamizu,
and Hozumi Tanaka. 1992. Continuous speech
recognition by context-dependent phonetic HMM
and an efficient algorithm for finding N-best sen-
tence hypotheses. In Proceedings of the Interna-
tional Conference on Acoustics, Speech, and Signal
Processing (ICASSP-92), pages 1.21-1.24. IEEE.
[Nagao and Takeuchi, 1994] Katashi Nagao
and Akikazu Takeuchi. 1994. Social interaction:
Multimodal conversation with social agents. In Pro-
ceedings of the Twelfth National Conference on Ar-
tificial Intelligence (AAAI-94). The MIT Press.
[Nagao et al., 19931 Katashi Nagao, KOiti Hasida,
and Takashi Miyata. 1993. Understanding spoken
natural language with omni-directional information
flow. In Proceedings of the Thirteenth International
Joint Conference on Artificial Intelligence (IJCAI-
93), pages 1268-1274. Morgan Kaufmann Publish-
ers, Inc.
[Nagao, 1992) Katashi Nagao. 1992. A preferential
constraint satisfaction technique for natural lan-
guage analysis. In Proceedings of the Tenth Euro-
pean Conference on Artificial Intelligence (ECAI-
92), pages 523-527. John Wiley SE Sons.
[Nagao, 1993] Katashi Nagao. 1993. Abduction and
dynamic preference in plan-based dialogue under-
standing. In Proceedings of the Thirteenth Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-93), pages 1186-1192. Morgan Kaufmann
Publishers, Inc.
[Neal et al., 19881 Jeannette G. Neal, Zuzana Dobes,
Keith E. Bettinger, and Jong S. Byoun. 1988. Multi-
modal references in human-computer dialogue. In
Proceedings of the Seventh National Conference on
Artificial Intelligence (AAAI-88), pages 819-823.
Morgan Kaufmann Publishers, Inc.
[Oviatt et al., 1993) Sharon L. Oviatt, Philip R. Co-
hen, and Michelle Wang. 1993. Reducing linguis-
tic variability in speech and handwriting through
selection of presentation format. In Proceedings
of the International Symposium on Spoken Dia-
logue (ISSD-93), pages 227-230. Waseda University,
Tokyo, Japan.
[Shneiderman, 1983) Ben Shneiderman. 1983. Direct
manipulation: A step beyond programming lan-
guages. IEEE Computer, 16:57-69.
[Stock, 1991] Oliviero Stock. 1991. Natural language
and exploration of an information space: the AL-
FRESCO interactive system. In Proceedings of the
Twelfth International Joint Conference on Artifi-
cial Intelligence (IJCAI-91), pages 972-978. Mor-
gan Kaufmann Publishers, Inc.
[Suchman, 19871 Lucy Suchman. 1987. Plans and Sit-
uated Actions. Cambridge University Press.
[Takeuchi and Franks, 19921 Akikazu Takeuchi and
Steve Franks. 1992. A rapid face construction lab.
Technical Report SCSL-TR-92-010, Sony Computer
Science Laboratory Inc., Tokyo, Japan.
[Takeuchi and Nagao, 1993) Akikazu Takeuchi and
Katashi Nagao. 1993. Communicative facial dis-
plays as a new conversational modality. In Proceed-
ings of ACM/IFIP INTERCHI&apos;93: Conference on
Human Factors in Computing Systems, pages 187-
193. ACM Press.
[Waters, 1987) Keith Waters. 1987. A muscle model
for animating three-dimensional facial expression.
Computer Graphics, 21(4):17-24.
</reference>
<page confidence="0.99896">
109
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.926631">
<title confidence="0.998444">SPEECH DIALOGUE WITH FACIAL DISPLAYS: MULTIMODAL HUMAN-COMPUTER CONVERSATION</title>
<author confidence="0.998354">Katashi Nagao</author>
<author confidence="0.998354">Akikazu Takeuchi</author>
<affiliation confidence="0.999884">Sony Computer Science Laboratory Inc.</affiliation>
<address confidence="0.997282">3-14-13 Higashi-got anda, Shinagawa-ku, Tokyo 141, Japan</address>
<email confidence="0.998295">E-mail:{nagao,takeuchi}Ocsl.sony.co.jp</email>
<abstract confidence="0.996906095238095">Human face-to-face conversation is an ideal model for human-computer dialogue. One of the major features of face-to-face communication is its multiplicity of communication channels that act on multiple modalities. To realize a natural multimodal dialogue, it is necessary to study how humans perceive information and determine the information to which humans are sensitive. A face is an independent communication channel that conveys emotional and conversational signals, encoded as facial expressions. We have developed an experimental system that integrates speech dialogue and facial animation, to investigate the effect of introducing communicative facial expressions as a new modality in human-computer conversation. Our experiments have showen that facial expressions are helpful, especially upon first contact with the system. We have also discovered that featuring facial expressions at an early stage improves subsequent interaction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Put-That-There: Voice and gesture at the graphics interface.</title>
<date>1980</date>
<journal>Computer Graphics,</journal>
<pages>14--3</pages>
<marker>1980</marker>
<rawString>[Bolt, 1980) Richard A. Bolt. 1980. Put-That-There: Voice and gesture at the graphics interface. Computer Graphics, 14(3):262-270.</rawString>
</citation>
<citation valid="true">
<title>Discourse-oriented facial displays in conversation.</title>
<date>1991</date>
<journal>Research on Language and Social Interaction,</journal>
<pages>25--163</pages>
<contexts>
<context position="9378" citStr="[1991]" startWordPosition="1424" endWordPosition="1424">entially the same as &amp;quot;facial expressions,&amp;quot; but is less reminiscent of emotion. In this paper, therefore, we use &amp;quot;facial displays.&amp;quot; 103 A face is an independent communication channel that conveys emotional and conversational signals, encoded as facial displays. Facial displays can be also regarded as being a modality because the human brain has a special circuit dedicated to their processing. Table 1 lists all the communicative facial displays used in the experiments described in a later section. The categorization framework, terminology, and individual displays are based on the work of Chovil [1991], with the exception of the emphasizer, underliner, and facial shrug. These were coined by Ekman [1969]. Table 1: Communicative Facial Displays Used in the Experiments. (Categorization based mostly on Chovil [1991]) Syntactic Display 1. Exclamation mark Eyebrow raising 2. Question mark Eyebrow raising or lowering 3. Emphasizer Eyebrow raising or lowering 4. Underliner Longer eyebrow raising 5. Punctuation Eyebrow movement 6. End of an utterance Eyebrow raising 7. Beginning of a story Eyebrow raising 8. Story continuation Avoid eye contact 9. End of a story Eye contact Speaker Display 10. Think</context>
</contexts>
<marker>1991</marker>
<rawString>[Chovil, 19911 Nicole Chovil. 1991. Discourse-oriented facial displays in conversation. Research on Language and Social Interaction, 25:163-194.</rawString>
</citation>
<citation valid="true">
<title>Guides 3.0.</title>
<date>1991</date>
<booktitle>In Proceedings of ACM CHI&apos;91: Conference on Human Factors in Computing Systems,</booktitle>
<pages>447--448</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="9378" citStr="[1991]" startWordPosition="1424" endWordPosition="1424">entially the same as &amp;quot;facial expressions,&amp;quot; but is less reminiscent of emotion. In this paper, therefore, we use &amp;quot;facial displays.&amp;quot; 103 A face is an independent communication channel that conveys emotional and conversational signals, encoded as facial displays. Facial displays can be also regarded as being a modality because the human brain has a special circuit dedicated to their processing. Table 1 lists all the communicative facial displays used in the experiments described in a later section. The categorization framework, terminology, and individual displays are based on the work of Chovil [1991], with the exception of the emphasizer, underliner, and facial shrug. These were coined by Ekman [1969]. Table 1: Communicative Facial Displays Used in the Experiments. (Categorization based mostly on Chovil [1991]) Syntactic Display 1. Exclamation mark Eyebrow raising 2. Question mark Eyebrow raising or lowering 3. Emphasizer Eyebrow raising or lowering 4. Underliner Longer eyebrow raising 5. Punctuation Eyebrow movement 6. End of an utterance Eyebrow raising 7. Beginning of a story Eyebrow raising 8. Story continuation Avoid eye contact 9. End of a story Eye contact Speaker Display 10. Think</context>
</contexts>
<marker>1991</marker>
<rawString>[Don et al., 19911 Abbe Don, Tim Oren, and Brenda Laurel. 1991. Guides 3.0. In Proceedings of ACM CHI&apos;91: Conference on Human Factors in Computing Systems, pages 447-448. ACM Press.</rawString>
</citation>
<citation valid="true">
<title>The repertoire of nonverbal behavior: Categories, origins, usages, and coding.</title>
<date>1969</date>
<journal>Semiotica,</journal>
<pages>1--49</pages>
<contexts>
<context position="9481" citStr="[1969]" startWordPosition="1441" endWordPosition="1441">, we use &amp;quot;facial displays.&amp;quot; 103 A face is an independent communication channel that conveys emotional and conversational signals, encoded as facial displays. Facial displays can be also regarded as being a modality because the human brain has a special circuit dedicated to their processing. Table 1 lists all the communicative facial displays used in the experiments described in a later section. The categorization framework, terminology, and individual displays are based on the work of Chovil [1991], with the exception of the emphasizer, underliner, and facial shrug. These were coined by Ekman [1969]. Table 1: Communicative Facial Displays Used in the Experiments. (Categorization based mostly on Chovil [1991]) Syntactic Display 1. Exclamation mark Eyebrow raising 2. Question mark Eyebrow raising or lowering 3. Emphasizer Eyebrow raising or lowering 4. Underliner Longer eyebrow raising 5. Punctuation Eyebrow movement 6. End of an utterance Eyebrow raising 7. Beginning of a story Eyebrow raising 8. Story continuation Avoid eye contact 9. End of a story Eye contact Speaker Display 10. Thinking/Remembering Eyebrow raising or lowering, 11. Facial shrug: closing the eyes, &amp;quot;I don&apos;t know&amp;quot; pulling</context>
</contexts>
<marker>1969</marker>
<rawString>[Ekman and Friesen, 19691 Paul Ekman and Wallace V. Friesen. 1969. The repertoire of nonverbal behavior: Categories, origins, usages, and coding. Semiotica, 1:49-98.</rawString>
</citation>
<citation valid="true">
<title>Facial Action Coding System.. Consulting Psychologists Press,</title>
<date>1978</date>
<location>Palo Alto, California.</location>
<marker>1978</marker>
<rawString>[Ekman and Friesen, 1978] Paul Ekman and Wallace V. Friesen. 1978. Facial Action Coding System.. Consulting Psychologists Press, Palo Alto, California.</rawString>
</citation>
<citation valid="true">
<title>Unmasking the Face. Consulting Psychologists Press,</title>
<date>1984</date>
<location>Palo Alto, California.</location>
<marker>1984</marker>
<rawString>[Ekman and Friesen, 1984) Paul Ekman and Wallace V. Friesen. 1984. Unmasking the Face. Consulting Psychologists Press, Palo Alto, California.</rawString>
</citation>
<citation valid="true">
<title>Joint utterance: Intrasentential speaker/hearer switch as an emergent phenomenon.</title>
<date>1993</date>
<booktitle>In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (IJCAI-93),</booktitle>
<pages>1193--1199</pages>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<marker>1993</marker>
<rawString>[Hasida et al., 1993) Koiti Hasida, Katashi Nagao, and Takashi Miyata. 1993. Joint utterance: Intrasentential speaker/hearer switch as an emergent phenomenon. In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (IJCAI-93), pages 1193-1199. Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<title>Katunobu Itou, Satoru Hayamizu, and Hozumi Tanaka.</title>
<date>1992</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP-92),</booktitle>
<pages>1--21</pages>
<publisher>IEEE.</publisher>
<marker>1992</marker>
<rawString>[Itou et al., 1992) Katunobu Itou, Satoru Hayamizu, and Hozumi Tanaka. 1992. Continuous speech recognition by context-dependent phonetic HMM and an efficient algorithm for finding N-best sentence hypotheses. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP-92), pages 1.21-1.24. IEEE.</rawString>
</citation>
<citation valid="true">
<title>Katashi Nagao and Akikazu Takeuchi.</title>
<date>1994</date>
<booktitle>In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94).</booktitle>
<publisher>The MIT Press.</publisher>
<marker>1994</marker>
<rawString>[Nagao and Takeuchi, 1994] Katashi Nagao and Akikazu Takeuchi. 1994. Social interaction: Multimodal conversation with social agents. In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94). The MIT Press.</rawString>
</citation>
<citation valid="true">
<title>Understanding spoken natural language with omni-directional information flow.</title>
<date>1993</date>
<booktitle>In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (IJCAI93),</booktitle>
<pages>1268--1274</pages>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<marker>1993</marker>
<rawString>[Nagao et al., 19931 Katashi Nagao, KOiti Hasida, and Takashi Miyata. 1993. Understanding spoken natural language with omni-directional information flow. In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (IJCAI93), pages 1268-1274. Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<title>A preferential constraint satisfaction technique for natural language analysis.</title>
<date>1992</date>
<booktitle>In Proceedings of the Tenth European Conference on Artificial Intelligence (ECAI92),</booktitle>
<pages>523--527</pages>
<publisher>John Wiley SE Sons.</publisher>
<marker>1992</marker>
<rawString>[Nagao, 1992) Katashi Nagao. 1992. A preferential constraint satisfaction technique for natural language analysis. In Proceedings of the Tenth European Conference on Artificial Intelligence (ECAI92), pages 523-527. John Wiley SE Sons.</rawString>
</citation>
<citation valid="true">
<title>Abduction and dynamic preference in plan-based dialogue understanding.</title>
<date>1993</date>
<booktitle>In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (IJCAI-93),</booktitle>
<pages>1186--1192</pages>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<marker>1993</marker>
<rawString>[Nagao, 1993] Katashi Nagao. 1993. Abduction and dynamic preference in plan-based dialogue understanding. In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (IJCAI-93), pages 1186-1192. Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<title>Multimodal references in human-computer dialogue.</title>
<date>1988</date>
<booktitle>In Proceedings of the Seventh National Conference on Artificial Intelligence (AAAI-88),</booktitle>
<pages>819--823</pages>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<marker>1988</marker>
<rawString>[Neal et al., 19881 Jeannette G. Neal, Zuzana Dobes, Keith E. Bettinger, and Jong S. Byoun. 1988. Multimodal references in human-computer dialogue. In Proceedings of the Seventh National Conference on Artificial Intelligence (AAAI-88), pages 819-823. Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<title>Reducing linguistic variability in speech and handwriting through selection of presentation format.</title>
<date>1993</date>
<booktitle>In Proceedings of the International Symposium on Spoken Dialogue (ISSD-93),</booktitle>
<pages>227--230</pages>
<institution>Waseda University,</institution>
<location>Tokyo, Japan.</location>
<marker>1993</marker>
<rawString>[Oviatt et al., 1993) Sharon L. Oviatt, Philip R. Cohen, and Michelle Wang. 1993. Reducing linguistic variability in speech and handwriting through selection of presentation format. In Proceedings of the International Symposium on Spoken Dialogue (ISSD-93), pages 227-230. Waseda University, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<title>Ben Shneiderman.</title>
<date>1983</date>
<journal>IEEE Computer,</journal>
<pages>16--57</pages>
<contexts>
<context position="4073" citStr="[1983]" startWordPosition="592" endWordPosition="592">n conventional multimodal systems. This work will be reported in a separate publication. In this paper, we concentrate on the role of â€¢nonverbal modality for increasing flexibility of human-computer dialogue and reducing the mental barriers that many users associate with computer systems. Research Overview of Multimodal Dialogues Multimodal dialogues that combine verbal and nonverbal communication have been pursued 102 mainly from the following three viewpoints. 1. Combining direct manipulation with natural language (deictic) expressions &amp;quot;Direct manipulation (DM)&amp;quot; was suggested by Shneiderman [1983]. The user can interact directly with graphical objects displayed on the computer screen with rapid, incremental, reversible operations whose effects on the objects of interest are immediately visible. The semantics of natural language (NL) expressions is anchored to real-world objects and events by means of pointing and demonstrating actions and deictic expressions such as &amp;quot;this,&amp;quot; &amp;quot;that,&amp;quot; &amp;quot;here,&amp;quot; &amp;quot;there,&amp;quot; &amp;quot;then,&amp;quot; and &amp;quot;now.&amp;quot; Some research on dialogue systems has combined deictic gestures and natural language such as Put-That-There [Bolt, 1980], CUBRICON [Neal et al., 1988], and ALFRESCO [Stock</context>
</contexts>
<marker>1983</marker>
<rawString>[Shneiderman, 1983) Ben Shneiderman. 1983. Direct manipulation: A step beyond programming languages. IEEE Computer, 16:57-69.</rawString>
</citation>
<citation valid="true">
<title>Natural language and exploration of an information space: the ALFRESCO interactive system.</title>
<date>1991</date>
<booktitle>In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence (IJCAI-91),</booktitle>
<pages>972--978</pages>
<publisher>Morgan Kaufmann Publishers, Inc.</publisher>
<contexts>
<context position="9378" citStr="[1991]" startWordPosition="1424" endWordPosition="1424">entially the same as &amp;quot;facial expressions,&amp;quot; but is less reminiscent of emotion. In this paper, therefore, we use &amp;quot;facial displays.&amp;quot; 103 A face is an independent communication channel that conveys emotional and conversational signals, encoded as facial displays. Facial displays can be also regarded as being a modality because the human brain has a special circuit dedicated to their processing. Table 1 lists all the communicative facial displays used in the experiments described in a later section. The categorization framework, terminology, and individual displays are based on the work of Chovil [1991], with the exception of the emphasizer, underliner, and facial shrug. These were coined by Ekman [1969]. Table 1: Communicative Facial Displays Used in the Experiments. (Categorization based mostly on Chovil [1991]) Syntactic Display 1. Exclamation mark Eyebrow raising 2. Question mark Eyebrow raising or lowering 3. Emphasizer Eyebrow raising or lowering 4. Underliner Longer eyebrow raising 5. Punctuation Eyebrow movement 6. End of an utterance Eyebrow raising 7. Beginning of a story Eyebrow raising 8. Story continuation Avoid eye contact 9. End of a story Eye contact Speaker Display 10. Think</context>
</contexts>
<marker>1991</marker>
<rawString>[Stock, 1991] Oliviero Stock. 1991. Natural language and exploration of an information space: the ALFRESCO interactive system. In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence (IJCAI-91), pages 972-978. Morgan Kaufmann Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<title>Plans and Situated Actions.</title>
<date>1987</date>
<publisher>Cambridge University Press.</publisher>
<marker>1987</marker>
<rawString>[Suchman, 19871 Lucy Suchman. 1987. Plans and Situated Actions. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<title>A rapid face construction lab.</title>
<date>1992</date>
<tech>Technical Report SCSL-TR-92-010,</tech>
<institution>Sony Computer Science Laboratory Inc.,</institution>
<location>Tokyo, Japan.</location>
<marker>1992</marker>
<rawString>[Takeuchi and Franks, 19921 Akikazu Takeuchi and Steve Franks. 1992. A rapid face construction lab. Technical Report SCSL-TR-92-010, Sony Computer Science Laboratory Inc., Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<title>Akikazu Takeuchi</title>
<date>1993</date>
<booktitle>In Proceedings of ACM/IFIP INTERCHI&apos;93: Conference on Human Factors in Computing Systems,</booktitle>
<pages>187--193</pages>
<publisher>ACM Press.</publisher>
<marker>1993</marker>
<rawString>[Takeuchi and Nagao, 1993) Akikazu Takeuchi and Katashi Nagao. 1993. Communicative facial displays as a new conversational modality. In Proceedings of ACM/IFIP INTERCHI&apos;93: Conference on Human Factors in Computing Systems, pages 187-193. ACM Press.</rawString>
</citation>
<citation valid="true">
<title>A muscle model for animating three-dimensional facial expression.</title>
<date>1987</date>
<journal>Computer Graphics,</journal>
<pages>21--4</pages>
<marker>1987</marker>
<rawString>[Waters, 1987) Keith Waters. 1987. A muscle model for animating three-dimensional facial expression. Computer Graphics, 21(4):17-24.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>