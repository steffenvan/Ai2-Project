<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012726">
<title confidence="0.991818">
Automatic Code Assignment to Medical Text
</title>
<author confidence="0.97966">
Koby Crammer and Mark Dredze and Kuzman Ganchev and Partha Pratim Talukdar
</author>
<affiliation confidence="0.988189">
Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA
</affiliation>
<email confidence="0.996486">
{crammer|mdredze|kuzman|partha}@seas.upenn.edu
</email>
<author confidence="0.995984">
Steven Carroll
</author>
<affiliation confidence="0.999256">
Division of Oncology, The Children’s Hospital of Philadelphia, Philadelphia, PA
</affiliation>
<email confidence="0.996484">
carroll@genome.chop.edu
</email>
<sectionHeader confidence="0.998589" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99967675">
Code assignment is important for handling
large amounts of electronic medical data in
the modern hospital. However, only expert
annotators with extensive training can as-
sign codes. We present a system for the
assignment of ICD-9-CM clinical codes to
free text radiology reports. Our system as-
signs a code configuration, predicting one or
more codes for each document. We com-
bine three coding systems into a single learn-
ing system for higher accuracy. We compare
our system on a real world medical dataset
with both human annotators and other auto-
mated systems, achieving nearly the maxi-
mum score on the Computational Medicine
Center’s challenge.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980375">
The modern hospital generates tremendous amounts
of data: medical records, lab reports, doctor notes,
and numerous other sources of information. As hos-
pitals move towards fully electronic record keeping,
the volume of this data only increases. While many
medical systems encourage the use of structured in-
formation, including assigning standardized codes,
most medical data, and often times the most impor-
tant information, is stored as unstructured text.
This daunting amount of medical text creates
exciting opportunities for applications of learning
methods, such as search, document classification,
data mining, information extraction, and relation ex-
traction (Shortliffe and Cimino, 2006). These ap-
plications have the potential for considerable bene-
fit to the medical community as they can leverage
information collected by hospitals and provide in-
centives for electronic record storage. Much of the
data generated by medical personnel is unused past
the clinical visit, often times because there is no way
to simply and quickly apply the wealth of informa-
tion. Medical NLP holds the promise of both greater
care for individual patients and enhanced knowledge
about health care.
In this work we explore the assignment of ICD-9-
CM codes to clinical reports. We focus on this prac-
tical problem since it is representative of the type
of task faced by medical personnel on a daily ba-
sis. Many hospitals organize and code documents
for later retrieval using different coding standards.
Often times, these standards are extremely complex
and only trained expert coders can properly perform
the task, making the process of coding documents
both expensive and unreliable since a coder must se-
lect from thousands of codes a small number for a
given report. An accurate automated system would
reduce costs, simplify the task for coders, and create
a greater consensus and standardization of hospital
data.
This paper addresses some of the challenges asso-
ciated with ICD-9-CM code assignment to clinical
free text, as well as general issues facing applica-
tions of NLP to medical text. We present our auto-
mated system for code assignment developed for the
Computational Medicine Center’s challenge. Our
approach uses several classification systems, each
with the goal of predicting the exact code configu-
ration for a medical report. We then use a learning
</bodyText>
<page confidence="0.980944">
129
</page>
<note confidence="0.7427745">
BioNLP 2007: Biological, translational, and clinical language processing, pages 129–136,
Prague, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999036">
system to combine our predictions for superior per-
formance.
This paper is organized as follows. First, we ex-
plain our task and difficulties in detail. Next we de-
scribe our three automated systems and features. We
combine the three approaches to create a single su-
perior system. We evaluate our system on clinical
reports and show accuracy approaching human per-
formance and the challenge’s best score.
</bodyText>
<sectionHeader confidence="0.987519" genericHeader="introduction">
2 Task Overview
</sectionHeader>
<bodyText confidence="0.999920823529412">
The health care system employs a large number of
categorization and classification systems to assist
data management for a variety of tasks, including
patient care, record storage and retrieval, statistical
analysis, insurance, and billing. One of these sys-
tems is the International Classification of Diseases,
Ninth Revision, Clinical Modification (ICD-9-CM)
which is the official system of assigning codes to di-
agnoses and procedures associated with hospital uti-
lization in the United States. 1 The coding system
is based on World Health Organization guidelines.
An ICD-9-CM code indicates a classification of a
disease, symptom, procedure, injury, or information
from the personal history. Codes are organized hier-
archically, where top level entries are general group-
ings (e.g. “diseases of the respiratory system”) and
bottom level codes indicate specific symptoms or
diseases and their location (e.g. “pneumonia in as-
pergillosis”). Each specific, low-level code consists
of 4 or 5 digits, with a decimal after the third. Higher
level codes typically include only 3 digits. Overall,
there are thousands of codes that cover a broad range
of medical conditions.
Codes are assigned to medical reports by doc-
tors, nurses and other trained experts based on com-
plex coding guidelines (National Center for Health
Statistics, 2006). A particular medical report can be
assigned any number of relevant codes. For exam-
ple, if a patient exhibits a cough, fever and wheez-
ing, all three codes should be assigned. In addi-
tion to finding appropriate codes for each condition,
complex rules guide code assignment. For exam-
ple, a diagnosis code should always be assigned if a
diagnosis is reached, a diagnosis code should never
</bodyText>
<footnote confidence="0.995885">
1http://www.cdc.gov/nchs/about/otheract/
icd9/abticd9.htm
</footnote>
<bodyText confidence="0.999949666666667">
be assigned when the diagnosis is unclear, a symp-
tom should never be assigned when a diagnosis is
present, and the most specific code is preferred. This
means that codes that seem appropriate to a report
should be omitted in specific cases. For example,
a patient with hallucinations should be coded 780.1
(hallucinations) but for visual hallucinations, the
correct code is 368.16. The large number of codes
and complexity of assignment rules make this a diffi-
cult problem for humans (inter-annotator agreement
is low). Therefore, an automated system that sug-
gested or assigned codes could make medical data
more consistent.
These complexities make the problem difficult
for NLP systems. Consider the task as multi-class,
multi-label. For a given document, many codes may
seem appropriate but it may not be clear to the algo-
rithm how many to assign. Furthermore, the codes
are not independent and different labels can inter-
act to either increase or decrease the likelihood of
the other. Consider a report that says, “patient re-
ports cough and fever.” The presence of the words
cough and fever indicate codes 786.2 (cough) and
780.6 (fever). However, if the report continues to
state that “patient has pneumonia” then these codes
are dropped in favor of 486 (pneumonia). Further-
more, if the report then says “verify clinically”, then
the diagnosis is uncertain and only codes 786.2 and
780.6 apply. Clearly, this is a challenging problem,
especially for an automated system.
</bodyText>
<subsectionHeader confidence="0.99016">
2.1 Corpus
</subsectionHeader>
<bodyText confidence="0.999744785714286">
We built and evaluated our system in accordance
with the Computational Medicine Center’s (CMC)
2007 Medical Natural Language Processing Chal-
lenge.2 Since release of medical data must strictly
follow HIPAA standards, the challenge corpus un-
derwent extensive treatment for disambiguation,
anonymization, and careful scrubbing. A detailed
description of data preparation is found in Compu-
tational Medicine Center (2007). We describe the
corpus here to provide context for our task.
The training corpus is comprised of 978 radiolog-
ical reports taken from real medical records. A test
corpus contains 976 unlabeled documents. Radiol-
ogy reports have two text fields, clinical history and
</bodyText>
<footnote confidence="0.980733">
2www.computationalmedicine.org/challenge
</footnote>
<page confidence="0.997048">
130
</page>
<bodyText confidence="0.999993477611941">
impression. The physician ordering the x-ray writes
the clinical history, which contains patient informa-
tion for the radiologist, including history and current
symptoms. Sometimes a guess as to the diagnosis
appears (“evaluate for asthma”). The descriptions
are sometimes whole sentences and other times sin-
gle words (“cough”). The radiologist writes the im-
pression to summarize his or her findings. It con-
tains a short analysis and often times a best guess as
to the diagnosis. At times this field is terse, (“pneu-
monia” or “normal kidneys”) and at others it con-
tains an entire paragraph of text. Together, these two
fields are used to assign ICD-9-CM codes, which
justify a certain procedure, possibly for reimburse-
ment by the insurance company.
Only a small percentage of ICD-9-CM codes ap-
pear in the challenge. In total, the reports include 45
different codes arranged in 94 configurations (com-
binations). Some of these codes appear frequently,
while others are rare, appearing only a single time.
The test set is restricted so that each configuration
appears at least once in the training set, although
there is no further guarantee as to the test set’s distri-
bution over codes. Therefore, in addition to a large
number of codes, there is variability in the amount
of data for each code. Four codes have over 100
examples each and 24 codes have 10 or fewer doc-
uments, with 10 of these codes having only a single
document.
Since code annotation is a difficult task, each doc-
ument in the corpus was evaluated by three expert
annotators. A gold annotation was created by tak-
ing the majority of the annotators; if two of the three
annotators provided a code, that code is used in the
gold configuration. This approach means that a doc-
ument’s configuration may be a construction of mul-
tiple annotators and may not match any of the three
annotators exactly. Both the individual and the ma-
jority annotations are included with the training cor-
pus.
While others have attempted ICD-9 code classi-
fication, our task differs in two respects (Section 7
provides an overview of previous work). First, pre-
vious work has used discharge reports, which are
typically longer with more text fields. Second, while
most systems are evaluated as a recommendation
system, offering the top k codes and then scoring
recall at k, our task is to provide the exact configu-
ration. The CMC challenge evaluated systems using
an F1 score, so we are penalized if we suggest any
label that does not appear in the majority annotation.
To estimate task difficulty we measured the inter-
annotator score for the training set using the three
annotations provided. We scored two annotations
with the micro average F1, which weighs each code
assignment equally (see Section 5 for details on
evaluation metrics). If an annotator omitted a code
and included an extra code, he or she is penalized
with a false positive (omitting a code) and a false
negative (adding an extra code). We measured anno-
tators against each other; the average f-measure was
74.85 (standard deviation of .06). These scores were
low since annotators chose from an unrestricted set
of codes, many of which were not included in the fi-
nal majority annotation. However, these scores still
indicate the human accuracy for this task using an
unrestricted label set. 3
</bodyText>
<sectionHeader confidence="0.954608" genericHeader="method">
3 Code Assignment System
</sectionHeader>
<bodyText confidence="0.999998307692308">
We developed three automated systems guided by
our above analysis. First, we designed a learning
system that used natural language features from the
official code descriptions and the text of each re-
port. It is general purpose and labels all 45 codes
and 94 configurations (labels). Second, we built a
rule based system that assigned codes based on the
overlap between the reports and code descriptions,
similar to how an annotator may search code de-
scriptions for appropriate labels. Finally, a special-
ized system aimed at the most common codes imple-
mented a policy that mimics the guidelines a medical
staffer would use to assign these codes.
</bodyText>
<subsectionHeader confidence="0.999944">
3.1 Learning System
</subsectionHeader>
<bodyText confidence="0.96208075">
We begin with some notational definitions. In what
follows, x denotes the generic input document (ra-
diology report), Y denotes the set of possible label-
ings (code configurations) of x, and y*(x) the cor-
rect labeling of x. For each pair of document x
and labeling y E Y , we compute a vector-valued
feature representation f(x, y). A linear model is
3We also measured each annotator with the majority codes,
taking the average score (87.48), and the best annotator with
the majority label (92.8). However, these numbers are highly
biased since the annotator influences the majority labeling. We
observe that our final system still exceeds the average score.
</bodyText>
<page confidence="0.992525">
131
</page>
<bodyText confidence="0.999971933333333">
given by a weight vector w. Given this weight vec-
tor w, the score w · f(x, y) ranks possible labelings
of x, and we denote by Yk�,,,(x) the set of k top
scoring labelings for x. For some structured prob-
lems, a factorization of f(x, y) is required to enable
a dynamic program for inference. For our problem,
we know all the possible configurations in advance
(there are 94 of them) so we can pick the highest
scoring y E Y by trying them all. For each docu-
ment x and possible labeling y, we compute a score
using w and the feature representation f(x, y). The
top scoring y is output as the correct label. Section
3.1.1 describes our feature function f(x, y) while
Section 3.1.2 describes how we find a good weight
vector w.
</bodyText>
<sectionHeader confidence="0.641948" genericHeader="method">
3.1.1 Features
</sectionHeader>
<bodyText confidence="0.999951500000001">
Problem representation is one of the most impor-
tant aspects of a learning system. In our case, this
is defined by the set of features f(x, y). Ideally we
would like a linear combination of our features to ex-
actly specify the true labeling of all the instances, but
we want to have a small total number of features so
that we can accurately estimate their values. We sep-
arate our features into two classes: label specific fea-
tures and transfer features. For simplicity, we index
features by their name. Label specific features are
only present for a single label. For example, a simple
class of label specific features is the conjunction of a
word in the document with an ICD-9-CM code in the
label. Thus, for each word we create 94 features, i.e.
the word conjoined with every label. These features
tend to be very powerful, since weights for them can
encode very specific information about the way doc-
tors talk about a disease, such as the feature “con-
tains word pneumonia and label contains code 486”.
Unfortunately, the cost of this power is that there are
a large number of these features, making parameter
estimation difficult for rare labels. In contrast, trans-
fer features can be present in multiple labels. An
example of a transfer feature might be “the impres-
sion contains all the words in the code descriptions
of the codes in this label”. Transfer features allow us
to generalize from one label to another by learning
things like “if all the words of the label description
occur in the impression, then this label is likely” but
have the drawback that we cannot learn specific de-
tails about common labels. For example, we cannot
learn that the word “pneumonia” in the impression
is negatively correlated with the code cough. The
inclusion of both label specific and transfer features
allows us to learn specificity where we have a large
number of examples and generality for rare codes.
Before feature extraction we normalized the re-
ports’ text by converting it to lower case and by
replacing all numbers (and digit sequences) with a
single token (NUM). We also prepared a synonym
dictionary for a subset of the tokens and n-grams
present in the training data. The synonym dictionary
was based on MeSH4, the Medical Subject Headings
vocabulary, in which synonyms are listed as terms
under the same concept. All ngrams and tokens
in the training data which had mappings defined in
the synonym dictionary were then replaced by their
normalized token; e.g. all mentions of “nocturnal
enuresis” or “nighttime urinary incontinence” were
replaced by the token “bedwetting”. Additionally,
we constructed descriptions for each code automati-
cally from the official ICD-9-CM code descriptions
in National Center for Health Statistics (2006). We
also created a mapping between code and code type
(diagnosis or symptom) using the guidelines.
Our system used the following features. The de-
scriptions of particular features are in quotes, while
schemes for constructing features are not.
</bodyText>
<listItem confidence="0.988671785714286">
• “this configuration contains a disease code”,
“this configuration contains a symptom code”,
“this configuration contains an ambiguous
code” and “this configuration contains both dis-
ease and symptom codes”.5
• With the exception of stop-words, all words of
the impression and history conjoined with each
label in the configuration; pairs of words con-
joined with each label; words conjoined with
pairs of labels. For example, “the impression
contains ‘pneumonia’ and the label contains
codes 786.2 and 780.6”.
• A feature indicating when the history or im-
pression contains a complete code description
</listItem>
<footnote confidence="0.9225006">
4www.nlm.nih.gov/mesh
5We included a feature for configurations that had both dis-
ease and symptom codes because they appeared in the training
data, even though coding guidelines prohibit these configura-
tions.
</footnote>
<page confidence="0.99293">
132
</page>
<bodyText confidence="0.999627625">
for the label; one for a word in common with
the code description for one of the codes in the
label; a common word conjoined with the pres-
ence of a negation word nearby (“no”, “not”,
etc.); a word in common with a code descrip-
tion not present in the label. We applied similar
features using negative words associated with
each code.
</bodyText>
<listItem confidence="0.973599785714286">
• A feature indicating when a soft negation word
appears in the text (“probable”, “possible”,
“suspected”, etc.) conjoined with words that
follow; the token length of a text field (“im-
pression length=3”); a conjunction of a feature
indicating a short text field with the words in
the field (“impression length=1 and ‘pneumo-
nia’ ”)
• A feature indicating each n-gram sequence that
appears in both the impression and clinical his-
tory; the conjunction of certain terms where
one appears in the history and the other in the
impression (e.g. “cough in history and pneu-
monia in impression”).
</listItem>
<subsectionHeader confidence="0.923993">
3.1.2 Learning Technique
</subsectionHeader>
<bodyText confidence="0.999988142857143">
Using these feature representations, we now learn
a weight vector w that scores the correct labelings
of the data higher than incorrect labelings. We used
a k-best version of the MIRA algorithm (Crammer,
2004; McDonald et al., 2005). MIRA is an online
learning algorithm that for each training document
x updates the weight vector w according to the rule:
</bodyText>
<equation confidence="0.998315">
wnew = arg min
w I Iw − woldI I
s.t. by E Yk,wold(x) :
w - f(x, y*(x)) − w - f(x, y) ? L(y*(x), y)
</equation>
<bodyText confidence="0.984729888888889">
where L(y*(x), y) is a measure of the loss of label-
ing y with respect to the correct labeling y*(x). For
our experiments, we set k to 30 and iterated over the
training data 10 times. Two standard modifications
to this approach also helped. First, rather than using
just the final weight vector, we average all weight
vectors. This has a smoothing effect that improves
performance on most problems. The second modifi-
cation is the introduction of slack variables:
</bodyText>
<equation confidence="0.943990166666667">
wnew = arg min
w I Iw − woldI I + γ � ξi
i
s.t. by E Yk,wold(x) :
w - f(x, y*(x)) − w - f(x, y) ? L(y*(x), y) − ξi
bi E 11 ... kJ : ξi ? 0.
</equation>
<bodyText confidence="0.998135">
We used a γ of 10−3 in our experiments.
The most straightforward loss function is the 0/1
loss, which is one if y does not equal y*(x) and zero
otherwise. Since we are evaluated based on the num-
ber of false negative and false positive ICD-9-CM
codes assigned to all the documents, we used a loss
that is the sum of the number of false positive and the
number of false negative labels that y assigns with
respect to y*(x).
Finally, we only used features that were possi-
ble for some labeling of the test data by using only
the test data to construct our feature alphabet. This
forced the learner to focus on hypotheses that could
be used at test time and resulted in a 1% increase in
F-measure in our final system on the test data.
</bodyText>
<subsectionHeader confidence="0.99984">
3.2 Rule Based System
</subsectionHeader>
<bodyText confidence="0.99996252173913">
Since some of the configurations appear a small
number of times in our corpus (some only once),
we built a rule based system that requires no train-
ing. The system uses a description of the ICD-9-CM
codes and their types, similar to the list used by our
learning system (Section 3.1.1). The code descrip-
tions include between one and four short descrip-
tions, such as “reactive airway disease”, “asthma”,
and “chronic obstructive pulmonary disease”. We
treat each of these descriptions as a bag of words.
For a given report, the system parses both the clini-
cal history and impression into sentences, using “.”
as a sentence divider. Each sentence is the checked
to see if all of the words in a code description appear
in the sentence. If a match is found, we set a flag
corresponding to the code. However, if the code is
a disease, we search for a negation word in the sen-
tence, removing the flag if a negation word is found.
Once all code descriptions have been evaluated, we
check if there are any flags set for disease codes. If
so, we remove all symptom code flags. We then emit
a code corresponding to each set flag. This simple
system does not enforce configuration restrictions;
</bodyText>
<page confidence="0.998112">
133
</page>
<bodyText confidence="0.9994616">
we may predict a code configuration that does not
appear in our training data. Adding this restriction
improved precision but hurt recall, leading to a slight
decrease in F1 score. We therefore omitted the re-
striction from our system.
</bodyText>
<subsectionHeader confidence="0.999767">
3.3 Automatic Coding Policies
</subsectionHeader>
<bodyText confidence="0.999931">
As we described in Section 2, enforcing coding
guidelines can be a complex task. While a learning
system may have trouble coding a document, a hu-
man may be able to define a simple policy for cod-
ing. Since some of the most frequent codes in our
dataset have this property, we decided to implement
such an automatic coding policy. We selected two
related sets of codes to target with a rule based sys-
tem, a set of codes found in pneumonia reports and
a set for urinary tract infection/reflux reports.
Reports related to pneumonia are the most com-
mon in our dataset and include codes for pneumo-
nia, asthma, fever, cough and wheezing; we handle
them with a single policy. Our policy is as follows:
</bodyText>
<listItem confidence="0.988970416666667">
• Search for a small set of keywords (e.g.
“cough”, “fever”) to determine if a code should
be applied.
• If “pneumonia” appears unnegated in the im-
pression and the impression is short, or if it oc-
curs in the clinical history and is not preceded
by phrases such as “evaluate for” or “history
of”, apply pneumonia code and stop.
• Use the same rule to code asthma by looking
for “asthma” or “reactive airway disease”.
• If no diagnosis is found, code all non-negated
symptoms (cough, fever, wheezing).
</listItem>
<bodyText confidence="0.9999775">
We selected 80% of the training set to evaluate in the
construction of our rules. We then ran the finished
system on both this training set and the held out 20%
of the data. The system achieved F1 scores of 87%
on the training set and 84% on the held out data for
these five codes. The comparable scores indicates
that we did not over-fit the training data.
We designed a similar policy for two other related
codes, urinary tract infection and vesicoureteral re-
flux. We found these codes to be more complex as
they included a wide range of kidney disorders. On
these two codes, our system achieved 78% on the
train set and 76% on the held out data. Overall, au-
tomatically applying our two policies yielded high
confidence predictions for a significant subset of the
corpus.
</bodyText>
<sectionHeader confidence="0.965472" genericHeader="method">
4 Combined System
</sectionHeader>
<bodyText confidence="0.99996295">
Since our three systems take complimentary ap-
proaches to the problem, we combined them to im-
prove performance. First, we took our automatic
policy and rule based systems and cascaded them; if
the automatic policy system does not apply a code,
the rule based system classifies the report. We used
a cascaded approach since the automatic policy sys-
tem was very accurate when it was able to assign
a code. Therefore, the rule based system defers to
the policy system when it is triggered. Next, we in-
cluded the prediction of the cascaded system as a
feature for our learning system. We used two fea-
ture rules: “cascaded-system predicted exactly this
label” and “cascaded-system predicted one of the
codes in this label”. As we show, this yielded our
most accurate system. While we could have used a
meta-classifier to combine the three systems, includ-
ing the rule based systems as features to the learning
system allowed it to learn the appropriate weights
for the rule based predictions.
</bodyText>
<sectionHeader confidence="0.991546" genericHeader="method">
5 Evaluation Metric
</sectionHeader>
<bodyText confidence="0.999977">
Evaluation metrics for this task are often based on
recommendation systems, where the system returns
a list of the top k codes for selection by the user. As
a result, typical metrics are “recall at k” and aver-
age precision (Larkey and Croft, 1995). Instead, our
goal was to predict the exact configuration, returning
exactly the number of codes predicted to be on the
report. The competition used a micro-averaged F1
score to evaluate predictions. A contingency table
(confusion matrix) is computed by summing over
each predicted code for each document by predic-
tion type (true positive, false positive, false negative)
weighing each code assignment equally. F1 score
is computed based on the resultant table. If specific
codes or under-coding is favored, we can modify our
learning loss function as described in Section 3.1.2.
A detailed treatment of this evaluation metric can be
found in Computational Medicine Center (2007).
</bodyText>
<page confidence="0.995086">
134
</page>
<table confidence="0.999850666666667">
System Precision Recall F1
BL 61.86 72.58 66.79
RULE 81.9 82.0 82.0
CASCADE 86.04 84.56 85.3
LEARN 85.5 83.6 84.6
CASCADE+LEARN 87.1 85.9 86.5
</table>
<tableCaption confidence="0.818357833333333">
Table 1: Performance of our systems on the provided
labeled training data (F1 score). The learning sys-
tems (CASCADE+LEARN and LEARN) were eval-
uated on ten random split of the data while RULE
was evaluated on all of the training data. We include
a simple rule based system (BL ) as a baseline.
</tableCaption>
<sectionHeader confidence="0.999956" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999918555555556">
We evaluated our systems on the labeled training
data of 978 radiology reports. For each report, each
system predicted an exact configuration of codes
(i.e. one of 94 possible labels). We score each sys-
tem using a micro-averaged F1 score. Since we only
had labels for the training data, we divided the data
using an 80/20 training test split and averaged results
over 10 runs for our learning systems. We evaluated
the following systems:
</bodyText>
<listItem confidence="0.420874">
•
</listItem>
<bodyText confidence="0.9887165">
RULE: The rule based system based on ICD-
9-CM code descriptions (Section 3.2).
CASCADE: The automatic code policy system
(Section 3.3) cascaded with RULE (Section 4).
LEARN: The learning system with both label
specific and transfer features (Section 3.1).
</bodyText>
<listItem confidence="0.797258666666667">
• CASCADE+LEARN : Our combined system
that incorporates CASCADE predictions as a
feature to LEARN (Section 4).
</listItem>
<bodyText confidence="0.999670222222222">
For a baseline, we built a simple system that ap-
plies the official ICD-9-CM code descriptions to find
the correct labels (BL ). For each code in the train-
ing set, the system generates text-segments related to
it. During testing, for each new document, the sys-
tem checks if any text-segment (as discovered dur-
ing training) appears in the document. If so, the cor-
responding code is predicted. The results from our
four systems and baseline are shown in Table 1.
</bodyText>
<table confidence="0.9981674">
System Train Test
CASCADE 85.3 84
CASCADE+LEARN 86.5 87.60
Average - 76.6
Best - 89.08
</table>
<tableCaption confidence="0.987382">
Table 2: Performance of two systems on the train
</tableCaption>
<bodyText confidence="0.998305041666667">
and test data. Results obtained from the web sub-
mission interface were rounded. Average and Best
are the average and best f-measures of the 44 sub-
mitted systems (standard deviation 13.40).
Each of our systems easily beats the baseline, and
the average inter-annotator score for this task. Ad-
ditionally, we were able to evaluate two of our sys-
tems on the test data using a web interface as pro-
vided by the competition. The test set contains 976
documents (about the same as the training set) and
is drawn the from same distribution as the training
data. Our test results were comparable to perfor-
mance on the training data, showing that we did
not over-fit to the training data (Table 2). Addi-
tionally, our combined system (CASCADE+LEARN
) achieved a score of 87.60%, beating our training
data performance and exceeding the average inter-
annotator score. Out of 44 submitted systems, the
average score on test data was 76.7% (standard devi-
ation of 13.40) and the maximum score was 89.08%.
Our system scored 4th overall and was less than
1.5% behind the best system. Overall, in comparison
with our baselines and over 40 systems, we perform
very well on this task.
</bodyText>
<sectionHeader confidence="0.999265" genericHeader="related work">
7 Related Work
</sectionHeader>
<figure confidence="0.7282915">
•
•
</figure>
<bodyText confidence="0.999886846153846">
There have been several attempts at ICD-9-CM
code classification and related problems for med-
ical records. The specific problem of ICD-9-CM
code assignment was studied by Lussier et al. (2000)
through an exploratory study. Larkey and Croft
(1995) designed classifiers for the automatic assign-
ment of ICD-9 codes to discharge summaries. Dis-
charge summaries tend to be considerably longer
than our data and contain multiple text fields. Ad-
ditionally, the number of codes per document has
a larger range, varying between 1 and 15 codes.
Larkey and Croft use three classifiers: K-nearest
neighbors, relevance feedback, and bayesian inde-
</bodyText>
<page confidence="0.996493">
135
</page>
<bodyText confidence="0.999976178571428">
pendence. Similar to our approach, they tag items
as negated and try to identify diagnosis and symp-
tom terms. Additionally, their final system combines
all three models. A direct comparison is not possi-
ble due to the difference in data and evaluation met-
rics; they use average precision and recall at k. On
a comparable metric, “principal code is top candi-
date”, their best system achieves 59.9% accuracy. de
Lima et al. (1998) rely on the hierarchical nature of
medical codes to design a hierarchical classification
scheme. This approach is likely to help on our task
as well but we were unable to test this since the lim-
ited number of codes removes any hierarchy. Other
approaches have used a variety of NLP techniques
(Satomura and Amaral, 1992).
Others have used natural language systems for the
analysis of medical records (Zweigenbaum, 1994).
Chapman and Haug (1999) studied radiology re-
ports looking for cases of pneumonia, a goal sim-
ilar to that of our automatic coding policy system.
Meystre and Haug (2005) processed medical records
to harvest potential entries for a medical problem
list, an important part of electronic medical records.
Chuang et al. (2002) studied Charlson comorbidi-
ties derived from processing discharge reports and
chest x-ray reports and compared them with admin-
istrative data. Additionally, Friedman et al. (1994)
applies NLP techniques to radiology reports.
</bodyText>
<sectionHeader confidence="0.99926" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999935">
We have presented a learning system that processes
radiology reports and assigns ICD-9-CM codes.
Each of our systems achieves results comparable
with an inter-annotator baseline for our training data.
A combined system improves over each individ-
ual system. Finally, we show that on test data un-
available during system development, our final sys-
tem continues to perform well, exceeding the inter-
annotator baseline and achieving the 4th best score
out of 44 systems entered in the CMC challenge.
</bodyText>
<sectionHeader confidence="0.998502" genericHeader="acknowledgments">
9 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9963843">
We thank Andrew Lippa for his extensive medical
wisdom. Dredze is supported by an NDSEG fel-
lowship; Ganchev and Talukdar by NSF ITR EIA-
0205448; and Crammer by DARPA under Contract
No. NBCHD03001. Any opinions, findings, and
conclusions or recommendations expressed in this
material are those of the author(s) and do not nec-
essarily reflect the views of the DARPA or the De-
partment of Interior-National Business Center (DOI-
NBC).
</bodyText>
<sectionHeader confidence="0.998452" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999716022727273">
W.W. Chapman and P.J. Haug. 1999. Comparing expert sys-
tems for identifying chest x-ray reports that support pneu-
monia. In AMIA Symposium, pages 216–20.
JH Chuang, C Friedman, and G Hripcsak. 2002. A com-
parison of the charlson comorbidities derived from medical
language processing and administrative data. AMIA Sympo-
sium, pages 160–4.
Computational Medicine Center. 2007. The
computational medicine center’s 2007 med-
ical natural language processing challenge.
http://computationalmedicine.org/challenge/index.php.
Koby Crammer. 2004. Online Learning of Complex Categorial
Problems. Ph.D. thesis, Hebrew Univeristy of Jerusalem.
Luciano R. S. de Lima, Alberto H. F. Laender, and Berthier A.
Ribeiro-Neto. 1998. A hierarchical approach to the auto-
matic categorization of medical documents. In CIKM.
C Friedman, PO Alderson, JH Austin, JJ Cimino, and SB John-
son. 1994. A general natural-language text processor for
clinical radiology. Journal of the American Medical Infor-
matics Association, 1:161–74.
Leah S. Larkey and W. Bruce Croft. 1995. Automatic assign-
ment of icd9 codes to discharge summaries. Technical re-
port, University of Massachusetts at Amherst, Amherst, MA.
YA Lussier, C Friedman, L Shagina, and P Eng. 2000. Au-
tomating icd-9-cm encoding using medical language pro-
cessing: A feasibility study.
Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005.
Flexible text segmentation with structured multilabel classi-
fication. In HLT/EMNLP.
Stephane Meystre and Peter J Haug. 2005. Automation of a
problem list using natural language processing. BMC Medi-
cal Informatics and Decision Making.
National Center for Health Statistics. 2006. Icd-
9-cm official guidelines for coding and reporting.
http://www.cdc.gov/nchs/datawh/ftpserv/ftpicd9/ftpicd9.htm.
Y Satomura and MB Amaral. 1992. Automated diagnostic in-
dexing by natural language processing. Medical Informat-
ics, 17:149–163.
Edward H. Shortliffe and James J. Cimino, editors. 2006.
Biomedical Informatics: Computer Applications in Health
Care and Biomedicine. Springer.
P. Zweigenbaum. 1994. Menelas: an access system for medical
records using natural language. Comput Methods Programs
Biomed, 45:117–20.
</reference>
<page confidence="0.998799">
136
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.438727">
<title confidence="0.997741">Automatic Code Assignment to Medical Text</title>
<author confidence="0.838489">Crammer Dredze Ganchev Pratim</author>
<affiliation confidence="0.92435">Department of Computer and Information Science, University of Pennsylvania, Philadelphia,</affiliation>
<author confidence="0.929286">Steven Carroll</author>
<affiliation confidence="0.993892">Division of Oncology, The Children’s Hospital of Philadelphia, Philadelphia,</affiliation>
<email confidence="0.999858">carroll@genome.chop.edu</email>
<abstract confidence="0.96371">Code assignment is important for handling large amounts of electronic medical data in the modern hospital. However, only expert annotators with extensive training can assign codes. We present a system for the assignment of ICD-9-CM clinical codes to free text radiology reports. Our system assigns a code configuration, predicting one or more codes for each document. We combine three coding systems into a single learning system for higher accuracy. We compare our system on a real world medical dataset with both human annotators and other automated systems, achieving nearly the maximum score on the Computational Medicine Center’s challenge.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>W W Chapman</author>
<author>P J Haug</author>
</authors>
<title>Comparing expert systems for identifying chest x-ray reports that support pneumonia.</title>
<date>1999</date>
<booktitle>In AMIA Symposium,</booktitle>
<pages>216--20</pages>
<contexts>
<context position="29806" citStr="Chapman and Haug (1999)" startWordPosition="4960" endWordPosition="4963">n metrics; they use average precision and recall at k. On a comparable metric, “principal code is top candidate”, their best system achieves 59.9% accuracy. de Lima et al. (1998) rely on the hierarchical nature of medical codes to design a hierarchical classification scheme. This approach is likely to help on our task as well but we were unable to test this since the limited number of codes removes any hierarchy. Other approaches have used a variety of NLP techniques (Satomura and Amaral, 1992). Others have used natural language systems for the analysis of medical records (Zweigenbaum, 1994). Chapman and Haug (1999) studied radiology reports looking for cases of pneumonia, a goal similar to that of our automatic coding policy system. Meystre and Haug (2005) processed medical records to harvest potential entries for a medical problem list, an important part of electronic medical records. Chuang et al. (2002) studied Charlson comorbidities derived from processing discharge reports and chest x-ray reports and compared them with administrative data. Additionally, Friedman et al. (1994) applies NLP techniques to radiology reports. 8 Conclusion We have presented a learning system that processes radiology repor</context>
</contexts>
<marker>Chapman, Haug, 1999</marker>
<rawString>W.W. Chapman and P.J. Haug. 1999. Comparing expert systems for identifying chest x-ray reports that support pneumonia. In AMIA Symposium, pages 216–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>JH Chuang</author>
<author>C Friedman</author>
<author>G Hripcsak</author>
</authors>
<title>A comparison of the charlson comorbidities derived from medical language processing and administrative data. AMIA Symposium,</title>
<date>2002</date>
<pages>160--4</pages>
<contexts>
<context position="30103" citStr="Chuang et al. (2002)" startWordPosition="5008" endWordPosition="5011">to help on our task as well but we were unable to test this since the limited number of codes removes any hierarchy. Other approaches have used a variety of NLP techniques (Satomura and Amaral, 1992). Others have used natural language systems for the analysis of medical records (Zweigenbaum, 1994). Chapman and Haug (1999) studied radiology reports looking for cases of pneumonia, a goal similar to that of our automatic coding policy system. Meystre and Haug (2005) processed medical records to harvest potential entries for a medical problem list, an important part of electronic medical records. Chuang et al. (2002) studied Charlson comorbidities derived from processing discharge reports and chest x-ray reports and compared them with administrative data. Additionally, Friedman et al. (1994) applies NLP techniques to radiology reports. 8 Conclusion We have presented a learning system that processes radiology reports and assigns ICD-9-CM codes. Each of our systems achieves results comparable with an inter-annotator baseline for our training data. A combined system improves over each individual system. Finally, we show that on test data unavailable during system development, our final system continues to pe</context>
</contexts>
<marker>Chuang, Friedman, Hripcsak, 2002</marker>
<rawString>JH Chuang, C Friedman, and G Hripcsak. 2002. A comparison of the charlson comorbidities derived from medical language processing and administrative data. AMIA Symposium, pages 160–4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Computational Medicine Center</author>
</authors>
<title>The computational medicine center’s</title>
<date>2007</date>
<note>http://computationalmedicine.org/challenge/index.php.</note>
<contexts>
<context position="7646" citStr="Center (2007)" startWordPosition="1171" endWordPosition="1172">e report then says “verify clinically”, then the diagnosis is uncertain and only codes 786.2 and 780.6 apply. Clearly, this is a challenging problem, especially for an automated system. 2.1 Corpus We built and evaluated our system in accordance with the Computational Medicine Center’s (CMC) 2007 Medical Natural Language Processing Challenge.2 Since release of medical data must strictly follow HIPAA standards, the challenge corpus underwent extensive treatment for disambiguation, anonymization, and careful scrubbing. A detailed description of data preparation is found in Computational Medicine Center (2007). We describe the corpus here to provide context for our task. The training corpus is comprised of 978 radiological reports taken from real medical records. A test corpus contains 976 unlabeled documents. Radiology reports have two text fields, clinical history and 2www.computationalmedicine.org/challenge 130 impression. The physician ordering the x-ray writes the clinical history, which contains patient information for the radiologist, including history and current symptoms. Sometimes a guess as to the diagnosis appears (“evaluate for asthma”). The descriptions are sometimes whole sentences a</context>
<context position="25263" citStr="Center (2007)" startWordPosition="4196" endWordPosition="4197">ng exactly the number of codes predicted to be on the report. The competition used a micro-averaged F1 score to evaluate predictions. A contingency table (confusion matrix) is computed by summing over each predicted code for each document by prediction type (true positive, false positive, false negative) weighing each code assignment equally. F1 score is computed based on the resultant table. If specific codes or under-coding is favored, we can modify our learning loss function as described in Section 3.1.2. A detailed treatment of this evaluation metric can be found in Computational Medicine Center (2007). 134 System Precision Recall F1 BL 61.86 72.58 66.79 RULE 81.9 82.0 82.0 CASCADE 86.04 84.56 85.3 LEARN 85.5 83.6 84.6 CASCADE+LEARN 87.1 85.9 86.5 Table 1: Performance of our systems on the provided labeled training data (F1 score). The learning systems (CASCADE+LEARN and LEARN) were evaluated on ten random split of the data while RULE was evaluated on all of the training data. We include a simple rule based system (BL ) as a baseline. 6 Results We evaluated our systems on the labeled training data of 978 radiology reports. For each report, each system predicted an exact configuration of cod</context>
</contexts>
<marker>Center, 2007</marker>
<rawString>Computational Medicine Center. 2007. The computational medicine center’s 2007 medical natural language processing challenge. http://computationalmedicine.org/challenge/index.php.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
</authors>
<title>Online Learning of Complex Categorial Problems.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Hebrew Univeristy of Jerusalem.</institution>
<contexts>
<context position="18291" citStr="Crammer, 2004" startWordPosition="2950" endWordPosition="2951"> conjunction of a feature indicating a short text field with the words in the field (“impression length=1 and ‘pneumonia’ ”) • A feature indicating each n-gram sequence that appears in both the impression and clinical history; the conjunction of certain terms where one appears in the history and the other in the impression (e.g. “cough in history and pneumonia in impression”). 3.1.2 Learning Technique Using these feature representations, we now learn a weight vector w that scores the correct labelings of the data higher than incorrect labelings. We used a k-best version of the MIRA algorithm (Crammer, 2004; McDonald et al., 2005). MIRA is an online learning algorithm that for each training document x updates the weight vector w according to the rule: wnew = arg min w I Iw − woldI I s.t. by E Yk,wold(x) : w - f(x, y*(x)) − w - f(x, y) ? L(y*(x), y) where L(y*(x), y) is a measure of the loss of labeling y with respect to the correct labeling y*(x). For our experiments, we set k to 30 and iterated over the training data 10 times. Two standard modifications to this approach also helped. First, rather than using just the final weight vector, we average all weight vectors. This has a smoothing effect</context>
</contexts>
<marker>Crammer, 2004</marker>
<rawString>Koby Crammer. 2004. Online Learning of Complex Categorial Problems. Ph.D. thesis, Hebrew Univeristy of Jerusalem.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luciano R S de Lima</author>
<author>Alberto H F Laender</author>
<author>Berthier A Ribeiro-Neto</author>
</authors>
<title>A hierarchical approach to the automatic categorization of medical documents.</title>
<date>1998</date>
<booktitle>In CIKM.</booktitle>
<marker>de Lima, Laender, Ribeiro-Neto, 1998</marker>
<rawString>Luciano R. S. de Lima, Alberto H. F. Laender, and Berthier A. Ribeiro-Neto. 1998. A hierarchical approach to the automatic categorization of medical documents. In CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Friedman</author>
<author>PO Alderson</author>
<author>JH Austin</author>
<author>JJ Cimino</author>
<author>SB Johnson</author>
</authors>
<title>A general natural-language text processor for clinical radiology.</title>
<date>1994</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<pages>1--161</pages>
<contexts>
<context position="30281" citStr="Friedman et al. (1994)" startWordPosition="5033" endWordPosition="5036">omura and Amaral, 1992). Others have used natural language systems for the analysis of medical records (Zweigenbaum, 1994). Chapman and Haug (1999) studied radiology reports looking for cases of pneumonia, a goal similar to that of our automatic coding policy system. Meystre and Haug (2005) processed medical records to harvest potential entries for a medical problem list, an important part of electronic medical records. Chuang et al. (2002) studied Charlson comorbidities derived from processing discharge reports and chest x-ray reports and compared them with administrative data. Additionally, Friedman et al. (1994) applies NLP techniques to radiology reports. 8 Conclusion We have presented a learning system that processes radiology reports and assigns ICD-9-CM codes. Each of our systems achieves results comparable with an inter-annotator baseline for our training data. A combined system improves over each individual system. Finally, we show that on test data unavailable during system development, our final system continues to perform well, exceeding the interannotator baseline and achieving the 4th best score out of 44 systems entered in the CMC challenge. 9 Acknowledgements We thank Andrew Lippa for hi</context>
</contexts>
<marker>Friedman, Alderson, Austin, Cimino, Johnson, 1994</marker>
<rawString>C Friedman, PO Alderson, JH Austin, JJ Cimino, and SB Johnson. 1994. A general natural-language text processor for clinical radiology. Journal of the American Medical Informatics Association, 1:161–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leah S Larkey</author>
<author>W Bruce Croft</author>
</authors>
<title>Automatic assignment of icd9 codes to discharge summaries.</title>
<date>1995</date>
<tech>Technical report,</tech>
<institution>University of Massachusetts at Amherst,</institution>
<location>Amherst, MA.</location>
<contexts>
<context position="24583" citStr="Larkey and Croft, 1995" startWordPosition="4089" endWordPosition="4092">ted exactly this label” and “cascaded-system predicted one of the codes in this label”. As we show, this yielded our most accurate system. While we could have used a meta-classifier to combine the three systems, including the rule based systems as features to the learning system allowed it to learn the appropriate weights for the rule based predictions. 5 Evaluation Metric Evaluation metrics for this task are often based on recommendation systems, where the system returns a list of the top k codes for selection by the user. As a result, typical metrics are “recall at k” and average precision (Larkey and Croft, 1995). Instead, our goal was to predict the exact configuration, returning exactly the number of codes predicted to be on the report. The competition used a micro-averaged F1 score to evaluate predictions. A contingency table (confusion matrix) is computed by summing over each predicted code for each document by prediction type (true positive, false positive, false negative) weighing each code assignment equally. F1 score is computed based on the resultant table. If specific codes or under-coding is favored, we can modify our learning loss function as described in Section 3.1.2. A detailed treatmen</context>
<context position="28544" citStr="Larkey and Croft (1995)" startWordPosition="4753" endWordPosition="4756"> and exceeding the average interannotator score. Out of 44 submitted systems, the average score on test data was 76.7% (standard deviation of 13.40) and the maximum score was 89.08%. Our system scored 4th overall and was less than 1.5% behind the best system. Overall, in comparison with our baselines and over 40 systems, we perform very well on this task. 7 Related Work • • There have been several attempts at ICD-9-CM code classification and related problems for medical records. The specific problem of ICD-9-CM code assignment was studied by Lussier et al. (2000) through an exploratory study. Larkey and Croft (1995) designed classifiers for the automatic assignment of ICD-9 codes to discharge summaries. Discharge summaries tend to be considerably longer than our data and contain multiple text fields. Additionally, the number of codes per document has a larger range, varying between 1 and 15 codes. Larkey and Croft use three classifiers: K-nearest neighbors, relevance feedback, and bayesian inde135 pendence. Similar to our approach, they tag items as negated and try to identify diagnosis and symptom terms. Additionally, their final system combines all three models. A direct comparison is not possible due </context>
</contexts>
<marker>Larkey, Croft, 1995</marker>
<rawString>Leah S. Larkey and W. Bruce Croft. 1995. Automatic assignment of icd9 codes to discharge summaries. Technical report, University of Massachusetts at Amherst, Amherst, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>YA Lussier</author>
<author>C Friedman</author>
<author>L Shagina</author>
<author>P Eng</author>
</authors>
<title>Automating icd-9-cm encoding using medical language processing: A feasibility study.</title>
<date>2000</date>
<contexts>
<context position="28490" citStr="Lussier et al. (2000)" startWordPosition="4745" endWordPosition="4748">ore of 87.60%, beating our training data performance and exceeding the average interannotator score. Out of 44 submitted systems, the average score on test data was 76.7% (standard deviation of 13.40) and the maximum score was 89.08%. Our system scored 4th overall and was less than 1.5% behind the best system. Overall, in comparison with our baselines and over 40 systems, we perform very well on this task. 7 Related Work • • There have been several attempts at ICD-9-CM code classification and related problems for medical records. The specific problem of ICD-9-CM code assignment was studied by Lussier et al. (2000) through an exploratory study. Larkey and Croft (1995) designed classifiers for the automatic assignment of ICD-9 codes to discharge summaries. Discharge summaries tend to be considerably longer than our data and contain multiple text fields. Additionally, the number of codes per document has a larger range, varying between 1 and 15 codes. Larkey and Croft use three classifiers: K-nearest neighbors, relevance feedback, and bayesian inde135 pendence. Similar to our approach, they tag items as negated and try to identify diagnosis and symptom terms. Additionally, their final system combines all </context>
</contexts>
<marker>Lussier, Friedman, Shagina, Eng, 2000</marker>
<rawString>YA Lussier, C Friedman, L Shagina, and P Eng. 2000. Automating icd-9-cm encoding using medical language processing: A feasibility study.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Flexible text segmentation with structured multilabel classification.</title>
<date>2005</date>
<booktitle>In HLT/EMNLP.</booktitle>
<contexts>
<context position="18315" citStr="McDonald et al., 2005" startWordPosition="2952" endWordPosition="2955"> a feature indicating a short text field with the words in the field (“impression length=1 and ‘pneumonia’ ”) • A feature indicating each n-gram sequence that appears in both the impression and clinical history; the conjunction of certain terms where one appears in the history and the other in the impression (e.g. “cough in history and pneumonia in impression”). 3.1.2 Learning Technique Using these feature representations, we now learn a weight vector w that scores the correct labelings of the data higher than incorrect labelings. We used a k-best version of the MIRA algorithm (Crammer, 2004; McDonald et al., 2005). MIRA is an online learning algorithm that for each training document x updates the weight vector w according to the rule: wnew = arg min w I Iw − woldI I s.t. by E Yk,wold(x) : w - f(x, y*(x)) − w - f(x, y) ? L(y*(x), y) where L(y*(x), y) is a measure of the loss of labeling y with respect to the correct labeling y*(x). For our experiments, we set k to 30 and iterated over the training data 10 times. Two standard modifications to this approach also helped. First, rather than using just the final weight vector, we average all weight vectors. This has a smoothing effect that improves performan</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Flexible text segmentation with structured multilabel classification. In HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephane Meystre</author>
<author>Peter J Haug</author>
</authors>
<title>Automation of a problem list using natural language processing.</title>
<date>2005</date>
<journal>BMC Medical Informatics and Decision Making.</journal>
<contexts>
<context position="29950" citStr="Meystre and Haug (2005)" startWordPosition="4985" endWordPosition="4988">9% accuracy. de Lima et al. (1998) rely on the hierarchical nature of medical codes to design a hierarchical classification scheme. This approach is likely to help on our task as well but we were unable to test this since the limited number of codes removes any hierarchy. Other approaches have used a variety of NLP techniques (Satomura and Amaral, 1992). Others have used natural language systems for the analysis of medical records (Zweigenbaum, 1994). Chapman and Haug (1999) studied radiology reports looking for cases of pneumonia, a goal similar to that of our automatic coding policy system. Meystre and Haug (2005) processed medical records to harvest potential entries for a medical problem list, an important part of electronic medical records. Chuang et al. (2002) studied Charlson comorbidities derived from processing discharge reports and chest x-ray reports and compared them with administrative data. Additionally, Friedman et al. (1994) applies NLP techniques to radiology reports. 8 Conclusion We have presented a learning system that processes radiology reports and assigns ICD-9-CM codes. Each of our systems achieves results comparable with an inter-annotator baseline for our training data. A combine</context>
</contexts>
<marker>Meystre, Haug, 2005</marker>
<rawString>Stephane Meystre and Peter J Haug. 2005. Automation of a problem list using natural language processing. BMC Medical Informatics and Decision Making.</rawString>
</citation>
<citation valid="true">
<title>for Health Statistics.</title>
<date>2006</date>
<institution>National Center</institution>
<note>http://www.cdc.gov/nchs/datawh/ftpserv/ftpicd9/ftpicd9.htm.</note>
<contexts>
<context position="16081" citStr="(2006)" startWordPosition="2594" endWordPosition="2594">-grams present in the training data. The synonym dictionary was based on MeSH4, the Medical Subject Headings vocabulary, in which synonyms are listed as terms under the same concept. All ngrams and tokens in the training data which had mappings defined in the synonym dictionary were then replaced by their normalized token; e.g. all mentions of “nocturnal enuresis” or “nighttime urinary incontinence” were replaced by the token “bedwetting”. Additionally, we constructed descriptions for each code automatically from the official ICD-9-CM code descriptions in National Center for Health Statistics (2006). We also created a mapping between code and code type (diagnosis or symptom) using the guidelines. Our system used the following features. The descriptions of particular features are in quotes, while schemes for constructing features are not. • “this configuration contains a disease code”, “this configuration contains a symptom code”, “this configuration contains an ambiguous code” and “this configuration contains both disease and symptom codes”.5 • With the exception of stop-words, all words of the impression and history conjoined with each label in the configuration; pairs of words conjoine</context>
</contexts>
<marker>2006</marker>
<rawString>National Center for Health Statistics. 2006. Icd9-cm official guidelines for coding and reporting. http://www.cdc.gov/nchs/datawh/ftpserv/ftpicd9/ftpicd9.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Satomura</author>
<author>MB Amaral</author>
</authors>
<title>Automated diagnostic indexing by natural language processing.</title>
<date>1992</date>
<journal>Medical Informatics,</journal>
<pages>17--149</pages>
<contexts>
<context position="29682" citStr="Satomura and Amaral, 1992" startWordPosition="4942" endWordPosition="4945">, their final system combines all three models. A direct comparison is not possible due to the difference in data and evaluation metrics; they use average precision and recall at k. On a comparable metric, “principal code is top candidate”, their best system achieves 59.9% accuracy. de Lima et al. (1998) rely on the hierarchical nature of medical codes to design a hierarchical classification scheme. This approach is likely to help on our task as well but we were unable to test this since the limited number of codes removes any hierarchy. Other approaches have used a variety of NLP techniques (Satomura and Amaral, 1992). Others have used natural language systems for the analysis of medical records (Zweigenbaum, 1994). Chapman and Haug (1999) studied radiology reports looking for cases of pneumonia, a goal similar to that of our automatic coding policy system. Meystre and Haug (2005) processed medical records to harvest potential entries for a medical problem list, an important part of electronic medical records. Chuang et al. (2002) studied Charlson comorbidities derived from processing discharge reports and chest x-ray reports and compared them with administrative data. Additionally, Friedman et al. (1994) </context>
</contexts>
<marker>Satomura, Amaral, 1992</marker>
<rawString>Y Satomura and MB Amaral. 1992. Automated diagnostic indexing by natural language processing. Medical Informatics, 17:149–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward H Shortliffe</author>
<author>J James</author>
</authors>
<date>2006</date>
<booktitle>Biomedical Informatics: Computer Applications in Health Care and Biomedicine.</booktitle>
<editor>Cimino, editors.</editor>
<publisher>Springer.</publisher>
<marker>Shortliffe, James, 2006</marker>
<rawString>Edward H. Shortliffe and James J. Cimino, editors. 2006. Biomedical Informatics: Computer Applications in Health Care and Biomedicine. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Zweigenbaum</author>
</authors>
<title>Menelas: an access system for medical records using natural language.</title>
<date>1994</date>
<booktitle>Comput Methods Programs Biomed,</booktitle>
<pages>45--117</pages>
<contexts>
<context position="29781" citStr="Zweigenbaum, 1994" startWordPosition="4958" endWordPosition="4959">n data and evaluation metrics; they use average precision and recall at k. On a comparable metric, “principal code is top candidate”, their best system achieves 59.9% accuracy. de Lima et al. (1998) rely on the hierarchical nature of medical codes to design a hierarchical classification scheme. This approach is likely to help on our task as well but we were unable to test this since the limited number of codes removes any hierarchy. Other approaches have used a variety of NLP techniques (Satomura and Amaral, 1992). Others have used natural language systems for the analysis of medical records (Zweigenbaum, 1994). Chapman and Haug (1999) studied radiology reports looking for cases of pneumonia, a goal similar to that of our automatic coding policy system. Meystre and Haug (2005) processed medical records to harvest potential entries for a medical problem list, an important part of electronic medical records. Chuang et al. (2002) studied Charlson comorbidities derived from processing discharge reports and chest x-ray reports and compared them with administrative data. Additionally, Friedman et al. (1994) applies NLP techniques to radiology reports. 8 Conclusion We have presented a learning system that </context>
</contexts>
<marker>Zweigenbaum, 1994</marker>
<rawString>P. Zweigenbaum. 1994. Menelas: an access system for medical records using natural language. Comput Methods Programs Biomed, 45:117–20.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>