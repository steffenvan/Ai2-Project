<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.838279">
A FULLY STATISTICAL APPROACH TO NATURAL LANGUAGE
INTERFACES
Scott Miller, David Stallard, Robert Bobrow, Richard Schwartz
BBN Systems and Technologies
70 Fawcett Street
</note>
<address confidence="0.626555">
Cambridge, MA 02138
</address>
<email confidence="0.980334">
szmiller@bbn.com, stallard@bbn.com, rusty @bbn.com, schwartz@bbn.com
</email>
<sectionHeader confidence="0.997305" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999438142857143">
We present a natural language interface system which is
based entirely on trained statistical models. The system
consists of three stages of processing: parsing, semantic
interpretation, and discourse. Each of these stages is
modeled as a statistical process. The models are fully
integrated, resulting in an end-to-end system that maps input
utterances into meaning representation frames.
</bodyText>
<sectionHeader confidence="0.998767" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.996318129032258">
A recent trend in natural language processing has been
toward a greater emphasis on statistical approaches,
beginning with the success of statistical part-of-speech
tagging programs (Church 1988), and continuing with other
work using statistical part-of-speech tagging programs, such
as BBN PLUM (Weischedel et al. 1993) and NYU Proteus
(Grishman and Sterling 1993). More recently, statistical
methods have been applied to domain-specific semantic
parsing (Miller et al. 1994), and to the more difficult problem
of wide-coverage syntactic parsing (Magerman 1995).
Nevertheless, most natural language systems remain
primarily rule based, and even systems that do use statistical
techniques, such as AT&amp;T Chronus (Levin and Pieraccini
1995), continue to require a significant rule based
component. Development of a complete end-to-end
statistical understanding system has been the focus of several
ongoing research efforts, including (Miller et al. 1995) and
(Koppelman et al. 1995). In this paper, we present such a
system. The overall structure of our approach is
conventional, consisting of a parser, a semantic interpreter,
and a discourse module. The implementation and integration
of these elements is far less conventional. Within each
module, every processing step is assigned a probability value,
and very large numbers of alternative theories are pursued in
parallel. The individual modules are integrated through an
n-best paradigm, in which many theories are passed from one
stage to the next, together with their associated probability
scores. The meaning of a sentence is determined by taking
the highest scoring theory from among the n-best possibilities
produced by the final stage in the model.
Some key advantages to statistical modeling techniques are:
</bodyText>
<listItem confidence="0.887059666666667">
• All knowledge required by the system is acquired
through training examples, thereby eliminating the need
for hand-written rules. In parsing for example, it is
</listItem>
<bodyText confidence="0.95330775">
sufficient to provide the system with examples
specifying the correct parses for a set of training
examples. There is no need to specify an exact set of
rules or a detailed procedure for producing such parses.
</bodyText>
<listItem confidence="0.668247">
• All decisions made by the system are graded, and there
</listItem>
<bodyText confidence="0.985951310344828">
are principled techniques for estimating the gradations.
The system is thus free to pursue unusual theories, while
remaining aware of the fact that they are unlikely. In the
event that a more likely theory exists, then the more
likely theory is selected, but if no more likely
interpretation can be found, the unlikely interpretation is
accepted.
The focus of this work is primarily to extract sufficient
information from each utterance to give an appropriate
response to a user&apos;s request. A variety of problems regarded
as standard in computational linguistics, such as
quantification, reference and the like, are thus ignored.
To evaluate our approach, we trained an experimental system
using data from the Air Travel Information (ATIS) domain
(Bates et al. 1990; Price 1990). The selection of ATIS was
motivated by three concerns. First, a large corpus of ATIS
sentences already exists and is readily available. Second,
ATIS provides an existing evaluation methodology, complete
with independent training and test corpora, and scoring
programs. Finally, evaluating on a common corpus makes it
easy to compare the performance of the system with those
based on different approaches.
We have evaluated our system on the same blind test sets
used in the ARPA evaluations (Pallett et al. 1995), and
present a preliminary result at the conclusion of this paper.
The remainder of the paper is divided into four sections, one
describing the overall structure of our models, and one for
each of the three major components of parsing, semantic
interpretation and discourse.
</bodyText>
<sectionHeader confidence="0.751607" genericHeader="method">
2. Model Structure
</sectionHeader>
<bodyText confidence="0.8942026">
Given a string of input words W and a discourse history H,
the task of a statistical language understanding system is to
search among the many possible discourse-dependent
meanings MD for the most likely meaning Mo:
Mo = arg max P(MD I W, H).
</bodyText>
<page confidence="0.989134">
55
</page>
<bodyText confidence="0.998933666666667">
Directly modeling P(MD I W, H) is difficult because the gap
that the model must span is large. A common approach in
non-statistical natural language systems is to bridge this gap
by introducing intermediate representations such as parse
structure and pre-discourse sentence meaning. Introducing
these intermediate levels into the statistical framework gives:
</bodyText>
<equation confidence="0.632143">
Mo = arg max P(MD IW,H ,Ms,T)P(Ms,T IW,H)
MD Ms,T
</equation>
<bodyText confidence="0.698880333333333">
where T denotes a semantic parse tree, and Ms denotes pre-
discourse sentence meaning. This expression can be
simplified by introducing two independence assumptions:
</bodyText>
<listItem confidence="0.9894388">
1. Neither the parse tree T, nor the pre-discourse meaning
Ms, depends on the discourse history H.
2. The post-discourse meaning MD does not depend on the
words W or the parse structure T, once the pre-discourse
meaning Ms is determined.
</listItem>
<equation confidence="0.891639909090909">
Under these assumptions,
Mo = argmax P(MD I H, Ms) P(Ms,T1W) .
M0 ms,T
Next, the probability P(Ms,T1W) can be rewritten using
Bayes rule as:
5,T) P(W Ms,T)
P(Ms,T1W)=
P(W)
leading to:
Mo = argmax P(MD I H, Ms) P(-M -s&apos;T) P(W 1 Ms,T)
MD Ms,T P(W)
</equation>
<bodyText confidence="0.990638">
Now, since P(W) is constant for any given word string, the
problem of finding meaning MD that maximizes
</bodyText>
<equation confidence="0.957453444444444">
P(Ms,T) P(W I Ms,T)
poiD H,M s)
P(W)
M5 ,T
is equivalent to finding MD that maximizes
P(MD IH,Ms)P(Ms,T)P(WIMs,T).
M5 ,T
Mo = argmax P(MD 1 H, M s) P(Ms,T) P(W 1 Ms,T).
MD Ms,T
</equation>
<bodyText confidence="0.939419333333333">
We now introduce a third independence assumption:
3. The probability of words W does not depend on meaning
Ms, given that parse T is known.
This assumption is justified because the word tags in our
parse representation specify both semantic and syntactic class
information. Under this assumption:
</bodyText>
<equation confidence="0.6038935">
Mo = argmax IP(MD 1 H , Ms) P(Ms,T) P(W IT)
MD Ms,T
</equation>
<bodyText confidence="0.9999498">
Finally, we assume that most of the probability mass for each
discourse-dependent meaning is focused on a single parse
tree and on a single pre-discourse meaning. Under this
(Viterbi) assumption, the summation operator can be
replaced by the maximization operator, yielding:
</bodyText>
<equation confidence="0.949481">
Mo = arg max( max (P( M D I H, Ms) P(Ms,T) P(W IT)))
AID MS &apos;T
</equation>
<bodyText confidence="0.865562666666667">
This expression corresponds to the computation actually
performed by our system which is shown in Figure 1.
Processing proceeds in three stages:
</bodyText>
<listItem confidence="0.838296333333333">
1. Word string W arrives at the parsing model. The full
space of possible parses T is searched for n-best
candidates according to the measure P(T) P(W IT) .
</listItem>
<bodyText confidence="0.779811111111111">
These parses, together with their probability scores, are
passed to the semantic interpretation model.
2. The constrained space of candidate parses T (received
from the parsing model), combined with the full space
of possible pre-discourse meanings Ms, is searched for
n-best candidates according to the measure
P(Ms,T) P(W IT). These pre-discourse meanings,
together with their associated probability scores, are
passed to the discourse model.
</bodyText>
<figure confidence="0.758880625">
Thus, V
MS MD
&gt; Discourse
Model
W T I. Semantic
Parsing Interpretation
Model Model
P(T)P(WIT) P(Als,T)P(WIT) P(MDIMs,H)P(M,T)P(WIT)
</figure>
<figureCaption confidence="0.999116">
Figure 1: Overview of statistical processing.
</figureCaption>
<page confidence="0.971085">
56
</page>
<bodyText confidence="0.9038424">
3. The constrained space of candidate pre-discourse
meanings Ms (received from the semantic interpretation
model), combined with the full space of possible post-
discourse meanings MD, is searched for the single
candidate that maximizes
P( MD I H,M s) P(Ms,T) P(W IT) ,conditioned on the
current history H. The discourse history is then updated
and the post-discourse meaning is returned.
We now proceed to a detailed discussion of each of these
three stages, beginning with parsing.
</bodyText>
<sectionHeader confidence="0.894569" genericHeader="method">
3. Parsing
</sectionHeader>
<bodyText confidence="0.85281">
Our parse representation is essentially syntactic in form,
patterned on a simplified head-centered theory of phrase
structure. In content, however, the parse trees are as much
semantic as syntactic. Specifically, each parse node indicates
both a semantic and a syntactic class (excepting a few types
that serve purely syntactic functions). Figure 2 shows a
sample parse of a typical ATIS sentence. The
semantic,/syntactic character of this representation offers
several advantages:
</bodyText>
<listItem confidence="0.986942666666667">
I. Annotation: Well-founded syntactic principles provide
a framework for designing an organized and consistent
annotation schema.
2. Decoding: Semantic and syntactic constraints are
simultaneously available during the decoding process;
the decoder searches for parses that are both
syntactically and semantically coherent.
3. Semantic Interpretation: Semantic/syntactic parse trees
are immediately useful to the semantic interpretation
</listItem>
<bodyText confidence="0.9718518">
/top°
/wh-question
process: semantic labels identify the basic units of
meaning, while syntactic structures help identify
relationships between those units.
</bodyText>
<subsectionHeader confidence="0.998898">
3.1 Statistical Parsing Model
</subsectionHeader>
<bodyText confidence="0.999873">
The parsing model is a probabilistic recursive transition
network similar to those described in (Miller et al. 1994) and
(Seneff 1992). The probability of a parse tree T given a word
string W is rewritten using Bayes rule as:
</bodyText>
<equation confidence="0.996164">
P(T IW) = P(T) P(W I T)
P(W)
</equation>
<bodyText confidence="0.9895976">
Since P(W) is constant for any given word string, candidate
parses can be ranked by considering only the product P(T)
P(W I 7). The probability P(7) is modeled by state transition
probabilities in the recursive transition network, and P(W I 7)
is modeled by word transition probabilities.
</bodyText>
<listItem confidence="0.997096">
• State transition probabilities have the form
</listItem>
<equation confidence="0.867483">
P(staten I staten_i,statenp) . For example,
</equation>
<bodyText confidence="0.644057">
P(location/pp I arrivaL/vp-head, arrival/vp) is the
probability of a location/pp following an arrivaMvp-
head within an arrivallvp constituent.
</bodyText>
<listItem confidence="0.929327">
• Word transition probabilities have the form
</listItem>
<figure confidence="0.903416090909091">
P(word, I word „...1,tag) . For example,
P(&amp;quot;class&amp;quot; I &amp;quot;first&amp;quot;, class-of-service/npr) is the probability
of the word sequence &amp;quot;first class&amp;quot; given the tag
class-of-service/npr.
Each parse tree T corresponds directly with a path through
the recursive transition network. The probability
P(7) P(W I 7) is simply the product of each transition
flight
/np
flight-constraints
/rel-clause
departure
/vp
flight
/corenp
f&apos;&apos;&apos;&apos;&apos;&apos;&apos;
departure location
&apos;pp/PP
/NO&apos; /NO
location city
/prep /npr
I I
in Atlanta
flight departure departure city
/np-head /comp /vp-head /prep /npr
I I I
flights that leave from Boston
arrival
/vp-head
arrive
time
/wh-head /aux /det
When do the
</figure>
<figureCaption confidence="0.998584">
Figure 2: A sample parse tree.
</figureCaption>
<page confidence="0.990554">
57
</page>
<bodyText confidence="0.995696">
probability along the path corresponding to T.
</bodyText>
<subsectionHeader confidence="0.999541">
3.2 Training the Parsing Model
</subsectionHeader>
<bodyText confidence="0.999512125">
Transition probabilities are estimated directly by observing
occurrence and transition frequencies in a training corpus of
annotated parse trees. These estimates are then smoothed to
overcome sparse data limitations. The semantic/syntactic
parse labels, described above, provide a further advantage in
terms of smoothing: for cases of undertrained probability
estimates, the model backs off to independent syntactic and
semantic probabilities as follows:
</bodyText>
<table confidence="0.8706876">
Ps(semlsynu I semlsynn_i ,seml syn ad=
A(semlsynn I semlsynn_i ,sem/syn up)
x P(semlsynn 1 semlsyn„A , sem/syn
+ A(semlsyn„ 1 seml syn_1 , seml syn up)
x P(sem„ I sem1p)P(synu1 syn„.4,syn„p)
</table>
<bodyText confidence="0.999742">
where A. is estimated as in (Placeway et al. 1993). Backing
off to independent semantic and syntactic probabilities
potentially provides more precise estimates than the usual
strategy of backing off directly form bigram to unigram
models.
</bodyText>
<subsectionHeader confidence="0.999755">
3.3 Searching the Parsing Model
</subsectionHeader>
<bodyText confidence="0.9999625">
In order to explore the space of possible parses efficiently,
the parsing model is searched using a decoder based on an
adaptation of the Earley parsing algorithm (Earley 1970).
This adaptation, related to that of (Stolcke 1995), involves
reformulating the Earley algorithm to work with probabilistic
recursive transition networks rather than with deterministic
production rules. For details of the decoder, see (Miller
1996).
</bodyText>
<sectionHeader confidence="0.974477" genericHeader="method">
4. Semantic Interpretation
</sectionHeader>
<bodyText confidence="0.99988275">
Both pre-discourse and post-discourse meanings in our
current system are represented using a simple frame
representation. Figure 3 shows a sample semantic frame
corresponding to the parse in Figure 2.
</bodyText>
<figure confidence="0.9114415">
Air-Transportation
Show: (Arrival-Time)
Origin: (City &amp;quot;Boston&amp;quot;)
Destination: (City &amp;quot;Atlanta&amp;quot;)
</figure>
<figureCaption confidence="0.999671">
Figure 3: A sample semantic frame.
</figureCaption>
<bodyText confidence="0.996343777777778">
Recall that the semantic interpreter is required to compute
P(M s ,T) P(W IT) . The conditional word probability
P(W IT) has already been computed during the parsing
phase and need not be recomputed. The current problem,
then, is to compute the prior probability of meaning Ms and
parse T occurring together. Our strategy is to embed the
instructions for constructing Ms directly into parse T ,
resulting in an augmented tree structure. For example, the
instructions needed to create the frame shown in Figure 3 are:
</bodyText>
<listItem confidence="0.9993105">
1. Create an Air-Transportation frame.
2. Fill the Show slot with Arrival-Time.
3. Fill the Origin slot with (City &amp;quot;Boston&amp;quot;)
4. Fill the Destination slot with (City &amp;quot;Atlanta&amp;quot;)
</listItem>
<bodyText confidence="0.8655965">
These instructions are attached to the parse tree at the points
indicated by the circled numbers (see
</bodyText>
<figureCaption confidence="0.5755555">
Figure 2). The probability P(M s,T) is then simply the
prior probability of producing the augmented tree structure.
</figureCaption>
<subsectionHeader confidence="0.937908">
4.1 Statistical Interpretation Model
</subsectionHeader>
<bodyText confidence="0.99930975">
Meanings Ms are decomposed into two parts: the frame type
FT, and the slot fillers S. The frame type is always attached
to the topmost node in the augmented parse tree, while the
slot filling instructions are attached to nodes lower down in
the tree. Except for the topmost node, all parse nodes are
required to have some slot filling operation. For nodes that
do not directly trigger any slot fill operation, the special
operation null is attached. The probability P(Ms, 7) is then:
</bodyText>
<equation confidence="0.81358">
P(M s ,T) = P(FT,S,T)= P(F7) P(T I FT) P(S I FT ,T) .
</equation>
<bodyText confidence="0.9999746">
Obviously, the prior probabilities P(FT) can be obtained
directly from the training data. To compute P(T I FT), each
of the state transitions from the previous parsing model are
simply rescored conditioned on the frame type. The new
state transition probabilities are:
</bodyText>
<equation confidence="0.50822">
P(stateu I staten_i, stateup , FT) .
</equation>
<bodyText confidence="0.999856555555556">
To compute P(S I FT, 7) , we make the independence
assumption that slot filling operations depend only on the
frame type, the slot operations already performed, and on the
local parse structure around the operation. This local
neighborhood consists of the parse node itself, its two left
siblings, its two right siblings, and its four immediate
ancestors. Further, the syntactic and semantic components of
these nodes are considered independently. Under these
assumptions, the probability of a slot fill operation is:
</bodyText>
<equation confidence="0.899336">
P(slotu I FT ,Su_i,setnu_2,...,sem,...,semu+2,
5Ynn-2 ,•• • 9S)nn ,•••,5Ynn+2,
semupi,.. • ,semus SY nUp 1 , • • • SrtUp 4 )
</equation>
<bodyText confidence="0.996901">
and the probability P(S I FT, 7) is simply the product of all
such slot fill operations in the augmented tree.
</bodyText>
<subsectionHeader confidence="0.9743075">
4.2 Training the Semantic Interpretation
Model
</subsectionHeader>
<bodyText confidence="0.9999496">
Transition probabilities are estimated from a training corpus
of augmented trees. Unlike probabilities in the parsing
model, there obviously is not sufficient training data to
estimate slot fill probabilities directly. Instead, these
probabilities are estimated by statistical decision trees similar
</bodyText>
<page confidence="0.995972">
58
</page>
<bodyText confidence="0.999963133333333">
to those used in the Spatter parser (Magerman 1995). Unlike
more common decision tree classifiers, which simply classify
sets of conditions, statistical decision trees give a probability
distribution over all possible outcomes. Statistical decision
trees are constructed in a two phase process. In the first
phase, a decision tree is constructed in the standard fashion
using entropy reduction to guide the construction process.
This phase is the same as for classifier models, and the
distributions at the leaves are often extremely sharp,
sometimes consisting of one outcome with probability 1, and
all others with probability 0. In the second phase, these
distributions are smoothed by mixing together distributions
of various nodes in the decision tree. As in (Magerman
1995), mixture weights are determined by deleted
interpolation on a separate block of training data.
</bodyText>
<subsectionHeader confidence="0.998128">
4.3 Searching the Semantic Interpretation
Model
</subsectionHeader>
<bodyText confidence="0.999965833333334">
Searching the interpretation model proceeds in two phases.
In the first phase, every parse T received from the parsing
model is rescored for every possible frame type, computing
P(T I F7) (our current model includes only a half dozen
different types, so this computation is tractable). Each of
these theories is combined with the corresponding prior
probability P(FT) yielding P(FT) P(T I PT). The n-best of
these theories are then passed to the second phase of the
interpretation process. This phase searches the space of slot
filling operations using a simple beam search procedure. For
each combination of FT and T, the beam search procedure
considers all possible combinations of fill operations, while
pruning partial theories that fall beneath the threshold
imposed by the beam limit. The surviving theories are then
combined with the conditional word probabilities P(W I 7),
computed during the parsing model. The final result of these
steps is the n-best set of candidate pre-discourse meanings,
scored according to the measure P(Ms,T) P(W IT) .
</bodyText>
<sectionHeader confidence="0.986711" genericHeader="method">
5. Discourse Processing
</sectionHeader>
<bodyText confidence="0.999936">
The discourse module computes the most probable post-
discourse meaning of an utterance from its pre-discourse
meaning and the discourse history, according to the measure:
</bodyText>
<equation confidence="0.871581">
P(MD I H,Ms) P(Ms,T) P(W IT) .
</equation>
<bodyText confidence="0.997277142857143">
Because pronouns can usually be ignored in the ATIS
domain, our work does not treat the problem of pronominal
reference. Our probability model is instead shaped by the
key discourse problem of the ATIS domain, which is the
inheritance of constraints from context. This inheritance
phenomenon, similar in spirit to one-anaphora, is illustrated
by the following dialog::
</bodyText>
<footnote confidence="0.846246">
USER1: I want to fly from Boston to Denver.
SYSTEM1: &lt;displays Boston to Denver flights&gt;
USER2: Which flights are available on Tuesday?
</footnote>
<bodyText confidence="0.945757235294117">
SYSTEM2: &lt;displays Boston to Denver flights for
Tuesday&gt;
In USER2, it is obvious from context that the user is asking
about flights whose ORIGIN is BOSTON and whose
DESTINATION is DENVER, and not all flights between any
two cities. Constraints are not always inherited, however.
For example, in the following continuation of this dialogue:
USER3: Show me return flights from Denver to Boston,
it is intuitively much less likely that the user means the &amp;quot;on
Tuesday&amp;quot; constraint to continue to apply.
The discourse history H simply consists of the list of all post-
discourse frame representations for all previous utterances in
the current session with the system. These frames are the
source of candidate constraints to be inherited. For most
utterances, we make the simplifying assumption that we need
only look at the last (i.e. most recent) frame in this list, which
we call M.
</bodyText>
<subsectionHeader confidence="0.989354">
5.1 Statistical Discourse Model
</subsectionHeader>
<bodyText confidence="0.999892333333333">
The statistical discourse model maps a 23 element input
vector X onto a 23 element output vector Y. These vectors
have the following interpretations:
</bodyText>
<listItem confidence="0.999559333333333">
• X represents the combination of previous meaning Mp
and the pre-discourse meaning Ms.
• Y represents the post-discourse meaning MD.
</listItem>
<equation confidence="0.632599">
Thus,
P(M D I H , Ms) = P(Y 1 X) .
</equation>
<bodyText confidence="0.973333454545455">
The 23 elements in vectors X and Y correspond to the 23
possible slots in the frame schema. Each element in X can
have one of five values, specifying the relationship between
the filler of the corresponding slot in Mp and Ms:
INITIAL - slot filled in Ms but not in Mp
TACIT - slot filled in Mp but not in Ms
REITERATE - slot filled in both MA and Ms; value the
same
CHANGE - slot filled in both Mp and Ms; value
different
IRRELEVANT - slot not filled in either Mp or Ms
Output vector Y is constructed by directly copying all fields
from input vector X except those labeled TACIT. These
direct copying operations are assigned probability 1. For
fields labeled TACIT, the corresponding field in Y is filled
with either INHERITED or NOT-INHERITED. The
probability of each of these operations is determined by a
statistical decision tree model. The discourse model contains
23 such statistical decision trees, one for each slot position.
An ordering is imposed on the set of frame slots, such that
inheritance decisions for slots higher in the order are
conditioned on the decisions for slots lower in the order.
</bodyText>
<page confidence="0.995257">
59
</page>
<bodyText confidence="0.634181">
The probability P(Y I X) is then the product of all 23 statistical models to additional linguistic phenomena such as
quantification and anaphora resolution.
decision probabilities:
</bodyText>
<equation confidence="0.619395">
P(Y I X) = P(yilX) P(y2 IX ,Y1) ••• P(Y23 IX ,Y1,Y2,•••,y22) •
</equation>
<subsectionHeader confidence="0.997321">
5.2 Training the Discourse Model
</subsectionHeader>
<bodyText confidence="0.999982142857143">
The discourse model is trained from a corpus annotated with
both pre-discourse and post-discourse semantic frames.
Corresponding pairs of input and output (X, Y) vectors are
computed from these annotations, which are then used to
train the 23 statistical decision trees. The training procedure
for estimating these decision tree models is similar to that
used for training the semantic interpretation model.
</bodyText>
<subsectionHeader confidence="0.997398">
5.3 Searching The Discourse Model
</subsectionHeader>
<bodyText confidence="0.952794363636364">
Searching the discourse model begins by selecting a meaning
frame Mp from the history stack H, and combining it with
each pre-discourse meaning Ms received from the semantic
interpretation model. This process yields a set of candidate
input vectors X. Then, for each vector X, a search process
exhaustively constructs and scores all possible output vectors
Y according to the measure P(Y I X) (this computation is
feasible because the number of TACIT fields is normally
small). These scores are combined with the pre-discourse
scores P(Ms,T) P(W IT) , already computed by the
semantic interpretation process. This computation yields:
</bodyText>
<equation confidence="0.809339333333333">
P(Y I X) P(M s,T) P(W IT),
which is equivalent to:
P( MD I H,M s) P(Ms,T) P(W IT) .
</equation>
<bodyText confidence="0.999738">
The highest scoring theory is then selected, and a
straightforward computation derives the final meaning frame
MD from output vector Y.
</bodyText>
<sectionHeader confidence="0.997124" genericHeader="evaluation">
6. Experimental Results
</sectionHeader>
<bodyText confidence="0.999978111111111">
We have trained and evaluated the system on a common
corpus of utterances collected from naive users in the ATIS
domain. In this test, the system was trained on approximately
4000 ATIS 2 and ATIS 3 sentences, and then evaluated on
the December 1994 test material (which was held aside as a
blind test set). The combined system produced an error rate
of 21.6%. Work on the system is ongoing, however, and
interested parties are encouraged to contact the authors for
more recent results.
</bodyText>
<sectionHeader confidence="0.993108" genericHeader="conclusions">
7. Conclusion
</sectionHeader>
<bodyText confidence="0.9999856">
We have presented a fully trained statistical natural language
interface system, with separate models corresponding to the
classical processing steps of parsing, semantic interpretation
and discourse. Much work remains to be done in order to
refine the statistical modeling techniques, and to extend the
</bodyText>
<sectionHeader confidence="0.982193" genericHeader="acknowledgments">
8. Acknowledgments
</sectionHeader>
<bodyText confidence="0.9996165">
We wish to thank Robert Ingria for his effort in supervising
the annotation of the training corpus, and for his helpful
technical suggestions.
This work was supported by the Advanced Research Projects
Agency and monitored by the Office of Naval Research
under Contract No. NO0014-91-C-0115, and by Ft. Huachuca
under Contract Nos. DABT63-94-C-0061 and DABT63-94-
C-0063 . The content of the information does not necessarily
reflect the position or the policy of the Government and no
official endorsement should be inferred.
</bodyText>
<sectionHeader confidence="0.996722" genericHeader="references">
9. References
</sectionHeader>
<reference confidence="0.99889653125">
Bates, M., Boisen, S., and Makhoul, J. &amp;quot;Developing an
Evaluation Methodology for Spoken Language Systems.&amp;quot;
Speech and Natural Language Workshop, Hidden Valley,
Pennsylvania, 102-108.
Church, K. &amp;quot;A Stochastic Parts Program and Noun Phrase
Parser for Unrestricted Text.&amp;quot; Second Conference on Applied
Natural Language Processing, Austin, Texas.
Earley, J. (1970). &amp;quot;An Efficient Context-Free Parsing
Algorithm.&amp;quot; Communications of the ACM, 6,451-455.
Grishman, R., and Sterling, J. &amp;quot;Description of the Proteus
System as Used for MUC-5.&amp;quot; Fifth Message Understanding
Conference, Baltimore, Maryland, 181-194.
Koppelman, J., Pietra, S. D., Epstein, M., Roukos, S., and
Ward, T. &amp;quot;A statistical approach to language modeling for the
ATIS task.&amp;quot; Eurospeech 1995, Madrid.
Levin, E., and Pieraccini, R. &amp;quot;CHRONUS: The Next
Generation.&amp;quot; Spoken Language Systems Technology
Workshop, Austin, Texas, 269-271.
Magerman, D. &amp;quot;Statistical Decision Tree Models for
Parsing.&amp;quot; 33rd Annual Meeting of the Association for
Computational Linguistics, Cambridge, Massachusetts, 276-
283.
Miller, S. (1996). &amp;quot;Hidden Understanding Models,&amp;quot;
Northeastern University, Boston, MA.
Miller, S., Bates, M., Bobrow, R., Ingria, R., Maldioul, J.,
and Schwartz, R. &amp;quot;Recent Progress in Hidden Understanding
Models.&amp;quot; Spoken Language Systems Technology Workshop,
Austin, Texas, 276-280.
Miller, S., Bobrow, R., Ingria, R., and Schwartz, R. &amp;quot;Hidden
Understanding Models of Natural Language.&amp;quot; 32nd Annual
Meeting of the Association &apos;for Computational Linguistics,
Las Cruces, New Mexico, 25-32.
</reference>
<page confidence="0.966976">
60
</page>
<reference confidence="0.999945363636364">
Pallett, D., Fiscus, J., Fisher, W., Garofolo, J., Lund, B.,
Martin, A., and Przybocki, M. &amp;quot;1994 Benchmark Tests for
the ARPA Spoken Language Program.&amp;quot; Spoken Language
Systems Technology Workshop, Austin, Texas.
Placeway, P., Schwartz, R., Fung, P., and Nguyen, L. &amp;quot;The
Estimation of Powerful Language Models from Small and
Large Corpora.&amp;quot; IEEE ICASSP, 33-36.
Price, P. &amp;quot;Evaluation of Spoken Language Systems: the ATIS
Domain.&amp;quot; Speech and Natural Language Workshop, Hidden
Valley, Pennsylvania, 91-95.
Seneff, S. (1992). &amp;quot;TINA: A Natural Language System for
Spoken Language Applications.&amp;quot; Computational Linguistics,
18,1, 61-86.
Stolcke, A. (1995). &amp;quot;An Efficient Probabilistic Context-Free
Parsing Algorithm that Computes Prefix Probabilites.&amp;quot;
Computational Linguistics, 21(2), 165-201.
Weischedel, R., Ayuso, D., Boisen, S., Fox, H., Ingria, R.,
Matsukawa, T., Papageorgiou, C., MacLaughlin, D.,
Kitagawa, M., Sakai, T., Abe, J., Hosihi, H., Miyamoto, Y.,
and Miller, S. &amp;quot;Description of the PLUM System as Used for
MUC-5.&amp;quot; Fifth Message Understanding Conference,
Baltimore, Maryland, 93-107.
</reference>
<page confidence="0.999274">
61
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.973724">
<title confidence="0.993427">A FULLY STATISTICAL APPROACH TO NATURAL LANGUAGE INTERFACES</title>
<author confidence="0.999977">Scott Miller</author>
<author confidence="0.999977">David Stallard</author>
<author confidence="0.999977">Robert Bobrow</author>
<author confidence="0.999977">Richard Schwartz</author>
<affiliation confidence="0.992974">BBN Systems and Technologies</affiliation>
<address confidence="0.9997975">70 Fawcett Street Cambridge, MA 02138</address>
<email confidence="0.998914">szmiller@bbn.com,stallard@bbn.com,rusty@bbn.com,schwartz@bbn.com</email>
<abstract confidence="0.99934675">We present a natural language interface system which is based entirely on trained statistical models. The system consists of three stages of processing: parsing, semantic interpretation, and discourse. Each of these stages is modeled as a statistical process. The models are fully integrated, resulting in an end-to-end system that maps input utterances into meaning representation frames.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>M Bates</author>
<author>S Boisen</author>
<author>J Makhoul</author>
</authors>
<title>Developing an Evaluation Methodology for Spoken Language Systems.&amp;quot; Speech and Natural Language Workshop,</title>
<pages>102--108</pages>
<location>Hidden Valley, Pennsylvania,</location>
<marker>Bates, Boisen, Makhoul, </marker>
<rawString>Bates, M., Boisen, S., and Makhoul, J. &amp;quot;Developing an Evaluation Methodology for Spoken Language Systems.&amp;quot; Speech and Natural Language Workshop, Hidden Valley, Pennsylvania, 102-108.</rawString>
</citation>
<citation valid="false">
<authors>
<author>K Church</author>
</authors>
<title>A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text.&amp;quot;</title>
<booktitle>Second Conference on Applied Natural Language Processing,</booktitle>
<location>Austin, Texas.</location>
<marker>Church, </marker>
<rawString>Church, K. &amp;quot;A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text.&amp;quot; Second Conference on Applied Natural Language Processing, Austin, Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An Efficient Context-Free Parsing Algorithm.&amp;quot;</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<pages>6--451</pages>
<contexts>
<context position="11939" citStr="Earley 1970" startWordPosition="1840" endWordPosition="1841">seml syn ad= A(semlsynn I semlsynn_i ,sem/syn up) x P(semlsynn 1 semlsyn„A , sem/syn + A(semlsyn„ 1 seml syn_1 , seml syn up) x P(sem„ I sem1p)P(synu1 syn„.4,syn„p) where A. is estimated as in (Placeway et al. 1993). Backing off to independent semantic and syntactic probabilities potentially provides more precise estimates than the usual strategy of backing off directly form bigram to unigram models. 3.3 Searching the Parsing Model In order to explore the space of possible parses efficiently, the parsing model is searched using a decoder based on an adaptation of the Earley parsing algorithm (Earley 1970). This adaptation, related to that of (Stolcke 1995), involves reformulating the Earley algorithm to work with probabilistic recursive transition networks rather than with deterministic production rules. For details of the decoder, see (Miller 1996). 4. Semantic Interpretation Both pre-discourse and post-discourse meanings in our current system are represented using a simple frame representation. Figure 3 shows a sample semantic frame corresponding to the parse in Figure 2. Air-Transportation Show: (Arrival-Time) Origin: (City &amp;quot;Boston&amp;quot;) Destination: (City &amp;quot;Atlanta&amp;quot;) Figure 3: A sample semantic</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Earley, J. (1970). &amp;quot;An Efficient Context-Free Parsing Algorithm.&amp;quot; Communications of the ACM, 6,451-455.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Grishman</author>
<author>J Sterling</author>
</authors>
<title>Description of the Proteus System as Used for MUC-5.&amp;quot; Fifth Message Understanding Conference,</title>
<pages>181--194</pages>
<location>Baltimore, Maryland,</location>
<marker>Grishman, Sterling, </marker>
<rawString>Grishman, R., and Sterling, J. &amp;quot;Description of the Proteus System as Used for MUC-5.&amp;quot; Fifth Message Understanding Conference, Baltimore, Maryland, 181-194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Koppelman</author>
<author>S D Pietra</author>
<author>M Epstein</author>
<author>S Roukos</author>
<author>T Ward</author>
</authors>
<title>A statistical approach to language modeling for the ATIS task.&amp;quot; Eurospeech</title>
<date>1995</date>
<location>Madrid.</location>
<contexts>
<context position="1653" citStr="Koppelman et al. 1995" startWordPosition="225" endWordPosition="228">g 1993). More recently, statistical methods have been applied to domain-specific semantic parsing (Miller et al. 1994), and to the more difficult problem of wide-coverage syntactic parsing (Magerman 1995). Nevertheless, most natural language systems remain primarily rule based, and even systems that do use statistical techniques, such as AT&amp;T Chronus (Levin and Pieraccini 1995), continue to require a significant rule based component. Development of a complete end-to-end statistical understanding system has been the focus of several ongoing research efforts, including (Miller et al. 1995) and (Koppelman et al. 1995). In this paper, we present such a system. The overall structure of our approach is conventional, consisting of a parser, a semantic interpreter, and a discourse module. The implementation and integration of these elements is far less conventional. Within each module, every processing step is assigned a probability value, and very large numbers of alternative theories are pursued in parallel. The individual modules are integrated through an n-best paradigm, in which many theories are passed from one stage to the next, together with their associated probability scores. The meaning of a sentence</context>
</contexts>
<marker>Koppelman, Pietra, Epstein, Roukos, Ward, 1995</marker>
<rawString>Koppelman, J., Pietra, S. D., Epstein, M., Roukos, S., and Ward, T. &amp;quot;A statistical approach to language modeling for the ATIS task.&amp;quot; Eurospeech 1995, Madrid.</rawString>
</citation>
<citation valid="false">
<authors>
<author>E Levin</author>
<author>R Pieraccini</author>
</authors>
<title>CHRONUS: The Next Generation.&amp;quot;</title>
<booktitle>Spoken Language Systems Technology Workshop,</booktitle>
<pages>269--271</pages>
<location>Austin, Texas,</location>
<marker>Levin, Pieraccini, </marker>
<rawString>Levin, E., and Pieraccini, R. &amp;quot;CHRONUS: The Next Generation.&amp;quot; Spoken Language Systems Technology Workshop, Austin, Texas, 269-271.</rawString>
</citation>
<citation valid="false">
<authors>
<author>D Magerman</author>
</authors>
<title>Statistical Decision Tree Models for Parsing.&amp;quot; 33rd Annual Meeting of the Association for Computational Linguistics,</title>
<pages>276--283</pages>
<location>Cambridge, Massachusetts,</location>
<marker>Magerman, </marker>
<rawString>Magerman, D. &amp;quot;Statistical Decision Tree Models for Parsing.&amp;quot; 33rd Annual Meeting of the Association for Computational Linguistics, Cambridge, Massachusetts, 276-283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
</authors>
<title>Hidden Understanding Models,&amp;quot;</title>
<date>1996</date>
<institution>Northeastern University,</institution>
<location>Boston, MA.</location>
<contexts>
<context position="12188" citStr="Miller 1996" startWordPosition="1874" endWordPosition="1875">tic and syntactic probabilities potentially provides more precise estimates than the usual strategy of backing off directly form bigram to unigram models. 3.3 Searching the Parsing Model In order to explore the space of possible parses efficiently, the parsing model is searched using a decoder based on an adaptation of the Earley parsing algorithm (Earley 1970). This adaptation, related to that of (Stolcke 1995), involves reformulating the Earley algorithm to work with probabilistic recursive transition networks rather than with deterministic production rules. For details of the decoder, see (Miller 1996). 4. Semantic Interpretation Both pre-discourse and post-discourse meanings in our current system are represented using a simple frame representation. Figure 3 shows a sample semantic frame corresponding to the parse in Figure 2. Air-Transportation Show: (Arrival-Time) Origin: (City &amp;quot;Boston&amp;quot;) Destination: (City &amp;quot;Atlanta&amp;quot;) Figure 3: A sample semantic frame. Recall that the semantic interpreter is required to compute P(M s ,T) P(W IT) . The conditional word probability P(W IT) has already been computed during the parsing phase and need not be recomputed. The current problem, then, is to compute </context>
</contexts>
<marker>Miller, 1996</marker>
<rawString>Miller, S. (1996). &amp;quot;Hidden Understanding Models,&amp;quot; Northeastern University, Boston, MA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Miller</author>
<author>M Bates</author>
<author>R Bobrow</author>
<author>R Ingria</author>
<author>J Maldioul</author>
<author>R Schwartz</author>
</authors>
<title>Recent Progress in Hidden Understanding Models.&amp;quot; Spoken Language Systems Technology Workshop,</title>
<pages>276--280</pages>
<location>Austin, Texas,</location>
<marker>Miller, Bates, Bobrow, Ingria, Maldioul, Schwartz, </marker>
<rawString>Miller, S., Bates, M., Bobrow, R., Ingria, R., Maldioul, J., and Schwartz, R. &amp;quot;Recent Progress in Hidden Understanding Models.&amp;quot; Spoken Language Systems Technology Workshop, Austin, Texas, 276-280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
<author>R Bobrow</author>
<author>R Ingria</author>
<author>R Schwartz</author>
</authors>
<title>Hidden Understanding Models of Natural Language.&amp;quot;</title>
<date></date>
<booktitle>32nd Annual Meeting of the Association &apos;for Computational Linguistics,</booktitle>
<pages>25--32</pages>
<location>Las Cruces, New</location>
<marker>Miller, Bobrow, Ingria, Schwartz, </marker>
<rawString>Miller, S., Bobrow, R., Ingria, R., and Schwartz, R. &amp;quot;Hidden Understanding Models of Natural Language.&amp;quot; 32nd Annual Meeting of the Association &apos;for Computational Linguistics, Las Cruces, New Mexico, 25-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Pallett</author>
<author>J Fiscus</author>
<author>W Fisher</author>
<author>J Garofolo</author>
<author>B Lund</author>
<author>A Martin</author>
<author>M Przybocki</author>
</authors>
<title>Benchmark Tests for the ARPA Spoken Language Program.&amp;quot; Spoken Language Systems Technology Workshop,</title>
<date>1994</date>
<location>Austin, Texas.</location>
<marker>Pallett, Fiscus, Fisher, Garofolo, Lund, Martin, Przybocki, 1994</marker>
<rawString>Pallett, D., Fiscus, J., Fisher, W., Garofolo, J., Lund, B., Martin, A., and Przybocki, M. &amp;quot;1994 Benchmark Tests for the ARPA Spoken Language Program.&amp;quot; Spoken Language Systems Technology Workshop, Austin, Texas.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Placeway</author>
<author>R Schwartz</author>
<author>P Fung</author>
<author>L Nguyen</author>
</authors>
<title>The Estimation of Powerful Language Models from Small and Large Corpora.&amp;quot;</title>
<journal>IEEE ICASSP,</journal>
<pages>33--36</pages>
<marker>Placeway, Schwartz, Fung, Nguyen, </marker>
<rawString>Placeway, P., Schwartz, R., Fung, P., and Nguyen, L. &amp;quot;The Estimation of Powerful Language Models from Small and Large Corpora.&amp;quot; IEEE ICASSP, 33-36.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Price</author>
</authors>
<title>Evaluation of Spoken Language Systems: the ATIS Domain.&amp;quot; Speech and Natural Language Workshop,</title>
<pages>91--95</pages>
<location>Hidden Valley, Pennsylvania,</location>
<marker>Price, </marker>
<rawString>Price, P. &amp;quot;Evaluation of Spoken Language Systems: the ATIS Domain.&amp;quot; Speech and Natural Language Workshop, Hidden Valley, Pennsylvania, 91-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Seneff</author>
</authors>
<title>TINA: A Natural Language System for Spoken Language Applications.&amp;quot;</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<pages>61--86</pages>
<contexts>
<context position="9388" citStr="Seneff 1992" startWordPosition="1448" endWordPosition="1449">mantic and syntactic constraints are simultaneously available during the decoding process; the decoder searches for parses that are both syntactically and semantically coherent. 3. Semantic Interpretation: Semantic/syntactic parse trees are immediately useful to the semantic interpretation /top° /wh-question process: semantic labels identify the basic units of meaning, while syntactic structures help identify relationships between those units. 3.1 Statistical Parsing Model The parsing model is a probabilistic recursive transition network similar to those described in (Miller et al. 1994) and (Seneff 1992). The probability of a parse tree T given a word string W is rewritten using Bayes rule as: P(T IW) = P(T) P(W I T) P(W) Since P(W) is constant for any given word string, candidate parses can be ranked by considering only the product P(T) P(W I 7). The probability P(7) is modeled by state transition probabilities in the recursive transition network, and P(W I 7) is modeled by word transition probabilities. • State transition probabilities have the form P(staten I staten_i,statenp) . For example, P(location/pp I arrivaL/vp-head, arrival/vp) is the probability of a location/pp following an arriv</context>
</contexts>
<marker>Seneff, 1992</marker>
<rawString>Seneff, S. (1992). &amp;quot;TINA: A Natural Language System for Spoken Language Applications.&amp;quot; Computational Linguistics, 18,1, 61-86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>An Efficient Probabilistic Context-Free Parsing Algorithm that Computes Prefix Probabilites.&amp;quot;</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<pages>165--201</pages>
<contexts>
<context position="11991" citStr="Stolcke 1995" startWordPosition="1848" endWordPosition="1849"> P(semlsynn 1 semlsyn„A , sem/syn + A(semlsyn„ 1 seml syn_1 , seml syn up) x P(sem„ I sem1p)P(synu1 syn„.4,syn„p) where A. is estimated as in (Placeway et al. 1993). Backing off to independent semantic and syntactic probabilities potentially provides more precise estimates than the usual strategy of backing off directly form bigram to unigram models. 3.3 Searching the Parsing Model In order to explore the space of possible parses efficiently, the parsing model is searched using a decoder based on an adaptation of the Earley parsing algorithm (Earley 1970). This adaptation, related to that of (Stolcke 1995), involves reformulating the Earley algorithm to work with probabilistic recursive transition networks rather than with deterministic production rules. For details of the decoder, see (Miller 1996). 4. Semantic Interpretation Both pre-discourse and post-discourse meanings in our current system are represented using a simple frame representation. Figure 3 shows a sample semantic frame corresponding to the parse in Figure 2. Air-Transportation Show: (Arrival-Time) Origin: (City &amp;quot;Boston&amp;quot;) Destination: (City &amp;quot;Atlanta&amp;quot;) Figure 3: A sample semantic frame. Recall that the semantic interpreter is requ</context>
</contexts>
<marker>Stolcke, 1995</marker>
<rawString>Stolcke, A. (1995). &amp;quot;An Efficient Probabilistic Context-Free Parsing Algorithm that Computes Prefix Probabilites.&amp;quot; Computational Linguistics, 21(2), 165-201.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Weischedel</author>
<author>D Ayuso</author>
<author>S Boisen</author>
<author>H Fox</author>
<author>R Ingria</author>
<author>T Matsukawa</author>
<author>C Papageorgiou</author>
<author>D MacLaughlin</author>
<author>M Kitagawa</author>
<author>T Sakai</author>
<author>J Abe</author>
<author>H Hosihi</author>
<author>Y Miyamoto</author>
<author>S Miller</author>
</authors>
<title>Description of the PLUM System as Used for MUC-5.&amp;quot; Fifth Message Understanding Conference,</title>
<pages>93--107</pages>
<location>Baltimore, Maryland,</location>
<marker>Weischedel, Ayuso, Boisen, Fox, Ingria, Matsukawa, Papageorgiou, MacLaughlin, Kitagawa, Sakai, Abe, Hosihi, Miyamoto, Miller, </marker>
<rawString>Weischedel, R., Ayuso, D., Boisen, S., Fox, H., Ingria, R., Matsukawa, T., Papageorgiou, C., MacLaughlin, D., Kitagawa, M., Sakai, T., Abe, J., Hosihi, H., Miyamoto, Y., and Miller, S. &amp;quot;Description of the PLUM System as Used for MUC-5.&amp;quot; Fifth Message Understanding Conference, Baltimore, Maryland, 93-107.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>