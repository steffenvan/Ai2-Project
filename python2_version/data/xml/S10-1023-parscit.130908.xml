<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000050">
<title confidence="0.9867145">
FCC: Modeling Probabilities with GIZA++ for Task #2 and #3 of
SemEval-2
</title>
<author confidence="0.998003">
Darnes Vilari˜no, Carlos Balderas, David Pinto, Miguel Rodriguez, Saul Le´on
</author>
<affiliation confidence="0.999685">
Faculty of Computer Science, BUAP
</affiliation>
<address confidence="0.717423">
Puebla, Mexico
</address>
<email confidence="0.999177">
{darnes,mrodriguez,dpinto}@cs.buap.mx
</email>
<sectionHeader confidence="0.997416" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9992588125">
In this paper we present a naive approach
to tackle the problem of cross-lingual
WSD and cross-lingual lexical substitu-
tion which correspond to the Task #2 and
#3 of the SemEval-2 competition. We used
a bilingual statistical dictionary, which is
calculated with Giza++ by using the EU-
ROPARL parallel corpus, in order to cal-
culate the probability of a source word to
be translated to a target word (which is as-
sumed to be the correct sense of the source
word but in a different language). Two ver-
sions of the probabilistic model are tested:
unweighted and weighted. The obtained
values show that the unweighted version
performs better thant the weighted one.
</bodyText>
<sectionHeader confidence="0.999482" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998653527777778">
Word Sense Disambiguation (WSD) is con-
sidered one of the most important prob-
lems in Natural Language Processing
(Agirre and Edmonds, 2006). It is claimed
that WSD is essential for those applications that
require of language comprehension modules
such as search engines, machine translation
systems, automatic answer machines, second life
agents, etc. Moreover, with the huge amounts
of information in Internet and the fact that this
information is continuosly growing in different
languages, we are encourage to deal with cross-
lingual scenarios where WSD systems are also
needed. Despite the WSD task has been studied
for a long time, the expected feeling is that WSD
should be integrated into real applications such as
mono and multi-lingual search engines, machine
translation systems, automatic answer machines,
etc (Agirre and Edmonds, 2006). Different stud-
ies on this issue have demonstrated that those
applications benefit from WSD, such as in the
case of machine translation (Chan et al., 2007;
Carpuat and Wu., 2007). On the other hand,
Lexical Substitution (LS) refers to the process
of finding a substitute word for a source word
in a given sentence. The LS task needs to be
approached by firstly disambiguating the source
word, therefore, these two tasks (WSD and LS)
are somehow related.
Since we are describing the modules of our
system, we did not provide information of the
datasets used. For details about the corpora,
see the task description paper for both tasks (#2
and #3) in this volume (Mihalcea et al., 2010;
Lefever and Hoste, 2010). Description about the
other teams are also described in the same papers.
</bodyText>
<sectionHeader confidence="0.937976" genericHeader="method">
2 A Naive Approach to WSD and LS
</sectionHeader>
<bodyText confidence="0.999732428571429">
In this section it is presented an overview of the
presented system, but also we further discuss the
particularities of the general approach for each
task evaluated. We will start this section by
explaining the manner we deal with the Cross-
Lingual Word Sense Disambiguation (C-WSD)
problem.
</bodyText>
<subsectionHeader confidence="0.9293805">
2.1 Cross-Lingual Word Sense
Disambiguation
</subsectionHeader>
<bodyText confidence="0.999673214285714">
We have approached the cross-lingual word sense
disambiguation task by means of a probabilistic
system which considers the probability of a word
sense (in a target language), given a sentence (in a
source language) containing the ambiguous word.
In particular, we used the Naive Bayes classifier
in two different ways. First, we calculated the
probability of each word in the source language
of being associated/translated to the corresponding
word (in the target language). The probabilities
were estimated by means of a bilingual statistical
dictionary which is calculated using the Giza++
system over the EUROPARL parallel corpus. We
filtered this corpus by selecting only those sen-
</bodyText>
<page confidence="0.97747">
112
</page>
<bodyText confidence="0.961974090909091">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 112–116,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
tences which included some senses of the ambigu-
ous word which were obtained by translating this
ambiguous word on the Google search engine.
In Figure 1 we may see the complete process for
approaching the problem of cross-lingual WSD.
The second approach considered a weighted
probability for each word in the source sentence.
The closer a word of the sentence to the ambigu-
ous word, the higher the weight given to it.
In other words, given an English sentence S =
{w1, w2, ··· ,wk, ··· , wk+1, · · · } with the am-
biguous word wk in position k. Let us consider
N candidate translations of wk, {tk1, tk2, · · · , tkN}
obtained somehow (we will further discuss about
this issue in this section). We are insterested on
finding the most probable candidate translations
for the polysemous word wk. Therefore, we may
use a Naive Bayes classifier which considers the
probability of tki given wk. A formal description
of the classifier is given as follows.
</bodyText>
<equation confidence="0.99885">
p(tki |S) = p(tki |w1, w2, ··· , wk,··· ) (1)
i )
p(tk i |S) = p(tk i )p(w1, w2, ·· · , wk, · · · |tk
p(w1, w2, · · · , wk, · · · ) (2)
</equation>
<bodyText confidence="0.999857428571429">
We are interested on finding the argument that
maximizes p(tki |S), therefore, we may to calculate
the denominator. Moreover, if we assume that all
the different translations are equally distributed,
then Eq. (2) may be approximated by Eq. (3).
approach, the unweighted version. Algorithm 1
provides details about the implementation.
</bodyText>
<equation confidence="0.849222666666667">
BestSenseu(wk) = arg max
ti
with i = 1, · · · , N.
</equation>
<construct confidence="0.438242666666667">
Algorithm 1: An unweighted naive Bayes ap-
proach to cross-lingual WSD
Input: A set Q of sentences:
</construct>
<equation confidence="0.866838333333333">
Q = {S1, S2, ··· };
Dictionary = p(w|t): A bilingual statistical
dictionary;
</equation>
<bodyText confidence="0.501507">
Output: The best word/sense for each
</bodyText>
<figure confidence="0.9516575">
ambiguous word wj ∈ Sl
1 for l fori=
2 = 1 to |Q |do
3 1toNdo
4 Pl,i = 1;
for j = 1 to |Sl |do
5 foreach wj ∈ Sl do
6 if wj ∈ Dictionary then
7 Pl,i = Pl,i ∗ p(wj|tki );
8 else
9 Pl,i = Pl,i ∗ E;
10 end
11 end
12 end
13 end
14 end
15 return arg maxt� rj|Sl1 p(wj  |tk)
|S|
H p(wj|tki ) (5)
j=1
</figure>
<bodyText confidence="0.9920245">
A second approach (weighted version) is also
proposed as shown in Eq. (6). Algorithm 2 pro-
</bodyText>
<equation confidence="0.9571695">
p(tki|w1, w2, · · · , wk, · · · ) ≈ p(w1, w2, · · · , wk, · · · |tki)vides details about its implementation.
(3)
</equation>
<bodyText confidence="0.99979475">
The complete calculation of Eq. (3) requires to
apply the chain rule. However, if we assumed that
the words of the sentence are independent, then we
may rewrite Eq. (3) as Eq. (4).
</bodyText>
<equation confidence="0.989801333333333">
|S|
p(tki |w1, w2, ··· , wk, · · · ) ≈ H p(wj|tki ) (4)
j=1
</equation>
<bodyText confidence="0.999839">
The best translation is obtained as shown in Eq.
(5). Nevertheless the position of the ambiguous
word, we are only considering a product of the
probabilites of translation. Thus, we named this
</bodyText>
<equation confidence="0.986317">
BestSensew(wk) =
</equation>
<bodyText confidence="0.999774571428572">
With respect to the N candidate translations
of the polysemous word wk, {tk1, tk2, · · · , tkN}, we
have used of the Google translator1. Google pro-
vides all the possible translations for wk with
the corresponding grammatical category. There-
fore, we are able to use those translations that
match with the same grammatical category of the
</bodyText>
<footnote confidence="0.891655">
1http://translate.google.com.mx/
</footnote>
<table confidence="0.647291">
arg max |S |1
ti H p(wj|tk i ) ∗ (6)
j=1 k − j + 1
</table>
<page confidence="0.965666">
113
</page>
<figureCaption confidence="0.824465">
Figure 1: An overview of the presented approach for cross-lingual word sense disambiguation
Algorithm 2: A weighted naive Bayes ap-
proach to cross-lingual WSD
</figureCaption>
<bodyText confidence="0.741646">
Input: A set Q of sentences:
</bodyText>
<equation confidence="0.860934333333333">
Q = JS1, S2, ··· };
Dictionary = p(w|t): A bilingual statistical
dictionary;
</equation>
<bodyText confidence="0.929881">
Output: The best word/sense for each
ambiguous word wj E Sl
</bodyText>
<equation confidence="0.994640545454545">
1 for l = 1 to |Q |do
for i = 1toNdo
Pl,i = 1;
for j = 1 to |Sl |do
foreach wj E Sl do
if wj E Dictionary then
Pl,i =
Pl,i * p(wj|tki ) * 1
k−j+1;
else
Pl,i = Pl,i * E;
</equation>
<listItem confidence="0.800879">
end
end
end
end
14 end
15 return arg maxt� l l�s&apos;1 p(wj |tki ) * k−j+1
</listItem>
<bodyText confidence="0.95971">
ambiguous word. Even if we attempted other
approaches such as selecting the most probable
translations from the statistical dictionary, we con-
firmed that by using the Google online transla-
tor we obtain the best results. We consider that
this result is derived from the fact that Google has
a better language model than we have, because
our bilingual statistical dictionary was trained only
with the EUROPARL parallel corpus.
The experimental results of both, the un-
weighted and the weighted versions of the pre-
sented approach for cross-lingual word sense dis-
ambiguation are given in Section 3.
</bodyText>
<subsectionHeader confidence="0.999739">
2.2 Cross-Lingual Lexical Substitution
</subsectionHeader>
<bodyText confidence="0.999757928571429">
This module is based on the cross-lingual word
sense disambiguation system. Once we knew
the best word/sense (Spanish) for the ambigu-
ous word(English), we lemmatized the Spanish
word. Thereafter, we searched, at WordNet, the
synonyms of this word (sense) that agree with
the grammatical category (noun, verb, etc) of the
query (source polysemous word), and we return
those synonyms as possible lexical substitutes.
Notice again that this task is complemented by the
WSD solver.
In Figure 2 we may see the complete process of
approaching the problem of cross-lingual lexical
substitution.
</bodyText>
<page confidence="0.54213">
2
</page>
<figure confidence="0.9424533">
3
4
5
6
7
8
9
10
11
12
</figure>
<page confidence="0.8020745">
13
114
</page>
<figureCaption confidence="0.998039">
Figure 2: An overview of the presented approach for cross-lingual lexical substitution
</figureCaption>
<sectionHeader confidence="0.996538" genericHeader="method">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.999939333333333">
In this section we present the obtained results for
both, the cross-lingual word sense disambiguation
task and the cross-lingual lexical substitution task.
</bodyText>
<subsectionHeader confidence="0.9779755">
3.1 Cross-Lingual Word Sense
Disambiguation
</subsectionHeader>
<bodyText confidence="0.999969653846154">
In Table 2 we may see the results we have ob-
tained with the different versions of the presented
approach. In the same Table we can find a com-
parison of our runs with others presented at the
SemEval-2 competition. In particular, we have
tested four different runs which correspond to two
evaluations for each different version of the prob-
abilistic classifier. The description of each run is
given in Table 1.
We obtained a better performance with those
runs that were evaluated with the five best trans-
lations (oof) than with those that were evaluated
with only the best ones. This fact lead us to con-
sider in further work to improve the ranking of the
translations found by our system. On other hand,
the unweighted version of the proposed classifier
improved the weighted one. This behavior was un-
expected, because in the development dataset, the
results were opposite. We consider that the prob-
lem comes from taking into account the entire sen-
tence instead of a neighborhood (windows) around
the ambiguous word. We will further investigate
about this issue. We got a better performance than
other systems, and those runs that outperformed
our system runs did it by around 3% of precision
and recall in the case of the oof evaluation.
</bodyText>
<subsectionHeader confidence="0.999964">
3.2 Cross-Lingual Lexical Substitution
</subsectionHeader>
<bodyText confidence="0.999995714285714">
In Table 3 we may see the obtained results for
the cross-lingual lexical substitution task. The ob-
tained results are low in comparison with the best
one. Since this task relies on the C-WSD task, then
a lower performance on the C-WSD task will con-
duct to a even lower performance in C-LS. Firstly,
we need to improve the C-WSD solver. In partic-
ular, we need to improve the ranking procedure in
order to obtain a better translation of the source
ambiguous word. Moreover, we consider that the
use of language modeling would be of high ben-
efit, since we could test whether or not a given
translation together with the terms in its context
would have high probability in the target language.
</bodyText>
<page confidence="0.997644">
115
</page>
<table confidence="0.9776606">
Run name Description
FCC-WSD1 : Best translation (one target word) / unweighted version
FCC-WSD2 : Five best translations (five target words - oof) / unweighted version
FCC-WSD3 : Best translation (one target word) / weighted version
FCC-WSD4 : Five best translations (five target words - oof) / weighted version
</table>
<tableCaption confidence="0.980318">
Table 1: Description of runs
</tableCaption>
<table confidence="0.999808375">
System name Precision (%) Recall (%) System name Precision (%) Recall (%)
UvT-v 23.42 23.42 UvT-v 42.17 42.17
UvT-g 19.92 19.92 UvT-g 43.12 43.12
FCC-WSD1 15.09 15.09 FCC-WSD2 40.76 40.76
FCC-WSD3 14.43 14.43 FCC-WSD4 38.46 38.46
UHD-1 20.48 16.33 UHD-1 38.78 31.81
UHD-2 20.2 16.09 UHD-2 37.74 31.3
T3-COLEUR 19.78 19.59 T3-COLEUR 35.84 35.46
</table>
<tableCaption confidence="0.8739195">
a) Best translation b) Five best translations (oof)
Table 2: Evaluation of the cross-lingual word sense disambiguation task
Table 3: Evaluation of the cross-lingual lexical
substitution task (the ten best results - oot)
</tableCaption>
<sectionHeader confidence="0.997001" genericHeader="conclusions">
4 Conclusions and Further Work
</sectionHeader>
<bodyText confidence="0.9990045">
In this paper we have presented a system for cross-
lingual word sense disambiguation and cross-
lingual lexical substitution. The approach uses a
Naive Bayes classifier which is fed with the prob-
abilities obtained from a bilingual statistical dic-
tionary. Two different versions of the classifier,
unweighted and weighted were tested. The results
were compared with those of an international com-
petition, obtaining a good performance. As fur-
ther work, we need to improve the ranking mod-
ule of the cross-lingual WSD classifier. Moreover,
we consider that the use of a language model for
Spanish would highly improve the results on the
cross-lingual lexical substitution task.
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999710333333333">
This work has been partially supported by CONA-
CYT (Project #106625) and PROMEP (Grant
#103.5/09/4213).
</bodyText>
<sectionHeader confidence="0.99884" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997597807692308">
[Agirre and Edmonds2006] E. Agirre and P. Edmonds.
2006. Word Sense Disambiguation, Text, Speech
and Language Technology. Springer.
[Carpuat and Wu.2007] M. Carpuat and D. Wu. 2007.
Improving statistical machine translation using word
sense disambiguation. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLPCoNLL), pages 61–72.
[Chan et al.2007] Y.S. Chan, H.T. Ng, and D. Chiang.
2007. Word sense disambiguation improves statisti-
cal machine translation. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, pages 33–40.
[Lefever and Hoste2010] E. Lefever and V. Hoste.
2010. Semeval-2010 task3:cross-lingual word
sense disambiguation. In Proceedings of the
Fifth International Workshop on Semantic Evalu-
ations (SemEval-2010). Association for Computa-
tional Linguistics.
[Mihalcea et al.2010] R. Mihalcea, R. Sinha, and
D. McCarthy. 2010. Semeval-2010 task2:cross-
lingual lexical substitution. In Proceedings of the
Fifth International Workshop on Semantic Evalu-
ations (SemEval-2010). Association for Computa-
tional Linguistics.
</reference>
<figure confidence="0.998362117647059">
System name
Precision (%) Recall (%)
SWAT-E
SWAT-S
UvT-v
UvT-g
UBA-W
WLVUSP
UBA-T
USPWLV
ColSlm
ColEur
TYO
IRST-1
FCC-LS
IRSTbs
174.59 174.59
97.98 97.98
58.91 58.91
55.29 55.29
52.75 52.75
48.48 48.48
47.99 47.99
47.6 47.6
43.91 46.61
41.72 44.77
34.54 35.46
31.48 33.14
23.9 23.9
8.33 29.74
DICT
DICTCORP
44.04 44.04
42.65 42.65
</figure>
<page confidence="0.975994">
116
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.394693">
<title confidence="0.5501825">FCC: Modeling Probabilities with GIZA++ for Task #2 and #3 of SemEval-2</title>
<author confidence="0.877133">Carlos Balderas</author>
<author confidence="0.877133">David Pinto</author>
<author confidence="0.877133">Miguel Rodriguez</author>
<author confidence="0.877133">Saul Le´on</author>
<affiliation confidence="0.936273">Faculty of Computer Science, BUAP</affiliation>
<address confidence="0.78432">Puebla, Mexico</address>
<abstract confidence="0.9983546875">In this paper we present a naive approach to tackle the problem of cross-lingual WSD and cross-lingual lexical substitution which correspond to the Task #2 and a bilingual statistical dictionary, which is calculated with Giza++ by using the EU- ROPARL parallel corpus, in order to calculate the probability of a source word to be translated to a target word (which is assumed to be the correct sense of the source word but in a different language). Two versions of the probabilistic model are tested: unweighted and weighted. The obtained values show that the unweighted version performs better thant the weighted one.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>P Edmonds</author>
</authors>
<title>Word Sense Disambiguation, Text, Speech and Language Technology.</title>
<date>2006</date>
<publisher>Springer.</publisher>
<marker>[Agirre and Edmonds2006]</marker>
<rawString>E. Agirre and P. Edmonds. 2006. Word Sense Disambiguation, Text, Speech and Language Technology. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Carpuat</author>
<author>D Wu</author>
</authors>
<title>Improving statistical machine translation using word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>61--72</pages>
<marker>[Carpuat and Wu.2007]</marker>
<rawString>M. Carpuat and D. Wu. 2007. Improving statistical machine translation using word sense disambiguation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 61–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y S Chan</author>
<author>H T Ng</author>
<author>D Chiang</author>
</authors>
<title>Word sense disambiguation improves statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>33--40</pages>
<marker>[Chan et al.2007]</marker>
<rawString>Y.S. Chan, H.T. Ng, and D. Chiang. 2007. Word sense disambiguation improves statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Lefever</author>
<author>V Hoste</author>
</authors>
<title>Semeval-2010 task3:cross-lingual word sense disambiguation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fifth International Workshop on Semantic Evaluations (SemEval-2010). Association for Computational Linguistics.</booktitle>
<marker>[Lefever and Hoste2010]</marker>
<rawString>E. Lefever and V. Hoste. 2010. Semeval-2010 task3:cross-lingual word sense disambiguation. In Proceedings of the Fifth International Workshop on Semantic Evaluations (SemEval-2010). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>R Sinha</author>
<author>D McCarthy</author>
</authors>
<title>Semeval-2010 task2:crosslingual lexical substitution.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fifth International Workshop on Semantic Evaluations (SemEval-2010). Association for Computational Linguistics.</booktitle>
<marker>[Mihalcea et al.2010]</marker>
<rawString>R. Mihalcea, R. Sinha, and D. McCarthy. 2010. Semeval-2010 task2:crosslingual lexical substitution. In Proceedings of the Fifth International Workshop on Semantic Evaluations (SemEval-2010). Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>