<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000804">
<title confidence="0.9959325">
The Effect of Higher-Order Dependency Features in Discriminative
Phrase-Structure Parsing
</title>
<author confidence="0.957131">
Gregory F. Coppola and Mark Steedman
</author>
<affiliation confidence="0.9979795">
School of Informatics
The University of Edinburgh
</affiliation>
<email confidence="0.9888595">
g.f.coppola@sms.ed.ac.uk
steedman@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.997318" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999831230769231">
Higher-order dependency features are
known to improve dependency parser ac-
curacy. We investigate the incorporation
of such features into a cube decoding
phrase-structure parser. We find consid-
erable gains in accuracy on the range of
standard metrics. What is especially in-
teresting is that we find strong, statisti-
cally significant gains on dependency re-
covery on out-of-domain tests (Brown vs.
WSJ). This suggests that higher-order de-
pendency features are not simply over-
fitting the training material.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.976864862068966">
Higher-order dependency features encode more
complex sub-parts of a dependency tree struc-
ture than first-order, bigram head-modifier rela-
tionships.1 The clear trend in dependency pars-
ing has been that the addition of such higher-order
features improves parse accuracy (McDonald &amp;
Pereira, 2006; Carreras, 2007; Koo &amp; Collins,
2010; Zhang &amp; Nivre, 2011; Zhang &amp; McDonald,
2012). This finding suggests that the same ben-
efits might be observed in phrase-structure pars-
ing. But, this is not necessarily implied. Phrase-
structure parsers are generally stronger than de-
pendency parsers (Petrov et al., 2010; Petrov &amp;
McDonald, 2012), and make use of more kinds
of information. So, it might be that the infor-
mation modelled by higher-order dependency fea-
tures adds less of a benefit in the phrase-structure
case.
1Examples of first-order and higher-order dependency
features are given in §3.2.
To investigate this issue, we experiment using
Huang’s (2008) cube decoding algorithm. This
algorithm allows structured prediction with non-
local features, as discussed in §2. Collins’s (1997)
strategy of expanding the phrase-structure parser’s
dynamic program to incorporate head-modifier de-
pendency information would not scale to the com-
plex kinds of dependencies we will consider. Us-
ing Huang’s algorithm, we can indeed incorporate
arbitrary types of dependency feature, using a sin-
gle, simple dynamic program.
Compared to the baseline, non-local feature
set of Collins (2000) and Charniak &amp; Johnson
(2005), we find that higher-order dependencies
do in fact tend to improve performance signifi-
cantly on both dependency and constituency ac-
curacy metrics. Our most interesting finding,
though, is that higher-order dependency features
show a consistent and unambiguous contribution
to the dependency accuracy, both labelled and un-
labelled, of our phrase-structure parsers on out-
of-domain tests (which means, here, trained on
WSJ, but tested on BROWN). In fact, the gains are
even stronger on out-of-domain tests than on in-
domain tests. One might have thought that higher-
order dependencies, being rather specific by na-
ture, would tend to pick out only very rare events,
and so only serve to over-fit the training material,
but this is not what we find. We speculate as to
what this might mean in §5.2.
The cube decoding paradigm requires a first-
stage parser to prune the output space. For this, we
use the generative parser of Petrov et al. (2006).
We can use this parser’s model score as a fea-
ture in our discriminative model at no additional
cost. However, doing so conflates the contribu-
tion to accuracy of the generative model, on the
one hand, and the discriminatively trained, hand-
</bodyText>
<page confidence="0.958441">
610
</page>
<bodyText confidence="0.956236875">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 610–616,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
written, features, on the other. Future systems
might use the same or a similar feature set to
ours, but in an architecture that does not include
any generative parser. On the other hand, some
systems might indeed incorporate this generative
model’s score. So, we need to know exactly what
the generative model is contributing to the accu-
racy of a generative-discriminative model combi-
nation. Thus, we conduct experiments in sets: in
some cases the generative model score is used, and
in others it is not used.
Compared to the faster and more psycholog-
ically plausible shift-reduce parsers (Zhang &amp;
Nivre, 2011; Zhang &amp; Clark, 2011), cube decod-
ing is a computationally expensive method. But,
cube decoding provides a relatively exact envi-
ronment with which to compare different feature
sets, has close connections with modern phrase-
based machine translation methods (Huang &amp; Chi-
ang, 2007), and produces very accurate parsers. In
some cases, one might want to use a slower, but
more accurate, parser during the training stage of
a semi-supervised parser training strategy. For ex-
ample, Petrov et al. (2010) have shown that a fast
parser (Nivre et al., 2007) can be profitably trained
from the output of a slower but more accurate one
(Petrov et al., 2006), in a strategy they call uptrain-
ing.
We make the source code for these experiments
available.2
</bodyText>
<sectionHeader confidence="0.9938695" genericHeader="method">
2 Phrase-Structure Parsing with
Non-Local Features
</sectionHeader>
<subsectionHeader confidence="0.993183">
2.1 Non-Local Features
</subsectionHeader>
<bodyText confidence="0.999962090909091">
To decode using exact dynamic programming (i.e.,
CKY), one must restrict oneself to the use of only
local features. Local features are those that fac-
tor according to the individual rule productions of
the parse. For example, a feature indicating the
presence of the rule S → NP VP is local.3 But,
a feature that indicates that the head word of this
S is, e.g., joined, is non-local, because the head
word of a phrase cannot be determined by look-
ing at a single rule production. To find a phrase’s
head word (or tag), we must recursively find the
</bodyText>
<footnote confidence="0.989230428571429">
2See http://gfcoppola.net/code.php. This
software is available for free for non-profit research uses.
3A feature indicating that, e.g., the first word dominated
by S is Pierre is also local, since the words of the sentence
are constant across hypothesized parses, and words can be
referred to by their position with respect to a given rule pro-
duction. See Huang (2008) for more details.
</footnote>
<bodyText confidence="0.9998707">
head phrase of each local rule production, until we
reach a terminal node (or tag node). This recursion
would not be allowed in standard CKY. Many dis-
criminative parsers have used only local features
(Taskar et al., 2004; Turian et al., 2007; Finkel
et al., 2008). However, Huang (2008) shows that
the use of non-local features does in fact contribute
substantially to parser performance. And, our de-
sire to make heavy use of head-word dependency
relations necessitates the use of non-local features.
</bodyText>
<subsectionHeader confidence="0.999689">
2.2 Cube Decoding
</subsectionHeader>
<bodyText confidence="0.999993277777778">
While the use of non-local features destroys the
ability to do exact search, we can still do inex-
act search using Huang’s (2008) cube decoding
algorithm.4 A tractable first-stage parser prunes
the space of possible parses, and outputs a forest,
which is a set of rule production instances that can
be used to make a parse for the given sentence,
and which is significantly pruned compared to the
entire space allowed by the grammar. The size of
this forest is at most cubic in the length of the sen-
tence (Billot &amp; Lang, 1989), but implicitly repre-
sents exponentially many parses. To decode, we
fix an beam width of k (an integer). Then, when
parsing, we visit each node n in the same bottom-
up order we would use for Viterbi decoding, and
compute a list of the top k parses to n, according
to a global linear model (Collins, 2002), using the
trees that have survived the beam at earlier nodes.
</bodyText>
<subsectionHeader confidence="0.999557">
2.3 The First-Stage Parser
</subsectionHeader>
<bodyText confidence="0.999933125">
As noted, we require a first-stage parser to prune
the search space.5 As a by-product of this pruning
procedure, we are able to use the model score of
the first-stage parser as a feature in our ultimate
model at no additional cost. As a first-stage parser,
we use Huang et al.’s (2010) implementation of
the LA-PCFG parser of Petrov et al. (2006), which
uses a generative, latent-variable model.
</bodyText>
<sectionHeader confidence="0.99993" genericHeader="method">
3 Features
</sectionHeader>
<subsectionHeader confidence="0.999756">
3.1 Phrase-Structure Features
</subsectionHeader>
<bodyText confidence="0.9694975">
Our phrase-structure feature set is taken from
Collins (2000), Charniak &amp; Johnson (2005), and
</bodyText>
<footnote confidence="0.999283285714286">
4This algorithm is closely related to the algorithm for
phrase-based machine translation using a language model
(Huang &amp; Chiang, 2007).
5All work in this paradigm has used a generative parser as
the first-stage parser. But, this is arguably a historical acci-
dent. We could just as well use a discriminative parser with
only local features, like Petrov &amp; Klein (2007a).
</footnote>
<page confidence="0.996702">
611
</page>
<bodyText confidence="0.999545428571429">
Huang (2008). Some features are omitted, with
choices made based on the ablation studies of
Johnson &amp; Ural (2010). This feature set, which we
call phrase, contains the following, mostly non-
local, features, which are described and depicted
in Charniak &amp; Johnson (2005), Huang (2008), and
Johnson &amp; Ural (2010):
</bodyText>
<listItem confidence="0.997455684210526">
• CoPar The depth (number of levels) of par-
allelism between adjacent conjuncts
• CoParLen The difference in length between
adjacent conjuncts
• Edges The words or (part-of-speech) tags on
the outside and inside edges of a given XP6
• NGrams Sub-parts of a given rule production
• NGramTree An n-gram of the input sen-
tence, or the tags, along with the minimal tree
containing that n-gram
• HeadTree A sub-tree containing the path
from a word to its maximal projection, along
with all siblings of all nodes in that path
• Heads Head-modifier bigrams
• Rule A single rule production
• Tag The tag of a given word
• Word The tag of and first XP above a word
• WProj The tag of and maximal projection of
a word
</listItem>
<bodyText confidence="0.712765">
Heads is a first-order dependency feature.
</bodyText>
<subsectionHeader confidence="0.996654">
3.2 Dependency Parsing Features
</subsectionHeader>
<bodyText confidence="0.999991764705882">
McDonald et al. (2005) showed that chart-based
dependency parsing, based on Eisner’s (1996) al-
gorithm, could be successfully approached in a
discriminative framework. In this earliest work,
each feature function could only refer to a sin-
gle, bigram head-modifier relationship, e.g., Mod-
ifier, below. Subsequent work (McDonald &amp;
Pereira, 2006; Carreras, 2007; Koo &amp; Collins,
2010) looked at allowing features to access more
complex, higher-order relationships, including tri-
gram and 4-gram relationships, e.g., all features
apart from Modifier, below. With the ability to
incorporate non-local phrase-structure parse fea-
tures (Huang, 2008), we can recognize depen-
dency features of arbitrary order (cf. Zhang &amp;
McDonald (2012)). Our dependency feature set,
which we call deps, contains:
</bodyText>
<listItem confidence="0.981825">
• Modifier head and modifier
</listItem>
<footnote confidence="0.9432045">
6The tags outside of a given XP are approximated using
the marginally most likely tags given the parse.
</footnote>
<listItem confidence="0.9969048">
• Sibling head, modifier m, and m’s nearest in-
ner sibling
• Grandchild head, modifier m, and one of
m’s modifiers
• Sibling+Grandchild head, modifier m, m’s
nearest inner sibling, and one of m’s modi-
fiers
• Grandchild+Grandsibling head, modifier
m, one of m’s modifiers g, and g’s inner sib-
ling
</listItem>
<bodyText confidence="0.99979275">
These features are insensitive to arc labels in the
present experiments, but future work will incorpo-
rate arc labels. Each feature class contains more
and less lexicalized versions.
</bodyText>
<subsectionHeader confidence="0.511786">
3.3 Generative Model Score Feature
</subsectionHeader>
<bodyText confidence="0.999988571428571">
Finally, we have a feature set, gen, contain-
ing only one feature function. This feature
maps a parse to the logarithm of the MAX-RULE-
PRODUCT score of that parse according to the LA-
PCFG parsing model, which is trained separately.
This score has the character of a conditional like-
lihood for the parse (see Petrov &amp; Klein (2007b)).
</bodyText>
<sectionHeader confidence="0.997402" genericHeader="method">
4 Training
</sectionHeader>
<bodyText confidence="0.99988345">
We have two feature sets phrase and deps, for
which we fix weights using parallel stochastic op-
timization of a structured SVM objective (Collins,
2002; Taskar et al., 2004; Crammer et al., 2006;
Martins et al., 2010; McDonald et al., 2010). To
the single feature in the set gen (i.e. the genera-
tive model score), we give the weight 1.
The combined models, phrase+deps, phrase+gen,
and phrase+deps+gen, are then model combinations
of the first three. The combination weights
for these combinations are obtained using Och’s
(2003) Minimum Error-Rate Training (MERT).
The MERT stage helps to avoid feature under-
training (Sutton et al., 2005), and avoids the prob-
lem of scaling involved in a model that contains
mostly boolean features, but one, real-valued, log-
scale feature. Training is conducted in three stages
(SVM, MERT, SVM), so that there is no influence of
any data outside the given training set (WSJ2-21)
on the combination weights.
</bodyText>
<sectionHeader confidence="0.999716" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.6674">
5.1 Methods
</subsectionHeader>
<bodyText confidence="0.973631">
All models are trained on WSJ2-21, with WSJ22
used to pick the stopping iteration for online
</bodyText>
<page confidence="0.984733">
612
</page>
<table confidence="0.999929">
Test Set
WSJ BROWN
Type Model F1 UAS LAS F1 UAS LAS
G LA-PCFG 90.3 93.7 91.5 85.1 88.7 85.0
D phrase 91.2 93.9 91.0 86.1 89.4 85.1
deps — 93.3 — — 89.3 —
phrase+deps 91.7 94.4 91.5 86.4 90.1 85.9
G+D phrase+gen 92.1 94.7 92.6 87.0 90.0 86.5
phrase+deps+gen 92.4 94.9 92.8 87.4 90.7 87.1
</table>
<tableCaption confidence="0.68141575">
Table 1: Performance of the various models in cube decoding experiments, on the WSJ test set (in-
domain) and the BROWN test set (out-of-domain). G abbreviates generative, D abbreviates discrim-
inative, and G+D a combination. Some cells are empty because deps features are only sensitive to
unlabelled dependencies. Best results in D and G+D conditions appear in bold face.
</tableCaption>
<table confidence="0.999735375">
Test Set
Hypothesis WSJ BROWN
Greater Lesser F1 UAS LAS F1 UAS LAS
phrase+deps phrase .042 .029 .018 .140 .022 .009
phrase+deps deps — &lt;.001 — — .012 —
phrase+gen phrase .013 .003 &lt;.001 .016 .090 &lt;.001
phrase+deps+gen phrase+gen .030 .122 .151 .059 .008 .020
phrase+deps+gen phrase+deps .019 .020 &lt;.001 .008 .040 &lt;.001
</table>
<tableCaption confidence="0.923037">
Table 2: Results of statistical significance evaluations of hypotheses of the form X’s accuracy is greater
than Y’s on the various test sets and metrics. Bold face indicates p &lt; .05.
</tableCaption>
<bodyText confidence="0.99465980952381">
optimization, as is standard. The test sets are
WSJ23 (in-domain test set), and BROWN9 (out-of-
domain test set) from the Penn Treebank (Mar-
cus et al., 1993).7 We evaluate using harmonic
mean between labelled bracket recall and preci-
sion (EVALB F1), unlabelled dependency accuracy
(UAS), and labelled dependency accuracy (LAS).
Dependencies are extracted from full output trees
using the algorithm of de Marneffe &amp; Manning
(2008). We chose this dependency extractor,
firstly, because it is natively meant to be run on
the output of phrase-structure parsers, rather than
on gold trees with function tags and traces still
present, as is, e.g., the Penn-Converter of Johans-
son &amp; Nugues (2007). Also, this is the extractor
that was used in a recent shared task (Petrov &amp;
McDonald, 2012). We use EVALB and eval.pl to
calculate scores.
For hypothesis testing, we used the paired boot-
strap test recently empirically evaluated in the con-
text of NLP by Berg-Kirkpatrick et al. (2012). This
</bodyText>
<footnote confidence="0.643907">
7Following Gildea (2001), the BROWN test set is usually
divided into 10 parts. If we start indexing at 0, then the last
(test) section has index 9. We received the BROWN data splits
from David McClosky, p.c.
</footnote>
<bodyText confidence="0.9994342">
involves drawing b subsamples of size n with re-
placement from the test set in question, and check-
ing relative performance of the models on the sub-
sample (see the reference). We use b = 106 and
n = 500 in all tests.
</bodyText>
<subsectionHeader confidence="0.835642">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999987588235294">
The performance of the models is shown in Table
1, and Table 2 depicts the results of significance
tests of differences between key model pairs.
We find that adding in the higher-order depen-
dency feature set, deps, makes a statistically sig-
nificant improvement in accuracy on most met-
rics, in most conditions. On the in-domain WSJ
test set, we find that phrase+deps is significantly
better than either of its component parts on all
metrics. But, phrase+deps+gen is significantly bet-
ter than phrase+gen only on F1, but not on UAS
or LAS. However, on the out-of-domain BROWN
tests, we find that adding deps always adds con-
siderably, and in a statistically significant way, to
both LAS and UAS. That is, not only is phrase+deps
better at dependency recovery than its component
parts, but phrase+deps+gen is also considerably bet-
</bodyText>
<page confidence="0.998449">
613
</page>
<bodyText confidence="0.999707098039216">
ter on dependency recovery than phrase+gen, which
represents the previous state-of-the-art in this vein
of research (Huang, 2008). This result is perhaps
counter-intuitive, in the sense that one might have
supposed that higher-order dependency features,
being highly specific by nature, might only have
only served to over-fit the training material. How-
ever, this result shows otherwise. Note that the
dependency features include various levels of lex-
icalization. It might be that the more unlexical-
ized features capture something about the struc-
ture of correct parses, that transfers well out-of-
domain. Future work should investigate this. And,
it of course remains to be seen how this result will
transfer to other train-test domain pairs.
To our knowledge, this is the first work to
specifically separate the role of the generative
model feature from the other features of Collins
(2000) and Charniak &amp; Johnson (2005). We note
that, even without the gen feature, the discrimi-
native parsing models are very strong, but adding
gen nevertheless yields considerable gains. Thus,
while a fully discriminative model, perhaps im-
plemented using a shift-reduce algorithm, can be
expected to do very well, if the best accuracy is
necessary (e.g., in a semi-supervised training strat-
egy), it still seems to pay to use the generative-
discriminative model combination. Note that the
LAS scores of our models without gen are rela-
tively weak. This is presumably largely because
our dependency features are, at present, not sen-
sitive to arc labels, so our results probably under-
estimate the capability of our general framework
with respect to labelled dependency recovery.
Table 3 compares our work with Huang’s
(2008). Note that our model phrase+gen uses es-
sentially the same features as Huang (2008), so
the fact that our phrase+gen is noticeably more ac-
curate on F1 is presumably due to the benefits
in reduced feature under-training achieved by the
MERT combination strategy. Also, our phrase+deps
model is as accurate as Huang’s, without even us-
ing the generative model score feature. Table 4
compares our work to McClosky et al.’s (2006)
domain adaptation work with the Charniak &amp;
Johnson (2005) parser. Their three models shown
have been trained on: i) the WSJ (supervised,
out-of-domain), ii) the WSJ plus 2.5 million sen-
tences of automatically labelled NANC newswire
text (semi-supervised, out-of-domain), and iii) the
BROWN corpus (supervised, in-domain). We test
</bodyText>
<table confidence="0.9997976">
Type Model WSJ
G+D Huang (2008) 91.7
D phrase+deps 91.7
G+D phrase+gen 92.1
G+D phrase+deps+gen 92.4
</table>
<tableCaption confidence="0.980698666666667">
Table 3: Comparison of constituency parsing re-
sults in the cube decoding framework, on the WSJ
test set. On G+D, D, see Table 1.
</tableCaption>
<table confidence="0.9998516">
Parser Training Data BROWN F1
CJ WSJ 85.2
CJ WSJ+NANC 87.8
CJ BROWN 88.4
Our Best WSJ 87.4
</table>
<tableCaption confidence="0.999861">
Table 4: Comparison of our best model,
</tableCaption>
<bodyText confidence="0.96878975">
phrase+deps+gen, on BROWN, with the Charniak &amp;
Johnson (2005) parser, denoted CJ, as reported in
McClosky et al. (2006). Underline indicates best
trained on WSJ, bold face indicates best overall.
on BROWN. We see that our best (WSJ-trained)
model is over 2% more accurate (absolute F1
difference) than the Charniak &amp; Johnson (2005)
parser trained on the same data. In fact, our
best model is nearly as good as McClosky et al.’s
(2006) self-trained, semi-supervised model. Of
course, the self-training strategy is orthogonal to
the improvements we have made.
</bodyText>
<sectionHeader confidence="0.996903" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999927272727273">
We have shown that the addition of higher-order
dependency features into a cube decoding phase-
structure parser leads to statistically significant
gains in accuracy. The most interesting finding
is that these gains are clearly observed on out-of-
domain tests. This seems to imply that higher-
order dependency features do not merely over-fit
the training material. Future work should look at
other train-test domain pairs, as well as look at ex-
actly which higher-order dependency features are
most important to out-of-domain accuracy.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999807666666667">
This work was supported by the Scottish Infor-
matics and Computer Science Alliance, The Uni-
versity of Edinburgh’s School of Informatics, and
ERC Advanced Fellowship 249520 GRAMPLUS.
We thank Zhongqiang Huang for his extensive
help in getting started with his LA-PCFG parser.
</bodyText>
<page confidence="0.998066">
614
</page>
<sectionHeader confidence="0.991416" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.740752714285714">
Berg-Kirkpatrick, T., Burkett, D., &amp; Klein, D.
(2012). An empirical investigation of statistical
significance in NLP. In EMNLP, 995–1005.
Billot, S., &amp; Lang, B. (1989). The structure of
shared forests in ambiguous parsing. In ACL,
143–151.
Carreras, X. (2007). Experiments with a higher-
order projective dependency parser. In Pro-
ceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, 957–961.
Charniak, E., &amp; Johnson, M. (2005). Coarse-to-
fine n-best parsing and MaxEnt discriminative
reranking. In ACL, 173–180.
Collins, M. (1997). Three generative, lexicalised
models for statistical parsing. In ACL, 16–23.
Collins, M. (2000). Discriminative reranking for
natural language parsing. In ICML, 175–182.
Collins, M. (2002). Discriminative training meth-
ods for Hidden Markov Models: theory and
experiments with perceptron algorithms. In
EMNLP, 1–8.
Crammer, K., Dekel, O., Keshet, J., Shalev-
Shwartz, S., &amp; Singer, Y. (2006). Online
passive-aggressive algorithms. JMLR, 7, 551–
585.
Eisner, J. (1996). Three new probabilistic mod-
els for dependency parsing: An exploration. In
COLING, 340–345.
</bodyText>
<reference confidence="0.97412023880597">
Finkel, J. R., Kleeman, A., &amp; Manning, C. D.
(2008). Efficient, feature-based, conditional
random field parsing. In ACL, 959–967.
Gildea, D. (2001). Corpus variation and parser
performance. In EMNLP, 167–202.
Huang, L. (2008). Forest reranking: Discrimina-
tive parsing with non-local features. In ACL,
586–594.
Huang, L., &amp; Chiang, D. (2007). Forest rescor-
ing: Faster decoding with integrated language
models. In ACL.
Huang, Z., Harper, M., &amp; Petrov, S. (2010). Self-
training with products of latent variable gram-
mars. In EMNLP, 12–22.
Johansson, R., &amp; Nugues, P. (2007). Extended
constituent-to-dependency conversion for En-
glish. In Proc. of the 16th Nordic Conference on
Computational Linguistics (NODALIDA), 105–
112.
Johnson, M., &amp; Ural, A. E. (2010). Reranking the
Berkeley and Brown parsers. In HLT-NAACL,
665–668.
Koo, T., &amp; Collins, M. (2010). Efficient third-
order dependency parsers. In ACL, 1–11.
Marcus, M. P., Santorini, B., &amp; Marcinkiewicz,
M. A. (1993). Building a large annotated corpus
of English: The Penn Treebank. Computational
Linguistics, 19(2), 313–330.
de Marneffe, M.-C., &amp; Manning, C. D. (2008).
The Stanford typed dependencies representa-
tion. In Coling 2008: Proceedings of the work-
shop on Cross-Framework and Cross-Domain
Parser Evaluation, 1–8.
Martins, A. F., Gimpel, K., Smith, N. A., Xing,
E. P., Figueiredo, M. A., &amp; Aguiar, P. M.
(2010). Learning structured classifiers with dual
coordinate ascent. Technical report, DTIC Doc-
ument.
McClosky, D., Charniak, E., &amp; Johnson, M.
(2006). Reranking and self-training for parser
adaptation. In ACL, 337–344.
McDonald, R., &amp; Pereira, F. (2006). Online learn-
ing of approximate dependency parsing algo-
rithms. In EACL, 81–88.
McDonald, R. T., Crammer, K., &amp; Pereira, F. C. N.
(2005). Online large-margin training of depen-
dency parsers. In ACL, 91–98.
McDonald, R. T., Hall, K., &amp; Mann, G. (2010).
Distributed training strategies for the structured
perceptron. In HLT-NAACL, 456–464.
Nivre, J., Hall, J., Nilsson, J., Chanev, A., Eryigit,
G., K¨ubler, S., Marinov, S., &amp; Marsi, E. (2007).
Maltparser: A language-independent system for
data-driven dependency parsing. Natural Lan-
guage Engineering, 13(2), 95–135.
Och, F. J. (2003). Minimum error rate training
in statistical machine translation. In ACL, 160–
167.
Petrov, S., Barrett, L., Thibaux, R., &amp; Klein, D.
(2006). Learning accurate, compact, and inter-
pretable tree annotation. In ACL, 433–440.
Petrov, S., Chang, P.-C., Ringgaard, M., &amp; Al-
shawi, H. (2010). Uptraining for accurate deter-
ministic question parsing. In EMNLP, 705–713.
Petrov, S., &amp; Klein, D. (2007a). Discriminative
log-linear grammars with latent variables. In
NIPS.
</reference>
<page confidence="0.983749">
615
</page>
<reference confidence="0.999853769230769">
Petrov, S., &amp; Klein, D. (2007b). Improved infer-
ence for unlexicalized parsing. In HLT-NAACL,
404–411.
Petrov, S., &amp; McDonald, R. (2012). Overview of
the 2012 shared task on parsing the web. Notes
of the First Workshop on Syntactic Analysis of
Non-Canonical Language (SANCL).
Sutton, C., Sindelar, M., &amp; McCallum, A. (2005).
Feature bagging: Preventing weight undertrain-
ing in structured discriminative learning. In
HLT-NAACL.
Taskar, B., Klein, D., Collins, M., Koller, D., &amp;
Manning, C. D. (2004). Max-margin parsing.
In EMNLP, 1–8.
Turian, J., Wellington, B., &amp; Melamed, I. D.
(2007). Scalable discriminative learning for nat-
ural language parsing and translation. In NIPS,
1409–1416.
Zhang, H., &amp; McDonald, R. (2012). General-
ized higher-order dependency parsing with cube
pruning. In EMNLP, 238–242.
Zhang, Y., &amp; Clark, S. (2011). Shift-reduce CCG
parsing. In ACL, 683–692.
Zhang, Y., &amp; Nivre, J. (2011). Transition-based
dependency parsing with rich non-local fea-
tures. In ACL, 188–293.
</reference>
<page confidence="0.998562">
616
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.968160">
<title confidence="0.996706">The Effect of Higher-Order Dependency Features in Phrase-Structure Parsing</title>
<author confidence="0.999455">Gregory F Coppola</author>
<author confidence="0.999455">Mark</author>
<affiliation confidence="0.9960455">School of The University of</affiliation>
<email confidence="0.988697">steedman@inf.ed.ac.uk</email>
<abstract confidence="0.999458571428572">Higher-order dependency features are known to improve dependency parser accuracy. We investigate the incorporation of such features into a cube decoding We find considerable gains in accuracy on the range of standard metrics. What is especially interesting is that we find strong, statistically significant gains on dependency reon tests (Brown vs. WSJ). This suggests that higher-order dependency features are not simply overfitting the training material.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>A Kleeman</author>
<author>C D Manning</author>
</authors>
<title>Efficient, feature-based, conditional random field parsing.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>959--967</pages>
<contexts>
<context position="6234" citStr="Finkel et al., 2008" startWordPosition="987" endWordPosition="990">.php. This software is available for free for non-profit research uses. 3A feature indicating that, e.g., the first word dominated by S is Pierre is also local, since the words of the sentence are constant across hypothesized parses, and words can be referred to by their position with respect to a given rule production. See Huang (2008) for more details. head phrase of each local rule production, until we reach a terminal node (or tag node). This recursion would not be allowed in standard CKY. Many discriminative parsers have used only local features (Taskar et al., 2004; Turian et al., 2007; Finkel et al., 2008). However, Huang (2008) shows that the use of non-local features does in fact contribute substantially to parser performance. And, our desire to make heavy use of head-word dependency relations necessitates the use of non-local features. 2.2 Cube Decoding While the use of non-local features destroys the ability to do exact search, we can still do inexact search using Huang’s (2008) cube decoding algorithm.4 A tractable first-stage parser prunes the space of possible parses, and outputs a forest, which is a set of rule production instances that can be used to make a parse for the given sentence</context>
</contexts>
<marker>Finkel, Kleeman, Manning, 2008</marker>
<rawString>Finkel, J. R., Kleeman, A., &amp; Manning, C. D. (2008). Efficient, feature-based, conditional random field parsing. In ACL, 959–967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>In EMNLP,</booktitle>
<pages>167--202</pages>
<contexts>
<context position="14402" citStr="Gildea (2001)" startWordPosition="2350" endWordPosition="2351">rithm of de Marneffe &amp; Manning (2008). We chose this dependency extractor, firstly, because it is natively meant to be run on the output of phrase-structure parsers, rather than on gold trees with function tags and traces still present, as is, e.g., the Penn-Converter of Johansson &amp; Nugues (2007). Also, this is the extractor that was used in a recent shared task (Petrov &amp; McDonald, 2012). We use EVALB and eval.pl to calculate scores. For hypothesis testing, we used the paired bootstrap test recently empirically evaluated in the context of NLP by Berg-Kirkpatrick et al. (2012). This 7Following Gildea (2001), the BROWN test set is usually divided into 10 parts. If we start indexing at 0, then the last (test) section has index 9. We received the BROWN data splits from David McClosky, p.c. involves drawing b subsamples of size n with replacement from the test set in question, and checking relative performance of the models on the subsample (see the reference). We use b = 106 and n = 500 in all tests. 5.2 Results The performance of the models is shown in Table 1, and Table 2 depicts the results of significance tests of differences between key model pairs. We find that adding in the higher-order depe</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Gildea, D. (2001). Corpus variation and parser performance. In EMNLP, 167–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>586--594</pages>
<contexts>
<context position="5952" citStr="Huang (2008)" startWordPosition="940" endWordPosition="941">ture that indicates that the head word of this S is, e.g., joined, is non-local, because the head word of a phrase cannot be determined by looking at a single rule production. To find a phrase’s head word (or tag), we must recursively find the 2See http://gfcoppola.net/code.php. This software is available for free for non-profit research uses. 3A feature indicating that, e.g., the first word dominated by S is Pierre is also local, since the words of the sentence are constant across hypothesized parses, and words can be referred to by their position with respect to a given rule production. See Huang (2008) for more details. head phrase of each local rule production, until we reach a terminal node (or tag node). This recursion would not be allowed in standard CKY. Many discriminative parsers have used only local features (Taskar et al., 2004; Turian et al., 2007; Finkel et al., 2008). However, Huang (2008) shows that the use of non-local features does in fact contribute substantially to parser performance. And, our desire to make heavy use of head-word dependency relations necessitates the use of non-local features. 2.2 Cube Decoding While the use of non-local features destroys the ability to do</context>
<context position="8326" citStr="Huang (2008)" startWordPosition="1344" endWordPosition="1345">the LA-PCFG parser of Petrov et al. (2006), which uses a generative, latent-variable model. 3 Features 3.1 Phrase-Structure Features Our phrase-structure feature set is taken from Collins (2000), Charniak &amp; Johnson (2005), and 4This algorithm is closely related to the algorithm for phrase-based machine translation using a language model (Huang &amp; Chiang, 2007). 5All work in this paradigm has used a generative parser as the first-stage parser. But, this is arguably a historical accident. We could just as well use a discriminative parser with only local features, like Petrov &amp; Klein (2007a). 611 Huang (2008). Some features are omitted, with choices made based on the ablation studies of Johnson &amp; Ural (2010). This feature set, which we call phrase, contains the following, mostly nonlocal, features, which are described and depicted in Charniak &amp; Johnson (2005), Huang (2008), and Johnson &amp; Ural (2010): • CoPar The depth (number of levels) of parallelism between adjacent conjuncts • CoParLen The difference in length between adjacent conjuncts • Edges The words or (part-of-speech) tags on the outside and inside edges of a given XP6 • NGrams Sub-parts of a given rule production • NGramTree An n-gram of</context>
<context position="10043" citStr="Huang, 2008" startWordPosition="1624" endWordPosition="1625">wed that chart-based dependency parsing, based on Eisner’s (1996) algorithm, could be successfully approached in a discriminative framework. In this earliest work, each feature function could only refer to a single, bigram head-modifier relationship, e.g., Modifier, below. Subsequent work (McDonald &amp; Pereira, 2006; Carreras, 2007; Koo &amp; Collins, 2010) looked at allowing features to access more complex, higher-order relationships, including trigram and 4-gram relationships, e.g., all features apart from Modifier, below. With the ability to incorporate non-local phrase-structure parse features (Huang, 2008), we can recognize dependency features of arbitrary order (cf. Zhang &amp; McDonald (2012)). Our dependency feature set, which we call deps, contains: • Modifier head and modifier 6The tags outside of a given XP are approximated using the marginally most likely tags given the parse. • Sibling head, modifier m, and m’s nearest inner sibling • Grandchild head, modifier m, and one of m’s modifiers • Sibling+Grandchild head, modifier m, m’s nearest inner sibling, and one of m’s modifiers • Grandchild+Grandsibling head, modifier m, one of m’s modifiers g, and g’s inner sibling These features are insens</context>
<context position="15771" citStr="Huang, 2008" startWordPosition="2585" endWordPosition="2586">nd that phrase+deps is significantly better than either of its component parts on all metrics. But, phrase+deps+gen is significantly better than phrase+gen only on F1, but not on UAS or LAS. However, on the out-of-domain BROWN tests, we find that adding deps always adds considerably, and in a statistically significant way, to both LAS and UAS. That is, not only is phrase+deps better at dependency recovery than its component parts, but phrase+deps+gen is also considerably bet613 ter on dependency recovery than phrase+gen, which represents the previous state-of-the-art in this vein of research (Huang, 2008). This result is perhaps counter-intuitive, in the sense that one might have supposed that higher-order dependency features, being highly specific by nature, might only have only served to over-fit the training material. However, this result shows otherwise. Note that the dependency features include various levels of lexicalization. It might be that the more unlexicalized features capture something about the structure of correct parses, that transfers well out-ofdomain. Future work should investigate this. And, it of course remains to be seen how this result will transfer to other train-test d</context>
<context position="17429" citStr="Huang (2008)" startWordPosition="2849" endWordPosition="2850">pected to do very well, if the best accuracy is necessary (e.g., in a semi-supervised training strategy), it still seems to pay to use the generativediscriminative model combination. Note that the LAS scores of our models without gen are relatively weak. This is presumably largely because our dependency features are, at present, not sensitive to arc labels, so our results probably underestimate the capability of our general framework with respect to labelled dependency recovery. Table 3 compares our work with Huang’s (2008). Note that our model phrase+gen uses essentially the same features as Huang (2008), so the fact that our phrase+gen is noticeably more accurate on F1 is presumably due to the benefits in reduced feature under-training achieved by the MERT combination strategy. Also, our phrase+deps model is as accurate as Huang’s, without even using the generative model score feature. Table 4 compares our work to McClosky et al.’s (2006) domain adaptation work with the Charniak &amp; Johnson (2005) parser. Their three models shown have been trained on: i) the WSJ (supervised, out-of-domain), ii) the WSJ plus 2.5 million sentences of automatically labelled NANC newswire text (semi-supervised, ou</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Huang, L. (2008). Forest reranking: Discriminative parsing with non-local features. In ACL, 586–594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>D Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="4502" citStr="Huang &amp; Chiang, 2007" startWordPosition="690" endWordPosition="694">d to know exactly what the generative model is contributing to the accuracy of a generative-discriminative model combination. Thus, we conduct experiments in sets: in some cases the generative model score is used, and in others it is not used. Compared to the faster and more psychologically plausible shift-reduce parsers (Zhang &amp; Nivre, 2011; Zhang &amp; Clark, 2011), cube decoding is a computationally expensive method. But, cube decoding provides a relatively exact environment with which to compare different feature sets, has close connections with modern phrasebased machine translation methods (Huang &amp; Chiang, 2007), and produces very accurate parsers. In some cases, one might want to use a slower, but more accurate, parser during the training stage of a semi-supervised parser training strategy. For example, Petrov et al. (2010) have shown that a fast parser (Nivre et al., 2007) can be profitably trained from the output of a slower but more accurate one (Petrov et al., 2006), in a strategy they call uptraining. We make the source code for these experiments available.2 2 Phrase-Structure Parsing with Non-Local Features 2.1 Non-Local Features To decode using exact dynamic programming (i.e., CKY), one must </context>
<context position="8075" citStr="Huang &amp; Chiang, 2007" startWordPosition="1299" endWordPosition="1302"> prune the search space.5 As a by-product of this pruning procedure, we are able to use the model score of the first-stage parser as a feature in our ultimate model at no additional cost. As a first-stage parser, we use Huang et al.’s (2010) implementation of the LA-PCFG parser of Petrov et al. (2006), which uses a generative, latent-variable model. 3 Features 3.1 Phrase-Structure Features Our phrase-structure feature set is taken from Collins (2000), Charniak &amp; Johnson (2005), and 4This algorithm is closely related to the algorithm for phrase-based machine translation using a language model (Huang &amp; Chiang, 2007). 5All work in this paradigm has used a generative parser as the first-stage parser. But, this is arguably a historical accident. We could just as well use a discriminative parser with only local features, like Petrov &amp; Klein (2007a). 611 Huang (2008). Some features are omitted, with choices made based on the ablation studies of Johnson &amp; Ural (2010). This feature set, which we call phrase, contains the following, mostly nonlocal, features, which are described and depicted in Charniak &amp; Johnson (2005), Huang (2008), and Johnson &amp; Ural (2010): • CoPar The depth (number of levels) of parallelism</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Huang, L., &amp; Chiang, D. (2007). Forest rescoring: Faster decoding with integrated language models. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Huang</author>
<author>M Harper</author>
<author>S Petrov</author>
</authors>
<title>Selftraining with products of latent variable grammars.</title>
<date>2010</date>
<booktitle>In EMNLP,</booktitle>
<pages>12--22</pages>
<marker>Huang, Harper, Petrov, 2010</marker>
<rawString>Huang, Z., Harper, M., &amp; Petrov, S. (2010). Selftraining with products of latent variable grammars. In EMNLP, 12–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Johansson</author>
<author>P Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for English.</title>
<date>2007</date>
<booktitle>In Proc. of the 16th Nordic Conference on Computational Linguistics (NODALIDA),</booktitle>
<pages>105--112</pages>
<contexts>
<context position="14086" citStr="Johansson &amp; Nugues (2007)" startWordPosition="2294" endWordPosition="2298">in test set), and BROWN9 (out-ofdomain test set) from the Penn Treebank (Marcus et al., 1993).7 We evaluate using harmonic mean between labelled bracket recall and precision (EVALB F1), unlabelled dependency accuracy (UAS), and labelled dependency accuracy (LAS). Dependencies are extracted from full output trees using the algorithm of de Marneffe &amp; Manning (2008). We chose this dependency extractor, firstly, because it is natively meant to be run on the output of phrase-structure parsers, rather than on gold trees with function tags and traces still present, as is, e.g., the Penn-Converter of Johansson &amp; Nugues (2007). Also, this is the extractor that was used in a recent shared task (Petrov &amp; McDonald, 2012). We use EVALB and eval.pl to calculate scores. For hypothesis testing, we used the paired bootstrap test recently empirically evaluated in the context of NLP by Berg-Kirkpatrick et al. (2012). This 7Following Gildea (2001), the BROWN test set is usually divided into 10 parts. If we start indexing at 0, then the last (test) section has index 9. We received the BROWN data splits from David McClosky, p.c. involves drawing b subsamples of size n with replacement from the test set in question, and checking</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Johansson, R., &amp; Nugues, P. (2007). Extended constituent-to-dependency conversion for English. In Proc. of the 16th Nordic Conference on Computational Linguistics (NODALIDA), 105– 112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>A E Ural</author>
</authors>
<title>Reranking the Berkeley and Brown parsers.</title>
<date>2010</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>665--668</pages>
<contexts>
<context position="8427" citStr="Johnson &amp; Ural (2010)" startWordPosition="1359" endWordPosition="1362">3 Features 3.1 Phrase-Structure Features Our phrase-structure feature set is taken from Collins (2000), Charniak &amp; Johnson (2005), and 4This algorithm is closely related to the algorithm for phrase-based machine translation using a language model (Huang &amp; Chiang, 2007). 5All work in this paradigm has used a generative parser as the first-stage parser. But, this is arguably a historical accident. We could just as well use a discriminative parser with only local features, like Petrov &amp; Klein (2007a). 611 Huang (2008). Some features are omitted, with choices made based on the ablation studies of Johnson &amp; Ural (2010). This feature set, which we call phrase, contains the following, mostly nonlocal, features, which are described and depicted in Charniak &amp; Johnson (2005), Huang (2008), and Johnson &amp; Ural (2010): • CoPar The depth (number of levels) of parallelism between adjacent conjuncts • CoParLen The difference in length between adjacent conjuncts • Edges The words or (part-of-speech) tags on the outside and inside edges of a given XP6 • NGrams Sub-parts of a given rule production • NGramTree An n-gram of the input sentence, or the tags, along with the minimal tree containing that n-gram • HeadTree A sub</context>
</contexts>
<marker>Johnson, Ural, 2010</marker>
<rawString>Johnson, M., &amp; Ural, A. E. (2010). Reranking the Berkeley and Brown parsers. In HLT-NAACL, 665–668.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>M Collins</author>
</authors>
<title>Efficient thirdorder dependency parsers.</title>
<date>2010</date>
<booktitle>In ACL,</booktitle>
<pages>1--11</pages>
<contexts>
<context position="1081" citStr="Koo &amp; Collins, 2010" startWordPosition="148" endWordPosition="151"> of standard metrics. What is especially interesting is that we find strong, statistically significant gains on dependency recovery on out-of-domain tests (Brown vs. WSJ). This suggests that higher-order dependency features are not simply overfitting the training material. 1 Introduction Higher-order dependency features encode more complex sub-parts of a dependency tree structure than first-order, bigram head-modifier relationships.1 The clear trend in dependency parsing has been that the addition of such higher-order features improves parse accuracy (McDonald &amp; Pereira, 2006; Carreras, 2007; Koo &amp; Collins, 2010; Zhang &amp; Nivre, 2011; Zhang &amp; McDonald, 2012). This finding suggests that the same benefits might be observed in phrase-structure parsing. But, this is not necessarily implied. Phrasestructure parsers are generally stronger than dependency parsers (Petrov et al., 2010; Petrov &amp; McDonald, 2012), and make use of more kinds of information. So, it might be that the information modelled by higher-order dependency features adds less of a benefit in the phrase-structure case. 1Examples of first-order and higher-order dependency features are given in §3.2. To investigate this issue, we experiment usi</context>
<context position="9784" citStr="Koo &amp; Collins, 2010" startWordPosition="1587" endWordPosition="1590">bigrams • Rule A single rule production • Tag The tag of a given word • Word The tag of and first XP above a word • WProj The tag of and maximal projection of a word Heads is a first-order dependency feature. 3.2 Dependency Parsing Features McDonald et al. (2005) showed that chart-based dependency parsing, based on Eisner’s (1996) algorithm, could be successfully approached in a discriminative framework. In this earliest work, each feature function could only refer to a single, bigram head-modifier relationship, e.g., Modifier, below. Subsequent work (McDonald &amp; Pereira, 2006; Carreras, 2007; Koo &amp; Collins, 2010) looked at allowing features to access more complex, higher-order relationships, including trigram and 4-gram relationships, e.g., all features apart from Modifier, below. With the ability to incorporate non-local phrase-structure parse features (Huang, 2008), we can recognize dependency features of arbitrary order (cf. Zhang &amp; McDonald (2012)). Our dependency feature set, which we call deps, contains: • Modifier head and modifier 6The tags outside of a given XP are approximated using the marginally most likely tags given the parse. • Sibling head, modifier m, and m’s nearest inner sibling • G</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Koo, T., &amp; Collins, M. (2010). Efficient thirdorder dependency parsers. In ACL, 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>313--330</pages>
<contexts>
<context position="13554" citStr="Marcus et al., 1993" startWordPosition="2212" endWordPosition="2216"> Lesser F1 UAS LAS F1 UAS LAS phrase+deps phrase .042 .029 .018 .140 .022 .009 phrase+deps deps — &lt;.001 — — .012 — phrase+gen phrase .013 .003 &lt;.001 .016 .090 &lt;.001 phrase+deps+gen phrase+gen .030 .122 .151 .059 .008 .020 phrase+deps+gen phrase+deps .019 .020 &lt;.001 .008 .040 &lt;.001 Table 2: Results of statistical significance evaluations of hypotheses of the form X’s accuracy is greater than Y’s on the various test sets and metrics. Bold face indicates p &lt; .05. optimization, as is standard. The test sets are WSJ23 (in-domain test set), and BROWN9 (out-ofdomain test set) from the Penn Treebank (Marcus et al., 1993).7 We evaluate using harmonic mean between labelled bracket recall and precision (EVALB F1), unlabelled dependency accuracy (UAS), and labelled dependency accuracy (LAS). Dependencies are extracted from full output trees using the algorithm of de Marneffe &amp; Manning (2008). We chose this dependency extractor, firstly, because it is natively meant to be run on the output of phrase-structure parsers, rather than on gold trees with function tags and traces still present, as is, e.g., the Penn-Converter of Johansson &amp; Nugues (2007). Also, this is the extractor that was used in a recent shared task </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus, M. P., Santorini, B., &amp; Marcinkiewicz, M. A. (1993). Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2), 313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-C de Marneffe</author>
<author>C D Manning</author>
</authors>
<title>The Stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation,</booktitle>
<volume>1</volume>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>de Marneffe, M.-C., &amp; Manning, C. D. (2008). The Stanford typed dependencies representation. In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F Martins</author>
<author>K Gimpel</author>
<author>N A Smith</author>
<author>E P Xing</author>
<author>M A Figueiredo</author>
<author>P M Aguiar</author>
</authors>
<title>Learning structured classifiers with dual coordinate ascent.</title>
<date>2010</date>
<tech>Technical report, DTIC Document.</tech>
<contexts>
<context position="11391" citStr="Martins et al., 2010" startWordPosition="1849" endWordPosition="1852">d less lexicalized versions. 3.3 Generative Model Score Feature Finally, we have a feature set, gen, containing only one feature function. This feature maps a parse to the logarithm of the MAX-RULEPRODUCT score of that parse according to the LAPCFG parsing model, which is trained separately. This score has the character of a conditional likelihood for the parse (see Petrov &amp; Klein (2007b)). 4 Training We have two feature sets phrase and deps, for which we fix weights using parallel stochastic optimization of a structured SVM objective (Collins, 2002; Taskar et al., 2004; Crammer et al., 2006; Martins et al., 2010; McDonald et al., 2010). To the single feature in the set gen (i.e. the generative model score), we give the weight 1. The combined models, phrase+deps, phrase+gen, and phrase+deps+gen, are then model combinations of the first three. The combination weights for these combinations are obtained using Och’s (2003) Minimum Error-Rate Training (MERT). The MERT stage helps to avoid feature undertraining (Sutton et al., 2005), and avoids the problem of scaling involved in a model that contains mostly boolean features, but one, real-valued, logscale feature. Training is conducted in three stages (SVM</context>
</contexts>
<marker>Martins, Gimpel, Smith, Xing, Figueiredo, Aguiar, 2010</marker>
<rawString>Martins, A. F., Gimpel, K., Smith, N. A., Xing, E. P., Figueiredo, M. A., &amp; Aguiar, P. M. (2010). Learning structured classifiers with dual coordinate ascent. Technical report, DTIC Document.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McClosky</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Reranking and self-training for parser adaptation.</title>
<date>2006</date>
<booktitle>In ACL,</booktitle>
<pages>337--344</pages>
<contexts>
<context position="18581" citStr="McClosky et al. (2006)" startWordPosition="3036" endWordPosition="3039">nces of automatically labelled NANC newswire text (semi-supervised, out-of-domain), and iii) the BROWN corpus (supervised, in-domain). We test Type Model WSJ G+D Huang (2008) 91.7 D phrase+deps 91.7 G+D phrase+gen 92.1 G+D phrase+deps+gen 92.4 Table 3: Comparison of constituency parsing results in the cube decoding framework, on the WSJ test set. On G+D, D, see Table 1. Parser Training Data BROWN F1 CJ WSJ 85.2 CJ WSJ+NANC 87.8 CJ BROWN 88.4 Our Best WSJ 87.4 Table 4: Comparison of our best model, phrase+deps+gen, on BROWN, with the Charniak &amp; Johnson (2005) parser, denoted CJ, as reported in McClosky et al. (2006). Underline indicates best trained on WSJ, bold face indicates best overall. on BROWN. We see that our best (WSJ-trained) model is over 2% more accurate (absolute F1 difference) than the Charniak &amp; Johnson (2005) parser trained on the same data. In fact, our best model is nearly as good as McClosky et al.’s (2006) self-trained, semi-supervised model. Of course, the self-training strategy is orthogonal to the improvements we have made. 6 Conclusion We have shown that the addition of higher-order dependency features into a cube decoding phasestructure parser leads to statistically significant ga</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>McClosky, D., Charniak, E., &amp; Johnson, M. (2006). Reranking and self-training for parser adaptation. In ACL, 337–344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In EACL,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="1044" citStr="McDonald &amp; Pereira, 2006" startWordPosition="142" endWordPosition="145">onsiderable gains in accuracy on the range of standard metrics. What is especially interesting is that we find strong, statistically significant gains on dependency recovery on out-of-domain tests (Brown vs. WSJ). This suggests that higher-order dependency features are not simply overfitting the training material. 1 Introduction Higher-order dependency features encode more complex sub-parts of a dependency tree structure than first-order, bigram head-modifier relationships.1 The clear trend in dependency parsing has been that the addition of such higher-order features improves parse accuracy (McDonald &amp; Pereira, 2006; Carreras, 2007; Koo &amp; Collins, 2010; Zhang &amp; Nivre, 2011; Zhang &amp; McDonald, 2012). This finding suggests that the same benefits might be observed in phrase-structure parsing. But, this is not necessarily implied. Phrasestructure parsers are generally stronger than dependency parsers (Petrov et al., 2010; Petrov &amp; McDonald, 2012), and make use of more kinds of information. So, it might be that the information modelled by higher-order dependency features adds less of a benefit in the phrase-structure case. 1Examples of first-order and higher-order dependency features are given in §3.2. To inve</context>
<context position="9746" citStr="McDonald &amp; Pereira, 2006" startWordPosition="1581" endWordPosition="1584"> nodes in that path • Heads Head-modifier bigrams • Rule A single rule production • Tag The tag of a given word • Word The tag of and first XP above a word • WProj The tag of and maximal projection of a word Heads is a first-order dependency feature. 3.2 Dependency Parsing Features McDonald et al. (2005) showed that chart-based dependency parsing, based on Eisner’s (1996) algorithm, could be successfully approached in a discriminative framework. In this earliest work, each feature function could only refer to a single, bigram head-modifier relationship, e.g., Modifier, below. Subsequent work (McDonald &amp; Pereira, 2006; Carreras, 2007; Koo &amp; Collins, 2010) looked at allowing features to access more complex, higher-order relationships, including trigram and 4-gram relationships, e.g., all features apart from Modifier, below. With the ability to incorporate non-local phrase-structure parse features (Huang, 2008), we can recognize dependency features of arbitrary order (cf. Zhang &amp; McDonald (2012)). Our dependency feature set, which we call deps, contains: • Modifier head and modifier 6The tags outside of a given XP are approximated using the marginally most likely tags given the parse. • Sibling head, modifie</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>McDonald, R., &amp; Pereira, F. (2006). Online learning of approximate dependency parsing algorithms. In EACL, 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R T McDonald</author>
<author>K Crammer</author>
<author>F C N Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In ACL,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="9427" citStr="McDonald et al. (2005)" startWordPosition="1535" endWordPosition="1538">ags on the outside and inside edges of a given XP6 • NGrams Sub-parts of a given rule production • NGramTree An n-gram of the input sentence, or the tags, along with the minimal tree containing that n-gram • HeadTree A sub-tree containing the path from a word to its maximal projection, along with all siblings of all nodes in that path • Heads Head-modifier bigrams • Rule A single rule production • Tag The tag of a given word • Word The tag of and first XP above a word • WProj The tag of and maximal projection of a word Heads is a first-order dependency feature. 3.2 Dependency Parsing Features McDonald et al. (2005) showed that chart-based dependency parsing, based on Eisner’s (1996) algorithm, could be successfully approached in a discriminative framework. In this earliest work, each feature function could only refer to a single, bigram head-modifier relationship, e.g., Modifier, below. Subsequent work (McDonald &amp; Pereira, 2006; Carreras, 2007; Koo &amp; Collins, 2010) looked at allowing features to access more complex, higher-order relationships, including trigram and 4-gram relationships, e.g., all features apart from Modifier, below. With the ability to incorporate non-local phrase-structure parse featur</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>McDonald, R. T., Crammer, K., &amp; Pereira, F. C. N. (2005). Online large-margin training of dependency parsers. In ACL, 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R T McDonald</author>
<author>K Hall</author>
<author>G Mann</author>
</authors>
<title>Distributed training strategies for the structured perceptron.</title>
<date>2010</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>456--464</pages>
<contexts>
<context position="11415" citStr="McDonald et al., 2010" startWordPosition="1853" endWordPosition="1856">sions. 3.3 Generative Model Score Feature Finally, we have a feature set, gen, containing only one feature function. This feature maps a parse to the logarithm of the MAX-RULEPRODUCT score of that parse according to the LAPCFG parsing model, which is trained separately. This score has the character of a conditional likelihood for the parse (see Petrov &amp; Klein (2007b)). 4 Training We have two feature sets phrase and deps, for which we fix weights using parallel stochastic optimization of a structured SVM objective (Collins, 2002; Taskar et al., 2004; Crammer et al., 2006; Martins et al., 2010; McDonald et al., 2010). To the single feature in the set gen (i.e. the generative model score), we give the weight 1. The combined models, phrase+deps, phrase+gen, and phrase+deps+gen, are then model combinations of the first three. The combination weights for these combinations are obtained using Och’s (2003) Minimum Error-Rate Training (MERT). The MERT stage helps to avoid feature undertraining (Sutton et al., 2005), and avoids the problem of scaling involved in a model that contains mostly boolean features, but one, real-valued, logscale feature. Training is conducted in three stages (SVM, MERT, SVM), so that th</context>
</contexts>
<marker>McDonald, Hall, Mann, 2010</marker>
<rawString>McDonald, R. T., Hall, K., &amp; Mann, G. (2010). Distributed training strategies for the structured perceptron. In HLT-NAACL, 456–464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
<author>A Chanev</author>
<author>G Eryigit</author>
<author>S K¨ubler</author>
<author>S Marinov</author>
<author>E Marsi</author>
</authors>
<title>Maltparser: A language-independent system for data-driven dependency parsing.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<pages>95--135</pages>
<marker>Nivre, Hall, Nilsson, Chanev, Eryigit, K¨ubler, Marinov, Marsi, 2007</marker>
<rawString>Nivre, J., Hall, J., Nilsson, J., Chanev, A., Eryigit, G., K¨ubler, S., Marinov, S., &amp; Marsi, E. (2007). Maltparser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13(2), 95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL, 160–</booktitle>
<pages>167</pages>
<marker>Och, 2003</marker>
<rawString>Och, F. J. (2003). Minimum error rate training in statistical machine translation. In ACL, 160– 167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>L Barrett</author>
<author>R Thibaux</author>
<author>D Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In ACL,</booktitle>
<pages>433--440</pages>
<contexts>
<context position="3183" citStr="Petrov et al. (2006)" startWordPosition="481" endWordPosition="484">d unlabelled, of our phrase-structure parsers on outof-domain tests (which means, here, trained on WSJ, but tested on BROWN). In fact, the gains are even stronger on out-of-domain tests than on indomain tests. One might have thought that higherorder dependencies, being rather specific by nature, would tend to pick out only very rare events, and so only serve to over-fit the training material, but this is not what we find. We speculate as to what this might mean in §5.2. The cube decoding paradigm requires a firststage parser to prune the output space. For this, we use the generative parser of Petrov et al. (2006). We can use this parser’s model score as a feature in our discriminative model at no additional cost. However, doing so conflates the contribution to accuracy of the generative model, on the one hand, and the discriminatively trained, hand610 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 610–616, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics written, features, on the other. Future systems might use the same or a similar feature set to ours, but in an architecture that does not include any generative parser. </context>
<context position="4868" citStr="Petrov et al., 2006" startWordPosition="755" endWordPosition="758">cube decoding is a computationally expensive method. But, cube decoding provides a relatively exact environment with which to compare different feature sets, has close connections with modern phrasebased machine translation methods (Huang &amp; Chiang, 2007), and produces very accurate parsers. In some cases, one might want to use a slower, but more accurate, parser during the training stage of a semi-supervised parser training strategy. For example, Petrov et al. (2010) have shown that a fast parser (Nivre et al., 2007) can be profitably trained from the output of a slower but more accurate one (Petrov et al., 2006), in a strategy they call uptraining. We make the source code for these experiments available.2 2 Phrase-Structure Parsing with Non-Local Features 2.1 Non-Local Features To decode using exact dynamic programming (i.e., CKY), one must restrict oneself to the use of only local features. Local features are those that factor according to the individual rule productions of the parse. For example, a feature indicating the presence of the rule S → NP VP is local.3 But, a feature that indicates that the head word of this S is, e.g., joined, is non-local, because the head word of a phrase cannot be det</context>
<context position="7756" citStr="Petrov et al. (2006)" startWordPosition="1254" endWordPosition="1257">ing, we visit each node n in the same bottomup order we would use for Viterbi decoding, and compute a list of the top k parses to n, according to a global linear model (Collins, 2002), using the trees that have survived the beam at earlier nodes. 2.3 The First-Stage Parser As noted, we require a first-stage parser to prune the search space.5 As a by-product of this pruning procedure, we are able to use the model score of the first-stage parser as a feature in our ultimate model at no additional cost. As a first-stage parser, we use Huang et al.’s (2010) implementation of the LA-PCFG parser of Petrov et al. (2006), which uses a generative, latent-variable model. 3 Features 3.1 Phrase-Structure Features Our phrase-structure feature set is taken from Collins (2000), Charniak &amp; Johnson (2005), and 4This algorithm is closely related to the algorithm for phrase-based machine translation using a language model (Huang &amp; Chiang, 2007). 5All work in this paradigm has used a generative parser as the first-stage parser. But, this is arguably a historical accident. We could just as well use a discriminative parser with only local features, like Petrov &amp; Klein (2007a). 611 Huang (2008). Some features are omitted, w</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Petrov, S., Barrett, L., Thibaux, R., &amp; Klein, D. (2006). Learning accurate, compact, and interpretable tree annotation. In ACL, 433–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>P-C Chang</author>
<author>M Ringgaard</author>
<author>H Alshawi</author>
</authors>
<title>Uptraining for accurate deterministic question parsing.</title>
<date>2010</date>
<booktitle>In EMNLP,</booktitle>
<pages>705--713</pages>
<contexts>
<context position="1350" citStr="Petrov et al., 2010" startWordPosition="191" endWordPosition="194">ial. 1 Introduction Higher-order dependency features encode more complex sub-parts of a dependency tree structure than first-order, bigram head-modifier relationships.1 The clear trend in dependency parsing has been that the addition of such higher-order features improves parse accuracy (McDonald &amp; Pereira, 2006; Carreras, 2007; Koo &amp; Collins, 2010; Zhang &amp; Nivre, 2011; Zhang &amp; McDonald, 2012). This finding suggests that the same benefits might be observed in phrase-structure parsing. But, this is not necessarily implied. Phrasestructure parsers are generally stronger than dependency parsers (Petrov et al., 2010; Petrov &amp; McDonald, 2012), and make use of more kinds of information. So, it might be that the information modelled by higher-order dependency features adds less of a benefit in the phrase-structure case. 1Examples of first-order and higher-order dependency features are given in §3.2. To investigate this issue, we experiment using Huang’s (2008) cube decoding algorithm. This algorithm allows structured prediction with nonlocal features, as discussed in §2. Collins’s (1997) strategy of expanding the phrase-structure parser’s dynamic program to incorporate head-modifier dependency information w</context>
<context position="4719" citStr="Petrov et al. (2010)" startWordPosition="727" endWordPosition="730">in others it is not used. Compared to the faster and more psychologically plausible shift-reduce parsers (Zhang &amp; Nivre, 2011; Zhang &amp; Clark, 2011), cube decoding is a computationally expensive method. But, cube decoding provides a relatively exact environment with which to compare different feature sets, has close connections with modern phrasebased machine translation methods (Huang &amp; Chiang, 2007), and produces very accurate parsers. In some cases, one might want to use a slower, but more accurate, parser during the training stage of a semi-supervised parser training strategy. For example, Petrov et al. (2010) have shown that a fast parser (Nivre et al., 2007) can be profitably trained from the output of a slower but more accurate one (Petrov et al., 2006), in a strategy they call uptraining. We make the source code for these experiments available.2 2 Phrase-Structure Parsing with Non-Local Features 2.1 Non-Local Features To decode using exact dynamic programming (i.e., CKY), one must restrict oneself to the use of only local features. Local features are those that factor according to the individual rule productions of the parse. For example, a feature indicating the presence of the rule S → NP VP </context>
</contexts>
<marker>Petrov, Chang, Ringgaard, Alshawi, 2010</marker>
<rawString>Petrov, S., Chang, P.-C., Ringgaard, M., &amp; Alshawi, H. (2010). Uptraining for accurate deterministic question parsing. In EMNLP, 705–713.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Klein</author>
</authors>
<title>Discriminative log-linear grammars with latent variables.</title>
<date>2007</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="8306" citStr="Petrov &amp; Klein (2007" startWordPosition="1339" endWordPosition="1342">’s (2010) implementation of the LA-PCFG parser of Petrov et al. (2006), which uses a generative, latent-variable model. 3 Features 3.1 Phrase-Structure Features Our phrase-structure feature set is taken from Collins (2000), Charniak &amp; Johnson (2005), and 4This algorithm is closely related to the algorithm for phrase-based machine translation using a language model (Huang &amp; Chiang, 2007). 5All work in this paradigm has used a generative parser as the first-stage parser. But, this is arguably a historical accident. We could just as well use a discriminative parser with only local features, like Petrov &amp; Klein (2007a). 611 Huang (2008). Some features are omitted, with choices made based on the ablation studies of Johnson &amp; Ural (2010). This feature set, which we call phrase, contains the following, mostly nonlocal, features, which are described and depicted in Charniak &amp; Johnson (2005), Huang (2008), and Johnson &amp; Ural (2010): • CoPar The depth (number of levels) of parallelism between adjacent conjuncts • CoParLen The difference in length between adjacent conjuncts • Edges The words or (part-of-speech) tags on the outside and inside edges of a given XP6 • NGrams Sub-parts of a given rule production • NG</context>
<context position="11160" citStr="Petrov &amp; Klein (2007" startWordPosition="1810" endWordPosition="1813">+Grandsibling head, modifier m, one of m’s modifiers g, and g’s inner sibling These features are insensitive to arc labels in the present experiments, but future work will incorporate arc labels. Each feature class contains more and less lexicalized versions. 3.3 Generative Model Score Feature Finally, we have a feature set, gen, containing only one feature function. This feature maps a parse to the logarithm of the MAX-RULEPRODUCT score of that parse according to the LAPCFG parsing model, which is trained separately. This score has the character of a conditional likelihood for the parse (see Petrov &amp; Klein (2007b)). 4 Training We have two feature sets phrase and deps, for which we fix weights using parallel stochastic optimization of a structured SVM objective (Collins, 2002; Taskar et al., 2004; Crammer et al., 2006; Martins et al., 2010; McDonald et al., 2010). To the single feature in the set gen (i.e. the generative model score), we give the weight 1. The combined models, phrase+deps, phrase+gen, and phrase+deps+gen, are then model combinations of the first three. The combination weights for these combinations are obtained using Och’s (2003) Minimum Error-Rate Training (MERT). The MERT stage help</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Petrov, S., &amp; Klein, D. (2007a). Discriminative log-linear grammars with latent variables. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="8306" citStr="Petrov &amp; Klein (2007" startWordPosition="1339" endWordPosition="1342">’s (2010) implementation of the LA-PCFG parser of Petrov et al. (2006), which uses a generative, latent-variable model. 3 Features 3.1 Phrase-Structure Features Our phrase-structure feature set is taken from Collins (2000), Charniak &amp; Johnson (2005), and 4This algorithm is closely related to the algorithm for phrase-based machine translation using a language model (Huang &amp; Chiang, 2007). 5All work in this paradigm has used a generative parser as the first-stage parser. But, this is arguably a historical accident. We could just as well use a discriminative parser with only local features, like Petrov &amp; Klein (2007a). 611 Huang (2008). Some features are omitted, with choices made based on the ablation studies of Johnson &amp; Ural (2010). This feature set, which we call phrase, contains the following, mostly nonlocal, features, which are described and depicted in Charniak &amp; Johnson (2005), Huang (2008), and Johnson &amp; Ural (2010): • CoPar The depth (number of levels) of parallelism between adjacent conjuncts • CoParLen The difference in length between adjacent conjuncts • Edges The words or (part-of-speech) tags on the outside and inside edges of a given XP6 • NGrams Sub-parts of a given rule production • NG</context>
<context position="11160" citStr="Petrov &amp; Klein (2007" startWordPosition="1810" endWordPosition="1813">+Grandsibling head, modifier m, one of m’s modifiers g, and g’s inner sibling These features are insensitive to arc labels in the present experiments, but future work will incorporate arc labels. Each feature class contains more and less lexicalized versions. 3.3 Generative Model Score Feature Finally, we have a feature set, gen, containing only one feature function. This feature maps a parse to the logarithm of the MAX-RULEPRODUCT score of that parse according to the LAPCFG parsing model, which is trained separately. This score has the character of a conditional likelihood for the parse (see Petrov &amp; Klein (2007b)). 4 Training We have two feature sets phrase and deps, for which we fix weights using parallel stochastic optimization of a structured SVM objective (Collins, 2002; Taskar et al., 2004; Crammer et al., 2006; Martins et al., 2010; McDonald et al., 2010). To the single feature in the set gen (i.e. the generative model score), we give the weight 1. The combined models, phrase+deps, phrase+gen, and phrase+deps+gen, are then model combinations of the first three. The combination weights for these combinations are obtained using Och’s (2003) Minimum Error-Rate Training (MERT). The MERT stage help</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Petrov, S., &amp; Klein, D. (2007b). Improved inference for unlexicalized parsing. In HLT-NAACL, 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>R McDonald</author>
</authors>
<title>Overview of the 2012 shared task on parsing the web. Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL).</title>
<date>2012</date>
<contexts>
<context position="1376" citStr="Petrov &amp; McDonald, 2012" startWordPosition="195" endWordPosition="198">igher-order dependency features encode more complex sub-parts of a dependency tree structure than first-order, bigram head-modifier relationships.1 The clear trend in dependency parsing has been that the addition of such higher-order features improves parse accuracy (McDonald &amp; Pereira, 2006; Carreras, 2007; Koo &amp; Collins, 2010; Zhang &amp; Nivre, 2011; Zhang &amp; McDonald, 2012). This finding suggests that the same benefits might be observed in phrase-structure parsing. But, this is not necessarily implied. Phrasestructure parsers are generally stronger than dependency parsers (Petrov et al., 2010; Petrov &amp; McDonald, 2012), and make use of more kinds of information. So, it might be that the information modelled by higher-order dependency features adds less of a benefit in the phrase-structure case. 1Examples of first-order and higher-order dependency features are given in §3.2. To investigate this issue, we experiment using Huang’s (2008) cube decoding algorithm. This algorithm allows structured prediction with nonlocal features, as discussed in §2. Collins’s (1997) strategy of expanding the phrase-structure parser’s dynamic program to incorporate head-modifier dependency information would not scale to the comp</context>
<context position="14179" citStr="Petrov &amp; McDonald, 2012" startWordPosition="2312" endWordPosition="2315">7 We evaluate using harmonic mean between labelled bracket recall and precision (EVALB F1), unlabelled dependency accuracy (UAS), and labelled dependency accuracy (LAS). Dependencies are extracted from full output trees using the algorithm of de Marneffe &amp; Manning (2008). We chose this dependency extractor, firstly, because it is natively meant to be run on the output of phrase-structure parsers, rather than on gold trees with function tags and traces still present, as is, e.g., the Penn-Converter of Johansson &amp; Nugues (2007). Also, this is the extractor that was used in a recent shared task (Petrov &amp; McDonald, 2012). We use EVALB and eval.pl to calculate scores. For hypothesis testing, we used the paired bootstrap test recently empirically evaluated in the context of NLP by Berg-Kirkpatrick et al. (2012). This 7Following Gildea (2001), the BROWN test set is usually divided into 10 parts. If we start indexing at 0, then the last (test) section has index 9. We received the BROWN data splits from David McClosky, p.c. involves drawing b subsamples of size n with replacement from the test set in question, and checking relative performance of the models on the subsample (see the reference). We use b = 106 and </context>
</contexts>
<marker>Petrov, McDonald, 2012</marker>
<rawString>Petrov, S., &amp; McDonald, R. (2012). Overview of the 2012 shared task on parsing the web. Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sutton</author>
<author>M Sindelar</author>
<author>A McCallum</author>
</authors>
<title>Feature bagging: Preventing weight undertraining in structured discriminative learning.</title>
<date>2005</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="11814" citStr="Sutton et al., 2005" startWordPosition="1915" endWordPosition="1918">e sets phrase and deps, for which we fix weights using parallel stochastic optimization of a structured SVM objective (Collins, 2002; Taskar et al., 2004; Crammer et al., 2006; Martins et al., 2010; McDonald et al., 2010). To the single feature in the set gen (i.e. the generative model score), we give the weight 1. The combined models, phrase+deps, phrase+gen, and phrase+deps+gen, are then model combinations of the first three. The combination weights for these combinations are obtained using Och’s (2003) Minimum Error-Rate Training (MERT). The MERT stage helps to avoid feature undertraining (Sutton et al., 2005), and avoids the problem of scaling involved in a model that contains mostly boolean features, but one, real-valued, logscale feature. Training is conducted in three stages (SVM, MERT, SVM), so that there is no influence of any data outside the given training set (WSJ2-21) on the combination weights. 5 Experiments 5.1 Methods All models are trained on WSJ2-21, with WSJ22 used to pick the stopping iteration for online 612 Test Set WSJ BROWN Type Model F1 UAS LAS F1 UAS LAS G LA-PCFG 90.3 93.7 91.5 85.1 88.7 85.0 D phrase 91.2 93.9 91.0 86.1 89.4 85.1 deps — 93.3 — — 89.3 — phrase+deps 91.7 94.4</context>
</contexts>
<marker>Sutton, Sindelar, McCallum, 2005</marker>
<rawString>Sutton, C., Sindelar, M., &amp; McCallum, A. (2005). Feature bagging: Preventing weight undertraining in structured discriminative learning. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>D Klein</author>
<author>M Collins</author>
<author>D Koller</author>
<author>C D Manning</author>
</authors>
<title>Max-margin parsing.</title>
<date>2004</date>
<booktitle>In EMNLP,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="6191" citStr="Taskar et al., 2004" startWordPosition="979" endWordPosition="982">ly find the 2See http://gfcoppola.net/code.php. This software is available for free for non-profit research uses. 3A feature indicating that, e.g., the first word dominated by S is Pierre is also local, since the words of the sentence are constant across hypothesized parses, and words can be referred to by their position with respect to a given rule production. See Huang (2008) for more details. head phrase of each local rule production, until we reach a terminal node (or tag node). This recursion would not be allowed in standard CKY. Many discriminative parsers have used only local features (Taskar et al., 2004; Turian et al., 2007; Finkel et al., 2008). However, Huang (2008) shows that the use of non-local features does in fact contribute substantially to parser performance. And, our desire to make heavy use of head-word dependency relations necessitates the use of non-local features. 2.2 Cube Decoding While the use of non-local features destroys the ability to do exact search, we can still do inexact search using Huang’s (2008) cube decoding algorithm.4 A tractable first-stage parser prunes the space of possible parses, and outputs a forest, which is a set of rule production instances that can be </context>
<context position="11347" citStr="Taskar et al., 2004" startWordPosition="1841" endWordPosition="1844">labels. Each feature class contains more and less lexicalized versions. 3.3 Generative Model Score Feature Finally, we have a feature set, gen, containing only one feature function. This feature maps a parse to the logarithm of the MAX-RULEPRODUCT score of that parse according to the LAPCFG parsing model, which is trained separately. This score has the character of a conditional likelihood for the parse (see Petrov &amp; Klein (2007b)). 4 Training We have two feature sets phrase and deps, for which we fix weights using parallel stochastic optimization of a structured SVM objective (Collins, 2002; Taskar et al., 2004; Crammer et al., 2006; Martins et al., 2010; McDonald et al., 2010). To the single feature in the set gen (i.e. the generative model score), we give the weight 1. The combined models, phrase+deps, phrase+gen, and phrase+deps+gen, are then model combinations of the first three. The combination weights for these combinations are obtained using Och’s (2003) Minimum Error-Rate Training (MERT). The MERT stage helps to avoid feature undertraining (Sutton et al., 2005), and avoids the problem of scaling involved in a model that contains mostly boolean features, but one, real-valued, logscale feature</context>
</contexts>
<marker>Taskar, Klein, Collins, Koller, Manning, 2004</marker>
<rawString>Taskar, B., Klein, D., Collins, M., Koller, D., &amp; Manning, C. D. (2004). Max-margin parsing. In EMNLP, 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turian</author>
<author>B Wellington</author>
<author>I D Melamed</author>
</authors>
<title>Scalable discriminative learning for natural language parsing and translation.</title>
<date>2007</date>
<booktitle>In NIPS,</booktitle>
<pages>1409--1416</pages>
<contexts>
<context position="6212" citStr="Turian et al., 2007" startWordPosition="983" endWordPosition="986">://gfcoppola.net/code.php. This software is available for free for non-profit research uses. 3A feature indicating that, e.g., the first word dominated by S is Pierre is also local, since the words of the sentence are constant across hypothesized parses, and words can be referred to by their position with respect to a given rule production. See Huang (2008) for more details. head phrase of each local rule production, until we reach a terminal node (or tag node). This recursion would not be allowed in standard CKY. Many discriminative parsers have used only local features (Taskar et al., 2004; Turian et al., 2007; Finkel et al., 2008). However, Huang (2008) shows that the use of non-local features does in fact contribute substantially to parser performance. And, our desire to make heavy use of head-word dependency relations necessitates the use of non-local features. 2.2 Cube Decoding While the use of non-local features destroys the ability to do exact search, we can still do inexact search using Huang’s (2008) cube decoding algorithm.4 A tractable first-stage parser prunes the space of possible parses, and outputs a forest, which is a set of rule production instances that can be used to make a parse </context>
</contexts>
<marker>Turian, Wellington, Melamed, 2007</marker>
<rawString>Turian, J., Wellington, B., &amp; Melamed, I. D. (2007). Scalable discriminative learning for natural language parsing and translation. In NIPS, 1409–1416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhang</author>
<author>R McDonald</author>
</authors>
<title>Generalized higher-order dependency parsing with cube pruning.</title>
<date>2012</date>
<booktitle>In EMNLP,</booktitle>
<pages>238--242</pages>
<contexts>
<context position="1127" citStr="Zhang &amp; McDonald, 2012" startWordPosition="156" endWordPosition="159">nteresting is that we find strong, statistically significant gains on dependency recovery on out-of-domain tests (Brown vs. WSJ). This suggests that higher-order dependency features are not simply overfitting the training material. 1 Introduction Higher-order dependency features encode more complex sub-parts of a dependency tree structure than first-order, bigram head-modifier relationships.1 The clear trend in dependency parsing has been that the addition of such higher-order features improves parse accuracy (McDonald &amp; Pereira, 2006; Carreras, 2007; Koo &amp; Collins, 2010; Zhang &amp; Nivre, 2011; Zhang &amp; McDonald, 2012). This finding suggests that the same benefits might be observed in phrase-structure parsing. But, this is not necessarily implied. Phrasestructure parsers are generally stronger than dependency parsers (Petrov et al., 2010; Petrov &amp; McDonald, 2012), and make use of more kinds of information. So, it might be that the information modelled by higher-order dependency features adds less of a benefit in the phrase-structure case. 1Examples of first-order and higher-order dependency features are given in §3.2. To investigate this issue, we experiment using Huang’s (2008) cube decoding algorithm. Thi</context>
<context position="10129" citStr="Zhang &amp; McDonald (2012)" startWordPosition="1636" endWordPosition="1639">, could be successfully approached in a discriminative framework. In this earliest work, each feature function could only refer to a single, bigram head-modifier relationship, e.g., Modifier, below. Subsequent work (McDonald &amp; Pereira, 2006; Carreras, 2007; Koo &amp; Collins, 2010) looked at allowing features to access more complex, higher-order relationships, including trigram and 4-gram relationships, e.g., all features apart from Modifier, below. With the ability to incorporate non-local phrase-structure parse features (Huang, 2008), we can recognize dependency features of arbitrary order (cf. Zhang &amp; McDonald (2012)). Our dependency feature set, which we call deps, contains: • Modifier head and modifier 6The tags outside of a given XP are approximated using the marginally most likely tags given the parse. • Sibling head, modifier m, and m’s nearest inner sibling • Grandchild head, modifier m, and one of m’s modifiers • Sibling+Grandchild head, modifier m, m’s nearest inner sibling, and one of m’s modifiers • Grandchild+Grandsibling head, modifier m, one of m’s modifiers g, and g’s inner sibling These features are insensitive to arc labels in the present experiments, but future work will incorporate arc l</context>
</contexts>
<marker>Zhang, McDonald, 2012</marker>
<rawString>Zhang, H., &amp; McDonald, R. (2012). Generalized higher-order dependency parsing with cube pruning. In EMNLP, 238–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Clark</author>
</authors>
<title>Shift-reduce CCG parsing.</title>
<date>2011</date>
<booktitle>In ACL,</booktitle>
<pages>683--692</pages>
<contexts>
<context position="4246" citStr="Zhang &amp; Clark, 2011" startWordPosition="652" endWordPosition="655">tures, on the other. Future systems might use the same or a similar feature set to ours, but in an architecture that does not include any generative parser. On the other hand, some systems might indeed incorporate this generative model’s score. So, we need to know exactly what the generative model is contributing to the accuracy of a generative-discriminative model combination. Thus, we conduct experiments in sets: in some cases the generative model score is used, and in others it is not used. Compared to the faster and more psychologically plausible shift-reduce parsers (Zhang &amp; Nivre, 2011; Zhang &amp; Clark, 2011), cube decoding is a computationally expensive method. But, cube decoding provides a relatively exact environment with which to compare different feature sets, has close connections with modern phrasebased machine translation methods (Huang &amp; Chiang, 2007), and produces very accurate parsers. In some cases, one might want to use a slower, but more accurate, parser during the training stage of a semi-supervised parser training strategy. For example, Petrov et al. (2010) have shown that a fast parser (Nivre et al., 2007) can be profitably trained from the output of a slower but more accurate one</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Zhang, Y., &amp; Clark, S. (2011). Shift-reduce CCG parsing. In ACL, 683–692.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>J Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In ACL,</booktitle>
<pages>188--293</pages>
<contexts>
<context position="1102" citStr="Zhang &amp; Nivre, 2011" startWordPosition="152" endWordPosition="155"> What is especially interesting is that we find strong, statistically significant gains on dependency recovery on out-of-domain tests (Brown vs. WSJ). This suggests that higher-order dependency features are not simply overfitting the training material. 1 Introduction Higher-order dependency features encode more complex sub-parts of a dependency tree structure than first-order, bigram head-modifier relationships.1 The clear trend in dependency parsing has been that the addition of such higher-order features improves parse accuracy (McDonald &amp; Pereira, 2006; Carreras, 2007; Koo &amp; Collins, 2010; Zhang &amp; Nivre, 2011; Zhang &amp; McDonald, 2012). This finding suggests that the same benefits might be observed in phrase-structure parsing. But, this is not necessarily implied. Phrasestructure parsers are generally stronger than dependency parsers (Petrov et al., 2010; Petrov &amp; McDonald, 2012), and make use of more kinds of information. So, it might be that the information modelled by higher-order dependency features adds less of a benefit in the phrase-structure case. 1Examples of first-order and higher-order dependency features are given in §3.2. To investigate this issue, we experiment using Huang’s (2008) cub</context>
<context position="4224" citStr="Zhang &amp; Nivre, 2011" startWordPosition="648" endWordPosition="651">guistics written, features, on the other. Future systems might use the same or a similar feature set to ours, but in an architecture that does not include any generative parser. On the other hand, some systems might indeed incorporate this generative model’s score. So, we need to know exactly what the generative model is contributing to the accuracy of a generative-discriminative model combination. Thus, we conduct experiments in sets: in some cases the generative model score is used, and in others it is not used. Compared to the faster and more psychologically plausible shift-reduce parsers (Zhang &amp; Nivre, 2011; Zhang &amp; Clark, 2011), cube decoding is a computationally expensive method. But, cube decoding provides a relatively exact environment with which to compare different feature sets, has close connections with modern phrasebased machine translation methods (Huang &amp; Chiang, 2007), and produces very accurate parsers. In some cases, one might want to use a slower, but more accurate, parser during the training stage of a semi-supervised parser training strategy. For example, Petrov et al. (2010) have shown that a fast parser (Nivre et al., 2007) can be profitably trained from the output of a slower</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Zhang, Y., &amp; Nivre, J. (2011). Transition-based dependency parsing with rich non-local features. In ACL, 188–293.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>