<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000056">
<note confidence="0.630499666666667">
SENSEVAL-3: Third International Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, Barcelona, Spain, July 2004
Association for Computational Linguistics
</note>
<title confidence="0.536108">
Using a Word Sense Disambiguation system for translation
disambiguation: the LIA-LIDILEM team experiment
</title>
<author confidence="0.727821">
Gr6goire MOREAU DE MONTCHEUIL
Marc EL-BEZE
</author>
<affiliation confidence="0.718122">
Laboratoire informatique d&apos;Avignon, Universite d&apos;Avignon
</affiliation>
<email confidence="0.974847">
{moreau, elbeze}@lia.univ-avignon.fr
</email>
<author confidence="0.778133">
Boxing CHEN, Olivier KRAIF
</author>
<affiliation confidence="0.691499">
LIDILEM
</affiliation>
<address confidence="0.3973895">
Universite Stendhal Grenoble 3
{Chen, Kraif}@u-grenoble3.fr
</address>
<sectionHeader confidence="0.893651" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999778">
This paper presents an original WSD method,
based on a mixture of three algorithms working
on the local context of target units. The results on
the task    () of
Senseval 3 were 60.3% of precision and recall for
the T sub-task, and 64.1% for TS. We attempted
to improve the method by constituting synonym-
like classes from an English-French aligned cor-
pus, but without any gain in the results.
</bodyText>
<sectionHeader confidence="0.990175" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999828375">
The CARMEL Project aims at gathering literary
texts with translations in 4 languages (French,
English, Spanish and Italian), and implementing
Word Sense Disambiguation and Thematic Iden-
tification methods, taking advantage of the multi-
lingual context of each unit. In this preliminary
study, conducted in the framework of the MLS
task of Senseval 3, we try to address two related
issues. On the one hand, we would like to deter-
mine the results of a state-of-the-art supervised
WSD method applied to a translation-tagged cor-
pus, compared to a manually sense-tagged cor-
pus. On the other hand, we try to check whether
the results of the translation-disambiguation task
can be improved using a complementary re-
source, namely a bilingual parallel corpus, that
allows us to create synonym-like classes, in order
to make training contexts more general. Section 2
is dedicated to the WSD algorithm description.
Section 3 deals with the aligning techniques that
have been implemented to constitute a word-level
aligned corpus. A simple method for synonym-
like class extraction is described. Section 4 gives
the results and their discussion.
</bodyText>
<sectionHeader confidence="0.884854" genericHeader="method">
2 WSD Tool Description
</sectionHeader>
<bodyText confidence="0.9996288">
To assign senses or translation tags, we use
weighted mixtures of three algorithms: decision
trees, a probabilistic method and k-nearest-
neighbours. We first describe individually each
algorithm, and then our mixing methods.
</bodyText>
<subsectionHeader confidence="0.899059">
2.1 Semantic Classification Tree
</subsectionHeader>
<bodyText confidence="0.9977181">
De Mori and Kuhn (1995) have proposed to ap-
ply a Decision Tree approach to the sense disam-
biguation problem. The tree grown for each
target word is called Semantic Classification Tree
(SCT). In a first step, we construct the decision
tree with the training corpus. To each node is as-
sociated a question which leads to splitting the
population of examples into two sub-populations.
The choice of the question is done by maximiza-
tion of the entropy gain using Gini criterion:
</bodyText>
<equation confidence="0.9419844">
() 1  �
= − E
 

where ( )  
</equation>
<bodyText confidence="0.9812144">
  is the distribution of probabili-
ties of senses in the population .
For a population  split by question  into a par-
tition of two sub-populations  and , the en-
tropy gain is:
For our WSD system, node-questions are queries
of presence (or absence) of a single word in the
example context, independently of its position. If
several words maximize the entropy gain, a pref-
erence is given to the longest one, assuming that
it will bear more specification. In a second step,
the system uses the decision tree to find a path
from the root to a leaf: () where  is the
root,  is the leaf, and each node  has the prob-
abilities of senses ()s. Since the questions are
binary, unseen events may have very undesirable
effect on the path selection. Therefore, the path is
used to smooth the score calculation, as sug-
gested by Breiman   (1984). So, when the
path contains three or more nodes, the final score
</bodyText>
<equation confidence="0.77136675">
z
for each sense  is:   =E  + 
( ) , −  ,�
=o
</equation>
<bodyText confidence="0.996232">
and when the path length is two:
</bodyText>
<equation confidence="0.992479666666667">
    −��   (  ) +  ( ) � �
( )= ( ) �     �
()=&apos;0 ,1+&apos;1 ,0 .
</equation>
<bodyText confidence="0.990691">
Note that weights  and are positive, sum to
1, and have been determined empirically.
</bodyText>
<subsectionHeader confidence="0.997276">
2.2 Probabilistic Approach
</subsectionHeader>
<bodyText confidence="0.995760111111111">
As proposed by Bahl   (1988) for performing
a fast match in the framework of an automatic
speech recognition system, the problem of word
sense disambiguation can be seen as a polling
function. Each term found in the context of the
=( ) − +,
target word will vote for each
possible sense of the target word. We assume that
these terms have Poisson distributions
</bodyText>
<equation confidence="0.88840525">
 ,
  
( / ) = ! ,
currences of  in the current context.
</equation>
<bodyText confidence="0.973314">
After deriving some equations under the assump-
tion of the independence hypothesis, it is easy to
express the score of each sense  as

</bodyText>
<equation confidence="0.912956">
 ()  () .  ,

</equation>
<bodyText confidence="0.999848888888889">
 is estimated as the number of examples pro-
vided for this sense divided by the total number
of examples. For each word  (found in the train-
ing contexts), ws is estimated as the average of
occurrences of this word in the examples related
to sense .
Note that if a term  does not appear in the train-
ing instances for ,  has a default value,
smaller of all other , but not zero.
</bodyText>
<subsectionHeader confidence="0.983017">
2.3 K-Nearest Neighbours
</subsectionHeader>
<bodyText confidence="0.89170475">
The KNN algorithm is a dynamic disambiguation
method. Into the set of learning examples, the
system selects a subset of the  most similar with
the ambiguous example. If =( )−+ and
</bodyText>
<equation confidence="0.6813785">
 =   −  + are
&apos; ( &apos; ) &apos;   &apos;&apos; two contexts of an ambiguous
</equation>
<bodyText confidence="0.9983715">
lemma (where  and  are the ambiguous
word), the similarity between  and  is the
square of the number of identical words at the
same relative position:
</bodyText>
<equation confidence="0.78528625">
+ l2
(,&apos;)4E(_1&apos;)J,where =() and
 
=−
</equation>
<bodyText confidence="0.999643307692308">
(). The motivation for adding a square
to the simple similarity measure was to enlarge
the domain variation of the scores obtained by
the different senses, to make it comparable to the
one estimated through the SCT scheme.
To cope with the problem of similarity score tie
(more than  examples have a similarity greater
or equal to this of the th most similar example),
the constraint ( in our case) is relaxed so that
the vote relies on more than  neighbours.
In all cases, each neighbour votes proportionally
with its similarity. After normalization, the score
of each sense is:
</bodyText>
<equation confidence="0.9765418">
(,)
()= ()
E(,)
 

</equation>
<bodyText confidence="0.999422666666667">
where  is the ambiguous context,  the set
of neighbours, and  the set of training
examples for the sense .
</bodyText>
<subsectionHeader confidence="0.782013">
2.4 Two different mixtures
</subsectionHeader>
<bodyText confidence="0.9998115">
To increase the performances, we merge the in-
dividual result of the three algorithms. We have
first used a natural mixing method, which calcu-
lates the weighted sum of the individual scores:
</bodyText>
<equation confidence="0.558027">
0(�=WeO&apos;�)+WP..4)+WKN.M()
</equation>
<bodyText confidence="0.999975545454545">
We don&apos;t actually have any heuristic for deter-
mining the weights and therefore, we consider
that each algorithm has the same weight (i.e. W=
1/3). This basic mixture will be denoted mix-0.
Undesirable behaviour of this kind of mixture has
been observed, as, for example, the SCT domi-
nates the raw mixture. Despite the smoothing, the
top choice of the SCT is given an exaggerated
weight. So, we have developed another merging
strategy mix-1, which takes into account the rank
of a sense in the different results of an algorithm:
</bodyText>
<equation confidence="0.98889575">
lg()= E(lg(&apos;)lg()) .
 
&apos; 

Then, 1()=  
+ +
WKNN
R()R P i,(s)()
</equation>
<bodyText confidence="0.962907111111111">
Last, we filter the results to keep only a few
senses (generally 1 or 2): a sense is conserved if
its score is greater of  of the best score.
3 Use of a bilingual parallel corpus
Many methods have been proposed to improve
WSD using multilingual material. Ide  
(2001) showed that a multilingual corpus allows
to discriminate senses as well as human annota-
tors, and Diab &amp; Resnik (2002) have imple-
</bodyText>
<equation confidence="0.855344">
, where  is number of oc-
+
 −
E
</equation>
<bodyText confidence="0.999514188405797">
mented an unsupervised WSD method using an
artificial machine translated multilingual corpus.
We decided to test the following hypotheses: 1/
using an English-French aligned corpus, it is pos-
sible to identify a kind of synonymic relationship
between English units. 2/ from this relationship,
we may create more general classes among Eng-
lish words, and train our WSD models on these
classes, in order to give more consistency to the
semantic content of the training contexts: for in-
stance, if the word huge does not appear in the
training contexts, it cannot bring any information
for the WSD of a new example; but if large does
occur in the training, and if huge is known as
synonym of large, the occurrence of huge may be
taken into account and improve WSD.
As in Diab &amp; Resnik (2002), hypothesis 1 states
that if two units el and e2 are often translated by
the same French unit f, they may share some
common sense. f can be polysemous and el and
e2 may correspond to various senses, but these
senses must be somehow related. To reinforce
this hypothesis, and to adapt the classes to the
task, specific word classes are made from each
target-word sentence set.
The implementation of hypothesis 1 may face
two sources of noise: wrong word-to-word
alignments and homographs. We tried to filter
out this noise using two criteria:
- Word pairs frequencies: every aligned word-
pair that occurs less than 5 times through the
aligned corpus is discarded.
- Class stability: The classes are constructed it-
eratively: from an English word e, it is possible
to get the set T_4(e) of every associated words in
French. From this set, a new extraction yields the
set E(e) of the English corresponding words.
Stability is reached when En=En&amp;quot;. The classes
finally constitute a partition of the English word
set. For instance, huge and large fall in the same
class after 3 iterations: E1(huge)={huge, impor-
tant,vast}, E2(huge)={huge,important,vast,
large}=E3(huge) and E1(large)={large, vast}
E2(large)={huge, important, vast, large}=
E3(large). Noisy word alignments generally re-
sult in the cascading fusion of non related word
sets: they yield large classes that become stable
only after a lot of iterations. Thus, we decided to
discard every non stable class after three itera-
tions, each word being considered as a singleton.
For sentence alignment, we have used a combina-
tion of clues (cognates, sentence length) with an
algorithm that yielded results similar to the best
system of the Arcade Campaign (Langlais &amp;
Wronis, 2000). For lexical alignment, we im-
plemented an original method based on probabil-
istic models for co-occurrence, word position and
POS correspondence. Evaluated on a manually
aligned corpus (149 aligned sentences extracted
from Flaubert&apos;s Madame Bovary), this method
yielded precision and recall over 90% for the
content words.
Then, we applied these algorithms to the literary
multilingual corpus that we are developing for
the Carmel Project (travel stories from the late
19th century). The corpus has previously been
tokenized and POS tagged. It included around
850,000 words in each language, and 359,123
content word pairs have been extracted.
</bodyText>
<sectionHeader confidence="0.99371" genericHeader="method">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.978226">
4.1 Default Treatment
</subsectionHeader>
<bodyText confidence="0.999987791666667">
In order to improve WSD, it is important for dif-
ferent reasons (mainly a better coverage) to tag
the words at a POS and morphological level.
Thanks to the POS tags, we can eliminate deter-
minants and punctuations in the target word con-
text. Moreover, some groups of words are
substituted by a pseudo-lemma for instance:
numbers by CD, all pronouns by PRONOUN,
days of week by DAY, months by MONTH.
Afterwards, the context is reduced to a short
window2: 3 words before and 3 words after the
ambiguous word, within the same sentence.
Finally, the context words are lemmatized (or
class-reduced), except the ambiguous word that
keeps its form (including its case). Our assump-
tion is that it may have an impact on the sense
determination (for example bank and Bank)
In the case of the Translation and Sense test, we
append the English sense&apos;s identifier to the am-
biguous word. In this way, the WSD program and
the context windowing don&apos;t need any change.
But this choice has the drawback of linking to-
gether the form and the English sense. See the
two last lines of the example given in Table 1.
</bodyText>
<footnote confidence="0.7134265">
12 The SCT is subject to capture noise when the context is
too large, as shown in Crestan and El-Beze (2001)
</footnote>
<table confidence="0.996215454545455">
Initial con- Even before the huge new projects be-
text gan, the Strip&apos;s recent expansion
squeezed smaller competitors.
Filtered Even before huge new projects began
context Strip &apos;s recent expansion squeezed
smaller competitors
FL context huge new project began strip &apos;s recent
FL context huge new project began-19 strip &apos;s re-
with sense cent
FC context important new project began-19 strip
with sense &apos;s recent
</table>
<tableCaption confidence="0.99987">
Table 1: examples from the begin contexts
</tableCaption>
<sectionHeader confidence="0.789199" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.997393">
Our results for MLS task are displayed on table 2
(only mix-0 results have been submitted).
One can observe that using the sense-tag as addi-
tional information, the results are significantly
improved (we only obtain 0.593 with mix-0, and
0.603 with mix-1, when removing sense&apos;s identi-
fier in sub-task TS).
</bodyText>
<table confidence="0.9982802">
Sub-task T Sub-task TS
P and R Using Using Using Using
lemmas classes lemmas classes
mix-0 0.603 0.603 0.641 0.641
mix-1 0.607 0.607 0.645 0.645
</table>
<tableCaption confidence="0.995237">
Table 2: results for MLS task
</tableCaption>
<bodyText confidence="0.999988941176471">
We note that the global results without and with
classes are the same, and this is quite disappoint-
ing. Indeed, for the T sub-task, the results were a
bit higher for certain words and lower for others,
and the exact identity of the global precision is
fortuitous. For the TS sub-task, the results were
identical. The gain that may be brought by se-
mantically more consistent contexts (the classes)
may be compensated by the noise inherited from
the aligning and clustering methods. In addition,
the lack of improvement may be due to the dis-
crepancy between our English-French corpus and
the task: classes built from an English-French
corpus may not suit the English-Hindi examples;
moreover, classes were extracted from a 19th
Century literary corpus, quite heterogeneous with
the Training and the Test corpora.
</bodyText>
<sectionHeader confidence="0.995303" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999950909090909">
The results of our WSD method are correct,
while a bit inferior to the expected results for a
traditional WSD task. Translation disambiguation
may require specific methods, involving more
contrastive data.
The use of a bilingual corpus to improve our
WSD method has not been very conclusive. The
substitution of context words with more general
synonym-like word classes may be worth further
experiments, using an appropriate bilingual mate-
rial.
</bodyText>
<sectionHeader confidence="0.997607" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.992397">
Thanks to RIAM which funds the Carmel Pro-
ject, and to our partners, ACCE and SINEQUA.
</bodyText>
<sectionHeader confidence="0.996634" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999302733333333">
E. Crestan &amp; M. El-Beze. 2001. Improving Su-
pervised WSD by Including Rough Semantic
Features in a Multi-Level View of the Context.
Sempro 2001.
M. Diab and P. Resnik. 2002. An Unsupervised
Method for Word Sense Tagging using Paral-
lel Corpora, in Proc. ofACL-02, Philadelphia.
C. de Loupy, M. El-Beze, P.-F. Marteau. 1998.
WSD based on three short context methods,
Senseval Workshop, Herstmontceux.
N. Ide, T. Erjavec and D. Tufis. 2001. Automatic
Sense Tagging Using Parallel Corpora. In
Proc. of the Sixth Natural Language Process-
ing Pacific Rim Symposium, Tokyo, 83-9.
J. Veronis and P. Langlais. 2000. Evaluation of
parallel text alignment systems — The
ARCADE project. In Parallel Text Processing,
J. Veronis, ed., Dordrecht, Nederlands: Kluwer
Academic Publishers, pp. 49-68.
L. Breiman, J. H. Friedman, R. A. Olshen, and C.
J. Stone. 1984. Classification and Regression
Trees. Wadsworth Inc.
L. Bahl, R. Bakis, P. de Souza and R. Mercer.
1988. Obtaining candidate words by polling in
a large vocabulary speech recognition system.
In Proc. ICASP, vol. 1, pp. 489-92.
R. De Mori, R. Kuhn. 1995. The Application of
Semantic Classification Trees to Natural Lan-
guage Understanding, IEEE Transactions on
PAMI, vol. 17, no. 5, pp. 449-460.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.123443">
<note confidence="0.776029333333333">SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July 2004 Association for Computational Linguistics</note>
<title confidence="0.90345">Using a Word Sense Disambiguation system for translation disambiguation: the LIA-LIDILEM team experiment Gr6goire MOREAU DE</title>
<author confidence="0.998187">Marc EL-BEZE</author>
<affiliation confidence="0.529746">Laboratoire informatique d&apos;Avignon, Universite d&apos;Avignon</affiliation>
<email confidence="0.828064">moreau@lia.univ-avignon.fr</email>
<email confidence="0.828064">elbeze@lia.univ-avignon.fr</email>
<author confidence="0.973219">Boxing CHEN</author>
<author confidence="0.973219">Olivier KRAIF</author>
<affiliation confidence="0.976705">LIDILEM Universite Stendhal Grenoble</affiliation>
<address confidence="0.732729">{Chen, Kraif}@u-grenoble3.fr</address>
<abstract confidence="0.9950665">This paper presents an original WSD method, based on a mixture of three algorithms working on the local context of target units. The results on task   of Senseval 3 were 60.3% of precision and recall for the T sub-task, and 64.1% for TS. We attempted to improve the method by constituting synonymlike classes from an English-French aligned corpus, but without any gain in the results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Crestan</author>
<author>M El-Beze</author>
</authors>
<title>Improving Supervised WSD by Including Rough Semantic Features in a Multi-Level View of the Context. Sempro</title>
<date>2001</date>
<contexts>
<context position="12131" citStr="Crestan and El-Beze (2001)" startWordPosition="2069" endWordPosition="2072">(or class-reduced), except the ambiguous word that keeps its form (including its case). Our assumption is that it may have an impact on the sense determination (for example bank and Bank) In the case of the Translation and Sense test, we append the English sense&apos;s identifier to the ambiguous word. In this way, the WSD program and the context windowing don&apos;t need any change. But this choice has the drawback of linking together the form and the English sense. See the two last lines of the example given in Table 1. 12 The SCT is subject to capture noise when the context is too large, as shown in Crestan and El-Beze (2001) Initial con- Even before the huge new projects betext gan, the Strip&apos;s recent expansion squeezed smaller competitors. Filtered Even before huge new projects began context Strip &apos;s recent expansion squeezed smaller competitors FL context huge new project began strip &apos;s recent FL context huge new project began-19 strip &apos;s rewith sense cent FC context important new project began-19 strip with sense &apos;s recent Table 1: examples from the begin contexts 4.2 Results Our results for MLS task are displayed on table 2 (only mix-0 results have been submitted). One can observe that using the sense-tag as </context>
</contexts>
<marker>Crestan, El-Beze, 2001</marker>
<rawString>E. Crestan &amp; M. El-Beze. 2001. Improving Supervised WSD by Including Rough Semantic Features in a Multi-Level View of the Context. Sempro 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Diab</author>
<author>P Resnik</author>
</authors>
<title>An Unsupervised Method for Word Sense Tagging using Parallel Corpora, in</title>
<date>2002</date>
<booktitle>Proc. ofACL-02,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="7695" citStr="Diab &amp; Resnik (2002)" startWordPosition="1324" endWordPosition="1327">her merging strategy mix-1, which takes into account the rank of a sense in the different results of an algorithm: lg()= E(lg(&apos;)lg()) .   &apos;   Then, 1()=   + + WKNN R()R P i,(s)() Last, we filter the results to keep only a few senses (generally 1 or 2): a sense is conserved if its score is greater of  of the best score. 3 Use of a bilingual parallel corpus Many methods have been proposed to improve WSD using multilingual material. Ide   (2001) showed that a multilingual corpus allows to discriminate senses as well as human annotators, and Diab &amp; Resnik (2002) have imple, where  is number of oc+  − E mented an unsupervised WSD method using an artificial machine translated multilingual corpus. We decided to test the following hypotheses: 1/ using an English-French aligned corpus, it is possible to identify a kind of synonymic relationship between English units. 2/ from this relationship, we may create more general classes among English words, and train our WSD models on these classes, in order to give more consistency to the semantic content of the training contexts: for instance, if the word huge does not appear in the training contexts, it canno</context>
</contexts>
<marker>Diab, Resnik, 2002</marker>
<rawString>M. Diab and P. Resnik. 2002. An Unsupervised Method for Word Sense Tagging using Parallel Corpora, in Proc. ofACL-02, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C de Loupy</author>
<author>M El-Beze</author>
<author>P-F Marteau</author>
</authors>
<title>WSD based on three short context methods, Senseval Workshop,</title>
<date>1998</date>
<location>Herstmontceux.</location>
<marker>de Loupy, El-Beze, Marteau, 1998</marker>
<rawString>C. de Loupy, M. El-Beze, P.-F. Marteau. 1998. WSD based on three short context methods, Senseval Workshop, Herstmontceux.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ide</author>
<author>T Erjavec</author>
<author>D Tufis</author>
</authors>
<title>Automatic Sense Tagging Using Parallel Corpora.</title>
<date>2001</date>
<booktitle>In Proc. of the Sixth Natural Language Processing Pacific Rim Symposium,</booktitle>
<pages>83--9</pages>
<location>Tokyo,</location>
<marker>Ide, Erjavec, Tufis, 2001</marker>
<rawString>N. Ide, T. Erjavec and D. Tufis. 2001. Automatic Sense Tagging Using Parallel Corpora. In Proc. of the Sixth Natural Language Processing Pacific Rim Symposium, Tokyo, 83-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Veronis</author>
<author>P Langlais</author>
</authors>
<title>Evaluation of parallel text alignment systems — The ARCADE project.</title>
<date>2000</date>
<booktitle>In Parallel Text Processing,</booktitle>
<pages>49--68</pages>
<editor>J. Veronis, ed.,</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht, Nederlands:</location>
<marker>Veronis, Langlais, 2000</marker>
<rawString>J. Veronis and P. Langlais. 2000. Evaluation of parallel text alignment systems — The ARCADE project. In Parallel Text Processing, J. Veronis, ed., Dordrecht, Nederlands: Kluwer Academic Publishers, pp. 49-68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breiman</author>
<author>J H Friedman</author>
<author>R A Olshen</author>
<author>C J Stone</author>
</authors>
<title>Classification and Regression Trees.</title>
<date>1984</date>
<publisher>Wadsworth Inc.</publisher>
<marker>Breiman, Friedman, Olshen, Stone, 1984</marker>
<rawString>L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. 1984. Classification and Regression Trees. Wadsworth Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Bahl</author>
<author>R Bakis</author>
<author>P de Souza</author>
<author>R Mercer</author>
</authors>
<title>Obtaining candidate words by polling in a large vocabulary speech recognition system.</title>
<date>1988</date>
<booktitle>In Proc. ICASP,</booktitle>
<volume>1</volume>
<pages>489--92</pages>
<marker>Bahl, Bakis, de Souza, Mercer, 1988</marker>
<rawString>L. Bahl, R. Bakis, P. de Souza and R. Mercer. 1988. Obtaining candidate words by polling in a large vocabulary speech recognition system. In Proc. ICASP, vol. 1, pp. 489-92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R De Mori</author>
<author>R Kuhn</author>
</authors>
<title>The Application of Semantic Classification Trees to Natural Language Understanding,</title>
<date>1995</date>
<journal>IEEE Transactions on PAMI,</journal>
<volume>17</volume>
<pages>449--460</pages>
<marker>De Mori, Kuhn, 1995</marker>
<rawString>R. De Mori, R. Kuhn. 1995. The Application of Semantic Classification Trees to Natural Language Understanding, IEEE Transactions on PAMI, vol. 17, no. 5, pp. 449-460.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>