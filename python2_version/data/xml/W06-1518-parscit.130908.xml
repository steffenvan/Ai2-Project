<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.065837">
<title confidence="0.99559">
Using LTAG-Based Features for Semantic Role Labeling
</title>
<author confidence="0.99504">
Yudong Liu and Anoop Sarkar
</author>
<affiliation confidence="0.977393">
Computing Science Department
Simon Fraser University
</affiliation>
<address confidence="0.494656">
British Columbia, Canada, V5A 1S6
</address>
<email confidence="0.988719">
yudongl,anoop@cs.sfu.ca
</email>
<sectionHeader confidence="0.993685" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998064">
Semantic role labeling (SRL) methods
typically use features from syntactic parse
trees. We propose a novel method that
uses Lexicalized Tree-Adjoining Gram-
mar (LTAG) based features for this task.
We convert parse trees into LTAG deriva-
tion trees where the semantic roles are
treated as hidden information learned by
supervised learning on annotated data de-
rived from PropBank. We extracted var-
ious features from the LTAG derivation
trees and trained a discriminative decision
list model to predict semantic roles. We
present our results on the full CoNLL 2005
SRL task.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999926870967742">
Semantic role labeling (SRL) is a natural exten-
sion of the syntactic parsing task. In SRL, par-
ticular syntactic constituents in a parse tree for a
sentence are identified with semantic roles. The
labels assigned to various types of arguments and
adjuncts differ in different annotation schemes.
In this paper, we use the PropBank corpus of
predicate-argument structures (Palmer, Gildea and
Kingsbury, 2005). We assume we are given a syn-
tactic parse tree and a particular predicate in the
sentence for which we then identify the arguments
and adjuncts and their labels. In this paper we
compare two models for the identification of se-
mantic role labels in a parse tree: A model that
uses a path in the parse tree (or the derived tree in
TAG terminology) and various associated features
related to this, and we compare this model with a
model that converts the syntactic parse tree into
a Lexicalized Tree-Adjoining Grammar (LTAG)
derivation tree and uses features extracted from the
elementary trees and the LTAG derivation tree.
In each model the features of that model are
used in a discriminative model for semantic role
labeling. The model is a simple decision list
learner that uses tree patterns extracted from the
LTAG derivation trees in order to classify con-
stituents into their semantic roles. We present re-
sults on the full CoNLL 2005 SRL task (Carreras
and M`arquez, 2005) a dataset built by combining
the Treebank and parser data with the PropBank
annotations.
</bodyText>
<sectionHeader confidence="0.806042" genericHeader="introduction">
2 Background about SRL
</sectionHeader>
<bodyText confidence="0.983527757575757">
A semantic role is defined to be the relationship
that a syntactic constituent has with the predicate.
For example, the following sentence, taken from
the PropBank corpus, shows the annotation of se-
mantic roles:
[A0 Late buying] [V gave] [A2 the Paris
Bourse] [A1 a parachute] [AM-TMP after its
free fall early in the day].
Here, the arguments for the predicate gave are
defined in the PropBank Frame Scheme (Palmer,
Gildea and Kingsbury, 2005) as:
V: verb A2: beneficiary
A0: giver AM-TMP: temporal
A1: thing given
Recognizing and labeling semantic argu-
ments is a key task for answering “Who”,
“When”,“What”, “Where”, “Why”, etc. questions
in Information Extraction, Question Answering,
Summarization (Melli et al, 2005), and, in general,
in all NLP tasks in which some kind of semantic
interpretation is needed.
Most previous research treats the semantic role
labeling task as a classification problem, and di-
vides it into two phases: argument identification
and argument classification. Argument identifi-
cation involves classifying each syntactic element
in a sentence into either an argument or a non-
argument. Argument classification involves clas-
sifying each argument identified into a specific se-
mantic role. A variety of machine learning meth-
ods have been applied to this task. One of the most
important steps in building an accurate classifier is
feature selection. Different from the widely used
</bodyText>
<page confidence="0.965004">
127
</page>
<bodyText confidence="0.951464166666667">
Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 127–132,
Sydney, July 2006. c�2006 Association for Computational Linguistics
feature functions that are based on the syntactic
parse tree (Gildea and Jurafsky, 2002), we explore
the use of LTAG-based features in a simple dis-
criminative decision-list learner.
</bodyText>
<sectionHeader confidence="0.994507" genericHeader="method">
3 LTAG Based Feature Extraction
</sectionHeader>
<bodyText confidence="0.9998485">
In this section, we introduce the main components
of our system. First, we do a pruning on the given
parse trees with certain constraints. Then we de-
compose the pruned parse trees into a set of LTAG
elementary trees. For each constituent in question,
we extract features from its corresponding deriva-
tion tree. We train using these features in a deci-
sion list model.
</bodyText>
<subsectionHeader confidence="0.999671">
3.1 Pruning the Parse Trees
</subsectionHeader>
<bodyText confidence="0.999944666666667">
Given a parse tree, the pruning component identi-
fies the predicate in the tree and then only admits
those nodes that are sisters to the path from the
predicate to the root. It is commonly used in the
SRL community (cf. (Xue and Palmer, 2004)) and
our experiments show that 91% of the SRL targets
can be recovered despite this aggressive pruning.
There are two advantages to this pruning: the ma-
chine learning method used for prediction of SRLs
is not overwhelmed with a large number of non-
SRL nodes; and the process is far more efficient
as 80% of the target nodes in a full parse tree are
pruned away in this step. We make two enhance-
ments to the pruned Propbank tree: we enrich the
sister nodes with their head information, which is
a part-of-speech tag and word pair: (t, w) and PP
nodes are expanded to include the NP complement
of the PP (including the head information). Note
that the target SRL node is still the PP. Figure 1
shows the pruned parse tree for a sentence from
PropBank section 24.
</bodyText>
<subsectionHeader confidence="0.996937">
3.2 LTAG-based Decomposition
</subsectionHeader>
<bodyText confidence="0.997663363636363">
As next step, we decompose the pruned tree
around the predicate using standard head-
percolation based heuristic rules1 to convert a
Treebank tree into a LTAG derivation tree. We
do not use any sophistical adjunct/argument or
other extraction heuristics using empty elements
(as we don’t have access to them in the CoNLL
2005 data). Also, we do not use any substitution
nodes in our elementary trees: instead we exclu-
sively use adjunction or sister adjunction for the
attachment of sub-derivations. As a result the
</bodyText>
<footnote confidence="0.926539">
1using http://www.isi.edu/∼chiang/software/treep/treep.html
</footnote>
<bodyText confidence="0.999703142857143">
root node in an LTAG derivation tree is a spinal
elementary tree and the derivation tree provides
the path from the predicate to the constituent in
question. Figure 2 shows the resulting elementary
tree after decomposition of the pruned tree. For
each of the elementary trees we consider their
labeling in the derivation tree to be their semantic
role labels from the training data. Figure 3 is the
derivation tree for the entire pruned tree.
Note that the LTAG-based decomposition of the
parse tree allows us to use features that are distinct
from the usual parse tree path features used for
SRL. For example, the typical parse tree feature
from Figure 2 used to identify constituent (NP (NN
terminal)) as A0 would be the parse tree fragment:
NPT NP I SBAR I S I V P I S I V P I
VBG cover (the arrows signify the path through
the parse tree). Using the LTAG-based decompo-
sition means that our SRL model can use any fea-
tures from the derivation tree such as in Figure 2,
including the elementary tree shapes.
</bodyText>
<subsectionHeader confidence="0.992912">
3.3 Decision List Model for SRL
</subsectionHeader>
<bodyText confidence="0.9999955">
Before we train or test our model, we convert
the training, development and test data into LTAG
derivation trees as described in the previous sec-
tion. In our model we make an independence as-
sumption that each semantic role is assigned to
each constituent independently, conditional only
on the path from the predicate elementary tree
to the constituent elementary tree in the deriva-
tion tree. Different elementary tree siblings in the
LTAG derivation tree do not influence each other
in our current models. Figure 4 shows the differ-
ent derivation trees for the target constituent (NP
(NN terminal)): each providing a distinct semantic
role labeling for a particular constituent. We use
a decision list learner for identifying SRLs based
on LTAG-based features. In this model, LTAG el-
ementary trees are combined with some distance
information as features to do the semantic role la-
beling. The rationale for using a simple DL learner
is given in (Gildea and Jurafsky, 2002) where es-
sentially it based on their experience with the set-
ting of backoff weights for smoothing, it is stated
that the most specific single feature matching the
training data is enough to predict the SRL on test
data. For simplicity, we only consider one inter-
mediate elementary tree (if any) at one time in-
stead of multiple intermediate trees along the path
from the predicate to the argument.
</bodyText>
<page confidence="0.968136">
128
</page>
<figure confidence="0.999817722222222">
S
IN-H
NN-H
covering
face
with
NP
PRP-H
He
VBZ-H
backflips
VP-H
PP
,
IN-H
into
NP
NP-H
SBAR
, WHNP-H
WDT-H
which
VBZ-H
explodes
S
VP-H
,
,
NN-H
terminal
S
VP-H
VBG-H NP PP
NP
NNS-H
microchips
</figure>
<figureCaption confidence="0.9586905">
Figure 1: The pruned tree for the sentence “He back�ips into a desktop computer terminal, which ex-
plodes, covering Huntz Hall ’s face with microchips.”
</figureCaption>
<figure confidence="0.99783324">
predicate: S A1: NP A2: PP NULL: S NULL: , R-A0: SBAR
VP-H
VBG-H
cover
NN-H
face
IN-H NP
with NNS-H
microchips
VP-H
VBZ-H
explodes
WHNP-H
WDT-H
which
,
A0: NP NULL: PP NULL: S NULL: NP
NP-H IN-H
NN-H into
terminal
VP-H
VBZ-H
backflips
PRP-H
He
</figure>
<figureCaption confidence="0.999836">
Figure 2: The resulting elementary trees after decomposition of the pruned tree.
</figureCaption>
<page confidence="0.914772">
129
</page>
<equation confidence="0.964113571428571">
S(backflips)
NP(he) PP(into)
NP(terminal)
,(,) SBAR(which)
S(explodes)
,(,) S(cover)
NP(face) PP(with)
</equation>
<figureCaption confidence="0.999667">
Figure 3: The LTAG derivation tree (with no semantic role labels) corresponding to the pruned tree.
</figureCaption>
<figure confidence="0.991644416666667">
A0: NP-NP(NN,terminal)
R-A0: SBAR-WHNP(WDT,which)
NULL: S-VP(VBZ,explodes)
predicate: S-VPH(VBG,cover)
A1: NP-NP(NN,terminal)
R-A0: SBAR-WHNP(WDT,which)
NULL: S-VP(VBZ,explodes)
predicate: S-VPH(VBG,cover)
A0: NP-NP(NN,terminal)
R-A0: SBAR-WHNP(WDT,which)
AM-ADV: S-VP(VBZ,explodes)
predicate: S-VPH(VBG,cover)
</figure>
<figureCaption confidence="0.9989505">
Figure 4: Different LTAG derivation trees corresponding to different assignments of semantic roles to
constituents. The constituent in question is (NP (NN terminal)).
</figureCaption>
<figure confidence="0.99977984375">
NP
VP
VP
SBAR
PP
S
VBZ
S
VP
VBG
cover
NP
S
VP
explodes
IN
with
NNS
VP
VBG
S
SBAR
WHNP
WDT
which
NP
NN
terminal
S microchips VP cover
VP VBG
VBG cover
cover
</figure>
<figureCaption confidence="0.999706">
Figure 5: Tree patterns in tree pattern matching
</figureCaption>
<page confidence="0.990978">
130
</page>
<bodyText confidence="0.999793804347826">
The input to the learning algorithm is labeled
examples of the form (xi, yz). yz is the label (either
NULL for no SRL, or the SRL) of the ith example.
xi is a feature vector (P, A, Dist, Position, R-
type, tz E tI, Distti), where P is the predicate
elementary tree, A is the tree for the constituent
being labeled with a SRL, tI is a set of interme-
diate elementary trees between the predicate tree
and the argument tree. Each P, A, I tree consists
of the elementary tree template plus the tag, word
pair: (t, w).
All possible combinations of fully-
lexicalized/postag/un-lexicalized elementary
trees are used for each example. Dist and Distti
denote the distance to the predicate from the
argument tree and the intermediate elementary
tree respectively. Position is interpreted as the
position that the target is relative to the predicate.
R-type denotes the relation type of the predicate
and the target constituent. 3 types are defined: if
the predicate dominates (directly or undirectly)
the argument in the derivation tree, we have the
relation of type-1; if the other way around, the
argument dominates (directly or undirectly) the
predicate then we have the relation of type-2; and
finally type-3 means that neither the predicate
or the argument dominate each other in the
derivation tree and instead are dominated (again,
directly or indirectly) by another elementary tree.
The output of the learning algorithm is a func-
tion h(x, y) which is an estimate of the conditional
probability p(y  |x) of seeing SRL y given pat-
tern x. h is interpreted as a decision list of rules
x ==&gt;. y ranked by the score h(x, y). In testing,
we simply pick the first rule that matches the par-
ticular test example x. We trained different mod-
els using the same learning algorithm. In addition
to the LTAG-based method, we also implemented
a pattern matching based method on the derived
(parse) tree using the same model. In this method,
instead of considering each intermediate elemen-
tary tree between the predicate and the argument,
we extract the whole path from the predicate to the
argument. So the input is more like a tree than a
discrete feature vector. Figure 5 shows the patterns
that are extracted from the same pruned tree.
</bodyText>
<sectionHeader confidence="0.995907" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.995691333333333">
We use the PropBank corpus of predicate-
argument structures (Palmer, Gildea and Kings-
bury, 2005) as our source of annotated data for the
</bodyText>
<table confidence="0.998928">
dev = Sec24 p(%) r(%) f(%)
test = Sec23
M1: dev 78.42 77.03 77.72
M1: test 80.52 79.40 79.96
M2: dev 81.11 79.39 80.24
M2: test 83.47 81.82 82.64
M3: dev 80.98 79.56 80.26
M3: test 81.86 83.34 82.60
</table>
<tableCaption confidence="0.999805">
Table 1: Results on the CoNLL 2005 shared task
</tableCaption>
<bodyText confidence="0.976792375">
using gold standard parse trees. M1 is the LTAG-
based model, M2 is the derived tree pattern match-
ing Model, M3 is a hybrid model
SRL task. However, there are many different ways
to evaluate performance on the PropBank, leading
to incomparable results. To avoid such a situation,
in this paper we use the CoNLL 2005 shared SRL
task data (Carreras and M`arquez, 2005) which
provides a standard train/test split, a standard
method for training and testing on various prob-
lematic cases involving coordination. However, in
some cases, the CoNLL 2005 data is not ideal for
the use of LTAG-based features as some “deep” in-
formation cannot be recovered due to the fact that
trace information and other empty categories like
PRO are removed entirely from the training data.
As a result some of the features that undo long-
distance movement via trace information in the
TreeBank as used in (Chen and Rambow, 2003)
cannot be exploited in our model. Our results are
shown in Table 1. Note that we test on the gold
standard parse trees because we want to compare
a model using features from the derived parse trees
to the model using the LTAG derivation trees.
</bodyText>
<sectionHeader confidence="0.999971" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999953928571429">
In the community of SRL researchers (cf. (Gildea
and Jurafsky, 2002; Punyakanok, Roth and Yih,
2005; Pradhan et al, 2005; Toutanova et al.,
2005)), the focus has been on two different aspects
of the SRL task: (a) finding appropriate features,
and (b) resolving the parsing accuracy problem by
combining multiple parsers/predictions. Systems
that use parse trees as a source of feature func-
tions for their models have typically outperformed
shallow parsing models on the SRL task. Typi-
cal features extracted from a parse tree is the path
from the predicate to the constituent and various
generalizations based on this path (such as phrase
type, position, etc.). Notably the voice (passive or
</bodyText>
<page confidence="0.995231">
131
</page>
<bodyText confidence="0.99999141025641">
active) of the verb is often used and recovered us-
ing a heuristic rule. We also use the passive/active
voice by labeling this information into the parse
tree. However, in contrast with other work, in this
paper we do not focus on the problem of parse ac-
curacy: where the parser output may not contain
the constituent that is required for recovering all
SRLs.
There has been some previous work in SRL
that uses LTAG-based decomposition of the parse
tree and we compare our work to this more
closely. (Chen and Rambow, 2003) discuss a
model for SRL that uses LTAG-based decompo-
sition of parse trees (as is typically done for sta-
tistical LTAG parsing). Instead of using the typi-
cal parse tree features used in typical SRL models,
(Chen and Rambow, 2003) uses the path within
the elementary tree from the predicate to the con-
stituent argument. They only recover seman-
tic roles for those constituents that are localized
within a single elementary tree for the predicate,
ignoring cases that occur outside the elementary
tree. In contrast, we recover all SRLs regardless
of locality within the elementary tree. As a result,
if we do not compare the machine learning meth-
ods involved in the two approaches, but rather the
features used in learning, our features are a natural
generalization of (Chen and Rambow, 2003).
Our approach is also very akin to the approach
in (Shen and Joshi, 2005) which uses PropBank
information to recover an LTAG treebank as if it
were hidden data underlying the Penn Treebank.
This is similar to our approach of having several
possible LTAG derivations representing recovery
of SRLs. However, (Shen and Joshi, 2005) do
not focus on the SRL task, and in both of these
instances of previous work using LTAG for SRL,
we cannot directly compare our performance with
theirs due to differing assumptions about the task.
</bodyText>
<sectionHeader confidence="0.997836" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9999718125">
In this paper, we proposed a novel model for
SRL using features extracted from LTAG deriva-
tion trees. A simple decision list learner is applied
to train on the tree patterns containing new fea-
tures. This simple learning method enables us to
quickly explore new features for this task. How-
ever, this work is still preliminary: a lot of addi-
tional work is required to be competitive with the
state-of-the-art SRL systems. In particular, we do
not deal with automatically parsed data yet, which
leads to a drop in our performance. We also do not
incorporate various other features commonly used
for SRL, as our goal in this paper was to make a
direct comparison between simple pattern match-
ing features on the derived tree and compare them
to features from LTAG derivation trees.
</bodyText>
<sectionHeader confidence="0.999193" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999969822222222">
X. Carreras and L. M`arquez 2005. Introduction to
the CoNLL-2005 Shared Task. In Proc. of CoNLL
2005.
J. Chen and O. Rambow. 2003. Use of Deep Linguis-
tic Features for the Recognition and Labeling of Se-
mantic Arguments. In Proceedings of the 2003 Con-
ference on Empirical Methods in Natural Language
Processing, Sapporo, Japan, 2003.
D. Gildea and D. Jurafsky. 2002. Automatic Label-
ing of Semantic Roles. Computational Linguistics,
58(3):245–288
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Seman-
tic Roles. Computational Linguistics, 31(1).
G. Melli and Y. Wang and Y. Liu and M. Kashani and Z.
Shi and B. Gu and A. Sarkar and F. Popowich 2005.
Description of SQUASH, the SFU Question An-
swering Summary Handler for the DUC-2005 Sum-
marization Task. In Proceeding of Document Un-
derstanding Conference (DUC-2005)
S. Pradhan, K. Hacioglu, W. Ward, J. H. Martin, and
D. Jurafsky. 2005. Semantic Role Chunking Com-
bining Complementary Syntactic Views, In Pro-
ceedings of the 9th Conference on Natural Language
Learning (CoNLL 2005), Ann Arbor, MI, 2005.
V. Punyakanok, D. Roth, and W Yih. 2005. Gener-
alized Inference with Multiple Semantic Role La-
beling Systems (shared task paper). Proc. of the
Annual Conference on Computational Natural Lan-
guage Learning (CoNLL) pp. 181-184
Ruppenhofer, Josef, Collin F. Baker and Charles J. Fill-
more. 2002. The FrameNet Database and Soft-
ware Tools. In Braasch, Anna and Claus Povlsen
(eds.), Proceedings of the Tenth Euralex Interna-
tional Congress. Copenhagen, Denmark. Vol. I: 371-
375.
L. Shen and A. Joshi. 2005. Building an LTAG Tree-
bank. Technical Report MS-CIS-05-15, CIS Depart-
ment, University of Pennsylvania.
K. Toutanova, A. Haghighi, and C. D. Manning. 2005.
Joint learning improves semantic role labeling. ACL
2005
N. Xue and M. Palmer. 2004. Calibrating Features
for Semantic Role Labeling, In Proceedings of
EMNLP-2004. Barcelona, Spain.
</reference>
<page confidence="0.997732">
132
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.250352">
<title confidence="0.999637">Using LTAG-Based Features for Semantic Role Labeling</title>
<author confidence="0.750327">Liu</author>
<affiliation confidence="0.899107">Computing Science</affiliation>
<address confidence="0.685383">Simon Fraser British Columbia, Canada, V5A</address>
<email confidence="0.990572">yudongl,anoop@cs.sfu.ca</email>
<abstract confidence="0.9810225">Semantic role labeling (SRL) methods typically use features from syntactic parse trees. We propose a novel method that uses Lexicalized Tree-Adjoining Grammar (LTAG) based features for this task. We convert parse trees into LTAG derivation trees where the semantic roles are treated as hidden information learned by supervised learning on annotated data derived from PropBank. We extracted various features from the LTAG derivation trees and trained a discriminative decision list model to predict semantic roles. We present our results on the full CoNLL 2005 SRL task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L M`arquez</author>
</authors>
<title>Introduction to the CoNLL-2005 Shared Task.</title>
<date>2005</date>
<booktitle>In Proc. of CoNLL</booktitle>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>X. Carreras and L. M`arquez 2005. Introduction to the CoNLL-2005 Shared Task. In Proc. of CoNLL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chen</author>
<author>O Rambow</author>
</authors>
<title>Use of Deep Linguistic Features for the Recognition and Labeling of Semantic Arguments.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Sapporo, Japan,</location>
<contexts>
<context position="13583" citStr="Chen and Rambow, 2003" startWordPosition="2252" endWordPosition="2255">n this paper we use the CoNLL 2005 shared SRL task data (Carreras and M`arquez, 2005) which provides a standard train/test split, a standard method for training and testing on various problematic cases involving coordination. However, in some cases, the CoNLL 2005 data is not ideal for the use of LTAG-based features as some “deep” information cannot be recovered due to the fact that trace information and other empty categories like PRO are removed entirely from the training data. As a result some of the features that undo longdistance movement via trace information in the TreeBank as used in (Chen and Rambow, 2003) cannot be exploited in our model. Our results are shown in Table 1. Note that we test on the gold standard parse trees because we want to compare a model using features from the derived parse trees to the model using the LTAG derivation trees. 5 Related Work In the community of SRL researchers (cf. (Gildea and Jurafsky, 2002; Punyakanok, Roth and Yih, 2005; Pradhan et al, 2005; Toutanova et al., 2005)), the focus has been on two different aspects of the SRL task: (a) finding appropriate features, and (b) resolving the parsing accuracy problem by combining multiple parsers/predictions. Systems</context>
<context position="15060" citStr="Chen and Rambow, 2003" startWordPosition="2505" endWordPosition="2508">zations based on this path (such as phrase type, position, etc.). Notably the voice (passive or 131 active) of the verb is often used and recovered using a heuristic rule. We also use the passive/active voice by labeling this information into the parse tree. However, in contrast with other work, in this paper we do not focus on the problem of parse accuracy: where the parser output may not contain the constituent that is required for recovering all SRLs. There has been some previous work in SRL that uses LTAG-based decomposition of the parse tree and we compare our work to this more closely. (Chen and Rambow, 2003) discuss a model for SRL that uses LTAG-based decomposition of parse trees (as is typically done for statistical LTAG parsing). Instead of using the typical parse tree features used in typical SRL models, (Chen and Rambow, 2003) uses the path within the elementary tree from the predicate to the constituent argument. They only recover semantic roles for those constituents that are localized within a single elementary tree for the predicate, ignoring cases that occur outside the elementary tree. In contrast, we recover all SRLs regardless of locality within the elementary tree. As a result, if w</context>
</contexts>
<marker>Chen, Rambow, 2003</marker>
<rawString>J. Chen and O. Rambow. 2003. Use of Deep Linguistic Features for the Recognition and Labeling of Semantic Arguments. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, Sapporo, Japan, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic Labeling of Semantic Roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>58</volume>
<issue>3</issue>
<contexts>
<context position="3958" citStr="Gildea and Jurafsky, 2002" startWordPosition="620" endWordPosition="623">element in a sentence into either an argument or a nonargument. Argument classification involves classifying each argument identified into a specific semantic role. A variety of machine learning methods have been applied to this task. One of the most important steps in building an accurate classifier is feature selection. Different from the widely used 127 Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 127–132, Sydney, July 2006. c�2006 Association for Computational Linguistics feature functions that are based on the syntactic parse tree (Gildea and Jurafsky, 2002), we explore the use of LTAG-based features in a simple discriminative decision-list learner. 3 LTAG Based Feature Extraction In this section, we introduce the main components of our system. First, we do a pruning on the given parse trees with certain constraints. Then we decompose the pruned parse trees into a set of LTAG elementary trees. For each constituent in question, we extract features from its corresponding derivation tree. We train using these features in a decision list model. 3.1 Pruning the Parse Trees Given a parse tree, the pruning component identifies the predicate in the tree </context>
<context position="8095" citStr="Gildea and Jurafsky, 2002" startWordPosition="1328" endWordPosition="1331">tituent elementary tree in the derivation tree. Different elementary tree siblings in the LTAG derivation tree do not influence each other in our current models. Figure 4 shows the different derivation trees for the target constituent (NP (NN terminal)): each providing a distinct semantic role labeling for a particular constituent. We use a decision list learner for identifying SRLs based on LTAG-based features. In this model, LTAG elementary trees are combined with some distance information as features to do the semantic role labeling. The rationale for using a simple DL learner is given in (Gildea and Jurafsky, 2002) where essentially it based on their experience with the setting of backoff weights for smoothing, it is stated that the most specific single feature matching the training data is enough to predict the SRL on test data. For simplicity, we only consider one intermediate elementary tree (if any) at one time instead of multiple intermediate trees along the path from the predicate to the argument. 128 S IN-H NN-H covering face with NP PRP-H He VBZ-H backflips VP-H PP , IN-H into NP NP-H SBAR , WHNP-H WDT-H which VBZ-H explodes S VP-H , , NN-H terminal S VP-H VBG-H NP PP NP NNS-H microchips Figure </context>
<context position="13910" citStr="Gildea and Jurafsky, 2002" startWordPosition="2311" endWordPosition="2314">e “deep” information cannot be recovered due to the fact that trace information and other empty categories like PRO are removed entirely from the training data. As a result some of the features that undo longdistance movement via trace information in the TreeBank as used in (Chen and Rambow, 2003) cannot be exploited in our model. Our results are shown in Table 1. Note that we test on the gold standard parse trees because we want to compare a model using features from the derived parse trees to the model using the LTAG derivation trees. 5 Related Work In the community of SRL researchers (cf. (Gildea and Jurafsky, 2002; Punyakanok, Roth and Yih, 2005; Pradhan et al, 2005; Toutanova et al., 2005)), the focus has been on two different aspects of the SRL task: (a) finding appropriate features, and (b) resolving the parsing accuracy problem by combining multiple parsers/predictions. Systems that use parse trees as a source of feature functions for their models have typically outperformed shallow parsing models on the SRL task. Typical features extracted from a parse tree is the path from the predicate to the constituent and various generalizations based on this path (such as phrase type, position, etc.). Notabl</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>D. Gildea and D. Jurafsky. 2002. Automatic Labeling of Semantic Roles. Computational Linguistics, 58(3):245–288</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The Proposition Bank: An Annotated Corpus of Semantic Roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>M. Palmer, D. Gildea, and P. Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics, 31(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Melli</author>
<author>Y Wang</author>
<author>Y Liu</author>
<author>M Kashani</author>
<author>Z Shi</author>
<author>B Gu</author>
<author>A Sarkar</author>
<author>F Popowich</author>
</authors>
<date>2005</date>
<booktitle>Description of SQUASH, the SFU Question Answering Summary Handler for the DUC-2005 Summarization Task. In Proceeding of Document Understanding Conference (DUC-2005)</booktitle>
<contexts>
<context position="3003" citStr="Melli et al, 2005" startWordPosition="476" endWordPosition="479">For example, the following sentence, taken from the PropBank corpus, shows the annotation of semantic roles: [A0 Late buying] [V gave] [A2 the Paris Bourse] [A1 a parachute] [AM-TMP after its free fall early in the day]. Here, the arguments for the predicate gave are defined in the PropBank Frame Scheme (Palmer, Gildea and Kingsbury, 2005) as: V: verb A2: beneficiary A0: giver AM-TMP: temporal A1: thing given Recognizing and labeling semantic arguments is a key task for answering “Who”, “When”,“What”, “Where”, “Why”, etc. questions in Information Extraction, Question Answering, Summarization (Melli et al, 2005), and, in general, in all NLP tasks in which some kind of semantic interpretation is needed. Most previous research treats the semantic role labeling task as a classification problem, and divides it into two phases: argument identification and argument classification. Argument identification involves classifying each syntactic element in a sentence into either an argument or a nonargument. Argument classification involves classifying each argument identified into a specific semantic role. A variety of machine learning methods have been applied to this task. One of the most important steps in b</context>
</contexts>
<marker>Melli, Wang, Liu, Kashani, Shi, Gu, Sarkar, Popowich, 2005</marker>
<rawString>G. Melli and Y. Wang and Y. Liu and M. Kashani and Z. Shi and B. Gu and A. Sarkar and F. Popowich 2005. Description of SQUASH, the SFU Question Answering Summary Handler for the DUC-2005 Summarization Task. In Proceeding of Document Understanding Conference (DUC-2005)</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pradhan</author>
<author>K Hacioglu</author>
<author>W Ward</author>
<author>J H Martin</author>
<author>D Jurafsky</author>
</authors>
<title>Semantic Role Chunking Combining Complementary Syntactic Views,</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th Conference on Natural Language Learning (CoNLL</booktitle>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="13963" citStr="Pradhan et al, 2005" startWordPosition="2320" endWordPosition="2323">at trace information and other empty categories like PRO are removed entirely from the training data. As a result some of the features that undo longdistance movement via trace information in the TreeBank as used in (Chen and Rambow, 2003) cannot be exploited in our model. Our results are shown in Table 1. Note that we test on the gold standard parse trees because we want to compare a model using features from the derived parse trees to the model using the LTAG derivation trees. 5 Related Work In the community of SRL researchers (cf. (Gildea and Jurafsky, 2002; Punyakanok, Roth and Yih, 2005; Pradhan et al, 2005; Toutanova et al., 2005)), the focus has been on two different aspects of the SRL task: (a) finding appropriate features, and (b) resolving the parsing accuracy problem by combining multiple parsers/predictions. Systems that use parse trees as a source of feature functions for their models have typically outperformed shallow parsing models on the SRL task. Typical features extracted from a parse tree is the path from the predicate to the constituent and various generalizations based on this path (such as phrase type, position, etc.). Notably the voice (passive or 131 active) of the verb is of</context>
</contexts>
<marker>Pradhan, Hacioglu, Ward, Martin, Jurafsky, 2005</marker>
<rawString>S. Pradhan, K. Hacioglu, W. Ward, J. H. Martin, and D. Jurafsky. 2005. Semantic Role Chunking Combining Complementary Syntactic Views, In Proceedings of the 9th Conference on Natural Language Learning (CoNLL 2005), Ann Arbor, MI, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>Generalized Inference with Multiple Semantic Role Labeling Systems (shared task paper).</title>
<date>2005</date>
<booktitle>Proc. of the Annual Conference on Computational Natural Language Learning (CoNLL)</booktitle>
<pages>181--184</pages>
<marker>Punyakanok, Roth, Yih, 2005</marker>
<rawString>V. Punyakanok, D. Roth, and W Yih. 2005. Generalized Inference with Multiple Semantic Role Labeling Systems (shared task paper). Proc. of the Annual Conference on Computational Natural Language Learning (CoNLL) pp. 181-184</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Ruppenhofer</author>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
</authors>
<date>2002</date>
<booktitle>The FrameNet Database and Software Tools. In Braasch, Anna and Claus Povlsen (eds.), Proceedings of the Tenth Euralex International Congress. Copenhagen, Denmark. Vol. I:</booktitle>
<pages>371--375</pages>
<marker>Ruppenhofer, Baker, Fillmore, 2002</marker>
<rawString>Ruppenhofer, Josef, Collin F. Baker and Charles J. Fillmore. 2002. The FrameNet Database and Software Tools. In Braasch, Anna and Claus Povlsen (eds.), Proceedings of the Tenth Euralex International Congress. Copenhagen, Denmark. Vol. I: 371-375.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>A Joshi</author>
</authors>
<title>Building an LTAG Treebank.</title>
<date>2005</date>
<tech>Technical Report MS-CIS-05-15,</tech>
<institution>CIS Department, University of Pennsylvania.</institution>
<contexts>
<context position="15922" citStr="Shen and Joshi, 2005" startWordPosition="2651" endWordPosition="2654">h within the elementary tree from the predicate to the constituent argument. They only recover semantic roles for those constituents that are localized within a single elementary tree for the predicate, ignoring cases that occur outside the elementary tree. In contrast, we recover all SRLs regardless of locality within the elementary tree. As a result, if we do not compare the machine learning methods involved in the two approaches, but rather the features used in learning, our features are a natural generalization of (Chen and Rambow, 2003). Our approach is also very akin to the approach in (Shen and Joshi, 2005) which uses PropBank information to recover an LTAG treebank as if it were hidden data underlying the Penn Treebank. This is similar to our approach of having several possible LTAG derivations representing recovery of SRLs. However, (Shen and Joshi, 2005) do not focus on the SRL task, and in both of these instances of previous work using LTAG for SRL, we cannot directly compare our performance with theirs due to differing assumptions about the task. 6 Conclusion and Future Work In this paper, we proposed a novel model for SRL using features extracted from LTAG derivation trees. A simple decisi</context>
</contexts>
<marker>Shen, Joshi, 2005</marker>
<rawString>L. Shen and A. Joshi. 2005. Building an LTAG Treebank. Technical Report MS-CIS-05-15, CIS Department, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>A Haghighi</author>
<author>C D Manning</author>
</authors>
<title>Joint learning improves semantic role labeling.</title>
<date>2005</date>
<publisher>ACL</publisher>
<contexts>
<context position="13988" citStr="Toutanova et al., 2005" startWordPosition="2324" endWordPosition="2327">and other empty categories like PRO are removed entirely from the training data. As a result some of the features that undo longdistance movement via trace information in the TreeBank as used in (Chen and Rambow, 2003) cannot be exploited in our model. Our results are shown in Table 1. Note that we test on the gold standard parse trees because we want to compare a model using features from the derived parse trees to the model using the LTAG derivation trees. 5 Related Work In the community of SRL researchers (cf. (Gildea and Jurafsky, 2002; Punyakanok, Roth and Yih, 2005; Pradhan et al, 2005; Toutanova et al., 2005)), the focus has been on two different aspects of the SRL task: (a) finding appropriate features, and (b) resolving the parsing accuracy problem by combining multiple parsers/predictions. Systems that use parse trees as a source of feature functions for their models have typically outperformed shallow parsing models on the SRL task. Typical features extracted from a parse tree is the path from the predicate to the constituent and various generalizations based on this path (such as phrase type, position, etc.). Notably the voice (passive or 131 active) of the verb is often used and recovered us</context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2005</marker>
<rawString>K. Toutanova, A. Haghighi, and C. D. Manning. 2005. Joint learning improves semantic role labeling. ACL 2005</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>M Palmer</author>
</authors>
<title>Calibrating Features for Semantic Role Labeling,</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP-2004.</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="4720" citStr="Xue and Palmer, 2004" startWordPosition="753" endWordPosition="756">, we introduce the main components of our system. First, we do a pruning on the given parse trees with certain constraints. Then we decompose the pruned parse trees into a set of LTAG elementary trees. For each constituent in question, we extract features from its corresponding derivation tree. We train using these features in a decision list model. 3.1 Pruning the Parse Trees Given a parse tree, the pruning component identifies the predicate in the tree and then only admits those nodes that are sisters to the path from the predicate to the root. It is commonly used in the SRL community (cf. (Xue and Palmer, 2004)) and our experiments show that 91% of the SRL targets can be recovered despite this aggressive pruning. There are two advantages to this pruning: the machine learning method used for prediction of SRLs is not overwhelmed with a large number of nonSRL nodes; and the process is far more efficient as 80% of the target nodes in a full parse tree are pruned away in this step. We make two enhancements to the pruned Propbank tree: we enrich the sister nodes with their head information, which is a part-of-speech tag and word pair: (t, w) and PP nodes are expanded to include the NP complement of the P</context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>N. Xue and M. Palmer. 2004. Calibrating Features for Semantic Role Labeling, In Proceedings of EMNLP-2004. Barcelona, Spain.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>