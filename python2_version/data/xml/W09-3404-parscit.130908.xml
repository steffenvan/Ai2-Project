<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000036">
<title confidence="0.98656">
Analysis and Development of Urdu POS Tagged Corpus
</title>
<author confidence="0.985269">
Ahmed Muaz
</author>
<affiliation confidence="0.955163">
Center for Research in Urdu
</affiliation>
<address confidence="0.5030995">
Language Processing
NUCES, Pakistan
</address>
<email confidence="0.960718">
ahmed.muaz@nu.edu.pk
</email>
<author confidence="0.838201">
Aasim Ali
</author>
<affiliation confidence="0.824975">
Center for Research in Urdu
</affiliation>
<address confidence="0.761753">
Language Processing,
NUCES, Pakistan
</address>
<email confidence="0.963121">
aasim.ali@nu.edu.pk
</email>
<author confidence="0.913188">
Sarmad Hussain
</author>
<affiliation confidence="0.878892">
Center for Research in Urdu
</affiliation>
<address confidence="0.48164">
Language Processing
NUCES, Pakistan
</address>
<email confidence="0.927476">
sarmad.hussain@nu.edu.pk
</email>
<sectionHeader confidence="0.991192" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999733071428571">
In this paper, two corpora of Urdu (with 110K
and 120K words) tagged with different POS
tagsets are used to train TnT and Tree taggers.
Error analysis of both taggers is done to identi-
fy frequent confusions in tagging. Based on
the analysis of tagging, and syntactic structure
of Urdu, a more refined tagset is derived. The
existing tagged corpora are tagged with the
new tagset to develop a single corpus of 230K
words and the TnT tagger is retrained. The re-
sults show improvement in tagging accuracy
for individual corpora to 94.2% and also for
the merged corpus to 91%. Implications of
these results are discussed.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999845357142857">
There is increasing amount of work on computa-
tional modeling of Urdu language. As various
groups work on the language, diversity in analy-
sis is also developed. In this context, there has
been some work on Urdu part of speech (POS)
tagging, which has caused multiple tagsets to
appear. Thus, there is also need to converge
these efforts.
Current work compares the existing tag-
sets of Urdu being used for tagging corpora in an
attempt to look at the differences, and understand
the reasons for the variation. The work then un-
dertakes experiments to develop a common tag-
set, which is syntactically and computationally
coherent. The aim is to make a robust tagset and
then to port the differently tagged Urdu corpora
onto the same tagset. As Urdu already has very
few annotated corpora, this will help consolidat-
ing them for better modeling.
The next sections present the existing tag-
sets and accuracies of the POS taggers reported
using them. Sections 4 and 5 present baseline
experiment and the methodology used for the
analysis for updating the tagset. Section 6 de-
scribes the proposed tagset. Section 7 reports
experiments comparing the new tagset with ex-
isting ones. Section 8 discusses the results
achieved and future directions.
</bodyText>
<sectionHeader confidence="0.989115" genericHeader="introduction">
2 Relevant Resources of Urdu
</sectionHeader>
<subsectionHeader confidence="0.866736">
2.1 Urdu Corpora
</subsectionHeader>
<bodyText confidence="0.999852033333333">
Several annotated corpora have been built during
last few years to facilitate computational
processing for Urdu language. The initial work
was undertaken through EMILLE project to
build multi-lingual corpora for South Asian lan-
guages (McEnery et al., 2000). They released
200,000 words parallel corpus of English, Urdu,
Bengali, Gujarati, Hindi and Punjabi. In addition,
there are 1,640,000 words of Urdu text in this
corpus. These text collections are also annotated
with part of speech tags (Hardie 2003).
Center for Research in Urdu Language
Processing (CRULP1) gathered 18 million words
corpus in order to build a lexicon. It has cleaned
text from news websites from multiple domains
(Ijaz et.al. 2007). Following this work, a syntac-
tic tagset was developed based on work by exist-
ing grammarians and a corpus of 110,000 words
was manually tagged. This annotated corpus is
available through the center (Sajjad 2007, Hus-
sain 2008).
Recently an English-Urdu parallel cor-
pus has also been developed by CRULP, by
translating the first 140,000 words of PENN
Treebank corpus. In addition, a tagset has also
been designed following the PENN Treebank
guidelines. These words have been tagged ma-
nually with this new tagset. This collection is
also available from CRULP, and the tagset is still
unpublished.
</bodyText>
<subsectionHeader confidence="0.997287">
2.2 Urdu Part of Speech tagsets
</subsectionHeader>
<bodyText confidence="0.999925">
Hardie (2003) developed the first POS tagset for
Urdu using EAGLES guidelines for computa-
tional processing. The tagset contains 282 mor-
pho-syntactic tags, differentiating on the basis of
number, gender and other morphological details
</bodyText>
<footnote confidence="0.986782">
1 www.crulp.org
</footnote>
<page confidence="0.986079">
24
</page>
<note confidence="0.727304">
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 24–31,
Suntec, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999300113924051">
in addition to the syntactic categories. Punctua-
tion marks are tagged as they are, and not in-
cluded in 282 tags. The tags include the gender
and number agreement, in addition to syntactic
information.
The complications of Urdu tagset design
are also discussed. One of these complexities is
word segmentation issue of the language. Suffix-
es in Urdu are written with an orthographic
space. Words are separated on the basis of space
and so suffixes are treated same as lexical words.
Hence it is hard to assign accurate tag for an au-
tomatic tagger. Although the tagset is designed
considering details, but due to larger number of
tags it is hard to get a high accuracy with a small
sized corpus. Due to its morphological depen-
dence and its large size, this tagset is not consi-
dered in our analysis.
Two much smaller tagsets are considered
for this work. They are compared in detail in
Section 6. The first tagset, containing 42 tags, is
designed by Sajjad (2007), based on the work of
Urdu grammarians (e.g. Schmidt 1999, Haq
1987, Javed 1981, Platts 1909) and computation-
al work by Hardie (2003). The main features of
the tagset include multiple pronouns (PP, KP,
AKP, AP, RP, REP, G, and GR) and demonstra-
tives (PD, KD, AD, and RD). It has only one tag
for all forms of verbs (VB), except for auxiliaries
to show aspect (AA) and tense (TA) information
about the verb. All noun types are assigned sin-
gle tag (NN) except for Proper Nouns (PN). It
also has a special tag NEG to mark any occur-
rence negation words (� “not” and �� “no” or
“neither”) regardless of context. It also has a tag
SE to mark every occurrence of _� (“from”)
without considering the context. Another exam-
ple of such a context-free lexical tag is WALA to
mark every occurrence (including all the inflec-
tions) of the word 219. This tagset is referred to as
T1 subsequently in this paper.
Recently Sajjad and Schmid (2009) used
the tagged data of 107,514 words and carried out
an experiment for tagger comparison. A total of
100,000 words are used as training set and rest as
test data. Four taggers (TnT, Tree, RF and SVM)
are trained using training corpus and then tested
accordingly. Reported results of this work show
that SVM tagger is the most accurate, showing
94.15% correct prediction of tags. Remaining
three taggers have accuracies of 93.02% (Tree
tagger), 93.28% (RF tagger) and 93.40% (TnT
tagger).
Another tagset has recently been devel-
oped as a part of a project to develop English-
Urdu parallel corpus at CRULP, following the
Penn Treebank guidelines (Santorini 1990). It
contains 46 tags, with fewer grades of pronouns
(PR, PRP$, PRRF, PRRFP$, and PRRL) and
demonstratives (DM and DMRL), as compared
to T1. It has several tags for verbs on the basis of
their forms and semantics (VB, VBI, VBL,
VBLI, and VBT) in addition to the tags for aux-
iliaries showing aspect (AUXA) and tense
(AUXT). The NN tag is assigned for both singu-
lar and plural nouns and includes adverbial kaf
pronoun, kaf pronoun, and adverbial pronoun
categories of T1. Yet, it has several other grades
of common nouns (NNC, NNCR, NNCM). It
also has two shades of Proper Nouns (NNP ,
NNPC), which are helpful in identifying phrase
boundary of compound proper nouns. It also has
a tag WALA that is assigned to every occurrence
(and inflection) of word 219 (wala). However,
marking of token _ (“from”) is context depen-
dent: either it is CM when marking case or it is
RBRP when occurring as an adverbial particle.
This tagset is referred to as T2 subsequently in
this paper.
</bodyText>
<sectionHeader confidence="0.973687" genericHeader="method">
3 Tools and Resource Selection
</sectionHeader>
<bodyText confidence="0.9997552">
The decision of selecting the tagger, the tagset,
and the data is the starting point for the task of
POS tagging. This section gives details of the
taggers chosen and the corpora used for the expe-
riments conducted.
</bodyText>
<subsectionHeader confidence="0.999977">
3.1 Selection of taggers
</subsectionHeader>
<bodyText confidence="0.999703117647059">
There are a number of existing taggers available
for tagging. Two POS taggers are used in the
initial step of this work to compare the initial
tagging accuracies.
One of the selected taggers is Trigram-
and-Tag (TnT). It is a trigram based HMM tag-
ger in which two preceding tags are used to find
the transition probability of a tag. Brants (2000)
tested PENN Treebank (English) and NEGRA
(German) corpora and reported 96-97% accuracy
of the tagger.
Schmid (1994) proposed probabilistic
POS tagger that uses decision trees to store the
transition probabilities. The trained decision tree
is used for identification of highest probable tags.
Schmid reported an accuracy of 95-96% on
PENN Treebank for this tagger.
</bodyText>
<page confidence="0.987922">
25
</page>
<bodyText confidence="0.999944333333333">
Both taggers give good accuracy for Ur-
du tagging, as reported by Sajjad and Schmid
(2009).
</bodyText>
<subsectionHeader confidence="0.999616">
3.2 Data Used for Experimentation
</subsectionHeader>
<bodyText confidence="0.999718272727273">
Corpora annotated with the different tagsets are
acquired from CRULP. The corpus originally
tagged with T1 tagset is referred to as C1 (news
from non-business domain) and the corpus in-
itially annotated with T2 tagset is referred to as
C2 (news from business domain), subsequently
in the current work. Both C1 and C2 are taken
and cleaned. The data is re-counted and approx-
imately 100,000 words are separated for training
and rest are kept for testing. The details of data
are given in Tables 1 and 2 below.
</bodyText>
<tableCaption confidence="0.998448">
Table 1. Number of tokens in Urdu corpora
</tableCaption>
<table confidence="0.998695">
Tokens C1 C2
Training 101,428 102,454
Testing 8,670 21,181
Total 110,098 123,635
</table>
<tableCaption confidence="0.99694">
Table 2. Number of sentences in Urdu corpora
</tableCaption>
<table confidence="0.988275">
Sentences C1 C2
Training 4,584 3,509
Testing 404 755
Total 4,988 4,264
</table>
<sectionHeader confidence="0.985994" genericHeader="method">
4 Baseline Estimation
</sectionHeader>
<bodyText confidence="0.926167083333333">
The comparison is initiated with training of ex-
isting tagsets on their respective annotated data
(T1 on C1 and T2 on C2). Both corpora are
tested on TnT and Tree Tagger to obtain the con-
fusion matrices for errors. These confusion ma-
trices are used to analyze misclassification of
tags. TnT tagger shows that overall accuracy of
using T1 with C1 is 93.01% and is significantly
better than using T2 with C2, which gives
88.13% accuracy. Tree tagger is also trained on
the corpora. The overall accuracy of T1 on C1
(93.37%) is better than that of T2 on C2
(90.49%). The results are shown in Table 3.
Table 3. Results of both tagsets on their respec-
tive corpora with TnT and Tree taggers
T1 on C1 T2 on C2
TnT Tagger 93.01% 88.13%
Tree Tagger 93.37% 90.49%
The accuracies reported (for T1 on C1) by
Sajjad and Schmid (2009) are comparable to
these accuracies. They have reported 93.40% for
TnT Tagger and 93.02% for Tree Tagger.
Further experimentation is performed only
using TnT tagger.
</bodyText>
<sectionHeader confidence="0.995644" genericHeader="method">
5 Methodology
</sectionHeader>
<bodyText confidence="0.997222555555556">
The current work aims to build a larger corpus of
around 230,000 manually tagged words for Urdu
by combining C1 and C2. These collections are
initially annotated with two different tagsets (T1
and T2 respectively, and as described above).
For this unification, it was necessary to indentify
the differences in the tagsets on which these cor-
pora are annotated, analyzed the differences and
then port them to unified tagset.
The work starts with the baseline estimation
(described in Section 4 above). The results of
baseline estimation are used to derive a new tag-
set (detailed in Section 6 below), referred to as
T3 in this paper. Then a series of experiments are
executed to compare the performance of three
tagsets (T1, T2, and T3) on data from two differ-
ent domains (C1 and C2), as reported in Section
7 below and summarized in Table 4.
</bodyText>
<tableCaption confidence="0.997099">
Table 4. Summary of experiments conducted
</tableCaption>
<table confidence="0.99917125">
Experiment Tagset Corpus
0 Baseline Estimation: T1 C1
Original tagsets with
respective corpora
T2 C2
1 Experiment1: For T3 C1
comparison of results
of T1 and T3 on C1
2 Experiment2: For T3 C2
comparison of T1, T2
and T3 on C2
T1 C2
3 Experiment3: Compar- T3 C2
ison of T1 and T3 with
no unknowns
T1 C2
4 Experiment4: Compar- T3 C1+C2
ison of T1 and T3 over
complete corpus
T1 C1+C2
</table>
<bodyText confidence="0.998644333333333">
The performance of T1 on C1 is already
better than T2 on C2, so the first comparison for
the merged tagset T3 is with T1 on C1, which is
the basis of the first experiment. Then the per-
formance of better performing tagsets (T1 and
T3) are compared on the corpus C2 in the second
</bodyText>
<page confidence="0.978995">
26
</page>
<bodyText confidence="0.9998975">
experiment to compare them with T2. One possi-
ble reason of relatively better performance could
be the difference in application of open classes
for unknown words in the test data. Therefore,
the third experiment is performed using the same
data as in second experiment (i.e. corpus C2)
with combined lexicon of training and test data
(i.e. no unknown words). Finally, an experiment
is conducted with the merged corpus. Following
table summarizes these experiments.
</bodyText>
<sectionHeader confidence="0.982028" genericHeader="method">
6 Tagset design
</sectionHeader>
<bodyText confidence="0.999172">
After establishing the baseline, the existing tag-
sets are reviewed with the following guidelines:
</bodyText>
<listItem confidence="0.961702333333333">
• Focus on the syntactic variation (instead of
morphological or semantic motivation) to ei-
ther collapse existing tags or introduce new
ones
• Focus on word level tagging and not try to ac-
commodate phrase level tagging (e.g. to sup-
port chunking, compounding or other similar
tasks)
• Tag according to the syntactic role instead of
having a fixed tag for a string, where possible
• Use PENN Treebank nomenclature to keep the
tagset easy to follow and share
</listItem>
<bodyText confidence="0.999310705882353">
Comparison of T1 and T2 showed that there
are 33 tags in both tagsets which represent same
syntactic categories, as shown in Appendix A.
The tag I (Intensifier) in T2 labels the words
which are marked as ADV in T1. The words an-
notated as NNC, NNCR and NNCM (under T2)
are all labeled as NN under T1. The words
tagged as VBL, VBLI, VBI, and VBLI (under
T2) are all labeled as VB under T1. Range of
distinct tags for demonstratives of T1 are all
mapped to DM in T2 except RD (of T1) which
maps to DMRL (of T2).
In order to identify the issues in tagging, a
detailed error analysis of existing tagsets is per-
formed. Following tables represent the major tag
confusions for tagging C2 with T2 using Tree
and TnT taggers.
</bodyText>
<tableCaption confidence="0.767089">
Table 5. Major misclassifications in C2 with T2
tagset using Tree tagger
</tableCaption>
<table confidence="0.96175575">
Tag Total Errors Maximum
tokens misclassification
VB 888 214 183 VBL
VBL 328 168 151 VB
VBI 202 47 38 VBLI
VBLI 173 52 46 VBI
AUXT 806 145 121 VBT
Table 6. Major misclassifications in C2 with T2
tagset using TnT-tagger
Tag Total Error Maximum
tokens misclassification
VB 888 240 181 VBL
VBL 328 154 135 VB
VBI 202 46 34 VBLI
VBLI 173 61 55 VBI
AUXT 806 136 111 VBT
</table>
<bodyText confidence="0.999509977777778">
The proposed tagset for Urdu part-of-
speech tagging contains 32 tags. The construc-
tion of new tagset (T3) is initiated by adopting
T2 as the baseline, because T2 uses the tagging
conventions of PENN Treebank. There are 17
tags in T3 that are same as in T1 and T2. These
tags (CC, CD, DM, DMRL, JJ, NN, OD, PM,
PRP, PRP$, PRRF, PRRF$, PRRL, Q, RB, SM,
SYM) are not discussed in detail. The complete
tagset along with short description and examples
of each tag is given in Appendix B.
RBRP (Adverbial Particle) and CM
(Case Marker) are merged to make up a new tag
PP (Postposition), so every postposition particle
comes under this new tag ignoring semantic con-
text. I (Intensifier) is used to mark the intensifi-
cation of an adjective, which is a semantic grada-
tion, and syntactically merged with Q (Quantifi-
er). NNCM (Noun after Case Marker), NNC
(Noun Continuation), NNCR (Continuing Noun
Termination) are merged into NN (Noun) be-
cause syntactically they always behave similarly
and the difference is motivated by phrase level
marking. U (Unit) is also merged with NN be-
cause the difference is semantically motivated.
DATE is not syntactic, and may be either
treated as NN (Noun) or CD (Cardinal), depend-
ing upon the context. Similarly, R (Reduplica-
tion), MOPE (Meaningless Pre-word), and MO-
PO (Meaningless Post-word) always occur in
pair with NN, JJ, or another tag. Thus they are
phrasal level tags, and can be replaced by rele-
vant word level tag in context. NNPC (Proper
Noun Continuation) tag identifies compounding
but syntactically behaves as NNP (Proper Noun),
and is not used.
VBL (Light Verb) is used in complex predi-
cates (Butt 1995), but its syntactic similarity with
VB (Verb) is a major source of confusion in au-
tomatic tagging. It is collapsed with VB (Verb).
Similarly, VBLI (Light Verb Infinitive) is
merged with VBI (Verb Infinitive). AUXT
(Tense Auxiliary) is highly misclassified as VBT
(To be Verb) because both occur as last token in
a clause or sentence, and both include tense in-
</bodyText>
<page confidence="0.995481">
27
</page>
<bodyText confidence="0.999487352941177">
formation. The word is labeled as VBT only
when there is no other verb in the sentence or
clause, otherwise these words are tagged as
AUXT. The syntactic similarity of both tags is
also evident from statistically misclassifying
AUXT as VBT. Therefore both are collapsed
into single tag VBT (Tense Verb).
In T1, NEG (Negation) is used to mark all
the negation words without context, but they
mostly occur as adverbs. Therefore, NEG tag is
removed. Similarly, SE (Postposition 5 ,
“from”) is not separated from postpositions and
marked accordingly. PRT (Pre-Title) and POT
(Post-Title) always occur before or after Proper
Noun, respectively. Therefore, they behave as
Proper Nouns, hence proposed to be labeled as
NNP (Proper Noun).
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="method">
7 Experiments
</sectionHeader>
<bodyText confidence="0.999981909090909">
After designing a new tagset, a series of experi-
ments are conducted to investigate the proposed
changes. The rationale of the sequence of expe-
riments has been discussed in Section 5 above,
however the reasoning for each experiment is
also given below. As T2 tags have much more
semantic and phrasal information, and C2 tagged
with T2 shows lower accuracy than T1 on C1,
therefore further experiments are conducted to
compare the performance of T1 and T3 only.
Comparisons on C2 with T3 may also be drawn.
</bodyText>
<subsectionHeader confidence="0.964293">
7.1 Experiment 1
</subsectionHeader>
<bodyText confidence="0.9997435">
As baseline estimation shows that T1 on C1 out-
performs T2 on C2, the first experiment is to
compare the performance of T3 on C1. In this
experiment C1 is semi-automatically tagged with
T3. TnT tagger is then trained and tested. T3
gives 93.44% accuracy, which is slightly better
than the results already obtained for T1
(93.01%). The results are summarized in Table 7.
</bodyText>
<tableCaption confidence="0.992708">
Table 7. Accuracies of T3 and T1 on C1
</tableCaption>
<table confidence="0.948575666666667">
Corpus Tagset Accuracy
C1 T3 93.44%
C1 T1 93.01%
</table>
<subsectionHeader confidence="0.833744">
7.2 Experiment 2
</subsectionHeader>
<bodyText confidence="0.999315916666667">
Now to test the effect of change in domain of the
corpus, the performance T1 and T3 on C2 is
compared in this experiment. C2 is manually
tagged with T3, then trained and tested using
TnT tagger. The results obtained with T3 are
91.98%, which are significantly better than the
results already obtained for T2 on C2 (88.13%).
C2 is also semi-automatically re-tagged
with T1. T1 shows better performance (91.31%)
than T2 (88.13%). However, the accuracy of us-
ing T3 (on C2) is still slightly higher. The re-
sults are summarized in Table 8.
</bodyText>
<tableCaption confidence="0.99779">
Table 8. Accuracies of T3 on C1, and accura-
</tableCaption>
<table confidence="0.8953345">
cies of T3 and T1 on C2
Corpus Tagset Accuracy
C2 T3 91.98%
C2 T1 91.31%
</table>
<subsectionHeader confidence="0.916315">
7.3 Experiment 3
</subsectionHeader>
<bodyText confidence="0.9999628">
Due to the change in open class set there may be
a difference of performance on unknown words,
therefore in this experiment, all the unknown
words of test set are also included in the vocabu-
lary. This experiment again involves T3 and T1
with C2. Combined lexica are built using testing
and training parts of the corpus, to eliminate the
factor of unknown words. This experiment also
shows that T3 performs better than T1, as shown
in Table 9.
</bodyText>
<tableCaption confidence="0.9703545">
Table 9. Accuracies of T3 and T1 with ALL
known words in test data
</tableCaption>
<table confidence="0.969676666666667">
Corpus Tagset Accuracy
C2 T3 94.21%
C2 T1 93.47%
</table>
<subsectionHeader confidence="0.950882">
7.4 Experiment 4
</subsectionHeader>
<bodyText confidence="0.99985625">
Finally both corpora (C1 and C2) were com-
bined, forming a training set of 203,882 words
and a test set of 29,851 words. The lexica are
generated only from the training set. Then TnT
tagger is trained separately for both T1 and T3
tagsets and the accuracies are compared. The
results show that T3 gives better tagging accura-
cy, as shown in Table 10.
</bodyText>
<tableCaption confidence="0.9373405">
Table 10. Accuracies of T3 and T1 using
combined C1 and C2 corpora
</tableCaption>
<table confidence="0.949659">
Corpus Tagset Accuracy
C1+C2 T3 90.99%
C1+C2 T1 90.00%
</table>
<bodyText confidence="0.5410895">
Partial confusion matrices for both the tag-
sets are given in Tables 11 and 12.
</bodyText>
<page confidence="0.997385">
28
</page>
<bodyText confidence="0.999120666666667">
The error analysis shows that the accuracy
drops for both tagsets when trained on multi-
domain corpus, which is expected. The highest
error count is for the confusion between noun
and adjective. There is also confusion between
proper and common nouns. T3 also gives signif-
icant confusion between personal pronouns and
demonstratives, as they represent the same lexi-
cal entries.
</bodyText>
<tableCaption confidence="0.984729">
Table 11. Major misclassifications in merged
corpus with T1 using TnT tagger
</tableCaption>
<table confidence="0.99994652631579">
Tag Total Error Maximum
tokens misclassification
A 18 5 3 ADJ
AD 18 7 4 ADJ
ADJ 2510 551 371 NN
ADV 431 165 59 ADJ
INT 8 6 6 ADV
KD 16 9 6 Q
KER 77 28 19 P
NN 7642 548 218 PN
OR 75 24 9 Q
PD 205 55 12 PP
PN 2246 385 264 NN
PP 239 51 11 PD
Q 324 119 53 ADJ
QW 24 12 11 VB
RD 71 62 61 RP
RP 11 5 2 NN
U 24 8 8 NN
</table>
<tableCaption confidence="0.9921175">
Table 12. Major misclassifications in merged
corpus with T3 using TnT tagger
</tableCaption>
<table confidence="0.999878416666667">
Tag Total Error Maximum
tokens misclassification
CVRP 77 24 15 PP
DM 242 77 58 PRP
DMRL 71 64 63 PRRL
INJ 8 6 6 RB
JJ 2510 547 376 NN
JJRP 18 4 4 JJ
NN 7830 589 234 NNP
NNP 2339 390 267 NN
OD 75 23 8 JJ
PRP 642 119 33 DM
</table>
<sectionHeader confidence="0.969824" genericHeader="method">
8 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.999880803921569">
The current work looks at the existing tagsets of
Urdu being used for tagging corpora and analyz-
es them from two perspectives. First, the tagsets
are analyzed to see their linguistic level differ-
ences. Second, they are compared based on their
inter-tag confusion after training with two differ-
ent POS taggers. These analyses are used to de-
rive a more robust tagset.
The results show that collapsing categories
which are not syntactically motivated improves
the tagging accuracy in general. Specifically,
light and regular verbs are merged, because they
may come in similar syntactic frames. Redupli-
cated categories are given the same category tag
(instead of a special repetition tag). Units and
dates are also not considered separately as the
differences have been semantically motivated
and they can be categorized with existing tags at
syntactic level.
Though, the measuring unit is currently
treated as a noun, it could be collapsed as an ad-
jective as well. The difference is sometimes lex-
ical, where kilogram is more adjectival, vs.
minute is more nominal in nature in Urdu,
though both are units.
NNP (Proper Noun) tag could also have
been collapsed with NN (Common Noun), as
Urdu does not make clear between them at syn-
tactic level. However, these two tags are kept
separate due to their cross-linguistic importance.
One may expect that extending the genre or
domain of corpus reduces accuracy of tagging
because of increase in the variety in the syntactic
patterns and diverse use of lexical items. One
may also expect more accuracy with increase in
size. The current results show that effect on ad-
ditional domain (when C1 and C2 are mixed) is
more pronounced than the increase in size (from
approximately 100k to 200k), reducing accuracy
from 94.21% (T3 with C2) to 90.99% (T3 with
C1 + C2). The increase in accuracy for T3 vs.
T1 may be caused by reduced size of T3. How-
ever, the proposed reduction does not compro-
mise the syntactic word level information, as the
collapsed categories are where they were either
semantically motivated or motivated due to
phrasal level tags.
The work has been motivated to consolidate
the existing Urdu corpora annotated with differ-
ent tagsets. This consolidation will help build
more robust computational models for Urdu.
</bodyText>
<sectionHeader confidence="0.998572" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.974115">
Brants, T. 2000. TnT – A statistical part-of-speech
tagger. Proceedings of the Sixth Applied Natural
Language Processing Conference ANLP-2000
Seattle, WA, USA.
</reference>
<page confidence="0.990231">
29
</page>
<reference confidence="0.988398836734694">
Butt, M. 1995. The structure of complex predicates
in Urdu. CSLI, USA. ISBN: 1881526585.
Haq, A,. 1987. \ن و فD ودرا Amjuman-e-Taraqqi
Urdu.
Hardie, A. 2003. Developing a tag-set for automated
part-of-speech tagging in Urdu. Archer, D, Rayson,
P, Wilson, A, and McEnery, T (eds.) Proceedings
of the Corpus Linguistics 2003 conference. UCREL
Technical Papers Volume 16. Department of Lin-
guistics, Lancaster University, UK.
Hussain, S. 2008. Resources for Urdu Language
Processing. The Proceedings of the 6th Workshop
on Asian Language Resources, IJCNLP’08, IIIT
Hyderabad, India.
Ijaz, M. and Hussain, S. 2007. Corpus Based Urdu
Lexicon Development. The Proceedings of Confe-
rence on Language Technology (CLT07), Universi-
ty of Peshawar, Pakistan.
Javed, I. 1981. &lt;اK ودرا t5ن Taraqqi Urdu Bureau,
New Delhi, India.
Platts, J. 1909. A grammar of the Hindustani or Urdu
language. Reprinted by Sang-e-Meel Publishers,
Lahore, Pakistan.
Sajjad, H. 2007. Statistical Part of Speech Tagger for
Urdu. Unpublished MS Thesis, National Universi-
ty of Computer and Emerging Sciences, Lahore,
Pakistan.
Sajjad, H. and Schmid, H. 2009. Tagging Urdu Text
with Parts Of Speech: A Tagger Comparison.12th
conference of the European chapter of the associa-
tion for computational Linguistics
Santorini, B. 1990. Part_of_Speech Tagging Guide-
lines for the Penn Treebank Project (3rd printing,
2nd revision). Accessed from
ftp://ftp.cis.upenn.edu/pub/treebank/doc/tagguide.ps.gz
on 3rd May, 2009.
Schmid, H. 1994. Probabilistic part-of-speech tag-
ging using decision trees. Institut für Maschinelle
Sprachverarbeitung, Universität Stuttgart, Germa-
ny.
Schmidt, R. 1999. Urdu: an essential grammar. Rout-
ledge, London, UK.
McEnery, A., Baker, J. Gaizauskas, R. and Cunning-
ham, H. 2000. EMILLE: towards a corpus of South
Asian languages, British Computing Society Ma-
chine Translation Specialist Group, London, UK.
Appendix A: Mappings of tags between Tag-
sets T1 and T2.
Tagset T1 Tagset T2
</reference>
<sectionHeader confidence="0.906645" genericHeader="method">
1. A JJRP
2. AA AUXA
3. ADJ JJ
4. ADV RB
5. CA CD
6. CC CC
7. DATE DATE
8. EXP SYM
9. FR FR
10. G PRP$
11. GR PRRFP$
12. I ITRP
13. INT INJ
</sectionHeader>
<reference confidence="0.97807365">
14. KER KER
15. MUL MUL
16. NN NN
17. OR OD
18. P CM
19. PD DM
20. PM PM
21. PN NNP
22. PP PR
23. Q Q
24. QW QW
25. RD DMRL
26. REP PRRL
27. RP PRRF
28. SC SC
29. SE RBRP
30. SM SM
31. TA AUXT
32. U U
33. WALA WALA
</reference>
<page confidence="0.996425">
30
</page>
<sectionHeader confidence="0.736107" genericHeader="method">
Appendix B: New Tagset T3.
</sectionHeader>
<table confidence="0.9988934">
Tag Meaning Example
AUX Auxiliary P 5hB L gTWO May
CC Coordinate Conjunction ōƁرذ 5 ںوراد ہlH t5Oh@ ی sYOزO Or
CD Cardinal Lرŷادا ہد&gt;O ی ا One
CVRP Conjunctive Verb Par- ن t5ار 7 5Q6 زWJ tŲ6 L vYƇ ōjK tţY^ناJ After
ticle
DM Demonstrative 586ا655Pن تdKاو Ňی ا Ŵœƽ7 Like this
DMRL Demonstrative Relative 5Ź? 8 لB 23 ہو &gt; 5 ہرادا t5HCا ہو That
Yý
FR Fraction YO 5WmM 5 دآ Half
INJ Interjection 5 ت6 YL !ہ او Hurrah
ITRP Intensive Particle او 5˜ر t56 t5ن m8 اlM ن Too
JJ Adjective 8B 5 ںTM 8 Wj6 Taller
JJRP Adjective Particle Yý 5hB چB L تý&gt;و t5 pl6 t5 5˜ر ý6 As
MRP Multiplicative Particle Kر t5Mد Double
NN Noun 7 ںPاJا YO زIآ 5 لB Year
NNP Proper Noun lL 5 ٹ6ار Robert
OD Ordinal 6`WO qWO&lt;UƎر l7 First
PM Phrase Marker ، ,
PP Postposition 5P 5Q6 8 ن pYWLر ڈر6 To
PRP Pronoun Personal یL W^7 نP ýا 5 لkdTBا L رŷ یF ہو They
5
PRP$ Pronoun Personal 5tŲ?ا WYM YƉ یYO My
Possessive
PRRF Pronoun Reflexive t5]6 L پآ 5ƀا 5 t5SkL Oneself
PRRF$ Pronoun Reflexive 8Jد t5kT&gt;ا 5ƀ ا Own
Possessive
PRRL Pronoun Relative 5Ź? 8 لB 23 ہو &gt; 5 ہرادا t5HCا ہو That
Yý
Q Quantitative گN W? Some
QW Question Word Ŷ ےL sYgی ںYL W`O یا Why
RB Adverb t5M t5YƇ _YkP Always
SC Subordinate Conjunction ںیLن [L hنYL t5 5Lر WTL Because
SM Sentence Marker ؟ ?
SYM any Symbol $ $
VB Verb 58 5˜? ےSL 5WlO Wanted
VBI Verb Infinitive form 5N 5 5&gt; 5 5ا To go
VBT Verb Tense 5 kH 6K ر `8 Is
WALA Association Marking 5او Ŵ5mLر Associated
Morpheme او 5L یر&gt; Bearing
</table>
<page confidence="0.999365">
31
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.031598">
<title confidence="0.970147">Analysis and Development of Urdu POS Tagged Corpus</title>
<author confidence="0.888111">Ahmed</author>
<affiliation confidence="0.850972">Center for Research in Language NUCES,</affiliation>
<email confidence="0.885488">ahmed.muaz@nu.edu.pk</email>
<affiliation confidence="0.638541">Aasim Center for Research in Language NUCES,</affiliation>
<email confidence="0.868052">aasim.ali@nu.edu.pk</email>
<author confidence="0.36901">Sarmad</author>
<affiliation confidence="0.805834">Center for Research in Language NUCES,</affiliation>
<email confidence="0.972053">sarmad.hussain@nu.edu.pk</email>
<abstract confidence="0.9995844">In this paper, two corpora of Urdu (with 110K and 120K words) tagged with different POS tagsets are used to train TnT and Tree taggers. Error analysis of both taggers is done to identify frequent confusions in tagging. Based on the analysis of tagging, and syntactic structure of Urdu, a more refined tagset is derived. The existing tagged corpora are tagged with the new tagset to develop a single corpus of 230K words and the TnT tagger is retrained. The results show improvement in tagging accuracy for individual corpora to 94.2% and also for the merged corpus to 91%. Implications of these results are discussed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Brants</author>
</authors>
<title>TnT – A statistical part-of-speech tagger.</title>
<date>2000</date>
<booktitle>Proceedings of the Sixth Applied Natural Language Processing Conference ANLP-2000</booktitle>
<location>Seattle, WA, USA.</location>
<contexts>
<context position="8135" citStr="Brants (2000)" startWordPosition="1365" endWordPosition="1366">Tools and Resource Selection The decision of selecting the tagger, the tagset, and the data is the starting point for the task of POS tagging. This section gives details of the taggers chosen and the corpora used for the experiments conducted. 3.1 Selection of taggers There are a number of existing taggers available for tagging. Two POS taggers are used in the initial step of this work to compare the initial tagging accuracies. One of the selected taggers is Trigramand-Tag (TnT). It is a trigram based HMM tagger in which two preceding tags are used to find the transition probability of a tag. Brants (2000) tested PENN Treebank (English) and NEGRA (German) corpora and reported 96-97% accuracy of the tagger. Schmid (1994) proposed probabilistic POS tagger that uses decision trees to store the transition probabilities. The trained decision tree is used for identification of highest probable tags. Schmid reported an accuracy of 95-96% on PENN Treebank for this tagger. 25 Both taggers give good accuracy for Urdu tagging, as reported by Sajjad and Schmid (2009). 3.2 Data Used for Experimentation Corpora annotated with the different tagsets are acquired from CRULP. The corpus originally tagged with T1</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Brants, T. 2000. TnT – A statistical part-of-speech tagger. Proceedings of the Sixth Applied Natural Language Processing Conference ANLP-2000 Seattle, WA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Butt</author>
</authors>
<title>The structure of complex predicates</title>
<date>1995</date>
<booktitle>in Urdu. CSLI, USA. ISBN:</booktitle>
<pages>1881526585</pages>
<contexts>
<context position="15770" citStr="Butt 1995" startWordPosition="2701" endWordPosition="2702">g. U (Unit) is also merged with NN because the difference is semantically motivated. DATE is not syntactic, and may be either treated as NN (Noun) or CD (Cardinal), depending upon the context. Similarly, R (Reduplication), MOPE (Meaningless Pre-word), and MOPO (Meaningless Post-word) always occur in pair with NN, JJ, or another tag. Thus they are phrasal level tags, and can be replaced by relevant word level tag in context. NNPC (Proper Noun Continuation) tag identifies compounding but syntactically behaves as NNP (Proper Noun), and is not used. VBL (Light Verb) is used in complex predicates (Butt 1995), but its syntactic similarity with VB (Verb) is a major source of confusion in automatic tagging. It is collapsed with VB (Verb). Similarly, VBLI (Light Verb Infinitive) is merged with VBI (Verb Infinitive). AUXT (Tense Auxiliary) is highly misclassified as VBT (To be Verb) because both occur as last token in a clause or sentence, and both include tense in27 formation. The word is labeled as VBT only when there is no other verb in the sentence or clause, otherwise these words are tagged as AUXT. The syntactic similarity of both tags is also evident from statistically misclassifying AUXT as VB</context>
</contexts>
<marker>Butt, 1995</marker>
<rawString>Butt, M. 1995. The structure of complex predicates in Urdu. CSLI, USA. ISBN: 1881526585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haq</author>
</authors>
<date>1987</date>
<note>D ودرا Amjuman-e-Taraqqi Urdu.</note>
<contexts>
<context position="5027" citStr="Haq 1987" startWordPosition="817" endWordPosition="818">s of space and so suffixes are treated same as lexical words. Hence it is hard to assign accurate tag for an automatic tagger. Although the tagset is designed considering details, but due to larger number of tags it is hard to get a high accuracy with a small sized corpus. Due to its morphological dependence and its large size, this tagset is not considered in our analysis. Two much smaller tagsets are considered for this work. They are compared in detail in Section 6. The first tagset, containing 42 tags, is designed by Sajjad (2007), based on the work of Urdu grammarians (e.g. Schmidt 1999, Haq 1987, Javed 1981, Platts 1909) and computational work by Hardie (2003). The main features of the tagset include multiple pronouns (PP, KP, AKP, AP, RP, REP, G, and GR) and demonstratives (PD, KD, AD, and RD). It has only one tag for all forms of verbs (VB), except for auxiliaries to show aspect (AA) and tense (TA) information about the verb. All noun types are assigned single tag (NN) except for Proper Nouns (PN). It also has a special tag NEG to mark any occurrence negation words (� “not” and �� “no” or “neither”) regardless of context. It also has a tag SE to mark every occurrence of _� (“from”)</context>
</contexts>
<marker>Haq, 1987</marker>
<rawString>Haq, A,. 1987. \ن و فD ودرا Amjuman-e-Taraqqi Urdu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hardie</author>
</authors>
<title>Developing a tag-set for automated part-of-speech tagging</title>
<date>2003</date>
<booktitle>Proceedings of the Corpus Linguistics 2003 conference. UCREL Technical Papers Volume 16. Department of Linguistics,</booktitle>
<editor>in Urdu. Archer, D, Rayson, P, Wilson, A, and McEnery, T (eds.)</editor>
<publisher>University, UK.</publisher>
<location>Lancaster</location>
<contexts>
<context position="2772" citStr="Hardie 2003" startWordPosition="443" endWordPosition="444">ion 8 discusses the results achieved and future directions. 2 Relevant Resources of Urdu 2.1 Urdu Corpora Several annotated corpora have been built during last few years to facilitate computational processing for Urdu language. The initial work was undertaken through EMILLE project to build multi-lingual corpora for South Asian languages (McEnery et al., 2000). They released 200,000 words parallel corpus of English, Urdu, Bengali, Gujarati, Hindi and Punjabi. In addition, there are 1,640,000 words of Urdu text in this corpus. These text collections are also annotated with part of speech tags (Hardie 2003). Center for Research in Urdu Language Processing (CRULP1) gathered 18 million words corpus in order to build a lexicon. It has cleaned text from news websites from multiple domains (Ijaz et.al. 2007). Following this work, a syntactic tagset was developed based on work by existing grammarians and a corpus of 110,000 words was manually tagged. This annotated corpus is available through the center (Sajjad 2007, Hussain 2008). Recently an English-Urdu parallel corpus has also been developed by CRULP, by translating the first 140,000 words of PENN Treebank corpus. In addition, a tagset has also be</context>
<context position="5093" citStr="Hardie (2003)" startWordPosition="828" endWordPosition="829"> Hence it is hard to assign accurate tag for an automatic tagger. Although the tagset is designed considering details, but due to larger number of tags it is hard to get a high accuracy with a small sized corpus. Due to its morphological dependence and its large size, this tagset is not considered in our analysis. Two much smaller tagsets are considered for this work. They are compared in detail in Section 6. The first tagset, containing 42 tags, is designed by Sajjad (2007), based on the work of Urdu grammarians (e.g. Schmidt 1999, Haq 1987, Javed 1981, Platts 1909) and computational work by Hardie (2003). The main features of the tagset include multiple pronouns (PP, KP, AKP, AP, RP, REP, G, and GR) and demonstratives (PD, KD, AD, and RD). It has only one tag for all forms of verbs (VB), except for auxiliaries to show aspect (AA) and tense (TA) information about the verb. All noun types are assigned single tag (NN) except for Proper Nouns (PN). It also has a special tag NEG to mark any occurrence negation words (� “not” and �� “no” or “neither”) regardless of context. It also has a tag SE to mark every occurrence of _� (“from”) without considering the context. Another example of such a contex</context>
</contexts>
<marker>Hardie, 2003</marker>
<rawString>Hardie, A. 2003. Developing a tag-set for automated part-of-speech tagging in Urdu. Archer, D, Rayson, P, Wilson, A, and McEnery, T (eds.) Proceedings of the Corpus Linguistics 2003 conference. UCREL Technical Papers Volume 16. Department of Linguistics, Lancaster University, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hussain</author>
</authors>
<title>Resources for Urdu Language Processing.</title>
<date>2008</date>
<booktitle>The Proceedings of the 6th Workshop on Asian Language Resources, IJCNLP’08, IIIT Hyderabad,</booktitle>
<contexts>
<context position="3198" citStr="Hussain 2008" startWordPosition="512" endWordPosition="514"> Bengali, Gujarati, Hindi and Punjabi. In addition, there are 1,640,000 words of Urdu text in this corpus. These text collections are also annotated with part of speech tags (Hardie 2003). Center for Research in Urdu Language Processing (CRULP1) gathered 18 million words corpus in order to build a lexicon. It has cleaned text from news websites from multiple domains (Ijaz et.al. 2007). Following this work, a syntactic tagset was developed based on work by existing grammarians and a corpus of 110,000 words was manually tagged. This annotated corpus is available through the center (Sajjad 2007, Hussain 2008). Recently an English-Urdu parallel corpus has also been developed by CRULP, by translating the first 140,000 words of PENN Treebank corpus. In addition, a tagset has also been designed following the PENN Treebank guidelines. These words have been tagged manually with this new tagset. This collection is also available from CRULP, and the tagset is still unpublished. 2.2 Urdu Part of Speech tagsets Hardie (2003) developed the first POS tagset for Urdu using EAGLES guidelines for computational processing. The tagset contains 282 morpho-syntactic tags, differentiating on the basis of number, gend</context>
</contexts>
<marker>Hussain, 2008</marker>
<rawString>Hussain, S. 2008. Resources for Urdu Language Processing. The Proceedings of the 6th Workshop on Asian Language Resources, IJCNLP’08, IIIT Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ijaz</author>
<author>S Hussain</author>
</authors>
<title>Corpus Based Urdu Lexicon Development. The</title>
<date>2007</date>
<booktitle>Proceedings of Conference on Language Technology (CLT07),</booktitle>
<institution>University of Peshawar, Pakistan.</institution>
<marker>Ijaz, Hussain, 2007</marker>
<rawString>Ijaz, M. and Hussain, S. 2007. Corpus Based Urdu Lexicon Development. The Proceedings of Conference on Language Technology (CLT07), University of Peshawar, Pakistan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Javed</author>
</authors>
<title>K ودرا t5ن Taraqqi Urdu Bureau,</title>
<date>1981</date>
<location>New Delhi, India.</location>
<contexts>
<context position="5039" citStr="Javed 1981" startWordPosition="819" endWordPosition="820"> and so suffixes are treated same as lexical words. Hence it is hard to assign accurate tag for an automatic tagger. Although the tagset is designed considering details, but due to larger number of tags it is hard to get a high accuracy with a small sized corpus. Due to its morphological dependence and its large size, this tagset is not considered in our analysis. Two much smaller tagsets are considered for this work. They are compared in detail in Section 6. The first tagset, containing 42 tags, is designed by Sajjad (2007), based on the work of Urdu grammarians (e.g. Schmidt 1999, Haq 1987, Javed 1981, Platts 1909) and computational work by Hardie (2003). The main features of the tagset include multiple pronouns (PP, KP, AKP, AP, RP, REP, G, and GR) and demonstratives (PD, KD, AD, and RD). It has only one tag for all forms of verbs (VB), except for auxiliaries to show aspect (AA) and tense (TA) information about the verb. All noun types are assigned single tag (NN) except for Proper Nouns (PN). It also has a special tag NEG to mark any occurrence negation words (� “not” and �� “no” or “neither”) regardless of context. It also has a tag SE to mark every occurrence of _� (“from”) without con</context>
</contexts>
<marker>Javed, 1981</marker>
<rawString>Javed, I. 1981. &lt;اK ودرا t5ن Taraqqi Urdu Bureau, New Delhi, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Platts</author>
</authors>
<title>A grammar of the Hindustani or Urdu language. Reprinted by Sang-e-Meel Publishers,</title>
<date>1909</date>
<location>Lahore, Pakistan.</location>
<contexts>
<context position="5053" citStr="Platts 1909" startWordPosition="821" endWordPosition="822">ixes are treated same as lexical words. Hence it is hard to assign accurate tag for an automatic tagger. Although the tagset is designed considering details, but due to larger number of tags it is hard to get a high accuracy with a small sized corpus. Due to its morphological dependence and its large size, this tagset is not considered in our analysis. Two much smaller tagsets are considered for this work. They are compared in detail in Section 6. The first tagset, containing 42 tags, is designed by Sajjad (2007), based on the work of Urdu grammarians (e.g. Schmidt 1999, Haq 1987, Javed 1981, Platts 1909) and computational work by Hardie (2003). The main features of the tagset include multiple pronouns (PP, KP, AKP, AP, RP, REP, G, and GR) and demonstratives (PD, KD, AD, and RD). It has only one tag for all forms of verbs (VB), except for auxiliaries to show aspect (AA) and tense (TA) information about the verb. All noun types are assigned single tag (NN) except for Proper Nouns (PN). It also has a special tag NEG to mark any occurrence negation words (� “not” and �� “no” or “neither”) regardless of context. It also has a tag SE to mark every occurrence of _� (“from”) without considering the c</context>
</contexts>
<marker>Platts, 1909</marker>
<rawString>Platts, J. 1909. A grammar of the Hindustani or Urdu language. Reprinted by Sang-e-Meel Publishers, Lahore, Pakistan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sajjad</author>
</authors>
<title>Statistical Part of Speech Tagger for Urdu. Unpublished MS Thesis,</title>
<date>2007</date>
<institution>National University of Computer and Emerging Sciences,</institution>
<location>Lahore, Pakistan.</location>
<contexts>
<context position="3183" citStr="Sajjad 2007" startWordPosition="510" endWordPosition="511">nglish, Urdu, Bengali, Gujarati, Hindi and Punjabi. In addition, there are 1,640,000 words of Urdu text in this corpus. These text collections are also annotated with part of speech tags (Hardie 2003). Center for Research in Urdu Language Processing (CRULP1) gathered 18 million words corpus in order to build a lexicon. It has cleaned text from news websites from multiple domains (Ijaz et.al. 2007). Following this work, a syntactic tagset was developed based on work by existing grammarians and a corpus of 110,000 words was manually tagged. This annotated corpus is available through the center (Sajjad 2007, Hussain 2008). Recently an English-Urdu parallel corpus has also been developed by CRULP, by translating the first 140,000 words of PENN Treebank corpus. In addition, a tagset has also been designed following the PENN Treebank guidelines. These words have been tagged manually with this new tagset. This collection is also available from CRULP, and the tagset is still unpublished. 2.2 Urdu Part of Speech tagsets Hardie (2003) developed the first POS tagset for Urdu using EAGLES guidelines for computational processing. The tagset contains 282 morpho-syntactic tags, differentiating on the basis </context>
<context position="4959" citStr="Sajjad (2007)" startWordPosition="805" endWordPosition="806">u are written with an orthographic space. Words are separated on the basis of space and so suffixes are treated same as lexical words. Hence it is hard to assign accurate tag for an automatic tagger. Although the tagset is designed considering details, but due to larger number of tags it is hard to get a high accuracy with a small sized corpus. Due to its morphological dependence and its large size, this tagset is not considered in our analysis. Two much smaller tagsets are considered for this work. They are compared in detail in Section 6. The first tagset, containing 42 tags, is designed by Sajjad (2007), based on the work of Urdu grammarians (e.g. Schmidt 1999, Haq 1987, Javed 1981, Platts 1909) and computational work by Hardie (2003). The main features of the tagset include multiple pronouns (PP, KP, AKP, AP, RP, REP, G, and GR) and demonstratives (PD, KD, AD, and RD). It has only one tag for all forms of verbs (VB), except for auxiliaries to show aspect (AA) and tense (TA) information about the verb. All noun types are assigned single tag (NN) except for Proper Nouns (PN). It also has a special tag NEG to mark any occurrence negation words (� “not” and �� “no” or “neither”) regardless of c</context>
</contexts>
<marker>Sajjad, 2007</marker>
<rawString>Sajjad, H. 2007. Statistical Part of Speech Tagger for Urdu. Unpublished MS Thesis, National University of Computer and Emerging Sciences, Lahore, Pakistan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sajjad</author>
<author>H Schmid</author>
</authors>
<title>Tagging Urdu Text with Parts Of Speech: A Tagger Comparison.12th conference of the European chapter of the association for computational Linguistics</title>
<date>2009</date>
<contexts>
<context position="5888" citStr="Sajjad and Schmid (2009)" startWordPosition="972" endWordPosition="975">s of verbs (VB), except for auxiliaries to show aspect (AA) and tense (TA) information about the verb. All noun types are assigned single tag (NN) except for Proper Nouns (PN). It also has a special tag NEG to mark any occurrence negation words (� “not” and �� “no” or “neither”) regardless of context. It also has a tag SE to mark every occurrence of _� (“from”) without considering the context. Another example of such a context-free lexical tag is WALA to mark every occurrence (including all the inflections) of the word 219. This tagset is referred to as T1 subsequently in this paper. Recently Sajjad and Schmid (2009) used the tagged data of 107,514 words and carried out an experiment for tagger comparison. A total of 100,000 words are used as training set and rest as test data. Four taggers (TnT, Tree, RF and SVM) are trained using training corpus and then tested accordingly. Reported results of this work show that SVM tagger is the most accurate, showing 94.15% correct prediction of tags. Remaining three taggers have accuracies of 93.02% (Tree tagger), 93.28% (RF tagger) and 93.40% (TnT tagger). Another tagset has recently been developed as a part of a project to develop EnglishUrdu parallel corpus at CR</context>
<context position="8593" citStr="Sajjad and Schmid (2009)" startWordPosition="1434" endWordPosition="1437">cted taggers is Trigramand-Tag (TnT). It is a trigram based HMM tagger in which two preceding tags are used to find the transition probability of a tag. Brants (2000) tested PENN Treebank (English) and NEGRA (German) corpora and reported 96-97% accuracy of the tagger. Schmid (1994) proposed probabilistic POS tagger that uses decision trees to store the transition probabilities. The trained decision tree is used for identification of highest probable tags. Schmid reported an accuracy of 95-96% on PENN Treebank for this tagger. 25 Both taggers give good accuracy for Urdu tagging, as reported by Sajjad and Schmid (2009). 3.2 Data Used for Experimentation Corpora annotated with the different tagsets are acquired from CRULP. The corpus originally tagged with T1 tagset is referred to as C1 (news from non-business domain) and the corpus initially annotated with T2 tagset is referred to as C2 (news from business domain), subsequently in the current work. Both C1 and C2 are taken and cleaned. The data is re-counted and approximately 100,000 words are separated for training and rest are kept for testing. The details of data are given in Tables 1 and 2 below. Table 1. Number of tokens in Urdu corpora Tokens C1 C2 Tr</context>
<context position="10216" citStr="Sajjad and Schmid (2009)" startWordPosition="1720" endWordPosition="1723">trices for errors. These confusion matrices are used to analyze misclassification of tags. TnT tagger shows that overall accuracy of using T1 with C1 is 93.01% and is significantly better than using T2 with C2, which gives 88.13% accuracy. Tree tagger is also trained on the corpora. The overall accuracy of T1 on C1 (93.37%) is better than that of T2 on C2 (90.49%). The results are shown in Table 3. Table 3. Results of both tagsets on their respective corpora with TnT and Tree taggers T1 on C1 T2 on C2 TnT Tagger 93.01% 88.13% Tree Tagger 93.37% 90.49% The accuracies reported (for T1 on C1) by Sajjad and Schmid (2009) are comparable to these accuracies. They have reported 93.40% for TnT Tagger and 93.02% for Tree Tagger. Further experimentation is performed only using TnT tagger. 5 Methodology The current work aims to build a larger corpus of around 230,000 manually tagged words for Urdu by combining C1 and C2. These collections are initially annotated with two different tagsets (T1 and T2 respectively, and as described above). For this unification, it was necessary to indentify the differences in the tagsets on which these corpora are annotated, analyzed the differences and then port them to unified tagse</context>
</contexts>
<marker>Sajjad, Schmid, 2009</marker>
<rawString>Sajjad, H. and Schmid, H. 2009. Tagging Urdu Text with Parts Of Speech: A Tagger Comparison.12th conference of the European chapter of the association for computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Santorini</author>
</authors>
<title>Part_of_Speech Tagging Guidelines for the Penn Treebank Project (3rd printing, 2nd revision). Accessed from ftp://ftp.cis.upenn.edu/pub/treebank/doc/tagguide.ps.gz on 3rd</title>
<date>1990</date>
<contexts>
<context position="6548" citStr="Santorini 1990" startWordPosition="1083" endWordPosition="1084">rried out an experiment for tagger comparison. A total of 100,000 words are used as training set and rest as test data. Four taggers (TnT, Tree, RF and SVM) are trained using training corpus and then tested accordingly. Reported results of this work show that SVM tagger is the most accurate, showing 94.15% correct prediction of tags. Remaining three taggers have accuracies of 93.02% (Tree tagger), 93.28% (RF tagger) and 93.40% (TnT tagger). Another tagset has recently been developed as a part of a project to develop EnglishUrdu parallel corpus at CRULP, following the Penn Treebank guidelines (Santorini 1990). It contains 46 tags, with fewer grades of pronouns (PR, PRP$, PRRF, PRRFP$, and PRRL) and demonstratives (DM and DMRL), as compared to T1. It has several tags for verbs on the basis of their forms and semantics (VB, VBI, VBL, VBLI, and VBT) in addition to the tags for auxiliaries showing aspect (AUXA) and tense (AUXT). The NN tag is assigned for both singular and plural nouns and includes adverbial kaf pronoun, kaf pronoun, and adverbial pronoun categories of T1. Yet, it has several other grades of common nouns (NNC, NNCR, NNCM). It also has two shades of Proper Nouns (NNP , NNPC), which are</context>
</contexts>
<marker>Santorini, 1990</marker>
<rawString>Santorini, B. 1990. Part_of_Speech Tagging Guidelines for the Penn Treebank Project (3rd printing, 2nd revision). Accessed from ftp://ftp.cis.upenn.edu/pub/treebank/doc/tagguide.ps.gz on 3rd May, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees. Institut für Maschinelle Sprachverarbeitung,</title>
<date>1994</date>
<location>Universität Stuttgart, Germany.</location>
<contexts>
<context position="8251" citStr="Schmid (1994)" startWordPosition="1382" endWordPosition="1383">r the task of POS tagging. This section gives details of the taggers chosen and the corpora used for the experiments conducted. 3.1 Selection of taggers There are a number of existing taggers available for tagging. Two POS taggers are used in the initial step of this work to compare the initial tagging accuracies. One of the selected taggers is Trigramand-Tag (TnT). It is a trigram based HMM tagger in which two preceding tags are used to find the transition probability of a tag. Brants (2000) tested PENN Treebank (English) and NEGRA (German) corpora and reported 96-97% accuracy of the tagger. Schmid (1994) proposed probabilistic POS tagger that uses decision trees to store the transition probabilities. The trained decision tree is used for identification of highest probable tags. Schmid reported an accuracy of 95-96% on PENN Treebank for this tagger. 25 Both taggers give good accuracy for Urdu tagging, as reported by Sajjad and Schmid (2009). 3.2 Data Used for Experimentation Corpora annotated with the different tagsets are acquired from CRULP. The corpus originally tagged with T1 tagset is referred to as C1 (news from non-business domain) and the corpus initially annotated with T2 tagset is re</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Schmid, H. 1994. Probabilistic part-of-speech tagging using decision trees. Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schmidt</author>
</authors>
<title>Urdu: an essential grammar.</title>
<date>1999</date>
<location>Routledge, London, UK.</location>
<contexts>
<context position="5017" citStr="Schmidt 1999" startWordPosition="815" endWordPosition="816">ed on the basis of space and so suffixes are treated same as lexical words. Hence it is hard to assign accurate tag for an automatic tagger. Although the tagset is designed considering details, but due to larger number of tags it is hard to get a high accuracy with a small sized corpus. Due to its morphological dependence and its large size, this tagset is not considered in our analysis. Two much smaller tagsets are considered for this work. They are compared in detail in Section 6. The first tagset, containing 42 tags, is designed by Sajjad (2007), based on the work of Urdu grammarians (e.g. Schmidt 1999, Haq 1987, Javed 1981, Platts 1909) and computational work by Hardie (2003). The main features of the tagset include multiple pronouns (PP, KP, AKP, AP, RP, REP, G, and GR) and demonstratives (PD, KD, AD, and RD). It has only one tag for all forms of verbs (VB), except for auxiliaries to show aspect (AA) and tense (TA) information about the verb. All noun types are assigned single tag (NN) except for Proper Nouns (PN). It also has a special tag NEG to mark any occurrence negation words (� “not” and �� “no” or “neither”) regardless of context. It also has a tag SE to mark every occurrence of _</context>
</contexts>
<marker>Schmidt, 1999</marker>
<rawString>Schmidt, R. 1999. Urdu: an essential grammar. Routledge, London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McEnery</author>
<author>J Gaizauskas Baker</author>
<author>R</author>
<author>H Cunningham</author>
</authors>
<title>EMILLE: towards a corpus of South Asian languages,</title>
<date>2000</date>
<journal>British Computing Society Machine Translation Specialist Group,</journal>
<location>London, UK.</location>
<contexts>
<context position="2522" citStr="McEnery et al., 2000" startWordPosition="402" endWordPosition="405">ggers reported using them. Sections 4 and 5 present baseline experiment and the methodology used for the analysis for updating the tagset. Section 6 describes the proposed tagset. Section 7 reports experiments comparing the new tagset with existing ones. Section 8 discusses the results achieved and future directions. 2 Relevant Resources of Urdu 2.1 Urdu Corpora Several annotated corpora have been built during last few years to facilitate computational processing for Urdu language. The initial work was undertaken through EMILLE project to build multi-lingual corpora for South Asian languages (McEnery et al., 2000). They released 200,000 words parallel corpus of English, Urdu, Bengali, Gujarati, Hindi and Punjabi. In addition, there are 1,640,000 words of Urdu text in this corpus. These text collections are also annotated with part of speech tags (Hardie 2003). Center for Research in Urdu Language Processing (CRULP1) gathered 18 million words corpus in order to build a lexicon. It has cleaned text from news websites from multiple domains (Ijaz et.al. 2007). Following this work, a syntactic tagset was developed based on work by existing grammarians and a corpus of 110,000 words was manually tagged. This </context>
</contexts>
<marker>McEnery, Baker, R, Cunningham, 2000</marker>
<rawString>McEnery, A., Baker, J. Gaizauskas, R. and Cunningham, H. 2000. EMILLE: towards a corpus of South Asian languages, British Computing Society Machine Translation Specialist Group, London, UK.</rawString>
</citation>
<citation valid="false">
<title>Appendix A: Mappings of tags between Tagsets T1 and T2.</title>
<marker></marker>
<rawString>Appendix A: Mappings of tags between Tagsets T1 and T2.</rawString>
</citation>
<citation valid="false">
<authors>
<author>RB</author>
</authors>
<journal>CA CD 6. CC CC</journal>
<volume>7</volume>
<marker>RB, </marker>
<rawString>RB 5. CA CD 6. CC CC 7.</rawString>
</citation>
<citation valid="false">
<authors>
<author>DATE DATE</author>
</authors>
<journal>EXP SYM 9. G PRP$</journal>
<volume>11</volume>
<marker>DATE, </marker>
<rawString>DATE DATE 8. EXP SYM 9. G PRP$ 11.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>