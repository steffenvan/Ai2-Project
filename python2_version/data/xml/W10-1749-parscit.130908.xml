<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.977725">
LRscore for Evaluating Lexical and Reordering Quality in MT
</title>
<author confidence="0.996953">
Alexandra Birch Miles Osborne
</author>
<affiliation confidence="0.999581">
University of Edinburgh University of Edinburgh
</affiliation>
<address confidence="0.528215">
United Kingdom United Kingdom
</address>
<email confidence="0.956654">
a.c.birch-mayne@s0454866.ed.ac.uk miles@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.996827" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998745">
The ability to measure the quality of word
order in translations is an important goal
for research in machine translation. Cur-
rent machine translation metrics do not
adequately measure the reordering perfor-
mance of translation systems. We present
a novel metric, the LRscore, which di-
rectly measures reordering success. The
reordering component is balanced by a
lexical metric. Capturing the two most im-
portant elements of translation success in
a simple combined metric with only one
parameter results in an intuitive, shallow,
language independent metric.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974969230769">
The main purpose of MT evaluation is to de-
termine “to what extent the makers of a system
have succeeded in mimicking the human transla-
tor” (Krauwer, 1993). But machine translation
has no “ground truth” as there are many possi-
ble correct translations. It is impossible to judge
whether a translation is incorrect or simply un-
known and it is even harder to judge the the degree
to which it is incorrect. Even so, automatic met-
rics are necessary. It is nearly impossible to collect
enough human judgments for evaluating incre-
mental improvements in research systems, or for
tuning statistical machine translation system pa-
rameters. Automatic metrics are also much faster
and cheaper than human evaluation and they pro-
duce reproducible results.
Machine translation research relies heavily
upon automatic metrics to evaluate the perfor-
mance of models. However, current metrics rely
upon indirect methods for measuring the quality
of the word order, and their ability to capture re-
ordering performance has been demonstrated to be
poor (Birch et al., 2010). There are two main ap-
proaches to capturing reordering. The first way
to measure the quality of word order is to count
the number of matching n-grams between the ref-
erence and the hypothesis. This is the approach
taken by the BLEU score (Papineni et al., 2002).
This method discounts any n-gram which is not
identical to a reference n-gram, and also does not
consider the relative position of the strings. They
can be anywhere in the sentence. Another com-
mon approach is typified by METEOR (Banerjee
and Lavie, 2005) and TER (Snover et al., 2006).
They calculate an ordering penalty for a hypoth-
esis based on the minimum number of chunks the
translation needs to be broken into in order to align
it to the reference. The disadvantage of the second
approach is that aligning sentences with very dif-
ferent words can be inaccurate. Also there is no
notion of how far these blocks are out of order.
More sophisticated metrics, such as the RTE met-
ric (Pad´o et al., 2009), use higher level syntactic or
even semantic analysis to determine the quality of
the translation. These approaches are useful, but
can be very slow, require annotation, they are lan-
guage dependent and their parameters are hard to
train. For most research work shallow metrics are
more appropriate.
Apart from failing to capture reordering perfor-
mance, another common criticism of most cur-
rent automatic MT metrics is that a particular
score value reported does not give insights into
quality (Przybocki et al., 2009). This is because
there is no intrinsic significance of a difference
in scores. Ideally, the scores that the metrics re-
port would be meaningful and stand on their own.
However, the most one can say is that higher is
better for accuracy metrics and lower is better for
error metrics.
We present a novel metric, the LRscore, which
explicitly measures the quality of word order in
machine translations. It then combines the re-
ordering metric with a metric measuring lexical
success. This results in a comprehensive met-
</bodyText>
<page confidence="0.978913">
327
</page>
<note confidence="0.4522365">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 327–332,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999127448275862">
ric which measures the two most fundamental as-
pects of translation. We argue that the LRscore
is intuitive and meaningful because it is a simple,
decomposable metric with only one parameter to
train.
The LRscore has many of the properties that are
deemed to be desirable in a recent metric eval-
uation campaign (Przybocki et al., 2009). The
LRscore is language independent. The reorder-
ing component relies on abstract alignments and
word positions and not on words at all. The lex-
ical component of the system can be any mean-
ingful metric for a particular target language. In
our experiments we use 1-gram BLEU and 4-gram
BLEU, however, if a researcher was interested in
morphologically rich languages, a different met-
ric which scores partially correct words might be
more appropriate. The LRscore is a shallow met-
ric, which means that it is reasonably fast to run.
This is important in order to be useful for train-
ing of the translation model parameters. A final
advantage is that the LRscore is a sentence level
metric. This means that human judgments can be
directly compared to system scores and helps re-
searchers to understand what changes they are see-
ing between systems.
In this paper we start by describing the reorder-
ing metrics and then we present the LRscore. Fi-
nally we discuss related work and conclude.
</bodyText>
<sectionHeader confidence="0.97923" genericHeader="method">
2 Reordering Metrics
</sectionHeader>
<bodyText confidence="0.999902943661972">
The relative ordering of words in the source and
target sentences is encoded in alignments. We
can interpret alignments as permutations. This
allows us to apply research into metrics for or-
dered encodings to our primary tasks of measur-
ing and evaluating reorderings. A word alignment
over a sentence pair allows us to transcribe the
source word positions in the order of the aligned
target words. Permutations have already been
used to describe reorderings (Eisner and Tromble,
2006), primarily to develop a reordering model
which uses ordering costs to score possible per-
mutations. Here we use permutations to evaluate
reordering performance based on the methods pre-
sented in (Birch et al., 2010).
The ordering of the words in the target sentence
can be seen as a permutation of the words in the
source sentence. The source sentence s of length
N consists of the word positions s0 · · · si · · · sN.
Using an alignment function where a source word
at position i is mapped to a target word at position
j with the function a : i —* j, we can reorder the
source word positions to reflect the order of the
words in the target. This gives us a permutation.
A permutation is a bijective function from a set
of natural numbers 1, 2, · · · , N to itself. We will
name our permutations 7r and Q. The ith symbol
of a permutation 7r will be denoted as 7r(i), and
the inverse of the permutation 7r−1 is defined so
that if 7r(i) = j then 7r−1(j) = i. The identity, or
monotone, permutation id is the permutation for
which id(i) = i for all i. Table 1 shows the per-
mutations associated with the example alignments
in Figure 1. The permutations are calculated by
iterating over the source words, and recording the
ordering of the aligned target words.
Permutations encode one-one relations,
whereas alignments contain null alignments and
one-many, many-one and many-many relations.
For now, we make some simplifying assumptions
to allow us to work with permutations. Source
words aligned to null (a(i) —* null) are assigned
the target word position immediately after the
target word position of the previous source word
(7r(i) = 7r(i − 1) + 1). Where multiple source
words are aligned to the same target word or
phrase, a many-to-one relation, the target ordering
is assumed to be monotone. When one source
word is aligned to multiple target words, a one-to-
many relation, the source word is assumed to be
aligned to the first target word.
A translation can potentially have many valid
word orderings. However, we can be reason-
ably certain that the ordering of reference sentence
must be acceptable. We therefore compare the or-
dering of a translation with that of the reference
sentence. The underlying assumption is that most
reasonable word orderings should be fairly similar
to the reference. The assumption that the reference
is somehow similar to the translation is necessary
for all automatic machine translation metrics. We
propose using permutation distance metrics to per-
form the comparison.
There are many different ways of measuring
distance between two permutations, with different
solutions originating in different domains (statis-
tics, computer science, molecular biology, ... ).
Real numbered data leads to measures such as Eu-
clidean distance, binary data to measures such as
Hamming distance. But for ordered sets, there
are many different options, and the best one de-
</bodyText>
<page confidence="0.984122">
328
</page>
<figure confidence="0.9881238">
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10
</figure>
<equation confidence="0.7079973">
t1 t1
t2 t2
t3 t3
t4 t4
t5 t6
t6 t5
t7 t7
t8 t8
t9 t9
t10 t10
</equation>
<figure confidence="0.989313727272727">
(a)
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10
(b)
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10
</figure>
<equation confidence="0.949342727272727">
t6 t10
t7 t1
t8 t2
t9 t3
t10 t4
t1 t5
t2 t6
t3 t7
t4 t8
t5 t9
(c) (d)
</equation>
<figureCaption confidence="0.77367575">
Figure 1: Synthetic examples: a translation and three reference scenarios. (a) is a monotone translation,
(b) is a reference with one short distance word order difference, (c) is a reference where the order of the
two halves has been swapped, and (d) is a reference with a long distance reordering of the first target
word.
</figureCaption>
<bodyText confidence="0.995537826086957">
pends on the task at hand. We choose a few
metrics which are widely used, efficient to calcu-
late and capture certain properties of the reorder-
ing. In particular, they are sensitive to the num-
ber of words that are out of order. Three of the
metrics, Kendall’s tau, Spearman’s rho and Spear-
man’s footrule distances also take into account the
distance between positions in the reference and
translation sentences, or the size of the reordering.
An obvious disadvantage of this approach is the
fact that we need alignments, either between the
source and the reference, and the source and the
translation, or directly between the reference and
the translation. If accuracy is paramount, the test
set could include manual alignments and the sys-
tems could directly output the source-translation
alignments. Outputting the alignment informa-
tion should require a trivial change to the decoder.
Alignments can also be automatically generated
using the alignment model that aligns the training
data.
Distance metrics increase as the quality of trans-
lation decreases. We invert the scale of the dis-
</bodyText>
<figure confidence="0.943227">
(a) (1 2 3 4 5 6 7 8 9 10)
(b) (1 2 3 4 •6 •5 •7 8 9 10)
(c) (6 7 8 9 10 •1 2 3 4 5)
(d) (2 3 4 5 6 7 8 9 10 •1)
</figure>
<tableCaption confidence="0.6300378">
Table 1: Permutations extracted from the sentence
pairs shown in Figure 1: (a) is a monotone permu-
tation and (b), (c) and (d) are permutations with
different amounts of disorder, where bullet points
highlight non-sequential neighbors.
</tableCaption>
<bodyText confidence="0.98969725">
tance metrics in order to easily compare them with
other metrics where increases in the metrics mean
increases in translation quality. All permutation
distance metrics are thus subtracted from 1. Note
that the two permutations we refer to 7r and Q are
relative to the source sentence, and not to the ref-
erence: the source-reference permutation is com-
pared to the source-translation permutation.
</bodyText>
<subsectionHeader confidence="0.982206">
2.1 Hamming Distance
</subsectionHeader>
<bodyText confidence="0.9989565">
The Hamming distance (Hamming, 1950) mea-
sures the number of disagreements between two
</bodyText>
<page confidence="0.996089">
329
</page>
<bodyText confidence="0.988300181818182">
permutations. The Hamming distance for permu-
tations was proposed by (Ronald, 1998) and is also
known as the exact match distance. It is defined
as follows:
2004) is similar to a Kendall’s tau metric on lexical
items. ROUGE-S is an F-measure of ordered pairs
of words in the translation. As far as we know,
Kendall’s tau has not been used as a reordering
metric before.
LRscore
where xi =
</bodyText>
<equation confidence="0.935280333333333">
1 otherwise
Pn i=1 xi
dH(7r, Q) = 1−
n
~ 0 if 7r(i) = Q(i)
3
</equation>
<bodyText confidence="0.771189">
The goal of much machine translation research is
</bodyText>
<equation confidence="0.948160285714286">
Z
1 if 7r (i) &lt; 7r (j) and Q(i) &gt; Q(j)
where zij = 0 otherwise
Z = (n2 − n)
2
7r
sequences.
</equation>
<bodyText confidence="0.996250375">
, Q are the two permutations and the
normalization constant Z is n, the length of the
permutation. We are interested in the Hamming
distance for its ability to capture the amount of ab-
solute disorder that exists between two permuta-
tions. The Hamming distance is widely utilized in
coding theory to measure the discrepancy between
two binary
</bodyText>
<subsectionHeader confidence="0.924574">
2.2 Kendall
s Tau Distance
</subsectionHeader>
<bodyText confidence="0.9504492">
Kendall
s tau distance is the minimum number
of transpositions of two adjacent symbols nec-
essary to transform one permutation into an-
other (Kendall, 1938; Kendall and Gibbons,
1990). This is sometimes known as the swap dis-
tance or the inversion distance and can be inter-
preted as a function of the probability of observing
concordant and discordan
’
</bodyText>
<equation confidence="0.7497715">
’
t pairs (Kerridge, 1975).
</equation>
<bodyText confidence="0.878389875">
It is defined as follows:
s tau metric is possibly the most in-
teresting for measuring reordering as it is sensitive
to all relative orderings. It consequently measures
not only how many reordering there are but also
the distance that words are reordered.
In statistics, Spearman
s rho and Kendall
</bodyText>
<subsectionHeader confidence="0.441505">
s tau
</subsectionHeader>
<bodyText confidence="0.956503878048781">
are widely used non-parametric measures of as-
sociation for two rankings. In natural language
processing research, Kendall
s tau has been used
as a means of estimating the distance between
a system-generated and a human-generated gold-
standard order for the sentence ordering task (La-
pata, 2003). Kendall
s tau has also been used
in machine translation as a cost function in a re-
ordering model (Eisner and Tromble, 2006) and
an MT metric called ROUGE-S (Lin an
’
’
’
’
’
d Och,
score. If we use the 1-gram BLEU score,
then the LRscore relies purely upon the reorder-
ing metric for all word ordering evaluation. We
also use the 4-gram BLEU score, BLEU4, as it is
an important baseline and the values it reports are
very familiar to machine translation researchers.
BLEU4 also contains a notion of word ordering
based on longer matching n-grams. However, it
is aware only of very local orderings. It does not
measure the magnitude of the orderings like the
reordering metrics do, and it is dependent on ex-
act lexical overlap which does not affect the re-
ordering metric. The two components are there-
fore largely orthogonal and there is a benefit in
combining them. Both the BLEU score and the
reordering distance metric apply a brevity penalty
to account for tran
BLEU1,
slations of different lengths.
The formula for calculating the LRscore is as
follows:
LRscore = a * R + (1 − a)BLEU
Where the reordering metri
</bodyText>
<equation confidence="0.980549333333333">
c R is calculated as fol-
lows:
R=d*BP
</equation>
<bodyText confidence="0.91538175">
or the
tau distance
as the reordering
distance d an
Kendall’s
dτ
d then we apply the brevity penalty
BP. The brevity penalty is calculated as:
</bodyText>
<equation confidence="0.9617855">
1 if t &gt; r
e1−r/t if t &lt; r
</equation>
<bodyText confidence="0.995784714285714">
Where
The Kendall
either to improve the quality of the words used in
the output, or their ordering. We use the reordering
metrics and combine them with a measurement of
lexical performance to produce a comprehensive
metric, the LRscore. The LRscore is a linear in-
terpolation of a reordering metric with the BLEU
Where we either take the Hamming distance dH
where t is the length of the translation, and r is
the closest reference length. R is calculated at the
sentence level, an
d the scores are averaged over a
test set. This average is then combined with the
</bodyText>
<equation confidence="0.442156">
~ BP =
</equation>
<page confidence="0.985074">
330
</page>
<bodyText confidence="0.99998575862069">
system level lexical score. The Lexical metric is
the BLEU score which sums the log precision of
n-grams. In our paper we set the n-gram length to
either be one or four.
The only parameter in the metric α balances the
contribution of reordering and the lexical compo-
nents. There is no analytic solution for optimizing
this parameter, and we use greedy hillclimbing in
order to find the optimal setting. We optimize the
sentence level correlation of the metric to human
judgments of accuracy as provided by the WMT
2010 shared task. As hillclimbing can end up in a
local minima, we perform 20 random restarts, and
retaining only the parameter value with the best
consistency result. Random-restart hill climbing is
a surprisingly effective algorithm in many cases. It
turns out that it is often better to spend CPU time
exploring the space, rather than carefully optimiz-
ing from an initial condition.
The brevity penalty applies to both the reorder-
ing metric and the BLEU score. We do not set
a parameter to regulate the impact of the brevity
penalty, as we want to retain BLEU scores that are
comparable with BLEU scores computed in pub-
lished research. And as we do not regulate the
brevity penalty in the BLEU score, we do not wish
to do so for the reordering metric either. It there-
fore impacts on both the reordering and the lexical
components equally.
</bodyText>
<sectionHeader confidence="0.733227" genericHeader="method">
4 Correlation with Human Judgments
</sectionHeader>
<bodyText confidence="0.99982285">
It has been common to use seven-point fluency
and adequacy scores as the main human evalua-
tion task. These scores are intended to be absolute
scores and comparable across sentences. Seven-
point fluency and adequacy judgements are quite
unreliable at a sentence level and so it seems du-
bious that they would be reliable across sentences.
However, having absolute scores does have the ad-
vantage of making it easy to calculate the correla-
tion coefficients of the metric with human judge-
ments. Using rank judgements, we do not have
absolute scores and thus we cannot compare trans-
lations across different sentences.
We therefore take the method adopted in the
2009 workshop on machine translation (Callison-
Burch et al., 2009). We ascertained how consis-
tent the automatic metrics were with the human
judgements by calculating consistency in the fol-
lowing manner. We take each pairwise compari-
son of translation output for single sentences by a
</bodyText>
<table confidence="0.998455">
Metric de-en es-en fr-en cz-en
BLEU4 58.72 55.48 57.71 57.24
LR-HB1 60.37 60.55 58.59 53.70
LR-HB4 60.49 58.88 58.80 57.74
LR-KB1 60.67 58.54 58.46 54.20
LR-KB4 61.07 59.86 58.59 58.92
</table>
<tableCaption confidence="0.7265145">
Table 2: The percentage consistency between hu-
man judgements of rank and metrics. The LRscore
variations (LR-*) are optimised for consistency for
each language pair.
</tableCaption>
<bodyText confidence="0.999811428571429">
particular judge, and we recorded whether or not
the metrics were consistent with the human rank.
Ie. we counted cases where both the metric and the
human judged agree that one system is better than
another. We divided this by the total umber of pair-
wise comparisons to get a percentage. There were
many ties in the human data, but metrics rarely
give the same score to two different translations.
We therefore excluded pairs that the human anno-
tators ranked as ties. The human ranking data and
the system outputs from the 2009 Workshop on
Machine Translation (Callison-Burch et al., 2009)
have been used to evaluate the LRscore.
We optimise the sentence level consistency of
the metric. As hillclimbing can end up in a local
minima, we perform 20 random restarts, and re-
taining only the parameter value with the best con-
sistency result. Random-restart hill climbing is a
surprisingly effective algorithm in many cases. It
turns out that it is often better to spend CPU time
exploring the space, rather than carefully optimis-
ing from an initial condition.
Table 2 reports the optimal consistency of the
LRscore and baseline metrics with human judge-
ments for each language pair. The table also
reports the individual component results. The
LRscore variations are named as follows: LR
refers to the LRscore, “H” refers to the Hamming
distance and “K” to Kendall’s tau distance. “B1”
and “B4” refer to the smoothed BLEU score with
the 1-gram and 4-gram scores. The LRscore is the
metric which is most consistent with human judge-
ment. This is an important result which shows
that combining lexical and reordering information
makes for a stronger metric.
</bodyText>
<sectionHeader confidence="0.999988" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.9845665">
(Wong and Kit, 2009) also suggest a metric which
combines a word choice and a word order com-
</bodyText>
<page confidence="0.99597">
331
</page>
<bodyText confidence="0.999944133333333">
ponent. They propose a type of F-measure which
uses a matching function M to calculate precision
and recall. M combines the number of matched
words, weighted by their tfidf importance, with
their position difference score, and finally sub-
tracting a score for unmatched words. Includ-
ing unmatched words in the in M function un-
dermines the interpretation of the supposed F-
measure. The reordering component is the average
difference of absolute and relative word positions
which has no clear meaning. This score is not intu-
itive or easily decomposable and it is more similar
to METEOR, with synonym and stem functional-
ity mixed with a reordering penalty, than to our
metric.
</bodyText>
<sectionHeader confidence="0.999816" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999668">
We propose the LRscore which combines a lexi-
cal and a reordering metric. This results in a met-
ric which is both meaningful and accurately mea-
sures the word order performance of the transla-
tion model.
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999858409090909">
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for MT evaluation with improved
correlation with human judgments. In Workshop on
Intrinsic and Extrinsic Evaluation Measures for MT
and/or Summarization.
Alexandra Birch, Phil Blunsom, and Miles Osborne.
2010. Metrics for MT Evaluation: Evaluating Re-
ordering. Machine Translation (to appear).
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1–28, Athens, Greece,
March. Association for Computational Linguistics.
Jason Eisner and Roy W. Tromble. 2006. Local search
with very large-scale neighborhoods for optimal
permutations in machine translation. In Proceed-
ings of the HLT-NAACL Workshop on Computation-
ally Hard Problems and Joint Inference in Speech
and Language Processing, pages 57–75, New York,
June.
Richard Hamming. 1950. Error detecting and error
correcting codes. Bell System Technical Journal,
26(2):147–160.
M. Kendall and J. Dickinson Gibbons. 1990. Rank
Correlation Methods. Oxford University Press,
New York.
Maurice Kendall. 1938. A new measure of rank corre-
lation. Biometrika, 30:81–89.
D Kerridge. 1975. The interpretation of rank correla-
tions. Applied Statistics, 2:257–258.
S. Krauwer. 1993. Evaluation of MT systems: a pro-
grammatic view. Machine Translation, 8(1):59–66.
Mirella Lapata. 2003. Probabilistic text structur-
ing: Experiments with sentence ordering. Compu-
tational Linguistics, 29(2):263–317.
Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic evaluation of machine translation quality us-
ing longest common subsequence and skip-bigram
statistics. In Proceedings of the 42nd Meeting
of the Association for Computational Linguistics
(ACL’04), Main Volume, pages 605–612, Barcelona,
Spain, July.
Sebastian Pad´o, Daniel Cer, Michel Galley, Dan Juraf-
sky, and Christopher D. Manning. 2009. Measur-
ing machine translation quality as semantic equiva-
lence: A metric based on entailment features. Ma-
chine Translation.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the Association for Computational Linguistics,
pages 311–318, Philadelphia, USA.
Mark Przybocki, Kay Peterson, S´ebastien Bronsart,
and Gregory Sanders. 2009. The nist 2008 metrics
for machine translation challengeoverview, method-
ology, metrics, and results. Machine Translation.
S Ronald. 1998. More distance functions for order-
based encodings. In the IEEE Conference on Evolu-
tionary Computation, pages 558–563.
Matthew Snover, Bonnie Dorr, R Schwartz, L Micci-
ulla, and J Makhoul. 2006. A study of translation
edit rate with targeted human annotation. In AMTA.
B. Wong and C. Kit. 2009. ATEC: automatic eval-
uation of machine translation via word choice and
word order. Machine Translation, pages 1–15.
</reference>
<page confidence="0.998251">
332
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.289275">
<title confidence="0.996472">LRscore for Evaluating Lexical and Reordering Quality in MT</title>
<author confidence="0.999948">Alexandra Birch Miles Osborne</author>
<affiliation confidence="0.668547">University of Edinburgh University of Edinburgh United Kingdom United</affiliation>
<email confidence="0.641393">a.c.birch-mayne@s0454866.ed.ac.ukmiles@inf.ed.ac.uk</email>
<abstract confidence="0.998387266666667">The ability to measure the quality of word order in translations is an important goal for research in machine translation. Current machine translation metrics do not adequately measure the reordering performance of translation systems. We present a novel metric, the LRscore, which directly measures reordering success. The reordering component is balanced by a lexical metric. Capturing the two most important elements of translation success in a simple combined metric with only one parameter results in an intuitive, shallow, language independent metric.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.</booktitle>
<contexts>
<context position="2368" citStr="Banerjee and Lavie, 2005" startWordPosition="369" endWordPosition="372">r, and their ability to capture reordering performance has been demonstrated to be poor (Birch et al., 2010). There are two main approaches to capturing reordering. The first way to measure the quality of word order is to count the number of matching n-grams between the reference and the hypothesis. This is the approach taken by the BLEU score (Papineni et al., 2002). This method discounts any n-gram which is not identical to a reference n-gram, and also does not consider the relative position of the strings. They can be anywhere in the sentence. Another common approach is typified by METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). They calculate an ordering penalty for a hypothesis based on the minimum number of chunks the translation needs to be broken into in order to align it to the reference. The disadvantage of the second approach is that aligning sentences with very different words can be inaccurate. Also there is no notion of how far these blocks are out of order. More sophisticated metrics, such as the RTE metric (Pad´o et al., 2009), use higher level syntactic or even semantic analysis to determine the quality of the translation. These approaches are useful, but can be very slow,</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for MT evaluation with improved correlation with human judgments. In Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Phil Blunsom</author>
<author>Miles Osborne</author>
</authors>
<title>Metrics for MT Evaluation: Evaluating Reordering. Machine Translation</title>
<date>2010</date>
<note>(to appear).</note>
<contexts>
<context position="1851" citStr="Birch et al., 2010" startWordPosition="279" endWordPosition="282">are necessary. It is nearly impossible to collect enough human judgments for evaluating incremental improvements in research systems, or for tuning statistical machine translation system parameters. Automatic metrics are also much faster and cheaper than human evaluation and they produce reproducible results. Machine translation research relies heavily upon automatic metrics to evaluate the performance of models. However, current metrics rely upon indirect methods for measuring the quality of the word order, and their ability to capture reordering performance has been demonstrated to be poor (Birch et al., 2010). There are two main approaches to capturing reordering. The first way to measure the quality of word order is to count the number of matching n-grams between the reference and the hypothesis. This is the approach taken by the BLEU score (Papineni et al., 2002). This method discounts any n-gram which is not identical to a reference n-gram, and also does not consider the relative position of the strings. They can be anywhere in the sentence. Another common approach is typified by METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). They calculate an ordering penalty for a hypothesis</context>
<context position="6076" citStr="Birch et al., 2010" startWordPosition="985" endWordPosition="988">n alignments. We can interpret alignments as permutations. This allows us to apply research into metrics for ordered encodings to our primary tasks of measuring and evaluating reorderings. A word alignment over a sentence pair allows us to transcribe the source word positions in the order of the aligned target words. Permutations have already been used to describe reorderings (Eisner and Tromble, 2006), primarily to develop a reordering model which uses ordering costs to score possible permutations. Here we use permutations to evaluate reordering performance based on the methods presented in (Birch et al., 2010). The ordering of the words in the target sentence can be seen as a permutation of the words in the source sentence. The source sentence s of length N consists of the word positions s0 · · · si · · · sN. Using an alignment function where a source word at position i is mapped to a target word at position j with the function a : i —* j, we can reorder the source word positions to reflect the order of the words in the target. This gives us a permutation. A permutation is a bijective function from a set of natural numbers 1, 2, · · · , N to itself. We will name our permutations 7r and Q. The ith s</context>
</contexts>
<marker>Birch, Blunsom, Osborne, 2010</marker>
<rawString>Alexandra Birch, Phil Blunsom, and Miles Osborne. 2010. Metrics for MT Evaluation: Evaluating Reordering. Machine Translation (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<date>2009</date>
<booktitle>Findings of the 2009 Workshop on Statistical Machine Translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--28</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="18376" citStr="Callison-Burch et al., 2009" startWordPosition="3164" endWordPosition="3167">or consistency for each language pair. particular judge, and we recorded whether or not the metrics were consistent with the human rank. Ie. we counted cases where both the metric and the human judged agree that one system is better than another. We divided this by the total umber of pairwise comparisons to get a percentage. There were many ties in the human data, but metrics rarely give the same score to two different translations. We therefore excluded pairs that the human annotators ranked as ties. The human ranking data and the system outputs from the 2009 Workshop on Machine Translation (Callison-Burch et al., 2009) have been used to evaluate the LRscore. We optimise the sentence level consistency of the metric. As hillclimbing can end up in a local minima, we perform 20 random restarts, and retaining only the parameter value with the best consistency result. Random-restart hill climbing is a surprisingly effective algorithm in many cases. It turns out that it is often better to spend CPU time exploring the space, rather than carefully optimising from an initial condition. Table 2 reports the optimal consistency of the LRscore and baseline metrics with human judgements for each language pair. The table a</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Schroeder, 2009</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh Schroeder. 2009. Findings of the 2009 Workshop on Statistical Machine Translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 1–28, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Roy W Tromble</author>
</authors>
<title>Local search with very large-scale neighborhoods for optimal permutations in machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLT-NAACL Workshop on Computationally Hard Problems and Joint Inference in Speech and Language Processing,</booktitle>
<pages>57--75</pages>
<location>New York,</location>
<contexts>
<context position="5862" citStr="Eisner and Tromble, 2006" startWordPosition="951" endWordPosition="954">tart by describing the reordering metrics and then we present the LRscore. Finally we discuss related work and conclude. 2 Reordering Metrics The relative ordering of words in the source and target sentences is encoded in alignments. We can interpret alignments as permutations. This allows us to apply research into metrics for ordered encodings to our primary tasks of measuring and evaluating reorderings. A word alignment over a sentence pair allows us to transcribe the source word positions in the order of the aligned target words. Permutations have already been used to describe reorderings (Eisner and Tromble, 2006), primarily to develop a reordering model which uses ordering costs to score possible permutations. Here we use permutations to evaluate reordering performance based on the methods presented in (Birch et al., 2010). The ordering of the words in the target sentence can be seen as a permutation of the words in the source sentence. The source sentence s of length N consists of the word positions s0 · · · si · · · sN. Using an alignment function where a source word at position i is mapped to a target word at position j with the function a : i —* j, we can reorder the source word positions to refle</context>
<context position="13337" citStr="Eisner and Tromble, 2006" startWordPosition="2288" endWordPosition="2291"> as it is sensitive to all relative orderings. It consequently measures not only how many reordering there are but also the distance that words are reordered. In statistics, Spearman s rho and Kendall s tau are widely used non-parametric measures of association for two rankings. In natural language processing research, Kendall s tau has been used as a means of estimating the distance between a system-generated and a human-generated goldstandard order for the sentence ordering task (Lapata, 2003). Kendall s tau has also been used in machine translation as a cost function in a reordering model (Eisner and Tromble, 2006) and an MT metric called ROUGE-S (Lin an ’ ’ ’ ’ ’ d Och, score. If we use the 1-gram BLEU score, then the LRscore relies purely upon the reordering metric for all word ordering evaluation. We also use the 4-gram BLEU score, BLEU4, as it is an important baseline and the values it reports are very familiar to machine translation researchers. BLEU4 also contains a notion of word ordering based on longer matching n-grams. However, it is aware only of very local orderings. It does not measure the magnitude of the orderings like the reordering metrics do, and it is dependent on exact lexical overla</context>
</contexts>
<marker>Eisner, Tromble, 2006</marker>
<rawString>Jason Eisner and Roy W. Tromble. 2006. Local search with very large-scale neighborhoods for optimal permutations in machine translation. In Proceedings of the HLT-NAACL Workshop on Computationally Hard Problems and Joint Inference in Speech and Language Processing, pages 57–75, New York, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Hamming</author>
</authors>
<title>Error detecting and error correcting codes.</title>
<date>1950</date>
<journal>Bell System Technical Journal,</journal>
<volume>26</volume>
<issue>2</issue>
<contexts>
<context position="11226" citStr="Hamming, 1950" startWordPosition="1914" endWordPosition="1915">: (a) is a monotone permutation and (b), (c) and (d) are permutations with different amounts of disorder, where bullet points highlight non-sequential neighbors. tance metrics in order to easily compare them with other metrics where increases in the metrics mean increases in translation quality. All permutation distance metrics are thus subtracted from 1. Note that the two permutations we refer to 7r and Q are relative to the source sentence, and not to the reference: the source-reference permutation is compared to the source-translation permutation. 2.1 Hamming Distance The Hamming distance (Hamming, 1950) measures the number of disagreements between two 329 permutations. The Hamming distance for permutations was proposed by (Ronald, 1998) and is also known as the exact match distance. It is defined as follows: 2004) is similar to a Kendall’s tau metric on lexical items. ROUGE-S is an F-measure of ordered pairs of words in the translation. As far as we know, Kendall’s tau has not been used as a reordering metric before. LRscore where xi = 1 otherwise Pn i=1 xi dH(7r, Q) = 1− n ~ 0 if 7r(i) = Q(i) 3 The goal of much machine translation research is Z 1 if 7r (i) &lt; 7r (j) and Q(i) &gt; Q(j) where zij</context>
</contexts>
<marker>Hamming, 1950</marker>
<rawString>Richard Hamming. 1950. Error detecting and error correcting codes. Bell System Technical Journal, 26(2):147–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kendall</author>
<author>J Dickinson Gibbons</author>
</authors>
<title>Rank Correlation Methods.</title>
<date>1990</date>
<publisher>Oxford University Press,</publisher>
<location>New York.</location>
<contexts>
<context position="12419" citStr="Kendall and Gibbons, 1990" startWordPosition="2132" endWordPosition="2135"> 7r (j) and Q(i) &gt; Q(j) where zij = 0 otherwise Z = (n2 − n) 2 7r sequences. , Q are the two permutations and the normalization constant Z is n, the length of the permutation. We are interested in the Hamming distance for its ability to capture the amount of absolute disorder that exists between two permutations. The Hamming distance is widely utilized in coding theory to measure the discrepancy between two binary 2.2 Kendall s Tau Distance Kendall s tau distance is the minimum number of transpositions of two adjacent symbols necessary to transform one permutation into another (Kendall, 1938; Kendall and Gibbons, 1990). This is sometimes known as the swap distance or the inversion distance and can be interpreted as a function of the probability of observing concordant and discordan ’ ’ t pairs (Kerridge, 1975). It is defined as follows: s tau metric is possibly the most interesting for measuring reordering as it is sensitive to all relative orderings. It consequently measures not only how many reordering there are but also the distance that words are reordered. In statistics, Spearman s rho and Kendall s tau are widely used non-parametric measures of association for two rankings. In natural language process</context>
</contexts>
<marker>Kendall, Gibbons, 1990</marker>
<rawString>M. Kendall and J. Dickinson Gibbons. 1990. Rank Correlation Methods. Oxford University Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maurice Kendall</author>
</authors>
<title>A new measure of rank correlation.</title>
<date>1938</date>
<journal>Biometrika,</journal>
<pages>30--81</pages>
<contexts>
<context position="12391" citStr="Kendall, 1938" startWordPosition="2130" endWordPosition="2131">Z 1 if 7r (i) &lt; 7r (j) and Q(i) &gt; Q(j) where zij = 0 otherwise Z = (n2 − n) 2 7r sequences. , Q are the two permutations and the normalization constant Z is n, the length of the permutation. We are interested in the Hamming distance for its ability to capture the amount of absolute disorder that exists between two permutations. The Hamming distance is widely utilized in coding theory to measure the discrepancy between two binary 2.2 Kendall s Tau Distance Kendall s tau distance is the minimum number of transpositions of two adjacent symbols necessary to transform one permutation into another (Kendall, 1938; Kendall and Gibbons, 1990). This is sometimes known as the swap distance or the inversion distance and can be interpreted as a function of the probability of observing concordant and discordan ’ ’ t pairs (Kerridge, 1975). It is defined as follows: s tau metric is possibly the most interesting for measuring reordering as it is sensitive to all relative orderings. It consequently measures not only how many reordering there are but also the distance that words are reordered. In statistics, Spearman s rho and Kendall s tau are widely used non-parametric measures of association for two rankings.</context>
</contexts>
<marker>Kendall, 1938</marker>
<rawString>Maurice Kendall. 1938. A new measure of rank correlation. Biometrika, 30:81–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kerridge</author>
</authors>
<title>The interpretation of rank correlations.</title>
<date>1975</date>
<journal>Applied Statistics,</journal>
<pages>2--257</pages>
<contexts>
<context position="12614" citStr="Kerridge, 1975" startWordPosition="2169" endWordPosition="2170">ng distance for its ability to capture the amount of absolute disorder that exists between two permutations. The Hamming distance is widely utilized in coding theory to measure the discrepancy between two binary 2.2 Kendall s Tau Distance Kendall s tau distance is the minimum number of transpositions of two adjacent symbols necessary to transform one permutation into another (Kendall, 1938; Kendall and Gibbons, 1990). This is sometimes known as the swap distance or the inversion distance and can be interpreted as a function of the probability of observing concordant and discordan ’ ’ t pairs (Kerridge, 1975). It is defined as follows: s tau metric is possibly the most interesting for measuring reordering as it is sensitive to all relative orderings. It consequently measures not only how many reordering there are but also the distance that words are reordered. In statistics, Spearman s rho and Kendall s tau are widely used non-parametric measures of association for two rankings. In natural language processing research, Kendall s tau has been used as a means of estimating the distance between a system-generated and a human-generated goldstandard order for the sentence ordering task (Lapata, 2003). </context>
</contexts>
<marker>Kerridge, 1975</marker>
<rawString>D Kerridge. 1975. The interpretation of rank correlations. Applied Statistics, 2:257–258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Krauwer</author>
</authors>
<title>Evaluation of MT systems: a programmatic view.</title>
<date>1993</date>
<journal>Machine Translation,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="957" citStr="Krauwer, 1993" startWordPosition="139" endWordPosition="140">nslation. Current machine translation metrics do not adequately measure the reordering performance of translation systems. We present a novel metric, the LRscore, which directly measures reordering success. The reordering component is balanced by a lexical metric. Capturing the two most important elements of translation success in a simple combined metric with only one parameter results in an intuitive, shallow, language independent metric. 1 Introduction The main purpose of MT evaluation is to determine “to what extent the makers of a system have succeeded in mimicking the human translator” (Krauwer, 1993). But machine translation has no “ground truth” as there are many possible correct translations. It is impossible to judge whether a translation is incorrect or simply unknown and it is even harder to judge the the degree to which it is incorrect. Even so, automatic metrics are necessary. It is nearly impossible to collect enough human judgments for evaluating incremental improvements in research systems, or for tuning statistical machine translation system parameters. Automatic metrics are also much faster and cheaper than human evaluation and they produce reproducible results. Machine transl</context>
</contexts>
<marker>Krauwer, 1993</marker>
<rawString>S. Krauwer. 1993. Evaluation of MT systems: a programmatic view. Machine Translation, 8(1):59–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
</authors>
<title>Probabilistic text structuring: Experiments with sentence ordering.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>2</issue>
<contexts>
<context position="13212" citStr="Lapata, 2003" startWordPosition="2266" endWordPosition="2268">Kerridge, 1975). It is defined as follows: s tau metric is possibly the most interesting for measuring reordering as it is sensitive to all relative orderings. It consequently measures not only how many reordering there are but also the distance that words are reordered. In statistics, Spearman s rho and Kendall s tau are widely used non-parametric measures of association for two rankings. In natural language processing research, Kendall s tau has been used as a means of estimating the distance between a system-generated and a human-generated goldstandard order for the sentence ordering task (Lapata, 2003). Kendall s tau has also been used in machine translation as a cost function in a reordering model (Eisner and Tromble, 2006) and an MT metric called ROUGE-S (Lin an ’ ’ ’ ’ ’ d Och, score. If we use the 1-gram BLEU score, then the LRscore relies purely upon the reordering metric for all word ordering evaluation. We also use the 4-gram BLEU score, BLEU4, as it is an important baseline and the values it reports are very familiar to machine translation researchers. BLEU4 also contains a notion of word ordering based on longer matching n-grams. However, it is aware only of very local orderings. I</context>
</contexts>
<marker>Lapata, 2003</marker>
<rawString>Mirella Lapata. 2003. Probabilistic text structuring: Experiments with sentence ordering. Computational Linguistics, 29(2):263–317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>605--612</pages>
<location>Barcelona, Spain,</location>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 605–612, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Daniel Cer</author>
<author>Michel Galley</author>
<author>Dan Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Measuring machine translation quality as semantic equivalence: A metric based on entailment features. Machine Translation.</title>
<date>2009</date>
<marker>Pad´o, Cer, Galley, Jurafsky, Manning, 2009</marker>
<rawString>Sebastian Pad´o, Daniel Cer, Michel Galley, Dan Jurafsky, and Christopher D. Manning. 2009. Measuring machine translation quality as semantic equivalence: A metric based on entailment features. Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, USA.</location>
<contexts>
<context position="2112" citStr="Papineni et al., 2002" startWordPosition="326" endWordPosition="329">an evaluation and they produce reproducible results. Machine translation research relies heavily upon automatic metrics to evaluate the performance of models. However, current metrics rely upon indirect methods for measuring the quality of the word order, and their ability to capture reordering performance has been demonstrated to be poor (Birch et al., 2010). There are two main approaches to capturing reordering. The first way to measure the quality of word order is to count the number of matching n-grams between the reference and the hypothesis. This is the approach taken by the BLEU score (Papineni et al., 2002). This method discounts any n-gram which is not identical to a reference n-gram, and also does not consider the relative position of the strings. They can be anywhere in the sentence. Another common approach is typified by METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). They calculate an ordering penalty for a hypothesis based on the minimum number of chunks the translation needs to be broken into in order to align it to the reference. The disadvantage of the second approach is that aligning sentences with very different words can be inaccurate. Also there is no notion of how </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the Association for Computational Linguistics, pages 311–318, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Przybocki</author>
<author>Kay Peterson</author>
<author>S´ebastien Bronsart</author>
<author>Gregory Sanders</author>
</authors>
<title>The nist 2008 metrics for machine translation challengeoverview, methodology, metrics, and results. Machine Translation.</title>
<date>2009</date>
<contexts>
<context position="3336" citStr="Przybocki et al., 2009" startWordPosition="533" endWordPosition="536">ar these blocks are out of order. More sophisticated metrics, such as the RTE metric (Pad´o et al., 2009), use higher level syntactic or even semantic analysis to determine the quality of the translation. These approaches are useful, but can be very slow, require annotation, they are language dependent and their parameters are hard to train. For most research work shallow metrics are more appropriate. Apart from failing to capture reordering performance, another common criticism of most current automatic MT metrics is that a particular score value reported does not give insights into quality (Przybocki et al., 2009). This is because there is no intrinsic significance of a difference in scores. Ideally, the scores that the metrics report would be meaningful and stand on their own. However, the most one can say is that higher is better for accuracy metrics and lower is better for error metrics. We present a novel metric, the LRscore, which explicitly measures the quality of word order in machine translations. It then combines the reordering metric with a metric measuring lexical success. This results in a comprehensive met327 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and Metr</context>
</contexts>
<marker>Przybocki, Peterson, Bronsart, Sanders, 2009</marker>
<rawString>Mark Przybocki, Kay Peterson, S´ebastien Bronsart, and Gregory Sanders. 2009. The nist 2008 metrics for machine translation challengeoverview, methodology, metrics, and results. Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ronald</author>
</authors>
<title>More distance functions for orderbased encodings.</title>
<date>1998</date>
<booktitle>In the IEEE Conference on Evolutionary Computation,</booktitle>
<pages>558--563</pages>
<contexts>
<context position="11362" citStr="Ronald, 1998" startWordPosition="1935" endWordPosition="1936">on-sequential neighbors. tance metrics in order to easily compare them with other metrics where increases in the metrics mean increases in translation quality. All permutation distance metrics are thus subtracted from 1. Note that the two permutations we refer to 7r and Q are relative to the source sentence, and not to the reference: the source-reference permutation is compared to the source-translation permutation. 2.1 Hamming Distance The Hamming distance (Hamming, 1950) measures the number of disagreements between two 329 permutations. The Hamming distance for permutations was proposed by (Ronald, 1998) and is also known as the exact match distance. It is defined as follows: 2004) is similar to a Kendall’s tau metric on lexical items. ROUGE-S is an F-measure of ordered pairs of words in the translation. As far as we know, Kendall’s tau has not been used as a reordering metric before. LRscore where xi = 1 otherwise Pn i=1 xi dH(7r, Q) = 1− n ~ 0 if 7r(i) = Q(i) 3 The goal of much machine translation research is Z 1 if 7r (i) &lt; 7r (j) and Q(i) &gt; Q(j) where zij = 0 otherwise Z = (n2 − n) 2 7r sequences. , Q are the two permutations and the normalization constant Z is n, the length of the permut</context>
</contexts>
<marker>Ronald, 1998</marker>
<rawString>S Ronald. 1998. More distance functions for orderbased encodings. In the IEEE Conference on Evolutionary Computation, pages 558–563.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
<author>J Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In AMTA.</booktitle>
<contexts>
<context position="2398" citStr="Snover et al., 2006" startWordPosition="375" endWordPosition="378">rdering performance has been demonstrated to be poor (Birch et al., 2010). There are two main approaches to capturing reordering. The first way to measure the quality of word order is to count the number of matching n-grams between the reference and the hypothesis. This is the approach taken by the BLEU score (Papineni et al., 2002). This method discounts any n-gram which is not identical to a reference n-gram, and also does not consider the relative position of the strings. They can be anywhere in the sentence. Another common approach is typified by METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). They calculate an ordering penalty for a hypothesis based on the minimum number of chunks the translation needs to be broken into in order to align it to the reference. The disadvantage of the second approach is that aligning sentences with very different words can be inaccurate. Also there is no notion of how far these blocks are out of order. More sophisticated metrics, such as the RTE metric (Pad´o et al., 2009), use higher level syntactic or even semantic analysis to determine the quality of the translation. These approaches are useful, but can be very slow, require annotation, they are </context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, R Schwartz, L Micciulla, and J Makhoul. 2006. A study of translation edit rate with targeted human annotation. In AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Wong</author>
<author>C Kit</author>
</authors>
<title>ATEC: automatic evaluation of machine translation via word choice and word order. Machine Translation,</title>
<date>2009</date>
<pages>1--15</pages>
<contexts>
<context position="19472" citStr="Wong and Kit, 2009" startWordPosition="3347" endWordPosition="3350">ports the optimal consistency of the LRscore and baseline metrics with human judgements for each language pair. The table also reports the individual component results. The LRscore variations are named as follows: LR refers to the LRscore, “H” refers to the Hamming distance and “K” to Kendall’s tau distance. “B1” and “B4” refer to the smoothed BLEU score with the 1-gram and 4-gram scores. The LRscore is the metric which is most consistent with human judgement. This is an important result which shows that combining lexical and reordering information makes for a stronger metric. 5 Related Work (Wong and Kit, 2009) also suggest a metric which combines a word choice and a word order com331 ponent. They propose a type of F-measure which uses a matching function M to calculate precision and recall. M combines the number of matched words, weighted by their tfidf importance, with their position difference score, and finally subtracting a score for unmatched words. Including unmatched words in the in M function undermines the interpretation of the supposed Fmeasure. The reordering component is the average difference of absolute and relative word positions which has no clear meaning. This score is not intuitiv</context>
</contexts>
<marker>Wong, Kit, 2009</marker>
<rawString>B. Wong and C. Kit. 2009. ATEC: automatic evaluation of machine translation via word choice and word order. Machine Translation, pages 1–15.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>