<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.034352">
<title confidence="0.995713">
TectoMT: Highly Modular MT System
with Tectogrammatics Used as Transfer Layer*
</title>
<author confidence="0.999089">
Zdenˇek Zabokrtsk´y, Jan Pt´aˇcek, Petr Pajas
</author>
<affiliation confidence="0.8644">
Institute of Formal and Applied Linguistics
Charles University, Prague, Czech Republic
</affiliation>
<email confidence="0.998594">
{zabokrtsky,ptacek,pajas}@ufal.mff.cuni.cz
</email>
<sectionHeader confidence="0.996647" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998357">
We present a new English→Czech machine
translation system combining linguistically
motivated layers of language description (as
defined in the Prague Dependency Treebank
annotation scenario) with statistical NLP ap-
proaches.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965421052631">
We describe a new MT system (called Tec-
toMT) based on the conventional analysis-transfer-
synthesis architecture. We use the layers of language
description defined in the Prague Dependency Tree-
bank 2.0 (PDT for short, (Hajiˇc and others, 2006)),
namely (1) word layer – raw text, no linguistic
annotation, (2) morphological layer – sequence of
tagged and lemmatized tokens, (3) analytical layer
– each sentence represented as a surface-syntactic
dependency tree, and (4) tectogrammatical layer –
each sentence represented as a deep-syntactic de-
pendency tree in which only autosemantic words do
have nodes of their own; prefixes w-, m-, a-, or t-
will be used for denoting these layers.1
We use ‘Praguian’ tectogrammatics (introduced
in (Sgall, 1967)) as the transfer layer because
we believe that, first, it largely abstracts from
language-specific (inflection, agglutination, func-
tional words... ) means of expressing non-lexical
</bodyText>
<footnote confidence="0.9938758">
*The research reported in this paper is financially supported
by grants GAAV ˇCR 1ET101120503 and MSM0021620838.
1In addition, we use also p-layer (phrase structures) as an
a-layer alternative, the only reason for which is that we do not
have a working a-layer parser for English at this moment.
</footnote>
<bodyText confidence="0.998980676470588">
meanings, second, it allows for a natural transfer
factorization, and third, local tree contexts in t-trees
carry more information (esp. for lexical choice) than
local linear contexts in the original sentences.
In order to facilitate separating the transfer of lex-
icalization from the transfer of syntactization, we in-
troduce the concept of formeme. Each t-node’s has
a formeme attribute capturing which morphosyntac-
tic form has been (in the case of analysis) or will
be (synthesis) used for the t-node in the surface sen-
tence shape. Here are some examples of formemes
we use for English: n:subj (semantic noun (sn) in
subject position), n:for+X (sn with preposition for),
n:X+ago (sn with postposition ago), n:poss (posses-
sive form of sn), v:because+fin (semantic verb (sv)
as a subordinating finite clause introduced by be-
cause), v:without+ger (sv as a gerund after without),
adj:attr (semantic adjective (sa) in attributive posi-
tion), adj:compl (sa in complement position).
The presented system intensively uses the PDT
technology (data formats, software tools). Special
attention is paid to modularity: the translation is im-
plemented (in Perl) as a long sequence of processing
modules (called blocks) with relatively tiny, well-
defined tasks, so that each module is independently
testable, improvable, or substitutable. TectoMT al-
lows to easily combine blocks based on different
approaches, from blocks using complex probabilis-
tic solutions (e.g., B2, B6, B35, see the next section),
through blocks applying simpler Machine Learning
techniques (e.g., B69) or empirically based heuris-
tics (e.g., B7, B25, B36, B71), to blocks implementing
‘crisp’ linguistic rules (e.g., B48-B51, B59). There are
also blocks for trivial technical tasks (e.g., B33, B72).
</bodyText>
<page confidence="0.966004">
167
</page>
<note confidence="0.474458">
Proceedings of the Third Workshop on Statistical Machine Translation, pages 167–170,
</note>
<page confidence="0.426098">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<figureCaption confidence="0.974686666666667">
Figure 1: MT ‘pyramid’ as implemented in TectoMT. All the representations are rooted with artificial nodes, serving
only as labels. Virtually, the pyramid is bottomed with the input sentence on the source side (She has never laughed in
her new boss’s office.) and its automatic translation on the target side (Nikdy se nesm´ala v ´uˇradu sv´eho nov´eho ˇs´efa.).
</figureCaption>
<figure confidence="0.999112543209876">
She has
office
ADVP
VP
English t-layer
Czech t-layer
laugh
v:fin
smát_se
v:fin
#PersPron
n:subj never
adv:
English a-layer
English p-layer
S
laughed
boss
n:poss
office
n:in+X
#PersPron
n:1
nikdy
adv:
úřad
n:v+6
šéf
n:2
NPB
VP She has never
in
Nikdy
D........1A... seP7
never
laughed
in
PP
NPB
boss
.
#PersPron
n:poss
new
adj:attr
#PersPron
adj:attr
nový
adj:attr
Czech a-layer
nesmála
VpFS...3..NA..
Z.
vR
&apos;s
her new
úřadu
N.IS6.....A...
NPB
office .
her new
boss &apos;s
English m-layer
laughed
laugh VBN in
in IN her
her PRP$ new
new JJ boss
boss NN &apos;s
&apos;s POS office
office NN .. .
šéfa
N.MS2.....A...
She
she PRP has
have VBZ never
never RB
svého
P8MS2
nového
AAMS2....1A...
</figure>
<sectionHeader confidence="0.977927" genericHeader="method">
2 Translation Procedure
</sectionHeader>
<bodyText confidence="0.998554833333333">
The structure of this section directly renders the se-
quence of blocks currently used for English-Czech
translation in TectoMT. The intermediate stages of
the translation process are illustrated in Figure 1;
identifiers of the blocks affecting on the translation
of the sample sentence are typeset in bold.
</bodyText>
<subsectionHeader confidence="0.845507">
2.1 From English w-layer to English m-layer
</subsectionHeader>
<bodyText confidence="0.759133666666667">
B1: Segment the source English text into sentences.
B2: Split the sentences into sequences of tokens,
roughly according to Penn Treebank (PTB for short;
(Marcus et al., 1994)) conventions. B3: Tag the
tokens with PTB-style POS tags using a tagger
(Brants, 2000). B4: Fix some tagging errors sys-
tematically made by the tagger using a rule-based
corrector. B5: Lemmatize the tokens using morpha,
(Minnen et al., 2000).
</bodyText>
<subsectionHeader confidence="0.958473">
2.2 From English m-layer to English p-layer
</subsectionHeader>
<bodyText confidence="0.9282115">
B6: Build PTB-style phrase-structure tree for each
sentence using a parser (Collins, 1999).
</bodyText>
<subsectionHeader confidence="0.995318">
2.3 From English p-layer to English a-layer
</subsectionHeader>
<bodyText confidence="0.9838819">
B7: In each phrase, mark the head node (using a set
of heuristic rules). B8: Convert phrase-structure trees
to a-trees. B9: Apply some heuristic rules to fix ap-
position constructions. B10: Apply another heuris-
tic rules for reattaching incorrectly positioned nodes.
B11: Unify the way in which multiword prepositions
(such as because of) and subordinating conjunctions
(such as provided that) are treated. B12: Assign an-
alytical functions (only if necessary for a correct
treatment of coordination/apposition constructions).
</bodyText>
<subsectionHeader confidence="0.989567">
2.4 From English a-layer to English t-layer
</subsectionHeader>
<bodyText confidence="0.999962259259259">
B13: Mark a-nodes which are auxiliary (such as
prepositions, subordinating conjunctions, auxiliary
verbs, selected types of particles, etc.) B14: Mark not
as an auxiliary node too (but only if it is connected to
a verb form). B15: Build t-trees. Each a-node cluster
formed by an autosemantic node and possibly sev-
eral associated auxiliary nodes is ‘collapsed’ into a
single t-node. T-tree dependency edges are derived
from a-tree edges connecting the a-node clusters.
B16: Explicitely distinguish t-nodes that are mem-
bers of coordination (conjuncts) from shared modi-
fiers. It is necessary as they all are attached below
the coordination conjunction t-node. B17: Modify
t-lemmas in specific cases. E.g., all kinds of per-
sonal pronouns are represented by the ‘artificial’ t-
lemma #PersPron. B18: Assign functors that are nec-
essary for proper treatment of coordination and ap-
position constructions. B19: Distribute shared auxil-
iary words in coordination constructions. B20: Mark
t-nodes that are roots of t-subtrees corresponding to
finite verb clauses. B21: Mark passive verb forms.
B22: Assign (a subset of) functors. B23: Mark t-nodes
corresponding to infinitive verbs. B24: Mark t-nodes
which are roots of t-subtrees corresponding to rel-
ative clauses. B25: Identify coreference links be-
tween relative pronouns (or other relative pronom-
inal word) and their nominal antecedents. B26: Mark
</bodyText>
<page confidence="0.996799">
168
</page>
<bodyText confidence="0.999983272727273">
t-nodes that are the roots of t-subtrees correspond-
ing to direct speeches. B27: Mark t-nodes that are
the roots of t-subtrees corresponding to parenthe-
sized expressions. B28: Fill the nodetype attribute
(rough classification of t-nodes). B29: Fill the sem-
pos attribute (fine-grained classification of t-nodes).
B30: Fill the grammateme attributes (semantically in-
dispensable morphological categories, such as num-
ber for nouns, tense for verbs). B31: Determine the
formeme of each t-node. B32: Mark personal names,
distinguish male and female first names if possible.
</bodyText>
<subsectionHeader confidence="0.985995">
2.5 From English t-layer to Czech t-layer
</subsectionHeader>
<bodyText confidence="0.9463947">
B33: Initiate the target-side t-trees, simply by cloning
the source-side t-trees. B34: In each t-node, trans-
late its formeme.2 B35: Translate t-lemma in each
t-node as its most probable target-language counter-
part (which is compliant with the previously chosen
formeme), according to a probabilistic dictionary.3
B36: Apply manual rules for fixing the formeme and
lexeme choices, which are otherwise systematically
wrong and are reasonably frequent. B37: Fill the gen-
der grammateme in t-nodes corresponding to deno-
tative nouns (it follows from the chosen t-lemma).4
B38: Fill the aspect grammateme in t-nodes corre-
sponding to verbs. Information about aspect (perfec-
tive/imperfective) is necessary for making decisions
about forming complex future tense in Czech. B39:
Apply rule-based correction of translated date/time
expressions (several templates such as 1970’s, July
1, etc.). B40: Fix grammateme values in places where
the English-Czech grammateme correspondence is
not trivial (e.g., if an English gerund expression
is translated using Czech subordinating clause, the
2The translation mapping from English formemes to Czech
formemes was obtained as follows: we analyzed 10,000 sen-
tence pairs from the WMT’08 training data up to the t-layer
(using a tagger shipped with the PDT and parser (McDonald et
al., 2005) for Czech), added formemes to t-trees on both sides,
aligned the t-trees (using a set of weighted heuristic rules, simi-
larly to (Menezes and Richardson, 2001)), and from the aligned
t-node pairs extracted for each English formeme its most fre-
quent Czech counterpart.
</bodyText>
<footnote confidence="0.999151">
3The dictionary was created by merging the translation dic-
tionary from PCEDT ((Cuˇrin and others, 2004)) and a trans-
lation dictionary extracted from a part of the parallel corpus
Czeng ((Bojar and ˇZabokrtsk´y, 2006)) aligned at word-level by
Giza++ ((Och and Ney, 2003)).
4Czech nouns have grammatical gender which is (among
others) important for resolving grammatical agreement.
</footnote>
<bodyText confidence="0.999205235294118">
tense grammateme has to be filled). B41: Negate
verb forms where some arguments of the verbs bear
negative meaning (double negation in Czech). B42:
Verb t-nodes in active voice that have transitive t-
lemma and no accusative object, are turned to re-
flexives. B43: The t-nodes with genitive formeme
or prepositional-group formeme, whose counterpart
English t-nodes are located in pre-modification po-
sition, are moved to post-modification position. B44:
Reverse the dependency orientation between nu-
meric expressions and counted nouns, if the value
of the numeric expression is greater than four and
the noun without the numeral would be expressed in
nominative or accusative case. B45: Find coreference
links from personal pronouns to their antecedents,
if the latter are in subject position (needed later for
reflexivization).
</bodyText>
<subsectionHeader confidence="0.986933">
2.6 From Czech t-layer to Czech a-layer
</subsectionHeader>
<bodyText confidence="0.907294586206896">
B46: Create initial a-trees by cloning t-trees. B47:
Fill the surface morphological categories (gender,
number, case, negation, etc.) with values derived
from values of grammatemes, formeme, seman-
tic part of speech etc. B48: Propagate the values
of gender and number of relative pronouns from
their antecedents (along the coreference links). B49:
Propagate the values of gender, number and person
according to the subject-predicate agreement (i.e.,
from subjects to the finite verbs). B50: Resolve agree-
ment of adjectivals in attributive positions (copying
gender/number/case from their governing nouns).
B51: Resolve complement agreement (copying gen-
der/number from subject to adjectival complement).
B52: Apply pro-drop – deletion of personal pronouns
in subject positions. B53: Add preposition a-nodes
(if implied by the t-node’s formeme). B54: Add a-
nodes for subordinating conjunction (if implied by
the t-node’s formeme). B55: Add a-nodes corre-
sponding to reflexive particles for reflexiva tantum
verbs. B56: Add an a-node representing the auxiliary
verb byt (to be) in the case of compound passive
verb forms. B57: Add a-nodes representing modal
verbs, accordingly to the deontic modality gram-
mateme. B58: Add the auxiliary verb byt in imperfec-
tive future-tense complex verb forms. B59: Add verb
forms such as by/bys/bychom expressing conditional
verb modality. B60: Add auxiliary verb forms such
as jsem/jste in past-tense complex verb forms. B61:
</bodyText>
<page confidence="0.996298">
169
</page>
<bodyText confidence="0.999844052631579">
Partition a-trees into finite clauses (a-nodes belong-
ing to the same clause are coindexed). B62: In each
clause, a-nodes which represent clitics are moved to
the so called second position in the clause (accord-
ing to Wackernagel’s law). B63: Add a-nodes cor-
responding to sentence-final punctuation mark. B64:
Add a-nodes corresponding to commas on bound-
aries between governing and subordinated clauses.
B65: Add a-nodes corresponding to commas in front
of conjunction ale and also commas in multiple co-
ordinations. B66: Add pairs of parenthesis a-nodes.
B67: Choose morphological lemmas in a-nodes cor-
responding to personal pronouns. B68: Generate
the resulting word forms (derived from lemmas and
tags) using Czech word form generator described in
(Hajiˇc, 2004). B69: Vocalize prepositions k, s, v, and
z (accordingly to the prefix of the following word).
B70: Capitalize the first word in each sentence as well
as in each direct speech.
</bodyText>
<subsectionHeader confidence="0.975187">
2.7 From Czech a-layer to Czech w-layer
</subsectionHeader>
<bodyText confidence="0.99786825">
B71: Create the resulting sentences by flattening the
a-trees. Heuristic rules for proper spacing around
punctuation marks are used. B72: Create the resulting
text by concatenating the resulting sentences.
</bodyText>
<sectionHeader confidence="0.99767" genericHeader="method">
3 Final remarks
</sectionHeader>
<bodyText confidence="0.999992035714286">
We believe that the potential contribution of tec-
togrammatical layer of language representation for
MT is the following: it abstracts from many
language-specific phenomena (which could reduce
the notorious data-sparsity problem) and offers a
natural factorization of the translation task (which
could be useful for formulating independence as-
sumptions when building probabilistic models). Of
course, the question naturally arises whether these
properties can ever outbalance the disadvantages, es-
pecially cumulation and interference of errors made
on different layers, considerable technical complex-
ity, and the need for detailed linguistic insight. In
our opinion, this question still remains open. On
one hand, the translation quality offered now by Tec-
toMT is below the state-of-the-art system according
to the preliminary evaluation of the WMT08 Shared
Task. But on the other hand, the potential of tec-
togrammatics has not been used fully, and more-
over there are still many components with only pilot
heuristic implementation which increase the number
of translation errors and which can be relatively eas-
ily substituted by corpus-based solutions. In the near
future, we plan to focus especially on the transfer
blocks, which are currently based on the naive as-
sumption of isomorphism of the source and target
t-trees and which do not make use of the target lan-
guage model, so far.
</bodyText>
<sectionHeader confidence="0.99889" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999853902439024">
Ondˇrej Bojar and Zdenˇek ˇZabokrtsk´y. 2006. CzEng:
Czech-English Parallel Corpus, Release version 0.5.
Prague Bulletin of Mathematical Linguistics, 86:59–
62.
Thorsten Brants. 2000. TnT - A Statistical Part-of-
Speech Tagger . pages 224–231, Seattle.
Michael Collins. 1999. Head-driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia.
Jan Cuˇr´ın et al. 2004. Prague Czech - English Depen-
dency Treebank, Version 1.0. CD-ROM, Linguistics
Data Consortium, LDC Catalog No.: LDC2004T25,
Philadelphia.
Jan Hajiˇc et al. 2006. Prague Dependency Treebank 2.0.
CD-ROM, Linguistic Data Consortium, LDC Catalog
No.: LDC2006T01, Philadelphia.
Jan Hajiˇc. 2004. Disambiguation of Rich Inflection –
Computational Morphology of Czech. Charles Uni-
versity – The Karolinum Press, Prague.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005. Non-Projective Dependency Pars-
ing using Spanning Tree Algorithms. In Proceedings
of HTL/EMNLP, pages 523–530, Vancouver, Canada.
Arul Menezes and Stephen D. Richardson. 2001. A best-
first alignment algorithm for automatic extraction of
transfer mappings from bilingual corpora. In Proceed-
ings of the workshop on Data-driven methods in ma-
chine translation, volume 14, pages 1–8.
Guido Minnen, John Carroll, and Darren Pearce. 2000.
Robust Applied Morphological Generation. In Pro-
ceedings of the 1st International Natural Language
Generation Conference, pages 201–208, Israel.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19–51.
Petr Sgall. 1967. Generativnipopis jazyka a ˇcesk´a dek-
linace. Academia, Prague.
</reference>
<page confidence="0.997426">
170
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.404800">
<title confidence="0.992765">TectoMT: Highly Modular MT Tectogrammatics Used as Transfer</title>
<author confidence="0.954349">Zdenˇek Zabokrtsk´y</author>
<author confidence="0.954349">Jan Pt´aˇcek</author>
<author confidence="0.954349">Petr</author>
<affiliation confidence="0.6768805">Institute of Formal and Applied Charles University, Prague, Czech</affiliation>
<abstract confidence="0.976840428571428">present a new machine translation system combining linguistically motivated layers of language description (as defined in the Prague Dependency Treebank annotation scenario) with statistical NLP approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Zdenˇek ˇZabokrtsk´y</author>
</authors>
<title>CzEng: Czech-English Parallel Corpus, Release version 0.5.</title>
<date>2006</date>
<journal>Prague Bulletin of Mathematical Linguistics,</journal>
<volume>86</volume>
<pages>62</pages>
<marker>Bojar, ˇZabokrtsk´y, 2006</marker>
<rawString>Ondˇrej Bojar and Zdenˇek ˇZabokrtsk´y. 2006. CzEng: Czech-English Parallel Corpus, Release version 0.5. Prague Bulletin of Mathematical Linguistics, 86:59– 62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT - A Statistical Part-ofSpeech Tagger .</title>
<date>2000</date>
<pages>224--231</pages>
<location>Seattle.</location>
<contexts>
<context position="5323" citStr="Brants, 2000" startWordPosition="809" endWordPosition="810">e The structure of this section directly renders the sequence of blocks currently used for English-Czech translation in TectoMT. The intermediate stages of the translation process are illustrated in Figure 1; identifiers of the blocks affecting on the translation of the sample sentence are typeset in bold. 2.1 From English w-layer to English m-layer B1: Segment the source English text into sentences. B2: Split the sentences into sequences of tokens, roughly according to Penn Treebank (PTB for short; (Marcus et al., 1994)) conventions. B3: Tag the tokens with PTB-style POS tags using a tagger (Brants, 2000). B4: Fix some tagging errors systematically made by the tagger using a rule-based corrector. B5: Lemmatize the tokens using morpha, (Minnen et al., 2000). 2.2 From English m-layer to English p-layer B6: Build PTB-style phrase-structure tree for each sentence using a parser (Collins, 1999). 2.3 From English p-layer to English a-layer B7: In each phrase, mark the head node (using a set of heuristic rules). B8: Convert phrase-structure trees to a-trees. B9: Apply some heuristic rules to fix apposition constructions. B10: Apply another heuristic rules for reattaching incorrectly positioned nodes.</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. TnT - A Statistical Part-ofSpeech Tagger . pages 224–231, Seattle.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia.</location>
<contexts>
<context position="5613" citStr="Collins, 1999" startWordPosition="854" endWordPosition="855">e typeset in bold. 2.1 From English w-layer to English m-layer B1: Segment the source English text into sentences. B2: Split the sentences into sequences of tokens, roughly according to Penn Treebank (PTB for short; (Marcus et al., 1994)) conventions. B3: Tag the tokens with PTB-style POS tags using a tagger (Brants, 2000). B4: Fix some tagging errors systematically made by the tagger using a rule-based corrector. B5: Lemmatize the tokens using morpha, (Minnen et al., 2000). 2.2 From English m-layer to English p-layer B6: Build PTB-style phrase-structure tree for each sentence using a parser (Collins, 1999). 2.3 From English p-layer to English a-layer B7: In each phrase, mark the head node (using a set of heuristic rules). B8: Convert phrase-structure trees to a-trees. B9: Apply some heuristic rules to fix apposition constructions. B10: Apply another heuristic rules for reattaching incorrectly positioned nodes. B11: Unify the way in which multiword prepositions (such as because of) and subordinating conjunctions (such as provided that) are treated. B12: Assign analytical functions (only if necessary for a correct treatment of coordination/apposition constructions). 2.4 From English a-layer to En</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Cuˇr´ın</author>
</authors>
<title>Prague Czech - English Dependency Treebank,</title>
<date>2004</date>
<booktitle>Version 1.0. CD-ROM, Linguistics Data Consortium, LDC Catalog No.: LDC2004T25,</booktitle>
<location>Philadelphia.</location>
<marker>Cuˇr´ın, 2004</marker>
<rawString>Jan Cuˇr´ın et al. 2004. Prague Czech - English Dependency Treebank, Version 1.0. CD-ROM, Linguistics Data Consortium, LDC Catalog No.: LDC2004T25, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
</authors>
<date>2006</date>
<booktitle>Prague Dependency Treebank 2.0. CD-ROM, Linguistic Data Consortium, LDC Catalog No.: LDC2006T01,</booktitle>
<location>Philadelphia.</location>
<marker>Hajiˇc, 2006</marker>
<rawString>Jan Hajiˇc et al. 2006. Prague Dependency Treebank 2.0. CD-ROM, Linguistic Data Consortium, LDC Catalog No.: LDC2006T01, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
</authors>
<date>2004</date>
<booktitle>Disambiguation of Rich Inflection – Computational Morphology of Czech.</booktitle>
<publisher>Charles University – The Karolinum Press,</publisher>
<location>Prague.</location>
<marker>Hajiˇc, 2004</marker>
<rawString>Jan Hajiˇc. 2004. Disambiguation of Rich Inflection – Computational Morphology of Czech. Charles University – The Karolinum Press, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1994</date>
<contexts>
<context position="5236" citStr="Marcus et al., 1994" startWordPosition="793" endWordPosition="796">e she PRP has have VBZ never never RB svého P8MS2 nového AAMS2....1A... 2 Translation Procedure The structure of this section directly renders the sequence of blocks currently used for English-Czech translation in TectoMT. The intermediate stages of the translation process are illustrated in Figure 1; identifiers of the blocks affecting on the translation of the sample sentence are typeset in bold. 2.1 From English w-layer to English m-layer B1: Segment the source English text into sentences. B2: Split the sentences into sequences of tokens, roughly according to Penn Treebank (PTB for short; (Marcus et al., 1994)) conventions. B3: Tag the tokens with PTB-style POS tags using a tagger (Brants, 2000). B4: Fix some tagging errors systematically made by the tagger using a rule-based corrector. B5: Lemmatize the tokens using morpha, (Minnen et al., 2000). 2.2 From English m-layer to English p-layer B6: Build PTB-style phrase-structure tree for each sentence using a parser (Collins, 1999). 2.3 From English p-layer to English a-layer B7: In each phrase, mark the head node (using a set of heuristic rules). B8: Convert phrase-structure trees to a-trees. B9: Apply some heuristic rules to fix apposition construc</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1994. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-Projective Dependency Parsing using Spanning Tree Algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of HTL/EMNLP,</booktitle>
<pages>523--530</pages>
<location>Vancouver, Canada.</location>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005. Non-Projective Dependency Parsing using Spanning Tree Algorithms. In Proceedings of HTL/EMNLP, pages 523–530, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arul Menezes</author>
<author>Stephen D Richardson</author>
</authors>
<title>A bestfirst alignment algorithm for automatic extraction of transfer mappings from bilingual corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of the workshop on Data-driven methods in machine translation,</booktitle>
<volume>14</volume>
<pages>1--8</pages>
<contexts>
<context position="9704" citStr="Menezes and Richardson, 2001" startWordPosition="1466" endWordPosition="1469">es such as 1970’s, July 1, etc.). B40: Fix grammateme values in places where the English-Czech grammateme correspondence is not trivial (e.g., if an English gerund expression is translated using Czech subordinating clause, the 2The translation mapping from English formemes to Czech formemes was obtained as follows: we analyzed 10,000 sentence pairs from the WMT’08 training data up to the t-layer (using a tagger shipped with the PDT and parser (McDonald et al., 2005) for Czech), added formemes to t-trees on both sides, aligned the t-trees (using a set of weighted heuristic rules, similarly to (Menezes and Richardson, 2001)), and from the aligned t-node pairs extracted for each English formeme its most frequent Czech counterpart. 3The dictionary was created by merging the translation dictionary from PCEDT ((Cuˇrin and others, 2004)) and a translation dictionary extracted from a part of the parallel corpus Czeng ((Bojar and ˇZabokrtsk´y, 2006)) aligned at word-level by Giza++ ((Och and Ney, 2003)). 4Czech nouns have grammatical gender which is (among others) important for resolving grammatical agreement. tense grammateme has to be filled). B41: Negate verb forms where some arguments of the verbs bear negative mea</context>
</contexts>
<marker>Menezes, Richardson, 2001</marker>
<rawString>Arul Menezes and Stephen D. Richardson. 2001. A bestfirst alignment algorithm for automatic extraction of transfer mappings from bilingual corpora. In Proceedings of the workshop on Data-driven methods in machine translation, volume 14, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>John Carroll</author>
<author>Darren Pearce</author>
</authors>
<title>Robust Applied Morphological Generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st International Natural Language Generation Conference,</booktitle>
<pages>201--208</pages>
<contexts>
<context position="5477" citStr="Minnen et al., 2000" startWordPosition="832" endWordPosition="835">tages of the translation process are illustrated in Figure 1; identifiers of the blocks affecting on the translation of the sample sentence are typeset in bold. 2.1 From English w-layer to English m-layer B1: Segment the source English text into sentences. B2: Split the sentences into sequences of tokens, roughly according to Penn Treebank (PTB for short; (Marcus et al., 1994)) conventions. B3: Tag the tokens with PTB-style POS tags using a tagger (Brants, 2000). B4: Fix some tagging errors systematically made by the tagger using a rule-based corrector. B5: Lemmatize the tokens using morpha, (Minnen et al., 2000). 2.2 From English m-layer to English p-layer B6: Build PTB-style phrase-structure tree for each sentence using a parser (Collins, 1999). 2.3 From English p-layer to English a-layer B7: In each phrase, mark the head node (using a set of heuristic rules). B8: Convert phrase-structure trees to a-trees. B9: Apply some heuristic rules to fix apposition constructions. B10: Apply another heuristic rules for reattaching incorrectly positioned nodes. B11: Unify the way in which multiword prepositions (such as because of) and subordinating conjunctions (such as provided that) are treated. B12: Assign a</context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2000</marker>
<rawString>Guido Minnen, John Carroll, and Darren Pearce. 2000. Robust Applied Morphological Generation. In Proceedings of the 1st International Natural Language Generation Conference, pages 201–208, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="10083" citStr="Och and Ney, 2003" startWordPosition="1526" endWordPosition="1529"> t-layer (using a tagger shipped with the PDT and parser (McDonald et al., 2005) for Czech), added formemes to t-trees on both sides, aligned the t-trees (using a set of weighted heuristic rules, similarly to (Menezes and Richardson, 2001)), and from the aligned t-node pairs extracted for each English formeme its most frequent Czech counterpart. 3The dictionary was created by merging the translation dictionary from PCEDT ((Cuˇrin and others, 2004)) and a translation dictionary extracted from a part of the parallel corpus Czeng ((Bojar and ˇZabokrtsk´y, 2006)) aligned at word-level by Giza++ ((Och and Ney, 2003)). 4Czech nouns have grammatical gender which is (among others) important for resolving grammatical agreement. tense grammateme has to be filled). B41: Negate verb forms where some arguments of the verbs bear negative meaning (double negation in Czech). B42: Verb t-nodes in active voice that have transitive tlemma and no accusative object, are turned to reflexives. B43: The t-nodes with genitive formeme or prepositional-group formeme, whose counterpart English t-nodes are located in pre-modification position, are moved to post-modification position. B44: Reverse the dependency orientation betw</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Sgall</author>
</authors>
<title>Generativnipopis jazyka a ˇcesk´a deklinace.</title>
<date>1967</date>
<location>Academia, Prague.</location>
<contexts>
<context position="1248" citStr="Sgall, 1967" startWordPosition="175" endWordPosition="176">language description defined in the Prague Dependency Treebank 2.0 (PDT for short, (Hajiˇc and others, 2006)), namely (1) word layer – raw text, no linguistic annotation, (2) morphological layer – sequence of tagged and lemmatized tokens, (3) analytical layer – each sentence represented as a surface-syntactic dependency tree, and (4) tectogrammatical layer – each sentence represented as a deep-syntactic dependency tree in which only autosemantic words do have nodes of their own; prefixes w-, m-, a-, or twill be used for denoting these layers.1 We use ‘Praguian’ tectogrammatics (introduced in (Sgall, 1967)) as the transfer layer because we believe that, first, it largely abstracts from language-specific (inflection, agglutination, functional words... ) means of expressing non-lexical *The research reported in this paper is financially supported by grants GAAV ˇCR 1ET101120503 and MSM0021620838. 1In addition, we use also p-layer (phrase structures) as an a-layer alternative, the only reason for which is that we do not have a working a-layer parser for English at this moment. meanings, second, it allows for a natural transfer factorization, and third, local tree contexts in t-trees carry more inf</context>
</contexts>
<marker>Sgall, 1967</marker>
<rawString>Petr Sgall. 1967. Generativnipopis jazyka a ˇcesk´a deklinace. Academia, Prague.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>