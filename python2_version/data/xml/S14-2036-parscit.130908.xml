<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000084">
<title confidence="0.990582">
DCU: Aspect-based Polarity Classification for SemEval Task 4
</title>
<author confidence="0.989627">
Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab Barman
Dasha Bogdanova, Jennifer Foster and Lamia Tounsi
</author>
<affiliation confidence="0.99446425">
CNGL Centre for Global Intelligent Content
National Centre for Language Technology
School of Computing
Dublin City University
</affiliation>
<address confidence="0.65863">
Dublin, Ireland
</address>
<email confidence="0.9903405">
{jwagner,parora,scortes,ubarman}@computing.dcu.ie
{dbogdanova,jfoster,ltounsi}@computing.dcu.ie
</email>
<sectionHeader confidence="0.998558" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999983111111111">
We describe the work carried out by DCU
on the Aspect Based Sentiment Analysis
task at SemEval 2014. Our team submit-
ted one constrained run for the restaurant
domain and one for the laptop domain for
sub-task B (aspect term polarity predic-
tion), ranking highest out of 36 systems on
the restaurant test set and joint highest out
of 32 systems on the laptop test set.
</bodyText>
<sectionHeader confidence="0.999391" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999688272727273">
This paper describes DCU’s participation in the
Aspect Term Polarity sub-task of the Aspect Based
Sentiment Analysis task at SemEval 2014, which
focuses on predicting the sentiment polarity of as-
pect terms for a restaurant and a laptop dataset.
Given, for example, the sentence I have had so
many problems with the computer and the aspect
term the computer, the task is to predict whether
the sentiment expressed towards the aspect term is
positive, negative, neutral or conflict.
Our polarity classification system uses super-
vised machine learning with support vector ma-
chines (SVM) (Boser et al., 1992) to classify an
aspect term into one of the four classes. The fea-
tures we employ are word n-grams (with n rang-
ing from 1 to 5) in a window around the aspect
term, as well as features derived from scores as-
signed by a sentiment lexicon. Furthermore, to
reduce data sparsity, we experiment with replacing
sentiment-bearing words in our n-gram feature set
with their polarity scores according to the lexicon
and/or their part-of-speech tag.
</bodyText>
<footnote confidence="0.90547775">
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.999745411764706">
The paper is organised as follows: in Section 2,
we describe the sentiment lexicons used in this
work and detail the process by which they are
combined, filtered and extended; in Section 3, we
describe our baseline method, a heuristic approach
which makes use of the sentiment lexicon, fol-
lowed by our machine learning method which in-
corporates the rule-based method as features in ad-
dition to word n-gram features; in Section 4, we
present the results of both methods on the training
and test data, and perform an error analysis on the
test set; in Section 5, we compare our approach to
previous research in sentiment classification; Sec-
tion 6 discusses efficiency of our system and on-
going work to improve its speed; finally, in Sec-
tion 7, we conclude and provide suggestions as to
how this research could be fruitfully extended.
</bodyText>
<sectionHeader confidence="0.983889" genericHeader="method">
2 Sentiment Lexicons
</sectionHeader>
<bodyText confidence="0.973892">
The following four lexicons are employed:
</bodyText>
<listItem confidence="0.9823195">
1. MPQA1 (Wilson et al., 2005) classifies a
word or a stem and its part of speech tag
into positive, negative, both or neutral with
a strong or weak subjectivity.
2. SentiWordNet2 (Baccianella et al., 2010)
specifies the positive, negative and objective
scores of a synset and its part of speech tag.
3. General Inquirer3 indicates whether a word
expresses positive or negative sentiment.
4. Bing Liu’s Opinion Lexicon4 (Hu and Liu,
</listItem>
<footnote confidence="0.962543428571428">
1http://mpqa.cs.pitt.edu/lexicons/
subj_lexicon/
2http://sentiwordnet.isti.cnr.it/
3http://www.wjh.harvard.edu/˜inquirer/
inqtabs.txt
4http://www.cs.uic.edu/˜liub/FBS/
sentiment-analysis.html#lexicon
</footnote>
<page confidence="0.969897">
223
</page>
<note confidence="0.7297205">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 223–229,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.962578">
2004) indicates whether a word expresses
positive or negative sentiment.
</bodyText>
<subsectionHeader confidence="0.99428">
2.1 Lexicon Combination
</subsectionHeader>
<bodyText confidence="0.999940166666667">
Since the four lexicons differ in their level of detail
and in how they present information, it is neces-
sary, when combining them, to consolidate the in-
formation and present it in a uniform manner. Our
combination strategy assigns a sentiment score to
a word as follows:
</bodyText>
<listItem confidence="0.803138363636364">
• MPQA: 1 for strong positive subjectivity, -1
for strong negative subjectivity, 0.5 for weak
positive subjectivity, -0.5 for weak negative
subjectivity, and 0 otherwise
• SentiWordNet: The positive score if the pos-
itive score is greater than the negative and ob-
jective scores, the negative score if the nega-
tive score is greater than the positive and the
objective scores, and 0 otherwise
• General Inquirer and Bing Liu’s Opinion
Lexicon: 1 for positive and -1 for negative
</listItem>
<bodyText confidence="0.9992935">
The above four scores are summed to arrive at a
final score between -4 and 4 for a word.5
</bodyText>
<subsectionHeader confidence="0.999679">
2.2 Lexicon Filtering
</subsectionHeader>
<bodyText confidence="0.999955888888889">
Initial experiments with our sentiment lexicon and
the training data led us to believe that there were
many irrelevant entries that, although capable of
conveying sentiment in some other context, were
not contributing to the sentiment of aspect terms
in the two domains of the task. Therefore, these
words are manually filtered from the lexicon. Ex-
amples of deleted words are just, clearly, indi-
rectly, really and back.
</bodyText>
<subsectionHeader confidence="0.999928">
2.3 Adding Domain-Specific Words
</subsectionHeader>
<bodyText confidence="0.999908625">
A manual inspection of the training data revealed
words missing from the merged sentiment lexicon
but which do express sentiment in these domains.
Examples are mouthwatering, watery and better-
configured. We add these to the lexicon with a
score of either 1 or -1 (depending on their polarity
in the training data). We also add words (e.g. zesty,
acrid) from an online list of culinary terms.6
</bodyText>
<footnote confidence="0.9936975">
5We also tried to vote over the four lexicon scores but this
did not improve over summing.
6http://world-food-and-wine.com/
describing-food
</footnote>
<subsectionHeader confidence="0.998272">
2.4 Handling Variation
</subsectionHeader>
<bodyText confidence="0.999932285714285">
In order to ensure that all inflected forms of a
word are covered, we lemmatise the words in the
training data using the IMS TreeTagger (Schmid,
1994) and we construct new possibilities using a
suffix list. To correct misspelled words, we con-
sider the corrected form of a misspelled word to be
the form with the highest frequency in a reference
corpus7 among all the forms within an edit dis-
tance of 1 and 2 from the misspelled word (Norvig,
2012). Multi-word expressions of the form x-y
are added with the polarity of xy or x, as in laid-
back/laidback and well-shaped/well. Expressions
x y, are added with the polarity of x-y, as in so
so/so-so.
</bodyText>
<sectionHeader confidence="0.999165" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.999887857142857">
We first build a rule-based system which classi-
fies the polarity of an aspect term based solely on
the scores assigned by the sentiment lexicon. We
then explore different ways of converting the rule-
based system into features which can then be com-
bined with bag-of-n-gram features in a supervised
machine learning set-up.
</bodyText>
<subsectionHeader confidence="0.996841">
3.1 Rule-Based Approach
</subsectionHeader>
<bodyText confidence="0.999322384615384">
In order to predict the polarity of an aspect term,
we sum the polarity scores of all the words in the
surrounding sentence according to our sentiment
lexicon. Since not all the sentiment words occur-
ring in a sentence influence the polarity of the as-
pect term to the same extent, it is important to
weight the score of each sentiment word by its dis-
tance to the aspect term. Therefore, for each word
in the sentence which is found in our lexicon we
take the score from the lexicon and divide it by its
distance to the aspect term. The distance is calcu-
lated using the sum of the following three distance
functions:
</bodyText>
<listItem confidence="0.99584825">
• Token Distance: This function calculates the
difference in the position of the sentiment
word and the aspect term by counting the to-
kens between them.
</listItem>
<footnote confidence="0.817645333333333">
7The reference corpus consists of about a million
words retrieved from several public domain books from
Project Gutenberg (http://www.gutenberg.org/),
</footnote>
<bodyText confidence="0.9215295">
lists of most frequent words from Wiktionary (http:
//en.wiktionary.org/wiki/Wiktionary:
Frequency_lists) and the British National Corpus
(http://www.kilgarriff.co.uk/bnc-readme.
html) and two thousand laptop reviews crawled from CNET
(http://www.cnet.com/).
</bodyText>
<page confidence="0.985782">
224
</page>
<bodyText confidence="0.882884125">
• Discourse Chunk Distance: This function
counts the discourse chunks that must be
crossed in order to get from the sentiment
word to the aspect term. If the sentiment
word and the aspect term are in the same
discourse chunk, then the distance is zero.
We use the discourse segmenter described in
(Tofiloski et al., 2009).
</bodyText>
<listItem confidence="0.615103090909091">
• Dependency Path Distance: This function
calculates the shortest path between the sen-
timent word and the aspect term in a syntac-
tic dependency graph for the sentence, pro-
duced by parsing the sentence with a PCFG-
LA parser (Attia et al., 2010) trained on con-
sumer review data (Le Roux et al., 2012)8,
and converting the resulting phrase-structure
tree into a dependency graph using the Stan-
ford converter (de Marneffe and Manning,
2008) (version 3.3.1).
</listItem>
<bodyText confidence="0.999884125">
Since our lexicon also contains multi-word ex-
pressions such as finger licking, we also look up
bigrams and trigrams from the input sentence in
our lexicon. Negation is handled by reversing the
polarity of sentiment words that appear within a
window of three words of the following negators:
not, n’t, no and never.
For each aspect term, we use the distance-
weighted sum of the polarity scores to predict one
of the three classes positive, negative and neutral.9
After experimenting with various thresholds we
settled on the following simple strategy: if the po-
larity score for an aspect term is greater than zero
then it is classified as positive, if the score is less
than zero, then it is classified as negative, other-
wise it is classified as neutral.
</bodyText>
<subsectionHeader confidence="0.999328">
3.2 Machine Learning Approach
</subsectionHeader>
<bodyText confidence="0.9916550625">
We train a four-way SVM classifier for each do-
main (laptop and restaurant), using Weka’s SMO
implementation (Platt, 1998; Hall et al., 2009).10
8To facilitate parsing, the data was normalised using the
process described in (Le Roux et al., 2012) with minor mod-
ifications, e. g. treatment of non-breakable space characters,
abbreviations and emoticons. The normalised version of the
data was used for all experiments.
9We also experimented with classifying aspect terms as
conflict when the individual scores for positive and negative
sentiment were both relatively high. However, this proved
unsuccessful.
10We also experimented with logistic regression, random
forests, k-nearest neighbour, naive Bayes and multi-layer per-
ceptron in Weka, but did not match performance of an SVM
trained with default parameters.
</bodyText>
<table confidence="0.9990935">
Transf. n c n-gram Freq.
-L— 2 2 cord with 1
AL— 2 2 &lt;aspect&gt; with 56
ALS– 1 4 &lt;negu080&gt; 595
ALSR- 1 4 &lt;negu080&gt; 502
AL— 2 4 and skip 1
ALSR- 2 4 and &lt;negu080&gt; 25
ALSRP 1 4 &lt;negu080&gt;/vb 308
</table>
<tableCaption confidence="0.809972">
Table 1: 7 of the 2,640 bag-of-n-gram features
extracted for the aspect term cord from the lap-
</tableCaption>
<bodyText confidence="0.992829363636363">
top training sentence I charge it at night and skip
taking the cord with me because of the good bat-
tery life. The last column shows the frequency of
the feature in the training data. Transformations:
A=aspect, L=lowercase, S=score, R=restricted to
certain POS, P=POS annotation
Our system submission uses bag-of-n-gram fea-
tures and features derived from the rule-based ap-
proach. Decisions about parameters are made in 5-
fold cross-validation on the training data provided
for the task.
</bodyText>
<subsectionHeader confidence="0.476116">
3.2.1 Bag-of-N-gram Features
</subsectionHeader>
<bodyText confidence="0.999971166666667">
We extract features encoding the presence of spe-
cific lower-cased n-grams (L) (n = 1, ..., 5) in
the context of the aspect term to be classified (c
words to the left and c words to the right with
c = 1, ..., 5, inf) for 10 combinations of trans-
formations: replacement of the aspect term with
&lt;ASPECT&gt; (A), replacement of sentiment words
with a discretised score (S), restriction (R) of the
sentiment word replacement to certain parts-of-
speech, and annotation of the discretised score
with the POS (P) of the sentiment word. An ex-
ample is shown in Table 1.
</bodyText>
<subsectionHeader confidence="0.988697">
3.2.2 Adding Rule-Based Score Features
</subsectionHeader>
<bodyText confidence="0.999562">
We explore two approaches for incorporating in-
formation from the rule-based approach (Sec-
tion 3.1) into our SVM classifier. The first ap-
proach is to encode polarity scores directly as the
following four features:
</bodyText>
<listItem confidence="0.9992566">
1. distance-weighted sum of scores of positive
words in the sentence
2. distance-weighted sum of scores of negative
words in the sentence
3. number of positive words in the sentence
</listItem>
<page confidence="0.972236">
225
</page>
<figure confidence="0.345126">
4. number of negative words in the sentence
Dataset System Training Test
</figure>
<bodyText confidence="0.9999125">
The second approach is less direct: for each do-
main, we train J48 decision trees with minimum
leaf size 60 using the four rule-based features de-
scribed above. We then use the decision rules
and the conjunctions leading from the root node
to each leaf node to binarise the above four basic
score features, producing 122 features. Further-
more, we add normalised absolute values, rank of
values and interval indicators, producing 48 fea-
tures.
</bodyText>
<subsectionHeader confidence="0.721084">
3.2.3 Submitted Runs
</subsectionHeader>
<bodyText confidence="0.999991173913043">
We eliminate features that have redundant value
columns for the training data, and we apply fre-
quency thresholds (13, 18, 25 and 35) to further
reduce the number of features. We perform a grid-
search to optimise the parameters C and -y of the
SVM RBF kernel. We choose the system to sub-
mit based on average cross-validation accuracy.
We experiment with combinations of the three fea-
ture sets described above. We choose the bina-
rised features over the raw rule-based scores be-
cause cross-validation results are inferior for the
rule-based scores in initial experiments with fea-
ture frequency threshold 35: 70.26 vs. 71.36 for
laptop and 72.06 vs. 72.15 for restaurant. There-
fore, we decide to focus on systems with binarised
score features for lower feature frequency thresh-
olds, which are more CPU-intensive to train. For
both domains, the system we end up submitting
is a combination of the n-gram features and the
binarised features with parameters C = 3.981,
-y = 0.003311 for the laptop data, C = 1.445,
-y = 0.003311 for the restaurant data, and a fre-
quency threshold of 13.
</bodyText>
<sectionHeader confidence="0.999799" genericHeader="method">
4 Results and Analysis
</sectionHeader>
<bodyText confidence="0.999789083333333">
Table 2 shows the training and test accuracy of
the task baseline system (Pontiki et al., 2014), a
majority baseline classifying everything as posi-
tive, our rule-based system and our submitted sys-
tem. The restaurant domain has a higher accuracy
than the laptop domain for all systems, the SVM
system outperforms the rule-based system on both
domains, and the test accuracy is higher than the
training accuracy for all systems in the restaurant
domain.
We observe that the majority of our systems’ er-
rors fall into the following categories:
</bodyText>
<table confidence="0.998055125">
Laptop Baseline — 51.1%
Laptop All positive 41.9% 52.1%
Laptop Rule-based 65.4% 67.7%
Laptop SVM 72.3% 70.5%
Restaurant Baseline — 64.3%
Restaurant All positive 58.6% 64.2%
Restaurant Rule-based 69.5% 77.8%
Restaurant SVM 72.7% 81.0%
</table>
<tableCaption confidence="0.998248">
Table 2: Accuracy of the task baseline system, a
</tableCaption>
<bodyText confidence="0.720488666666667">
system classifying everything as positive, our rule-
based system and our submitted SVM-based sys-
tem on train (5-fold cross-validation) and test sets
</bodyText>
<listItem confidence="0.944839769230769">
• Sentiment not expressed explicitly: The
sentiment cannot be inferred from local lexi-
cal and syntactic information, e. g. The sushi
is cut in blocks bigger than my cell phone.
• Non-obvious expression of negation: For
example, The Management was less than ac-
comodating [sic]. The rule-based approach
does not capture such cases and there are
not enough similar training examples for the
SVM to learn to correctly classify them.
• Conflict cases: The training data contains
too few examples of conflict sentences for the
system to learn to detect them.11
</listItem>
<bodyText confidence="0.99996035">
For the restaurant domain, there are more than
fifty cases where the rule-based approach fails to
detect sentiment, but the machine learning ap-
proach classifies it correctly. Most of these cases
contain no sentiment lexicon words, thus the rule-
based system marks them as being neutral. How-
ever, the machine learning system was able to fig-
ure out the correct polarity. Examples of such
cases include Try the rose roll (not on menu) and
The gnocchi literally melts in your mouth!. Fur-
thermore, in the laptop domain, a number of the
errors made by the rule-based system arise from
the ambiguous nature of some lexicon words. For
example, the sentence Only 2 usb ports ... seems
kind of ... limited is misclassified because the
word kind is considered to be positive.
There are a few cases where the rule-based sys-
tem outperforms the machine learning one. It hap-
pens when a sentence contains a rare word with
strong polarity, e. g. the word heavenly in The
</bodyText>
<footnote confidence="0.552147">
11We only classify one test instance as conflict.
</footnote>
<page confidence="0.997734">
226
</page>
<bodyText confidence="0.989261">
chocolate raspberry cake is heavenly - not too
sweet, but full offlavor.
</bodyText>
<sectionHeader confidence="0.999921" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999982868421053">
The use of supervised machine learning with bag-
of-word or bag-of-n-gram feature sets has been
a standard approach to the problem of sentiment
polarity classification since the seminal work by
Pang et al. (2002) on movie review polarity pre-
diction. Heuristic methods which rely on a lexi-
con of sentiment words have also been widespread
and much of the research in this area has been
devoted to the unsupervised induction of good
quality sentiment indicators (see, for example,
Hatzivassiloglou and McKeown (1997) and Tur-
ney (2002), and Liu (2010) for an overview). The
integration of sentiment lexicon scores as fea-
tures in supervised machine learning to supple-
ment standard bag-of-n-gram features has also
been employed before (see, for example, Bak-
liwal et al. (2013)). The replacement of train-
ing/test words with scores/labels from sentiment
lexicons has also been used by Baccianella et
al. (2009), who supplement n-grams such as hor-
rible location with generalised expressions such
as NEGATIVE location. Linguistic features which
capture generalisations at the level of syntax (Mat-
sumoto et al., 2005), semantics (Johansson and
Moschitti, 2010) and discourse (Lazaridou et al.,
2013) have also been widely applied. In using bi-
narised features derived from the nodes of a deci-
sion tree, we are following our recent work which
uses the same technique in a different task: quality
estimation for machine translation (Rubino et al.,
2012; Rubino et al., 2013).
The main novelty in our system lies not in the
individual techniques but rather in they way they
are combined and integrated. For example, our
combination of token/chunk/dependency path dis-
tance used to weight the relationship between a
sentiment word and the aspect term has – to the
best of our knowledge – not been applied before.
</bodyText>
<sectionHeader confidence="0.997876" genericHeader="method">
6 Efficiency
</sectionHeader>
<bodyText confidence="0.9779618">
Building a system for a shared task, we focus
solely on the accuracy of the system in all our deci-
sions. For example, we parse all training and test
data multiple times using different grammars to
increase sentence coverage from 99.87% to 100%.
To offer a more practical system, we work on
implementing a simplified, fully automated sys-
tem that is more efficient. So far, we replaced
time-consuming parsing with POS tagging. The
system accepts as input and generates as output
valid SemEval ABSA XML documents.12 After
extracting the text and the aspect terms from the
input, the text is normalised using the process de-
scribed in Footnote 8. The feature extraction is
performed as described in Section 3 with the fol-
lowing modifications:
• The POS information used by the n-gram
feature extractor is obtained using the IMS
TreeTagger (Schmid, 1994) instead of using
the PCFG-LA parser (Attia et al., 2010).
</bodyText>
<listItem confidence="0.757875333333333">
• The distance used by the rule-based approach
is the token distance only, instead of a com-
bination of three distance functions.
</listItem>
<bodyText confidence="0.999954633333333">
The sentiment lexicon and the classification mod-
els used are described in Sections 2 and 3 respec-
tively.
The test sets containing 800 sentences are POS
tagged in less than half a second each. Surpris-
ingly, accuracy of aspect term polarity prediction
increases to 71.4% (from 70.5% for the submitted
system) on the laptop test set, using the same SVM
parameters as for the submitted system. However,
we see a degradation to 78.8% (from 81.0% for the
submitted system) for the restaurant test set. This
is an encouraging result as the SVM parameters
are not yet fully optimised for the slightly different
information and as the remaining modifications to
be implemented should not change accuracy any
further.
The next bottleneck that needs to be addressed
before the system can be used in applications re-
quiring quick responses is the current implementa-
tion of the n-gram feature extractor: It enumerates
all n-grams (for all context window sizes and n-
gram transformations) only to then intersect these
features with the list of selected features. For the
shared task, this made sense as we initially need
all features to make our selection of features, and
as we only need to run the feature extractor a few
times. For a practical system that has to process
new test sets frequently, however, it will be more
efficient to check for each selected feature whether
the respective event occurs in the input.
</bodyText>
<footnote confidence="0.961367">
12We validate documents using the XML schema defini-
tion provided on the shared task website.
</footnote>
<page confidence="0.996619">
227
</page>
<sectionHeader confidence="0.995596" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999985581395349">
We have described our aspect term polarity predic-
tion system, which employs supervised machine
learning using a combination of n-grams and sen-
timent lexicon features. Although our submitted
system performs very well, it is interesting to note
that our rule-based system is not that far behind.
This suggests that a state-of-the-art system can be
build without machine learning and that careful
design of the other system components is impor-
tant. However, the very good performance of our
machine-learning-based system also suggests that
word n-gram features do provide useful informa-
tion that is missed by a sentiment lexicon alone,
and that it is always worthwhile to perform careful
parameter tuning to eke out as much as possible
from such an approach.
Future work should investigate how much each
system component contributes to the overall per-
formance, e. g. lexicon combination, lemmatisa-
tion, spelling correction, other normalisations,
negation handling, distance function and n-gram
feature transformations. There is also room for
improvements in most of these components, e. g.
our handling of complex negations. Detection of
conflicts also needs more attention. Features in-
dicating the presence of trigger words for negation
and conflicts that are currently used only internally
in the rule-based component could be added to the
SVM feature set. It would also be interesting to
see how the compositional approach described by
Socher et al. (2013) handles these difficult cases.
The score features could be easily augmented by
breaking down scores by the four employed lexi-
cons. This way, the SVM can choose to combine
the information from these scores differently than
just summing them, allowing it to learn more com-
plex relations. Lexicon filtering and addition of
domain-specific entries could be automated to re-
duce the time needed to adjust to a new domain.
Finally, machine learning methods that can effi-
ciently handle large feature sets such as logistic
regression should be tried with the full feature set
(not applying frequency thresholds).
</bodyText>
<sectionHeader confidence="0.997752" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9985345">
This research is supported by the Science Foun-
dation Ireland (Grant 12/CE/I2267) as part of
CNGL (www.cngl.ie) at Dublin City University.
The authors wish to acknowledge the DJEI/DES/
SFI/HEA Irish Centre for High-End Computing
(ICHEC) for the provision of computational facil-
ities and support. We are grateful to Qun Liu and
Josef van Genabith for their helpful comments.
</bodyText>
<sectionHeader confidence="0.995676" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995332191489362">
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statis-
tical latent-variable parsing models for arabic, en-
glish and french. In Proceedings of the NAACL
HLT 2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 67–75.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2009. Multi-facet rating of product reviews.
In Proceedings of ECIR, pages 461–472.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An Enhanced Lex-
ical Resource for Sentiment Analysis and Opinion
Mining. In Proceedings of the Seventh Conference
on International Language Resources and Evalua-
tion (LREC’10).
Akshat Bakliwal, Jennifer Foster, Jennifer van der Puil,
Ron O’Brien, Lamia Tounsi, and Mark Hughes.
2013. Sentiment analysis of political tweets: To-
wards an accurate classifier. In Proceedings of the
NAACL Workshop on Language Analysis in Social
Media, pages 49–58.
Bernhard E. Boser, Isabelle M. Guyon, and
Vladimir N. Vapnik. 1992. A training algo-
rithm for optimal margin classifiers. In Proceedings
of the Fifth Annual Workshop on Computational
Learning Theory, pages 144–152.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies rep-
resentation. In COLING 2008 Workshop on Cross-
framework and Cross-domain Parser Evaluation.,
pages 1–8.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
ACM SIGKDD Explorations Newsletter, 11(1):10–
18.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the 35th Annual Meeting
of the ACL and the 8th Conference of the European
Chapter of the ACL, pages 174–181.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ’04, pages
168–177.
</reference>
<page confidence="0.974336">
228
</page>
<reference confidence="0.999608151898734">
Richard Johansson and Alessandro Moschitti. 2010.
Syntactic and semantic structure for opinion ex-
pression detection. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 67–76.
Angeliki Lazaridou, Ivan Titov, and Caroline
Sporleder. 2013. A bayesian model for joint
unsupervised induction of sentiment, aspect and
discourse representations. In Proceedings of
the 51th Annual Meeting of the Association for
Computational Linguistics, pages 1630–1639.
Joseph Le Roux, Jennifer Foster, Joachim Wagner, Ra-
sul Samad Zadeh Kaljahi, and Anton Bryl. 2012.
DCU-Paris13 systems for the SANCL 2012 shared
task. Notes of the First Workshop on Syntactic
Analysis of Non-Canonical Language (SANCL).
Bing Liu. 2010. Sentiment analysis and subjectivity.
In Handbook of Natural Language Processing.
Shotaro Matsumoto, Hiroya Takamura, and Manubu
Okumura, 2005. Advances in Knowledge Discovery
and Data Mining, volume 3518 of Lecture Notes in
Computer Science, chapter Sentiment Classification
Using Word Sub-sequences and Dependency Sub-
trees, pages 301–311.
Peter Norvig. 2012. How to write a spelling corrector.
http://norvig.com/spell-correct.
html. [Online; accessed 2014-03-19].
Po Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP, pages 79–86.
John C. Platt. 1998. Fast training of support vec-
tor machines using sequential minimal optimization.
In B. Schoelkopf, C. Burges, and A. Smola, edi-
tors, Advances in Kernel Methods - Support Vector
Learning, pages 185–208.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4:
Aspect based sentiment analysis. In Proceedings of
the International Workshop on Semantic Evaluation
(SemEval).
Raphael Rubino, Jennifer Foster, Joachim Wagner, Jo-
hann Roturier, Rasul Samad Zadeh Kaljahi, and Fred
Hollowood. 2012. Dcu-symantec submission for
the wmt 2012 quality estimation task. In Proceed-
ings of the Seventh Workshop on Statistical Machine
Translation, pages 138–144.
Raphael Rubino, Joachim Wagner, Jennifer Foster, Jo-
hann Roturier, Rasoul Samad Zadeh Kaljahi, and
Fred Hollowood. 2013. DCU-Symantec at the
WMT 2013 quality estimation shared task. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 392–397.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44–49.
Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of EMNLP, pages 1631–
1642.
Milan Tofiloski, Julian Brooke, and Maite Taboada.
2009. A syntactic and lexical-based discourse seg-
menter. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, ACLShort ’09, pages 77–
80.
Peter Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classi-
cation of reviews. In Proceedings of the ACL, pages
417–424.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT ’05, pages 347–354.
</reference>
<page confidence="0.998895">
229
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.335126">
<title confidence="0.615788">DCU: Aspect-based Polarity Classification for SemEval Task 4</title>
<author confidence="0.7622155">Joachim Wagner</author>
<author confidence="0.7622155">Piyush Arora</author>
<author confidence="0.7622155">Santiago Cortes</author>
<author confidence="0.7622155">Utsab Dasha Bogdanova</author>
<author confidence="0.7622155">Jennifer Foster</author>
<author confidence="0.7622155">Lamia</author>
<affiliation confidence="0.99323875">CNGL Centre for Global Intelligent National Centre for Language School of Dublin City</affiliation>
<address confidence="0.666457">Dublin,</address>
<abstract confidence="0.9964129">We describe the work carried out by DCU on the Aspect Based Sentiment Analysis task at SemEval 2014. Our team submitted one constrained run for the restaurant domain and one for the laptop domain for sub-task B (aspect term polarity prediction), ranking highest out of 36 systems on the restaurant test set and joint highest out of 32 systems on the laptop test set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohammed Attia</author>
<author>Jennifer Foster</author>
<author>Deirdre Hogan</author>
<author>Joseph Le Roux</author>
<author>Lamia Tounsi</author>
<author>Josef van Genabith</author>
</authors>
<title>Handling unknown words in statistical latent-variable parsing models for arabic, english and french.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages,</booktitle>
<pages>67--75</pages>
<marker>Attia, Foster, Hogan, Le Roux, Tounsi, van Genabith, 2010</marker>
<rawString>Mohammed Attia, Jennifer Foster, Deirdre Hogan, Joseph Le Roux, Lamia Tounsi, and Josef van Genabith. 2010. Handling unknown words in statistical latent-variable parsing models for arabic, english and french. In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 67–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Multi-facet rating of product reviews.</title>
<date>2009</date>
<booktitle>In Proceedings of ECIR,</booktitle>
<pages>461--472</pages>
<contexts>
<context position="17199" citStr="Baccianella et al. (2009)" startWordPosition="2793" endWordPosition="2796">y on a lexicon of sentiment words have also been widespread and much of the research in this area has been devoted to the unsupervised induction of good quality sentiment indicators (see, for example, Hatzivassiloglou and McKeown (1997) and Turney (2002), and Liu (2010) for an overview). The integration of sentiment lexicon scores as features in supervised machine learning to supplement standard bag-of-n-gram features has also been employed before (see, for example, Bakliwal et al. (2013)). The replacement of training/test words with scores/labels from sentiment lexicons has also been used by Baccianella et al. (2009), who supplement n-grams such as horrible location with generalised expressions such as NEGATIVE location. Linguistic features which capture generalisations at the level of syntax (Matsumoto et al., 2005), semantics (Johansson and Moschitti, 2010) and discourse (Lazaridou et al., 2013) have also been widely applied. In using binarised features derived from the nodes of a decision tree, we are following our recent work which uses the same technique in a different task: quality estimation for machine translation (Rubino et al., 2012; Rubino et al., 2013). The main novelty in our system lies not </context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2009</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2009. Multi-facet rating of product reviews. In Proceedings of ECIR, pages 461–472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh Conference on International Language Resources and Evaluation (LREC’10).</booktitle>
<contexts>
<context position="3156" citStr="Baccianella et al., 2010" startWordPosition="495" endWordPosition="498">training and test data, and perform an error analysis on the test set; in Section 5, we compare our approach to previous research in sentiment classification; Section 6 discusses efficiency of our system and ongoing work to improve its speed; finally, in Section 7, we conclude and provide suggestions as to how this research could be fruitfully extended. 2 Sentiment Lexicons The following four lexicons are employed: 1. MPQA1 (Wilson et al., 2005) classifies a word or a stem and its part of speech tag into positive, negative, both or neutral with a strong or weak subjectivity. 2. SentiWordNet2 (Baccianella et al., 2010) specifies the positive, negative and objective scores of a synset and its part of speech tag. 3. General Inquirer3 indicates whether a word expresses positive or negative sentiment. 4. Bing Liu’s Opinion Lexicon4 (Hu and Liu, 1http://mpqa.cs.pitt.edu/lexicons/ subj_lexicon/ 2http://sentiwordnet.isti.cnr.it/ 3http://www.wjh.harvard.edu/˜inquirer/ inqtabs.txt 4http://www.cs.uic.edu/˜liub/FBS/ sentiment-analysis.html#lexicon 223 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 223–229, Dublin, Ireland, August 23-24, 2014. 2004) indicates whether a word e</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining. In Proceedings of the Seventh Conference on International Language Resources and Evaluation (LREC’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akshat Bakliwal</author>
<author>Jennifer Foster</author>
<author>Jennifer van der Puil</author>
<author>Ron O’Brien</author>
<author>Lamia Tounsi</author>
<author>Mark Hughes</author>
</authors>
<title>Sentiment analysis of political tweets: Towards an accurate classifier.</title>
<date>2013</date>
<booktitle>In Proceedings of the NAACL Workshop on Language Analysis in Social Media,</booktitle>
<pages>49--58</pages>
<marker>Bakliwal, Foster, van der Puil, O’Brien, Tounsi, Hughes, 2013</marker>
<rawString>Akshat Bakliwal, Jennifer Foster, Jennifer van der Puil, Ron O’Brien, Lamia Tounsi, and Mark Hughes. 2013. Sentiment analysis of political tweets: Towards an accurate classifier. In Proceedings of the NAACL Workshop on Language Analysis in Social Media, pages 49–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernhard E Boser</author>
<author>Isabelle M Guyon</author>
<author>Vladimir N Vapnik</author>
</authors>
<title>A training algorithm for optimal margin classifiers.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fifth Annual Workshop on Computational Learning Theory,</booktitle>
<pages>144--152</pages>
<contexts>
<context position="1404" citStr="Boser et al., 1992" startWordPosition="206" endWordPosition="209">1 Introduction This paper describes DCU’s participation in the Aspect Term Polarity sub-task of the Aspect Based Sentiment Analysis task at SemEval 2014, which focuses on predicting the sentiment polarity of aspect terms for a restaurant and a laptop dataset. Given, for example, the sentence I have had so many problems with the computer and the aspect term the computer, the task is to predict whether the sentiment expressed towards the aspect term is positive, negative, neutral or conflict. Our polarity classification system uses supervised machine learning with support vector machines (SVM) (Boser et al., 1992) to classify an aspect term into one of the four classes. The features we employ are word n-grams (with n ranging from 1 to 5) in a window around the aspect term, as well as features derived from scores assigned by a sentiment lexicon. Furthermore, to reduce data sparsity, we experiment with replacing sentiment-bearing words in our n-gram feature set with their polarity scores according to the lexicon and/or their part-of-speech tag. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence det</context>
</contexts>
<marker>Boser, Guyon, Vapnik, 1992</marker>
<rawString>Bernhard E. Boser, Isabelle M. Guyon, and Vladimir N. Vapnik. 1992. A training algorithm for optimal margin classifiers. In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, pages 144–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>In COLING 2008 Workshop on Crossframework and Cross-domain Parser Evaluation.,</booktitle>
<pages>1--8</pages>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The stanford typed dependencies representation. In COLING 2008 Workshop on Crossframework and Cross-domain Parser Evaluation., pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: an update.</title>
<date>2009</date>
<journal>ACM SIGKDD Explorations Newsletter,</journal>
<volume>11</volume>
<issue>1</issue>
<pages>18</pages>
<contexts>
<context position="9575" citStr="Hall et al., 2009" startWordPosition="1526" endWordPosition="1529">nd never. For each aspect term, we use the distanceweighted sum of the polarity scores to predict one of the three classes positive, negative and neutral.9 After experimenting with various thresholds we settled on the following simple strategy: if the polarity score for an aspect term is greater than zero then it is classified as positive, if the score is less than zero, then it is classified as negative, otherwise it is classified as neutral. 3.2 Machine Learning Approach We train a four-way SVM classifier for each domain (laptop and restaurant), using Weka’s SMO implementation (Platt, 1998; Hall et al., 2009).10 8To facilitate parsing, the data was normalised using the process described in (Le Roux et al., 2012) with minor modifications, e. g. treatment of non-breakable space characters, abbreviations and emoticons. The normalised version of the data was used for all experiments. 9We also experimented with classifying aspect terms as conflict when the individual scores for positive and negative sentiment were both relatively high. However, this proved unsuccessful. 10We also experimented with logistic regression, random forests, k-nearest neighbour, naive Bayes and multi-layer perceptron in Weka, </context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA data mining software: an update. ACM SIGKDD Explorations Newsletter, 11(1):10– 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the ACL and the 8th Conference of the European Chapter of the ACL,</booktitle>
<pages>174--181</pages>
<contexts>
<context position="16810" citStr="Hatzivassiloglou and McKeown (1997)" startWordPosition="2731" endWordPosition="2734"> one test instance as conflict. 226 chocolate raspberry cake is heavenly - not too sweet, but full offlavor. 5 Related Work The use of supervised machine learning with bagof-word or bag-of-n-gram feature sets has been a standard approach to the problem of sentiment polarity classification since the seminal work by Pang et al. (2002) on movie review polarity prediction. Heuristic methods which rely on a lexicon of sentiment words have also been widespread and much of the research in this area has been devoted to the unsupervised induction of good quality sentiment indicators (see, for example, Hatzivassiloglou and McKeown (1997) and Turney (2002), and Liu (2010) for an overview). The integration of sentiment lexicon scores as features in supervised machine learning to supplement standard bag-of-n-gram features has also been employed before (see, for example, Bakliwal et al. (2013)). The replacement of training/test words with scores/labels from sentiment lexicons has also been used by Baccianella et al. (2009), who supplement n-grams such as horrible location with generalised expressions such as NEGATIVE location. Linguistic features which capture generalisations at the level of syntax (Matsumoto et al., 2005), seman</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997. Predicting the semantic orientation of adjectives. In Proceedings of the 35th Annual Meeting of the ACL and the 8th Conference of the European Chapter of the ACL, pages 174–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04,</booktitle>
<pages>168--177</pages>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04, pages 168–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Syntactic and semantic structure for opinion expression detection.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>67--76</pages>
<contexts>
<context position="17446" citStr="Johansson and Moschitti, 2010" startWordPosition="2828" endWordPosition="2831">urney (2002), and Liu (2010) for an overview). The integration of sentiment lexicon scores as features in supervised machine learning to supplement standard bag-of-n-gram features has also been employed before (see, for example, Bakliwal et al. (2013)). The replacement of training/test words with scores/labels from sentiment lexicons has also been used by Baccianella et al. (2009), who supplement n-grams such as horrible location with generalised expressions such as NEGATIVE location. Linguistic features which capture generalisations at the level of syntax (Matsumoto et al., 2005), semantics (Johansson and Moschitti, 2010) and discourse (Lazaridou et al., 2013) have also been widely applied. In using binarised features derived from the nodes of a decision tree, we are following our recent work which uses the same technique in a different task: quality estimation for machine translation (Rubino et al., 2012; Rubino et al., 2013). The main novelty in our system lies not in the individual techniques but rather in they way they are combined and integrated. For example, our combination of token/chunk/dependency path distance used to weight the relationship between a sentiment word and the aspect term has – to the be</context>
</contexts>
<marker>Johansson, Moschitti, 2010</marker>
<rawString>Richard Johansson and Alessandro Moschitti. 2010. Syntactic and semantic structure for opinion expression detection. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 67–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angeliki Lazaridou</author>
<author>Ivan Titov</author>
<author>Caroline Sporleder</author>
</authors>
<title>A bayesian model for joint unsupervised induction of sentiment, aspect and discourse representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1630--1639</pages>
<contexts>
<context position="17485" citStr="Lazaridou et al., 2013" startWordPosition="2834" endWordPosition="2837"> The integration of sentiment lexicon scores as features in supervised machine learning to supplement standard bag-of-n-gram features has also been employed before (see, for example, Bakliwal et al. (2013)). The replacement of training/test words with scores/labels from sentiment lexicons has also been used by Baccianella et al. (2009), who supplement n-grams such as horrible location with generalised expressions such as NEGATIVE location. Linguistic features which capture generalisations at the level of syntax (Matsumoto et al., 2005), semantics (Johansson and Moschitti, 2010) and discourse (Lazaridou et al., 2013) have also been widely applied. In using binarised features derived from the nodes of a decision tree, we are following our recent work which uses the same technique in a different task: quality estimation for machine translation (Rubino et al., 2012; Rubino et al., 2013). The main novelty in our system lies not in the individual techniques but rather in they way they are combined and integrated. For example, our combination of token/chunk/dependency path distance used to weight the relationship between a sentiment word and the aspect term has – to the best of our knowledge – not been applied </context>
</contexts>
<marker>Lazaridou, Titov, Sporleder, 2013</marker>
<rawString>Angeliki Lazaridou, Ivan Titov, and Caroline Sporleder. 2013. A bayesian model for joint unsupervised induction of sentiment, aspect and discourse representations. In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics, pages 1630–1639.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Le Roux</author>
<author>Jennifer Foster</author>
<author>Joachim Wagner</author>
<author>Rasul Samad Zadeh Kaljahi</author>
<author>Anton Bryl</author>
</authors>
<date>2012</date>
<booktitle>DCU-Paris13 systems for the SANCL 2012 shared task. Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL).</booktitle>
<marker>Le Roux, Foster, Wagner, Kaljahi, Bryl, 2012</marker>
<rawString>Joseph Le Roux, Jennifer Foster, Joachim Wagner, Rasul Samad Zadeh Kaljahi, and Anton Bryl. 2012. DCU-Paris13 systems for the SANCL 2012 shared task. Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment analysis and subjectivity.</title>
<date>2010</date>
<booktitle>In Handbook of Natural Language Processing.</booktitle>
<contexts>
<context position="16844" citStr="Liu (2010)" startWordPosition="2740" endWordPosition="2741">e is heavenly - not too sweet, but full offlavor. 5 Related Work The use of supervised machine learning with bagof-word or bag-of-n-gram feature sets has been a standard approach to the problem of sentiment polarity classification since the seminal work by Pang et al. (2002) on movie review polarity prediction. Heuristic methods which rely on a lexicon of sentiment words have also been widespread and much of the research in this area has been devoted to the unsupervised induction of good quality sentiment indicators (see, for example, Hatzivassiloglou and McKeown (1997) and Turney (2002), and Liu (2010) for an overview). The integration of sentiment lexicon scores as features in supervised machine learning to supplement standard bag-of-n-gram features has also been employed before (see, for example, Bakliwal et al. (2013)). The replacement of training/test words with scores/labels from sentiment lexicons has also been used by Baccianella et al. (2009), who supplement n-grams such as horrible location with generalised expressions such as NEGATIVE location. Linguistic features which capture generalisations at the level of syntax (Matsumoto et al., 2005), semantics (Johansson and Moschitti, 201</context>
</contexts>
<marker>Liu, 2010</marker>
<rawString>Bing Liu. 2010. Sentiment analysis and subjectivity. In Handbook of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shotaro Matsumoto</author>
<author>Hiroya Takamura</author>
<author>Manubu Okumura</author>
</authors>
<date>2005</date>
<booktitle>Advances in Knowledge Discovery and Data Mining,</booktitle>
<volume>3518</volume>
<pages>301--311</pages>
<contexts>
<context position="17403" citStr="Matsumoto et al., 2005" startWordPosition="2822" endWordPosition="2826">vassiloglou and McKeown (1997) and Turney (2002), and Liu (2010) for an overview). The integration of sentiment lexicon scores as features in supervised machine learning to supplement standard bag-of-n-gram features has also been employed before (see, for example, Bakliwal et al. (2013)). The replacement of training/test words with scores/labels from sentiment lexicons has also been used by Baccianella et al. (2009), who supplement n-grams such as horrible location with generalised expressions such as NEGATIVE location. Linguistic features which capture generalisations at the level of syntax (Matsumoto et al., 2005), semantics (Johansson and Moschitti, 2010) and discourse (Lazaridou et al., 2013) have also been widely applied. In using binarised features derived from the nodes of a decision tree, we are following our recent work which uses the same technique in a different task: quality estimation for machine translation (Rubino et al., 2012; Rubino et al., 2013). The main novelty in our system lies not in the individual techniques but rather in they way they are combined and integrated. For example, our combination of token/chunk/dependency path distance used to weight the relationship between a sentime</context>
</contexts>
<marker>Matsumoto, Takamura, Okumura, 2005</marker>
<rawString>Shotaro Matsumoto, Hiroya Takamura, and Manubu Okumura, 2005. Advances in Knowledge Discovery and Data Mining, volume 3518 of Lecture Notes in Computer Science, chapter Sentiment Classification Using Word Sub-sequences and Dependency Subtrees, pages 301–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Norvig</author>
</authors>
<title>How to write a spelling corrector.</title>
<date>2012</date>
<pages>2014--03</pages>
<note>http://norvig.com/spell-correct. html. [Online; accessed</note>
<contexts>
<context position="6136" citStr="Norvig, 2012" startWordPosition="961" endWordPosition="962">.6 5We also tried to vote over the four lexicon scores but this did not improve over summing. 6http://world-food-and-wine.com/ describing-food 2.4 Handling Variation In order to ensure that all inflected forms of a word are covered, we lemmatise the words in the training data using the IMS TreeTagger (Schmid, 1994) and we construct new possibilities using a suffix list. To correct misspelled words, we consider the corrected form of a misspelled word to be the form with the highest frequency in a reference corpus7 among all the forms within an edit distance of 1 and 2 from the misspelled word (Norvig, 2012). Multi-word expressions of the form x-y are added with the polarity of xy or x, as in laidback/laidback and well-shaped/well. Expressions x y, are added with the polarity of x-y, as in so so/so-so. 3 Methodology We first build a rule-based system which classifies the polarity of an aspect term based solely on the scores assigned by the sentiment lexicon. We then explore different ways of converting the rulebased system into features which can then be combined with bag-of-n-gram features in a supervised machine learning set-up. 3.1 Rule-Based Approach In order to predict the polarity of an asp</context>
</contexts>
<marker>Norvig, 2012</marker>
<rawString>Peter Norvig. 2012. How to write a spelling corrector. http://norvig.com/spell-correct. html. [Online; accessed 2014-03-19].</rawString>
</citation>
<citation valid="true">
<authors>
<author>Po Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="16509" citStr="Pang et al. (2002)" startWordPosition="2683" endWordPosition="2686">limited is misclassified because the word kind is considered to be positive. There are a few cases where the rule-based system outperforms the machine learning one. It happens when a sentence contains a rare word with strong polarity, e. g. the word heavenly in The 11We only classify one test instance as conflict. 226 chocolate raspberry cake is heavenly - not too sweet, but full offlavor. 5 Related Work The use of supervised machine learning with bagof-word or bag-of-n-gram feature sets has been a standard approach to the problem of sentiment polarity classification since the seminal work by Pang et al. (2002) on movie review polarity prediction. Heuristic methods which rely on a lexicon of sentiment words have also been widespread and much of the research in this area has been devoted to the unsupervised induction of good quality sentiment indicators (see, for example, Hatzivassiloglou and McKeown (1997) and Turney (2002), and Liu (2010) for an overview). The integration of sentiment lexicon scores as features in supervised machine learning to supplement standard bag-of-n-gram features has also been employed before (see, for example, Bakliwal et al. (2013)). The replacement of training/test words </context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Po Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of EMNLP, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Platt</author>
</authors>
<title>Fast training of support vector machines using sequential minimal optimization.</title>
<date>1998</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning,</booktitle>
<pages>185--208</pages>
<editor>In B. Schoelkopf, C. Burges, and A. Smola, editors,</editor>
<contexts>
<context position="9555" citStr="Platt, 1998" startWordPosition="1524" endWordPosition="1525">ot, n’t, no and never. For each aspect term, we use the distanceweighted sum of the polarity scores to predict one of the three classes positive, negative and neutral.9 After experimenting with various thresholds we settled on the following simple strategy: if the polarity score for an aspect term is greater than zero then it is classified as positive, if the score is less than zero, then it is classified as negative, otherwise it is classified as neutral. 3.2 Machine Learning Approach We train a four-way SVM classifier for each domain (laptop and restaurant), using Weka’s SMO implementation (Platt, 1998; Hall et al., 2009).10 8To facilitate parsing, the data was normalised using the process described in (Le Roux et al., 2012) with minor modifications, e. g. treatment of non-breakable space characters, abbreviations and emoticons. The normalised version of the data was used for all experiments. 9We also experimented with classifying aspect terms as conflict when the individual scores for positive and negative sentiment were both relatively high. However, this proved unsuccessful. 10We also experimented with logistic regression, random forests, k-nearest neighbour, naive Bayes and multi-layer </context>
</contexts>
<marker>Platt, 1998</marker>
<rawString>John C. Platt. 1998. Fast training of support vector machines using sequential minimal optimization. In B. Schoelkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 185–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Pontiki</author>
<author>Dimitrios Galanis</author>
<author>John Pavlopoulos</author>
<author>Harris Papageorgiou</author>
<author>Ion Androutsopoulos</author>
<author>Suresh Manandhar</author>
</authors>
<title>Semeval-2014 task 4: Aspect based sentiment analysis.</title>
<date>2014</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation (SemEval).</booktitle>
<contexts>
<context position="13773" citStr="Pontiki et al., 2014" startWordPosition="2231" endWordPosition="2234">ure frequency threshold 35: 70.26 vs. 71.36 for laptop and 72.06 vs. 72.15 for restaurant. Therefore, we decide to focus on systems with binarised score features for lower feature frequency thresholds, which are more CPU-intensive to train. For both domains, the system we end up submitting is a combination of the n-gram features and the binarised features with parameters C = 3.981, -y = 0.003311 for the laptop data, C = 1.445, -y = 0.003311 for the restaurant data, and a frequency threshold of 13. 4 Results and Analysis Table 2 shows the training and test accuracy of the task baseline system (Pontiki et al., 2014), a majority baseline classifying everything as positive, our rule-based system and our submitted system. The restaurant domain has a higher accuracy than the laptop domain for all systems, the SVM system outperforms the rule-based system on both domains, and the test accuracy is higher than the training accuracy for all systems in the restaurant domain. We observe that the majority of our systems’ errors fall into the following categories: Laptop Baseline — 51.1% Laptop All positive 41.9% 52.1% Laptop Rule-based 65.4% 67.7% Laptop SVM 72.3% 70.5% Restaurant Baseline — 64.3% Restaurant All pos</context>
</contexts>
<marker>Pontiki, Galanis, Pavlopoulos, Papageorgiou, Androutsopoulos, Manandhar, 2014</marker>
<rawString>Maria Pontiki, Dimitrios Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014. Semeval-2014 task 4: Aspect based sentiment analysis. In Proceedings of the International Workshop on Semantic Evaluation (SemEval).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Rubino</author>
<author>Jennifer Foster</author>
<author>Joachim Wagner</author>
<author>Johann Roturier</author>
<author>Rasul Samad Zadeh Kaljahi</author>
<author>Fred Hollowood</author>
</authors>
<title>Dcu-symantec submission for the wmt 2012 quality estimation task.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>138--144</pages>
<contexts>
<context position="17735" citStr="Rubino et al., 2012" startWordPosition="2877" endWordPosition="2880">ores/labels from sentiment lexicons has also been used by Baccianella et al. (2009), who supplement n-grams such as horrible location with generalised expressions such as NEGATIVE location. Linguistic features which capture generalisations at the level of syntax (Matsumoto et al., 2005), semantics (Johansson and Moschitti, 2010) and discourse (Lazaridou et al., 2013) have also been widely applied. In using binarised features derived from the nodes of a decision tree, we are following our recent work which uses the same technique in a different task: quality estimation for machine translation (Rubino et al., 2012; Rubino et al., 2013). The main novelty in our system lies not in the individual techniques but rather in they way they are combined and integrated. For example, our combination of token/chunk/dependency path distance used to weight the relationship between a sentiment word and the aspect term has – to the best of our knowledge – not been applied before. 6 Efficiency Building a system for a shared task, we focus solely on the accuracy of the system in all our decisions. For example, we parse all training and test data multiple times using different grammars to increase sentence coverage from </context>
</contexts>
<marker>Rubino, Foster, Wagner, Roturier, Kaljahi, Hollowood, 2012</marker>
<rawString>Raphael Rubino, Jennifer Foster, Joachim Wagner, Johann Roturier, Rasul Samad Zadeh Kaljahi, and Fred Hollowood. 2012. Dcu-symantec submission for the wmt 2012 quality estimation task. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 138–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Rubino</author>
<author>Joachim Wagner</author>
<author>Jennifer Foster</author>
<author>Johann Roturier</author>
<author>Rasoul Samad Zadeh Kaljahi</author>
<author>Fred Hollowood</author>
</authors>
<title>DCU-Symantec at the WMT 2013 quality estimation shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>392--397</pages>
<contexts>
<context position="17757" citStr="Rubino et al., 2013" startWordPosition="2881" endWordPosition="2884">iment lexicons has also been used by Baccianella et al. (2009), who supplement n-grams such as horrible location with generalised expressions such as NEGATIVE location. Linguistic features which capture generalisations at the level of syntax (Matsumoto et al., 2005), semantics (Johansson and Moschitti, 2010) and discourse (Lazaridou et al., 2013) have also been widely applied. In using binarised features derived from the nodes of a decision tree, we are following our recent work which uses the same technique in a different task: quality estimation for machine translation (Rubino et al., 2012; Rubino et al., 2013). The main novelty in our system lies not in the individual techniques but rather in they way they are combined and integrated. For example, our combination of token/chunk/dependency path distance used to weight the relationship between a sentiment word and the aspect term has – to the best of our knowledge – not been applied before. 6 Efficiency Building a system for a shared task, we focus solely on the accuracy of the system in all our decisions. For example, we parse all training and test data multiple times using different grammars to increase sentence coverage from 99.87% to 100%. To off</context>
</contexts>
<marker>Rubino, Wagner, Foster, Roturier, Kaljahi, Hollowood, 2013</marker>
<rawString>Raphael Rubino, Joachim Wagner, Jennifer Foster, Johann Roturier, Rasoul Samad Zadeh Kaljahi, and Fred Hollowood. 2013. DCU-Symantec at the WMT 2013 quality estimation shared task. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 392–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<contexts>
<context position="5839" citStr="Schmid, 1994" startWordPosition="907" endWordPosition="908">icon but which do express sentiment in these domains. Examples are mouthwatering, watery and betterconfigured. We add these to the lexicon with a score of either 1 or -1 (depending on their polarity in the training data). We also add words (e.g. zesty, acrid) from an online list of culinary terms.6 5We also tried to vote over the four lexicon scores but this did not improve over summing. 6http://world-food-and-wine.com/ describing-food 2.4 Handling Variation In order to ensure that all inflected forms of a word are covered, we lemmatise the words in the training data using the IMS TreeTagger (Schmid, 1994) and we construct new possibilities using a suffix list. To correct misspelled words, we consider the corrected form of a misspelled word to be the form with the highest frequency in a reference corpus7 among all the forms within an edit distance of 1 and 2 from the misspelled word (Norvig, 2012). Multi-word expressions of the form x-y are added with the polarity of xy or x, as in laidback/laidback and well-shaped/well. Expressions x y, are added with the polarity of x-y, as in so so/so-so. 3 Methodology We first build a rule-based system which classifies the polarity of an aspect term based s</context>
<context position="18954" citStr="Schmid, 1994" startWordPosition="3084" endWordPosition="3085"> to 100%. To offer a more practical system, we work on implementing a simplified, fully automated system that is more efficient. So far, we replaced time-consuming parsing with POS tagging. The system accepts as input and generates as output valid SemEval ABSA XML documents.12 After extracting the text and the aspect terms from the input, the text is normalised using the process described in Footnote 8. The feature extraction is performed as described in Section 3 with the following modifications: • The POS information used by the n-gram feature extractor is obtained using the IMS TreeTagger (Schmid, 1994) instead of using the PCFG-LA parser (Attia et al., 2010). • The distance used by the rule-based approach is the token distance only, instead of a combination of three distance functions. The sentiment lexicon and the classification models used are described in Sections 2 and 3 respectively. The test sets containing 800 sentences are POS tagged in less than half a second each. Surprisingly, accuracy of aspect term polarity prediction increases to 71.4% (from 70.5% for the submitted system) on the laptop test set, using the same SVM parameters as for the submitted system. However, we see a degr</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the International Conference on New Methods in Language Processing, pages 44–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1631--1642</pages>
<contexts>
<context position="22112" citStr="Socher et al. (2013)" startWordPosition="3593" endWordPosition="3596"> overall performance, e. g. lexicon combination, lemmatisation, spelling correction, other normalisations, negation handling, distance function and n-gram feature transformations. There is also room for improvements in most of these components, e. g. our handling of complex negations. Detection of conflicts also needs more attention. Features indicating the presence of trigger words for negation and conflicts that are currently used only internally in the rule-based component could be added to the SVM feature set. It would also be interesting to see how the compositional approach described by Socher et al. (2013) handles these difficult cases. The score features could be easily augmented by breaking down scores by the four employed lexicons. This way, the SVM can choose to combine the information from these scores differently than just summing them, allowing it to learn more complex relations. Lexicon filtering and addition of domain-specific entries could be automated to reduce the time needed to adjust to a new domain. Finally, machine learning methods that can efficiently handle large feature sets such as logistic regression should be tried with the full feature set (not applying frequency threshol</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of EMNLP, pages 1631– 1642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Milan Tofiloski</author>
<author>Julian Brooke</author>
<author>Maite Taboada</author>
</authors>
<title>A syntactic and lexical-based discourse segmenter.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort ’09,</booktitle>
<pages>77--80</pages>
<contexts>
<context position="8197" citStr="Tofiloski et al., 2009" startWordPosition="1294" endWordPosition="1297">Gutenberg (http://www.gutenberg.org/), lists of most frequent words from Wiktionary (http: //en.wiktionary.org/wiki/Wiktionary: Frequency_lists) and the British National Corpus (http://www.kilgarriff.co.uk/bnc-readme. html) and two thousand laptop reviews crawled from CNET (http://www.cnet.com/). 224 • Discourse Chunk Distance: This function counts the discourse chunks that must be crossed in order to get from the sentiment word to the aspect term. If the sentiment word and the aspect term are in the same discourse chunk, then the distance is zero. We use the discourse segmenter described in (Tofiloski et al., 2009). • Dependency Path Distance: This function calculates the shortest path between the sentiment word and the aspect term in a syntactic dependency graph for the sentence, produced by parsing the sentence with a PCFGLA parser (Attia et al., 2010) trained on consumer review data (Le Roux et al., 2012)8, and converting the resulting phrase-structure tree into a dependency graph using the Stanford converter (de Marneffe and Manning, 2008) (version 3.3.1). Since our lexicon also contains multi-word expressions such as finger licking, we also look up bigrams and trigrams from the input sentence in ou</context>
</contexts>
<marker>Tofiloski, Brooke, Taboada, 2009</marker>
<rawString>Milan Tofiloski, Julian Brooke, and Maite Taboada. 2009. A syntactic and lexical-based discourse segmenter. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort ’09, pages 77– 80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Thumbs up or thumbs down? semantic orientation applied to unsupervised classication of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>417--424</pages>
<contexts>
<context position="16828" citStr="Turney (2002)" startWordPosition="2736" endWordPosition="2738">olate raspberry cake is heavenly - not too sweet, but full offlavor. 5 Related Work The use of supervised machine learning with bagof-word or bag-of-n-gram feature sets has been a standard approach to the problem of sentiment polarity classification since the seminal work by Pang et al. (2002) on movie review polarity prediction. Heuristic methods which rely on a lexicon of sentiment words have also been widespread and much of the research in this area has been devoted to the unsupervised induction of good quality sentiment indicators (see, for example, Hatzivassiloglou and McKeown (1997) and Turney (2002), and Liu (2010) for an overview). The integration of sentiment lexicon scores as features in supervised machine learning to supplement standard bag-of-n-gram features has also been employed before (see, for example, Bakliwal et al. (2013)). The replacement of training/test words with scores/labels from sentiment lexicons has also been used by Baccianella et al. (2009), who supplement n-grams such as horrible location with generalised expressions such as NEGATIVE location. Linguistic features which capture generalisations at the level of syntax (Matsumoto et al., 2005), semantics (Johansson an</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter Turney. 2002. Thumbs up or thumbs down? semantic orientation applied to unsupervised classication of reviews. In Proceedings of the ACL, pages 417–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="2980" citStr="Wilson et al., 2005" startWordPosition="465" endWordPosition="468">hine learning method which incorporates the rule-based method as features in addition to word n-gram features; in Section 4, we present the results of both methods on the training and test data, and perform an error analysis on the test set; in Section 5, we compare our approach to previous research in sentiment classification; Section 6 discusses efficiency of our system and ongoing work to improve its speed; finally, in Section 7, we conclude and provide suggestions as to how this research could be fruitfully extended. 2 Sentiment Lexicons The following four lexicons are employed: 1. MPQA1 (Wilson et al., 2005) classifies a word or a stem and its part of speech tag into positive, negative, both or neutral with a strong or weak subjectivity. 2. SentiWordNet2 (Baccianella et al., 2010) specifies the positive, negative and objective scores of a synset and its part of speech tag. 3. General Inquirer3 indicates whether a word expresses positive or negative sentiment. 4. Bing Liu’s Opinion Lexicon4 (Hu and Liu, 1http://mpqa.cs.pitt.edu/lexicons/ subj_lexicon/ 2http://sentiwordnet.isti.cnr.it/ 3http://www.wjh.harvard.edu/˜inquirer/ inqtabs.txt 4http://www.cs.uic.edu/˜liub/FBS/ sentiment-analysis.html#lexic</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 347–354.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>