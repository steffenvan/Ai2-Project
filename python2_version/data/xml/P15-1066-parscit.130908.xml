<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000082">
<title confidence="0.992176">
Learning the Semantics of Manipulation Action
</title>
<author confidence="0.957656">
Yezhou Yangt and Yiannis Aloimonost and Cornelia Ferm¨ullert and Eren Erdal Aksoyt
</author>
<affiliation confidence="0.957909">
t UMIACS, University of Maryland, College Park, MD, USA
</affiliation>
<email confidence="0.816827">
{yzyang, yiannis, fer}@umiacs.umd.edu
</email>
<affiliation confidence="0.952396">
t Karlsruhe Institute of Technology, Karlsruhe, Germany
</affiliation>
<email confidence="0.998617">
eren.aksoy@kit.edu
</email>
<sectionHeader confidence="0.993946" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999555115384615">
In this paper we present a formal compu-
tational framework for modeling manip-
ulation actions. The introduced formal-
ism leads to semantics of manipulation ac-
tion and has applications to both observ-
ing and understanding human manipula-
tion actions as well as executing them with
a robotic mechanism (e.g. a humanoid
robot). It is based on a Combinatory Cat-
egorial Grammar. The goal of the intro-
duced framework is to: (1) represent ma-
nipulation actions with both syntax and se-
mantic parts, where the semantic part em-
ploys A-calculus; (2) enable a probabilis-
tic semantic parsing schema to learn the
lambda-calculus representation of manip-
ulation action from an annotated action
corpus of videos; (3) use (1) and (2) to de-
velop a system that visually observes ma-
nipulation actions and understands their
meaning while it can reason beyond ob-
servations using propositional logic and
axiom schemata. The experiments con-
ducted on a public available large manip-
ulation action dataset validate the theoret-
ical framework and our implementation.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999774735849057">
Autonomous robots will need to learn the actions
that humans perform. They will need to recognize
these actions when they see them and they will
need to perform these actions themselves. This re-
quires a formal system to represent the action se-
mantics. This representation needs to store the se-
mantic information about the actions, be encoded
in a machine readable language, and inherently be
in a programmable fashion in order to enable rea-
soning beyond observation. A formal represen-
tation of this kind has a variety of other appli-
cations such as intelligent manufacturing, human
robot collaboration, action planning and policy de-
sign, etc.
In this paper, we are concerned with manipula-
tion actions, that is actions performed by agents
(humans or robots) on objects, resulting in some
physical change of the object. However most of
the current AI systems require manually defined
semantic rules. In this work, we propose a com-
putational linguistics framework, which is based
on probabilistic semantic parsing with Combina-
tory Categorial Grammar (CCG), to learn manip-
ulation action semantics (lexicon entries) from an-
notations. We later show that this learned lexicon
is able to make our system reason about manipu-
lation action goals beyond just observation. Thus
the intelligent system can not only imitate human
movements, but also imitate action goals.
Understanding actions by observation and exe-
cuting them are generally considered as dual prob-
lems for intelligent agents. The sensori-motor
bridge connecting the two tasks is essential, and
a great amount of attention in AI, Robotics as well
as Neurophysiology has been devoted to investi-
gating it. Experiments conducted on primates have
discovered that certain neurons, the so-called mir-
ror neurons, fire during both observation and ex-
ecution of identical manipulation tasks (Rizzolatti
et al., 2001; Gazzola et al., 2007). This suggests
that the same process is involved in both the obser-
vation and execution of actions. From a function-
alist point of view, such a process should be able
to first build up a semantic structure from obser-
vations, and then the decomposition of that same
structure should occur when the intelligent agent
executes commands.
Additionally, studies in linguistics (Steedman,
2002) suggest that the language faculty develops
in humans as a direct adaptation of a more primi-
tive apparatus for planning goal-directed action in
the world by composing affordances of tools and
consequences of actions. It is this more primitive
</bodyText>
<page confidence="0.984024">
676
</page>
<note confidence="0.978204666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 676–686,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.998540409090909">
apparatus that is our major interest in this paper.
Such an apparatus is composed of a “syntax part”
and a “semantic part”. In the syntax part, every lin-
guistic element is categorized as either a function
or a basic type, and is associated with a syntactic
category which either identifies it as a function or a
basic type. In the semantic part, a semantic trans-
lation is attached following the syntactic category
explicitly.
Combinatory Categorial Grammar (CCG) intro-
duced by (Steedman, 2000) is a theory that can
be used to represent such structures with a small
set of combinators such as functional application
and type-raising. What do we gain though from
such a formal description of action? This is simi-
lar to asking what one gains from a formal descrip-
tion of language as a generative system. Chom-
skys contribution to language research was exactly
this: the formal description of language through
the formulation of the Generative and Transforma-
tional Grammar (Chomsky, 1957). It revolution-
ized language research opening up new roads for
the computational analysis of language, provid-
ing researchers with common, generative language
structures and syntactic operations, on which lan-
guage analysis tools were built. A grammar for
action would contribute to providing a common
framework of the syntax and semantics of action,
so that basic tools for action understanding can be
built, tools that researchers can use when develop-
ing action interpretation systems, without having
to start development from scratch. The same tools
can be used by robots to execute actions.
In this paper, we propose an approach for learn-
ing the semantic meaning of manipulation action
through a probabilistic semantic parsing frame-
work based on CCG theory. For example, we want
to learn from an annotated training action corpus
that the action “Cut” is a function which has two
arguments: a subject and a patient. Also, the ac-
tion consequence of “Cut” is a separation of the
patient. Using formal logic representation, our
system will learn the semantic representations of
“Cut”:
</bodyText>
<subsectionHeader confidence="0.466869">
Cut :=(AP\NP)/NP : λx.λy.cut(x, y) → divided(y)
</subsectionHeader>
<bodyText confidence="0.997655090909091">
Here cut(x, y) is a primitive function. We will fur-
ther introduce the representation in Sec. 3. Since
our action representation is in a common calculus
form, it enables naturally further logical reasoning
beyond visual observation.
The advantage of our approach is twofold: 1)
Learning semantic representations from annota-
tions helps an intelligent agent to enrich automat-
ically its own knowledge about actions; 2) The
formal logic representation of the action could be
used to infer the object-wise consequence after a
certain manipulation, and can also be used to plan
a set of actions to reach a certain action goal. We
further validate our approach on a large publicly
available manipulation action dataset (MANIAC)
from (Aksoy et al., 2014), achieving promising ex-
perimental results. Moreover, we believe that our
work, even though it only considers the domain of
manipulation actions, is also a promising example
of a more closely intertwined computer vision and
computational linguistics system. The diagram in
Fig.1 depicts the framework of the system.
</bodyText>
<figureCaption confidence="0.9820575">
Figure 1: A CCG based semantic parsing frame-
work for manipulation actions.
</figureCaption>
<sectionHeader confidence="0.99794" genericHeader="introduction">
2 Related Works
</sectionHeader>
<bodyText confidence="0.99993625">
Reasoning beyond appearance: The very small
number of works in computer vision, which aim to
reason beyond appearance models, are also related
to this paper. (Xie et al., 2013) proposed that be-
yond state-of-the-art computer vision techniques,
we could possibly infer implicit information (such
as functional objects) from video, and they call
them “Dark Matter” and “Dark Energy”. (Yang
et al., 2013) used stochastic tracking and graph-
cut based segmentation to infer manipulation con-
sequences beyond appearance. (Joo et al., 2014)
used a ranking SVM to predict the persuasive mo-
tivation (or the intention) of the photographer who
captured an image. More recently, (Pirsiavash et
al., 2014) seeks to infer the motivation of the per-
son in the image by mining knowledge stored in
</bodyText>
<page confidence="0.997642">
677
</page>
<bodyText confidence="0.992237696078432">
a large corpus using natural language processing
techniques. Different from these fairly general in-
vestigations about reasoning beyond appearance,
our paper seeks to learn manipulation actions se-
mantics in logic forms through CCG, and further
infer hidden action consequences beyond appear-
ance through reasoning.
Action Recognition and Understanding: Hu-
man activity recognition and understanding has
been studied heavily in Computer Vision recently,
and there is a large range of applications for this
work in areas like human-computer interactions,
biometrics, and video surveillance. Both visual
recognition methods, and the non-visual descrip-
tion methods using motion capture systems have
been used. A few good surveys of the former can
be found in (Moeslund et al., 2006) and (Turaga et
al., 2008). Most of the focus has been on recog-
nizing single human actions like walking, jump-
ing, or running etc. (Ben-Arie et al., 2002; Yilmaz
and Shah, 2005). Approaches to more complex ac-
tions have employed parametric approaches, such
as HMMs (Kale et al., 2004) to learn the transi-
tion between feature representations in individual
frames e.g. (Saisan et al., 2001; Chaudhry et al.,
2009). More recently, (Aksoy et al., 2011; Ak-
soy et al., 2014) proposed a semantic event chain
(SEC) representation to model and learn the se-
mantic segment-wise relationship transition from
spatial-temporal video segmentation.
There also have been many syntactic ap-
proaches to human activity recognition which used
the concept of context-free grammars, because
such grammars provide a sound theoretical basis
for modeling structured processes. Tracing back
to the middle 90’s, (Brand, 1996) used a grammar
to recognize disassembly tasks that contain hand
manipulations. (Ryoo and Aggarwal, 2006) used
the context-free grammar formalism to recognize
composite human activities and multi-person in-
teractions. It is a two level hierarchical approach
where the lower-levels are composed of HMMs
and Bayesian Networks while the higher-level in-
teractions are modeled by CFGs. To deal with
errors from low-level processes such as tracking,
stochastic grammars such as stochastic CFGs were
also used (Ivanov and Bobick, 2000; Moore and
Essa, 2002). More recently, (Kuehne et al., 2014)
proposed to model goal-directed human activi-
ties using Hidden Markov Models and treat sub-
actions just like words in speech. These works
proved that grammar based approaches are prac-
tical in activity recognition systems, and shed
insight onto human manipulation action under-
standing. However, as mentioned, thinking about
manipulation actions solely from the viewpoint
of recognition has obvious limitations. In this
work we adopt principles from CFG based activ-
ity recognition systems, with extensions to a CCG
grammar that accommodates not only the hierar-
chical structure of human activity but also action
semantics representations. It enables the system
to serve as the core parsing engine for both ma-
nipulation action recognition and execution.
Manipulation Action Grammar: As men-
tioned before, (Chomsky, 1993) suggested that a
minimalist generative grammar, similar to the one
of human language, also exists for action under-
standing and execution. The works closest related
to this paper are (Pastra and Aloimonos, 2012;
Summers-Stay et al., 2013; Guha et al., 2013).
(Pastra and Aloimonos, 2012) first discussed a
Chomskyan grammar for understanding complex
actions as a theoretical concept, and (Summers-
Stay et al., 2013) provided an implementation of
such a grammar using as perceptual input only
objects. More recently, (Yang et al., 2014) pro-
posed a set of context-free grammar rules for ma-
nipulation action understanding, and (Yang et al.,
2015) applied it on unconstrained instructional
videos. However, these approaches only con-
sider the syntactic structure of manipulation ac-
tions without coupling semantic rules using λ ex-
pressions, which limits the capability of doing rea-
soning and prediction.
Combinatory Categorial Grammar and Se-
mantic Parsing: CCG based semantic parsing
originally was used mainly to translate natural
language sentences to their desired semantic rep-
resentations as λ-calculus formulas (Zettlemoyer
and Collins, 2005; Zettlemoyer and Collins,
2007). (Mooney, 2008) presented a framework
of grounded language acquisition: the interpre-
tation of language entities into semantically in-
formed structures in the context of perception and
actuation. The concept has been applied success-
fully in tasks such as robot navigation (Matuszek
et al., 2011), forklift operation (Tellex et al., 2014)
and of human-robot interaction (Matuszek et al.,
2014). In this work, instead of grounding natural
language sentences directly, we ground informa-
tion obtained from visual perception into seman-
</bodyText>
<page confidence="0.994725">
678
</page>
<bodyText confidence="0.9459705">
tically informed structures, specifically in the do-
main of manipulation actions.
</bodyText>
<sectionHeader confidence="0.9923495" genericHeader="method">
3 A CCG Framework for Manipulation
Actions
</sectionHeader>
<bodyText confidence="0.999996090909091">
Before we dive into the semantic parsing of ma-
nipulation actions, a brief introduction to the Com-
binatory Categorial Grammar framework in Lin-
guistics is necessary. We will only introduce re-
lated concepts and formalisms. For a complete
background reading, we would like to refer read-
ers to (Steedman, 2000). We will first give a brief
introduction to CCG and then introduce a fun-
damental combinator, i.e., functional application.
The introduction is followed by examples to show
how the combinator is applied to parse actions.
</bodyText>
<subsectionHeader confidence="0.998918">
3.1 Manipulation Action Semantics
</subsectionHeader>
<bodyText confidence="0.999549">
The semantic expression in our representation of
manipulation actions uses a typed A-calculus lan-
guage. The formal system has two basic types:
entities and functions. Entities in manipulation
actions are Objects or Hands, and functions are
the Actions. Our lambda-calculus expressions are
formed from the following items:
Constants: Constants can be either entities or
functions. For example, Knife is an entity (i.e., it
is of type N) and Cucumber is an entity too (i.e., it
is of type N). Cut is an action function that maps
entities to entities. When the event Knife Cut Cu-
cumber happened, the expression cut(Knife, Cu-
cumber) returns an entity of type AP, aka. Action
Phrase. Constants like divided are status functions
that map entities to truth values. The expression
divided(cucumber) returns a true value after the
event (Knife Cut Cucumber) happened.
Logical connectors: The A-calculus expression
has logical connectors like conjunction (∧), dis-
junction (V), negation(¬) and implication(→).
For example, the expression
</bodyText>
<equation confidence="0.9589">
connected(tomato, cucumber)∧
divided(tomato) ∧ divided(cucumber)
</equation>
<bodyText confidence="0.999957166666667">
represents the joint status that the sliced tomato
merged with the sliced cucumber. It can be
regarded as a simplified goal status for “mak-
ing a cucumber tomato salad”. The expression
¬connected(spoon, bowl) represents the status
after the spoon finished stirring the bowl.
</bodyText>
<equation confidence="0.588405">
λx.cut(x, cucumber) → divided(cucumber)
</equation>
<bodyText confidence="0.999979428571429">
represents that if the cucumber is cut by x, then
the status of the cucumber is divided.
A expressions: lambda expressions represent
functions with unknown arguments. For example,
Ax.cut(knife, x) is a function from entities to en-
tities, which is of type NP after any entities of type
N that is cut by knife.
</bodyText>
<subsectionHeader confidence="0.998505">
3.2 Combinatory Categorial Grammar
</subsectionHeader>
<bodyText confidence="0.999967928571429">
The semantic parsing formalism underlying our
framework for manipulation actions is that of
combinatory categorial grammar (CCG) (Steed-
man, 2000). A CCG specifies one or more logi-
cal forms for each element or combination of ele-
ments for manipulation actions. In our formalism,
an element of Action is associated with a syntac-
tic “category” which identifies it as functions, and
specifies the type and directionality of their argu-
ments and the type of their result. For example, ac-
tion “Cut” is a function from patient object phrase
(NP) on the right into predicates, and into func-
tions from subject object phrase (NP) on the left
into a sub action phrase (AP):
</bodyText>
<equation confidence="0.99156">
Cut := (AP\NP)/NP. (1)
</equation>
<bodyText confidence="0.99997825">
As a matter of fact, the pure categorial gram-
mar is a conext-free grammar presented in the ac-
cepting, rather than the producing direction. The
expression (1) is just an accepting form for Ac-
tion “Cut” following the context-free grammar.
While it is now convenient to write derivations as
follows, they are equivalent to conventional tree
structure derivations in Figure. 3.2.
</bodyText>
<figure confidence="0.784767666666667">
Knife Cut Cucumber
N N
NP (AP\NP)/NP NP
</figure>
<figureCaption confidence="0.999675">
Figure 2: Example of conventional tree structure.
</figureCaption>
<bodyText confidence="0.9980255">
The semantic type is encoded in these cate-
gories, and their translation can be made explicit
</bodyText>
<figure confidence="0.992904153846154">
AP
AP
AP\NP
&lt;
AP
NP
N
Cucumber
NP
N
Knife
A
Cut
</figure>
<page confidence="0.992202">
679
</page>
<bodyText confidence="0.9988661">
in an expanded notation. Basically a λ-calculus
expression is attached with the syntactic category.
A colon operator is used to separate syntactical
and semantic expressions, and the right side of the
colon is assumed to have lower precedence than
the left side of the colon. Which is intuitive as any
explanation of manipulation actions should first
obey syntactical rules, then semantic rules. Now
the basic element, Action “Cut”, can be further
represented by:
</bodyText>
<subsubsectionHeader confidence="0.425914">
Cut :=(AP\NP)/NP : λx.λy.cut(x, y) → divided(y).
</subsubsectionHeader>
<bodyText confidence="0.999958125">
(AP\NP)/NP denotes a phrase of type AP,
which requires an element of type NP to specify
what object was cut, and requires another element
of type NP to further complement what effector
initiates the cut action. λx.λy.cut(x, y) is the λ-
calculus representation for this function. Since the
functions are closely related to the state update,
—* divided(y) further points out the status expres-
sion after the action was performed.
A CCG system has a set of combinatory rules
which describe how adjacent syntatic categories
in a string can be recursively combined. In the
setting of manipulation actions, we want to point
out that similar combinatory rules are also appli-
cable. Especially the functional application rules
are essential in our system.
</bodyText>
<subsectionHeader confidence="0.995278">
3.3 Functional application
</subsectionHeader>
<bodyText confidence="0.999886">
The functional application rules with semantics
can be expressed in the following form:
</bodyText>
<equation confidence="0.980535">
A/B : f B : g =&gt; A : f(g) (2)
B : g A\B : f =&gt; A : f(g) (3)
</equation>
<bodyText confidence="0.990292857142857">
Rule. (2) says that a string with type A/B can be
combined with a right-adjacent string of type B to
form a new string of type A. At the same time, it
also specifies how the semantics of the category A
can be compositionally built out of the semantics
for A/B and B. Rule. (3) is a symmetric form of
Rule. (2).
In the domain of manipulation actions, follow-
ing derivation is an example CCG parse. This
parse shows how the system can parse an ob-
servation (“Knife Cut Cucumber”) into a se-
mantic representation (cut(knife, cucumber) —*
divided(cucumber)) using the functional appli-
cation rules.
</bodyText>
<figure confidence="0.918548">
Knife Cut Cucumber
N N
NP (AP\NP)/NP NP
knife λx.λy.cut(x, y) cucumber
knife → divided(y) cucumber
&gt;
AP
cut(knife, cucumber)
→ divided(cucumber)
</figure>
<sectionHeader confidence="0.939106" genericHeader="method">
4 Learning Model and Semantic Parsing
</sectionHeader>
<bodyText confidence="0.999948583333333">
After having defined the formalism and applica-
tion rule, instead of manually writing down all the
possible CCG representations for each entity, we
would like to apply a learning technique to de-
rive them from the paired training corpus. Here
we adopt the learning model of (Zettlemoyer and
Collins, 2005), and use it to assign weights to the
semantic representation of actions. Since an ac-
tion may have multiple possible syntactic and se-
mantic representations assigned to it, we use the
probabilistic model to assign weights to these rep-
resentations.
</bodyText>
<subsectionHeader confidence="0.99915">
4.1 Learning Approach
</subsectionHeader>
<bodyText confidence="0.999648833333333">
First we assume that complete syntactic parses of
the observed action are available, and in fact a ma-
nipulation action can have several different parses.
The parsing uses a probabilistic combinatorial cat-
egorial grammar framework similar to the one
given by (Zettlemoyer and Collins, 2007). We as-
sume a probabilistic categorial grammar (PCCG)
based on a log linear model. M denotes a manipu-
lation task, L denotes the semantic representation
of the task, and T denotes its parse tree. The prob-
ability of a particular syntactic and semantic parse
is given as:
</bodyText>
<equation confidence="0.99656">
ef(L,T,M)·Θ
P(L, T|M; O) = E(L,T) ef(L,T,M)·Θ (4)
</equation>
<bodyText confidence="0.999786714285714">
where f is a mapping of the triple (L, T, M) to
feature vectors E Rd, and the O E Rd represents
the weights to be learned. Here we use only lexi-
cal features, where each feature counts the number
of times a lexical entry is used in T. Parsing a ma-
nipulation task under PCCG equates to finding L
such that P(L|M; O) is maximized:
</bodyText>
<equation confidence="0.986047666666667">
argmaxLP(L|M; O)
�= argmaxL P(L,T|M;O). (5)
T
AP\NP
λx.cut(x, cucumber)
→ divided(cucumber)
</equation>
<page confidence="0.958166">
680
</page>
<bodyText confidence="0.99998425">
We use dynamic programming techniques to
calculate the most probable parse for the manipu-
lation task. In this paper, the implementation from
(Baral et al., 2011) is adopted, where an inverse-λ
technique is used to generalize new semantic rep-
resentations. The generalization of lexicon rules
are essential for our system to deal with unknown
actions presented during the testing phase.
</bodyText>
<sectionHeader confidence="0.999861" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997394">
5.1 Manipulation Action (MANIAC) Dataset
</subsectionHeader>
<bodyText confidence="0.999969424242424">
(Aksoy et al., 2014) provides a manipulation ac-
tion dataset with 8 different manipulation actions
(cutting, chopping, stirring, putting, taking, hid-
ing, uncovering, and pushing), each of which con-
sists of 15 different versions performed by 5 dif-
ferent human actors1. There are in total 30 differ-
ent objects manipulated in all demonstrations. All
manipulations were recorded with the Microsoft
Kinect sensor and serve as training data here.
The MANIAC data set contains another 20 long
and complex chained manipulation sequences
(e.g. “making a sandwich”) which consist of a to-
tal of 103 different versions of these 8 manipula-
tion tasks performed in different orders with novel
objects under different circumstances. These serve
as testing data for our experiments.
(Aksoy et al., 2014; Aksoy and W¨org¨otter,
2015) developed a semantic event chain based
model free decomposition approach. It is an un-
supervised probabilistic method that measures the
frequency of the changes in the spatial relations
embedded in event chains, in order to extract the
subject and patient visual segments. It also decom-
poses the long chained complex testing actions
into their primitive action components according
to the spatio-temporal relations of the manipula-
tor. Since the visual recognition is not the core
of this work, we omit the details here and refer
the interested reader to (Aksoy et al., 2014; Aksoy
and W¨org¨otter, 2015). All these features make the
MANIAC dataset a great testing bed for both the
theoretical framework and the implemented sys-
tem presented in this work.
</bodyText>
<subsectionHeader confidence="0.996828">
5.2 Training Corpus
</subsectionHeader>
<bodyText confidence="0.99896">
We first created a training corpus by annotating
the 120 training clips from the MANIAC dataset,
</bodyText>
<footnote confidence="0.95855">
1Dataset available for download at https:
//fortknox.physik3.gwdg.de/cns/index.
php?page=maniac-dataset.
</footnote>
<bodyText confidence="0.992593545454545">
in the format of observed triplets (subject action
patient) and a corresponding semantic representa-
tion of the action as well as its consequence. The
semantic representations in λ-calculus format are
given by human annotators after watching each ac-
tion clip. A set of sample training pairs are given
in Table.1 (one from each action category in the
training set). Since every training clip contains
one single full execution of each manipulation ac-
tion considered, the training corpus thus has a total
of 120 paired training samples.
</bodyText>
<table confidence="0.99761205">
Snapshot triplet semantic representation
cleaver chopping carrot chopping(cleaver, carrot)
— divided(carrot)
spatula cutting pepper cutting(spatula, pepper)
— divided(pepper)
spoon stirring bucket stirring(spoon, bucket)
cup take down bucket take down(cup, bucket)
— —connected(cup, bucket)
∧moved(cup)
cup put on top bowl put on top(cup, bowl)
— on top(cup, bowl)
∧moved(cup)
bucket hiding ball hiding(bucket, ball)
— contained(bucket, ball)
∧moved(bucket)
hand pushing box pushing(hand, box)
— moved(box)
box uncover apple uncover(box, apple)
— appear(apple)
∧moved(box)
</table>
<tableCaption confidence="0.8459625">
Table 1: Example annotations from training cor-
pus, one per manipulation action category.
</tableCaption>
<bodyText confidence="0.999324">
We also assume the system knows that every
“object” involved in the corpus is an entity of its
own type, for example:
</bodyText>
<equation confidence="0.985971">
Knife := N : knife
Bowl := N : bowl
</equation>
<bodyText confidence="0.99997975">
Additionally, we assume the syntactic form of
each “action” has a main type (AP\NP)/NP
(see Sec. 3.2). These two sets of rules form the
initial seed lexicon for learning.
</bodyText>
<subsectionHeader confidence="0.996354">
5.3 Learned Lexicon
</subsectionHeader>
<bodyText confidence="0.999982333333333">
We applied the learning technique mentioned in
Sec. 4, and we used the NL2KR implementa-
tion from (Baral et al., 2011). The system learns
and generalizes a set of lexicon entries (syntactic
and semantic) for each action categories from the
training corpus accompanied with a set of weights.
</bodyText>
<page confidence="0.997434">
681
</page>
<bodyText confidence="0.9976975">
We list the one with the largest weight for each ac-
tion here respectively:
</bodyText>
<figure confidence="0.964572666666667">
Chopping :=(AP\NP)/NP : Ax.Ay.chopping(x, y)
→ divided(y)
Cutting :=(AP\NP)/NP : Ax.Ay.cutting(x, y)
→ divided(y)
Stirring :=(AP\NP)/NP : Ax.Ay.stirring(x, y)
Take down :=(AP\NP)/NP : Ax.Ay.take down(x, y)
→ ¬connected(x, y) n moved(x)
Put on top :=(AP\NP)/NP : Ax.Ay.put on top(x, y)
→ on top(x, y) n moved(x)
Hiding :=(AP\NP)/NP : Ax.Ay.hiding(x, y)
→ contained(x,y) n moved(x)
Pushing :=(AP\NP)/NP : Ax.Ay.pushing(x, y)
→ moved(y)
Uncover :=(AP\NP)/NP : Ax.Ay.uncover(x, y)
→ appear(y) n moved(x).
</figure>
<bodyText confidence="0.99687775">
The set of seed lexicon and the learned lexicon
entries are further used to probabilistically parse
the detected triplet sequences from the 20 long
manipulation activities in the testing set.
</bodyText>
<subsectionHeader confidence="0.996662">
5.4 Deducing Semantics
</subsectionHeader>
<bodyText confidence="0.999983227272727">
Using the decomposition technique from (Aksoy
et al., 2014; Aksoy and W¨org¨otter, 2015), the re-
ported system is able to detect a sequence of ac-
tion triplets in the form of (Subject Action Pa-
tient) from each of the testing sequence in MA-
NIAC dataset. Briefly speaking, the event chain
representation (Aksoy et al., 2011) of the observed
long manipulation activity is first scanned to esti-
mate the main manipulator, i.e. the hand, and ma-
nipulated objects, e.g. knife, in the scene without
employing any visual feature-based object recog-
nition method. Solely based on the interactions
between the hand and manipulated objects in the
scene, the event chain is partitioned into chunks.
These chunks are further fragmented into sub-
units to detect parallel action streams. Each parsed
Semantic Event Chain (SEC) chunk is then com-
pared with the model SECs in the library to decide
whether the current SEC sample belongs to one
of the known manipulation models or represents a
novel manipulation. SEC models, stored in the li-
brary, are learned in an on-line unsupervised fash-
ion using the semantics of manipulations derived
from a given set of training data in order to create
a large vocabulary of single atomic manipulations.
For the different testing sequence, the number
of triplets detected ranges from two to seven. In to-
tal, we are able to collect 90 testing detections and
they serve as the testing corpus. However, since
many of the objects used in the testing data are not
present in the training set, an object model-free ap-
proach is adopted and thus “subject” and “patient”
fields are filled with segment IDs instead of a spe-
cific object name. Fig. 3 and 4 show several ex-
amples of the detected triplets accompanied with a
set of key frames from the testing sequences. Nev-
ertheless, the method we used here can 1) gener-
alize the unknown segments into the category of
object entities and 2) generalize the unknown ac-
tions (those that do not exist in the training corpus)
into the category of action function. This is done
by automatically generalizing the following two
types of lexicon entries using the inverse-A tech-
nique from (Baral et al., 2011):
</bodyText>
<figure confidence="0.3853475">
Object [ID] :=N : object [ID]
Unknown :=(AP\NP)/NP : Ax.Ay.unknown(x, y)
</figure>
<bodyText confidence="0.999908333333333">
Among the 90 detected triplets, using the
learned lexicon we are able to parse all of them
into semantic representations. Here we pick the
representation with the highest probability after
parsing as the individual action semantic represen-
tation. The “parsed semantics” rows of Fig. 3 and
4 show several example action semantics on test-
ing sequences. Taking the fourth sub-action from
Fig. 4 as an example, the visually detected triplets
based on segmentation and spatial decomposition
is (Object 014, Chopping, Object 011). Af-
ter semantic parsing, the system predicts that
divided(Object 011). The complete training cor-
pus and parsed results of the testing set will be
made publicly available for future research.
</bodyText>
<subsectionHeader confidence="0.998364">
5.5 Reasoning Beyond Observations
</subsectionHeader>
<bodyText confidence="0.999405266666667">
As mentioned before, because of the use of A-
calculus for representing action semantics, the ob-
tained data can naturally be used to do logical rea-
soning beyond observations. This by itself is a
very interesting research topic and it is beyond this
paper’s scope. However by applying a couple of
common sense Axioms on the testing data, we can
provide some flavor of this idea.
Case study one: See the “final action conse-
quence and reasoning” row of Fig. 3 for case one.
Using propositional logic and axiom schema, we
can represent the common sense statement (“if an
object x is contained in object y, and object z is
on top of object y, then object z is on top of object
x”) as follows:
</bodyText>
<page confidence="0.996072">
682
</page>
<figureCaption confidence="0.949497333333333">
Figure 3: System output on complex chained manipulation testing sequence one. The segmentation
output and detected triplets are from (Aksoy and W¨org¨otter, 2015)
.
Figure 4: System output on the 18th complex chained manipulation testing sequence. The segmentation
output and detected triplets are from (Aksoy and W¨org¨otter, 2015)
.
</figureCaption>
<construct confidence="0.9511335">
Axiom (1): Ix, y, z, contained(y, x) n
on top(z, y) —* on top(z, x).
</construct>
<bodyText confidence="0.9971998">
Then it is trivial to deduce an additional fi-
nal action consequence in this scenario that
(on top(object 007, object 009)). This matches
the fact: the yellow box which is put on top of the
red bucket is also on top of the black ball.
Case study two: See the “final action conse-
quence and reasoning” row of Fig. 4 for a more
complicated case. Using propositional logic and
axiom schema, we can represent three common
sense statements:
</bodyText>
<listItem confidence="0.9904005">
1) “if an object y is contained in object x, and
object z is contained in object y, then object z is
contained in object x”;
2) “if an object x is contained in object y, and
object y is divided, then object x is divided”;
3) “if an object x is contained in object y, and
object y is on top of object z, then object x is on
top of object z” as follows:
</listItem>
<construct confidence="0.9757285">
Axiom (2): Ix, y, z, contained(y, x) n
contained(z, y) —* contained(z, x).
Axiom (3): Ix, y, contained(y, x) n
divided(y) —* divided(x).
Axiom (4): Ix, y, z, contained(y, x) n
on top(y, z) —* on top(x, z).
</construct>
<bodyText confidence="0.957554785714286">
With these common sense Axioms, the system
is able to deduce several additional final action
consequences in this scenario:
divided(object 005) ∧ divided(object 010)
∧ on top(object 005, object 012)
∧ on top(object 010, object 012).
From Fig. 4, we can see that these additional
consequences indeed match the facts: 1) the bread
and cheese which are covered by ham are also di-
vided, even though from observation the system
only detected the ham being cut; 2) the divided
bread and cheese are also on top of the plate, even
though from observation the system only detected
the ham being put on top of the plate.
</bodyText>
<page confidence="0.998405">
683
</page>
<bodyText confidence="0.999990333333333">
We applied the four Axioms on the 20 testing
action sequences and deduced the “hidden” conse-
quences from observation. To evaluate our system
performance quantitatively, we first annotated all
the final action consequences (both obvious and
“hidden” ones) from the 20 testing sequences as
ground-truth facts. In total there are 122 conse-
quences annotated. Using perception only (Aksoy
and W¨org¨otter, 2015), due to the decomposition
errors (such as the red font ones in Fig. 4) the sys-
tem can detect 91 consequences correctly, yielding
a 74% correct rate. After applying the four Ax-
ioms and reasoning, our system is able to detect
105 consequences correctly, yielding a 86% cor-
rect rate. Overall, this is a 15.4% of improvement.
Here we want to mention a caveat: there are def-
initely other common sense Axioms that we are
not able to address in the current implementation.
However, from the case studies presented, we can
see that using the presented formal framework, our
system is able to reason about manipulation action
goals instead of just observing what is happening
visually. This capability is essential for intelligent
agents to imitate action goals from observation.
</bodyText>
<sectionHeader confidence="0.996185" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999543317073171">
In this paper we presented a formal computa-
tional framework for modeling manipulation ac-
tions based on a Combinatory Categorial Gram-
mar. An empirical study on a large manipula-
tion action dataset validates that 1) with the intro-
duced formalism, a learning system can be devised
to deduce the semantic meaning of manipulation
actions in λ-schema; 2) with the learned schema
and several common sense Axioms, our system is
able to reason beyond just observation and deduce
“hidden” action consequences, yielding a decent
performance improvement.
Due to the limitation of current testing scenar-
ios, we conducted experiments only considering a
relatively small set of seed lexicon rules and log-
ical expressions. Nevertheless, we want to men-
tion that the presented CCG framework can also
be extended to learn the formal logic representa-
tion of more complex manipulation action seman-
tics. For example, the temporal order of manipula-
tion actions can be modeled by considering a seed
rule such as AP\AP : λf.λg.before(f(·), g(·)),
where before(·, ·) is a temporal predicate. For
actions in this paper we consider seed main type
(AP\NP)/NP. For more general manipulation
scenarios, based on whether the action is transi-
tive or intransitive, the main types of action can be
extended to include AP\NP.
Moreover, the logical expressions can also be
extended to include universal quantification b and
existential quantification 1. Thus, manipulation
action such as “knife cut every tomato” can be
parsed into a representation as bx.tomato(x) ∧
cut(knife, x) —* divided(x) (the parse is given
in the following chart). Here, the concept “every”
has a main type of NP\NP and semantic mean-
ing of bx.f(x). The same framework can also
extended to have other combinatory rules such as
composition and type-raising (Steedman, 2002).
These are parts of the future work along the line of
the presented work.
</bodyText>
<table confidence="0.9876206">
Knife Cut every Tomato
N N
NP (AP\NP)/NP NP\NP NP
knife Ax.Ay.cut(x, y) ∀x.f (x) tomato
knife → divided(y) ∀x.f (x) tomato
</table>
<equation confidence="0.9050535">
NP
∀x.tomato(x)
AP
∀y.tomato(y) ∧ cut(knife, y) → divided(y)
</equation>
<bodyText confidence="0.999973866666667">
The presented computational linguistic frame-
work enables an intelligent agent to predict and
reason action goals from observation, and thus has
many potential applications such as human inten-
tion prediction, robot action policy planning, hu-
man robot collaboration etc. We believe that our
formalism of manipulation actions bridges com-
putational linguistics, vision and robotics, and
opens further research in Artificial Intelligence
and Robotics. As the robotics industry is moving
towards robots that function safely, effectively and
autonomously to perform tasks in real-world un-
structured environments, they will need to be able
to understand the meaning of actions and acquire
human-like common-sense reasoning capabilities.
</bodyText>
<sectionHeader confidence="0.998319" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.96278525">
This research was funded in part by the sup-
port of the European Union under the Cogni-
tive Systems program (project POETICON++),
the National Science Foundation under INSPIRE
grant SMA 1248056, and by DARPA through
U.S. Army grant W911NF-14-1-0384 under the
Project: Shared Perception, Cognition and Rea-
soning for Autonomy.
</bodyText>
<figure confidence="0.564448333333333">
AP\NP
∀y.Ax.tomato(y) ∧ cut(x, y) → divided(y)
&lt;
</figure>
<page confidence="0.993835">
684
</page>
<note confidence="0.9039252">
Jungseock Joo, Weixin Li, Francis F Steen, and Song-
Chun Zhu. 2014. Visual persuasion: Inferring com-
municative intents of images. In Computer Vision
and Pattern Recognition (CVPR), 2014 IEEE Con-
ference on, pages 216–223. IEEE.
</note>
<sectionHeader confidence="0.486165" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.8337885">
E E. Aksoy and F. W¨org¨otter. 2015. Semantic decom-
position and recognition of long and complex ma-
nipulation action sequences. International Journal
of Computer Vision, page Under Review.
</bodyText>
<reference confidence="0.998363125">
E.E. Aksoy, A. Abramov, J. D¨orr, K. Ning, B. Dellen,
and F. W¨org¨otter. 2011. Learning the semantics of
object–action relations by observation. The Interna-
tional Journal of Robotics Research, 30(10):1229–
1249.
E E. Aksoy, M. Tamosiunaite, and F. W¨org¨otter. 2014.
Model-free incremental learning of the semantics
of manipulation actions. Robotics and Autonomous
Systems, pages 1–42.
Chitta Baral, Juraj Dzifcak, Marcos Alvarez Gonzalez,
and Jiayu Zhou. 2011. Using inverse λ and gener-
alization to translate english to formal languages. In
Proceedings of the Ninth International Conference
on Computational Semantics, pages 35–44. Associ-
ation for Computational Linguistics.
Jezekiel Ben-Arie, Zhiqian Wang, Purvin Pandit, and
Shyamsundar Rajaram. 2002. Human activ-
ity recognition using multidimensional indexing.
Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 24(8):1091–1104.
Matthew Brand. 1996. Understanding manipulation in
video. In Proceedings of the Second International
Conference on Automatic Face and Gesture Recog-
nition, pages 94–99, Killington,VT. IEEE.
R. Chaudhry, A. Ravichandran, G. Hager, and R. Vidal.
2009. Histograms of oriented optical flow and binet-
cauchy kernels on nonlinear dynamical systems for
the recognition of human actions. In Proceedings
of the 2009 IEEE Intenational Conference on Com-
puter Vision and Pattern Recognition, pages 1932–
1939, Miami,FL. IEEE.
N. Chomsky. 1957. Syntactic Structures. Mouton de
Gruyter.
Noam Chomsky. 1993. Lectures on government and
binding: The Pisa lectures. Walter de Gruyter.
V Gazzola, G Rizzolatti, B Wicker, and C Keysers.
2007. The anthropomorphic brain: the mirror neu-
ron system responds to human and robotic actions.
Neuroimage, 35(4):1674–1684.
Anupam Guha, Yezhou Yang, Cornelia Ferm¨uller, and
Yiannis Aloimonos. 2013. Minimalist plans for in-
terpreting manipulation actions. Proceedings of the
2013 IEEE/RSJ International Conference on Intelli-
gent Robots and Systems, pages 5908–5914.
Yuri A. Ivanov and Aaron F. Bobick. 2000. Recogni-
tion of visual activities and interactions by stochastic
parsing. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 22(8):852–872.
A. Kale, A. Sundaresan, AN Rajagopalan, N.P. Cun-
toor, A.K. Roy-Chowdhury, V. Kruger, and R. Chel-
lappa. 2004. Identification of humans using
gait. IEEE Transactions on Image Processing,
13(9):1163–1173.
Hilde Kuehne, Ali Arslan, and Thomas Serre. 2014.
The language of actions: Recovering the syntax
and semantics of goal-directed human activities. In
Computer Vision and Pattern Recognition (CVPR),
2014 IEEE Conference on, pages 780–787. IEEE.
Cynthia Matuszek, Nicholas FitzGerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2011. A joint
model of language and perception for grounded at-
tribute learning. In International Conference on Ma-
chine learning (ICML).
Cynthia Matuszek, Liefeng Bo, Luke Zettlemoyer, and
Dieter Fox. 2014. Learning from unscripted deictic
gesture and language for human-robot interactions.
In Twenty-Eighth AAAI Conference on Artificial In-
telligence.
T.B. Moeslund, A. Hilton, and V. Kr¨uger. 2006. A
survey of advances in vision-based human motion
capture and analysis. Computer vision and image
understanding, 104(2):90–126.
Raymond J Mooney. 2008. Learning to connect lan-
guage and perception. In AAAI, pages 1598–1601.
Darnell Moore and Irfan Essa. 2002. Recognizing
multitasked activities from video using stochastic
context-free grammar. In Proceedings of the Na-
tional Conference on Artificial Intelligence, pages
770–776, Menlo Park, CA. AAAI.
K. Pastra and Y. Aloimonos. 2012. The mini-
malist grammar of action. Philosophical Transac-
tions of the Royal Society B: Biological Sciences,
367(1585):103–117.
Hamed Pirsiavash, Carl Vondrick, and Antonio Tor-
ralba. 2014. Inferring the why in images. arXiv
preprint arXiv:1406.5472.
Giacomo Rizzolatti, Leonardo Fogassi, and Vittorio
Gallese. 2001. Neurophysiological mechanisms un-
derlying the understanding and imitation of action.
Nature Reviews Neuroscience, 2(9):661–670.
Michael S Ryoo and Jake K Aggarwal. 2006. Recogni-
tion of composite human activities through context-
free grammar based representation. In Proceedings
of the 2006 IEEE Conference on Computer Vision
and Pattern Recognition, volume 2, pages 1709–
1718, New York City, NY. IEEE.
</reference>
<page confidence="0.985224">
685
</page>
<reference confidence="0.999916727272727">
P. Saisan, G. Doretto, Y.N. Wu, and S. Soatto. 2001.
Dynamic texture recognition. In Proceedings of the
2001 IEEE Intenational Conference on Computer
Vision and Pattern Recognition, volume 2, pages
58–63, Kauai, HI. IEEE.
Mark Steedman. 2000. The syntactic process, vol-
ume 35. MIT Press.
Mark Steedman. 2002. Plans, affordances, and combi-
natory grammar. Linguistics and Philosophy, 25(5-
6):723–753.
D. Summers-Stay, C.L. Teo, Y. Yang, C. Ferm¨uller,
and Y. Aloimonos. 2013. Using a minimal ac-
tion grammar for activity understanding in the real
world. In Proceedings of the 2013 IEEE/RSJ Inter-
national Conference on Intelligent Robots and Sys-
tems, pages 4104–4111, Vilamoura, Portugal. IEEE.
Stefanie Tellex, Pratiksha Thaker, Joshua Joseph, and
Nicholas Roy. 2014. Learning perceptually
grounded word meanings from unaligned parallel
data. Machine Learning, 94(2):151–167.
P. Turaga, R. Chellappa, V.S. Subrahmanian, and
O. Udrea. 2008. Machine recognition of human ac-
tivities: A survey. IEEE Transactions on Circuits
and Systems for Video Technology, 18(11):1473–
1488.
Dan Xie, Sinisa Todorovic, and Song-Chun Zhu. 2013.
Inferring “dark matter” and “dark energy” from
videos. In Computer Vision (ICCV), 2013 IEEE In-
ternational Conference on, pages 2224–2231. IEEE.
Yezhou Yang, Cornelia Ferm¨uller, and Yiannis Aloi-
monos. 2013. Detection of manipulation action
consequences (MAC). In Proceedings of the 2013
IEEE Conference on Computer Vision and Pattern
Recognition, pages 2563–2570, Portland, OR. IEEE.
Y. Yang, A. Guha, C. Fermuller, and Y. Aloimonos.
2014. A cognitive system for understanding hu-
man manipulation actions. Advances in Cognitive
Sysytems, 3:67–86.
Yezhou Yang, Yi Li, Cornelia Fermuller, and Yiannis
Aloimonos. 2015. Robot learning manipulation ac-
tion plans by “watching” unconstrained videos from
the world wide web. In The Twenty-Ninth AAAI
Conference on Artificial Intelligence (AAAI-15).
A. Yilmaz and M. Shah. 2005. Actions sketch: A
novel action representation. In Proceedings of the
2005 IEEE Intenational Conference on Computer
Vision and Pattern Recognition, volume 1, pages
984–989, San Diego, CA. IEEE.
Luke S Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In UAI.
Luke S Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed ccg grammars for parsing to
logical form. In EMNLP-CoNLL, pages 678–687.
</reference>
<page confidence="0.998745">
686
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.491907">
<title confidence="0.999632">Learning the Semantics of Manipulation Action</title>
<author confidence="0.858238">Erdal</author>
<affiliation confidence="0.856901">University of Maryland, College Park, MD, yiannis, Institute of Technology, Karlsruhe,</affiliation>
<email confidence="0.999914">eren.aksoy@kit.edu</email>
<abstract confidence="0.99875862962963">In this paper we present a formal computational framework for modeling manipulation actions. The introduced formalism leads to semantics of manipulation action and has applications to both observing and understanding human manipulation actions as well as executing them with a robotic mechanism (e.g. a humanoid robot). It is based on a Combinatory Categorial Grammar. The goal of the introduced framework is to: (1) represent manipulation actions with both syntax and semantic parts, where the semantic part em- (2) enable a probabilistic semantic parsing schema to learn the representation of manipulation action from an annotated action corpus of videos; (3) use (1) and (2) to develop a system that visually observes manipulation actions and understands their meaning while it can reason beyond observations using propositional logic and axiom schemata. The experiments conducted on a public available large manipulation action dataset validate the theoretical framework and our implementation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E E Aksoy</author>
<author>A Abramov</author>
<author>J D¨orr</author>
<author>K Ning</author>
<author>B Dellen</author>
<author>F W¨org¨otter</author>
</authors>
<title>Learning the semantics of object–action relations by observation.</title>
<date>2011</date>
<journal>The International Journal of Robotics Research,</journal>
<volume>30</volume>
<issue>10</issue>
<pages>1249</pages>
<marker>Aksoy, Abramov, D¨orr, Ning, Dellen, W¨org¨otter, 2011</marker>
<rawString>E.E. Aksoy, A. Abramov, J. D¨orr, K. Ning, B. Dellen, and F. W¨org¨otter. 2011. Learning the semantics of object–action relations by observation. The International Journal of Robotics Research, 30(10):1229– 1249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E E Aksoy</author>
<author>M Tamosiunaite</author>
<author>F W¨org¨otter</author>
</authors>
<title>Model-free incremental learning of the semantics of manipulation actions. Robotics and Autonomous Systems,</title>
<date>2014</date>
<pages>1--42</pages>
<marker>Aksoy, Tamosiunaite, W¨org¨otter, 2014</marker>
<rawString>E E. Aksoy, M. Tamosiunaite, and F. W¨org¨otter. 2014. Model-free incremental learning of the semantics of manipulation actions. Robotics and Autonomous Systems, pages 1–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chitta Baral</author>
<author>Juraj Dzifcak</author>
<author>Marcos Alvarez Gonzalez</author>
<author>Jiayu Zhou</author>
</authors>
<title>Using inverse λ and generalization to translate english to formal languages.</title>
<date>2011</date>
<booktitle>In Proceedings of the Ninth International Conference on Computational Semantics,</booktitle>
<pages>35--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="20650" citStr="Baral et al., 2011" startWordPosition="3269" endWordPosition="3272"> T|M; O) = E(L,T) ef(L,T,M)·Θ (4) where f is a mapping of the triple (L, T, M) to feature vectors E Rd, and the O E Rd represents the weights to be learned. Here we use only lexical features, where each feature counts the number of times a lexical entry is used in T. Parsing a manipulation task under PCCG equates to finding L such that P(L|M; O) is maximized: argmaxLP(L|M; O) �= argmaxL P(L,T|M;O). (5) T AP\NP λx.cut(x, cucumber) → divided(cucumber) 680 We use dynamic programming techniques to calculate the most probable parse for the manipulation task. In this paper, the implementation from (Baral et al., 2011) is adopted, where an inverse-λ technique is used to generalize new semantic representations. The generalization of lexicon rules are essential for our system to deal with unknown actions presented during the testing phase. 5 Experiments 5.1 Manipulation Action (MANIAC) Dataset (Aksoy et al., 2014) provides a manipulation action dataset with 8 different manipulation actions (cutting, chopping, stirring, putting, taking, hiding, uncovering, and pushing), each of which consists of 15 different versions performed by 5 different human actors1. There are in total 30 different objects manipulated in</context>
<context position="24377" citStr="Baral et al., 2011" startWordPosition="3841" endWordPosition="3844">) box uncover apple uncover(box, apple) — appear(apple) ∧moved(box) Table 1: Example annotations from training corpus, one per manipulation action category. We also assume the system knows that every “object” involved in the corpus is an entity of its own type, for example: Knife := N : knife Bowl := N : bowl Additionally, we assume the syntactic form of each “action” has a main type (AP\NP)/NP (see Sec. 3.2). These two sets of rules form the initial seed lexicon for learning. 5.3 Learned Lexicon We applied the learning technique mentioned in Sec. 4, and we used the NL2KR implementation from (Baral et al., 2011). The system learns and generalizes a set of lexicon entries (syntactic and semantic) for each action categories from the training corpus accompanied with a set of weights. 681 We list the one with the largest weight for each action here respectively: Chopping :=(AP\NP)/NP : Ax.Ay.chopping(x, y) → divided(y) Cutting :=(AP\NP)/NP : Ax.Ay.cutting(x, y) → divided(y) Stirring :=(AP\NP)/NP : Ax.Ay.stirring(x, y) Take down :=(AP\NP)/NP : Ax.Ay.take down(x, y) → ¬connected(x, y) n moved(x) Put on top :=(AP\NP)/NP : Ax.Ay.put on top(x, y) → on top(x, y) n moved(x) Hiding :=(AP\NP)/NP : Ax.Ay.hiding(x,</context>
<context position="27498" citStr="Baral et al., 2011" startWordPosition="4353" endWordPosition="4356">is adopted and thus “subject” and “patient” fields are filled with segment IDs instead of a specific object name. Fig. 3 and 4 show several examples of the detected triplets accompanied with a set of key frames from the testing sequences. Nevertheless, the method we used here can 1) generalize the unknown segments into the category of object entities and 2) generalize the unknown actions (those that do not exist in the training corpus) into the category of action function. This is done by automatically generalizing the following two types of lexicon entries using the inverse-A technique from (Baral et al., 2011): Object [ID] :=N : object [ID] Unknown :=(AP\NP)/NP : Ax.Ay.unknown(x, y) Among the 90 detected triplets, using the learned lexicon we are able to parse all of them into semantic representations. Here we pick the representation with the highest probability after parsing as the individual action semantic representation. The “parsed semantics” rows of Fig. 3 and 4 show several example action semantics on testing sequences. Taking the fourth sub-action from Fig. 4 as an example, the visually detected triplets based on segmentation and spatial decomposition is (Object 014, Chopping, Object 011). </context>
</contexts>
<marker>Baral, Dzifcak, Gonzalez, Zhou, 2011</marker>
<rawString>Chitta Baral, Juraj Dzifcak, Marcos Alvarez Gonzalez, and Jiayu Zhou. 2011. Using inverse λ and generalization to translate english to formal languages. In Proceedings of the Ninth International Conference on Computational Semantics, pages 35–44. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jezekiel Ben-Arie</author>
<author>Zhiqian Wang</author>
<author>Purvin Pandit</author>
<author>Shyamsundar Rajaram</author>
</authors>
<title>Human activity recognition using multidimensional indexing. Pattern Analysis and Machine Intelligence,</title>
<date>2002</date>
<journal>IEEE Transactions on,</journal>
<volume>24</volume>
<issue>8</issue>
<contexts>
<context position="9100" citStr="Ben-Arie et al., 2002" startWordPosition="1427" endWordPosition="1430">oning. Action Recognition and Understanding: Human activity recognition and understanding has been studied heavily in Computer Vision recently, and there is a large range of applications for this work in areas like human-computer interactions, biometrics, and video surveillance. Both visual recognition methods, and the non-visual description methods using motion capture systems have been used. A few good surveys of the former can be found in (Moeslund et al., 2006) and (Turaga et al., 2008). Most of the focus has been on recognizing single human actions like walking, jumping, or running etc. (Ben-Arie et al., 2002; Yilmaz and Shah, 2005). Approaches to more complex actions have employed parametric approaches, such as HMMs (Kale et al., 2004) to learn the transition between feature representations in individual frames e.g. (Saisan et al., 2001; Chaudhry et al., 2009). More recently, (Aksoy et al., 2011; Aksoy et al., 2014) proposed a semantic event chain (SEC) representation to model and learn the semantic segment-wise relationship transition from spatial-temporal video segmentation. There also have been many syntactic approaches to human activity recognition which used the concept of context-free gramm</context>
</contexts>
<marker>Ben-Arie, Wang, Pandit, Rajaram, 2002</marker>
<rawString>Jezekiel Ben-Arie, Zhiqian Wang, Purvin Pandit, and Shyamsundar Rajaram. 2002. Human activity recognition using multidimensional indexing. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 24(8):1091–1104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Brand</author>
</authors>
<title>Understanding manipulation in video.</title>
<date>1996</date>
<booktitle>In Proceedings of the Second International Conference on Automatic Face and Gesture Recognition,</booktitle>
<pages>94--99</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="9842" citStr="Brand, 1996" startWordPosition="1541" endWordPosition="1542"> to learn the transition between feature representations in individual frames e.g. (Saisan et al., 2001; Chaudhry et al., 2009). More recently, (Aksoy et al., 2011; Aksoy et al., 2014) proposed a semantic event chain (SEC) representation to model and learn the semantic segment-wise relationship transition from spatial-temporal video segmentation. There also have been many syntactic approaches to human activity recognition which used the concept of context-free grammars, because such grammars provide a sound theoretical basis for modeling structured processes. Tracing back to the middle 90’s, (Brand, 1996) used a grammar to recognize disassembly tasks that contain hand manipulations. (Ryoo and Aggarwal, 2006) used the context-free grammar formalism to recognize composite human activities and multi-person interactions. It is a two level hierarchical approach where the lower-levels are composed of HMMs and Bayesian Networks while the higher-level interactions are modeled by CFGs. To deal with errors from low-level processes such as tracking, stochastic grammars such as stochastic CFGs were also used (Ivanov and Bobick, 2000; Moore and Essa, 2002). More recently, (Kuehne et al., 2014) proposed to </context>
</contexts>
<marker>Brand, 1996</marker>
<rawString>Matthew Brand. 1996. Understanding manipulation in video. In Proceedings of the Second International Conference on Automatic Face and Gesture Recognition, pages 94–99, Killington,VT. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Chaudhry</author>
<author>A Ravichandran</author>
<author>G Hager</author>
<author>R Vidal</author>
</authors>
<title>Histograms of oriented optical flow and binetcauchy kernels on nonlinear dynamical systems for the recognition of human actions.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 IEEE Intenational Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>pages</pages>
<publisher>IEEE.</publisher>
<location>Miami,FL.</location>
<contexts>
<context position="9357" citStr="Chaudhry et al., 2009" startWordPosition="1468" endWordPosition="1471">and video surveillance. Both visual recognition methods, and the non-visual description methods using motion capture systems have been used. A few good surveys of the former can be found in (Moeslund et al., 2006) and (Turaga et al., 2008). Most of the focus has been on recognizing single human actions like walking, jumping, or running etc. (Ben-Arie et al., 2002; Yilmaz and Shah, 2005). Approaches to more complex actions have employed parametric approaches, such as HMMs (Kale et al., 2004) to learn the transition between feature representations in individual frames e.g. (Saisan et al., 2001; Chaudhry et al., 2009). More recently, (Aksoy et al., 2011; Aksoy et al., 2014) proposed a semantic event chain (SEC) representation to model and learn the semantic segment-wise relationship transition from spatial-temporal video segmentation. There also have been many syntactic approaches to human activity recognition which used the concept of context-free grammars, because such grammars provide a sound theoretical basis for modeling structured processes. Tracing back to the middle 90’s, (Brand, 1996) used a grammar to recognize disassembly tasks that contain hand manipulations. (Ryoo and Aggarwal, 2006) used the </context>
</contexts>
<marker>Chaudhry, Ravichandran, Hager, Vidal, 2009</marker>
<rawString>R. Chaudhry, A. Ravichandran, G. Hager, and R. Vidal. 2009. Histograms of oriented optical flow and binetcauchy kernels on nonlinear dynamical systems for the recognition of human actions. In Proceedings of the 2009 IEEE Intenational Conference on Computer Vision and Pattern Recognition, pages 1932– 1939, Miami,FL. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Syntactic Structures. Mouton de Gruyter.</title>
<date>1957</date>
<contexts>
<context position="5109" citStr="Chomsky, 1957" startWordPosition="802" endWordPosition="803">is attached following the syntactic category explicitly. Combinatory Categorial Grammar (CCG) introduced by (Steedman, 2000) is a theory that can be used to represent such structures with a small set of combinators such as functional application and type-raising. What do we gain though from such a formal description of action? This is similar to asking what one gains from a formal description of language as a generative system. Chomskys contribution to language research was exactly this: the formal description of language through the formulation of the Generative and Transformational Grammar (Chomsky, 1957). It revolutionized language research opening up new roads for the computational analysis of language, providing researchers with common, generative language structures and syntactic operations, on which language analysis tools were built. A grammar for action would contribute to providing a common framework of the syntax and semantics of action, so that basic tools for action understanding can be built, tools that researchers can use when developing action interpretation systems, without having to start development from scratch. The same tools can be used by robots to execute actions. In this</context>
</contexts>
<marker>Chomsky, 1957</marker>
<rawString>N. Chomsky. 1957. Syntactic Structures. Mouton de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Lectures on government and binding: The Pisa lectures. Walter de Gruyter.</title>
<date>1993</date>
<contexts>
<context position="11243" citStr="Chomsky, 1993" startWordPosition="1751" endWordPosition="1752">recognition systems, and shed insight onto human manipulation action understanding. However, as mentioned, thinking about manipulation actions solely from the viewpoint of recognition has obvious limitations. In this work we adopt principles from CFG based activity recognition systems, with extensions to a CCG grammar that accommodates not only the hierarchical structure of human activity but also action semantics representations. It enables the system to serve as the core parsing engine for both manipulation action recognition and execution. Manipulation Action Grammar: As mentioned before, (Chomsky, 1993) suggested that a minimalist generative grammar, similar to the one of human language, also exists for action understanding and execution. The works closest related to this paper are (Pastra and Aloimonos, 2012; Summers-Stay et al., 2013; Guha et al., 2013). (Pastra and Aloimonos, 2012) first discussed a Chomskyan grammar for understanding complex actions as a theoretical concept, and (SummersStay et al., 2013) provided an implementation of such a grammar using as perceptual input only objects. More recently, (Yang et al., 2014) proposed a set of context-free grammar rules for manipulation act</context>
</contexts>
<marker>Chomsky, 1993</marker>
<rawString>Noam Chomsky. 1993. Lectures on government and binding: The Pisa lectures. Walter de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Gazzola</author>
<author>G Rizzolatti</author>
<author>B Wicker</author>
<author>C Keysers</author>
</authors>
<title>The anthropomorphic brain: the mirror neuron system responds to human and robotic actions.</title>
<date>2007</date>
<journal>Neuroimage,</journal>
<volume>35</volume>
<issue>4</issue>
<contexts>
<context position="3232" citStr="Gazzola et al., 2007" startWordPosition="501" endWordPosition="504"> Thus the intelligent system can not only imitate human movements, but also imitate action goals. Understanding actions by observation and executing them are generally considered as dual problems for intelligent agents. The sensori-motor bridge connecting the two tasks is essential, and a great amount of attention in AI, Robotics as well as Neurophysiology has been devoted to investigating it. Experiments conducted on primates have discovered that certain neurons, the so-called mirror neurons, fire during both observation and execution of identical manipulation tasks (Rizzolatti et al., 2001; Gazzola et al., 2007). This suggests that the same process is involved in both the observation and execution of actions. From a functionalist point of view, such a process should be able to first build up a semantic structure from observations, and then the decomposition of that same structure should occur when the intelligent agent executes commands. Additionally, studies in linguistics (Steedman, 2002) suggest that the language faculty develops in humans as a direct adaptation of a more primitive apparatus for planning goal-directed action in the world by composing affordances of tools and consequences of action</context>
</contexts>
<marker>Gazzola, Rizzolatti, Wicker, Keysers, 2007</marker>
<rawString>V Gazzola, G Rizzolatti, B Wicker, and C Keysers. 2007. The anthropomorphic brain: the mirror neuron system responds to human and robotic actions. Neuroimage, 35(4):1674–1684.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anupam Guha</author>
<author>Yezhou Yang</author>
<author>Cornelia Ferm¨uller</author>
<author>Yiannis Aloimonos</author>
</authors>
<title>Minimalist plans for interpreting manipulation actions.</title>
<date>2013</date>
<booktitle>Proceedings of the 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems,</booktitle>
<pages>5908--5914</pages>
<marker>Guha, Yang, Ferm¨uller, Aloimonos, 2013</marker>
<rawString>Anupam Guha, Yezhou Yang, Cornelia Ferm¨uller, and Yiannis Aloimonos. 2013. Minimalist plans for interpreting manipulation actions. Proceedings of the 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5908–5914.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuri A Ivanov</author>
<author>Aaron F Bobick</author>
</authors>
<title>Recognition of visual activities and interactions by stochastic parsing.</title>
<date>2000</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>22</volume>
<issue>8</issue>
<contexts>
<context position="10368" citStr="Ivanov and Bobick, 2000" startWordPosition="1617" endWordPosition="1620">eoretical basis for modeling structured processes. Tracing back to the middle 90’s, (Brand, 1996) used a grammar to recognize disassembly tasks that contain hand manipulations. (Ryoo and Aggarwal, 2006) used the context-free grammar formalism to recognize composite human activities and multi-person interactions. It is a two level hierarchical approach where the lower-levels are composed of HMMs and Bayesian Networks while the higher-level interactions are modeled by CFGs. To deal with errors from low-level processes such as tracking, stochastic grammars such as stochastic CFGs were also used (Ivanov and Bobick, 2000; Moore and Essa, 2002). More recently, (Kuehne et al., 2014) proposed to model goal-directed human activities using Hidden Markov Models and treat subactions just like words in speech. These works proved that grammar based approaches are practical in activity recognition systems, and shed insight onto human manipulation action understanding. However, as mentioned, thinking about manipulation actions solely from the viewpoint of recognition has obvious limitations. In this work we adopt principles from CFG based activity recognition systems, with extensions to a CCG grammar that accommodates n</context>
</contexts>
<marker>Ivanov, Bobick, 2000</marker>
<rawString>Yuri A. Ivanov and Aaron F. Bobick. 2000. Recognition of visual activities and interactions by stochastic parsing. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8):852–872.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kale</author>
<author>A Sundaresan</author>
<author>AN Rajagopalan</author>
<author>N P Cuntoor</author>
<author>A K Roy-Chowdhury</author>
<author>V Kruger</author>
<author>R Chellappa</author>
</authors>
<title>Identification of humans using gait.</title>
<date>2004</date>
<journal>IEEE Transactions on Image Processing,</journal>
<volume>13</volume>
<issue>9</issue>
<contexts>
<context position="9230" citStr="Kale et al., 2004" startWordPosition="1448" endWordPosition="1451"> recently, and there is a large range of applications for this work in areas like human-computer interactions, biometrics, and video surveillance. Both visual recognition methods, and the non-visual description methods using motion capture systems have been used. A few good surveys of the former can be found in (Moeslund et al., 2006) and (Turaga et al., 2008). Most of the focus has been on recognizing single human actions like walking, jumping, or running etc. (Ben-Arie et al., 2002; Yilmaz and Shah, 2005). Approaches to more complex actions have employed parametric approaches, such as HMMs (Kale et al., 2004) to learn the transition between feature representations in individual frames e.g. (Saisan et al., 2001; Chaudhry et al., 2009). More recently, (Aksoy et al., 2011; Aksoy et al., 2014) proposed a semantic event chain (SEC) representation to model and learn the semantic segment-wise relationship transition from spatial-temporal video segmentation. There also have been many syntactic approaches to human activity recognition which used the concept of context-free grammars, because such grammars provide a sound theoretical basis for modeling structured processes. Tracing back to the middle 90’s, (</context>
</contexts>
<marker>Kale, Sundaresan, Rajagopalan, Cuntoor, Roy-Chowdhury, Kruger, Chellappa, 2004</marker>
<rawString>A. Kale, A. Sundaresan, AN Rajagopalan, N.P. Cuntoor, A.K. Roy-Chowdhury, V. Kruger, and R. Chellappa. 2004. Identification of humans using gait. IEEE Transactions on Image Processing, 13(9):1163–1173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hilde Kuehne</author>
<author>Ali Arslan</author>
<author>Thomas Serre</author>
</authors>
<title>The language of actions: Recovering the syntax and semantics of goal-directed human activities.</title>
<date>2014</date>
<booktitle>In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on,</booktitle>
<pages>780--787</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="10429" citStr="Kuehne et al., 2014" startWordPosition="1627" endWordPosition="1630">to the middle 90’s, (Brand, 1996) used a grammar to recognize disassembly tasks that contain hand manipulations. (Ryoo and Aggarwal, 2006) used the context-free grammar formalism to recognize composite human activities and multi-person interactions. It is a two level hierarchical approach where the lower-levels are composed of HMMs and Bayesian Networks while the higher-level interactions are modeled by CFGs. To deal with errors from low-level processes such as tracking, stochastic grammars such as stochastic CFGs were also used (Ivanov and Bobick, 2000; Moore and Essa, 2002). More recently, (Kuehne et al., 2014) proposed to model goal-directed human activities using Hidden Markov Models and treat subactions just like words in speech. These works proved that grammar based approaches are practical in activity recognition systems, and shed insight onto human manipulation action understanding. However, as mentioned, thinking about manipulation actions solely from the viewpoint of recognition has obvious limitations. In this work we adopt principles from CFG based activity recognition systems, with extensions to a CCG grammar that accommodates not only the hierarchical structure of human activity but also</context>
</contexts>
<marker>Kuehne, Arslan, Serre, 2014</marker>
<rawString>Hilde Kuehne, Ali Arslan, and Thomas Serre. 2014. The language of actions: Recovering the syntax and semantics of goal-directed human activities. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 780–787. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Matuszek</author>
<author>Nicholas FitzGerald</author>
<author>Luke Zettlemoyer</author>
<author>Liefeng Bo</author>
<author>Dieter Fox</author>
</authors>
<title>A joint model of language and perception for grounded attribute learning.</title>
<date>2011</date>
<booktitle>In International Conference on Machine learning (ICML).</booktitle>
<contexts>
<context position="12708" citStr="Matuszek et al., 2011" startWordPosition="1968" endWordPosition="1971">its the capability of doing reasoning and prediction. Combinatory Categorial Grammar and Semantic Parsing: CCG based semantic parsing originally was used mainly to translate natural language sentences to their desired semantic representations as λ-calculus formulas (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). (Mooney, 2008) presented a framework of grounded language acquisition: the interpretation of language entities into semantically informed structures in the context of perception and actuation. The concept has been applied successfully in tasks such as robot navigation (Matuszek et al., 2011), forklift operation (Tellex et al., 2014) and of human-robot interaction (Matuszek et al., 2014). In this work, instead of grounding natural language sentences directly, we ground information obtained from visual perception into seman678 tically informed structures, specifically in the domain of manipulation actions. 3 A CCG Framework for Manipulation Actions Before we dive into the semantic parsing of manipulation actions, a brief introduction to the Combinatory Categorial Grammar framework in Linguistics is necessary. We will only introduce related concepts and formalisms. For a complete ba</context>
</contexts>
<marker>Matuszek, FitzGerald, Zettlemoyer, Bo, Fox, 2011</marker>
<rawString>Cynthia Matuszek, Nicholas FitzGerald, Luke Zettlemoyer, Liefeng Bo, and Dieter Fox. 2011. A joint model of language and perception for grounded attribute learning. In International Conference on Machine learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Matuszek</author>
<author>Liefeng Bo</author>
<author>Luke Zettlemoyer</author>
<author>Dieter Fox</author>
</authors>
<title>Learning from unscripted deictic gesture and language for human-robot interactions.</title>
<date>2014</date>
<booktitle>In Twenty-Eighth AAAI Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="12805" citStr="Matuszek et al., 2014" startWordPosition="1982" endWordPosition="1985"> Parsing: CCG based semantic parsing originally was used mainly to translate natural language sentences to their desired semantic representations as λ-calculus formulas (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). (Mooney, 2008) presented a framework of grounded language acquisition: the interpretation of language entities into semantically informed structures in the context of perception and actuation. The concept has been applied successfully in tasks such as robot navigation (Matuszek et al., 2011), forklift operation (Tellex et al., 2014) and of human-robot interaction (Matuszek et al., 2014). In this work, instead of grounding natural language sentences directly, we ground information obtained from visual perception into seman678 tically informed structures, specifically in the domain of manipulation actions. 3 A CCG Framework for Manipulation Actions Before we dive into the semantic parsing of manipulation actions, a brief introduction to the Combinatory Categorial Grammar framework in Linguistics is necessary. We will only introduce related concepts and formalisms. For a complete background reading, we would like to refer readers to (Steedman, 2000). We will first give a brief </context>
</contexts>
<marker>Matuszek, Bo, Zettlemoyer, Fox, 2014</marker>
<rawString>Cynthia Matuszek, Liefeng Bo, Luke Zettlemoyer, and Dieter Fox. 2014. Learning from unscripted deictic gesture and language for human-robot interactions. In Twenty-Eighth AAAI Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T B Moeslund</author>
<author>A Hilton</author>
<author>V Kr¨uger</author>
</authors>
<title>A survey of advances in vision-based human motion capture and analysis. Computer vision and image understanding,</title>
<date>2006</date>
<pages>104--2</pages>
<marker>Moeslund, Hilton, Kr¨uger, 2006</marker>
<rawString>T.B. Moeslund, A. Hilton, and V. Kr¨uger. 2006. A survey of advances in vision-based human motion capture and analysis. Computer vision and image understanding, 104(2):90–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to connect language and perception.</title>
<date>2008</date>
<booktitle>In AAAI,</booktitle>
<pages>1598--1601</pages>
<contexts>
<context position="12430" citStr="Mooney, 2008" startWordPosition="1928" endWordPosition="1929">rules for manipulation action understanding, and (Yang et al., 2015) applied it on unconstrained instructional videos. However, these approaches only consider the syntactic structure of manipulation actions without coupling semantic rules using λ expressions, which limits the capability of doing reasoning and prediction. Combinatory Categorial Grammar and Semantic Parsing: CCG based semantic parsing originally was used mainly to translate natural language sentences to their desired semantic representations as λ-calculus formulas (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). (Mooney, 2008) presented a framework of grounded language acquisition: the interpretation of language entities into semantically informed structures in the context of perception and actuation. The concept has been applied successfully in tasks such as robot navigation (Matuszek et al., 2011), forklift operation (Tellex et al., 2014) and of human-robot interaction (Matuszek et al., 2014). In this work, instead of grounding natural language sentences directly, we ground information obtained from visual perception into seman678 tically informed structures, specifically in the domain of manipulation actions. 3 </context>
</contexts>
<marker>Mooney, 2008</marker>
<rawString>Raymond J Mooney. 2008. Learning to connect language and perception. In AAAI, pages 1598–1601.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Darnell Moore</author>
<author>Irfan Essa</author>
</authors>
<title>Recognizing multitasked activities from video using stochastic context-free grammar.</title>
<date>2002</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence,</booktitle>
<pages>770--776</pages>
<publisher>AAAI.</publisher>
<location>Menlo Park, CA.</location>
<contexts>
<context position="10391" citStr="Moore and Essa, 2002" startWordPosition="1621" endWordPosition="1624">ing structured processes. Tracing back to the middle 90’s, (Brand, 1996) used a grammar to recognize disassembly tasks that contain hand manipulations. (Ryoo and Aggarwal, 2006) used the context-free grammar formalism to recognize composite human activities and multi-person interactions. It is a two level hierarchical approach where the lower-levels are composed of HMMs and Bayesian Networks while the higher-level interactions are modeled by CFGs. To deal with errors from low-level processes such as tracking, stochastic grammars such as stochastic CFGs were also used (Ivanov and Bobick, 2000; Moore and Essa, 2002). More recently, (Kuehne et al., 2014) proposed to model goal-directed human activities using Hidden Markov Models and treat subactions just like words in speech. These works proved that grammar based approaches are practical in activity recognition systems, and shed insight onto human manipulation action understanding. However, as mentioned, thinking about manipulation actions solely from the viewpoint of recognition has obvious limitations. In this work we adopt principles from CFG based activity recognition systems, with extensions to a CCG grammar that accommodates not only the hierarchica</context>
</contexts>
<marker>Moore, Essa, 2002</marker>
<rawString>Darnell Moore and Irfan Essa. 2002. Recognizing multitasked activities from video using stochastic context-free grammar. In Proceedings of the National Conference on Artificial Intelligence, pages 770–776, Menlo Park, CA. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Pastra</author>
<author>Y Aloimonos</author>
</authors>
<title>The minimalist grammar of action.</title>
<date>2012</date>
<journal>Philosophical Transactions of the Royal Society B: Biological Sciences,</journal>
<volume>367</volume>
<issue>1585</issue>
<contexts>
<context position="11453" citStr="Pastra and Aloimonos, 2012" startWordPosition="1782" endWordPosition="1785">mitations. In this work we adopt principles from CFG based activity recognition systems, with extensions to a CCG grammar that accommodates not only the hierarchical structure of human activity but also action semantics representations. It enables the system to serve as the core parsing engine for both manipulation action recognition and execution. Manipulation Action Grammar: As mentioned before, (Chomsky, 1993) suggested that a minimalist generative grammar, similar to the one of human language, also exists for action understanding and execution. The works closest related to this paper are (Pastra and Aloimonos, 2012; Summers-Stay et al., 2013; Guha et al., 2013). (Pastra and Aloimonos, 2012) first discussed a Chomskyan grammar for understanding complex actions as a theoretical concept, and (SummersStay et al., 2013) provided an implementation of such a grammar using as perceptual input only objects. More recently, (Yang et al., 2014) proposed a set of context-free grammar rules for manipulation action understanding, and (Yang et al., 2015) applied it on unconstrained instructional videos. However, these approaches only consider the syntactic structure of manipulation actions without coupling semantic rul</context>
</contexts>
<marker>Pastra, Aloimonos, 2012</marker>
<rawString>K. Pastra and Y. Aloimonos. 2012. The minimalist grammar of action. Philosophical Transactions of the Royal Society B: Biological Sciences, 367(1585):103–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hamed Pirsiavash</author>
<author>Carl Vondrick</author>
<author>Antonio Torralba</author>
</authors>
<title>Inferring the why in images. arXiv preprint arXiv:1406.5472.</title>
<date>2014</date>
<contexts>
<context position="8081" citStr="Pirsiavash et al., 2014" startWordPosition="1267" endWordPosition="1270">mputer vision, which aim to reason beyond appearance models, are also related to this paper. (Xie et al., 2013) proposed that beyond state-of-the-art computer vision techniques, we could possibly infer implicit information (such as functional objects) from video, and they call them “Dark Matter” and “Dark Energy”. (Yang et al., 2013) used stochastic tracking and graphcut based segmentation to infer manipulation consequences beyond appearance. (Joo et al., 2014) used a ranking SVM to predict the persuasive motivation (or the intention) of the photographer who captured an image. More recently, (Pirsiavash et al., 2014) seeks to infer the motivation of the person in the image by mining knowledge stored in 677 a large corpus using natural language processing techniques. Different from these fairly general investigations about reasoning beyond appearance, our paper seeks to learn manipulation actions semantics in logic forms through CCG, and further infer hidden action consequences beyond appearance through reasoning. Action Recognition and Understanding: Human activity recognition and understanding has been studied heavily in Computer Vision recently, and there is a large range of applications for this work i</context>
</contexts>
<marker>Pirsiavash, Vondrick, Torralba, 2014</marker>
<rawString>Hamed Pirsiavash, Carl Vondrick, and Antonio Torralba. 2014. Inferring the why in images. arXiv preprint arXiv:1406.5472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giacomo Rizzolatti</author>
<author>Leonardo Fogassi</author>
<author>Vittorio Gallese</author>
</authors>
<title>Neurophysiological mechanisms underlying the understanding and imitation of action.</title>
<date>2001</date>
<journal>Nature Reviews Neuroscience,</journal>
<volume>2</volume>
<issue>9</issue>
<contexts>
<context position="3209" citStr="Rizzolatti et al., 2001" startWordPosition="497" endWordPosition="500"> beyond just observation. Thus the intelligent system can not only imitate human movements, but also imitate action goals. Understanding actions by observation and executing them are generally considered as dual problems for intelligent agents. The sensori-motor bridge connecting the two tasks is essential, and a great amount of attention in AI, Robotics as well as Neurophysiology has been devoted to investigating it. Experiments conducted on primates have discovered that certain neurons, the so-called mirror neurons, fire during both observation and execution of identical manipulation tasks (Rizzolatti et al., 2001; Gazzola et al., 2007). This suggests that the same process is involved in both the observation and execution of actions. From a functionalist point of view, such a process should be able to first build up a semantic structure from observations, and then the decomposition of that same structure should occur when the intelligent agent executes commands. Additionally, studies in linguistics (Steedman, 2002) suggest that the language faculty develops in humans as a direct adaptation of a more primitive apparatus for planning goal-directed action in the world by composing affordances of tools and</context>
</contexts>
<marker>Rizzolatti, Fogassi, Gallese, 2001</marker>
<rawString>Giacomo Rizzolatti, Leonardo Fogassi, and Vittorio Gallese. 2001. Neurophysiological mechanisms underlying the understanding and imitation of action. Nature Reviews Neuroscience, 2(9):661–670.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael S Ryoo</author>
<author>Jake K Aggarwal</author>
</authors>
<title>Recognition of composite human activities through contextfree grammar based representation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<volume>2</volume>
<pages>1709--1718</pages>
<publisher>IEEE.</publisher>
<location>New York City, NY.</location>
<contexts>
<context position="9947" citStr="Ryoo and Aggarwal, 2006" startWordPosition="1554" endWordPosition="1557"> al., 2001; Chaudhry et al., 2009). More recently, (Aksoy et al., 2011; Aksoy et al., 2014) proposed a semantic event chain (SEC) representation to model and learn the semantic segment-wise relationship transition from spatial-temporal video segmentation. There also have been many syntactic approaches to human activity recognition which used the concept of context-free grammars, because such grammars provide a sound theoretical basis for modeling structured processes. Tracing back to the middle 90’s, (Brand, 1996) used a grammar to recognize disassembly tasks that contain hand manipulations. (Ryoo and Aggarwal, 2006) used the context-free grammar formalism to recognize composite human activities and multi-person interactions. It is a two level hierarchical approach where the lower-levels are composed of HMMs and Bayesian Networks while the higher-level interactions are modeled by CFGs. To deal with errors from low-level processes such as tracking, stochastic grammars such as stochastic CFGs were also used (Ivanov and Bobick, 2000; Moore and Essa, 2002). More recently, (Kuehne et al., 2014) proposed to model goal-directed human activities using Hidden Markov Models and treat subactions just like words in s</context>
</contexts>
<marker>Ryoo, Aggarwal, 2006</marker>
<rawString>Michael S Ryoo and Jake K Aggarwal. 2006. Recognition of composite human activities through contextfree grammar based representation. In Proceedings of the 2006 IEEE Conference on Computer Vision and Pattern Recognition, volume 2, pages 1709– 1718, New York City, NY. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Saisan</author>
<author>G Doretto</author>
<author>Y N Wu</author>
<author>S Soatto</author>
</authors>
<title>Dynamic texture recognition.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 IEEE Intenational Conference on Computer Vision and Pattern Recognition,</booktitle>
<volume>2</volume>
<pages>58--63</pages>
<publisher>IEEE.</publisher>
<location>Kauai, HI.</location>
<contexts>
<context position="9333" citStr="Saisan et al., 2001" startWordPosition="1464" endWordPosition="1467">actions, biometrics, and video surveillance. Both visual recognition methods, and the non-visual description methods using motion capture systems have been used. A few good surveys of the former can be found in (Moeslund et al., 2006) and (Turaga et al., 2008). Most of the focus has been on recognizing single human actions like walking, jumping, or running etc. (Ben-Arie et al., 2002; Yilmaz and Shah, 2005). Approaches to more complex actions have employed parametric approaches, such as HMMs (Kale et al., 2004) to learn the transition between feature representations in individual frames e.g. (Saisan et al., 2001; Chaudhry et al., 2009). More recently, (Aksoy et al., 2011; Aksoy et al., 2014) proposed a semantic event chain (SEC) representation to model and learn the semantic segment-wise relationship transition from spatial-temporal video segmentation. There also have been many syntactic approaches to human activity recognition which used the concept of context-free grammars, because such grammars provide a sound theoretical basis for modeling structured processes. Tracing back to the middle 90’s, (Brand, 1996) used a grammar to recognize disassembly tasks that contain hand manipulations. (Ryoo and A</context>
</contexts>
<marker>Saisan, Doretto, Wu, Soatto, 2001</marker>
<rawString>P. Saisan, G. Doretto, Y.N. Wu, and S. Soatto. 2001. Dynamic texture recognition. In Proceedings of the 2001 IEEE Intenational Conference on Computer Vision and Pattern Recognition, volume 2, pages 58–63, Kauai, HI. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The syntactic process, volume 35.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4619" citStr="Steedman, 2000" startWordPosition="721" endWordPosition="722">al Language Processing, pages 676–686, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics apparatus that is our major interest in this paper. Such an apparatus is composed of a “syntax part” and a “semantic part”. In the syntax part, every linguistic element is categorized as either a function or a basic type, and is associated with a syntactic category which either identifies it as a function or a basic type. In the semantic part, a semantic translation is attached following the syntactic category explicitly. Combinatory Categorial Grammar (CCG) introduced by (Steedman, 2000) is a theory that can be used to represent such structures with a small set of combinators such as functional application and type-raising. What do we gain though from such a formal description of action? This is similar to asking what one gains from a formal description of language as a generative system. Chomskys contribution to language research was exactly this: the formal description of language through the formulation of the Generative and Transformational Grammar (Chomsky, 1957). It revolutionized language research opening up new roads for the computational analysis of language, providi</context>
<context position="13376" citStr="Steedman, 2000" startWordPosition="2073" endWordPosition="2074">an-robot interaction (Matuszek et al., 2014). In this work, instead of grounding natural language sentences directly, we ground information obtained from visual perception into seman678 tically informed structures, specifically in the domain of manipulation actions. 3 A CCG Framework for Manipulation Actions Before we dive into the semantic parsing of manipulation actions, a brief introduction to the Combinatory Categorial Grammar framework in Linguistics is necessary. We will only introduce related concepts and formalisms. For a complete background reading, we would like to refer readers to (Steedman, 2000). We will first give a brief introduction to CCG and then introduce a fundamental combinator, i.e., functional application. The introduction is followed by examples to show how the combinator is applied to parse actions. 3.1 Manipulation Action Semantics The semantic expression in our representation of manipulation actions uses a typed A-calculus language. The formal system has two basic types: entities and functions. Entities in manipulation actions are Objects or Hands, and functions are the Actions. Our lambda-calculus expressions are formed from the following items: Constants: Constants ca</context>
<context position="15526" citStr="Steedman, 2000" startWordPosition="2399" endWordPosition="2401">nected(spoon, bowl) represents the status after the spoon finished stirring the bowl. λx.cut(x, cucumber) → divided(cucumber) represents that if the cucumber is cut by x, then the status of the cucumber is divided. A expressions: lambda expressions represent functions with unknown arguments. For example, Ax.cut(knife, x) is a function from entities to entities, which is of type NP after any entities of type N that is cut by knife. 3.2 Combinatory Categorial Grammar The semantic parsing formalism underlying our framework for manipulation actions is that of combinatory categorial grammar (CCG) (Steedman, 2000). A CCG specifies one or more logical forms for each element or combination of elements for manipulation actions. In our formalism, an element of Action is associated with a syntactic “category” which identifies it as functions, and specifies the type and directionality of their arguments and the type of their result. For example, action “Cut” is a function from patient object phrase (NP) on the right into predicates, and into functions from subject object phrase (NP) on the left into a sub action phrase (AP): Cut := (AP\NP)/NP. (1) As a matter of fact, the pure categorial grammar is a conext-</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The syntactic process, volume 35. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Plans, affordances, and combinatory grammar.</title>
<date>2002</date>
<journal>Linguistics and Philosophy,</journal>
<pages>25--5</pages>
<contexts>
<context position="3618" citStr="Steedman, 2002" startWordPosition="565" endWordPosition="566"> it. Experiments conducted on primates have discovered that certain neurons, the so-called mirror neurons, fire during both observation and execution of identical manipulation tasks (Rizzolatti et al., 2001; Gazzola et al., 2007). This suggests that the same process is involved in both the observation and execution of actions. From a functionalist point of view, such a process should be able to first build up a semantic structure from observations, and then the decomposition of that same structure should occur when the intelligent agent executes commands. Additionally, studies in linguistics (Steedman, 2002) suggest that the language faculty develops in humans as a direct adaptation of a more primitive apparatus for planning goal-directed action in the world by composing affordances of tools and consequences of actions. It is this more primitive 676 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 676–686, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics apparatus that is our major interest in this paper. Such an apparatus is composed of a “synt</context>
<context position="34038" citStr="Steedman, 2002" startWordPosition="5455" endWordPosition="5456">ction is transitive or intransitive, the main types of action can be extended to include AP\NP. Moreover, the logical expressions can also be extended to include universal quantification b and existential quantification 1. Thus, manipulation action such as “knife cut every tomato” can be parsed into a representation as bx.tomato(x) ∧ cut(knife, x) —* divided(x) (the parse is given in the following chart). Here, the concept “every” has a main type of NP\NP and semantic meaning of bx.f(x). The same framework can also extended to have other combinatory rules such as composition and type-raising (Steedman, 2002). These are parts of the future work along the line of the presented work. Knife Cut every Tomato N N NP (AP\NP)/NP NP\NP NP knife Ax.Ay.cut(x, y) ∀x.f (x) tomato knife → divided(y) ∀x.f (x) tomato NP ∀x.tomato(x) AP ∀y.tomato(y) ∧ cut(knife, y) → divided(y) The presented computational linguistic framework enables an intelligent agent to predict and reason action goals from observation, and thus has many potential applications such as human intention prediction, robot action policy planning, human robot collaboration etc. We believe that our formalism of manipulation actions bridges computatio</context>
</contexts>
<marker>Steedman, 2002</marker>
<rawString>Mark Steedman. 2002. Plans, affordances, and combinatory grammar. Linguistics and Philosophy, 25(5-6):723–753.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Summers-Stay</author>
<author>C L Teo</author>
<author>Y Yang</author>
<author>C Ferm¨uller</author>
<author>Y Aloimonos</author>
</authors>
<title>Using a minimal action grammar for activity understanding in the real world.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems,</booktitle>
<pages>4104--4111</pages>
<publisher>IEEE.</publisher>
<location>Vilamoura, Portugal.</location>
<marker>Summers-Stay, Teo, Yang, Ferm¨uller, Aloimonos, 2013</marker>
<rawString>D. Summers-Stay, C.L. Teo, Y. Yang, C. Ferm¨uller, and Y. Aloimonos. 2013. Using a minimal action grammar for activity understanding in the real world. In Proceedings of the 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 4104–4111, Vilamoura, Portugal. IEEE.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Stefanie Tellex</author>
</authors>
<title>Pratiksha Thaker,</title>
<location>Joshua Joseph, and</location>
<marker>Tellex, </marker>
<rawString>Stefanie Tellex, Pratiksha Thaker, Joshua Joseph, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Roy</author>
</authors>
<title>Learning perceptually grounded word meanings from unaligned parallel data.</title>
<date>2014</date>
<booktitle>Machine Learning,</booktitle>
<volume>94</volume>
<issue>2</issue>
<marker>Roy, 2014</marker>
<rawString>Nicholas Roy. 2014. Learning perceptually grounded word meanings from unaligned parallel data. Machine Learning, 94(2):151–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turaga</author>
<author>R Chellappa</author>
<author>V S Subrahmanian</author>
<author>O Udrea</author>
</authors>
<title>Machine recognition of human activities: A survey.</title>
<date>2008</date>
<booktitle>IEEE Transactions on Circuits and Systems for Video Technology,</booktitle>
<volume>18</volume>
<issue>11</issue>
<pages>1488</pages>
<contexts>
<context position="8974" citStr="Turaga et al., 2008" startWordPosition="1404" endWordPosition="1407">ion actions semantics in logic forms through CCG, and further infer hidden action consequences beyond appearance through reasoning. Action Recognition and Understanding: Human activity recognition and understanding has been studied heavily in Computer Vision recently, and there is a large range of applications for this work in areas like human-computer interactions, biometrics, and video surveillance. Both visual recognition methods, and the non-visual description methods using motion capture systems have been used. A few good surveys of the former can be found in (Moeslund et al., 2006) and (Turaga et al., 2008). Most of the focus has been on recognizing single human actions like walking, jumping, or running etc. (Ben-Arie et al., 2002; Yilmaz and Shah, 2005). Approaches to more complex actions have employed parametric approaches, such as HMMs (Kale et al., 2004) to learn the transition between feature representations in individual frames e.g. (Saisan et al., 2001; Chaudhry et al., 2009). More recently, (Aksoy et al., 2011; Aksoy et al., 2014) proposed a semantic event chain (SEC) representation to model and learn the semantic segment-wise relationship transition from spatial-temporal video segmentat</context>
</contexts>
<marker>Turaga, Chellappa, Subrahmanian, Udrea, 2008</marker>
<rawString>P. Turaga, R. Chellappa, V.S. Subrahmanian, and O. Udrea. 2008. Machine recognition of human activities: A survey. IEEE Transactions on Circuits and Systems for Video Technology, 18(11):1473– 1488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Xie</author>
<author>Sinisa Todorovic</author>
<author>Song-Chun Zhu</author>
</authors>
<title>Inferring “dark matter” and “dark energy” from videos.</title>
<date>2013</date>
<booktitle>In Computer Vision (ICCV), 2013 IEEE International Conference on,</booktitle>
<pages>2224--2231</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="7568" citStr="Xie et al., 2013" startWordPosition="1189" endWordPosition="1192">(MANIAC) from (Aksoy et al., 2014), achieving promising experimental results. Moreover, we believe that our work, even though it only considers the domain of manipulation actions, is also a promising example of a more closely intertwined computer vision and computational linguistics system. The diagram in Fig.1 depicts the framework of the system. Figure 1: A CCG based semantic parsing framework for manipulation actions. 2 Related Works Reasoning beyond appearance: The very small number of works in computer vision, which aim to reason beyond appearance models, are also related to this paper. (Xie et al., 2013) proposed that beyond state-of-the-art computer vision techniques, we could possibly infer implicit information (such as functional objects) from video, and they call them “Dark Matter” and “Dark Energy”. (Yang et al., 2013) used stochastic tracking and graphcut based segmentation to infer manipulation consequences beyond appearance. (Joo et al., 2014) used a ranking SVM to predict the persuasive motivation (or the intention) of the photographer who captured an image. More recently, (Pirsiavash et al., 2014) seeks to infer the motivation of the person in the image by mining knowledge stored in</context>
</contexts>
<marker>Xie, Todorovic, Zhu, 2013</marker>
<rawString>Dan Xie, Sinisa Todorovic, and Song-Chun Zhu. 2013. Inferring “dark matter” and “dark energy” from videos. In Computer Vision (ICCV), 2013 IEEE International Conference on, pages 2224–2231. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yezhou Yang</author>
<author>Cornelia Ferm¨uller</author>
<author>Yiannis Aloimonos</author>
</authors>
<title>Detection of manipulation action consequences (MAC).</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>2563--2570</pages>
<publisher>IEEE.</publisher>
<location>Portland, OR.</location>
<marker>Yang, Ferm¨uller, Aloimonos, 2013</marker>
<rawString>Yezhou Yang, Cornelia Ferm¨uller, and Yiannis Aloimonos. 2013. Detection of manipulation action consequences (MAC). In Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition, pages 2563–2570, Portland, OR. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>A Guha</author>
<author>C Fermuller</author>
<author>Y Aloimonos</author>
</authors>
<title>A cognitive system for understanding human manipulation actions.</title>
<date>2014</date>
<booktitle>Advances in Cognitive Sysytems,</booktitle>
<pages>3--67</pages>
<contexts>
<context position="11777" citStr="Yang et al., 2014" startWordPosition="1832" endWordPosition="1835">ion and execution. Manipulation Action Grammar: As mentioned before, (Chomsky, 1993) suggested that a minimalist generative grammar, similar to the one of human language, also exists for action understanding and execution. The works closest related to this paper are (Pastra and Aloimonos, 2012; Summers-Stay et al., 2013; Guha et al., 2013). (Pastra and Aloimonos, 2012) first discussed a Chomskyan grammar for understanding complex actions as a theoretical concept, and (SummersStay et al., 2013) provided an implementation of such a grammar using as perceptual input only objects. More recently, (Yang et al., 2014) proposed a set of context-free grammar rules for manipulation action understanding, and (Yang et al., 2015) applied it on unconstrained instructional videos. However, these approaches only consider the syntactic structure of manipulation actions without coupling semantic rules using λ expressions, which limits the capability of doing reasoning and prediction. Combinatory Categorial Grammar and Semantic Parsing: CCG based semantic parsing originally was used mainly to translate natural language sentences to their desired semantic representations as λ-calculus formulas (Zettlemoyer and Collins,</context>
</contexts>
<marker>Yang, Guha, Fermuller, Aloimonos, 2014</marker>
<rawString>Y. Yang, A. Guha, C. Fermuller, and Y. Aloimonos. 2014. A cognitive system for understanding human manipulation actions. Advances in Cognitive Sysytems, 3:67–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yezhou Yang</author>
<author>Yi Li</author>
<author>Cornelia Fermuller</author>
<author>Yiannis Aloimonos</author>
</authors>
<title>Robot learning manipulation action plans by “watching” unconstrained videos from the world wide web.</title>
<date>2015</date>
<booktitle>In The Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15).</booktitle>
<contexts>
<context position="11885" citStr="Yang et al., 2015" startWordPosition="1850" endWordPosition="1853">list generative grammar, similar to the one of human language, also exists for action understanding and execution. The works closest related to this paper are (Pastra and Aloimonos, 2012; Summers-Stay et al., 2013; Guha et al., 2013). (Pastra and Aloimonos, 2012) first discussed a Chomskyan grammar for understanding complex actions as a theoretical concept, and (SummersStay et al., 2013) provided an implementation of such a grammar using as perceptual input only objects. More recently, (Yang et al., 2014) proposed a set of context-free grammar rules for manipulation action understanding, and (Yang et al., 2015) applied it on unconstrained instructional videos. However, these approaches only consider the syntactic structure of manipulation actions without coupling semantic rules using λ expressions, which limits the capability of doing reasoning and prediction. Combinatory Categorial Grammar and Semantic Parsing: CCG based semantic parsing originally was used mainly to translate natural language sentences to their desired semantic representations as λ-calculus formulas (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). (Mooney, 2008) presented a framework of grounded language acquisition</context>
</contexts>
<marker>Yang, Li, Fermuller, Aloimonos, 2015</marker>
<rawString>Yezhou Yang, Yi Li, Cornelia Fermuller, and Yiannis Aloimonos. 2015. Robot learning manipulation action plans by “watching” unconstrained videos from the world wide web. In The Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Yilmaz</author>
<author>M Shah</author>
</authors>
<title>Actions sketch: A novel action representation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 IEEE Intenational Conference on Computer Vision and Pattern Recognition,</booktitle>
<volume>1</volume>
<pages>984--989</pages>
<publisher>IEEE.</publisher>
<location>San Diego, CA.</location>
<contexts>
<context position="9124" citStr="Yilmaz and Shah, 2005" startWordPosition="1431" endWordPosition="1434">on and Understanding: Human activity recognition and understanding has been studied heavily in Computer Vision recently, and there is a large range of applications for this work in areas like human-computer interactions, biometrics, and video surveillance. Both visual recognition methods, and the non-visual description methods using motion capture systems have been used. A few good surveys of the former can be found in (Moeslund et al., 2006) and (Turaga et al., 2008). Most of the focus has been on recognizing single human actions like walking, jumping, or running etc. (Ben-Arie et al., 2002; Yilmaz and Shah, 2005). Approaches to more complex actions have employed parametric approaches, such as HMMs (Kale et al., 2004) to learn the transition between feature representations in individual frames e.g. (Saisan et al., 2001; Chaudhry et al., 2009). More recently, (Aksoy et al., 2011; Aksoy et al., 2014) proposed a semantic event chain (SEC) representation to model and learn the semantic segment-wise relationship transition from spatial-temporal video segmentation. There also have been many syntactic approaches to human activity recognition which used the concept of context-free grammars, because such gramma</context>
</contexts>
<marker>Yilmaz, Shah, 2005</marker>
<rawString>A. Yilmaz and M. Shah. 2005. Actions sketch: A novel action representation. In Proceedings of the 2005 IEEE Intenational Conference on Computer Vision and Pattern Recognition, volume 1, pages 984–989, San Diego, CA. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In UAI.</booktitle>
<contexts>
<context position="12382" citStr="Zettlemoyer and Collins, 2005" startWordPosition="1920" endWordPosition="1923">tly, (Yang et al., 2014) proposed a set of context-free grammar rules for manipulation action understanding, and (Yang et al., 2015) applied it on unconstrained instructional videos. However, these approaches only consider the syntactic structure of manipulation actions without coupling semantic rules using λ expressions, which limits the capability of doing reasoning and prediction. Combinatory Categorial Grammar and Semantic Parsing: CCG based semantic parsing originally was used mainly to translate natural language sentences to their desired semantic representations as λ-calculus formulas (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). (Mooney, 2008) presented a framework of grounded language acquisition: the interpretation of language entities into semantically informed structures in the context of perception and actuation. The concept has been applied successfully in tasks such as robot navigation (Matuszek et al., 2011), forklift operation (Tellex et al., 2014) and of human-robot interaction (Matuszek et al., 2014). In this work, instead of grounding natural language sentences directly, we ground information obtained from visual perception into seman678 tically informed structures, specif</context>
<context position="19188" citStr="Zettlemoyer and Collins, 2005" startWordPosition="3021" endWordPosition="3024">ation (“Knife Cut Cucumber”) into a semantic representation (cut(knife, cucumber) —* divided(cucumber)) using the functional application rules. Knife Cut Cucumber N N NP (AP\NP)/NP NP knife λx.λy.cut(x, y) cucumber knife → divided(y) cucumber &gt; AP cut(knife, cucumber) → divided(cucumber) 4 Learning Model and Semantic Parsing After having defined the formalism and application rule, instead of manually writing down all the possible CCG representations for each entity, we would like to apply a learning technique to derive them from the paired training corpus. Here we adopt the learning model of (Zettlemoyer and Collins, 2005), and use it to assign weights to the semantic representation of actions. Since an action may have multiple possible syntactic and semantic representations assigned to it, we use the probabilistic model to assign weights to these representations. 4.1 Learning Approach First we assume that complete syntactic parses of the observed action are available, and in fact a manipulation action can have several different parses. The parsing uses a probabilistic combinatorial categorial grammar framework similar to the one given by (Zettlemoyer and Collins, 2007). We assume a probabilistic categorial gra</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Online learning of relaxed ccg grammars for parsing to logical form. In EMNLP-CoNLL,</title>
<date>2007</date>
<pages>678--687</pages>
<contexts>
<context position="12414" citStr="Zettlemoyer and Collins, 2007" startWordPosition="1924" endWordPosition="1927">ed a set of context-free grammar rules for manipulation action understanding, and (Yang et al., 2015) applied it on unconstrained instructional videos. However, these approaches only consider the syntactic structure of manipulation actions without coupling semantic rules using λ expressions, which limits the capability of doing reasoning and prediction. Combinatory Categorial Grammar and Semantic Parsing: CCG based semantic parsing originally was used mainly to translate natural language sentences to their desired semantic representations as λ-calculus formulas (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). (Mooney, 2008) presented a framework of grounded language acquisition: the interpretation of language entities into semantically informed structures in the context of perception and actuation. The concept has been applied successfully in tasks such as robot navigation (Matuszek et al., 2011), forklift operation (Tellex et al., 2014) and of human-robot interaction (Matuszek et al., 2014). In this work, instead of grounding natural language sentences directly, we ground information obtained from visual perception into seman678 tically informed structures, specifically in the domain of manipula</context>
<context position="19746" citStr="Zettlemoyer and Collins, 2007" startWordPosition="3109" endWordPosition="3112">pus. Here we adopt the learning model of (Zettlemoyer and Collins, 2005), and use it to assign weights to the semantic representation of actions. Since an action may have multiple possible syntactic and semantic representations assigned to it, we use the probabilistic model to assign weights to these representations. 4.1 Learning Approach First we assume that complete syntactic parses of the observed action are available, and in fact a manipulation action can have several different parses. The parsing uses a probabilistic combinatorial categorial grammar framework similar to the one given by (Zettlemoyer and Collins, 2007). We assume a probabilistic categorial grammar (PCCG) based on a log linear model. M denotes a manipulation task, L denotes the semantic representation of the task, and T denotes its parse tree. The probability of a particular syntactic and semantic parse is given as: ef(L,T,M)·Θ P(L, T|M; O) = E(L,T) ef(L,T,M)·Θ (4) where f is a mapping of the triple (L, T, M) to feature vectors E Rd, and the O E Rd represents the weights to be learned. Here we use only lexical features, where each feature counts the number of times a lexical entry is used in T. Parsing a manipulation task under PCCG equates </context>
</contexts>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>Luke S Zettlemoyer and Michael Collins. 2007. Online learning of relaxed ccg grammars for parsing to logical form. In EMNLP-CoNLL, pages 678–687.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>