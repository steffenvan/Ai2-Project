<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000140">
<title confidence="0.905436">
Japanese Dialogue Corpus of Multi-Level Annotation
</title>
<note confidence="0.608216">
The Japanese Discourse Research Initiative
http://www.slp.cs.ritsumei.ac.jp/dtag/
</note>
<sectionHeader confidence="0.913494" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999868315789474">
This paper describes a Japanese
dialogue corpus annotated with
multi-level information built by the
Japanese Discourse Research Initia-
tive, Japanese Society for Artificial
Intelligence. The annotation in-
formation consists of speech, tran-
scription delimited by slash units,
prosodic, part of speech, dialogue
acts and dialogue segmentation. In
the project, we used the corpus for
obtaining new findings by examining
the relationship between linguistic
information and dialogue acts, that
between prosodic information and
dialogue segment, and the charac-
teristics of agreement/disagreement
expressions and non-sentence ele-
ments.
</bodyText>
<sectionHeader confidence="0.996305" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99994243902439">
This paper describes a Japanese dialogue cor-
pus annotated with multi-level information
such as speech, linguistic and discourse infor-
mation built by the Japanese Discourse Re-
search Initiative, supported by Japanese So-
ciety for Artificial Intelligence.
Dialogue corpora are now indispensable to
speech and language research communities.
The corpora have been used not only for ex-
amining the relationship between speech and
linguistic phenomena, but also for building
speech and language understanding systems.
Sharing corpora among researchers is most
desirable since creating the corpora needs
considerable cost like writing and revising an-
notation manuals, annotating the data, and
checking the consistency and reliability of the
annotated data. Discourse Research Initia-
tive was set up in March of 1996 by US, Eu-
ropean, and Japanese researchers to develop
standardized discourse annotation schemes
(Carletta et al., 1997; Core et al., 1998).
The efforts of the initiative have been called
&apos;standardization&apos;, but this naming is mislead-
ing at least. In typical standardizing ef-
forts, as done in audio-visual and telecom-
munication technologies, commercial compa-
nies try to expand the market for their prod-
ucts or interfaces by the standard. The ob-
jective of standardizing efforts in discourse is
to promote interactions among discourse re-
searchers and thereby provide a solid founda-
tion for corpus-based discourse research, dis-
pensing with duplicating resource making ef-
forts and increasing sharable resources.
In cooperation with this initiative,
Japanese Discourse Research Initiative has
started in Japan in May 1996, supported by
Japanese Society for Artificial Intelligence
(JDRI, 1996; Ichikawa et al., 1999). The
activities of the initiative involve:
</bodyText>
<listItem confidence="0.997281333333333">
• creating and revising annotation schemes
based on the survey of the existing
schemes and annotation experiments,
• annotating corpora based on the pro-
posed annotation schemes, and
• doing research using the corpora not only
</listItem>
<bodyText confidence="0.711597666666667">
for examining the utility of the schemes
and corpora but also for obtaining new
findings.
</bodyText>
<page confidence="0.896026">
1
</page>
<figure confidence="0.868862">
[Dialogue segment
</figure>
<figureCaption confidence="0.9426525">
Figure 1: The relations among the annotation
information
</figureCaption>
<bodyText confidence="0.999964125">
In the following, a Japanese dialogue
corpus of multi-level annotation is demon-
strated. The annotation schemes deal with
the information for speech, transcription seg-
mented by utterance units, called &apos;slash
units,&apos; prosody, part of speech, dialogue acts
and dialogue segment. Figure 1 shows the re-
lations among the annotation information.
</bodyText>
<sectionHeader confidence="0.520791" genericHeader="introduction">
2 Speech Sound and Transcription
</sectionHeader>
<bodyText confidence="0.999944">
The corpus consists of a collection of 14 task-
oriented dialogues, each performed by two na-
tive speakers of Japanese. The total time of
the 14 dialogues is 53 minutes. The tasks in-
clude scheduling, route guidance, telephone
shopping, and so on. We set the roles of the
two speakers and the goal of the task but no
pre-defined scenarios. For example, in the
scheduling task, the speakers were given the
roles of a private secretary and a client, and
asked to arrange a meeing appointment.
The speech sound of the two speakers partici-
pating in a dialogue was recorded on separate
channels, which enables us to perform accu-
rate acoustic/prosodic analysis even for over-
lapped talks. The transcription contains or-
thographic representations in Kanji and the
starting and ending time of each utterance,
where an utterance is defined as a continuous
speech region delimited by pauses of 400 msec
or longer.
</bodyText>
<sectionHeader confidence="0.848486" genericHeader="method">
3 Prosodic Information and
</sectionHeader>
<subsectionHeader confidence="0.738515">
Part-of-speech
</subsectionHeader>
<bodyText confidence="0.999941">
The prosodic information and the part-
of-speech tags were assigned (semi-
)automatically using the speech sound
and the transcription.
</bodyText>
<subsectionHeader confidence="0.998656">
3.1 Prosodic information
</subsectionHeader>
<bodyText confidence="0.99995925">
Prosody has been widely recognized as one
of the important factors which relate to dis-
course structures, dialogue acts, informa-
tion status, and so on. Informative corpora
should, in the first place, contain some form
of prosodic information.
At this stage, our corpus merely includes,
as prosodic information, raw values of fun-
damental frequency, voicing probability, and
rms energy, which were obtained from the
speech sound using speech analysis software
ESPS/waves+ (Entropic, 1996) and simple
post-processing for smoothing. The future
corpus will contain more abstract descriptions
of prosodic events such as accents and bound-
ary tones.
</bodyText>
<subsectionHeader confidence="0.999926">
3.2 Part-of-speech
</subsectionHeader>
<bodyText confidence="0.999993176470588">
The part-of-speech is another basic in-
formation for speech recognition, syntac-
tic/semantic parsing, and dialogue processing
as well as linguistic and psycholinguistic anal-
ysis of spoken discourse.
Part-of-speech tags were, first, obtained au-
tomatically from the transcription using the
morphological analysis system ChaSen (Mat-
sumoto et al., 1999), and, then, corrected
manually. The tag set was extended to cover
filled pauses and contracted forms peculiar to
spontaneous speech, and some dialects. The
tagged corpus will be used as a part of the
training data for the statistical learning mod-
ule of ChaSen to improve its performance for
spontaneous speech, which can be used for fu-
ture applications.
</bodyText>
<subsectionHeader confidence="0.999667">
3.3 Word alignment
</subsectionHeader>
<bodyText confidence="0.9996515">
In some applications such as co-reference res-
olution utilizing prosodic correlates of given-
new status of words, it is useful to know the
prosodic information of particular words or
</bodyText>
<figure confidence="0.9954554">
{Dialogue acts
Word alignment
Speech sound
Transcription I
ESPS ChaSen
</figure>
<page confidence="0.998459">
2
</page>
<bodyText confidence="0.999921882352941">
phrases. In order to obtain such informa-
tion, the correspondence between the word se-
quence and the speech sound must be given.
Our corpus contains the information for the
starting and the ending time of every word.
The time-stamp of each word in an ut-
terance was obtained automatically from the
speech sound and the part-of-speech using the
forced alignment function of speech recogni-
tion software HTK (Odell et al., 1997) with
the tri-phone model for Japanese speech de-
veloped by the IPA dictation software project
(Shikano et al., 1998). Apparent errors were
corrected manually with reference to sound
waveforms and spectrograms obtained and
displayed on a screen by ESPS/waves+ (En-
tropic, 1996).
</bodyText>
<sectionHeader confidence="0.974358" genericHeader="method">
4 Utterance Units
</sectionHeader>
<subsectionHeader confidence="0.927183">
4.1 Slash units
</subsectionHeader>
<bodyText confidence="0.999985272727273">
In the transcription, an utterance is defined
as a continuous speech region delimited by
pauses of 400 msec or longer. However, this
definition of the utterances does not corre-
spond to the units for discourse annotation.
For example, the utterances are sometimes
interrupted by the partner. For reliable dis-
course annotation, analysis units must be con-
structed from the utterances defined above.
Following Meteer and Taylor (1995), we call
such a unit &apos;slash unit.&apos;
</bodyText>
<sectionHeader confidence="0.6714935" genericHeader="method">
4.2 Criteria for determining slash
units
</sectionHeader>
<bodyText confidence="0.986529857142857">
The criteria for determining slash units in
Japanese were defined with reference to those
for English (Meteer and Taylor, 1995). The
slash units were annotated manually with ref-
erence to the speech sound and transcription
of dialogues.
Single utterances as slash unit Single
utterances which can be thought to represent
sentences conceptually are qualified as a slash
unit. Figure 2 shows examples of slash units
by single utterances (slash units are delimited
by the symbol 7&apos;).
In the cases where the word order is in-
verted, the utterances are regarded as a slash
</bodyText>
<figure confidence="0.8959307">
A: hai / ;{response}
(yes.)
A: kochira chin annai sisutemu desu /
;{a single sentence}
(This is the sightseeing guide
system.)
A: ryoukin niha fukurnarete orimasen ga
betto 1200 en de goyoui sasete
itadakimasu /
;{a complex sentence}
</figure>
<figureCaption confidence="0.538575">
(This is not included in the charge.
We offer the service for
the separate charge of 1200 yen.)
Figure 2: Examples of single utterances as
slash unit
</figureCaption>
<figure confidence="0.992058166666667">
IA: shuppatsu chiten kara —
(From the starting point)
A: -- nishi gawa ni
the west)
A: -- sukoshi dake ikimasu I
..,(move a little)
</figure>
<figureCaption confidence="0.9546445">
Figure 3: An example of multiple utterances
as slash unit
</figureCaption>
<bodyText confidence="0.992283142857143">
unit only if the utterances with normalized
word order are qualified as a slash unit.
A sequence of one speaker&apos;s speech that ter-
minates with a hesitation, an interruption and
a slip of the tongue, but does not continue in
the speaker&apos;s next utterance is also qualified
as a slash unit.
Multiple utterances as slash unit When
collection of multiple utterances form a sen-
tence, as in Figure 3, they are qualified as one
slash unit. In slash units spanning multiple
utterances, the symbol &apos;--&apos;is marked both at
the end of the first utterance and at the start
of the last utterance.
</bodyText>
<subsectionHeader confidence="0.998199">
4.3 Non sentence elements
</subsectionHeader>
<bodyText confidence="0.9998615">
Non sentence elements consist of `aidutr, con-
junction markers, discourse markers, fillers
</bodyText>
<page confidence="0.967303">
3
</page>
<figure confidence="0.952196333333333">
sukoshi dake itte /
(move a little)
B: un /
(ok)
A: {D de} hidari naname shitani
({D then} to your left and down) )
</figure>
<figureCaption confidence="0.9715725">
Figure 4: An example of a slash unit defined
by discourse markers
</figureCaption>
<bodyText confidence="0.999484633333333">
and non speech elements, which are enclosed
by {B ...}, {C ...}, {D ...}, {F ...}, and
{N }, respectively. These elements can be
used to define a slash unit. For example, when
`aiduti&apos; is expressed by the words such as &amp;quot;hal
(yes, yeah, right)&amp;quot;, &amp;quot;un (yes, yeah, right)&amp;quot; and
&amp;quot;ee (mmm, yeah)&amp;quot; or by word repetition, it is
regarded as an utterance. Otherwise, `aiduti&apos;
is not qualified as an independent slash unit.
The main function of discourse markers is
to show the relations between utterances, like
starting a new topic, changing topics, and
restarting an interrupted conversation. The
words such as &amp;quot;mazu (first, firstly)&amp;quot;, &amp;quot;dewa
(then, ok)&amp;quot;, &amp;quot;tsumari (I mean, that means
that)&amp;quot; and &amp;quot;sorede (and so)&amp;quot; may become dis-
course markers when they appear at the head
of the utterances. An utterance just before
the one with discourse markers is qualified as
a slash unit (Figure 4).
In the Switchboard project(Meteer and
Taylor, 1995), our {B ...} (aiduti) category is
not regarded as a separate category. However
in Japanese dialogue, signals that indicate a
hearer&apos;s attention to speaker&apos;s utterances, are
expressed frequently. For this reason, we cre-
ated &apos;aiduti&apos; as a separate category. Other-
wise {A ... } (aside), {E }(Explict editing
term), the restart and the repair are not an-
notated in our scheme at the present stage.
</bodyText>
<sectionHeader confidence="0.997678" genericHeader="method">
5 Dialogue Acts
</sectionHeader>
<bodyText confidence="0.9998894">
Identifying dialogue act of the slash unit is
difficult task because the mapping between
surface form and dialogue act is not obvious.
In addition, some slash units have more than
one function, e.g. answering question with
stating additional information. Considering
above problems, DAMSL architecture codes
various functions at one utterance, such as
forward looking function, backward looking
function, etc.
However, it is difficult to determine the
function of the isolated utterance. We had
shown that assumptions of dialogue structure
and exchange structure improved agreement
score among coders (Ichikawa et al., 1999).
Therefore, we define our dialogue act tagging
scheme as hierarchical refinement from the ex-
change structure.
The annotation scheme for dialogue acts
includes a set of rules to identify the func-
tion of each slash unit based on the theory of
speech act (Searle, 1969) and discourse anal-
ysis (Coulthhard, 1992; Stenstrom, 1994).
This scheme provides a basis for examining
the local structure of dialogues.
</bodyText>
<listItem confidence="0.994512857142857">
• Task-oriented dialogue
(Opening)
Problem solving
(Closing)
• Problem solving
Exchange+
• Exchange
</listItem>
<figure confidence="0.5445406">
Initiation
(Response)/Initiation*
(Response)*
(Follow-up)
(Follow-up)
</figure>
<figureCaption confidence="0.998255">
Figure 5: Model for task-oriented dialogues
</figureCaption>
<bodyText confidence="0.997736666666667">
In general, a dialogue 1 is modeled with
problem solving subdialogues, sometimes pre-
ceded by opening subdialogue (e.g., greeting)
and followed by closing subdialogue (e.g., ex-
pressing gratitude). A problem solving sub-
dialogue consists of initiating and responding
</bodyText>
<footnote confidence="0.97796325">
1In this paper, we limit our attention to task-
oriented dialogues, which are the main target of the
study in computational linguistics and spoken dia-
logue research.
</footnote>
<page confidence="0.98795">
4
</page>
<figure confidence="0.9840615">
(Initiation)
41 A: chikatetsu no ekimei ha?
(What &apos; s the name of the subway
station?)
(Response)
42 B: chikatetsu no teramachi eki ni
narimasu
(The name of the subway station is
Teramachi.)
(Follow-up)
43 A: hai
(0k.)
</figure>
<figureCaption confidence="0.9940845">
Figure 6: An example problem solving subdi-
alogue with the exchange structure
</figureCaption>
<listItem confidence="0.979089333333333">
• Dialogue management
Open, Close
• Initiation
</listItem>
<construct confidence="0.94893275">
Request, Suggest, Persuade, Propose,
Confirm, Yes-no question, Wh-question,
Promise, Demand, Inform, Other assert,
Other initiate.
</construct>
<listItem confidence="0.983351666666667">
• Response
Positive, Negative, Answer, Other re-
sponse.
• Follow-up
Understand
• Response with Initiation
</listItem>
<bodyText confidence="0.997904555555555">
The element of this category is repre-
sented as Response type / Initiation type.)
utterances, sometimes followed by following
up utterances (Figure 5).
Figure 6 shows an example problem solving
subdialogue with the exchange structure.
In this scheme, dialogue acts, the elements
of the exchange structure, are classified into
the tags shown in Figure 7.
</bodyText>
<sectionHeader confidence="0.996000333333333" genericHeader="method">
6 Dialogue Structure and
Constraints on Multiple
Exchanges
</sectionHeader>
<subsectionHeader confidence="0.999088">
6.1 Dialogue Segment
</subsectionHeader>
<bodyText confidence="0.998747647058824">
In the previous discourse model(Grosz and
Sidner, 1986), a discourse segment has a be-
ginning and an ending utterances and may
have smaller discourse segments in it. It is not
an easy task to identify such segments with
the nesting structure for spoken dialogues,
because the structure of a dialogue is often
very complicated due to the interaction of two
speakers. In a preliminary experiment of cod-
ing segments in spoken dialogues, there were a
lot of disagreements on the granularity or the
relation of the segments and on identifying
ending utterances of the segment. An alterna-
tive scheme of coding the dialogue structure
(DS) is necessary to build dialogue corpora
annotated with the discourse level structure.
Our scheme annotates spoken dialogues
</bodyText>
<figureCaption confidence="0.999656">
Figure 7: The classification of dialogue acts
</figureCaption>
<bodyText confidence="0.9995867">
with boundary marking of the DS, instead of
identifying a beginning and an ending utter-
ance of each DS. A building block of dialogue
segments is identified based on the exchanges
explained in Section 5. A dialogue segment
(DS) tag is inserted before initiating utter-
ances because the initiating utterances can be
thought of as a start of new discourse seg-
ments.
The DS tag consists of a topic break index
(TBI), a topic name and a segment relation.
TBI signifies the degree of topic dissimilarity
between the DSs. TBI takes the value of 1 or
2: the boundary with TBI 2 is less continuous
than the one with TBI 1 with regard to the
topic. The topic name is labeled by coders&apos;
subjective judgment. The segment relation
indicates the one between the preceding and
the following segments, which is classified into
the following categories.
</bodyText>
<listItem confidence="0.957933">
• clarification
</listItem>
<bodyText confidence="0.993015">
suspends the exchange and makes a clar-
ification in order to obtain information
necessary to answer the partner&apos;s utter-
ance;
</bodyText>
<page confidence="0.96835">
5
</page>
<figure confidence="0.837534285714286">
(lc: room for a lecture: ]
38 A: {F el heyawa dou simashou ka?
(How about meeting room?)
[1: small-sized meeting room: clarification]
39 B: heya wa shou-kaigishitsu wa aite masu ka?
(Can I use the small-sized meeting room?)
40 A: {F to} kayoubi no .(F e} 14 ji hart kara wa IF e} shou-kaigisitsu wa aite imasen
(The small meeting room is not available from 14:30 on Tuesday.)
[1:the large-sized meeting room: ]
41 A: dai-kaigishitsu ga tukae masu
(You can use the large meeting room.)
[1: room for a lecture: return]
42 B: {D soreja} dai-kaigishitsu de onegai shimasu
Please book the large meeting room.)
</figure>
<figureCaption confidence="0.997452">
Figure 8: An example dialogue with the dialogue segment tags
</figureCaption>
<listItem confidence="0.871987">
• interruption
</listItem>
<bodyText confidence="0.937129">
starts a different topic from the previous
one during or after the partner&apos;s explana-
tory utterances; and
</bodyText>
<listItem confidence="0.966047">
• return
</listItem>
<bodyText confidence="0.9529515">
goes back to the previous topic after the
clarification or the interruption.
Figure 8 shows an example dialogue anno-
tated with the DS tags.
</bodyText>
<subsectionHeader confidence="0.984137">
6.2 Constraints on multiple
exchanges
</subsectionHeader>
<bodyText confidence="0.998822235294118">
Annotation of dialogue segments mostly de-
pends on the coders&apos; intuitive judgment on
topic dissimilarity between the segments. In
order to lighten the burden of the coders&apos;
judgment, the structural constraints on multi-
ple exchanges are experimentally introduced.
The constraints can be classified into two
types: one concerns embedding exchanges
(relevance type 1) and the other is neighbor-
ing exchanges (relevance type 2).
In relevance type I, the relation of an initi-
ating utterance and its responding utterance
is shown by attaching the id number of the ini-
tiating utterance to the responding utterance.
This id number can indicates non-adjacent
initiation-response pairs including embedded
exchanges inside.
In relevance type 2, the structures of neigh-
boring exchanges such as chaining, coupling,
elliptical coupling (Stenstr6m, 1994) are in-
troduced. Chaining takes the pattern of [A:I
B:R] [A:I B:R] (in both exchanges, speaker
A initiates an utterance and speaker B re-
sponds to A). Coupling is the pattern of [A:I
B:R] [B:I A:R]. (speaker A initiates, speaker
B both responds and initiates and speaker
A responds to B). Elliptical coupling is the
pattern of [A:I] [B:I A:11], equivalent to the
one in which B&apos;s second response is omitted
in coupling. Relevance type 2 shows whether
the above structures of neighboring exchanges
can be observed or not. Figure 9 shows an ex-
ample of annotation of relevance types 1 and
2.
</bodyText>
<sectionHeader confidence="0.830683" genericHeader="method">
7 Corpus Building Tools
</sectionHeader>
<bodyText confidence="0.999689">
In the experiments, various tools for tran-
scription and annotation were used. For tran-
scription, the automatics segmentizer (TIME)
and the online transcriber (PV) were used
(Horiuchi et al., 1999). The former lists up
</bodyText>
<page confidence="0.978926">
6
</page>
<figure confidence="0.910642533333333">
[&lt;Yes-no question&gt; &lt;relevance no&gt;]
27 A: hatsuka no jyuuji kara ha aite irun de syou ka?
(Is the room available from 10am on the 20th?)
[&lt;Yes-no question&gt; &lt;relevance yes&gt;]
28 B: kousyuu shitsu desu ka?
(Are you mentioning the seminar room?)
[&lt;Positive&gt; &lt;0028&gt;]
29 A: hai
(Yes.)
[&lt;Negative&gt; &lt;0027&gt;]
30 B: hatsuka ha aite orirnasen
(It is not available on the 20th.)
[&lt;Understand&gt;]
31 A: soudesu ka
(Ok .)
</figure>
<figureCaption confidence="0.99988">
Figure 9: An example dialogue with relevance types 1 and 2
</figureCaption>
<bodyText confidence="0.99997415">
candidates for unit utterances according to
the parameter for the length of silences. The
latter displays energy measurement of each
speaker&apos;s utterance on the two windows using
a speech data file. Users can see any part of
a dialogue using the scroll bar, and can hear
speech for both speakers or each speaker by
selecting any region of the windows using a
mouse.
For prosodic and part of speech annotation,
the speech analysis software ESPS/waves+
(Entropic, 1996), speech recognition software
HTK (Odell et al., 1997) with the tri-phone
model for Japanese speech developed by the
IPA dictation software project (Shikano et al.,
1998) and the morphological analysis system
ChaSen (Matsumoto et al., 1999) were used.
For discourse annotation, Dialogue Anno-
tation Tool (DAT) had been used in the previ-
ous experiments (Core and Allen, 1997). Al-
though DAT had a consistency check between
some functions in one sentence, we need more
wide-ranging consistency check because our
scheme has assumptions of dialogue structure
and exchange structure. Therefore it is dis-
satisfying but the modification of the tool to
our need is not easy. Thus, for the moment,
we decided to use just a simple transcription
viewer and sound player (TV) (Horiuchi et
al., 1999), which enables us to hear the sound
of utterances on the transcription.
Our project does not have any intention to
create new tools. Rather we do want to use
any existing tools if they suit or can be eas-
ily modified to satisfy our needs. The tools
of MATE project(Carletta and Isard, 1999),
which also directs multi-level annotation, can
be a good candidate for our project. In the
near future, we will examine if we can effec-
tively use their tools in the project.
</bodyText>
<sectionHeader confidence="0.994104" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.9999908">
This paper described a Japanese dialogue cor-
pus annotated with multi-level information
built by the Japanese Discourse Research Ini-
tiative supported by Japanese Society for Ar-
tificial Intelligence. The annotation informa-
tion includes speech, transcription delimited
by slash units, prosodic, part of speech, dia-
logue acts and dialogue segmentation. In the
project (JSAI, 2000), we used the corpus for
obtaining new findings by examining:
</bodyText>
<page confidence="0.996714">
7
</page>
<listItem confidence="0.991054571428571">
• the relationship between linguistic infor-
mation and dialogue acts
• the relationship between prosodic infor-
mation and dialogue segment, and
• the characteristics of agree-
ment/disagreement expressions and
non-sentence elements.
</listItem>
<bodyText confidence="0.99948675">
This year we plan to quadruple the size of the
corpus and make it publicly available as soon
as we finish the annotation and its verifica-
tion.
</bodyText>
<sectionHeader confidence="0.999678" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999822662162162">
J. Carletta and A. Isard. 1999. The MATE Anno-
tation Workbench: User Requirements. In The
Proceedings of the ACL&apos;99 Workshop on To-
wards Standards and Tools for Discourse Tag-
ging, pages 11-17.
J. Carletta, N. Dahlback, N. Reithinger,
and M. A. Walker. 1997. Standards
for Dialogue Coding in Natural Language
Processing. ftp: //ftp .cs .uni-sb . de/pub/
dagstuhl/reporte/97/9706.ps.gz.
M. Core and J. Allen. 1997. Coding Dialogues
with the DAMSL Annotation Scheme. In The
Proceedings of AAAI Fall Symposium on Com-
municative Action in Humans and Machines,
pages 28-35.
M. Core, M. Ishizaki, J. Moore, C. Nakatani,
N. Reithinger, D. Traum, and S. Tutiya. 1998.
The Report of the Third Workshop of the
Discourse Research Initiative, Chiba Corpus
Project. Technical Report 3, Chiba University.
M. Coulthhard, editor. 1992. Advances in Spoken
Discourse Analysis. Routledge.
Entropic Research Laboratory, Inc. 1996.
ESPS/waves+ 5.1.1 Reference Guide.
Grosz, B. J. and Sidner, C. L. 1986. Attention, In-
tentions, and the Structure of Discourse, Com-
putational Linguistics, 12(3), pages 175-204.
Y. Horiuchi, Y. Nakano, H. Koiso, M. Ishizaki,
H. Suzuki, M. Okada, M. Makiko, S. Tutiya,
and A. Ichikawa. 1999. The Design and Sta-
tistical Characterization of the Japanese Map
Task Dialogue Corpus. Japanese Society of Ar-
tificial Intelligence, 14(2).
A. Ichikawa, M. Araki, Y. Horiuchi., M. Ishizaki,
S. Itabashi, T. Itoh, H. Kashioka, K. Kato,
H. Kikuchi, H. Koiso, T. Kumagai, A. Kure-
matsu, K. Maekawa, S. Nakazato, M. Tamoto,
S. Tutiya, Y. Yamashita, and T. Yoshimura.
1999. Evaluation of Annotation Schemes for
Japanese Discourse. In Proceedings of ACL&apos;99
Workshop on Towards Standards and Tools for
Discourse Tagging, pages 26-34.
Japanese Discourse Research Initiative. http://
maw. sip. cs . rit sumei . ac . jp/dtag/.
Y. Matsumoto, A. Kitauchi, T. Yamashita,
Y. Hirano, H. Matsuda, and M. Asa-
hara. Japanese morphological analysis sys-
tem ChaSen version 2.0 manual (2nd edi-
tion). 1999. Technical Report NAIST-IS-
TR99012, Graduate School of Information Sci-
ence, Nara Institute of Science and Technol-
ogy. http://cl.aist-nara.ac. jp/lab/nit/
chasen./manual2/manual.pdf.
M. Meteer and A. Taylor. 1995. Dysflu-
ency Annotation Stylebook for the Switch-
board Corpus. ftp: //f tp c is . upenn . edu/
pub/treebank/smbd/doc/DFL-book .ps .gz.
Japanese Society for Artificial Intelligence. 2000.
Technical Report of SIG on Spoken Language
Understanding and Dialogue Processing. SIG-
SLUD-9903.
J. Odell, D. 011ason, V. Valtchev, and P. Wood-
land. 1997. The HTK Book (for HTK Ver-
sion 2.1). Cambridge University
J. R. Searle. 1969. Speech Acts: An Essay in the
Philosopy of Language. Cambridge University
Press.
K. Shikano, T. Kawahara, K. Ito, K. Takeda,
A. Yamada, T. Utsuro, T. Kobayashi, N. Mine-
matsu, and M. Yamamoto. 1998. The Devel-
opment of Basic Softwares for the Dictation of
Japanese Speech: Research Report 1998.
A. B. Stenstrom. 1994. An Introduction to Spoken
Interaction. Longman.
</reference>
<page confidence="0.998492">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.693885">
<title confidence="0.9028425">Japanese Dialogue Corpus of Multi-Level Annotation The Japanese Discourse Research</title>
<web confidence="0.998988">http://www.slp.cs.ritsumei.ac.jp/dtag/</web>
<abstract confidence="0.99154275">This paper describes a Japanese dialogue corpus annotated with multi-level information built by the Japanese Discourse Research Initiative, Japanese Society for Artificial The annotation formation consists of speech, transcription delimited by slash units, prosodic, part of speech, dialogue acts and dialogue segmentation. In the project, we used the corpus for obtaining new findings by examining the relationship between linguistic information and dialogue acts, that between prosodic information and dialogue segment, and the characteristics of agreement/disagreement expressions and non-sentence elements.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Carletta</author>
<author>A Isard</author>
</authors>
<title>The MATE Annotation Workbench: User Requirements.</title>
<date>1999</date>
<booktitle>In The Proceedings of the ACL&apos;99 Workshop on Towards Standards and Tools for Discourse Tagging,</booktitle>
<pages>11--17</pages>
<contexts>
<context position="19746" citStr="Carletta and Isard, 1999" startWordPosition="3137" endWordPosition="3140">, we need more wide-ranging consistency check because our scheme has assumptions of dialogue structure and exchange structure. Therefore it is dissatisfying but the modification of the tool to our need is not easy. Thus, for the moment, we decided to use just a simple transcription viewer and sound player (TV) (Horiuchi et al., 1999), which enables us to hear the sound of utterances on the transcription. Our project does not have any intention to create new tools. Rather we do want to use any existing tools if they suit or can be easily modified to satisfy our needs. The tools of MATE project(Carletta and Isard, 1999), which also directs multi-level annotation, can be a good candidate for our project. In the near future, we will examine if we can effectively use their tools in the project. 8 Conclusion This paper described a Japanese dialogue corpus annotated with multi-level information built by the Japanese Discourse Research Initiative supported by Japanese Society for Artificial Intelligence. The annotation information includes speech, transcription delimited by slash units, prosodic, part of speech, dialogue acts and dialogue segmentation. In the project (JSAI, 2000), we used the corpus for obtaining </context>
</contexts>
<marker>Carletta, Isard, 1999</marker>
<rawString>J. Carletta and A. Isard. 1999. The MATE Annotation Workbench: User Requirements. In The Proceedings of the ACL&apos;99 Workshop on Towards Standards and Tools for Discourse Tagging, pages 11-17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
<author>N Dahlback</author>
<author>N Reithinger</author>
<author>M A Walker</author>
</authors>
<title>Standards for Dialogue Coding in Natural Language Processing. ftp: //ftp .cs .uni-sb .</title>
<date>1997</date>
<note>de/pub/ dagstuhl/reporte/97/9706.ps.gz.</note>
<contexts>
<context position="1706" citStr="Carletta et al., 1997" startWordPosition="231" endWordPosition="234">and language research communities. The corpora have been used not only for examining the relationship between speech and linguistic phenomena, but also for building speech and language understanding systems. Sharing corpora among researchers is most desirable since creating the corpora needs considerable cost like writing and revising annotation manuals, annotating the data, and checking the consistency and reliability of the annotated data. Discourse Research Initiative was set up in March of 1996 by US, European, and Japanese researchers to develop standardized discourse annotation schemes (Carletta et al., 1997; Core et al., 1998). The efforts of the initiative have been called &apos;standardization&apos;, but this naming is misleading at least. In typical standardizing efforts, as done in audio-visual and telecommunication technologies, commercial companies try to expand the market for their products or interfaces by the standard. The objective of standardizing efforts in discourse is to promote interactions among discourse researchers and thereby provide a solid foundation for corpus-based discourse research, dispensing with duplicating resource making efforts and increasing sharable resources. In cooperati</context>
</contexts>
<marker>Carletta, Dahlback, Reithinger, Walker, 1997</marker>
<rawString>J. Carletta, N. Dahlback, N. Reithinger, and M. A. Walker. 1997. Standards for Dialogue Coding in Natural Language Processing. ftp: //ftp .cs .uni-sb . de/pub/ dagstuhl/reporte/97/9706.ps.gz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Core</author>
<author>J Allen</author>
</authors>
<title>Coding Dialogues with the DAMSL Annotation Scheme.</title>
<date>1997</date>
<booktitle>In The Proceedings of AAAI Fall Symposium on Communicative Action in Humans and Machines,</booktitle>
<pages>28--35</pages>
<contexts>
<context position="19044" citStr="Core and Allen, 1997" startWordPosition="3016" endWordPosition="3019">rt of a dialogue using the scroll bar, and can hear speech for both speakers or each speaker by selecting any region of the windows using a mouse. For prosodic and part of speech annotation, the speech analysis software ESPS/waves+ (Entropic, 1996), speech recognition software HTK (Odell et al., 1997) with the tri-phone model for Japanese speech developed by the IPA dictation software project (Shikano et al., 1998) and the morphological analysis system ChaSen (Matsumoto et al., 1999) were used. For discourse annotation, Dialogue Annotation Tool (DAT) had been used in the previous experiments (Core and Allen, 1997). Although DAT had a consistency check between some functions in one sentence, we need more wide-ranging consistency check because our scheme has assumptions of dialogue structure and exchange structure. Therefore it is dissatisfying but the modification of the tool to our need is not easy. Thus, for the moment, we decided to use just a simple transcription viewer and sound player (TV) (Horiuchi et al., 1999), which enables us to hear the sound of utterances on the transcription. Our project does not have any intention to create new tools. Rather we do want to use any existing tools if they su</context>
</contexts>
<marker>Core, Allen, 1997</marker>
<rawString>M. Core and J. Allen. 1997. Coding Dialogues with the DAMSL Annotation Scheme. In The Proceedings of AAAI Fall Symposium on Communicative Action in Humans and Machines, pages 28-35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Core</author>
<author>M Ishizaki</author>
<author>J Moore</author>
<author>C Nakatani</author>
<author>N Reithinger</author>
<author>D Traum</author>
<author>S Tutiya</author>
</authors>
<title>Corpus Project.</title>
<date>1998</date>
<booktitle>The Report of the Third Workshop of the Discourse Research Initiative,</booktitle>
<tech>Technical Report 3,</tech>
<institution>Chiba University.</institution>
<location>Chiba</location>
<contexts>
<context position="1726" citStr="Core et al., 1998" startWordPosition="235" endWordPosition="238">ommunities. The corpora have been used not only for examining the relationship between speech and linguistic phenomena, but also for building speech and language understanding systems. Sharing corpora among researchers is most desirable since creating the corpora needs considerable cost like writing and revising annotation manuals, annotating the data, and checking the consistency and reliability of the annotated data. Discourse Research Initiative was set up in March of 1996 by US, European, and Japanese researchers to develop standardized discourse annotation schemes (Carletta et al., 1997; Core et al., 1998). The efforts of the initiative have been called &apos;standardization&apos;, but this naming is misleading at least. In typical standardizing efforts, as done in audio-visual and telecommunication technologies, commercial companies try to expand the market for their products or interfaces by the standard. The objective of standardizing efforts in discourse is to promote interactions among discourse researchers and thereby provide a solid foundation for corpus-based discourse research, dispensing with duplicating resource making efforts and increasing sharable resources. In cooperation with this initiat</context>
</contexts>
<marker>Core, Ishizaki, Moore, Nakatani, Reithinger, Traum, Tutiya, 1998</marker>
<rawString>M. Core, M. Ishizaki, J. Moore, C. Nakatani, N. Reithinger, D. Traum, and S. Tutiya. 1998. The Report of the Third Workshop of the Discourse Research Initiative, Chiba Corpus Project. Technical Report 3, Chiba University.</rawString>
</citation>
<citation valid="true">
<date>1992</date>
<booktitle>Advances in Spoken Discourse Analysis.</booktitle>
<editor>M. Coulthhard, editor.</editor>
<publisher>Routledge.</publisher>
<marker>1992</marker>
<rawString>M. Coulthhard, editor. 1992. Advances in Spoken Discourse Analysis. Routledge.</rawString>
</citation>
<citation valid="false">
<date>1996</date>
<institution>Entropic Research Laboratory, Inc.</institution>
<note>ESPS/waves+ 5.1.1 Reference Guide.</note>
<marker>1996</marker>
<rawString>Entropic Research Laboratory, Inc. 1996. ESPS/waves+ 5.1.1 Reference Guide.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>C L Sidner</author>
</authors>
<date>1986</date>
<journal>Attention, Intentions, and the Structure of Discourse, Computational Linguistics,</journal>
<volume>12</volume>
<issue>3</issue>
<pages>175--204</pages>
<contexts>
<context position="13377" citStr="Grosz and Sidner, 1986" startWordPosition="2079" endWordPosition="2082">Other assert, Other initiate. • Response Positive, Negative, Answer, Other response. • Follow-up Understand • Response with Initiation The element of this category is represented as Response type / Initiation type.) utterances, sometimes followed by following up utterances (Figure 5). Figure 6 shows an example problem solving subdialogue with the exchange structure. In this scheme, dialogue acts, the elements of the exchange structure, are classified into the tags shown in Figure 7. 6 Dialogue Structure and Constraints on Multiple Exchanges 6.1 Dialogue Segment In the previous discourse model(Grosz and Sidner, 1986), a discourse segment has a beginning and an ending utterances and may have smaller discourse segments in it. It is not an easy task to identify such segments with the nesting structure for spoken dialogues, because the structure of a dialogue is often very complicated due to the interaction of two speakers. In a preliminary experiment of coding segments in spoken dialogues, there were a lot of disagreements on the granularity or the relation of the segments and on identifying ending utterances of the segment. An alternative scheme of coding the dialogue structure (DS) is necessary to build di</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Grosz, B. J. and Sidner, C. L. 1986. Attention, Intentions, and the Structure of Discourse, Computational Linguistics, 12(3), pages 175-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Horiuchi</author>
<author>Y Nakano</author>
<author>H Koiso</author>
<author>M Ishizaki</author>
<author>H Suzuki</author>
<author>M Okada</author>
<author>M Makiko</author>
<author>S Tutiya</author>
<author>A Ichikawa</author>
</authors>
<date>1999</date>
<booktitle>The Design and Statistical Characterization of the Japanese Map Task Dialogue Corpus. Japanese Society of Artificial Intelligence,</booktitle>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="17718" citStr="Horiuchi et al., 1999" startWordPosition="2796" endWordPosition="2799"> [B:I A:R]. (speaker A initiates, speaker B both responds and initiates and speaker A responds to B). Elliptical coupling is the pattern of [A:I] [B:I A:11], equivalent to the one in which B&apos;s second response is omitted in coupling. Relevance type 2 shows whether the above structures of neighboring exchanges can be observed or not. Figure 9 shows an example of annotation of relevance types 1 and 2. 7 Corpus Building Tools In the experiments, various tools for transcription and annotation were used. For transcription, the automatics segmentizer (TIME) and the online transcriber (PV) were used (Horiuchi et al., 1999). The former lists up 6 [&lt;Yes-no question&gt; &lt;relevance no&gt;] 27 A: hatsuka no jyuuji kara ha aite irun de syou ka? (Is the room available from 10am on the 20th?) [&lt;Yes-no question&gt; &lt;relevance yes&gt;] 28 B: kousyuu shitsu desu ka? (Are you mentioning the seminar room?) [&lt;Positive&gt; &lt;0028&gt;] 29 A: hai (Yes.) [&lt;Negative&gt; &lt;0027&gt;] 30 B: hatsuka ha aite orirnasen (It is not available on the 20th.) [&lt;Understand&gt;] 31 A: soudesu ka (Ok .) Figure 9: An example dialogue with relevance types 1 and 2 candidates for unit utterances according to the parameter for the length of silences. The latter displays energy </context>
<context position="19456" citStr="Horiuchi et al., 1999" startWordPosition="3084" endWordPosition="3087">, 1998) and the morphological analysis system ChaSen (Matsumoto et al., 1999) were used. For discourse annotation, Dialogue Annotation Tool (DAT) had been used in the previous experiments (Core and Allen, 1997). Although DAT had a consistency check between some functions in one sentence, we need more wide-ranging consistency check because our scheme has assumptions of dialogue structure and exchange structure. Therefore it is dissatisfying but the modification of the tool to our need is not easy. Thus, for the moment, we decided to use just a simple transcription viewer and sound player (TV) (Horiuchi et al., 1999), which enables us to hear the sound of utterances on the transcription. Our project does not have any intention to create new tools. Rather we do want to use any existing tools if they suit or can be easily modified to satisfy our needs. The tools of MATE project(Carletta and Isard, 1999), which also directs multi-level annotation, can be a good candidate for our project. In the near future, we will examine if we can effectively use their tools in the project. 8 Conclusion This paper described a Japanese dialogue corpus annotated with multi-level information built by the Japanese Discourse Re</context>
</contexts>
<marker>Horiuchi, Nakano, Koiso, Ishizaki, Suzuki, Okada, Makiko, Tutiya, Ichikawa, 1999</marker>
<rawString>Y. Horiuchi, Y. Nakano, H. Koiso, M. Ishizaki, H. Suzuki, M. Okada, M. Makiko, S. Tutiya, and A. Ichikawa. 1999. The Design and Statistical Characterization of the Japanese Map Task Dialogue Corpus. Japanese Society of Artificial Intelligence, 14(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ichikawa</author>
<author>M Araki</author>
<author>Y Horiuchi</author>
<author>M Ishizaki</author>
<author>S Itabashi</author>
<author>T Itoh</author>
<author>H Kashioka</author>
<author>K Kato</author>
<author>H Kikuchi</author>
<author>H Koiso</author>
<author>T Kumagai</author>
<author>A Kurematsu</author>
<author>K Maekawa</author>
<author>S Nakazato</author>
<author>M Tamoto</author>
<author>S Tutiya</author>
<author>Y Yamashita</author>
<author>T Yoshimura</author>
</authors>
<title>Evaluation of Annotation Schemes for Japanese Discourse.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL&apos;99 Workshop on Towards Standards and Tools for Discourse Tagging,</booktitle>
<pages>26--34</pages>
<contexts>
<context position="2497" citStr="Ichikawa et al., 1999" startWordPosition="350" endWordPosition="353"> in audio-visual and telecommunication technologies, commercial companies try to expand the market for their products or interfaces by the standard. The objective of standardizing efforts in discourse is to promote interactions among discourse researchers and thereby provide a solid foundation for corpus-based discourse research, dispensing with duplicating resource making efforts and increasing sharable resources. In cooperation with this initiative, Japanese Discourse Research Initiative has started in Japan in May 1996, supported by Japanese Society for Artificial Intelligence (JDRI, 1996; Ichikawa et al., 1999). The activities of the initiative involve: • creating and revising annotation schemes based on the survey of the existing schemes and annotation experiments, • annotating corpora based on the proposed annotation schemes, and • doing research using the corpora not only for examining the utility of the schemes and corpora but also for obtaining new findings. 1 [Dialogue segment Figure 1: The relations among the annotation information In the following, a Japanese dialogue corpus of multi-level annotation is demonstrated. The annotation schemes deal with the information for speech, transcription </context>
<context position="11261" citStr="Ichikawa et al., 1999" startWordPosition="1763" endWordPosition="1766">entifying dialogue act of the slash unit is difficult task because the mapping between surface form and dialogue act is not obvious. In addition, some slash units have more than one function, e.g. answering question with stating additional information. Considering above problems, DAMSL architecture codes various functions at one utterance, such as forward looking function, backward looking function, etc. However, it is difficult to determine the function of the isolated utterance. We had shown that assumptions of dialogue structure and exchange structure improved agreement score among coders (Ichikawa et al., 1999). Therefore, we define our dialogue act tagging scheme as hierarchical refinement from the exchange structure. The annotation scheme for dialogue acts includes a set of rules to identify the function of each slash unit based on the theory of speech act (Searle, 1969) and discourse analysis (Coulthhard, 1992; Stenstrom, 1994). This scheme provides a basis for examining the local structure of dialogues. • Task-oriented dialogue (Opening) Problem solving (Closing) • Problem solving Exchange+ • Exchange Initiation (Response)/Initiation* (Response)* (Follow-up) (Follow-up) Figure 5: Model for task-</context>
</contexts>
<marker>Ichikawa, Araki, Horiuchi, Ishizaki, Itabashi, Itoh, Kashioka, Kato, Kikuchi, Koiso, Kumagai, Kurematsu, Maekawa, Nakazato, Tamoto, Tutiya, Yamashita, Yoshimura, 1999</marker>
<rawString>A. Ichikawa, M. Araki, Y. Horiuchi., M. Ishizaki, S. Itabashi, T. Itoh, H. Kashioka, K. Kato, H. Kikuchi, H. Koiso, T. Kumagai, A. Kurematsu, K. Maekawa, S. Nakazato, M. Tamoto, S. Tutiya, Y. Yamashita, and T. Yoshimura. 1999. Evaluation of Annotation Schemes for Japanese Discourse. In Proceedings of ACL&apos;99 Workshop on Towards Standards and Tools for Discourse Tagging, pages 26-34.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Japanese Discourse</author>
</authors>
<title>Research Initiative. http:// maw. sip. cs . rit sumei . ac .</title>
<tech>jp/dtag/.</tech>
<marker>Discourse, </marker>
<rawString>Japanese Discourse Research Initiative. http:// maw. sip. cs . rit sumei . ac . jp/dtag/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Matsumoto</author>
<author>A Kitauchi</author>
<author>T Yamashita</author>
<author>Y Hirano</author>
<author>H Matsuda</author>
<author>M Asahara</author>
</authors>
<title>Japanese morphological analysis system ChaSen version 2.0 manual (2nd edition).</title>
<date>1999</date>
<tech>Technical Report NAIST-ISTR99012,</tech>
<institution>Graduate School of Information Science, Nara Institute of Science and Technology.</institution>
<note>http://cl.aist-nara.ac. jp/lab/nit/ chasen./manual2/manual.pdf.</note>
<contexts>
<context position="5410" citStr="Matsumoto et al., 1999" startWordPosition="797" endWordPosition="801">which were obtained from the speech sound using speech analysis software ESPS/waves+ (Entropic, 1996) and simple post-processing for smoothing. The future corpus will contain more abstract descriptions of prosodic events such as accents and boundary tones. 3.2 Part-of-speech The part-of-speech is another basic information for speech recognition, syntactic/semantic parsing, and dialogue processing as well as linguistic and psycholinguistic analysis of spoken discourse. Part-of-speech tags were, first, obtained automatically from the transcription using the morphological analysis system ChaSen (Matsumoto et al., 1999), and, then, corrected manually. The tag set was extended to cover filled pauses and contracted forms peculiar to spontaneous speech, and some dialects. The tagged corpus will be used as a part of the training data for the statistical learning module of ChaSen to improve its performance for spontaneous speech, which can be used for future applications. 3.3 Word alignment In some applications such as co-reference resolution utilizing prosodic correlates of givennew status of words, it is useful to know the prosodic information of particular words or {Dialogue acts Word alignment Speech sound Tr</context>
<context position="18911" citStr="Matsumoto et al., 1999" startWordPosition="2994" endWordPosition="2997">s. The latter displays energy measurement of each speaker&apos;s utterance on the two windows using a speech data file. Users can see any part of a dialogue using the scroll bar, and can hear speech for both speakers or each speaker by selecting any region of the windows using a mouse. For prosodic and part of speech annotation, the speech analysis software ESPS/waves+ (Entropic, 1996), speech recognition software HTK (Odell et al., 1997) with the tri-phone model for Japanese speech developed by the IPA dictation software project (Shikano et al., 1998) and the morphological analysis system ChaSen (Matsumoto et al., 1999) were used. For discourse annotation, Dialogue Annotation Tool (DAT) had been used in the previous experiments (Core and Allen, 1997). Although DAT had a consistency check between some functions in one sentence, we need more wide-ranging consistency check because our scheme has assumptions of dialogue structure and exchange structure. Therefore it is dissatisfying but the modification of the tool to our need is not easy. Thus, for the moment, we decided to use just a simple transcription viewer and sound player (TV) (Horiuchi et al., 1999), which enables us to hear the sound of utterances on t</context>
</contexts>
<marker>Matsumoto, Kitauchi, Yamashita, Hirano, Matsuda, Asahara, 1999</marker>
<rawString>Y. Matsumoto, A. Kitauchi, T. Yamashita, Y. Hirano, H. Matsuda, and M. Asahara. Japanese morphological analysis system ChaSen version 2.0 manual (2nd edition). 1999. Technical Report NAIST-ISTR99012, Graduate School of Information Science, Nara Institute of Science and Technology. http://cl.aist-nara.ac. jp/lab/nit/ chasen./manual2/manual.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Meteer</author>
<author>A Taylor</author>
</authors>
<title>Dysfluency Annotation Stylebook for the Switchboard Corpus. ftp: //f tp c is . upenn . edu/ pub/treebank/smbd/doc/DFL-book .ps .gz.</title>
<date>1995</date>
<contexts>
<context position="7201" citStr="Meteer and Taylor (1995)" startWordPosition="1086" endWordPosition="1089"> Apparent errors were corrected manually with reference to sound waveforms and spectrograms obtained and displayed on a screen by ESPS/waves+ (Entropic, 1996). 4 Utterance Units 4.1 Slash units In the transcription, an utterance is defined as a continuous speech region delimited by pauses of 400 msec or longer. However, this definition of the utterances does not correspond to the units for discourse annotation. For example, the utterances are sometimes interrupted by the partner. For reliable discourse annotation, analysis units must be constructed from the utterances defined above. Following Meteer and Taylor (1995), we call such a unit &apos;slash unit.&apos; 4.2 Criteria for determining slash units The criteria for determining slash units in Japanese were defined with reference to those for English (Meteer and Taylor, 1995). The slash units were annotated manually with reference to the speech sound and transcription of dialogues. Single utterances as slash unit Single utterances which can be thought to represent sentences conceptually are qualified as a slash unit. Figure 2 shows examples of slash units by single utterances (slash units are delimited by the symbol 7&apos;). In the cases where the word order is invert</context>
<context position="10227" citStr="Meteer and Taylor, 1995" startWordPosition="1604" endWordPosition="1607">tition, it is regarded as an utterance. Otherwise, `aiduti&apos; is not qualified as an independent slash unit. The main function of discourse markers is to show the relations between utterances, like starting a new topic, changing topics, and restarting an interrupted conversation. The words such as &amp;quot;mazu (first, firstly)&amp;quot;, &amp;quot;dewa (then, ok)&amp;quot;, &amp;quot;tsumari (I mean, that means that)&amp;quot; and &amp;quot;sorede (and so)&amp;quot; may become discourse markers when they appear at the head of the utterances. An utterance just before the one with discourse markers is qualified as a slash unit (Figure 4). In the Switchboard project(Meteer and Taylor, 1995), our {B ...} (aiduti) category is not regarded as a separate category. However in Japanese dialogue, signals that indicate a hearer&apos;s attention to speaker&apos;s utterances, are expressed frequently. For this reason, we created &apos;aiduti&apos; as a separate category. Otherwise {A ... } (aside), {E }(Explict editing term), the restart and the repair are not annotated in our scheme at the present stage. 5 Dialogue Acts Identifying dialogue act of the slash unit is difficult task because the mapping between surface form and dialogue act is not obvious. In addition, some slash units have more than one functi</context>
</contexts>
<marker>Meteer, Taylor, 1995</marker>
<rawString>M. Meteer and A. Taylor. 1995. Dysfluency Annotation Stylebook for the Switchboard Corpus. ftp: //f tp c is . upenn . edu/ pub/treebank/smbd/doc/DFL-book .ps .gz.</rawString>
</citation>
<citation valid="true">
<title>Japanese Society for Artificial Intelligence.</title>
<date>2000</date>
<booktitle>Technical Report of SIG on Spoken Language Understanding and Dialogue Processing. SIGSLUD-9903.</booktitle>
<marker>2000</marker>
<rawString>Japanese Society for Artificial Intelligence. 2000. Technical Report of SIG on Spoken Language Understanding and Dialogue Processing. SIGSLUD-9903.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Odell</author>
<author>D 011ason</author>
<author>V Valtchev</author>
<author>P Woodland</author>
</authors>
<date>1997</date>
<booktitle>The HTK Book (for HTK Version 2.1).</booktitle>
<publisher>Cambridge University</publisher>
<marker>Odell, 011ason, Valtchev, Woodland, 1997</marker>
<rawString>J. Odell, D. 011ason, V. Valtchev, and P. Woodland. 1997. The HTK Book (for HTK Version 2.1). Cambridge University</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Searle</author>
</authors>
<title>Speech Acts: An Essay in the Philosopy of Language.</title>
<date>1969</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="11528" citStr="Searle, 1969" startWordPosition="1810" endWordPosition="1811">ems, DAMSL architecture codes various functions at one utterance, such as forward looking function, backward looking function, etc. However, it is difficult to determine the function of the isolated utterance. We had shown that assumptions of dialogue structure and exchange structure improved agreement score among coders (Ichikawa et al., 1999). Therefore, we define our dialogue act tagging scheme as hierarchical refinement from the exchange structure. The annotation scheme for dialogue acts includes a set of rules to identify the function of each slash unit based on the theory of speech act (Searle, 1969) and discourse analysis (Coulthhard, 1992; Stenstrom, 1994). This scheme provides a basis for examining the local structure of dialogues. • Task-oriented dialogue (Opening) Problem solving (Closing) • Problem solving Exchange+ • Exchange Initiation (Response)/Initiation* (Response)* (Follow-up) (Follow-up) Figure 5: Model for task-oriented dialogues In general, a dialogue 1 is modeled with problem solving subdialogues, sometimes preceded by opening subdialogue (e.g., greeting) and followed by closing subdialogue (e.g., expressing gratitude). A problem solving subdialogue consists of initiating</context>
</contexts>
<marker>Searle, 1969</marker>
<rawString>J. R. Searle. 1969. Speech Acts: An Essay in the Philosopy of Language. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Shikano</author>
<author>T Kawahara</author>
<author>K Ito</author>
<author>K Takeda</author>
<author>A Yamada</author>
<author>T Utsuro</author>
<author>T Kobayashi</author>
<author>N Minematsu</author>
<author>M Yamamoto</author>
</authors>
<title>The Development of Basic Softwares for the Dictation of Japanese Speech: Research Report</title>
<date>1998</date>
<contexts>
<context position="6576" citStr="Shikano et al., 1998" startWordPosition="990" endWordPosition="993"> words or {Dialogue acts Word alignment Speech sound Transcription I ESPS ChaSen 2 phrases. In order to obtain such information, the correspondence between the word sequence and the speech sound must be given. Our corpus contains the information for the starting and the ending time of every word. The time-stamp of each word in an utterance was obtained automatically from the speech sound and the part-of-speech using the forced alignment function of speech recognition software HTK (Odell et al., 1997) with the tri-phone model for Japanese speech developed by the IPA dictation software project (Shikano et al., 1998). Apparent errors were corrected manually with reference to sound waveforms and spectrograms obtained and displayed on a screen by ESPS/waves+ (Entropic, 1996). 4 Utterance Units 4.1 Slash units In the transcription, an utterance is defined as a continuous speech region delimited by pauses of 400 msec or longer. However, this definition of the utterances does not correspond to the units for discourse annotation. For example, the utterances are sometimes interrupted by the partner. For reliable discourse annotation, analysis units must be constructed from the utterances defined above. Following</context>
<context position="18841" citStr="Shikano et al., 1998" startWordPosition="2984" endWordPosition="2987">unit utterances according to the parameter for the length of silences. The latter displays energy measurement of each speaker&apos;s utterance on the two windows using a speech data file. Users can see any part of a dialogue using the scroll bar, and can hear speech for both speakers or each speaker by selecting any region of the windows using a mouse. For prosodic and part of speech annotation, the speech analysis software ESPS/waves+ (Entropic, 1996), speech recognition software HTK (Odell et al., 1997) with the tri-phone model for Japanese speech developed by the IPA dictation software project (Shikano et al., 1998) and the morphological analysis system ChaSen (Matsumoto et al., 1999) were used. For discourse annotation, Dialogue Annotation Tool (DAT) had been used in the previous experiments (Core and Allen, 1997). Although DAT had a consistency check between some functions in one sentence, we need more wide-ranging consistency check because our scheme has assumptions of dialogue structure and exchange structure. Therefore it is dissatisfying but the modification of the tool to our need is not easy. Thus, for the moment, we decided to use just a simple transcription viewer and sound player (TV) (Horiuch</context>
</contexts>
<marker>Shikano, Kawahara, Ito, Takeda, Yamada, Utsuro, Kobayashi, Minematsu, Yamamoto, 1998</marker>
<rawString>K. Shikano, T. Kawahara, K. Ito, K. Takeda, A. Yamada, T. Utsuro, T. Kobayashi, N. Minematsu, and M. Yamamoto. 1998. The Development of Basic Softwares for the Dictation of Japanese Speech: Research Report 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A B Stenstrom</author>
</authors>
<title>An Introduction to Spoken Interaction.</title>
<date>1994</date>
<publisher>Longman.</publisher>
<contexts>
<context position="11587" citStr="Stenstrom, 1994" startWordPosition="1818" endWordPosition="1819">tterance, such as forward looking function, backward looking function, etc. However, it is difficult to determine the function of the isolated utterance. We had shown that assumptions of dialogue structure and exchange structure improved agreement score among coders (Ichikawa et al., 1999). Therefore, we define our dialogue act tagging scheme as hierarchical refinement from the exchange structure. The annotation scheme for dialogue acts includes a set of rules to identify the function of each slash unit based on the theory of speech act (Searle, 1969) and discourse analysis (Coulthhard, 1992; Stenstrom, 1994). This scheme provides a basis for examining the local structure of dialogues. • Task-oriented dialogue (Opening) Problem solving (Closing) • Problem solving Exchange+ • Exchange Initiation (Response)/Initiation* (Response)* (Follow-up) (Follow-up) Figure 5: Model for task-oriented dialogues In general, a dialogue 1 is modeled with problem solving subdialogues, sometimes preceded by opening subdialogue (e.g., greeting) and followed by closing subdialogue (e.g., expressing gratitude). A problem solving subdialogue consists of initiating and responding 1In this paper, we limit our attention to t</context>
</contexts>
<marker>Stenstrom, 1994</marker>
<rawString>A. B. Stenstrom. 1994. An Introduction to Spoken Interaction. Longman.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>