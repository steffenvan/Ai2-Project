<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.028729">
<title confidence="0.989892">
User Simulation as Testing for Spoken Dialog Systems
</title>
<author confidence="0.995654">
Hua Ai* Fuliang Weng
</author>
<affiliation confidence="0.9904135">
Intelligent Systems Program Research and Technology Center
University of Pittsburgh Robert Bosch LLC
</affiliation>
<address confidence="0.559579">
210 S. Bouquet St., Pittsburg, PA 15260 4009 Miranda Ave., Palo Alto, CA 94304
</address>
<email confidence="0.992601">
Hua@cs.pitt.edu Fuliang.weng@us.bosch.com
</email>
<sectionHeader confidence="0.993689" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99986475">
We propose to use user simulation for testing
during the development of a sophisticated dia-
log system. While the limited behaviors of the
state-of-the-art user simulation may not cover
important aspects in the dialog system testing,
our proposed approach extends the functional-
ity of the simulation so that it can be used at
least for the early stage testing before the sys-
tem reaches stable performance for evaluation
involving human users. The proposed ap-
proach includes a set of evaluation measures
that can be computed automatically from the
interaction logs between the user simulator
and the dialog system. We first validate these
measures on human user dialogs using user
satisfaction scores. We also build a regression
model to estimate the user satisfaction scores
using these evaluation measures. Then, we
apply the evaluation measures on a simulated
dialog corpus trained from the real user cor-
pus. We show that the user satisfaction scores
estimated from the simulated corpus are not
statistically different from the real users’ satis-
faction scores.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997924125">
Spoken dialog systems are being widely used in
daily life. The increasing demands of such systems
require shorter system development cycles and
better automatic system developing techniques. As
a result, machine learning techniques are applied to
learn dialog strategies automatically, such as rein-
forcement learning (Singh et al., 2002; Williams &amp;
Young, 2007), supervised learning (Henderson et
</bodyText>
<note confidence="0.622252">
* This study was conducted when the author was an intern at
Bosch RTC.
</note>
<bodyText confidence="0.9998576">
al., 2005), etc. These techniques require a signifi-
cant amount of training data for the automatic
learners to sufficiently explore the vast space of
possible dialog states and strategies. However, it is
always hard to obtain training corpora that are
large enough to ensure that the learned strategies
are reliable. User simulation is an attempt to solve
this problem by generating synthetic training cor-
pora using computer simulated users. The simu-
lated users are built to mimic real users&apos; behaviors
to some extent while allowing them to be pro-
grammed to explore unseen but still possible user
behaviors. These simulated users can interact with
the dialog systems to generate large amounts of
training data in a low-cost and time-efficient man-
ner. Many previous studies (Scheffler, 2002;
Pietquin, 2004) have shown that the dialog strate-
gies learned from the simulated training data out-
perform the hand-crafted strategies. There are also
studies that use user simulation to train speech rec-
ognition and understanding components (Chung,
2004).
While user simulation is largely used in dialog
system training, it has only been used in limited
scope for testing specific dialog system compo-
nents in the system evaluation phase (López-Cózar
et al., 2003; Filisko and Seneff, 2006). This is
partly because the state-of-the-art simulated users
have quite limited abilities in mimicking human
users&apos; behaviors and typically over-generate possi-
ble dialog behaviors. This is not a major problem
when using simulated dialog corpus as the training
corpus for dialog strategy learning because the
over-generated simulation behaviors would only
provide the machine learners with a broader dialog
state space to explore (Ai et al., 2007). However,
realistic user behaviors are highly desired in the
testing phase because the systems are evaluated
and adjusted based on the analysis of the dialogs
generated in this phase. Therefore, we would ex-
</bodyText>
<page confidence="0.978006">
164
</page>
<note confidence="0.869022">
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 164–171,
Columbus, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999816723404255">
pect that these user behaviors are what we will see
in the final evaluation with human users. In this
case, any over-generated dialog behaviors may
cause the system to be blamed for untargeted func-
tions. What is more, the simulated users cannot
provide subjective user satisfaction feedback
which is also important for improving the systems.
Since it is expensive and time-consuming to test
every version of the system with a significant
amount of paid subjects, the testing during the de-
velopment is typically constrained to a limited
number of users, and often, to repeated users who
are colleagues or developers themselves. Thus, the
system performance is not always optimized for
the intended users.
Our ultimate goal is to supplement human test-
ing with simulated users during the development to
speed up the system development towards desired
performance. This would be especially useful in
the early development stage, since it would avoid
conducting tests with human users when they may
feel extremely frustrated due to the malfunction of
the unstable system.
As a first attempt, we try to extend the state-of-
the-art user simulation by incorporating a set of
new but straightforward evaluation measures for
automatically assessing the dialog system perform-
ance. These evaluation measures focus on three
basic aspects of task-oriented dialog systems: un-
derstanding ability, efficiency, and the appropri-
ateness of the system actions. They are first
applied on a corpus generated between a dialog
system and a group of human users to demonstrate
the validity of these measures with the human us-
ers&apos; satisfaction scores. Results show that these
measures are significantly correlated with the hu-
man users&apos; satisfactions. Then, a regression model
is built to predict the user satisfaction scores using
these evaluation measures. We also apply the re-
gression model on a simulated dialog corpus
trained from the above real user corpus, and show
that the user satisfaction scores estimated from the
simulated dialogs do not differ significantly from
the real users’ satisfaction scores. Finally, we con-
clude that these evaluation measures can be used to
assess the system performance based on the esti-
mated user satisfaction.
</bodyText>
<sectionHeader confidence="0.938945" genericHeader="introduction">
2 User Simulation Techniques
</sectionHeader>
<bodyText confidence="0.999962137254902">
Most user simulation models are trained from dia-
log corpora generated by human users. Earlier
models predict user actions based on simple rela-
tions between the system actions and the following
user responses. (Eckert et al., 1997) first suggest a
bigram model to predict the next user&apos;s action
based on the previous system&apos;s action. (Levin et al.,
2000) add constraints to the bigram model to ac-
cept the expected dialog acts only. However, their
basic assumption of making the next user&apos;s action
dependent only on the system&apos;s previous action is
oversimplified. Later, many studies model more
comprehensive user behaviors by adding user goals
to constrain the user actions (Scheffler, 2002; Piet-
quin, 2004). These simulated users mimic real user
behaviors in a statistical way, conditioning the user
actions on the user goals and the dialog contexts.
More recent research defines agenda for simulated
users to complete a set of settled goals (Schatz-
mann et al., 2007). This type of simulated user up-
dates the agenda and the current goal based on the
changes of the dialog states.
In this study, we build a simulated user similar
to (Schatzmann et al., 2007) in which the simulated
user keeps a list of its goals and another agenda of
actions to complete the goals. In our restaurant se-
lection domain, the users’ tasks are to find a de-
sired restaurant based on several constraints
specified by the task scenarios. We consider these
restaurant constraints as the goals for the simulated
user. At the beginning of the dialog, the simulated
user randomly generates an agenda for the list of
the ordered goals corresponding to the three con-
straints in requesting a restaurant. An agenda con-
tains multiple ordered items, each of which
consists of the number of constraints and the spe-
cific constraints to be included in each user utter-
ance. During the dialog, the simulated user updates
its list of goals by removing the constraints that
have been understood by the system. It also re-
moves from its agenda the unnecessary actions that
are related to the already filled goals while adding
new actions. New actions are added according to
the last system’s question (such as requesting the
user to repeat the last utterance) as well as the
simulated user’s current goals. The actions that
address the last system’s question are given higher
priorities then other actions in the agenda. For ex-
ample, if the dialog system fails to understand the
last user utterance and thus requests a clarification,
the simulated user will satisfy the system’s request
</bodyText>
<page confidence="0.998027">
165
</page>
<bodyText confidence="0.999983833333333">
before moving on to discuss a new constraint. The
simulated user updated the agenda with the new
actions after each user turn.
The current simulated user interacts with the
system on the word level. It generates a string of
words by instantiating its current action using pre-
defined templates derived from previously col-
lected corpora with real users. Random lexical
errors are added to simulate a spoken language
understanding performance with a word error rate
of 15% and a semantic error rate of 11% based on
previous experience (Weng et al., 2006).
</bodyText>
<sectionHeader confidence="0.968584" genericHeader="method">
3 System and Corpus
</sectionHeader>
<bodyText confidence="0.999343882352941">
CHAT (Conversational Helper for Automotive
Tasks) is a spoken dialog system that supports na-
vigation, restaurant selection and mp3 player ap-
plications. The system is specifically designed for
users to interact with devices and receive services
while performing other cognitive demanding, or
primary tasks such as driving (Weng et al., 2007).
CHAT deploys a combination of off-the-shelf
components, components used in previous lan-
guage applications, and components specifically
developed as part of this project. The core compo-
nents of the system include a statistical language
understanding (SLU) module with multiple under-
standing strategies for imperfect input, an informa-
tion-state-update dialog manager (DM) that
handles multiple dialog threads and mixed initia-
tives (Mirkovic and Cavedon, 2005), a knowledge
manager (KM) that controls access to ontology-
based domain knowledge, and a content optimizer
that connects the DM and the KM for resolving
ambiguities from the users&apos; requests, regulating the
amount of information to be presented to the user,
as well as providing recommendations to users. In
addition, we use Nuance 8.51 with dynamic gram-
mars and classbased n-grams, for speech recogni-
tion, and Nuance Vocalizer 3.0 for text-to-speech
synthesis (TTS). However, the two speech compo-
nents, i.e., the recognizer and TTS are not used in
the version of the system that interacts with the
simulated users.
The CHAT system was tested for the navigation
domain, the restaurant selection and the MP3 mu-
sic player. In this study, we focus on the dialog
corpus collected on the restaurant domain only. A
</bodyText>
<footnote confidence="0.882106">
1 See http://www.nuance.com for details.
</footnote>
<bodyText confidence="0.999743416666667">
small number of human users were used as dry-run
tests for the system development from November,
2005 to January, 2006. We group the adjacent dry-
runs to represent system improvement stages on a
weekly basis. Table 1 shows the improvement
stages, the dry-run dates which each stage in-
cludes, and the number of subjects tested in each
stage. A final evaluation was conducted during
January 19-31, 2006, without any further system
modifications. This final evaluation involved 20
paid subjects who were recruited via internet ad-
vertisement.
Only the users in the final evaluation completed
user satisfaction surveys after interacting with the
system. In the survey, users were asked to rate the
conversation from 6 perspectives, each on a 5-
point scale: whether the system was easy to use,
whether the system understood the user well,
whether the interaction pattern was natural,
whether the system&apos;s actions were appropriate,
whether the system acted as expected, and whether
the user was willing to use the system on a regular
base. A user satisfaction score was computed as
the average of the 6 ratings.
</bodyText>
<table confidence="0.97429">
Stage Dry-run Dates Users
1 11/21/05, 11/22/05 2
2 11/30/05, 12/1/05, 12/2/05 3
3 12/7/05, 12/8/05 2
4 12/13/05, 12/14/05, 12/15/05 5
5 12/19/05, 12/20/05, 12/21/05 4
6 12/27/05, 12/28/05 2
7 1/4/06, 1/5/06 2
8 1/10/06, 1/11/06, 1/13/06 4
9 1/16/06, 1/17/06 3
</table>
<tableCaption confidence="0.99904">
Table 1: Dry-runs
</tableCaption>
<bodyText confidence="0.999921416666667">
Nine tasks of restaurant selections were used in
both dry-runs and the final evaluation using 12
constraints in total (e.g., cuisine type, price level,
location). These 12 constraints are spread across
the nine tasks evenly with three constraints per
task. In addition, each task is carefully worded
based on the task-constrained and language-
unconstrained guideline. In other words, we want
the users to form an intended mental context while
trying to prevent them from copying the exact
phrasing in the task description. During the dry-
runs, the users randomly pick three to four tasks to
</bodyText>
<page confidence="0.995164">
166
</page>
<table confidence="0.9998404">
Utterances Constraints Understood by the Action Types &amp; Under-
System standing Status
User1: I&apos;m looking for a five star Thai [Italian] Action: req_info
restaurant accept credit cards Slots conveyed: 3 slots
System1: There are 7 restaurants that have service level=five star Action: pro_info
a five star rating, accept credit cards, and cuisine type=Thai Slots accepted: 3 slots
serve Thai food in my database. Here are payment type=credit card Completely matched: 2 slots
the first few: TeJUS, Slam Corner Cate,
Setha&apos;s Restaurant. Should I continue?
User2: I need to look for Italian restaurant Action: rereq_info
Slot conveyed: 1 slot
System2: Are you looking for a restaurant service level=five star Action: exp_confirm
that serves Italian food? cuisine type=Italian (unconfirmed) Slot updated: 1 slot
payment type=credit card Completely matched: 2.5 slot
User3: yes Action: exp_confirm
Slot conveyed: 1 slot
System3: I found 38 restaurants that serve service level=five star Action: pro_info
Italian food, accept credit cards and have a cuisine type=Italian (confirmed) Slots accepted: 3 slots
five star rating, for example, Toto Café ... payment type=credit card Completely matched: 3 slots
User4: I&apos;m done. Action: close_dialog
</table>
<tableCaption confidence="0.998712">
Table 2: A Sample Dialog with Annotations.
</tableCaption>
<bodyText confidence="0.999801833333333">
test the system, while in the final evaluation each
user is required to complete all of the 9 tasks. As a
result of the final evaluation in the restaurant do-
main with 2500 restaurants, we reached a task
completion rate of 94% with a word recognition
rate of 85%, and a semantic accuracy rate of 89%.
</bodyText>
<sectionHeader confidence="0.998097" genericHeader="method">
4 Evaluation Measures
</sectionHeader>
<bodyText confidence="0.999919">
In this section, we describe in detail the evaluation
measures covering three basic aspects of task-
oriented dialog systems: understanding ability, ef-
ficiency, and the appropriateness of the system
actions.
</bodyText>
<subsectionHeader confidence="0.999492">
4.1 Understanding Ability Measures
</subsectionHeader>
<bodyText confidence="0.999970260869565">
Human-human dialog is a process to reach mutual
understandings between the dialog partners by ex-
changing information through the dialog. This in-
formation exchanging process also takes place in
the interaction between users and spoken dialog
systems. In a task-oriented conversation, the dialog
system&apos;s major task is to understand the users&apos;
needs in order to provide the right service. In the
information-state update framework, the system
continuously updates its information-states during
the dialog while the users are conveying their re-
quirements. If a misunderstanding occurs, there
would be a mismatch between the users’ require-
ments and the system’s understandings. Thus, the
error recovery dialog is needed to fix the mis-
matches. The error recovery dialog can be initiated
either by the system by asking the user to rephrase
or to repeat the previous utterance, or by the user
to restate the previous request.
We use the percent of agreement between the
system&apos;s and the user&apos;s understandings (under-
standingAgreement) to measure how well the
system understands the user. The computation of
this measure is illustrated through the example dia-
log in Table 2. In this table, the first column shows
the system utterances and the user utterances re-
ceived by the system. The correct words are shown
in square brackets immediately after the misunder-
stood words (E.g., in Utterance “User1”). The sec-
ond column represents semantic content from the
users’ utterances in the form of constraint-value
pairs based on the system’s understandings. This
information can be automatically retrieved from
the system logs. The third column includes the ac-
tion types of the current system/user utterances.
Since the dialog manager is an information-
updating dialog manager that manages information
in the format of slots, this column also shows the
number of slots that are exchanged in the utterance
and the number of matched slots. In our task do-
main, the user can request information (req_info),
request the same information again (rereq_info),
answer an explicit confirmation (exp_confirm),
and close a dialog (close_dialog). The system can
provide information (pro_info) or explicitly con-
firms (exp_confirm) the information. Another
</bodyText>
<page confidence="0.990163">
167
</page>
<bodyText confidence="0.999757310344828">
available system action that is not shown in this
example is to ask the user to repeat/rephrase (re-
phrase), where the user can respond by providing
the information again (repro_info).
In our experiment, we measure the understand-
ings between the users and the system by compar-
ing the values of the constraints that are specified
by the users with their values understood by the
system. In this dialog, the user specified all con-
straints in the first utterance:
Service level = Five star
Cuisine type = Italian
Payment type = Credit card
The first system utterance shows that the system
understood two constraints but misunderstood the
cuisine type, thus the percent agreement of mutual
understandings is 2/3 at this time. Then, the user
restated the cuisine type and the second system
utterance confirmed this information. Since the
system only asks for explicit information when its
confidence is low, we count the system&apos;s under-
standing on the cuisine type as a 50% match with
the user&apos;s. Therefore, the total percent agreement is
2.5/3. The user then confirmed that the system had
correctly understood all constraints. Therefore, the
system provided the restaurant information in the
last utterance. The system&apos;s understanding matches
100% with the user&apos;s at this point.
The percent agreement of system/user under-
standings over the entire dialog is calculated by
averaging the percent agreement after each turn. In
this example, understandingAgreement is (2/3 +
2.5/3 + 1)/3 =83.3%. We hypothesize that the
higher the understandingAgreement is, the better
the system performs, and thus the more the user is
satisfied. The matches of understandings can be
calculated automatically from the user simulation
and the system logs. However, since we work with
human users&apos; dialogs in the first part of this study,
we manually annotated the semantic contents (e.g.,
cuisine name) in the real user corpus.
Previous studies (E.g., Walker et al., 1997) use a
corpus level semantic accuracy measure (semanti-
cAccuracy) to capture the system’s understanding
ability. SemanticAccuracy is defined in the stan-
dard way as the total number of correctly under-
stood constraints divided by the total number of
constraints mentioned in the entire dialog. The un-
derstandingAgreement measure we introduce here
is essentially the averaged per-sentence semantic
accuracy, which emphasizes the utterance level
perception rather than a single corpus level aver-
age. The intuition behind this new measure is that
it is better for the system to always understand
something to keep a conversation going than for
the system to understand really well sometimes but
really bad at other times. We compute both meas-
ures in our experiments for comparison.
</bodyText>
<subsectionHeader confidence="0.984672">
4.2 Efficiency Measure
</subsectionHeader>
<bodyText confidence="0.999934975">
Efficiency is another important measure of the sys-
tem performance. A standard efficiency measure is
the number of dialog turns. However, we would
like to take into account the user&apos;s dialog strategy
because how the user specifies the restaurant selec-
tion constraints has a certain impact on the dialog
pace. Comparing two situations where one user
specifies the three constraints of selecting a restau-
rant in three separate utterances, while another user
specifies all the constraints in one utterance, we
will find that the total number of dialog turns in the
second situation is smaller assuming perfect under-
standings. Thus, we propose to use the ratio be-
tween the number of turns in the perfect
understanding situation and the number of turns in
practice (efficiencyRatio) to measure the system
efficiency. The larger the efficiencyRatio is, the
closer the actual number of turns is to the perfect
understanding situation. In the example in Table 2,
because the user chose to specify all the constraints
in one utterance, the dialog length would be 2 turns
in perfect understanding situation (excluding the
last user turn which is always &amp;quot;I&apos;m done&amp;quot;). How-
ever, the actual dialog length is 6 turns. Thus, the
efficiencyRatio is 2/6.
Since our task scenarios always contain three
constraints, we can calculate the length of the er-
ror-free dialogs based on the user’s strategy. When
the user specifies all constraints in the first utter-
ance, the ideal dialog will have only 2 turns; when
the user specifies two constraints in one utterance
and the other constraints in a separate utterance,
the ideal dialog will have 4 turns; when the user
specifies all constraints one by one, the ideal dia-
log will have 6 turns. Thus, in the simulation envi-
ronment, the length of the ideal dialog can be
calculated from the simulated users’ agenda. Then,
the efficiencyRatio can be calculated automati-
cally. We manually computed this measure for the
real users’ dialogs.
</bodyText>
<page confidence="0.973426">
168
</page>
<bodyText confidence="0.9487318">
Similarly, in order to compare with previous In this section, we first validate the proposed
studies, we also investigate the total number of measures using real users’ satisfaction scores, and
dialog turns (dialogTurns) proposed as the effi- then show the differentiating power of these meas-
ciency measure (E.g., Möller et al., 2007). ures through the improvement curves plotted on
the dry-run data.
</bodyText>
<subsectionHeader confidence="0.890676">
4.3 Action Appropriateness Measure
</subsectionHeader>
<bodyText confidence="0.999993435897436">
This measure aims to evaluate the appropriateness
of the system actions. The definition of appropri-
ateness can vary on different tasks and different
system design requirements. For example, some
systems always ask users to explicitly confirm
their utterances due to high security needs. In this
case, an explicit confirmation after each user utter-
ance is an appropriate system action. However, in
other cases, frequent explicit confirmations may be
considered as inappropriate because they may irri-
tate the users. In our task domain, we define the
only inappropriate system action to be providing
information based on misunderstood user require-
ments. In this situation, the system is not aware of
its misunderstanding error. Instead of conducting
an appropriate error-recovering dialog, the system
provides wrong information to the user which we
hypothesize will decrease the user’s satisfaction.
We use the percentage of appropriate system ac-
tions out of the total number of system actions
(percentAppropriate) to measure the appropriate-
ness of system actions. In the example in Table 2,
only the first system action is inappropriate in all 3
system actions. Thus, the percent system action
appropriateness is 2/3. Since we can detect the sys-
tem’s misunderstanding and the system’s action in
the simulated dialog environment, this measure can
be calculated automatically for the simulated dia-
logs. For the real user corpus, we manually coded
the inappropriate system utterances.
Note that the definition of appropriate action we
use here is fairly loose. This is partly due to the
simplicity of our task domain and the limited pos-
sible system/user actions. Nevertheless, there is
also an advantage of the loose definition: we do
not bias towards one particular dialog strategy
since our goal here is to find some general and eas-
ily measurable system performance factors that are
correlated with the user satisfaction.
</bodyText>
<sectionHeader confidence="0.915674" genericHeader="method">
5 Investigating Evaluation Measures on
the Real User Corpus
</sectionHeader>
<subsectionHeader confidence="0.999895">
5.1 Validating Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.999445">
To validate the evaluation measures introduced in
Section 4, we use Pearson’s correlation to examine
how well these evaluation measures can predict the
user satisfaction scores. Here, we only look at the
dialog corpus in final evaluation because only
these users filled out the user satisfaction surveys.
For each user, we compute the average value of the
evaluation measures across all dialogs generated
by that user.
</bodyText>
<table confidence="0.710034">
Evaluation Measure Correlation P-value
understandingAgreement 0.354 0.05
semanticAccuracy 0.304 0.08
efficiencyRatio 0.406 0.02
dialogTurns -0.321 0.05
percentAppropriate 0.454 0.01
</table>
<tableCaption confidence="0.293599">
Table3: Correlations with User Satisfaction Scores.
</tableCaption>
<bodyText confidence="0.998650961538461">
Table 3 lists the correlation between the evalua-
tion measures and the user satisfaction scores, as
well as the p-value for each correlation. The corre-
lation describes a linear relationship between these
measures and the user satisfaction scores. For the
measures that describe the system’s understanding
abilities and the measures that describe the sys-
tem’s efficiency, our newly proposed measures
show higher correlations with the user satisfaction
scores than their counterparts. Therefore, in the
rest of the study, we drop the two measures used
by the previous studies, i.e., semanticAccuracy and
dialogTurns.
We observe that the user satisfaction scores are
significantly positively correlated with all the three
proposed measures. These correlations confirms
our expectations: user satisfaction is higher when
the system’s understanding matches better with the
users’ requirements; when the dialog efficiency is
closer to the situation of perfect understanding; or
when the system&apos;s actions are mostly appropriate.
We suggest that these measures can serve as indi-
cators for user satisfaction.
We further use all the measures to build a re-
gression model to predict the user satisfaction
score. The prediction model is:
</bodyText>
<page confidence="0.998167">
169
</page>
<figureCaption confidence="0.999919">
Figure 1: The Improvement Curves on Dry-run Data
</figureCaption>
<figure confidence="0.985054529411765">
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
11/21/05 11/30/05 12/05/05 12/13/05 12/19/05 12/27/05 01/04/06 01/10/06 01/16/06
understandingAgreertient efficiencyRatio percentAppropriate
User Satisfaction
= 6.123*percentAppropriate
+2.854*efficiencyRatio --- (1)
+0.864*understandingAgreement - 4.67
</figure>
<bodyText confidence="0.9994253">
The R-square is 0.655, which indicates that
65.5% of the user satisfaction scores can be ex-
plained by this model. While this prediction model
has much room for improvement, we suggest that
it can be used to estimate the users’ satisfaction
scores for simulated users in the early system test-
ing stage to quickly assess the system&apos;s perform-
ance. Since the weights are tuned based on the data
from this specific application, the prediction model
may not be used directly for other domains.
</bodyText>
<subsectionHeader confidence="0.9855925">
5.2 Assessing the Differentiating Power of the
Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.999993904761905">
Since this set of evaluation measures intends to
evaluate the system&apos;s performance in the develop-
ment stage, we would like the measures to be able
to reflect small changes made in the system and to
indicate whether these changes show the right
trend of increased user satisfaction in reality. A set
of good evaluation measures should be sensible to
subtle system changes.
We assess the differentiating power of the eval-
uation measures using the dialog corpus collected
during the dry-runs. The system was tested on a
weekly basis as explained in Table 1. For each im-
provement stage, we compute the values for the
three evaluation measures averaging across all dia-
logs from all users. Figure 1 shows the three im-
provement curves based on these three measures.
The x-axis shows the first date of each improve-
ment stage; the y-axis shows the value of the eval-
uation measures. We observe that all three curves
show the right trends that indicate the system’s
improvements over the development stages.
</bodyText>
<sectionHeader confidence="0.985663" genericHeader="method">
6 Applying the Evaluation Measures on
the Simulated Corpus
</sectionHeader>
<bodyText confidence="0.999995516129032">
We train a goal and agenda driven user simulation
model from the final evaluation dialog corpus with
the real users. The simulation model interacts with
the dialog system 20 times (each time the simula-
tion model represents a different simulated user),
generating nine dialogs on all of the nine tasks
each time. In each interaction, the simulated users
generate their agenda randomly based on a uniform
distribution. The simulated corpus consists of 180
dialogs from 20 simulated users, which is of the
same size as the real user corpus. The values of the
evaluation measures are computed automatically at
the end of each simulated dialog.
We compute the estimated user satisfaction score
using Equation 1 for each simulated user. We then
compare the user satisfaction scores of the 20 si-
mulated users with the satisfaction scores of the 20
real users. The average and the standard deviation
of the user satisfaction scores for real users are
(3.79, 0.72), and the ones for simulated users are
(3.77, 1.34). Using two-tailed t-test at significance
level p&lt;0.05, we observe that there are no statisti-
cally significant differences between the two pools
of scores. Therefore, we suggest that the user satis-
faction estimated from the simulated dialog corpus
can be used to assess the system performance.
However, these average scores only offer us one
perspective in comparing the real with the simu-
lated user satisfaction. In the future, we would like
to look further into the differences between the
distributions of these user satisfaction scores.
</bodyText>
<sectionHeader confidence="0.997868" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9996872">
User simulation has been increasingly used in gen-
erating large corpora for using machine learning
techniques to automate dialog system design.
However, user simulation has not been used much
in testing dialog systems. There are two major con-
</bodyText>
<page confidence="0.987177">
170
</page>
<bodyText confidence="0.998898266666667">
cerns: 1. we are not sure how well the state-of-the-
art user simulation can mimic realistic user behav-
iors; 2. we do not get important feedback on user
satisfaction when replacing human users with
simulated users. In this study, we suggest that
while the simulated users might not be mature to
use in the final system evaluation stage, they can
be used in the early testing stages of the system
development cycle to make sure that the system is
functioning in the desired way. We further propose
a set of evaluation measures that can be extracted
from the simulation logs to assess the system per-
formance. We validate these evaluation measures
on human user dialogs and examine the differenti-
ating power of these measures. We suggest that
these measures can be used to guide the develop-
ment of the system towards improving user satis-
faction. We also apply the evaluation measures on
a simulation corpus trained from the real user dia-
logs. We show that the user satisfaction scores es-
timated on the simulated dialogs do not
significantly differ statistically from the real users’
satisfaction scores. Therefore, we suggest that the
estimated user satisfaction can be used to assess
the system performance while testing with simu-
lated users.
In the future, we would like to confirm our pro-
posed evaluation measures by testing them on dia-
log systems that allows more complicated dialog
structures and systems on other domains.
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999685">
The authors would like to thank Zhongchao
Fei, Zhe Feng, Junkuo Cao, and Baoshi Yan
for their help during the simulation system de-
velopment and the three anonymous reviewers
for their insightful suggestions. All the remain-
ing errors are ours.
</bodyText>
<sectionHeader confidence="0.999276" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999838723076923">
H. Ai, J. Tetreault, and D. Litman. 2007. Comparing
User Simulation Models for Dialog Strategy Learn-
ing. In Proc. NAACL-HLT (short paper session).
G. Chung. 2004. Developing a Flexible Spoken Dialog
System Using Simulation. In Proc. of ACL 04.
W. Eckert, E. Levin, and R. Pieraccini. 1997. User
Modeling for Spoken Dialogue System Evaluation. In
Proc. of IEEE workshop on ASRU.
E. Filisko and S. Seneff. 2006. Learning Decision Mod-
els in Spoken Dialogue Systems Via User Simulation.
In Proc. of AAAI Workshop on Statistical and Em-
pirical Approaches for Spoken Dialogue Systems.
J. Henderson, O. Lemon, and K. Georgila. 2005. Hybrid
Reinforcement/Supervised Learning for Dialogue
Policies from COMMUNICATOR data. In IJCAI
workshop on Knowledge and Reasoning in Practical
Dialogue Systems.
E. Levin, R. Pieraccini, and W. Eckert. 2000. A Stochas-
tic Model of Human-Machine Interaction For learn-
ing Dialogue Strategies. IEEE Trans. On Speech and
Audio Processing, 8(1):11-23.
R. López-Cózar, A. De la Torre, J. C. Segura and A. J.
Rubio. (2003). Assessment of dialogue systems by
means of a new simulation technique. Speech Com-
munication (40): 387-407.
D. Mirkovic and L. Cavedon. 2005. Practical multi-
domain, multi-device dialogue management,
PACLING&apos;05: 6th Meeting of the Pacific Association
for Computational Linguistics.
Sebastian Möller, Jan Krebber and Paula Smeele. 2006.
Evaluating the speech output component of a smart-
home system. Speech Communication (48): 1-27.
O. Pietquin, O. 2004. A Framework for Unsupervised
Learning of Dialog Strategies. Ph.D. diss., Faculte
Polytechnique de Mons.
K. Scheffler. 2002. Automatic Design of Spoken Dialog
Systems. Ph.D. diss., Cambridge University.
S. Singh, D. Litman, M. Kearns, and M. Walker. 2002.
Optimizing DialogueManagement with Reinforce-
ment Learning: Experiments with the NJFun System.
Journal of Artificial Intelligence Research (JAIR),
vol. 16.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye,
and Young. S. 2007. Agenda-Based User Simulation
for Bootstrapping a POMDP Dialogue System. In
Proc. of NAACL-HLT (short paper session).
F. Weng, S. Varges, B. Raghunathan, F. Ratiu, H. Pon-
Barry, B. Lathrop, Q. Zhang, H. Bratt, T. Scheideck,
R. Mishra, K. Xu, M. Purvey, A. Lien, M. Raya, S.
Peters, Y. Meng, J. Russell, L. Cavedon, E. Shri-
berg, and H. Schmidt. 2006. CHAT: A Conversa-
tional Helper for Automotive Tasks. In Proc. of
Interspeech.
F. Weng, B. Yan, Z. Feng, F. Ratiu, M. Raya, B. Lath-
rop, A. Lien, S. Varges, R. Mishra, F. Lin, M. Purver,
H. Bratt, Y. Meng, S. Peters, T. Scheideck, B. Rag-
hunathan and Z. Zhang. 2007. CHAT to your destina-
tion. In Proc. Of 8th SIGdial workshop on Discourse
and Dialogue.
J. Williams and S. Young. 2006. Partially Observable
Markov Decision Processes for Spoken Dialog Sys-
tems. Computer Speech and Language.
M. Walker, D. Litman, C. Kamm, and A. Abella. 1997.
PARADISE: A Framework for Evaluating Spoken
Dialogue Agents. In Proceedings of the 35th ACL.
</reference>
<page confidence="0.998195">
171
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.851996">
<title confidence="0.999881">User Simulation as Testing for Spoken Dialog Systems</title>
<author confidence="0.967036">Fuliang Weng</author>
<affiliation confidence="0.998744">Intelligent Systems Program Research and Technology Center University of Pittsburgh Robert Bosch LLC</affiliation>
<address confidence="0.99924">210 S. Bouquet St., Pittsburg, PA 15260 4009 Miranda Ave., Palo Alto, CA 94304</address>
<email confidence="0.917709">Hua@cs.pitt.eduFuliang.weng@us.bosch.com</email>
<abstract confidence="0.99850248">We propose to use user simulation for testing during the development of a sophisticated dialog system. While the limited behaviors of the state-of-the-art user simulation may not cover important aspects in the dialog system testing, our proposed approach extends the functionality of the simulation so that it can be used at least for the early stage testing before the system reaches stable performance for evaluation involving human users. The proposed approach includes a set of evaluation measures that can be computed automatically from the interaction logs between the user simulator and the dialog system. We first validate these measures on human user dialogs using user satisfaction scores. We also build a regression model to estimate the user satisfaction scores using these evaluation measures. Then, we apply the evaluation measures on a simulated dialog corpus trained from the real user corpus. We show that the user satisfaction scores estimated from the simulated corpus are not statistically different from the real users’ satisfaction scores.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Ai</author>
<author>J Tetreault</author>
<author>D Litman</author>
</authors>
<title>Comparing User Simulation Models for Dialog Strategy Learning.</title>
<date>2007</date>
<booktitle>In Proc. NAACL-HLT</booktitle>
<note>(short paper session).</note>
<contexts>
<context position="3575" citStr="Ai et al., 2007" startWordPosition="547" endWordPosition="550"> it has only been used in limited scope for testing specific dialog system components in the system evaluation phase (López-Cózar et al., 2003; Filisko and Seneff, 2006). This is partly because the state-of-the-art simulated users have quite limited abilities in mimicking human users&apos; behaviors and typically over-generate possible dialog behaviors. This is not a major problem when using simulated dialog corpus as the training corpus for dialog strategy learning because the over-generated simulation behaviors would only provide the machine learners with a broader dialog state space to explore (Ai et al., 2007). However, realistic user behaviors are highly desired in the testing phase because the systems are evaluated and adjusted based on the analysis of the dialogs generated in this phase. Therefore, we would ex164 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 164–171, Columbus, June 2008. c�2008 Association for Computational Linguistics pect that these user behaviors are what we will see in the final evaluation with human users. In this case, any over-generated dialog behaviors may cause the system to be blamed for untargeted functions. What is more, the simulated users</context>
</contexts>
<marker>Ai, Tetreault, Litman, 2007</marker>
<rawString>H. Ai, J. Tetreault, and D. Litman. 2007. Comparing User Simulation Models for Dialog Strategy Learning. In Proc. NAACL-HLT (short paper session).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Chung</author>
</authors>
<title>Developing a Flexible Spoken Dialog System Using Simulation.</title>
<date>2004</date>
<booktitle>In Proc. of ACL 04.</booktitle>
<contexts>
<context position="2893" citStr="Chung, 2004" startWordPosition="445" endWordPosition="446">ed users. The simulated users are built to mimic real users&apos; behaviors to some extent while allowing them to be programmed to explore unseen but still possible user behaviors. These simulated users can interact with the dialog systems to generate large amounts of training data in a low-cost and time-efficient manner. Many previous studies (Scheffler, 2002; Pietquin, 2004) have shown that the dialog strategies learned from the simulated training data outperform the hand-crafted strategies. There are also studies that use user simulation to train speech recognition and understanding components (Chung, 2004). While user simulation is largely used in dialog system training, it has only been used in limited scope for testing specific dialog system components in the system evaluation phase (López-Cózar et al., 2003; Filisko and Seneff, 2006). This is partly because the state-of-the-art simulated users have quite limited abilities in mimicking human users&apos; behaviors and typically over-generate possible dialog behaviors. This is not a major problem when using simulated dialog corpus as the training corpus for dialog strategy learning because the over-generated simulation behaviors would only provide t</context>
</contexts>
<marker>Chung, 2004</marker>
<rawString>G. Chung. 2004. Developing a Flexible Spoken Dialog System Using Simulation. In Proc. of ACL 04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Eckert</author>
<author>E Levin</author>
<author>R Pieraccini</author>
</authors>
<title>User Modeling for Spoken Dialogue System Evaluation.</title>
<date>1997</date>
<booktitle>In Proc. of IEEE workshop on ASRU.</booktitle>
<contexts>
<context position="6421" citStr="Eckert et al., 1997" startWordPosition="993" endWordPosition="996">l on a simulated dialog corpus trained from the above real user corpus, and show that the user satisfaction scores estimated from the simulated dialogs do not differ significantly from the real users’ satisfaction scores. Finally, we conclude that these evaluation measures can be used to assess the system performance based on the estimated user satisfaction. 2 User Simulation Techniques Most user simulation models are trained from dialog corpora generated by human users. Earlier models predict user actions based on simple relations between the system actions and the following user responses. (Eckert et al., 1997) first suggest a bigram model to predict the next user&apos;s action based on the previous system&apos;s action. (Levin et al., 2000) add constraints to the bigram model to accept the expected dialog acts only. However, their basic assumption of making the next user&apos;s action dependent only on the system&apos;s previous action is oversimplified. Later, many studies model more comprehensive user behaviors by adding user goals to constrain the user actions (Scheffler, 2002; Pietquin, 2004). These simulated users mimic real user behaviors in a statistical way, conditioning the user actions on the user goals and </context>
</contexts>
<marker>Eckert, Levin, Pieraccini, 1997</marker>
<rawString>W. Eckert, E. Levin, and R. Pieraccini. 1997. User Modeling for Spoken Dialogue System Evaluation. In Proc. of IEEE workshop on ASRU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Filisko</author>
<author>S Seneff</author>
</authors>
<title>Learning Decision Models in Spoken Dialogue Systems Via User Simulation.</title>
<date>2006</date>
<contexts>
<context position="3128" citStr="Filisko and Seneff, 2006" startWordPosition="481" endWordPosition="484">alog systems to generate large amounts of training data in a low-cost and time-efficient manner. Many previous studies (Scheffler, 2002; Pietquin, 2004) have shown that the dialog strategies learned from the simulated training data outperform the hand-crafted strategies. There are also studies that use user simulation to train speech recognition and understanding components (Chung, 2004). While user simulation is largely used in dialog system training, it has only been used in limited scope for testing specific dialog system components in the system evaluation phase (López-Cózar et al., 2003; Filisko and Seneff, 2006). This is partly because the state-of-the-art simulated users have quite limited abilities in mimicking human users&apos; behaviors and typically over-generate possible dialog behaviors. This is not a major problem when using simulated dialog corpus as the training corpus for dialog strategy learning because the over-generated simulation behaviors would only provide the machine learners with a broader dialog state space to explore (Ai et al., 2007). However, realistic user behaviors are highly desired in the testing phase because the systems are evaluated and adjusted based on the analysis of the d</context>
</contexts>
<marker>Filisko, Seneff, 2006</marker>
<rawString>E. Filisko and S. Seneff. 2006. Learning Decision Models in Spoken Dialogue Systems Via User Simulation.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proc. of AAAI Workshop on Statistical and Empirical Approaches for Spoken Dialogue Systems.</booktitle>
<marker></marker>
<rawString>In Proc. of AAAI Workshop on Statistical and Empirical Approaches for Spoken Dialogue Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Henderson</author>
<author>O Lemon</author>
<author>K Georgila</author>
</authors>
<title>Hybrid Reinforcement/Supervised Learning for Dialogue Policies from COMMUNICATOR data.</title>
<date>2005</date>
<booktitle>In IJCAI workshop on Knowledge and Reasoning in Practical Dialogue Systems.</booktitle>
<marker>Henderson, Lemon, Georgila, 2005</marker>
<rawString>J. Henderson, O. Lemon, and K. Georgila. 2005. Hybrid Reinforcement/Supervised Learning for Dialogue Policies from COMMUNICATOR data. In IJCAI workshop on Knowledge and Reasoning in Practical Dialogue Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Levin</author>
<author>R Pieraccini</author>
<author>W Eckert</author>
</authors>
<title>A Stochastic Model of Human-Machine Interaction For learning Dialogue Strategies.</title>
<date>2000</date>
<booktitle>IEEE Trans. On Speech and Audio Processing,</booktitle>
<pages>8--1</pages>
<contexts>
<context position="6544" citStr="Levin et al., 2000" startWordPosition="1014" endWordPosition="1017"> from the simulated dialogs do not differ significantly from the real users’ satisfaction scores. Finally, we conclude that these evaluation measures can be used to assess the system performance based on the estimated user satisfaction. 2 User Simulation Techniques Most user simulation models are trained from dialog corpora generated by human users. Earlier models predict user actions based on simple relations between the system actions and the following user responses. (Eckert et al., 1997) first suggest a bigram model to predict the next user&apos;s action based on the previous system&apos;s action. (Levin et al., 2000) add constraints to the bigram model to accept the expected dialog acts only. However, their basic assumption of making the next user&apos;s action dependent only on the system&apos;s previous action is oversimplified. Later, many studies model more comprehensive user behaviors by adding user goals to constrain the user actions (Scheffler, 2002; Pietquin, 2004). These simulated users mimic real user behaviors in a statistical way, conditioning the user actions on the user goals and the dialog contexts. More recent research defines agenda for simulated users to complete a set of settled goals (Schatzmann</context>
</contexts>
<marker>Levin, Pieraccini, Eckert, 2000</marker>
<rawString>E. Levin, R. Pieraccini, and W. Eckert. 2000. A Stochastic Model of Human-Machine Interaction For learning Dialogue Strategies. IEEE Trans. On Speech and Audio Processing, 8(1):11-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R López-Cózar</author>
<author>A De la Torre</author>
<author>J C Segura</author>
<author>A J Rubio</author>
</authors>
<title>Assessment of dialogue systems by means of a new simulation technique.</title>
<date>2003</date>
<journal>Speech Communication</journal>
<volume>40</volume>
<pages>387--407</pages>
<contexts>
<context position="3101" citStr="López-Cózar et al., 2003" startWordPosition="477" endWordPosition="480">s can interact with the dialog systems to generate large amounts of training data in a low-cost and time-efficient manner. Many previous studies (Scheffler, 2002; Pietquin, 2004) have shown that the dialog strategies learned from the simulated training data outperform the hand-crafted strategies. There are also studies that use user simulation to train speech recognition and understanding components (Chung, 2004). While user simulation is largely used in dialog system training, it has only been used in limited scope for testing specific dialog system components in the system evaluation phase (López-Cózar et al., 2003; Filisko and Seneff, 2006). This is partly because the state-of-the-art simulated users have quite limited abilities in mimicking human users&apos; behaviors and typically over-generate possible dialog behaviors. This is not a major problem when using simulated dialog corpus as the training corpus for dialog strategy learning because the over-generated simulation behaviors would only provide the machine learners with a broader dialog state space to explore (Ai et al., 2007). However, realistic user behaviors are highly desired in the testing phase because the systems are evaluated and adjusted bas</context>
</contexts>
<marker>López-Cózar, Torre, Segura, Rubio, 2003</marker>
<rawString>R. López-Cózar, A. De la Torre, J. C. Segura and A. J. Rubio. (2003). Assessment of dialogue systems by means of a new simulation technique. Speech Communication (40): 387-407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mirkovic</author>
<author>L Cavedon</author>
</authors>
<title>Practical multidomain, multi-device dialogue management, PACLING&apos;05: 6th Meeting of the Pacific Association for Computational Linguistics.</title>
<date>2005</date>
<contexts>
<context position="10110" citStr="Mirkovic and Cavedon, 2005" startWordPosition="1592" endWordPosition="1595">ically designed for users to interact with devices and receive services while performing other cognitive demanding, or primary tasks such as driving (Weng et al., 2007). CHAT deploys a combination of off-the-shelf components, components used in previous language applications, and components specifically developed as part of this project. The core components of the system include a statistical language understanding (SLU) module with multiple understanding strategies for imperfect input, an information-state-update dialog manager (DM) that handles multiple dialog threads and mixed initiatives (Mirkovic and Cavedon, 2005), a knowledge manager (KM) that controls access to ontologybased domain knowledge, and a content optimizer that connects the DM and the KM for resolving ambiguities from the users&apos; requests, regulating the amount of information to be presented to the user, as well as providing recommendations to users. In addition, we use Nuance 8.51 with dynamic grammars and classbased n-grams, for speech recognition, and Nuance Vocalizer 3.0 for text-to-speech synthesis (TTS). However, the two speech components, i.e., the recognizer and TTS are not used in the version of the system that interacts with the si</context>
</contexts>
<marker>Mirkovic, Cavedon, 2005</marker>
<rawString>D. Mirkovic and L. Cavedon. 2005. Practical multidomain, multi-device dialogue management, PACLING&apos;05: 6th Meeting of the Pacific Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Möller</author>
<author>Jan Krebber</author>
<author>Paula Smeele</author>
</authors>
<title>Evaluating the speech output component of a smarthome system.</title>
<date>2006</date>
<journal>Speech Communication</journal>
<volume>48</volume>
<pages>1--27</pages>
<marker>Möller, Krebber, Smeele, 2006</marker>
<rawString>Sebastian Möller, Jan Krebber and Paula Smeele. 2006. Evaluating the speech output component of a smarthome system. Speech Communication (48): 1-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Pietquin</author>
<author>O</author>
</authors>
<title>A Framework for Unsupervised Learning of Dialog Strategies.</title>
<date>2004</date>
<booktitle>Ph.D. diss., Faculte Polytechnique de</booktitle>
<location>Mons.</location>
<marker>Pietquin, O, 2004</marker>
<rawString>O. Pietquin, O. 2004. A Framework for Unsupervised Learning of Dialog Strategies. Ph.D. diss., Faculte Polytechnique de Mons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Scheffler</author>
</authors>
<title>Automatic Design of Spoken Dialog Systems. Ph.D. diss.,</title>
<date>2002</date>
<institution>Cambridge University.</institution>
<contexts>
<context position="2638" citStr="Scheffler, 2002" startWordPosition="407" endWordPosition="408">nd strategies. However, it is always hard to obtain training corpora that are large enough to ensure that the learned strategies are reliable. User simulation is an attempt to solve this problem by generating synthetic training corpora using computer simulated users. The simulated users are built to mimic real users&apos; behaviors to some extent while allowing them to be programmed to explore unseen but still possible user behaviors. These simulated users can interact with the dialog systems to generate large amounts of training data in a low-cost and time-efficient manner. Many previous studies (Scheffler, 2002; Pietquin, 2004) have shown that the dialog strategies learned from the simulated training data outperform the hand-crafted strategies. There are also studies that use user simulation to train speech recognition and understanding components (Chung, 2004). While user simulation is largely used in dialog system training, it has only been used in limited scope for testing specific dialog system components in the system evaluation phase (López-Cózar et al., 2003; Filisko and Seneff, 2006). This is partly because the state-of-the-art simulated users have quite limited abilities in mimicking human </context>
<context position="6880" citStr="Scheffler, 2002" startWordPosition="1068" endWordPosition="1069">uman users. Earlier models predict user actions based on simple relations between the system actions and the following user responses. (Eckert et al., 1997) first suggest a bigram model to predict the next user&apos;s action based on the previous system&apos;s action. (Levin et al., 2000) add constraints to the bigram model to accept the expected dialog acts only. However, their basic assumption of making the next user&apos;s action dependent only on the system&apos;s previous action is oversimplified. Later, many studies model more comprehensive user behaviors by adding user goals to constrain the user actions (Scheffler, 2002; Pietquin, 2004). These simulated users mimic real user behaviors in a statistical way, conditioning the user actions on the user goals and the dialog contexts. More recent research defines agenda for simulated users to complete a set of settled goals (Schatzmann et al., 2007). This type of simulated user updates the agenda and the current goal based on the changes of the dialog states. In this study, we build a simulated user similar to (Schatzmann et al., 2007) in which the simulated user keeps a list of its goals and another agenda of actions to complete the goals. In our restaurant select</context>
</contexts>
<marker>Scheffler, 2002</marker>
<rawString>K. Scheffler. 2002. Automatic Design of Spoken Dialog Systems. Ph.D. diss., Cambridge University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Singh</author>
<author>D Litman</author>
<author>M Kearns</author>
<author>M Walker</author>
</authors>
<title>Optimizing DialogueManagement with Reinforcement Learning: Experiments with the NJFun System.</title>
<date>2002</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<volume>16</volume>
<contexts>
<context position="1718" citStr="Singh et al., 2002" startWordPosition="257" endWordPosition="260">n measures. Then, we apply the evaluation measures on a simulated dialog corpus trained from the real user corpus. We show that the user satisfaction scores estimated from the simulated corpus are not statistically different from the real users’ satisfaction scores. 1 Introduction Spoken dialog systems are being widely used in daily life. The increasing demands of such systems require shorter system development cycles and better automatic system developing techniques. As a result, machine learning techniques are applied to learn dialog strategies automatically, such as reinforcement learning (Singh et al., 2002; Williams &amp; Young, 2007), supervised learning (Henderson et * This study was conducted when the author was an intern at Bosch RTC. al., 2005), etc. These techniques require a significant amount of training data for the automatic learners to sufficiently explore the vast space of possible dialog states and strategies. However, it is always hard to obtain training corpora that are large enough to ensure that the learned strategies are reliable. User simulation is an attempt to solve this problem by generating synthetic training corpora using computer simulated users. The simulated users are bui</context>
</contexts>
<marker>Singh, Litman, Kearns, Walker, 2002</marker>
<rawString>S. Singh, D. Litman, M. Kearns, and M. Walker. 2002. Optimizing DialogueManagement with Reinforcement Learning: Experiments with the NJFun System. Journal of Artificial Intelligence Research (JAIR), vol. 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S</author>
</authors>
<title>Agenda-Based User Simulation for Bootstrapping a POMDP Dialogue System. In</title>
<date>2007</date>
<booktitle>Proc. of NAACL-HLT</booktitle>
<note>(short paper session).</note>
<marker>S, 2007</marker>
<rawString>J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and Young. S. 2007. Agenda-Based User Simulation for Bootstrapping a POMDP Dialogue System. In Proc. of NAACL-HLT (short paper session).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Weng</author>
<author>S Varges</author>
<author>B Raghunathan</author>
<author>F Ratiu</author>
<author>H PonBarry</author>
<author>B Lathrop</author>
<author>Q Zhang</author>
<author>H Bratt</author>
<author>T Scheideck</author>
<author>R Mishra</author>
<author>K Xu</author>
<author>M Purvey</author>
<author>A Lien</author>
<author>M Raya</author>
<author>S Peters</author>
<author>Y Meng</author>
<author>J Russell</author>
<author>L Cavedon</author>
<author>E Shriberg</author>
<author>H Schmidt</author>
</authors>
<title>CHAT: A Conversational Helper for Automotive Tasks.</title>
<date>2006</date>
<booktitle>In Proc. of Interspeech.</booktitle>
<contexts>
<context position="9289" citStr="Weng et al., 2006" startWordPosition="1473" endWordPosition="1476">fication, the simulated user will satisfy the system’s request 165 before moving on to discuss a new constraint. The simulated user updated the agenda with the new actions after each user turn. The current simulated user interacts with the system on the word level. It generates a string of words by instantiating its current action using predefined templates derived from previously collected corpora with real users. Random lexical errors are added to simulate a spoken language understanding performance with a word error rate of 15% and a semantic error rate of 11% based on previous experience (Weng et al., 2006). 3 System and Corpus CHAT (Conversational Helper for Automotive Tasks) is a spoken dialog system that supports navigation, restaurant selection and mp3 player applications. The system is specifically designed for users to interact with devices and receive services while performing other cognitive demanding, or primary tasks such as driving (Weng et al., 2007). CHAT deploys a combination of off-the-shelf components, components used in previous language applications, and components specifically developed as part of this project. The core components of the system include a statistical language u</context>
</contexts>
<marker>Weng, Varges, Raghunathan, Ratiu, PonBarry, Lathrop, Zhang, Bratt, Scheideck, Mishra, Xu, Purvey, Lien, Raya, Peters, Meng, Russell, Cavedon, Shriberg, Schmidt, 2006</marker>
<rawString>F. Weng, S. Varges, B. Raghunathan, F. Ratiu, H. PonBarry, B. Lathrop, Q. Zhang, H. Bratt, T. Scheideck, R. Mishra, K. Xu, M. Purvey, A. Lien, M. Raya, S. Peters, Y. Meng, J. Russell, L. Cavedon, E. Shriberg, and H. Schmidt. 2006. CHAT: A Conversational Helper for Automotive Tasks. In Proc. of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Weng</author>
<author>B Yan</author>
<author>Z Feng</author>
<author>F Ratiu</author>
<author>M Raya</author>
<author>B Lathrop</author>
<author>A Lien</author>
<author>S Varges</author>
<author>R Mishra</author>
<author>F Lin</author>
<author>M Purver</author>
<author>H Bratt</author>
<author>Y Meng</author>
<author>S Peters</author>
<author>T Scheideck</author>
<author>B Raghunathan</author>
<author>Z Zhang</author>
</authors>
<title>CHAT to your destination.</title>
<date>2007</date>
<booktitle>In Proc. Of 8th SIGdial workshop on Discourse and Dialogue.</booktitle>
<contexts>
<context position="9651" citStr="Weng et al., 2007" startWordPosition="1528" endWordPosition="1531">rived from previously collected corpora with real users. Random lexical errors are added to simulate a spoken language understanding performance with a word error rate of 15% and a semantic error rate of 11% based on previous experience (Weng et al., 2006). 3 System and Corpus CHAT (Conversational Helper for Automotive Tasks) is a spoken dialog system that supports navigation, restaurant selection and mp3 player applications. The system is specifically designed for users to interact with devices and receive services while performing other cognitive demanding, or primary tasks such as driving (Weng et al., 2007). CHAT deploys a combination of off-the-shelf components, components used in previous language applications, and components specifically developed as part of this project. The core components of the system include a statistical language understanding (SLU) module with multiple understanding strategies for imperfect input, an information-state-update dialog manager (DM) that handles multiple dialog threads and mixed initiatives (Mirkovic and Cavedon, 2005), a knowledge manager (KM) that controls access to ontologybased domain knowledge, and a content optimizer that connects the DM and the KM fo</context>
</contexts>
<marker>Weng, Yan, Feng, Ratiu, Raya, Lathrop, Lien, Varges, Mishra, Lin, Purver, Bratt, Meng, Peters, Scheideck, Raghunathan, Zhang, 2007</marker>
<rawString>F. Weng, B. Yan, Z. Feng, F. Ratiu, M. Raya, B. Lathrop, A. Lien, S. Varges, R. Mishra, F. Lin, M. Purver, H. Bratt, Y. Meng, S. Peters, T. Scheideck, B. Raghunathan and Z. Zhang. 2007. CHAT to your destination. In Proc. Of 8th SIGdial workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Williams</author>
<author>S Young</author>
</authors>
<title>Partially Observable Markov Decision Processes for Spoken Dialog Systems. Computer Speech and Language.</title>
<date>2006</date>
<marker>Williams, Young, 2006</marker>
<rawString>J. Williams and S. Young. 2006. Partially Observable Markov Decision Processes for Spoken Dialog Systems. Computer Speech and Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>D Litman</author>
<author>C Kamm</author>
<author>A Abella</author>
</authors>
<title>PARADISE: A Framework for Evaluating Spoken Dialogue Agents.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th ACL.</booktitle>
<contexts>
<context position="18947" citStr="Walker et al., 1997" startWordPosition="2991" endWordPosition="2994"> over the entire dialog is calculated by averaging the percent agreement after each turn. In this example, understandingAgreement is (2/3 + 2.5/3 + 1)/3 =83.3%. We hypothesize that the higher the understandingAgreement is, the better the system performs, and thus the more the user is satisfied. The matches of understandings can be calculated automatically from the user simulation and the system logs. However, since we work with human users&apos; dialogs in the first part of this study, we manually annotated the semantic contents (e.g., cuisine name) in the real user corpus. Previous studies (E.g., Walker et al., 1997) use a corpus level semantic accuracy measure (semanticAccuracy) to capture the system’s understanding ability. SemanticAccuracy is defined in the standard way as the total number of correctly understood constraints divided by the total number of constraints mentioned in the entire dialog. The understandingAgreement measure we introduce here is essentially the averaged per-sentence semantic accuracy, which emphasizes the utterance level perception rather than a single corpus level average. The intuition behind this new measure is that it is better for the system to always understand something </context>
</contexts>
<marker>Walker, Litman, Kamm, Abella, 1997</marker>
<rawString>M. Walker, D. Litman, C. Kamm, and A. Abella. 1997. PARADISE: A Framework for Evaluating Spoken Dialogue Agents. In Proceedings of the 35th ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>