<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.997719">
Unsupervised Dependency Parsing with Acoustic Cues
</title>
<author confidence="0.924076">
John K Pate†$ Sharon Goldwater†
</author>
<email confidence="0.816463">
j.k.pate@sms.ed.ac.uk sgwater@inf.ed.ac.uk
</email>
<author confidence="0.55281">
†ILCC, School of Informatics $Department of Computing
</author>
<affiliation confidence="0.974597">
University of Edinburgh Macquarie University
</affiliation>
<address confidence="0.609704">
Edinburgh, EH8 9AB, UK Sydney, NSW 2109, Australia
</address>
<sectionHeader confidence="0.989264" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99969885">
Unsupervised parsing is a difficult task that
infants readily perform. Progress has been
made on this task using text-based models, but
few computational approaches have considered
how infants might benefit from acoustic cues.
This paper explores the hypothesis that word
duration can help with learning syntax. We de-
scribe how duration information can be incor-
porated into an unsupervised Bayesian depen-
dency parser whose only other source of infor-
mation is the words themselves (without punc-
tuation or parts of speech). Our results, evalu-
ated on both adult-directed and child-directed
utterances, show that using word duration can
improve parse quality relative to words-only
baselines. These results support the idea that
acoustic cues provide useful evidence about
syntactic structure for language-learning in-
fants, and motivate the use of word duration
cues in NLP tasks with speech.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999824418604651">
Unsupervised learning of syntax is difficult for NLP
systems, yet infants perform this task routinely. Pre-
vious work in NLP has focused on using the implicit
syntactic information available in part-of-speech
(POS) tags (Klein and Manning, 2004), punctuation
(Seginer, 2007; Spitkovsky et al., 2011b; Ponvert et
al., 2011), and syntactic similarities between related
languages (Cohen and Smith, 2009; Cohen et al.,
2011). However, these approaches likely use the data
in a very different way from children: neither POS
tags nor punctuation are observed during language
acquisition (although see Spitkovsky et al. (2011a)
and Christodoulopoulos et al. (2012) for encourag-
ing results using unsupervised POS tags), and many
children learn in a broadly monolingual environment.
This paper explores a possible source of information
that NLP systems typically ignore: word duration, or
the length of time taken to pronounce each word.
There are good reasons to think that word dura-
tion might be useful for learning syntax. First, the
well-established Prosodic Bootstrapping hypothesis
(Gleitman and Wanner, 1982) proposes that infants
use acoustic-prosodic cues (such as word duration)
to help them identify syntactic structure, because
prosodic and syntactic structures sometimes coincide.
More recently, we proposed (Pate and Goldwater,
2011) that infants might use word duration as a di-
rect cue to syntactic structure (i.e., without requir-
ing intermediate prosodic structure), because words
in high-probability syntactic structures tend to be
pronounced more quickly (Gahl and Garnsey, 2004;
Gahl et al., 2006; Tily et al., 2009).
Like most recent work on unsupervised parsing,
we focus on learning syntactic dependencies. Our
work is based on Headden et al. (2009)’s Bayesian
version of the Dependency Model with Valence
(DMV) (Klein and Manning, 2004), using interpo-
lated backoff techniques to incorporate multiple infor-
mation sources per token. However, whereas Head-
den et al. used words and POS tags as input, we
use words and word duration information, presenting
three variants of their model that use this information
in slightly different ways.1
</bodyText>
<footnote confidence="0.957164333333333">
1By using neither gold-standard nor learned POS tags as
input, our work differs from nearly all previous work on unsuper-
vised dependency parsing. While learned tags might be plausible
</footnote>
<page confidence="0.993844">
63
</page>
<bodyText confidence="0.95854125">
Transactions of the Association for Computational Linguistics, 1 (2013) 63–74. Action Editor: Brian Roark.
Submitted 9/2012; Published 3/2013. c�2013 Association for Computational Linguistics.
To our knowledge, this is the first work to incor-
porate acoustic cues into an unsupervised system for
learning full syntactic parses. The methods in this
paper were inspired by our previous approach (Pate
and Goldwater, 2011), which showed that word dura-
tion measurements could improve the performance
of an unsupervised lexicalized syntactic chunker over
a words-only baseline. However, that work was lim-
ited to HMM-like sequence models, tested on adult-
directed speech (ADS) only, and none of the models
outperformed uniform-branching baselines. Here, we
extend our results to full dependency parsing, and
experiment on transcripts of both spontaneous ADS
and child-directed speech (CDS). Our models us-
ing word duration outperform words-only baselines,
along with the Common Cover Link parser of Seginer
(2007), and the Unsupervised Partial Parser of Pon-
vert et al. (2011), unsupervised lexicalized parsers
that have obtained state-of-the-art results on standard
newswire treebanks (though their performance here
is worse, as our input lacks punctuation). We also
outperform uniform-branching baselines.
</bodyText>
<sectionHeader confidence="0.975267" genericHeader="introduction">
2 Syntax and Word Duration
</sectionHeader>
<bodyText confidence="0.9999862">
Before presenting our models and experiments, we
first discuss why word duration might be a useful cue
to syntax. This section reviews the two possible rea-
sons mentioned above: duration as a cue to prosodic
structure, or as a cue to predictability.
</bodyText>
<subsectionHeader confidence="0.977154">
2.1 Prosodic Bootstrapping
</subsectionHeader>
<bodyText confidence="0.99944382">
Prosody is the structure of speech as conveyed by
rhythm and intonation, which are, in turn, conveyed
by such measurable phenomena as variation in fun-
damental frequency, word duration, and spectral tilt.
Prosodic structure is typically analyzed as imposing
a shallow, hierarchical grouping structure on speech,
with the ends of prosodic phrases (constituents) be-
ing cued in part by lengthening the last word of the
phrase (Beckman and Pierrehumbert, 1986).
The Prosodic Bootstrapping hypothesis (Gleit-
man and Wanner, 1982) points out that prosodic
phrases are often also syntactic phrases, and proposes
that language-acquiring infants exploit this correla-
tion. Specifically, if infants can learn about prosodic
phrase structure using word duration (and fundamen-
in a model of language acquisition, gold tags certainly are not.
tal frequency), they may be able to identify syntactic
phrases more easily using word strings and prosodic
trees than using word strings alone.
Several behavioral experiments support the con-
nection between prosody and syntax and the prosodic
bootstrapping hypothesis specifically. For example,
there is evidence that adults use prosodic information
for syntactic disambiguation (Millotte et al., 2007;
Price et al., 1991) and to help in learning the syntax
of an artificial language (Morgan et al., 1987), while
infants can use acoustic-prosodic cues for utterance-
internal clause segmentation (Seidl, 2007).
On the computational side, we are aware of only
our previous HMM-based chunkers (Pate and Gold-
water, 2011), which learned shallow syntax from
words, words and word durations, or words and hand-
annotated prosody. Using these chunkers, we found
that using words plus prosodic annotation worked
better than just words, and words plus word duration
worked even better. While these results are consistent
with the prosodic bootstrapping hypothesis, we sug-
gested that predictability bootstrapping (see below)
might be a more plausible explanation.
Other computational work has combined prosody
with syntax, but only in supervised systems, and typi-
cally using hand-annotated prosodic information. For
example, Huang and Harper (2010) used annotated
prosodic breaks as a kind of punctuation in a su-
pervised PCFG, while prosodic breaks learned in a
semi-supervised way have been used as features for
parse reranking (Kahn et al., 2005) or PCFG state-
splitting (Dreyer and Shafran, 2007). In contrast to
these methods, our approach observes neither parse
trees nor prosodic annotations.
</bodyText>
<subsectionHeader confidence="0.999726">
2.2 Predictability Bootstrapping
</subsectionHeader>
<bodyText confidence="0.999627583333333">
On the basis of our HMM chunkers, we introduced
the predictability bootstrapping hypothesis (Pate and
Goldwater, 2011): the idea that word durations could
be a useful cue to syntactic structure not (or not only)
because they provide information about prosodic
structure, but because they are a direct cue to syntac-
tic predictability. It is well-established that talkers
tend to pronounce words more quickly when they
are more predictable, as measured by, e.g., word
frequency, n-gram probability, or whether or not the
word has been previously mentioned (Aylett and Turk,
2004; Bell et al., 2009). However, syntactic proba-
</bodyText>
<page confidence="0.998842">
64
</page>
<figureCaption confidence="0.9094455">
you threw it right at the basket
Figure 1: Example unlabeled dependency parse.
</figureCaption>
<bodyText confidence="0.999925321428571">
bility also seems to matter, with studies showing that
verbs tend to be pronounced more quickly when they
are in their preferred syntactic frame—transitive vs.
intransitive or direct object vs. sentential comple-
ment (Gahl and Garnsey, 2004; Gahl et al., 2006;
Tily et al., 2009). While this syntactic evidence is
only for verbs, together with the evidence that effects
of other notions of predictability, it suggests that such
syntactic effects may also be widespread. If so, the
duration of a word could give clues as to whether it
is being used in a high-probability or low-probability
structure, and thus what the correct structure is.
We found that our syntactic chunkers benefited
more from duration information than prosodic an-
notations, providing some preliminary evidence in
favor of predictability bootstrapping, but not ruling
out prosodic bootstrapping. So, we are left with two
plausible mechanisms by which word duration could
help with learning syntax. Slow pronunciations may
cue the end of a prosodic phrase, which is sometimes
also the end of a syntactic phrase. Alternatively, slow
pronunciations may indicate that the hidden syntactic
structure is low probability, facilitating the induc-
tion of a probabilistic grammar. This paper will not
seek to determine which mechanism is useful, instead
taking the presence of two possible mechanisms as
encouraging for the prospect of incorporating word
duration into unsupervised parsing.
</bodyText>
<sectionHeader confidence="0.905963" genericHeader="method">
3 Models2
</sectionHeader>
<bodyText confidence="0.999875">
As mentioned, we will be incorporating word dura-
tion into unsupervised dependency parsing, produc-
ing analyses like the one in Figure 1. Each arc is
between two words, with the head at the non-arrow
end of the arc, and the dependent at the arrow end.
One word, the root, depends on no word, and all
other words depend on exactly one word. Following
previous work on unsupervised dependency parsing,
we will not label the arcs.
</bodyText>
<footnote confidence="0.931998">
2The implementation of these models is available at
http://github.com/jpate/predictabilityParsing
</footnote>
<subsectionHeader confidence="0.914759">
3.1 Dependency Model with Valence
</subsectionHeader>
<bodyText confidence="0.999985606060606">
All of our models are ultimately based on the De-
pendency Model with Valence (DMV) of Klein and
Manning (2004), a generative, probabilistic model
for projective (i.e. no crossing arcs), unlabeled de-
pendency parses, such as the one in Figure 1.
The DMV generates dependency parses using
three probability distributions, which together com-
prise model parameters 0. First, the root of the
sentence is drawn from Proot. Second, we decide
whether to stop generating dependents of the head
h in direction dir E {left, right} with probability
Pstop(·|h, dir, v), where v is T if h has a dir-ward
dependent and F otherwise. If we decide to stop,
then h takes no more dependents in the direction of
dir. If we don’t stop, we use the third probability
distribution Pchoose(d|h, dir) to determine which de-
pendent d to generate. The second and third step
repeat for each generated word until all words have
stopped generating in both directions.
The DMV was the first unsupervised parsing
model to outperform a uniform-branching baseline
on the Wall Street Journal corpus. It was trained
using EM to obtain a maximum-likelihood estimate
of the parameters 0, and learned from POS tags to
avoid rare events. However, all work on syntactic
predictability effects on word duration has been lexi-
calized (looking at, e.g., the transitivity bias of par-
ticular verbs). In addition, it is unlikely that children
have access to the correct parts of speech when first
learning syntactic structure. Thus, we want a DMV
variant that learns from words rather than POS tags.
We therefore adopt several extensions to the DMV
due to Headden et al. (2009), described next.
</bodyText>
<subsectionHeader confidence="0.996291">
3.2 The DMV with Backoff
</subsectionHeader>
<bodyText confidence="0.998857545454545">
Headden et al. (2009) sought to improve the DMV by
incorporating lexical information in addition to POS
tags. However, arcs between particular words are
rare, so they modified the DMV in two ways to deal
with this sparsity. First, they switched from MLE to a
Bayesian approach, estimating a probability distribu-
tion over model parameters 0 and dependency trees
T given the training corpus C and a prior distribution
α over models: P(T, 0|C, α).
Headden et al. avoided overestimating the proba-
bility of rare events that happen to occur in the train-
</bodyText>
<page confidence="0.999273">
65
</page>
<bodyText confidence="0.999734545454546">
ing data by picking α to assign low probability to
models θ which give high probability to rare events.
Accordingly, models that overcommit to rare events
will contribute little to the final average over models.
Specifically, Headden et al. use Dirichlet priors, with
α being the Dirichlet hyperparameters.
Headden et al.’s second innovation was to adapt in-
terpolated backoff methods from language modeling
with n-grams, where one can estimate the probabil-
ity of word wn given word wn−1 by interpolating
between unigram and bigram probability estimates:
</bodyText>
<equation confidence="0.997643">
Pˆ(wn|wn−1) _ λP(wn|wn−1) + (1 − λ)P(wn)
</equation>
<bodyText confidence="0.972471104166667">
with λ ∈ [0, 1]. Ideally, λ should be large when wn−1
is frequent, and small when wn−1 is rare. Headden et
al. (2009) apply this method to the DMV by backing
off from Choose and Stop distributions that condition
on both head word and POS to distributions that
condition on only the head POS.
In the equation above, λ is a scalar parameter.
However, it actually specifies a probability distri-
bution over the decision to back off (B) or not back
off (¬B), and we can use different notation to reflect
this view. Specifically, λstop(·) and λchoose(·) will
represent our backoff distributions for the Stop and
Choose decision, respectively. Using hp and dp to
represent head and dependent POS tag and hw and
dw to represent head and dependent word, one of the
models Headden et al. explored estimates:
Pˆchoose(dp|hw, hp, dir, val) _
λchoose(¬B|hw, hp, dir)Pchoose(dp|hw, hp, dir)
+λchoose(B|hw, hp, dir) Pchoose(dp|hp, dir) (1)
with an analogous backoff for Pstop. We can see
from Equation 1 that ˆPchoose backs off from a dis-
tribution that conditions on hw to a distribution that
marginalizes out hw, and that the extent of backoff
varies across hw; we can use this to back off more
when we have less evidence about hw. This model
only conditions on words; it does not generate them
in the dependents. This means it is actually a condi-
tional, rather than fully generative, model of observed
POS tags and unobserved syntax conditioned on the
observed words.
Since identifying the true posterior distribution
P(T, θ|C, α) is intractable, Headden et al. use Mean-
field Variational Bayes (Kurihara and Sato, 2006;
Johnson, 2007), which finds an approximation to the
posterior using an iterative EM-like algorithm. In the
E-step of VBEM, expected counts E(ri) are gathered
for each latent variable using the Inside-Outside algo-
rithm, exactly as in the E-step of traditional EM. The
Maximization step differs from the M-Step of EM in
two ways. First, the expected counts for each value
of the latent variable ri are incremented by the hy-
perparameter αi. Second, the numerator and denom-
inator are scaled by the function exp(ψ(·)), which
reduces the probability of rare events. Specifically,
the Pchoose distribution is estimated using expecta-
tions for each arc adp,h,dir from head h to dependent
POS tag dp in direction dir, and the update equation
for Pchoose from iteration n to n + 1 is:
</bodyText>
<equation confidence="0.999214">
Pˆ n+1
choose(dp|h, dir) _
exp(ψ(En(adp,h,dir) + αdp,h,dir)) (2)
</equation>
<bodyText confidence="0.9974226">
where h is the head POS tag for the backoff distri-
bution, and the head (word, POS) pair for the no
backoff distribution. The update equation for Pstop
is analogous.
Now consider the update equations for λchoose:
</bodyText>
<equation confidence="0.99727825">
ˆλn+1
choose(¬B|hw, hp, dir) _
exp(ψ(α,B + Ec(En(ac,hw,hp,dir))))
exp(ψ(αB + α,B + Ec(En(ac,hw,hp,dir))))
ˆλn+1
choose(B|hw, hp, dir) _
exp(ψ(αB))
exp(ψ(αB + α,B + Ec(En(ac,hw,hp,dir))))
</equation>
<bodyText confidence="0.9998712">
Only the ¬B numerator includes the expected counts,
so as we see hw in direction dir more often, the ¬B
numerator will swamp the B numerator. By picking
αB larger than α,B, we can bias our λ distribution to
prefer backing off until we expect at least αB − α,B
arcs out of hw with tag hp in the direction of dir.
To obtain good performance, Headden et al. re-
placed each word that appeared fewer than 100 times
in the training data with the token “UNK.” We will
also use such an UNK cutoff.
</bodyText>
<subsectionHeader confidence="0.999043">
3.3 DMV with Duration
</subsectionHeader>
<bodyText confidence="0.9999715">
We explore three models. One is a straightforward
application of the DMV with Backoff to words and
</bodyText>
<equation confidence="0.944319">
exp(ψ(Ec(En(ac,h,dir) + αc,h,dir)))
</equation>
<page confidence="0.764316">
66
</page>
<bodyText confidence="0.9480815625">
(quantized) word duration, and the other two are fully-
generative variants. We also consider using words
and POS tags as input to these models. Backoff mod-
els are given two streams of information, providing
two of word identity, POS tag, or word duration for
each observed token. We call one stream the “back-
off” stream, and the other the “extra” stream. Backoff
models learn a probability distribution conditioning
on both streams, backing off to condition on only the
backoff stream.
Our first words and duration model takes the du-
ration as the extra stream and the word identity as
the backoff stream, and, using ha to represent the
acoustic information for the head, defines:
Pˆchoose(dw|hw, ha, dir) _
Achoose(¬B|hw, ha, dir)Pchoose(dw|hw, ha, dir)
+Achoose(B|hw, ha, dir)Pchoose(dw|hw, dir) (3)
with an analogous backoff scheme for Pstop. We will
refer to this conditional model as “Cond.” in our
experiments. This equation is similar to Equation 1,
except it uses words and duration instead of words
and POS tags, and backs off to, not away from, words.
We back off to the sparse words, rather than the less
sparse duration, because duration provides almost no
information about syntax in isolation.3
Directly modelling the extra stream among the
dependents may allow us to capture selectional re-
strictions in POS and words models, or exploit ef-
fects of syntactic predictability on dependent dura-
tion. We therefore explore variants that generate both
streams in the dependents. First, we examine a model
(“Joint”) that generates them jointly:
</bodyText>
<equation confidence="0.5545442">
ˆPchoose(dw, da|hw, hp, dir) _
Achoose(¬B|hw, ha, dir)
Pchoose(dw, da|hw, ha, dir)
+Achoose(B|hw, ha, dir)
Pchoose(dw, da|hw, dir) (4)
</equation>
<bodyText confidence="0.8008585">
However, this joint model will have a very large state-
space and may suffer from the same data sparsity, so
we also explore a model (“Indep.”) that generates the
3Preliminary dev-set experiments confirmed this intuition, as
models that backed off to word duration performed poorly.
extra and backoff independently:
</bodyText>
<construct confidence="0.5791285">
ˆPchoose(dw, da|hw, hp, dir) _
Achoose(¬B|hw, ha, dir)
Pchoose backoff (dw|hw, ha, dir)
Pchoose extra(da|hw, ha, dir)
+ Achoose(B|hw, ha, dir)
Pchoose backoff (dw|hw, dir)
</construct>
<equation confidence="0.519243">
Pchoose extra(da|hw, dir) (5)
</equation>
<bodyText confidence="0.999551904761905">
We also modified the DMV with Backoff to handle
heavily lexicalized models. In Headden et al. (2009),
arcs between words that never appear in the same
sentence are given probability mass only by virtue
of the backoff distribution to POS tags, which all
appear in the same sentence at least once. We want to
avoid relying on POS tags, and we also want to use
held-out development and test sets to avoid implicitly
overfitting the data when exploring different model
structures. To this end, we add one extra αUNKhyper-
parameter to the Dirichlet prior of Pchoose for each
combination of conditioning events. This hyperpa-
rameter reserves probability mass for a head h to take
a word dw as a dependent if h and dw never appeared
together in the training data. The amount of probabil-
ity mass reserved decreases as we see hw more often.
This is implemented in training by adding αUNK to
the denominator of the Pchoose update equation for
each h and dir. At test time, if a word dw appears
as an unseen dependent for head h, h takes dw as a
dependent with probability:
</bodyText>
<equation confidence="0.999359">
Pˆchoose(dw|h, dir) _ (6)
exp(ψ(
exp(ψ(αUNK + Ec(Elast(rc,h,dir) + αc,h,dir)))
</equation>
<bodyText confidence="0.999839666666667">
Here, h may be a word, (word, POS) pair, or (word,
duration) pair. Since this event by definition never
occurs in the training data, αUNK does not appear in
the numerator during training. 4
Finally, the conditional model ignores the extra
stream in Proot, and the generative models estimate
</bodyText>
<footnote confidence="0.571974">
4Note also that αUNK is different from a global UNK cutoff,
which is imposed in preprocessing, and so effects every occur-
rence of an an UNK’d word in the model. αUNK affects only
dependents in Pchoose, and treats a dependent as UNK iff it did
not occur on that particular side of that particular head word in
any sentence. We used both global UNK cutoffs (optimized on
the dev set) and these αUNK hyperparameters.
</footnote>
<equation confidence="0.636642">
αUNK))
</equation>
<page confidence="0.915269">
67
</page>
<table confidence="0.9998836">
Train Dev Test
Word tokens 42,505 1,765 2,571
Word types 7,804 818 1,134
Sentences 6,007 233 357
Word tokens 24,998 2,980 3,052
Word types 2,647 760 767
Sentences 3,998 488 491
Word tokens 20,954 2,127 2,206
Word types 1,390 482 488
Sentences 6,249 424 449
</table>
<tableCaption confidence="0.996278">
Table 1: Statistics for our three corpora.
</tableCaption>
<bodyText confidence="0.9237815">
Proot over both streams jointly and independently,
respectively.
</bodyText>
<sectionHeader confidence="0.999374" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.820358">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999934">
We evaluate on three datasets: wsj10, sentences of
length 10 or less from the Wall Street Journal por-
tion of the Penn Treebank; swbdnxt10, sentences
of length 10 or less from the Switchboard dataset
of ADS used by Pate and Goldwater (2011); and
brent, part of the Brent corpus of CDS (Brent and
Siskind, 2001). Table 1 presents corpus statistics.
</bodyText>
<equation confidence="0.494523">
4.1.1 wsj10
</equation>
<bodyText confidence="0.9999246">
We present a new evaluation of the DMV with
Backoff on wsj10, which does not have any acous-
tic information, simply to verify that αoNK performs
sensibly on a standard corpus. Additionally, Headden
et al. (2009) use an intensive initializer that relies on
dozens of random restarts, and so, strictly speaking,
only show that the backoff technology is useful for
good initializations. Our new evaluation will show
that the backoff technology provides a substantial
benefit even for harmonic initialization.
wsj10 was created in the standard way; all punc-
tuation and traces were removed, and sentences con-
taining more than ten tokens were discarded. For
our fully lexicalized version of wsj10, all words
were lowercased, and numbers were replaced with
the token “NUMBER.”5 Following standard practice,
we used sections 2-21 for training, section 22 for
development, and section 23 for test. wsj10 con-
tains hand-annotated constituency parses, not depen-
dency parses, so we used the standard “constituency-
</bodyText>
<footnote confidence="0.489717">
5Numbers were treated in this way only in wsj10.
</footnote>
<bodyText confidence="0.960690666666667">
to-dependency” conversion tool of Johansson and
Nugues (2007) to obtain high-quality CoNLL-style
dependency parses.
</bodyText>
<equation confidence="0.57534">
4.1.2 swbdnxt10
</equation>
<bodyText confidence="0.999930571428571">
Next, we evaluate on swbdnxt10, which con-
tains all sentences up to length 10 from the same
sections of the swbdnxt version of Switchboard
used by Pate and Goldwater (2011). Short sentences
are usually formulaic discourse responses (e.g. “oh
ok”), so this dataset also excludes sentences shorter
than three words. As our models successfully use
word durations, this evaluation provides an important
replication of the basic result from Pate and Goldwa-
ter (2011) with a different kind of syntactic model.
swbdnxt10 has a forced alignment of a
dictionary-based phonetic transcription of each ut-
terance to audio, providing our word duration infor-
mation. As a very simple model of hyper-articulation
and hypo-articulation, we classify a word as in the
longest third duration, shortest third, or middle third.
To minimize effects of word form, this classification
was based on vowel count (counting a diphthong as
one vowel): each word with n vowels is classified as
in the shortest, longest, or middle tercile of duration
among words with n vowels.
Like wsj10, swbdnxt10 is annotated only
with constituency parses, so to provide approximate
“gold-standard” dependencies, we used the same
constituency-to-dependency conversion tool as for
wsj10. We evaluated 200 randomly-selected sen-
tences to check the accuracy of the conversion tool,
which was designed for newspaper text. Excluding
arcs involving words with no clear role in depen-
dency structure (such as “um”), about 86% of the
arcs were correct. While this rate is uncomfortably
low, it is still much higher than unsupervised depen-
dency parsers typically achieve, and so may provide
a reasonable measure of relative dependency parse
quality among competing systems.
</bodyText>
<subsectionHeader confidence="0.531897">
4.1.3 brent
</subsectionHeader>
<bodyText confidence="0.996915333333333">
We also evaluated our models on the “Large Brent”
dataset introduced in Rytting et al. (2010), a por-
tion of the Brent corpus of child-directed speech
(Brent and Siskind, 2001). We call this corpus
brent. It consists of utterances from four of the
mothers in Brent and Siskind’s (2001) study, and, like
</bodyText>
<figure confidence="0.812101333333333">
wsj10
swbdnxt10
brent
</figure>
<page confidence="0.996344">
68
</page>
<bodyText confidence="0.999550619047619">
swbdnxt10, has a forced alignment from which we
obtain duration terciles. Rytting et al. (2010) used
a 90%/10% train/test partition. We extracted every
ninth utterance from the original training partition to
create a dev set, producing an 80%/10%/10% parti-
tion. We also separated clitics from their base word.
This dataset only has 186 sentences longer than ten
words, with a maximum length of 22 words, so we
discarded only sentences shorter than three words
from the evaluation sets.
The Brent corpus is distributed via CHILDES
(MacWhinney, 2000) with automatic dependency an-
notations. However, these are not hand-corrected,
and rely on a different tokenization of the dataset
than is present on the transcription tier. To produce a
reliable gold-standard,6 we annotated all sentences of
length 2 or greater from the development and test sets
with dependencies drawn from the Stanford Typed
Dependency set (de Marneffe and Manning, 2008)
using the annotation tool used for the Copenhagen
Dependency Treebank (Kromann, 2003).
</bodyText>
<subsectionHeader confidence="0.98852">
4.2 Parameters
</subsectionHeader>
<bodyText confidence="0.999990785714286">
In all experiments, hyperparameters for Proot, Pstop,
and Pchoose (and their backed-off distributions, and
including αllNK) were 1, αB was 10, and α¬B was 1.
VBEM was run on the training set until the data
log-likelihood changed by less than 0.001%, and
then the parameters were held fixed and used to
obtain Viterbi parses for the evaluation sentences.
Finally, we explored different global UNK cutoffs,
replacing each word that appeared less than c times
with the token UNK. We ran each model for each
c ∈ {0, 1, 25, 50, 100}, and picked the best-scoring
c on the development set for running on the test set
and presentation here. We used a harmonic initializer
similar to the one in Klein and Manning (2004).
</bodyText>
<subsectionHeader confidence="0.996635">
4.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.999962285714286">
In addition to evaluating the various incarnations of
the DMV with backoff and input types, we compare
to uniform branching baselines, the Common Cover
Link (CCL) parser of Seginer (2007), and the Unsu-
pervised Partial Parser (UPP) of Ponvert et al. (2011).
The UPP produces a constituency parse from words
and punctuation using a series of finite-state chun-
</bodyText>
<footnote confidence="0.987184">
6Available at http://homepages.inf.ed.ac.uk/s0930006/brentDep/
</footnote>
<bodyText confidence="0.9999616875">
kers; we use the best-performing (Probabilistic Right
Linear Grammar) version. The CCL parser produces
a constituency parse using a novel “Cover Link” rep-
resentation, scoring these links heuristically. Both
CCL and UPP rely on punctuation (though according
to Ponvert et al. (2011), UPP less so), which our in-
put is missing. The left-headed “LH” (right-headed
“RH”) baseline assumes that each word takes the first
word to its right (left) as a dependent, and corre-
sponds to a uniform right-branching (left-branching)
constituency baseline.
We evaluate the output of all models in terms
of both constituency scores and dependency accu-
racy. Our wsj10 and swbdnxt10 corpora are
originally annotated for constituency structure, with
the dependency gold standard derived as described
above, while our brent corpus is originally anno-
tated for dependency structure, with the constituency
gold standard derived by defining a constituent to
span a head and each of its dependents (ignoring
any one-word “constituents”). As the CCL and
UPP parsers don’t produce dependencies, only con-
stituency scores are provided.
For constituency scores, we present the standard
unlabeled Precision, Recall, and F-measure scores.
For dependency scores, we present Directed attach-
ment accuracy, Undirected attachment accuracy, and
the “Neutral Edge Detection” (NED) score intro-
duced by Schwartz et al. (2011). Directed attachment
accuracy counts an arc as a true positive if it correctly
identifies both a head and a dependent, whereas undi-
rected attachment accuracy ignores arc direction in
counting true positives. NED counts an arc as a true
positive if it would be a true positive under the Undi-
rected attachment score, or if the proposed head is
the gold-standard grandparent of the proposed depen-
dent. This avoids penalizing parses for flipping an
arc, such as making determiners, rather than nouns,
the head of noun phrases.
To assess statistical significance, we carried out
stratified shuffling tests, with 10, 000 random shuf-
fles, for all measures. Tables indicate significance
differences between the backoff models and the most
competitive baseline model on that measure, indi-
cated by an italic score. A star (∗) indicates p &lt; 0.05,
and a dagger (†) indicates p &lt; 0.01. To see the di-
rection of a significant difference (i.e. whether the
backoff model is better or worse than the baseline),
</bodyText>
<page confidence="0.99823">
69
</page>
<table confidence="0.999533806451613">
UNK Dependency wsj10
Dir. Undir. NED Constituency
P R F
Wds 25 32.5 52.5 67.0 49.5 48.5 49.0
POS 46.4 63.8 78.1 59.2 58.1 58.6
Wds 25 29.4 52.4 70.5 51.3 52.6 52.0
POS 43.5 61.9 77.3 59.7 57.1 58.4
Cond. 50 49.9† 66.1† 79.6∗ 64.2† 61.9† 63.0†
Joint 50 46.0 63.7 79.0 62.0† 59.1 60.5∗
Indep. 25 52.5† 68.0† 83.5† 63.5† 61.5† 62.5†
LH — 26.0 55.8 74.3 53.1 69.6 60.3
RH — 31.2 56.4 61.4 25.8 33.8 29.3
CCL — — — — 50.8 40.7 45.2
UPP — — — — 52.8 37.2 43.7
EM
VB
Wds+POS
swbdnxt10
UNK Dependency Constituency
Dir. Undir. NED P R F
25 30.6 50.9 66.8 45.4 47.1 46.3
53.0 65.0 76.8 52.5 52.9 52.7
25 36.1 54.9 72.7 49.0 50.0 49.5
51.3 62.5 74.3 47.1 46.6 46.8
100 45.5† 62.4† 77.8 58.4† 58.9† 58.7†
1 49.4† 63.7 79.6† 60.0† 52.9 56.3†
100 55.7† 65.8 74.6† 61.5† 57.9† 59.6†
— 24.1 50.8 72.7 60.8 82.5 70.0
— 29.2 52.0 57.9 22.2 30.1 25.5
— — — — 53.6 47.4 50.3
— — — — 60.0 46.6 52.4
</table>
<tableCaption confidence="0.985869">
Table 2: Performance on wsj10 and swbdnxt10 for models using words and POS tags only. Bold scores indicate
the best performance of all models and baselines on that measure.
</tableCaption>
<bodyText confidence="0.583514">
† Significantly different from best non-uniform baseline (italics) by a stratified shuffling test, p &lt; 0.01; ∗: p &lt; 0.05.
look to the scores themselves.
</bodyText>
<sectionHeader confidence="0.999465" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999977545454545">
In all results, when a model sees only one kind of
information, that is expressed by writing out the ab-
breviation for the relevant stream: “Wds” for words,
“POS” for Part-Of-Speech, “Dur” for word duration.
For baseline models that see two streams, the abbre-
viations are joined by a “×” symbol (as they treat
input pairs as atoms drawn in the cross-product of the
two streams’ vocabulary). For the backoff models,
the abbreviations are joined by a “+” symbol (as they
combine the information sources with a weighted
sum), with the “extra” stream name first.
</bodyText>
<subsectionHeader confidence="0.826441">
5.1 Results: wsj10
</subsectionHeader>
<bodyText confidence="0.999876902439024">
The left half of Table 2 presents results on wsj10.
For the baseline models, the first column with hori-
zontal text indicates the input, while for the backoff
(Wds+POS) models, the first column with horizontal
text indicates whether and how the extra stream is
modeled in dependents (as described in Section 3.3).
The EM model with POS input is largely a repli-
cation of the original DMV, differing in the use of
separate train, dev, and test sets, and possibly the
details of the harmonic initializer. Our replication
achieves an undirected attachment score of 63.8 on
the test set, similar to the score of 64.5 reported by
Klein and Manning (2004) when training and evalu-
ating on all of wsj10. Cohen et al. (2008) use the
same train/dev/test partition that we do, and report
a directed attachment score of 45.8, similar to our
directed attachment score of 46.4.
The VB model which learns from POS tags does
not outperform the EM model which learns from POS
tags, suggesting that data sparsity does not hurt the
DMV when using POS tags. As expected, the words-
only models perform much worse than both the POS
input models and the uniform LH baseline. VB does
improve the words-only constituency performance.
The Cond. and Indep. backoff models outperform
the POS-only baseline on all measures, but the Joint
backoff model does not demonstrate a clear advan-
tage over the POS-only baseline on any measure. The
success of the Indep. model indicates that modelling
dependent word identity does provide enough infor-
mation to justify the increase in sparsity. The failure
of the Joint model to provide a further improvement
indicates that the extra information in the full joint
over dependents does not justify the large increase
in parameters. We also see that several models out-
perform the LH baseline on dependencies, but the
advantage is much less in F-Score, underscoring the
loss of information in the conversion of dependen-
cies to constituencies. Finally, all models outperform
CCL and UPP on F-score, emphasizing their reliance
on the punctuation we removed.
</bodyText>
<table confidence="0.922393448275862">
Dependency Constituency
UNK Dir. Undir. NED P R F
Wds 25 30.6 50.9 66.8 45.4 47.1 46.3
Wds×Dur 25 26.1 46.5 62.0 45.6 48.7 47.1
Wds 25 36.4 55.1 73.0 49.1 50.0 49.6
Wds×Dur 25 31.8 51.7 71.3 49.2 55.9 52.3
Cond. 25 32.6† 55.1 74.5† 59.1† 71.4† 64.7†
Joint 50 31.8† 51.8† 70.8∗ 54.4† 60.5† 57.3†
Indep. 50 40.3† 59.1† 76.0† 56.1† 61.7† 58.8†
LH — 24.1 50.8 72.7 60.8 82.5 70.0
RH — 29.2 52.0 57.9 22.2 30.1 25.5
CCL — — — — 53.6 47.4 50.3
UPP — — — — 60.0 46.6 52.4
EM
VB
Dur+Wds
Switchboard Model Performance
45 50 55 60
Constituency F−score
45 50 55 60 65 70 75
Wds
WdsxDur
● Cond.
Joint
Indep.
● LH
●
●
Undirected Attachment Score
</table>
<tableCaption confidence="0.994302333333333">
Table 3: Performance on swbdnxt10 for models using words and duration. The scatterplot includes a subset of the
information in the table: F-score and undirected attachment accuracy for backoff models and VB and LH baseline.
Bold, italics, and significance annotations as in Table 2.
</tableCaption>
<subsectionHeader confidence="0.885189">
5.2 Results: swbdnxt10
</subsectionHeader>
<bodyText confidence="0.999723833333333">
The right half of Table 2 presents performance fig-
ures on swbdnxt10 for input involving words and
POS tags. As expected, the EM and VB baselines
perform best when learning from gold-standard POS
tags, and we again see no benefit for the VB POS-
only model compared to the EM POS-only model.
The POS-only baselines far outperform the uniform-
attachment baselines on the dependency measures; to
our knowledge this is the first demonstration outside
the newspaper domain that the DMV outperforms a
uniform branching strategy on these measures.
The other comparisons among systems listed in
Table 2 are largely inconclusive. Models do com-
paratively well on either the constituency or depen-
dency evaluation, but not both. The backoff mod-
els outperform the baseline POS-only models in the
constituency evaluation, but underperform or match
those same models in the dependency evaluation.
Conversely, most models outperform the LH base-
line in the dependency evaluation, but not in the
constituency evaluation. There are probably two
causes for the ambiguity in these results. First, the
noise in the dependency gold-standard may have over-
whelmed any advantage from backoff. Second, as we
saw with wsj10, the conversion from dependencies
to constituencies removes information, which may
explain the failure of any model to outperform the
LH baseline in the constituency evaluation.
Table 3 presents performance figures on
swbdnxt10 for input involving words and duration,
including a scatter-plot of Undirected attachment
against constituency F-Score for the interesting
comparisons. In the scatter-plot, models up and
to the right performed better, and we see that the
negative correlation between the dependency and
constituency evaluations persists in words and dura-
tion input. VB substantially outperforms EM in the
baselines, indicating that good smoothing is helpful
when learning from words. Other comparisons
are again ambiguous; the dependency evaluation
is noisy, and backoff models outperform baseline
models on the constituency evaluation but not the
LH baseline. Still, the backoff models outperform
all words-only baselines in constituency score, with
two performing slightly worse in dependency score
and one performing much better. So there is some
evidence that word duration is useful, but we will
find clearer evidence on the brent corpus.
</bodyText>
<subsectionHeader confidence="0.834964">
5.3 Results: brent
</subsectionHeader>
<bodyText confidence="0.999916888888889">
Table 4 presents results on the brent dataset. VB
is even more effective than in the other datasets for
improving performance among baseline models, lead-
ing to double-digit improvements on some measures.
Moreover, the best dev-set UNK cutoff drops to 1
for all VB models, indicating that, on this dataset,
VB provides good smoothing even in models without
backoff. This difference between datasets is likely
related to differences in vocabulary diversity; the
</bodyText>
<page confidence="0.994845">
71
</page>
<table confidence="0.985012461538461">
Dependency Constituency
UNK Dir. Undir. NED P R F
Wds 25 36.9 56.3 70.7 52.4 69.5 59.8
Wds×Dur 25 31.3 51.1 66.9 50.7 64.7 56.9
Wds 1 51.2 64.2 77.3 63.3 68.1 66.0
Wds×Dur 1 47.0 60.5 74.0 66.2 64.9 65.5
Cond. 1 53.1* 65.5* 78.7* 65.4 68.6 67.0*
Joint 1 50.7 63.0 76.3 65.6 65.41 65.5
Indep. 1 53.2 66.71 79.61 61.51 67.9 64.5
LH — 28.3 53.6 78.3 47.9 85.6 61.4
RH — 27.2 48.8 61.1 26.2 46.8 33.6
CCL — — — — 41.7 58.8 48.8
UPP — — — — 56.8 63.8 60.1
</table>
<figure confidence="0.7225354375">
EM
VB
Dur+Wds
Brent Model Performance
50 55 60 65 70
Constituency F−score
60 62 64 66 68 70
Wds
WdsxDur
● Cond.
Joint
Indep.
● LH
●
●
Undirected Attachment Score
</figure>
<tableCaption confidence="0.979602333333333">
Table 4: Performance on brent for models using words and duration. The scatterplot includes a subset of the
information in the table: F-score and undirected attachment accuracy for backoff models and VB and LH baseline.
Bold, italics, and significance annotations as in Table 2.
</tableCaption>
<bodyText confidence="0.9995901">
type:token ratio in the brent training set is about
1:15, compared to 1:5 and 1:9 in the wsj10 and
swbdnxt10 training sets, respectively.
More importantly for our main hypothesis, all
three backoff models using words and duration out-
perform the words-only baselines (including CCL
and UPP) on all dependency measures—the most
accurate measures on this corpus, which has hand-
annotated dependencies—and the Cond. model also
wins on F-score.
</bodyText>
<sectionHeader confidence="0.99954" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999678125">
In this paper, we showed how to use the DMV with
Backoff and two fully-generative variants to explore
the utility of word duration in fully lexicalized un-
supervised dependency parsing. Although other re-
searchers have incorporated features beyond words
and POS tags into DMV-like models (e.g., semantics:
Naseem and Barzilay (2011); morphology: Berg-
Kirkpatrick et al. (2009)), we believe this is the first
example based on Headden et al. (2009)’s backoff
method. As far as we know, our work is also the first
test of a DMV-based model on transcribed conver-
sational speech and the first to outperform uniform-
branching baselines without using either POS tags or
punctuation in the input. Our results show that fully-
lexicalized models can do well if they are smoothed
properly and exploit multiple cues.
Our experiments also suggest that CDS is espe-
cially easy to learn from. Model performance on
the brent dataset was generally higher than on
swbdnxt10, with a much lower UNK threshold.
This latter point, and the fact that brent has a much
lower word type/token ratio than the other datasets,
suggest that CDS provides more and clearer evidence
about words’ syntactic behavior.
Finally, our results provide more evidence, using
a different, more powerful syntactic model than that
of Pate and Goldwater (2011), that word duration
is a useful cue for unsupervised parsing. We found
that several ways of incorporating duration were use-
ful, although the extra sparsity of Joint emissions
was not justified in any of our investigations. Our
results are consistent with both the prosodic and pre-
dictability bootstrapping hypotheses of language ac-
quisition, providing the first computational support
for these using a full syntactic parsing model and
tested on child-directed speech. While our models do
not provide a mechanistic account of how children
might use duration information to help with learning
syntax, they do show that this information is useful
in principle, even without any knowledge of latent
prosodic structure or its relationship to syntax. In ad-
dition, our results suggest it may be useful to explore
using word duration to enrich NLP tasks in speech-
related technologies, such as syntactically-inspired
language models for text-to-speech generation. In
the future, we also hope to investigate why duration
is helpful, designing experiments to tease apart the
role of prosody and predictability in learning syntax.
</bodyText>
<page confidence="0.997491">
72
</page>
<sectionHeader confidence="0.996294" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999544211538461">
Matthew Aylett and Alice Turk. 2004. The smooth signal
redundancy hypothesis: A functional explanation for re-
lationships between redundancy, prosodic prominence,
and duration in spontaneous speech. Language and
Speech, 47(1):31–56.
Mary Beckman and Janet Pierrehumbert. 1986. Intona-
tional structure in Japanese and English. Phonology
Yearbook, 3:255–309.
Alan Bell, Jason M Brenier, Michelle Gregory, Cynthia
Girand, and Dan Jurafsky. 2009. Predictability effects
on durations of content and function words in conver-
sational English. Journal of Memory and Language,
60:92–111.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e, John
DeNero, and Dan Klein. 2009. Painless unsupervised
learning with features. In Proceedings of NAACL.
Michael R Brent and Jeffrey M Siskind. 2001. The role
of exposure to isolated words in early vocabulary de-
velopment. Cognition, 81:31–44.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2012. Turning the pipeline into a loop:
Iterated unsupervised dependency parsing and PoS in-
duction. In Proceedings of the NAACL-HLT Workshop
on the Induction of Linguistic Structure, pages 96–99,
Montr´eal, Canada, June. Association for Computational
Linguistics.
Shay B Cohen and Noah A Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proceedings of
NAACL.
Shay B Cohen, Kevin Gimpel, and Noah A Smith. 2008.
Logistic normal priors for unsupervised probabilistic
grammar induction. In Advances in Neural Information
Processing Systems 22.
Shay B Cohen, Dipanjan Das, and Noah A Smith. 2011.
Unsupervised structure prediction with non-parallel
multilingual guidance. In Proceedings of EMNLP.
Marie-Catherine de Marneffe and Christopher D Manning.
2008. Stanford typed dependencies manual. Technical
report.
Markus Dreyer and Izhak Shafran. 2007. Exploiting
prosody for PCFGs with latent annotations. In Pro-
ceedings of Interspeech, Antwerp, Belgium, August.
Susanne Gahl and Susan M Garnsey. 2004. Knowledge of
grammar, knowledge of usage: Syntactic probabilities
affect pronunciation variation. Language, 80:748–775.
Susanne Gahl, Susan M Garnsey, Cynthia Fisher, and
Laura Matzen. 2006. “That sounds unlikely”: Syntac-
tic probabilities affect pronunciation. In Proceedings
of the 27th meeting of the Cognitive Science Society.
Lila Gleitman and Eric Wanner. 1982. Language acqui-
sition: The state of the art. In Eric Wanner and Lila
Gleitman, editors, Language acquisition: The state of
the art, pages 3–48. Cambridge University Press, Cam-
bridge, UK.
Will Headden, Mark Johnson, and David McClosky. 2009.
Improved unsupervised dependency parsing with richer
contexts and smoothing. In Proceedings of NAACL-
HLT.
Zhongqiang Huang and Mary Harper. 2010. Appropri-
ately handled prosodic breaks help PCFG parsing. In
Proceedings of NAACL-HLT, pages 37–45, Los Ange-
les, California, June. Association for Computational
Linguistics.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proceedings of NODALIDA 2007.
Mark Johnson. 2007. Why doesn’t EM find good HMM
POS-taggers. In Proceedings ofEMNLP-CoNLL, pages
296–305.
Jeremy G Kahn, Matthew Lease, Eugene Charniak, Mark
Johnson, and Mari Ostendorf. 2005. Effective use of
prosody in parsing conversational speech. In Proceed-
ings of HLT-EMNLP, pages 233–240.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of ACL,
pages 479–486.
Matthias Trautner Kromann. 2003. The Danish Depen-
dency Treebank and the DTAG treebank tool. In Pro-
ceedings of the Second Workshop on Treebanks and
Linguistic Theories, pages 217–220.
Kenichi Kurihara and Taisuke Sato. 2006. Variational
Bayesian grammar induction for natural language. In
Proceedings of the International Colloquium on Gram-
matical Inference, pages 84–96.
Brian MacWhinney. 2000. The CHILDES project: Tools
for analyzing talk. Lawrence Erlbaum Associates, Mah-
wah, NJ, third edition.
S´everine Millotte, Roger Wales, and Anne Christophe.
2007. Phrasal prosody disambiguates syntax. Lan-
guage and Cognitive Processes, 22(6):898–909.
James L Morgan, Richard P Meier, and Elissa L Newport.
1987. Structural packaging in the input to language
learning: contributions of prosodic and morphologi-
cal marking of phrases to the acquisition of language.
Cognitive Psychology, 19:498–550.
Tahira Naseem and Regina Barzilay. 2011. Using seman-
tic cues to learn syntax. In Proceedings of AAAI.
John K Pate and Sharon Goldwater. 2011. Unsupervised
syntactic chunking with acoustic cues: computational
models for prosodic bootstrapping. In Proceedings
of the 2nd ACL workshop on Cognitive Modeling and
Computational Linguistics.
</reference>
<page confidence="0.983704">
73
</page>
<reference confidence="0.997671323529412">
Elias Ponvert, Jason Baldridge, and Katrin Erk. 2011.
Simple unsupervised grammar induction from raw text
with cascaded finite state models. In Proceedings of
ACL-HLT.
Patti J Price, Mari Ostendorf, Stefanie Shattuck-Hufnagel,
and Cynthia Fong. 1991. The use of prosody in syntac-
tic disambiguation. In Proceedings of the HLT work-
shop on Speech and Natural Language, pages 372–377,
Morristown, NJ, USA. Association for Computational
Linguistics.
C Anton Rytting, Chris Brew, and Eric Fosler-Lussier.
2010. Segmenting words from natural speech: subseg-
mental variation in segmental cues. Journal of Child
Language, 37(3):513–543.
Roy Schwartz, Omri Abend, Roi Reichart, and Ari Rap-
poport1. 2011. Neutralizing linguistically problematic
annotations in unsupervised dependency parsing evalu-
ation. In Proceedings of the 49th ACL, pages 663–672.
Yoav Seginer. 2007. Fast unsupervised incremental pars-
ing. In Proceedings of ACL.
Amanda Seidl. 2007. Infants’ use and weighting of
prosodic cues in clause segmentation. Journal of Mem-
ory and Language, 57(1):24–48.
Valentin I Spitkovsky, Hiyan Alshawi, Angel X Chang,
and Daniel Jurafsky. 2011a. Unsupervised dependency
parsing without gold part-of-speech tags. In Proceed-
ings of EMNLP.
Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky.
2011b. Punctuation: Making a point in unsupervised
dependency parsing. In Proceedings of CoNLL.
Harry Tily, Susanne Gahl, Inbal Arnon, Neal Snider,
Anubha Kothari, and Joan Bresnan. 2009. Syntactic
probabilities affect pronunciation variation in sponta-
neous speech. Language and Cognition, 1(2):147–165.
</reference>
<page confidence="0.999113">
74
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.370901">
<title confidence="0.75226">Unsupervised Dependency Parsing with Acoustic Cues K</title>
<author confidence="0.491662">j k patesms ed ac uk sgwaterinf ed ac uk</author>
<affiliation confidence="0.999816">School of Informatics of Computing University of Edinburgh Macquarie University</affiliation>
<address confidence="0.999713">Edinburgh, EH8 9AB, UK Sydney, NSW 2109, Australia</address>
<abstract confidence="0.996316380952381">Unsupervised parsing is a difficult task that infants readily perform. Progress has been made on this task using text-based models, but few computational approaches have considered how infants might benefit from acoustic cues. paper explores the hypothesis that help with learning syntax. We describe how duration information can be incorporated into an unsupervised Bayesian dependency parser whose only other source of information is the words themselves (without punctuation or parts of speech). Our results, evaluated on both adult-directed and child-directed utterances, show that using word duration can improve parse quality relative to words-only baselines. These results support the idea that acoustic cues provide useful evidence about syntactic structure for language-learning infants, and motivate the use of word duration cues in NLP tasks with speech.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Matthew Aylett</author>
<author>Alice Turk</author>
</authors>
<title>The smooth signal redundancy hypothesis: A functional explanation for relationships between redundancy, prosodic prominence, and duration in spontaneous speech.</title>
<date>2004</date>
<journal>Language and Speech,</journal>
<volume>47</volume>
<issue>1</issue>
<contexts>
<context position="8245" citStr="Aylett and Turk, 2004" startWordPosition="1224" endWordPosition="1227">nnotations. 2.2 Predictability Bootstrapping On the basis of our HMM chunkers, we introduced the predictability bootstrapping hypothesis (Pate and Goldwater, 2011): the idea that word durations could be a useful cue to syntactic structure not (or not only) because they provide information about prosodic structure, but because they are a direct cue to syntactic predictability. It is well-established that talkers tend to pronounce words more quickly when they are more predictable, as measured by, e.g., word frequency, n-gram probability, or whether or not the word has been previously mentioned (Aylett and Turk, 2004; Bell et al., 2009). However, syntactic proba64 you threw it right at the basket Figure 1: Example unlabeled dependency parse. bility also seems to matter, with studies showing that verbs tend to be pronounced more quickly when they are in their preferred syntactic frame—transitive vs. intransitive or direct object vs. sentential complement (Gahl and Garnsey, 2004; Gahl et al., 2006; Tily et al., 2009). While this syntactic evidence is only for verbs, together with the evidence that effects of other notions of predictability, it suggests that such syntactic effects may also be widespread. If </context>
</contexts>
<marker>Aylett, Turk, 2004</marker>
<rawString>Matthew Aylett and Alice Turk. 2004. The smooth signal redundancy hypothesis: A functional explanation for relationships between redundancy, prosodic prominence, and duration in spontaneous speech. Language and Speech, 47(1):31–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Beckman</author>
<author>Janet Pierrehumbert</author>
</authors>
<date>1986</date>
<booktitle>Intonational structure in Japanese and English. Phonology Yearbook,</booktitle>
<pages>3--255</pages>
<contexts>
<context position="5582" citStr="Beckman and Pierrehumbert, 1986" startWordPosition="825" endWordPosition="828">cue to syntax. This section reviews the two possible reasons mentioned above: duration as a cue to prosodic structure, or as a cue to predictability. 2.1 Prosodic Bootstrapping Prosody is the structure of speech as conveyed by rhythm and intonation, which are, in turn, conveyed by such measurable phenomena as variation in fundamental frequency, word duration, and spectral tilt. Prosodic structure is typically analyzed as imposing a shallow, hierarchical grouping structure on speech, with the ends of prosodic phrases (constituents) being cued in part by lengthening the last word of the phrase (Beckman and Pierrehumbert, 1986). The Prosodic Bootstrapping hypothesis (Gleitman and Wanner, 1982) points out that prosodic phrases are often also syntactic phrases, and proposes that language-acquiring infants exploit this correlation. Specifically, if infants can learn about prosodic phrase structure using word duration (and fundamenin a model of language acquisition, gold tags certainly are not. tal frequency), they may be able to identify syntactic phrases more easily using word strings and prosodic trees than using word strings alone. Several behavioral experiments support the connection between prosody and syntax and </context>
</contexts>
<marker>Beckman, Pierrehumbert, 1986</marker>
<rawString>Mary Beckman and Janet Pierrehumbert. 1986. Intonational structure in Japanese and English. Phonology Yearbook, 3:255–309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Bell</author>
<author>Jason M Brenier</author>
<author>Michelle Gregory</author>
<author>Cynthia Girand</author>
<author>Dan Jurafsky</author>
</authors>
<title>Predictability effects on durations of content and function words in conversational English.</title>
<date>2009</date>
<journal>Journal of Memory and Language,</journal>
<pages>60--92</pages>
<contexts>
<context position="8265" citStr="Bell et al., 2009" startWordPosition="1228" endWordPosition="1231">ability Bootstrapping On the basis of our HMM chunkers, we introduced the predictability bootstrapping hypothesis (Pate and Goldwater, 2011): the idea that word durations could be a useful cue to syntactic structure not (or not only) because they provide information about prosodic structure, but because they are a direct cue to syntactic predictability. It is well-established that talkers tend to pronounce words more quickly when they are more predictable, as measured by, e.g., word frequency, n-gram probability, or whether or not the word has been previously mentioned (Aylett and Turk, 2004; Bell et al., 2009). However, syntactic proba64 you threw it right at the basket Figure 1: Example unlabeled dependency parse. bility also seems to matter, with studies showing that verbs tend to be pronounced more quickly when they are in their preferred syntactic frame—transitive vs. intransitive or direct object vs. sentential complement (Gahl and Garnsey, 2004; Gahl et al., 2006; Tily et al., 2009). While this syntactic evidence is only for verbs, together with the evidence that effects of other notions of predictability, it suggests that such syntactic effects may also be widespread. If so, the duration of </context>
</contexts>
<marker>Bell, Brenier, Gregory, Girand, Jurafsky, 2009</marker>
<rawString>Alan Bell, Jason M Brenier, Michelle Gregory, Cynthia Girand, and Dan Jurafsky. 2009. Predictability effects on durations of content and function words in conversational English. Journal of Memory and Language, 60:92–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Painless unsupervised learning with features.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<marker>Berg-Kirkpatrick, Bouchard-Cˆot´e, DeNero, Klein, 2009</marker>
<rawString>Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e, John DeNero, and Dan Klein. 2009. Painless unsupervised learning with features. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael R Brent</author>
<author>Jeffrey M Siskind</author>
</authors>
<title>The role of exposure to isolated words in early vocabulary development.</title>
<date>2001</date>
<journal>Cognition,</journal>
<pages>81--31</pages>
<contexts>
<context position="21459" citStr="Brent and Siskind, 2001" startWordPosition="3408" endWordPosition="3411">,007 233 357 Word tokens 24,998 2,980 3,052 Word types 2,647 760 767 Sentences 3,998 488 491 Word tokens 20,954 2,127 2,206 Word types 1,390 482 488 Sentences 6,249 424 449 Table 1: Statistics for our three corpora. Proot over both streams jointly and independently, respectively. 4 Experimental Setup 4.1 Datasets We evaluate on three datasets: wsj10, sentences of length 10 or less from the Wall Street Journal portion of the Penn Treebank; swbdnxt10, sentences of length 10 or less from the Switchboard dataset of ADS used by Pate and Goldwater (2011); and brent, part of the Brent corpus of CDS (Brent and Siskind, 2001). Table 1 presents corpus statistics. 4.1.1 wsj10 We present a new evaluation of the DMV with Backoff on wsj10, which does not have any acoustic information, simply to verify that αoNK performs sensibly on a standard corpus. Additionally, Headden et al. (2009) use an intensive initializer that relies on dozens of random restarts, and so, strictly speaking, only show that the backoff technology is useful for good initializations. Our new evaluation will show that the backoff technology provides a substantial benefit even for harmonic initialization. wsj10 was created in the standard way; all pu</context>
<context position="24592" citStr="Brent and Siskind, 2001" startWordPosition="3895" endWordPosition="3898">ences to check the accuracy of the conversion tool, which was designed for newspaper text. Excluding arcs involving words with no clear role in dependency structure (such as “um”), about 86% of the arcs were correct. While this rate is uncomfortably low, it is still much higher than unsupervised dependency parsers typically achieve, and so may provide a reasonable measure of relative dependency parse quality among competing systems. 4.1.3 brent We also evaluated our models on the “Large Brent” dataset introduced in Rytting et al. (2010), a portion of the Brent corpus of child-directed speech (Brent and Siskind, 2001). We call this corpus brent. It consists of utterances from four of the mothers in Brent and Siskind’s (2001) study, and, like wsj10 swbdnxt10 brent 68 swbdnxt10, has a forced alignment from which we obtain duration terciles. Rytting et al. (2010) used a 90%/10% train/test partition. We extracted every ninth utterance from the original training partition to create a dev set, producing an 80%/10%/10% partition. We also separated clitics from their base word. This dataset only has 186 sentences longer than ten words, with a maximum length of 22 words, so we discarded only sentences shorter than </context>
</contexts>
<marker>Brent, Siskind, 2001</marker>
<rawString>Michael R Brent and Jeffrey M Siskind. 2001. The role of exposure to isolated words in early vocabulary development. Cognition, 81:31–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos Christodoulopoulos</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Turning the pipeline into a loop: Iterated unsupervised dependency parsing and PoS induction.</title>
<date>2012</date>
<booktitle>In Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure,</booktitle>
<pages>96--99</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="1845" citStr="Christodoulopoulos et al. (2012)" startWordPosition="264" endWordPosition="267"> syntax is difficult for NLP systems, yet infants perform this task routinely. Previous work in NLP has focused on using the implicit syntactic information available in part-of-speech (POS) tags (Klein and Manning, 2004), punctuation (Seginer, 2007; Spitkovsky et al., 2011b; Ponvert et al., 2011), and syntactic similarities between related languages (Cohen and Smith, 2009; Cohen et al., 2011). However, these approaches likely use the data in a very different way from children: neither POS tags nor punctuation are observed during language acquisition (although see Spitkovsky et al. (2011a) and Christodoulopoulos et al. (2012) for encouraging results using unsupervised POS tags), and many children learn in a broadly monolingual environment. This paper explores a possible source of information that NLP systems typically ignore: word duration, or the length of time taken to pronounce each word. There are good reasons to think that word duration might be useful for learning syntax. First, the well-established Prosodic Bootstrapping hypothesis (Gleitman and Wanner, 1982) proposes that infants use acoustic-prosodic cues (such as word duration) to help them identify syntactic structure, because prosodic and syntactic str</context>
</contexts>
<marker>Christodoulopoulos, Goldwater, Steedman, 2012</marker>
<rawString>Christos Christodoulopoulos, Sharon Goldwater, and Mark Steedman. 2012. Turning the pipeline into a loop: Iterated unsupervised dependency parsing and PoS induction. In Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 96–99, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Noah A Smith</author>
</authors>
<title>Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="1587" citStr="Cohen and Smith, 2009" startWordPosition="225" endWordPosition="228">elines. These results support the idea that acoustic cues provide useful evidence about syntactic structure for language-learning infants, and motivate the use of word duration cues in NLP tasks with speech. 1 Introduction Unsupervised learning of syntax is difficult for NLP systems, yet infants perform this task routinely. Previous work in NLP has focused on using the implicit syntactic information available in part-of-speech (POS) tags (Klein and Manning, 2004), punctuation (Seginer, 2007; Spitkovsky et al., 2011b; Ponvert et al., 2011), and syntactic similarities between related languages (Cohen and Smith, 2009; Cohen et al., 2011). However, these approaches likely use the data in a very different way from children: neither POS tags nor punctuation are observed during language acquisition (although see Spitkovsky et al. (2011a) and Christodoulopoulos et al. (2012) for encouraging results using unsupervised POS tags), and many children learn in a broadly monolingual environment. This paper explores a possible source of information that NLP systems typically ignore: word duration, or the length of time taken to pronounce each word. There are good reasons to think that word duration might be useful for</context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>Shay B Cohen and Noah A Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Logistic normal priors for unsupervised probabilistic grammar induction.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems 22.</booktitle>
<contexts>
<context position="31816" citStr="Cohen et al. (2008)" startWordPosition="5103" endWordPosition="5106">with horizontal text indicates the input, while for the backoff (Wds+POS) models, the first column with horizontal text indicates whether and how the extra stream is modeled in dependents (as described in Section 3.3). The EM model with POS input is largely a replication of the original DMV, differing in the use of separate train, dev, and test sets, and possibly the details of the harmonic initializer. Our replication achieves an undirected attachment score of 63.8 on the test set, similar to the score of 64.5 reported by Klein and Manning (2004) when training and evaluating on all of wsj10. Cohen et al. (2008) use the same train/dev/test partition that we do, and report a directed attachment score of 45.8, similar to our directed attachment score of 46.4. The VB model which learns from POS tags does not outperform the EM model which learns from POS tags, suggesting that data sparsity does not hurt the DMV when using POS tags. As expected, the wordsonly models perform much worse than both the POS input models and the uniform LH baseline. VB does improve the words-only constituency performance. The Cond. and Indep. backoff models outperform the POS-only baseline on all measures, but the Joint backoff</context>
</contexts>
<marker>Cohen, Gimpel, Smith, 2008</marker>
<rawString>Shay B Cohen, Kevin Gimpel, and Noah A Smith. 2008. Logistic normal priors for unsupervised probabilistic grammar induction. In Advances in Neural Information Processing Systems 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Unsupervised structure prediction with non-parallel multilingual guidance.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1608" citStr="Cohen et al., 2011" startWordPosition="229" endWordPosition="232">upport the idea that acoustic cues provide useful evidence about syntactic structure for language-learning infants, and motivate the use of word duration cues in NLP tasks with speech. 1 Introduction Unsupervised learning of syntax is difficult for NLP systems, yet infants perform this task routinely. Previous work in NLP has focused on using the implicit syntactic information available in part-of-speech (POS) tags (Klein and Manning, 2004), punctuation (Seginer, 2007; Spitkovsky et al., 2011b; Ponvert et al., 2011), and syntactic similarities between related languages (Cohen and Smith, 2009; Cohen et al., 2011). However, these approaches likely use the data in a very different way from children: neither POS tags nor punctuation are observed during language acquisition (although see Spitkovsky et al. (2011a) and Christodoulopoulos et al. (2012) for encouraging results using unsupervised POS tags), and many children learn in a broadly monolingual environment. This paper explores a possible source of information that NLP systems typically ignore: word duration, or the length of time taken to pronounce each word. There are good reasons to think that word duration might be useful for learning syntax. Fir</context>
</contexts>
<marker>Cohen, Das, Smith, 2011</marker>
<rawString>Shay B Cohen, Dipanjan Das, and Noah A Smith. 2011. Unsupervised structure prediction with non-parallel multilingual guidance. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<date>2008</date>
<note>Stanford typed dependencies manual. Technical report.</note>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D Manning. 2008. Stanford typed dependencies manual. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Izhak Shafran</author>
</authors>
<title>Exploiting prosody for PCFGs with latent annotations.</title>
<date>2007</date>
<booktitle>In Proceedings of Interspeech,</booktitle>
<location>Antwerp, Belgium,</location>
<contexts>
<context position="7536" citStr="Dreyer and Shafran, 2007" startWordPosition="1117" endWordPosition="1120"> even better. While these results are consistent with the prosodic bootstrapping hypothesis, we suggested that predictability bootstrapping (see below) might be a more plausible explanation. Other computational work has combined prosody with syntax, but only in supervised systems, and typically using hand-annotated prosodic information. For example, Huang and Harper (2010) used annotated prosodic breaks as a kind of punctuation in a supervised PCFG, while prosodic breaks learned in a semi-supervised way have been used as features for parse reranking (Kahn et al., 2005) or PCFG statesplitting (Dreyer and Shafran, 2007). In contrast to these methods, our approach observes neither parse trees nor prosodic annotations. 2.2 Predictability Bootstrapping On the basis of our HMM chunkers, we introduced the predictability bootstrapping hypothesis (Pate and Goldwater, 2011): the idea that word durations could be a useful cue to syntactic structure not (or not only) because they provide information about prosodic structure, but because they are a direct cue to syntactic predictability. It is well-established that talkers tend to pronounce words more quickly when they are more predictable, as measured by, e.g., word f</context>
</contexts>
<marker>Dreyer, Shafran, 2007</marker>
<rawString>Markus Dreyer and Izhak Shafran. 2007. Exploiting prosody for PCFGs with latent annotations. In Proceedings of Interspeech, Antwerp, Belgium, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susanne Gahl</author>
<author>Susan M Garnsey</author>
</authors>
<title>Knowledge of grammar, knowledge of usage: Syntactic probabilities affect pronunciation variation.</title>
<date>2004</date>
<journal>Language,</journal>
<pages>80--748</pages>
<contexts>
<context position="2775" citStr="Gahl and Garnsey, 2004" startWordPosition="400" endWordPosition="403"> word duration might be useful for learning syntax. First, the well-established Prosodic Bootstrapping hypothesis (Gleitman and Wanner, 1982) proposes that infants use acoustic-prosodic cues (such as word duration) to help them identify syntactic structure, because prosodic and syntactic structures sometimes coincide. More recently, we proposed (Pate and Goldwater, 2011) that infants might use word duration as a direct cue to syntactic structure (i.e., without requiring intermediate prosodic structure), because words in high-probability syntactic structures tend to be pronounced more quickly (Gahl and Garnsey, 2004; Gahl et al., 2006; Tily et al., 2009). Like most recent work on unsupervised parsing, we focus on learning syntactic dependencies. Our work is based on Headden et al. (2009)’s Bayesian version of the Dependency Model with Valence (DMV) (Klein and Manning, 2004), using interpolated backoff techniques to incorporate multiple information sources per token. However, whereas Headden et al. used words and POS tags as input, we use words and word duration information, presenting three variants of their model that use this information in slightly different ways.1 1By using neither gold-standard nor </context>
<context position="8612" citStr="Gahl and Garnsey, 2004" startWordPosition="1282" endWordPosition="1285">dictability. It is well-established that talkers tend to pronounce words more quickly when they are more predictable, as measured by, e.g., word frequency, n-gram probability, or whether or not the word has been previously mentioned (Aylett and Turk, 2004; Bell et al., 2009). However, syntactic proba64 you threw it right at the basket Figure 1: Example unlabeled dependency parse. bility also seems to matter, with studies showing that verbs tend to be pronounced more quickly when they are in their preferred syntactic frame—transitive vs. intransitive or direct object vs. sentential complement (Gahl and Garnsey, 2004; Gahl et al., 2006; Tily et al., 2009). While this syntactic evidence is only for verbs, together with the evidence that effects of other notions of predictability, it suggests that such syntactic effects may also be widespread. If so, the duration of a word could give clues as to whether it is being used in a high-probability or low-probability structure, and thus what the correct structure is. We found that our syntactic chunkers benefited more from duration information than prosodic annotations, providing some preliminary evidence in favor of predictability bootstrapping, but not ruling ou</context>
</contexts>
<marker>Gahl, Garnsey, 2004</marker>
<rawString>Susanne Gahl and Susan M Garnsey. 2004. Knowledge of grammar, knowledge of usage: Syntactic probabilities affect pronunciation variation. Language, 80:748–775.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susanne Gahl</author>
<author>Susan M Garnsey</author>
<author>Cynthia Fisher</author>
<author>Laura Matzen</author>
</authors>
<title>That sounds unlikely”: Syntactic probabilities affect pronunciation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 27th meeting of the Cognitive Science Society.</booktitle>
<contexts>
<context position="2794" citStr="Gahl et al., 2006" startWordPosition="404" endWordPosition="407">useful for learning syntax. First, the well-established Prosodic Bootstrapping hypothesis (Gleitman and Wanner, 1982) proposes that infants use acoustic-prosodic cues (such as word duration) to help them identify syntactic structure, because prosodic and syntactic structures sometimes coincide. More recently, we proposed (Pate and Goldwater, 2011) that infants might use word duration as a direct cue to syntactic structure (i.e., without requiring intermediate prosodic structure), because words in high-probability syntactic structures tend to be pronounced more quickly (Gahl and Garnsey, 2004; Gahl et al., 2006; Tily et al., 2009). Like most recent work on unsupervised parsing, we focus on learning syntactic dependencies. Our work is based on Headden et al. (2009)’s Bayesian version of the Dependency Model with Valence (DMV) (Klein and Manning, 2004), using interpolated backoff techniques to incorporate multiple information sources per token. However, whereas Headden et al. used words and POS tags as input, we use words and word duration information, presenting three variants of their model that use this information in slightly different ways.1 1By using neither gold-standard nor learned POS tags as</context>
<context position="8631" citStr="Gahl et al., 2006" startWordPosition="1286" endWordPosition="1289">established that talkers tend to pronounce words more quickly when they are more predictable, as measured by, e.g., word frequency, n-gram probability, or whether or not the word has been previously mentioned (Aylett and Turk, 2004; Bell et al., 2009). However, syntactic proba64 you threw it right at the basket Figure 1: Example unlabeled dependency parse. bility also seems to matter, with studies showing that verbs tend to be pronounced more quickly when they are in their preferred syntactic frame—transitive vs. intransitive or direct object vs. sentential complement (Gahl and Garnsey, 2004; Gahl et al., 2006; Tily et al., 2009). While this syntactic evidence is only for verbs, together with the evidence that effects of other notions of predictability, it suggests that such syntactic effects may also be widespread. If so, the duration of a word could give clues as to whether it is being used in a high-probability or low-probability structure, and thus what the correct structure is. We found that our syntactic chunkers benefited more from duration information than prosodic annotations, providing some preliminary evidence in favor of predictability bootstrapping, but not ruling out prosodic bootstra</context>
</contexts>
<marker>Gahl, Garnsey, Fisher, Matzen, 2006</marker>
<rawString>Susanne Gahl, Susan M Garnsey, Cynthia Fisher, and Laura Matzen. 2006. “That sounds unlikely”: Syntactic probabilities affect pronunciation. In Proceedings of the 27th meeting of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lila Gleitman</author>
<author>Eric Wanner</author>
</authors>
<title>Language acquisition: The state of the art.</title>
<date>1982</date>
<booktitle>In Eric Wanner and Lila Gleitman, editors, Language acquisition: The state of the art,</booktitle>
<pages>3--48</pages>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="2294" citStr="Gleitman and Wanner, 1982" startWordPosition="332" endWordPosition="335">fferent way from children: neither POS tags nor punctuation are observed during language acquisition (although see Spitkovsky et al. (2011a) and Christodoulopoulos et al. (2012) for encouraging results using unsupervised POS tags), and many children learn in a broadly monolingual environment. This paper explores a possible source of information that NLP systems typically ignore: word duration, or the length of time taken to pronounce each word. There are good reasons to think that word duration might be useful for learning syntax. First, the well-established Prosodic Bootstrapping hypothesis (Gleitman and Wanner, 1982) proposes that infants use acoustic-prosodic cues (such as word duration) to help them identify syntactic structure, because prosodic and syntactic structures sometimes coincide. More recently, we proposed (Pate and Goldwater, 2011) that infants might use word duration as a direct cue to syntactic structure (i.e., without requiring intermediate prosodic structure), because words in high-probability syntactic structures tend to be pronounced more quickly (Gahl and Garnsey, 2004; Gahl et al., 2006; Tily et al., 2009). Like most recent work on unsupervised parsing, we focus on learning syntactic </context>
<context position="5649" citStr="Gleitman and Wanner, 1982" startWordPosition="833" endWordPosition="837">ove: duration as a cue to prosodic structure, or as a cue to predictability. 2.1 Prosodic Bootstrapping Prosody is the structure of speech as conveyed by rhythm and intonation, which are, in turn, conveyed by such measurable phenomena as variation in fundamental frequency, word duration, and spectral tilt. Prosodic structure is typically analyzed as imposing a shallow, hierarchical grouping structure on speech, with the ends of prosodic phrases (constituents) being cued in part by lengthening the last word of the phrase (Beckman and Pierrehumbert, 1986). The Prosodic Bootstrapping hypothesis (Gleitman and Wanner, 1982) points out that prosodic phrases are often also syntactic phrases, and proposes that language-acquiring infants exploit this correlation. Specifically, if infants can learn about prosodic phrase structure using word duration (and fundamenin a model of language acquisition, gold tags certainly are not. tal frequency), they may be able to identify syntactic phrases more easily using word strings and prosodic trees than using word strings alone. Several behavioral experiments support the connection between prosody and syntax and the prosodic bootstrapping hypothesis specifically. For example, th</context>
</contexts>
<marker>Gleitman, Wanner, 1982</marker>
<rawString>Lila Gleitman and Eric Wanner. 1982. Language acquisition: The state of the art. In Eric Wanner and Lila Gleitman, editors, Language acquisition: The state of the art, pages 3–48. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Headden</author>
<author>Mark Johnson</author>
<author>David McClosky</author>
</authors>
<title>Improved unsupervised dependency parsing with richer contexts and smoothing.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACLHLT.</booktitle>
<contexts>
<context position="2950" citStr="Headden et al. (2009)" startWordPosition="430" endWordPosition="433">-prosodic cues (such as word duration) to help them identify syntactic structure, because prosodic and syntactic structures sometimes coincide. More recently, we proposed (Pate and Goldwater, 2011) that infants might use word duration as a direct cue to syntactic structure (i.e., without requiring intermediate prosodic structure), because words in high-probability syntactic structures tend to be pronounced more quickly (Gahl and Garnsey, 2004; Gahl et al., 2006; Tily et al., 2009). Like most recent work on unsupervised parsing, we focus on learning syntactic dependencies. Our work is based on Headden et al. (2009)’s Bayesian version of the Dependency Model with Valence (DMV) (Klein and Manning, 2004), using interpolated backoff techniques to incorporate multiple information sources per token. However, whereas Headden et al. used words and POS tags as input, we use words and word duration information, presenting three variants of their model that use this information in slightly different ways.1 1By using neither gold-standard nor learned POS tags as input, our work differs from nearly all previous work on unsupervised dependency parsing. While learned tags might be plausible 63 Transactions of the Asso</context>
<context position="12014" citStr="Headden et al. (2009)" startWordPosition="1831" endWordPosition="1834">rm-branching baseline on the Wall Street Journal corpus. It was trained using EM to obtain a maximum-likelihood estimate of the parameters 0, and learned from POS tags to avoid rare events. However, all work on syntactic predictability effects on word duration has been lexicalized (looking at, e.g., the transitivity bias of particular verbs). In addition, it is unlikely that children have access to the correct parts of speech when first learning syntactic structure. Thus, we want a DMV variant that learns from words rather than POS tags. We therefore adopt several extensions to the DMV due to Headden et al. (2009), described next. 3.2 The DMV with Backoff Headden et al. (2009) sought to improve the DMV by incorporating lexical information in addition to POS tags. However, arcs between particular words are rare, so they modified the DMV in two ways to deal with this sparsity. First, they switched from MLE to a Bayesian approach, estimating a probability distribution over model parameters 0 and dependency trees T given the training corpus C and a prior distribution α over models: P(T, 0|C, α). Headden et al. avoided overestimating the probability of rare events that happen to occur in the train65 ing dat</context>
<context position="13319" citStr="Headden et al. (2009)" startWordPosition="2049" endWordPosition="2052">rare events. Accordingly, models that overcommit to rare events will contribute little to the final average over models. Specifically, Headden et al. use Dirichlet priors, with α being the Dirichlet hyperparameters. Headden et al.’s second innovation was to adapt interpolated backoff methods from language modeling with n-grams, where one can estimate the probability of word wn given word wn−1 by interpolating between unigram and bigram probability estimates: Pˆ(wn|wn−1) _ λP(wn|wn−1) + (1 − λ)P(wn) with λ ∈ [0, 1]. Ideally, λ should be large when wn−1 is frequent, and small when wn−1 is rare. Headden et al. (2009) apply this method to the DMV by backing off from Choose and Stop distributions that condition on both head word and POS to distributions that condition on only the head POS. In the equation above, λ is a scalar parameter. However, it actually specifies a probability distribution over the decision to back off (B) or not back off (¬B), and we can use different notation to reflect this view. Specifically, λstop(·) and λchoose(·) will represent our backoff distributions for the Stop and Choose decision, respectively. Using hp and dp to represent head and dependent POS tag and hw and dw to represe</context>
<context position="18996" citStr="Headden et al. (2009)" startWordPosition="2981" endWordPosition="2984"> (4) However, this joint model will have a very large statespace and may suffer from the same data sparsity, so we also explore a model (“Indep.”) that generates the 3Preliminary dev-set experiments confirmed this intuition, as models that backed off to word duration performed poorly. extra and backoff independently: ˆPchoose(dw, da|hw, hp, dir) _ Achoose(¬B|hw, ha, dir) Pchoose backoff (dw|hw, ha, dir) Pchoose extra(da|hw, ha, dir) + Achoose(B|hw, ha, dir) Pchoose backoff (dw|hw, dir) Pchoose extra(da|hw, dir) (5) We also modified the DMV with Backoff to handle heavily lexicalized models. In Headden et al. (2009), arcs between words that never appear in the same sentence are given probability mass only by virtue of the backoff distribution to POS tags, which all appear in the same sentence at least once. We want to avoid relying on POS tags, and we also want to use held-out development and test sets to avoid implicitly overfitting the data when exploring different model structures. To this end, we add one extra αUNKhyperparameter to the Dirichlet prior of Pchoose for each combination of conditioning events. This hyperparameter reserves probability mass for a head h to take a word dw as a dependent if </context>
<context position="21719" citStr="Headden et al. (2009)" startWordPosition="3451" endWordPosition="3454">ly, respectively. 4 Experimental Setup 4.1 Datasets We evaluate on three datasets: wsj10, sentences of length 10 or less from the Wall Street Journal portion of the Penn Treebank; swbdnxt10, sentences of length 10 or less from the Switchboard dataset of ADS used by Pate and Goldwater (2011); and brent, part of the Brent corpus of CDS (Brent and Siskind, 2001). Table 1 presents corpus statistics. 4.1.1 wsj10 We present a new evaluation of the DMV with Backoff on wsj10, which does not have any acoustic information, simply to verify that αoNK performs sensibly on a standard corpus. Additionally, Headden et al. (2009) use an intensive initializer that relies on dozens of random restarts, and so, strictly speaking, only show that the backoff technology is useful for good initializations. Our new evaluation will show that the backoff technology provides a substantial benefit even for harmonic initialization. wsj10 was created in the standard way; all punctuation and traces were removed, and sentences containing more than ten tokens were discarded. For our fully lexicalized version of wsj10, all words were lowercased, and numbers were replaced with the token “NUMBER.”5 Following standard practice, we used sec</context>
<context position="38719" citStr="Headden et al. (2009)" startWordPosition="6245" endWordPosition="6248">ng CCL and UPP) on all dependency measures—the most accurate measures on this corpus, which has handannotated dependencies—and the Cond. model also wins on F-score. 6 Conclusion In this paper, we showed how to use the DMV with Backoff and two fully-generative variants to explore the utility of word duration in fully lexicalized unsupervised dependency parsing. Although other researchers have incorporated features beyond words and POS tags into DMV-like models (e.g., semantics: Naseem and Barzilay (2011); morphology: BergKirkpatrick et al. (2009)), we believe this is the first example based on Headden et al. (2009)’s backoff method. As far as we know, our work is also the first test of a DMV-based model on transcribed conversational speech and the first to outperform uniformbranching baselines without using either POS tags or punctuation in the input. Our results show that fullylexicalized models can do well if they are smoothed properly and exploit multiple cues. Our experiments also suggest that CDS is especially easy to learn from. Model performance on the brent dataset was generally higher than on swbdnxt10, with a much lower UNK threshold. This latter point, and the fact that brent has a much lower</context>
</contexts>
<marker>Headden, Johnson, McClosky, 2009</marker>
<rawString>Will Headden, Mark Johnson, and David McClosky. 2009. Improved unsupervised dependency parsing with richer contexts and smoothing. In Proceedings of NAACLHLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Mary Harper</author>
</authors>
<title>Appropriately handled prosodic breaks help PCFG parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>37--45</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="7286" citStr="Huang and Harper (2010)" startWordPosition="1075" endWordPosition="1078">011), which learned shallow syntax from words, words and word durations, or words and handannotated prosody. Using these chunkers, we found that using words plus prosodic annotation worked better than just words, and words plus word duration worked even better. While these results are consistent with the prosodic bootstrapping hypothesis, we suggested that predictability bootstrapping (see below) might be a more plausible explanation. Other computational work has combined prosody with syntax, but only in supervised systems, and typically using hand-annotated prosodic information. For example, Huang and Harper (2010) used annotated prosodic breaks as a kind of punctuation in a supervised PCFG, while prosodic breaks learned in a semi-supervised way have been used as features for parse reranking (Kahn et al., 2005) or PCFG statesplitting (Dreyer and Shafran, 2007). In contrast to these methods, our approach observes neither parse trees nor prosodic annotations. 2.2 Predictability Bootstrapping On the basis of our HMM chunkers, we introduced the predictability bootstrapping hypothesis (Pate and Goldwater, 2011): the idea that word durations could be a useful cue to syntactic structure not (or not only) becau</context>
</contexts>
<marker>Huang, Harper, 2010</marker>
<rawString>Zhongqiang Huang and Mary Harper. 2010. Appropriately handled prosodic breaks help PCFG parsing. In Proceedings of NAACL-HLT, pages 37–45, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for English.</title>
<date>2007</date>
<booktitle>In Proceedings of NODALIDA</booktitle>
<contexts>
<context position="22618" citStr="Johansson and Nugues (2007)" startWordPosition="3588" endWordPosition="3591">rmonic initialization. wsj10 was created in the standard way; all punctuation and traces were removed, and sentences containing more than ten tokens were discarded. For our fully lexicalized version of wsj10, all words were lowercased, and numbers were replaced with the token “NUMBER.”5 Following standard practice, we used sections 2-21 for training, section 22 for development, and section 23 for test. wsj10 contains hand-annotated constituency parses, not dependency parses, so we used the standard “constituency5Numbers were treated in this way only in wsj10. to-dependency” conversion tool of Johansson and Nugues (2007) to obtain high-quality CoNLL-style dependency parses. 4.1.2 swbdnxt10 Next, we evaluate on swbdnxt10, which contains all sentences up to length 10 from the same sections of the swbdnxt version of Switchboard used by Pate and Goldwater (2011). Short sentences are usually formulaic discourse responses (e.g. “oh ok”), so this dataset also excludes sentences shorter than three words. As our models successfully use word durations, this evaluation provides an important replication of the basic result from Pate and Goldwater (2011) with a different kind of syntactic model. swbdnxt10 has a forced ali</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for English. In Proceedings of NODALIDA 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Why doesn’t EM find good HMM POS-taggers.</title>
<date>2007</date>
<booktitle>In Proceedings ofEMNLP-CoNLL,</booktitle>
<pages>296--305</pages>
<contexts>
<context position="14823" citStr="Johnson, 2007" startWordPosition="2302" endWordPosition="2303">s off from a distribution that conditions on hw to a distribution that marginalizes out hw, and that the extent of backoff varies across hw; we can use this to back off more when we have less evidence about hw. This model only conditions on words; it does not generate them in the dependents. This means it is actually a conditional, rather than fully generative, model of observed POS tags and unobserved syntax conditioned on the observed words. Since identifying the true posterior distribution P(T, θ|C, α) is intractable, Headden et al. use Meanfield Variational Bayes (Kurihara and Sato, 2006; Johnson, 2007), which finds an approximation to the posterior using an iterative EM-like algorithm. In the E-step of VBEM, expected counts E(ri) are gathered for each latent variable using the Inside-Outside algorithm, exactly as in the E-step of traditional EM. The Maximization step differs from the M-Step of EM in two ways. First, the expected counts for each value of the latent variable ri are incremented by the hyperparameter αi. Second, the numerator and denominator are scaled by the function exp(ψ(·)), which reduces the probability of rare events. Specifically, the Pchoose distribution is estimated us</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Mark Johnson. 2007. Why doesn’t EM find good HMM POS-taggers. In Proceedings ofEMNLP-CoNLL, pages 296–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeremy G Kahn</author>
<author>Matthew Lease</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
<author>Mari Ostendorf</author>
</authors>
<title>Effective use of prosody in parsing conversational speech.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP,</booktitle>
<pages>233--240</pages>
<contexts>
<context position="7486" citStr="Kahn et al., 2005" startWordPosition="1109" endWordPosition="1112"> words, and words plus word duration worked even better. While these results are consistent with the prosodic bootstrapping hypothesis, we suggested that predictability bootstrapping (see below) might be a more plausible explanation. Other computational work has combined prosody with syntax, but only in supervised systems, and typically using hand-annotated prosodic information. For example, Huang and Harper (2010) used annotated prosodic breaks as a kind of punctuation in a supervised PCFG, while prosodic breaks learned in a semi-supervised way have been used as features for parse reranking (Kahn et al., 2005) or PCFG statesplitting (Dreyer and Shafran, 2007). In contrast to these methods, our approach observes neither parse trees nor prosodic annotations. 2.2 Predictability Bootstrapping On the basis of our HMM chunkers, we introduced the predictability bootstrapping hypothesis (Pate and Goldwater, 2011): the idea that word durations could be a useful cue to syntactic structure not (or not only) because they provide information about prosodic structure, but because they are a direct cue to syntactic predictability. It is well-established that talkers tend to pronounce words more quickly when they </context>
</contexts>
<marker>Kahn, Lease, Charniak, Johnson, Ostendorf, 2005</marker>
<rawString>Jeremy G Kahn, Matthew Lease, Eugene Charniak, Mark Johnson, and Mari Ostendorf. 2005. Effective use of prosody in parsing conversational speech. In Proceedings of HLT-EMNLP, pages 233–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Corpusbased induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>479--486</pages>
<contexts>
<context position="1433" citStr="Klein and Manning, 2004" startWordPosition="204" endWordPosition="207">r results, evaluated on both adult-directed and child-directed utterances, show that using word duration can improve parse quality relative to words-only baselines. These results support the idea that acoustic cues provide useful evidence about syntactic structure for language-learning infants, and motivate the use of word duration cues in NLP tasks with speech. 1 Introduction Unsupervised learning of syntax is difficult for NLP systems, yet infants perform this task routinely. Previous work in NLP has focused on using the implicit syntactic information available in part-of-speech (POS) tags (Klein and Manning, 2004), punctuation (Seginer, 2007; Spitkovsky et al., 2011b; Ponvert et al., 2011), and syntactic similarities between related languages (Cohen and Smith, 2009; Cohen et al., 2011). However, these approaches likely use the data in a very different way from children: neither POS tags nor punctuation are observed during language acquisition (although see Spitkovsky et al. (2011a) and Christodoulopoulos et al. (2012) for encouraging results using unsupervised POS tags), and many children learn in a broadly monolingual environment. This paper explores a possible source of information that NLP systems t</context>
<context position="3038" citStr="Klein and Manning, 2004" startWordPosition="443" endWordPosition="446">ause prosodic and syntactic structures sometimes coincide. More recently, we proposed (Pate and Goldwater, 2011) that infants might use word duration as a direct cue to syntactic structure (i.e., without requiring intermediate prosodic structure), because words in high-probability syntactic structures tend to be pronounced more quickly (Gahl and Garnsey, 2004; Gahl et al., 2006; Tily et al., 2009). Like most recent work on unsupervised parsing, we focus on learning syntactic dependencies. Our work is based on Headden et al. (2009)’s Bayesian version of the Dependency Model with Valence (DMV) (Klein and Manning, 2004), using interpolated backoff techniques to incorporate multiple information sources per token. However, whereas Headden et al. used words and POS tags as input, we use words and word duration information, presenting three variants of their model that use this information in slightly different ways.1 1By using neither gold-standard nor learned POS tags as input, our work differs from nearly all previous work on unsupervised dependency parsing. While learned tags might be plausible 63 Transactions of the Association for Computational Linguistics, 1 (2013) 63–74. Action Editor: Brian Roark. Submi</context>
<context position="10499" citStr="Klein and Manning (2004)" startWordPosition="1579" endWordPosition="1582"> duration into unsupervised dependency parsing, producing analyses like the one in Figure 1. Each arc is between two words, with the head at the non-arrow end of the arc, and the dependent at the arrow end. One word, the root, depends on no word, and all other words depend on exactly one word. Following previous work on unsupervised dependency parsing, we will not label the arcs. 2The implementation of these models is available at http://github.com/jpate/predictabilityParsing 3.1 Dependency Model with Valence All of our models are ultimately based on the Dependency Model with Valence (DMV) of Klein and Manning (2004), a generative, probabilistic model for projective (i.e. no crossing arcs), unlabeled dependency parses, such as the one in Figure 1. The DMV generates dependency parses using three probability distributions, which together comprise model parameters 0. First, the root of the sentence is drawn from Proot. Second, we decide whether to stop generating dependents of the head h in direction dir E {left, right} with probability Pstop(·|h, dir, v), where v is T if h has a dir-ward dependent and F otherwise. If we decide to stop, then h takes no more dependents in the direction of dir. If we don’t sto</context>
<context position="26496" citStr="Klein and Manning (2004)" startWordPosition="4205" endWordPosition="4208">f distributions, and including αllNK) were 1, αB was 10, and α¬B was 1. VBEM was run on the training set until the data log-likelihood changed by less than 0.001%, and then the parameters were held fixed and used to obtain Viterbi parses for the evaluation sentences. Finally, we explored different global UNK cutoffs, replacing each word that appeared less than c times with the token UNK. We ran each model for each c ∈ {0, 1, 25, 50, 100}, and picked the best-scoring c on the development set for running on the test set and presentation here. We used a harmonic initializer similar to the one in Klein and Manning (2004). 4.3 Evaluation In addition to evaluating the various incarnations of the DMV with backoff and input types, we compare to uniform branching baselines, the Common Cover Link (CCL) parser of Seginer (2007), and the Unsupervised Partial Parser (UPP) of Ponvert et al. (2011). The UPP produces a constituency parse from words and punctuation using a series of finite-state chun6Available at http://homepages.inf.ed.ac.uk/s0930006/brentDep/ kers; we use the best-performing (Probabilistic Right Linear Grammar) version. The CCL parser produces a constituency parse using a novel “Cover Link” representati</context>
<context position="31750" citStr="Klein and Manning (2004)" startWordPosition="5090" endWordPosition="5093">2 presents results on wsj10. For the baseline models, the first column with horizontal text indicates the input, while for the backoff (Wds+POS) models, the first column with horizontal text indicates whether and how the extra stream is modeled in dependents (as described in Section 3.3). The EM model with POS input is largely a replication of the original DMV, differing in the use of separate train, dev, and test sets, and possibly the details of the harmonic initializer. Our replication achieves an undirected attachment score of 63.8 on the test set, similar to the score of 64.5 reported by Klein and Manning (2004) when training and evaluating on all of wsj10. Cohen et al. (2008) use the same train/dev/test partition that we do, and report a directed attachment score of 45.8, similar to our directed attachment score of 46.4. The VB model which learns from POS tags does not outperform the EM model which learns from POS tags, suggesting that data sparsity does not hurt the DMV when using POS tags. As expected, the wordsonly models perform much worse than both the POS input models and the uniform LH baseline. VB does improve the words-only constituency performance. The Cond. and Indep. backoff models outpe</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Christopher D. Manning. 2004. Corpusbased induction of syntactic structure: Models of dependency and constituency. In Proceedings of ACL, pages 479–486.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Trautner Kromann</author>
</authors>
<title>The Danish Dependency Treebank and the DTAG treebank tool.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second Workshop on Treebanks and Linguistic Theories,</booktitle>
<pages>217--220</pages>
<contexts>
<context position="25769" citStr="Kromann, 2003" startWordPosition="4082" endWordPosition="4083">iscarded only sentences shorter than three words from the evaluation sets. The Brent corpus is distributed via CHILDES (MacWhinney, 2000) with automatic dependency annotations. However, these are not hand-corrected, and rely on a different tokenization of the dataset than is present on the transcription tier. To produce a reliable gold-standard,6 we annotated all sentences of length 2 or greater from the development and test sets with dependencies drawn from the Stanford Typed Dependency set (de Marneffe and Manning, 2008) using the annotation tool used for the Copenhagen Dependency Treebank (Kromann, 2003). 4.2 Parameters In all experiments, hyperparameters for Proot, Pstop, and Pchoose (and their backed-off distributions, and including αllNK) were 1, αB was 10, and α¬B was 1. VBEM was run on the training set until the data log-likelihood changed by less than 0.001%, and then the parameters were held fixed and used to obtain Viterbi parses for the evaluation sentences. Finally, we explored different global UNK cutoffs, replacing each word that appeared less than c times with the token UNK. We ran each model for each c ∈ {0, 1, 25, 50, 100}, and picked the best-scoring c on the development set f</context>
</contexts>
<marker>Kromann, 2003</marker>
<rawString>Matthias Trautner Kromann. 2003. The Danish Dependency Treebank and the DTAG treebank tool. In Proceedings of the Second Workshop on Treebanks and Linguistic Theories, pages 217–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenichi Kurihara</author>
<author>Taisuke Sato</author>
</authors>
<title>Variational Bayesian grammar induction for natural language.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Colloquium on Grammatical Inference,</booktitle>
<pages>84--96</pages>
<contexts>
<context position="14807" citStr="Kurihara and Sato, 2006" startWordPosition="2298" endWordPosition="2301">tion 1 that ˆPchoose backs off from a distribution that conditions on hw to a distribution that marginalizes out hw, and that the extent of backoff varies across hw; we can use this to back off more when we have less evidence about hw. This model only conditions on words; it does not generate them in the dependents. This means it is actually a conditional, rather than fully generative, model of observed POS tags and unobserved syntax conditioned on the observed words. Since identifying the true posterior distribution P(T, θ|C, α) is intractable, Headden et al. use Meanfield Variational Bayes (Kurihara and Sato, 2006; Johnson, 2007), which finds an approximation to the posterior using an iterative EM-like algorithm. In the E-step of VBEM, expected counts E(ri) are gathered for each latent variable using the Inside-Outside algorithm, exactly as in the E-step of traditional EM. The Maximization step differs from the M-Step of EM in two ways. First, the expected counts for each value of the latent variable ri are incremented by the hyperparameter αi. Second, the numerator and denominator are scaled by the function exp(ψ(·)), which reduces the probability of rare events. Specifically, the Pchoose distribution</context>
</contexts>
<marker>Kurihara, Sato, 2006</marker>
<rawString>Kenichi Kurihara and Taisuke Sato. 2006. Variational Bayesian grammar induction for natural language. In Proceedings of the International Colloquium on Grammatical Inference, pages 84–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian MacWhinney</author>
</authors>
<title>The CHILDES project: Tools for analyzing talk. Lawrence Erlbaum Associates, Mahwah, NJ, third edition.</title>
<date>2000</date>
<contexts>
<context position="25292" citStr="MacWhinney, 2000" startWordPosition="4010" endWordPosition="4011">Brent and Siskind’s (2001) study, and, like wsj10 swbdnxt10 brent 68 swbdnxt10, has a forced alignment from which we obtain duration terciles. Rytting et al. (2010) used a 90%/10% train/test partition. We extracted every ninth utterance from the original training partition to create a dev set, producing an 80%/10%/10% partition. We also separated clitics from their base word. This dataset only has 186 sentences longer than ten words, with a maximum length of 22 words, so we discarded only sentences shorter than three words from the evaluation sets. The Brent corpus is distributed via CHILDES (MacWhinney, 2000) with automatic dependency annotations. However, these are not hand-corrected, and rely on a different tokenization of the dataset than is present on the transcription tier. To produce a reliable gold-standard,6 we annotated all sentences of length 2 or greater from the development and test sets with dependencies drawn from the Stanford Typed Dependency set (de Marneffe and Manning, 2008) using the annotation tool used for the Copenhagen Dependency Treebank (Kromann, 2003). 4.2 Parameters In all experiments, hyperparameters for Proot, Pstop, and Pchoose (and their backed-off distributions, and</context>
</contexts>
<marker>MacWhinney, 2000</marker>
<rawString>Brian MacWhinney. 2000. The CHILDES project: Tools for analyzing talk. Lawrence Erlbaum Associates, Mahwah, NJ, third edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S´everine Millotte</author>
<author>Roger Wales</author>
<author>Anne Christophe</author>
</authors>
<date>2007</date>
<booktitle>Phrasal prosody disambiguates syntax. Language and Cognitive Processes,</booktitle>
<pages>22--6</pages>
<contexts>
<context position="6353" citStr="Millotte et al., 2007" startWordPosition="935" endWordPosition="938">at language-acquiring infants exploit this correlation. Specifically, if infants can learn about prosodic phrase structure using word duration (and fundamenin a model of language acquisition, gold tags certainly are not. tal frequency), they may be able to identify syntactic phrases more easily using word strings and prosodic trees than using word strings alone. Several behavioral experiments support the connection between prosody and syntax and the prosodic bootstrapping hypothesis specifically. For example, there is evidence that adults use prosodic information for syntactic disambiguation (Millotte et al., 2007; Price et al., 1991) and to help in learning the syntax of an artificial language (Morgan et al., 1987), while infants can use acoustic-prosodic cues for utteranceinternal clause segmentation (Seidl, 2007). On the computational side, we are aware of only our previous HMM-based chunkers (Pate and Goldwater, 2011), which learned shallow syntax from words, words and word durations, or words and handannotated prosody. Using these chunkers, we found that using words plus prosodic annotation worked better than just words, and words plus word duration worked even better. While these results are cons</context>
</contexts>
<marker>Millotte, Wales, Christophe, 2007</marker>
<rawString>S´everine Millotte, Roger Wales, and Anne Christophe. 2007. Phrasal prosody disambiguates syntax. Language and Cognitive Processes, 22(6):898–909.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James L Morgan</author>
<author>Richard P Meier</author>
<author>Elissa L Newport</author>
</authors>
<title>Structural packaging in the input to language learning: contributions of prosodic and morphological marking of phrases to the acquisition of language. Cognitive Psychology,</title>
<date>1987</date>
<contexts>
<context position="6457" citStr="Morgan et al., 1987" startWordPosition="954" endWordPosition="957"> phrase structure using word duration (and fundamenin a model of language acquisition, gold tags certainly are not. tal frequency), they may be able to identify syntactic phrases more easily using word strings and prosodic trees than using word strings alone. Several behavioral experiments support the connection between prosody and syntax and the prosodic bootstrapping hypothesis specifically. For example, there is evidence that adults use prosodic information for syntactic disambiguation (Millotte et al., 2007; Price et al., 1991) and to help in learning the syntax of an artificial language (Morgan et al., 1987), while infants can use acoustic-prosodic cues for utteranceinternal clause segmentation (Seidl, 2007). On the computational side, we are aware of only our previous HMM-based chunkers (Pate and Goldwater, 2011), which learned shallow syntax from words, words and word durations, or words and handannotated prosody. Using these chunkers, we found that using words plus prosodic annotation worked better than just words, and words plus word duration worked even better. While these results are consistent with the prosodic bootstrapping hypothesis, we suggested that predictability bootstrapping (see b</context>
</contexts>
<marker>Morgan, Meier, Newport, 1987</marker>
<rawString>James L Morgan, Richard P Meier, and Elissa L Newport. 1987. Structural packaging in the input to language learning: contributions of prosodic and morphological marking of phrases to the acquisition of language. Cognitive Psychology, 19:498–550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Regina Barzilay</author>
</authors>
<title>Using semantic cues to learn syntax.</title>
<date>2011</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="38606" citStr="Naseem and Barzilay (2011)" startWordPosition="6226" endWordPosition="6229">or our main hypothesis, all three backoff models using words and duration outperform the words-only baselines (including CCL and UPP) on all dependency measures—the most accurate measures on this corpus, which has handannotated dependencies—and the Cond. model also wins on F-score. 6 Conclusion In this paper, we showed how to use the DMV with Backoff and two fully-generative variants to explore the utility of word duration in fully lexicalized unsupervised dependency parsing. Although other researchers have incorporated features beyond words and POS tags into DMV-like models (e.g., semantics: Naseem and Barzilay (2011); morphology: BergKirkpatrick et al. (2009)), we believe this is the first example based on Headden et al. (2009)’s backoff method. As far as we know, our work is also the first test of a DMV-based model on transcribed conversational speech and the first to outperform uniformbranching baselines without using either POS tags or punctuation in the input. Our results show that fullylexicalized models can do well if they are smoothed properly and exploit multiple cues. Our experiments also suggest that CDS is especially easy to learn from. Model performance on the brent dataset was generally highe</context>
</contexts>
<marker>Naseem, Barzilay, 2011</marker>
<rawString>Tahira Naseem and Regina Barzilay. 2011. Using semantic cues to learn syntax. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John K Pate</author>
<author>Sharon Goldwater</author>
</authors>
<title>Unsupervised syntactic chunking with acoustic cues: computational models for prosodic bootstrapping.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2nd ACL workshop on Cognitive Modeling and Computational Linguistics.</booktitle>
<contexts>
<context position="2526" citStr="Pate and Goldwater, 2011" startWordPosition="363" endWordPosition="366">nd many children learn in a broadly monolingual environment. This paper explores a possible source of information that NLP systems typically ignore: word duration, or the length of time taken to pronounce each word. There are good reasons to think that word duration might be useful for learning syntax. First, the well-established Prosodic Bootstrapping hypothesis (Gleitman and Wanner, 1982) proposes that infants use acoustic-prosodic cues (such as word duration) to help them identify syntactic structure, because prosodic and syntactic structures sometimes coincide. More recently, we proposed (Pate and Goldwater, 2011) that infants might use word duration as a direct cue to syntactic structure (i.e., without requiring intermediate prosodic structure), because words in high-probability syntactic structures tend to be pronounced more quickly (Gahl and Garnsey, 2004; Gahl et al., 2006; Tily et al., 2009). Like most recent work on unsupervised parsing, we focus on learning syntactic dependencies. Our work is based on Headden et al. (2009)’s Bayesian version of the Dependency Model with Valence (DMV) (Klein and Manning, 2004), using interpolated backoff techniques to incorporate multiple information sources per </context>
<context position="3944" citStr="Pate and Goldwater, 2011" startWordPosition="580" endWordPosition="583">ly different ways.1 1By using neither gold-standard nor learned POS tags as input, our work differs from nearly all previous work on unsupervised dependency parsing. While learned tags might be plausible 63 Transactions of the Association for Computational Linguistics, 1 (2013) 63–74. Action Editor: Brian Roark. Submitted 9/2012; Published 3/2013. c�2013 Association for Computational Linguistics. To our knowledge, this is the first work to incorporate acoustic cues into an unsupervised system for learning full syntactic parses. The methods in this paper were inspired by our previous approach (Pate and Goldwater, 2011), which showed that word duration measurements could improve the performance of an unsupervised lexicalized syntactic chunker over a words-only baseline. However, that work was limited to HMM-like sequence models, tested on adultdirected speech (ADS) only, and none of the models outperformed uniform-branching baselines. Here, we extend our results to full dependency parsing, and experiment on transcripts of both spontaneous ADS and child-directed speech (CDS). Our models using word duration outperform words-only baselines, along with the Common Cover Link parser of Seginer (2007), and the Unsu</context>
<context position="6667" citStr="Pate and Goldwater, 2011" startWordPosition="984" endWordPosition="988">ings and prosodic trees than using word strings alone. Several behavioral experiments support the connection between prosody and syntax and the prosodic bootstrapping hypothesis specifically. For example, there is evidence that adults use prosodic information for syntactic disambiguation (Millotte et al., 2007; Price et al., 1991) and to help in learning the syntax of an artificial language (Morgan et al., 1987), while infants can use acoustic-prosodic cues for utteranceinternal clause segmentation (Seidl, 2007). On the computational side, we are aware of only our previous HMM-based chunkers (Pate and Goldwater, 2011), which learned shallow syntax from words, words and word durations, or words and handannotated prosody. Using these chunkers, we found that using words plus prosodic annotation worked better than just words, and words plus word duration worked even better. While these results are consistent with the prosodic bootstrapping hypothesis, we suggested that predictability bootstrapping (see below) might be a more plausible explanation. Other computational work has combined prosody with syntax, but only in supervised systems, and typically using hand-annotated prosodic information. For example, Huan</context>
<context position="21389" citStr="Pate and Goldwater (2011)" startWordPosition="3395" endWordPosition="3398">t Word tokens 42,505 1,765 2,571 Word types 7,804 818 1,134 Sentences 6,007 233 357 Word tokens 24,998 2,980 3,052 Word types 2,647 760 767 Sentences 3,998 488 491 Word tokens 20,954 2,127 2,206 Word types 1,390 482 488 Sentences 6,249 424 449 Table 1: Statistics for our three corpora. Proot over both streams jointly and independently, respectively. 4 Experimental Setup 4.1 Datasets We evaluate on three datasets: wsj10, sentences of length 10 or less from the Wall Street Journal portion of the Penn Treebank; swbdnxt10, sentences of length 10 or less from the Switchboard dataset of ADS used by Pate and Goldwater (2011); and brent, part of the Brent corpus of CDS (Brent and Siskind, 2001). Table 1 presents corpus statistics. 4.1.1 wsj10 We present a new evaluation of the DMV with Backoff on wsj10, which does not have any acoustic information, simply to verify that αoNK performs sensibly on a standard corpus. Additionally, Headden et al. (2009) use an intensive initializer that relies on dozens of random restarts, and so, strictly speaking, only show that the backoff technology is useful for good initializations. Our new evaluation will show that the backoff technology provides a substantial benefit even for </context>
<context position="22860" citStr="Pate and Goldwater (2011)" startWordPosition="3626" endWordPosition="3629">ers were replaced with the token “NUMBER.”5 Following standard practice, we used sections 2-21 for training, section 22 for development, and section 23 for test. wsj10 contains hand-annotated constituency parses, not dependency parses, so we used the standard “constituency5Numbers were treated in this way only in wsj10. to-dependency” conversion tool of Johansson and Nugues (2007) to obtain high-quality CoNLL-style dependency parses. 4.1.2 swbdnxt10 Next, we evaluate on swbdnxt10, which contains all sentences up to length 10 from the same sections of the swbdnxt version of Switchboard used by Pate and Goldwater (2011). Short sentences are usually formulaic discourse responses (e.g. “oh ok”), so this dataset also excludes sentences shorter than three words. As our models successfully use word durations, this evaluation provides an important replication of the basic result from Pate and Goldwater (2011) with a different kind of syntactic model. swbdnxt10 has a forced alignment of a dictionary-based phonetic transcription of each utterance to audio, providing our word duration information. As a very simple model of hyper-articulation and hypo-articulation, we classify a word as in the longest third duration, </context>
<context position="39583" citStr="Pate and Goldwater (2011)" startWordPosition="6387" endWordPosition="6390">t. Our results show that fullylexicalized models can do well if they are smoothed properly and exploit multiple cues. Our experiments also suggest that CDS is especially easy to learn from. Model performance on the brent dataset was generally higher than on swbdnxt10, with a much lower UNK threshold. This latter point, and the fact that brent has a much lower word type/token ratio than the other datasets, suggest that CDS provides more and clearer evidence about words’ syntactic behavior. Finally, our results provide more evidence, using a different, more powerful syntactic model than that of Pate and Goldwater (2011), that word duration is a useful cue for unsupervised parsing. We found that several ways of incorporating duration were useful, although the extra sparsity of Joint emissions was not justified in any of our investigations. Our results are consistent with both the prosodic and predictability bootstrapping hypotheses of language acquisition, providing the first computational support for these using a full syntactic parsing model and tested on child-directed speech. While our models do not provide a mechanistic account of how children might use duration information to help with learning syntax, </context>
</contexts>
<marker>Pate, Goldwater, 2011</marker>
<rawString>John K Pate and Sharon Goldwater. 2011. Unsupervised syntactic chunking with acoustic cues: computational models for prosodic bootstrapping. In Proceedings of the 2nd ACL workshop on Cognitive Modeling and Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elias Ponvert</author>
<author>Jason Baldridge</author>
<author>Katrin Erk</author>
</authors>
<title>Simple unsupervised grammar induction from raw text with cascaded finite state models.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT.</booktitle>
<contexts>
<context position="1510" citStr="Ponvert et al., 2011" startWordPosition="215" endWordPosition="218">that using word duration can improve parse quality relative to words-only baselines. These results support the idea that acoustic cues provide useful evidence about syntactic structure for language-learning infants, and motivate the use of word duration cues in NLP tasks with speech. 1 Introduction Unsupervised learning of syntax is difficult for NLP systems, yet infants perform this task routinely. Previous work in NLP has focused on using the implicit syntactic information available in part-of-speech (POS) tags (Klein and Manning, 2004), punctuation (Seginer, 2007; Spitkovsky et al., 2011b; Ponvert et al., 2011), and syntactic similarities between related languages (Cohen and Smith, 2009; Cohen et al., 2011). However, these approaches likely use the data in a very different way from children: neither POS tags nor punctuation are observed during language acquisition (although see Spitkovsky et al. (2011a) and Christodoulopoulos et al. (2012) for encouraging results using unsupervised POS tags), and many children learn in a broadly monolingual environment. This paper explores a possible source of information that NLP systems typically ignore: word duration, or the length of time taken to pronounce each</context>
<context position="4592" citStr="Ponvert et al. (2011)" startWordPosition="676" endWordPosition="680">ration measurements could improve the performance of an unsupervised lexicalized syntactic chunker over a words-only baseline. However, that work was limited to HMM-like sequence models, tested on adultdirected speech (ADS) only, and none of the models outperformed uniform-branching baselines. Here, we extend our results to full dependency parsing, and experiment on transcripts of both spontaneous ADS and child-directed speech (CDS). Our models using word duration outperform words-only baselines, along with the Common Cover Link parser of Seginer (2007), and the Unsupervised Partial Parser of Ponvert et al. (2011), unsupervised lexicalized parsers that have obtained state-of-the-art results on standard newswire treebanks (though their performance here is worse, as our input lacks punctuation). We also outperform uniform-branching baselines. 2 Syntax and Word Duration Before presenting our models and experiments, we first discuss why word duration might be a useful cue to syntax. This section reviews the two possible reasons mentioned above: duration as a cue to prosodic structure, or as a cue to predictability. 2.1 Prosodic Bootstrapping Prosody is the structure of speech as conveyed by rhythm and into</context>
<context position="26768" citStr="Ponvert et al. (2011)" startWordPosition="4249" endWordPosition="4252">, we explored different global UNK cutoffs, replacing each word that appeared less than c times with the token UNK. We ran each model for each c ∈ {0, 1, 25, 50, 100}, and picked the best-scoring c on the development set for running on the test set and presentation here. We used a harmonic initializer similar to the one in Klein and Manning (2004). 4.3 Evaluation In addition to evaluating the various incarnations of the DMV with backoff and input types, we compare to uniform branching baselines, the Common Cover Link (CCL) parser of Seginer (2007), and the Unsupervised Partial Parser (UPP) of Ponvert et al. (2011). The UPP produces a constituency parse from words and punctuation using a series of finite-state chun6Available at http://homepages.inf.ed.ac.uk/s0930006/brentDep/ kers; we use the best-performing (Probabilistic Right Linear Grammar) version. The CCL parser produces a constituency parse using a novel “Cover Link” representation, scoring these links heuristically. Both CCL and UPP rely on punctuation (though according to Ponvert et al. (2011), UPP less so), which our input is missing. The left-headed “LH” (right-headed “RH”) baseline assumes that each word takes the first word to its right (le</context>
</contexts>
<marker>Ponvert, Baldridge, Erk, 2011</marker>
<rawString>Elias Ponvert, Jason Baldridge, and Katrin Erk. 2011. Simple unsupervised grammar induction from raw text with cascaded finite state models. In Proceedings of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patti J Price</author>
<author>Mari Ostendorf</author>
<author>Stefanie Shattuck-Hufnagel</author>
<author>Cynthia Fong</author>
</authors>
<title>The use of prosody in syntactic disambiguation.</title>
<date>1991</date>
<booktitle>In Proceedings of the HLT workshop on Speech and Natural Language,</booktitle>
<pages>372--377</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6374" citStr="Price et al., 1991" startWordPosition="939" endWordPosition="942">nfants exploit this correlation. Specifically, if infants can learn about prosodic phrase structure using word duration (and fundamenin a model of language acquisition, gold tags certainly are not. tal frequency), they may be able to identify syntactic phrases more easily using word strings and prosodic trees than using word strings alone. Several behavioral experiments support the connection between prosody and syntax and the prosodic bootstrapping hypothesis specifically. For example, there is evidence that adults use prosodic information for syntactic disambiguation (Millotte et al., 2007; Price et al., 1991) and to help in learning the syntax of an artificial language (Morgan et al., 1987), while infants can use acoustic-prosodic cues for utteranceinternal clause segmentation (Seidl, 2007). On the computational side, we are aware of only our previous HMM-based chunkers (Pate and Goldwater, 2011), which learned shallow syntax from words, words and word durations, or words and handannotated prosody. Using these chunkers, we found that using words plus prosodic annotation worked better than just words, and words plus word duration worked even better. While these results are consistent with the proso</context>
</contexts>
<marker>Price, Ostendorf, Shattuck-Hufnagel, Fong, 1991</marker>
<rawString>Patti J Price, Mari Ostendorf, Stefanie Shattuck-Hufnagel, and Cynthia Fong. 1991. The use of prosody in syntactic disambiguation. In Proceedings of the HLT workshop on Speech and Natural Language, pages 372–377, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Anton Rytting</author>
<author>Chris Brew</author>
<author>Eric Fosler-Lussier</author>
</authors>
<title>Segmenting words from natural speech: subsegmental variation in segmental cues.</title>
<date>2010</date>
<journal>Journal of Child Language,</journal>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="24510" citStr="Rytting et al. (2010)" startWordPosition="3881" endWordPosition="3884">ependency conversion tool as for wsj10. We evaluated 200 randomly-selected sentences to check the accuracy of the conversion tool, which was designed for newspaper text. Excluding arcs involving words with no clear role in dependency structure (such as “um”), about 86% of the arcs were correct. While this rate is uncomfortably low, it is still much higher than unsupervised dependency parsers typically achieve, and so may provide a reasonable measure of relative dependency parse quality among competing systems. 4.1.3 brent We also evaluated our models on the “Large Brent” dataset introduced in Rytting et al. (2010), a portion of the Brent corpus of child-directed speech (Brent and Siskind, 2001). We call this corpus brent. It consists of utterances from four of the mothers in Brent and Siskind’s (2001) study, and, like wsj10 swbdnxt10 brent 68 swbdnxt10, has a forced alignment from which we obtain duration terciles. Rytting et al. (2010) used a 90%/10% train/test partition. We extracted every ninth utterance from the original training partition to create a dev set, producing an 80%/10%/10% partition. We also separated clitics from their base word. This dataset only has 186 sentences longer than ten word</context>
</contexts>
<marker>Rytting, Brew, Fosler-Lussier, 2010</marker>
<rawString>C Anton Rytting, Chris Brew, and Eric Fosler-Lussier. 2010. Segmenting words from natural speech: subsegmental variation in segmental cues. Journal of Child Language, 37(3):513–543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Schwartz</author>
<author>Omri Abend</author>
<author>Roi Reichart</author>
<author>Ari Rappoport1</author>
</authors>
<title>Neutralizing linguistically problematic annotations in unsupervised dependency parsing evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th ACL,</booktitle>
<pages>663--672</pages>
<marker>Schwartz, Abend, Reichart, Rappoport1, 2011</marker>
<rawString>Roy Schwartz, Omri Abend, Roi Reichart, and Ari Rappoport1. 2011. Neutralizing linguistically problematic annotations in unsupervised dependency parsing evaluation. In Proceedings of the 49th ACL, pages 663–672.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Seginer</author>
</authors>
<title>Fast unsupervised incremental parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1461" citStr="Seginer, 2007" startWordPosition="209" endWordPosition="210">cted and child-directed utterances, show that using word duration can improve parse quality relative to words-only baselines. These results support the idea that acoustic cues provide useful evidence about syntactic structure for language-learning infants, and motivate the use of word duration cues in NLP tasks with speech. 1 Introduction Unsupervised learning of syntax is difficult for NLP systems, yet infants perform this task routinely. Previous work in NLP has focused on using the implicit syntactic information available in part-of-speech (POS) tags (Klein and Manning, 2004), punctuation (Seginer, 2007; Spitkovsky et al., 2011b; Ponvert et al., 2011), and syntactic similarities between related languages (Cohen and Smith, 2009; Cohen et al., 2011). However, these approaches likely use the data in a very different way from children: neither POS tags nor punctuation are observed during language acquisition (although see Spitkovsky et al. (2011a) and Christodoulopoulos et al. (2012) for encouraging results using unsupervised POS tags), and many children learn in a broadly monolingual environment. This paper explores a possible source of information that NLP systems typically ignore: word durati</context>
<context position="4530" citStr="Seginer (2007)" startWordPosition="668" endWordPosition="669">h (Pate and Goldwater, 2011), which showed that word duration measurements could improve the performance of an unsupervised lexicalized syntactic chunker over a words-only baseline. However, that work was limited to HMM-like sequence models, tested on adultdirected speech (ADS) only, and none of the models outperformed uniform-branching baselines. Here, we extend our results to full dependency parsing, and experiment on transcripts of both spontaneous ADS and child-directed speech (CDS). Our models using word duration outperform words-only baselines, along with the Common Cover Link parser of Seginer (2007), and the Unsupervised Partial Parser of Ponvert et al. (2011), unsupervised lexicalized parsers that have obtained state-of-the-art results on standard newswire treebanks (though their performance here is worse, as our input lacks punctuation). We also outperform uniform-branching baselines. 2 Syntax and Word Duration Before presenting our models and experiments, we first discuss why word duration might be a useful cue to syntax. This section reviews the two possible reasons mentioned above: duration as a cue to prosodic structure, or as a cue to predictability. 2.1 Prosodic Bootstrapping Pro</context>
<context position="26700" citStr="Seginer (2007)" startWordPosition="4239" endWordPosition="4240">o obtain Viterbi parses for the evaluation sentences. Finally, we explored different global UNK cutoffs, replacing each word that appeared less than c times with the token UNK. We ran each model for each c ∈ {0, 1, 25, 50, 100}, and picked the best-scoring c on the development set for running on the test set and presentation here. We used a harmonic initializer similar to the one in Klein and Manning (2004). 4.3 Evaluation In addition to evaluating the various incarnations of the DMV with backoff and input types, we compare to uniform branching baselines, the Common Cover Link (CCL) parser of Seginer (2007), and the Unsupervised Partial Parser (UPP) of Ponvert et al. (2011). The UPP produces a constituency parse from words and punctuation using a series of finite-state chun6Available at http://homepages.inf.ed.ac.uk/s0930006/brentDep/ kers; we use the best-performing (Probabilistic Right Linear Grammar) version. The CCL parser produces a constituency parse using a novel “Cover Link” representation, scoring these links heuristically. Both CCL and UPP rely on punctuation (though according to Ponvert et al. (2011), UPP less so), which our input is missing. The left-headed “LH” (right-headed “RH”) b</context>
</contexts>
<marker>Seginer, 2007</marker>
<rawString>Yoav Seginer. 2007. Fast unsupervised incremental parsing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amanda Seidl</author>
</authors>
<title>Infants’ use and weighting of prosodic cues in clause segmentation.</title>
<date>2007</date>
<journal>Journal of Memory and Language,</journal>
<volume>57</volume>
<issue>1</issue>
<contexts>
<context position="6559" citStr="Seidl, 2007" startWordPosition="969" endWordPosition="970"> not. tal frequency), they may be able to identify syntactic phrases more easily using word strings and prosodic trees than using word strings alone. Several behavioral experiments support the connection between prosody and syntax and the prosodic bootstrapping hypothesis specifically. For example, there is evidence that adults use prosodic information for syntactic disambiguation (Millotte et al., 2007; Price et al., 1991) and to help in learning the syntax of an artificial language (Morgan et al., 1987), while infants can use acoustic-prosodic cues for utteranceinternal clause segmentation (Seidl, 2007). On the computational side, we are aware of only our previous HMM-based chunkers (Pate and Goldwater, 2011), which learned shallow syntax from words, words and word durations, or words and handannotated prosody. Using these chunkers, we found that using words plus prosodic annotation worked better than just words, and words plus word duration worked even better. While these results are consistent with the prosodic bootstrapping hypothesis, we suggested that predictability bootstrapping (see below) might be a more plausible explanation. Other computational work has combined prosody with syntax</context>
</contexts>
<marker>Seidl, 2007</marker>
<rawString>Amanda Seidl. 2007. Infants’ use and weighting of prosodic cues in clause segmentation. Journal of Memory and Language, 57(1):24–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Angel X Chang</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Unsupervised dependency parsing without gold part-of-speech tags.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1486" citStr="Spitkovsky et al., 2011" startWordPosition="211" endWordPosition="214">directed utterances, show that using word duration can improve parse quality relative to words-only baselines. These results support the idea that acoustic cues provide useful evidence about syntactic structure for language-learning infants, and motivate the use of word duration cues in NLP tasks with speech. 1 Introduction Unsupervised learning of syntax is difficult for NLP systems, yet infants perform this task routinely. Previous work in NLP has focused on using the implicit syntactic information available in part-of-speech (POS) tags (Klein and Manning, 2004), punctuation (Seginer, 2007; Spitkovsky et al., 2011b; Ponvert et al., 2011), and syntactic similarities between related languages (Cohen and Smith, 2009; Cohen et al., 2011). However, these approaches likely use the data in a very different way from children: neither POS tags nor punctuation are observed during language acquisition (although see Spitkovsky et al. (2011a) and Christodoulopoulos et al. (2012) for encouraging results using unsupervised POS tags), and many children learn in a broadly monolingual environment. This paper explores a possible source of information that NLP systems typically ignore: word duration, or the length of time</context>
</contexts>
<marker>Spitkovsky, Alshawi, Chang, Jurafsky, 2011</marker>
<rawString>Valentin I Spitkovsky, Hiyan Alshawi, Angel X Chang, and Daniel Jurafsky. 2011a. Unsupervised dependency parsing without gold part-of-speech tags. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Punctuation: Making a point in unsupervised dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="1486" citStr="Spitkovsky et al., 2011" startWordPosition="211" endWordPosition="214">directed utterances, show that using word duration can improve parse quality relative to words-only baselines. These results support the idea that acoustic cues provide useful evidence about syntactic structure for language-learning infants, and motivate the use of word duration cues in NLP tasks with speech. 1 Introduction Unsupervised learning of syntax is difficult for NLP systems, yet infants perform this task routinely. Previous work in NLP has focused on using the implicit syntactic information available in part-of-speech (POS) tags (Klein and Manning, 2004), punctuation (Seginer, 2007; Spitkovsky et al., 2011b; Ponvert et al., 2011), and syntactic similarities between related languages (Cohen and Smith, 2009; Cohen et al., 2011). However, these approaches likely use the data in a very different way from children: neither POS tags nor punctuation are observed during language acquisition (although see Spitkovsky et al. (2011a) and Christodoulopoulos et al. (2012) for encouraging results using unsupervised POS tags), and many children learn in a broadly monolingual environment. This paper explores a possible source of information that NLP systems typically ignore: word duration, or the length of time</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2011</marker>
<rawString>Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. 2011b. Punctuation: Making a point in unsupervised dependency parsing. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harry Tily</author>
<author>Susanne Gahl</author>
<author>Inbal Arnon</author>
<author>Neal Snider</author>
<author>Anubha Kothari</author>
<author>Joan Bresnan</author>
</authors>
<title>Syntactic probabilities affect pronunciation variation in spontaneous speech.</title>
<date>2009</date>
<journal>Language and Cognition,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="2814" citStr="Tily et al., 2009" startWordPosition="408" endWordPosition="411"> syntax. First, the well-established Prosodic Bootstrapping hypothesis (Gleitman and Wanner, 1982) proposes that infants use acoustic-prosodic cues (such as word duration) to help them identify syntactic structure, because prosodic and syntactic structures sometimes coincide. More recently, we proposed (Pate and Goldwater, 2011) that infants might use word duration as a direct cue to syntactic structure (i.e., without requiring intermediate prosodic structure), because words in high-probability syntactic structures tend to be pronounced more quickly (Gahl and Garnsey, 2004; Gahl et al., 2006; Tily et al., 2009). Like most recent work on unsupervised parsing, we focus on learning syntactic dependencies. Our work is based on Headden et al. (2009)’s Bayesian version of the Dependency Model with Valence (DMV) (Klein and Manning, 2004), using interpolated backoff techniques to incorporate multiple information sources per token. However, whereas Headden et al. used words and POS tags as input, we use words and word duration information, presenting three variants of their model that use this information in slightly different ways.1 1By using neither gold-standard nor learned POS tags as input, our work dif</context>
<context position="8651" citStr="Tily et al., 2009" startWordPosition="1290" endWordPosition="1293">lkers tend to pronounce words more quickly when they are more predictable, as measured by, e.g., word frequency, n-gram probability, or whether or not the word has been previously mentioned (Aylett and Turk, 2004; Bell et al., 2009). However, syntactic proba64 you threw it right at the basket Figure 1: Example unlabeled dependency parse. bility also seems to matter, with studies showing that verbs tend to be pronounced more quickly when they are in their preferred syntactic frame—transitive vs. intransitive or direct object vs. sentential complement (Gahl and Garnsey, 2004; Gahl et al., 2006; Tily et al., 2009). While this syntactic evidence is only for verbs, together with the evidence that effects of other notions of predictability, it suggests that such syntactic effects may also be widespread. If so, the duration of a word could give clues as to whether it is being used in a high-probability or low-probability structure, and thus what the correct structure is. We found that our syntactic chunkers benefited more from duration information than prosodic annotations, providing some preliminary evidence in favor of predictability bootstrapping, but not ruling out prosodic bootstrapping. So, we are le</context>
</contexts>
<marker>Tily, Gahl, Arnon, Snider, Kothari, Bresnan, 2009</marker>
<rawString>Harry Tily, Susanne Gahl, Inbal Arnon, Neal Snider, Anubha Kothari, and Joan Bresnan. 2009. Syntactic probabilities affect pronunciation variation in spontaneous speech. Language and Cognition, 1(2):147–165.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>