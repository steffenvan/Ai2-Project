<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001228">
<title confidence="0.715061">
The Johns Hopkins SENSEVAL2 System Descriptions
</title>
<author confidence="0.66346">
David Yarowsky, Silviu Cucerzan, Radu Florian,
Charles Schafer and Richard Wicentowski
</author>
<affiliation confidence="0.75079725">
{ yarowsky,silviu,rflorian , cschafer ,richardw} ©es jhu .edu
Department of Computer Science
Johns Hopkins University
Baltimore, Maryland, 21218, USA
</affiliation>
<sectionHeader confidence="0.900865" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99995352631579">
This article describes the Johns Hopkins Univer-
sity (JHU) sense disambiguation systems that par-
ticipated in seven SENSEVAL2 tasks: four super-
vised lexical choice systems (Basque, English, Span-
ish, Swedish), one unsupervised lexical choice sys-
tem (Italian) and two supervised all-words systems
(Czech, Estonian). The common core supervised
system utilizes voting-based classifier combination
over several diverse systems, including decision lists
(Yarowsky, 2000), a cosine-based vector model and
two Bayesian classifiers. The classifiers employed a
rich set of features, including words, lemmas and
part-of-speech informatino modeled in several syn-
tactic relationships (e.g. verb-object), bag-of-words
context and local collocational n-grams. The all-
words systems relied heavily on morphological anal-
ysis in the two highly inflected languages. The un-
supervised Italian system was a hierarchical class
model using the Italian WordNet.
</bodyText>
<sectionHeader confidence="0.828973" genericHeader="method">
1 The Feature Space
</sectionHeader>
<bodyText confidence="0.999095888888889">
The JHU SENSEVAL2 systems utilized a rich fea-
ture space based on raw words, lemmas and part-
of-speech (POS) tags in a variety of positional re-
lationships to the target word. These positions in-
clude traditional bag-of-word context, local bigram
and trigram collocations and several syntactic re-
lationships based on predicate-argument structure
(described in Section 1.2). Their use is illustrated
on a sample English sentence for train in Figure 1.
</bodyText>
<subsectionHeader confidence="0.9702695">
1.1 Part-of-Speech Tagging and
Lemmatization
</subsectionHeader>
<bodyText confidence="0.999534">
Part-of-speech tagger availability varied across the
languages included in this sense-disambiguation sys-
tem evaluation. Transformation-based taggers (Ngai
and Florian, 2001) were trained on standard data
for English (Penn Treebank), Swedish (SUC-1 cor-
pus) and Estonian (MultextEast corpus). For Czech,
an available POS tagger (Haji and Hladka, 1998),
which includes lemmatization, was used. The re-
maining languages — Spanish, Italian and Basque —
were tagged using an unsupervised tagger (Cucerzan
</bodyText>
<table confidence="0.914263238095238">
&amp;quot;Many mothers do not even try to toilet train
their children until the age of 2 years or later ...&amp;quot;
Feature Word POS Lemma
type
Context ... ... ...
Context try VB try/v
Context to TO tO/T
Context toilet NN toilet/N
Context train VBP train/v
Context their DT their/D
Context ... ... ...
Syntactic (predicate-argument) features
Object children NNS Child/N
Prep until IN until/I
ObjPrep age NN age/N
Ngram collocational features
-1 bigram toilet NN toilet/N
+1 bigram their DT their/D
-2/-1 trigram to toilet . TO-NN tO/T toilet/N .
-1/ +1 trigram to . their TO-DT tO/T * their/D
+11+2 trigram their children DT-NN their/D child/N
</table>
<figureCaption confidence="0.998837">
Figure 1: Example sentence and extracted features
</figureCaption>
<bodyText confidence="0.9854572">
and Yarowsky, 2000). Lemmatization was per-
formed using a combination of supervised and un-
supervised methods (Yarowsky and Wicentowski,
2000), and using existing trie-based supervised mod-
els for English.
</bodyText>
<subsectionHeader confidence="0.991034">
1.2 Syntactic Features
</subsectionHeader>
<bodyText confidence="0.9919375">
Extracted syntactic relationships in the feature
space depended on the keyword&apos;s part of speech:
</bodyText>
<listItem confidence="0.995734">
* for verb keywords — the head noun of the
verb&apos;s object, particle/preposition and object-
of-preposition were extracted when available.
• for noun keywords — the headword of any verb-
object, subject-verb or noun-noun relationships
identified for the keyword.
• for adjective keywords — the head noun modified
by the adjective (if identifiable).
</listItem>
<bodyText confidence="0.918976">
These syntactic features were extracted using sim-
ple heuristic patterns and regular expressions over
the parts-of-speech surrounding the keyword.
</bodyText>
<page confidence="0.999263">
163
</page>
<sectionHeader confidence="0.925615" genericHeader="method">
2 Supervised Lexical Choice Systems
</sectionHeader>
<bodyText confidence="0.999794">
The supervised JHU systems utilize classifier com-
bination merging the results of five diverse learning
models.
</bodyText>
<subsectionHeader confidence="0.989962">
2.1 Core Algorithm Design
</subsectionHeader>
<bodyText confidence="0.999373333333333">
The lexical choice task can be cast as a classifica-
tion task: training data is given in the form of a set
of word-document pairs &apos;T = [(wi, Dii) , Sao (Sii
being the sense associated with the document
of keyword wi), labeled with the corresponding gold
standard class. The goal is to establish the clas-
sification of a set of unlabeled word-document pairs
= {(w, D)}. not previously seen in the train-
ing data. The training data T is used to estimate
class probabilities and then the sense classification
is made by choosing the class with the maximum a
posteriori class probability:
</bodyText>
<equation confidence="0.811048">
S = arg msax P (S&apos; ID) = arg msax P (SI) • P (DIS&apos;)
</equation>
<bodyText confidence="0.999977833333333">
The disambiguation models used in our exper-
iments are feature-based models. A feature is a
boolean function defined as fv, :FxD--&gt; {0, 1},
where F is the entire set of features and D is the
document space. An overview of the exploited fea-
ture space was given in Section 1.
</bodyText>
<subsectionHeader confidence="0.979568">
2.2 Vector-based Algorithms
</subsectionHeader>
<bodyText confidence="0.999985142857143">
Our Bayesian and cosine-based models use a com-
mon vector representation, capturing both tra-
ditional bag-of-words features and the extended
Ngram and predicate-argument features in a single
data structure.
In these models, a vector is created for each doc-
ument in the collection:
</bodyText>
<equation confidence="0.885438">
Di = (DJ) .
3=1,1F1
where F is the entire utilized feature space
Cii
3
</equation>
<bodyText confidence="0.999657454545455">
where ci3 is the the number of times the feature 43
appears in document Di, Ni is the number of words
in the document Di and 1473 is the weight associated
with the feature 43.
To avoid confusion between the same word in mul-
tiple feature roles, feature values are marked with
their positional type (e.g. children object, toilet_ L,
and their_R as distinct from children, toilet and
their in unmarked bag-of-words context).
The basic sense disambiguation algorithm pro-
ceeds as follows:
</bodyText>
<listItem confidence="0.982986666666667">
1. Vectors in the training data are assigned to
classes based on their classification;
2. For each vector in the test data, the a posteriori
</listItem>
<bodyText confidence="0.769404">
class distribution is computed as
</bodyText>
<equation confidence="0.9665295">
P(SID) =
Sim (D, Cs)
E sim (D,Cs,)
SI
</equation>
<bodyText confidence="0.9353518">
where Cs is the centroid corresponding to the
sense S and Sim is the similarity measure used
by the algorithm (cosine or Bayes).
3. The sample D is labeled with sense S if S =
arg max P (S&apos; ID).
</bodyText>
<subsubsectionHeader confidence="0.880775">
2.2.1 The Cosine-based Model
</subsubsectionHeader>
<bodyText confidence="0.99993825">
In this model, traditional cosine similarity is used
to compute similarity between a document D and
a centroid C. The weight associated with a feature
(F3) is its inverse document frequency Wi = log k,
where N is the total number of documents and N3
is the number of documents containing feature h.
Function words and POS tags were excluced from
the cosine vectors.
</bodyText>
<subsubsectionHeader confidence="0.981691">
2.2.2 The Bayesian Models
</subsubsectionHeader>
<bodyText confidence="0.8724172">
In the Bayes model, the Bayes similarity is computed
as:
Sim (Di, = P (Di, = P (Si) P
and the following assumption of independence is
made:
</bodyText>
<equation confidence="0.933535">
P (Dirs) = IJ P (f irs)
fc
</equation>
<bodyText confidence="0.999923666666667">
The probability distribution P (f3ICs) is obtained
by smoothing the word relative frequencies in the
cluster Cs. Given the lack of independence between
the word-based and lemma-based feature spaces,
these are utilized in two separate Bayesian models
with output combined in Section 2.5.
</bodyText>
<subsectionHeader confidence="0.988353">
2.3 Decision Lists
</subsectionHeader>
<bodyText confidence="0.9999915">
The decision list model we used in our system is
a non-hierarchical variant of the method of inter-
polated decision lists described in Yarowsky (2000).
For each feature L a smoothed log of likelihood ratio
(log pPY1_,IV)) is computed for each sense Si, with
smoothing based on an empirically estimated func-
tion of feature type and relative frequency. Can-
didate features are ordered by this smoothed ra-
tio (putting the best evidence first), and the re-
maining probabilities are computed via the interpo-
lation of the global and history-conditional proba-
bilities. By utilizing the single strongest-matching
evidence in context, non-independent feature spaces
combine readily without inflated confidence, and can
be mapped to accurate and robust probability esti-
mates as shown in Figure 2.
</bodyText>
<subsectionHeader confidence="0.987495">
2.4 Additional Details
</subsectionHeader>
<bodyText confidence="0.9994795">
The English task differs slightly from the other
lexical-choice tasks in that phrasal verbs are ex-
plicitly marked in the training and test data. To
make reasonable use of this information, when a
phrasal verb is marked, only corresponding phrasal
senses are considered; conversely when a phrasal
</bodyText>
<page confidence="0.983686">
164
</page>
<figure confidence="0.998697">
0.98
0.96
0.94
0.92
0.9
0.88
0.86
0.96 0.965 0.97 0.975 0.98 0.985 0.99 0.995
Raw Confidence Score
</figure>
<figureCaption confidence="0.9955605">
Figure 2: Mapping between raw confidence scores
and classification accuracy for English decision lists
</figureCaption>
<bodyText confidence="0.999552777777778">
verb is not marked, no phrasal senses are considered.
Likewise, when a training or test sentence matches
a compound noun in the observed sense inventory
(e.g. art_ gallery %I:06:00::) only the matching
phrasal sense(s) are considered unless there is at
least one non-phrasal sense tagged in the training
data for that compound (indicating the potential for
both compositional and non-compositional interpre-
tations).
</bodyText>
<subsectionHeader confidence="0.885588">
2.5 Classifier Combination
</subsectionHeader>
<bodyText confidence="0.999954384615385">
Several classifier combination approaches were inves-
tigated in the system development phase. They are
outlined below, along with their cross-validated per-
formance on the English lexical-sample training data
(in Table 1). In each case four individual classifiers
were combined: the cosine model, two Bayes models
(one based on words and one based on lemmasl),
and the decision-list model.
The first two model combination approches sim-
ply averages the output of the participating clas-
sifiers over each candidate sense tag, in terms of
P (S ID 2) and rank(S D) respectively, with each
classifier given an equal vote2.
The remaining methods assign potentially vari-
able weights to the votes of different classi-
fiers. Interestingly, Equal Weighting of all four
classifiers slightly outperforms classifier weighting
proportional to each model&apos;s aggregate accuracy
(Performance-Weighted voting), similar to the tech-
nique used for classifier combination in part-of-
speech tagging in van Halteren et al. (1998). Finally,
it was observed that on sentences where decision lists
have high model confidence their accuracy exceeds
other classifiers. Thus the most effective approach,
based on training-data cross validation, was found
to be a very basic Thresholded Model Voting:
</bodyText>
<footnote confidence="0.973871333333333">
10n training-set cross-validation it was observed that the
two systems were uncorrelated enough to make it useful to
keep both of them.
2 Decision lists are not included because they only assign a
probability to their selected classifier output but not to lower-
ranked candidates.
</footnote>
<listItem confidence="0.919390285714286">
• If the decision_ list _ confidence&gt; 0.985 (an em-
pirically selected threshold) then return the out-
put of the decision list;
• Otherwise, each system votes for the sense that
is most likely under it and, another vote is ob-
tained from the most probable class yielded by
linear interpolation of the 4 classifiers.
</listItem>
<bodyText confidence="0.9997428">
This simple top-performing approach was utilized in
the evaluation system, and is reasonably close to the
performance of an Oracle upper bound for classifier
combination (using the output of the single best clas-
sifier on each test instance — unknowable in practice).
</bodyText>
<table confidence="0.938545">
Model Averaging (excluding decision lists):
Probability interpolation voting .657 .728
Rank-averaged voting .652 .709
Weighted Model Voting (includes decision lists):
Equal-weighted Model Voting .667 .736
Performance-Weighted Voting .655 .724
Thresholded Model Voting .676 .746
Oracle Voting (Upper Bound) .734 .761
</table>
<tableCaption confidence="0.982357">
Table 1: Comparison of classifier combination meth-
ods on English (using 5-fold cross-validation)
</tableCaption>
<sectionHeader confidence="0.955985" genericHeader="method">
3 Supervised All-Words Systems
</sectionHeader>
<subsectionHeader confidence="0.996927">
3.1 Estonian All-words Task
</subsectionHeader>
<bodyText confidence="0.999993823529412">
Because of the importance of morphological analy-
sis in a highly inflected language such as Estonian,
a lemmatizer based on Yarowsky and Wicentowski
(2000) was first applied to all words in the train-
ing data (and, at evaluation time, the test data).
For each lemma, the P (sense lemma) distribution
was measured on the training data. For all lem-
mas exhibiting only one sense in the training data,
this sense was returned. Likewise, if there was in-
sufficient data for word-specific training (the sum of
the minority sense examples for the word in training
data was below a threshold) the majority sense in
training was returned for all instances of that lemma.
In the remaining cases where a lemma had more than
one sense in training, with sufficient minority exam-
ples to adequately be modeled, the generic JHU lex-
ical sample sense classifier was trained and applied.
</bodyText>
<subsectionHeader confidence="0.998834">
3.2 Czech All-words Task
</subsectionHeader>
<bodyText confidence="0.999658625">
Czech is another example of a highly inflected lan-
guage. A part-of-speech tagger and lemmatizer
kindly provided by Jan Haji 6 of Charles Univer-
sity (Haji 6 and Hladka, 1998) was first applied to
the data. Consistent with the spirit of evaluating
sense disambiguation rather than morphology, the
JHU system focused on those words where more
than one sense was possible for a root word (e.g.
</bodyText>
<figure confidence="0.9812515">
Classifier Combination Method
Accuracy
Fine Coarse
Fine Grained Sense Classification Accuracy
</figure>
<page confidence="0.995871">
165
</page>
<bodyText confidence="0.999966384615385">
the -1 and -2 suffixes in the Czech inventory). In
these cases, the fine-grained output of the Czech
lemmatizer was ignored (in both training and test)
and a generic lexical-sample sense classifier was ap-
plied to the sense-distinction tags extracted from the
lemmatized training data (see Section 2), using the
classification models employed in Estonian. When-
ever insufficient numbers of minority tagged exam-
ples were available for training a word-specific clas-
sifier, the majority sense for the POS-level lemma
was returned. Likewise, if only one possible sense
tag was observed for any POS-level lemma analysis,
then this unambiguous sense tag was returned.
</bodyText>
<sectionHeader confidence="0.994244" genericHeader="method">
4 Unsupervised Italian System
</sectionHeader>
<bodyText confidence="0.999951564102564">
The Italian task stands out from the group of lexical
choice tasks because no labelled training was data
provided for Italian; instead a subset of the Italian
Wordnet was provided. To obtain a sense classifier
for Italian, we employed an unsupervised method
that used hierarchical class models of the Wordnet
relationships among words (synonymy, hypernomy,
etc) and a large unannotated corpus of Italian news-
paper data to obtain sense centroids.
First, every relationship type in the Italian Word-
net received an initial weight, based on a roughly es-
timated measure of the relative dissimilarity of two
words in that relationship. For instance, the syn-
onymy relationship received a small weight (words
are semantically &amp;quot;close&amp;quot;), while other relationships
(has_near_synonym, causes, has_hypernym) re-
ceived proportionately larger weights (words are
more semantically distant). Starting from the senses
S of a target k, the wordnet relationships graph was
explored, up to a given distance (two links away),
creating &amp;quot;clouds&amp;quot; of similar words, Ms, together with
a similarity3 to the original sense, S.
For each of the words w in Ms, we extracted sen-
tences from the unannotated corpus that contained
the word w, and then considered them as being ex-
amples of context for the sense S of target k, and as-
signed them to the centroid Cs (the centroid of the
sense S) with a weight corresponding to the similar-
ity between the word w and the sense S (computed
using the wordnet graph). After all the documents
were distributed, the test documents were also as-
signed to the most probable cluster, similar to the
other lexical choice tasks.
The centroids were then allowed to adjust in a
manner similar to k-means clustering. At each
step, the centroids were recomputed, after which
each document migrated to the closest cluster (i.e.
arg maxs P (Cs1D)), and the process was repeated.
After the process converged, each test document was
</bodyText>
<footnote confidence="0.939923666666667">
3The weight on a path was computed as the sum of the
weights on the path, and the similarity was computed as
Sim (w, S) e—c(u&apos;,$) — large weights result in 0 similarity.
</footnote>
<table confidence="0.994773111111111">
Task Accuracy on Test Data
Fine-Grained Coarse-Grained
Basque .757 .971
English .642 .713
Spanish .712
Swedish .701 1.00
Italian .353 .423
Czech .935
Estonian .666
</table>
<tableCaption confidence="0.999188">
Table 2: Official JHU system performance
</tableCaption>
<bodyText confidence="0.96104925">
assigned the label corresponding to the sense cen-
troid it converged into. This process is completely
unsupervised, and the only structured resource that
was used is the provided Italian Wordnet subset.
</bodyText>
<sectionHeader confidence="0.999964" genericHeader="conclusions">
5 Results
</sectionHeader>
<bodyText confidence="0.998828230769231">
Table 2 lists the official performance of the JHU sys-
tems on unseen test data in the final SENSEVAL2
evaluation. Coarse-grained performance scores are
based on a hierarchical sense clustering given by the
task organizers in 4 of the languages. In the lexical
sample tasks, these scores were obtained after cor-
rection of a simple bug in the merger of final system
output as provided for in the SENSEVAL evaluation
protocols.
As illustrated in the comparative performance ta-
bles elsewhere in this volume, the JHU systems are
consistently very successful across all 7 languages
and 3 major system types described here.
</bodyText>
<sectionHeader confidence="0.999251" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999578772727273">
S. Cucerzan and D. Yarowsky. 2000. Language inde-
pendent minimally supervised induction of lexical
probabilities. In Proceedings of ACL-2000, pages
270-277, Hong Kong.
J. Haji 6 and Hladka. 1998. Tagging inflective lan-
guages: Prediction of morphological categories
for a rich, structured tagset. In Proceedings of
COLING/ACL-98, pages 483-490, Montréal.
G. Ngai and R. Florian. 2001. Transformation-
based learning in the fast lane. In Proceedings
of NAACL-2001, pages 40-47, Pittsburgh.
H. van Halteren, J. Zavrel and W. Daelemans.
1998. Improving Data Driven Wordclass Tag-
ging by System Combination In Proceedings of
COLING/ACL-1998, pages 491-497, Montreal.
D. Yarowsky and R. Wicehtowski. 2000. Minimally
supervised morphological analysis by multimodal
alignment. In Proceedings of ACL-2000, pages
207-216, Hong Kong.
D. Yarowsky. 2000. Hierarchical decision lists for
word sense disambiguation. Computers and the
Humanities, 34(2):179-186.
</reference>
<page confidence="0.998761">
166
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000417">
<title confidence="0.999445">The Johns Hopkins SENSEVAL2 System Descriptions</title>
<author confidence="0.857124333333333">David Yarowsky</author>
<author confidence="0.857124333333333">Silviu Cucerzan</author>
<author confidence="0.857124333333333">Radu Charles Schafer</author>
<author confidence="0.857124333333333">Richard Wicentowski yarowsky</author>
<author confidence="0.857124333333333">rflorian silviu</author>
<affiliation confidence="0.9994555">Department of Computer Johns Hopkins University</affiliation>
<address confidence="0.999986">Baltimore, Maryland, 21218, USA</address>
<abstract confidence="0.974782945244956">This article describes the Johns Hopkins University (JHU) sense disambiguation systems that participated in seven SENSEVAL2 tasks: four supervised lexical choice systems (Basque, English, Spanish, Swedish), one unsupervised lexical choice system (Italian) and two supervised all-words systems (Czech, Estonian). The common core supervised system utilizes voting-based classifier combination over several diverse systems, including decision lists (Yarowsky, 2000), a cosine-based vector model and two Bayesian classifiers. The classifiers employed a rich set of features, including words, lemmas and part-of-speech informatino modeled in several syntactic relationships (e.g. verb-object), bag-of-words context and local collocational n-grams. The allwords systems relied heavily on morphological analysis in the two highly inflected languages. The unsupervised Italian system was a hierarchical class model using the Italian WordNet. 1 The Feature Space The JHU SENSEVAL2 systems utilized a rich feature space based on raw words, lemmas and partof-speech (POS) tags in a variety of positional relationships to the target word. These positions include traditional bag-of-word context, local bigram and trigram collocations and several syntactic relationships based on predicate-argument structure (described in Section 1.2). Their use is illustrated a sample English sentence for Figure 1. 1.1 Part-of-Speech Tagging and Lemmatization Part-of-speech tagger availability varied across the languages included in this sense-disambiguation system evaluation. Transformation-based taggers (Ngai and Florian, 2001) were trained on standard data for English (Penn Treebank), Swedish (SUC-1 corpus) and Estonian (MultextEast corpus). For Czech, an available POS tagger (Haji and Hladka, 1998), which includes lemmatization, was used. The remaining languages — Spanish, Italian and Basque — were tagged using an unsupervised tagger (Cucerzan mothers do not even try to toilet their children until the age of 2 years or later ...&amp;quot; Feature type Word POS Lemma Context Context Context Context Context Context Context ... try to toilet train their ... ... try/v VB TO NN VBP DT tO/T ... toilet/N train/v their/D ... features ObjPrep age NN Child/N age/N Ngram collocational features -1 bigram +1 bigram +1 +11+2 trigram their NN their/D to toilet DT TO-NN TO-DT DT-NN tO/T toilet/N to . * their children their/D child/N Figure 1: Example sentence and extracted features and Yarowsky, 2000). Lemmatization was performed using a combination of supervised and unsupervised methods (Yarowsky and Wicentowski, 2000), and using existing trie-based supervised models for English. Features Extracted syntactic relationships in the feature space depended on the keyword&apos;s part of speech: * for verb keywords — the head noun of the verb&apos;s object, particle/preposition and objectof-preposition were extracted when available. • for noun keywords — the headword of any verbobject, subject-verb or noun-noun relationships identified for the keyword. • for adjective keywords — the head noun modified by the adjective (if identifiable). These syntactic features were extracted using simple heuristic patterns and regular expressions over the parts-of-speech surrounding the keyword. 163 2 Supervised Lexical Choice Systems The supervised JHU systems utilize classifier combination merging the results of five diverse learning models. 2.1 Core Algorithm Design The lexical choice task can be cast as a classification task: training data is given in the form of a set word-document pairs = [(wi, , being the sense associated with the document keyword labeled with the corresponding gold standard class. The goal is to establish the classification of a set of unlabeled word-document pairs D)}. previously seen in the training data. The training data T is used to estimate class probabilities and then the sense classification is made by choosing the class with the maximum a posteriori class probability: = (S&apos; ID) = • P (DIS&apos;) The disambiguation models used in our experiments are feature-based models. A feature is a function defined as :FxD--&gt; 1}, the entire set of features and the document space. An overview of the exploited feaspace was given in Section 2.2 Vector-based Algorithms Our Bayesian and cosine-based models use a common vector representation, capturing both traditional bag-of-words features and the extended Ngram and predicate-argument features in a single data structure. In these models, a vector is created for each document in the collection: = . the entire utilized feature space Cii 3 the the number of times the feature in document Ni the number of words the document and is the weight associated the feature To avoid confusion between the same word in multiple feature roles, feature values are marked with positional type (e.g. object, toilet_ L, distinct from toilet unmarked bag-of-words context). The basic sense disambiguation algorithm proceeds as follows: 1. Vectors in the training data are assigned to classes based on their classification; 2. For each vector in the test data, the a posteriori class distribution is computed as P(SID) = Cs) SI the centroid corresponding to the Sim is the similarity measure used by the algorithm (cosine or Bayes). The sample is with sense = max (S&apos; ID). 2.2.1 The Cosine-based Model In this model, traditional cosine similarity is used compute similarity between a document a centroid C. The weight associated with a feature its inverse document frequency = log the total number of documents and the number of documents containing feature Function words and POS tags were excluced from the cosine vectors. 2.2.2 The Bayesian Models In the Bayes model, the Bayes similarity is computed as: Sim (Di, = P (Di, = P (Si) P and the following assumption of independence is made: P (Dirs) = IJ P (f irs) fc probability distribution obtained by smoothing the word relative frequencies in the the lack of independence between the word-based and lemma-based feature spaces, these are utilized in two separate Bayesian models with output combined in Section 2.5. 2.3 Decision Lists The decision list model we used in our system is a non-hierarchical variant of the method of interpolated decision lists described in Yarowsky (2000). each feature smoothed log of likelihood ratio computed for each sense smoothing based on an empirically estimated function of feature type and relative frequency. Candidate features are ordered by this smoothed ratio (putting the best evidence first), and the remaining probabilities are computed via the interpolation of the global and history-conditional probabilities. By utilizing the single strongest-matching evidence in context, non-independent feature spaces combine readily without inflated confidence, and can be mapped to accurate and robust probability estimates as shown in Figure 2. 2.4 Additional Details The English task differs slightly from the other lexical-choice tasks in that phrasal verbs are explicitly marked in the training and test data. To make reasonable use of this information, when a phrasal verb is marked, only corresponding phrasal senses are considered; conversely when a phrasal 164 0.98 0.96 0.94 0.92 0.9 0.88 0.86 0.96 0.965 0.97 0.975 0.98 0.985 0.99 0.995 Raw Confidence Score Figure 2: Mapping between raw confidence scores and classification accuracy for English decision lists verb is not marked, no phrasal senses are considered. Likewise, when a training or test sentence matches a compound noun in the observed sense inventory art_ %I:06:00::) the matching phrasal sense(s) are considered unless there is at least one non-phrasal sense tagged in the training data for that compound (indicating the potential for both compositional and non-compositional interpretations). 2.5 Classifier Combination Several classifier combination approaches were investigated in the system development phase. They are outlined below, along with their cross-validated performance on the English lexical-sample training data (in Table 1). In each case four individual classifiers were combined: the cosine model, two Bayes models (one based on words and one based on lemmasl), and the decision-list model. The first two model combination approches simply averages the output of the participating classifiers over each candidate sense tag, in terms of (S ID D) with each given an equal The remaining methods assign potentially variable weights to the votes of different classifiers. Interestingly, Equal Weighting of all four classifiers slightly outperforms classifier weighting proportional to each model&apos;s aggregate accuracy (Performance-Weighted voting), similar to the technique used for classifier combination in part-ofspeech tagging in van Halteren et al. (1998). Finally, it was observed that on sentences where decision lists have high model confidence their accuracy exceeds other classifiers. Thus the most effective approach, based on training-data cross validation, was found to be a very basic Thresholded Model Voting: training-set cross-validation it was observed that the two systems were uncorrelated enough to make it useful to keep both of them. 2Decision lists are not included because they only assign a probability to their selected classifier output but not to lowerranked candidates. • If the decision_ list _ confidence&gt; 0.985 (an empirically selected threshold) then return the output of the decision list; • Otherwise, each system votes for the sense that is most likely under it and, another vote is obtained from the most probable class yielded by linear interpolation of the 4 classifiers. This simple top-performing approach was utilized in the evaluation system, and is reasonably close to the performance of an Oracle upper bound for classifier combination (using the output of the single best classifier on each test instance — unknowable in practice). Model Averaging (excluding decision lists): Probability interpolation voting .657 .728 Rank-averaged voting .652 .709 Weighted Model Voting (includes decision lists): Equal-weighted Model Voting .667 .736 Performance-Weighted Voting .655 .724 Thresholded Model Voting .676 .746 Oracle Voting (Upper Bound) .734 .761 Table 1: Comparison of classifier combination methods on English (using 5-fold cross-validation) 3 Supervised All-Words Systems 3.1 Estonian All-words Task Because of the importance of morphological analysis in a highly inflected language such as Estonian, a lemmatizer based on Yarowsky and Wicentowski (2000) was first applied to all words in the training data (and, at evaluation time, the test data). each lemma, the lemma)distribution was measured on the training data. For all lemmas exhibiting only one sense in the training data, this sense was returned. Likewise, if there was insufficient data for word-specific training (the sum of the minority sense examples for the word in training data was below a threshold) the majority sense in training was returned for all instances of that lemma. In the remaining cases where a lemma had more than one sense in training, with sufficient minority examples to adequately be modeled, the generic JHU lexical sample sense classifier was trained and applied. 3.2 Czech All-words Task Czech is another example of a highly inflected language. A part-of-speech tagger and lemmatizer kindly provided by Jan Haji 6 of Charles University (Haji 6 and Hladka, 1998) was first applied to the data. Consistent with the spirit of evaluating sense disambiguation rather than morphology, the JHU system focused on those words where more than one sense was possible for a root word (e.g. Classifier Combination Method Accuracy Fine Coarse Fine Grained Sense Classification Accuracy 165 the -1 and -2 suffixes in the Czech inventory). In these cases, the fine-grained output of the Czech lemmatizer was ignored (in both training and test) and a generic lexical-sample sense classifier was applied to the sense-distinction tags extracted from the lemmatized training data (see Section 2), using the classification models employed in Estonian. Whenever insufficient numbers of minority tagged examples were available for training a word-specific classifier, the majority sense for the POS-level lemma was returned. Likewise, if only one possible sense tag was observed for any POS-level lemma analysis, then this unambiguous sense tag was returned. 4 Unsupervised Italian System The Italian task stands out from the group of lexical choice tasks because no labelled training was data provided for Italian; instead a subset of the Italian Wordnet was provided. To obtain a sense classifier for Italian, we employed an unsupervised method that used hierarchical class models of the Wordnet relationships among words (synonymy, hypernomy, etc) and a large unannotated corpus of Italian newspaper data to obtain sense centroids. First, every relationship type in the Italian Wordnet received an initial weight, based on a roughly estimated measure of the relative dissimilarity of two in that relationship. For instance, the synreceived a small weight (words are semantically &amp;quot;close&amp;quot;), while other relationships causes, has_hypernym) received proportionately larger weights (words are more semantically distant). Starting from the senses a target wordnet relationships graph was explored, up to a given distance (two links away), &amp;quot;clouds&amp;quot; of similar words, with to the original sense, each of the words w in extracted sentences from the unannotated corpus that contained the word w, and then considered them as being exof context for the sense target asthem to the centroid centroid of the a weight corresponding to the similarbetween the word w and the sense using the wordnet graph). After all the documents were distributed, the test documents were also assigned to the most probable cluster, similar to the other lexical choice tasks. The centroids were then allowed to adjust in a manner similar to k-means clustering. At each step, the centroids were recomputed, after which each document migrated to the closest cluster (i.e. maxs (Cs1D)),and the process was repeated. After the process converged, each test document was weight on a path was computed as the sum of the weights on the path, and the similarity was computed as (w, S) — large weights result in 0 similarity.</abstract>
<note confidence="0.71778675">Task Accuracy on Test Data Fine-Grained Coarse-Grained Basque .757 .971 English .642 .713 Spanish .712 Swedish .701 1.00 Italian .353 .423 Czech .935</note>
<abstract confidence="0.957385321428571">Estonian .666 Table 2: Official JHU system performance assigned the label corresponding to the sense centroid it converged into. This process is completely unsupervised, and the only structured resource that was used is the provided Italian Wordnet subset. 5 Results Table 2 lists the official performance of the JHU systems on unseen test data in the final SENSEVAL2 evaluation. Coarse-grained performance scores are based on a hierarchical sense clustering given by the task organizers in 4 of the languages. In the lexical sample tasks, these scores were obtained after correction of a simple bug in the merger of final system output as provided for in the SENSEVAL evaluation protocols. As illustrated in the comparative performance tables elsewhere in this volume, the JHU systems are consistently very successful across all 7 languages and 3 major system types described here. References S. Cucerzan and D. Yarowsky. 2000. Language independent minimally supervised induction of lexical In of ACL-2000, 270-277, Hong Kong. J. Haji 6 and Hladka. 1998. Tagging inflective languages: Prediction of morphological categories a rich, structured tagset. In of</abstract>
<address confidence="0.376585">483-490, Montréal.</address>
<note confidence="0.797976857142857">G. Ngai and R. Florian. 2001. Transformationlearning in the fast lane. In NAACL-2001, 40-47, Pittsburgh. H. van Halteren, J. Zavrel and W. Daelemans. 1998. Improving Data Driven Wordclass Tagby System Combination In of 491-497, Montreal. D. Yarowsky and R. Wicehtowski. 2000. Minimally supervised morphological analysis by multimodal In of ACL-2000, 207-216, Hong Kong. D. Yarowsky. 2000. Hierarchical decision lists for sense disambiguation. and the 166</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
<author>D Yarowsky</author>
</authors>
<title>Language independent minimally supervised induction of lexical probabilities.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL-2000,</booktitle>
<pages>270--277</pages>
<location>Hong Kong.</location>
<marker>Cucerzan, Yarowsky, 2000</marker>
<rawString>S. Cucerzan and D. Yarowsky. 2000. Language independent minimally supervised induction of lexical probabilities. In Proceedings of ACL-2000, pages 270-277, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Haji</author>
</authors>
<title>Tagging inflective languages: Prediction of morphological categories for a rich, structured tagset.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL-98,</booktitle>
<pages>483--490</pages>
<location>Montréal.</location>
<marker>Haji, 1998</marker>
<rawString>J. Haji 6 and Hladka. 1998. Tagging inflective languages: Prediction of morphological categories for a rich, structured tagset. In Proceedings of COLING/ACL-98, pages 483-490, Montréal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ngai</author>
<author>R Florian</author>
</authors>
<title>Transformationbased learning in the fast lane.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL-2001,</booktitle>
<pages>40--47</pages>
<location>Pittsburgh.</location>
<contexts>
<context position="1913" citStr="Ngai and Florian, 2001" startWordPosition="260" endWordPosition="263">feature space based on raw words, lemmas and partof-speech (POS) tags in a variety of positional relationships to the target word. These positions include traditional bag-of-word context, local bigram and trigram collocations and several syntactic relationships based on predicate-argument structure (described in Section 1.2). Their use is illustrated on a sample English sentence for train in Figure 1. 1.1 Part-of-Speech Tagging and Lemmatization Part-of-speech tagger availability varied across the languages included in this sense-disambiguation system evaluation. Transformation-based taggers (Ngai and Florian, 2001) were trained on standard data for English (Penn Treebank), Swedish (SUC-1 corpus) and Estonian (MultextEast corpus). For Czech, an available POS tagger (Haji and Hladka, 1998), which includes lemmatization, was used. The remaining languages — Spanish, Italian and Basque — were tagged using an unsupervised tagger (Cucerzan &amp;quot;Many mothers do not even try to toilet train their children until the age of 2 years or later ...&amp;quot; Feature Word POS Lemma type Context ... ... ... Context try VB try/v Context to TO tO/T Context toilet NN toilet/N Context train VBP train/v Context their DT their/D Context .</context>
</contexts>
<marker>Ngai, Florian, 2001</marker>
<rawString>G. Ngai and R. Florian. 2001. Transformationbased learning in the fast lane. In Proceedings of NAACL-2001, pages 40-47, Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H van Halteren</author>
<author>J Zavrel</author>
<author>W Daelemans</author>
</authors>
<title>Improving Data Driven Wordclass Tagging by System Combination</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL-1998,</booktitle>
<pages>491--497</pages>
<location>Montreal.</location>
<marker>van Halteren, Zavrel, Daelemans, 1998</marker>
<rawString>H. van Halteren, J. Zavrel and W. Daelemans. 1998. Improving Data Driven Wordclass Tagging by System Combination In Proceedings of COLING/ACL-1998, pages 491-497, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
<author>R Wicehtowski</author>
</authors>
<title>Minimally supervised morphological analysis by multimodal alignment.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL-2000,</booktitle>
<pages>207--216</pages>
<location>Hong Kong.</location>
<marker>Yarowsky, Wicehtowski, 2000</marker>
<rawString>D. Yarowsky and R. Wicehtowski. 2000. Minimally supervised morphological analysis by multimodal alignment. In Proceedings of ACL-2000, pages 207-216, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Hierarchical decision lists for word sense disambiguation. Computers and the Humanities,</title>
<date>2000</date>
<pages>34--2</pages>
<contexts>
<context position="756" citStr="Yarowsky, 2000" startWordPosition="97" endWordPosition="98">wsky,silviu,rflorian , cschafer ,richardw} ©es jhu .edu Department of Computer Science Johns Hopkins University Baltimore, Maryland, 21218, USA Abstract This article describes the Johns Hopkins University (JHU) sense disambiguation systems that participated in seven SENSEVAL2 tasks: four supervised lexical choice systems (Basque, English, Spanish, Swedish), one unsupervised lexical choice system (Italian) and two supervised all-words systems (Czech, Estonian). The common core supervised system utilizes voting-based classifier combination over several diverse systems, including decision lists (Yarowsky, 2000), a cosine-based vector model and two Bayesian classifiers. The classifiers employed a rich set of features, including words, lemmas and part-of-speech informatino modeled in several syntactic relationships (e.g. verb-object), bag-of-words context and local collocational n-grams. The allwords systems relied heavily on morphological analysis in the two highly inflected languages. The unsupervised Italian system was a hierarchical class model using the Italian WordNet. 1 The Feature Space The JHU SENSEVAL2 systems utilized a rich feature space based on raw words, lemmas and partof-speech (POS) t</context>
<context position="2935" citStr="Yarowsky, 2000" startWordPosition="428" endWordPosition="429">ars or later ...&amp;quot; Feature Word POS Lemma type Context ... ... ... Context try VB try/v Context to TO tO/T Context toilet NN toilet/N Context train VBP train/v Context their DT their/D Context ... ... ... Syntactic (predicate-argument) features Object children NNS Child/N Prep until IN until/I ObjPrep age NN age/N Ngram collocational features -1 bigram toilet NN toilet/N +1 bigram their DT their/D -2/-1 trigram to toilet . TO-NN tO/T toilet/N . -1/ +1 trigram to . their TO-DT tO/T * their/D +11+2 trigram their children DT-NN their/D child/N Figure 1: Example sentence and extracted features and Yarowsky, 2000). Lemmatization was performed using a combination of supervised and unsupervised methods (Yarowsky and Wicentowski, 2000), and using existing trie-based supervised models for English. 1.2 Syntactic Features Extracted syntactic relationships in the feature space depended on the keyword&apos;s part of speech: * for verb keywords — the head noun of the verb&apos;s object, particle/preposition and objectof-preposition were extracted when available. • for noun keywords — the headword of any verbobject, subject-verb or noun-noun relationships identified for the keyword. • for adjective keywords — the head nou</context>
<context position="7125" citStr="Yarowsky (2000)" startWordPosition="1137" endWordPosition="1138">Bayes model, the Bayes similarity is computed as: Sim (Di, = P (Di, = P (Si) P and the following assumption of independence is made: P (Dirs) = IJ P (f irs) fc The probability distribution P (f3ICs) is obtained by smoothing the word relative frequencies in the cluster Cs. Given the lack of independence between the word-based and lemma-based feature spaces, these are utilized in two separate Bayesian models with output combined in Section 2.5. 2.3 Decision Lists The decision list model we used in our system is a non-hierarchical variant of the method of interpolated decision lists described in Yarowsky (2000). For each feature L a smoothed log of likelihood ratio (log pPY1_,IV)) is computed for each sense Si, with smoothing based on an empirically estimated function of feature type and relative frequency. Candidate features are ordered by this smoothed ratio (putting the best evidence first), and the remaining probabilities are computed via the interpolation of the global and history-conditional probabilities. By utilizing the single strongest-matching evidence in context, non-independent feature spaces combine readily without inflated confidence, and can be mapped to accurate and robust probabili</context>
</contexts>
<marker>Yarowsky, 2000</marker>
<rawString>D. Yarowsky. 2000. Hierarchical decision lists for word sense disambiguation. Computers and the Humanities, 34(2):179-186.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>