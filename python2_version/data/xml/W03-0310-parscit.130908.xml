<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000707">
<title confidence="0.987111">
Bootstrapping Parallel Corpora
</title>
<author confidence="0.998524">
Chris Callison-Burch Miles Osborne
</author>
<affiliation confidence="0.999695">
School of Informatics School of Informatics
University of Edinburgh University of Edinburgh
</affiliation>
<email confidence="0.993606">
callison-burch@ed.ac.uk miles@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.982768" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99994495">
We present two methods for the automatic cre-
ation of parallel corpora. Whereas previous
work into the automatic construction ofparallel
corpora has focused on harvesting them from
the web, we examine the use of existing paral-
lel corpora to bootstrap data for new language
pairs. First, we extend existing parallel cor-
pora using co-training, wherein machine trans-
lations are selectively added to training corpora
with multiple source texts. Retraining transla-
tion models yields modest improvements. Sec-
ond, we simulate the creation of training data
for a language pair for which a parallel corpus
is not available. Starting with no human trans-
lations from German to English we produce a
German to English translation model with 45%
accuracy using parallel corpora in other lan-
guages. This suggests the method may be use-
ful in the creation of parallel corpora for lan-
guages with scarce resources.
</bodyText>
<sectionHeader confidence="0.995167" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999966942307692">
Statistical translation models (such as those formulated in
Brown et al. (1993)) are trained from bilingual sentence-
aligned texts. The bilingual data used for constructing
translation models is often gathered from government
documents produced in multiple languages. For exam-
ple, the Candide system (Berger et al., 1994) was trained
on ten years’ worth of Canadian Parliament proceed-
ings, which consists of 2.87 million parallel sentences
in French and English. While the Candide system was
widely regarded as successful, its success is not indica-
tive of the potential for statistical translation between ar-
bitrary language pairs. The reason for this is that collec-
tions of parallel texts as large as the Canadian Hansards
are rare.
Al-Onaizan et al. (2000) explains in simple terms the
reasons that using large amounts of training data en-
sures translation quality: if a program sees a partic-
ular word or phrase one thousand times during train-
ing, it is more likely to learn a correct translation than
if sees it ten times, or once, or never. Increasing the
amount of training material therefore leads to improved
quality. This is illustrated in Figure 1, which plots
translation accuracy (measured as 100 minus word er-
ror rate) for French⇒English, German⇒English, and
Spanish⇒English translation models trained on incre-
mentally larger parallel corpora. The quality of the
translations produced by each system increases over the
100,000 training items, and the graph suggests the the
trend would continue if more data were added. Notice
that the rate of improvement is slow: after 90,000 manu-
ally provided training sentences pairs, we only see a 4-6%
change in performance. Sufficient performance for sta-
tistical models may therefore only come when we have
access to many millions of aligned sentences.
One approach that has been proposed to address the
problem of limited training data is to harvest the web for
bilingual texts (Resnik, 1998). The STRAND method au-
tomatically gathers web pages that are potential transla-
tions of each other by looking for documents in one lan-
guage which have links whose text contains the name of
another language. For example, if an English web page
had a link with the text “Espa˜nol” or “en Espa˜nol” then
the page linked to is treated as a candidate translation of
the English page. Further checks verify the plausibility
of its being a translation (Smith, 2002).
Instead of attempting to gather new translations from
the web, we describe an alternate method for automat-
ically creating parallel corpora. Specifically, we exam-
ine the use of existing translations as a resource to boot-
strap more training data, and to create data for new lan-
guage pairs. We generate translation models from exist-
ing data and use them to produce translations of new sen-
</bodyText>
<figure confidence="0.924117625">
64
62
60
58
56
54
52
50
</figure>
<page confidence="0.640478666666667">
48
46
44
</page>
<figure confidence="0.949736">
10000 20000
</figure>
<figureCaption confidence="0.9763775">
Figure 1: Translation accuracy plotted against training
corpus size
</figureCaption>
<bodyText confidence="0.9999753">
tences. Incorporating this machine-created parallel data
to the original set, and retraining the translation models
improves the translation accuracy. To perform the retrain-
ing we use co-training (Blum and Mitchell, 1998; Abney,
2002) which is a weakly supervised learning technique
that relies on having distinct views of the items being
classified. The views that we employ for co-training are
multiple source documents.
Section 2 motivates the use of weakly supervised learn-
ing, and introduces co-training for machine translation.
Section 3 reports our experimental results. One experi-
ment shows that co-training can modestly benefit trans-
lation systems trained from similarly sized corpora. A
second experiment shows that co-training can have a dra-
matic benefit when the size of initial training corpora are
mismatched. This suggests that co-training for statisti-
cal machine translation is especially useful for languages
with impoverished training corpora. Section 4 discusses
the implications of our experiments, and discusses ways
which our methods might be used more practically.
</bodyText>
<sectionHeader confidence="0.961725" genericHeader="method">
2 Co-training for Statistical Machine
Translation
</sectionHeader>
<bodyText confidence="0.9998126">
Most statistical natural language processing tasks use su-
pervised machine learning, meaning that they require
training data that contains examples that have been an-
notated with some sort of labels. Two conflicting factors
make this reliance on annotated training data a problem:
</bodyText>
<listItem confidence="0.981457">
• The accuracy of machine learning improves as more
data is available (as we have shown for statistical
machine translation in Figure 1).
• Annotated training data usually has some cost asso-
ciated with its creation. This cost can often be sub-
</listItem>
<bodyText confidence="0.999900088888889">
stantial, as with the Penn Treebank (Marcus et al.,
1993).
There has recently been considerable interest in weakly
supervised learning within the statistical NLP commu-
nity. The goal of weakly supervised learning is to reduce
the cost of creating new annotated corpora by (semi-) au-
tomating the process.
Co-training is a weakly supervised learning techniques
which uses an initially small amount of human labeled
data to automatically bootstrap larger sets of machine la-
beled training data. In co-training implementations mul-
tiple learners are used to label new examples and re-
trained on some of each other’s labeled examples. The
use of multiple learners increases the chance that use-
ful information will be added; an example which is eas-
ily labeled by one learner may be difficult for the other
and therefore adding the confidently labeled example will
provide information in the next round of training.
Self-training is a weakly supervised method in which
a single learner retrains on the labels that it applies to
unlabeled data itself. We describe its application to
machine translation in order to clarify how co-training
would work. In self-training a translation model would be
trained for a language pair, say German⇒English, from
a German-English parallel corpus. It would then produce
English translations for a set of German sentences. The
machine translated German-English sentences would be
added to the initial bilingual corpus, and the translation
model would be retrained.
Co-training for machine translation is slightly more
complicated. Rather than using a single translation
model to translate a monolingual corpus, it uses mul-
tiple translation models to translate a bi- or multi-
lingual corpus. For example, translation models could
be trained for German⇒English, French⇒English and
Spanish⇒English from appropriate bilingual corpora,
and then used to translate a German-French-Spanish par-
allel corpus into English. Since there are three candidate
English translations for each sentence alignment, the best
translation out of the three can be selected and used to
retrain the models. The process is illustrated in Figure 2.
Co-training thus automatically increases the size of
parallel corpora. There are a number of reasons why
machine translated items added during co-training can be
useful in the next round of training:
</bodyText>
<listItem confidence="0.590031875">
• vocabulary acquisition – One problem that arises
from having a small training corpus is incomplete
word coverage. Without a word occurring in its
training corpus it is unlikely that a translation model
will produce a reasonable translation of it. Because
the initial training corpora can come from different
sources, a collection of translation models will be
more likely to have encountered a word before. This
</listItem>
<table confidence="0.950071923076923">
1
some french sentenc some english sentence
French English
2
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
some french sentence
some english sentence
German English Spanish English
some french sentence some french sentence
some english sentence some english sentence
some french sentence some english sentence some french sentence some english sentence
some french sentence some english sentence some french sentence some english sentence
some french sentence some english sentence some french sentence some english sentence
some french sentence some french sentence
some english sentence some english sentence
some french sentence some english sentence some french sentence some english sentence
some french sentence some english sentence some french sentence some english sentence
</table>
<figure confidence="0.99360347311828">
blaues Blue house
French German Spanish English target
Maison bleu
blaues Haus
Casa azul
???
Blue
maison
House
4
blaues Haus
Maison bleu
Casa azul
Blue house
Blue
maison
blaues Blue house
Haus
some french
some english sentence
sentence
French English
some french sentenc some english sentence
some french
some english sentence
sentence
some french
some english sentence
sentence
some french
some english sentence
sentence
some french
some english sentence
sentence
some french
some english sentence
sentence
German English
some french sentenc some english sentence
some french
some english sentence
sentence
some french
some english sentence
sentence
some french
some english sentence
sentence
some french
some english sentence
sentence
some french
some english sentence
sentence
some french
some english sentence
sentence
Spanish English
some french sentenc some english sentence
some french
some english sentence
sentence
some french
some english sentence
sentence
some french
some english sentence
sentence
some french
some english sentence
sentence
some french
some english sentence
sentence
some french
some english sentence
sentence
+
Maison
bleu
Blue
house
+
blaues
Haus
Blue
house
+
Casa azul
Blue
house
3
</figure>
<figureCaption confidence="0.999065">
Figure 2: Co-training using German, French, and Spanish sources to produce English machine translations
</figureCaption>
<bodyText confidence="0.989298655172414">
leads to vocabulary acquisition during co-training.
• coping with morphology – The problem mentioned
above is further exacerbated by the fact that most
current statistical translation formulations have an
incomplete treatment of morphology. This would be
a problem if the training data for a Spanish transla-
tion model contained the masculine form of a adjec-
tive, but not the feminine. Because languages vary
in how they use morphology (some languages have
grammatical gender whereas others don’t) one lan-
guage’s translation model might have the translation
of a particular word form whereas another’s would
not. Thus co-training can increase the inventory of
word forms and reduce the problem that morphol-
ogy poses to simple statistical translation models.
• improved word order – A significant source of er-
rors in statistical machine translation is the word re-
ordering problem (Och et al., 1999). The word or-
der between related languages is often similar while
word order between distant language may differ sig-
nificantly. By including more examples through co-
training with related languages, the translation mod-
els for distant languages will better learn word order
mappings to the target language.
In all these cases the diversity afforded by multiple trans-
lation models increases the chances that the machine
translated sentences added to the initial bilingual corpora
will be accurate. Our co-training algorithm allows many
source languages to be used.
</bodyText>
<sectionHeader confidence="0.996249" genericHeader="evaluation">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.9999815">
In order to conduct co-training experiments we first
needed to assemble appropriate corpora. The corpus used
in our experiments was assembled from the data used in
the (Och and Ney, 2001) multiple source translation pa-
per. The data was gathered from the Bulletin of the Eu-
ropean Union which is published on the Internet in the
eleven official languages of the European Union. We
used a subset of the data to create a multi-lingual cor-
pus, aligning sentences between French, Spanish, Ger-
man, Italian and Portuguese (Simard, 1999). Addition-
ally we created bilingual corpora between English and
each of the five languages using sentences that were not
included in the multi-lingual corpus.
Och and Ney (2001) used the data to find a transla-
tion that was most probable given multiple source strings.
Och and Ney found that multi-source translations using
two source languages reduced word error rate when com-
pared to using source strings from a single language.
For multi-source translations using source strings in six
languages a greater reduction in word error rate was
achieved. Our work is similar in spirit, although instead
of using multi-source translation at the time of transla-
tion, we integrate it into the training stage. Whereas
Och and Ney use multiple source strings to improve the
quality of one translation only, our co-training method at-
tempts to improve the accuracy of all translation models
by bootstrapping more training data from multiple source
documents.
</bodyText>
<subsectionHeader confidence="0.995184">
3.1 Software
</subsectionHeader>
<bodyText confidence="0.998543">
The software that we used to train the statistical mod-
els and to produce the translations was GIZA++ (Och
and Ney, 2000), the CMU-Cambridge Language Model-
ing Toolkit (Clarkson and Rosenfeld, 1997), and the ISI
ReWrite Decoder. The sizes of the language models used
in each experiment were fixed throughout, in order to en-
sure that any gains that were made were not due to the
trivial reason of the language model improving (which
could be done by building a larger monolingual corpus of
the target language).
The experiments that we conducted used GIZA++ to
produce IBM Model 4 translation models. It should be
observed, however, that our co-training algorithm is en-
tirely general and may be applied to any formulation of
statistical machine translation which relies on parallel
</bodyText>
<table confidence="0.999302571428571">
Round Number
Translation Pair 0 1 2 3
French⇒English 55.2 56.3 57.0 55.5
Spanish⇒English 57.2 57.8 57.6 56.9
German⇒English 45.1 46.3 47.4 47.6
Italian⇒English 53.8 54.0 53.6 53.5
Portuguese⇒Eng 55.2 55.2 55.7 54.3
</table>
<tableCaption confidence="0.8096375">
Table 1: Co-training results over three rounds
corpora for its training data.
</tableCaption>
<subsectionHeader confidence="0.995983">
3.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999991428571429">
The performance of translation models was evaluated us-
ing a held-out set of 1,000 sentences in each language,
with reference translations into English. Each translation
model was used to produce translation of these sentences
and the machine translations were compared to the ref-
erence human translations using word error rate (WER).
The results are reported in terms of increasing accuracy,
rather than decreasing error. We define accuracy as 100
minus WER.
Other evaluation metrics such as position independent
WER or the Bleu method (Papineni et al., 2001) could
have been used. While WER may not be the best measure
of translation quality, it is sufficient to track performance
improvements in the following experiments.
</bodyText>
<subsectionHeader confidence="0.99987">
3.3 Co-training
</subsectionHeader>
<bodyText confidence="0.999674416666667">
Table 1 gives the result of co-training using the most
accurate translation from the candidate translations pro-
duced by five translation models. Each translation model
was initially trained on bilingual corpora consisting of
around 20,000 human translated sentences. These trans-
lation models were used to translate 63,000 sentences, of
which the top 10,000 were selected for the first round.
At the next round 53,000 sentences were translated and
the top 10,000 sentences were selected for the second
round. The final candidate pool contained 43,000 trans-
lations and again the top 10,000 were selected. The table
indicates that gains may be had from co-training. Each
of the translation models improves over its initial training
size at some point in the co-training. The German to En-
glish translation model improves the most – exhibiting a
2.5% improvement in accuracy.
The table further indicates that co-training for ma-
chine translation suffers the same problem reported in
Pierce and Cardie (2001): gains above the accuracy of
the initial corpus are achieved, but decline as after a cer-
tain number of machine translations are added to the
training set. This could be due in part to the manner
in items are selected for each round. Because the best
translations are transferred from the candidate pool to the
</bodyText>
<figure confidence="0.724761">
10000 1500
</figure>
<figureCaption confidence="0.9562995">
Figure 3: “Coaching” of German to English by a French
to English translation model
</figureCaption>
<figure confidence="0.943168">
100000 150000 200000 250000 300000 350000 400000
Training Corpus Size (number of sentence pairs)
</figure>
<figureCaption confidence="0.9467935">
Figure 4: “Coaching” of German to English by multiple
translation models
</figureCaption>
<figure confidence="0.426504">
0 dro Re) or Aracy
</figure>
<bodyText confidence="0.999915625">
training pool at each round the number of “easy” trans-
lations diminishes over time. Because of this, the av-
erage accuracy of the training corpora decreased with
each round, and the amount of noise being introduced
increased. The accuracy gains from co-training might
extend for additional rounds if the size of the candidate
pool were increased, or if some method were employed
to reduce the amount of noise being introduced.
</bodyText>
<subsectionHeader confidence="0.994746">
3.4 Coaching
</subsectionHeader>
<bodyText confidence="0.9999404">
In order to simulate using co-training for language pairs
without extensive parallel corpora, we experimented with
a variation on co-training for machine translation that
we call “coaching”. It employs two translation models
of vastly different size. In this case we used a French
</bodyText>
<figure confidence="0.9020165625">
45.2
45
44.8
44.6
44.4
44.2
44
43.8
Coaching of German
30
29.5
29
28.5
28
27.5
27
</figure>
<bodyText confidence="0.999738666666667">
to English translation model built from 60,000 human
translated sentences and a German to English translation
model that contained no human translated sentences. The
German-English translation model was meant to repre-
sent a language pair with extremely impoverished paral-
lel corpus. Coaching is therefore a special case of co-
training in that one view (the superior one) never retrains
upon material provided by the other (inferior) view.
A German-English parallel corpus was created by tak-
ing a French-German parallel corpus, translating the
French sentences into English and then aligning the trans-
lations with the German sentences. In this experiment the
machine translations produced by the French⇒English
translation model were always selected. Figure 3 shows
the performance of the resulting German to English trans-
lation model for various sized machine produced parallel
corpora.
We explored this method further by translating 100,000
sentences with each of the non-German translation mod-
els from the co-training experiment in Section 3.3. The
result was a German-English corpus containing 400,000
sentence pairs. The performance of the resulting model
matches the initial accuracy of the model. Thus machine-
translated corpora achieved equivalent quality to human-
translated corpora after two orders of magnitude more
data was added.
The graphs illustrate that increasing the performance
of translation models may be achievable using machine
translations alone. Rather than the 2.5% improvement
gained in co-training experiments wherein models of sim-
ilar sizes were used, coaching achieves an 18%(+) im-
provement by pairing translation models of radically dif-
ferent sizes.
</bodyText>
<sectionHeader confidence="0.995233" genericHeader="conclusions">
4 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999896">
In this paper we presented two methods for the automatic
creation of additional parallel corpora. Co-training uses a
number of different human translated parallel corpora to
create additional data for each of them, leading to modest
increases in translation quality. Coaching uses existing
resources to create a fully machine translated corpora –
essentially reverse engineering the knowledge present in
the human translated corpora and transferring that to an-
other language. This has significant implications for the
feasibility of using statistical translation methods for lan-
guage pairs for which extensive parallel corpora do not
exist.
A setting in which this would become extremely use-
ful is if the European Union extends membership to a
new country like Turkey, and wants develop translation
resources for its language. One can imagine that sizable
parallel corpora might be available between Turkish and a
few EU languages like Greek and Italian. However, there
may be no parallel corpora between Turkish and Finnish.
Our methods could exploit existing parallel corpora be-
tween the current EU language and use machine transla-
tions from Greek and Italian in order to create a machine
translation system between Turkish and Finnish.
We plan to extend our work by moving from co-
training and its variants to another weakly supervised
learning method, active learning. Active learning incor-
porates human translations along with machine transla-
tions, which should ensure better resulting quality than
using machine translations alone. It will reduce the cost
of creating a parallel corpus entirely by hand, by selec-
tively and judiciously querying a human translator. In
order to make the most effective use of the human trans-
lator’s time we will be required to design an effective se-
lection algorithm, which is something that was neglected
in our current research. An effective selection algorithm
for active learning will be one which chooses those exam-
ples which will add the most information to the machine
translation system, and therefore minimizes the amount
of time a human needs to spend translating sentences.
</bodyText>
<sectionHeader confidence="0.996468" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999769966666667">
Steve Abney. 2002. Bootstrapping. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics.
Yaser Al-Onaizan, Ulrich Germann, Ulf Hermjakob,
Kevin Knight, Philipp Koehn, Daniel Marcu, and Ya-
mada Kenji. 2000. Translating with scarce resources.
In Proceedings of the National Conference on Artificial
Intelligence (AAAI).
Adam Berger, Peter Brown, Stephen Della Pietra, Vin-
cent Della Pietra, John Gillett, John Lafferty, Robert
Mercer, Harry Printz, and Lubos Ures. 1994. The
Candide system for machine translation.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the Workshop on Computational Learning The-
ory.
Peter Brown, Stephen Della Pietra, Vincent Della Pietra,
and Robert Mercer. 1993. The mathematics of ma-
chine translation: Parameter estimation. Compuata-
tional Linguistics, 19(2):263–311, June.
Philip Clarkson and Ronald Rosenfeld. 1997. Statistical
language modeling using the CMU-Cambridge toolkit.
In ESCA Eurospeech Proceedings.
Mitchell P. Marcus, Beatrice Santori, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19.
Franz Joseph Och and Herman Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, pages 440–447, Hong Kong, October.
Franz Joseph Och and Herman Ney. 2001. Statistical
multi-source translation. In MT Summit 2001, pages
253–258, Santiago de Compostela, Spain, September.
Franz Joseph Och, Christop Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proceedings of the Joint Confer-
ence of Empirical Methods in Natural Language Pro-
cessing and Very Large Corpora, College Park, Mary-
land, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation. IBM Research Report,
September.
David Pierce and Claire Cardie. 2001. Limitations of
co-training for natural language learning from large
datasets. In Proceedings of the 2001 Conference on
Empirical Methods in Natural Language Processing.
Philip Resnik. 1998. Parallel strands: A preliminary
investigation into mining the web for bilingual text.
In Third Conference of the Association for Machine
Translation in the Americas.
Michel Simard. 1999. Text-translation alignment:
Aligning three or more versions of a text. In Jean
Veronis, editor, Parallel Text Processing. Kluwer Aca-
demic.
Noah Smith. 2002. From words to corpora: Recognizing
translation. In Proceedings of the 2002 Conference on
Empirical Methods in Natural Language Processing,
Philadelphia, Pennsylvania.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.965716">
<title confidence="0.999708">Bootstrapping Parallel Corpora</title>
<author confidence="0.999963">Chris Callison-Burch Miles Osborne</author>
<affiliation confidence="0.9998615">School of Informatics School of Informatics University of Edinburgh University of Edinburgh</affiliation>
<email confidence="0.984144">callison-burch@ed.ac.ukmiles@inf.ed.ac.uk</email>
<abstract confidence="0.999104714285714">We present two methods for the automatic creation of parallel corpora. Whereas previous work into the automatic construction ofparallel corpora has focused on harvesting them from the web, we examine the use of existing parallel corpora to bootstrap data for new language pairs. First, we extend existing parallel corpora using co-training, wherein machine translations are selectively added to training corpora with multiple source texts. Retraining translation models yields modest improvements. Second, we simulate the creation of training data for a language pair for which a parallel corpus is not available. Starting with no human translations from German to English we produce a German to English translation model with 45% accuracy using parallel corpora in other languages. This suggests the method may be useful in the creation of parallel corpora for languages with scarce resources.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steve Abney</author>
</authors>
<date>2002</date>
<booktitle>Bootstrapping. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4254" citStr="Abney, 2002" startWordPosition="675" endWordPosition="676">g parallel corpora. Specifically, we examine the use of existing translations as a resource to bootstrap more training data, and to create data for new language pairs. We generate translation models from existing data and use them to produce translations of new sen64 62 60 58 56 54 52 50 48 46 44 10000 20000 Figure 1: Translation accuracy plotted against training corpus size tences. Incorporating this machine-created parallel data to the original set, and retraining the translation models improves the translation accuracy. To perform the retraining we use co-training (Blum and Mitchell, 1998; Abney, 2002) which is a weakly supervised learning technique that relies on having distinct views of the items being classified. The views that we employ for co-training are multiple source documents. Section 2 motivates the use of weakly supervised learning, and introduces co-training for machine translation. Section 3 reports our experimental results. One experiment shows that co-training can modestly benefit translation systems trained from similarly sized corpora. A second experiment shows that co-training can have a dramatic benefit when the size of initial training corpora are mismatched. This sugge</context>
</contexts>
<marker>Abney, 2002</marker>
<rawString>Steve Abney. 2002. Bootstrapping. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Ulrich Germann</author>
<author>Ulf Hermjakob</author>
<author>Kevin Knight</author>
<author>Philipp Koehn</author>
<author>Daniel Marcu</author>
<author>Yamada Kenji</author>
</authors>
<title>Translating with scarce resources.</title>
<date>2000</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="1877" citStr="Al-Onaizan et al. (2000)" startWordPosition="282" endWordPosition="285">e bilingual data used for constructing translation models is often gathered from government documents produced in multiple languages. For example, the Candide system (Berger et al., 1994) was trained on ten years’ worth of Canadian Parliament proceedings, which consists of 2.87 million parallel sentences in French and English. While the Candide system was widely regarded as successful, its success is not indicative of the potential for statistical translation between arbitrary language pairs. The reason for this is that collections of parallel texts as large as the Canadian Hansards are rare. Al-Onaizan et al. (2000) explains in simple terms the reasons that using large amounts of training data ensures translation quality: if a program sees a particular word or phrase one thousand times during training, it is more likely to learn a correct translation than if sees it ten times, or once, or never. Increasing the amount of training material therefore leads to improved quality. This is illustrated in Figure 1, which plots translation accuracy (measured as 100 minus word error rate) for French⇒English, German⇒English, and Spanish⇒English translation models trained on incrementally larger parallel corpora. The</context>
</contexts>
<marker>Al-Onaizan, Germann, Hermjakob, Knight, Koehn, Marcu, Kenji, 2000</marker>
<rawString>Yaser Al-Onaizan, Ulrich Germann, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Daniel Marcu, and Yamada Kenji. 2000. Translating with scarce resources. In Proceedings of the National Conference on Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Peter Brown</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>John Gillett</author>
<author>John Lafferty</author>
<author>Robert Mercer</author>
<author>Harry Printz</author>
<author>Lubos Ures</author>
</authors>
<title>The Candide system for machine translation.</title>
<date>1994</date>
<contexts>
<context position="1440" citStr="Berger et al., 1994" startWordPosition="211" endWordPosition="214">le. Starting with no human translations from German to English we produce a German to English translation model with 45% accuracy using parallel corpora in other languages. This suggests the method may be useful in the creation of parallel corpora for languages with scarce resources. 1 Introduction Statistical translation models (such as those formulated in Brown et al. (1993)) are trained from bilingual sentencealigned texts. The bilingual data used for constructing translation models is often gathered from government documents produced in multiple languages. For example, the Candide system (Berger et al., 1994) was trained on ten years’ worth of Canadian Parliament proceedings, which consists of 2.87 million parallel sentences in French and English. While the Candide system was widely regarded as successful, its success is not indicative of the potential for statistical translation between arbitrary language pairs. The reason for this is that collections of parallel texts as large as the Canadian Hansards are rare. Al-Onaizan et al. (2000) explains in simple terms the reasons that using large amounts of training data ensures translation quality: if a program sees a particular word or phrase one thou</context>
</contexts>
<marker>Berger, Brown, Pietra, Pietra, Gillett, Lafferty, Mercer, Printz, Ures, 1994</marker>
<rawString>Adam Berger, Peter Brown, Stephen Della Pietra, Vincent Della Pietra, John Gillett, John Lafferty, Robert Mercer, Harry Printz, and Lubos Ures. 1994. The Candide system for machine translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the Workshop on Computational Learning Theory.</booktitle>
<contexts>
<context position="4240" citStr="Blum and Mitchell, 1998" startWordPosition="671" endWordPosition="674">for automatically creating parallel corpora. Specifically, we examine the use of existing translations as a resource to bootstrap more training data, and to create data for new language pairs. We generate translation models from existing data and use them to produce translations of new sen64 62 60 58 56 54 52 50 48 46 44 10000 20000 Figure 1: Translation accuracy plotted against training corpus size tences. Incorporating this machine-created parallel data to the original set, and retraining the translation models improves the translation accuracy. To perform the retraining we use co-training (Blum and Mitchell, 1998; Abney, 2002) which is a weakly supervised learning technique that relies on having distinct views of the items being classified. The views that we employ for co-training are multiple source documents. Section 2 motivates the use of weakly supervised learning, and introduces co-training for machine translation. Section 3 reports our experimental results. One experiment shows that co-training can modestly benefit translation systems trained from similarly sized corpora. A second experiment shows that co-training can have a dramatic benefit when the size of initial training corpora are mismatch</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the Workshop on Computational Learning Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Brown</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>Robert Mercer</author>
</authors>
<title>The mathematics of machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Compuatational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1199" citStr="Brown et al. (1993)" startWordPosition="176" endWordPosition="179">are selectively added to training corpora with multiple source texts. Retraining translation models yields modest improvements. Second, we simulate the creation of training data for a language pair for which a parallel corpus is not available. Starting with no human translations from German to English we produce a German to English translation model with 45% accuracy using parallel corpora in other languages. This suggests the method may be useful in the creation of parallel corpora for languages with scarce resources. 1 Introduction Statistical translation models (such as those formulated in Brown et al. (1993)) are trained from bilingual sentencealigned texts. The bilingual data used for constructing translation models is often gathered from government documents produced in multiple languages. For example, the Candide system (Berger et al., 1994) was trained on ten years’ worth of Canadian Parliament proceedings, which consists of 2.87 million parallel sentences in French and English. While the Candide system was widely regarded as successful, its success is not indicative of the potential for statistical translation between arbitrary language pairs. The reason for this is that collections of paral</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter Brown, Stephen Della Pietra, Vincent Della Pietra, and Robert Mercer. 1993. The mathematics of machine translation: Parameter estimation. Compuatational Linguistics, 19(2):263–311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Clarkson</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>Statistical language modeling using the CMU-Cambridge toolkit.</title>
<date>1997</date>
<booktitle>In ESCA Eurospeech Proceedings.</booktitle>
<contexts>
<context position="13872" citStr="Clarkson and Rosenfeld, 1997" startWordPosition="2147" endWordPosition="2150">ror rate was achieved. Our work is similar in spirit, although instead of using multi-source translation at the time of translation, we integrate it into the training stage. Whereas Och and Ney use multiple source strings to improve the quality of one translation only, our co-training method attempts to improve the accuracy of all translation models by bootstrapping more training data from multiple source documents. 3.1 Software The software that we used to train the statistical models and to produce the translations was GIZA++ (Och and Ney, 2000), the CMU-Cambridge Language Modeling Toolkit (Clarkson and Rosenfeld, 1997), and the ISI ReWrite Decoder. The sizes of the language models used in each experiment were fixed throughout, in order to ensure that any gains that were made were not due to the trivial reason of the language model improving (which could be done by building a larger monolingual corpus of the target language). The experiments that we conducted used GIZA++ to produce IBM Model 4 translation models. It should be observed, however, that our co-training algorithm is entirely general and may be applied to any formulation of statistical machine translation which relies on parallel Round Number Tran</context>
</contexts>
<marker>Clarkson, Rosenfeld, 1997</marker>
<rawString>Philip Clarkson and Ronald Rosenfeld. 1997. Statistical language modeling using the CMU-Cambridge toolkit. In ESCA Eurospeech Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santori</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<contexts>
<context position="5736" citStr="Marcus et al., 1993" startWordPosition="899" endWordPosition="902">ining for Statistical Machine Translation Most statistical natural language processing tasks use supervised machine learning, meaning that they require training data that contains examples that have been annotated with some sort of labels. Two conflicting factors make this reliance on annotated training data a problem: • The accuracy of machine learning improves as more data is available (as we have shown for statistical machine translation in Figure 1). • Annotated training data usually has some cost associated with its creation. This cost can often be substantial, as with the Penn Treebank (Marcus et al., 1993). There has recently been considerable interest in weakly supervised learning within the statistical NLP community. The goal of weakly supervised learning is to reduce the cost of creating new annotated corpora by (semi-) automating the process. Co-training is a weakly supervised learning techniques which uses an initially small amount of human labeled data to automatically bootstrap larger sets of machine labeled training data. In co-training implementations multiple learners are used to label new examples and retrained on some of each other’s labeled examples. The use of multiple learners in</context>
</contexts>
<marker>Marcus, Santori, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santori, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Joseph Och</author>
<author>Herman Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>440--447</pages>
<location>Hong Kong,</location>
<contexts>
<context position="13796" citStr="Och and Ney, 2000" startWordPosition="2137" endWordPosition="2140">ng source strings in six languages a greater reduction in word error rate was achieved. Our work is similar in spirit, although instead of using multi-source translation at the time of translation, we integrate it into the training stage. Whereas Och and Ney use multiple source strings to improve the quality of one translation only, our co-training method attempts to improve the accuracy of all translation models by bootstrapping more training data from multiple source documents. 3.1 Software The software that we used to train the statistical models and to produce the translations was GIZA++ (Och and Ney, 2000), the CMU-Cambridge Language Modeling Toolkit (Clarkson and Rosenfeld, 1997), and the ISI ReWrite Decoder. The sizes of the language models used in each experiment were fixed throughout, in order to ensure that any gains that were made were not due to the trivial reason of the language model improving (which could be done by building a larger monolingual corpus of the target language). The experiments that we conducted used GIZA++ to produce IBM Model 4 translation models. It should be observed, however, that our co-training algorithm is entirely general and may be applied to any formulation o</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Joseph Och and Herman Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 440–447, Hong Kong, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Joseph Och</author>
<author>Herman Ney</author>
</authors>
<title>Statistical multi-source translation.</title>
<date>2001</date>
<booktitle>In MT Summit</booktitle>
<pages>253--258</pages>
<location>Santiago de Compostela, Spain,</location>
<contexts>
<context position="12374" citStr="Och and Ney, 2001" startWordPosition="1905" endWordPosition="1908"> through cotraining with related languages, the translation models for distant languages will better learn word order mappings to the target language. In all these cases the diversity afforded by multiple translation models increases the chances that the machine translated sentences added to the initial bilingual corpora will be accurate. Our co-training algorithm allows many source languages to be used. 3 Experimental Results In order to conduct co-training experiments we first needed to assemble appropriate corpora. The corpus used in our experiments was assembled from the data used in the (Och and Ney, 2001) multiple source translation paper. The data was gathered from the Bulletin of the European Union which is published on the Internet in the eleven official languages of the European Union. We used a subset of the data to create a multi-lingual corpus, aligning sentences between French, Spanish, German, Italian and Portuguese (Simard, 1999). Additionally we created bilingual corpora between English and each of the five languages using sentences that were not included in the multi-lingual corpus. Och and Ney (2001) used the data to find a translation that was most probable given multiple source </context>
</contexts>
<marker>Och, Ney, 2001</marker>
<rawString>Franz Joseph Och and Herman Ney. 2001. Statistical multi-source translation. In MT Summit 2001, pages 253–258, Santiago de Compostela, Spain, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Joseph Och</author>
<author>Christop Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint Conference of Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<location>College Park, Maryland,</location>
<contexts>
<context position="11602" citStr="Och et al., 1999" startWordPosition="1785" endWordPosition="1788">g data for a Spanish translation model contained the masculine form of a adjective, but not the feminine. Because languages vary in how they use morphology (some languages have grammatical gender whereas others don’t) one language’s translation model might have the translation of a particular word form whereas another’s would not. Thus co-training can increase the inventory of word forms and reduce the problem that morphology poses to simple statistical translation models. • improved word order – A significant source of errors in statistical machine translation is the word reordering problem (Och et al., 1999). The word order between related languages is often similar while word order between distant language may differ significantly. By including more examples through cotraining with related languages, the translation models for distant languages will better learn word order mappings to the target language. In all these cases the diversity afforded by multiple translation models increases the chances that the machine translated sentences added to the initial bilingual corpora will be accurate. Our co-training algorithm allows many source languages to be used. 3 Experimental Results In order to con</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Franz Joseph Och, Christop Tillmann, and Hermann Ney. 1999. Improved alignment models for statistical machine translation. In Proceedings of the Joint Conference of Empirical Methods in Natural Language Processing and Very Large Corpora, College Park, Maryland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<tech>IBM Research Report,</tech>
<contexts>
<context position="15322" citStr="Papineni et al., 2001" startWordPosition="2378" endWordPosition="2381">ver three rounds corpora for its training data. 3.2 Evaluation The performance of translation models was evaluated using a held-out set of 1,000 sentences in each language, with reference translations into English. Each translation model was used to produce translation of these sentences and the machine translations were compared to the reference human translations using word error rate (WER). The results are reported in terms of increasing accuracy, rather than decreasing error. We define accuracy as 100 minus WER. Other evaluation metrics such as position independent WER or the Bleu method (Papineni et al., 2001) could have been used. While WER may not be the best measure of translation quality, it is sufficient to track performance improvements in the following experiments. 3.3 Co-training Table 1 gives the result of co-training using the most accurate translation from the candidate translations produced by five translation models. Each translation model was initially trained on bilingual corpora consisting of around 20,000 human translated sentences. These translation models were used to translate 63,000 sentences, of which the top 10,000 were selected for the first round. At the next round 53,000 s</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. Bleu: a method for automatic evaluation of machine translation. IBM Research Report, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Pierce</author>
<author>Claire Cardie</author>
</authors>
<title>Limitations of co-training for natural language learning from large datasets.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="16505" citStr="Pierce and Cardie (2001)" startWordPosition="2563" endWordPosition="2566">e first round. At the next round 53,000 sentences were translated and the top 10,000 sentences were selected for the second round. The final candidate pool contained 43,000 translations and again the top 10,000 were selected. The table indicates that gains may be had from co-training. Each of the translation models improves over its initial training size at some point in the co-training. The German to English translation model improves the most – exhibiting a 2.5% improvement in accuracy. The table further indicates that co-training for machine translation suffers the same problem reported in Pierce and Cardie (2001): gains above the accuracy of the initial corpus are achieved, but decline as after a certain number of machine translations are added to the training set. This could be due in part to the manner in items are selected for each round. Because the best translations are transferred from the candidate pool to the 10000 1500 Figure 3: “Coaching” of German to English by a French to English translation model 100000 150000 200000 250000 300000 350000 400000 Training Corpus Size (number of sentence pairs) Figure 4: “Coaching” of German to English by multiple translation models 0 dro Re) or Aracy traini</context>
</contexts>
<marker>Pierce, Cardie, 2001</marker>
<rawString>David Pierce and Claire Cardie. 2001. Limitations of co-training for natural language learning from large datasets. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Parallel strands: A preliminary investigation into mining the web for bilingual text.</title>
<date>1998</date>
<booktitle>In Third Conference of the Association for Machine Translation in the Americas.</booktitle>
<contexts>
<context position="3063" citStr="Resnik, 1998" startWordPosition="476" endWordPosition="477">arger parallel corpora. The quality of the translations produced by each system increases over the 100,000 training items, and the graph suggests the the trend would continue if more data were added. Notice that the rate of improvement is slow: after 90,000 manually provided training sentences pairs, we only see a 4-6% change in performance. Sufficient performance for statistical models may therefore only come when we have access to many millions of aligned sentences. One approach that has been proposed to address the problem of limited training data is to harvest the web for bilingual texts (Resnik, 1998). The STRAND method automatically gathers web pages that are potential translations of each other by looking for documents in one language which have links whose text contains the name of another language. For example, if an English web page had a link with the text “Espa˜nol” or “en Espa˜nol” then the page linked to is treated as a candidate translation of the English page. Further checks verify the plausibility of its being a translation (Smith, 2002). Instead of attempting to gather new translations from the web, we describe an alternate method for automatically creating parallel corpora. S</context>
</contexts>
<marker>Resnik, 1998</marker>
<rawString>Philip Resnik. 1998. Parallel strands: A preliminary investigation into mining the web for bilingual text. In Third Conference of the Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Simard</author>
</authors>
<title>Text-translation alignment: Aligning three or more versions of a text.</title>
<date>1999</date>
<booktitle>Parallel Text Processing.</booktitle>
<editor>In Jean Veronis, editor,</editor>
<publisher>Kluwer Academic.</publisher>
<contexts>
<context position="12715" citStr="Simard, 1999" startWordPosition="1964" endWordPosition="1965">o-training algorithm allows many source languages to be used. 3 Experimental Results In order to conduct co-training experiments we first needed to assemble appropriate corpora. The corpus used in our experiments was assembled from the data used in the (Och and Ney, 2001) multiple source translation paper. The data was gathered from the Bulletin of the European Union which is published on the Internet in the eleven official languages of the European Union. We used a subset of the data to create a multi-lingual corpus, aligning sentences between French, Spanish, German, Italian and Portuguese (Simard, 1999). Additionally we created bilingual corpora between English and each of the five languages using sentences that were not included in the multi-lingual corpus. Och and Ney (2001) used the data to find a translation that was most probable given multiple source strings. Och and Ney found that multi-source translations using two source languages reduced word error rate when compared to using source strings from a single language. For multi-source translations using source strings in six languages a greater reduction in word error rate was achieved. Our work is similar in spirit, although instead o</context>
</contexts>
<marker>Simard, 1999</marker>
<rawString>Michel Simard. 1999. Text-translation alignment: Aligning three or more versions of a text. In Jean Veronis, editor, Parallel Text Processing. Kluwer Academic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah Smith</author>
</authors>
<title>From words to corpora: Recognizing translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="3520" citStr="Smith, 2002" startWordPosition="555" endWordPosition="556">ned sentences. One approach that has been proposed to address the problem of limited training data is to harvest the web for bilingual texts (Resnik, 1998). The STRAND method automatically gathers web pages that are potential translations of each other by looking for documents in one language which have links whose text contains the name of another language. For example, if an English web page had a link with the text “Espa˜nol” or “en Espa˜nol” then the page linked to is treated as a candidate translation of the English page. Further checks verify the plausibility of its being a translation (Smith, 2002). Instead of attempting to gather new translations from the web, we describe an alternate method for automatically creating parallel corpora. Specifically, we examine the use of existing translations as a resource to bootstrap more training data, and to create data for new language pairs. We generate translation models from existing data and use them to produce translations of new sen64 62 60 58 56 54 52 50 48 46 44 10000 20000 Figure 1: Translation accuracy plotted against training corpus size tences. Incorporating this machine-created parallel data to the original set, and retraining the tra</context>
</contexts>
<marker>Smith, 2002</marker>
<rawString>Noah Smith. 2002. From words to corpora: Recognizing translation. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, Philadelphia, Pennsylvania.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>