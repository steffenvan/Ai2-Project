<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000017">
<title confidence="0.913647">
An Extension of BLANC to System Mentions
</title>
<author confidence="0.788282">
Xiaoqiang Luo Sameer Pradhan
</author>
<affiliation confidence="0.573037">
Google Inc. Harvard Medical School
</affiliation>
<address confidence="0.461674">
111 8th Ave, New York, NY 10011 300 Longwood Ave., Boston, MA 02115
</address>
<email confidence="0.93915">
xql@google.com sameer.pradhan@childrens.harvard.edu
</email>
<author confidence="0.773338">
Marta Recasens Eduard Hovy
</author>
<affiliation confidence="0.776468">
Google Inc. Carnegie Mellon University
</affiliation>
<address confidence="0.9007265">
1600 Amphitheatre Pkwy, 5000 Forbes Ave.
Mountain View, CA 94043 Pittsburgh, PA 15213
</address>
<email confidence="0.99933">
recasens@google.com hovy@cmu.edu
</email>
<sectionHeader confidence="0.994797" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999162083333333">
BLANC is a link-based coreference eval-
uation metric for measuring the qual-
ity of coreference systems on gold men-
tions. This paper extends the original
BLANC (“BLANC-gold” henceforth) to
system mentions, removing the gold men-
tion assumption. The proposed BLANC
falls back seamlessly to the original one if
system mentions are identical to gold men-
tions, and it is shown to strongly correlate
with existing metrics on the 2011 and 2012
CoNLL data.
</bodyText>
<sectionHeader confidence="0.998424" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99992335483871">
Coreference resolution aims at identifying natu-
ral language expressions (or mentions) that refer
to the same entity. It entails partitioning (often
imperfect) mentions into equivalence classes. A
critically important problem is how to measure the
quality of a coreference resolution system. Many
evaluation metrics have been proposed in the past
two decades, including the MUC measure (Vilain
et al., 1995), B-cubed (Bagga and Baldwin, 1998),
CEAF (Luo, 2005) and, more recently, BLANC-
gold (Recasens and Hovy, 2011). B-cubed and
CEAF treat entities as sets of mentions and mea-
sure the agreement between key (or gold standard)
entities and response (or system-generated) enti-
ties, while MUC and BLANC-gold are link-based.
In particular, MUC measures the degree of
agreement between key coreference links (i.e.,
links among mentions within entities) and re-
sponse coreference links, while non-coreference
links (i.e., links formed by mentions from different
entities) are not explicitly taken into account. This
leads to a phenomenon where coreference systems
outputting large entities are scored more favorably
than those outputting small entities (Luo, 2005).
BLANC (Recasens and Hovy, 2011), on the other
hand, considers both coreference links and non-
coreference links. It calculates recall, precision
and F-measure separately on coreference and non-
coreference links in the usual way, and defines
the overall recall, precision and F-measure as the
mean of the respective measures for coreference
and non-coreference links.
The BLANC-gold metric was developed with
the assumption that response mentions and key
mentions are identical. In reality, however, men-
tions need to be detected from natural language
text and the result is, more often than not, im-
perfect: some key mentions may be missing in
the response, and some response mentions may be
spurious—so-called “twinless” mentions by Stoy-
anov et al. (2009). Therefore, the identical-
mention-set assumption limits BLANC-gold’s ap-
plicability when gold mentions are not available,
or when one wants to have a single score mea-
suring both the quality of mention detection and
coreference resolution. The goal of this paper is
to extend the BLANC-gold metric to imperfect re-
sponse mentions.
We first briefly review the original definition of
BLANC, and rewrite its definition using set nota-
tion. We then argue that the gold-mention assump-
tion in Recasens and Hovy (2011) can be lifted
without changing the original definition. In fact,
the proposed BLANC metric subsumes the origi-
nal one in that its value is identical to the original
one when response mentions are identical to key
mentions.
The rest of the paper is organized as follows.
We introduce the notions used in this paper in
Section 2. We then present the original BLANC-
gold in Section 3 using the set notation defined in
Section 2. This paves the way to generalize it to
</bodyText>
<page confidence="0.985252">
24
</page>
<bodyText confidence="0.814081">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 24–29,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
imperfect system mentions, which is presented in
Section 4. The proposed BLANC is applied to the
CoNLL 2011 and 2012 shared task participants,
and the scores and its correlations with existing
metrics are shown in Section 5.
</bodyText>
<sectionHeader confidence="0.975751" genericHeader="introduction">
2 Notations
</sectionHeader>
<bodyText confidence="0.981681923076923">
To facilitate the presentation, we define the nota-
tions used in the paper.
We use key to refer to gold standard mentions or
entities, and response to refer to system mentions
or entities. The collection of key entities is denoted
by K = {ki}|K |i=1, where ki is the ith key entity;
accordingly, R = {rj}|R|
j=1 is the set of response
entities, and rj is the jth response entity. We as-
sume that mentions in {ki} and {rj} are unique;
in other words, there is no duplicate mention.
Let Ck(i) and Cr(j) be the set of coreference
links formed by mentions in ki and rj:
</bodyText>
<equation confidence="0.999979">
Ck(i) = {(m1, m2) : m1 E ki, m2 E ki, m1 =� m2}
Cr(j) = {(m1, m2) : m1 E rj, m2 E rj, m1 =� m2}
</equation>
<bodyText confidence="0.9974361">
As can be seen, a link is an undirected edge be-
tween two mentions, and it can be equivalently
represented by a pair of mentions. Note that when
an entity consists of a single mention, its corefer-
ence link set is empty.
Let Nk(i, j) (i =� j) be key non-coreference
links formed between mentions in ki and those
in kj, and let Nr(i, j) (i =� j) be response non-
coreference links formed between mentions in ri
and those in rj, respectively:
</bodyText>
<equation confidence="0.9905935">
Nk(i,j) = {(m1, m2) : m1 E ki, m2 E kj}
Nr(i, j) = {(m1, m2) : m1 E ri, m2 E rj}
</equation>
<bodyText confidence="0.9946096">
Note that the non-coreference link set is empty
when all mentions are in the same entity.
We use the same letter and subscription with-
out the index in parentheses to denote the union of
sets, e.g.,
</bodyText>
<equation confidence="0.940408">
Ck = UiCk(i), Nk = UiOjNk(i, j)
Cr = UjCr(j), Nr = UiojNr(i, j)
</equation>
<bodyText confidence="0.9901542">
We use Tk = Ck U Nk and Tr = CrUNrto
denote the total set of key links and total set of
response links, respectively. Clearly, Ck and Nk
form a partition of Tk since Ck n Nk = 0, Tk =
Ck U Nk. Likewise, Cr and Nr form a partition of
Tr.
We say that a key link l1 E Tk equals a response
link l2 E Tr if and only if the pair of mentions
from which the links are formed are identical. We
write l1 = l2 if two links are equal. It is easy to
see that the gold mention assumption—same set
of response mentions as the set of key mentions—
can be equivalently stated as Tk = Tr (this does
not necessarily mean that Ck = Cr or Nk = Nr).
We also use  |·  |to denote the size of a set.
</bodyText>
<sectionHeader confidence="0.996651" genericHeader="method">
3 Original BLANC
</sectionHeader>
<bodyText confidence="0.999338222222222">
BLANC-gold is adapted from Rand Index (Rand,
1971), a metric for clustering objects. Rand Index
is defined as the ratio between the number of cor-
rect within-cluster links plus the number of correct
cross-cluster links, and the total number of links.
When Tk = Tr, Rand Index can be applied di-
rectly since coreference resolution reduces to a
clustering problem where mentions are partitioned
into clusters (entities):
</bodyText>
<equation confidence="0.979718">
Rand Index = |Ck n Cr |+ |Nk n Nr |(1)
1 (|Tk|(|Tk |− 1))
2
</equation>
<bodyText confidence="0.999804583333333">
In practice, though, the simple-minded adoption
of Rand Index is not satisfactory since the number
of non-coreference links often overwhelms that of
coreference links (Recasens and Hovy, 2011), or,
|Nk |» |Ck |and |Nr |» |Cr|. Rand Index, if
used without modification, would not be sensitive
to changes of coreference links.
BLANC-gold solves this problem by averaging
the F-measure computed over coreference links
and the F-measure over non-coreference links.
Using the notations in Section 2, the recall, pre-
cision, and F-measure on coreference links are:
</bodyText>
<equation confidence="0.984649714285714">
R(g) =  |Ck n Cr  |(2)
|Ck n Cr |+ |Ck n Nr|
PC(g)  |Ck n Cr |
; (4)
R(g)
c + P (g)
c
</equation>
<bodyText confidence="0.997349">
Similarly, the recall, precision, and F-measure on
non-coreference links are computed as:
</bodyText>
<equation confidence="0.999750666666667">
R(g) = |Nk n Nr |(5)
|Nk n Cr |+ |Nk n Nr|
P(g) = |Nk n Nr |(6)
|Nr n Ck |+ |Nr n Nk|
n F(g) = 2R(g) P(g) n
=|
P�
Cr n Ck  |+  |Cr n Nk  |(3)
c F(g) = 2R(g)P(g) c
. (7)
R(g)
n + P (g) n
</equation>
<page confidence="0.963677">
25
</page>
<bodyText confidence="0.993019">
Finally, the BLANC-gold metric is the arithmetic
average of F(g)
</bodyText>
<equation confidence="0.989694">
c and F (g)
n :
n
BLANC(g) = F(g) c+ F(g)
</equation>
<bodyText confidence="0.9458615">
2
Superscript g in these equations highlights the fact
that they are meant for coreference systems with
gold mentions.
Eqn. (8) indicates that BLANC-gold assigns
equal weight to F(g)
c , the F-measure from coref-
erence links, and F(g)
n , the F-measure from non-
coreference links. This avoids the problem that
|Nk |» |Ck |and |Nr |» |Cr|, should the original
Rand Index be used.
In Eqn. (2) - (3) and Eqn. (5) - (6), denominators
are written as a sum of disjoint subsets so they can
be related to the contingency table in (Recasens
and Hovy, 2011). Under the assumption that Tk =
Tr, it is clear that Ck = (Ck n Cr) u (Ck n Nr),
Cr=(Ckn Cr) u (Nk n Cr), and soon.
</bodyText>
<sectionHeader confidence="0.998131" genericHeader="method">
4 BLANC for Imperfect Response
</sectionHeader>
<subsectionHeader confidence="0.639674">
Mentions
</subsectionHeader>
<bodyText confidence="0.999719523809524">
Under the assumption that the key and response
mention sets are identical (which implies that
Tk = Tr), Equations (2) to (7) make sense. For
example, Rc is the ratio of the number of correct
coreference links over the number of key corefer-
ence links; Pc is the ratio of the number of cor-
rect coreference links over the number of response
coreference links, and so on.
However, when response mentions are not iden-
tical to key mentions, a key coreference link may
not appear in either Cr or Nr, so Equations (2) to
(7) cannot be applied directly to systems with im-
perfect mentions. For instance, if the key entities
are {a,b,c} {d,e}; and the response entities
are {b,c} {e,f,g}, then the key coreference
link (a,b) is not seen on the response side; sim-
ilarly, it is possible that a response link does not
appear on the key side either: (c,f) and (f,g)
are not in the key in the above example.
To account for missing or spurious links, we ob-
serve that
</bodyText>
<listItem confidence="0.999866571428571">
• Ck \ Tr are key coreference links missing in
the response;
• Nk \ Tr are key non-coreference links miss-
ing in the response;
• Cr \ Tk are response coreference links miss-
ing in the key;
• Nr \ Tk are response non-coreference links
</listItem>
<bodyText confidence="0.984847">
missing in the key,
and we propose to extend the coreference F-
measure and non-coreference F-measure as fol-
lows. Coreference recall, precision and F-measure
are changed to:
</bodyText>
<equation confidence="0.999213875">
|Ck ∩ Cr|
Rc = (9)
|Ck ∩ Cr |+ |Ck ∩ Nr |+ |Ck \ Tr|
|Ck ∩ Cr|
Pc = |Cr ∩ Ck |+ |Cr ∩ Nk |+ |Cr \ Tk |(10)
Fc
= 2RcPc (11)
Rc + Pc
</equation>
<bodyText confidence="0.917761">
Non-coreference recall, precision and F-measure
are changed to:
</bodyText>
<equation confidence="0.996602">
Rn
= |Nk ∩ Nr |(12)
 |Nk ∩ Cr |+  |Nk ∩ Nr  |+  |Nk \ Tr |
Pn
= |Nk ∩ Nr |(13)
|Nr ∩ Ck |+ |Nr ∩ Nk |+ |Nr \ Tk|
Fn
= 2RnPn (14)
Rn + Pn .
</equation>
<bodyText confidence="0.995992">
The proposed BLANC continues to be the arith-
metic average of Fc and Fn:
</bodyText>
<equation confidence="0.991889">
Fc + Fn
BLANC =
2
</equation>
<bodyText confidence="0.9935255">
We observe that the definition of the proposed
BLANC, Equ. (9)-(14) subsume the BLANC-
gold (2) to (7) due to the following proposition:
If Tk = Tr, then BLANC = BLANC(g).
</bodyText>
<equation confidence="0.87796675">
Proof. We only need to show that Rc = R(g)
c ,
Pc = P(g)
c , Rn = R(g)
</equation>
<bodyText confidence="0.784418625">
n , and Pn = P(g)
n . We prove
the first one (the other proofs are similar and elided
due to space limitations). Since Tk = Tr and
Ck C Tk, we have Ck C Tr; thus Ck \Tr = ∅, and
|Ck n Tr |= 0. This establishes that Rc = R(g)
c.
Indeed, since Ck is a union of three disjoint sub-
</bodyText>
<equation confidence="0.65677275">
sets: Ck = (Ck n Cr) u (Ck n Nr) u (Ck \ Tr),
R(g)
c and Rc can be unified as |Ck∩Cr|. Unification
|CK|
</equation>
<bodyText confidence="0.999352">
for other component recalls and precisions can be
done similarly. So the final definition of BLANC
can be succinctly stated as:
</bodyText>
<equation confidence="0.998812428571429">
2|Nk ∩ Nr|
|Ck |+ |Cr|, Fn = |Nk |+ |Nr |(18)
Fc + Fn
BLANC = (19)
2
. (8)
. (15)
|Nk |, Pn = |Nk ∩ Nr |(17)
|Nr|
Rn = |Nk ∩ Nr|
Rc =|Ck∩Cr||C
k |, Pc = |Ck ∩ Cr |(16) |Cr|
2|Ck ∩ Cr|
Fc =
</equation>
<page confidence="0.974866">
26
</page>
<subsectionHeader confidence="0.98589">
4.1 Boundary Cases
</subsectionHeader>
<bodyText confidence="0.9864897">
Care has to be taken when counts of the BLANC
definition are 0. This can happen when all key
(or response) mentions are in one cluster or are
all singletons: the former case will lead to Nk = ∅
(or Nr = ∅); the latter will lead to Ck = ∅ (or
Cr = ∅). Observe that as long as |Ck |+ |Cr |&gt; 0,
Fc in (18) is well-defined; as long as |Nk|+|Nr |&gt;
0, Fn in (18) is well-defined. So we only need to
augment the BLANC definition for the following
cases:
</bodyText>
<listItem confidence="0.996469888888889">
(1) If Ck = Cr = ∅ and Nk = Nr = ∅, then
BLANC = I(Mk = Mr), where I(·) is an in-
dicator function whose value is 1 if its argument
is true, and 0 otherwise. Mk and Mr are the key
and response mention set. This can happen when a
document has no more than one mention and there
is no link.
(2) If Ck = Cr = ∅ and |Nk |+ |Nr |&gt; 0, then
BLANC = Fn. This is the case where the key
and response side has only entities consisting of
singleton mentions. Since there is no coreference
link, BLANC reduces to the non-coreference F-
measure Fn.
(3) If Nk = Nr = ∅ and |Ck |+ |Cr |&gt; 0, then
BLANC = Fc. This is the case where all mentions
in the key and response are in one entity. Since
there is no non-coreference link, BLANC reduces
to the coreference F-measure Fc.
</listItem>
<subsectionHeader confidence="0.997269">
4.2 Toy Examples
</subsectionHeader>
<bodyText confidence="0.998344">
We walk through a few examples and show how
BLANC is calculated in detail. In all the examples
below, each lower-case letter represents a mention;
mentions in an entity are closed in {}; two letters
in () represent a link.
Example 1. Key entities are {abc} and {d}; re-
sponse entities are {bc} and {de}. Obviously,
</bodyText>
<equation confidence="0.8438445">
Ck = {(ab), (bc), (ac)};
Nk = {(ad), (bd), (cd)};
Cr = {(bc), (de)};
Nr = {(bd), (be), (cd), (ce)}.
Therefore, Ck ∩ Cr = {(bc)}, Nk ∩ Nr =
{(bd), (cd)}, and Rc= 13, Pc = 12, Fc = 25; Rn =
2 3, Pn = 24, Fn = 47. Finally, BLANC = 17
35.
</equation>
<bodyText confidence="0.8413734">
Example 2. Key entity is {a}; response entity
is {b}. This is boundary case (1): BLANC = 0.
Example 3. Key entities are {a}{b}{c}; re-
sponse entities are {a}{b}{d}. This is boundary
case (2): there are no coreference links. Since
</bodyText>
<equation confidence="0.577196">
Nk = {(ab), (bc), (ca)},
</equation>
<table confidence="0.998049">
Participant R P BLANC
lee 50.23 49.28 48.84
sapena 40.68 49.05 44.47
nugues 47.83 44.22 45.95
chang 44.71 47.48 45.49
stoyanov 49.37 29.80 34.58
santos 46.74 37.33 41.33
song 36.88 39.69 30.92
sobha 35.42 39.56 36.31
yang 47.95 29.12 36.09
charton 42.32 31.54 35.65
hao 45.41 32.75 36.98
zhou 29.93 45.58 34.95
kobdani 32.29 33.01 32.57
xinxin 36.83 34.39 35.02
kummerfeld 34.84 29.53 30.98
zhang 30.10 43.96 35.71
zhekova 26.40 15.32 15.37
irwin 3.62 28.28 6.28
</table>
<tableCaption confidence="0.966347">
Table 1: The proposed BLANC scores of the
CoNLL-2011 shared task participants.
</tableCaption>
<equation confidence="0.823482833333333">
Nr = {(ab), (bd), (ad)},
we have
1 1
Nk∩Nr={(ab)}, and Rn= 3,Pn= 3.
So BLANC = Fn = 1
3.
</equation>
<bodyText confidence="0.802418714285714">
Example 4. Key entity is {abc}; response entity
is {bc}. This is boundary case (3): there are no
non-coreference links. Since
Ck = {(ab), (bc), (ca)}, and Cr = {(bc)},
we have
Ck∩Cr={(bc)},and Rc= 13,Pc=1,
So BLANC = Fc = 2 4 = 1 2.
</bodyText>
<sectionHeader confidence="0.999974" genericHeader="evaluation">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.999489">
5.1 CoNLL-2011/12
</subsectionHeader>
<bodyText confidence="0.999993375">
We have updated the publicly available CoNLL
coreference scorer1 with the proposed BLANC,
and used it to compute the proposed BLANC
scores for all the CoNLL 2011 (Pradhan et al.,
2011) and 2012 (Pradhan et al., 2012) participants
in the official track, where participants had to au-
tomatically predict the mentions. Tables 1 and 2
report the updated results.2
</bodyText>
<subsectionHeader confidence="0.999978">
5.2 Correlation with Other Measures
</subsectionHeader>
<bodyText confidence="0.999265">
Figure 1 shows how the proposed BLANC mea-
sure works when compared with existing met-
rics such as MUC, B-cubed and CEAF, us-
ing the BLANC and F1 scores. The proposed
BLANC is highly positively correlated with the
</bodyText>
<footnote confidence="0.985908333333333">
1http://code.google.com/p/reference-coreference-scorers
2The order is kept the same as in Pradhan et al. (2011) and
Pradhan et al. (2012) for easy comparison.
</footnote>
<page confidence="0.994625">
27
</page>
<table confidence="0.995480333333334">
Participant R P BLANC
Language: Arabic
fernandes 33.43 44.66 37.99
bjorkelund 32.65 45.47 37.93
uryupina 31.62 35.26 33.02
stamborg 32.59 36.92 34.50
chen 31.81 31.52 30.82
zhekova 11.04 62.58 18.51
li 4.60 56.63 8.42
Language: English
fernandes 54.91 63.66 58.75
martschat 52.00 58.84 55.04
bjorkelund 52.01 59.55 55.42
chang 52.85 55.03 53.86
chen 50.52 56.82 52.87
chunyang 51.19 55.47 52.65
stamborg 54.39 54.88 54.42
yuan 50.58 54.29 52.11
xu 45.99 54.59 46.47
shou 49.55 52.46 50.44
uryupina 44.15 48.89 46.04
songyang 40.60 50.85 45.10
zhekova 41.46 33.13 34.80
xinxin 44.39 32.79 36.54
li 25.17 52.96 31.85
Language: Chinese
chen 48.45 62.44 54.10
yuan 53.15 40.75 43.20
bjorkelund 47.58 45.93 44.22
xu 44.11 36.45 38.45
fernandes 42.36 61.72 49.63
stamborg 39.60 55.12 45.89
uryupina 33.44 56.01 41.88
martschat 27.24 62.33 37.89
chunyang 37.43 36.18 36.77
xinxin 36.46 39.79 37.85
li 21.61 62.94 30.37
chang 18.74 40.76 25.68
zhekova 21.50 37.18 22.89
</table>
<tableCaption confidence="0.9857795">
Table 2: The proposed BLANC scores of the
CoNLL-2012 shared task participants.
</tableCaption>
<table confidence="0.9979424">
R P F1
MUC 0.975 0.844 0.935
B-cubed 0.981 0.942 0.966
CEAF-m 0.941 0.923 0.966
CEAF-e 0.797 0.781 0.919
</table>
<tableCaption confidence="0.97326575">
Table 3: Pearson’s r correlation coefficients be-
tween the proposed BLANC and the other coref-
erence measures based on the CoNLL 2011/2012
results. All p-values are significant at &lt; 0.001.
</tableCaption>
<figureCaption confidence="0.82779725">
Figure 1: Correlation plot between the proposed
BLANC and the other measures based on the
CoNLL 2011/2012 results. All values are F1
scores.
</figureCaption>
<bodyText confidence="0.999389222222222">
other measures along R, P and F1 (Table 3),
showing that BLANC is able to capture most
entity-based similarities measured by B-cubed and
CEAF. However, the CoNLL data sets come from
OntoNotes (Hovy et al., 2006), where singleton
entities are not annotated, and BLANC has a wider
dynamic range on data sets with singletons (Re-
casens and Hovy, 2011). So the correlations will
likely be lower on data sets with singleton entities.
</bodyText>
<sectionHeader confidence="0.999199" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.965102">
The original BLANC-gold (Recasens and Hovy,
2011) requires that system mentions be identical
to gold mentions, which limits the metric’s utility
since detected system mentions often have missing
key mentions or spurious mentions. The proposed
BLANC is free from this assumption, and we
have shown that it subsumes the original BLANC-
gold. Since BLANC works on imperfect system
mentions, we have used it to score the CoNLL
2011 and 2012 coreference systems. The BLANC
scores show strong correlation with existing met-
rics, especially B-cubed and CEAF-m.
● ● ●
</bodyText>
<sectionHeader confidence="0.998065" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9951554">
We would like to thank the three anonymous re-
viewers for their invaluable suggestions for im-
proving the paper. This work was partially sup-
ported by grants R01LM10090 from the National
Library of Medicine.
</bodyText>
<figure confidence="0.999930471264367">
0 10 20 30 40 50 60 70 0 10 20 30 40 50 60 70
MUC B−.W
0 10 20 30 40 50 60 70 0 10 20 30 40 50 60 70
CEAF−m CEAF−e
BLANC
10 20 30 40 50 60
●
●
●
●
●
●
●
●
● ● ● ●
● ●
● ● ●
● ●
● ●
●
●●
●
●
●
●
●
●
BLANC
10 20 30 40 50 60
●
●
●
●
●
●
●
●●
● ● ●
●
●
● ● ●
● ● ●
●
●
●●●●●
●
●
BLANC
10 20 30 40 50 60
●
●
●
●
●
●● ●
● ● ●
● ● ●
● ●
● ● ● ● ●
●●
● ● ● ● ●
● ●
● ● ● ● ●
●
BLANC
10 20 30 40 50 60
●
●
●
●
●
●
● ●
● ● ●
● ● ●
● ● ●
● ●
●
● ● ●
● ●
● ● ●
● ●
● ●
●
● ●
●
●
</figure>
<page confidence="0.997033">
28
</page>
<sectionHeader confidence="0.993553" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99966124489796">
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of
the Linguistic Coreference Workshop at The First In-
ternational Conference on Language Resources and
Evaluation (LREC’98), pages 563–566.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% solution. In Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers, pages
57–60, New York City, USA, June. Association for
Computational Linguistics.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proc. of Human Language
Technology (HLT)/Empirical Methods in Natural
Language Processing (EMNLP).
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. CoNLL-2011 shared task: Modeling
unrestricted coreference in OntoNotes. In Proceed-
ings of the Fifteenth Conference on Computational
Natural Language Learning: Shared Task, pages 1–
27, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Joint Confer-
ence on EMNLP and CoNLL - Shared Task, pages
1–40, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
W. M. Rand. 1971. Objective criteria for the evalua-
tion of clustering methods. Journal of the American
Statistical Association, 66(336):846–850.
M. Recasens and E. Hovy. 2011. BLANC: Implement-
ing the Rand index for coreference evaluation. Nat-
ural Language Engineering, 17:485–510, 10.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state-
of-the-art. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP: Volume 2 - Volume 2,
ACL ’09, pages 656–664, Stroudsburg, PA, USA.
Association for Computational Linguistics.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, , and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In In Proc. of MUC6, pages 45–52.
</reference>
<page confidence="0.999117">
29
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.432397">
<title confidence="0.999769">An Extension of BLANC to System Mentions</title>
<author confidence="0.997214">Xiaoqiang Luo Sameer Pradhan</author>
<affiliation confidence="0.984126">Google Inc. Harvard Medical School</affiliation>
<address confidence="0.996661">111 8th Ave, New York, NY 10011 300 Longwood Ave., Boston, MA 02115</address>
<email confidence="0.996067">xql@google.comsameer.pradhan@childrens.harvard.edu</email>
<author confidence="0.998919">Marta Recasens Eduard Hovy</author>
<affiliation confidence="0.99897">Google Inc. Carnegie Mellon University</affiliation>
<address confidence="0.9996605">1600 Amphitheatre Pkwy, 5000 Forbes Ave. Mountain View, CA 94043 Pittsburgh, PA 15213</address>
<email confidence="0.99818">recasens@google.comhovy@cmu.edu</email>
<abstract confidence="0.953738076923077">BLANC is a link-based coreference evaluation metric for measuring the quality of coreference systems on gold mentions. This paper extends the original BLANC (“BLANC-gold” henceforth) to system mentions, removing the gold mention assumption. The proposed BLANC falls back seamlessly to the original one if system mentions are identical to gold mentions, and it is shown to strongly correlate with existing metrics on the 2011 and 2012 CoNLL data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In Proceedings of the Linguistic Coreference Workshop at The First International Conference on Language Resources and Evaluation (LREC’98),</booktitle>
<pages>563--566</pages>
<contexts>
<context position="1321" citStr="Bagga and Baldwin, 1998" startWordPosition="192" endWordPosition="195">sly to the original one if system mentions are identical to gold mentions, and it is shown to strongly correlate with existing metrics on the 2011 and 2012 CoNLL data. 1 Introduction Coreference resolution aims at identifying natural language expressions (or mentions) that refer to the same entity. It entails partitioning (often imperfect) mentions into equivalence classes. A critically important problem is how to measure the quality of a coreference resolution system. Many evaluation metrics have been proposed in the past two decades, including the MUC measure (Vilain et al., 1995), B-cubed (Bagga and Baldwin, 1998), CEAF (Luo, 2005) and, more recently, BLANCgold (Recasens and Hovy, 2011). B-cubed and CEAF treat entities as sets of mentions and measure the agreement between key (or gold standard) entities and response (or system-generated) entities, while MUC and BLANC-gold are link-based. In particular, MUC measures the degree of agreement between key coreference links (i.e., links among mentions within entities) and response coreference links, while non-coreference links (i.e., links formed by mentions from different entities) are not explicitly taken into account. This leads to a phenomenon where core</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In Proceedings of the Linguistic Coreference Workshop at The First International Conference on Language Resources and Evaluation (LREC’98), pages 563–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>OntoNotes: The 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers,</booktitle>
<pages>57--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="16643" citStr="Hovy et al., 2006" startWordPosition="3047" endWordPosition="3050">35 B-cubed 0.981 0.942 0.966 CEAF-m 0.941 0.923 0.966 CEAF-e 0.797 0.781 0.919 Table 3: Pearson’s r correlation coefficients between the proposed BLANC and the other coreference measures based on the CoNLL 2011/2012 results. All p-values are significant at &lt; 0.001. Figure 1: Correlation plot between the proposed BLANC and the other measures based on the CoNLL 2011/2012 results. All values are F1 scores. other measures along R, P and F1 (Table 3), showing that BLANC is able to capture most entity-based similarities measured by B-cubed and CEAF. However, the CoNLL data sets come from OntoNotes (Hovy et al., 2006), where singleton entities are not annotated, and BLANC has a wider dynamic range on data sets with singletons (Recasens and Hovy, 2011). So the correlations will likely be lower on data sets with singleton entities. 6 Conclusion The original BLANC-gold (Recasens and Hovy, 2011) requires that system mentions be identical to gold mentions, which limits the metric’s utility since detected system mentions often have missing key mentions or spurious mentions. The proposed BLANC is free from this assumption, and we have shown that it subsumes the original BLANCgold. Since BLANC works on imperfect s</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. OntoNotes: The 90% solution. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, pages 57–60, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>In Proc. of Human Language Technology (HLT)/Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="1339" citStr="Luo, 2005" startWordPosition="197" endWordPosition="198">m mentions are identical to gold mentions, and it is shown to strongly correlate with existing metrics on the 2011 and 2012 CoNLL data. 1 Introduction Coreference resolution aims at identifying natural language expressions (or mentions) that refer to the same entity. It entails partitioning (often imperfect) mentions into equivalence classes. A critically important problem is how to measure the quality of a coreference resolution system. Many evaluation metrics have been proposed in the past two decades, including the MUC measure (Vilain et al., 1995), B-cubed (Bagga and Baldwin, 1998), CEAF (Luo, 2005) and, more recently, BLANCgold (Recasens and Hovy, 2011). B-cubed and CEAF treat entities as sets of mentions and measure the agreement between key (or gold standard) entities and response (or system-generated) entities, while MUC and BLANC-gold are link-based. In particular, MUC measures the degree of agreement between key coreference links (i.e., links among mentions within entities) and response coreference links, while non-coreference links (i.e., links formed by mentions from different entities) are not explicitly taken into account. This leads to a phenomenon where coreference systems ou</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>Xiaoqiang Luo. 2005. On coreference resolution performance metrics. In Proc. of Human Language Technology (HLT)/Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Lance Ramshaw</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Ralph Weischedel</author>
<author>Nianwen Xue</author>
</authors>
<title>CoNLL-2011 shared task: Modeling unrestricted coreference in OntoNotes.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>1--27</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="14375" citStr="Pradhan et al., 2011" startWordPosition="2684" endWordPosition="2687">28 6.28 Table 1: The proposed BLANC scores of the CoNLL-2011 shared task participants. Nr = {(ab), (bd), (ad)}, we have 1 1 Nk∩Nr={(ab)}, and Rn= 3,Pn= 3. So BLANC = Fn = 1 3. Example 4. Key entity is {abc}; response entity is {bc}. This is boundary case (3): there are no non-coreference links. Since Ck = {(ab), (bc), (ca)}, and Cr = {(bc)}, we have Ck∩Cr={(bc)},and Rc= 13,Pc=1, So BLANC = Fc = 2 4 = 1 2. 5 Results 5.1 CoNLL-2011/12 We have updated the publicly available CoNLL coreference scorer1 with the proposed BLANC, and used it to compute the proposed BLANC scores for all the CoNLL 2011 (Pradhan et al., 2011) and 2012 (Pradhan et al., 2012) participants in the official track, where participants had to automatically predict the mentions. Tables 1 and 2 report the updated results.2 5.2 Correlation with Other Measures Figure 1 shows how the proposed BLANC measure works when compared with existing metrics such as MUC, B-cubed and CEAF, using the BLANC and F1 scores. The proposed BLANC is highly positively correlated with the 1http://code.google.com/p/reference-coreference-scorers 2The order is kept the same as in Pradhan et al. (2011) and Pradhan et al. (2012) for easy comparison. 27 Participant R P B</context>
</contexts>
<marker>Pradhan, Ramshaw, Marcus, Palmer, Weischedel, Xue, 2011</marker>
<rawString>Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, Martha Palmer, Ralph Weischedel, and Nianwen Xue. 2011. CoNLL-2011 shared task: Modeling unrestricted coreference in OntoNotes. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task, pages 1– 27, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Alessandro Moschitti</author>
<author>Nianwen Xue</author>
<author>Olga Uryupina</author>
<author>Yuchen Zhang</author>
</authors>
<title>CoNLL2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes.</title>
<date>2012</date>
<booktitle>In Joint Conference on EMNLP and CoNLL - Shared Task,</booktitle>
<pages>1--40</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="14407" citStr="Pradhan et al., 2012" startWordPosition="2690" endWordPosition="2693">ANC scores of the CoNLL-2011 shared task participants. Nr = {(ab), (bd), (ad)}, we have 1 1 Nk∩Nr={(ab)}, and Rn= 3,Pn= 3. So BLANC = Fn = 1 3. Example 4. Key entity is {abc}; response entity is {bc}. This is boundary case (3): there are no non-coreference links. Since Ck = {(ab), (bc), (ca)}, and Cr = {(bc)}, we have Ck∩Cr={(bc)},and Rc= 13,Pc=1, So BLANC = Fc = 2 4 = 1 2. 5 Results 5.1 CoNLL-2011/12 We have updated the publicly available CoNLL coreference scorer1 with the proposed BLANC, and used it to compute the proposed BLANC scores for all the CoNLL 2011 (Pradhan et al., 2011) and 2012 (Pradhan et al., 2012) participants in the official track, where participants had to automatically predict the mentions. Tables 1 and 2 report the updated results.2 5.2 Correlation with Other Measures Figure 1 shows how the proposed BLANC measure works when compared with existing metrics such as MUC, B-cubed and CEAF, using the BLANC and F1 scores. The proposed BLANC is highly positively correlated with the 1http://code.google.com/p/reference-coreference-scorers 2The order is kept the same as in Pradhan et al. (2011) and Pradhan et al. (2012) for easy comparison. 27 Participant R P BLANC Language: Arabic fernandes </context>
</contexts>
<marker>Pradhan, Moschitti, Xue, Uryupina, Zhang, 2012</marker>
<rawString>Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012. CoNLL2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes. In Joint Conference on EMNLP and CoNLL - Shared Task, pages 1–40, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W M Rand</author>
</authors>
<title>Objective criteria for the evaluation of clustering methods.</title>
<date>1971</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>66</volume>
<issue>336</issue>
<contexts>
<context position="6390" citStr="Rand, 1971" startWordPosition="1097" endWordPosition="1098"> and Nk form a partition of Tk since Ck n Nk = 0, Tk = Ck U Nk. Likewise, Cr and Nr form a partition of Tr. We say that a key link l1 E Tk equals a response link l2 E Tr if and only if the pair of mentions from which the links are formed are identical. We write l1 = l2 if two links are equal. It is easy to see that the gold mention assumption—same set of response mentions as the set of key mentions— can be equivalently stated as Tk = Tr (this does not necessarily mean that Ck = Cr or Nk = Nr). We also use |· |to denote the size of a set. 3 Original BLANC BLANC-gold is adapted from Rand Index (Rand, 1971), a metric for clustering objects. Rand Index is defined as the ratio between the number of correct within-cluster links plus the number of correct cross-cluster links, and the total number of links. When Tk = Tr, Rand Index can be applied directly since coreference resolution reduces to a clustering problem where mentions are partitioned into clusters (entities): Rand Index = |Ck n Cr |+ |Nk n Nr |(1) 1 (|Tk|(|Tk |− 1)) 2 In practice, though, the simple-minded adoption of Rand Index is not satisfactory since the number of non-coreference links often overwhelms that of coreference links (Recas</context>
</contexts>
<marker>Rand, 1971</marker>
<rawString>W. M. Rand. 1971. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical Association, 66(336):846–850.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Recasens</author>
<author>E Hovy</author>
</authors>
<title>BLANC: Implementing the Rand index for coreference evaluation.</title>
<date>2011</date>
<journal>Natural Language Engineering,</journal>
<volume>17</volume>
<pages>10</pages>
<contexts>
<context position="1395" citStr="Recasens and Hovy, 2011" startWordPosition="204" endWordPosition="207"> and it is shown to strongly correlate with existing metrics on the 2011 and 2012 CoNLL data. 1 Introduction Coreference resolution aims at identifying natural language expressions (or mentions) that refer to the same entity. It entails partitioning (often imperfect) mentions into equivalence classes. A critically important problem is how to measure the quality of a coreference resolution system. Many evaluation metrics have been proposed in the past two decades, including the MUC measure (Vilain et al., 1995), B-cubed (Bagga and Baldwin, 1998), CEAF (Luo, 2005) and, more recently, BLANCgold (Recasens and Hovy, 2011). B-cubed and CEAF treat entities as sets of mentions and measure the agreement between key (or gold standard) entities and response (or system-generated) entities, while MUC and BLANC-gold are link-based. In particular, MUC measures the degree of agreement between key coreference links (i.e., links among mentions within entities) and response coreference links, while non-coreference links (i.e., links formed by mentions from different entities) are not explicitly taken into account. This leads to a phenomenon where coreference systems outputting large entities are scored more favorably than t</context>
<context position="3297" citStr="Recasens and Hovy (2011)" startWordPosition="496" endWordPosition="499">g in the response, and some response mentions may be spurious—so-called “twinless” mentions by Stoyanov et al. (2009). Therefore, the identicalmention-set assumption limits BLANC-gold’s applicability when gold mentions are not available, or when one wants to have a single score measuring both the quality of mention detection and coreference resolution. The goal of this paper is to extend the BLANC-gold metric to imperfect response mentions. We first briefly review the original definition of BLANC, and rewrite its definition using set notation. We then argue that the gold-mention assumption in Recasens and Hovy (2011) can be lifted without changing the original definition. In fact, the proposed BLANC metric subsumes the original one in that its value is identical to the original one when response mentions are identical to key mentions. The rest of the paper is organized as follows. We introduce the notions used in this paper in Section 2. We then present the original BLANCgold in Section 3 using the set notation defined in Section 2. This paves the way to generalize it to 24 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 24–29, Baltimore, Maryl</context>
<context position="7009" citStr="Recasens and Hovy, 2011" startWordPosition="1198" endWordPosition="1201">1971), a metric for clustering objects. Rand Index is defined as the ratio between the number of correct within-cluster links plus the number of correct cross-cluster links, and the total number of links. When Tk = Tr, Rand Index can be applied directly since coreference resolution reduces to a clustering problem where mentions are partitioned into clusters (entities): Rand Index = |Ck n Cr |+ |Nk n Nr |(1) 1 (|Tk|(|Tk |− 1)) 2 In practice, though, the simple-minded adoption of Rand Index is not satisfactory since the number of non-coreference links often overwhelms that of coreference links (Recasens and Hovy, 2011), or, |Nk |» |Ck |and |Nr |» |Cr|. Rand Index, if used without modification, would not be sensitive to changes of coreference links. BLANC-gold solves this problem by averaging the F-measure computed over coreference links and the F-measure over non-coreference links. Using the notations in Section 2, the recall, precision, and F-measure on coreference links are: R(g) = |Ck n Cr |(2) |Ck n Cr |+ |Ck n Nr| PC(g) |Ck n Cr | ; (4) R(g) c + P (g) c Similarly, the recall, precision, and F-measure on non-coreference links are computed as: R(g) = |Nk n Nr |(5) |Nk n Cr |+ |Nk n Nr| P(g) = |Nk n Nr |(</context>
<context position="8386" citStr="Recasens and Hovy, 2011" startWordPosition="1470" endWordPosition="1473">ic is the arithmetic average of F(g) c and F (g) n : n BLANC(g) = F(g) c+ F(g) 2 Superscript g in these equations highlights the fact that they are meant for coreference systems with gold mentions. Eqn. (8) indicates that BLANC-gold assigns equal weight to F(g) c , the F-measure from coreference links, and F(g) n , the F-measure from noncoreference links. This avoids the problem that |Nk |» |Ck |and |Nr |» |Cr|, should the original Rand Index be used. In Eqn. (2) - (3) and Eqn. (5) - (6), denominators are written as a sum of disjoint subsets so they can be related to the contingency table in (Recasens and Hovy, 2011). Under the assumption that Tk = Tr, it is clear that Ck = (Ck n Cr) u (Ck n Nr), Cr=(Ckn Cr) u (Nk n Cr), and soon. 4 BLANC for Imperfect Response Mentions Under the assumption that the key and response mention sets are identical (which implies that Tk = Tr), Equations (2) to (7) make sense. For example, Rc is the ratio of the number of correct coreference links over the number of key coreference links; Pc is the ratio of the number of correct coreference links over the number of response coreference links, and so on. However, when response mentions are not identical to key mentions, a key co</context>
<context position="16779" citStr="Recasens and Hovy, 2011" startWordPosition="3069" endWordPosition="3073">en the proposed BLANC and the other coreference measures based on the CoNLL 2011/2012 results. All p-values are significant at &lt; 0.001. Figure 1: Correlation plot between the proposed BLANC and the other measures based on the CoNLL 2011/2012 results. All values are F1 scores. other measures along R, P and F1 (Table 3), showing that BLANC is able to capture most entity-based similarities measured by B-cubed and CEAF. However, the CoNLL data sets come from OntoNotes (Hovy et al., 2006), where singleton entities are not annotated, and BLANC has a wider dynamic range on data sets with singletons (Recasens and Hovy, 2011). So the correlations will likely be lower on data sets with singleton entities. 6 Conclusion The original BLANC-gold (Recasens and Hovy, 2011) requires that system mentions be identical to gold mentions, which limits the metric’s utility since detected system mentions often have missing key mentions or spurious mentions. The proposed BLANC is free from this assumption, and we have shown that it subsumes the original BLANCgold. Since BLANC works on imperfect system mentions, we have used it to score the CoNLL 2011 and 2012 coreference systems. The BLANC scores show strong correlation with exis</context>
</contexts>
<marker>Recasens, Hovy, 2011</marker>
<rawString>M. Recasens and E. Hovy. 2011. BLANC: Implementing the Rand index for coreference evaluation. Natural Language Engineering, 17:485–510, 10.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Veselin Stoyanov</author>
<author>Nathan Gilbert</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
</authors>
<title>Conundrums in noun phrase coreference resolution: Making sense of the stateof-the-art.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2, ACL ’09,</booktitle>
<pages>656--664</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2790" citStr="Stoyanov et al. (2009)" startWordPosition="414" endWordPosition="418">ll, precision and F-measure separately on coreference and noncoreference links in the usual way, and defines the overall recall, precision and F-measure as the mean of the respective measures for coreference and non-coreference links. The BLANC-gold metric was developed with the assumption that response mentions and key mentions are identical. In reality, however, mentions need to be detected from natural language text and the result is, more often than not, imperfect: some key mentions may be missing in the response, and some response mentions may be spurious—so-called “twinless” mentions by Stoyanov et al. (2009). Therefore, the identicalmention-set assumption limits BLANC-gold’s applicability when gold mentions are not available, or when one wants to have a single score measuring both the quality of mention detection and coreference resolution. The goal of this paper is to extend the BLANC-gold metric to imperfect response mentions. We first briefly review the original definition of BLANC, and rewrite its definition using set notation. We then argue that the gold-mention assumption in Recasens and Hovy (2011) can be lifted without changing the original definition. In fact, the proposed BLANC metric s</context>
</contexts>
<marker>Stoyanov, Gilbert, Cardie, Riloff, 2009</marker>
<rawString>Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and Ellen Riloff. 2009. Conundrums in noun phrase coreference resolution: Making sense of the stateof-the-art. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2, ACL ’09, pages 656–664, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Vilain</author>
<author>J Burger</author>
<author>J Aberdeen</author>
<author>D Connolly</author>
</authors>
<title>A model-theoretic coreference scoring scheme. In</title>
<date>1995</date>
<booktitle>In Proc. of MUC6,</booktitle>
<pages>45--52</pages>
<contexts>
<context position="1286" citStr="Vilain et al., 1995" startWordPosition="187" endWordPosition="190">oposed BLANC falls back seamlessly to the original one if system mentions are identical to gold mentions, and it is shown to strongly correlate with existing metrics on the 2011 and 2012 CoNLL data. 1 Introduction Coreference resolution aims at identifying natural language expressions (or mentions) that refer to the same entity. It entails partitioning (often imperfect) mentions into equivalence classes. A critically important problem is how to measure the quality of a coreference resolution system. Many evaluation metrics have been proposed in the past two decades, including the MUC measure (Vilain et al., 1995), B-cubed (Bagga and Baldwin, 1998), CEAF (Luo, 2005) and, more recently, BLANCgold (Recasens and Hovy, 2011). B-cubed and CEAF treat entities as sets of mentions and measure the agreement between key (or gold standard) entities and response (or system-generated) entities, while MUC and BLANC-gold are link-based. In particular, MUC measures the degree of agreement between key coreference links (i.e., links among mentions within entities) and response coreference links, while non-coreference links (i.e., links formed by mentions from different entities) are not explicitly taken into account. Th</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, 1995</marker>
<rawString>M. Vilain, J. Burger, J. Aberdeen, D. Connolly, , and L. Hirschman. 1995. A model-theoretic coreference scoring scheme. In In Proc. of MUC6, pages 45–52.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>