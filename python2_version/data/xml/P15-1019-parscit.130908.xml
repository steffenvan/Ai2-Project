<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000523">
<title confidence="0.988984">
Generative Event Schema Induction with Entity Disambiguation
</title>
<author confidence="0.951459">
Kiem-Hieu Nguyen1, 2 Xavier Tannier3,1 Olivier Ferret2 Romaric Besanc¸on2
</author>
<listItem confidence="0.820331666666667">
(1) LIMSI-CNRS
(2) CEA, LIST, Laboratoire Vision et Ingnierie des Contenus, F-91191, Gif-sur-Yvette
(3) Univ. Paris-Sud
</listItem>
<email confidence="0.995917">
{nguyen,xtannier}@limsi.fr, {olivier.ferret,romaric.besancon}@cea.fr
</email>
<sectionHeader confidence="0.997354" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999914125">
This paper presents a generative model to
event schema induction. Previous meth-
ods in the literature only use head words
to represent entities. However, elements
other than head words contain useful in-
formation. For instance, an armed man
is more discriminative than man. Our
model takes into account this information
and precisely represents it using proba-
bilistic topic distributions. We illustrate
that such information plays an important
role in parameter estimation. Mostly, it
makes topic distributions more coherent
and more discriminative. Experimental
results on benchmark dataset empirically
confirm this enhancement.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999933322580646">
Information Extraction was initially defined (and
is still defined) by the MUC evaluations (Grish-
man and Sundheim, 1996) and more specifically
by the task of template filling. The objective of
this task is to assign event roles to individual tex-
tual mentions. A template defines a specific type
of events (e.g. earthquakes), associated with se-
mantic roles (or slots) hold by entities (for earth-
quakes, their location, date, magnitude and the
damages they caused (Jean-Louis et al., 2011)).
Schema induction is the task of learning these
templates with no supervision from unlabeled text.
We focus here on event schema induction and con-
tinue the trend of generative models proposed ear-
lier for this task. The idea is to group together
entities corresponding to the same role in an event
template based on the similarity of the relations
that these entities hold with predicates. For ex-
ample, in a corpus about terrorist attacks, enti-
ties that are objects of verbs to kill, to attack can
be grouped together and characterized by a role
named VICTIM. The output of this identification
operation is a set of clusters of which members
are both words and relations, associated with their
probability (see an example later in Figure 4).
These clusters are not labeled but each of them
represents an event slot.
Our approach here is to improve this initial idea
by entity disambiguation. Some ambiguous enti-
ties, such as man or soldier, can match two differ-
ent slots (victim or perpetrator). An entity such as
terrorist can be mixed up with victims when arti-
cles relate that a terrorist has been killed by police
(and thus is object of to kill). Our hypothesis is
that the immediate context of entities is helpful for
disambiguating them. For example, the fact that
man is associated with armed, dangerous, heroic
or innocent can lead to a better attribution and def-
inition of roles. We then introduce relations be-
tween entities and their attributes in the model by
means of syntactic relations.
The document level, which is generally a cen-
ter notion in topic modeling, is not used in our
generative model. This results in a simpler, more
intuitive model, where observations are generated
from slots, that are defined by probabilistic dis-
tributions on entities, predicates and syntactic at-
tributes. This model offers room for further exten-
sions since multiple observations on an entity can
be represented in the same manner.
Model parameters are estimated by Gibbs sam-
pling. We evaluate the performance of this ap-
proach by an automatic and empiric mapping be-
tween slots from the system and slots from the ref-
erence in a way similar to previous work in the
domain.
The rest of this paper is organized as follows:
Section 2 briefly presents previous work; in Sec-
tion 3, we detail our entity and relation represen-
tation; we describe our generative model in Sec-
tion 4, before presenting our experiments and eval-
uations in Section 5.
</bodyText>
<page confidence="0.967432">
188
</page>
<note confidence="0.983218">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 188–197,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.999126" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99984794">
Despite efforts made for making template fill-
ing as generic as possible, it still depends heav-
ily on the type of events. Mixing generic
processes with a restrictive number of domain-
specific rules (Freedman et al., 2011) or exam-
ples (Grishman and He, 2014) is a way to reduce
the amount of effort needed for adapting a sys-
tem to another domain. The approaches of On-
demand information extraction (Hasegawa et al.,
2004; Sekine, 2006) and Preemptive Information
Extraction (Shinyama and Sekine, 2006) tried to
overcome this difficulty in another way by exploit-
ing templates induced from representative docu-
ments selected by queries.
Event schema induction takes root in work
on the acquisition from text of knowledge struc-
tures, such as the Memory Organization Pack-
ets (Schank, 1980), used by early text under-
standing systems (DeJong, 1982) and more re-
cently by Ferret and Grau (1997). First attempts
for applying such processes to schema induc-
tion have been made in the fields of Informa-
tion Extraction (Collier, 1998), Automatic Sum-
marization (Harabagiu, 2004) and event Question-
Answering (Filatova et al., 2006; Filatova, 2008).
More recently, work after (Hasegawa et al.,
2004) has developed weakly supervised forms
of Information Extraction including schema in-
duction in their objectives. However, they have
been mainly applied to binary relation extraction
in practice (Eichler et al., 2008; Rosenfeld and
Feldman, 2007; Min et al., 2012). In parallel,
several approaches were proposed for perform-
ing specifically schema induction in already ex-
isting frameworks: clause graph clustering (Qiu
et al., 2008), event sequence alignment (Reg-
neri et al., 2010) or LDA-based approach relying
on FrameNet-like semantic frames (Bejan, 2008).
More event-specific generative models were pro-
posed by Chambers (2013) and Cheung et al.
(2013). Finally, Chambers and Jurafsky (2008),
Chambers and Jurafsky (2009), Chambers and Ju-
rafsky (2011), improved by Balasubramanian et al.
(2013), and Chambers (2013) focused specifically
on the induction of event roles and the identifica-
tion of chains of events for building representa-
tions from texts by exploiting coreference resolu-
tion or the temporal ordering of events. All this
work is also linked to work about the induction of
scripts from texts, more or less closely linked to
</bodyText>
<figure confidence="0.817090285714286">
Attributes Head Triggers
#1 [armed:amod] man [attack:nsubj,
kill:nsubj]
#2 [police:nn] station [attack:dobj]
#3 [] policeman [kill:dobj]
#4 [innocent:amod, man [wound:dobj]
young:amod]
</figure>
<figureCaption confidence="0.9945775">
Figure 1: Entity representation as tuples of ([at-
tributes], head, [triggers]).
</figureCaption>
<bodyText confidence="0.937291833333333">
events, such as (Frermann et al., 2014), (Pichotta
and Mooney, 2014) or (Modi and Titov, 2014).
The work we present in this article is in line
with Chambers (2013), which will be described in
more details in Section 5, together with a quanti-
tative and qualitative comparison.
</bodyText>
<sectionHeader confidence="0.996877" genericHeader="method">
3 Entity Representation
</sectionHeader>
<bodyText confidence="0.98963334375">
An entity is represented as a triple containing: a
head word h, a list A of attribute relations and a
list T of trigger relations. Consider the following
example:
(1) Two armed men attacked the police station
and killed a policeman. An innocent young
man was also wounded.
As illustrated in Figure 1, four entities, equiva-
lent to four separated triples, are generated from
the text above. Head words are extracted from
noun phrases. A trigger relation is composed
of a predicate (attack, kill, wound) and a depen-
dency type (subject, object). An attribute rela-
tion is composed of an argument (armed, police,
young) and a dependency type (adjectival, nomi-
nal or verbal modifier). In the relationship to trig-
gers, a head word is argument, but in the relation-
ship to attributes, it is predicate. We use Stanford
NLP toolkit (Manning et al., 2014) for parsing and
coreference resolution.
A head word is extracted if it is a nominal or
proper noun and it is related to at least one trig-
ger; pronouns are omitted. A trigger of an head
word is extracted if it is a verb or an eventive noun
and the head word serves as its subject, object, or
preposition. We use the categories noun.EVENT
and noun.ACT in WordNet as a list of eventive
nouns. A head word can have more than one trig-
ger. These multiple relations can come from a syn-
tactic coordination inside a single sentence, as it
is the case in the first sentence of the illustrating
example. They can also represent a coreference
</bodyText>
<page confidence="0.998918">
189
</page>
<figureCaption confidence="0.997011">
Figure 2: Generative model for event induction.
</figureCaption>
<bodyText confidence="0.999865">
chain across sentences, as we use coreference res-
olution to merge the triggers of mentions corefer-
ing to the same entity in a document. Coreferences
are useful sources for event induction (Chambers
and Jurafsky, 2011; Chambers, 2013). Finally, an
attribute is extracted if it is an adjective, a noun or
a verb and serves as an adjective, verbal or nom-
inal modifier of a head word. If there are several
modifiers, only the closest to the head word is se-
lected. This “best selection” heuristic allows to
omit non-discriminative attributes for the entity.
</bodyText>
<sectionHeader confidence="0.999879" genericHeader="method">
4 Generative Model
</sectionHeader>
<subsectionHeader confidence="0.998056">
4.1 Model Description
</subsectionHeader>
<bodyText confidence="0.995393307692308">
Figure 2 shows the plate notation of our model.
For each triple representing an entity e, the model
first assigns a slot s for the entity from an uni-
form distribution uni(1, K). Its head word h is
then generated from a multinominal distribution
7rs. Each ti of event trigger relations Te is gen-
erated from a multinominal distribution φs. Each
aj of attribute relations Ae is similarly generated
from a multinominal distribution θs. The distri-
butions θ, 7r, and φ are generated from Dirichlet
priors dir(α), dir(Q) and dir(γ) respectively.
Given a set of entities E, our model (7r, φ, θ) is
defined by
</bodyText>
<equation confidence="0.987944">
Pπ,φ,θ(E) = 11 Pπ,φ,θ(e) (2)
e∈E
</equation>
<bodyText confidence="0.985314">
where the probability of each entity e is defined by
</bodyText>
<equation confidence="0.9407505">
Pπ,φ,θ(e) = P(s)
× P(h|s)
11 × P(t|s)
t∈Te
11 × P(a|s) (3)
a∈Ae
</equation>
<bodyText confidence="0.542756">
The generative story is as follows:
</bodyText>
<figure confidence="0.414920090909091">
for slot s +— 1 to K do
Generate an attribute distribution θs from a
Dirichlet prior dir(α);
Generate a head distribution irs from a Dirichlet
prior dir(Q);
Generate a trigger distribution φs from a Dirichlet
prior dir(ry);
end
for entity e E E do
Generate a slot s from a uniform distribution
uni(1, K);
Generate a head h from a multinominal distribution
irs;
for i +— 1 to JTeJ do
Generate a trigger ti from a multinominal
distribution φs;
end
forj+— 1 to JAeJ do
Generate an attribute aj from a multinominal
distribution φs;
end
end
</figure>
<subsectionHeader confidence="0.980399">
4.2 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.999959066666667">
For parameter estimation, we use the Gibbs sam-
pling method (Griffiths, 2002). The slot variable
s is sampled by integrating out all the other vari-
ables.
Previous models (Cheung et al., 2013; Cham-
bers, 2013) are based on document-level topic
modeling, which originated from models such as
Latent Dirichlet Allocation (Blei et al., 2003).
Our model is, instead, independent from docu-
ment contexts. Its input is a sequence of entity
triples. Document boundary is only used in a post-
processing step of filtering (see Section 5.3 for
more details). There is a universal slot distribu-
tion instead of each slot distribution for one doc-
ument. Furthermore, slot prior is ignored by us-
ing a uniform distribution as a particular case of
categorical probability. Sampling-based slot as-
signment could depend on initial states and ran-
dom seeds. In our implementation of Gibbs sam-
pling, we use 2,000 burn-in of overall 10,000 it-
erations. The purpose of burn-in is to assure that
parameters converge to a stable state before esti-
mating the probability distributions. Moreover, an
interval step of 100 is applied between consecutive
samples in order to avoid too strong coherence.
Particularly, for tracking changes in probabili-
ties resulting from attribute relations, we ran in
the first stage a specific burn-in with only heads
and trigger relations. This stable state was then
used as initialization for the second burn-in in
</bodyText>
<figure confidence="0.983815102564102">
dir(γ)
dir(β)
dir(α)
φ
t
T
π
h
s
θ a
A
#tuples
uni(1,K)
190
P(terrorist|ATTACKvictim)
0.0035
0.0025
0.0015
0.0005
0.004
0.003
0.002
0.001
Using attributes
No attribute
P(terrorist|ATTACKperpetrator)
0.035
0.025
0.015
0.005
0.04
0.03
0.02
0.01
0
Using attributes
No attribute
10 20 30 40 50 60 70 80 90 100 10 20 30 40 50 60 70 80 90 100
BURN_IN iterations (x20) BURN_IN iterations (x20)
</figure>
<figureCaption confidence="0.988902333333333">
Figure 3: Probability convergence when using attributes in sampling. The use of attributes is started
at point 50 (i.e., 50% of burn-in phase). The dotted line shows convergence without attributes; the
continuous line shows convergence with attributes.
</figureCaption>
<figure confidence="0.998582115384615">
0.3
0.25
P(Will:dobj|ATTACKvictim)
0.2
0.15
0.1
0.05
0
(a) P(terrorist|ATTACK victim)
Using attributes
No attribute
10 20 30 40 50 60 70 80 90 100
BURN_IN iterations (x20)
(c) P(kill : dobjIATTACK victim)
(b) P(terroristIATTACK perp)
Using attributes
No attribute
10 20 30 40 50 60 70 80 90 100
BURN_IN iterations (x20)
(d) P(kill : dobjIATTACK perp)
0.02
0
P(kill:dobj|ATTACKpsrpstrator)
0.015
0.01
0.005
</figure>
<bodyText confidence="0.999677866666667">
which attributes, heads, and triggers were used al-
together. This specific experimental setting made
us understand how the attributes modified distri-
butions. We observed that non-ambiguous words
or relations (i.e. explode, murder:nsubj) were only
slightly modified whereas probabilities of ambigu-
ous words such as man, soldier or triggers such as
kill:dobj or attack:nsubj converged smoothly to a
different stable state that was semantically more
coherent. For instance, the model interestingly re-
alized that even if a terrorist was killed (e.g. by
police), he was not actually a real victim of an at-
tack. Figure 3 shows probability convergences of
terrorist and kill:dobj given ATTACK victim and
ATTACK perpetrator.
</bodyText>
<sectionHeader confidence="0.99815" genericHeader="evaluation">
5 Evaluations
</sectionHeader>
<bodyText confidence="0.999932315789474">
In order to compare with related work, we eval-
uated our method on the Message Understanding
Conference (MUC-4) corpus (Sundheim, 1991)
using precision, recall and F-score as conventional
metrics for template extraction.
In what follows, we first introduce the MUC-
4 corpus (Section 5.1.1), we detail the mapping
technique between learned slots and reference
slots (5.1.2) as well as the hyper-parameters of
our model (5.1.3). Next, we present a first exper-
iment (Section 5.2) showing how using attribute
relations improves overall results. The second ex-
periment (Section 5.3) studies the impact of doc-
ument classification. We then compare our re-
sults with previous approaches, more particularly
with Chambers (2013), from both quantitative and
qualitative points of view (Section 5.4). Finally,
Section 5.5 is dedicated to error analysis, with a
special emphasis on sources of false positives.
</bodyText>
<subsectionHeader confidence="0.514378">
5.1 Experimental Setups
5.1.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999945333333333">
The MUC-4 corpus contains 1,700 news articles
about terrorist incidents happening in Latin Amer-
ica. The corpus is divided into 1,300 documents
</bodyText>
<page confidence="0.993943">
191
</page>
<bodyText confidence="0.99992555">
for the development set and four test sets, each
containing 100 documents.
We follow the rules in the literature to guarantee
comparable results (Patwardhan and Riloff, 2007;
Chambers and Jurafsky, 2011). The evaluation fo-
cuses on four template types – ARSON, ATTACK,
BOMBING, KIDNAPPING – and four slots – Perpe-
trator, Instrument, Target, and Victim. Perpetrator
is merged from Perpetrator Individual and Perpe-
trator Organization. The matching between sys-
tem answers and references is based on head word
matching. A head word is defined as the right-
most word of the phrase or as the right-most word
of the first ‘of’ if the phrase contains any. Op-
tional templates and slots are ignored when calcu-
lating recall. Template types are ignored in eval-
uation: this means that a perpetrator of BOMBING
in the answers could be compared to a perpetrator
of ARSON, ATTACK, BOMBING or KIDNAPPING in
the reference.
</bodyText>
<subsubsectionHeader confidence="0.982419">
5.1.2 Slot Mapping
</subsubsectionHeader>
<bodyText confidence="0.999984363636364">
The model learns K slots and assigns each entity
in a document to one of the learned slots. Slot
mapping consists in matching each reference slot
to an equivalent learned slot.
Note that among the K learned slots, some are
irrelevant while others, sometimes of high quality,
contain entities that are not part of the reference
(spatio-temporal information, protagonist context,
etc.). For this reason, it makes sense to have much
more learned slots than expected event slots.
Similarly to previous work in the literature, we
implemented an automatic empirical-driven slot
mapping. Each reference slot was mapped to
the learned slot that performed the best on the
task of template extraction according to the F-
score metric. Here, two identical slots of two
different templates, such as ATTACK victim and
KIDNAPPING victim, must to be mapped sepa-
rately. Figure 4 shows the most common words of
two learned slots which were mapped to BOMB-
ING instrument and KIDNAPPING victim. This
mapping is then kept for testing.
</bodyText>
<subsubsectionHeader confidence="0.972029">
5.1.3 Parameter Tuning
</subsubsectionHeader>
<bodyText confidence="0.999822666666667">
We first tuned hyper-parameters of the models on
the development set. The number of slots was set
to K = 35. Dirichlet priors were set to α = 0.1,
Q = 1 and γ = 0.1. The model was learned from
the whole dataset. Slot mapping was done on tst1
and tst2. Outputs from tst3 and tst4 were eval-
</bodyText>
<table confidence="0.790525428571429">
BOMBING instrument
Attributes Heads Triggers
car:nn bomb explode:nsubj
powerful:amod fire hear:dobj
explosive:amod explosion place:dobj
dynamite:nn blow cause:nsubj
heavy:amod charge set:dobj
KIDNAPPING victim
Attributes Heads Triggers
several:amod people arrest:dobj
other:amod person kidnap:dobj
responsible:amod man release:dobj
military:amod member kill:dobj
young:amod leader identify:prep as
</table>
<figureCaption confidence="0.93399325">
Figure 4: Attribute, head and trigger distributions
learned by the model HT+A for learned slots that
were mapped to BOMBING instrument and KID-
NAPPING victim.
</figureCaption>
<bodyText confidence="0.878176">
uated using references and were averaged across
ten runs.
</bodyText>
<subsectionHeader confidence="0.995585">
5.2 Experiment 1: Using Entity Attributes
</subsectionHeader>
<bodyText confidence="0.999950580645161">
In this experiment, two versions of our model are
compared: HT+A uses entity heads, event trigger
relations and entity attribute relations. HT uses
only entity heads and event triggers and omits at-
tributes.
We studied the gain brought by attribute re-
lations with a focus on their effect when coref-
erence information was available or was miss-
ing. The variations on the model input are named
single, multi and coref. Single input has only
one event trigger for each entity. A text like
an armed man attacked the police station and
killed a policeman results in two triples for the
entity man: (armed:amod, man, attack:nsubj) and
(armed:amod, man, kill:nsubj). In multi input, one
entity can have several event triggers, leading for
the text above to the triple (armed:amod, man, [at-
tack:nsubj, kill:nsubj]). The coref input is richer
than multi in that, in addition to triggers from the
same sentence, triggers linked to the same coref-
ered entity are merged together. For instance, if
man in the above example corefers with he in
He was arrested three hours later, the merged
triple becomes (armed:amod, man, [attack:nsubj,
kill:nsubj, arrest:dobj]). The plate notations of
these model+data combinations are given in Fig-
ure 5.
Table 1 shows a consistent improvement when
using attributes, both with and without corefer-
ences. The best performance of 40.62 F-score is
obtained by the full model on inputs with coref-
</bodyText>
<page confidence="0.981646">
192
</page>
<figure confidence="0.999946725">
(a)
a
h
t
A
s
#tuples
φ
π
θ
uni(1,K)
(c)
φ
π
t
h
s
#tuples
uni(1,K)
h
t
T
s
#tuples
(b)
φ
π
uni(1,K)
(d)
φ
π
θ
t
T
h
s
a
A
#tuples
uni(1,K)
</figure>
<figureCaption confidence="0.991763">
Figure 5: Model variants (Dirichlet priors are omitted for simplicity): 5a) HT model ran on single data.
This model is equivalent to 5b) with T=1; 5b) HT model ran on multi data; 5c) HT+A model ran on
single data; 5d) HT+A model ran on multi data.
</figureCaption>
<table confidence="0.9997754">
Data HT F P HT+A F
P R R
Single 29.59 51.17 37.48 30.22 52.41 38.33
Multi 29.32 52.21 37.52 30.82 51.68 38.55
Coref 39.99 53.53 40.01 32.42 54.59 40.62
</table>
<tableCaption confidence="0.999933">
Table 1: Improvement from using attributes.
</tableCaption>
<bodyText confidence="0.998680666666667">
erences. Using both attributes in the model and
coreference to generate input data results in a gain
of 3 F-score points.
</bodyText>
<subsectionHeader confidence="0.9655">
5.3 Experiment 2: Document Classification
</subsectionHeader>
<bodyText confidence="0.999973571428572">
In the second experiment, we evaluated our model
with a post-processing step of document classifi-
cation.
The MUC-4 corpus contains many “irrelevant”
documents. A document is irrelevant if it contains
no template. Among 1,300 documents in the de-
velopment set, 567 are irrelevant. The most chal-
lenging part is that there are many terrorist entities,
e.g. bomb, force, guerrilla, occurring in irrelevant
documents. That makes filtering out those docu-
ments important, but difficult. As document clas-
sification is not explicitly performed by our model,
a post-processing step is needed. Document clas-
sification is expected to reduce false positives in ir-
relevant documents while not dramatically reduc-
ing recall.
Given a document d with slot-assigned entities
and a set of mapped slots Sm resulting from slot
mapping, we have to decide whether this docu-
ment is relevant or not. We define the relevance
score of a document as:
</bodyText>
<equation confidence="0.8819045">
relevance(d) = EeEd:seESm EtETe P(t|se) (4)
EeEd EtETe P(t|se)
</equation>
<bodyText confidence="0.9996042">
where e is an entity in the document d; se is the
slot value assigned to e; and t is an event trigger in
the list of triggers Te.
The equation (4) defines the score of an entity as
the sum of the conditional probabilities of triggers
given a slot. The relevance score of the document
is proportional to the score of the entities assigned
to mapped slots. If this relevance score is higher
than a threshold A, then the document is consid-
ered as relevant. The value of A = 0.02 was tuned
</bodyText>
<page confidence="0.998717">
193
</page>
<table confidence="0.999771">
System P R F
HT+A 32.42 54.59 40.62
HT+A + doc. classification 35.57 53.89 42.79
HT+A + oracle classification 44.58 54.59 49.08
</table>
<tableCaption confidence="0.988289">
Table 2: Improvement from document classifica-
</tableCaption>
<bodyText confidence="0.992117222222222">
tion as post-processing.
on the development set by maximizing the F-score
of document classification.
Table 2 shows the improvement when applying
document classification. The precision increases
as false positives from irrelevant documents are fil-
tered out. The loss of recall comes from relevant
documents that are mistakenly filtered out. How-
ever, this loss is not significant and the overall F-
score finally increases by 5%. We also compare
our results to an “oracle” classifier that would re-
move all irrelevant documents while preserving all
relevant ones. The performance of this oracle clas-
sification shows that there are some room for fur-
ther improvement from document classification.
Irrelevant document filtering is a technique ap-
plied by most supervised and unsupervised ap-
proaches. Supervised methods prefer relevance
detection at sentence or phrase-level (Patwardhan
and Riloff, 2009; Patwardhan and Riloff, 2007).
As for several unsupervised methods, Chambers
(2013) includes document classification in his
topic model. Chambers and Jurafsky (2011) and
Cheung et al. (2013) use the learned clusters to
classify documents by estimating the relevance of
a document with respect to a template from post-
hoc statistics about event triggers.
</bodyText>
<subsectionHeader confidence="0.99949">
5.4 Comparison to State-of-the-Art
</subsectionHeader>
<bodyText confidence="0.872063333333333">
For comparing in more depth our results to the
state-of-the-art in the literature. we reimple-
mented the method proposed in Chambers (2013)
and integrated our attribute distributions into his
model (as shown in Figure 6).
The main differences between this model and
ours are the following:
1. The full template model of Chambers (2013)
adds a distribution ψ linking events to docu-
ments. This makes the model more complex
and maybe less intuitive since there is no rea-
son to connect documents and slots (a docu-
ment may contain references to several tem-
plates and slot mapping does not depend on
document level). A benefit of this document
</bodyText>
<table confidence="0.9985932">
System P R F
Cheung et al. (2013) 32 37 34
Chambers and Jurafsky (2011) 48 25 33
Chambers (2013) (paper values) 41 41 41
HT+A + doc. classification 36 54 43
</table>
<tableCaption confidence="0.9678395">
Table 3: Comparison to state-of-the-art unsuper-
vised systems.
</tableCaption>
<bodyText confidence="0.999185125">
distribution is that it leads to a free classifi-
cation of irrelevant documents, thus avoid-
ing a pre- or post-processing for classifica-
tion. However, this issue of document rel-
evance is very specific to the MUC corpus
and the evaluation method; In a more general
use case, there would be no “irrelevant” doc-
uments, only documents on various topics.
</bodyText>
<listItem confidence="0.947164347826087">
2. Each entity is linked to an event variable e.
This event generates a predicate for each
entity mention (recall that mentions of an
entity are all occurrences of this entity in
the documents, for example in a corefer-
ence chain). Our work instead focus on
the fact that a probabilistic model could
have multiple observations at the same po-
sition. Multiple triggers and multiple at-
tributes are treated equally. The sources
of multiple attributes and multiple triggers
are not only from document-level corefer-
ences but also from dependency relations (or
even from domain-level entity coreferences if
available). Hence, our model arguably gener-
alizes better in terms of both modeling and
input data.
3. Chambers (2013) applies a heuristic con-
straint during the sampling process, impos-
ing that subject and object of the same predi-
cate (e.g. kill:nsubj and kill:dobj) are not dis-
tributed into the same slot. Our model does
not require this heuristic.
</listItem>
<bodyText confidence="0.979339">
Some details concerning data preprocessing and
model parameters are not fully specified by Cham-
bers (2013); for this reason, our implementation
of the model (applied on the same data) leads
to slightly different results than those published.
That is why we present the two results here (pa-
per values in Table 3, reimplementation values in
</bodyText>
<tableCaption confidence="0.889388333333333">
Table 4).
Table 3 shows that our model outperforms the
others on recall by a large margin. It achieves the
</tableCaption>
<page confidence="0.983185">
194
</page>
<figure confidence="0.999936551724138">
(a)
τ
φ
π
p
h
t
M
s
e
#tuples
#docs
ψ
(b)
τ
φ
π
θ
a
p
h
t
A
M
s
e
#tuples
#docs
ψ
</figure>
<figureCaption confidence="0.9940475">
Figure 6: Variation of Chambers (2013) model: 6a) Original model; 6b) Original model + attribute
distributions.
</figureCaption>
<table confidence="0.962646333333333">
Chambers (2013) P R F
Original reimpl. 38.65 42.68 40.56
Original reimpl. + Attribute 39.25 43.68 41.31
</table>
<tableCaption confidence="0.9889915">
Table 4: Performance on reimplementation of
Chambers (2013).
</tableCaption>
<bodyText confidence="0.997119666666667">
best overall F-score. In addition, as stated by our
experiments, precision could be further improved
by more sophisticated document classification. In-
terestingly, using attributes also proves to be use-
ful in the model proposed by Chambers (2013) (as
shown in Table 4).
</bodyText>
<subsectionHeader confidence="0.927021">
5.5 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999994722222222">
We performed an error analysis on the output of
HT+A + doc. classification to detect the origin
of false positives (FPs). 38% of FPs are mentions
that never occur in the reference. Within this 38%,
attacker and killer are among the most frequent er-
rors. These words could refer to a perpetrator of an
attack. These mentions, however, do not occur in
the reference, possibly because human annotators
consider them as too generic terms. Apart from
such generic terms, other assignments are obvious
errors of the system, e.g. window, door or wall as
physical target; action or massacre as perpetrator;
explosion or shooting as instrument. These kinds
of errors are due to the fact that in our model, as in
the one of Chambers (2013), the number of slots
is fixed and is not equivalent to the real number of
reference slots.
On the other hand, 62% of FPs are mentions of
entities that occur at least once in the reference.
On top of the list are perpetrators such as guer-
rilla, group and rebel. The model is capable of as-
signing guerrilla to attribution slot if it is accom-
panied by a trigger like announce:nsubj. How-
ever, triggers that describe quasi-terrorism events
(e.g. menace, threatening, military conflict) are
also grouped into perpetrator slots. Similarly,
mentions of frequent words such as bomb (instru-
ment), building, house, office (targets) tend to be
systematically grouped into these slots, regardless
of their relations. Increasing the number of slots
(to sharpen their content) does not help overall.
This is due to the fact that the MUC corpus is
very small and is biased towards terrorism events.
Adding a higher level of template type as in Cham-
bers (2013) partially solves the problem but makes
recall decrease (as shown in Table 3).
</bodyText>
<sectionHeader confidence="0.994289" genericHeader="conclusions">
6 Conclusions and Perspectives
</sectionHeader>
<bodyText confidence="0.999955384615385">
We presented a generative model for representing
the roles played by the entities in an event tem-
plate. We focused on using immediate contexts of
entities and proposed a simpler and more effective
model than those proposed in previous work. We
evaluated this model on the MUC-4 corpus.
Even if our results outperform other unsuper-
vised approaches, we are still far from results ob-
tained by supervised systems. Improvements can
be obtained by several ways. First, the character-
istics of the MUC-4 corpus are a limiting factor.
The corpus is small and roles are similar from a
template to another, which does not reflect reality.
</bodyText>
<page confidence="0.996884">
195
</page>
<bodyText confidence="0.999896">
A bigger corpus, even partially annotated but pre-
senting a better variety of templates, could lead to
very different approaches.
As we showed, our model comes with a unified
representation of all types of relations. This opens
the way to the use of multiple types of relations
(syntactic, semantic, thematic, etc.) to refine the
clusters.
Last but not least, the evaluation protocol, that
became a kind of de facto standard, is very much
imperfect. Most notably, the way of finally map-
ping with reference slots can have a great influence
on the results.
</bodyText>
<sectionHeader confidence="0.975451" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.9994805">
This work was partially financed by the Foun-
dation for Scientific Cooperation “Campus Paris-
Saclay” (FSC) under the project Digiteo ASTRE
No. 2013-0774D.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999665847058823">
Niranjan Balasubramanian, Stephen Soderland,
Mausam, and Oren Etzioni. 2013. Generating
Coherent Event Schemas at Scale. In 2013 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP 2013), pages 1721–1731,
Seattle, Washington, USA, October.
Cosmin Adrian Bejan. 2008. Unsupervised Discovery
of Event Scenarios from Texts. In Twenty-First In-
ternational Florida Artificial Intelligence Research
Society Conference (FLAIRS 2008), pages 124–129,
Coconut Grove, Florida.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993–1022, March.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised Learning of Narrative Event Chains. In
ACL-08: HLT, pages 789–797, Columbus, Ohio,
June.
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised Learning of Narrative Schemas and their
Participants. In Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP (ACL-IJCNLP’09), pages 602–610, Suntec,
Singapore, August.
Nathanael Chambers and Dan Jurafsky. 2011.
Template-Based Information Extraction without the
Templates. In 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (ACL 2011), pages 976–986,
Portland, Oregon, USA, June.
Nathanael Chambers. 2013. Event Schema Induction
with a Probabilistic Entity-Driven Model. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1797–
1807, Seattle, Washington, USA, October.
Kit Jackie Chi Cheung, Hoifung Poon, and Lucy Van-
derwende. 2013. Probabilistic Frame Induction. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 837–846.
R. Collier. 1998. Automatic Template Creation for
Information Extraction. Ph.D. thesis, University of
Sheffield.
Gerald DeJong. 1982. An overview of the FRUMP
system. In W. Lehnert and M. Ringle, editors,
Strategies for natural language processing, pages
149–176. Lawrence Erlbaum Associates.
Kathrin Eichler, Holmer Hemsen, and G¨unter Neu-
mann. 2008. Unsupervised Relation Extraction
From Web Documents. In 6th Conference on Lan-
guage Resources and Evaluation (LREC’08), Mar-
rakech, Morocco.
Olivier Ferret and Brigitte Grau. 1997. An Aggre-
gation Procedure for Building Episodic Memory.
In 15th International Joint Conference on Artificial
Intelligence (IJCAI-97), pages 280–285, Nagoya,
Japan.
Elena Filatova, Vasileios Hatzivassiloglou, and Kath-
leen McKeown. 2006. Automatic Creation of
Domain Templates. In 21st International Confer-
ence on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Lin-
guistics (COLING-ACL 2006), pages 207–214, Syd-
ney, Australia.
Elena Filatova. 2008. Unsupervised Relation Learning
for Event-Focused Question-Answering and Domain
Modelling. Ph.D. thesis, Columbia University.
Marjorie Freedman, Lance Ramshaw, Elizabeth
Boschee, Ryan Gabbard, Gary Kratkiewicz, Nico-
las Ward, and Ralph Weischedel. 2011. Ex-
treme Extraction – Machine Reading in a Week. In
2011 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2011), pages 1437–
1446, Edinburgh, Scotland, UK., July.
Lea Frermann, Ivan Titov, and Manfred Pinkal. 2014.
A Hierarchical Bayesian Model for Unsupervised
Induction of Script Knowledge. In 14th Conference
of the European Chapter of the Association for Com-
putational Linguistics (EACL 2014), pages 49–57,
Gothenburg, Sweden, April.
Tom Griffiths. 2002. Gibbs sampling in the genera-
tive model of Latent Dirichlet Allocation. Technical
report, Stanford University.
</reference>
<page confidence="0.98717">
196
</page>
<reference confidence="0.999885336842105">
Ralph Grishman and Yifan He. 2014. An Informa-
tion Extraction Customizer. In Petr Sojka, Ale Hork,
Ivan Kopeek, and Karel Pala, editors, 17th Inter-
national Conference on Text, Speech and Dialogue
(TSD 2014), volume 8655 of Lecture Notes in Com-
puter Science, pages 3–10. Springer International
Publishing.
Ralph Grishman and Beth Sundheim. 1996. Mes-
sage Understanding Conference-6: A Brief History.
In 16th International Conference on Computational
linguistics (COLING’96), pages 466–471, Copen-
hagen, Denmark.
Sanda Harabagiu. 2004. Incremental Topic Repre-
sentation. In Proceedings of the 20th International
Conference on Computational Linguistics (COL-
ING’04), Geneva, Switzerland, August.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grish-
man. 2004. Discovering Relations among Named
Entities from Large Corpora. In 42nd Meeting
of the Association for Computational Linguistics
(ACL’04), pages 415–422, Barcelona, Spain.
Ludovic Jean-Louis, Romaric Besanon, and Olivier
Ferret. 2011. Text Segmentation and Graph-based
Method for Template Filling in Information Extrac-
tion. In 5th International Joint Conference on Nat-
ural Language Processing (IJCNLP 2011), pages
723–731, Chiang Mai, Thailand.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP Natural Lan-
guage Processing Toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations, pages
55–60, Baltimore, USA, jun.
Bonan Min, Shuming Shi, Ralph Grishman, and Chin-
Yew Lin. 2012. Ensemble Semantics for Large-
scale Unsupervised Relation Extraction. In 2012
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL 2012, pages
1027–1037, Jeju Island, Korea.
Ashutosh Modi and Ivan Titov. 2014. Inducing neural
models of script knowledge. In Eighteenth Confer-
ence on Computational Natural Language Learning
(CoNLL 2014), pages 49–57, Ann Arbor, Michigan.
Siddharth Patwardhan and Ellen Riloff. 2007. Ef-
fective Information Extraction with Semantic Affin-
ity Patterns and Relevant Regions. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP-CoNLL
2007), pages 717–727, Prague, Czech Republic,
June.
Siddharth Patwardhan and Ellen Riloff. 2009. A Uni-
fied Model of Phrasal and Sentential Evidence for
Information Extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2009), pages 151–160.
Karl Pichotta and Raymond Mooney. 2014. Statistical
script learning with multi-argument events. In 14th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics (EACL 2014),
pages 220–229, Gothenburg, Sweden.
Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2008.
Modeling Context in Scenario Template Creation.
In Third International Joint Conference on Natural
Language Processing (IJCNLP 2008), pages 157–
164, Hyderabad, India.
Michaela Regneri, Alexander Koller, and Manfred
Pinkal. 2010. Learning Script Knowledge with Web
Experiments. In 48th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2010),
pages 979–988, Uppsala, Sweden, July.
Benjamin Rosenfeld and Ronen Feldman. 2007. Clus-
tering for unsupervised relation identification. In
Sixteenth ACM conference on Conference on in-
formation and knowledge management (CIKM’07),
pages 411–418, Lisbon, Portugal.
Roger C. Schank. 1980. Language and memory. Cog-
nitive Science, 4:243–284.
Satoshi Sekine. 2006. On-demand information
extraction. In 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics (COLING-ACL 2006), pages 731–738, Sydney,
Australia.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive Information Extraction using Unrestricted Rela-
tion Discovery. In HLT-NAACL 2006, pages 304–
311, New York City, USA.
Beth M. Sundheim. 1991. Third Message Understand-
ing Evaluation and Conference (MUC-3): Phase 1
Status Report. In Proceedings of the Workshop on
Speech and Natural Language, HLT ’91, pages 301–
305.
</reference>
<page confidence="0.998177">
197
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.588248">
<title confidence="0.98425">Generative Event Schema Induction with Entity Disambiguation</title>
<note confidence="0.8634795">2Xavier Olivier Romaric (1) (2) CEA, LIST, Laboratoire Vision et Ingnierie des Contenus, F-91191, (3) Univ.</note>
<abstract confidence="0.998919588235294">This paper presents a generative model to event schema induction. Previous methods in the literature only use head words to represent entities. However, elements other than head words contain useful in- For instance, armed man more discriminative than Our model takes into account this information and precisely represents it using probabilistic topic distributions. We illustrate that such information plays an important role in parameter estimation. Mostly, it makes topic distributions more coherent and more discriminative. Experimental results on benchmark dataset empirically confirm this enhancement.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Niranjan Balasubramanian</author>
<author>Stephen Soderland</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Generating Coherent Event Schemas at Scale.</title>
<date>2013</date>
<booktitle>In 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013),</booktitle>
<pages>1721--1731</pages>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="6150" citStr="Balasubramanian et al. (2013)" startWordPosition="961" endWordPosition="964">tion extraction in practice (Eichler et al., 2008; Rosenfeld and Feldman, 2007; Min et al., 2012). In parallel, several approaches were proposed for performing specifically schema induction in already existing frameworks: clause graph clustering (Qiu et al., 2008), event sequence alignment (Regneri et al., 2010) or LDA-based approach relying on FrameNet-like semantic frames (Bejan, 2008). More event-specific generative models were proposed by Chambers (2013) and Cheung et al. (2013). Finally, Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Chambers and Jurafsky (2011), improved by Balasubramanian et al. (2013), and Chambers (2013) focused specifically on the induction of event roles and the identification of chains of events for building representations from texts by exploiting coreference resolution or the temporal ordering of events. All this work is also linked to work about the induction of scripts from texts, more or less closely linked to Attributes Head Triggers #1 [armed:amod] man [attack:nsubj, kill:nsubj] #2 [police:nn] station [attack:dobj] #3 [] policeman [kill:dobj] #4 [innocent:amod, man [wound:dobj] young:amod] Figure 1: Entity representation as tuples of ([attributes], head, [trigge</context>
</contexts>
<marker>Balasubramanian, Soderland, Mausam, Etzioni, 2013</marker>
<rawString>Niranjan Balasubramanian, Stephen Soderland, Mausam, and Oren Etzioni. 2013. Generating Coherent Event Schemas at Scale. In 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013), pages 1721–1731, Seattle, Washington, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cosmin Adrian Bejan</author>
</authors>
<title>Unsupervised Discovery of Event Scenarios from Texts.</title>
<date>2008</date>
<booktitle>In Twenty-First International Florida Artificial Intelligence Research Society Conference (FLAIRS</booktitle>
<pages>124--129</pages>
<location>Coconut Grove, Florida.</location>
<contexts>
<context position="5911" citStr="Bejan, 2008" startWordPosition="928" endWordPosition="929">2008). More recently, work after (Hasegawa et al., 2004) has developed weakly supervised forms of Information Extraction including schema induction in their objectives. However, they have been mainly applied to binary relation extraction in practice (Eichler et al., 2008; Rosenfeld and Feldman, 2007; Min et al., 2012). In parallel, several approaches were proposed for performing specifically schema induction in already existing frameworks: clause graph clustering (Qiu et al., 2008), event sequence alignment (Regneri et al., 2010) or LDA-based approach relying on FrameNet-like semantic frames (Bejan, 2008). More event-specific generative models were proposed by Chambers (2013) and Cheung et al. (2013). Finally, Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Chambers and Jurafsky (2011), improved by Balasubramanian et al. (2013), and Chambers (2013) focused specifically on the induction of event roles and the identification of chains of events for building representations from texts by exploiting coreference resolution or the temporal ordering of events. All this work is also linked to work about the induction of scripts from texts, more or less closely linked to Attributes Head Tri</context>
</contexts>
<marker>Bejan, 2008</marker>
<rawString>Cosmin Adrian Bejan. 2008. Unsupervised Discovery of Event Scenarios from Texts. In Twenty-First International Florida Artificial Intelligence Research Society Conference (FLAIRS 2008), pages 124–129, Coconut Grove, Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="10859" citStr="Blei et al., 2003" startWordPosition="1763" endWordPosition="1766">rm distribution uni(1, K); Generate a head h from a multinominal distribution irs; for i +— 1 to JTeJ do Generate a trigger ti from a multinominal distribution φs; end forj+— 1 to JAeJ do Generate an attribute aj from a multinominal distribution φs; end end 4.2 Parameter Estimation For parameter estimation, we use the Gibbs sampling method (Griffiths, 2002). The slot variable s is sampled by integrating out all the other variables. Previous models (Cheung et al., 2013; Chambers, 2013) are based on document-level topic modeling, which originated from models such as Latent Dirichlet Allocation (Blei et al., 2003). Our model is, instead, independent from document contexts. Its input is a sequence of entity triples. Document boundary is only used in a postprocessing step of filtering (see Section 5.3 for more details). There is a universal slot distribution instead of each slot distribution for one document. Furthermore, slot prior is ignored by using a uniform distribution as a particular case of categorical probability. Sampling-based slot assignment could depend on initial states and random seeds. In our implementation of Gibbs sampling, we use 2,000 burn-in of overall 10,000 iterations. The purpose </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised Learning of Narrative Event Chains.</title>
<date>2008</date>
<booktitle>In ACL-08: HLT,</booktitle>
<pages>789--797</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="6047" citStr="Chambers and Jurafsky (2008)" startWordPosition="946" endWordPosition="949"> including schema induction in their objectives. However, they have been mainly applied to binary relation extraction in practice (Eichler et al., 2008; Rosenfeld and Feldman, 2007; Min et al., 2012). In parallel, several approaches were proposed for performing specifically schema induction in already existing frameworks: clause graph clustering (Qiu et al., 2008), event sequence alignment (Regneri et al., 2010) or LDA-based approach relying on FrameNet-like semantic frames (Bejan, 2008). More event-specific generative models were proposed by Chambers (2013) and Cheung et al. (2013). Finally, Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Chambers and Jurafsky (2011), improved by Balasubramanian et al. (2013), and Chambers (2013) focused specifically on the induction of event roles and the identification of chains of events for building representations from texts by exploiting coreference resolution or the temporal ordering of events. All this work is also linked to work about the induction of scripts from texts, more or less closely linked to Attributes Head Triggers #1 [armed:amod] man [attack:nsubj, kill:nsubj] #2 [police:nn] station [attack:dobj] #3 [] policeman [kill:dobj] #4 [innocent:amod,</context>
</contexts>
<marker>Chambers, Jurafsky, 2008</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2008. Unsupervised Learning of Narrative Event Chains. In ACL-08: HLT, pages 789–797, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised Learning of Narrative Schemas and their Participants.</title>
<date>2009</date>
<booktitle>In Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP’09),</booktitle>
<pages>602--610</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="6077" citStr="Chambers and Jurafsky (2009)" startWordPosition="950" endWordPosition="953"> their objectives. However, they have been mainly applied to binary relation extraction in practice (Eichler et al., 2008; Rosenfeld and Feldman, 2007; Min et al., 2012). In parallel, several approaches were proposed for performing specifically schema induction in already existing frameworks: clause graph clustering (Qiu et al., 2008), event sequence alignment (Regneri et al., 2010) or LDA-based approach relying on FrameNet-like semantic frames (Bejan, 2008). More event-specific generative models were proposed by Chambers (2013) and Cheung et al. (2013). Finally, Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Chambers and Jurafsky (2011), improved by Balasubramanian et al. (2013), and Chambers (2013) focused specifically on the induction of event roles and the identification of chains of events for building representations from texts by exploiting coreference resolution or the temporal ordering of events. All this work is also linked to work about the induction of scripts from texts, more or less closely linked to Attributes Head Triggers #1 [armed:amod] man [attack:nsubj, kill:nsubj] #2 [police:nn] station [attack:dobj] #3 [] policeman [kill:dobj] #4 [innocent:amod, man [wound:dobj] young:amod] </context>
</contexts>
<marker>Chambers, Jurafsky, 2009</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2009. Unsupervised Learning of Narrative Schemas and their Participants. In Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP’09), pages 602–610, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Template-Based Information Extraction without the Templates.</title>
<date>2011</date>
<booktitle>In 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL</booktitle>
<pages>976--986</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="6107" citStr="Chambers and Jurafsky (2011)" startWordPosition="954" endWordPosition="958">ey have been mainly applied to binary relation extraction in practice (Eichler et al., 2008; Rosenfeld and Feldman, 2007; Min et al., 2012). In parallel, several approaches were proposed for performing specifically schema induction in already existing frameworks: clause graph clustering (Qiu et al., 2008), event sequence alignment (Regneri et al., 2010) or LDA-based approach relying on FrameNet-like semantic frames (Bejan, 2008). More event-specific generative models were proposed by Chambers (2013) and Cheung et al. (2013). Finally, Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Chambers and Jurafsky (2011), improved by Balasubramanian et al. (2013), and Chambers (2013) focused specifically on the induction of event roles and the identification of chains of events for building representations from texts by exploiting coreference resolution or the temporal ordering of events. All this work is also linked to work about the induction of scripts from texts, more or less closely linked to Attributes Head Triggers #1 [armed:amod] man [attack:nsubj, kill:nsubj] #2 [police:nn] station [attack:dobj] #3 [] policeman [kill:dobj] #4 [innocent:amod, man [wound:dobj] young:amod] Figure 1: Entity representatio</context>
<context position="8798" citStr="Chambers and Jurafsky, 2011" startWordPosition="1404" endWordPosition="1407">ubject, object, or preposition. We use the categories noun.EVENT and noun.ACT in WordNet as a list of eventive nouns. A head word can have more than one trigger. These multiple relations can come from a syntactic coordination inside a single sentence, as it is the case in the first sentence of the illustrating example. They can also represent a coreference 189 Figure 2: Generative model for event induction. chain across sentences, as we use coreference resolution to merge the triggers of mentions corefering to the same entity in a document. Coreferences are useful sources for event induction (Chambers and Jurafsky, 2011; Chambers, 2013). Finally, an attribute is extracted if it is an adjective, a noun or a verb and serves as an adjective, verbal or nominal modifier of a head word. If there are several modifiers, only the closest to the head word is selected. This “best selection” heuristic allows to omit non-discriminative attributes for the entity. 4 Generative Model 4.1 Model Description Figure 2 shows the plate notation of our model. For each triple representing an entity e, the model first assigns a slot s for the entity from an uniform distribution uni(1, K). Its head word h is then generated from a mul</context>
<context position="14994" citStr="Chambers and Jurafsky, 2011" startWordPosition="2412" endWordPosition="2415"> previous approaches, more particularly with Chambers (2013), from both quantitative and qualitative points of view (Section 5.4). Finally, Section 5.5 is dedicated to error analysis, with a special emphasis on sources of false positives. 5.1 Experimental Setups 5.1.1 Datasets The MUC-4 corpus contains 1,700 news articles about terrorist incidents happening in Latin America. The corpus is divided into 1,300 documents 191 for the development set and four test sets, each containing 100 documents. We follow the rules in the literature to guarantee comparable results (Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). The evaluation focuses on four template types – ARSON, ATTACK, BOMBING, KIDNAPPING – and four slots – Perpetrator, Instrument, Target, and Victim. Perpetrator is merged from Perpetrator Individual and Perpetrator Organization. The matching between system answers and references is based on head word matching. A head word is defined as the rightmost word of the phrase or as the right-most word of the first ‘of’ if the phrase contains any. Optional templates and slots are ignored when calculating recall. Template types are ignored in evaluation: this means that a perpetrator of BOMBING in the a</context>
<context position="22562" citStr="Chambers and Jurafsky (2011)" startWordPosition="3656" endWordPosition="3659">so compare our results to an “oracle” classifier that would remove all irrelevant documents while preserving all relevant ones. The performance of this oracle classification shows that there are some room for further improvement from document classification. Irrelevant document filtering is a technique applied by most supervised and unsupervised approaches. Supervised methods prefer relevance detection at sentence or phrase-level (Patwardhan and Riloff, 2009; Patwardhan and Riloff, 2007). As for several unsupervised methods, Chambers (2013) includes document classification in his topic model. Chambers and Jurafsky (2011) and Cheung et al. (2013) use the learned clusters to classify documents by estimating the relevance of a document with respect to a template from posthoc statistics about event triggers. 5.4 Comparison to State-of-the-Art For comparing in more depth our results to the state-of-the-art in the literature. we reimplemented the method proposed in Chambers (2013) and integrated our attribute distributions into his model (as shown in Figure 6). The main differences between this model and ours are the following: 1. The full template model of Chambers (2013) adds a distribution ψ linking events to do</context>
</contexts>
<marker>Chambers, Jurafsky, 2011</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2011. Template-Based Information Extraction without the Templates. In 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL 2011), pages 976–986, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
</authors>
<title>Event Schema Induction with a Probabilistic Entity-Driven Model.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1797--1807</pages>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="5983" citStr="Chambers (2013)" startWordPosition="938" endWordPosition="939">d weakly supervised forms of Information Extraction including schema induction in their objectives. However, they have been mainly applied to binary relation extraction in practice (Eichler et al., 2008; Rosenfeld and Feldman, 2007; Min et al., 2012). In parallel, several approaches were proposed for performing specifically schema induction in already existing frameworks: clause graph clustering (Qiu et al., 2008), event sequence alignment (Regneri et al., 2010) or LDA-based approach relying on FrameNet-like semantic frames (Bejan, 2008). More event-specific generative models were proposed by Chambers (2013) and Cheung et al. (2013). Finally, Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Chambers and Jurafsky (2011), improved by Balasubramanian et al. (2013), and Chambers (2013) focused specifically on the induction of event roles and the identification of chains of events for building representations from texts by exploiting coreference resolution or the temporal ordering of events. All this work is also linked to work about the induction of scripts from texts, more or less closely linked to Attributes Head Triggers #1 [armed:amod] man [attack:nsubj, kill:nsubj] #2 [police:nn] stat</context>
<context position="8815" citStr="Chambers, 2013" startWordPosition="1408" endWordPosition="1409">n. We use the categories noun.EVENT and noun.ACT in WordNet as a list of eventive nouns. A head word can have more than one trigger. These multiple relations can come from a syntactic coordination inside a single sentence, as it is the case in the first sentence of the illustrating example. They can also represent a coreference 189 Figure 2: Generative model for event induction. chain across sentences, as we use coreference resolution to merge the triggers of mentions corefering to the same entity in a document. Coreferences are useful sources for event induction (Chambers and Jurafsky, 2011; Chambers, 2013). Finally, an attribute is extracted if it is an adjective, a noun or a verb and serves as an adjective, verbal or nominal modifier of a head word. If there are several modifiers, only the closest to the head word is selected. This “best selection” heuristic allows to omit non-discriminative attributes for the entity. 4 Generative Model 4.1 Model Description Figure 2 shows the plate notation of our model. For each triple representing an entity e, the model first assigns a slot s for the entity from an uniform distribution uni(1, K). Its head word h is then generated from a multinominal distrib</context>
<context position="10730" citStr="Chambers, 2013" startWordPosition="1745" endWordPosition="1747">(Q); Generate a trigger distribution φs from a Dirichlet prior dir(ry); end for entity e E E do Generate a slot s from a uniform distribution uni(1, K); Generate a head h from a multinominal distribution irs; for i +— 1 to JTeJ do Generate a trigger ti from a multinominal distribution φs; end forj+— 1 to JAeJ do Generate an attribute aj from a multinominal distribution φs; end end 4.2 Parameter Estimation For parameter estimation, we use the Gibbs sampling method (Griffiths, 2002). The slot variable s is sampled by integrating out all the other variables. Previous models (Cheung et al., 2013; Chambers, 2013) are based on document-level topic modeling, which originated from models such as Latent Dirichlet Allocation (Blei et al., 2003). Our model is, instead, independent from document contexts. Its input is a sequence of entity triples. Document boundary is only used in a postprocessing step of filtering (see Section 5.3 for more details). There is a universal slot distribution instead of each slot distribution for one document. Furthermore, slot prior is ignored by using a uniform distribution as a particular case of categorical probability. Sampling-based slot assignment could depend on initial </context>
<context position="14426" citStr="Chambers (2013)" startWordPosition="2328" endWordPosition="2329">nce (MUC-4) corpus (Sundheim, 1991) using precision, recall and F-score as conventional metrics for template extraction. In what follows, we first introduce the MUC4 corpus (Section 5.1.1), we detail the mapping technique between learned slots and reference slots (5.1.2) as well as the hyper-parameters of our model (5.1.3). Next, we present a first experiment (Section 5.2) showing how using attribute relations improves overall results. The second experiment (Section 5.3) studies the impact of document classification. We then compare our results with previous approaches, more particularly with Chambers (2013), from both quantitative and qualitative points of view (Section 5.4). Finally, Section 5.5 is dedicated to error analysis, with a special emphasis on sources of false positives. 5.1 Experimental Setups 5.1.1 Datasets The MUC-4 corpus contains 1,700 news articles about terrorist incidents happening in Latin America. The corpus is divided into 1,300 documents 191 for the development set and four test sets, each containing 100 documents. We follow the rules in the literature to guarantee comparable results (Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). The evaluation focuses on four</context>
<context position="22480" citStr="Chambers (2013)" startWordPosition="3647" endWordPosition="3648">not significant and the overall Fscore finally increases by 5%. We also compare our results to an “oracle” classifier that would remove all irrelevant documents while preserving all relevant ones. The performance of this oracle classification shows that there are some room for further improvement from document classification. Irrelevant document filtering is a technique applied by most supervised and unsupervised approaches. Supervised methods prefer relevance detection at sentence or phrase-level (Patwardhan and Riloff, 2009; Patwardhan and Riloff, 2007). As for several unsupervised methods, Chambers (2013) includes document classification in his topic model. Chambers and Jurafsky (2011) and Cheung et al. (2013) use the learned clusters to classify documents by estimating the relevance of a document with respect to a template from posthoc statistics about event triggers. 5.4 Comparison to State-of-the-Art For comparing in more depth our results to the state-of-the-art in the literature. we reimplemented the method proposed in Chambers (2013) and integrated our attribute distributions into his model (as shown in Figure 6). The main differences between this model and ours are the following: 1. The</context>
<context position="24705" citStr="Chambers (2013)" startWordPosition="4016" endWordPosition="4017">tion (recall that mentions of an entity are all occurrences of this entity in the documents, for example in a coreference chain). Our work instead focus on the fact that a probabilistic model could have multiple observations at the same position. Multiple triggers and multiple attributes are treated equally. The sources of multiple attributes and multiple triggers are not only from document-level coreferences but also from dependency relations (or even from domain-level entity coreferences if available). Hence, our model arguably generalizes better in terms of both modeling and input data. 3. Chambers (2013) applies a heuristic constraint during the sampling process, imposing that subject and object of the same predicate (e.g. kill:nsubj and kill:dobj) are not distributed into the same slot. Our model does not require this heuristic. Some details concerning data preprocessing and model parameters are not fully specified by Chambers (2013); for this reason, our implementation of the model (applied on the same data) leads to slightly different results than those published. That is why we present the two results here (paper values in Table 3, reimplementation values in Table 4). Table 3 shows that o</context>
<context position="25990" citStr="Chambers (2013)" startWordPosition="4238" endWordPosition="4239">es the 194 (a) τ φ π p h t M s e #tuples #docs ψ (b) τ φ π θ a p h t A M s e #tuples #docs ψ Figure 6: Variation of Chambers (2013) model: 6a) Original model; 6b) Original model + attribute distributions. Chambers (2013) P R F Original reimpl. 38.65 42.68 40.56 Original reimpl. + Attribute 39.25 43.68 41.31 Table 4: Performance on reimplementation of Chambers (2013). best overall F-score. In addition, as stated by our experiments, precision could be further improved by more sophisticated document classification. Interestingly, using attributes also proves to be useful in the model proposed by Chambers (2013) (as shown in Table 4). 5.5 Error Analysis We performed an error analysis on the output of HT+A + doc. classification to detect the origin of false positives (FPs). 38% of FPs are mentions that never occur in the reference. Within this 38%, attacker and killer are among the most frequent errors. These words could refer to a perpetrator of an attack. These mentions, however, do not occur in the reference, possibly because human annotators consider them as too generic terms. Apart from such generic terms, other assignments are obvious errors of the system, e.g. window, door or wall as physical t</context>
<context position="27706" citStr="Chambers (2013)" startWordPosition="4529" endWordPosition="4531">it is accompanied by a trigger like announce:nsubj. However, triggers that describe quasi-terrorism events (e.g. menace, threatening, military conflict) are also grouped into perpetrator slots. Similarly, mentions of frequent words such as bomb (instrument), building, house, office (targets) tend to be systematically grouped into these slots, regardless of their relations. Increasing the number of slots (to sharpen their content) does not help overall. This is due to the fact that the MUC corpus is very small and is biased towards terrorism events. Adding a higher level of template type as in Chambers (2013) partially solves the problem but makes recall decrease (as shown in Table 3). 6 Conclusions and Perspectives We presented a generative model for representing the roles played by the entities in an event template. We focused on using immediate contexts of entities and proposed a simpler and more effective model than those proposed in previous work. We evaluated this model on the MUC-4 corpus. Even if our results outperform other unsupervised approaches, we are still far from results obtained by supervised systems. Improvements can be obtained by several ways. First, the characteristics of the </context>
</contexts>
<marker>Chambers, 2013</marker>
<rawString>Nathanael Chambers. 2013. Event Schema Induction with a Probabilistic Entity-Driven Model. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1797– 1807, Seattle, Washington, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kit Jackie Chi Cheung</author>
<author>Hoifung Poon</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Probabilistic Frame Induction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>837--846</pages>
<contexts>
<context position="6008" citStr="Cheung et al. (2013)" startWordPosition="941" endWordPosition="944">forms of Information Extraction including schema induction in their objectives. However, they have been mainly applied to binary relation extraction in practice (Eichler et al., 2008; Rosenfeld and Feldman, 2007; Min et al., 2012). In parallel, several approaches were proposed for performing specifically schema induction in already existing frameworks: clause graph clustering (Qiu et al., 2008), event sequence alignment (Regneri et al., 2010) or LDA-based approach relying on FrameNet-like semantic frames (Bejan, 2008). More event-specific generative models were proposed by Chambers (2013) and Cheung et al. (2013). Finally, Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Chambers and Jurafsky (2011), improved by Balasubramanian et al. (2013), and Chambers (2013) focused specifically on the induction of event roles and the identification of chains of events for building representations from texts by exploiting coreference resolution or the temporal ordering of events. All this work is also linked to work about the induction of scripts from texts, more or less closely linked to Attributes Head Triggers #1 [armed:amod] man [attack:nsubj, kill:nsubj] #2 [police:nn] station [attack:dobj] #3 [] p</context>
<context position="10713" citStr="Cheung et al., 2013" startWordPosition="1741" endWordPosition="1744">a Dirichlet prior dir(Q); Generate a trigger distribution φs from a Dirichlet prior dir(ry); end for entity e E E do Generate a slot s from a uniform distribution uni(1, K); Generate a head h from a multinominal distribution irs; for i +— 1 to JTeJ do Generate a trigger ti from a multinominal distribution φs; end forj+— 1 to JAeJ do Generate an attribute aj from a multinominal distribution φs; end end 4.2 Parameter Estimation For parameter estimation, we use the Gibbs sampling method (Griffiths, 2002). The slot variable s is sampled by integrating out all the other variables. Previous models (Cheung et al., 2013; Chambers, 2013) are based on document-level topic modeling, which originated from models such as Latent Dirichlet Allocation (Blei et al., 2003). Our model is, instead, independent from document contexts. Its input is a sequence of entity triples. Document boundary is only used in a postprocessing step of filtering (see Section 5.3 for more details). There is a universal slot distribution instead of each slot distribution for one document. Furthermore, slot prior is ignored by using a uniform distribution as a particular case of categorical probability. Sampling-based slot assignment could d</context>
<context position="22587" citStr="Cheung et al. (2013)" startWordPosition="3661" endWordPosition="3664">cle” classifier that would remove all irrelevant documents while preserving all relevant ones. The performance of this oracle classification shows that there are some room for further improvement from document classification. Irrelevant document filtering is a technique applied by most supervised and unsupervised approaches. Supervised methods prefer relevance detection at sentence or phrase-level (Patwardhan and Riloff, 2009; Patwardhan and Riloff, 2007). As for several unsupervised methods, Chambers (2013) includes document classification in his topic model. Chambers and Jurafsky (2011) and Cheung et al. (2013) use the learned clusters to classify documents by estimating the relevance of a document with respect to a template from posthoc statistics about event triggers. 5.4 Comparison to State-of-the-Art For comparing in more depth our results to the state-of-the-art in the literature. we reimplemented the method proposed in Chambers (2013) and integrated our attribute distributions into his model (as shown in Figure 6). The main differences between this model and ours are the following: 1. The full template model of Chambers (2013) adds a distribution ψ linking events to documents. This makes the m</context>
</contexts>
<marker>Cheung, Poon, Vanderwende, 2013</marker>
<rawString>Kit Jackie Chi Cheung, Hoifung Poon, and Lucy Vanderwende. 2013. Probabilistic Frame Induction. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 837–846.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collier</author>
</authors>
<title>Automatic Template Creation for Information Extraction.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Sheffield.</institution>
<contexts>
<context position="5193" citStr="Collier, 1998" startWordPosition="824" endWordPosition="825">a et al., 2004; Sekine, 2006) and Preemptive Information Extraction (Shinyama and Sekine, 2006) tried to overcome this difficulty in another way by exploiting templates induced from representative documents selected by queries. Event schema induction takes root in work on the acquisition from text of knowledge structures, such as the Memory Organization Packets (Schank, 1980), used by early text understanding systems (DeJong, 1982) and more recently by Ferret and Grau (1997). First attempts for applying such processes to schema induction have been made in the fields of Information Extraction (Collier, 1998), Automatic Summarization (Harabagiu, 2004) and event QuestionAnswering (Filatova et al., 2006; Filatova, 2008). More recently, work after (Hasegawa et al., 2004) has developed weakly supervised forms of Information Extraction including schema induction in their objectives. However, they have been mainly applied to binary relation extraction in practice (Eichler et al., 2008; Rosenfeld and Feldman, 2007; Min et al., 2012). In parallel, several approaches were proposed for performing specifically schema induction in already existing frameworks: clause graph clustering (Qiu et al., 2008), event </context>
</contexts>
<marker>Collier, 1998</marker>
<rawString>R. Collier. 1998. Automatic Template Creation for Information Extraction. Ph.D. thesis, University of Sheffield.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald DeJong</author>
</authors>
<title>An overview of the FRUMP system.</title>
<date>1982</date>
<booktitle>Strategies for natural language processing,</booktitle>
<pages>149--176</pages>
<editor>In W. Lehnert and M. Ringle, editors,</editor>
<contexts>
<context position="5014" citStr="DeJong, 1982" startWordPosition="793" endWordPosition="794">xamples (Grishman and He, 2014) is a way to reduce the amount of effort needed for adapting a system to another domain. The approaches of Ondemand information extraction (Hasegawa et al., 2004; Sekine, 2006) and Preemptive Information Extraction (Shinyama and Sekine, 2006) tried to overcome this difficulty in another way by exploiting templates induced from representative documents selected by queries. Event schema induction takes root in work on the acquisition from text of knowledge structures, such as the Memory Organization Packets (Schank, 1980), used by early text understanding systems (DeJong, 1982) and more recently by Ferret and Grau (1997). First attempts for applying such processes to schema induction have been made in the fields of Information Extraction (Collier, 1998), Automatic Summarization (Harabagiu, 2004) and event QuestionAnswering (Filatova et al., 2006; Filatova, 2008). More recently, work after (Hasegawa et al., 2004) has developed weakly supervised forms of Information Extraction including schema induction in their objectives. However, they have been mainly applied to binary relation extraction in practice (Eichler et al., 2008; Rosenfeld and Feldman, 2007; Min et al., 2</context>
</contexts>
<marker>DeJong, 1982</marker>
<rawString>Gerald DeJong. 1982. An overview of the FRUMP system. In W. Lehnert and M. Ringle, editors, Strategies for natural language processing, pages 149–176. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathrin Eichler</author>
<author>Holmer Hemsen</author>
<author>G¨unter Neumann</author>
</authors>
<title>Unsupervised Relation Extraction From Web Documents.</title>
<date>2008</date>
<booktitle>In 6th Conference on Language Resources and Evaluation (LREC’08),</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="5570" citStr="Eichler et al., 2008" startWordPosition="876" endWordPosition="879">, 1980), used by early text understanding systems (DeJong, 1982) and more recently by Ferret and Grau (1997). First attempts for applying such processes to schema induction have been made in the fields of Information Extraction (Collier, 1998), Automatic Summarization (Harabagiu, 2004) and event QuestionAnswering (Filatova et al., 2006; Filatova, 2008). More recently, work after (Hasegawa et al., 2004) has developed weakly supervised forms of Information Extraction including schema induction in their objectives. However, they have been mainly applied to binary relation extraction in practice (Eichler et al., 2008; Rosenfeld and Feldman, 2007; Min et al., 2012). In parallel, several approaches were proposed for performing specifically schema induction in already existing frameworks: clause graph clustering (Qiu et al., 2008), event sequence alignment (Regneri et al., 2010) or LDA-based approach relying on FrameNet-like semantic frames (Bejan, 2008). More event-specific generative models were proposed by Chambers (2013) and Cheung et al. (2013). Finally, Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Chambers and Jurafsky (2011), improved by Balasubramanian et al. (2013), and Chambers (2013</context>
</contexts>
<marker>Eichler, Hemsen, Neumann, 2008</marker>
<rawString>Kathrin Eichler, Holmer Hemsen, and G¨unter Neumann. 2008. Unsupervised Relation Extraction From Web Documents. In 6th Conference on Language Resources and Evaluation (LREC’08), Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Ferret</author>
<author>Brigitte Grau</author>
</authors>
<title>An Aggregation Procedure for Building Episodic Memory.</title>
<date>1997</date>
<booktitle>In 15th International Joint Conference on Artificial Intelligence (IJCAI-97),</booktitle>
<pages>280--285</pages>
<location>Nagoya, Japan.</location>
<contexts>
<context position="5058" citStr="Ferret and Grau (1997)" startWordPosition="800" endWordPosition="803">a way to reduce the amount of effort needed for adapting a system to another domain. The approaches of Ondemand information extraction (Hasegawa et al., 2004; Sekine, 2006) and Preemptive Information Extraction (Shinyama and Sekine, 2006) tried to overcome this difficulty in another way by exploiting templates induced from representative documents selected by queries. Event schema induction takes root in work on the acquisition from text of knowledge structures, such as the Memory Organization Packets (Schank, 1980), used by early text understanding systems (DeJong, 1982) and more recently by Ferret and Grau (1997). First attempts for applying such processes to schema induction have been made in the fields of Information Extraction (Collier, 1998), Automatic Summarization (Harabagiu, 2004) and event QuestionAnswering (Filatova et al., 2006; Filatova, 2008). More recently, work after (Hasegawa et al., 2004) has developed weakly supervised forms of Information Extraction including schema induction in their objectives. However, they have been mainly applied to binary relation extraction in practice (Eichler et al., 2008; Rosenfeld and Feldman, 2007; Min et al., 2012). In parallel, several approaches were p</context>
</contexts>
<marker>Ferret, Grau, 1997</marker>
<rawString>Olivier Ferret and Brigitte Grau. 1997. An Aggregation Procedure for Building Episodic Memory. In 15th International Joint Conference on Artificial Intelligence (IJCAI-97), pages 280–285, Nagoya, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen McKeown</author>
</authors>
<title>Automatic Creation of Domain Templates.</title>
<date>2006</date>
<booktitle>In 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL</booktitle>
<pages>207--214</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="5287" citStr="Filatova et al., 2006" startWordPosition="835" endWordPosition="838">e, 2006) tried to overcome this difficulty in another way by exploiting templates induced from representative documents selected by queries. Event schema induction takes root in work on the acquisition from text of knowledge structures, such as the Memory Organization Packets (Schank, 1980), used by early text understanding systems (DeJong, 1982) and more recently by Ferret and Grau (1997). First attempts for applying such processes to schema induction have been made in the fields of Information Extraction (Collier, 1998), Automatic Summarization (Harabagiu, 2004) and event QuestionAnswering (Filatova et al., 2006; Filatova, 2008). More recently, work after (Hasegawa et al., 2004) has developed weakly supervised forms of Information Extraction including schema induction in their objectives. However, they have been mainly applied to binary relation extraction in practice (Eichler et al., 2008; Rosenfeld and Feldman, 2007; Min et al., 2012). In parallel, several approaches were proposed for performing specifically schema induction in already existing frameworks: clause graph clustering (Qiu et al., 2008), event sequence alignment (Regneri et al., 2010) or LDA-based approach relying on FrameNet-like seman</context>
</contexts>
<marker>Filatova, Hatzivassiloglou, McKeown, 2006</marker>
<rawString>Elena Filatova, Vasileios Hatzivassiloglou, and Kathleen McKeown. 2006. Automatic Creation of Domain Templates. In 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL 2006), pages 207–214, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
</authors>
<title>Unsupervised Relation Learning for Event-Focused Question-Answering and Domain Modelling.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>Columbia University.</institution>
<contexts>
<context position="5304" citStr="Filatova, 2008" startWordPosition="839" endWordPosition="840">ome this difficulty in another way by exploiting templates induced from representative documents selected by queries. Event schema induction takes root in work on the acquisition from text of knowledge structures, such as the Memory Organization Packets (Schank, 1980), used by early text understanding systems (DeJong, 1982) and more recently by Ferret and Grau (1997). First attempts for applying such processes to schema induction have been made in the fields of Information Extraction (Collier, 1998), Automatic Summarization (Harabagiu, 2004) and event QuestionAnswering (Filatova et al., 2006; Filatova, 2008). More recently, work after (Hasegawa et al., 2004) has developed weakly supervised forms of Information Extraction including schema induction in their objectives. However, they have been mainly applied to binary relation extraction in practice (Eichler et al., 2008; Rosenfeld and Feldman, 2007; Min et al., 2012). In parallel, several approaches were proposed for performing specifically schema induction in already existing frameworks: clause graph clustering (Qiu et al., 2008), event sequence alignment (Regneri et al., 2010) or LDA-based approach relying on FrameNet-like semantic frames (Bejan</context>
</contexts>
<marker>Filatova, 2008</marker>
<rawString>Elena Filatova. 2008. Unsupervised Relation Learning for Event-Focused Question-Answering and Domain Modelling. Ph.D. thesis, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marjorie Freedman</author>
<author>Lance Ramshaw</author>
<author>Elizabeth Boschee</author>
<author>Ryan Gabbard</author>
<author>Gary Kratkiewicz</author>
<author>Nicolas Ward</author>
<author>Ralph Weischedel</author>
</authors>
<title>Extreme Extraction – Machine Reading in a Week.</title>
<date>2011</date>
<booktitle>In 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP 2011),</booktitle>
<pages>1437--1446</pages>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="4396" citStr="Freedman et al., 2011" startWordPosition="691" endWordPosition="694">esentation; we describe our generative model in Section 4, before presenting our experiments and evaluations in Section 5. 188 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 188–197, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Related Work Despite efforts made for making template filling as generic as possible, it still depends heavily on the type of events. Mixing generic processes with a restrictive number of domainspecific rules (Freedman et al., 2011) or examples (Grishman and He, 2014) is a way to reduce the amount of effort needed for adapting a system to another domain. The approaches of Ondemand information extraction (Hasegawa et al., 2004; Sekine, 2006) and Preemptive Information Extraction (Shinyama and Sekine, 2006) tried to overcome this difficulty in another way by exploiting templates induced from representative documents selected by queries. Event schema induction takes root in work on the acquisition from text of knowledge structures, such as the Memory Organization Packets (Schank, 1980), used by early text understanding syst</context>
</contexts>
<marker>Freedman, Ramshaw, Boschee, Gabbard, Kratkiewicz, Ward, Weischedel, 2011</marker>
<rawString>Marjorie Freedman, Lance Ramshaw, Elizabeth Boschee, Ryan Gabbard, Gary Kratkiewicz, Nicolas Ward, and Ralph Weischedel. 2011. Extreme Extraction – Machine Reading in a Week. In 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP 2011), pages 1437– 1446, Edinburgh, Scotland, UK., July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lea Frermann</author>
<author>Ivan Titov</author>
<author>Manfred Pinkal</author>
</authors>
<title>A Hierarchical Bayesian Model for Unsupervised Induction of Script Knowledge.</title>
<date>2014</date>
<booktitle>In 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2014),</booktitle>
<pages>49--57</pages>
<location>Gothenburg, Sweden,</location>
<contexts>
<context position="6795" citStr="Frermann et al., 2014" startWordPosition="1057" endWordPosition="1060">ocused specifically on the induction of event roles and the identification of chains of events for building representations from texts by exploiting coreference resolution or the temporal ordering of events. All this work is also linked to work about the induction of scripts from texts, more or less closely linked to Attributes Head Triggers #1 [armed:amod] man [attack:nsubj, kill:nsubj] #2 [police:nn] station [attack:dobj] #3 [] policeman [kill:dobj] #4 [innocent:amod, man [wound:dobj] young:amod] Figure 1: Entity representation as tuples of ([attributes], head, [triggers]). events, such as (Frermann et al., 2014), (Pichotta and Mooney, 2014) or (Modi and Titov, 2014). The work we present in this article is in line with Chambers (2013), which will be described in more details in Section 5, together with a quantitative and qualitative comparison. 3 Entity Representation An entity is represented as a triple containing: a head word h, a list A of attribute relations and a list T of trigger relations. Consider the following example: (1) Two armed men attacked the police station and killed a policeman. An innocent young man was also wounded. As illustrated in Figure 1, four entities, equivalent to four sepa</context>
</contexts>
<marker>Frermann, Titov, Pinkal, 2014</marker>
<rawString>Lea Frermann, Ivan Titov, and Manfred Pinkal. 2014. A Hierarchical Bayesian Model for Unsupervised Induction of Script Knowledge. In 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2014), pages 49–57, Gothenburg, Sweden, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Griffiths</author>
</authors>
<title>Gibbs sampling in the generative model of Latent Dirichlet Allocation.</title>
<date>2002</date>
<tech>Technical report,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="10600" citStr="Griffiths, 2002" startWordPosition="1723" endWordPosition="1724">do Generate an attribute distribution θs from a Dirichlet prior dir(α); Generate a head distribution irs from a Dirichlet prior dir(Q); Generate a trigger distribution φs from a Dirichlet prior dir(ry); end for entity e E E do Generate a slot s from a uniform distribution uni(1, K); Generate a head h from a multinominal distribution irs; for i +— 1 to JTeJ do Generate a trigger ti from a multinominal distribution φs; end forj+— 1 to JAeJ do Generate an attribute aj from a multinominal distribution φs; end end 4.2 Parameter Estimation For parameter estimation, we use the Gibbs sampling method (Griffiths, 2002). The slot variable s is sampled by integrating out all the other variables. Previous models (Cheung et al., 2013; Chambers, 2013) are based on document-level topic modeling, which originated from models such as Latent Dirichlet Allocation (Blei et al., 2003). Our model is, instead, independent from document contexts. Its input is a sequence of entity triples. Document boundary is only used in a postprocessing step of filtering (see Section 5.3 for more details). There is a universal slot distribution instead of each slot distribution for one document. Furthermore, slot prior is ignored by usi</context>
</contexts>
<marker>Griffiths, 2002</marker>
<rawString>Tom Griffiths. 2002. Gibbs sampling in the generative model of Latent Dirichlet Allocation. Technical report, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>Yifan He</author>
</authors>
<title>An Information Extraction Customizer.</title>
<date>2014</date>
<booktitle>17th International Conference on Text, Speech and Dialogue (TSD 2014),</booktitle>
<volume>8655</volume>
<pages>3--10</pages>
<editor>In Petr Sojka, Ale Hork, Ivan Kopeek, and Karel Pala, editors,</editor>
<publisher>Springer International Publishing.</publisher>
<contexts>
<context position="4432" citStr="Grishman and He, 2014" startWordPosition="698" endWordPosition="701">ve model in Section 4, before presenting our experiments and evaluations in Section 5. 188 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 188–197, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Related Work Despite efforts made for making template filling as generic as possible, it still depends heavily on the type of events. Mixing generic processes with a restrictive number of domainspecific rules (Freedman et al., 2011) or examples (Grishman and He, 2014) is a way to reduce the amount of effort needed for adapting a system to another domain. The approaches of Ondemand information extraction (Hasegawa et al., 2004; Sekine, 2006) and Preemptive Information Extraction (Shinyama and Sekine, 2006) tried to overcome this difficulty in another way by exploiting templates induced from representative documents selected by queries. Event schema induction takes root in work on the acquisition from text of knowledge structures, such as the Memory Organization Packets (Schank, 1980), used by early text understanding systems (DeJong, 1982) and more recently</context>
</contexts>
<marker>Grishman, He, 2014</marker>
<rawString>Ralph Grishman and Yifan He. 2014. An Information Extraction Customizer. In Petr Sojka, Ale Hork, Ivan Kopeek, and Karel Pala, editors, 17th International Conference on Text, Speech and Dialogue (TSD 2014), volume 8655 of Lecture Notes in Computer Science, pages 3–10. Springer International Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>Beth Sundheim</author>
</authors>
<title>Message Understanding Conference-6: A Brief History.</title>
<date>1996</date>
<booktitle>In 16th International Conference on Computational linguistics (COLING’96),</booktitle>
<pages>466--471</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="1096" citStr="Grishman and Sundheim, 1996" startWordPosition="140" endWordPosition="144">ntities. However, elements other than head words contain useful information. For instance, an armed man is more discriminative than man. Our model takes into account this information and precisely represents it using probabilistic topic distributions. We illustrate that such information plays an important role in parameter estimation. Mostly, it makes topic distributions more coherent and more discriminative. Experimental results on benchmark dataset empirically confirm this enhancement. 1 Introduction Information Extraction was initially defined (and is still defined) by the MUC evaluations (Grishman and Sundheim, 1996) and more specifically by the task of template filling. The objective of this task is to assign event roles to individual textual mentions. A template defines a specific type of events (e.g. earthquakes), associated with semantic roles (or slots) hold by entities (for earthquakes, their location, date, magnitude and the damages they caused (Jean-Louis et al., 2011)). Schema induction is the task of learning these templates with no supervision from unlabeled text. We focus here on event schema induction and continue the trend of generative models proposed earlier for this task. The idea is to g</context>
</contexts>
<marker>Grishman, Sundheim, 1996</marker>
<rawString>Ralph Grishman and Beth Sundheim. 1996. Message Understanding Conference-6: A Brief History. In 16th International Conference on Computational linguistics (COLING’96), pages 466–471, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
</authors>
<title>Incremental Topic Representation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING’04),</booktitle>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="5236" citStr="Harabagiu, 2004" startWordPosition="829" endWordPosition="830">ive Information Extraction (Shinyama and Sekine, 2006) tried to overcome this difficulty in another way by exploiting templates induced from representative documents selected by queries. Event schema induction takes root in work on the acquisition from text of knowledge structures, such as the Memory Organization Packets (Schank, 1980), used by early text understanding systems (DeJong, 1982) and more recently by Ferret and Grau (1997). First attempts for applying such processes to schema induction have been made in the fields of Information Extraction (Collier, 1998), Automatic Summarization (Harabagiu, 2004) and event QuestionAnswering (Filatova et al., 2006; Filatova, 2008). More recently, work after (Hasegawa et al., 2004) has developed weakly supervised forms of Information Extraction including schema induction in their objectives. However, they have been mainly applied to binary relation extraction in practice (Eichler et al., 2008; Rosenfeld and Feldman, 2007; Min et al., 2012). In parallel, several approaches were proposed for performing specifically schema induction in already existing frameworks: clause graph clustering (Qiu et al., 2008), event sequence alignment (Regneri et al., 2010) o</context>
</contexts>
<marker>Harabagiu, 2004</marker>
<rawString>Sanda Harabagiu. 2004. Incremental Topic Representation. In Proceedings of the 20th International Conference on Computational Linguistics (COLING’04), Geneva, Switzerland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takaaki Hasegawa</author>
<author>Satoshi Sekine</author>
<author>Ralph Grishman</author>
</authors>
<title>Discovering Relations among Named Entities from Large Corpora.</title>
<date>2004</date>
<booktitle>In 42nd Meeting of the Association for Computational Linguistics (ACL’04),</booktitle>
<pages>415--422</pages>
<location>Barcelona,</location>
<contexts>
<context position="4593" citStr="Hasegawa et al., 2004" startWordPosition="727" endWordPosition="730">onal Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 188–197, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Related Work Despite efforts made for making template filling as generic as possible, it still depends heavily on the type of events. Mixing generic processes with a restrictive number of domainspecific rules (Freedman et al., 2011) or examples (Grishman and He, 2014) is a way to reduce the amount of effort needed for adapting a system to another domain. The approaches of Ondemand information extraction (Hasegawa et al., 2004; Sekine, 2006) and Preemptive Information Extraction (Shinyama and Sekine, 2006) tried to overcome this difficulty in another way by exploiting templates induced from representative documents selected by queries. Event schema induction takes root in work on the acquisition from text of knowledge structures, such as the Memory Organization Packets (Schank, 1980), used by early text understanding systems (DeJong, 1982) and more recently by Ferret and Grau (1997). First attempts for applying such processes to schema induction have been made in the fields of Information Extraction (Collier, 1998)</context>
</contexts>
<marker>Hasegawa, Sekine, Grishman, 2004</marker>
<rawString>Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman. 2004. Discovering Relations among Named Entities from Large Corpora. In 42nd Meeting of the Association for Computational Linguistics (ACL’04), pages 415–422, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ludovic Jean-Louis</author>
<author>Romaric Besanon</author>
<author>Olivier Ferret</author>
</authors>
<title>Text Segmentation and Graph-based Method for Template Filling in Information Extraction.</title>
<date>2011</date>
<booktitle>In 5th International Joint Conference on Natural Language Processing (IJCNLP 2011),</booktitle>
<pages>723--731</pages>
<location>Chiang Mai, Thailand.</location>
<contexts>
<context position="1463" citStr="Jean-Louis et al., 2011" startWordPosition="201" endWordPosition="204">ns more coherent and more discriminative. Experimental results on benchmark dataset empirically confirm this enhancement. 1 Introduction Information Extraction was initially defined (and is still defined) by the MUC evaluations (Grishman and Sundheim, 1996) and more specifically by the task of template filling. The objective of this task is to assign event roles to individual textual mentions. A template defines a specific type of events (e.g. earthquakes), associated with semantic roles (or slots) hold by entities (for earthquakes, their location, date, magnitude and the damages they caused (Jean-Louis et al., 2011)). Schema induction is the task of learning these templates with no supervision from unlabeled text. We focus here on event schema induction and continue the trend of generative models proposed earlier for this task. The idea is to group together entities corresponding to the same role in an event template based on the similarity of the relations that these entities hold with predicates. For example, in a corpus about terrorist attacks, entities that are objects of verbs to kill, to attack can be grouped together and characterized by a role named VICTIM. The output of this identification opera</context>
</contexts>
<marker>Jean-Louis, Besanon, Ferret, 2011</marker>
<rawString>Ludovic Jean-Louis, Romaric Besanon, and Olivier Ferret. 2011. Text Segmentation and Graph-based Method for Template Filling in Information Extraction. In 5th International Joint Conference on Natural Language Processing (IJCNLP 2011), pages 723–731, Chiang Mai, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP Natural Language Processing Toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<location>Baltimore, USA,</location>
<contexts>
<context position="7898" citStr="Manning et al., 2014" startWordPosition="1244" endWordPosition="1247"> policeman. An innocent young man was also wounded. As illustrated in Figure 1, four entities, equivalent to four separated triples, are generated from the text above. Head words are extracted from noun phrases. A trigger relation is composed of a predicate (attack, kill, wound) and a dependency type (subject, object). An attribute relation is composed of an argument (armed, police, young) and a dependency type (adjectival, nominal or verbal modifier). In the relationship to triggers, a head word is argument, but in the relationship to attributes, it is predicate. We use Stanford NLP toolkit (Manning et al., 2014) for parsing and coreference resolution. A head word is extracted if it is a nominal or proper noun and it is related to at least one trigger; pronouns are omitted. A trigger of an head word is extracted if it is a verb or an eventive noun and the head word serves as its subject, object, or preposition. We use the categories noun.EVENT and noun.ACT in WordNet as a list of eventive nouns. A head word can have more than one trigger. These multiple relations can come from a syntactic coordination inside a single sentence, as it is the case in the first sentence of the illustrating example. They c</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60, Baltimore, USA, jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonan Min</author>
<author>Shuming Shi</author>
<author>Ralph Grishman</author>
<author>ChinYew Lin</author>
</authors>
<title>Ensemble Semantics for Largescale Unsupervised Relation Extraction.</title>
<date>2012</date>
<booktitle>In 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2012,</booktitle>
<pages>1027--1037</pages>
<location>Jeju Island,</location>
<contexts>
<context position="5618" citStr="Min et al., 2012" startWordPosition="884" endWordPosition="887">DeJong, 1982) and more recently by Ferret and Grau (1997). First attempts for applying such processes to schema induction have been made in the fields of Information Extraction (Collier, 1998), Automatic Summarization (Harabagiu, 2004) and event QuestionAnswering (Filatova et al., 2006; Filatova, 2008). More recently, work after (Hasegawa et al., 2004) has developed weakly supervised forms of Information Extraction including schema induction in their objectives. However, they have been mainly applied to binary relation extraction in practice (Eichler et al., 2008; Rosenfeld and Feldman, 2007; Min et al., 2012). In parallel, several approaches were proposed for performing specifically schema induction in already existing frameworks: clause graph clustering (Qiu et al., 2008), event sequence alignment (Regneri et al., 2010) or LDA-based approach relying on FrameNet-like semantic frames (Bejan, 2008). More event-specific generative models were proposed by Chambers (2013) and Cheung et al. (2013). Finally, Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Chambers and Jurafsky (2011), improved by Balasubramanian et al. (2013), and Chambers (2013) focused specifically on the induction of event</context>
</contexts>
<marker>Min, Shi, Grishman, Lin, 2012</marker>
<rawString>Bonan Min, Shuming Shi, Ralph Grishman, and ChinYew Lin. 2012. Ensemble Semantics for Largescale Unsupervised Relation Extraction. In 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2012, pages 1027–1037, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashutosh Modi</author>
<author>Ivan Titov</author>
</authors>
<title>Inducing neural models of script knowledge.</title>
<date>2014</date>
<booktitle>In Eighteenth Conference on Computational Natural Language Learning (CoNLL</booktitle>
<pages>49--57</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="6850" citStr="Modi and Titov, 2014" startWordPosition="1066" endWordPosition="1069">the identification of chains of events for building representations from texts by exploiting coreference resolution or the temporal ordering of events. All this work is also linked to work about the induction of scripts from texts, more or less closely linked to Attributes Head Triggers #1 [armed:amod] man [attack:nsubj, kill:nsubj] #2 [police:nn] station [attack:dobj] #3 [] policeman [kill:dobj] #4 [innocent:amod, man [wound:dobj] young:amod] Figure 1: Entity representation as tuples of ([attributes], head, [triggers]). events, such as (Frermann et al., 2014), (Pichotta and Mooney, 2014) or (Modi and Titov, 2014). The work we present in this article is in line with Chambers (2013), which will be described in more details in Section 5, together with a quantitative and qualitative comparison. 3 Entity Representation An entity is represented as a triple containing: a head word h, a list A of attribute relations and a list T of trigger relations. Consider the following example: (1) Two armed men attacked the police station and killed a policeman. An innocent young man was also wounded. As illustrated in Figure 1, four entities, equivalent to four separated triples, are generated from the text above. Head </context>
</contexts>
<marker>Modi, Titov, 2014</marker>
<rawString>Ashutosh Modi and Ivan Titov. 2014. Inducing neural models of script knowledge. In Eighteenth Conference on Computational Natural Language Learning (CoNLL 2014), pages 49–57, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Ellen Riloff</author>
</authors>
<title>Effective Information Extraction with Semantic Affinity Patterns and Relevant Regions.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</booktitle>
<pages>717--727</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="14964" citStr="Patwardhan and Riloff, 2007" startWordPosition="2408" endWordPosition="2411">then compare our results with previous approaches, more particularly with Chambers (2013), from both quantitative and qualitative points of view (Section 5.4). Finally, Section 5.5 is dedicated to error analysis, with a special emphasis on sources of false positives. 5.1 Experimental Setups 5.1.1 Datasets The MUC-4 corpus contains 1,700 news articles about terrorist incidents happening in Latin America. The corpus is divided into 1,300 documents 191 for the development set and four test sets, each containing 100 documents. We follow the rules in the literature to guarantee comparable results (Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). The evaluation focuses on four template types – ARSON, ATTACK, BOMBING, KIDNAPPING – and four slots – Perpetrator, Instrument, Target, and Victim. Perpetrator is merged from Perpetrator Individual and Perpetrator Organization. The matching between system answers and references is based on head word matching. A head word is defined as the rightmost word of the phrase or as the right-most word of the first ‘of’ if the phrase contains any. Optional templates and slots are ignored when calculating recall. Template types are ignored in evaluation: this means that a p</context>
<context position="22426" citStr="Patwardhan and Riloff, 2007" startWordPosition="3638" endWordPosition="3641"> documents that are mistakenly filtered out. However, this loss is not significant and the overall Fscore finally increases by 5%. We also compare our results to an “oracle” classifier that would remove all irrelevant documents while preserving all relevant ones. The performance of this oracle classification shows that there are some room for further improvement from document classification. Irrelevant document filtering is a technique applied by most supervised and unsupervised approaches. Supervised methods prefer relevance detection at sentence or phrase-level (Patwardhan and Riloff, 2009; Patwardhan and Riloff, 2007). As for several unsupervised methods, Chambers (2013) includes document classification in his topic model. Chambers and Jurafsky (2011) and Cheung et al. (2013) use the learned clusters to classify documents by estimating the relevance of a document with respect to a template from posthoc statistics about event triggers. 5.4 Comparison to State-of-the-Art For comparing in more depth our results to the state-of-the-art in the literature. we reimplemented the method proposed in Chambers (2013) and integrated our attribute distributions into his model (as shown in Figure 6). The main differences</context>
</contexts>
<marker>Patwardhan, Riloff, 2007</marker>
<rawString>Siddharth Patwardhan and Ellen Riloff. 2007. Effective Information Extraction with Semantic Affinity Patterns and Relevant Regions. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2007), pages 717–727, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Ellen Riloff</author>
</authors>
<title>A Unified Model of Phrasal and Sentential Evidence for Information Extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>151--160</pages>
<contexts>
<context position="22396" citStr="Patwardhan and Riloff, 2009" startWordPosition="3634" endWordPosition="3637">of recall comes from relevant documents that are mistakenly filtered out. However, this loss is not significant and the overall Fscore finally increases by 5%. We also compare our results to an “oracle” classifier that would remove all irrelevant documents while preserving all relevant ones. The performance of this oracle classification shows that there are some room for further improvement from document classification. Irrelevant document filtering is a technique applied by most supervised and unsupervised approaches. Supervised methods prefer relevance detection at sentence or phrase-level (Patwardhan and Riloff, 2009; Patwardhan and Riloff, 2007). As for several unsupervised methods, Chambers (2013) includes document classification in his topic model. Chambers and Jurafsky (2011) and Cheung et al. (2013) use the learned clusters to classify documents by estimating the relevance of a document with respect to a template from posthoc statistics about event triggers. 5.4 Comparison to State-of-the-Art For comparing in more depth our results to the state-of-the-art in the literature. we reimplemented the method proposed in Chambers (2013) and integrated our attribute distributions into his model (as shown in F</context>
</contexts>
<marker>Patwardhan, Riloff, 2009</marker>
<rawString>Siddharth Patwardhan and Ellen Riloff. 2009. A Unified Model of Phrasal and Sentential Evidence for Information Extraction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP 2009), pages 151–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Pichotta</author>
<author>Raymond Mooney</author>
</authors>
<title>Statistical script learning with multi-argument events.</title>
<date>2014</date>
<booktitle>In 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2014),</booktitle>
<pages>220--229</pages>
<location>Gothenburg,</location>
<contexts>
<context position="6824" citStr="Pichotta and Mooney, 2014" startWordPosition="1061" endWordPosition="1064">e induction of event roles and the identification of chains of events for building representations from texts by exploiting coreference resolution or the temporal ordering of events. All this work is also linked to work about the induction of scripts from texts, more or less closely linked to Attributes Head Triggers #1 [armed:amod] man [attack:nsubj, kill:nsubj] #2 [police:nn] station [attack:dobj] #3 [] policeman [kill:dobj] #4 [innocent:amod, man [wound:dobj] young:amod] Figure 1: Entity representation as tuples of ([attributes], head, [triggers]). events, such as (Frermann et al., 2014), (Pichotta and Mooney, 2014) or (Modi and Titov, 2014). The work we present in this article is in line with Chambers (2013), which will be described in more details in Section 5, together with a quantitative and qualitative comparison. 3 Entity Representation An entity is represented as a triple containing: a head word h, a list A of attribute relations and a list T of trigger relations. Consider the following example: (1) Two armed men attacked the police station and killed a policeman. An innocent young man was also wounded. As illustrated in Figure 1, four entities, equivalent to four separated triples, are generated </context>
</contexts>
<marker>Pichotta, Mooney, 2014</marker>
<rawString>Karl Pichotta and Raymond Mooney. 2014. Statistical script learning with multi-argument events. In 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2014), pages 220–229, Gothenburg, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Qiu</author>
<author>Min-Yen Kan</author>
<author>Tat-Seng Chua</author>
</authors>
<title>Modeling Context in Scenario Template Creation.</title>
<date>2008</date>
<booktitle>In Third International Joint Conference on Natural Language Processing (IJCNLP</booktitle>
<pages>157--164</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="5785" citStr="Qiu et al., 2008" startWordPosition="908" endWordPosition="911">xtraction (Collier, 1998), Automatic Summarization (Harabagiu, 2004) and event QuestionAnswering (Filatova et al., 2006; Filatova, 2008). More recently, work after (Hasegawa et al., 2004) has developed weakly supervised forms of Information Extraction including schema induction in their objectives. However, they have been mainly applied to binary relation extraction in practice (Eichler et al., 2008; Rosenfeld and Feldman, 2007; Min et al., 2012). In parallel, several approaches were proposed for performing specifically schema induction in already existing frameworks: clause graph clustering (Qiu et al., 2008), event sequence alignment (Regneri et al., 2010) or LDA-based approach relying on FrameNet-like semantic frames (Bejan, 2008). More event-specific generative models were proposed by Chambers (2013) and Cheung et al. (2013). Finally, Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Chambers and Jurafsky (2011), improved by Balasubramanian et al. (2013), and Chambers (2013) focused specifically on the induction of event roles and the identification of chains of events for building representations from texts by exploiting coreference resolution or the temporal ordering of events. All </context>
</contexts>
<marker>Qiu, Kan, Chua, 2008</marker>
<rawString>Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2008. Modeling Context in Scenario Template Creation. In Third International Joint Conference on Natural Language Processing (IJCNLP 2008), pages 157– 164, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michaela Regneri</author>
<author>Alexander Koller</author>
<author>Manfred Pinkal</author>
</authors>
<title>Learning Script Knowledge with Web Experiments.</title>
<date>2010</date>
<booktitle>In 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010),</booktitle>
<pages>979--988</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="5834" citStr="Regneri et al., 2010" startWordPosition="915" endWordPosition="919">ation (Harabagiu, 2004) and event QuestionAnswering (Filatova et al., 2006; Filatova, 2008). More recently, work after (Hasegawa et al., 2004) has developed weakly supervised forms of Information Extraction including schema induction in their objectives. However, they have been mainly applied to binary relation extraction in practice (Eichler et al., 2008; Rosenfeld and Feldman, 2007; Min et al., 2012). In parallel, several approaches were proposed for performing specifically schema induction in already existing frameworks: clause graph clustering (Qiu et al., 2008), event sequence alignment (Regneri et al., 2010) or LDA-based approach relying on FrameNet-like semantic frames (Bejan, 2008). More event-specific generative models were proposed by Chambers (2013) and Cheung et al. (2013). Finally, Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Chambers and Jurafsky (2011), improved by Balasubramanian et al. (2013), and Chambers (2013) focused specifically on the induction of event roles and the identification of chains of events for building representations from texts by exploiting coreference resolution or the temporal ordering of events. All this work is also linked to work about the induct</context>
</contexts>
<marker>Regneri, Koller, Pinkal, 2010</marker>
<rawString>Michaela Regneri, Alexander Koller, and Manfred Pinkal. 2010. Learning Script Knowledge with Web Experiments. In 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), pages 979–988, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Rosenfeld</author>
<author>Ronen Feldman</author>
</authors>
<title>Clustering for unsupervised relation identification.</title>
<date>2007</date>
<booktitle>In Sixteenth ACM conference on Conference on information and knowledge management (CIKM’07),</booktitle>
<pages>411--418</pages>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="5599" citStr="Rosenfeld and Feldman, 2007" startWordPosition="880" endWordPosition="883"> text understanding systems (DeJong, 1982) and more recently by Ferret and Grau (1997). First attempts for applying such processes to schema induction have been made in the fields of Information Extraction (Collier, 1998), Automatic Summarization (Harabagiu, 2004) and event QuestionAnswering (Filatova et al., 2006; Filatova, 2008). More recently, work after (Hasegawa et al., 2004) has developed weakly supervised forms of Information Extraction including schema induction in their objectives. However, they have been mainly applied to binary relation extraction in practice (Eichler et al., 2008; Rosenfeld and Feldman, 2007; Min et al., 2012). In parallel, several approaches were proposed for performing specifically schema induction in already existing frameworks: clause graph clustering (Qiu et al., 2008), event sequence alignment (Regneri et al., 2010) or LDA-based approach relying on FrameNet-like semantic frames (Bejan, 2008). More event-specific generative models were proposed by Chambers (2013) and Cheung et al. (2013). Finally, Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Chambers and Jurafsky (2011), improved by Balasubramanian et al. (2013), and Chambers (2013) focused specifically on the</context>
</contexts>
<marker>Rosenfeld, Feldman, 2007</marker>
<rawString>Benjamin Rosenfeld and Ronen Feldman. 2007. Clustering for unsupervised relation identification. In Sixteenth ACM conference on Conference on information and knowledge management (CIKM’07), pages 411–418, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger C Schank</author>
</authors>
<title>Language and memory.</title>
<date>1980</date>
<journal>Cognitive Science,</journal>
<pages>4--243</pages>
<contexts>
<context position="4957" citStr="Schank, 1980" startWordPosition="784" endWordPosition="785">mber of domainspecific rules (Freedman et al., 2011) or examples (Grishman and He, 2014) is a way to reduce the amount of effort needed for adapting a system to another domain. The approaches of Ondemand information extraction (Hasegawa et al., 2004; Sekine, 2006) and Preemptive Information Extraction (Shinyama and Sekine, 2006) tried to overcome this difficulty in another way by exploiting templates induced from representative documents selected by queries. Event schema induction takes root in work on the acquisition from text of knowledge structures, such as the Memory Organization Packets (Schank, 1980), used by early text understanding systems (DeJong, 1982) and more recently by Ferret and Grau (1997). First attempts for applying such processes to schema induction have been made in the fields of Information Extraction (Collier, 1998), Automatic Summarization (Harabagiu, 2004) and event QuestionAnswering (Filatova et al., 2006; Filatova, 2008). More recently, work after (Hasegawa et al., 2004) has developed weakly supervised forms of Information Extraction including schema induction in their objectives. However, they have been mainly applied to binary relation extraction in practice (Eichler</context>
</contexts>
<marker>Schank, 1980</marker>
<rawString>Roger C. Schank. 1980. Language and memory. Cognitive Science, 4:243–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
</authors>
<title>On-demand information extraction.</title>
<date>2006</date>
<booktitle>In 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL</booktitle>
<pages>731--738</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="4608" citStr="Sekine, 2006" startWordPosition="731" endWordPosition="732">e 7th International Joint Conference on Natural Language Processing, pages 188–197, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Related Work Despite efforts made for making template filling as generic as possible, it still depends heavily on the type of events. Mixing generic processes with a restrictive number of domainspecific rules (Freedman et al., 2011) or examples (Grishman and He, 2014) is a way to reduce the amount of effort needed for adapting a system to another domain. The approaches of Ondemand information extraction (Hasegawa et al., 2004; Sekine, 2006) and Preemptive Information Extraction (Shinyama and Sekine, 2006) tried to overcome this difficulty in another way by exploiting templates induced from representative documents selected by queries. Event schema induction takes root in work on the acquisition from text of knowledge structures, such as the Memory Organization Packets (Schank, 1980), used by early text understanding systems (DeJong, 1982) and more recently by Ferret and Grau (1997). First attempts for applying such processes to schema induction have been made in the fields of Information Extraction (Collier, 1998), Automatic Sum</context>
</contexts>
<marker>Sekine, 2006</marker>
<rawString>Satoshi Sekine. 2006. On-demand information extraction. In 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL 2006), pages 731–738, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
</authors>
<title>Preemptive Information Extraction using Unrestricted Relation Discovery. In HLT-NAACL</title>
<date>2006</date>
<pages>304--311</pages>
<location>New York City, USA.</location>
<contexts>
<context position="4674" citStr="Shinyama and Sekine, 2006" startWordPosition="737" endWordPosition="740">age Processing, pages 188–197, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Related Work Despite efforts made for making template filling as generic as possible, it still depends heavily on the type of events. Mixing generic processes with a restrictive number of domainspecific rules (Freedman et al., 2011) or examples (Grishman and He, 2014) is a way to reduce the amount of effort needed for adapting a system to another domain. The approaches of Ondemand information extraction (Hasegawa et al., 2004; Sekine, 2006) and Preemptive Information Extraction (Shinyama and Sekine, 2006) tried to overcome this difficulty in another way by exploiting templates induced from representative documents selected by queries. Event schema induction takes root in work on the acquisition from text of knowledge structures, such as the Memory Organization Packets (Schank, 1980), used by early text understanding systems (DeJong, 1982) and more recently by Ferret and Grau (1997). First attempts for applying such processes to schema induction have been made in the fields of Information Extraction (Collier, 1998), Automatic Summarization (Harabagiu, 2004) and event QuestionAnswering (Filatova</context>
</contexts>
<marker>Shinyama, Sekine, 2006</marker>
<rawString>Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive Information Extraction using Unrestricted Relation Discovery. In HLT-NAACL 2006, pages 304– 311, New York City, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth M Sundheim</author>
</authors>
<title>Third Message Understanding Evaluation and Conference (MUC-3): Phase 1 Status Report.</title>
<date>1991</date>
<booktitle>In Proceedings of the Workshop on Speech and Natural Language, HLT ’91,</booktitle>
<pages>301--305</pages>
<contexts>
<context position="13846" citStr="Sundheim, 1991" startWordPosition="2240" endWordPosition="2241">ghtly modified whereas probabilities of ambiguous words such as man, soldier or triggers such as kill:dobj or attack:nsubj converged smoothly to a different stable state that was semantically more coherent. For instance, the model interestingly realized that even if a terrorist was killed (e.g. by police), he was not actually a real victim of an attack. Figure 3 shows probability convergences of terrorist and kill:dobj given ATTACK victim and ATTACK perpetrator. 5 Evaluations In order to compare with related work, we evaluated our method on the Message Understanding Conference (MUC-4) corpus (Sundheim, 1991) using precision, recall and F-score as conventional metrics for template extraction. In what follows, we first introduce the MUC4 corpus (Section 5.1.1), we detail the mapping technique between learned slots and reference slots (5.1.2) as well as the hyper-parameters of our model (5.1.3). Next, we present a first experiment (Section 5.2) showing how using attribute relations improves overall results. The second experiment (Section 5.3) studies the impact of document classification. We then compare our results with previous approaches, more particularly with Chambers (2013), from both quantita</context>
</contexts>
<marker>Sundheim, 1991</marker>
<rawString>Beth M. Sundheim. 1991. Third Message Understanding Evaluation and Conference (MUC-3): Phase 1 Status Report. In Proceedings of the Workshop on Speech and Natural Language, HLT ’91, pages 301– 305.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>