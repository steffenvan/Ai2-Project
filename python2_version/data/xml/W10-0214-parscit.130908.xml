<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.953589">
Recognizing Stances in Ideological On-Line Debates
</title>
<author confidence="0.988758">
Swapna Somasundaran
</author>
<affiliation confidence="0.9989605">
Dept. of Computer Science
University of Pittsburgh
</affiliation>
<address confidence="0.550953">
Pittsburgh, PA 15260
</address>
<email confidence="0.99601">
swapna@cs.pitt.edu
</email>
<author confidence="0.982312">
Janyce Wiebe
</author>
<affiliation confidence="0.990528333333333">
Dept. of Computer Science and
The Intelligent Systems Program
University of Pittsburgh
</affiliation>
<address confidence="0.552231">
Pittsburgh, PA 15260
</address>
<email confidence="0.997749">
wiebe@cs.pitt.edu
</email>
<sectionHeader confidence="0.998593" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999094307692308">
This work explores the utility of sentiment and
arguing opinions for classifying stances in ide-
ological debates. In order to capture arguing
opinions in ideological stance taking, we con-
struct an arguing lexicon automatically from
a manually annotated corpus. We build su-
pervised systems employing sentiment and ar-
guing opinions and their targets as features.
Our systems perform substantially better than
a distribution-based baseline. Additionally,
by employing both types of opinion features,
we are able to perform better than a unigram-
based system.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999896153846154">
In this work, we explore if and how ideologi-
cal stances can be recognized using opinion analy-
sis. Following (Somasundaran and Wiebe, 2009),
stance, as used in this work, refers to an overall po-
sition held by a person toward an object, idea or
proposition. For example, in a debate “Do you be-
lieve in the existence of God?,” a person may take a
for-existence of God stance or an against existence
of God stance. Similarly, being pro-choice, believ-
ing in creationism, and supporting universal health-
care are all examples of ideological stances.
Online web forums discussing ideological and po-
litical hot-topics are popular.1 In this work, we are
</bodyText>
<footnote confidence="0.995062">
1http://www.opposingviews.com,
http://wiki.idebate.org, http://www.createdebate.com and
http://www.forandagainst.com are examples of such debating
websites.
</footnote>
<page confidence="0.993133">
116
</page>
<bodyText confidence="0.96705951724138">
interested in dual-sided debates (there are two pos-
sible polarizing sides that the participants can take).
For example, in a healthcare debate, participants can
take a for-healthcare stance or an against-healthcare
stance. Participants generally pick a side (the web-
sites provide a way for users to tag their stance)
and post an argument/justification supporting their
stance.
Personal opinions are clearly important in ideo-
logical stance taking, and debate posts provide out-
lets for expressing them. For instance, let us con-
sider the following snippet from a universal health-
care debate. Here the writer is expressing a nega-
tive sentiment2 regarding the government (the opin-
ion spans are highlighted in bold and their targets,
what the opinions are about, are highlighted in ital-
ics).
(1) Government is a disease pretending to be its
own cure. [side: against healthcare]
The writer’s negative sentiment is directed toward
the government, the initiator of universal healthcare.
This negative opinion reveals his against-healthcare
stance.
We observed that arguing, a less well explored
type of subjectivity, is prominently manifested in
ideological debates. As used in this work, arguing is
a type of linguistic subjectivity, where a person is ar-
guing for or against something or expressing a belief
about what is true, should be true or should be done
</bodyText>
<footnote confidence="0.55893025">
2As used in this work, sentiment is a type of linguistic sub-
jectivity, specifically positive and negative expressions of emo-
tions, judgments, and evaluations (Wilson and Wiebe, 2005;
Wilson, 2007; Somasundaran et al., 2008).
</footnote>
<note confidence="0.9858779">
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 116–124,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
in his or her view of the world (Wilson and Wiebe,
2005; Wilson, 2007; Somasundaran et al., 2008).
For instance, let us consider the following snippet
from a post supporting an against-existence of God
stance.
4. Experiments, results and analyses are presented in
Section 5. Related work is in Section 6 and conclu-
sions are in Section 7.
</note>
<sectionHeader confidence="0.840994" genericHeader="method">
2 Ideological Debates
</sectionHeader>
<bodyText confidence="0.996369219512195">
(2) Obviously that hasn’t happened, and to be
completely objective (as all scientists should
be) we must lean on the side of greatest evi-
dence which at the present time is for evolu-
tion. [side: against the existence of God]
In supporting their side, people not only express
their sentiments, but they also argue about what is
true (e.g., this is prominent in the existence of God
debate) and about what should or should not be done
(e.g., this is prominent in the healthcare debate).
In this work, we investigate whether sentiment
and arguing expressions of opinion are useful for
ideological stance classification. For this, we ex-
plore ways to capture relevant opinion information
as machine learning features into a supervised stance
classifier. While there is a large body of resources
for sentiment analysis (e.g., the sentiment lexicon
from (Wilson et al., 2005)), arguing analysis does
not seem to have a well established lexical resource.
In order to remedy this, using a simple automatic ap-
proach and a manually annotated corpus,3 we con-
struct an arguing lexicon. We create features called
opinion-target pairs, which encode not just the opin-
ion information, but also what the opinion is about,
its target. Systems employing sentiment-based and
arguing-based features alone, or both in combina-
tion, are analyzed. We also take a qualitative look
at features used by the learners to get insights about
the information captured by them.
We perform experiments on four different ideo-
logical domains. Our results show that systems us-
ing both sentiment and arguing features can perform
substantially better than a distribution-based base-
line and marginally better than a unigram-based sys-
tem. Our qualitative analysis suggests that opinion
features capture more insightful information than
using words alone.
The rest of this paper is organized as follows: We
first describe our ideological debate data in Section
2. We explain the construction of our arguing lexi-
con in Section 3 and our different systems in Section
</bodyText>
<footnote confidence="0.801287">
3MPQA corpus available at http://www.cs.pitt.edu/mpqa.
</footnote>
<bodyText confidence="0.999824558823529">
Political and ideological debates on hot issues are
popular on the web. In this work, we analyze the fol-
lowing domains: Existence of God, Healthcare, Gun
Rights, Gay Rights, Abortion and Creationism. Of
these, we use the first two for development and the
remaining four for experiments and analyses. Each
domain is a political/ideological issue and has two
polarizing stances: for and against.
Table 2 lists the domains, examples of debate top-
ics within each domain, the specific sides for each
debate topic, and the domain-level stances that cor-
respond to these sides. For example, consider the
Existence of God domain in Table 2. The two
stances in this domain are for-existence of God and
against-existence of God. “Do you believe in God”,
a specific debate topic within this domain, has two
sides: “Yes!!” and “No!!”. The former corresponds
to the for-existence of God stance and the latter maps
to the against-existence of God stance. The situa-
tion is different for the debate “God Does Not Ex-
ist”. Here, side “against” corresponds to the for-
existence of God stance, and side “for” corresponds
to the against-existence of God stance.
In general, we see in Table 2 that, while specific
debate topics may vary, in each case the two sides
for the topic correspond to the domain-level stances.
We download several debates for each domain and
manually map debate-level stances to the stances
for the domain. Table 2 also reports the number
of debates, and the total number of posts for each
domain. For instance, we collect 16 different de-
bates in the healthcare domain which gives us a total
of 336 posts. All debate posts have user-reported
debate-level stance tags.
</bodyText>
<subsectionHeader confidence="0.964114">
2.1 Observations
</subsectionHeader>
<bodyText confidence="0.999487">
Preliminary inspection of development data gave us
insights which shaped our approach. We discuss
some of our observations in this section.
</bodyText>
<subsectionHeader confidence="0.816367">
Arguing Opinion
</subsectionHeader>
<bodyText confidence="0.998324">
We found that arguing opinions are prominent
when people defend their ideological stances. We
</bodyText>
<page confidence="0.996892">
117
</page>
<table confidence="0.99970325">
Domain/Topics stances stance2
Healthcare (16 debates, 336 posts) for against
Should the US have universal health- Yes No
care Pro Con
Debate: Public insurance option in
US health care
Existence of God (7 debates, 486 for against
posts)
Do you believe in God Yes!! No!!
God Does Not Exist against for
Gun Rights (18 debates, 566 posts) for against
Should Guns Be Illegal against for
Debate: Right to bear arms in the US Yes No
Gay Rights (15 debates, 1186 posts) for against
Are people born gay Yes No
Is homosexuality a sin No Yes
Abortion (13 debates, 618 posts) for against
Should abortion be legal Yes No
Should south Dakota pass the abor- No Yes
tion ban
Creationism (15 debates, 729 posts) for against
Evolution Is A False Idea for against
Has evolution been scientifically It has It has
proved not
</table>
<tableCaption confidence="0.999876">
Table 1: Examples of debate topics and their stances
</tableCaption>
<bodyText confidence="0.972802944444444">
saw an instance of this in Example 2, where the par-
ticipant argues against the existence of God. He ar-
gues for what (he believes) is right (should be), and
is imperative (we must). He employs “Obviously”
to draw emphasis and then uses a superlative con-
struct (greatest) to argue for evolution.
Example 3 below illustrates arguing in a health-
care debate. The spans most certainly believe and
has or must do reveal arguing (ESSENTIAL, IM-
PORTANT are sentiments).
(3) ... I most certainly believe that there are
some ESSENTIAL, IMPORTANT things
that the government has or must do [side: for
healthcare]
Observe that the text spans revealing arguing can
be a single word or multiple words. This is differ-
ent from sentiment expressions that are more often
single words.
</bodyText>
<subsectionHeader confidence="0.514055">
Opinion Targets
</subsectionHeader>
<bodyText confidence="0.996311621621622">
As mentioned previously, a target is what an
opinion is about. Targets are vital for determining
stances. Opinions by themselves may not be as in-
formative as the combination of opinions and tar-
gets. For instance, in Example 1 the writer supports
an against-healthcare stance using a negative senti-
ment. There is a negative sentiment in the example
below (Example 4) too. However, in this case the
writer supports a for-healthcare stance. It is by un-
derstanding what the opinion is about, that we can
recognize the stance.
(4) Oh, the answer is GREEDY insurance com-
panies that buy your Rep &amp; Senator. [side: for
healthcare]
We also observed that targets, or in general items
that participants from either side choose to speak
about, by themselves may not be as informative as
opinions in conjunction with the targets. For in-
stance, Examples 1 and 3 both speak about the gov-
ernment but belong to opposing sides. Understand-
ing that the former example is negative toward the
government and the latter has a positive arguing
about the government helps us to understand the cor-
responding stances.
Examples 1, 3 and 4 also illustrate that there
are a variety of ways in which people support
their stances. The writers express opinions about
government, the initiator of healthcare and insur-
ance companies, and the parties hurt by government
run healthcare. Participants group government and
healthcare as essentially the same concept, while
they consider healthcare and insurance companies
as alternative concepts. By expressing opinions re-
garding a variety of items that are same or alternative
to main topic (healthcare, in these examples), they
are, in effect, revealing their stance (Somasundaran
et al., 2008).
</bodyText>
<sectionHeader confidence="0.878317" genericHeader="method">
3 Constructing an Arguing Lexicon
</sectionHeader>
<bodyText confidence="0.9999614">
Arguing is a relatively less explored category in sub-
jectivity. Due to this, there are no available lexicons
with arguing terms (clues). However, the MPQA
corpus (Version 2) is annotated with arguing sub-
jectivity (Wilson and Wiebe, 2005; Wilson, 2007).
There are two arguing categories: positive arguing
and negative arguing. We use this corpus to gener-
ate a ngram (up to trigram) arguing lexicon.
The examples below illustrate MPQA arguing an-
notations. Examples 5 and 7 illustrate positive argu-
</bodyText>
<page confidence="0.992136">
118
</page>
<bodyText confidence="0.9022875">
ing annotations and Example 6 illustrates negative
arguing.
</bodyText>
<listItem confidence="0.955453666666667">
(5) Iran insists its nuclear program is purely
for peaceful purposes.
(6) Officials in Panama denied that Mr. Chavez
or any of his family members had asked for
asylum.
(7) Putin remarked that the events in Chechnia
“could be interpreted only in the context
of the struggle against international terror-
ism.”
</listItem>
<bodyText confidence="0.907897413043479">
Inspection of these text spans reveal that arguing an-
notations can be considered to be comprised of two
pieces of information. The first piece of information
is what we call the arguing trigger expression. The
trigger is an indicator that an arguing is taking place,
and is the primary component that anchors the argu-
ing annotation. The second component is the ex-
pression that reveals more about the argument, and
can be considered to be secondary for the purposes
of detecting arguing. In Example 5, “insists”, by it-
self, conveys enough information to indicate that the
speaker is arguing. It is quite likely that a sentence
of the form “X insists Y” is going to be an arguing
sentence. Thus, “insists” is an arguing trigger.
Similarly, in Example 6, we see two arguing trig-
gers: “denied” and “denied that”. Each of these can
independently act as arguing triggers (For example,
in the constructs “X denied that Y” and “X denied
Y”). Finally, in Example 7, the arguing annotation
has the following independent trigger expressions
“could be * only”, “could be” and “could”. The wild
card in the first trigger expression indicates that there
could be zero or more words in its place.
Note that MPQA annotations do not provide this
primary/secondary distinction. We make this dis-
tinction to create general arguing clues such as “in-
sist”. Table 3 lists examples of arguing annotations
from the MPQA corpus and what we consider as
their arguing trigger expressions.
Notice that trigger words are generally at the be-
ginning of the annotations. Most of these are uni-
grams, bigrams or trigrams (though it is possible for
these to be longer, as seen in Example 7). Thus, we
can create a lexicon of arguing trigger expressions
Positive arguing annotations Trigger Expr.
actually reflects Israel’s determination ... actually
am convinced that improving ... am convinced
bear witness that Mohamed is his ... bear witness
can only rise to meet it by making ... can only
has always seen usama bin ladin’s ... has always
Negative Arguing Annotations Trigger Expr.
certainly not a foregone conclusion certainly not
has never been any clearer has never
not too cool for kids not too
rather than issuing a letter of ... rather than
there is no explanation for there is no
</bodyText>
<tableCaption confidence="0.979664">
Table 2: Arguing annotations from the MPQA corpus and
their corresponding trigger expressions
</tableCaption>
<bodyText confidence="0.499961">
by extracting the starting n-grams from the MPQA
annotations. The process of creating the lexicon is
as follows:
</bodyText>
<listItem confidence="0.9960038">
1. Generate a candidate Set from the annotations
in the corpus. Three candidates are extracted
from the stemmed version of each annotation:
the first word, the bigram starting at the first
word, and the trigram starting at the first word.
For example, if the annotation is “can only rise
to meet it by making some radical changes”,
the following candidates are extracted from it:
“can”, “can only” and “can only rise”.
2. Remove the candidates that are present in the
sentiment lexicon from (Wilson et al., 2005) (as
these are already accounted for in previous re-
search). For example, “actually”, which is a
trigger word in Table 3, is a neutral subjectivity
clue in the lexicon.
3. For each candidate in the candidate Set,
find the likelihood that it is a reliable indi-
cator of positive or negative arguing in the
MPQA corpus. These are likelihoods of the
form: P(positive arguing|candidate) _
</listItem>
<construct confidence="0.6691002">
#candidate is in a positive arguing span
#candidate is in the corpus
and P(negative arguing|candidate) _
#candidate is in a negative arguing span
#candidate is in the corpus
</construct>
<listItem confidence="0.771438666666667">
4. Make a lexicon entry for each candidate con-
sisting of the stemmed text and the two proba-
bilities described above.
</listItem>
<bodyText confidence="0.9906175">
This process results in an arguing lexicon
with 3762 entries, where 3094 entries have
</bodyText>
<page confidence="0.994144">
119
</page>
<table confidence="0.557661">
P(positive arguing|candidate) &gt; 0; and 668
entries have P(negative arguing|candidate) &gt; 0.
</table>
<tableCaption confidence="0.9627195">
Table 3 lists select interesting expressions from the
arguing lexicon.
</tableCaption>
<figureCaption confidence="0.556326916666667">
Entries indicative of Positive Arguing
be important to, would be better, would need to, be just the, be
the true, my opinion, the contrast, show the, prove to be, only
if, on the verge, ought to, be most, youve get to, render, man-
ifestation, ironically, once and for, no surprise, overwhelming
evidence, its clear, its clear that, it be evident, it be extremely,
it be quite, it would therefore
Entries indicative of Negative Arguing
be not simply, simply a, but have not, can not imagine, we dont
need, we can not do, threat against, ought not, nor will, never
again, far from be, would never, not completely, nothing will,
inaccurate and, inaccurate and, find no, no time, deny that
</figureCaption>
<tableCaption confidence="0.996975">
Table 3: Examples of positive argu-
</tableCaption>
<construct confidence="0.9673544">
ing (P(positive arguing|candidate) &gt;
P(negative arguing|candidate)) and negative
arguing (P(negative arguing|candidate) &gt;
P(positive arguing|candidate))from the arguing
lexicon
</construct>
<sectionHeader confidence="0.999231" genericHeader="method">
4 Features for Stance Classification
</sectionHeader>
<bodyText confidence="0.999799">
We construct opinion target pair features, which are
units that capture the combined information about
opinions and targets. These are encoded as binary
features into a standard machine learning algorithm.
</bodyText>
<subsectionHeader confidence="0.98412">
4.1 Arguing-based Features
</subsectionHeader>
<bodyText confidence="0.999853857142857">
We create arguing features primarily from our ar-
guing lexicon. We construct additional arguing fea-
tures using modal verbs and syntactic rules. The lat-
ter are motivated by the fact that modal verbs such
as “must”, “should” and “ought” are clear cases of
arguing, and are often involved in simple syntactic
patterns with clear targets.
</bodyText>
<subsectionHeader confidence="0.47759">
4.1.1 Arguing-lexicon Features
</subsectionHeader>
<bodyText confidence="0.999882521739131">
The process for creating features for a post using
the arguing lexicon is simple. For each sentence in
the post, we first determine if it contains a positive or
negative arguing expression by looking for trigram,
bigram and unigram matches (in that order) with the
arguing lexicon. We prevent the same text span from
matching twice – once a trigram match is found, a
substring bigram (or unigram) match with the same
text span is avoided. If there are multiple arguing ex-
pression matches found within a sentence, we deter-
mine the most prominent arguing polarity by adding
up the positive arguing probabilities and negative ar-
guing probabilities (provided in the lexicon) of all
the individual expressions.
Once the prominent arguing polarity is deter-
mined for a sentence, the prefix ap (arguing positive)
or an (arguing negative) is attached to all the content
words in that sentence to construct opinion-target
features. In essence, all content words (nouns, verbs,
adjectives and adverbs) in the sentence are assumed
to be the target. Arguing features are denoted as ap-
target (positive arguing toward target) and an-target
(negative arguing toward target).
</bodyText>
<subsubsectionHeader confidence="0.594596">
4.1.2 Modal Verb Features for Arguing
</subsubsectionHeader>
<bodyText confidence="0.999962647058824">
Modals words such as “must” and “should” are
usually good indicators of arguing. This is a small
closed set. Also, the target (what the arguing is
about) is syntactically associated with the modal
word, which means it can be relatively accurately
extracted by using a small set of syntactic rules.
For every modal detected, three features are cre-
ated by combining the modal word with its subject
and object. Note that all the different modals are
replaced by “should” while creating features. This
helps to create more general features. For exam-
ple, given a sentence “They must be available to
all people”, the method creates three features “they
should”, “should available” and “they should avail-
able”. These patterns are created independently of
the arguing lexicon matches, and added to the fea-
ture set for the post.
</bodyText>
<subsectionHeader confidence="0.98522">
4.2 Sentiment-based Features
</subsectionHeader>
<bodyText confidence="0.999978916666667">
Sentiment-based features are created independent of
arguing features. In order to detect sentiment opin-
ions, we use a sentiment lexicon (Wilson et al.,
2005). In addition to positive (+) and negative (−)
words, this lexicon also contains subjective words
that are themselves neutral (=) with respect to po-
larity. Examples of neutral entries are “absolutely”,
“amplify”, “believe”, and “think”.
We find the sentiment polarity of the entire sen-
tence and assign this polarity to each content word in
the sentence (denoted, for example, as target+). In
order to detect the sentence polarity, we use the Vote
</bodyText>
<page confidence="0.901698">
120
</page>
<bodyText confidence="0.992072210526316">
and Flip algorithm from Choi and Cardie (2009). racy metric.
This algorithm essentially counts the number of pos- 5.1 Results
itive, negative and neutral lexicon hits in a given ex- Table 4 shows the accuracy averaged over 10 fold
pression and accounts for negator words. The algo- cross-validation experiments for each domain. The
rithm is used as is, except for the default polarity first row (Overall) reports the accuracy calculated
assignment (as we do not know the most prominent over all 2232 posts in the data.
polarity in the corpus). Note that the Vote and Flip Overall, we notice that all the supervised systems
algorithm has been developed for expressions but we perform better than the distribution-based baseline.
employ it on sentences. Once the polarity of a sen- Observe that Unigram has a better performance than
tence is determined, we create sentiment features for Sentiment. The good performance of Unigram indi-
the sentence. This is done for all sentences in the cates that what participants choose to speak about is
post. a good indicator of ideological stance taking. This
5 Experiments result confirms previous researchers’ intuition that,
Experiments are carried out on debate posts from the in general, political orientation is a function of “au-
following four domains: Gun Rights, Gay Rights, thors’ attitudes over multiple issues rather than pos-
Abortion, and Creationism. For each domain, a cor- itive or negative sentiment with respect to a sin-
pus with equal class distribution is created as fol- gle issue” (Pang and Lee, 2008). Nevertheless, the
lows: we merge all debates and sample instances Arg+Sent system that uses both arguing and senti-
(posts) from the majority class to obtain equal num- ment features outperforms Unigram.
bers of instances for each stance. This gives us a We performed McNemar’s test to measure the dif-
total of 2232 posts in the corpus: 306 posts for the ference in system behaviors. The test was performed
Gun Rights domain, 846 posts for the Gay Rights on all pairs of supervised systems using all 2232
domain, 550 posts for the Abortion domain and 530 posts. The results show that there is a significant dif-
posts for the Creationism domain. ference between the classification behavior of Uni-
Our first baseline is a distribution-based baseline, gram and Arg+Sent systems (p &lt; 0.05). The dif-
which has an accuracy of 50%. We also construct ference between classifications of Unigram and Ar-
Unigram, a system based on unigram content infor- guing approaches significance (p &lt; 0.1). There is
mation, but no explicit opinion information. Un- no significant difference in the behaviors of all other
igrams are reliable for stance classification in po- system pairs.
litical domains (as seen in (Lin et al., 2006; Kim Moving on to detailed performance in each do-
and Hovy, 2007)). Intuitively, evoking a particular main, we see that Unigram outperforms Sentiment
topic can be indicative of a stance. For example, for all domains. Arguing and Arg+Sent outperform
a participant who chooses to speak about “child” Unigram for three domains (Guns, Gay Rights and
and “life” in an abortion debate is more likely from Abortion), while the situation is reversed for one do-
an against-abortion side, while someone speaking main (Creationism). We carried out separate t-tests
about “woman”, “rape” and “choice” is more likely for each domain, using the results from each test fold
from a for-abortion stance. as a data point. Our results indicate that the perfor-
We construct three systems that use opinion in- mance of Sentiment is significantly different from
</bodyText>
<table confidence="0.9023568">
formation: The Sentiment system that uses only the all other systems for all domains. However there is
sentiment features described in Section 4.2, the Ar- no significant difference between the performance of
guing system that uses only arguing features con- the remaining systems.
structed in Section 4.1, and the Arg+Sent system 5.2 Analysis
that uses both sentiment and arguing features. On manual inspection of the top features used by
All systems are implemented using a standard im- the classifiers for discriminating the stances, we
plementation of SVM in the Weka toolkit (Hall et found that there is an overlap between the content
al., 2009). We measure performance using the accu- words used by Unigram, Arg+Sent and Arguing. For
121
Domain (#posts) Distribution Unigram Sentiment Arguing Arg+Sent
Overall (2232) 50 62.50 55.02 62.59 63.93
Guns Rights (306) 50 66.67 58.82 69.28 70.59
Gay Rights (846) 50 61.70 52.84 62.05 63.71
Abortion (550) 50 59.1 54.73 59.46 60.55
Creationism (530) 50 64.91 56.60 62.83 63.96
</table>
<tableCaption confidence="0.999858">
Table 4: Accuracy of the different systems
</tableCaption>
<bodyText confidence="0.999980698113208">
example, in the Gay Rights domain, “understand”
and “equal” are amongst the top features in Uni-
gram, while “ap-understand” (positive arguing for
“understand”) and “ap-equal” are top features for
Arg+Sent.
However, we believe that Arg+Sent makes finer
and more insightful distinctions based on polarity of
opinions toward the same set of words. Table 5 lists
some interesting features in the Gay Rights domain
for Unigram and Arg+Sent. Depending on whether
positive or negative attribute weights were assigned
by the SVM learner, the features are either indicative
of for-gay rights or against-gay rights. Even though
the features for Unigram are intuitive, it is not ev-
ident if a word is evoked as, for example, a pitch,
concern, or denial. Also, we do not see a clear sep-
aration of the terms (for e.g., “bible” is an indicator
for against-gay rights while “christianity” is an indi-
cator for for-gay rights)
The arguing features from Arg+Sent seem to
be relatively more informative – positive arguing
about “christianity”, “corinthians”, “mormonism”
and “bible” are all indicative of against-gay rights
stance. These are indeed beliefs and concerns that
shape an against-gay rights stance. On the other
hand, negative arguings with these same words de-
note a for-gay rights stance. Presumably, these oc-
cur in refutations of the concerns influencing the op-
posite side. Likewise, the appeal for equal rights
for gays is captured positive arguing about “liberty”,
“independence”, “pursuit” and “suffrage”.
Interestingly, we found that our features also cap-
ture the ideas of opinion variety and same and alter-
native targets as defined in previous research (So-
masundaran et al., 2008) – in Table 5, items that
are similar (e.g., “christianity” and “corinthians”)
have similar opinions toward them for a given stance
(for e.g., ap-christianity and ap-corinthians belong
to against-gay rights stance while an-christianity and
an-corinthians belong to for-gay rights stance). Ad-
ditionally, items that are alternatives (e.g. “gay” and
“heterosexuality”) have opposite polarities associ-
ated with them for a given stance, that is, positive
arguing for “heterosexuality” and negative arguing
for “gay” reveal the the same stance.
In general, unigram features associate the choice
of topics with the stances, while the arguing features
can capture the concerns, defenses, appeals or de-
nials that signify each side (though we do not ex-
plicitly encode these fine-grained distinctions in this
work). Interestingly, we found that sentiment fea-
tures in Arg+Sent are not as informative as the argu-
ing features discussed above.
</bodyText>
<sectionHeader confidence="0.99994" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999911047619048">
Generally, research in identifying political view-
points has employed information from words in the
document (Malouf and Mullen, 2008; Mullen and
Malouf, 2006; Grefenstette et al., 2004; Laver et al.,
2003; Martin and Vanberg, 2008; Lin et al., 2006;
Lin, 2006). Specifically, Lin et al. observe that peo-
ple from opposing perspectives seem to use words
in differing frequencies. On similar lines, Kim and
Hovy (2007) use unigrams, bigrams and trigrams for
election prediction from forum posts. In contrast,
our work specifically employs sentiment-based and
arguing-based features to perform stance classifica-
tion in political debates. Our experiments are fo-
cused on determining how different opinion expres-
sions reinforce an overall political stance. Our re-
sults indicate that while unigram information is re-
liable, further improvements can be achieved in cer-
tain domains using our opinion-based approach. Our
work is also complementary to that by Greene and
Resnik (2009), which focuses on syntactic packag-
ing for recognizing perspectives.
</bodyText>
<page confidence="0.988442">
122
</page>
<table confidence="0.998641533333333">
For Gay Rights Against Gay Rights
Unigram Features
constitution, fundamental, rights, suffrage, pursuit, discrimina- pervert, hormone, liberty, fidelity, naval, retarded, orientation, pri-
tion, government, happiness, shame, wed, gay, heterosexual- vate, partner, kingdom, bible, sin, bigot
ity, chromosome, evolution, genetic, christianity, mormonism,
corinthians, procreate, adopt
Arguing Features from Arg+Sent
ap-constitution, ap-fundamental, ap-rights, ap-hormone, an-constitution, an-fundamental, an-rights, an-hormone,
ap-liberty, ap-independence, ap-suffrage, ap-pursuit, ap- an-liberty, an-independence, an-suffrage, an-pursuit, an-
discrimination, an-government, ap-fidelity, ap-happiness, discrimination, ap-government, an-fidelity, an-happiness,
an-pervert, an-naval, an-retarded, an-orientation, an-shame, ap-pervert, ap-naval, ap-retarded, ap-orientation, ap-shame,
ap-private, ap-wed, ap-gay, an-heterosexuality, ap-partner, an-private, an-wed, an-gay, ap-heterosexuality, an-partner,
ap-chromosome, ap-evolution, ap-genetic, an-kingdom, an- an-chromosome, an-evolution, an-genetic, ap-kingdom, ap-
christianity, an-mormonism, an-corinthians, an-bible, an-sin, christianity, ap-mormonism, ap-corinthians, ap-bible, ap-sin,
an-bigot, an-procreate, ap-adopt, ap-bigot, ap-procreate, an-adopt
</table>
<tableCaption confidence="0.999743">
Table 5: Examples of features associated with the stances in Gay Rights domain
</tableCaption>
<bodyText confidence="0.99982637037037">
Discourse-level participant relation, that is,
whether participants agree/disagree has been found
useful for determining political side-taking (Thomas
et al., 2006; Bansal et al., 2008; Agrawal et
al., 2003; Malouf and Mullen, 2008). Agree-
ment/disagreement relations are not the main focus
of our work. Other work in the area of polarizing po-
litical discourse analyze co-citations (Efron, 2004)
and linking patterns (Adamic and Glance, 2005). In
contrast, our focus is on document content and opin-
ion expressions.
Somasundaran et al. (2007b) have noted the use-
fulness of the arguing category for opinion QA. Our
tasks are different; they use arguing to retrieve rele-
vant answers, but not distinguish stances. Our work
is also different from related work in the domain of
product debates (Somasundaran and Wiebe, 2009)
in terms of the methodology.
Wilson (2007) manually adds positive/negative
arguing information to entries in a sentiment lexi-
con from (Wilson et al., 2005) and uses these as ar-
guing features. Our arguing trigger expressions are
separate from the sentiment lexicon entries and are
derived from a corpus. Our n-gram trigger expres-
sions are also different from manually created regu-
lar expression-based arguing lexicon for speech data
(Somasundaran et al., 2007a).
</bodyText>
<sectionHeader confidence="0.999363" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999932363636363">
In this paper, we explore recognizing stances in ide-
ological on-line debates. We created an arguing lex-
icon from the MPQA annotations in order to recog-
nize arguing, a prominent type of linguistic subjec-
tivity in ideological stance taking. We observed that
opinions or targets in isolation are not as informative
as their combination. Thus, we constructed opinion
target pair features to capture this information.
We performed supervised learning experiments
on four different domains. Our results show that
both unigram-based and opinion-based systems per-
form better than baseline methods. We found that,
even though our sentiment-based system is able to
perform better than the distribution-based baseline,
it does not perform at par with the unigram system.
However, overall, our arguing-based system does as
well as the unigram-based system, and our system
that uses both arguing and sentiment features obtains
further improvement. Our feature analysis suggests
that arguing features are more insightful than uni-
gram features, as they make finer distinctions that
reveal the underlying ideologies.
</bodyText>
<sectionHeader confidence="0.997495" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.859689666666667">
Lada A. Adamic and Natalie Glance. 2005. The political
blogosphere and the 2004 u.s. election: Divided they
blog. In LinkKDD.
Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan
Srikant, and Yirong Xu. 2003. Mining newsgroups
using networks arising from social behavior. In WWW.
Mohit Bansal, Claire Cardie, and Lillian Lee. 2008.
The power of negative thinking: Exploiting label dis-
agreement in the min-cut classification framework. In
</reference>
<page confidence="0.996221">
123
</page>
<note confidence="0.5473675">
Proceedings of the 22nd International Conference on
Computational Linguistics (COLING-2008).
</note>
<reference confidence="0.999145118811881">
Yejin Choi and Claire Cardie. 2009. Adapting a polarity
lexicon using integer linear programming for domain-
specific sentiment classification. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing, pages 590–598, Singapore,
August. Association for Computational Linguistics.
Miles Efron. 2004. Cultural orientation: Classifying
subjective documents by cocitation analysis. In AAAI
Fall Symposium on Style and Meaning in Language,
Art, and Music.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 503–511, Boulder, Colorado, June. Association
for Computational Linguistics.
Gregory Grefenstette, Yan Qu, James G. Shanahan, and
David A. Evans. 2004. Coupling niche browsers and
affect analysis for an opinion mining application. In
Proceeding of RIAO-04, Avignon, FR.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update. In
SIGKDD Explorations, Volume 11, Issue 1.
Soo-Min Kim and Eduard Hovy. 2007. Crystal: Ana-
lyzing predictive opinions on the web. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
1056–1064.
Michael Laver, Kenneth Benoit, and John Garry. 2003.
Extracting policy positions from political texts using
words as data. American Political Science Review,
97(2):311–331.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you
on? Identifying perspectives at the document and sen-
tence levels. In Proceedings of the 10th Conference on
Computational Natural Language Learning (CoNLL-
2006), pages 109–116, New York, New York.
Wei-Hao Lin. 2006. Identifying perspectives at the doc-
ument and sentence levels using statistical models. In
Proceedings of the Human Language Technology Con-
ference of the NAACL, Companion Volume: Doctoral
Consortium, pages 227–230, New York City, USA,
June. Association for Computational Linguistics.
Robert Malouf and Tony Mullen. 2008. Taking sides:
Graph-based user classification for informal online po-
litical discourse. Internet Research, 18(2).
Lanny W. Martin and Georg Vanberg. 2008. A ro-
bust transformation procedure for interpreting political
text. Political Analysis, 16(1):93–100.
Tony Mullen and Robert Malouf. 2006. A preliminary
investigation into sentiment analysis of informal po-
litical discourse. In AAAI 2006 Spring Symposium
on Computational Approaches to Analysing Weblogs
(AAAI-CAAW 2006).
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, Vol. 2(1-2):pp. 1–135.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings
of the Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
226–234, Suntec, Singapore, August. Association for
Computational Linguistics.
Swapna Somasundaran, Josef Ruppenhofer, and Janyce
Wiebe. 2007a. Detecting arguing and sentiment in
meetings. In SIGdial Workshop on Discourse and Di-
alogue, Antwerp, Belgium, September.
Swapna Somasundaran, Theresa Wilson, Janyce Wiebe,
and Veselin Stoyanov. 2007b. Qa with attitude: Ex-
ploiting opinion type analysis for improving question
answering in on-line discussions and the news. In In-
ternational Conference on Weblogs and Social Media,
Boulder, CO.
Swapna Somasundaran, Janyce Wiebe, and Josef Rup-
penhofer. 2008. Discourse level opinion interpreta-
tion. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 801–808, Manchester, UK, August.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from con-
gressional floor-debate transcripts. In Proceedings of
the 2006 Conference on Empirical Methods in Natural
Language Processing, pages 327–335, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Theresa Wilson and Janyce Wiebe. 2005. Annotating
attributions and private states. In Proceedings of ACL
Workshop on Frontiers in Corpus Annotation II: Pie in
the Sky.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In hltemnlp2005, pages 347–354,
Vancouver, Canada.
Theresa Wilson. 2007. Fine-grained Subjectivity and
Sentiment Analysis: Recognizing the Intensity, Polar-
ity, and Attitudes of private states. Ph.D. thesis, Intel-
ligent Systems Program, University of Pittsburgh.
</reference>
<page confidence="0.998328">
124
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.268684">
<title confidence="0.999752">Recognizing Stances in Ideological On-Line Debates</title>
<author confidence="0.907912">Swapna</author>
<affiliation confidence="0.9998955">Dept. of Computer University of</affiliation>
<address confidence="0.766072">Pittsburgh, PA</address>
<email confidence="0.99923">swapna@cs.pitt.edu</email>
<author confidence="0.5109">Janyce</author>
<affiliation confidence="0.998482333333333">Dept. of Computer Science The Intelligent Systems University of</affiliation>
<address confidence="0.776535">Pittsburgh, PA</address>
<email confidence="0.999615">wiebe@cs.pitt.edu</email>
<abstract confidence="0.998476714285714">This work explores the utility of sentiment and arguing opinions for classifying stances in ideological debates. In order to capture arguing opinions in ideological stance taking, we construct an arguing lexicon automatically from a manually annotated corpus. We build supervised systems employing sentiment and arguing opinions and their targets as features. Our systems perform substantially better than a distribution-based baseline. Additionally, by employing both types of opinion features, we are able to perform better than a unigrambased system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lada A Adamic</author>
<author>Natalie Glance</author>
</authors>
<title>The political blogosphere and the</title>
<date>2005</date>
<booktitle>In LinkKDD.</booktitle>
<contexts>
<context position="30337" citStr="Adamic and Glance, 2005" startWordPosition="4769" endWordPosition="4772">m, ap-corinthians, ap-bible, ap-sin, an-bigot, an-procreate, ap-adopt, ap-bigot, ap-procreate, an-adopt Table 5: Examples of features associated with the stances in Gay Rights domain Discourse-level participant relation, that is, whether participants agree/disagree has been found useful for determining political side-taking (Thomas et al., 2006; Bansal et al., 2008; Agrawal et al., 2003; Malouf and Mullen, 2008). Agreement/disagreement relations are not the main focus of our work. Other work in the area of polarizing political discourse analyze co-citations (Efron, 2004) and linking patterns (Adamic and Glance, 2005). In contrast, our focus is on document content and opinion expressions. Somasundaran et al. (2007b) have noted the usefulness of the arguing category for opinion QA. Our tasks are different; they use arguing to retrieve relevant answers, but not distinguish stances. Our work is also different from related work in the domain of product debates (Somasundaran and Wiebe, 2009) in terms of the methodology. Wilson (2007) manually adds positive/negative arguing information to entries in a sentiment lexicon from (Wilson et al., 2005) and uses these as arguing features. Our arguing trigger expressions</context>
</contexts>
<marker>Adamic, Glance, 2005</marker>
<rawString>Lada A. Adamic and Natalie Glance. 2005. The political blogosphere and the 2004 u.s. election: Divided they blog. In LinkKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rakesh Agrawal</author>
<author>Sridhar Rajagopalan</author>
<author>Ramakrishnan Srikant</author>
<author>Yirong Xu</author>
</authors>
<title>Mining newsgroups using networks arising from social behavior.</title>
<date>2003</date>
<booktitle>In WWW.</booktitle>
<contexts>
<context position="30102" citStr="Agrawal et al., 2003" startWordPosition="4733" endWordPosition="4736">ap-heterosexuality, an-partner, ap-chromosome, ap-evolution, ap-genetic, an-kingdom, an- an-chromosome, an-evolution, an-genetic, ap-kingdom, apchristianity, an-mormonism, an-corinthians, an-bible, an-sin, christianity, ap-mormonism, ap-corinthians, ap-bible, ap-sin, an-bigot, an-procreate, ap-adopt, ap-bigot, ap-procreate, an-adopt Table 5: Examples of features associated with the stances in Gay Rights domain Discourse-level participant relation, that is, whether participants agree/disagree has been found useful for determining political side-taking (Thomas et al., 2006; Bansal et al., 2008; Agrawal et al., 2003; Malouf and Mullen, 2008). Agreement/disagreement relations are not the main focus of our work. Other work in the area of polarizing political discourse analyze co-citations (Efron, 2004) and linking patterns (Adamic and Glance, 2005). In contrast, our focus is on document content and opinion expressions. Somasundaran et al. (2007b) have noted the usefulness of the arguing category for opinion QA. Our tasks are different; they use arguing to retrieve relevant answers, but not distinguish stances. Our work is also different from related work in the domain of product debates (Somasundaran and W</context>
</contexts>
<marker>Agrawal, Rajagopalan, Srikant, Xu, 2003</marker>
<rawString>Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan Srikant, and Yirong Xu. 2003. Mining newsgroups using networks arising from social behavior. In WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Claire Cardie</author>
<author>Lillian Lee</author>
</authors>
<title>The power of negative thinking: Exploiting label disagreement in the min-cut classification framework. In</title>
<date>2008</date>
<contexts>
<context position="30080" citStr="Bansal et al., 2008" startWordPosition="4729" endWordPosition="4732">ate, an-wed, an-gay, ap-heterosexuality, an-partner, ap-chromosome, ap-evolution, ap-genetic, an-kingdom, an- an-chromosome, an-evolution, an-genetic, ap-kingdom, apchristianity, an-mormonism, an-corinthians, an-bible, an-sin, christianity, ap-mormonism, ap-corinthians, ap-bible, ap-sin, an-bigot, an-procreate, ap-adopt, ap-bigot, ap-procreate, an-adopt Table 5: Examples of features associated with the stances in Gay Rights domain Discourse-level participant relation, that is, whether participants agree/disagree has been found useful for determining political side-taking (Thomas et al., 2006; Bansal et al., 2008; Agrawal et al., 2003; Malouf and Mullen, 2008). Agreement/disagreement relations are not the main focus of our work. Other work in the area of polarizing political discourse analyze co-citations (Efron, 2004) and linking patterns (Adamic and Glance, 2005). In contrast, our focus is on document content and opinion expressions. Somasundaran et al. (2007b) have noted the usefulness of the arguing category for opinion QA. Our tasks are different; they use arguing to retrieve relevant answers, but not distinguish stances. Our work is also different from related work in the domain of product debat</context>
</contexts>
<marker>Bansal, Cardie, Lee, 2008</marker>
<rawString>Mohit Bansal, Claire Cardie, and Lillian Lee. 2008. The power of negative thinking: Exploiting label disagreement in the min-cut classification framework. In</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Adapting a polarity lexicon using integer linear programming for domainspecific sentiment classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>590--598</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="20231" citStr="Choi and Cardie (2009)" startWordPosition="3259" endWordPosition="3262">ated independent of arguing features. In order to detect sentiment opinions, we use a sentiment lexicon (Wilson et al., 2005). In addition to positive (+) and negative (−) words, this lexicon also contains subjective words that are themselves neutral (=) with respect to polarity. Examples of neutral entries are “absolutely”, “amplify”, “believe”, and “think”. We find the sentiment polarity of the entire sentence and assign this polarity to each content word in the sentence (denoted, for example, as target+). In order to detect the sentence polarity, we use the Vote 120 and Flip algorithm from Choi and Cardie (2009). racy metric. This algorithm essentially counts the number of pos- 5.1 Results itive, negative and neutral lexicon hits in a given ex- Table 4 shows the accuracy averaged over 10 fold pression and accounts for negator words. The algo- cross-validation experiments for each domain. The rithm is used as is, except for the default polarity first row (Overall) reports the accuracy calculated assignment (as we do not know the most prominent over all 2232 posts in the data. polarity in the corpus). Note that the Vote and Flip Overall, we notice that all the supervised systems algorithm has been deve</context>
</contexts>
<marker>Choi, Cardie, 2009</marker>
<rawString>Yejin Choi and Claire Cardie. 2009. Adapting a polarity lexicon using integer linear programming for domainspecific sentiment classification. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 590–598, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miles Efron</author>
</authors>
<title>Cultural orientation: Classifying subjective documents by cocitation analysis.</title>
<date>2004</date>
<booktitle>In AAAI Fall Symposium on Style and Meaning in Language,</booktitle>
<location>Art, and Music.</location>
<contexts>
<context position="30290" citStr="Efron, 2004" startWordPosition="4764" endWordPosition="4765">, an-sin, christianity, ap-mormonism, ap-corinthians, ap-bible, ap-sin, an-bigot, an-procreate, ap-adopt, ap-bigot, ap-procreate, an-adopt Table 5: Examples of features associated with the stances in Gay Rights domain Discourse-level participant relation, that is, whether participants agree/disagree has been found useful for determining political side-taking (Thomas et al., 2006; Bansal et al., 2008; Agrawal et al., 2003; Malouf and Mullen, 2008). Agreement/disagreement relations are not the main focus of our work. Other work in the area of polarizing political discourse analyze co-citations (Efron, 2004) and linking patterns (Adamic and Glance, 2005). In contrast, our focus is on document content and opinion expressions. Somasundaran et al. (2007b) have noted the usefulness of the arguing category for opinion QA. Our tasks are different; they use arguing to retrieve relevant answers, but not distinguish stances. Our work is also different from related work in the domain of product debates (Somasundaran and Wiebe, 2009) in terms of the methodology. Wilson (2007) manually adds positive/negative arguing information to entries in a sentiment lexicon from (Wilson et al., 2005) and uses these as ar</context>
</contexts>
<marker>Efron, 2004</marker>
<rawString>Miles Efron. 2004. Cultural orientation: Classifying subjective documents by cocitation analysis. In AAAI Fall Symposium on Style and Meaning in Language, Art, and Music.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Greene</author>
<author>Philip Resnik</author>
</authors>
<title>More than words: Syntactic packaging and implicit sentiment.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>503--511</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="28445" citStr="Greene and Resnik (2009)" startWordPosition="4562" endWordPosition="4565">ords in differing frequencies. On similar lines, Kim and Hovy (2007) use unigrams, bigrams and trigrams for election prediction from forum posts. In contrast, our work specifically employs sentiment-based and arguing-based features to perform stance classification in political debates. Our experiments are focused on determining how different opinion expressions reinforce an overall political stance. Our results indicate that while unigram information is reliable, further improvements can be achieved in certain domains using our opinion-based approach. Our work is also complementary to that by Greene and Resnik (2009), which focuses on syntactic packaging for recognizing perspectives. 122 For Gay Rights Against Gay Rights Unigram Features constitution, fundamental, rights, suffrage, pursuit, discrimina- pervert, hormone, liberty, fidelity, naval, retarded, orientation, prition, government, happiness, shame, wed, gay, heterosexual- vate, partner, kingdom, bible, sin, bigot ity, chromosome, evolution, genetic, christianity, mormonism, corinthians, procreate, adopt Arguing Features from Arg+Sent ap-constitution, ap-fundamental, ap-rights, ap-hormone, an-constitution, an-fundamental, an-rights, an-hormone, ap-</context>
</contexts>
<marker>Greene, Resnik, 2009</marker>
<rawString>Stephan Greene and Philip Resnik. 2009. More than words: Syntactic packaging and implicit sentiment. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 503–511, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
<author>Yan Qu</author>
<author>James G Shanahan</author>
<author>David A Evans</author>
</authors>
<title>Coupling niche browsers and affect analysis for an opinion mining application.</title>
<date>2004</date>
<booktitle>In Proceeding of RIAO-04,</booktitle>
<location>Avignon, FR.</location>
<contexts>
<context position="27658" citStr="Grefenstette et al., 2004" startWordPosition="4441" endWordPosition="4444">r “gay” reveal the the same stance. In general, unigram features associate the choice of topics with the stances, while the arguing features can capture the concerns, defenses, appeals or denials that signify each side (though we do not explicitly encode these fine-grained distinctions in this work). Interestingly, we found that sentiment features in Arg+Sent are not as informative as the arguing features discussed above. 6 Related Work Generally, research in identifying political viewpoints has employed information from words in the document (Malouf and Mullen, 2008; Mullen and Malouf, 2006; Grefenstette et al., 2004; Laver et al., 2003; Martin and Vanberg, 2008; Lin et al., 2006; Lin, 2006). Specifically, Lin et al. observe that people from opposing perspectives seem to use words in differing frequencies. On similar lines, Kim and Hovy (2007) use unigrams, bigrams and trigrams for election prediction from forum posts. In contrast, our work specifically employs sentiment-based and arguing-based features to perform stance classification in political debates. Our experiments are focused on determining how different opinion expressions reinforce an overall political stance. Our results indicate that while un</context>
</contexts>
<marker>Grefenstette, Qu, Shanahan, Evans, 2004</marker>
<rawString>Gregory Grefenstette, Yan Qu, James G. Shanahan, and David A. Evans. 2004. Coupling niche browsers and affect analysis for an opinion mining application. In Proceeding of RIAO-04, Avignon, FR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: An update.</title>
<date>2009</date>
<booktitle>In SIGKDD Explorations, Volume 11, Issue 1.</booktitle>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The weka data mining software: An update. In SIGKDD Explorations, Volume 11, Issue 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Crystal: Analyzing predictive opinions on the web.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>1056--1064</pages>
<contexts>
<context position="27889" citStr="Kim and Hovy (2007)" startWordPosition="4480" endWordPosition="4483">explicitly encode these fine-grained distinctions in this work). Interestingly, we found that sentiment features in Arg+Sent are not as informative as the arguing features discussed above. 6 Related Work Generally, research in identifying political viewpoints has employed information from words in the document (Malouf and Mullen, 2008; Mullen and Malouf, 2006; Grefenstette et al., 2004; Laver et al., 2003; Martin and Vanberg, 2008; Lin et al., 2006; Lin, 2006). Specifically, Lin et al. observe that people from opposing perspectives seem to use words in differing frequencies. On similar lines, Kim and Hovy (2007) use unigrams, bigrams and trigrams for election prediction from forum posts. In contrast, our work specifically employs sentiment-based and arguing-based features to perform stance classification in political debates. Our experiments are focused on determining how different opinion expressions reinforce an overall political stance. Our results indicate that while unigram information is reliable, further improvements can be achieved in certain domains using our opinion-based approach. Our work is also complementary to that by Greene and Resnik (2009), which focuses on syntactic packaging for r</context>
</contexts>
<marker>Kim, Hovy, 2007</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2007. Crystal: Analyzing predictive opinions on the web. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 1056–1064.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Laver</author>
<author>Kenneth Benoit</author>
<author>John Garry</author>
</authors>
<title>Extracting policy positions from political texts using words as data.</title>
<date>2003</date>
<journal>American Political Science Review,</journal>
<volume>97</volume>
<issue>2</issue>
<contexts>
<context position="27678" citStr="Laver et al., 2003" startWordPosition="4445" endWordPosition="4448"> stance. In general, unigram features associate the choice of topics with the stances, while the arguing features can capture the concerns, defenses, appeals or denials that signify each side (though we do not explicitly encode these fine-grained distinctions in this work). Interestingly, we found that sentiment features in Arg+Sent are not as informative as the arguing features discussed above. 6 Related Work Generally, research in identifying political viewpoints has employed information from words in the document (Malouf and Mullen, 2008; Mullen and Malouf, 2006; Grefenstette et al., 2004; Laver et al., 2003; Martin and Vanberg, 2008; Lin et al., 2006; Lin, 2006). Specifically, Lin et al. observe that people from opposing perspectives seem to use words in differing frequencies. On similar lines, Kim and Hovy (2007) use unigrams, bigrams and trigrams for election prediction from forum posts. In contrast, our work specifically employs sentiment-based and arguing-based features to perform stance classification in political debates. Our experiments are focused on determining how different opinion expressions reinforce an overall political stance. Our results indicate that while unigram information is</context>
</contexts>
<marker>Laver, Benoit, Garry, 2003</marker>
<rawString>Michael Laver, Kenneth Benoit, and John Garry. 2003. Extracting policy positions from political texts using words as data. American Political Science Review, 97(2):311–331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei-Hao Lin</author>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Alexander Hauptmann</author>
</authors>
<title>Which side are you on? Identifying perspectives at the document and sentence levels.</title>
<date>2006</date>
<booktitle>In Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL2006),</booktitle>
<pages>109--116</pages>
<location>New York, New York.</location>
<contexts>
<context position="22946" citStr="Lin et al., 2006" startWordPosition="3703" endWordPosition="3706">is a significant difposts for the Creationism domain. ference between the classification behavior of UniOur first baseline is a distribution-based baseline, gram and Arg+Sent systems (p &lt; 0.05). The difwhich has an accuracy of 50%. We also construct ference between classifications of Unigram and ArUnigram, a system based on unigram content infor- guing approaches significance (p &lt; 0.1). There is mation, but no explicit opinion information. Un- no significant difference in the behaviors of all other igrams are reliable for stance classification in po- system pairs. litical domains (as seen in (Lin et al., 2006; Kim Moving on to detailed performance in each doand Hovy, 2007)). Intuitively, evoking a particular main, we see that Unigram outperforms Sentiment topic can be indicative of a stance. For example, for all domains. Arguing and Arg+Sent outperform a participant who chooses to speak about “child” Unigram for three domains (Guns, Gay Rights and and “life” in an abortion debate is more likely from Abortion), while the situation is reversed for one doan against-abortion side, while someone speaking main (Creationism). We carried out separate t-tests about “woman”, “rape” and “choice” is more like</context>
<context position="27722" citStr="Lin et al., 2006" startWordPosition="4453" endWordPosition="4456">e the choice of topics with the stances, while the arguing features can capture the concerns, defenses, appeals or denials that signify each side (though we do not explicitly encode these fine-grained distinctions in this work). Interestingly, we found that sentiment features in Arg+Sent are not as informative as the arguing features discussed above. 6 Related Work Generally, research in identifying political viewpoints has employed information from words in the document (Malouf and Mullen, 2008; Mullen and Malouf, 2006; Grefenstette et al., 2004; Laver et al., 2003; Martin and Vanberg, 2008; Lin et al., 2006; Lin, 2006). Specifically, Lin et al. observe that people from opposing perspectives seem to use words in differing frequencies. On similar lines, Kim and Hovy (2007) use unigrams, bigrams and trigrams for election prediction from forum posts. In contrast, our work specifically employs sentiment-based and arguing-based features to perform stance classification in political debates. Our experiments are focused on determining how different opinion expressions reinforce an overall political stance. Our results indicate that while unigram information is reliable, further improvements can be achie</context>
</contexts>
<marker>Lin, Wilson, Wiebe, Hauptmann, 2006</marker>
<rawString>Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and Alexander Hauptmann. 2006. Which side are you on? Identifying perspectives at the document and sentence levels. In Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL2006), pages 109–116, New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei-Hao Lin</author>
</authors>
<title>Identifying perspectives at the document and sentence levels using statistical models.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Doctoral Consortium,</booktitle>
<pages>227--230</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="27734" citStr="Lin, 2006" startWordPosition="4457" endWordPosition="4458">pics with the stances, while the arguing features can capture the concerns, defenses, appeals or denials that signify each side (though we do not explicitly encode these fine-grained distinctions in this work). Interestingly, we found that sentiment features in Arg+Sent are not as informative as the arguing features discussed above. 6 Related Work Generally, research in identifying political viewpoints has employed information from words in the document (Malouf and Mullen, 2008; Mullen and Malouf, 2006; Grefenstette et al., 2004; Laver et al., 2003; Martin and Vanberg, 2008; Lin et al., 2006; Lin, 2006). Specifically, Lin et al. observe that people from opposing perspectives seem to use words in differing frequencies. On similar lines, Kim and Hovy (2007) use unigrams, bigrams and trigrams for election prediction from forum posts. In contrast, our work specifically employs sentiment-based and arguing-based features to perform stance classification in political debates. Our experiments are focused on determining how different opinion expressions reinforce an overall political stance. Our results indicate that while unigram information is reliable, further improvements can be achieved in certa</context>
</contexts>
<marker>Lin, 2006</marker>
<rawString>Wei-Hao Lin. 2006. Identifying perspectives at the document and sentence levels using statistical models. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Doctoral Consortium, pages 227–230, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
<author>Tony Mullen</author>
</authors>
<title>Taking sides: Graph-based user classification for informal online political discourse.</title>
<date>2008</date>
<journal>Internet Research,</journal>
<volume>18</volume>
<issue>2</issue>
<contexts>
<context position="27606" citStr="Malouf and Mullen, 2008" startWordPosition="4433" endWordPosition="4436">uing for “heterosexuality” and negative arguing for “gay” reveal the the same stance. In general, unigram features associate the choice of topics with the stances, while the arguing features can capture the concerns, defenses, appeals or denials that signify each side (though we do not explicitly encode these fine-grained distinctions in this work). Interestingly, we found that sentiment features in Arg+Sent are not as informative as the arguing features discussed above. 6 Related Work Generally, research in identifying political viewpoints has employed information from words in the document (Malouf and Mullen, 2008; Mullen and Malouf, 2006; Grefenstette et al., 2004; Laver et al., 2003; Martin and Vanberg, 2008; Lin et al., 2006; Lin, 2006). Specifically, Lin et al. observe that people from opposing perspectives seem to use words in differing frequencies. On similar lines, Kim and Hovy (2007) use unigrams, bigrams and trigrams for election prediction from forum posts. In contrast, our work specifically employs sentiment-based and arguing-based features to perform stance classification in political debates. Our experiments are focused on determining how different opinion expressions reinforce an overall </context>
<context position="30128" citStr="Malouf and Mullen, 2008" startWordPosition="4737" endWordPosition="4740">-partner, ap-chromosome, ap-evolution, ap-genetic, an-kingdom, an- an-chromosome, an-evolution, an-genetic, ap-kingdom, apchristianity, an-mormonism, an-corinthians, an-bible, an-sin, christianity, ap-mormonism, ap-corinthians, ap-bible, ap-sin, an-bigot, an-procreate, ap-adopt, ap-bigot, ap-procreate, an-adopt Table 5: Examples of features associated with the stances in Gay Rights domain Discourse-level participant relation, that is, whether participants agree/disagree has been found useful for determining political side-taking (Thomas et al., 2006; Bansal et al., 2008; Agrawal et al., 2003; Malouf and Mullen, 2008). Agreement/disagreement relations are not the main focus of our work. Other work in the area of polarizing political discourse analyze co-citations (Efron, 2004) and linking patterns (Adamic and Glance, 2005). In contrast, our focus is on document content and opinion expressions. Somasundaran et al. (2007b) have noted the usefulness of the arguing category for opinion QA. Our tasks are different; they use arguing to retrieve relevant answers, but not distinguish stances. Our work is also different from related work in the domain of product debates (Somasundaran and Wiebe, 2009) in terms of th</context>
</contexts>
<marker>Malouf, Mullen, 2008</marker>
<rawString>Robert Malouf and Tony Mullen. 2008. Taking sides: Graph-based user classification for informal online political discourse. Internet Research, 18(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lanny W Martin</author>
<author>Georg Vanberg</author>
</authors>
<title>A robust transformation procedure for interpreting political text. Political Analysis,</title>
<date>2008</date>
<contexts>
<context position="27704" citStr="Martin and Vanberg, 2008" startWordPosition="4449" endWordPosition="4452"> unigram features associate the choice of topics with the stances, while the arguing features can capture the concerns, defenses, appeals or denials that signify each side (though we do not explicitly encode these fine-grained distinctions in this work). Interestingly, we found that sentiment features in Arg+Sent are not as informative as the arguing features discussed above. 6 Related Work Generally, research in identifying political viewpoints has employed information from words in the document (Malouf and Mullen, 2008; Mullen and Malouf, 2006; Grefenstette et al., 2004; Laver et al., 2003; Martin and Vanberg, 2008; Lin et al., 2006; Lin, 2006). Specifically, Lin et al. observe that people from opposing perspectives seem to use words in differing frequencies. On similar lines, Kim and Hovy (2007) use unigrams, bigrams and trigrams for election prediction from forum posts. In contrast, our work specifically employs sentiment-based and arguing-based features to perform stance classification in political debates. Our experiments are focused on determining how different opinion expressions reinforce an overall political stance. Our results indicate that while unigram information is reliable, further improve</context>
</contexts>
<marker>Martin, Vanberg, 2008</marker>
<rawString>Lanny W. Martin and Georg Vanberg. 2008. A robust transformation procedure for interpreting political text. Political Analysis, 16(1):93–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Mullen</author>
<author>Robert Malouf</author>
</authors>
<title>A preliminary investigation into sentiment analysis of informal political discourse.</title>
<date>2006</date>
<booktitle>In AAAI 2006 Spring Symposium on Computational Approaches to Analysing Weblogs (AAAI-CAAW</booktitle>
<contexts>
<context position="27631" citStr="Mullen and Malouf, 2006" startWordPosition="4437" endWordPosition="4440">” and negative arguing for “gay” reveal the the same stance. In general, unigram features associate the choice of topics with the stances, while the arguing features can capture the concerns, defenses, appeals or denials that signify each side (though we do not explicitly encode these fine-grained distinctions in this work). Interestingly, we found that sentiment features in Arg+Sent are not as informative as the arguing features discussed above. 6 Related Work Generally, research in identifying political viewpoints has employed information from words in the document (Malouf and Mullen, 2008; Mullen and Malouf, 2006; Grefenstette et al., 2004; Laver et al., 2003; Martin and Vanberg, 2008; Lin et al., 2006; Lin, 2006). Specifically, Lin et al. observe that people from opposing perspectives seem to use words in differing frequencies. On similar lines, Kim and Hovy (2007) use unigrams, bigrams and trigrams for election prediction from forum posts. In contrast, our work specifically employs sentiment-based and arguing-based features to perform stance classification in political debates. Our experiments are focused on determining how different opinion expressions reinforce an overall political stance. Our res</context>
</contexts>
<marker>Mullen, Malouf, 2006</marker>
<rawString>Tony Mullen and Robert Malouf. 2006. A preliminary investigation into sentiment analysis of informal political discourse. In AAAI 2006 Spring Symposium on Computational Approaches to Analysing Weblogs (AAAI-CAAW 2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<volume>Vol.</volume>
<pages>2--1</pages>
<contexts>
<context position="21741" citStr="Pang and Lee, 2008" startWordPosition="3503" endWordPosition="3506">entence. This is done for all sentences in the cates that what participants choose to speak about is post. a good indicator of ideological stance taking. This 5 Experiments result confirms previous researchers’ intuition that, Experiments are carried out on debate posts from the in general, political orientation is a function of “aufollowing four domains: Gun Rights, Gay Rights, thors’ attitudes over multiple issues rather than posAbortion, and Creationism. For each domain, a cor- itive or negative sentiment with respect to a sinpus with equal class distribution is created as fol- gle issue” (Pang and Lee, 2008). Nevertheless, the lows: we merge all debates and sample instances Arg+Sent system that uses both arguing and senti(posts) from the majority class to obtain equal num- ment features outperforms Unigram. bers of instances for each stance. This gives us a We performed McNemar’s test to measure the diftotal of 2232 posts in the corpus: 306 posts for the ference in system behaviors. The test was performed Gun Rights domain, 846 posts for the Gay Rights on all pairs of supervised systems using all 2232 domain, 550 posts for the Abortion domain and 530 posts. The results show that there is a signif</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, Vol. 2(1-2):pp. 1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Janyce Wiebe</author>
</authors>
<title>Recognizing stances in online debates.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>226--234</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="1017" citStr="Somasundaran and Wiebe, 2009" startWordPosition="142" endWordPosition="145">ng stances in ideological debates. In order to capture arguing opinions in ideological stance taking, we construct an arguing lexicon automatically from a manually annotated corpus. We build supervised systems employing sentiment and arguing opinions and their targets as features. Our systems perform substantially better than a distribution-based baseline. Additionally, by employing both types of opinion features, we are able to perform better than a unigrambased system. 1 Introduction In this work, we explore if and how ideological stances can be recognized using opinion analysis. Following (Somasundaran and Wiebe, 2009), stance, as used in this work, refers to an overall position held by a person toward an object, idea or proposition. For example, in a debate “Do you believe in the existence of God?,” a person may take a for-existence of God stance or an against existence of God stance. Similarly, being pro-choice, believing in creationism, and supporting universal healthcare are all examples of ideological stances. Online web forums discussing ideological and political hot-topics are popular.1 In this work, we are 1http://www.opposingviews.com, http://wiki.idebate.org, http://www.createdebate.com and http:/</context>
<context position="30713" citStr="Somasundaran and Wiebe, 2009" startWordPosition="4831" endWordPosition="4834">rawal et al., 2003; Malouf and Mullen, 2008). Agreement/disagreement relations are not the main focus of our work. Other work in the area of polarizing political discourse analyze co-citations (Efron, 2004) and linking patterns (Adamic and Glance, 2005). In contrast, our focus is on document content and opinion expressions. Somasundaran et al. (2007b) have noted the usefulness of the arguing category for opinion QA. Our tasks are different; they use arguing to retrieve relevant answers, but not distinguish stances. Our work is also different from related work in the domain of product debates (Somasundaran and Wiebe, 2009) in terms of the methodology. Wilson (2007) manually adds positive/negative arguing information to entries in a sentiment lexicon from (Wilson et al., 2005) and uses these as arguing features. Our arguing trigger expressions are separate from the sentiment lexicon entries and are derived from a corpus. Our n-gram trigger expressions are also different from manually created regular expression-based arguing lexicon for speech data (Somasundaran et al., 2007a). 7 Conclusions In this paper, we explore recognizing stances in ideological on-line debates. We created an arguing lexicon from the MPQA a</context>
</contexts>
<marker>Somasundaran, Wiebe, 2009</marker>
<rawString>Swapna Somasundaran and Janyce Wiebe. 2009. Recognizing stances in online debates. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 226–234, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Josef Ruppenhofer</author>
<author>Janyce Wiebe</author>
</authors>
<title>Detecting arguing and sentiment in meetings.</title>
<date>2007</date>
<booktitle>In SIGdial Workshop on Discourse and Dialogue,</booktitle>
<location>Antwerp, Belgium,</location>
<contexts>
<context position="30435" citStr="Somasundaran et al. (2007" startWordPosition="4785" endWordPosition="4788">-adopt Table 5: Examples of features associated with the stances in Gay Rights domain Discourse-level participant relation, that is, whether participants agree/disagree has been found useful for determining political side-taking (Thomas et al., 2006; Bansal et al., 2008; Agrawal et al., 2003; Malouf and Mullen, 2008). Agreement/disagreement relations are not the main focus of our work. Other work in the area of polarizing political discourse analyze co-citations (Efron, 2004) and linking patterns (Adamic and Glance, 2005). In contrast, our focus is on document content and opinion expressions. Somasundaran et al. (2007b) have noted the usefulness of the arguing category for opinion QA. Our tasks are different; they use arguing to retrieve relevant answers, but not distinguish stances. Our work is also different from related work in the domain of product debates (Somasundaran and Wiebe, 2009) in terms of the methodology. Wilson (2007) manually adds positive/negative arguing information to entries in a sentiment lexicon from (Wilson et al., 2005) and uses these as arguing features. Our arguing trigger expressions are separate from the sentiment lexicon entries and are derived from a corpus. Our n-gram trigger</context>
</contexts>
<marker>Somasundaran, Ruppenhofer, Wiebe, 2007</marker>
<rawString>Swapna Somasundaran, Josef Ruppenhofer, and Janyce Wiebe. 2007a. Detecting arguing and sentiment in meetings. In SIGdial Workshop on Discourse and Dialogue, Antwerp, Belgium, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Veselin Stoyanov</author>
</authors>
<title>Qa with attitude: Exploiting opinion type analysis for improving question answering in on-line discussions and the news.</title>
<date>2007</date>
<booktitle>In International Conference on Weblogs and Social</booktitle>
<location>Media, Boulder, CO.</location>
<contexts>
<context position="30435" citStr="Somasundaran et al. (2007" startWordPosition="4785" endWordPosition="4788">-adopt Table 5: Examples of features associated with the stances in Gay Rights domain Discourse-level participant relation, that is, whether participants agree/disagree has been found useful for determining political side-taking (Thomas et al., 2006; Bansal et al., 2008; Agrawal et al., 2003; Malouf and Mullen, 2008). Agreement/disagreement relations are not the main focus of our work. Other work in the area of polarizing political discourse analyze co-citations (Efron, 2004) and linking patterns (Adamic and Glance, 2005). In contrast, our focus is on document content and opinion expressions. Somasundaran et al. (2007b) have noted the usefulness of the arguing category for opinion QA. Our tasks are different; they use arguing to retrieve relevant answers, but not distinguish stances. Our work is also different from related work in the domain of product debates (Somasundaran and Wiebe, 2009) in terms of the methodology. Wilson (2007) manually adds positive/negative arguing information to entries in a sentiment lexicon from (Wilson et al., 2005) and uses these as arguing features. Our arguing trigger expressions are separate from the sentiment lexicon entries and are derived from a corpus. Our n-gram trigger</context>
</contexts>
<marker>Somasundaran, Wilson, Wiebe, Stoyanov, 2007</marker>
<rawString>Swapna Somasundaran, Theresa Wilson, Janyce Wiebe, and Veselin Stoyanov. 2007b. Qa with attitude: Exploiting opinion type analysis for improving question answering in on-line discussions and the news. In International Conference on Weblogs and Social Media, Boulder, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Janyce Wiebe</author>
<author>Josef Ruppenhofer</author>
</authors>
<title>Discourse level opinion interpretation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>801--808</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="3260" citStr="Somasundaran et al., 2008" startWordPosition="486" endWordPosition="489">or of universal healthcare. This negative opinion reveals his against-healthcare stance. We observed that arguing, a less well explored type of subjectivity, is prominently manifested in ideological debates. As used in this work, arguing is a type of linguistic subjectivity, where a person is arguing for or against something or expressing a belief about what is true, should be true or should be done 2As used in this work, sentiment is a type of linguistic subjectivity, specifically positive and negative expressions of emotions, judgments, and evaluations (Wilson and Wiebe, 2005; Wilson, 2007; Somasundaran et al., 2008). Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 116–124, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics in his or her view of the world (Wilson and Wiebe, 2005; Wilson, 2007; Somasundaran et al., 2008). For instance, let us consider the following snippet from a post supporting an against-existence of God stance. 4. Experiments, results and analyses are presented in Section 5. Related work is in Section 6 and conclusions are in Section 7. 2 Ideological Debates (2) Obviously that h</context>
<context position="11199" citStr="Somasundaran et al., 2008" startWordPosition="1789" endWordPosition="1792"> stances. Examples 1, 3 and 4 also illustrate that there are a variety of ways in which people support their stances. The writers express opinions about government, the initiator of healthcare and insurance companies, and the parties hurt by government run healthcare. Participants group government and healthcare as essentially the same concept, while they consider healthcare and insurance companies as alternative concepts. By expressing opinions regarding a variety of items that are same or alternative to main topic (healthcare, in these examples), they are, in effect, revealing their stance (Somasundaran et al., 2008). 3 Constructing an Arguing Lexicon Arguing is a relatively less explored category in subjectivity. Due to this, there are no available lexicons with arguing terms (clues). However, the MPQA corpus (Version 2) is annotated with arguing subjectivity (Wilson and Wiebe, 2005; Wilson, 2007). There are two arguing categories: positive arguing and negative arguing. We use this corpus to generate a ngram (up to trigram) arguing lexicon. The examples below illustrate MPQA arguing annotations. Examples 5 and 7 illustrate positive argu118 ing annotations and Example 6 illustrates negative arguing. (5) I</context>
<context position="26530" citStr="Somasundaran et al., 2008" startWordPosition="4271" endWordPosition="4275">d “bible” are all indicative of against-gay rights stance. These are indeed beliefs and concerns that shape an against-gay rights stance. On the other hand, negative arguings with these same words denote a for-gay rights stance. Presumably, these occur in refutations of the concerns influencing the opposite side. Likewise, the appeal for equal rights for gays is captured positive arguing about “liberty”, “independence”, “pursuit” and “suffrage”. Interestingly, we found that our features also capture the ideas of opinion variety and same and alternative targets as defined in previous research (Somasundaran et al., 2008) – in Table 5, items that are similar (e.g., “christianity” and “corinthians”) have similar opinions toward them for a given stance (for e.g., ap-christianity and ap-corinthians belong to against-gay rights stance while an-christianity and an-corinthians belong to for-gay rights stance). Additionally, items that are alternatives (e.g. “gay” and “heterosexuality”) have opposite polarities associated with them for a given stance, that is, positive arguing for “heterosexuality” and negative arguing for “gay” reveal the the same stance. In general, unigram features associate the choice of topics w</context>
</contexts>
<marker>Somasundaran, Wiebe, Ruppenhofer, 2008</marker>
<rawString>Swapna Somasundaran, Janyce Wiebe, and Josef Ruppenhofer. 2008. Discourse level opinion interpretation. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 801–808, Manchester, UK, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Thomas</author>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Get out the vote: Determining support or opposition from congressional floor-debate transcripts.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>327--335</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="30059" citStr="Thomas et al., 2006" startWordPosition="4725" endWordPosition="4728">, ap-partner, an-private, an-wed, an-gay, ap-heterosexuality, an-partner, ap-chromosome, ap-evolution, ap-genetic, an-kingdom, an- an-chromosome, an-evolution, an-genetic, ap-kingdom, apchristianity, an-mormonism, an-corinthians, an-bible, an-sin, christianity, ap-mormonism, ap-corinthians, ap-bible, ap-sin, an-bigot, an-procreate, ap-adopt, ap-bigot, ap-procreate, an-adopt Table 5: Examples of features associated with the stances in Gay Rights domain Discourse-level participant relation, that is, whether participants agree/disagree has been found useful for determining political side-taking (Thomas et al., 2006; Bansal et al., 2008; Agrawal et al., 2003; Malouf and Mullen, 2008). Agreement/disagreement relations are not the main focus of our work. Other work in the area of polarizing political discourse analyze co-citations (Efron, 2004) and linking patterns (Adamic and Glance, 2005). In contrast, our focus is on document content and opinion expressions. Somasundaran et al. (2007b) have noted the usefulness of the arguing category for opinion QA. Our tasks are different; they use arguing to retrieve relevant answers, but not distinguish stances. Our work is also different from related work in the do</context>
</contexts>
<marker>Thomas, Pang, Lee, 2006</marker>
<rawString>Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out the vote: Determining support or opposition from congressional floor-debate transcripts. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 327–335, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
</authors>
<title>Annotating attributions and private states.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL Workshop on Frontiers in Corpus Annotation II: Pie in the Sky.</booktitle>
<contexts>
<context position="3218" citStr="Wilson and Wiebe, 2005" startWordPosition="480" endWordPosition="483">ted toward the government, the initiator of universal healthcare. This negative opinion reveals his against-healthcare stance. We observed that arguing, a less well explored type of subjectivity, is prominently manifested in ideological debates. As used in this work, arguing is a type of linguistic subjectivity, where a person is arguing for or against something or expressing a belief about what is true, should be true or should be done 2As used in this work, sentiment is a type of linguistic subjectivity, specifically positive and negative expressions of emotions, judgments, and evaluations (Wilson and Wiebe, 2005; Wilson, 2007; Somasundaran et al., 2008). Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 116–124, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics in his or her view of the world (Wilson and Wiebe, 2005; Wilson, 2007; Somasundaran et al., 2008). For instance, let us consider the following snippet from a post supporting an against-existence of God stance. 4. Experiments, results and analyses are presented in Section 5. Related work is in Section 6 and conclusions are in Section 7. </context>
<context position="11471" citStr="Wilson and Wiebe, 2005" startWordPosition="1832" endWordPosition="1835">pants group government and healthcare as essentially the same concept, while they consider healthcare and insurance companies as alternative concepts. By expressing opinions regarding a variety of items that are same or alternative to main topic (healthcare, in these examples), they are, in effect, revealing their stance (Somasundaran et al., 2008). 3 Constructing an Arguing Lexicon Arguing is a relatively less explored category in subjectivity. Due to this, there are no available lexicons with arguing terms (clues). However, the MPQA corpus (Version 2) is annotated with arguing subjectivity (Wilson and Wiebe, 2005; Wilson, 2007). There are two arguing categories: positive arguing and negative arguing. We use this corpus to generate a ngram (up to trigram) arguing lexicon. The examples below illustrate MPQA arguing annotations. Examples 5 and 7 illustrate positive argu118 ing annotations and Example 6 illustrates negative arguing. (5) Iran insists its nuclear program is purely for peaceful purposes. (6) Officials in Panama denied that Mr. Chavez or any of his family members had asked for asylum. (7) Putin remarked that the events in Chechnia “could be interpreted only in the context of the struggle agai</context>
</contexts>
<marker>Wilson, Wiebe, 2005</marker>
<rawString>Theresa Wilson and Janyce Wiebe. 2005. Annotating attributions and private states. In Proceedings of ACL Workshop on Frontiers in Corpus Annotation II: Pie in the Sky.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In hltemnlp2005,</booktitle>
<pages>347--354</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="4707" citStr="Wilson et al., 2005" startWordPosition="720" endWordPosition="723">e, people not only express their sentiments, but they also argue about what is true (e.g., this is prominent in the existence of God debate) and about what should or should not be done (e.g., this is prominent in the healthcare debate). In this work, we investigate whether sentiment and arguing expressions of opinion are useful for ideological stance classification. For this, we explore ways to capture relevant opinion information as machine learning features into a supervised stance classifier. While there is a large body of resources for sentiment analysis (e.g., the sentiment lexicon from (Wilson et al., 2005)), arguing analysis does not seem to have a well established lexical resource. In order to remedy this, using a simple automatic approach and a manually annotated corpus,3 we construct an arguing lexicon. We create features called opinion-target pairs, which encode not just the opinion information, but also what the opinion is about, its target. Systems employing sentiment-based and arguing-based features alone, or both in combination, are analyzed. We also take a qualitative look at features used by the learners to get insights about the information captured by them. We perform experiments on</context>
<context position="15074" citStr="Wilson et al., 2005" startWordPosition="2433" endWordPosition="2436">tracting the starting n-grams from the MPQA annotations. The process of creating the lexicon is as follows: 1. Generate a candidate Set from the annotations in the corpus. Three candidates are extracted from the stemmed version of each annotation: the first word, the bigram starting at the first word, and the trigram starting at the first word. For example, if the annotation is “can only rise to meet it by making some radical changes”, the following candidates are extracted from it: “can”, “can only” and “can only rise”. 2. Remove the candidates that are present in the sentiment lexicon from (Wilson et al., 2005) (as these are already accounted for in previous research). For example, “actually”, which is a trigger word in Table 3, is a neutral subjectivity clue in the lexicon. 3. For each candidate in the candidate Set, find the likelihood that it is a reliable indicator of positive or negative arguing in the MPQA corpus. These are likelihoods of the form: P(positive arguing|candidate) _ #candidate is in a positive arguing span #candidate is in the corpus and P(negative arguing|candidate) _ #candidate is in a negative arguing span #candidate is in the corpus 4. Make a lexicon entry for each candidate </context>
<context position="19734" citStr="Wilson et al., 2005" startWordPosition="3178" endWordPosition="3181">s subject and object. Note that all the different modals are replaced by “should” while creating features. This helps to create more general features. For example, given a sentence “They must be available to all people”, the method creates three features “they should”, “should available” and “they should available”. These patterns are created independently of the arguing lexicon matches, and added to the feature set for the post. 4.2 Sentiment-based Features Sentiment-based features are created independent of arguing features. In order to detect sentiment opinions, we use a sentiment lexicon (Wilson et al., 2005). In addition to positive (+) and negative (−) words, this lexicon also contains subjective words that are themselves neutral (=) with respect to polarity. Examples of neutral entries are “absolutely”, “amplify”, “believe”, and “think”. We find the sentiment polarity of the entire sentence and assign this polarity to each content word in the sentence (denoted, for example, as target+). In order to detect the sentence polarity, we use the Vote 120 and Flip algorithm from Choi and Cardie (2009). racy metric. This algorithm essentially counts the number of pos- 5.1 Results itive, negative and neu</context>
<context position="30869" citStr="Wilson et al., 2005" startWordPosition="4855" endWordPosition="4858">course analyze co-citations (Efron, 2004) and linking patterns (Adamic and Glance, 2005). In contrast, our focus is on document content and opinion expressions. Somasundaran et al. (2007b) have noted the usefulness of the arguing category for opinion QA. Our tasks are different; they use arguing to retrieve relevant answers, but not distinguish stances. Our work is also different from related work in the domain of product debates (Somasundaran and Wiebe, 2009) in terms of the methodology. Wilson (2007) manually adds positive/negative arguing information to entries in a sentiment lexicon from (Wilson et al., 2005) and uses these as arguing features. Our arguing trigger expressions are separate from the sentiment lexicon entries and are derived from a corpus. Our n-gram trigger expressions are also different from manually created regular expression-based arguing lexicon for speech data (Somasundaran et al., 2007a). 7 Conclusions In this paper, we explore recognizing stances in ideological on-line debates. We created an arguing lexicon from the MPQA annotations in order to recognize arguing, a prominent type of linguistic subjectivity in ideological stance taking. We observed that opinions or targets in </context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In hltemnlp2005, pages 347–354, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
</authors>
<title>Fine-grained Subjectivity and Sentiment Analysis: Recognizing the Intensity, Polarity, and Attitudes of private states.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Intelligent Systems Program, University of Pittsburgh.</institution>
<contexts>
<context position="3232" citStr="Wilson, 2007" startWordPosition="484" endWordPosition="485">t, the initiator of universal healthcare. This negative opinion reveals his against-healthcare stance. We observed that arguing, a less well explored type of subjectivity, is prominently manifested in ideological debates. As used in this work, arguing is a type of linguistic subjectivity, where a person is arguing for or against something or expressing a belief about what is true, should be true or should be done 2As used in this work, sentiment is a type of linguistic subjectivity, specifically positive and negative expressions of emotions, judgments, and evaluations (Wilson and Wiebe, 2005; Wilson, 2007; Somasundaran et al., 2008). Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 116–124, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics in his or her view of the world (Wilson and Wiebe, 2005; Wilson, 2007; Somasundaran et al., 2008). For instance, let us consider the following snippet from a post supporting an against-existence of God stance. 4. Experiments, results and analyses are presented in Section 5. Related work is in Section 6 and conclusions are in Section 7. 2 Ideological </context>
<context position="11486" citStr="Wilson, 2007" startWordPosition="1836" endWordPosition="1837">nd healthcare as essentially the same concept, while they consider healthcare and insurance companies as alternative concepts. By expressing opinions regarding a variety of items that are same or alternative to main topic (healthcare, in these examples), they are, in effect, revealing their stance (Somasundaran et al., 2008). 3 Constructing an Arguing Lexicon Arguing is a relatively less explored category in subjectivity. Due to this, there are no available lexicons with arguing terms (clues). However, the MPQA corpus (Version 2) is annotated with arguing subjectivity (Wilson and Wiebe, 2005; Wilson, 2007). There are two arguing categories: positive arguing and negative arguing. We use this corpus to generate a ngram (up to trigram) arguing lexicon. The examples below illustrate MPQA arguing annotations. Examples 5 and 7 illustrate positive argu118 ing annotations and Example 6 illustrates negative arguing. (5) Iran insists its nuclear program is purely for peaceful purposes. (6) Officials in Panama denied that Mr. Chavez or any of his family members had asked for asylum. (7) Putin remarked that the events in Chechnia “could be interpreted only in the context of the struggle against internation</context>
<context position="30756" citStr="Wilson (2007)" startWordPosition="4840" endWordPosition="4841">agreement relations are not the main focus of our work. Other work in the area of polarizing political discourse analyze co-citations (Efron, 2004) and linking patterns (Adamic and Glance, 2005). In contrast, our focus is on document content and opinion expressions. Somasundaran et al. (2007b) have noted the usefulness of the arguing category for opinion QA. Our tasks are different; they use arguing to retrieve relevant answers, but not distinguish stances. Our work is also different from related work in the domain of product debates (Somasundaran and Wiebe, 2009) in terms of the methodology. Wilson (2007) manually adds positive/negative arguing information to entries in a sentiment lexicon from (Wilson et al., 2005) and uses these as arguing features. Our arguing trigger expressions are separate from the sentiment lexicon entries and are derived from a corpus. Our n-gram trigger expressions are also different from manually created regular expression-based arguing lexicon for speech data (Somasundaran et al., 2007a). 7 Conclusions In this paper, we explore recognizing stances in ideological on-line debates. We created an arguing lexicon from the MPQA annotations in order to recognize arguing, a</context>
</contexts>
<marker>Wilson, 2007</marker>
<rawString>Theresa Wilson. 2007. Fine-grained Subjectivity and Sentiment Analysis: Recognizing the Intensity, Polarity, and Attitudes of private states. Ph.D. thesis, Intelligent Systems Program, University of Pittsburgh.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>