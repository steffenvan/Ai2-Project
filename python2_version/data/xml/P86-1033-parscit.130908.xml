<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000083">
<note confidence="0.9328655">
LINGUISTIC COHERENCE: A PLAN-BASED ALTERNATIVE
Diane J. Litman
</note>
<author confidence="0.262526">
AT&amp;T Bell Laboratories
</author>
<affiliation confidence="0.26058">
3C-408A
</affiliation>
<address confidence="0.8835345">
600 Mountain Avenue
Murray Hill, NJ 079741
</address>
<email confidence="0.568308">
ABSTRACT
</email>
<bodyText confidence="0.9998969375">
To fully understand a sequence of utterances, one
must be able to infer implicit relationships between
the utterances. Although the identification of sets of
utterance relationships forms the basis for many
theories of discourse, the formalization and recogni-
tion of such relationships has proven to be an
extremely difficult computational task.
This paper presents a plan-based approach to the
representation and recognition of implicit relation-
ships between utterances. Relationships are formu-
lated as discourse plans. which allows their representa-
tion in terms of planning operators and their computa-
tion via a plan recognition process. By incorporating
complex inferential processes relating utterances into
a plan-based framework, a formalization and computa-
bility not available in the earlier works is provided.
</bodyText>
<sectionHeader confidence="0.990639" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.983526823529412">
In order to interpret a sequence of utterances
fully, one must know how the utterances cohere; that
is, one must be able to infer implicit relationships as
well as non-relationships between the utterances. Con-
sider the following fragment. taken from a terminal
transcript between a user and a computer operator
(Mann [12]):
Could you mount a magtape for me?
It&apos;s tape 1.
Such a fragment appears coherent because it is easy to
infer how the second utterance is related to the first.
Contrast this with the following fragment:
Could you mount a magtape for me?
It&apos;s snowing like crazy.
This sequence appears much less coherent since now
there is no obvious connection between the two utter-
ances. While one could postulate some connection
(e.g., the speaker&apos;s magtape contains a database of
places to go skiing), more likely one would say that
there is no relationship between the utterances. Furth-
&apos;This work was done at the Department of Computer Sci-
ence. University of Rochester. Rochester NY 14627. and support-
ed in part by DARPA under Grant N00014-82-K-0193. NSF under
Grant DCR8351665. and ONR under Grant N0014-80-C-0197.
ermore, because the second utterance violates an
expectation of discourse coherence (Reichman [16].
Hobbs [8], Grosz, Joshi, and Weinstein [6]), the utter-
ance seems inappropriate since there are no linguistic
clues (for example, prefacing the utterance with
&amp;quot;incidentally&amp;quot;) marking it as a topic change.
The identification and specification of sets of
linguistic relationships between utterances2 forms the
basis for many computational models of discourse
(Reichman [17], McKeown [14], Mann [13], Hobbs [8],
Cohen [3]). By limiting the relationships allowed in a
system and the ways in which relationships coherently
interact, efficient mechanisms for understanding and
generating well organized discourse can be developed.
Furthermore, the approach provides a framework for
explaining the use of surface linguistic phenomena
such as clue words, words like &amp;quot;incidentally&amp;quot; that often
correspond to particular relationships between utter-
ances. Unfortunately. while these theories propose
relationships that seem intuitive (e.g. &amp;quot;elaboration,&amp;quot; as
might be used in the first fragment above), there has
been little agreement on what the set of possible rela-
tionships should be, or even if such a set can be
defined. Furthermore, since the formalization of the
relationships has proven to be an extremely difficult
task, such theories typically have to depend on
unrealistic computational processes. For example.
Cohen [3] uses an oracle to recognize her &amp;quot;evidence&amp;quot;
relationships. Reichman&apos;s [17] use of a set of conver-
sational moves depends on the future development of
extremely sophisticated semantics modules. Hobbs [8]
acknowledges that his theory of coherence relations
&amp;quot;may seem to be appealing to magic,&amp;quot; since there are
several places where he appeals to as yet incomplete
subtheories. Finally, Mann [13] notes that his theory of
rhetorical predicates is currently descriptive rather
than constructive. McKeown&apos;s [14] implemented sys-
tem of rhetorical predicates is a notable exception, but
since her predicates have associated semantics
expressed in terms of a specific data base system the
approach is not particularly general.
=Although in some theories relationships hold between group
of utterances, in others between clauses of an utterance, these
distinctions will not be crucial for the purposes of this paper.
</bodyText>
<page confidence="0.997994">
215
</page>
<bodyText confidence="0.999957111111111">
This paper presents a new model for representing
and recognizing implicit relationships between utter-
ances. Underlying linguistic relationships are formu-
lated as discourse plans in a plan-based theory of
dialogue understanding. This allows the specification
and formalization of the relationships within a compu-
tational framework, and enables a plan recognition
algorithm to provide the link from the processing of
actual input to the recognition of underlying discourse
plans. Moreover, once a plan recognition system
incorporates knowledge of linguistic relationships, it
can then use the correlations between linguistic rela-
tionships and surface linguistic phenomena to guide its
processing. By incorporating domain independent
linguistic results into a plan recognition framework, a
formalization and computability generally not avail-
able in the earlier works is provided.
The next section illustrates the discourse plan
representation of domain independent knowledge
about communication as knowledge about the planning
process itself. A plan recognition process is then
developed to recognize such plans, using linguistic
clues, coherence preferences, and constraint satisfac-
tion. Finally, a detailed example of the processing of
a dialogue fragment is presented, illustrating the
recognition of various types of relationships between
utterances.
</bodyText>
<sectionHeader confidence="0.986725" genericHeader="method">
REPRESENTING COHERENCE USING DISCOURSE
</sectionHeader>
<subsectionHeader confidence="0.848568">
PLANS
</subsectionHeader>
<bodyText confidence="0.999963444444444">
In a plan-based approach to language understand-
ing, an utterance is considered understood when it has
been related to some underlying plan of the speaker.
While previous works have explicitly represented and
recognized the underlying task plans of a given
domain (e.g., mount a tape) (Grosz [5], Allen and Per-
rault [1], Sidner and Israel [21]. Carberry [2], Sidner
[24]), the ways that utterances could be related to such
plans were limited and not of particular concern. As a
result, only dialogues exhibiting a very limited set of
utterance relationships could be understood.
In this work, a set of domain-independent plans
about plans (i.e. meta-plans) called discourse plans are
introduced to explicitly represent, reason about, and
generalize such relationships. Discourse plans are
recognized from every utterance and represent plan
introduction, plan execution, plan specification, plan
debugging, plan abandonment, and so on, indepen-
dently of any domain. Although discourse plans can
refer to both domain plans or other discourse plans.
domain plans can only be accessed and manipulated
via discourse plans. For example, in the tape excerpt
above &amp;quot;Could you mount a magtape for me?&amp;quot; achieves
a discourse plan to introduce a domain plan to mount a
tape. &amp;quot;It&apos;s tape 1&amp;quot; then further specifies this domain
plan.
Except for the fact that they refer to other plans
(i.e. they take other plans as arguments), the represen-
tation of discourse plans is identical to the usual
representation of domain plans (Fikes and Nilsson [4],
Sacerdoti [18]). Every plan has a header, a parameter-
ized action description that names the plan. Action
descriptions are represented as operators on a
planner&apos;s world model and defined in terms of prere-
quisites, decompositions, and effects. Prerequisites are
conditions that need to hold (or to be made to hold) in
the world model before the action operator can be
applied. Effects are statements that are asserted into
the world model after the action has been successfully
executed. Decompositions enable hierarchical plan-
ning. Although the action description of the header
may be usefully thought of at one level of abstraction
as a single action achieving a goal. such an action
might not be executable, i.e. it might be an abstract as
opposed to primitive action. Abstract actions are in
actuality composed of primitive actions and possibly
other abstract action descriptions (i.e. other plans).
Finally, associated with each plan is a set of applica-
bility conditions called constraints.3 These are similar
to prerequisites, except that the planner never
attempts to achieve a constraint if it is false. The plan
recognizer will use such general plan descriptions to
recognize the particular plan instantiations underlying
an utterance.
</bodyText>
<figureCaption confidence="0.9907175">
HEADER: INTRODUCE-PLAN(speaker, hearer
DECOMPOSITION: action. plan)
EFFECTS: REQUEST(speaker. hearer. action)
CONSTRAINTS: WANT(hearer. plan)
NEXT(action. plan)
STEP(action, plan)
AGENT(action. hearer)
Figure 1. INTRODUCE-PLAN.
</figureCaption>
<bodyText confidence="0.968194916666667">
Figures 1, 2, and 3 present examples of discourse
plans (see Litman [10] for the complete set). The first
discourse plan, INTRODUCE-PLAN, takes a plan of
the speaker that involves the hearer and presents it to
the hearer (who is assumed cooperative). The decom-
position specifies a typical way to do this, via execu-
tion of the speech act (Searle [19]) REQUEST. The
constraints use a vocabulary for referring to and
describing plans and actions to specify that the only
actions requested will be those that are in the plan and
have the hearer as agent. Since the hearer is assumed
cooperative, he or she will then adopt as a goal the
</bodyText>
<footnote confidence="0.752152">
3These constraints should not be confused with the con-
straints of Stefik [25]. which are dynamical!&apos;, formulated during
hierarchical plan generation and represent the interactions
between subproblems.
</footnote>
<page confidence="0.998966">
216
</page>
<bodyText confidence="0.999748571428571">
joint plan containing the action (i.e. the first effect).
The second effect states that the action requested will
be the next action performed in the introduced plan.
Note that since INTRODUCE-PLAN has no prere-
quisites it can occur in any discourse context, i.e. it
does not need to be related to previous plans.
INTRODUCE-PLAN thus allows the recognition of
topic changes when a previous topic is completed as
well as recognition of interrupting topic changes (and
when not linguistically marked as such, of
incoherency) at any point in the dialogue. It also cap-
tures previously implicit knowledge that at the begin-
ning of a dialogue an underlying plan needs to be
recognized.
</bodyText>
<construct confidence="0.821045090909091">
HEADER: CONTINUE-PLAN(speaker, hearer, step
nextstep. plan)
PREREQUISITES: LAST(step. plan)
WANT(hearer. plan)
DECOMPOSITION: REQUEST(speaker. hearer. nextstep)
EFFECT: NEXT(nextstep. plan)
CONSTRAINTS: STEP(step. plan)
STEP(nextstep. plan)
AFTER(step. nextstep. plan)
AGENT(nextstep. hearer)
CANDO(hearer. nextstep)
</construct>
<figureCaption confidence="0.997259">
Figure 2. CONTINUE-PLAN.
</figureCaption>
<bodyText confidence="0.976209310344827">
The discourse plan in Figure 2, CONTINUE-
PLAN, takes an already introduced plan as defined by
the WANT prerequisite and moves execution to the
next step, where the previously executed step is
marked by the predicate LAST. One way of doing
this is to request the hearer to perform the step that
should occur after the previously executed step.
assuming of course that the step is something the
hearer actually can perform. This is captured by the
decomposition together with the constraints. As
above, the NEXT effect then updates the portion of
the plan to be executed. This discourse plan captures
the previously implicit relationship of coherent topic
continuation in task-oriented dialogues (without
interruptions), i.e. the fact that the discourse structure
follows the task structure (Grosz [5]).
Figure 3 presents CORRECT-PLAN, the last
discourse plan to be discussed. CORRECT-PLAN
inserts a repair step into a pre-existing plan that would
otherwise fail. More specifically, CORRECT-PLAN
takes a pre-existing plan having subparts that do not
interact as expected during execution, and debugs the
plan by adding a new goal to restore the expected
interactions. The pre-existing plan has subparts
laststep and nextstep, where laststep was supposed to
enable the performance of nextstep, but in reality did
not. The plan is corrected by adding newstep, which
HEADER: CORRECT-PLAN(speaker. hearer.
laststep, newstep. nextstep. plan)
</bodyText>
<construct confidence="0.8467316">
PREREQUISITES: WANT(hearer, plan)
LAST(laststep. plan)
DECOMPOSITION-I: REQUEST(speaker. hearer, newstep)
DECOMPOSITION-2: REQUEST(speaker. hearer, nextstep)
EFFECTS: STEP(nevvstep. plan)
AFTER(laststep. newstep. plan)
AFTER(newstep, nextstep. plan)
NEXT(newstep. plan)
CONSTRAINTS: STEP(laststep. plan)
STEP(nextstep. plan)
AFTER(laststep. nextstep. plan)
AGENT(newstep. hearer)
&amp;quot;CANDO(speaker, nextstep)
MODIFIES(newstep, laststep)
ENABLES(newstep. nextstep)
</construct>
<figureCaption confidence="0.992685">
Figure 3. CORRECT-PLAN.
</figureCaption>
<bodyText confidence="0.999698625">
enables the performance of nextstep and thus of the
rest of plan. The correction can be introduced by a
REQUEST for either nextstep or newstep. When
nextstep is requested, the hearer has to use the
knowledge that nextstep cannot currently be per-
formed to infer that a correction must be added to the
plan. When newstep is requested, the speaker expli-
citly provides the correction. The effects and con-
straints capture the plan situation described above and
should be self-explanatory with the exception of two
new terms. MODIFIES(action2, actionl) means that
action2 is a variant of actionl, for example, the same
action with different parameters or a new action
achieving the still required effects.
ENABLES(actionl, action2) means that false prere-
quisites of action2 are in the effects of action].
CORRECT-PLAN is an example of a topic interrup-
tion that relates to a previous topic,
To illustrate how these discourse plans represent
the relationships between utterances, consider a
naturally-occurring protocol (Sidner [22]) in which a
user interacts with a person simulating an editing sys-
tem to manipulate network structures in a knowledge
representation language:
</bodyText>
<listItem confidence="0.99963">
1) User: Hi. Please show the concept Person.
2) System: Drawing...OK.
3) User: Add a role called hobby.
4) System: OK.
5) User: Make the yr be Pastime.
</listItem>
<bodyText confidence="0.999471444444444">
Assume a typical task plan in this domain is to edit a
structure by accessing the structure then performing a
sequence of editing actions. The user&apos;s first request
thus introduces a plan to edit the concept person.
Each successive user utterance continues through the
plan by requesting the system to perform the various
editing actions. More specifically, the first utterance
would correspond to INTRODUCE-PLAN (User, Sys-
tem, show the concept Person, edit plan). Since one of
</bodyText>
<page confidence="0.988614">
217
</page>
<bodyText confidence="0.999814909090909">
the effects of INTRODUCE-PLAN is that the system
adopts the plan, the system responds by executing the
next action in the plan, i.e. by showing the concept
Person. The user&apos;s next utterance can then be recog-
nized as CONTINUE-PLAN (User, System, show the
concept Person, add hobby role to Person, edit plan),
and so on.
Now consider two variations of the above dialo-
gue. For example, imagine replacing utterance (5)
with the User&apos;s &amp;quot;No, leave more room please.&amp;quot; In this
case, since the system has anticipated the require-
ments of future editing actions incorrectly, the user
must interrupt execution of the editing task to correct
the system, i.e. CORRECT-PLAN(User. System, add
hobby role to Person, compress the concept Person,
next edit step, edit plan). Finally, imagine that utter-
ance (5) is again replaced, this time with &amp;quot;Do you
know if it&apos;s time for lunch yet?&amp;quot; Since eating lunch
cannot be related to the previous editing plan topic,
the system recognizes the utterance as a total change
of topic, i.e. INTRODUCE-PLAN(User, System, Sys-
tem tell User if time for lunch, eat lunch plan).
</bodyText>
<sectionHeader confidence="0.980668" genericHeader="method">
RECOGNIZING DISCOURSE PLANS
</sectionHeader>
<bodyText confidence="0.992521474576271">
This section presents a computational algorithm
for the recognition of discourse plans. Recall that the
previous lack of such an algorithm was in fact a major
force behind the last section&apos;s plan-based formaliza-
tion of the linguistic relationships. Previous work in
the area of domain plan recognition (Allen and Per-
rault [1], Sidner and Israel [21]. Carberry [2], Sidner
[24]) provides a partial solution to the recognition
problem. For example, since discourse plans are
represented identically to domain plans, the same pro-
cess of plan recognition can apply to both. In particu-
lar, every plan is recognized by an incremental process
of heuristic search. From an input, the plan recognizer
tries to find a plan for which the input is a step,4 and
then tries to find more abstract plans for which the
postulated plan is a step, and so on. After every step
of this chaining process. a set of heuristics prune the
candidate plan set based on assumptions regarding
rational planning behavior. For example. as in Allen
and Perrault [1] candidates whose effects are already
true are eliminated, since achieving these plans would
produce no change in the state of the world. As in
Carberry [2] and Sidner and Israel [21] the plan recog-
nition process is also incremental; if the heuristics
cannot uniquely determine an underlying plan, chain-
ing stops.
As mentioned above, however, this is not a full
solution. Since the plan recognizer is now recognizing
discourse as well as domain plans from a single utter-
ance, the set of recognition processes must be coordi-
4Plan chaining can also be done via effects and prerequisites.
To keep the example in the next section simple. plans have been
nated.5 An algorithm for coordinating the recognition
of domain and discourse plans from a single utterance
has been presented in Litman and Allen [9,11]. In
brief, the plan recognizer recognizes a discourse plan
from every utterance, then uses a process of constraint
satisfaction to initiate recognition of the domain and
any other discourse plans related to the utterance.
Furthermore, to record and monitor execution of the
discourse and domain plans active at any point in a
dialogue, a dialogue context in the form of a plan
stack is built and maintained by the plan recognizer.
Various models of discourse have argued that an ideal
interrupting topic structure follows a stack-like discip-
line (Reichman [17], Polanyi and Scha [15], Grosz and
Sidner [7]). The plan recognition algorithm will be
reviewed when tracing through the example of the
next section.
Since discourse plans reflect linguistic relation-
ships between utterances, the earlier work on domain
plan recognition can also be augmented in several
other ways. For example, the search process can be
constrained by adding heuristics that prefer discourse
plans corresponding to the most linguistically coherent
continuations of the dialogue. More specifically, in
the absence of any linguistic clues (as will be
described below), the plan recognizer will prefer rela-
tionships that, in the following order:
</bodyText>
<listItem confidence="0.958710285714286">
(1) continue a previous topic (e.g. CONTINUE-
PLAN)
(2) interrupt a topic for a semantically related topic
(e.g. CORRECT-PLAN, other corrections and
clarifications as in Litman [10])
(3) interrupt a topic for a totally unrelated topic (e.g.
INTRODUCE-PLAN).
</listItem>
<bodyText confidence="0.999876588235294">
Thus, while interruptions are not generally predicted,
they can be handled when they do occur. The heuris-
tics also follow the principle of Occam&apos;s razor, since
they are ordered to introduce as few new plans as pos-
sible. If within one of these preferences there are still
competing interpretations, the interpretation that most
corresponds to a stack discipline is preferred. For
example, a continuation resuming a recently inter-
rupted topic is preferred to continuation of a topic
interrupted earlier in the conversation.
Finally, since the plan recognizer now recognizes
implicit relationships between utterances, linguistic
clues signaling such relationships (Grosz [5]. Reich-
man [17]. Polanyi and Scha [15], Sidner [24], Cohen
[3], Grosz and Sidner [7]) should be exploitable by the
plan recognition algorithm. In other words, the plan
recognizer should be aware of correlations between
</bodyText>
<footnote confidence="0.7806554">
expressed so that chaining via decompositions is sufficient.
&apos;Although Wilensky [26] introduced meta-plans into a natur-
al language system to handle a totally different issue, that of con-
current goal interaction, he does not address details of coordina-
tion.
</footnote>
<page confidence="0.996833">
218
</page>
<bodyText confidence="0.998743791666667">
specific words and the discourse plans they typically
signal. Clues can then be used both to reinforce as
well as to overrule the preference ordering given
above. In fact, in the latter case clues ease the recog-
nition of topic relationships that would otherwise be
difficult (if not impossible (Cohen [3], Grosz and
Sidner [7], Sidner [24])) to understand. For example,
consider recognizing the topic change in the tape vari-
ation earlier, repeated below for convenience:
Could you mount a magtape for me?
It&apos;s snowing like crazy.
Using the coherence preferences the plan recognizer
first tries to interpret the second utterance as a con-
tinuation of the plan to mount a tape, then as a
related interruption of this plan. and only when these
efforts fail as an unrelated change of topic. This is
because a topic change is least expected in the
unmarked case. Now, imagine the speaker prefacing
the second utterance with a clue such as &amp;quot;incidentally.&amp;quot;
a word typically used to signal topic interruption.
Since the plan recognizer knows that &amp;quot;incidentally&amp;quot; is
a signal for an interruption, the search will not even
attempt to satisfy the first preference heuristic since a
signal for the second or third is explicitly present.
</bodyText>
<sectionHeader confidence="0.487991" genericHeader="method">
EXAMPLE
</sectionHeader>
<bodyText confidence="0.913840666666667">
This section uses the discourse plan representa-
tions and plan recognition algorithm of the previous
sections to illustrate the processing of the following
dialogue, a slightly modified portion of a scenario
(Sidner and Bates [23]) developed from the set of pro-
tocols described above:
User: Show me the generic concept called &amp;quot;employee.&amp;quot;
System:OK. &lt;system displays network&gt;
User: No, move the concept up.
System:OK. &lt;system redisplays network&gt;
User: Now, make an individual employee concept
whose first name is &amp;quot;Sam&amp;quot; and whose last
name is &amp;quot;Jones.&amp;quot;
Although the behavior to be described is fully speci-
fied by the theory, the implementation corresponds
only to the new model of plan recognition. All simu-
lated computational processes have been implemented
elsewhere, however. Litman [10] contains a full discus-
sion of the implementation.
Figure 4 presents the relevant domain plans for
this domain, taken from Sidner and Israel [21] with
minor modifications. ADD-DATA is a plan to add
new data into a network, while EXAMINE is a plan
to examine parts of a network. Both plans involve the
subplan CONSIDER-ASPECT, in which the user con-
siders some aspect of a network, for example by look-
ing at it (the decomposition shown), listening to a
description, or thinking about it.
The processing begins with a speech act analysis
of &amp;quot;Show me the generic concept called &apos;employee—
</bodyText>
<table confidence="0.97082075">
HEADER: ADD-DATA(user, netpiece, data,
screenLocation)
DECOMPOSITION: CONSIDER-ASPECT(user. netpiece)
PUT(system, data, screenLocation)
HEADER: EXAMINE(user. netpiece)
DECOMPOSITION: CONSIDER-ASPECT(user. netpiece)
HEADER: CONSIDER-ASPECT(user. netpiece)
DECOMPOSITION: DISPLAY(system, user, netpiece)
</table>
<figureCaption confidence="0.908610333333333">
Figure 4. Graphic Editor Domain Plans.
REQUEST (user. system. Dl:DISPLAY (sys-
tem. user, El))
</figureCaption>
<bodyText confidence="0.999587638888889">
where El stands for &amp;quot;the generic concept called
&apos;employee.— As in Allen and Perrault [1], determina-
tion of such a literal6 speech act is fairly straightfor-
ward. Imperatives indicate REQUESTS and the pro-
positional content (e.g. DISPLAY) is determined via
the standard syntactic and semantic analysis of most
parsers.
Since at the beginning of a dialogue there is no
discourse context, the plan recognizer tries to intro-
duce a plan (or plans) according to coherence prefer-
ence (3). Using the plan schemas of the second sec-
tion, the REQUEST above, and the process of for-
ward chaining via plan decomposition. the system pos-
tulates that the utterance is the decomposition of
INTRODUCE-PLAN( user, system. DI, ?plan), where
STEP(D1, ?plan) and AGENT(D1, system). The
hypothesis is then evaluated using the set of plan
heuristics, e.g. the effects of the plan must not
already be true and the constraints of every recog-
nized plan must be satisfiable. To &apos;satisfy the STEP
constraint a plan containing D1 will be created. Noth-
ing more needs to be done with respect to the second
constraint since it is already satisfied. Finally, since
INTRODUCE-PLAN is not a step in any other plan,
further chaining stops.
The system then expands the introduced plan con-
taining D1, using an analogous plan recognition pro-
cess. Since the display action could be a step of the
CONSIDER-ASPECT plan, which itself could be a
step of either the ADD-DATA or EXAMINE plans.
the domain plan is ambiguous. Note that heuristics
can not eliminate either possibility, since at the begin-
ning of the dialogue any domain plan is a reasonable
expectation. Chaining halts at this branch point and
since no more plans are introduced the process of plan
recognition also ends. The final hypothesis is that the
</bodyText>
<footnote confidence="0.9783515">
6See Litman [10] for a discussion of the treatment of indirect
speech acts (Searle [201).
</footnote>
<page confidence="0.998588">
219
</page>
<bodyText confidence="0.955440675675676">
user executed a discourse plan to introduce either the
domain plan ADD-DATA or EXAMINE.
Once the plan structures are recognized. their
effects are asserted and the postulated plans are
expanded top down to include any other steps (using
the information in the plan descriptions). The plan
recognizer then constructs a stack representing each
hypothesis, as shown in Figure 5. The first stack has
PLAN1 at the top, PLAN2 at the bottom, and encodes
the information that PLAN1 was executed while
PLAN2 will be executed upon completion of PLAN1.
The second stack is analogous. Solid lines represent
plan recognition inferences due to forward chaining,
while dotted lines represent inferences due to later
plan expansion. As desired, the plan recognizer has
constructed a plan-based interpretation of the utter-
ance in terms of expected discourse and domain plans,
an interpretation which can then be used to construct
and generate a response. For example, in either
hypothesis the system can pop the completed plan
introduction and execute D1, the next action in both
domain plans. Since the higher level plan containing
D1 is still ambiguous, deciding exactly what to do is an
interesting plan generation issue.
Unfortunately, the system chooses a display that
does not allow room for the insertion of a new con-
cept, leading to the user&apos;s response No, move the con-
cept up.&amp;quot; The utterance is parsed and input to the plan
recognizer as the clue word &amp;quot;no&amp;quot; (using the plan
recognizer&apos;s list of standard linguistic clues) followed
by the REQUEST(user, system, Ml:MOVE(system.
El, up)) (assuming the resolution of &amp;quot;the concept&amp;quot; to
El). The plan recognition algorithm then proceeds in
both contexts postulated above. Using the knowledge
that &amp;quot;no&amp;quot; typically does not signal a topic continuation,
the plan recognizer first modifies its default mode of
processing, i.e. the assumption that the REQUEST is
a CONTINUE-PLAN (preference 1) is overruled.
Note, however, that even without such a linguistic clue
recognition of a plan continuation would have ulti-
mately failed, since in both stacks CONTINUE-
PLAN&apos;s constraint STEP(M1. PLAN2/PLAN3) would
have failed. The clue thus allows the system to reach
reasonable hypotheses more efficiently. since unlikely
inferences are avoided.
Proceeding with preference (2), the system postu-
lates that either PLAN2 or PLAN3 is being corrected,
i.e., a discourse plan correcting one of the stacked
plans is hypothesized. Since the REQUEST matches
both decompositions of CORRECT-PLAN, there are
two possibilities: CORRECT-PLAN(user, system,
?laststep, Ml, ?nextstep, ?plan), and CORRECT-
PLAN(user, system, ?laststep, ?newstep, Ml, ?plan),
where the variables in each will be bound as a result
of constraint and prerequisite satisfaction from appli-
cation of the heuristics. For example, candidate plans
are only reasonable if their prerequisites were true,
i.e. (in both stacks and corrections) WANT(system,
?plan) and LAST(?laststep. ?plan). Assuming the plan
was executed in the context of PLAN2 or PLAN3
(after PLAN1 or PLANIa was popped and the
DISPLAY performed). ?plan could only have been
bound to PLAN2 or PLAN3. and ?laststep bound to
DI. Satisfaction of the constraints eliminates the
PLAN3 binding, since the constraints indicate at least
two steps in the plan. while PLAN3 contains a single
step described at different levels of abstraction. Satis-
faction of the constraints also eliminates the second
CORRECT-PLAN interpretation, since STEP( MI.
PLAN2) is not true. Thus only the first correction on
the first stack remains plausible, and in fact, using
PLAN2 and the first correction the rest of the con-
straints can be satisfied. In particular, the bindings
yield
</bodyText>
<table confidence="0.990476444444444">
PLAN1 [completed] PLANla [completed]
INTRODUCE-PLAN(user,system,D1,PLAN2) INTRODUCE-PLAN(user.system.D1.PLAN3)
REQUEST(user,system.D1) REQUEST(user.system.D1)
[LAST] [LAST]
PLAN2 PLAN3
ADD-DATA(user, El,. ?data, ?loc) EXAMINE(user,E1)
CONSIDER-ser.E1) PUT(sysiern.?data.?loc) CONSIDER-A3ECT(user.E1)
Dl:DISPLA(system.user.E1) DLDISPLAY(sysitem.user.E1)
[NEXT] [NEXT]
</table>
<figureCaption confidence="0.99717">
Figure 5. The Two Plan Stacks after the First Utterance.
</figureCaption>
<page confidence="0.977767">
220
</page>
<reference confidence="0.586863857142857">
(1) STEP(DI, PLAN2)
(2) STEP(P1, PLAN2)
(3) AFTER(DI, P1. PLAN2)
(4) AGENT(M1, system)
(5) -CANDO(user, PI)
(6) MODIFIES(M1, DI)
(7) ENABLES(Ml. P1)
</reference>
<bodyText confidence="0.9990634">
where P1 stands for PUT(system, ?data, ?loc).
resulting in the hypothesis CORRECT-PLAN(user.
system. DI, MI. PI, PLAN2). Note that a final possi-
ble hypothesis for the REQUEST, e.g. introduction of
a new plan. is discarded since it does not tie in with
any of the expectations (i.e. a preference (2) choice is
preferred over a preference (3) choice).
The effects of CORRECT-PLAN are asserted
(M1 is inserted into PLAN2 and marked as NEXT)
and CORRECT-PLAN is pushed on to the stack
suspending the plan corrected, as shown in Figure 6.
The system has thus recognized not only that an
interruption of ADD-DATA has occurred, but also
that the relationship of interruption is one of plan
correction. Note that unlike the first utterance, the
plan referred to by the second utterance is found in
the stack rather than constructed. Using the updated
stack, the system can then pop the completed correc-
tion and resume PLAN2 with the new (next) step Ml.
The system parses the user&apos;s next utterance
(&amp;quot;Now, make an individual employee concept whose
first name is &apos;Sam&apos; and whose last names is &apos;Jones&apos;&amp;quot;)
and again picks up an initial clue word, this time one
that explicitly marks the utterance as a continuation
and thus reinforces coherence preference (1). The
utterance can indeed be recognized as a continuation
of PLAN2, e.g. CONTINUE-PLAN( user, system.
Ml, MAKEI, PLAN2), analogously to the above
detailed explanations. M1 and PLAN2 are bound due
to prerequisite satisfaction, and MAKE1 chained
through P1 due to constraint satisfaction. The updated
stack is shown in Figure 7. At this stage. it would then
be appropriate for the system to pop the completed
CONTINUE plan and resume execution of PLAN2 by
performing MAKE1.
</bodyText>
<figure confidence="0.9973942">
PLAN4 [completed]
CLCORRECT-PLAN(user.system.DLMI.PLPLAN2)
REQUEST(user,system,M1)
[LAST!
PLAN2
ADD-DATA(user,E1.?data,?loc)
CONSIDER- SPECT(user,E1) MLMOVE system.El.up) Pl:PUT(system,?data,?loc)
[NEXT]
D1:DISPLAY(system,user,E1)
[LAST]
</figure>
<figureCaption confidence="0.998778">
Figure 6. The Plan Stack after the User&apos;s Second Utterance.
</figureCaption>
<figure confidence="0.9984882">
[completed]
CONTINUE-PLAN(user,system,M1,MAKE1,PLAN2)
REQUEST(user,system,MAKE1)
[LAST]
PLAN2
ADD-DATA(use .ELSamIones.?loc)
CONSIDER SPECT(user.E1) MLMOVE system.E1,up) PLPUT(s) stem.SamIones,?loc)
[LAST]
D1:DISPLAY(system,user,E1) MAKELMAKE(system,user,SamJones)
[NEXT]
</figure>
<figureCaption confidence="0.999972">
Figure 7. Continuation of the Domain Plan.
</figureCaption>
<page confidence="0.995359">
221
</page>
<sectionHeader confidence="0.979211" genericHeader="conclusions">
CONCLUSIONS
</sectionHeader>
<bodyText confidence="0.999984342857143">
This paper has presented a framework for both
representing as well as recognizing relationships
between utterances. The framework, based on the
assumption that people&apos;s utterances reflect underlying
plans, reformulates the complex inferential processes
relating utterances within a plan-based theory of
dialogue understanding. A set of meta-plans called
discourse plans were introduced to explicitly formalize
utterance relationships in terms of a small set of
underlying plan manipulations. Unlike previous
models of coherence, the representation was accom-
panied by a fully specified model of computation
based on a process of plan recognition. Constraint
satisfaction is used to coordinate the recognition of
discourse plans, domain plans. and their relationships.
Linguistic phenomena associated with coherence rela-
tionships are used to guide the discourse plan recogni-
tion process.
Although not the focus of this paper, the incor-
poration of topic relationships into a plan-based
framework can also be seen as an extension of work in
plan recognition. For example, Sidner [21,24]
analyzed debuggings (as in the dialogue above) in
terms of multiple plans underlying a single utterance.
As discussed fully in Litman and Allen [11], the
representation and recognition of discourse plans is a
systemization and generalization of this approach.
Use of even a small set of discourse plans enables the
principled understanding of previously problematic
classes of dialogues in several task-oriented domains.
Ultimately the generality of any plan-based approach
depends on the ability to represent any domain of
discourse in terms of a set of underlying plans.
Recent work by Grosz and Sidner [7] argues for the
validity of this assumption.
</bodyText>
<sectionHeader confidence="0.984953" genericHeader="acknowledgments">
ACKNOWLEDGEMENTS
</sectionHeader>
<bodyText confidence="0.999561">
I would like to thank Julia Hirschberg, Marcia
Derr, Mark Jones, Mark Kahrs, and Henry Kautz for
their helpful comments on drafts of this paper.
</bodyText>
<sectionHeader confidence="0.998557" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999964107692308">
1. J. F. Allen and C. R. Perrault, Analyzing
Intention in Utterances, Artificial Intelligence 15,
3 (1980), 143-178.
2. S. Carberry, Tracking User Goals in an
Information-Seeking Environment, AAAI,
Washington, D.C., August 1983. 59-63.
3. R. Cohen, A Computational Model for the
Analysis of Arguments, Ph.D. Thesis and Tech.
Rep. 151, University of Toronto. October 1983.
4. R. E. Fikes and N. J. Nilsson. STRIPS: A new
Approach to the Application of Theorem
Proving to Problem Solving. Artificial Intelligence
2, 3/4 (1971), 189-208.
5. B. J. Grosz, The Representation and Use of
Focus in Dialogue Understanding, Technical
Note 151, SRI, July 1977.
6. B. J. Grosz, A. K. Joshi and S. Weinstein.
Providing a Unified Account of Definite Noun
Phrases in Discourse. ACL. MIT, June 1983, 44-
50.
7. B. J. Grosz and C. L. Sidner, Discourse Structure
and the Proper Treatment of Interruptions,
IJCAI, Los Angeles, August 1985, 832-839.
8. J. R. Hobbs, On the Coherence and Structure of
Discourse, in The Structure of Discourse, L.
Polanyi (ed.), Ablex Publishing Corporation,
Forthcoming. Also CSLI (Stanford) Report No.
CSLI-85-37, October 1985.
9. D. J. Litman and J. F. Allen, A Plan Recognition
Model for Clarification Subdialogues, Coling84,
Stanford, July 1984, 302-311.
10. D. J. Litman, Plan Recognition and Discourse
Analysis: An Integrated Approach for
Understanding Dialogues, PhD Thesis and
Technical Report 170, University of Rochester,
1985.
11. D. J. Litman and J. F. Allen. A Plan Recognition
Model for Subdialogues in Conversation,
Cognitive Science. , to appear. , Also University
of Rochester Tech. Rep. 141. November 1984.
12. W. Mann, Corpus of Computer Operator
Transcripts, Unpublished Manuscript, ISI, 1970&apos;s.
13. W. C. Mann, Discourse Structures for Text
Generation, Coling84, Stanford, July 1984, 367-
375.
14. K. R. McKeown, Generating Natural Language
Text in Response to Questions about Database
Structure, PhD Thesis, University of
Pennsylvania, Philadelphia, 1982.
15. L. Polanyi and R. J. H. Scha, The Syntax of
Discourse, Text (Special Issue: Formal Methods
of Discourse Analysis) 3, 3 (1983), 261-270.
16. R. Reichman, Conversational Coherency.
Cognitive Science 2, 4 (1978), 283-328.
17. R. Reichman-Adar, Extended Person-Machine
Interfaces, Artificial Intelligence 22, 2 (1984),
157-218.
18. E. D. Sacerdoti. A Structure for Plans and
Behavior. Elsevier. New York. 1977.
19. J. R. Searle, in Speech Acts, an Essay in the
Philosophy of Language, Cambridge University
Press, New York, 1969.
20. J. R. Searle, Indirect Speech Acts, in Speech Acts.
vol. 3, P. Cole and Morgan (ed.), Academic
Press. New York, NY, 1975.
</reference>
<page confidence="0.967305">
222
</page>
<reference confidence="0.999802578947368">
21. C. L. Sidner and D. J. Israel. Recognizing
Intended Meaning and Speakers&apos; Plans, IJCAI,
Vancouver, 1981, 203-208.
22. C. L. Sidner, Protocols of Users Manipulating
Visually Presented Information with Natural
Language, Report 5128. Bolt Beranek and
Newman , September 1982.
23. C. L. Sidner and M. Bates. Requirements of
Natural Language Understanding in a System
with Graphic Displays. Report Number 5242,
Bolt Beranek and Newman Inc.. March 1983.
24. C. L. Sidner. Plan Parsing for Intended Response
Recognition in Discourse, Computational
Intelligence 1, 1 (February 1985). 1-10.
25. M. Stefik, Planning with Constraints (MOLGEN:
Part 1), Artificial Intelligence 16, (1981), 111-140.
26. R. Wilensky, Planning and Understanding,
Addison-Wesley Publishing company, Reading,
Massachusetts, 1983.
</reference>
<page confidence="0.99917">
223
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.866714">
<title confidence="0.991229">LINGUISTIC COHERENCE: A PLAN-BASED ALTERNATIVE</title>
<author confidence="0.999999">Diane J Litman</author>
<affiliation confidence="0.999948">AT&amp;T Bell Laboratories</affiliation>
<address confidence="0.967963">3C-408A 600 Mountain Avenue Hill, NJ</address>
<abstract confidence="0.997611823529412">To fully understand a sequence of utterances, one must be able to infer implicit relationships between the utterances. Although the identification of sets of utterance relationships forms the basis for many theories of discourse, the formalization and recognition of such relationships has proven to be an extremely difficult computational task. This paper presents a plan-based approach to the representation and recognition of implicit relationships between utterances. Relationships are formulated as discourse plans. which allows their representation in terms of planning operators and their computation via a plan recognition process. By incorporating complex inferential processes relating utterances into a plan-based framework, a formalization and computability not available in the earlier works is provided.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>STEP</author>
</authors>
<title>PLAN2) (2) STEP(P1, PLAN2) (3) AFTER(DI, P1. PLAN2) (4) AGENT(M1, system) (5) -CANDO(user,</title>
<journal>PI</journal>
<volume>6</volume>
<pages>1</pages>
<marker>STEP, </marker>
<rawString> (1) STEP(DI, PLAN2) (2) STEP(P1, PLAN2) (3) AFTER(DI, P1. PLAN2) (4) AGENT(M1, system) (5) -CANDO(user, PI) (6) MODIFIES(M1, DI) (7) ENABLES(Ml. P1)</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F Allen</author>
<author>C R Perrault</author>
</authors>
<title>Analyzing Intention in Utterances,</title>
<date>1980</date>
<journal>Artificial Intelligence</journal>
<volume>15</volume>
<pages>143--178</pages>
<contexts>
<context position="6142" citStr="[1]" startWordPosition="908" endWordPosition="908">ing linguistic clues, coherence preferences, and constraint satisfaction. Finally, a detailed example of the processing of a dialogue fragment is presented, illustrating the recognition of various types of relationships between utterances. REPRESENTING COHERENCE USING DISCOURSE PLANS In a plan-based approach to language understanding, an utterance is considered understood when it has been related to some underlying plan of the speaker. While previous works have explicitly represented and recognized the underlying task plans of a given domain (e.g., mount a tape) (Grosz [5], Allen and Perrault [1], Sidner and Israel [21]. Carberry [2], Sidner [24]), the ways that utterances could be related to such plans were limited and not of particular concern. As a result, only dialogues exhibiting a very limited set of utterance relationships could be understood. In this work, a set of domain-independent plans about plans (i.e. meta-plans) called discourse plans are introduced to explicitly represent, reason about, and generalize such relationships. Discourse plans are recognized from every utterance and represent plan introduction, plan execution, plan specification, plan debugging, plan abandonm</context>
<context position="15846" citStr="[1]" startWordPosition="2387" endWordPosition="2387">it&apos;s time for lunch yet?&amp;quot; Since eating lunch cannot be related to the previous editing plan topic, the system recognizes the utterance as a total change of topic, i.e. INTRODUCE-PLAN(User, System, System tell User if time for lunch, eat lunch plan). RECOGNIZING DISCOURSE PLANS This section presents a computational algorithm for the recognition of discourse plans. Recall that the previous lack of such an algorithm was in fact a major force behind the last section&apos;s plan-based formalization of the linguistic relationships. Previous work in the area of domain plan recognition (Allen and Perrault [1], Sidner and Israel [21]. Carberry [2], Sidner [24]) provides a partial solution to the recognition problem. For example, since discourse plans are represented identically to domain plans, the same process of plan recognition can apply to both. In particular, every plan is recognized by an incremental process of heuristic search. From an input, the plan recognizer tries to find a plan for which the input is a step,4 and then tries to find more abstract plans for which the postulated plan is a step, and so on. After every step of this chaining process. a set of heuristics prune the candidate pl</context>
<context position="23068" citStr="[1]" startWordPosition="3519" endWordPosition="3519">r thinking about it. The processing begins with a speech act analysis of &amp;quot;Show me the generic concept called &apos;employee— HEADER: ADD-DATA(user, netpiece, data, screenLocation) DECOMPOSITION: CONSIDER-ASPECT(user. netpiece) PUT(system, data, screenLocation) HEADER: EXAMINE(user. netpiece) DECOMPOSITION: CONSIDER-ASPECT(user. netpiece) HEADER: CONSIDER-ASPECT(user. netpiece) DECOMPOSITION: DISPLAY(system, user, netpiece) Figure 4. Graphic Editor Domain Plans. REQUEST (user. system. Dl:DISPLAY (system. user, El)) where El stands for &amp;quot;the generic concept called &apos;employee.— As in Allen and Perrault [1], determination of such a literal6 speech act is fairly straightforward. Imperatives indicate REQUESTS and the propositional content (e.g. DISPLAY) is determined via the standard syntactic and semantic analysis of most parsers. Since at the beginning of a dialogue there is no discourse context, the plan recognizer tries to introduce a plan (or plans) according to coherence preference (3). Using the plan schemas of the second section, the REQUEST above, and the process of forward chaining via plan decomposition. the system postulates that the utterance is the decomposition of INTRODUCE-PLAN( us</context>
</contexts>
<marker>1.</marker>
<rawString>J. F. Allen and C. R. Perrault, Analyzing Intention in Utterances, Artificial Intelligence 15, 3 (1980), 143-178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Carberry</author>
</authors>
<title>Tracking User Goals in an Information-Seeking Environment,</title>
<date>1983</date>
<pages>59--63</pages>
<location>AAAI, Washington, D.C.,</location>
<contexts>
<context position="6180" citStr="[2]" startWordPosition="914" endWordPosition="914">ences, and constraint satisfaction. Finally, a detailed example of the processing of a dialogue fragment is presented, illustrating the recognition of various types of relationships between utterances. REPRESENTING COHERENCE USING DISCOURSE PLANS In a plan-based approach to language understanding, an utterance is considered understood when it has been related to some underlying plan of the speaker. While previous works have explicitly represented and recognized the underlying task plans of a given domain (e.g., mount a tape) (Grosz [5], Allen and Perrault [1], Sidner and Israel [21]. Carberry [2], Sidner [24]), the ways that utterances could be related to such plans were limited and not of particular concern. As a result, only dialogues exhibiting a very limited set of utterance relationships could be understood. In this work, a set of domain-independent plans about plans (i.e. meta-plans) called discourse plans are introduced to explicitly represent, reason about, and generalize such relationships. Discourse plans are recognized from every utterance and represent plan introduction, plan execution, plan specification, plan debugging, plan abandonment, and so on, independently of any d</context>
<context position="15884" citStr="[2]" startWordPosition="2393" endWordPosition="2393"> lunch cannot be related to the previous editing plan topic, the system recognizes the utterance as a total change of topic, i.e. INTRODUCE-PLAN(User, System, System tell User if time for lunch, eat lunch plan). RECOGNIZING DISCOURSE PLANS This section presents a computational algorithm for the recognition of discourse plans. Recall that the previous lack of such an algorithm was in fact a major force behind the last section&apos;s plan-based formalization of the linguistic relationships. Previous work in the area of domain plan recognition (Allen and Perrault [1], Sidner and Israel [21]. Carberry [2], Sidner [24]) provides a partial solution to the recognition problem. For example, since discourse plans are represented identically to domain plans, the same process of plan recognition can apply to both. In particular, every plan is recognized by an incremental process of heuristic search. From an input, the plan recognizer tries to find a plan for which the input is a step,4 and then tries to find more abstract plans for which the postulated plan is a step, and so on. After every step of this chaining process. a set of heuristics prune the candidate plan set based on assumptions regarding </context>
</contexts>
<marker>2.</marker>
<rawString>S. Carberry, Tracking User Goals in an Information-Seeking Environment, AAAI, Washington, D.C., August 1983. 59-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Cohen</author>
</authors>
<title>A Computational Model for the Analysis of Arguments,</title>
<date>1983</date>
<tech>Ph.D. Thesis and Tech. Rep. 151,</tech>
<institution>University of Toronto.</institution>
<contexts>
<context position="2619" citStr="[3]" startWordPosition="397" endWordPosition="397">nt N00014-82-K-0193. NSF under Grant DCR8351665. and ONR under Grant N0014-80-C-0197. ermore, because the second utterance violates an expectation of discourse coherence (Reichman [16]. Hobbs [8], Grosz, Joshi, and Weinstein [6]), the utterance seems inappropriate since there are no linguistic clues (for example, prefacing the utterance with &amp;quot;incidentally&amp;quot;) marking it as a topic change. The identification and specification of sets of linguistic relationships between utterances2 forms the basis for many computational models of discourse (Reichman [17], McKeown [14], Mann [13], Hobbs [8], Cohen [3]). By limiting the relationships allowed in a system and the ways in which relationships coherently interact, efficient mechanisms for understanding and generating well organized discourse can be developed. Furthermore, the approach provides a framework for explaining the use of surface linguistic phenomena such as clue words, words like &amp;quot;incidentally&amp;quot; that often correspond to particular relationships between utterances. Unfortunately. while these theories propose relationships that seem intuitive (e.g. &amp;quot;elaboration,&amp;quot; as might be used in the first fragment above), there has been little agreeme</context>
<context position="19565" citStr="[3]" startWordPosition="2978" endWordPosition="2978">ciple of Occam&apos;s razor, since they are ordered to introduce as few new plans as possible. If within one of these preferences there are still competing interpretations, the interpretation that most corresponds to a stack discipline is preferred. For example, a continuation resuming a recently interrupted topic is preferred to continuation of a topic interrupted earlier in the conversation. Finally, since the plan recognizer now recognizes implicit relationships between utterances, linguistic clues signaling such relationships (Grosz [5]. Reichman [17]. Polanyi and Scha [15], Sidner [24], Cohen [3], Grosz and Sidner [7]) should be exploitable by the plan recognition algorithm. In other words, the plan recognizer should be aware of correlations between expressed so that chaining via decompositions is sufficient. &apos;Although Wilensky [26] introduced meta-plans into a natural language system to handle a totally different issue, that of concurrent goal interaction, he does not address details of coordination. 218 specific words and the discourse plans they typically signal. Clues can then be used both to reinforce as well as to overrule the preference ordering given above. In fact, in the lat</context>
</contexts>
<marker>3.</marker>
<rawString>R. Cohen, A Computational Model for the Analysis of Arguments, Ph.D. Thesis and Tech. Rep. 151, University of Toronto. October 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Fikes</author>
<author>N J Nilsson</author>
</authors>
<title>STRIPS: A new Approach to the Application of Theorem Proving to Problem Solving.</title>
<date>1971</date>
<journal>Artificial Intelligence</journal>
<volume>2</volume>
<pages>189--208</pages>
<contexts>
<context position="7351" citStr="[4]" startWordPosition="1098" endWordPosition="1098">nd so on, independently of any domain. Although discourse plans can refer to both domain plans or other discourse plans. domain plans can only be accessed and manipulated via discourse plans. For example, in the tape excerpt above &amp;quot;Could you mount a magtape for me?&amp;quot; achieves a discourse plan to introduce a domain plan to mount a tape. &amp;quot;It&apos;s tape 1&amp;quot; then further specifies this domain plan. Except for the fact that they refer to other plans (i.e. they take other plans as arguments), the representation of discourse plans is identical to the usual representation of domain plans (Fikes and Nilsson [4], Sacerdoti [18]). Every plan has a header, a parameterized action description that names the plan. Action descriptions are represented as operators on a planner&apos;s world model and defined in terms of prerequisites, decompositions, and effects. Prerequisites are conditions that need to hold (or to be made to hold) in the world model before the action operator can be applied. Effects are statements that are asserted into the world model after the action has been successfully executed. Decompositions enable hierarchical planning. Although the action description of the header may be usefully thoug</context>
</contexts>
<marker>4.</marker>
<rawString>R. E. Fikes and N. J. Nilsson. STRIPS: A new Approach to the Application of Theorem Proving to Problem Solving. Artificial Intelligence 2, 3/4 (1971), 189-208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
</authors>
<title>The Representation and Use of Focus</title>
<date>1977</date>
<booktitle>in Dialogue Understanding, Technical Note 151, SRI,</booktitle>
<contexts>
<context position="6118" citStr="[5]" startWordPosition="903" endWordPosition="903">recognize such plans, using linguistic clues, coherence preferences, and constraint satisfaction. Finally, a detailed example of the processing of a dialogue fragment is presented, illustrating the recognition of various types of relationships between utterances. REPRESENTING COHERENCE USING DISCOURSE PLANS In a plan-based approach to language understanding, an utterance is considered understood when it has been related to some underlying plan of the speaker. While previous works have explicitly represented and recognized the underlying task plans of a given domain (e.g., mount a tape) (Grosz [5], Allen and Perrault [1], Sidner and Israel [21]. Carberry [2], Sidner [24]), the ways that utterances could be related to such plans were limited and not of particular concern. As a result, only dialogues exhibiting a very limited set of utterance relationships could be understood. In this work, a set of domain-independent plans about plans (i.e. meta-plans) called discourse plans are introduced to explicitly represent, reason about, and generalize such relationships. Discourse plans are recognized from every utterance and represent plan introduction, plan execution, plan specification, plan </context>
<context position="11479" citStr="[5]" startWordPosition="1726" endWordPosition="1726">ed by the predicate LAST. One way of doing this is to request the hearer to perform the step that should occur after the previously executed step. assuming of course that the step is something the hearer actually can perform. This is captured by the decomposition together with the constraints. As above, the NEXT effect then updates the portion of the plan to be executed. This discourse plan captures the previously implicit relationship of coherent topic continuation in task-oriented dialogues (without interruptions), i.e. the fact that the discourse structure follows the task structure (Grosz [5]). Figure 3 presents CORRECT-PLAN, the last discourse plan to be discussed. CORRECT-PLAN inserts a repair step into a pre-existing plan that would otherwise fail. More specifically, CORRECT-PLAN takes a pre-existing plan having subparts that do not interact as expected during execution, and debugs the plan by adding a new goal to restore the expected interactions. The pre-existing plan has subparts laststep and nextstep, where laststep was supposed to enable the performance of nextstep, but in reality did not. The plan is corrected by adding newstep, which HEADER: CORRECT-PLAN(speaker. hearer.</context>
<context position="19503" citStr="[5]" startWordPosition="2967" endWordPosition="2967">andled when they do occur. The heuristics also follow the principle of Occam&apos;s razor, since they are ordered to introduce as few new plans as possible. If within one of these preferences there are still competing interpretations, the interpretation that most corresponds to a stack discipline is preferred. For example, a continuation resuming a recently interrupted topic is preferred to continuation of a topic interrupted earlier in the conversation. Finally, since the plan recognizer now recognizes implicit relationships between utterances, linguistic clues signaling such relationships (Grosz [5]. Reichman [17]. Polanyi and Scha [15], Sidner [24], Cohen [3], Grosz and Sidner [7]) should be exploitable by the plan recognition algorithm. In other words, the plan recognizer should be aware of correlations between expressed so that chaining via decompositions is sufficient. &apos;Although Wilensky [26] introduced meta-plans into a natural language system to handle a totally different issue, that of concurrent goal interaction, he does not address details of coordination. 218 specific words and the discourse plans they typically signal. Clues can then be used both to reinforce as well as to ove</context>
</contexts>
<marker>5.</marker>
<rawString>B. J. Grosz, The Representation and Use of Focus in Dialogue Understanding, Technical Note 151, SRI, July 1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>A K Joshi</author>
<author>S Weinstein</author>
</authors>
<title>Providing a Unified Account of Definite Noun Phrases</title>
<date>1983</date>
<booktitle>in Discourse. ACL. MIT,</booktitle>
<pages>44</pages>
<contexts>
<context position="2244" citStr="[6]" startWordPosition="343" endWordPosition="343">n between the two utterances. While one could postulate some connection (e.g., the speaker&apos;s magtape contains a database of places to go skiing), more likely one would say that there is no relationship between the utterances. Furth&apos;This work was done at the Department of Computer Science. University of Rochester. Rochester NY 14627. and supported in part by DARPA under Grant N00014-82-K-0193. NSF under Grant DCR8351665. and ONR under Grant N0014-80-C-0197. ermore, because the second utterance violates an expectation of discourse coherence (Reichman [16]. Hobbs [8], Grosz, Joshi, and Weinstein [6]), the utterance seems inappropriate since there are no linguistic clues (for example, prefacing the utterance with &amp;quot;incidentally&amp;quot;) marking it as a topic change. The identification and specification of sets of linguistic relationships between utterances2 forms the basis for many computational models of discourse (Reichman [17], McKeown [14], Mann [13], Hobbs [8], Cohen [3]). By limiting the relationships allowed in a system and the ways in which relationships coherently interact, efficient mechanisms for understanding and generating well organized discourse can be developed. Furthermore, the a</context>
</contexts>
<marker>6.</marker>
<rawString>B. J. Grosz, A. K. Joshi and S. Weinstein. Providing a Unified Account of Definite Noun Phrases in Discourse. ACL. MIT, June 1983, 44-</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>C L Sidner</author>
</authors>
<date>1985</date>
<booktitle>Discourse Structure and the Proper Treatment of Interruptions, IJCAI,</booktitle>
<pages>832--839</pages>
<location>Los Angeles,</location>
<contexts>
<context position="17962" citStr="[7]" startWordPosition="2738" endWordPosition="2738">n brief, the plan recognizer recognizes a discourse plan from every utterance, then uses a process of constraint satisfaction to initiate recognition of the domain and any other discourse plans related to the utterance. Furthermore, to record and monitor execution of the discourse and domain plans active at any point in a dialogue, a dialogue context in the form of a plan stack is built and maintained by the plan recognizer. Various models of discourse have argued that an ideal interrupting topic structure follows a stack-like discipline (Reichman [17], Polanyi and Scha [15], Grosz and Sidner [7]). The plan recognition algorithm will be reviewed when tracing through the example of the next section. Since discourse plans reflect linguistic relationships between utterances, the earlier work on domain plan recognition can also be augmented in several other ways. For example, the search process can be constrained by adding heuristics that prefer discourse plans corresponding to the most linguistically coherent continuations of the dialogue. More specifically, in the absence of any linguistic clues (as will be described below), the plan recognizer will prefer relationships that, in the fol</context>
<context position="19587" citStr="[7]" startWordPosition="2982" endWordPosition="2982">, since they are ordered to introduce as few new plans as possible. If within one of these preferences there are still competing interpretations, the interpretation that most corresponds to a stack discipline is preferred. For example, a continuation resuming a recently interrupted topic is preferred to continuation of a topic interrupted earlier in the conversation. Finally, since the plan recognizer now recognizes implicit relationships between utterances, linguistic clues signaling such relationships (Grosz [5]. Reichman [17]. Polanyi and Scha [15], Sidner [24], Cohen [3], Grosz and Sidner [7]) should be exploitable by the plan recognition algorithm. In other words, the plan recognizer should be aware of correlations between expressed so that chaining via decompositions is sufficient. &apos;Although Wilensky [26] introduced meta-plans into a natural language system to handle a totally different issue, that of concurrent goal interaction, he does not address details of coordination. 218 specific words and the discourse plans they typically signal. Clues can then be used both to reinforce as well as to overrule the preference ordering given above. In fact, in the latter case clues ease th</context>
</contexts>
<marker>7.</marker>
<rawString>B. J. Grosz and C. L. Sidner, Discourse Structure and the Proper Treatment of Interruptions, IJCAI, Los Angeles, August 1985, 832-839.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
</authors>
<title>On the Coherence and Structure of Discourse,</title>
<date>1985</date>
<booktitle>in The Structure of Discourse,</booktitle>
<tech>Report No. CSLI-85-37,</tech>
<editor>L. Polanyi (ed.),</editor>
<contexts>
<context position="2211" citStr="[8]" startWordPosition="338" endWordPosition="338">now there is no obvious connection between the two utterances. While one could postulate some connection (e.g., the speaker&apos;s magtape contains a database of places to go skiing), more likely one would say that there is no relationship between the utterances. Furth&apos;This work was done at the Department of Computer Science. University of Rochester. Rochester NY 14627. and supported in part by DARPA under Grant N00014-82-K-0193. NSF under Grant DCR8351665. and ONR under Grant N0014-80-C-0197. ermore, because the second utterance violates an expectation of discourse coherence (Reichman [16]. Hobbs [8], Grosz, Joshi, and Weinstein [6]), the utterance seems inappropriate since there are no linguistic clues (for example, prefacing the utterance with &amp;quot;incidentally&amp;quot;) marking it as a topic change. The identification and specification of sets of linguistic relationships between utterances2 forms the basis for many computational models of discourse (Reichman [17], McKeown [14], Mann [13], Hobbs [8], Cohen [3]). By limiting the relationships allowed in a system and the ways in which relationships coherently interact, efficient mechanisms for understanding and generating well organized discourse can</context>
<context position="3719" citStr="[8]" startWordPosition="558" endWordPosition="558">itive (e.g. &amp;quot;elaboration,&amp;quot; as might be used in the first fragment above), there has been little agreement on what the set of possible relationships should be, or even if such a set can be defined. Furthermore, since the formalization of the relationships has proven to be an extremely difficult task, such theories typically have to depend on unrealistic computational processes. For example. Cohen [3] uses an oracle to recognize her &amp;quot;evidence&amp;quot; relationships. Reichman&apos;s [17] use of a set of conversational moves depends on the future development of extremely sophisticated semantics modules. Hobbs [8] acknowledges that his theory of coherence relations &amp;quot;may seem to be appealing to magic,&amp;quot; since there are several places where he appeals to as yet incomplete subtheories. Finally, Mann [13] notes that his theory of rhetorical predicates is currently descriptive rather than constructive. McKeown&apos;s [14] implemented system of rhetorical predicates is a notable exception, but since her predicates have associated semantics expressed in terms of a specific data base system the approach is not particularly general. =Although in some theories relationships hold between group of utterances, in others </context>
</contexts>
<marker>8.</marker>
<rawString>J. R. Hobbs, On the Coherence and Structure of Discourse, in The Structure of Discourse, L. Polanyi (ed.), Ablex Publishing Corporation, Forthcoming. Also CSLI (Stanford) Report No. CSLI-85-37, October 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Litman</author>
<author>J F Allen</author>
</authors>
<title>A Plan Recognition Model for Clarification Subdialogues,</title>
<date>1984</date>
<pages>302--311</pages>
<location>Coling84, Stanford,</location>
<contexts>
<context position="17356" citStr="[9,11]" startWordPosition="2640" endWordPosition="2640">gnition process is also incremental; if the heuristics cannot uniquely determine an underlying plan, chaining stops. As mentioned above, however, this is not a full solution. Since the plan recognizer is now recognizing discourse as well as domain plans from a single utterance, the set of recognition processes must be coordi4Plan chaining can also be done via effects and prerequisites. To keep the example in the next section simple. plans have been nated.5 An algorithm for coordinating the recognition of domain and discourse plans from a single utterance has been presented in Litman and Allen [9,11]. In brief, the plan recognizer recognizes a discourse plan from every utterance, then uses a process of constraint satisfaction to initiate recognition of the domain and any other discourse plans related to the utterance. Furthermore, to record and monitor execution of the discourse and domain plans active at any point in a dialogue, a dialogue context in the form of a plan stack is built and maintained by the plan recognizer. Various models of discourse have argued that an ideal interrupting topic structure follows a stack-like discipline (Reichman [17], Polanyi and Scha [15], Grosz and Sidn</context>
</contexts>
<marker>9.</marker>
<rawString>D. J. Litman and J. F. Allen, A Plan Recognition Model for Clarification Subdialogues, Coling84, Stanford, July 1984, 302-311.</rawString>
</citation>
<citation valid="false">
<authors>
<author>D J Litman</author>
</authors>
<title>Plan Recognition and Discourse Analysis: An Integrated Approach for Understanding Dialogues,</title>
<tech>PhD Thesis and Technical Report 170,</tech>
<institution>University of Rochester,</institution>
<contexts>
<context position="8896" citStr="[10]" startWordPosition="1327" endWordPosition="1327"> a set of applicability conditions called constraints.3 These are similar to prerequisites, except that the planner never attempts to achieve a constraint if it is false. The plan recognizer will use such general plan descriptions to recognize the particular plan instantiations underlying an utterance. HEADER: INTRODUCE-PLAN(speaker, hearer DECOMPOSITION: action. plan) EFFECTS: REQUEST(speaker. hearer. action) CONSTRAINTS: WANT(hearer. plan) NEXT(action. plan) STEP(action, plan) AGENT(action. hearer) Figure 1. INTRODUCE-PLAN. Figures 1, 2, and 3 present examples of discourse plans (see Litman [10] for the complete set). The first discourse plan, INTRODUCE-PLAN, takes a plan of the speaker that involves the hearer and presents it to the hearer (who is assumed cooperative). The decomposition specifies a typical way to do this, via execution of the speech act (Searle [19]) REQUEST. The constraints use a vocabulary for referring to and describing plans and actions to specify that the only actions requested will be those that are in the plan and have the hearer as agent. Since the hearer is assumed cooperative, he or she will then adopt as a goal the 3These constraints should not be confuse</context>
<context position="18755" citStr="[10]" startWordPosition="2856" endWordPosition="2856">er work on domain plan recognition can also be augmented in several other ways. For example, the search process can be constrained by adding heuristics that prefer discourse plans corresponding to the most linguistically coherent continuations of the dialogue. More specifically, in the absence of any linguistic clues (as will be described below), the plan recognizer will prefer relationships that, in the following order: (1) continue a previous topic (e.g. CONTINUEPLAN) (2) interrupt a topic for a semantically related topic (e.g. CORRECT-PLAN, other corrections and clarifications as in Litman [10]) (3) interrupt a topic for a totally unrelated topic (e.g. INTRODUCE-PLAN). Thus, while interruptions are not generally predicted, they can be handled when they do occur. The heuristics also follow the principle of Occam&apos;s razor, since they are ordered to introduce as few new plans as possible. If within one of these preferences there are still competing interpretations, the interpretation that most corresponds to a stack discipline is preferred. For example, a continuation resuming a recently interrupted topic is preferred to continuation of a topic interrupted earlier in the conversation. F</context>
<context position="22000" citStr="[10]" startWordPosition="3366" endWordPosition="3366">ed portion of a scenario (Sidner and Bates [23]) developed from the set of protocols described above: User: Show me the generic concept called &amp;quot;employee.&amp;quot; System:OK. &lt;system displays network&gt; User: No, move the concept up. System:OK. &lt;system redisplays network&gt; User: Now, make an individual employee concept whose first name is &amp;quot;Sam&amp;quot; and whose last name is &amp;quot;Jones.&amp;quot; Although the behavior to be described is fully specified by the theory, the implementation corresponds only to the new model of plan recognition. All simulated computational processes have been implemented elsewhere, however. Litman [10] contains a full discussion of the implementation. Figure 4 presents the relevant domain plans for this domain, taken from Sidner and Israel [21] with minor modifications. ADD-DATA is a plan to add new data into a network, while EXAMINE is a plan to examine parts of a network. Both plans involve the subplan CONSIDER-ASPECT, in which the user considers some aspect of a network, for example by looking at it (the decomposition shown), listening to a description, or thinking about it. The processing begins with a speech act analysis of &amp;quot;Show me the generic concept called &apos;employee— HEADER: ADD-DAT</context>
<context position="24764" citStr="[10]" startWordPosition="3802" endWordPosition="3802">her chaining stops. The system then expands the introduced plan containing D1, using an analogous plan recognition process. Since the display action could be a step of the CONSIDER-ASPECT plan, which itself could be a step of either the ADD-DATA or EXAMINE plans. the domain plan is ambiguous. Note that heuristics can not eliminate either possibility, since at the beginning of the dialogue any domain plan is a reasonable expectation. Chaining halts at this branch point and since no more plans are introduced the process of plan recognition also ends. The final hypothesis is that the 6See Litman [10] for a discussion of the treatment of indirect speech acts (Searle [201). 219 user executed a discourse plan to introduce either the domain plan ADD-DATA or EXAMINE. Once the plan structures are recognized. their effects are asserted and the postulated plans are expanded top down to include any other steps (using the information in the plan descriptions). The plan recognizer then constructs a stack representing each hypothesis, as shown in Figure 5. The first stack has PLAN1 at the top, PLAN2 at the bottom, and encodes the information that PLAN1 was executed while PLAN2 will be executed upon c</context>
</contexts>
<marker>10.</marker>
<rawString>D. J. Litman, Plan Recognition and Discourse Analysis: An Integrated Approach for Understanding Dialogues, PhD Thesis and Technical Report 170, University of Rochester,</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Litman</author>
<author>J F Allen</author>
</authors>
<title>A Plan Recognition Model for Subdialogues in Conversation,</title>
<date>1984</date>
<journal>Cognitive Science. ,</journal>
<tech>Tech. Rep. 141.</tech>
<institution>University of Rochester</institution>
<note>to appear. , Also</note>
<contexts>
<context position="17356" citStr="[9,11]" startWordPosition="2640" endWordPosition="2640">gnition process is also incremental; if the heuristics cannot uniquely determine an underlying plan, chaining stops. As mentioned above, however, this is not a full solution. Since the plan recognizer is now recognizing discourse as well as domain plans from a single utterance, the set of recognition processes must be coordi4Plan chaining can also be done via effects and prerequisites. To keep the example in the next section simple. plans have been nated.5 An algorithm for coordinating the recognition of domain and discourse plans from a single utterance has been presented in Litman and Allen [9,11]. In brief, the plan recognizer recognizes a discourse plan from every utterance, then uses a process of constraint satisfaction to initiate recognition of the domain and any other discourse plans related to the utterance. Furthermore, to record and monitor execution of the discourse and domain plans active at any point in a dialogue, a dialogue context in the form of a plan stack is built and maintained by the plan recognizer. Various models of discourse have argued that an ideal interrupting topic structure follows a stack-like discipline (Reichman [17], Polanyi and Scha [15], Grosz and Sidn</context>
</contexts>
<marker>11.</marker>
<rawString>D. J. Litman and J. F. Allen. A Plan Recognition Model for Subdialogues in Conversation, Cognitive Science. , to appear. , Also University of Rochester Tech. Rep. 141. November 1984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Mann</author>
</authors>
<date>1970</date>
<institution>Corpus of Computer Operator Transcripts, Unpublished Manuscript,</institution>
<location>ISI,</location>
<contexts>
<context position="1298" citStr="[12]" startWordPosition="190" endWordPosition="190">tation in terms of planning operators and their computation via a plan recognition process. By incorporating complex inferential processes relating utterances into a plan-based framework, a formalization and computability not available in the earlier works is provided. INTRODUCTION In order to interpret a sequence of utterances fully, one must know how the utterances cohere; that is, one must be able to infer implicit relationships as well as non-relationships between the utterances. Consider the following fragment. taken from a terminal transcript between a user and a computer operator (Mann [12]): Could you mount a magtape for me? It&apos;s tape 1. Such a fragment appears coherent because it is easy to infer how the second utterance is related to the first. Contrast this with the following fragment: Could you mount a magtape for me? It&apos;s snowing like crazy. This sequence appears much less coherent since now there is no obvious connection between the two utterances. While one could postulate some connection (e.g., the speaker&apos;s magtape contains a database of places to go skiing), more likely one would say that there is no relationship between the utterances. Furth&apos;This work was done at the</context>
</contexts>
<marker>12.</marker>
<rawString>W. Mann, Corpus of Computer Operator Transcripts, Unpublished Manuscript, ISI, 1970&apos;s.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Mann</author>
</authors>
<title>Discourse Structures for Text Generation,</title>
<date>1984</date>
<pages>367</pages>
<location>Coling84, Stanford,</location>
<contexts>
<context position="2597" citStr="[13]" startWordPosition="393" endWordPosition="393">part by DARPA under Grant N00014-82-K-0193. NSF under Grant DCR8351665. and ONR under Grant N0014-80-C-0197. ermore, because the second utterance violates an expectation of discourse coherence (Reichman [16]. Hobbs [8], Grosz, Joshi, and Weinstein [6]), the utterance seems inappropriate since there are no linguistic clues (for example, prefacing the utterance with &amp;quot;incidentally&amp;quot;) marking it as a topic change. The identification and specification of sets of linguistic relationships between utterances2 forms the basis for many computational models of discourse (Reichman [17], McKeown [14], Mann [13], Hobbs [8], Cohen [3]). By limiting the relationships allowed in a system and the ways in which relationships coherently interact, efficient mechanisms for understanding and generating well organized discourse can be developed. Furthermore, the approach provides a framework for explaining the use of surface linguistic phenomena such as clue words, words like &amp;quot;incidentally&amp;quot; that often correspond to particular relationships between utterances. Unfortunately. while these theories propose relationships that seem intuitive (e.g. &amp;quot;elaboration,&amp;quot; as might be used in the first fragment above), there h</context>
<context position="3909" citStr="[13]" startWordPosition="588" endWordPosition="588">efined. Furthermore, since the formalization of the relationships has proven to be an extremely difficult task, such theories typically have to depend on unrealistic computational processes. For example. Cohen [3] uses an oracle to recognize her &amp;quot;evidence&amp;quot; relationships. Reichman&apos;s [17] use of a set of conversational moves depends on the future development of extremely sophisticated semantics modules. Hobbs [8] acknowledges that his theory of coherence relations &amp;quot;may seem to be appealing to magic,&amp;quot; since there are several places where he appeals to as yet incomplete subtheories. Finally, Mann [13] notes that his theory of rhetorical predicates is currently descriptive rather than constructive. McKeown&apos;s [14] implemented system of rhetorical predicates is a notable exception, but since her predicates have associated semantics expressed in terms of a specific data base system the approach is not particularly general. =Although in some theories relationships hold between group of utterances, in others between clauses of an utterance, these distinctions will not be crucial for the purposes of this paper. 215 This paper presents a new model for representing and recognizing implicit relation</context>
</contexts>
<marker>13.</marker>
<rawString>W. C. Mann, Discourse Structures for Text Generation, Coling84, Stanford, July 1984, 367-</rawString>
</citation>
<citation valid="true">
<authors>
<author>K R McKeown</author>
</authors>
<title>Generating Natural Language Text in Response to Questions about Database Structure, PhD Thesis,</title>
<date>1982</date>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia,</location>
<contexts>
<context position="2586" citStr="[14]" startWordPosition="391" endWordPosition="391">pported in part by DARPA under Grant N00014-82-K-0193. NSF under Grant DCR8351665. and ONR under Grant N0014-80-C-0197. ermore, because the second utterance violates an expectation of discourse coherence (Reichman [16]. Hobbs [8], Grosz, Joshi, and Weinstein [6]), the utterance seems inappropriate since there are no linguistic clues (for example, prefacing the utterance with &amp;quot;incidentally&amp;quot;) marking it as a topic change. The identification and specification of sets of linguistic relationships between utterances2 forms the basis for many computational models of discourse (Reichman [17], McKeown [14], Mann [13], Hobbs [8], Cohen [3]). By limiting the relationships allowed in a system and the ways in which relationships coherently interact, efficient mechanisms for understanding and generating well organized discourse can be developed. Furthermore, the approach provides a framework for explaining the use of surface linguistic phenomena such as clue words, words like &amp;quot;incidentally&amp;quot; that often correspond to particular relationships between utterances. Unfortunately. while these theories propose relationships that seem intuitive (e.g. &amp;quot;elaboration,&amp;quot; as might be used in the first fragment abov</context>
<context position="4022" citStr="[14]" startWordPosition="603" endWordPosition="603">uch theories typically have to depend on unrealistic computational processes. For example. Cohen [3] uses an oracle to recognize her &amp;quot;evidence&amp;quot; relationships. Reichman&apos;s [17] use of a set of conversational moves depends on the future development of extremely sophisticated semantics modules. Hobbs [8] acknowledges that his theory of coherence relations &amp;quot;may seem to be appealing to magic,&amp;quot; since there are several places where he appeals to as yet incomplete subtheories. Finally, Mann [13] notes that his theory of rhetorical predicates is currently descriptive rather than constructive. McKeown&apos;s [14] implemented system of rhetorical predicates is a notable exception, but since her predicates have associated semantics expressed in terms of a specific data base system the approach is not particularly general. =Although in some theories relationships hold between group of utterances, in others between clauses of an utterance, these distinctions will not be crucial for the purposes of this paper. 215 This paper presents a new model for representing and recognizing implicit relationships between utterances. Underlying linguistic relationships are formulated as discourse plans in a plan-based t</context>
</contexts>
<marker>14.</marker>
<rawString>K. R. McKeown, Generating Natural Language Text in Response to Questions about Database Structure, PhD Thesis, University of Pennsylvania, Philadelphia, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Polanyi</author>
<author>R J H Scha</author>
</authors>
<title>The Syntax of Discourse,</title>
<date>1983</date>
<journal>Text (Special Issue: Formal Methods of Discourse Analysis)</journal>
<volume>3</volume>
<pages>261--270</pages>
<contexts>
<context position="17940" citStr="[15]" startWordPosition="2734" endWordPosition="2734">man and Allen [9,11]. In brief, the plan recognizer recognizes a discourse plan from every utterance, then uses a process of constraint satisfaction to initiate recognition of the domain and any other discourse plans related to the utterance. Furthermore, to record and monitor execution of the discourse and domain plans active at any point in a dialogue, a dialogue context in the form of a plan stack is built and maintained by the plan recognizer. Various models of discourse have argued that an ideal interrupting topic structure follows a stack-like discipline (Reichman [17], Polanyi and Scha [15], Grosz and Sidner [7]). The plan recognition algorithm will be reviewed when tracing through the example of the next section. Since discourse plans reflect linguistic relationships between utterances, the earlier work on domain plan recognition can also be augmented in several other ways. For example, the search process can be constrained by adding heuristics that prefer discourse plans corresponding to the most linguistically coherent continuations of the dialogue. More specifically, in the absence of any linguistic clues (as will be described below), the plan recognizer will prefer relation</context>
<context position="19541" citStr="[15]" startWordPosition="2974" endWordPosition="2974">tics also follow the principle of Occam&apos;s razor, since they are ordered to introduce as few new plans as possible. If within one of these preferences there are still competing interpretations, the interpretation that most corresponds to a stack discipline is preferred. For example, a continuation resuming a recently interrupted topic is preferred to continuation of a topic interrupted earlier in the conversation. Finally, since the plan recognizer now recognizes implicit relationships between utterances, linguistic clues signaling such relationships (Grosz [5]. Reichman [17]. Polanyi and Scha [15], Sidner [24], Cohen [3], Grosz and Sidner [7]) should be exploitable by the plan recognition algorithm. In other words, the plan recognizer should be aware of correlations between expressed so that chaining via decompositions is sufficient. &apos;Although Wilensky [26] introduced meta-plans into a natural language system to handle a totally different issue, that of concurrent goal interaction, he does not address details of coordination. 218 specific words and the discourse plans they typically signal. Clues can then be used both to reinforce as well as to overrule the preference ordering given ab</context>
</contexts>
<marker>15.</marker>
<rawString>L. Polanyi and R. J. H. Scha, The Syntax of Discourse, Text (Special Issue: Formal Methods of Discourse Analysis) 3, 3 (1983), 261-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Reichman</author>
</authors>
<title>Conversational Coherency.</title>
<date>1978</date>
<journal>Cognitive Science</journal>
<volume>2</volume>
<pages>283--328</pages>
<contexts>
<context position="2200" citStr="[16]" startWordPosition="336" endWordPosition="336">erent since now there is no obvious connection between the two utterances. While one could postulate some connection (e.g., the speaker&apos;s magtape contains a database of places to go skiing), more likely one would say that there is no relationship between the utterances. Furth&apos;This work was done at the Department of Computer Science. University of Rochester. Rochester NY 14627. and supported in part by DARPA under Grant N00014-82-K-0193. NSF under Grant DCR8351665. and ONR under Grant N0014-80-C-0197. ermore, because the second utterance violates an expectation of discourse coherence (Reichman [16]. Hobbs [8], Grosz, Joshi, and Weinstein [6]), the utterance seems inappropriate since there are no linguistic clues (for example, prefacing the utterance with &amp;quot;incidentally&amp;quot;) marking it as a topic change. The identification and specification of sets of linguistic relationships between utterances2 forms the basis for many computational models of discourse (Reichman [17], McKeown [14], Mann [13], Hobbs [8], Cohen [3]). By limiting the relationships allowed in a system and the ways in which relationships coherently interact, efficient mechanisms for understanding and generating well organized di</context>
</contexts>
<marker>16.</marker>
<rawString>R. Reichman, Conversational Coherency. Cognitive Science 2, 4 (1978), 283-328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Reichman-Adar</author>
</authors>
<title>Extended Person-Machine Interfaces,</title>
<date>1984</date>
<journal>Artificial Intelligence</journal>
<volume>22</volume>
<pages>157--218</pages>
<contexts>
<context position="2572" citStr="[17]" startWordPosition="389" endWordPosition="389"> 14627. and supported in part by DARPA under Grant N00014-82-K-0193. NSF under Grant DCR8351665. and ONR under Grant N0014-80-C-0197. ermore, because the second utterance violates an expectation of discourse coherence (Reichman [16]. Hobbs [8], Grosz, Joshi, and Weinstein [6]), the utterance seems inappropriate since there are no linguistic clues (for example, prefacing the utterance with &amp;quot;incidentally&amp;quot;) marking it as a topic change. The identification and specification of sets of linguistic relationships between utterances2 forms the basis for many computational models of discourse (Reichman [17], McKeown [14], Mann [13], Hobbs [8], Cohen [3]). By limiting the relationships allowed in a system and the ways in which relationships coherently interact, efficient mechanisms for understanding and generating well organized discourse can be developed. Furthermore, the approach provides a framework for explaining the use of surface linguistic phenomena such as clue words, words like &amp;quot;incidentally&amp;quot; that often correspond to particular relationships between utterances. Unfortunately. while these theories propose relationships that seem intuitive (e.g. &amp;quot;elaboration,&amp;quot; as might be used in the first</context>
<context position="17917" citStr="[17]" startWordPosition="2730" endWordPosition="2730">s been presented in Litman and Allen [9,11]. In brief, the plan recognizer recognizes a discourse plan from every utterance, then uses a process of constraint satisfaction to initiate recognition of the domain and any other discourse plans related to the utterance. Furthermore, to record and monitor execution of the discourse and domain plans active at any point in a dialogue, a dialogue context in the form of a plan stack is built and maintained by the plan recognizer. Various models of discourse have argued that an ideal interrupting topic structure follows a stack-like discipline (Reichman [17], Polanyi and Scha [15], Grosz and Sidner [7]). The plan recognition algorithm will be reviewed when tracing through the example of the next section. Since discourse plans reflect linguistic relationships between utterances, the earlier work on domain plan recognition can also be augmented in several other ways. For example, the search process can be constrained by adding heuristics that prefer discourse plans corresponding to the most linguistically coherent continuations of the dialogue. More specifically, in the absence of any linguistic clues (as will be described below), the plan recogniz</context>
<context position="19518" citStr="[17]" startWordPosition="2970" endWordPosition="2970">ey do occur. The heuristics also follow the principle of Occam&apos;s razor, since they are ordered to introduce as few new plans as possible. If within one of these preferences there are still competing interpretations, the interpretation that most corresponds to a stack discipline is preferred. For example, a continuation resuming a recently interrupted topic is preferred to continuation of a topic interrupted earlier in the conversation. Finally, since the plan recognizer now recognizes implicit relationships between utterances, linguistic clues signaling such relationships (Grosz [5]. Reichman [17]. Polanyi and Scha [15], Sidner [24], Cohen [3], Grosz and Sidner [7]) should be exploitable by the plan recognition algorithm. In other words, the plan recognizer should be aware of correlations between expressed so that chaining via decompositions is sufficient. &apos;Although Wilensky [26] introduced meta-plans into a natural language system to handle a totally different issue, that of concurrent goal interaction, he does not address details of coordination. 218 specific words and the discourse plans they typically signal. Clues can then be used both to reinforce as well as to overrule the prefe</context>
</contexts>
<marker>17.</marker>
<rawString>R. Reichman-Adar, Extended Person-Machine Interfaces, Artificial Intelligence 22, 2 (1984), 157-218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E D Sacerdoti</author>
</authors>
<title>A Structure for Plans and Behavior.</title>
<date>1977</date>
<publisher>Elsevier.</publisher>
<location>New York.</location>
<contexts>
<context position="7367" citStr="[18]" startWordPosition="1100" endWordPosition="1100">endently of any domain. Although discourse plans can refer to both domain plans or other discourse plans. domain plans can only be accessed and manipulated via discourse plans. For example, in the tape excerpt above &amp;quot;Could you mount a magtape for me?&amp;quot; achieves a discourse plan to introduce a domain plan to mount a tape. &amp;quot;It&apos;s tape 1&amp;quot; then further specifies this domain plan. Except for the fact that they refer to other plans (i.e. they take other plans as arguments), the representation of discourse plans is identical to the usual representation of domain plans (Fikes and Nilsson [4], Sacerdoti [18]). Every plan has a header, a parameterized action description that names the plan. Action descriptions are represented as operators on a planner&apos;s world model and defined in terms of prerequisites, decompositions, and effects. Prerequisites are conditions that need to hold (or to be made to hold) in the world model before the action operator can be applied. Effects are statements that are asserted into the world model after the action has been successfully executed. Decompositions enable hierarchical planning. Although the action description of the header may be usefully thought of at one lev</context>
</contexts>
<marker>18.</marker>
<rawString>E. D. Sacerdoti. A Structure for Plans and Behavior. Elsevier. New York. 1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Searle</author>
</authors>
<title>in Speech Acts, an Essay in the Philosophy of Language,</title>
<date>1969</date>
<publisher>Cambridge University Press,</publisher>
<location>New York,</location>
<contexts>
<context position="9173" citStr="[19]" startWordPosition="1375" endWordPosition="1375">s underlying an utterance. HEADER: INTRODUCE-PLAN(speaker, hearer DECOMPOSITION: action. plan) EFFECTS: REQUEST(speaker. hearer. action) CONSTRAINTS: WANT(hearer. plan) NEXT(action. plan) STEP(action, plan) AGENT(action. hearer) Figure 1. INTRODUCE-PLAN. Figures 1, 2, and 3 present examples of discourse plans (see Litman [10] for the complete set). The first discourse plan, INTRODUCE-PLAN, takes a plan of the speaker that involves the hearer and presents it to the hearer (who is assumed cooperative). The decomposition specifies a typical way to do this, via execution of the speech act (Searle [19]) REQUEST. The constraints use a vocabulary for referring to and describing plans and actions to specify that the only actions requested will be those that are in the plan and have the hearer as agent. Since the hearer is assumed cooperative, he or she will then adopt as a goal the 3These constraints should not be confused with the constraints of Stefik [25]. which are dynamical!&apos;, formulated during hierarchical plan generation and represent the interactions between subproblems. 216 joint plan containing the action (i.e. the first effect). The second effect states that the action requested wil</context>
</contexts>
<marker>19.</marker>
<rawString>J. R. Searle, in Speech Acts, an Essay in the Philosophy of Language, Cambridge University Press, New York, 1969.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Searle</author>
</authors>
<title>Indirect Speech Acts, in Speech Acts.</title>
<date>1975</date>
<volume>3</volume>
<editor>P. Cole and Morgan (ed.),</editor>
<publisher>Academic Press.</publisher>
<location>New York, NY,</location>
<marker>20.</marker>
<rawString>J. R. Searle, Indirect Speech Acts, in Speech Acts. vol. 3, P. Cole and Morgan (ed.), Academic Press. New York, NY, 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Sidner</author>
<author>D J Israel</author>
</authors>
<title>Recognizing Intended Meaning and Speakers&apos; Plans,</title>
<date>1981</date>
<pages>203--208</pages>
<location>IJCAI, Vancouver,</location>
<contexts>
<context position="6166" citStr="[21]" startWordPosition="912" endWordPosition="912">oherence preferences, and constraint satisfaction. Finally, a detailed example of the processing of a dialogue fragment is presented, illustrating the recognition of various types of relationships between utterances. REPRESENTING COHERENCE USING DISCOURSE PLANS In a plan-based approach to language understanding, an utterance is considered understood when it has been related to some underlying plan of the speaker. While previous works have explicitly represented and recognized the underlying task plans of a given domain (e.g., mount a tape) (Grosz [5], Allen and Perrault [1], Sidner and Israel [21]. Carberry [2], Sidner [24]), the ways that utterances could be related to such plans were limited and not of particular concern. As a result, only dialogues exhibiting a very limited set of utterance relationships could be understood. In this work, a set of domain-independent plans about plans (i.e. meta-plans) called discourse plans are introduced to explicitly represent, reason about, and generalize such relationships. Discourse plans are recognized from every utterance and represent plan introduction, plan execution, plan specification, plan debugging, plan abandonment, and so on, independ</context>
<context position="15870" citStr="[21]" startWordPosition="2391" endWordPosition="2391">?&amp;quot; Since eating lunch cannot be related to the previous editing plan topic, the system recognizes the utterance as a total change of topic, i.e. INTRODUCE-PLAN(User, System, System tell User if time for lunch, eat lunch plan). RECOGNIZING DISCOURSE PLANS This section presents a computational algorithm for the recognition of discourse plans. Recall that the previous lack of such an algorithm was in fact a major force behind the last section&apos;s plan-based formalization of the linguistic relationships. Previous work in the area of domain plan recognition (Allen and Perrault [1], Sidner and Israel [21]. Carberry [2], Sidner [24]) provides a partial solution to the recognition problem. For example, since discourse plans are represented identically to domain plans, the same process of plan recognition can apply to both. In particular, every plan is recognized by an incremental process of heuristic search. From an input, the plan recognizer tries to find a plan for which the input is a step,4 and then tries to find more abstract plans for which the postulated plan is a step, and so on. After every step of this chaining process. a set of heuristics prune the candidate plan set based on assumpti</context>
<context position="22145" citStr="[21]" startWordPosition="3390" endWordPosition="3390">mployee.&amp;quot; System:OK. &lt;system displays network&gt; User: No, move the concept up. System:OK. &lt;system redisplays network&gt; User: Now, make an individual employee concept whose first name is &amp;quot;Sam&amp;quot; and whose last name is &amp;quot;Jones.&amp;quot; Although the behavior to be described is fully specified by the theory, the implementation corresponds only to the new model of plan recognition. All simulated computational processes have been implemented elsewhere, however. Litman [10] contains a full discussion of the implementation. Figure 4 presents the relevant domain plans for this domain, taken from Sidner and Israel [21] with minor modifications. ADD-DATA is a plan to add new data into a network, while EXAMINE is a plan to examine parts of a network. Both plans involve the subplan CONSIDER-ASPECT, in which the user considers some aspect of a network, for example by looking at it (the decomposition shown), listening to a description, or thinking about it. The processing begins with a speech act analysis of &amp;quot;Show me the generic concept called &apos;employee— HEADER: ADD-DATA(user, netpiece, data, screenLocation) DECOMPOSITION: CONSIDER-ASPECT(user. netpiece) PUT(system, data, screenLocation) HEADER: EXAMINE(user. ne</context>
</contexts>
<marker>21.</marker>
<rawString>C. L. Sidner and D. J. Israel. Recognizing Intended Meaning and Speakers&apos; Plans, IJCAI, Vancouver, 1981, 203-208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Sidner</author>
</authors>
<title>Protocols of Users Manipulating Visually Presented Information with Natural Language, Report 5128. Bolt Beranek and</title>
<date>1982</date>
<contexts>
<context position="13624" citStr="[22]" startWordPosition="2023" endWordPosition="2023"> plan situation described above and should be self-explanatory with the exception of two new terms. MODIFIES(action2, actionl) means that action2 is a variant of actionl, for example, the same action with different parameters or a new action achieving the still required effects. ENABLES(actionl, action2) means that false prerequisites of action2 are in the effects of action]. CORRECT-PLAN is an example of a topic interruption that relates to a previous topic, To illustrate how these discourse plans represent the relationships between utterances, consider a naturally-occurring protocol (Sidner [22]) in which a user interacts with a person simulating an editing system to manipulate network structures in a knowledge representation language: 1) User: Hi. Please show the concept Person. 2) System: Drawing...OK. 3) User: Add a role called hobby. 4) System: OK. 5) User: Make the yr be Pastime. Assume a typical task plan in this domain is to edit a structure by accessing the structure then performing a sequence of editing actions. The user&apos;s first request thus introduces a plan to edit the concept person. Each successive user utterance continues through the plan by requesting the system to per</context>
</contexts>
<marker>22.</marker>
<rawString>C. L. Sidner, Protocols of Users Manipulating Visually Presented Information with Natural Language, Report 5128. Bolt Beranek and Newman , September 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Sidner</author>
<author>M Bates</author>
</authors>
<title>Requirements of Natural Language Understanding in a System with Graphic Displays. Report Number 5242, Bolt Beranek and Newman Inc..</title>
<date>1983</date>
<contexts>
<context position="21443" citStr="[23]" startWordPosition="3281" endWordPosition="3281"> Now, imagine the speaker prefacing the second utterance with a clue such as &amp;quot;incidentally.&amp;quot; a word typically used to signal topic interruption. Since the plan recognizer knows that &amp;quot;incidentally&amp;quot; is a signal for an interruption, the search will not even attempt to satisfy the first preference heuristic since a signal for the second or third is explicitly present. EXAMPLE This section uses the discourse plan representations and plan recognition algorithm of the previous sections to illustrate the processing of the following dialogue, a slightly modified portion of a scenario (Sidner and Bates [23]) developed from the set of protocols described above: User: Show me the generic concept called &amp;quot;employee.&amp;quot; System:OK. &lt;system displays network&gt; User: No, move the concept up. System:OK. &lt;system redisplays network&gt; User: Now, make an individual employee concept whose first name is &amp;quot;Sam&amp;quot; and whose last name is &amp;quot;Jones.&amp;quot; Although the behavior to be described is fully specified by the theory, the implementation corresponds only to the new model of plan recognition. All simulated computational processes have been implemented elsewhere, however. Litman [10] contains a full discussion of the implemen</context>
</contexts>
<marker>23.</marker>
<rawString>C. L. Sidner and M. Bates. Requirements of Natural Language Understanding in a System with Graphic Displays. Report Number 5242, Bolt Beranek and Newman Inc.. March 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Sidner</author>
</authors>
<title>Plan Parsing for Intended Response Recognition in Discourse,</title>
<date>1985</date>
<journal>Computational Intelligence</journal>
<volume>1</volume>
<pages>1--10</pages>
<contexts>
<context position="6193" citStr="[24]" startWordPosition="916" endWordPosition="916">onstraint satisfaction. Finally, a detailed example of the processing of a dialogue fragment is presented, illustrating the recognition of various types of relationships between utterances. REPRESENTING COHERENCE USING DISCOURSE PLANS In a plan-based approach to language understanding, an utterance is considered understood when it has been related to some underlying plan of the speaker. While previous works have explicitly represented and recognized the underlying task plans of a given domain (e.g., mount a tape) (Grosz [5], Allen and Perrault [1], Sidner and Israel [21]. Carberry [2], Sidner [24]), the ways that utterances could be related to such plans were limited and not of particular concern. As a result, only dialogues exhibiting a very limited set of utterance relationships could be understood. In this work, a set of domain-independent plans about plans (i.e. meta-plans) called discourse plans are introduced to explicitly represent, reason about, and generalize such relationships. Discourse plans are recognized from every utterance and represent plan introduction, plan execution, plan specification, plan debugging, plan abandonment, and so on, independently of any domain. Althou</context>
<context position="15897" citStr="[24]" startWordPosition="2395" endWordPosition="2395">t be related to the previous editing plan topic, the system recognizes the utterance as a total change of topic, i.e. INTRODUCE-PLAN(User, System, System tell User if time for lunch, eat lunch plan). RECOGNIZING DISCOURSE PLANS This section presents a computational algorithm for the recognition of discourse plans. Recall that the previous lack of such an algorithm was in fact a major force behind the last section&apos;s plan-based formalization of the linguistic relationships. Previous work in the area of domain plan recognition (Allen and Perrault [1], Sidner and Israel [21]. Carberry [2], Sidner [24]) provides a partial solution to the recognition problem. For example, since discourse plans are represented identically to domain plans, the same process of plan recognition can apply to both. In particular, every plan is recognized by an incremental process of heuristic search. From an input, the plan recognizer tries to find a plan for which the input is a step,4 and then tries to find more abstract plans for which the postulated plan is a step, and so on. After every step of this chaining process. a set of heuristics prune the candidate plan set based on assumptions regarding rational plan</context>
<context position="19554" citStr="[24]" startWordPosition="2976" endWordPosition="2976">low the principle of Occam&apos;s razor, since they are ordered to introduce as few new plans as possible. If within one of these preferences there are still competing interpretations, the interpretation that most corresponds to a stack discipline is preferred. For example, a continuation resuming a recently interrupted topic is preferred to continuation of a topic interrupted earlier in the conversation. Finally, since the plan recognizer now recognizes implicit relationships between utterances, linguistic clues signaling such relationships (Grosz [5]. Reichman [17]. Polanyi and Scha [15], Sidner [24], Cohen [3], Grosz and Sidner [7]) should be exploitable by the plan recognition algorithm. In other words, the plan recognizer should be aware of correlations between expressed so that chaining via decompositions is sufficient. &apos;Although Wilensky [26] introduced meta-plans into a natural language system to handle a totally different issue, that of concurrent goal interaction, he does not address details of coordination. 218 specific words and the discourse plans they typically signal. Clues can then be used both to reinforce as well as to overrule the preference ordering given above. In fact,</context>
</contexts>
<marker>24.</marker>
<rawString>C. L. Sidner. Plan Parsing for Intended Response Recognition in Discourse, Computational Intelligence 1, 1 (February 1985). 1-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stefik</author>
</authors>
<title>Planning with Constraints (MOLGEN: Part 1),</title>
<date>1981</date>
<journal>Artificial Intelligence</journal>
<volume>16</volume>
<pages>111--140</pages>
<contexts>
<context position="9533" citStr="[25]" startWordPosition="1439" endWordPosition="1439"> discourse plan, INTRODUCE-PLAN, takes a plan of the speaker that involves the hearer and presents it to the hearer (who is assumed cooperative). The decomposition specifies a typical way to do this, via execution of the speech act (Searle [19]) REQUEST. The constraints use a vocabulary for referring to and describing plans and actions to specify that the only actions requested will be those that are in the plan and have the hearer as agent. Since the hearer is assumed cooperative, he or she will then adopt as a goal the 3These constraints should not be confused with the constraints of Stefik [25]. which are dynamical!&apos;, formulated during hierarchical plan generation and represent the interactions between subproblems. 216 joint plan containing the action (i.e. the first effect). The second effect states that the action requested will be the next action performed in the introduced plan. Note that since INTRODUCE-PLAN has no prerequisites it can occur in any discourse context, i.e. it does not need to be related to previous plans. INTRODUCE-PLAN thus allows the recognition of topic changes when a previous topic is completed as well as recognition of interrupting topic changes (and when n</context>
</contexts>
<marker>25.</marker>
<rawString>M. Stefik, Planning with Constraints (MOLGEN: Part 1), Artificial Intelligence 16, (1981), 111-140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Wilensky</author>
</authors>
<title>Planning and Understanding,</title>
<date>1983</date>
<publisher>Addison-Wesley Publishing company,</publisher>
<location>Reading, Massachusetts,</location>
<contexts>
<context position="19806" citStr="[26]" startWordPosition="3013" endWordPosition="3013">red. For example, a continuation resuming a recently interrupted topic is preferred to continuation of a topic interrupted earlier in the conversation. Finally, since the plan recognizer now recognizes implicit relationships between utterances, linguistic clues signaling such relationships (Grosz [5]. Reichman [17]. Polanyi and Scha [15], Sidner [24], Cohen [3], Grosz and Sidner [7]) should be exploitable by the plan recognition algorithm. In other words, the plan recognizer should be aware of correlations between expressed so that chaining via decompositions is sufficient. &apos;Although Wilensky [26] introduced meta-plans into a natural language system to handle a totally different issue, that of concurrent goal interaction, he does not address details of coordination. 218 specific words and the discourse plans they typically signal. Clues can then be used both to reinforce as well as to overrule the preference ordering given above. In fact, in the latter case clues ease the recognition of topic relationships that would otherwise be difficult (if not impossible (Cohen [3], Grosz and Sidner [7], Sidner [24])) to understand. For example, consider recognizing the topic change in the tape var</context>
</contexts>
<marker>26.</marker>
<rawString>R. Wilensky, Planning and Understanding, Addison-Wesley Publishing company, Reading, Massachusetts, 1983.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>