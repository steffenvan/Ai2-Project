<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007952">
<title confidence="0.993061">
How bad is the problem of PP-attachment? A comparison of English,
German and Swedish
</title>
<author confidence="0.998117">
Martin Volk
</author>
<affiliation confidence="0.993676">
Stockholm University
Department of Linguistics
</affiliation>
<address confidence="0.982216">
106 91 Stockholm, Sweden
</address>
<email confidence="0.999237">
volk@ling.su.se
</email>
<sectionHeader confidence="0.993901" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999941263157895">
The correct attachment of prepositional
phrases (PPs) is a central disambigua-
tion problem in parsing natural languages.
This paper compares the baseline situation
in English, German and Swedish based
on manual PP attachments in various tree-
banks for these languages. We argue that
cross-language comparisons of the disam-
biguation results in previous research is
impossible because of the different selec-
tion procedures when building the training
and test sets. We perform uniform tree-
bank queries and show that English has the
highest noun attachment rate followed by
Swedish and German. We also show that
the high rate in English is dominated by
the preposition of. From our study we de-
rive a list of criteria for profiling data sets
for PP attachment experiments.
</bodyText>
<sectionHeader confidence="0.99914" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999550636363636">
Any computer system for natural language
processing has to struggle with the problem of am-
biguities. If the system is meant to extract precise
information from a text, these ambiguities must
be resolved. One of the most frequent ambigu-
ities arises from the attachment of prepositional
phrases (PPs). Simply stated, a PP that follows
a noun (in English, German or Swedish) can be
attached to the noun or to the verb.
In the last decade various methods for the res-
olution of PP attachment ambiguities have been
proposed. The seminal paper by (Hindle and
Rooth, 1993) started a sequence of studies for
English. We investigated similar methods for Ger-
man (Volk, 2001; Volk, 2002). Recently other
languages (such as Dutch (Vandeghinste, 2002) or
Swedish (Aasa, 2004)) have followed.
In the PP attachment research for other lan-
guages there is often a comparison of the dis-
ambiguation accuracy with the English results.
But are the results really comparable across lan-
guages? Are we starting from the same base-
line when working on PP attachment in struc-
turally similar languages like English, German and
Swedish? Is the problem of PP attachment equally
bad (equally frequent and of equal balance) for
these three languages? These are the questions we
will discuss in this paper.
In order to find answers to these questions we
have taken a closer look at the training and test
data used in various experiments. And we have
queried the most important treebanks for the three
languages under investigation.
</bodyText>
<sectionHeader confidence="0.979884" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999904111111111">
(Hindle and Rooth, 1993) did not have access to a
large treebank. Therefore they proposed an unsu-
pervised method for resolving PP attachment am-
biguities. And they evaluated their method against
880 English triples verb-noun-preposition (V-N-P)
which they had extracted from randomly selected,
ambiguously located PPs in a corpus. For exam-
ple, the sentence ”Timex had requested duty-free
treatment for many types of watches” results in
the V-N-P triple (request, treatment, for). These
triples were manually annotated by both authors
with either noun or verb attachment based on the
complete sentence context. Interestingly, 586 of
these triples (67%) were judged as noun attach-
ments and only 33% as verb attachments. And
(Hindle and Rooth, 1993) reported on 80% at-
tachment accuracy, an improvement of 13% over
the baseline (i.e. guessing noun attachment in all
</bodyText>
<note confidence="0.994006">
Proceedings of the Third ACL-SIGSEM Workshop on Prepositions, pages 81–88,
Trento, Italy, April 2006. c�2006 Association for Computational Linguistics
</note>
<page confidence="0.99875">
81
</page>
<bodyText confidence="0.999672288888889">
cases).
A year later (Ratnaparkhi et al., 1994) published
a supervised approach to the PP attachment prob-
lem. They had extracted quadruples V-N-P-N1
(plus the accompanying attachment decision) from
both an IBM computer manuals treebank (about
9000 tuples) and from the Wall Street Journal
(WSJ) section of the Penn treebank (about 24,000
tuples). The latter tuple set has been reused by
subsequent research, so let us focus on this one.2
(Ratnaparkhi et al., 1994) used 20,801 tuples for
training and 3097 tuples for evaluation. They re-
ported on 81.6% correct attachments.
But have they solved the same problem as (Hin-
dle and Rooth, 1993)? What was the initial bias
towards noun attachment in their data? It turns out
that their training set (the 20,801 tuples) contains
only 52% noun attachments, while their test set
(the 3097 tuples) contains 59% noun attachments.
The difference in noun attachments between these
two sets is striking, but (Ratnaparkhi et al., 1994)
do not discuss this (and we also do not have an
explanation for this). But it makes obvious that
(Ratnaparkhi et al., 1994) were tackling a prob-
lem different from (Hindle and Rooth, 1993) given
the fact that their baseline was at 59% guessing
noun attachment (rather than 67% in the Hindle
and Rooth experiments).3
Of course, the baseline is not a direct indica-
tor of the difficulty of the disambiguation task.
We may construct (artificial) cases with low base-
lines and a simple distribution of PP attachment
tendencies. For example, we may construct the
case that a language has 100 different prepositions,
where 50 prepositions always introduce noun at-
tachments, and the other 50 prepositions always
require verb attachments. If we also assume that
both groups occur with the same frequency, we
have a 50% baseline but still a trivial disambigua-
tion task.
But in reality the baseline puts the disambigua-
tion result into perspective. If, for instance, the
baseline is 60% and the disambiguation result is
80% correct attachments, then we will claim that
our disambiguation procedure is useful. Whereas
</bodyText>
<footnote confidence="0.99667675">
1The V-N-P-N quadruples also contain the head noun of
the NP within the PP.
2The Ratnaparkhi training and test sets were later distrib-
uted together with a development set of 4039 V-N-P-N tuples.
3It should be noted that important subsequent research,
e.g. by (Collins and Brooks, 1995; Stetina and Nagao, 1997),
used the Ratnaparkhi data sets and thus allowed for good
comparability.
</footnote>
<bodyText confidence="0.999937195121951">
if we have a baseline of 80% and the disambigua-
tion result is 75%, then the procedure can be dis-
carded.
So what are the baselines reported for other lan-
guages? And is it possible to use the same extrac-
tion mechanisms for V-N-P-N tuples in order to
come to comparable baselines?
We did an in-depth study on German PP at-
tachment (Volk, 2001). We compiled our own
treebank by annotating 3000 sentences from the
weekly computer journal ComputerZeitung. We
had first annotated a larger number of subsequent
sentences with Part-of-Speech tags, and based on
these PoS tags, we selected 3000 sentences that
contained at least one full verb plus the sequence
of a noun followed by a preposition. After annotat-
ing the 3000 sentences with complete syntax trees
we used a Prolog program to extract V-N-P-N tu-
ples with the accompanying attachment decisions.
This lead to 4562 tuples out of which 61% were
marked as noun attachments. We used the same
procedure to extract tuples from the first 10,000
sentences of the NEGRA treebank. This resulted
in 6064 tuples with 56% noun attachment (for a
detailed overview see (Volk, 2001) p. 86). Again
we observe a substantial difference in the baseline.
When our student J¨orgen Aasa worked on repli-
cating our German experiments for Swedish, he
used a Swedish treebank from the 1980s for the
extraction of test data. He extracted V-N-P-N tu-
ples from SynTag, a treebank with 5100 newspa-
per sentences built by (J¨arborg, 1986). And Aasa
was able to extract 2893 tuples out of which 73.8%
were marked as noun attachments (Aasa, 2004)
(p. 25). This was a surprisingly high figure, and
we wondered whether this indicated a tendency in
Swedish to avoid the PP in the ambiguous posi-
tion unless it was to be attached to the noun. But
again the extraction process was done with a spe-
cial purpose extraction program whose correctness
was hard to verify.
</bodyText>
<sectionHeader confidence="0.814122" genericHeader="method">
3 Querying Treebanks with
TIGER-Search
</sectionHeader>
<bodyText confidence="0.997716">
We therefore decided to check the attachment ten-
dencies of PPs in various treebanks for the three
languages in question with the same tool and with
queries that are as uniform as possible.
For English we used the WSJ section of the
Penn Treebank, for German we used our own
ComputerZeitung treebank (3000 sentences), the
</bodyText>
<page confidence="0.992325">
82
</page>
<bodyText confidence="0.983223460526316">
NEGRA treebank (10,000 sentences) and the re-
cently released version of the TIGER treebank
(50,000 sentences). For Swedish we used the
SynTag treebank mentioned above and one sec-
tion of the Talbanken treebank (6100 sentences).
All these treebanks consist of constituent structure
trees, and they are in representation formats which
allow them to be loaded into TIGER-Search. This
enables us to query them all in similar manners
and to get a fairer comparison of the attachment
tendencies.
TIGER-Search is a powerful treebank query
tool developed at the University of Stuttgart
(K¨onig and Lezius, 2002). Its query language
allows for feature-value descriptions of syntax
graphs. It is similar in expressiveness to tgrep (Ro-
hde, 2005) but it comes with graphical output and
highlighting of the syntax trees plus some nice sta-
tistics functions.
Our experiments for determining attachment
tendencies proceed along the following lines. For
each treebank we first query for all sequences of a
noun immediately followed by a PP (henceforth
noun+PP sequences). The dot being the prece-
dence operator, we use the query:
[pos=&amp;quot;NN&amp;quot;] . [cat=&amp;quot;PP&amp;quot;]
This query will match twice in the tree in fig-
ure 1. It gives us the frequency of all ambiguously
located PP. We disregard the fact that in certain
clause positions a PP in such a sequence cannot
be verb-attached and is thus not ambiguous. For
example, an English noun+PP sequence in subject
position is not ambiguous with respect to PP at-
tachment since the PP cannot attach to the verb.
Similar restrictions apply to German and Swedish.
In order to determine how many of these se-
quences are annotated as noun attachments, we
query for noun phrases that contain both a noun
and an immediately following PP. This query will
look like:
#np_mum:[cat=&amp;quot;NP&amp;quot;] &gt;
#np_child:[cat=&amp;quot;NP&amp;quot;] &amp;
#np_mum &gt; #pp:[cat=&amp;quot;PP&amp;quot;] &amp;
#np_child &gt;* #noun:[pos=&amp;quot;NN&amp;quot;] &amp;
#noun . #pp
All strings starting with # are variables and the
&gt; symbol is the dominance operator. So, this
query says: Search for an NP (and call it np mum)
that immediately dominates another NP (np child)
AND that immediately dominates a PP, AND the
np child dominates a noun which is immediately
followed by the PP.
This query presupposes that a PP which is at-
tached to a noun is actually annotated with the
structure (NP (NP (... N)) (PP)) which is true for
the Penn treebank (compare to the tree in figure 1).
But the German treebanks represent this type of at-
tachment rather as (NP (... N) (PP)) which means
that the query needs to be adapted accordingly.4
Such queries give us the frequency of all
noun+PP sequences and the frequency of all such
sequences with noun attachments. These frequen-
cies allow us to calculate the noun attachment rate
(NAR) in our treebanks.
We assume that all PPs in noun+PP sequences
which are not attached to a noun are attached to a
verb. This means we ignore the very few cases of
such PPs that might be attached to adjectives (as
for instance the second PP in ”due for revision in
1990”).
Different annotation schemes require modifica-
tions to these basic queries, and different noun
classes (regular nouns, proper names, deverbal
nouns etc.) allow for a more detailed investiga-
tion. We now present the results for each language
in turn.
</bodyText>
<sectionHeader confidence="0.51735" genericHeader="method">
3.1 Results for English
</sectionHeader>
<bodyText confidence="0.999646466666667">
We used sections 0 to 12 of the WSJ part of the
Penn Treebank (Marcus et al., 1993) with a total
of 24,618 sentences for our experiments. Our start
query reveals that an ambiguously located PP (i.e.
a noun+PP sequence) occurs in 13,191 (54%) of
these sentences, and it occurs a total of 20,858
times (a rate of 0.84 occurrences per sentences
with respect to all sentences in the treebank).
Searching for noun attachments with the second
query described in section 3 we learn that 15,273
noun+PP sequences are annotated as noun attach-
ments. And we catch another 547 noun attach-
ments if we query for noun phrases that contain
two PPs in sequence.5 In these cases the sec-
ond PP is also attached to a noun, although not
</bodyText>
<footnote confidence="0.999388">
4There are a few occurrences of this latter structure in the
Penn Treebank which should probably count as annotation
errors.
5See (Merlo et al., 1997) for a discussion of these cases
and an approach in automatically disambiguating them.
</footnote>
<equation confidence="0.984404333333333">
NAR =
freq(noun + PP)
freq(noun + PP, noun attachm)
</equation>
<page confidence="0.994074">
83
</page>
<figureCaption confidence="0.999654">
Figure 1: Noun phrase tree from the Penn Treebank
</figureCaption>
<bodyText confidence="0.999973689655172">
to the noun immediately preceding it (as for ex-
ample in the tree in figure 1). With some simi-
lar queries we located another 110 cases of noun
attachments (most of which are probably anno-
tation errors if the annotation guidelines are ap-
plied strictly). This means that we found a total
of 15,930 cases of noun attachment which corre-
sponds to a noun attachment rate of 76.4% (by
comparison to the 20,858 occurrences).
This is a surprisingly high number. Neither
(Hindle and Rooth, 1993) with 67% nor (Ratna-
parkhi et al., 1994) with 59% noun attachment
were anywhere close to this figure. What have we
done differently?
One aspect is that we only queried for singu-
lar nouns (NN) in the Penn Treebank where plural
nouns (NNS) and proper names (NNP and NNPS)
have separate PoS tags. Using analogous queries
for plural nouns we found that they exhibit a NAR
of 71.7%. Whereas the queries for proper names
(singular and plural names taken together) account
for a NAR of 54.5%.
Another reason for the discrepancy in the NAR
between Ratnaparkhi’s data and our calculations
certainly comes from the fact that we queried
for all sequences noun+PP as possibly ambiguous
whereas they looked only at such sequences within
verb phrases. But since we will do the same in
both German and Swedish, this is still worthwhile.
</bodyText>
<sectionHeader confidence="0.581597" genericHeader="method">
3.2 Results for German
</sectionHeader>
<bodyText confidence="0.988344">
The three German treebanks which we investigate
are all annotated in more or less the same man-
ner, i.e. according to the NEGRA guidelines which
were slightly refined for the TIGER project. This
enabled us to use the same set of queries for all
</bodyText>
<table confidence="0.994978666666667">
CZ NEGRA TIGER
size 3000 10,000 50,000
noun+PP seq 4355 6,938 39,634
occur rate 1.4 0.7 0.8
noun attachm 2743 4102 23,969
NAR 63.0% 59.1% 60.5%
</table>
<tableCaption confidence="0.99917">
Table 1: Results for the German treebanks
</tableCaption>
<bodyText confidence="0.999620923076923">
three of them. Since the German guidelines distin-
guish between node labels for coordinated phrases
(e.g. CNP and CPP) and non-coordinated phrases
(e.g. NP and PP), these distinctions needed to be
taken into account. Table 1 summarizes the re-
sults.
Our own ComputerZeitung treebank (CZ) has a
much higher occurrence rate of ambiguously lo-
cated PPs because the sentences were preselected
for this phenomenon. The general NEGRA and
TIGER treebanks have an occurrence rate that is
similar to English (0.8). The NAR varies between
59.1% for the NEGRA treebank and 63.0% for the
CZ treebank for regular nouns.
The German annotation also distinguishes be-
tween regular nouns and proper names. The
proper names show a much lower noun attach-
ment rate than the regular nouns. The NAR in the
CZ treebank is 22%, in the NEGRA treebank it is
20%, and in the TIGER treebank it is only 17%.
Here we suspect that the difference between the
CZ and the other treebanks is based on the differ-
ent text types. The computer journal CZ contains
more person names with affiliation (e.g. Stan Sug-
arman von der Firma Telemedia) and more com-
pany names with location (e.g. Aviso aus Finn-
</bodyText>
<page confidence="0.991777">
84
</page>
<bodyText confidence="0.999595578947369">
land) than a regular newspaper (that was used in
the NEGRA and TIGER corpora).
As mentioned above, our previous experiments
in (Volk, 2001) were based on sets of extracted
tuples from both the CZ and NEGRA treebanks.
Our extracted data set from the CZ treebank had
a noun attachment rate of 61%, and the one from
the NEGRA treebank had a noun attachment rate
of 56%.
So why are our new results based on TIGER-
Search queries two to three percents higher? The
main reason is that our old data sets included
proper names (with their low noun attachment
rate). But our extraction procedure comprised also
a number of other idiosyncracies. In an attempt
to harvest as many interesting V-N-P-N tuples as
possible from our treebanks we exploited coordi-
nated phrases and pronominal PPs. Some exam-
ples:
</bodyText>
<listItem confidence="0.7860255">
1. If the PP was preceded by a coordinated
noun phrase, we created as many tuples
as there were head nouns in the coordina-
tion. For example, the phrase ”den Aus-
tausch und die gemeinsame Nutzung von
Daten ... erm¨oglichen” leads to the tuples
(erm¨oglichen, Austausch, von, Daten) and
(erm¨oglichen, Nutzung, von, Daten) both
with the decision ’noun attachment’.
2. If the PP was introduced by coordinated
prepositions (e.g. Die Argumente f¨ur oder
gegen den Netzwerkcomputer), we created as
many tuples as there were prepositions.
3. If the verb group consists of coordinated
verbs (e.g. Infos f¨ur Online-Dienste aufbe-
reiten und gestalten), we created as many tu-
ples as there were verbs.
4. We regarded pronominal adverbs (darin,
</listItem>
<bodyText confidence="0.9027105">
dazu, hier¨uber, etc.) and reciprocal pronouns
(miteinander, untereinander, voneinander,
etc.) as equivalent to PPs and created tu-
ples when such pronominals appeared imme-
diately after a noun. See (Volk, 2003) for a
more detailed discussion of these pronouns.
</bodyText>
<sectionHeader confidence="0.677507" genericHeader="method">
3.3 Results for Swedish
</sectionHeader>
<bodyText confidence="0.999937814814815">
Currently there is no large-scale Swedish treebank
available. But there are some smaller treebanks
from the 80s which have recently been converted
to TIGER-XML so that they can also be queried
with TIGER-Search.
SynTag (J¨arborg, 1986) is a treebank consist-
ing of around 5100 sentences. Its conversion to
TIGER-XML is documented in (Hagstr¨om, 2004).
The treebank focuses on predicate-argument struc-
tures and some grammatical functions such as sub-
ject, head and adverbials. It is thus different from
the constituent structures that we find in the Penn
treebank or the German treebanks. We had to
adapt our queries accordingly. Since prepositional
phrases are not marked as such, we need to query
for constituents (marked as subject, as adverbial
or simply as argument) that start with a preposi-
tion. This results in a noun attachment rate of 73%
(which is very close to the rate reported by (Aasa,
2004)). Again this does not include proper names
which have a NAR of 44% in SynTag.
Let us compare these results to the second
Swedish treebank, Talbanken (first described by
(Telemann, 1974)). Talbanken was a remark-
able achievement in the 80s as it comes with two
written language parts (with a total of more than
10,000 sentences from student essays and from
newspapers) and two spoken language parts (with
another 10,000 trees from interviews and conver-
sations). We concentrated on the 6100 trees from
the written part taken from newspaper texts.
The occurrence rate in Talbanken is 0.76 (4658
noun+PP sequences in 6100 sentences), which is
similar to the rates observed for English and Ger-
man. The occurrence rate in SynTag is higher 0.93
(4737 noun+PP sequences in 5114 sentences).
Talbanken (in its converted form) is annotated
with constituent structure labels (NP, PP, VP etc.)
and also distinguishes coordinated phrases (CNP,
CPP, CVP etc.). The queries for determining the
noun attachment rate can thus be similar to the
queries over the German treebanks. In addition,
Talbanken comes with a rich set of grammatical
features as edge labels (e.g. there are different la-
bels for logical subject, dummy subject and other
subject).
We found that the NAR for regular nouns in
Talbanken is 60.5%. Talbanken distinguishes be-
tween regular nouns, deverbal nouns (often with
the derivation suffix -ing: tj¨anstg¨oring, utbildning,
¨ovning) and deadjectival nouns (mostly with the
derivation suffix -het: skyldighet, snabbhet, verk-
samhet). Not surprisingly, these special nouns
have higher NARs than the regular nouns. The
</bodyText>
<page confidence="0.999453">
85
</page>
<bodyText confidence="0.999986693548387">
deadjectival nouns have a NAR of 69.5%, and the
deverbal nouns even have a NAR of 77%. Taken
together (i.e. regarding all regular, deadjectival
and deverbal nouns) this results in a NAR of 64%.
Thus, the NARs which we obtain from the two
Swedish treebanks (SynTag 73% and Talbanken
64%) differ drastically. It is unclear what this dif-
ference depends on. The text genre (newspapers)
is the same in both cases. We have noticed that
SynTag contains a number of annotation errors,
but we don’t see that these errors favor noun at-
tachment of PPs in a systematic way. One aspect
might be the annotation decision in Talbanken to
annotate PPs in light verb constructions.
These are disturbing cases where the PP is a
child node of the sentence node S (which means
that it is interpreted as a verb attachment) with
the edge label OA (objektadverbial). Nivre (2005,
personal communication) pointed out that ”OA is
what some theoreticians would call a ’preposi-
tional object’ or a ’PP complement’, i.e. a com-
plement of the verb that semantically is close to
an object but which is realized as a prepositional
phrase.” In our judgement many of those cases
should be noun attachments (and thus be a child
of an NP).
For example, we looked at f¨oruts¨attning f¨or (=
prerequisite for) which occurs 14 times, out of
which 2 are annotated as OO (Other object) + OA,
11 are annotated as noun attachments, and 1 is er-
roneously annotated. If we compare that to be-
tydelse f¨or (= significance for) which occurs 16
times out of which 13 are annotated as OO+OA
and 3 are annotated as noun attachments, we won-
der.
First, it is obvious that there are inconsistencies
in the treebank. We cannot see any reason why
the 2 cases of f¨oruts¨attning f¨or are annotated dif-
ferently than the other 11 cases. The verbs do not
justify these discrepancies. For example, we have
skapa (= to create) with the verb attachments and
f¨orsvinna (= to disappear) with the noun attach-
ment cases. And we find ge (= to give) on both
sides.
Second, we find it hard to follow the argument
that the tendency for betydelse f¨or is stronger for
the OO+OA than for f¨oruts¨attning f¨or. It might be
based on the fact that betydelse f¨or is often used
with the verb ha (= to have) and thus may count
as a light verb construction with a verb group con-
sisting of both ha plus betydelse and the f¨or-PP
being interpreted as an object of this complex verb
group.
Third, unfortunately not all cases of PPs anno-
tated as objektadverbial can be regarded as noun
attachments. But after having looked at some 70
occurrences of such PPs immediately following a
noun, we estimate that around 30% should be noun
attachments.
Concluding our observations on Swedish let us
mention that the very few cases of proper names
in Talbanken have a NAR of 24%.
</bodyText>
<sectionHeader confidence="0.69446" genericHeader="method">
4 Comparison of the results
</sectionHeader>
<bodyText confidence="0.999966542857143">
For English we have computed a NAR of 76.4%
based on the Penn Treebank, for German we found
NARs between 59% and 63% based on three tree-
banks, and for Swedish we determined a puzzling
difference between 73% NAR in SynTag and 64%
NAR in Talbanken. So, why is the tendency of
a PP to attach to a preceding noun stronger in
English than in Swedish which in turn shows a
stronger tendency than German?
For English the answer is very clear. The strong
NAR is solely based on the dominance of the
preposition of. In our section of the Penn Tree-
bank we found 20,858 noun+PP sequences. Out
of these, 8412 (40% !!) were PPs with the prepo-
sition of. And 99% of all of-PPs are noun attach-
ments. So, the preposition of dominates the Eng-
lish NAR to the point that it should be treated sep-
arately.6
The Ratnaparkhi data sets (described above in
section 2) contain 30% tuples with the preposition
of in the test set and 27% of-tuples in the training
set. The higher percentage of of-tuples in the test
set may partially explain the higher NAR of 59%
(vs. 52% in the training set).
The dominance of of-tuples may also explain
the relatively high NAR for proper names in Eng-
lish (54.5%) in comparison to 17% - 22% in Ger-
man and similar figures for the Swedish Talbanken
corpus. The Penn Treebank represents names that
contain a PP (e.g. District of Columbia, American
Association ofIndividual Investors) with a regular
phrase structure. It turns out that 861 (35%) of the
2449 sequences ’proper name followed by PP’ are
based on of-PPs. The dominance becomes even
more obvious if we consider that the following
</bodyText>
<footnote confidence="0.99648075">
6This is actually what has been done in some research on
English PP attachment disambiguation. (Ratnaparkhi, 1998)
first assumes noun attachment for all of-PPs and then applies
his disambiguation methods to all remaining PPs.
</footnote>
<page confidence="0.997621">
86
</page>
<bodyText confidence="0.999929346153846">
prepositions on the frequency ranks are in (with
only 485 occurrences) and for (246 occurrences).
The dominance of the preposition of is so strong
in English that we will get a totally different pic-
ture of attachment preferences if we omit of-PPs.
The Ratnaparkhi training set without of-tuples is
left with a NAR of 35% (!) and the test set has a
NAR of 42%. In other words, English has a clear
tendency of attaching PPs to verbs if we ignore the
dominating of-PPs.
Neither German nor Swedish has such a dom-
inating preposition. There are, of course, prepo-
sitions in both languages that exhibit a clear ten-
dency towards noun attachment or verb attach-
ment. But they are not as frequent as the prepo-
sition of in English. For example, clear temporal
prepositions like German seit (= since) are much
more likely as verb attachments.
Closest to the English of is the Swedish prepo-
sition av which has a NAR of 88% in the Tal-
banken corpus. But its overall frequency does not
dominate the Swedish ranking. The most frequent
preposition in ambiguous positions is i (frequency:
651 and NAR: 53%) followed by av (frequency:
564; NAR: 88%) and f¨or (frequency: 460; NAR:
42%).
</bodyText>
<sectionHeader confidence="0.998903" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999733333333333">
The most important conclusion to be drawn from
the above experiments and observations is the im-
portance of profiling the data sets when working
and reporting on PP attachment experiments. The
profile should certainly answer the following ques-
tions:
</bodyText>
<listItem confidence="0.888501526315789">
1. What types of nouns where used when the tu-
ples were extracted? (regular nouns, proper
names, deverbal nouns, etc.)
2. Are there prepositions which dominate in fre-
quency and attachment rate (like the English
preposition of)? If so, how does the data set
look like without these dominating preposi-
tions?
3. What types of prepositions where regarded?
(regular prepositions, contracted prepositions
(e.g. in German am, im, zur), derived prepo-
sitions (e.g. English prepositions derived
from gerund verb forms following, including,
pending) etc.)
4. Is the extraction procedure restricted to
noun+PP sequences in the verb phrase, or
does it consider all such sequences?
5. What is the noun attachment rate in the data
set?
</listItem>
<bodyText confidence="0.999984">
In order to find dominating prepositions we sug-
gest a data profiling that includes the frequency
and NARs of all prepositions in the data set. This
will also give an overall picture of the number of
prepositions involved.
Our experiments have also shown the advan-
tages of large treebanks for comparative linguistic
studies. Such treebanks are even more valuable
if they come in the same representation schema
(e.g. TIGER-XML) so that they can be queried
with the same tools. TIGER-Search has proven
to be a suitable treebank query tool for our exper-
iments although its statistics function broke down
on some frequency counts we tried on large tree-
banks. For example, it was not possible to get a
list of all prepositions with occurrence frequencies
from a 50,000 sentence treebank.
Another item on our TIGER-Search wish list is
a batch mode so that we could run a set of queries
and obtain a list of frequencies. Currently we have
to trigger each query manually and copy the fre-
quency results manually to an Excel file.
Other than that, TIGER-Search is a wonderful
tool which allows for quick sanity checks of the
queries with the help of the highlighted tree struc-
ture displays in its GUI.
We have compared noun attachment rates in
English, German and Swedish over treebanks
from various sources and with various annotation
schemes. Of course, the results would be even
better comparable if the treebanks were built on
the same translated texts, i.e. on parallel corpora.
Currently, there are no large parallel treebanks
available. But our group works on such a par-
allel treebank for English, German and Swedish.
Design decisions and first results were reported
in (Volk and Samuelsson, 2004) and (Samuels-
son and Volk, 2005). We believe that such par-
allel treebanks will allow a more focused and
more detailed comparison of phenomena across
languages.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999315">
We would like to thank J¨orgen Aasa for discus-
sions on PP attachment in Swedish, and Joakim
</bodyText>
<page confidence="0.995463">
87
</page>
<bodyText confidence="0.998769">
Nivre, Johan Hall, Jens Nilsson at V¨axj¨o Univer-
sity for making the Swedish Talbanken treebank
available. We also thank the anonymous review-
ers for their discerning comments.
</bodyText>
<sectionHeader confidence="0.996352" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999889214285714">
J¨orgen Aasa. 2004. Unsupervised resolution of PP at-
tachment ambiguities in Swedish. Master’s thesis,
Stockholm University. Combined C/D level thesis.
Michael Collins and James Brooks. 1995. Prepo-
sitional phrase attachment through a backed-off
model. In Proc. of the Third Workshop on Very
Large Corpora.
Bo Hagstr¨om. 2004. A TIGER-XML version of Syn-
Tag. Master’s thesis, Stockhom University.
D. Hindle and M. Rooth. 1993. Structural ambigu-
ity and lexical relations. Computational Linguistics,
19(1):103–120.
Jerker J¨arborg. 1986. SynTag Dokumentation. Manual
f¨or SynTaggning. Technical report, Department of
Swedish, G¨oteborg University.
Esther K¨onig and Wolfgang Lezius. 2002. The TIGER
language - a description language for syntax graphs.
Part 1: User’s guidelines. Technical report.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn treebank. Computa-
tional Linguistics, 19(2):313–330.
P. Merlo, M.W. Crocker, and C. Berthouzoz. 1997. At-
taching multiple prepositional phrases: generalized
backed-off estimation. In Proceedings of the Second
Conference on Empirical Methods in Natural Lan-
guage Processing. Brown University, RI.
A. Ratnaparkhi, J. Reynar, and S. Roukos. 1994. A
maximum entropy model for prepositional phrase at-
tachment. In Proceedings of the ARPA Workshop
on Human Language Technology, Plainsboro, NJ,
March.
Adwait Ratnaparkhi. 1998. Statistical models for un-
supervised prepositional phrase attachment. In Pro-
ceedings of COLING-ACL-98, Montreal.
Douglas L. T. Rohde, 2005. TGrep2 User Man-
ual. MIT. Available from http://tedlab.mit.edu/
—dr/Tgrep2/.
Yvonne Samuelsson and Martin Volk. 2005. Presen-
tation and representation of parallel treebanks. In
Proc. of the Treebank-Workshop at Nodalida, Joen-
suu, May.
J. Stetina and M. Nagao. 1997. Corpus-based PP at-
tachment ambiguity resolution with a semantic dic-
tionary. In J. Zhou and K. Church, editors, Proc.
of the 5th Workshop on Very Large Corpora, pages
66–80, Beijing and Hongkong.
Ulf Telemann. 1974. Manual F¨or Grammatisk
Beskrivning Av Talad Och Skriven Svenska. Inst. f¨or
nordiska spr˚ak, Lund.
Vincent Vandeghinste. 2002. Resolving PP attachment
ambiguities using the WWW (abstract). In Compu-
tational Linguistics in the Netherlands, Groningen.
Martin Volk and Yvonne Samuelsson. 2004. Boot-
strapping parallel treebanks. In Proc. of Work-
shop on Linguistically Interpreted Corpora (LINC)
at COLING, Geneva.
Martin Volk. 2001. The automatic resolution ofprepo-
sitional phrase attachment ambiguities in German.
Habilitationsschrift, University of Zurich.
Martin Volk. 2002. Combining unsupervised and su-
pervised methods for PP attachment disambiguation.
In Proc. of COLING-2002, Taipeh.
Martin Volk. 2003. German prepositions and their kin.
a survey with respect to the resolution of PP attach-
ment ambiguities. In Proc. of ACL-SIGSEM Work-
shop: The Linguistic Dimensions of Prepositions
and their Use in Computational Linguistics For-
malisms and Applications, pages 77–88, Toulouse,
France, September. IRIT.
</reference>
<page confidence="0.999408">
88
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.432958">
<title confidence="0.999594">How bad is the problem of PP-attachment? A comparison of German and Swedish</title>
<author confidence="0.7461235">Martin Volk Stockholm</author>
<affiliation confidence="0.990983">Department of</affiliation>
<address confidence="0.988353">106 91 Stockholm, Sweden</address>
<email confidence="0.936799">volk@ling.su.se</email>
<abstract confidence="0.9972231">The correct attachment of prepositional phrases (PPs) is a central disambiguation problem in parsing natural languages. This paper compares the baseline situation in English, German and Swedish based on manual PP attachments in various treebanks for these languages. We argue that cross-language comparisons of the disambiguation results in previous research is impossible because of the different selection procedures when building the training and test sets. We perform uniform treebank queries and show that English has the highest noun attachment rate followed by Swedish and German. We also show that the high rate in English is dominated by preposition From our study we derive a list of criteria for profiling data sets for PP attachment experiments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J¨orgen Aasa</author>
</authors>
<title>Unsupervised resolution of PP attachment ambiguities in Swedish. Master’s thesis,</title>
<date>2004</date>
<institution>Stockholm University.</institution>
<note>Combined C/D level thesis.</note>
<contexts>
<context position="1736" citStr="Aasa, 2004" startWordPosition="276" endWordPosition="277">tion from a text, these ambiguities must be resolved. One of the most frequent ambiguities arises from the attachment of prepositional phrases (PPs). Simply stated, a PP that follows a noun (in English, German or Swedish) can be attached to the noun or to the verb. In the last decade various methods for the resolution of PP attachment ambiguities have been proposed. The seminal paper by (Hindle and Rooth, 1993) started a sequence of studies for English. We investigated similar methods for German (Volk, 2001; Volk, 2002). Recently other languages (such as Dutch (Vandeghinste, 2002) or Swedish (Aasa, 2004)) have followed. In the PP attachment research for other languages there is often a comparison of the disambiguation accuracy with the English results. But are the results really comparable across languages? Are we starting from the same baseline when working on PP attachment in structurally similar languages like English, German and Swedish? Is the problem of PP attachment equally bad (equally frequent and of equal balance) for these three languages? These are the questions we will discuss in this paper. In order to find answers to these questions we have taken a closer look at the training a</context>
<context position="7516" citStr="Aasa, 2004" startWordPosition="1223" endWordPosition="1224">o extract tuples from the first 10,000 sentences of the NEGRA treebank. This resulted in 6064 tuples with 56% noun attachment (for a detailed overview see (Volk, 2001) p. 86). Again we observe a substantial difference in the baseline. When our student J¨orgen Aasa worked on replicating our German experiments for Swedish, he used a Swedish treebank from the 1980s for the extraction of test data. He extracted V-N-P-N tuples from SynTag, a treebank with 5100 newspaper sentences built by (J¨arborg, 1986). And Aasa was able to extract 2893 tuples out of which 73.8% were marked as noun attachments (Aasa, 2004) (p. 25). This was a surprisingly high figure, and we wondered whether this indicated a tendency in Swedish to avoid the PP in the ambiguous position unless it was to be attached to the noun. But again the extraction process was done with a special purpose extraction program whose correctness was hard to verify. 3 Querying Treebanks with TIGER-Search We therefore decided to check the attachment tendencies of PPs in various treebanks for the three languages in question with the same tool and with queries that are as uniform as possible. For English we used the WSJ section of the Penn Treebank, </context>
<context position="18151" citStr="Aasa, 2004" startWordPosition="3028" endWordPosition="3029">onversion to TIGER-XML is documented in (Hagstr¨om, 2004). The treebank focuses on predicate-argument structures and some grammatical functions such as subject, head and adverbials. It is thus different from the constituent structures that we find in the Penn treebank or the German treebanks. We had to adapt our queries accordingly. Since prepositional phrases are not marked as such, we need to query for constituents (marked as subject, as adverbial or simply as argument) that start with a preposition. This results in a noun attachment rate of 73% (which is very close to the rate reported by (Aasa, 2004)). Again this does not include proper names which have a NAR of 44% in SynTag. Let us compare these results to the second Swedish treebank, Talbanken (first described by (Telemann, 1974)). Talbanken was a remarkable achievement in the 80s as it comes with two written language parts (with a total of more than 10,000 sentences from student essays and from newspapers) and two spoken language parts (with another 10,000 trees from interviews and conversations). We concentrated on the 6100 trees from the written part taken from newspaper texts. The occurrence rate in Talbanken is 0.76 (4658 noun+PP </context>
</contexts>
<marker>Aasa, 2004</marker>
<rawString>J¨orgen Aasa. 2004. Unsupervised resolution of PP attachment ambiguities in Swedish. Master’s thesis, Stockholm University. Combined C/D level thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>James Brooks</author>
</authors>
<title>Prepositional phrase attachment through a backed-off model.</title>
<date>1995</date>
<booktitle>In Proc. of the Third Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="5857" citStr="Collins and Brooks, 1995" startWordPosition="939" endWordPosition="942">occur with the same frequency, we have a 50% baseline but still a trivial disambiguation task. But in reality the baseline puts the disambiguation result into perspective. If, for instance, the baseline is 60% and the disambiguation result is 80% correct attachments, then we will claim that our disambiguation procedure is useful. Whereas 1The V-N-P-N quadruples also contain the head noun of the NP within the PP. 2The Ratnaparkhi training and test sets were later distributed together with a development set of 4039 V-N-P-N tuples. 3It should be noted that important subsequent research, e.g. by (Collins and Brooks, 1995; Stetina and Nagao, 1997), used the Ratnaparkhi data sets and thus allowed for good comparability. if we have a baseline of 80% and the disambiguation result is 75%, then the procedure can be discarded. So what are the baselines reported for other languages? And is it possible to use the same extraction mechanisms for V-N-P-N tuples in order to come to comparable baselines? We did an in-depth study on German PP attachment (Volk, 2001). We compiled our own treebank by annotating 3000 sentences from the weekly computer journal ComputerZeitung. We had first annotated a larger number of subsequen</context>
</contexts>
<marker>Collins, Brooks, 1995</marker>
<rawString>Michael Collins and James Brooks. 1995. Prepositional phrase attachment through a backed-off model. In Proc. of the Third Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Hagstr¨om</author>
</authors>
<title>A TIGER-XML version of SynTag. Master’s thesis,</title>
<date>2004</date>
<institution>Stockhom University.</institution>
<marker>Hagstr¨om, 2004</marker>
<rawString>Bo Hagstr¨om. 2004. A TIGER-XML version of SynTag. Master’s thesis, Stockhom University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
<author>M Rooth</author>
</authors>
<title>Structural ambiguity and lexical relations.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="1539" citStr="Hindle and Rooth, 1993" startWordPosition="244" endWordPosition="247">ng data sets for PP attachment experiments. 1 Introduction Any computer system for natural language processing has to struggle with the problem of ambiguities. If the system is meant to extract precise information from a text, these ambiguities must be resolved. One of the most frequent ambiguities arises from the attachment of prepositional phrases (PPs). Simply stated, a PP that follows a noun (in English, German or Swedish) can be attached to the noun or to the verb. In the last decade various methods for the resolution of PP attachment ambiguities have been proposed. The seminal paper by (Hindle and Rooth, 1993) started a sequence of studies for English. We investigated similar methods for German (Volk, 2001; Volk, 2002). Recently other languages (such as Dutch (Vandeghinste, 2002) or Swedish (Aasa, 2004)) have followed. In the PP attachment research for other languages there is often a comparison of the disambiguation accuracy with the English results. But are the results really comparable across languages? Are we starting from the same baseline when working on PP attachment in structurally similar languages like English, German and Swedish? Is the problem of PP attachment equally bad (equally frequ</context>
<context position="3230" citStr="Hindle and Rooth, 1993" startWordPosition="514" endWordPosition="517">ving PP attachment ambiguities. And they evaluated their method against 880 English triples verb-noun-preposition (V-N-P) which they had extracted from randomly selected, ambiguously located PPs in a corpus. For example, the sentence ”Timex had requested duty-free treatment for many types of watches” results in the V-N-P triple (request, treatment, for). These triples were manually annotated by both authors with either noun or verb attachment based on the complete sentence context. Interestingly, 586 of these triples (67%) were judged as noun attachments and only 33% as verb attachments. And (Hindle and Rooth, 1993) reported on 80% attachment accuracy, an improvement of 13% over the baseline (i.e. guessing noun attachment in all Proceedings of the Third ACL-SIGSEM Workshop on Prepositions, pages 81–88, Trento, Italy, April 2006. c�2006 Association for Computational Linguistics 81 cases). A year later (Ratnaparkhi et al., 1994) published a supervised approach to the PP attachment problem. They had extracted quadruples V-N-P-N1 (plus the accompanying attachment decision) from both an IBM computer manuals treebank (about 9000 tuples) and from the Wall Street Journal (WSJ) section of the Penn treebank (about</context>
<context position="4657" citStr="Hindle and Rooth, 1993" startWordPosition="744" endWordPosition="747">reported on 81.6% correct attachments. But have they solved the same problem as (Hindle and Rooth, 1993)? What was the initial bias towards noun attachment in their data? It turns out that their training set (the 20,801 tuples) contains only 52% noun attachments, while their test set (the 3097 tuples) contains 59% noun attachments. The difference in noun attachments between these two sets is striking, but (Ratnaparkhi et al., 1994) do not discuss this (and we also do not have an explanation for this). But it makes obvious that (Ratnaparkhi et al., 1994) were tackling a problem different from (Hindle and Rooth, 1993) given the fact that their baseline was at 59% guessing noun attachment (rather than 67% in the Hindle and Rooth experiments).3 Of course, the baseline is not a direct indicator of the difficulty of the disambiguation task. We may construct (artificial) cases with low baselines and a simple distribution of PP attachment tendencies. For example, we may construct the case that a language has 100 different prepositions, where 50 prepositions always introduce noun attachments, and the other 50 prepositions always require verb attachments. If we also assume that both groups occur with the same freq</context>
<context position="13005" citStr="Hindle and Rooth, 1993" startWordPosition="2160" endWordPosition="2163">utomatically disambiguating them. NAR = freq(noun + PP) freq(noun + PP, noun attachm) 83 Figure 1: Noun phrase tree from the Penn Treebank to the noun immediately preceding it (as for example in the tree in figure 1). With some similar queries we located another 110 cases of noun attachments (most of which are probably annotation errors if the annotation guidelines are applied strictly). This means that we found a total of 15,930 cases of noun attachment which corresponds to a noun attachment rate of 76.4% (by comparison to the 20,858 occurrences). This is a surprisingly high number. Neither (Hindle and Rooth, 1993) with 67% nor (Ratnaparkhi et al., 1994) with 59% noun attachment were anywhere close to this figure. What have we done differently? One aspect is that we only queried for singular nouns (NN) in the Penn Treebank where plural nouns (NNS) and proper names (NNP and NNPS) have separate PoS tags. Using analogous queries for plural nouns we found that they exhibit a NAR of 71.7%. Whereas the queries for proper names (singular and plural names taken together) account for a NAR of 54.5%. Another reason for the discrepancy in the NAR between Ratnaparkhi’s data and our calculations certainly comes from</context>
</contexts>
<marker>Hindle, Rooth, 1993</marker>
<rawString>D. Hindle and M. Rooth. 1993. Structural ambiguity and lexical relations. Computational Linguistics, 19(1):103–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerker J¨arborg</author>
</authors>
<title>SynTag Dokumentation. Manual f¨or SynTaggning.</title>
<date>1986</date>
<tech>Technical report,</tech>
<institution>Department of Swedish, G¨oteborg University.</institution>
<marker>J¨arborg, 1986</marker>
<rawString>Jerker J¨arborg. 1986. SynTag Dokumentation. Manual f¨or SynTaggning. Technical report, Department of Swedish, G¨oteborg University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Esther K¨onig</author>
<author>Wolfgang Lezius</author>
</authors>
<title>The TIGER language - a description language for syntax graphs. Part 1: User’s guidelines.</title>
<date>2002</date>
<tech>Technical report.</tech>
<marker>K¨onig, Lezius, 2002</marker>
<rawString>Esther K¨onig and Wolfgang Lezius. 2002. The TIGER language - a description language for syntax graphs. Part 1: User’s guidelines. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="11546" citStr="Marcus et al., 1993" startWordPosition="1906" endWordPosition="1909"> treebanks. We assume that all PPs in noun+PP sequences which are not attached to a noun are attached to a verb. This means we ignore the very few cases of such PPs that might be attached to adjectives (as for instance the second PP in ”due for revision in 1990”). Different annotation schemes require modifications to these basic queries, and different noun classes (regular nouns, proper names, deverbal nouns etc.) allow for a more detailed investigation. We now present the results for each language in turn. 3.1 Results for English We used sections 0 to 12 of the WSJ part of the Penn Treebank (Marcus et al., 1993) with a total of 24,618 sentences for our experiments. Our start query reveals that an ambiguously located PP (i.e. a noun+PP sequence) occurs in 13,191 (54%) of these sentences, and it occurs a total of 20,858 times (a rate of 0.84 occurrences per sentences with respect to all sentences in the treebank). Searching for noun attachments with the second query described in section 3 we learn that 15,273 noun+PP sequences are annotated as noun attachments. And we catch another 547 noun attachments if we query for noun phrases that contain two PPs in sequence.5 In these cases the second PP is also </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Merlo</author>
<author>M W Crocker</author>
<author>C Berthouzoz</author>
</authors>
<title>Attaching multiple prepositional phrases: generalized backed-off estimation.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing.</booktitle>
<publisher>Brown University, RI.</publisher>
<contexts>
<context position="12329" citStr="Merlo et al., 1997" startWordPosition="2042" endWordPosition="2045">ese sentences, and it occurs a total of 20,858 times (a rate of 0.84 occurrences per sentences with respect to all sentences in the treebank). Searching for noun attachments with the second query described in section 3 we learn that 15,273 noun+PP sequences are annotated as noun attachments. And we catch another 547 noun attachments if we query for noun phrases that contain two PPs in sequence.5 In these cases the second PP is also attached to a noun, although not 4There are a few occurrences of this latter structure in the Penn Treebank which should probably count as annotation errors. 5See (Merlo et al., 1997) for a discussion of these cases and an approach in automatically disambiguating them. NAR = freq(noun + PP) freq(noun + PP, noun attachm) 83 Figure 1: Noun phrase tree from the Penn Treebank to the noun immediately preceding it (as for example in the tree in figure 1). With some similar queries we located another 110 cases of noun attachments (most of which are probably annotation errors if the annotation guidelines are applied strictly). This means that we found a total of 15,930 cases of noun attachment which corresponds to a noun attachment rate of 76.4% (by comparison to the 20,858 occurr</context>
</contexts>
<marker>Merlo, Crocker, Berthouzoz, 1997</marker>
<rawString>P. Merlo, M.W. Crocker, and C. Berthouzoz. 1997. Attaching multiple prepositional phrases: generalized backed-off estimation. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing. Brown University, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
<author>J Reynar</author>
<author>S Roukos</author>
</authors>
<title>A maximum entropy model for prepositional phrase attachment.</title>
<date>1994</date>
<booktitle>In Proceedings of the ARPA Workshop on Human Language Technology,</booktitle>
<location>Plainsboro, NJ,</location>
<contexts>
<context position="3547" citStr="Ratnaparkhi et al., 1994" startWordPosition="561" endWordPosition="564">e V-N-P triple (request, treatment, for). These triples were manually annotated by both authors with either noun or verb attachment based on the complete sentence context. Interestingly, 586 of these triples (67%) were judged as noun attachments and only 33% as verb attachments. And (Hindle and Rooth, 1993) reported on 80% attachment accuracy, an improvement of 13% over the baseline (i.e. guessing noun attachment in all Proceedings of the Third ACL-SIGSEM Workshop on Prepositions, pages 81–88, Trento, Italy, April 2006. c�2006 Association for Computational Linguistics 81 cases). A year later (Ratnaparkhi et al., 1994) published a supervised approach to the PP attachment problem. They had extracted quadruples V-N-P-N1 (plus the accompanying attachment decision) from both an IBM computer manuals treebank (about 9000 tuples) and from the Wall Street Journal (WSJ) section of the Penn treebank (about 24,000 tuples). The latter tuple set has been reused by subsequent research, so let us focus on this one.2 (Ratnaparkhi et al., 1994) used 20,801 tuples for training and 3097 tuples for evaluation. They reported on 81.6% correct attachments. But have they solved the same problem as (Hindle and Rooth, 1993)? What wa</context>
<context position="13045" citStr="Ratnaparkhi et al., 1994" startWordPosition="2167" endWordPosition="2171">= freq(noun + PP) freq(noun + PP, noun attachm) 83 Figure 1: Noun phrase tree from the Penn Treebank to the noun immediately preceding it (as for example in the tree in figure 1). With some similar queries we located another 110 cases of noun attachments (most of which are probably annotation errors if the annotation guidelines are applied strictly). This means that we found a total of 15,930 cases of noun attachment which corresponds to a noun attachment rate of 76.4% (by comparison to the 20,858 occurrences). This is a surprisingly high number. Neither (Hindle and Rooth, 1993) with 67% nor (Ratnaparkhi et al., 1994) with 59% noun attachment were anywhere close to this figure. What have we done differently? One aspect is that we only queried for singular nouns (NN) in the Penn Treebank where plural nouns (NNS) and proper names (NNP and NNPS) have separate PoS tags. Using analogous queries for plural nouns we found that they exhibit a NAR of 71.7%. Whereas the queries for proper names (singular and plural names taken together) account for a NAR of 54.5%. Another reason for the discrepancy in the NAR between Ratnaparkhi’s data and our calculations certainly comes from the fact that we queried for all sequen</context>
</contexts>
<marker>Ratnaparkhi, Reynar, Roukos, 1994</marker>
<rawString>A. Ratnaparkhi, J. Reynar, and S. Roukos. 1994. A maximum entropy model for prepositional phrase attachment. In Proceedings of the ARPA Workshop on Human Language Technology, Plainsboro, NJ, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Statistical models for unsupervised prepositional phrase attachment.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL-98,</booktitle>
<location>Montreal.</location>
<contexts>
<context position="24292" citStr="Ratnaparkhi, 1998" startWordPosition="4087" endWordPosition="4088">es may also explain the relatively high NAR for proper names in English (54.5%) in comparison to 17% - 22% in German and similar figures for the Swedish Talbanken corpus. The Penn Treebank represents names that contain a PP (e.g. District of Columbia, American Association ofIndividual Investors) with a regular phrase structure. It turns out that 861 (35%) of the 2449 sequences ’proper name followed by PP’ are based on of-PPs. The dominance becomes even more obvious if we consider that the following 6This is actually what has been done in some research on English PP attachment disambiguation. (Ratnaparkhi, 1998) first assumes noun attachment for all of-PPs and then applies his disambiguation methods to all remaining PPs. 86 prepositions on the frequency ranks are in (with only 485 occurrences) and for (246 occurrences). The dominance of the preposition of is so strong in English that we will get a totally different picture of attachment preferences if we omit of-PPs. The Ratnaparkhi training set without of-tuples is left with a NAR of 35% (!) and the test set has a NAR of 42%. In other words, English has a clear tendency of attaching PPs to verbs if we ignore the dominating of-PPs. Neither German nor</context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>Adwait Ratnaparkhi. 1998. Statistical models for unsupervised prepositional phrase attachment. In Proceedings of COLING-ACL-98, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas L T Rohde</author>
</authors>
<date>2005</date>
<booktitle>TGrep2 User Manual. MIT. Available from http://tedlab.mit.edu/ —dr/Tgrep2/.</booktitle>
<contexts>
<context position="8925" citStr="Rohde, 2005" startWordPosition="1453" endWordPosition="1455">h we used the SynTag treebank mentioned above and one section of the Talbanken treebank (6100 sentences). All these treebanks consist of constituent structure trees, and they are in representation formats which allow them to be loaded into TIGER-Search. This enables us to query them all in similar manners and to get a fairer comparison of the attachment tendencies. TIGER-Search is a powerful treebank query tool developed at the University of Stuttgart (K¨onig and Lezius, 2002). Its query language allows for feature-value descriptions of syntax graphs. It is similar in expressiveness to tgrep (Rohde, 2005) but it comes with graphical output and highlighting of the syntax trees plus some nice statistics functions. Our experiments for determining attachment tendencies proceed along the following lines. For each treebank we first query for all sequences of a noun immediately followed by a PP (henceforth noun+PP sequences). The dot being the precedence operator, we use the query: [pos=&amp;quot;NN&amp;quot;] . [cat=&amp;quot;PP&amp;quot;] This query will match twice in the tree in figure 1. It gives us the frequency of all ambiguously located PP. We disregard the fact that in certain clause positions a PP in such a sequence cannot be</context>
</contexts>
<marker>Rohde, 2005</marker>
<rawString>Douglas L. T. Rohde, 2005. TGrep2 User Manual. MIT. Available from http://tedlab.mit.edu/ —dr/Tgrep2/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yvonne Samuelsson</author>
<author>Martin Volk</author>
</authors>
<title>Presentation and representation of parallel treebanks.</title>
<date>2005</date>
<booktitle>In Proc. of the Treebank-Workshop at Nodalida,</booktitle>
<location>Joensuu,</location>
<marker>Samuelsson, Volk, 2005</marker>
<rawString>Yvonne Samuelsson and Martin Volk. 2005. Presentation and representation of parallel treebanks. In Proc. of the Treebank-Workshop at Nodalida, Joensuu, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Stetina</author>
<author>M Nagao</author>
</authors>
<title>Corpus-based PP attachment ambiguity resolution with a semantic dictionary.</title>
<date>1997</date>
<booktitle>Proc. of the 5th Workshop on Very Large Corpora,</booktitle>
<pages>66--80</pages>
<editor>In J. Zhou and K. Church, editors,</editor>
<contexts>
<context position="5883" citStr="Stetina and Nagao, 1997" startWordPosition="943" endWordPosition="946">ncy, we have a 50% baseline but still a trivial disambiguation task. But in reality the baseline puts the disambiguation result into perspective. If, for instance, the baseline is 60% and the disambiguation result is 80% correct attachments, then we will claim that our disambiguation procedure is useful. Whereas 1The V-N-P-N quadruples also contain the head noun of the NP within the PP. 2The Ratnaparkhi training and test sets were later distributed together with a development set of 4039 V-N-P-N tuples. 3It should be noted that important subsequent research, e.g. by (Collins and Brooks, 1995; Stetina and Nagao, 1997), used the Ratnaparkhi data sets and thus allowed for good comparability. if we have a baseline of 80% and the disambiguation result is 75%, then the procedure can be discarded. So what are the baselines reported for other languages? And is it possible to use the same extraction mechanisms for V-N-P-N tuples in order to come to comparable baselines? We did an in-depth study on German PP attachment (Volk, 2001). We compiled our own treebank by annotating 3000 sentences from the weekly computer journal ComputerZeitung. We had first annotated a larger number of subsequent sentences with Part-of-S</context>
</contexts>
<marker>Stetina, Nagao, 1997</marker>
<rawString>J. Stetina and M. Nagao. 1997. Corpus-based PP attachment ambiguity resolution with a semantic dictionary. In J. Zhou and K. Church, editors, Proc. of the 5th Workshop on Very Large Corpora, pages 66–80, Beijing and Hongkong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulf Telemann</author>
</authors>
<title>Manual F¨or Grammatisk Beskrivning Av Talad Och Skriven Svenska. Inst. f¨or nordiska spr˚ak,</title>
<date>1974</date>
<location>Lund.</location>
<contexts>
<context position="18337" citStr="Telemann, 1974" startWordPosition="3059" endWordPosition="3060">It is thus different from the constituent structures that we find in the Penn treebank or the German treebanks. We had to adapt our queries accordingly. Since prepositional phrases are not marked as such, we need to query for constituents (marked as subject, as adverbial or simply as argument) that start with a preposition. This results in a noun attachment rate of 73% (which is very close to the rate reported by (Aasa, 2004)). Again this does not include proper names which have a NAR of 44% in SynTag. Let us compare these results to the second Swedish treebank, Talbanken (first described by (Telemann, 1974)). Talbanken was a remarkable achievement in the 80s as it comes with two written language parts (with a total of more than 10,000 sentences from student essays and from newspapers) and two spoken language parts (with another 10,000 trees from interviews and conversations). We concentrated on the 6100 trees from the written part taken from newspaper texts. The occurrence rate in Talbanken is 0.76 (4658 noun+PP sequences in 6100 sentences), which is similar to the rates observed for English and German. The occurrence rate in SynTag is higher 0.93 (4737 noun+PP sequences in 5114 sentences). Talb</context>
</contexts>
<marker>Telemann, 1974</marker>
<rawString>Ulf Telemann. 1974. Manual F¨or Grammatisk Beskrivning Av Talad Och Skriven Svenska. Inst. f¨or nordiska spr˚ak, Lund.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Vandeghinste</author>
</authors>
<title>Resolving PP attachment ambiguities using the WWW (abstract).</title>
<date>2002</date>
<booktitle>In Computational Linguistics in the Netherlands,</booktitle>
<location>Groningen.</location>
<contexts>
<context position="1712" citStr="Vandeghinste, 2002" startWordPosition="272" endWordPosition="273">meant to extract precise information from a text, these ambiguities must be resolved. One of the most frequent ambiguities arises from the attachment of prepositional phrases (PPs). Simply stated, a PP that follows a noun (in English, German or Swedish) can be attached to the noun or to the verb. In the last decade various methods for the resolution of PP attachment ambiguities have been proposed. The seminal paper by (Hindle and Rooth, 1993) started a sequence of studies for English. We investigated similar methods for German (Volk, 2001; Volk, 2002). Recently other languages (such as Dutch (Vandeghinste, 2002) or Swedish (Aasa, 2004)) have followed. In the PP attachment research for other languages there is often a comparison of the disambiguation accuracy with the English results. But are the results really comparable across languages? Are we starting from the same baseline when working on PP attachment in structurally similar languages like English, German and Swedish? Is the problem of PP attachment equally bad (equally frequent and of equal balance) for these three languages? These are the questions we will discuss in this paper. In order to find answers to these questions we have taken a close</context>
</contexts>
<marker>Vandeghinste, 2002</marker>
<rawString>Vincent Vandeghinste. 2002. Resolving PP attachment ambiguities using the WWW (abstract). In Computational Linguistics in the Netherlands, Groningen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Volk</author>
<author>Yvonne Samuelsson</author>
</authors>
<title>Bootstrapping parallel treebanks.</title>
<date>2004</date>
<booktitle>In Proc. of Workshop on Linguistically Interpreted Corpora (LINC) at COLING,</booktitle>
<location>Geneva.</location>
<marker>Volk, Samuelsson, 2004</marker>
<rawString>Martin Volk and Yvonne Samuelsson. 2004. Bootstrapping parallel treebanks. In Proc. of Workshop on Linguistically Interpreted Corpora (LINC) at COLING, Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Volk</author>
</authors>
<title>The automatic resolution ofprepositional phrase attachment ambiguities in German. Habilitationsschrift,</title>
<date>2001</date>
<institution>University of Zurich.</institution>
<contexts>
<context position="1637" citStr="Volk, 2001" startWordPosition="262" endWordPosition="263">has to struggle with the problem of ambiguities. If the system is meant to extract precise information from a text, these ambiguities must be resolved. One of the most frequent ambiguities arises from the attachment of prepositional phrases (PPs). Simply stated, a PP that follows a noun (in English, German or Swedish) can be attached to the noun or to the verb. In the last decade various methods for the resolution of PP attachment ambiguities have been proposed. The seminal paper by (Hindle and Rooth, 1993) started a sequence of studies for English. We investigated similar methods for German (Volk, 2001; Volk, 2002). Recently other languages (such as Dutch (Vandeghinste, 2002) or Swedish (Aasa, 2004)) have followed. In the PP attachment research for other languages there is often a comparison of the disambiguation accuracy with the English results. But are the results really comparable across languages? Are we starting from the same baseline when working on PP attachment in structurally similar languages like English, German and Swedish? Is the problem of PP attachment equally bad (equally frequent and of equal balance) for these three languages? These are the questions we will discuss in th</context>
<context position="6296" citStr="Volk, 2001" startWordPosition="1020" endWordPosition="1021">ets were later distributed together with a development set of 4039 V-N-P-N tuples. 3It should be noted that important subsequent research, e.g. by (Collins and Brooks, 1995; Stetina and Nagao, 1997), used the Ratnaparkhi data sets and thus allowed for good comparability. if we have a baseline of 80% and the disambiguation result is 75%, then the procedure can be discarded. So what are the baselines reported for other languages? And is it possible to use the same extraction mechanisms for V-N-P-N tuples in order to come to comparable baselines? We did an in-depth study on German PP attachment (Volk, 2001). We compiled our own treebank by annotating 3000 sentences from the weekly computer journal ComputerZeitung. We had first annotated a larger number of subsequent sentences with Part-of-Speech tags, and based on these PoS tags, we selected 3000 sentences that contained at least one full verb plus the sequence of a noun followed by a preposition. After annotating the 3000 sentences with complete syntax trees we used a Prolog program to extract V-N-P-N tuples with the accompanying attachment decisions. This lead to 4562 tuples out of which 61% were marked as noun attachments. We used the same pr</context>
<context position="15580" citStr="Volk, 2001" startWordPosition="2607" endWordPosition="2608">proper names show a much lower noun attachment rate than the regular nouns. The NAR in the CZ treebank is 22%, in the NEGRA treebank it is 20%, and in the TIGER treebank it is only 17%. Here we suspect that the difference between the CZ and the other treebanks is based on the different text types. The computer journal CZ contains more person names with affiliation (e.g. Stan Sugarman von der Firma Telemedia) and more company names with location (e.g. Aviso aus Finn84 land) than a regular newspaper (that was used in the NEGRA and TIGER corpora). As mentioned above, our previous experiments in (Volk, 2001) were based on sets of extracted tuples from both the CZ and NEGRA treebanks. Our extracted data set from the CZ treebank had a noun attachment rate of 61%, and the one from the NEGRA treebank had a noun attachment rate of 56%. So why are our new results based on TIGERSearch queries two to three percents higher? The main reason is that our old data sets included proper names (with their low noun attachment rate). But our extraction procedure comprised also a number of other idiosyncracies. In an attempt to harvest as many interesting V-N-P-N tuples as possible from our treebanks we exploited c</context>
</contexts>
<marker>Volk, 2001</marker>
<rawString>Martin Volk. 2001. The automatic resolution ofprepositional phrase attachment ambiguities in German. Habilitationsschrift, University of Zurich.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Volk</author>
</authors>
<title>Combining unsupervised and supervised methods for PP attachment disambiguation.</title>
<date>2002</date>
<booktitle>In Proc. of COLING-2002,</booktitle>
<location>Taipeh.</location>
<contexts>
<context position="1650" citStr="Volk, 2002" startWordPosition="264" endWordPosition="265">gle with the problem of ambiguities. If the system is meant to extract precise information from a text, these ambiguities must be resolved. One of the most frequent ambiguities arises from the attachment of prepositional phrases (PPs). Simply stated, a PP that follows a noun (in English, German or Swedish) can be attached to the noun or to the verb. In the last decade various methods for the resolution of PP attachment ambiguities have been proposed. The seminal paper by (Hindle and Rooth, 1993) started a sequence of studies for English. We investigated similar methods for German (Volk, 2001; Volk, 2002). Recently other languages (such as Dutch (Vandeghinste, 2002) or Swedish (Aasa, 2004)) have followed. In the PP attachment research for other languages there is often a comparison of the disambiguation accuracy with the English results. But are the results really comparable across languages? Are we starting from the same baseline when working on PP attachment in structurally similar languages like English, German and Swedish? Is the problem of PP attachment equally bad (equally frequent and of equal balance) for these three languages? These are the questions we will discuss in this paper. In </context>
</contexts>
<marker>Volk, 2002</marker>
<rawString>Martin Volk. 2002. Combining unsupervised and supervised methods for PP attachment disambiguation. In Proc. of COLING-2002, Taipeh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Volk</author>
</authors>
<title>German prepositions and their kin. a survey with respect to the resolution of PP attachment ambiguities.</title>
<date>2003</date>
<booktitle>In Proc. of ACL-SIGSEM Workshop: The Linguistic Dimensions of Prepositions and their Use in Computational Linguistics Formalisms and Applications,</booktitle>
<pages>77--88</pages>
<publisher>IRIT.</publisher>
<location>Toulouse, France,</location>
<contexts>
<context position="17173" citStr="Volk, 2003" startWordPosition="2869" endWordPosition="2870">h the decision ’noun attachment’. 2. If the PP was introduced by coordinated prepositions (e.g. Die Argumente f¨ur oder gegen den Netzwerkcomputer), we created as many tuples as there were prepositions. 3. If the verb group consists of coordinated verbs (e.g. Infos f¨ur Online-Dienste aufbereiten und gestalten), we created as many tuples as there were verbs. 4. We regarded pronominal adverbs (darin, dazu, hier¨uber, etc.) and reciprocal pronouns (miteinander, untereinander, voneinander, etc.) as equivalent to PPs and created tuples when such pronominals appeared immediately after a noun. See (Volk, 2003) for a more detailed discussion of these pronouns. 3.3 Results for Swedish Currently there is no large-scale Swedish treebank available. But there are some smaller treebanks from the 80s which have recently been converted to TIGER-XML so that they can also be queried with TIGER-Search. SynTag (J¨arborg, 1986) is a treebank consisting of around 5100 sentences. Its conversion to TIGER-XML is documented in (Hagstr¨om, 2004). The treebank focuses on predicate-argument structures and some grammatical functions such as subject, head and adverbials. It is thus different from the constituent structure</context>
</contexts>
<marker>Volk, 2003</marker>
<rawString>Martin Volk. 2003. German prepositions and their kin. a survey with respect to the resolution of PP attachment ambiguities. In Proc. of ACL-SIGSEM Workshop: The Linguistic Dimensions of Prepositions and their Use in Computational Linguistics Formalisms and Applications, pages 77–88, Toulouse, France, September. IRIT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>