<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001263">
<title confidence="0.9987345">
Learning Lexical Embeddings with Syntactic
and Lexicographic Knowledge
</title>
<author confidence="0.998786">
Tong Wang Abdel-rahman Mohamed Graeme Hirst
</author>
<affiliation confidence="0.998676">
University of Toronto Microsoft Research University of Toronto
</affiliation>
<email confidence="0.997093">
tong@cs.toronto.edu asamir@microsoft.com gh@cs.toronto.edu
</email>
<sectionHeader confidence="0.993865" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999898454545454">
We propose two improvements on lexi-
cal association used in embedding learn-
ing: factorizing individual dependency re-
lations and using lexicographic knowl-
edge from monolingual dictionaries. Both
proposals provide low-entropy lexical co-
occurrence information, and are empiri-
cally shown to improve embedding learn-
ing by performing notably better than sev-
eral popular embedding models in similar-
ity tasks.
</bodyText>
<sectionHeader confidence="0.762912" genericHeader="categories and subject descriptors">
1 Lexical Embeddings and Relatedness
</sectionHeader>
<bodyText confidence="0.999951833333333">
Lexical embeddings are essentially real-valued
distributed representations of words. As a vector-
space model, an embedding model approximates
semantic relatedness with the Euclidean distance
between embeddings, the result of which helps
better estimate the real lexical distribution in var-
ious NLP tasks. In recent years, researchers have
developed efficient and effective algorithms for
learning embeddings (Mikolov et al., 2013a; Pen-
nington et al., 2014) and extended model applica-
tions from language modelling to various areas in
NLP including lexical semantics (Mikolov et al.,
2013b) and parsing (Bansal et al., 2014).
To approximate semantic relatedness with ge-
ometric distance, objective functions are usu-
ally chosen to correlate positively with the Eu-
clidean similarity between the embeddings of re-
lated words. Maximizing such an objective func-
tion is then equivalent to adjusting the embeddings
so that those of the related words will be geomet-
rically closer.
The definition of relatedness among words can
have a profound influence on the quality of the
resulting embedding models. In most existing
studies, relatedness is defined by co-occurrence
within a window frame sliding over texts. Al-
though supported by the distributional hypothe-
sis (Harris, 1954), this definition suffers from two
major limitations. Firstly, the window frame size
is usually rather small (for efficiency and sparsity
considerations), which increases the false negative
rate by missing long-distance dependencies. Sec-
ondly, a window frame can (and often does) span
across different constituents in a sentence, result-
ing in an increased false positive rate by associ-
ating unrelated words. The problem is worsened
as the size of the window increases since each
false-positive n-gram will appear in two subsum-
ing false-positive (n + 1)-grams.
Several existing studies have addressed these
limitations of window-based contexts. Nonethe-
less, we hypothesize that lexical embedding learn-
ing can further benefit from (1) factorizing syntac-
tic relations into individual relations for structured
syntactic information and (2) defining relatedness
using lexicographic knowledge. We will show that
implementation of these ideas brings notable im-
provement in lexical similarity tasks.
</bodyText>
<sectionHeader confidence="0.999745" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999889411764706">
Lexical embeddings have traditionally been used
in language modelling as distributed representa-
tions of words (Bengio et al., 2003; Mnih and Hin-
ton, 2009) and have only recently been used in
other NLP tasks. Turian et al. (2010), for example,
used embeddings from existing language models
(Collobert and Weston, 2008; Mnih and Hinton,
2007) as unsupervised lexical features to improve
named entity recognition and chunking. Embed-
ding models gained further popularity thanks to
the simplicity and effectiveness of the word2vec
model (Mikolov et al., 2013a), which implicitly
factorizes the point-wise mutual information ma-
trix shifted by biases consisting of marginal counts
of individual words (Levy and Goldberg, 2014b).
Efficiency is greatly improved by approximating
the computationally costly softmax function with
</bodyText>
<page confidence="0.878828">
458
</page>
<bodyText confidence="0.968491555555556">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 458–463,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
negative sampling (similar to that of Collobert and
Weston 2008) or hierarchical softmax (similar to
that of Mnih and Hinton 2007).
To address the limitation of contextual locality
in many language models (including word2vec),
Huang et al. (2012) added a “global context score”
to the local n-gram score (Collobert and Weston,
2008). The concatenation of word vectors and
a “document vector” (centroid of the composing
word vectors weighted by idf) was used as model
input. Pennington et al. (2014) proposed to explic-
itly factorize the global co-occurrence matrix be-
tween words, and the resulting log bilinear model
achieved state-of-the-art performance in lexical
similarity, analogy, and named entity recognition.
Several later studies addressed the limitations
of window-based co-occurrence by extending the
word2vec model to predict words that are syn-
tactically related to target words. Levy and Gold-
berg (2014a) used syntactically related words non-
discriminatively as syntactic context. Bansal et al.
(2014) used a training corpus consisting of se-
quences of labels following certain manually com-
piled patterns. Zhao et al. (2014) employed
coarse-grained classifications of contexts accord-
ing to the hierarchical structures in a parse tree.
Semantic relations have also been explored as a
form of lexical association. Faruqui et al. (2015)
proposed to retrofit pre-trained embeddings (de-
rived using window-based contexts) to semantic
lexicons. The goal is to derive a set of embeddings
to capture relatedness suggested by semantic lex-
icons while maintaining their resemblance to the
corresponding window-based embeddings. Bolle-
gala et al. (2014) trained an embedding model with
lexical, part-of-speech, and dependency patterns
extracted from sentences containing frequently co-
occurring word pairs. Each relation was repre-
sented by a pattern representation matrix, which
was combined and updated together with the word
representation matrix (i.e., lexical embeddings) in
a bilinear objective function.
</bodyText>
<sectionHeader confidence="0.999338" genericHeader="method">
3 The Proposed Models
</sectionHeader>
<subsectionHeader confidence="0.999956">
3.1 Factorizing Dependency Relations
</subsectionHeader>
<bodyText confidence="0.999869923076923">
One strong limitation of the existing dependency-
based models is that no distinctions are made
among the many different types of dependency re-
lations. This is essentially a compromise to avoid
issues in model complexity and data sparsity, and
it precludes the possibility of studying individual
or interactive effects of individual dependency re-
lations on embedding learning.
Consequently, we propose a relation-dependent
model to predict dependents given a governor un-
der individual dependency relations. For example,
given a nominal governor apple of the adjective
modifier relation (amod), an embedding model
will be trained to assign higher probability to ob-
served adjectival dependents (e.g., red, sweet, etc.)
than to rarely or never observed ones (e.g., pur-
ple, savoury, etc.). If a model is able to accurately
make such predictions, it can then be said to “un-
derstand” the meaning of apple by possessing se-
mantic knowledge about its certain attributes. By
extension, similar models can be trained to learn
the meaning of the governors in other dependency
relations (e.g., adjectival governors in the inverse
relation amod−1, etc.).
The basic model uses an objective function sim-
ilar to that of Mikolov et al. (2013a):
</bodyText>
<equation confidence="0.657027">
E ˆdi[log6(−eT g e� ˆdi)],
</equation>
<bodyText confidence="0.99995025">
where e* and e* are the target and the output
embeddings for the corresponding words, respec-
tively, and 6 is the sigmoid function. The sub-
scripts g and d indicate whether an embedding cor-
respond to the governor or the dependent of a de-
pendency pair, and ˆd* correspond to random sam-
ples from the dependent vocabulary (drawn by un-
igram frequency).
</bodyText>
<subsectionHeader confidence="0.999605">
3.2 Incorporating Lexicographic Knowledge
</subsectionHeader>
<bodyText confidence="0.999935263157895">
Semantic information used in existing studies
(Section 2) either relies on specialized lexical re-
sources with limited availability or is obtained
from complex procedures that are difficult to repli-
cate. To address these issues, we propose to use
monolingual dictionaries as a simple yet effective
source of semantic knowledge. The defining rela-
tion has been demonstrated to have good perfor-
mance in various semantic tasks (Chodorow et al.,
1985; Alshawi, 1987). The inverse of the defining
relation (also known as the Olney Concordance In-
dex, Reichert et al. 1969) has also been proven use-
ful in building lexicographic taxonomies (Amsler,
1980) and identifying synonyms (Wang and Hirst,
2011). Therefore, we use both the defining rela-
tion and its inverse as sources of semantic associ-
ation in the proposed embedding models.
Lexicographic knowledge is represented by
adopting the same terminology used in syntactic
</bodyText>
<equation confidence="0.998701">
log6(eTg e�d) +
k
∑
i=1
</equation>
<page confidence="0.988842">
459
</page>
<bodyText confidence="0.999905857142857">
dependencies: definienda as governors and defini-
entia as dependents. For example, apple is related
to fruit and rosaceous as a governor under def, or
to cider and pippin as a dependent under def−1.
deeper structure with pre-training on the factor-
ized results (via the relation-dependent models) in
the first layer.
</bodyText>
<subsectionHeader confidence="0.9987075">
3.3 Combining Individual Knowledge
Sources
</subsectionHeader>
<bodyText confidence="0.999930181818182">
Sparsity is a prominent issue in the relation-
dependent models since each individual relation
only receives a limited share of the overall co-
occurrence information. We also propose a post-
hoc, relation-independent model that combines
the individual knowledge sources. The input of the
model is the structured knowledge from relation-
dependent models, for example, that something
can be red or sweet, or it can ripen or fall, etc.
The training objective is to predict the original
word given the relation-dependent embeddings,
with the intuition that if a model is trained to be
able to “solve the riddle” and predict that this
something is an apple, then the model is said
to possess generic, relation-independent knowl-
edge about the target word by learning from the
relation-dependent knowledge sources.
Given input word wI, its relation-independent
embedding is derived by applying a linear model
M on the concatenation of its relation-dependent
embeddings (˜ewI). The objective function of a
relation-independent model is then defined as
</bodyText>
<equation confidence="0.856726">
E ¯wi[logσ(−e0T¯wiM˜ewI)],
</equation>
<bodyText confidence="0.999408">
where e0∗ are the context embeddings for the corre-
sponding words. Since ˜ewI is a real-valued vector
(instead of a 1-hot vector as in relation-dependent
models), M can no longer be updated one column
at a time. Instead, updates are defined as:
</bodyText>
<equation confidence="0.8640875">
= [1 − σ(e0T wOM˜ewI)]e0 wO ˜eT wI
[1 − σ(−e0T wiM˜ewI)]e0 wi ˜eT wI.
</equation>
<bodyText confidence="0.999309666666667">
Training is quite efficient in practice due to the low
dimensionality of M; convergence is achieved af-
ter very few epochs.1
Note that this model is different from the non-
factorized models that conflate multiple depen-
dency relations because the proposed model is a
</bodyText>
<footnote confidence="0.485514">
1We also experimented with updating the relation-
dependent embeddings together with M, but this worsened
evaluation performance.
</footnote>
<sectionHeader confidence="0.997502" genericHeader="method">
4 Evaluations
</sectionHeader>
<subsectionHeader confidence="0.999912">
4.1 Training Data and Baselines
</subsectionHeader>
<bodyText confidence="0.999978896551724">
The Annotated English Gigaword (Napoles et al.,
2012) is used as the main training corpus. It con-
tains 4 billion words from news articles, parsed by
the Stanford Parser. A random subset with 17 mil-
lion words is also used to study the effect of train-
ing data size (dubbed 17M).
Semantic relations are derived from the defini-
tion text in the Online Plain Text English Dictio-
nary2. There are approximately 806,000 definition
pairs, 33,000 distinct definienda and 24,000 dis-
tinct defining words. The entire corpus has 1.25
million words in a 7.1MB file.
Three baseline systems are used for compar-
ison, including one non-factorized dependency-
based model DEP (Levy and Goldberg, 2014a)
and two window-based embedding models w2v
(or word2vec, Mikolov et al. 2013a) and GloVe
(Pennington et al., 2014). Embedding dimension
is 50 for all models (baselines as well as the pro-
posed). Embeddings in the window-based mod-
els are obtained by running the published software
for each of these systems on the Gigaword corpus
with default values for all hyper-parameters except
for vector size (50) and minimum word frequency
(100 for the entire Gigaword corpus; 5 for the 17M
subset). For the w2v model, for example, we used
the skip-gram model with the default value 5 as
window size, negative sample size, and epoch size,
and 0.025 as initial learning rate.
</bodyText>
<subsectionHeader confidence="0.97737">
4.2 Lexical Similarity
Relation-Dependent Models
</subsectionHeader>
<bodyText confidence="0.9993051">
Table 1 shows the results on four similarity
datasets: MC (Miller and Charles, 1991), RG
(Rubenstein and Goodenough, 1965), FG (or
wordsim353, Finkelstein et al. 2001), and SL (or
SimLex, Hill et al. 2014b). The first three datasets
consist of nouns, while the last one also includes
verbs (SLv) and adjectives (SLa) in addition to
nouns (SLn). Semantically, FG contains many
related pairs (e.g., movie–popcorn), whereas the
other three datasets are purely similarity oriented.
</bodyText>
<footnote confidence="0.933927">
2http://www.mso.anu.edu.au/˜ralph/
OPTED/
</footnote>
<equation confidence="0.9987367">
logσ(e0TwIM˜ewI) +
k
∑
i=1
∂
∂M
−
k
∑
i=1
</equation>
<page confidence="0.99783">
460
</page>
<table confidence="0.998965111111111">
Model MC RG FG SLn SLv SLa
amod .766 .798 .572 .566 .154 .466
amod−1 .272 .296 .220 .218 .248 .602
nsubj .442 .350 .376 .388 .392 .464
nn .596 .620 .514 .486 .130 .068
Baselines
DEP .640 .670 .510 .400 .240 .350
w2v .656 .618 .600 .382 .237 .560
GloVe .609 .629 .546 .346 .142 .517
</table>
<tableCaption confidence="0.99955">
Table 1: Correlation between human judgement
</tableCaption>
<bodyText confidence="0.990869714285714">
and cosine similarity of embeddings (trained on
the Gigaword corpus) on six similarity datasets.
Performance is measured by Spearman’s ρ be-
tween system scores and human judgements of
similarity between the pairs that accompany each
dataset.
When dependency information is factorized
into individual relations, models using the best-
performing relation for each dataset3 out-perform
the baselines by large margins on 5 out of the 6
datasets. In comparison, the advantage of the syn-
tactic information is not at all obvious when they
are used in a non-factorized fashion in the DEP
model; it out-performs the window-based meth-
ods (below the dashed line) on only 3 datasets
with limited margins. However, the window-based
methods consistently outperform the dependency-
based methods on the FG dataset, confirming our
intuition that window-based methods are better at
capturing relatedness than similarity.
When dependency relations are factorized into
individual types, sparsity is a rather prominent is-
sue especially when the training corpus is small.
With sufficient training data, however, factorized
models consistently outperform all baselines by
very large margins on all but the FG dataset. Av-
erage correlation (weighted by the size of each
sub-dataset corresponding to the three POS’s) on
the SL dataset is 0.531, outperforming the best re-
ported result on the dataset (Hill et al., 2014a).
3We did not hold out validation data to choose the best-
performing relations for each dataset. Our assumption is that
the dominant part-of-speech of the words in each dataset is
the determining factor of the top-performing syntactic rela-
tion for that dataset. Consequently, the choice of this re-
lation should be relatively constant without having to rely
on traditional parameter tuning. For the four noun datasets,
for example, we observed that amod is consistently the top-
performing relation, and we subsequently assumed similar
consistency on the verb and the adjective datasets. The
same observations and rationales apply for the relation-
independent experiments.
</bodyText>
<table confidence="0.9981675">
Model MC RG FG SLn SLv SLa
Rel. Dep. #1 .512 .486 .380 .354 .222 .394
Rel. Dep. #2 .390 .380 .360 .304 .206 .236
Rel. Indep. .570 .550 .392 .360 .238 .338
Baselines
DEP .530 .558 .506 .346 .138 .412
w2v .563 .491 .562 .287 .065 .379
GloVe .306 .368 .308 .132 −.007 .254
</table>
<tableCaption confidence="0.75573325">
Table 2: Lexical similarity performance of
relation-independent models (trained on the 17M
corpus) combining top two best-performing rela-
tions for each POS.
</tableCaption>
<bodyText confidence="0.9998132">
Although the co-occurrence data is sparse, it
is nonetheless highly “focused” (Levy and Gold-
berg, 2014a) with much lower entropy. As a result,
convergence is much faster when compared to the
non-factorized models such as DEP, which takes
up to 10 times more iterations to converge.
Among the individual dependency relations, the
most effective relations for nouns, adjectives, and
verbs are amod, amod−1, and nsubj, respec-
tively. For nouns, we observed a notable gap in
performance between amod and nn. Data inspec-
tion reveals that a much higher proportion of nn
modifiers are proper nouns (64.0% compared to
about 0.01% in amod). The comparison suggests
that, as noun modifiers, amod describes the at-
tributes of nominal concepts while nn are more
often instantiations, which apparently is semanti-
cally less informative. On the other hand, nn is
the better choice if the goal is to train embeddings
for proper nouns.
</bodyText>
<subsectionHeader confidence="0.887875">
Relation-Independent Model
</subsectionHeader>
<bodyText confidence="0.995709588235294">
The relation-independent model (Section 3.3) is
implemented by combining the top two best-
performing relations for each POS: amod and
dobj−1 for noun pairs, nsubj and dobj for
verb pairs, and amod−1 and dobj−1 for adjective
pairs.
Lexical similarity results on the 17M corpus
are listed in Table 2. The combined results
improve over the best relation-dependent mod-
els for all categories except for SLa (adjectives),
where only the top-performing relation-dependent
model (amod−1) yielded statistically significant
results and thus, results are worsened by com-
bining the second-best relation-dependent source
dobj−1 (which is essentially noise). Compar-
ing to baselines, the relation-independent model
achieves better results in four out of the six cat-
</bodyText>
<page confidence="0.99812">
461
</page>
<table confidence="0.997471">
Model MC RG FG SLn SLv SLa
def .640 .626 .378 .332 .320 .306
def−1 .740 .626 .436 .366 .332 .376
Combined .754 .722 .530 .410 .356 .412
w2v .656 .618 .600 .382 .237 .560
</table>
<tableCaption confidence="0.882300333333333">
Table 3: Lexical similarity performance of mod-
els using dictionary definitions and compared to
word2vec trained on the Gigaword corpus.
</tableCaption>
<bodyText confidence="0.653817">
egories.
</bodyText>
<subsectionHeader confidence="0.869117">
Using Dictionary Definitions
</subsectionHeader>
<bodyText confidence="0.999948416666667">
Embeddings trained on dictionary definitions are
also evaluated on the similarity datasets, and
the results are shown in Table 3. The individ-
ual relations (defining and inverse) perform sur-
prisingly well on the datasets when compared
to word2vec. The relation-independent model
brings consistent improvement by combining the
relations, and the results compare favourably to
word2vec trained on the entire Gigaword cor-
pus. Similar to dependency relations, lexico-
graphic information is also better at capturing sim-
ilarity than relatedness, as suggested by the results.
</bodyText>
<sectionHeader confidence="0.996565" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999852">
This study explored the notion of relatedness in
embedding models by incorporating syntactic and
lexicographic knowledge. Compared to exist-
ing syntax-based embedding models, the proposed
embedding models benefits from factorizing syn-
tactic information by individual dependency rela-
tions. Empirically, syntactic information from in-
dividual dependency types brings about notable
improvement in model performance at a much
higher rate of convergence. Lexicographic knowl-
edge from monolingual dictionaries also helps im-
prove lexical embedding learning. Embeddings
trained on a compact, knowledge-intensive re-
source rival state-of-the-art models trained on free
texts thousands of times larger in size.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999259166666667">
We thank Gerald Penn, Ruslan Salakhutdinov,
Suzanne Stevenson, and Xiaodan Zhu for their in-
sightful comments, as well as the anonymous re-
viewers for their valuable feedback. This study is
financially supported by the Natural Sciences and
Engineering Research Council of Canada.
</bodyText>
<sectionHeader confidence="0.988789" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.656884255319149">
Hiyan Alshawi. Processing dictionary definitions
with phrasal pattern hierarchies. Computational
Linguistics, 13(3-4):195–202, 1987.
Robert Amsler. The structure of the Merriam-
Webster Pocket Dictionary. PhD thesis, The
University of Texas at Austin, 1980.
Mohit Bansal, Kevin Gimpel, and Karen Livescu.
Tailoring continuous word representations for
dependency parsing. In Proceedings of the An-
nual Meeting of the Association for Computa-
tional Linguistics, 2014.
Yoshua Bengio, Holger Schwenk, Jean-S´ebastien
Sen´ecal, Fr´ederic Morin, and Jean-Luc Gau-
vain. Neural probabilistic language models. In
Innovations in Machine Learning, pages 137–
186. Springer, 2003.
Danushka Bollegala, Takanori Maehara, Yuichi
Yoshida, and Ken-ichi Kawarabayashi. Learn-
ing word representations from relational graphs.
arXiv preprint arXiv:1412.2378, 2014.
Martin Chodorow, Roy Byrd, and George Hei-
dorn. Extracting semantic hierarchies from a
large on-line dictionary. In Proceedings of
the 23rd Annual Meeting of the Association
for Computational Linguistics, pages 299–304,
Chicago, Illinois, USA, 1985.
Ronan Collobert and Jason Weston. A unified
architecture for natural language processing:
Deep neural networks with multitask learning.
In Proceedings of the 25th International Con-
ference on Machine Learning, pages 160–167.
ACM, 2008.
Manaal Faruqui, Jesse Dodge, Sujay Kumar
Jauhar, Chris Dyer, Eduard Hovy, and Noah A.
Smith. Retrofitting word vectors to semantic
lexicons. In Proceedings of the 2015 Confer-
ence of the North American Chapter of the As-
sociation for Computational Linguistics: Hu-
man Language Technologies, pages 1606–1615,
Denver, Colorado, 2015. Association for Com-
putational Linguistics.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Ma-
tias, Ehud Rivlin, Zach Solan, Gadi Wolfman,
and Eytan Ruppin. Placing search in context:
The concept revisited. In Proceedings of the
10th International Conference on World Wide
Web, pages 406–414. ACM, 2001.
</reference>
<page confidence="0.997349">
462
</page>
<reference confidence="0.999806164705882">
Zellig Harris. Distributional structure. Word, 10
(23):146–162, 1954.
Felix Hill, Kyunghyun Cho, Sebastien Jean, Co-
line Devin, and Yoshua Bengio. Embedding
word similarity with neural machine translation.
arXiv preprint arXiv:1412.6448, 2014a.
Felix Hill, Roi Reichart, and Anna Korhonen.
Simlex-999: Evaluating semantic models with
(genuine) similarity estimation. arXiv preprint
arXiv:1408.3456, 2014b.
Eric Huang, Richard Socher, Christopher D Man-
ning, and Andrew Ng. Improving word repre-
sentations via global context and multiple word
prototypes. In Proceedings of the 50th Annual
Meeting of the Association for Computational
Linguistics: Long Papers-Volume 1, pages 873–
882. Association for Computational Linguis-
tics, 2012.
Omer Levy and Yoav Goldberg. Dependency-
based word embeddings. In Proceedings of
the 52nd Annual Meeting of the Association for
Computational Linguistics, volume 2, 2014a.
Omer Levy and Yoav Goldberg. Neural word em-
bedding as implicit matrix factorization. In Ad-
vances in Neural Information Processing Sys-
tems, pages 2177–2185, 2014b.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. Efficient estimation of word repre-
sentations in vector space. In Proceedings of
the International Conference on Learning Rep-
resentations, 2013a.
Tomas Mikolov, Wen-tau Yih, and Geoffrey
Zweig. Linguistic regularities in continuous
space word representations. In Proceedings of
Human Language Technologies: The 2013 An-
nual Conference of the North American Chapter
of the Association for Computational Linguis-
tics, pages 746–751, 2013b.
George Miller and Walter Charles. Contextual cor-
relates of semantic similarity. Language and
Cognitive Processes, 6(1):1–28, 1991.
Andriy Mnih and Geoffrey Hinton. Three new
graphical models for statistical language mod-
elling. In Proceedings of the 24th International
Conference on Machine Learning, pages 641–
648. ACM, 2007.
Andriy Mnih and Geoffrey E Hinton. A scalable
hierarchical distributed language model. In Ad-
vances in Neural Information Processing Sys-
tems, pages 1081–1088, 2009.
Courtney Napoles, Matthew Gormley, and Ben-
jamin Van Durme. Annotated Gigaword. In
Proceedings of the Joint Workshop on Auto-
matic Knowledge Base Construction and Web-
scale Knowledge Extraction, pages 95–100. As-
sociation for Computational Linguistics, 2012.
Jeffrey Pennington, Richard Socher, and Christo-
pher D Manning. GloVe: Global vectors for
word representation. In Proceedings of the Con-
ference on Empirical Methods in Natural Lan-
guage Processing, 2014.
Richard Reichert, John Olney, and James Paris.
Two Dictionary Transcripts and Programs for
Processing Them – The Encoding Scheme,
Parsent and Conix., volume 1. DTIC Research
Report AD0691098, 1969.
Herbert Rubenstein and John Goodenough. Con-
textual correlates of synonymy. Communica-
tions of the ACM, 8(10):627–633, 1965.
Joseph Turian, Lev Ratinov, and Yoshua Ben-
gio. Word representations: a simple and general
method for semi-supervised learning. In Pro-
ceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages
384–394, Uppsala, Sweden, July 2010.
Tong Wang and Graeme Hirst. Exploring pat-
terns in dictionary definitions for synonym ex-
traction. Natural Language Engineering, 17,
2011.
Yinggong Zhao, Shujian Huang, Xinyu Dai, Jian-
bing Zhang, and Jiajun Chen. Learning word
embeddings from dependency relations. In Pro-
ceedings of 2014 International Conference on
Asian Language Processing (IALP), pages 123–
127. IEEE, 2014.
</reference>
<page confidence="0.999714">
463
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9746875">Learning Lexical Embeddings with and Lexicographic Knowledge</title>
<author confidence="0.999869">Tong Wang Abdel-rahman Mohamed Graeme Hirst</author>
<affiliation confidence="0.99971">University of Toronto Microsoft Research University of</affiliation>
<email confidence="0.989755">tong@cs.toronto.eduasamir@microsoft.comgh@cs.toronto.edu</email>
<abstract confidence="0.99557617721519">We propose two improvements on lexical association used in embedding learning: factorizing individual dependency relations and using lexicographic knowledge from monolingual dictionaries. Both proposals provide low-entropy lexical cooccurrence information, and are empirically shown to improve embedding learning by performing notably better than several popular embedding models in similarity tasks. 1 Lexical Embeddings and Relatedness Lexical embeddings are essentially real-valued distributed representations of words. As a vectorspace model, an embedding model approximates semantic relatedness with the Euclidean distance between embeddings, the result of which helps better estimate the real lexical distribution in various NLP tasks. In recent years, researchers have developed efficient and effective algorithms for learning embeddings (Mikolov et al., 2013a; Pennington et al., 2014) and extended model applications from language modelling to various areas in NLP including lexical semantics (Mikolov et al., 2013b) and parsing (Bansal et al., 2014). To approximate semantic relatedness with geometric distance, objective functions are usually chosen to correlate positively with the Euclidean similarity between the embeddings of related words. Maximizing such an objective function is then equivalent to adjusting the embeddings so that those of the related words will be geometrically closer. The definition of relatedness among words can have a profound influence on the quality of the resulting embedding models. In most existing studies, relatedness is defined by co-occurrence within a window frame sliding over texts. Alsupported by the hypothe- 1954), this definition suffers from two major limitations. Firstly, the window frame size is usually rather small (for efficiency and sparsity considerations), which increases the false negative rate by missing long-distance dependencies. Secondly, a window frame can (and often does) span across different constituents in a sentence, resulting in an increased false positive rate by associating unrelated words. The problem is worsened as the size of the window increases since each will appear in two subsumfalse-positive Several existing studies have addressed these limitations of window-based contexts. Nonetheless, we hypothesize that lexical embedding learning can further benefit from (1) factorizing syntactic relations into individual relations for structured syntactic information and (2) defining relatedness using lexicographic knowledge. We will show that implementation of these ideas brings notable improvement in lexical similarity tasks. 2 Related Work Lexical embeddings have traditionally been used in language modelling as distributed representations of words (Bengio et al., 2003; Mnih and Hinton, 2009) and have only recently been used in other NLP tasks. Turian et al. (2010), for example, used embeddings from existing language models (Collobert and Weston, 2008; Mnih and Hinton, 2007) as unsupervised lexical features to improve named entity recognition and chunking. Embedding models gained further popularity thanks to simplicity and effectiveness of the model (Mikolov et al., 2013a), which implicitly the mutual information matrix shifted by biases consisting of marginal counts of individual words (Levy and Goldberg, 2014b). Efficiency is greatly improved by approximating the computationally costly softmax function with</abstract>
<note confidence="0.9273465">458 Proceedings of the 53rd Annual Meeting of the Association for Computational the 7th International Joint Conference on Natural Language Processing (Short pages China, July 26-31, 2015. Association for Computational Linguistics negative sampling (similar to that of Collobert and Weston 2008) or hierarchical softmax (similar to that of Mnih and Hinton 2007). To address the limitation of contextual locality</note>
<abstract confidence="0.978419400576369">many language models (including Huang et al. (2012) added a “global context score” the local score (Collobert and Weston, 2008). The concatenation of word vectors and a “document vector” (centroid of the composing vectors weighted by was used as model input. Pennington et al. (2014) proposed to explicitly factorize the global co-occurrence matrix between words, and the resulting log bilinear model achieved state-of-the-art performance in lexical similarity, analogy, and named entity recognition. Several later studies addressed the limitations of window-based co-occurrence by extending the to predict words that are synto target words. Levy and Gold- (2014a) used syntactically related words nonsyntactic context. Bansal et al. (2014) used a training corpus consisting of sequences of labels following certain manually compiled patterns. Zhao et al. (2014) employed coarse-grained classifications of contexts according to the hierarchical structures in a parse tree. Semantic relations have also been explored as a form of lexical association. Faruqui et al. (2015) proposed to retrofit pre-trained embeddings (derived using window-based contexts) to semantic lexicons. The goal is to derive a set of embeddings to capture relatedness suggested by semantic lexicons while maintaining their resemblance to the corresponding window-based embeddings. Bollegala et al. (2014) trained an embedding model with lexical, part-of-speech, and dependency patterns extracted from sentences containing frequently cooccurring word pairs. Each relation was represented by a pattern representation matrix, which was combined and updated together with the word representation matrix (i.e., lexical embeddings) in a bilinear objective function. 3 The Proposed Models 3.1 Factorizing Dependency Relations One strong limitation of the existing dependencybased models is that no distinctions are made among the many different types of dependency relations. This is essentially a compromise to avoid issues in model complexity and data sparsity, and it precludes the possibility of studying individual or interactive effects of individual dependency relations on embedding learning. we propose a predict dependents given a governor unrelations. For example, a nominal governor the an embedding model will be trained to assign higher probability to obadjectival dependents (e.g., etc.) to rarely or never observed ones (e.g., puretc.). If a model is able to accurately make such predictions, it can then be said to “unthe meaning of possessing semantic knowledge about its certain attributes. By extension, similar models can be trained to learn the meaning of the governors in other dependency relations (e.g., adjectival governors in the inverse etc.). The basic model uses an objective function similar to that of Mikolov et al. (2013a): g the target and the output embeddings for the corresponding words, respecand the sigmoid function. The subwhether an embedding correspond to the governor or the dependent of a depair, and to random samples from the dependent vocabulary (drawn by unigram frequency). 3.2 Incorporating Lexicographic Knowledge Semantic information used in existing studies (Section 2) either relies on specialized lexical resources with limited availability or is obtained from complex procedures that are difficult to replicate. To address these issues, we propose to use monolingual dictionaries as a simple yet effective source of semantic knowledge. The defining relation has been demonstrated to have good performance in various semantic tasks (Chodorow et al., 1985; Alshawi, 1987). The inverse of the defining (also known as the Concordance In- Reichert et al. 1969) has also been proven useful in building lexicographic taxonomies (Amsler, 1980) and identifying synonyms (Wang and Hirst, 2011). Therefore, we use both the defining relation and its inverse as sources of semantic association in the proposed embedding models. Lexicographic knowledge is represented by adopting the same terminology used in syntactic + k ∑ 459 dependencies: definienda as governors and definias dependents. For example, related a governor under or a dependent under deeper structure with pre-training on the factorized results (via the relation-dependent models) in the first layer. 3.3 Combining Individual Knowledge Sources Sparsity is a prominent issue in the relationdependent models since each individual relation only receives a limited share of the overall cooccurrence information. We also propose a postthat combines the individual knowledge sources. The input of the model is the structured knowledge from relationmodels, for example, that be or it can etc. training objective is to predict the the relation-dependent embeddings, with the intuition that if a model is trained to be able to “solve the riddle” and predict that this an then the model is said to possess generic, relation-independent knowledge about the target word by learning from the relation-dependent knowledge sources. input word its relation-independent embedding is derived by applying a linear model the concatenation of its relation-dependent The objective function of a relation-independent model is then defined as are the context embeddings for the correwords. Since is a real-valued vector (instead of a 1-hot vector as in relation-dependent no longer be updated one column at a time. Instead, updates are defined as: Training is quite efficient in practice due to the low of convergence is achieved afvery few Note that this model is different from the nonfactorized models that conflate multiple dependency relations because the proposed model is a also experimented with updating the relationembeddings together with but this worsened evaluation performance. 4 Evaluations 4.1 Training Data and Baselines English Gigaword et al., 2012) is used as the main training corpus. It contains 4 billion words from news articles, parsed by the Stanford Parser. A random subset with 17 million words is also used to study the effect of traindata size (dubbed Semantic relations are derived from the definitext in the Plain Text English Dictio- There are approximately 806,000 definition pairs, 33,000 distinct definienda and 24,000 distinct defining words. The entire corpus has 1.25 million words in a 7.1MB file. Three baseline systems are used for comparison, including one non-factorized dependencymodel and Goldberg, 2014a) two window-based embedding models Mikolov et al. 2013a) and (Pennington et al., 2014). Embedding dimension is 50 for all models (baselines as well as the proposed). Embeddings in the window-based models are obtained by running the published software for each of these systems on the Gigaword corpus with default values for all hyper-parameters except for vector size (50) and minimum word frequency for the entire Gigaword corpus; 5 for the For the for example, we used the skip-gram model with the default value 5 as window size, negative sample size, and epoch size, and 0.025 as initial learning rate. 4.2 Lexical Similarity Relation-Dependent Models Table 1 shows the results on four similarity and Charles, 1991), and Goodenough, 1965), Finkelstein et al. 2001), and Hill et al. 2014b). The first three datasets consist of nouns, while the last one also includes and adjectives in addition to Semantically, many pairs (e.g., whereas the other three datasets are purely similarity oriented. OPTED/ + k ∑ ∂ − k ∑ 460 Model MC RG FG amod .572 .154 .466 .272 .296 .220 .218 .248 nsubj .442 .350 .376 .388 .464 nn .596 .620 .514 .486 .130 .068 Baselines DEP .640 .670 .510 .400 .240 .350 w2v .656 .618 .382 .237 .560 GloVe .609 .629 .546 .346 .142 .517 Table 1: Correlation between human judgement of embeddings (trained on the Gigaword corpus) on six similarity datasets. is measured by between system scores and human judgements of similarity between the pairs that accompany each dataset. When dependency information is factorized into individual relations, models using the bestrelation for each out-perform the baselines by large margins on 5 out of the 6 datasets. In comparison, the advantage of the syntactic information is not at all obvious when they used in a non-factorized fashion in the model; it out-performs the window-based methods (below the dashed line) on only 3 datasets with limited margins. However, the window-based methods consistently outperform the dependencymethods on the confirming our intuition that window-based methods are better at capturing relatedness than similarity. When dependency relations are factorized into individual types, sparsity is a rather prominent issue especially when the training corpus is small. With sufficient training data, however, factorized models consistently outperform all baselines by large margins on all but the Average correlation (weighted by the size of each sub-dataset corresponding to the three POS’s) on is 0.531, outperforming the best reported result on the dataset (Hill et al., 2014a). did not hold out validation data to choose the bestperforming relations for each dataset. Our assumption is that the dominant part-of-speech of the words in each dataset is the determining factor of the top-performing syntactic relation for that dataset. Consequently, the choice of this relation should be relatively constant without having to rely on traditional parameter tuning. For the four noun datasets, example, we observed that consistently the topperforming relation, and we subsequently assumed similar consistency on the verb and the adjective datasets. The same observations and rationales apply for the relationindependent experiments. Model MC RG FG Rel. Dep. #1 .512 .486 .380 .354 .222 .394 Rel. Dep. #2 .390 .380 .360 .304 .206 .236 Rel. Indep. .550 .392 .338 Baselines DEP .530 .346 .138 .412 w2v .563 .287 .065 .379 GloVe .306 .368 .308 .132 .254 Table 2: Lexical similarity performance of models (trained on the corpus) combining top two best-performing relations for each POS. Although the co-occurrence data is sparse, it is nonetheless highly “focused” (Levy and Goldberg, 2014a) with much lower entropy. As a result, convergence is much faster when compared to the models such as which takes up to 10 times more iterations to converge. Among the individual dependency relations, the most effective relations for nouns, adjectives, and are and respectively. For nouns, we observed a notable gap in between Data inspecreveals that a much higher proportion of modifiers are proper nouns (64.0% compared to 0.01% in The comparison suggests as noun modifiers, the atof nominal concepts while more often instantiations, which apparently is semantiless informative. On the other hand, the better choice if the goal is to train embeddings for proper nouns. Relation-Independent Model The relation-independent model (Section 3.3) is implemented by combining the top two bestrelations for each POS: for noun pairs, pairs, and and for adjective pairs. similarity results on the are listed in Table 2. The combined results improve over the best relation-dependent modfor all categories except for (adjectives), where only the top-performing relation-dependent yielded statistically significant results and thus, results are worsened by combining the second-best relation-dependent source (which is essentially noise). Comparing to baselines, the relation-independent model better results in four out of the six cat- 461 Model MC RG FG def .640 .626 .378 .332 .320 .306 .740 .626 .436 .366 .332 .376 Combined .530 .412 w2v .656 .618 .382 .237 Table 3: Lexical similarity performance of models using dictionary definitions and compared to on the Gigaword corpus. egories. Using Dictionary Definitions Embeddings trained on dictionary definitions are also evaluated on the similarity datasets, and the results are shown in Table 3. The individual relations (defining and inverse) perform surprisingly well on the datasets when compared The relation-independent model brings consistent improvement by combining the relations, and the results compare favourably to on the entire Gigaword corpus. Similar to dependency relations, lexicographic information is also better at capturing similarity than relatedness, as suggested by the results. 5 Conclusions This study explored the notion of relatedness in embedding models by incorporating syntactic and lexicographic knowledge. Compared to existing syntax-based embedding models, the proposed embedding models benefits from factorizing syntactic information by individual dependency relations. Empirically, syntactic information from individual dependency types brings about notable improvement in model performance at a much higher rate of convergence. Lexicographic knowledge from monolingual dictionaries also helps improve lexical embedding learning. Embeddings trained on a compact, knowledge-intensive resource rival state-of-the-art models trained on free texts thousands of times larger in size. Acknowledgments We thank Gerald Penn, Ruslan Salakhutdinov, Suzanne Stevenson, and Xiaodan Zhu for their insightful comments, as well as the anonymous reviewers for their valuable feedback. This study is financially supported by the Natural Sciences and Engineering Research Council of Canada. References Hiyan Alshawi. Processing dictionary definitions phrasal pattern hierarchies.</abstract>
<note confidence="0.785888487179487">13(3-4):195–202, 1987. Amsler. structure of the Merriam- Pocket PhD thesis, The University of Texas at Austin, 1980. Mohit Bansal, Kevin Gimpel, and Karen Livescu. Tailoring continuous word representations for parsing. In of the Annual Meeting of the Association for Computa- 2014. Yoshua Bengio, Holger Schwenk, Jean-S´ebastien Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain. Neural probabilistic language models. In in Machine pages 137– 186. Springer, 2003. Danushka Bollegala, Takanori Maehara, Yuichi Yoshida, and Ken-ichi Kawarabayashi. Learning word representations from relational graphs. preprint 2014. Martin Chodorow, Roy Byrd, and George Heidorn. Extracting semantic hierarchies from a on-line dictionary. In of the 23rd Annual Meeting of the Association Computational pages 299–304, Chicago, Illinois, USA, 1985. Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. of the 25th International Conon Machine pages 160–167. ACM, 2008. Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar, Chris Dyer, Eduard Hovy, and Noah A. Smith. Retrofitting word vectors to semantic In of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- Language pages 1606–1615, Denver, Colorado, 2015. Association for Computational Linguistics.</note>
<author confidence="0.686888">Lev Finkelstein</author>
<author confidence="0.686888">Evgeniy Gabrilovich</author>
<author confidence="0.686888">Yossi Ma-</author>
<affiliation confidence="0.40645">tias, Ehud Rivlin, Zach Solan, Gadi Wolfman,</affiliation>
<abstract confidence="0.757141333333333">and Eytan Ruppin. Placing search in context: concept revisited. In of the 10th International Conference on World Wide pages 406–414. ACM, 2001. 462 Harris. Distributional structure. 10 (23):146–162, 1954. Felix Hill, Kyunghyun Cho, Sebastien Jean, Coline Devin, and Yoshua Bengio. Embedding word similarity with neural machine translation. preprint 2014a. Felix Hill, Roi Reichart, and Anna Korhonen. Simlex-999: Evaluating semantic models with similarity estimation. preprint 2014b. Eric Huang, Richard Socher, Christopher D Manning, and Andrew Ng. Improving word representations via global context and multiple word In of the 50th Annual Meeting of the Association for Computational Long Papers-Volume pages 873– 882. Association for Computational Linguistics, 2012. Omer Levy and Yoav Goldberg. Dependencyword embeddings. In of the 52nd Annual Meeting of the Association for volume 2, 2014a. Omer Levy and Yoav Goldberg. Neural word emas implicit matrix factorization. In Advances in Neural Information Processing Syspages 2177–2185, 2014b. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word reprein vector space. In of the International Conference on Learning Rep- 2013a. Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous word representations. In of Human Language Technologies: The 2013 Annual Conference of the North American Chapter of the Association for Computational Linguispages 746–751, 2013b. George Miller and Walter Charles. Contextual corof semantic similarity. and 6(1):1–28, 1991. Andriy Mnih and Geoffrey Hinton. Three new graphical models for statistical language mod-</abstract>
<note confidence="0.876426210526316">In of the 24th International on Machine pages 641– 648. ACM, 2007. Andriy Mnih and Geoffrey E Hinton. A scalable distributed language model. In Advances in Neural Information Processing Syspages 1081–1088, 2009. Courtney Napoles, Matthew Gormley, and Benjamin Van Durme. Annotated Gigaword. In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web- Knowledge pages 95–100. Association for Computational Linguistics, 2012. Jeffrey Pennington, Richard Socher, and Christopher D Manning. GloVe: Global vectors for representation. In of the Conference on Empirical Methods in Natural Lan- 2014. Richard Reichert, John Olney, and James Paris.</note>
<title confidence="0.7015965">Two Dictionary Transcripts and Programs for Processing Them – The Encoding Scheme,</title>
<note confidence="0.748884863636364">and volume 1. DTIC Research Report AD0691098, 1969. Herbert Rubenstein and John Goodenough. Concorrelates of synonymy. Communicaof the 8(10):627–633, 1965. Joseph Turian, Lev Ratinov, and Yoshua Bengio. Word representations: a simple and general for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Asfor Computational pages 384–394, Uppsala, Sweden, July 2010. Tong Wang and Graeme Hirst. Exploring patterns in dictionary definitions for synonym ex- Language 17, 2011. Yinggong Zhao, Shujian Huang, Xinyu Dai, Jianbing Zhang, and Jiajun Chen. Learning word from dependency relations. In Proceedings of 2014 International Conference on Language Processing pages 123– 127. IEEE, 2014. 463</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hiyan Alshawi</author>
</authors>
<title>Processing dictionary definitions with phrasal pattern hierarchies.</title>
<date>1987</date>
<journal>Computational Linguistics,</journal>
<pages>13--3</pages>
<contexts>
<context position="8219" citStr="Alshawi, 1987" startWordPosition="1215" endWordPosition="1216">pendency pair, and ˆd* correspond to random samples from the dependent vocabulary (drawn by unigram frequency). 3.2 Incorporating Lexicographic Knowledge Semantic information used in existing studies (Section 2) either relies on specialized lexical resources with limited availability or is obtained from complex procedures that are difficult to replicate. To address these issues, we propose to use monolingual dictionaries as a simple yet effective source of semantic knowledge. The defining relation has been demonstrated to have good performance in various semantic tasks (Chodorow et al., 1985; Alshawi, 1987). The inverse of the defining relation (also known as the Olney Concordance Index, Reichert et al. 1969) has also been proven useful in building lexicographic taxonomies (Amsler, 1980) and identifying synonyms (Wang and Hirst, 2011). Therefore, we use both the defining relation and its inverse as sources of semantic association in the proposed embedding models. Lexicographic knowledge is represented by adopting the same terminology used in syntactic log6(eTg e�d) + k ∑ i=1 459 dependencies: definienda as governors and definientia as dependents. For example, apple is related to fruit and rosace</context>
</contexts>
<marker>Alshawi, 1987</marker>
<rawString>Hiyan Alshawi. Processing dictionary definitions with phrasal pattern hierarchies. Computational Linguistics, 13(3-4):195–202, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Amsler</author>
</authors>
<title>The structure of the MerriamWebster Pocket Dictionary.</title>
<date>1980</date>
<tech>PhD thesis,</tech>
<institution>The University of Texas at Austin,</institution>
<contexts>
<context position="8403" citStr="Amsler, 1980" startWordPosition="1245" endWordPosition="1246">isting studies (Section 2) either relies on specialized lexical resources with limited availability or is obtained from complex procedures that are difficult to replicate. To address these issues, we propose to use monolingual dictionaries as a simple yet effective source of semantic knowledge. The defining relation has been demonstrated to have good performance in various semantic tasks (Chodorow et al., 1985; Alshawi, 1987). The inverse of the defining relation (also known as the Olney Concordance Index, Reichert et al. 1969) has also been proven useful in building lexicographic taxonomies (Amsler, 1980) and identifying synonyms (Wang and Hirst, 2011). Therefore, we use both the defining relation and its inverse as sources of semantic association in the proposed embedding models. Lexicographic knowledge is represented by adopting the same terminology used in syntactic log6(eTg e�d) + k ∑ i=1 459 dependencies: definienda as governors and definientia as dependents. For example, apple is related to fruit and rosaceous as a governor under def, or to cider and pippin as a dependent under def−1. deeper structure with pre-training on the factorized results (via the relation-dependent models) in the </context>
</contexts>
<marker>Amsler, 1980</marker>
<rawString>Robert Amsler. The structure of the MerriamWebster Pocket Dictionary. PhD thesis, The University of Texas at Austin, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Karen Livescu</author>
</authors>
<title>Tailoring continuous word representations for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="1305" citStr="Bansal et al., 2014" startWordPosition="175" endWordPosition="178">exical embeddings are essentially real-valued distributed representations of words. As a vectorspace model, an embedding model approximates semantic relatedness with the Euclidean distance between embeddings, the result of which helps better estimate the real lexical distribution in various NLP tasks. In recent years, researchers have developed efficient and effective algorithms for learning embeddings (Mikolov et al., 2013a; Pennington et al., 2014) and extended model applications from language modelling to various areas in NLP including lexical semantics (Mikolov et al., 2013b) and parsing (Bansal et al., 2014). To approximate semantic relatedness with geometric distance, objective functions are usually chosen to correlate positively with the Euclidean similarity between the embeddings of related words. Maximizing such an objective function is then equivalent to adjusting the embeddings so that those of the related words will be geometrically closer. The definition of relatedness among words can have a profound influence on the quality of the resulting embedding models. In most existing studies, relatedness is defined by co-occurrence within a window frame sliding over texts. Although supported by t</context>
<context position="5066" citStr="Bansal et al. (2014)" startWordPosition="728" endWordPosition="731">(centroid of the composing word vectors weighted by idf) was used as model input. Pennington et al. (2014) proposed to explicitly factorize the global co-occurrence matrix between words, and the resulting log bilinear model achieved state-of-the-art performance in lexical similarity, analogy, and named entity recognition. Several later studies addressed the limitations of window-based co-occurrence by extending the word2vec model to predict words that are syntactically related to target words. Levy and Goldberg (2014a) used syntactically related words nondiscriminatively as syntactic context. Bansal et al. (2014) used a training corpus consisting of sequences of labels following certain manually compiled patterns. Zhao et al. (2014) employed coarse-grained classifications of contexts according to the hierarchical structures in a parse tree. Semantic relations have also been explored as a form of lexical association. Faruqui et al. (2015) proposed to retrofit pre-trained embeddings (derived using window-based contexts) to semantic lexicons. The goal is to derive a set of embeddings to capture relatedness suggested by semantic lexicons while maintaining their resemblance to the corresponding window-base</context>
</contexts>
<marker>Bansal, Gimpel, Livescu, 2014</marker>
<rawString>Mohit Bansal, Kevin Gimpel, and Karen Livescu. Tailoring continuous word representations for dependency parsing. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Holger Schwenk</author>
<author>Jean-S´ebastien Sen´ecal</author>
<author>Fr´ederic Morin</author>
<author>Jean-Luc Gauvain</author>
</authors>
<title>Neural probabilistic language models.</title>
<date>2003</date>
<booktitle>In Innovations in Machine Learning,</booktitle>
<pages>137--186</pages>
<publisher>Springer,</publisher>
<marker>Bengio, Schwenk, Sen´ecal, Morin, Gauvain, 2003</marker>
<rawString>Yoshua Bengio, Holger Schwenk, Jean-S´ebastien Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain. Neural probabilistic language models. In Innovations in Machine Learning, pages 137– 186. Springer, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danushka Bollegala</author>
<author>Takanori Maehara</author>
<author>Yuichi Yoshida</author>
<author>Ken-ichi Kawarabayashi</author>
</authors>
<title>Learning word representations from relational graphs. arXiv preprint arXiv:1412.2378,</title>
<date>2014</date>
<contexts>
<context position="5703" citStr="Bollegala et al. (2014)" startWordPosition="822" endWordPosition="826">ng corpus consisting of sequences of labels following certain manually compiled patterns. Zhao et al. (2014) employed coarse-grained classifications of contexts according to the hierarchical structures in a parse tree. Semantic relations have also been explored as a form of lexical association. Faruqui et al. (2015) proposed to retrofit pre-trained embeddings (derived using window-based contexts) to semantic lexicons. The goal is to derive a set of embeddings to capture relatedness suggested by semantic lexicons while maintaining their resemblance to the corresponding window-based embeddings. Bollegala et al. (2014) trained an embedding model with lexical, part-of-speech, and dependency patterns extracted from sentences containing frequently cooccurring word pairs. Each relation was represented by a pattern representation matrix, which was combined and updated together with the word representation matrix (i.e., lexical embeddings) in a bilinear objective function. 3 The Proposed Models 3.1 Factorizing Dependency Relations One strong limitation of the existing dependencybased models is that no distinctions are made among the many different types of dependency relations. This is essentially a compromise to</context>
</contexts>
<marker>Bollegala, Maehara, Yoshida, Kawarabayashi, 2014</marker>
<rawString>Danushka Bollegala, Takanori Maehara, Yuichi Yoshida, and Ken-ichi Kawarabayashi. Learning word representations from relational graphs. arXiv preprint arXiv:1412.2378, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Chodorow</author>
<author>Roy Byrd</author>
<author>George Heidorn</author>
</authors>
<title>Extracting semantic hierarchies from a large on-line dictionary.</title>
<date>1985</date>
<booktitle>In Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>299--304</pages>
<location>Chicago, Illinois, USA,</location>
<contexts>
<context position="8203" citStr="Chodorow et al., 1985" startWordPosition="1211" endWordPosition="1214">r the dependent of a dependency pair, and ˆd* correspond to random samples from the dependent vocabulary (drawn by unigram frequency). 3.2 Incorporating Lexicographic Knowledge Semantic information used in existing studies (Section 2) either relies on specialized lexical resources with limited availability or is obtained from complex procedures that are difficult to replicate. To address these issues, we propose to use monolingual dictionaries as a simple yet effective source of semantic knowledge. The defining relation has been demonstrated to have good performance in various semantic tasks (Chodorow et al., 1985; Alshawi, 1987). The inverse of the defining relation (also known as the Olney Concordance Index, Reichert et al. 1969) has also been proven useful in building lexicographic taxonomies (Amsler, 1980) and identifying synonyms (Wang and Hirst, 2011). Therefore, we use both the defining relation and its inverse as sources of semantic association in the proposed embedding models. Lexicographic knowledge is represented by adopting the same terminology used in syntactic log6(eTg e�d) + k ∑ i=1 459 dependencies: definienda as governors and definientia as dependents. For example, apple is related to </context>
</contexts>
<marker>Chodorow, Byrd, Heidorn, 1985</marker>
<rawString>Martin Chodorow, Roy Byrd, and George Heidorn. Extracting semantic hierarchies from a large on-line dictionary. In Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics, pages 299–304, Chicago, Illinois, USA, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning,</booktitle>
<pages>160--167</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="3271" citStr="Collobert and Weston, 2008" startWordPosition="470" endWordPosition="473">arning can further benefit from (1) factorizing syntactic relations into individual relations for structured syntactic information and (2) defining relatedness using lexicographic knowledge. We will show that implementation of these ideas brings notable improvement in lexical similarity tasks. 2 Related Work Lexical embeddings have traditionally been used in language modelling as distributed representations of words (Bengio et al., 2003; Mnih and Hinton, 2009) and have only recently been used in other NLP tasks. Turian et al. (2010), for example, used embeddings from existing language models (Collobert and Weston, 2008; Mnih and Hinton, 2007) as unsupervised lexical features to improve named entity recognition and chunking. Embedding models gained further popularity thanks to the simplicity and effectiveness of the word2vec model (Mikolov et al., 2013a), which implicitly factorizes the point-wise mutual information matrix shifted by biases consisting of marginal counts of individual words (Levy and Goldberg, 2014b). Efficiency is greatly improved by approximating the computationally costly softmax function with 458 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and t</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine Learning, pages 160–167. ACM, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manaal Faruqui</author>
<author>Jesse Dodge</author>
<author>Sujay Kumar Jauhar</author>
<author>Chris Dyer</author>
<author>Eduard Hovy</author>
<author>Noah A Smith</author>
</authors>
<title>Retrofitting word vectors to semantic lexicons.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1606--1615</pages>
<location>Denver, Colorado,</location>
<contexts>
<context position="5397" citStr="Faruqui et al. (2015)" startWordPosition="779" endWordPosition="782">l later studies addressed the limitations of window-based co-occurrence by extending the word2vec model to predict words that are syntactically related to target words. Levy and Goldberg (2014a) used syntactically related words nondiscriminatively as syntactic context. Bansal et al. (2014) used a training corpus consisting of sequences of labels following certain manually compiled patterns. Zhao et al. (2014) employed coarse-grained classifications of contexts according to the hierarchical structures in a parse tree. Semantic relations have also been explored as a form of lexical association. Faruqui et al. (2015) proposed to retrofit pre-trained embeddings (derived using window-based contexts) to semantic lexicons. The goal is to derive a set of embeddings to capture relatedness suggested by semantic lexicons while maintaining their resemblance to the corresponding window-based embeddings. Bollegala et al. (2014) trained an embedding model with lexical, part-of-speech, and dependency patterns extracted from sentences containing frequently cooccurring word pairs. Each relation was represented by a pattern representation matrix, which was combined and updated together with the word representation matrix</context>
</contexts>
<marker>Faruqui, Dodge, Jauhar, Dyer, Hovy, Smith, 2015</marker>
<rawString>Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar, Chris Dyer, Eduard Hovy, and Noah A. Smith. Retrofitting word vectors to semantic lexicons. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1606–1615, Denver, Colorado, 2015. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2001</date>
<booktitle>In Proceedings of the 10th International Conference on World Wide Web,</booktitle>
<pages>406--414</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="12434" citStr="Finkelstein et al. 2001" startWordPosition="1889" endWordPosition="1892"> the published software for each of these systems on the Gigaword corpus with default values for all hyper-parameters except for vector size (50) and minimum word frequency (100 for the entire Gigaword corpus; 5 for the 17M subset). For the w2v model, for example, we used the skip-gram model with the default value 5 as window size, negative sample size, and epoch size, and 0.025 as initial learning rate. 4.2 Lexical Similarity Relation-Dependent Models Table 1 shows the results on four similarity datasets: MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), FG (or wordsim353, Finkelstein et al. 2001), and SL (or SimLex, Hill et al. 2014b). The first three datasets consist of nouns, while the last one also includes verbs (SLv) and adjectives (SLa) in addition to nouns (SLn). Semantically, FG contains many related pairs (e.g., movie–popcorn), whereas the other three datasets are purely similarity oriented. 2http://www.mso.anu.edu.au/˜ralph/ OPTED/ logσ(e0TwIM˜ewI) + k ∑ i=1 ∂ ∂M − k ∑ i=1 460 Model MC RG FG SLn SLv SLa amod .766 .798 .572 .566 .154 .466 amod−1 .272 .296 .220 .218 .248 .602 nsubj .442 .350 .376 .388 .392 .464 nn .596 .620 .514 .486 .130 .068 Baselines DEP .640 .670 .510 .400</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2001</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. Placing search in context: The concept revisited. In Proceedings of the 10th International Conference on World Wide Web, pages 406–414. ACM, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<title>Distributional structure.</title>
<date>1954</date>
<journal>Word,</journal>
<volume>10</volume>
<pages>23--146</pages>
<contexts>
<context position="1948" citStr="Harris, 1954" startWordPosition="276" endWordPosition="277">tedness with geometric distance, objective functions are usually chosen to correlate positively with the Euclidean similarity between the embeddings of related words. Maximizing such an objective function is then equivalent to adjusting the embeddings so that those of the related words will be geometrically closer. The definition of relatedness among words can have a profound influence on the quality of the resulting embedding models. In most existing studies, relatedness is defined by co-occurrence within a window frame sliding over texts. Although supported by the distributional hypothesis (Harris, 1954), this definition suffers from two major limitations. Firstly, the window frame size is usually rather small (for efficiency and sparsity considerations), which increases the false negative rate by missing long-distance dependencies. Secondly, a window frame can (and often does) span across different constituents in a sentence, resulting in an increased false positive rate by associating unrelated words. The problem is worsened as the size of the window increases since each false-positive n-gram will appear in two subsuming false-positive (n + 1)-grams. Several existing studies have addressed </context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig Harris. Distributional structure. Word, 10 (23):146–162, 1954.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Felix Hill</author>
<author>Kyunghyun Cho</author>
<author>Sebastien Jean</author>
<author>Coline Devin</author>
<author>Yoshua Bengio</author>
</authors>
<title>Embedding word similarity with neural machine translation. arXiv preprint arXiv:1412.6448,</title>
<pages>2014</pages>
<marker>Hill, Cho, Jean, Devin, Bengio, </marker>
<rawString>Felix Hill, Kyunghyun Cho, Sebastien Jean, Coline Devin, and Yoshua Bengio. Embedding word similarity with neural machine translation. arXiv preprint arXiv:1412.6448, 2014a.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Felix Hill</author>
<author>Roi Reichart</author>
<author>Anna Korhonen</author>
</authors>
<title>Simlex-999: Evaluating semantic models with (genuine) similarity estimation. arXiv preprint arXiv:1408.3456,</title>
<pages>2014</pages>
<marker>Hill, Reichart, Korhonen, </marker>
<rawString>Felix Hill, Roi Reichart, and Anna Korhonen. Simlex-999: Evaluating semantic models with (genuine) similarity estimation. arXiv preprint arXiv:1408.3456, 2014b.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>873--882</pages>
<contexts>
<context position="4300" citStr="Huang et al. (2012)" startWordPosition="615" endWordPosition="618">ciency is greatly improved by approximating the computationally costly softmax function with 458 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 458–463, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics negative sampling (similar to that of Collobert and Weston 2008) or hierarchical softmax (similar to that of Mnih and Hinton 2007). To address the limitation of contextual locality in many language models (including word2vec), Huang et al. (2012) added a “global context score” to the local n-gram score (Collobert and Weston, 2008). The concatenation of word vectors and a “document vector” (centroid of the composing word vectors weighted by idf) was used as model input. Pennington et al. (2014) proposed to explicitly factorize the global co-occurrence matrix between words, and the resulting log bilinear model achieved state-of-the-art performance in lexical similarity, analogy, and named entity recognition. Several later studies addressed the limitations of window-based co-occurrence by extending the word2vec model to predict words tha</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric Huang, Richard Socher, Christopher D Manning, and Andrew Ng. Improving word representations via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 873– 882. Association for Computational Linguistics, 2012.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Dependencybased word embeddings.</title>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>2</volume>
<pages>2014</pages>
<marker>Levy, Goldberg, </marker>
<rawString>Omer Levy and Yoav Goldberg. Dependencybased word embeddings. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 2, 2014a.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Neural word embedding as implicit matrix factorization.</title>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>2177--2185</pages>
<marker>Levy, Goldberg, </marker>
<rawString>Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In Advances in Neural Information Processing Systems, pages 2177–2185, 2014b.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<booktitle>In Proceedings of the International Conference on Learning Representations, 2013a.</booktitle>
<marker>Mikolov, Chen, Corrado, Dean, </marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. In Proceedings of the International Conference on Learning Representations, 2013a.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<booktitle>In Proceedings of Human Language Technologies: The 2013 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>746--751</pages>
<marker>Mikolov, Yih, Zweig, </marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representations. In Proceedings of Human Language Technologies: The 2013 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 746–751, 2013b.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>Walter Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1991</date>
<journal>Language and Cognitive Processes,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="12351" citStr="Miller and Charles, 1991" startWordPosition="1877" endWordPosition="1880">well as the proposed). Embeddings in the window-based models are obtained by running the published software for each of these systems on the Gigaword corpus with default values for all hyper-parameters except for vector size (50) and minimum word frequency (100 for the entire Gigaword corpus; 5 for the 17M subset). For the w2v model, for example, we used the skip-gram model with the default value 5 as window size, negative sample size, and epoch size, and 0.025 as initial learning rate. 4.2 Lexical Similarity Relation-Dependent Models Table 1 shows the results on four similarity datasets: MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), FG (or wordsim353, Finkelstein et al. 2001), and SL (or SimLex, Hill et al. 2014b). The first three datasets consist of nouns, while the last one also includes verbs (SLv) and adjectives (SLa) in addition to nouns (SLn). Semantically, FG contains many related pairs (e.g., movie–popcorn), whereas the other three datasets are purely similarity oriented. 2http://www.mso.anu.edu.au/˜ralph/ OPTED/ logσ(e0TwIM˜ewI) + k ∑ i=1 ∂ ∂M − k ∑ i=1 460 Model MC RG FG SLn SLv SLa amod .766 .798 .572 .566 .154 .466 amod−1 .272 .296 .220 .218 .248 .602 nsubj .442 .350 .37</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>George Miller and Walter Charles. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):1–28, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th International Conference on Machine Learning,</booktitle>
<pages>641--648</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="3295" citStr="Mnih and Hinton, 2007" startWordPosition="474" endWordPosition="477">rom (1) factorizing syntactic relations into individual relations for structured syntactic information and (2) defining relatedness using lexicographic knowledge. We will show that implementation of these ideas brings notable improvement in lexical similarity tasks. 2 Related Work Lexical embeddings have traditionally been used in language modelling as distributed representations of words (Bengio et al., 2003; Mnih and Hinton, 2009) and have only recently been used in other NLP tasks. Turian et al. (2010), for example, used embeddings from existing language models (Collobert and Weston, 2008; Mnih and Hinton, 2007) as unsupervised lexical features to improve named entity recognition and chunking. Embedding models gained further popularity thanks to the simplicity and effectiveness of the word2vec model (Mikolov et al., 2013a), which implicitly factorizes the point-wise mutual information matrix shifted by biases consisting of marginal counts of individual words (Levy and Goldberg, 2014b). Efficiency is greatly improved by approximating the computationally costly softmax function with 458 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joi</context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. Three new graphical models for statistical language modelling. In Proceedings of the 24th International Conference on Machine Learning, pages 641– 648. ACM, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>1081--1088</pages>
<contexts>
<context position="3109" citStr="Mnih and Hinton, 2009" startWordPosition="443" endWordPosition="447">itive (n + 1)-grams. Several existing studies have addressed these limitations of window-based contexts. Nonetheless, we hypothesize that lexical embedding learning can further benefit from (1) factorizing syntactic relations into individual relations for structured syntactic information and (2) defining relatedness using lexicographic knowledge. We will show that implementation of these ideas brings notable improvement in lexical similarity tasks. 2 Related Work Lexical embeddings have traditionally been used in language modelling as distributed representations of words (Bengio et al., 2003; Mnih and Hinton, 2009) and have only recently been used in other NLP tasks. Turian et al. (2010), for example, used embeddings from existing language models (Collobert and Weston, 2008; Mnih and Hinton, 2007) as unsupervised lexical features to improve named entity recognition and chunking. Embedding models gained further popularity thanks to the simplicity and effectiveness of the word2vec model (Mikolov et al., 2013a), which implicitly factorizes the point-wise mutual information matrix shifted by biases consisting of marginal counts of individual words (Levy and Goldberg, 2014b). Efficiency is greatly improved b</context>
</contexts>
<marker>Mnih, Hinton, 2009</marker>
<rawString>Andriy Mnih and Geoffrey E Hinton. A scalable hierarchical distributed language model. In Advances in Neural Information Processing Systems, pages 1081–1088, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Napoles</author>
<author>Matthew Gormley</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Annotated Gigaword.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Webscale Knowledge Extraction,</booktitle>
<pages>95--100</pages>
<marker>Napoles, Gormley, Van Durme, 2012</marker>
<rawString>Courtney Napoles, Matthew Gormley, and Benjamin Van Durme. Annotated Gigaword. In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Webscale Knowledge Extraction, pages 95–100. Association for Computational Linguistics, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>GloVe: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<contexts>
<context position="1139" citStr="Pennington et al., 2014" startWordPosition="148" endWordPosition="152">rically shown to improve embedding learning by performing notably better than several popular embedding models in similarity tasks. 1 Lexical Embeddings and Relatedness Lexical embeddings are essentially real-valued distributed representations of words. As a vectorspace model, an embedding model approximates semantic relatedness with the Euclidean distance between embeddings, the result of which helps better estimate the real lexical distribution in various NLP tasks. In recent years, researchers have developed efficient and effective algorithms for learning embeddings (Mikolov et al., 2013a; Pennington et al., 2014) and extended model applications from language modelling to various areas in NLP including lexical semantics (Mikolov et al., 2013b) and parsing (Bansal et al., 2014). To approximate semantic relatedness with geometric distance, objective functions are usually chosen to correlate positively with the Euclidean similarity between the embeddings of related words. Maximizing such an objective function is then equivalent to adjusting the embeddings so that those of the related words will be geometrically closer. The definition of relatedness among words can have a profound influence on the quality </context>
<context position="4552" citStr="Pennington et al. (2014)" startWordPosition="656" endWordPosition="659"> Processing (Short Papers), pages 458–463, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics negative sampling (similar to that of Collobert and Weston 2008) or hierarchical softmax (similar to that of Mnih and Hinton 2007). To address the limitation of contextual locality in many language models (including word2vec), Huang et al. (2012) added a “global context score” to the local n-gram score (Collobert and Weston, 2008). The concatenation of word vectors and a “document vector” (centroid of the composing word vectors weighted by idf) was used as model input. Pennington et al. (2014) proposed to explicitly factorize the global co-occurrence matrix between words, and the resulting log bilinear model achieved state-of-the-art performance in lexical similarity, analogy, and named entity recognition. Several later studies addressed the limitations of window-based co-occurrence by extending the word2vec model to predict words that are syntactically related to target words. Levy and Goldberg (2014a) used syntactically related words nondiscriminatively as syntactic context. Bansal et al. (2014) used a training corpus consisting of sequences of labels following certain manually c</context>
<context position="11669" citStr="Pennington et al., 2014" startWordPosition="1765" endWordPosition="1768"> A random subset with 17 million words is also used to study the effect of training data size (dubbed 17M). Semantic relations are derived from the definition text in the Online Plain Text English Dictionary2. There are approximately 806,000 definition pairs, 33,000 distinct definienda and 24,000 distinct defining words. The entire corpus has 1.25 million words in a 7.1MB file. Three baseline systems are used for comparison, including one non-factorized dependencybased model DEP (Levy and Goldberg, 2014a) and two window-based embedding models w2v (or word2vec, Mikolov et al. 2013a) and GloVe (Pennington et al., 2014). Embedding dimension is 50 for all models (baselines as well as the proposed). Embeddings in the window-based models are obtained by running the published software for each of these systems on the Gigaword corpus with default values for all hyper-parameters except for vector size (50) and minimum word frequency (100 for the entire Gigaword corpus; 5 for the 17M subset). For the w2v model, for example, we used the skip-gram model with the default value 5 as window size, negative sample size, and epoch size, and 0.025 as initial learning rate. 4.2 Lexical Similarity Relation-Dependent Models Ta</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D Manning. GloVe: Global vectors for word representation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Reichert</author>
<author>John Olney</author>
<author>James Paris</author>
</authors>
<title>Two Dictionary Transcripts and Programs for Processing Them – The Encoding Scheme, Parsent and Conix.,</title>
<date>1969</date>
<volume>1</volume>
<note>DTIC Research Report AD0691098,</note>
<contexts>
<context position="8323" citStr="Reichert et al. 1969" startWordPosition="1231" endWordPosition="1234">am frequency). 3.2 Incorporating Lexicographic Knowledge Semantic information used in existing studies (Section 2) either relies on specialized lexical resources with limited availability or is obtained from complex procedures that are difficult to replicate. To address these issues, we propose to use monolingual dictionaries as a simple yet effective source of semantic knowledge. The defining relation has been demonstrated to have good performance in various semantic tasks (Chodorow et al., 1985; Alshawi, 1987). The inverse of the defining relation (also known as the Olney Concordance Index, Reichert et al. 1969) has also been proven useful in building lexicographic taxonomies (Amsler, 1980) and identifying synonyms (Wang and Hirst, 2011). Therefore, we use both the defining relation and its inverse as sources of semantic association in the proposed embedding models. Lexicographic knowledge is represented by adopting the same terminology used in syntactic log6(eTg e�d) + k ∑ i=1 459 dependencies: definienda as governors and definientia as dependents. For example, apple is related to fruit and rosaceous as a governor under def, or to cider and pippin as a dependent under def−1. deeper structure with pr</context>
</contexts>
<marker>Reichert, Olney, Paris, 1969</marker>
<rawString>Richard Reichert, John Olney, and James Paris. Two Dictionary Transcripts and Programs for Processing Them – The Encoding Scheme, Parsent and Conix., volume 1. DTIC Research Report AD0691098, 1969.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Rubenstein</author>
<author>John Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="12389" citStr="Rubenstein and Goodenough, 1965" startWordPosition="1882" endWordPosition="1885">gs in the window-based models are obtained by running the published software for each of these systems on the Gigaword corpus with default values for all hyper-parameters except for vector size (50) and minimum word frequency (100 for the entire Gigaword corpus; 5 for the 17M subset). For the w2v model, for example, we used the skip-gram model with the default value 5 as window size, negative sample size, and epoch size, and 0.025 as initial learning rate. 4.2 Lexical Similarity Relation-Dependent Models Table 1 shows the results on four similarity datasets: MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), FG (or wordsim353, Finkelstein et al. 2001), and SL (or SimLex, Hill et al. 2014b). The first three datasets consist of nouns, while the last one also includes verbs (SLv) and adjectives (SLa) in addition to nouns (SLn). Semantically, FG contains many related pairs (e.g., movie–popcorn), whereas the other three datasets are purely similarity oriented. 2http://www.mso.anu.edu.au/˜ralph/ OPTED/ logσ(e0TwIM˜ewI) + k ∑ i=1 ∂ ∂M − k ∑ i=1 460 Model MC RG FG SLn SLv SLa amod .766 .798 .572 .566 .154 .466 amod−1 .272 .296 .220 .218 .248 .602 nsubj .442 .350 .376 .388 .392 .464 nn .596 .620 .514 .48</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Herbert Rubenstein and John Goodenough. Contextual correlates of synonymy. Communications of the ACM, 8(10):627–633, 1965.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="3183" citStr="Turian et al. (2010)" startWordPosition="458" endWordPosition="461">ns of window-based contexts. Nonetheless, we hypothesize that lexical embedding learning can further benefit from (1) factorizing syntactic relations into individual relations for structured syntactic information and (2) defining relatedness using lexicographic knowledge. We will show that implementation of these ideas brings notable improvement in lexical similarity tasks. 2 Related Work Lexical embeddings have traditionally been used in language modelling as distributed representations of words (Bengio et al., 2003; Mnih and Hinton, 2009) and have only recently been used in other NLP tasks. Turian et al. (2010), for example, used embeddings from existing language models (Collobert and Weston, 2008; Mnih and Hinton, 2007) as unsupervised lexical features to improve named entity recognition and chunking. Embedding models gained further popularity thanks to the simplicity and effectiveness of the word2vec model (Mikolov et al., 2013a), which implicitly factorizes the point-wise mutual information matrix shifted by biases consisting of marginal counts of individual words (Levy and Goldberg, 2014b). Efficiency is greatly improved by approximating the computationally costly softmax function with 458 Proce</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394, Uppsala, Sweden, July 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Wang</author>
<author>Graeme Hirst</author>
</authors>
<title>Exploring patterns in dictionary definitions for synonym extraction.</title>
<date>2011</date>
<journal>Natural Language Engineering,</journal>
<volume>17</volume>
<contexts>
<context position="8451" citStr="Wang and Hirst, 2011" startWordPosition="1250" endWordPosition="1253"> on specialized lexical resources with limited availability or is obtained from complex procedures that are difficult to replicate. To address these issues, we propose to use monolingual dictionaries as a simple yet effective source of semantic knowledge. The defining relation has been demonstrated to have good performance in various semantic tasks (Chodorow et al., 1985; Alshawi, 1987). The inverse of the defining relation (also known as the Olney Concordance Index, Reichert et al. 1969) has also been proven useful in building lexicographic taxonomies (Amsler, 1980) and identifying synonyms (Wang and Hirst, 2011). Therefore, we use both the defining relation and its inverse as sources of semantic association in the proposed embedding models. Lexicographic knowledge is represented by adopting the same terminology used in syntactic log6(eTg e�d) + k ∑ i=1 459 dependencies: definienda as governors and definientia as dependents. For example, apple is related to fruit and rosaceous as a governor under def, or to cider and pippin as a dependent under def−1. deeper structure with pre-training on the factorized results (via the relation-dependent models) in the first layer. 3.3 Combining Individual Knowledge </context>
</contexts>
<marker>Wang, Hirst, 2011</marker>
<rawString>Tong Wang and Graeme Hirst. Exploring patterns in dictionary definitions for synonym extraction. Natural Language Engineering, 17, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yinggong Zhao</author>
<author>Shujian Huang</author>
<author>Xinyu Dai</author>
<author>Jianbing Zhang</author>
<author>Jiajun Chen</author>
</authors>
<title>Learning word embeddings from dependency relations.</title>
<date>2014</date>
<booktitle>In Proceedings of 2014 International Conference on Asian Language Processing (IALP),</booktitle>
<pages>123--127</pages>
<publisher>IEEE,</publisher>
<contexts>
<context position="5188" citStr="Zhao et al. (2014)" startWordPosition="748" endWordPosition="751">citly factorize the global co-occurrence matrix between words, and the resulting log bilinear model achieved state-of-the-art performance in lexical similarity, analogy, and named entity recognition. Several later studies addressed the limitations of window-based co-occurrence by extending the word2vec model to predict words that are syntactically related to target words. Levy and Goldberg (2014a) used syntactically related words nondiscriminatively as syntactic context. Bansal et al. (2014) used a training corpus consisting of sequences of labels following certain manually compiled patterns. Zhao et al. (2014) employed coarse-grained classifications of contexts according to the hierarchical structures in a parse tree. Semantic relations have also been explored as a form of lexical association. Faruqui et al. (2015) proposed to retrofit pre-trained embeddings (derived using window-based contexts) to semantic lexicons. The goal is to derive a set of embeddings to capture relatedness suggested by semantic lexicons while maintaining their resemblance to the corresponding window-based embeddings. Bollegala et al. (2014) trained an embedding model with lexical, part-of-speech, and dependency patterns ext</context>
</contexts>
<marker>Zhao, Huang, Dai, Zhang, Chen, 2014</marker>
<rawString>Yinggong Zhao, Shujian Huang, Xinyu Dai, Jianbing Zhang, and Jiajun Chen. Learning word embeddings from dependency relations. In Proceedings of 2014 International Conference on Asian Language Processing (IALP), pages 123– 127. IEEE, 2014.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>