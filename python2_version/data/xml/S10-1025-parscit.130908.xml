<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000030">
<title confidence="0.9965515">
SWAT: Cross-Lingual Lexical Substitution using Local Context Matching,
Bilingual Dictionaries and Machine Translation
</title>
<author confidence="0.999623">
Richard Wicentowski, Maria Kelly, Rachel Lee
</author>
<affiliation confidence="0.799139666666667">
Department of Computer Science
Swarthmore College
Swarthmore, PA 19081 USA
</affiliation>
<email confidence="0.99796">
richardw@cs.swarthmore.edu, {mkelly1,rlee1}@sccs.swarthmore.edu
</email>
<sectionHeader confidence="0.997373" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999915">
We present two systems that select the
most appropriate Spanish substitutes for
a marked word in an English test sen-
tence. These systems were official en-
tries to the SemEval-2010 Cross-Lingual
Lexical Substitution task. The first sys-
tem, SWAT-E, finds Spanish substitutions
by first finding English substitutions in
the English sentence and then translating
these substitutions into Spanish using an
English-Spanish dictionary. The second
system, SWAT-S, translates each English
sentence into Spanish and then finds the
Spanish substitutions in the Spanish sen-
tence. Both systems exceeded the base-
line and all other participating systems by
a wide margin using one of the two official
scoring metrics.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999677">
We present two systems submitted as official en-
tries to the SemEval-2010 Cross-Lingual Lexical
Substitution task (Mihalcea et al., 2010). In this
task, participants were asked to substitute a single
marked word in an English sentence with the most
appropriate Spanish translation(s) given the con-
text. On the surface, our two systems are very sim-
ilar, performing monolingual lexical substitution
and using translation tools and bilingual dictionar-
ies to make the transition from English to Spanish.
</bodyText>
<sectionHeader confidence="0.995532" genericHeader="introduction">
2 Scoring
</sectionHeader>
<bodyText confidence="0.999924636363636">
The task organizers used two scoring metrics
adapted from the SemEval-2007 English Lexical
Substitution task (McCarthy and Navigli, 2007).
For each test item i, human annotators provided a
multiset of substitutions, Ti, that formed the gold
standard. Given a system-provided multiset an-
swer Si for test item i, the best score for a sin-
gle test item is computed using (1). Systems were
allowed to provide an unlimited number of re-
sponses in Si, but each item’s best score was di-
vided by the number of answers provided in Si.
</bodyText>
<equation confidence="0.719707333333333">
� s∈Si frequency(s E Ti)
best score = (1)
|Si|·|Ti|
</equation>
<bodyText confidence="0.991759777777778">
The out-of-ten score, henceforth oot, limited sys-
tems to a maximum of 10 responses for each test
item. Unlike the best scoring method, the final
score for each test item in the oot method is not di-
vided by the actual number of responses provided
by the system; therefore, systems could maximize
their score by always providing exactly 10 re-
sponses. In addition, since Si is a multiset, the
10 responses in Si need not be unique.
</bodyText>
<subsectionHeader confidence="0.283174">
&amp;∈S, frequency(s E Ti)
</subsectionHeader>
<bodyText confidence="0.999322571428572">
impact on our systems can be found in Section 3.4.
The final best and oot score for the system is
computed by summing the individual scores for
each item and, for recall, dividing by the number
of tests items, and for precision, dividing by the
number of test items answered. Our systems pro-
vided a response to every test item, so precision
and recall are the same by this definition.
For both best and oot, the Mode recall (simi-
larly, Mode precision) measures the system’s abil-
ity to identify the substitute that was the annota-
tors’ most frequently chosen substitute, when such
a most frequent substitute existed (McCarthy and
Navigli, 2007).
</bodyText>
<sectionHeader confidence="0.997788" genericHeader="method">
3 Systems
</sectionHeader>
<bodyText confidence="0.868348125">
Our two entries were SWAT-E and SWAT-S. Both
systems used a two-step process to obtain a ranked
list of substitutes. The SWAT-E system first used a
monolingual lexical substitution algorithm to pro-
vide a ranked list of English substitutes and then
oot score = z (2)
|Ti|
Further details on the oot scoring method and its
</bodyText>
<page confidence="0.975315">
123
</page>
<bodyText confidence="0.908083444444444">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 123–128,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
these substitutes were translated into Spanish to
obtain the cross-lingual result. The SWAT-S sys-
tem performed these two steps in the reverse or-
der: first, the English sentences were translated
into Spanish and then the monolingual lexical sub-
stitution algorithm was run on the translated out-
put to provide a ranked list of Spanish substitutes.
</bodyText>
<subsectionHeader confidence="0.999831">
3.1 Syntagmatic coherence
</subsectionHeader>
<bodyText confidence="0.999449625">
The monolingual lexical substitution algorithm
used by both systems is an implementation of the
syntagmatic coherence criterion used by the IRST2
system (Giuliano et al., 2007) in the SemEval-
2007 Lexical Substitution task.
For a sentence H,,, containing the target word
w, the IRST2 algorithm first compiles a set, E, of
candidate substitutes for w from a dictionary, the-
saurus, or other lexical resource. For each e ∈ E,
He is formed by substituting e for w in H,,,. Each
n-gram (2 &lt; n &lt; 5) of He containing the sub-
stitute e is assigned a score, f, equal to how fre-
quently the n-gram appeared in a large corpus.
For all triples (e, n, f) where f &gt; 0, we add
(e, n, f) to E&apos;. E&apos; is then sorted by n, with ties
broken by f. The highest ranked item in E&apos;, there-
fore, is the triple containing the synonym e that
appeared in the longest, most frequently occurring
n-gram. Note that each candidate substitute e can
appear multiple times in E&apos;: once for each value
of n.
The list E&apos; becomes the final output of the syn-
tagmatic coherence criterion, providing a ranking
for all candidate substitutes in E.
</bodyText>
<subsectionHeader confidence="0.99812">
3.2 The SWAT-E system
3.2.1 Resources
</subsectionHeader>
<bodyText confidence="0.999948">
The SWAT-E system used the English Web1T 5-
gram corpus (Brants and Franz, 2006), the Span-
ish section of the Web1T European 5-gram cor-
pus (Brants and Franz, 2009), Roget’s online the-
saurus1, NLTK’s implementation of the Lancaster
Stemmer (Loper and Bird, 2002), Google’s online
English-Spanish dictionary2, and SpanishDict’s
online dictionary3. We formed a single Spanish-
English dictionary by combining the translations
found in both dictionaries.
</bodyText>
<footnote confidence="0.999976666666667">
1http://thesaurus.com
2http://www.google.com/dictionary
3http://www.spanishdict.com
</footnote>
<subsectionHeader confidence="0.956675">
3.2.2 Ranking substitutes
</subsectionHeader>
<bodyText confidence="0.999993872340426">
The first step in the SWAT-E algorithm is to create
a ranked list of English substitutes. For each En-
glish test sentence H,,, containing the target word
w, we use the syntagmatic coherence criterion de-
scribed above to create E&apos;, a ranking of the syn-
onyms of w taken from Roget’s thesaurus. We use
the Lancaster stemmer to ensure that we count all
morphologically similar lexical substitutes.
Next, we use a bilingual dictionary to convert
our candidate English substitutes into candidate
Spanish substitutes, forming a new ranked list S&apos;.
For each item (e, n, f) in E&apos;, and for each Spanish
translation s of e, we add the triple (s, n, f) to S&apos;.
Since different English words can have the same
Spanish translation s, we can end up with multiple
triples in S&apos; that have the same values for s and n.
For example, if s1 is a translation of both e1 and
e2, and the triples (e1, 4,87) and (e2, 4, 61) appear
in E&apos;, then S&apos; will contain the triples (s1, 4, 87)
and (s1, 4,61). We merge all such “duplicates” by
summing their frequencies. In this example, we
would replace the two triples containing s1 with a
new triple, (s1, 4, 148). After merging all dupli-
cates, we re-sort S&apos; by n, breaking ties by f. No-
tice that since triples are merged only when both s
and n are the same, Spanish substitutes can appear
multiple times in S&apos;: once for each value of n.
At this point, we have a ranked list of candi-
date Spanish substitutes, S&apos;. From this list S&apos;,
we keep only those Spanish substitutes that are
direct translations of our original word w. The
reason for doing this is that some of the transla-
tions of the synonyms of w have no overlapping
meaning with w. For example, the polysemous
English noun “bug” can mean a flaw in a com-
puter program (cf. test item 572). Our thesaurus
lists “hitch” as a synonym for this sense of “bug”.
Of course, “hitch” is also polysemous, and not ev-
ery translation of “hitch” into Spanish will have
a meaning that overlaps with the original “bug”
sense. Translations such as “enganche”, having
the “trailer hitch” sense, are certainly not appro-
priate substitutes for this, or any, sense of the word
“bug”. By keeping only those substitutes that are
also translations of the original word w, we main-
tain a cleaner list of candidate substitutes. We call
this filtered list of candidates S.
</bodyText>
<subsectionHeader confidence="0.909401">
3.2.3 Selecting substitutes
</subsectionHeader>
<bodyText confidence="0.9550505">
For each English sentence in the test set, we now
have a ranked list of cross-lingual lexical substi-
</bodyText>
<page confidence="0.985547">
124
</page>
<listItem confidence="0.9565275">
1: best = {(si, ni, fl)}
2: j ← 2
3: while (nj == ni) and (fj ≥ 0.75∗f1) do
4: best ← best U {(sj, nj, fj)}
5: j ← j + 1
6: end while
</listItem>
<figureCaption confidence="0.92499">
Figure 1: The method for selecting multiple an-
swers in the best method used by SWAT-E
</figureCaption>
<bodyText confidence="0.99787378125">
tutes, S. In the oot scoring method, we selected
the top 10 substitutes in the ranked list S. If there
were less than 10 items (but at least one item) in
S, we duplicated answers from our ranked list un-
til we had made 10 guesses. (See Section 3.4 for
further details on this process.) If there were no
items in our ranked list, we returned the most fre-
quent translations of w as determined by the un-
igram counts of these translations in the Spanish
Web1T corpus.
For our best answer, we returned multiple re-
sponses when the highest ranked substitutes had
similar frequencies. Since S was formed by trans-
ferring the frequency of each English substitute e
onto all of its Spanish translations, a single English
substitute that had appeared with high frequency
would lead to many Spanish substitutes, each with
high frequencies. (The frequencies need not be ex-
actly the same due to the merging step described
above.) In these cases, we hedged our bet by re-
turning each of these translations.
Representing the i-th item in S as (si, ni, fi),
our procedure for creating the best answer can be
found in Figure 1. We allow all items from S that
have the same value of n as the top ranked item
and have a frequency at least 75% that of the most
frequent item to be included in the best answer.
Of the 1000 test instances, we provided a single
“best” candidate 630 times, two candidates 253
times, three candidates 70 times, four candidates
30 times, and six candidates 17 times. (We never
returned five candidates).
</bodyText>
<subsectionHeader confidence="0.932445">
3.3 SWAT-S
3.3.1 Resources
</subsectionHeader>
<bodyText confidence="0.997803">
The SWAT-S system used both Google’s4 and Ya-
hoo’s5 online translation tools, the Spanish section
of the Web1T European 5-gram corpus, Roget’s
online thesaurus, TreeTagger (Schmid, 1994) for
</bodyText>
<footnote confidence="0.9998485">
4http://translate.google.com/
5http://babelfish.yahoo.com/
</footnote>
<bodyText confidence="0.97305075">
morphological analysis and both Google’s and Ya-
hoo’s6 English-Spanish dictionaries. We formed
a single Spanish-English dictionary by combining
the translations found in both dictionaries.
</bodyText>
<subsectionHeader confidence="0.948679">
3.3.2 Ranking substitutes
</subsectionHeader>
<bodyText confidence="0.99248888372093">
To find the cross-lingual lexical substitutes for a
target word in an English sentence, we first trans-
late the sentence into Spanish and then use the
syntagmatic coherence criterion on the translated
Spanish sentence.
In order to perform this monolingual Spanish
lexical substitution, we need to be able to iden-
tify the target word we are attempting to substitute
in the translated sentence. We experimented with
using Moses (Koehn et al., 2007) to perform the
machine translation and produce a word alignment
but we found that Google’s online translation tool
produced better translations than Moses did when
trained on the Europarl data we had available.
In the original English sentence, the target word
is marked with an XML tag. We had hoped that
Google’s translation tool would preserve the XML
tag around the translated target word, but that was
not the case. We also experimented with using
quotation marks around the target word instead of
the XML tag. The translation tool often preserved
quotation marks around the target word, but also
yielded a different, and anecdotally worse, transla-
tion than the same sentence without the quotation
marks. (We will, however, return to this strategy
as a backoff method.) Although we did not exper-
iment with using a stand-alone word alignment al-
gorithm to find the target word in the Spanish sen-
tence, Section 4.3 provides insights into the possi-
ble performance gains possible by doing so.
Without a word alignment, we were left with
the following problem: Given a translated Span-
ish sentence H, how could we identify the word w
that is the translation of the original English target
word, v? Our search strategy proceeded as fol-
lows.
1. We looked up v in our English-Spanish dictio-
nary and searched H for one of these trans-
lations (or a morphological variant), choosing
the matching translation as the Spanish target
word. If the search yielded multiple matches,
we chose the match that was in the most similar
position in the sentence to the position of v in
</bodyText>
<footnote confidence="0.999395">
6http://education.yahoo.com/reference/
dict en es/
</footnote>
<page confidence="0.995535">
125
</page>
<bodyText confidence="0.908974">
the English sentence. This method identified a
match in 801 of the 1000 test sentences.
</bodyText>
<listItem confidence="0.904620923076923">
2. If we had not found a match, we translated each
word in H back into English, one word at a
time. If one of the re-translated words was a
synonym of v, we chose that word as the target
word. If there were multiple matches, we again
used position to choose the target.
3. If we still had no match, we used Yahoo’s trans-
lation tool instead of Google’s, and repeated
steps 1. and 2. above.
4. If we still had no match, we reverted to using
Google’s translation tool, this time explicitly
offsetting the English target word with quota-
tion marks.
</listItem>
<bodyText confidence="0.999971642857143">
In 992 of the 1000 test sentences, this four-step
procedure produced a Spanish sentence Hw with
a target w. For each of these sentences, we pro-
duced E&apos;, the list of ranked Spanish substitutes
using the syntagmatic selection coherence crite-
rion described in Section 3.1. We used the Span-
ish Web1T corpus as a source of n-gram counts,
and we used the Spanish translations of v as the
candidate substitution set E. For the remaining
8 test sentences where we could not identify the
target word, we set E&apos; equal to the top 10 most
frequently occurring Spanish translations of v as
determined by the unigram counts of these trans-
lations in the Spanish Web1T corpus.
</bodyText>
<subsectionHeader confidence="0.972954">
3.3.3 Selecting substitutes
</subsectionHeader>
<bodyText confidence="0.999881571428571">
For each English sentence in the test set, we se-
lected the single best item in E&apos; as our answer for
the best scoring method.
For the oot scoring method, we wanted to en-
sure that the translated target word w, identified in
Section 3.3.2, was represented in our output, even
if this substitute was poorly ranked in E&apos;. If w ap-
peared in E&apos;, then our oot answer was simply the
first 10 entries in E&apos;. If w was not in E&apos;, then our
answer was the top 9 entries in E&apos; followed by w.
As we had done with our SWAT-E system, if
the oot answer contained less than 10 items, we
repeated answers until we had made 10 guesses.
See the following section for more information.
</bodyText>
<subsectionHeader confidence="0.99079">
3.4 oot selection details
</subsectionHeader>
<bodyText confidence="0.9866378">
The metric used to calculate oot precision in this
task (Mihalcea et al., 2010) favors systems that al-
ways propose 10 candidate substitutes over those
|Ti|
The final oot recall is just the average of these
scores over all test items. For test item i, Si is
the multiset of candidates provided by the system,
Ti is the multiset of responses provided by the an-
notators, and frequency(s E Ti) is the number of
times each item s appeared in Ti.
Assume that Ti = {feliz, feliz, contento, ale-
gre}. A system that produces Si = {feliz, con-
tento} would receive a score of 2+41 = 0.75. How-
ever a system that produces Si with feliz and con-
tento each appearing 5 times would receive a score
</bodyText>
<equation confidence="0.5198465">
of 5*2+5*1 =
4
</equation>
<bodyText confidence="0.999632380952381">
duced Si = {feliz, contento} plus 8 other responses
that were not in the gold standard would receive
the same score as the system that produced only
Si = {feliz, contento}, so there is never a penalty
for providing all 10 answers.
For this reason, in both of our systems, we en-
sure that our oot response always contains exactly
10 answers. To do this, we repeatedly append our
list of candidates to itself until the length of the list
is equal to or exceeds 10, then we truncate the list
to exactly 10 answers. For example, if our orig-
inal candidate list was [a, b, c, d], our final oot
response would be [a, b, c, d, a, b, c, d, a, b].
Notice that this is not the only way to produce
a response with 10 answers. An alternative would
be to produce a response containing [a, b, c, d]
followed by 6 other unique translations from the
English-Spanish dictionary. However, we found
that padding the response with unique answers
was far less effective than repeating the answers
returned by the syntagmatic coherence algorithm.
</bodyText>
<sectionHeader confidence="0.524366" genericHeader="method">
4 Analysis of Results
</sectionHeader>
<bodyText confidence="0.999911">
Table 1 shows the results of our two systems com-
pared to two baselines, DICT and DICTCORP, and
the upper bound for the task.7 Since all of these
systems provide an answer for every test instance,
precision and recall are always the same. The
upper bound for the best metric results from re-
turning a single answer equal to the annotators’
most frequent substitute. The upper bound for the
oot metric is obtained by returning the annotator’s
most frequent substitute repeated 10 times.
</bodyText>
<footnote confidence="0.4730195">
7Details on the baselines and the upper bound can be
found in (Mihalcea et al., 2010).
</footnote>
<table confidence="0.876566307692308">
that propose fewer than 10 substitutes. For each
test item the oot score is calculated as follows:
oot score =
EscS, frequency(s E Ti)
3.75. Importantly, a system that pro-
126
System R best R oot
Mode R Mode R
SWAT-E 21.5 43.2 174.6 66.9
SWAT-S 18.9 36.6 98.0 79.0
DICT 24.3 50.3 44.0 73.5
DICTCORP 15.1 29.2 29.2 29.2
upper bound 40.6 100.0 405.9 100.0
</table>
<tableCaption confidence="0.74250825">
Table 1: System performance using the two scor-
ing metrics, best and oot. All test instances were
answered, so precision equals recall. DICT and
DICTCORP are the two baselines.
</tableCaption>
<bodyText confidence="0.99957475">
Like the IRST2 system (Giuliano et al., 2007)
submitted in the 2007 Lexical Substitution task,
our system performed extremely well on the oot
scoring method while performing no better than
average on the best method. Further analysis
should be done to determine if this is due to a
flaw in the approach, or if there are other factors
at work.
</bodyText>
<subsectionHeader confidence="0.999544">
4.1 Analysis of the oot method
</subsectionHeader>
<bodyText confidence="0.99997575862069">
Our oot performance was certainly helped by the
fact that we chose to provide 10 answers for each
test item. One way to measure this is to score
both of our systems with all duplicate candidates
removed. We can see that the recall of both sys-
tems drops off sharply: SWAT-E drops from 174.6
to 36.3, and SWAT-S drops from 98.0 to 46.7. As
was shown in Section 3.4, the oot system should
always provide 10 answers; however, 12.8% of
the SWAT-S test responses, and only 3.2% of the
SWAT-E test responses contained no duplicates. In
fact, 38.4% of the SWAT-E responses contained
only a single unique answer. Providing duplicate
answers allowed us to express confidence in the
substitutes found. If duplicates were forbidden,
simply filling any remaining answers with other
translations taken from the English-Spanish dic-
tionary could only serve to increase performance.
Another way to measure the effect of always
providing 10 answers is to modify the responses
provided by the other systems so that they, too,
always provide 10 answers. Of the 14 submitted
systems, only 5 (including our systems) provided
10 answers for each test item. Neither of the two
baseline systems, DICT and DICTCORP, provided
10 answers for each test item. Using the algorithm
described in Section 3.4, we re-scored each of the
systems with answers duplicated so that each re-
sponse contained exactly 10 substitutes. As shown
</bodyText>
<table confidence="0.99961725">
System filled oot oot
R P R P
SWAT-E 174.6 174.6 174.6 174.6
IRST-1 126.0 132.6 31.5 33.1
SWAT-S 98.0 98.0 98.0 98.0
WLVUSP 86.1 86.1 48.5 48.5
DICT 71.1 71.1 44.0 44.0
DICTCORP 66.7 66.7 15.1 15.1
</table>
<tableCaption confidence="0.77604825">
Table 2: System performance using oot for the
top 4 systems when providing exactly 10 substi-
tutes for all answered test items (“filled oot”), as
well as the score as submitted (“oot”).
</tableCaption>
<bodyText confidence="0.9998375">
in Table 2, both systems still far exceed the base-
line, SWAT-E remains the top scoring system, and
SWAT-S drops to 3rd place behind IRST-1, which
had finished 12th with its original submission.
</bodyText>
<subsectionHeader confidence="0.998842">
4.2 Analysis of oot Mode R
</subsectionHeader>
<bodyText confidence="0.999892666666667">
Although the SWAT-E system outperformed the
SWAT-S system in best recall, best Mode recall
(“Mode R”), and oot recall, the SWAT-S system
outperformed the SWAT-E system by a large mar-
gin in oot Mode R (see Table 1). This result is
easily explained by first referring to the method
used to compute Mode recall: a score of 1 was
given to each test instance where the oot response
contained the annotators’ most frequently chosen
substitute; otherwise 0 was given. The average of
these scores yields Mode R. A system can max-
imize its Mode R score by always providing 10
unique answers. SWAT-E provided an average of
3.3 unique answers per test item and SWAT-S pro-
vided 6.9 unique answers per test item. By provid-
ing more than twice the number of unique answers
per test item, it is not at all surprising that SWAT-S
outperformed SWAT-E in the Mode R measure.
</bodyText>
<subsectionHeader confidence="0.99969">
4.3 Analysis of SWAT-S
</subsectionHeader>
<bodyText confidence="0.999993">
In the SWAT-S system, 801 (of 1000) test sen-
tences had a direct translation of the target word
present in Google’s Spanish translation (identi-
fied by step 1 in Section 3.3.2). In these cases,
the resulting output was better than those cases
where a more indirect approach (steps 2-4) was
necessary. The oot precision on the test sentences
where the target was found directly was 101.3,
whereas the precision of the test sentences where
a target was found more indirectly was only 84.6.
The 8 sentences where the unigram backoff was
</bodyText>
<page confidence="0.988343">
127
</page>
<table confidence="0.999584181818182">
SWAT-E P best P oot
Mode P Mode P
adjective 25.94 50.67 192.78 85.78
noun 22.34 40.44 197.87 59.11
verb 18.62 41.46 155.16 55.12
adverb 15.68 33.78 119.51 66.22
SWAT-S P Mode P P Mode P
adjective 21.70 40.00 126.41 86.67
noun 24.77 45.78 107.85 82.22
verb 13.58 27.80 69.04 71.71
adverb 10.46 22.97 80.26 66.22
</table>
<tableCaption confidence="0.906318">
Table 3: Precision of best and oot for both sys-
tems, analyzed by part of speech.
</tableCaption>
<bodyText confidence="0.999625444444444">
used had a precision of 77.4. This analysis in-
dicates that using a word alignment tool on the
translated sentence pairs would improve the per-
formance of the method. However, since the pre-
cision in those cases where the target word could
be identified was only 101.3, using a word align-
ment tool would almost certainly leave SWAT-S as
a distant second to the 174.6 precision achieved by
SWAT-E.
</bodyText>
<subsectionHeader confidence="0.999458">
4.4 Analysis by part-of-speech
</subsectionHeader>
<bodyText confidence="0.999995214285714">
Table 3 shows the performance of both systems
broken down by part-of-speech. In the IRST2 sys-
tem submitted to the 2007 Lexical Substitution
task, adverbs were the best performing word class,
followed distantly by adjectives, then nouns, and
finally verbs. However, in this task, we found that
adverbs were the hardest word class to correctly
substitute. Further analysis should be done to de-
termine if this is due to the difficulty of the partic-
ular words and sentences chosen in this task, the
added complexity of performing the lexical substi-
tution across two languages, or some independent
factor such as the choice of thesaurus used to form
the candidate set of substitutes.
</bodyText>
<sectionHeader confidence="0.999735" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999892863636363">
We presented two systems that participated in
the SemEval-2010 Cross-Lingual Lexical Substi-
tution task. Both systems use a two-step process
to obtain the lexical substitutes. SWAT-E first finds
English lexical substitutes in the English sentence
and then translates these substitutes into Spanish.
SWAT-S first translates the English sentences into
Spanish and then finds Spanish lexical substitutes
using these translations.
The official competition results showed that our
two systems performed much better than the other
systems on the oot scoring method, but that we
performed only about average on the best scoring
method.
The analysis provided here indicates that the oot
score for SWAT-E would hold even if every sys-
tem had its answers duplicated in order to ensure
10 answers were provided for each test item. We
also we showed that a word alignment tool would
likely improve the performance of SWAT-S, but
that this improvement would not be enough to sur-
pass SWAT-E.
</bodyText>
<sectionHeader confidence="0.999672" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999552189189189">
T. Brants and A. Franz. 2006. Web 1T 5-gram,
ver. 1. LDC2006T13, Linguistic Data Consortium,
Philadelphia.
T. Brants and A. Franz. 2009. Web 1T 5-gram, 10 Eu-
ropean Languages, ver. 1. LDC2009T25, Linguistic
Data Consortium, Philadelphia.
Claudio Giuliano, Alfio Gliozzo, and Carlo Strappa-
rava. 2007. FBK-irst: Lexical Substitution Task
Exploiting Domain and Syntagmatic Coherence. In
Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007).
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions.
E. Loper and S. Bird. 2002. NLTK: The Natural
Language Toolkit. In Proceedings of the ACL-02
Workshop on Effective Tools and Methodologies for
Teaching Natural Language Processing and Compu-
tational Linguistics.
D. McCarthy and R. Navigli. 2007. SemEval-2007
Task 10: English lexical substitution task. In Pro-
ceedings of SemEval-2007.
Rada Mihalcea, Ravi Sinha, and Diana McCarthy.
2010. Semeval-2010 Task 2: Cross-lingual lex-
ical substitution. In Proceedings of the 5th
International Workshop on Semantic Evaluations
(SemEval-2010).
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In International Con-
ference on New Methods in Language Processing.
</reference>
<page confidence="0.996697">
128
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.960282">
<title confidence="0.9953325">SWAT: Cross-Lingual Lexical Substitution using Local Context Matching, Bilingual Dictionaries and Machine Translation</title>
<author confidence="0.999981">Richard Wicentowski</author>
<author confidence="0.999981">Maria Kelly</author>
<author confidence="0.999981">Rachel Lee</author>
<affiliation confidence="0.9990645">Department of Computer Science Swarthmore College</affiliation>
<address confidence="0.987912">Swarthmore, PA 19081 USA</address>
<abstract confidence="0.998959947368421">We present two systems that select the most appropriate Spanish substitutes for a marked word in an English test sentence. These systems were official entries to the SemEval-2010 Cross-Lingual Lexical Substitution task. The first sysfinds Spanish substitutions by first finding English substitutions in the English sentence and then translating these substitutions into Spanish using an English-Spanish dictionary. The second translates each English sentence into Spanish and then finds the Spanish substitutions in the Spanish sentence. Both systems exceeded the baseline and all other participating systems by a wide margin using one of the two official scoring metrics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Brants</author>
<author>A Franz</author>
</authors>
<date>2006</date>
<booktitle>Web 1T 5-gram, ver. 1. LDC2006T13, Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="5309" citStr="Brants and Franz, 2006" startWordPosition="868" endWordPosition="871"> in a large corpus. For all triples (e, n, f) where f &gt; 0, we add (e, n, f) to E&apos;. E&apos; is then sorted by n, with ties broken by f. The highest ranked item in E&apos;, therefore, is the triple containing the synonym e that appeared in the longest, most frequently occurring n-gram. Note that each candidate substitute e can appear multiple times in E&apos;: once for each value of n. The list E&apos; becomes the final output of the syntagmatic coherence criterion, providing a ranking for all candidate substitutes in E. 3.2 The SWAT-E system 3.2.1 Resources The SWAT-E system used the English Web1T 5- gram corpus (Brants and Franz, 2006), the Spanish section of the Web1T European 5-gram corpus (Brants and Franz, 2009), Roget’s online thesaurus1, NLTK’s implementation of the Lancaster Stemmer (Loper and Bird, 2002), Google’s online English-Spanish dictionary2, and SpanishDict’s online dictionary3. We formed a single SpanishEnglish dictionary by combining the translations found in both dictionaries. 1http://thesaurus.com 2http://www.google.com/dictionary 3http://www.spanishdict.com 3.2.2 Ranking substitutes The first step in the SWAT-E algorithm is to create a ranked list of English substitutes. For each English test sentence H</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>T. Brants and A. Franz. 2006. Web 1T 5-gram, ver. 1. LDC2006T13, Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
<author>A Franz</author>
</authors>
<date>2009</date>
<booktitle>Web 1T 5-gram, 10 European Languages, ver. 1. LDC2009T25, Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="5391" citStr="Brants and Franz, 2009" startWordPosition="883" endWordPosition="886"> E&apos; is then sorted by n, with ties broken by f. The highest ranked item in E&apos;, therefore, is the triple containing the synonym e that appeared in the longest, most frequently occurring n-gram. Note that each candidate substitute e can appear multiple times in E&apos;: once for each value of n. The list E&apos; becomes the final output of the syntagmatic coherence criterion, providing a ranking for all candidate substitutes in E. 3.2 The SWAT-E system 3.2.1 Resources The SWAT-E system used the English Web1T 5- gram corpus (Brants and Franz, 2006), the Spanish section of the Web1T European 5-gram corpus (Brants and Franz, 2009), Roget’s online thesaurus1, NLTK’s implementation of the Lancaster Stemmer (Loper and Bird, 2002), Google’s online English-Spanish dictionary2, and SpanishDict’s online dictionary3. We formed a single SpanishEnglish dictionary by combining the translations found in both dictionaries. 1http://thesaurus.com 2http://www.google.com/dictionary 3http://www.spanishdict.com 3.2.2 Ranking substitutes The first step in the SWAT-E algorithm is to create a ranked list of English substitutes. For each English test sentence H,,, containing the target word w, we use the syntagmatic coherence criterion descr</context>
</contexts>
<marker>Brants, Franz, 2009</marker>
<rawString>T. Brants and A. Franz. 2009. Web 1T 5-gram, 10 European Languages, ver. 1. LDC2009T25, Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudio Giuliano</author>
<author>Alfio Gliozzo</author>
<author>Carlo Strapparava</author>
</authors>
<title>FBK-irst: Lexical Substitution Task Exploiting Domain and Syntagmatic Coherence.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007).</booktitle>
<contexts>
<context position="4269" citStr="Giuliano et al., 2007" startWordPosition="672" endWordPosition="675"> Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics these substitutes were translated into Spanish to obtain the cross-lingual result. The SWAT-S system performed these two steps in the reverse order: first, the English sentences were translated into Spanish and then the monolingual lexical substitution algorithm was run on the translated output to provide a ranked list of Spanish substitutes. 3.1 Syntagmatic coherence The monolingual lexical substitution algorithm used by both systems is an implementation of the syntagmatic coherence criterion used by the IRST2 system (Giuliano et al., 2007) in the SemEval2007 Lexical Substitution task. For a sentence H,,, containing the target word w, the IRST2 algorithm first compiles a set, E, of candidate substitutes for w from a dictionary, thesaurus, or other lexical resource. For each e ∈ E, He is formed by substituting e for w in H,,,. Each n-gram (2 &lt; n &lt; 5) of He containing the substitute e is assigned a score, f, equal to how frequently the n-gram appeared in a large corpus. For all triples (e, n, f) where f &gt; 0, we add (e, n, f) to E&apos;. E&apos; is then sorted by n, with ties broken by f. The highest ranked item in E&apos;, therefore, is the trip</context>
<context position="17408" citStr="Giuliano et al., 2007" startWordPosition="2987" endWordPosition="2990"> bound can be found in (Mihalcea et al., 2010). that propose fewer than 10 substitutes. For each test item the oot score is calculated as follows: oot score = EscS, frequency(s E Ti) 3.75. Importantly, a system that pro126 System R best R oot Mode R Mode R SWAT-E 21.5 43.2 174.6 66.9 SWAT-S 18.9 36.6 98.0 79.0 DICT 24.3 50.3 44.0 73.5 DICTCORP 15.1 29.2 29.2 29.2 upper bound 40.6 100.0 405.9 100.0 Table 1: System performance using the two scoring metrics, best and oot. All test instances were answered, so precision equals recall. DICT and DICTCORP are the two baselines. Like the IRST2 system (Giuliano et al., 2007) submitted in the 2007 Lexical Substitution task, our system performed extremely well on the oot scoring method while performing no better than average on the best method. Further analysis should be done to determine if this is due to a flaw in the approach, or if there are other factors at work. 4.1 Analysis of the oot method Our oot performance was certainly helped by the fact that we chose to provide 10 answers for each test item. One way to measure this is to score both of our systems with all duplicate candidates removed. We can see that the recall of both systems drops off sharply: SWAT-</context>
</contexts>
<marker>Giuliano, Gliozzo, Strapparava, 2007</marker>
<rawString>Claudio Giuliano, Alfio Gliozzo, and Carlo Strapparava. 2007. FBK-irst: Lexical Substitution Task Exploiting Domain and Syntagmatic Coherence. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo</booktitle>
<contexts>
<context position="10896" citStr="Koehn et al., 2007" startWordPosition="1824" endWordPosition="1827"> and Yahoo’s6 English-Spanish dictionaries. We formed a single Spanish-English dictionary by combining the translations found in both dictionaries. 3.3.2 Ranking substitutes To find the cross-lingual lexical substitutes for a target word in an English sentence, we first translate the sentence into Spanish and then use the syntagmatic coherence criterion on the translated Spanish sentence. In order to perform this monolingual Spanish lexical substitution, we need to be able to identify the target word we are attempting to substitute in the translated sentence. We experimented with using Moses (Koehn et al., 2007) to perform the machine translation and produce a word alignment but we found that Google’s online translation tool produced better translations than Moses did when trained on the Europarl data we had available. In the original English sentence, the target word is marked with an XML tag. We had hoped that Google’s translation tool would preserve the XML tag around the translated target word, but that was not the case. We also experimented with using quotation marks around the target word instead of the XML tag. The translation tool often preserved quotation marks around the target word, but al</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Loper</author>
<author>S Bird</author>
</authors>
<title>NLTK: The Natural Language Toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics.</booktitle>
<contexts>
<context position="5489" citStr="Loper and Bird, 2002" startWordPosition="897" endWordPosition="900">ple containing the synonym e that appeared in the longest, most frequently occurring n-gram. Note that each candidate substitute e can appear multiple times in E&apos;: once for each value of n. The list E&apos; becomes the final output of the syntagmatic coherence criterion, providing a ranking for all candidate substitutes in E. 3.2 The SWAT-E system 3.2.1 Resources The SWAT-E system used the English Web1T 5- gram corpus (Brants and Franz, 2006), the Spanish section of the Web1T European 5-gram corpus (Brants and Franz, 2009), Roget’s online thesaurus1, NLTK’s implementation of the Lancaster Stemmer (Loper and Bird, 2002), Google’s online English-Spanish dictionary2, and SpanishDict’s online dictionary3. We formed a single SpanishEnglish dictionary by combining the translations found in both dictionaries. 1http://thesaurus.com 2http://www.google.com/dictionary 3http://www.spanishdict.com 3.2.2 Ranking substitutes The first step in the SWAT-E algorithm is to create a ranked list of English substitutes. For each English test sentence H,,, containing the target word w, we use the syntagmatic coherence criterion described above to create E&apos;, a ranking of the synonyms of w taken from Roget’s thesaurus. We use the L</context>
</contexts>
<marker>Loper, Bird, 2002</marker>
<rawString>E. Loper and S. Bird. 2002. NLTK: The Natural Language Toolkit. In Proceedings of the ACL-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
<author>R Navigli</author>
</authors>
<title>SemEval-2007 Task 10: English lexical substitution task.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval-2007.</booktitle>
<contexts>
<context position="1674" citStr="McCarthy and Navigli, 2007" startWordPosition="233" endWordPosition="236">ems submitted as official entries to the SemEval-2010 Cross-Lingual Lexical Substitution task (Mihalcea et al., 2010). In this task, participants were asked to substitute a single marked word in an English sentence with the most appropriate Spanish translation(s) given the context. On the surface, our two systems are very similar, performing monolingual lexical substitution and using translation tools and bilingual dictionaries to make the transition from English to Spanish. 2 Scoring The task organizers used two scoring metrics adapted from the SemEval-2007 English Lexical Substitution task (McCarthy and Navigli, 2007). For each test item i, human annotators provided a multiset of substitutions, Ti, that formed the gold standard. Given a system-provided multiset answer Si for test item i, the best score for a single test item is computed using (1). Systems were allowed to provide an unlimited number of responses in Si, but each item’s best score was divided by the number of answers provided in Si. � s∈Si frequency(s E Ti) best score = (1) |Si|·|Ti| The out-of-ten score, henceforth oot, limited systems to a maximum of 10 responses for each test item. Unlike the best scoring method, the final score for each t</context>
<context position="3207" citStr="McCarthy and Navigli, 2007" startWordPosition="506" endWordPosition="509">ystems can be found in Section 3.4. The final best and oot score for the system is computed by summing the individual scores for each item and, for recall, dividing by the number of tests items, and for precision, dividing by the number of test items answered. Our systems provided a response to every test item, so precision and recall are the same by this definition. For both best and oot, the Mode recall (similarly, Mode precision) measures the system’s ability to identify the substitute that was the annotators’ most frequently chosen substitute, when such a most frequent substitute existed (McCarthy and Navigli, 2007). 3 Systems Our two entries were SWAT-E and SWAT-S. Both systems used a two-step process to obtain a ranked list of substitutes. The SWAT-E system first used a monolingual lexical substitution algorithm to provide a ranked list of English substitutes and then oot score = z (2) |Ti| Further details on the oot scoring method and its 123 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 123–128, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics these substitutes were translated into Spanish to obtain the cross-lingual result. Th</context>
</contexts>
<marker>McCarthy, Navigli, 2007</marker>
<rawString>D. McCarthy and R. Navigli. 2007. SemEval-2007 Task 10: English lexical substitution task. In Proceedings of SemEval-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Ravi Sinha</author>
<author>Diana McCarthy</author>
</authors>
<title>Semeval-2010 Task 2: Cross-lingual lexical substitution.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluations (SemEval-2010).</booktitle>
<contexts>
<context position="1164" citStr="Mihalcea et al., 2010" startWordPosition="156" endWordPosition="159">ystem, SWAT-E, finds Spanish substitutions by first finding English substitutions in the English sentence and then translating these substitutions into Spanish using an English-Spanish dictionary. The second system, SWAT-S, translates each English sentence into Spanish and then finds the Spanish substitutions in the Spanish sentence. Both systems exceeded the baseline and all other participating systems by a wide margin using one of the two official scoring metrics. 1 Introduction We present two systems submitted as official entries to the SemEval-2010 Cross-Lingual Lexical Substitution task (Mihalcea et al., 2010). In this task, participants were asked to substitute a single marked word in an English sentence with the most appropriate Spanish translation(s) given the context. On the surface, our two systems are very similar, performing monolingual lexical substitution and using translation tools and bilingual dictionaries to make the transition from English to Spanish. 2 Scoring The task organizers used two scoring metrics adapted from the SemEval-2007 English Lexical Substitution task (McCarthy and Navigli, 2007). For each test item i, human annotators provided a multiset of substitutions, Ti, that fo</context>
<context position="14603" citStr="Mihalcea et al., 2010" startWordPosition="2478" endWordPosition="2481"> method, we wanted to ensure that the translated target word w, identified in Section 3.3.2, was represented in our output, even if this substitute was poorly ranked in E&apos;. If w appeared in E&apos;, then our oot answer was simply the first 10 entries in E&apos;. If w was not in E&apos;, then our answer was the top 9 entries in E&apos; followed by w. As we had done with our SWAT-E system, if the oot answer contained less than 10 items, we repeated answers until we had made 10 guesses. See the following section for more information. 3.4 oot selection details The metric used to calculate oot precision in this task (Mihalcea et al., 2010) favors systems that always propose 10 candidate substitutes over those |Ti| The final oot recall is just the average of these scores over all test items. For test item i, Si is the multiset of candidates provided by the system, Ti is the multiset of responses provided by the annotators, and frequency(s E Ti) is the number of times each item s appeared in Ti. Assume that Ti = {feliz, feliz, contento, alegre}. A system that produces Si = {feliz, contento} would receive a score of 2+41 = 0.75. However a system that produces Si with feliz and contento each appearing 5 times would receive a score </context>
<context position="16832" citStr="Mihalcea et al., 2010" startWordPosition="2884" endWordPosition="2887">ntagmatic coherence algorithm. 4 Analysis of Results Table 1 shows the results of our two systems compared to two baselines, DICT and DICTCORP, and the upper bound for the task.7 Since all of these systems provide an answer for every test instance, precision and recall are always the same. The upper bound for the best metric results from returning a single answer equal to the annotators’ most frequent substitute. The upper bound for the oot metric is obtained by returning the annotator’s most frequent substitute repeated 10 times. 7Details on the baselines and the upper bound can be found in (Mihalcea et al., 2010). that propose fewer than 10 substitutes. For each test item the oot score is calculated as follows: oot score = EscS, frequency(s E Ti) 3.75. Importantly, a system that pro126 System R best R oot Mode R Mode R SWAT-E 21.5 43.2 174.6 66.9 SWAT-S 18.9 36.6 98.0 79.0 DICT 24.3 50.3 44.0 73.5 DICTCORP 15.1 29.2 29.2 29.2 upper bound 40.6 100.0 405.9 100.0 Table 1: System performance using the two scoring metrics, best and oot. All test instances were answered, so precision equals recall. DICT and DICTCORP are the two baselines. Like the IRST2 system (Giuliano et al., 2007) submitted in the 2007 L</context>
</contexts>
<marker>Mihalcea, Sinha, McCarthy, 2010</marker>
<rawString>Rada Mihalcea, Ravi Sinha, and Diana McCarthy. 2010. Semeval-2010 Task 2: Cross-lingual lexical substitution. In Proceedings of the 5th International Workshop on Semantic Evaluations (SemEval-2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In International Conference on New Methods in Language Processing.</booktitle>
<contexts>
<context position="10173" citStr="Schmid, 1994" startWordPosition="1724" endWordPosition="1725">We allow all items from S that have the same value of n as the top ranked item and have a frequency at least 75% that of the most frequent item to be included in the best answer. Of the 1000 test instances, we provided a single “best” candidate 630 times, two candidates 253 times, three candidates 70 times, four candidates 30 times, and six candidates 17 times. (We never returned five candidates). 3.3 SWAT-S 3.3.1 Resources The SWAT-S system used both Google’s4 and Yahoo’s5 online translation tools, the Spanish section of the Web1T European 5-gram corpus, Roget’s online thesaurus, TreeTagger (Schmid, 1994) for 4http://translate.google.com/ 5http://babelfish.yahoo.com/ morphological analysis and both Google’s and Yahoo’s6 English-Spanish dictionaries. We formed a single Spanish-English dictionary by combining the translations found in both dictionaries. 3.3.2 Ranking substitutes To find the cross-lingual lexical substitutes for a target word in an English sentence, we first translate the sentence into Spanish and then use the syntagmatic coherence criterion on the translated Spanish sentence. In order to perform this monolingual Spanish lexical substitution, we need to be able to identify the ta</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In International Conference on New Methods in Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>