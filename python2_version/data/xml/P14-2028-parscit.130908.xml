<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002665">
<title confidence="0.983502">
Improved Iterative Correction for Distant Spelling Errors
</title>
<author confidence="0.649647">
Sergey Gubanov Irina Galinskaya Alexey Baytin
</author>
<affiliation confidence="0.500759">
Yandex
</affiliation>
<address confidence="0.643109">
16 Leo Tolstoy St., Moscow, 119021 Russia
</address>
<email confidence="0.743501">
{esgv,galinskaya,baytin}@yandex-team.ru
</email>
<sectionHeader confidence="0.993849" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9991603">
Noisy channel models, widely used in
modern spellers, cope with typical mis-
spellings, but do not work well with infre-
quent and difficult spelling errors. In this
paper, we have improved the noisy chan-
nel approach by iterative stochastic search
for the best correction. The proposed al-
gorithm allowed us to avoid local minima
problem and improve the F1 measure by
6.6% on distant spelling errors.
</bodyText>
<sectionHeader confidence="0.999377" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999972528301887">
A speller is an essential part of any program as-
sociated with text input and processing — e-mail
system, search engine, browser, form editor etc.
To detect and correct spelling errors, the state of
the art spelling correction systems use the noisy
channel approach (Kernighan et al., 1990; Mays
et al., 1991; Brill and Moore, 2000). Its models
are usually trained on large corpora and provide
high effectiveness in correction of typical errors
(most of which consist of 1-2 wrong characters per
word), but does not work well for complex (multi-
character) and infrequent errors.
In this paper, we improved effectiveness of
the noisy channel for the correction of com-
plex errors. In most cases, these are cogni-
tive errors in loan words (folsvagen → volkswa-
gen), names of drugs (vobemzin → wobenzym),
names of brands (scatcher → skechers), scientific
terms (heksagidron → hexahedron) and last names
(Shwartzneger → Schwarzenegger). In all these
cases, the misspelled word contains many errors
and the corresponding error model penalty cannot
be compensated by the LM weight of its proper
form. As a result, either the misspelled word it-
self, or the other (less complicated, more frequent)
misspelling of the same word wins the likelihood
race.
To compensate for this defect of the noisy chan-
nel, the iterative approach (Cucerzan and Brill,
2004) is typically used. The search for the best
variant is repeated several times, what allows cor-
recting rather complex errors, but does not com-
pletely solve the problem of falling into local min-
ima. To overcome this issue we suggest to con-
sider more correction hypotheses. For this pur-
pose we used a method based on the simulated
annealing algorithm. We experimentally demon-
strate that the proposed method outperforms the
baseline noisy channel and iterative spellers.
Many authors employ machine learning to build
rankers that compensate for the drawbacks of the
noisy channel model: (Whitelaw et al., 2009; Gao
et al., 2010). These techniques can be combined
with the proposed method by replacing posterior
probability of single correction in our method with
an estimate obtained via discriminative training
method.
In our work, we focus on isolated word-error
correction (Kukich, 1992), which, in a sense, is a
harder task, than multi-word correction, because
there is no context available for misspelled words.
For experiments we used single-word queries to a
commercial search engine.
</bodyText>
<sectionHeader confidence="0.992319" genericHeader="method">
2 Baseline speller
</sectionHeader>
<subsectionHeader confidence="0.99582">
2.1 Noisy channel spelling correction
</subsectionHeader>
<bodyText confidence="0.9993684">
Noisy channel is a probabilistic model that defines
posterior probability P(q0|q1) of q0 being the in-
tended word, given the observed word q1; for such
model, the optimal decision rule p is the follow-
ing:
</bodyText>
<equation confidence="0.999895333333333">
p(q1) = arg max P(q0|q1);
qo (1)
P(q0|q1) ∝ Pdigt(q0 → q1)PLM(q0),
</equation>
<bodyText confidence="0.960817">
where PLM is the source (language) model, and
Pdigt is the error model. Given P (q0|q1) defined,
to correct the word q1 we could iterate through
</bodyText>
<page confidence="0.972939">
168
</page>
<bodyText confidence="0.92975675">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 168–173,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
all ever-observed words, and choose the one, that
maximizes the posterior probability. However,
the practical considerations demand that we do
not rank the whole list of words, but instead
choose between a limited number of hypotheses
h1, ..., hK:
</bodyText>
<listItem confidence="0.906064">
1. Given q1, generate a set of hypotheses
h1, ..., hK, such that
</listItem>
<equation confidence="0.998591666666667">
K
P(q0 = hk|q1) ≈ 1; (2)
k=1
</equation>
<listItem confidence="0.726542">
2. Choose the hypothesis hk that maximizes
</listItem>
<equation confidence="0.856816">
P(q0 = hk|q1).
</equation>
<bodyText confidence="0.998554333333333">
If hypotheses constitute a major part of the poste-
rior probability mass, it is highly unlikely that the
intended word is not among them.
</bodyText>
<subsectionHeader confidence="0.999431">
2.2 Baseline speller setup
</subsectionHeader>
<bodyText confidence="0.999507588235294">
In baseline speller we use a substring-based error
model Pdi.t(q0 → q1) described in (Brill and
Moore, 2000), the error model training method
and the hypotheses generator are similar to (Duan
and Hsu, 2011).
For building language (PLM,) and error (Pdi.t,)
models, we use words collected from the 6-months
query log of a commercial search engine.
Hypotheses generator is based on A* beam
search in a trie of words, and yields K hy-
potheses hk, for which the noisy channel scores
Pdi.t(hk → q1)PLM(hk) are highest possible.
Hypotheses generator has high K-best recall (see
Section 4.2) — in 91.8% cases the correct hy-
pothesis is found when K = 30, which confirms
the assumption about covering almost all posterior
probability mass (see Equation 2).
</bodyText>
<sectionHeader confidence="0.999395" genericHeader="method">
3 Improvements for noisy channel
</sectionHeader>
<subsectionHeader confidence="0.589329">
spelling correction
</subsectionHeader>
<bodyText confidence="0.999876666666667">
While choosing arg max of the posterior probabil-
ity is an optimal decision rule in theory, in practice
it might not be optimal, due to limitations of the
language and error modeling. For example, vobe-
mzin is corrected to more frequent misspelling
vobenzin (instead of correct form wobenzym) by
the noisy channel, because Pdi.t(vobemzin →
wobenzym) is too low (see Table 1).
There have been attempts (Cucerzan and Brill,
2004) to apply other rules, which would over-
come limitations of language and error models
with compensating changes described further.
</bodyText>
<table confidence="0.567581666666667">
c − log Pdi.t − log PLM �
vobenzin 2.289 31.75 34.04
wobenzym 12.52 26.02 38.54
</table>
<tableCaption confidence="0.958006">
Table 1: Noisy-channel scores for two corrections
of vobemzin
</tableCaption>
<subsectionHeader confidence="0.998287">
3.1 Iterative correction
</subsectionHeader>
<bodyText confidence="0.999961615384615">
Iterative spelling correction with E iterations uses
standard noisy-channel to correct the query q re-
peatedly E times. It is motivated by the assump-
tion, that we are more likely to successfully correct
the query if we take several short steps instead of
one big step (Cucerzan and Brill, 2004) .
Iterative correction is hill climbing in the space
of possible corrections: on each iteration we make
a transition to the best point in the neighbourhood,
i.e. to correction, that has maximal posterior prob-
ability P(c|q). As any local search method, itera-
tive correction is prone to local minima, stopping
before reaching the correct word.
</bodyText>
<subsectionHeader confidence="0.999885">
3.2 Stochastic iterative correction
</subsectionHeader>
<bodyText confidence="0.98825337037037">
A common method of avoiding local minima in
optimization is the simulated annealing algorithm,
key ideas from which can be adapted for spelling
correction task. In this section we propose such an
adaptation. Consider: we do not always transition
deterministically to the next best correction, but
instead transition randomly to a (potentially any)
correction with transition probability being equal
to the posterior P(ci|ci_1), where ci_1 is the cor-
rection we transition from, ci is the correction we
transition to, and P(·|·) is defined by Equation 1.
Iterative correction then turns into a random walk:
we start at word c0 = q and stop after E ran-
dom steps at some word cE, which becomes our
answer.
To turn random walk into deterministic spelling
correction algorithm, we de-randomize it, using
the following transformation. Described random
walk defines, for each word w, a probability
P(cE = w|q) of ending up in w after starting a
walk from the initial query q. With that probability
defined, our correction algorithm is the following:
given query q, pick c = arg maxc_, P(cE|q) as a
correction.
Probability of getting from c0 = q to some
cE = c is a sum, over all possible paths, of prob-
abilities of getting from q to c via specific path
</bodyText>
<page confidence="0.914981">
169
</page>
<equation confidence="0.999348625">
q = c0 → c1 → ... → cE−1 → cE = c:
XP(cE|c0) = YE P(ci|ci−1), (3)
c1EW i=1
...
cE−1EW
Pdist(ci → ci−1)PLM(ci)
P(ci|ci−1) = (4)
Pobserve (ci−1)
</equation>
<bodyText confidence="0.999006666666667">
where W is the set of all possible words, and
Pobserve(w) is the probability of observing w as
a query in the noisy-channel model.
Example: if we start a random walk from vobe-
mzin and make 3 steps, we most probably will end
up in the correct form wobenzym with P = 0.361.
A few of the most probable random walk paths
are shown in Table 2. Note, that despite the fact
that most probable path does not lead to the cor-
rect word, many other paths to wobenzym sum up
to 0.361, which is greater than probability of any
other word. Also note, that the method works only
because multiple misspellings of the same word
are presented in our model; for related research
see (Choudhury et al., 2007).
</bodyText>
<table confidence="0.68899625">
C0 --+ C1 --+ C2 --+ C3 P
vobemzin--+vobenzin--+vobenzin--+vobenzin 0.074
vobemzin--+vobenzim--+wobenzym--+wobenzym 0.065
vobemzin--+vobenzin--+vobenzim--+vobenzim 0.052
vobemzin--+vobenzim--+vobenzim--+wobenzym 0.034
vobemzin--+wobenzym--+wobenzym--+wobenzym 0.031
vobemzin--+wobenzim--+wobenzym--+wobenzym 0.028
vobemzin--+wobenzyn--+wobenzym--+wobenzym 0.022
</table>
<tableCaption confidence="0.596412">
Table 2: Most probable random walk paths start-
</tableCaption>
<bodyText confidence="0.935352142857143">
ing from c0 = q = vobemzin (the correct form is
in bold).
Also note, that while Equation 3 uses noisy-
channel posteriors, the method can use an arbitrary
discriminative model, for example the one from
(Gao et al., 2010), and benefit from a more accu-
rate posterior estimate.
</bodyText>
<subsectionHeader confidence="0.999027">
3.3 Additional heuristics
</subsectionHeader>
<bodyText confidence="0.999966733333333">
This section describes some common heuristic im-
provements, that, where possible, were applied
both to the baseline methods and to the proposed
algorithm.
Basic building block of every mentioned algo-
rithm is one-step noisy-channel correction. Each
basic correction proceeds as described in Sec-
tion 2.1: a small number of hypotheses h1, ..., hK
is generated for the query q, hypotheses are scored,
and scores are recomputed into normalized pos-
terior probabilities (see Equation 5). Posterior
probabilities are then either used to pick the best
correction (in baseline and simple iterative cor-
rection), or are accumulated to later compute the
score defined by Equation 3.
</bodyText>
<equation confidence="0.997111666666667">
score(hi) = Pdist(hi → q)λPLM(hi)
.P (hi|q) = score(hi) XK score(hj) (5)
j=1
</equation>
<bodyText confidence="0.999828928571428">
A standard log-linear weighing trick was ap-
plied to noisy-channel model components, see e.g.
(Whitelaw et al., 2009). A is the parameter that
controls the trade-off between precision and recall
(see Section 4.2) by emphasizing the importance
of either the high frequency of the correction or its
proximity to the query.
We have also found, that resulting posterior
probabilities emphasize the best hypothesis too
much: best hypothesis gets almost all probability
mass and other hypotheses get none. To compen-
sate for that, posteriors were smoothed by raising
each probability to some power -y &lt; 1 and re-
normalizing them afterward:
</bodyText>
<equation confidence="0.9933915">
Psmooth(hi|q) = P(hi|q)γ. XK P(hj|q)γ. (6)
j=1
</equation>
<bodyText confidence="0.999969769230769">
In a sense, -y is like temperature parameter in sim-
ulated annealing – it controls the entropy of the
walk and the final probability distribution. Unlike
in simulated annealing, we fix -y for all iterations
of the algorithm.
Finally, if posterior probability of the best hy-
pothesis was lower than threshold α, then the orig-
inal query q was used as the spell-checker output.
(Posterior is defined by Equation 6 for the baseline
and simple iterative methods and by Equations 3
and 6 for the proposed method). Parameter α con-
trols precision/recall trade-off (as well as A men-
tioned above).
</bodyText>
<sectionHeader confidence="0.999718" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.964761">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999616833333333">
To evaluate the proposed algorithm we have col-
lected two datasets. Both datasets were randomly
sampled from single-word user queries from the
1-week query log of a commercial search en-
gine. We annotated them with the help of pro-
fessional analyst. The difference between datasets
</bodyText>
<page confidence="0.991741">
170
</page>
<bodyText confidence="0.999895571428571">
is that one of them contained only queries with
low search performance: for which the number
of documents retrieved by the search engine was
less than a fixed threshold (we will address it as
the ”hard” dataset), while the other dataset had
no such restrictions (we will call it ”common”).
Dataset statistics are shown in Table 3.
</bodyText>
<table confidence="0.99672">
Dataset Queries Misspelled Avg. − log Pdiet
Common 2240 224 (10%) 5.98
Hard 2542 1484 (58%) 9.23
</table>
<tableCaption confidence="0.999942">
Table 3: Evaluation datasets.
</tableCaption>
<bodyText confidence="0.99981675">
Increased average error model score and er-
ror rate of ”common” dataset compared to ”hard”
shows, that we have indeed managed to collect
hard-to-correct queries in the ”hard” dataset.
</bodyText>
<subsectionHeader confidence="0.958459">
4.2 Experimental results
</subsectionHeader>
<bodyText confidence="0.999620583333333">
First of all, we evaluated the recall of hypothe-
ses generator using K-best recall — the number of
correct spelling corrections for misspelled queries
among K hypotheses divided by the total number
of misspelled queries in the test set. Resulting re-
call with K = 30 is 91.8% on ”hard” and 98.6%
on ”common”.
Next, three spelling correction methods were
tested: noisy channel, iterative correction and our
method (stochastic iterative correction).
For evaluation of spelling correction quality, we
use the following metrics:
</bodyText>
<listItem confidence="0.96678375">
• Precision: The number of correct spelling
corrections for misspelled words generated
by the system divided by the total number of
corrections generated by the system;
• Recall: The number of correct spelling cor-
rections for misspelled words generated by
the system divided by the total number of
misspelled words in the test set;
</listItem>
<bodyText confidence="0.999542666666667">
For hypotheses generator, K = 30 was fixed: re-
call of 91.8% was considered big enough. Pre-
cision/recall tradeoff parameters A and α (they
are applicable to each method, including baseline)
were iterated by the grid (0.2, 0.25, 0.3, ...,1.5) x
(0, 0.025,0.05,..., 1.0), and E (applicable to it-
erative and our method) and γ (just our method)
were iterated by the grid (2, 3, 4, 5, 7,10) x
(0.1, 0.15, ...1.0); for each set of parameters, pre-
cision and recall were measured on both datasets.
Pareto frontiers for precision and recall are shown
in Figures 1 and 2.
</bodyText>
<figureCaption confidence="0.974752">
Figure 1: Precision/recall Pareto frontiers on
”hard” dataset
Figure 2: Precision/recall Pareto frontiers on
”common” dataset
</figureCaption>
<bodyText confidence="0.99963035">
We were not able to reproduce superior perfor-
mance of the iterative method over the noisy chan-
nel, reported by (Cucerzan and Brill, 2004). Sup-
posedly, it is because the iterative method bene-
fits primarily from the sequential application of
split/join operations altering query decomposition
into words; since we are considering only one-
word queries, such decomposition does not matter.
On the ”hard” dataset the performance of the
noisy channel and the iterative methods is infe-
rior to our proposed method, see Figure 1. We
tested all three methods on the ”common” dataset
as well to evaluate if our handling of hard cases
affects the performance of our approach on the
common cases of spelling error. Our method per-
forms well on the common cases as well, as Fig-
ure 2 shows. The performance comparison for
the ”common” dataset shows comparable perfor-
mance for all considered methods.
Noisy channel and iterative methods’ frontiers
</bodyText>
<page confidence="0.996115">
171
</page>
<bodyText confidence="0.99938625">
are considerably inferior to the proposed method
on ”hard” dataset, which means that our method
works better. The results on ”common” dataset
show, that the proposed method doesn’t work
worse than baseline.
Next, we optimized parameters for each method
and each dataset separately to achieve the highest
F1 measure. Results are shown in Tables 4 and 5.
We can see, that, given the proper tuning, our
method can work better on any dataset (but it can-
not achieve the best performance on both datasets
at once). See Tables 4 and 5 for details.
</bodyText>
<table confidence="0.9993775">
Method A α 7 E Fl
Noisy channel 0.6 0.1 - - 55.8
Iterative 0.6 0.1 - 2 55.9
Stochastic iterative 0.9 0.2 0.35 3 62.5
</table>
<tableCaption confidence="0.962392">
Table 4: Best parameters and F1 on ”hard” dataset
</tableCaption>
<table confidence="0.9996975">
Method A α 7 E Fl
Noisy channel 0.75 0.225 - - 62.06
Iterative 0.8 0.275 - 2 63.15
Stochastic iterative 1.2 0.4 0.35 3 63.9
</table>
<tableCaption confidence="0.932504">
Table 5: Best parameters and F1 on ”common”
</tableCaption>
<bodyText confidence="0.989397">
dataset
Next, each parameter was separately iterated
(by a coarser grid); initial parameters for each
method were taken from Table 4. Such iteration
serves two purposes: to show the influence of pa-
rameters on algorithm performance, and to show
differences between datasets: in such setup pa-
rameters are virtually tuned using ”hard” dataset
and evaluated using ”common” dataset. Results
are shown in Table 6.
The proposed method is able to successfully
correct distant spelling errors with edit distance of
3 characters (see Table 7).
However, if our method is applied to shorter
and more frequent queries (as opposed to ”hard”
dataset), it tends to suggest frequent words as
false-positive corrections (for example, grid is cor-
rected to creed – Assassin’s Creed is popular video
game). As can be seen in Table 5, in order to fix
that, algorithm parameters need to be tuned more
towards precision.
</bodyText>
<sectionHeader confidence="0.994399" genericHeader="conclusions">
5 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.99195825">
In this paper we introduced the stochastic itera-
tive correction method for spell check corrections.
Our experimental evaluation showed that the pro-
posed method improved the performance of popu-
</bodyText>
<table confidence="0.9998629">
Fl, common Fl, hard
N.ch. It. Our N.ch. It. Our
A = 0.5 45.3 45.9 37.5 54.9 54.8 50.0
0.6 49.9 50.5 41.5 55.8 55.9 56.6
0.7 50.4 50.4 44.1 54.5 55.1 59.6
0.8 52.7 52.7 46.0 52.6 53.0 61.5
0.9 53.5 53.5 49.3 50.3 50.6 62.5
1.0 55.4 55.0 50.9 47.0 47.3 61.8
1.1 53.7 53.4 52.7 44.3 44.6 60.8
1.2 52.5 52.5 53.7 41.9 42.3 58.8
1.3 52.2 52.6 54.6 39.5 39.9 56.6
1.4 51.4 51.8 55.0 36.8 37.3 53.6
α = 0 41.0 41.5 33.0 52.9 53.1 58.3
0.1 49.9 50.6 35.6 55.8 55.9 59.7
0.15 59.4 59.8 43.2 55.8 55.6 61.6
0.2 60.8 61.3 49.4 51.0 51.0 62.5
0.25 54.0 54.0 54.9 46.3 46.3 61.1
0.3 46.3 46.3 57.3 39.2 39.2 58.4
0.4 25.8 25.8 53.9 22.3 22.3 50.3
E = 2 50.6 53.6 55.9 60.4
3 50.6 49.4 55.9 62.5
4 50.6 46.4 55.9 62.1
5 50.6 46.7 55.9 60.1
7 = 0.1 10.1 6.0
0.2 49.4 51.5
0.3 51.4 61.4
0.35 49.4 62.5
0.4 47.5 62.0
0.45 45.8 60.8
0.5 45.2 60.3
</table>
<tableCaption confidence="0.962201333333333">
Table 6: Per-coordinate iteration of parameters
from Table 4; per-method maximum is shown in
italic, per-dataset in bold
</tableCaption>
<table confidence="0.821263833333333">
Query Noisy channel Proposed method
akwamarin akvamarin aquamarine
maccartni maccartni mccartney
ariflaim ariflaim oriflame
epika epica replica
grid grid creed
</table>
<tableCaption confidence="0.893827">
Table 7: Correction examples for the noisy chan-
</tableCaption>
<bodyText confidence="0.985641352941176">
nel and the proposed method.
lar spelling correction approach – the noisy chan-
nel model – in the correction of difficult spelling
errors. We showed how to eliminate the local min-
ima issue of simulated annealing and proposed a
technique to make our algorithm deterministic.
The experiments conducted on the specialized
datasets have shown that our method significantly
improves the performance of the correction of
hard spelling errors (by 6.6% F1) while maintain-
ing good performance on common spelling errors.
In continuation of the work we are considering
to expand the method to correct errors in multi-
word queries, extend the method to work with dis-
criminative models, and use a query performance
prediction method, which tells for a query whether
our algorithm needs to be applied.
</bodyText>
<page confidence="0.996977">
172
</page>
<sectionHeader confidence="0.998316" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999568695652174">
Eric Brill and Robert C Moore. 2000. An improved er-
ror model for noisy channel spelling correction. In
Proceedings of the 38th Annual Meeting on Associa-
tion for Computational Linguistics, pages 286–293.
Association for Computational Linguistics.
Monojit Choudhury, Markose Thomas, Animesh
Mukherjee, Anupam Basu, and Niloy Ganguly.
2007. How difficult is it to develop a perfect spell-
checker? a cross-linguistic analysis through com-
plex network approach. In Proceedings of the sec-
ond workshop on TextGraphs: Graph-based algo-
rithms for natural language processing, pages 81–
88.
Silviu Cucerzan and Eric Brill. 2004. Spelling correc-
tion as an iterative process that exploits the collec-
tive knowledge of web users. In EMNLP, volume 4,
pages 293–300.
Huizhong Duan and Bo-June Paul Hsu. 2011. On-
line spelling correction for query completion. In
Proceedings of the 20th international conference on
World wide web, pages 117–126. ACM.
Jianfeng Gao, Xiaolong Li, Daniel Micol, Chris Quirk,
and Xu Sun. 2010. A large scale ranker-based sys-
tem for search query spelling correction. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics, pages 358–366. Associ-
ation for Computational Linguistics.
Mark D Kernighan, Kenneth W Church, and William A
Gale. 1990. A spelling correction program based on
a noisy channel model. In Proceedings of the 13th
conference on Computational linguistics-Volume 2,
pages 205–210. Association for Computational Lin-
guistics.
Karen Kukich. 1992. Techniques for automatically
correcting words in text. ACM Computing Surveys
(CSUR), 24(4):377–439.
Eric Mays, Fred J Damerau, and Robert L Mercer.
1991. Context based spelling correction. Informa-
tion Processing &amp; Management, 27(5):517–522.
Casey Whitelaw, Ben Hutchinson, Grace Y Chung, and
Gerard Ellis. 2009. Using the web for language
independent spellchecking and autocorrection. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
2-Volume 2, pages 890–899. Association for Com-
putational Linguistics.
</reference>
<page confidence="0.9991">
173
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.377657">
<title confidence="0.997687">Improved Iterative Correction for Distant Spelling Errors</title>
<author confidence="0.73771">Sergey Gubanov Irina Galinskaya Alexey Baytin</author>
<affiliation confidence="0.407672">Yandex</affiliation>
<address confidence="0.934236">16 Leo Tolstoy St., Moscow, 119021</address>
<abstract confidence="0.996038090909091">Noisy channel models, widely used in modern spellers, cope with typical misspellings, but do not work well with infrequent and difficult spelling errors. In this paper, we have improved the noisy channel approach by iterative stochastic search for the best correction. The proposed algorithm allowed us to avoid local minima and improve the by 6.6% on distant spelling errors.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Robert C Moore</author>
</authors>
<title>An improved error model for noisy channel spelling correction.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>286--293</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="944" citStr="Brill and Moore, 2000" startWordPosition="143" endWordPosition="146">nt and difficult spelling errors. In this paper, we have improved the noisy channel approach by iterative stochastic search for the best correction. The proposed algorithm allowed us to avoid local minima problem and improve the F1 measure by 6.6% on distant spelling errors. 1 Introduction A speller is an essential part of any program associated with text input and processing — e-mail system, search engine, browser, form editor etc. To detect and correct spelling errors, the state of the art spelling correction systems use the noisy channel approach (Kernighan et al., 1990; Mays et al., 1991; Brill and Moore, 2000). Its models are usually trained on large corpora and provide high effectiveness in correction of typical errors (most of which consist of 1-2 wrong characters per word), but does not work well for complex (multicharacter) and infrequent errors. In this paper, we improved effectiveness of the noisy channel for the correction of complex errors. In most cases, these are cognitive errors in loan words (folsvagen → volkswagen), names of drugs (vobemzin → wobenzym), names of brands (scatcher → skechers), scientific terms (heksagidron → hexahedron) and last names (Shwartzneger → Schwarzenegger). In </context>
<context position="4402" citStr="Brill and Moore, 2000" startWordPosition="702" endWordPosition="705">maximizes the posterior probability. However, the practical considerations demand that we do not rank the whole list of words, but instead choose between a limited number of hypotheses h1, ..., hK: 1. Given q1, generate a set of hypotheses h1, ..., hK, such that K P(q0 = hk|q1) ≈ 1; (2) k=1 2. Choose the hypothesis hk that maximizes P(q0 = hk|q1). If hypotheses constitute a major part of the posterior probability mass, it is highly unlikely that the intended word is not among them. 2.2 Baseline speller setup In baseline speller we use a substring-based error model Pdi.t(q0 → q1) described in (Brill and Moore, 2000), the error model training method and the hypotheses generator are similar to (Duan and Hsu, 2011). For building language (PLM,) and error (Pdi.t,) models, we use words collected from the 6-months query log of a commercial search engine. Hypotheses generator is based on A* beam search in a trie of words, and yields K hypotheses hk, for which the noisy channel scores Pdi.t(hk → q1)PLM(hk) are highest possible. Hypotheses generator has high K-best recall (see Section 4.2) — in 91.8% cases the correct hypothesis is found when K = 30, which confirms the assumption about covering almost all posteri</context>
</contexts>
<marker>Brill, Moore, 2000</marker>
<rawString>Eric Brill and Robert C Moore. 2000. An improved error model for noisy channel spelling correction. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 286–293. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monojit Choudhury</author>
<author>Markose Thomas</author>
<author>Animesh Mukherjee</author>
<author>Anupam Basu</author>
<author>Niloy Ganguly</author>
</authors>
<title>How difficult is it to develop a perfect spellchecker? a cross-linguistic analysis through complex network approach.</title>
<date>2007</date>
<booktitle>In Proceedings of the</booktitle>
<pages>81--88</pages>
<contexts>
<context position="8566" citStr="Choudhury et al., 2007" startWordPosition="1413" endWordPosition="1416"> probability of observing w as a query in the noisy-channel model. Example: if we start a random walk from vobemzin and make 3 steps, we most probably will end up in the correct form wobenzym with P = 0.361. A few of the most probable random walk paths are shown in Table 2. Note, that despite the fact that most probable path does not lead to the correct word, many other paths to wobenzym sum up to 0.361, which is greater than probability of any other word. Also note, that the method works only because multiple misspellings of the same word are presented in our model; for related research see (Choudhury et al., 2007). C0 --+ C1 --+ C2 --+ C3 P vobemzin--+vobenzin--+vobenzin--+vobenzin 0.074 vobemzin--+vobenzim--+wobenzym--+wobenzym 0.065 vobemzin--+vobenzin--+vobenzim--+vobenzim 0.052 vobemzin--+vobenzim--+vobenzim--+wobenzym 0.034 vobemzin--+wobenzym--+wobenzym--+wobenzym 0.031 vobemzin--+wobenzim--+wobenzym--+wobenzym 0.028 vobemzin--+wobenzyn--+wobenzym--+wobenzym 0.022 Table 2: Most probable random walk paths starting from c0 = q = vobemzin (the correct form is in bold). Also note, that while Equation 3 uses noisychannel posteriors, the method can use an arbitrary discriminative model, for example the</context>
</contexts>
<marker>Choudhury, Thomas, Mukherjee, Basu, Ganguly, 2007</marker>
<rawString>Monojit Choudhury, Markose Thomas, Animesh Mukherjee, Anupam Basu, and Niloy Ganguly. 2007. How difficult is it to develop a perfect spellchecker? a cross-linguistic analysis through complex network approach. In Proceedings of the second workshop on TextGraphs: Graph-based algorithms for natural language processing, pages 81– 88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
<author>Eric Brill</author>
</authors>
<title>Spelling correction as an iterative process that exploits the collective knowledge of web users.</title>
<date>2004</date>
<booktitle>In EMNLP,</booktitle>
<volume>4</volume>
<pages>293--300</pages>
<contexts>
<context position="1954" citStr="Cucerzan and Brill, 2004" startWordPosition="306" endWordPosition="309">rs in loan words (folsvagen → volkswagen), names of drugs (vobemzin → wobenzym), names of brands (scatcher → skechers), scientific terms (heksagidron → hexahedron) and last names (Shwartzneger → Schwarzenegger). In all these cases, the misspelled word contains many errors and the corresponding error model penalty cannot be compensated by the LM weight of its proper form. As a result, either the misspelled word itself, or the other (less complicated, more frequent) misspelling of the same word wins the likelihood race. To compensate for this defect of the noisy channel, the iterative approach (Cucerzan and Brill, 2004) is typically used. The search for the best variant is repeated several times, what allows correcting rather complex errors, but does not completely solve the problem of falling into local minima. To overcome this issue we suggest to consider more correction hypotheses. For this purpose we used a method based on the simulated annealing algorithm. We experimentally demonstrate that the proposed method outperforms the baseline noisy channel and iterative spellers. Many authors employ machine learning to build rankers that compensate for the drawbacks of the noisy channel model: (Whitelaw et al.,</context>
<context position="5518" citStr="Cucerzan and Brill, 2004" startWordPosition="885" endWordPosition="888"> the correct hypothesis is found when K = 30, which confirms the assumption about covering almost all posterior probability mass (see Equation 2). 3 Improvements for noisy channel spelling correction While choosing arg max of the posterior probability is an optimal decision rule in theory, in practice it might not be optimal, due to limitations of the language and error modeling. For example, vobemzin is corrected to more frequent misspelling vobenzin (instead of correct form wobenzym) by the noisy channel, because Pdi.t(vobemzin → wobenzym) is too low (see Table 1). There have been attempts (Cucerzan and Brill, 2004) to apply other rules, which would overcome limitations of language and error models with compensating changes described further. c − log Pdi.t − log PLM � vobenzin 2.289 31.75 34.04 wobenzym 12.52 26.02 38.54 Table 1: Noisy-channel scores for two corrections of vobemzin 3.1 Iterative correction Iterative spelling correction with E iterations uses standard noisy-channel to correct the query q repeatedly E times. It is motivated by the assumption, that we are more likely to successfully correct the query if we take several short steps instead of one big step (Cucerzan and Brill, 2004) . Iterati</context>
<context position="13936" citStr="Cucerzan and Brill, 2004" startWordPosition="2254" endWordPosition="2257">ted by the grid (0.2, 0.25, 0.3, ...,1.5) x (0, 0.025,0.05,..., 1.0), and E (applicable to iterative and our method) and γ (just our method) were iterated by the grid (2, 3, 4, 5, 7,10) x (0.1, 0.15, ...1.0); for each set of parameters, precision and recall were measured on both datasets. Pareto frontiers for precision and recall are shown in Figures 1 and 2. Figure 1: Precision/recall Pareto frontiers on ”hard” dataset Figure 2: Precision/recall Pareto frontiers on ”common” dataset We were not able to reproduce superior performance of the iterative method over the noisy channel, reported by (Cucerzan and Brill, 2004). Supposedly, it is because the iterative method benefits primarily from the sequential application of split/join operations altering query decomposition into words; since we are considering only oneword queries, such decomposition does not matter. On the ”hard” dataset the performance of the noisy channel and the iterative methods is inferior to our proposed method, see Figure 1. We tested all three methods on the ”common” dataset as well to evaluate if our handling of hard cases affects the performance of our approach on the common cases of spelling error. Our method performs well on the com</context>
</contexts>
<marker>Cucerzan, Brill, 2004</marker>
<rawString>Silviu Cucerzan and Eric Brill. 2004. Spelling correction as an iterative process that exploits the collective knowledge of web users. In EMNLP, volume 4, pages 293–300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huizhong Duan</author>
<author>Bo-June Paul Hsu</author>
</authors>
<title>Online spelling correction for query completion.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th international conference on World wide web,</booktitle>
<pages>117--126</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="4500" citStr="Duan and Hsu, 2011" startWordPosition="718" endWordPosition="721"> the whole list of words, but instead choose between a limited number of hypotheses h1, ..., hK: 1. Given q1, generate a set of hypotheses h1, ..., hK, such that K P(q0 = hk|q1) ≈ 1; (2) k=1 2. Choose the hypothesis hk that maximizes P(q0 = hk|q1). If hypotheses constitute a major part of the posterior probability mass, it is highly unlikely that the intended word is not among them. 2.2 Baseline speller setup In baseline speller we use a substring-based error model Pdi.t(q0 → q1) described in (Brill and Moore, 2000), the error model training method and the hypotheses generator are similar to (Duan and Hsu, 2011). For building language (PLM,) and error (Pdi.t,) models, we use words collected from the 6-months query log of a commercial search engine. Hypotheses generator is based on A* beam search in a trie of words, and yields K hypotheses hk, for which the noisy channel scores Pdi.t(hk → q1)PLM(hk) are highest possible. Hypotheses generator has high K-best recall (see Section 4.2) — in 91.8% cases the correct hypothesis is found when K = 30, which confirms the assumption about covering almost all posterior probability mass (see Equation 2). 3 Improvements for noisy channel spelling correction While c</context>
</contexts>
<marker>Duan, Hsu, 2011</marker>
<rawString>Huizhong Duan and Bo-June Paul Hsu. 2011. Online spelling correction for query completion. In Proceedings of the 20th international conference on World wide web, pages 117–126. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Xiaolong Li</author>
<author>Daniel Micol</author>
<author>Chris Quirk</author>
<author>Xu Sun</author>
</authors>
<title>A large scale ranker-based system for search query spelling correction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>358--366</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2578" citStr="Gao et al., 2010" startWordPosition="408" endWordPosition="411">ically used. The search for the best variant is repeated several times, what allows correcting rather complex errors, but does not completely solve the problem of falling into local minima. To overcome this issue we suggest to consider more correction hypotheses. For this purpose we used a method based on the simulated annealing algorithm. We experimentally demonstrate that the proposed method outperforms the baseline noisy channel and iterative spellers. Many authors employ machine learning to build rankers that compensate for the drawbacks of the noisy channel model: (Whitelaw et al., 2009; Gao et al., 2010). These techniques can be combined with the proposed method by replacing posterior probability of single correction in our method with an estimate obtained via discriminative training method. In our work, we focus on isolated word-error correction (Kukich, 1992), which, in a sense, is a harder task, than multi-word correction, because there is no context available for misspelled words. For experiments we used single-word queries to a commercial search engine. 2 Baseline speller 2.1 Noisy channel spelling correction Noisy channel is a probabilistic model that defines posterior probability P(q0|</context>
<context position="9194" citStr="Gao et al., 2010" startWordPosition="1483" endWordPosition="1486"> --+ C2 --+ C3 P vobemzin--+vobenzin--+vobenzin--+vobenzin 0.074 vobemzin--+vobenzim--+wobenzym--+wobenzym 0.065 vobemzin--+vobenzin--+vobenzim--+vobenzim 0.052 vobemzin--+vobenzim--+vobenzim--+wobenzym 0.034 vobemzin--+wobenzym--+wobenzym--+wobenzym 0.031 vobemzin--+wobenzim--+wobenzym--+wobenzym 0.028 vobemzin--+wobenzyn--+wobenzym--+wobenzym 0.022 Table 2: Most probable random walk paths starting from c0 = q = vobemzin (the correct form is in bold). Also note, that while Equation 3 uses noisychannel posteriors, the method can use an arbitrary discriminative model, for example the one from (Gao et al., 2010), and benefit from a more accurate posterior estimate. 3.3 Additional heuristics This section describes some common heuristic improvements, that, where possible, were applied both to the baseline methods and to the proposed algorithm. Basic building block of every mentioned algorithm is one-step noisy-channel correction. Each basic correction proceeds as described in Section 2.1: a small number of hypotheses h1, ..., hK is generated for the query q, hypotheses are scored, and scores are recomputed into normalized posterior probabilities (see Equation 5). Posterior probabilities are then either</context>
</contexts>
<marker>Gao, Li, Micol, Quirk, Sun, 2010</marker>
<rawString>Jianfeng Gao, Xiaolong Li, Daniel Micol, Chris Quirk, and Xu Sun. 2010. A large scale ranker-based system for search query spelling correction. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 358–366. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark D Kernighan</author>
<author>Kenneth W Church</author>
<author>William A Gale</author>
</authors>
<title>A spelling correction program based on a noisy channel model.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th conference on Computational linguistics-Volume 2,</booktitle>
<pages>205--210</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="901" citStr="Kernighan et al., 1990" startWordPosition="135" endWordPosition="138">ellings, but do not work well with infrequent and difficult spelling errors. In this paper, we have improved the noisy channel approach by iterative stochastic search for the best correction. The proposed algorithm allowed us to avoid local minima problem and improve the F1 measure by 6.6% on distant spelling errors. 1 Introduction A speller is an essential part of any program associated with text input and processing — e-mail system, search engine, browser, form editor etc. To detect and correct spelling errors, the state of the art spelling correction systems use the noisy channel approach (Kernighan et al., 1990; Mays et al., 1991; Brill and Moore, 2000). Its models are usually trained on large corpora and provide high effectiveness in correction of typical errors (most of which consist of 1-2 wrong characters per word), but does not work well for complex (multicharacter) and infrequent errors. In this paper, we improved effectiveness of the noisy channel for the correction of complex errors. In most cases, these are cognitive errors in loan words (folsvagen → volkswagen), names of drugs (vobemzin → wobenzym), names of brands (scatcher → skechers), scientific terms (heksagidron → hexahedron) and last</context>
</contexts>
<marker>Kernighan, Church, Gale, 1990</marker>
<rawString>Mark D Kernighan, Kenneth W Church, and William A Gale. 1990. A spelling correction program based on a noisy channel model. In Proceedings of the 13th conference on Computational linguistics-Volume 2, pages 205–210. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Kukich</author>
</authors>
<title>Techniques for automatically correcting words in text.</title>
<date>1992</date>
<journal>ACM Computing Surveys (CSUR),</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="2840" citStr="Kukich, 1992" startWordPosition="448" endWordPosition="449">r this purpose we used a method based on the simulated annealing algorithm. We experimentally demonstrate that the proposed method outperforms the baseline noisy channel and iterative spellers. Many authors employ machine learning to build rankers that compensate for the drawbacks of the noisy channel model: (Whitelaw et al., 2009; Gao et al., 2010). These techniques can be combined with the proposed method by replacing posterior probability of single correction in our method with an estimate obtained via discriminative training method. In our work, we focus on isolated word-error correction (Kukich, 1992), which, in a sense, is a harder task, than multi-word correction, because there is no context available for misspelled words. For experiments we used single-word queries to a commercial search engine. 2 Baseline speller 2.1 Noisy channel spelling correction Noisy channel is a probabilistic model that defines posterior probability P(q0|q1) of q0 being the intended word, given the observed word q1; for such model, the optimal decision rule p is the following: p(q1) = arg max P(q0|q1); qo (1) P(q0|q1) ∝ Pdigt(q0 → q1)PLM(q0), where PLM is the source (language) model, and Pdigt is the error model</context>
</contexts>
<marker>Kukich, 1992</marker>
<rawString>Karen Kukich. 1992. Techniques for automatically correcting words in text. ACM Computing Surveys (CSUR), 24(4):377–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Mays</author>
<author>Fred J Damerau</author>
<author>Robert L Mercer</author>
</authors>
<title>Context based spelling correction.</title>
<date>1991</date>
<journal>Information Processing &amp; Management,</journal>
<volume>27</volume>
<issue>5</issue>
<contexts>
<context position="920" citStr="Mays et al., 1991" startWordPosition="139" endWordPosition="142"> well with infrequent and difficult spelling errors. In this paper, we have improved the noisy channel approach by iterative stochastic search for the best correction. The proposed algorithm allowed us to avoid local minima problem and improve the F1 measure by 6.6% on distant spelling errors. 1 Introduction A speller is an essential part of any program associated with text input and processing — e-mail system, search engine, browser, form editor etc. To detect and correct spelling errors, the state of the art spelling correction systems use the noisy channel approach (Kernighan et al., 1990; Mays et al., 1991; Brill and Moore, 2000). Its models are usually trained on large corpora and provide high effectiveness in correction of typical errors (most of which consist of 1-2 wrong characters per word), but does not work well for complex (multicharacter) and infrequent errors. In this paper, we improved effectiveness of the noisy channel for the correction of complex errors. In most cases, these are cognitive errors in loan words (folsvagen → volkswagen), names of drugs (vobemzin → wobenzym), names of brands (scatcher → skechers), scientific terms (heksagidron → hexahedron) and last names (Shwartznege</context>
</contexts>
<marker>Mays, Damerau, Mercer, 1991</marker>
<rawString>Eric Mays, Fred J Damerau, and Robert L Mercer. 1991. Context based spelling correction. Information Processing &amp; Management, 27(5):517–522.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Casey Whitelaw</author>
<author>Ben Hutchinson</author>
<author>Grace Y Chung</author>
<author>Gerard Ellis</author>
</authors>
<title>Using the web for language independent spellchecking and autocorrection.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>2</volume>
<pages>890--899</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2559" citStr="Whitelaw et al., 2009" startWordPosition="404" endWordPosition="407">and Brill, 2004) is typically used. The search for the best variant is repeated several times, what allows correcting rather complex errors, but does not completely solve the problem of falling into local minima. To overcome this issue we suggest to consider more correction hypotheses. For this purpose we used a method based on the simulated annealing algorithm. We experimentally demonstrate that the proposed method outperforms the baseline noisy channel and iterative spellers. Many authors employ machine learning to build rankers that compensate for the drawbacks of the noisy channel model: (Whitelaw et al., 2009; Gao et al., 2010). These techniques can be combined with the proposed method by replacing posterior probability of single correction in our method with an estimate obtained via discriminative training method. In our work, we focus on isolated word-error correction (Kukich, 1992), which, in a sense, is a harder task, than multi-word correction, because there is no context available for misspelled words. For experiments we used single-word queries to a commercial search engine. 2 Baseline speller 2.1 Noisy channel spelling correction Noisy channel is a probabilistic model that defines posterio</context>
<context position="10137" citStr="Whitelaw et al., 2009" startWordPosition="1630" endWordPosition="1633">. Each basic correction proceeds as described in Section 2.1: a small number of hypotheses h1, ..., hK is generated for the query q, hypotheses are scored, and scores are recomputed into normalized posterior probabilities (see Equation 5). Posterior probabilities are then either used to pick the best correction (in baseline and simple iterative correction), or are accumulated to later compute the score defined by Equation 3. score(hi) = Pdist(hi → q)λPLM(hi) .P (hi|q) = score(hi) XK score(hj) (5) j=1 A standard log-linear weighing trick was applied to noisy-channel model components, see e.g. (Whitelaw et al., 2009). A is the parameter that controls the trade-off between precision and recall (see Section 4.2) by emphasizing the importance of either the high frequency of the correction or its proximity to the query. We have also found, that resulting posterior probabilities emphasize the best hypothesis too much: best hypothesis gets almost all probability mass and other hypotheses get none. To compensate for that, posteriors were smoothed by raising each probability to some power -y &lt; 1 and renormalizing them afterward: Psmooth(hi|q) = P(hi|q)γ. XK P(hj|q)γ. (6) j=1 In a sense, -y is like temperature par</context>
</contexts>
<marker>Whitelaw, Hutchinson, Chung, Ellis, 2009</marker>
<rawString>Casey Whitelaw, Ben Hutchinson, Grace Y Chung, and Gerard Ellis. 2009. Using the web for language independent spellchecking and autocorrection. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2, pages 890–899. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>