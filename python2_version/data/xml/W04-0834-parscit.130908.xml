<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.040259">
<note confidence="0.538791333333333">
SENSEVAL-3: Third International Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, Barcelona, Spain, July 2004
Association for Computational Linguistics
</note>
<title confidence="0.998212">
Supervised Word Sense Disambiguation with
Support Vector Machines and Multiple Knowledge Sources
</title>
<author confidence="0.98672">
Yoong Keok Lee and Hwee Tou Ng and Tee Kiah Chia
</author>
<affiliation confidence="0.926713333333333">
Department of Computer Science
National University of Singapore
3 Science Drive 2, Singapore 117543
</affiliation>
<email confidence="0.971994">
y.k.lee@alumni.nus.edu.sg
nght@comp.nus.edu.sg
chiateek@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.995221" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999986277777778">
We participated in the SENSEVAL-3 English lexi-
cal sample task and multilingual lexical sample task.
We adopted a supervised learning approach with
Support Vector Machines, using only the official
training data provided. No other external resources
were used. The knowledge sources used were part-
of-speech of neighboring words, single words in the
surrounding context, local collocations, and syntac-
tic relations. For the translation and sense subtask
of the multilingual lexical sample task, the English
sense given for the target word was also used as
an additional knowledge source. For the English
lexical sample task, we obtained fine-grained and
coarse-grained score (for both recall and precision)
of 0.724 and 0.788 respectively. For the multilin-
gual lexical sample task, we obtained recall (and
precision) of 0.634 for the translation subtask, and
0.673 for the translation and sense subtask.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999976292682927">
This paper describes the approach adopted by our
systems which participated in the English lexical
sample task and the multilingual lexical sample task
of SENSEVAL-3. The goal of the English lexical
sample task is to predict the correct sense of an am-
biguous English word , while that of the multi-
lingual lexical sample task is to predict the correct
Hindi (target language) translation of an ambiguous
English (source language) word .
The multilingual lexical sample task is further
subdivided into two subtasks: the translation sub-
task, as well as the translation and sense subtask.
The distinction is that for the translation and sense
subtask, the English sense of the target ambiguous
word is also provided (for both training and test
data).
In all, we submitted 3 systems: system nusels
for the English lexical sample task, system nusmlst
for the translation subtask, and system nusmlsts for
the translation and sense subtask.
All systems were based on the supervised word
sense disambiguation (WSD) system of Lee and Ng
(2002), and used Support Vector Machines (SVM)
learning. Only the training examples provided in the
official training corpus were used to train the sys-
tems, and no other external resources were used. In
particular, we did not use any external dictionary or
the sample sentences in the provided dictionary.
The knowledge sources used included part-of-
speech (POS) of neighboring words, single words in
the surrounding context, local collocations, and syn-
tactic relations, as described in Lee and Ng (2002).
For the translation and sense subtask of the multi-
lingual lexical sample task, the English sense given
for the target word was also used as an additional
knowledge source. All features encoding these
knowledge sources were used, without any feature
selection.
We next describe SVM learning and the com-
bined knowledge sources adopted. Much of the de-
scription follows that of Lee and Ng (2002).
</bodyText>
<sectionHeader confidence="0.88262" genericHeader="method">
2 Support Vector Machines (SVM)
</sectionHeader>
<bodyText confidence="0.99996532">
The SVM (Vapnik, 1995) performs optimization to
find a hyperplane with the largest margin that sepa-
rates training examples into two classes. A test ex-
ample is classified depending on the side of the hy-
perplane it lies in. Input features can be mapped into
high dimensional space before performing the opti-
mization and classification. A kernel function can
be used to reduce the computational cost of training
and testing in high dimensional space. If the train-
ing examples are nonseparable, a regularization pa-
rameter ( by default) can be used to control
the trade-off between achieving a large margin and
a low training error. We used the implementation
of SVM in WEKA (Witten and Frank, 2000), where
each nominal feature with possible values is con-
verted into binary (0 or 1) features. If a nominal
feature takes the th value, then the th binary fea-
ture is set to 1 and all the other binary features are
set to 0. The default linear kernel is used. Since
SVM only handles binary (2-class) classification,
we built one binary classifier for each sense class.
Note that our supervised learning approach made
use of a single learning algorithm, without combin-
ing multiple learning algorithms as adopted in other
research (such as (Florian et al., 2002)).
</bodyText>
<sectionHeader confidence="0.960625" genericHeader="method">
3 Multiple Knowledge Sources
</sectionHeader>
<bodyText confidence="0.999973117647059">
To disambiguate a word occurrence , systems
nusels and nusmlst used the first four knowledge
sources listed below. System nusmlsts used the
English sense given for the target ambiguous word
as an additional knowledge source. Previous re-
search (Ng and Lee, 1996; Stevenson and Wilks,
2001; Florian et al., 2002; Lee and Ng, 2002) has
shown that a combination of knowledge sources im-
proves WSD accuracy.
Our experiments on the provided training data
of the SENSEVAL-3 translation and sense subtask
also indicated that the additional knowledge source
of the English sense of the target word further im-
proved accuracy (See Section 4.3 for details).
We did not attempt feature selection since our
previous research (Lee and Ng, 2002) indicated that
SVM performs better without feature selection.
</bodyText>
<subsectionHeader confidence="0.9752205">
3.1 Part-of-Speech (POS) of Neighboring
Words
</subsectionHeader>
<bodyText confidence="0.999500625">
We use 7 features to encode this knowledge source:
is
the POS of the th token to the left (right) of , and
is the POS of . A token can be a word or a
punctuation symbol, and each of these neighboring
tokens must be in the same sentence as . We use a
sentence segmentation program (Reynar and Ratna-
parkhi,1997) and a POS tagger (Ratnaparkhi,1996)
to segment the tokens surrounding into sentences
and assign POS tags to these tokens.
For example, to disambiguate the word
bars in the POS-tagged sentence “Reid/NNP
saw/VBD me/PRP looking/VBG at/IN the/DT
iron/NN bars/NNS ./.”, the POS feature vector is
where denotes
the POS tag of a null token.
</bodyText>
<subsectionHeader confidence="0.999931">
3.2 Single Words in the Surrounding Context
</subsectionHeader>
<bodyText confidence="0.999572217391304">
For this knowledge source, we consider all sin-
gle words (unigrams) in the surrounding context of
, and these words can be in a different sentence
from . For each training or test example, the
SENSEVAL-3 official data set provides a few sen-
tences as the surrounding context. In the results re-
ported here, we consider all words in the provided
context.
Specifically, all tokens in the surrounding context
of are converted to lower case and replaced by
their morphological root forms. Tokens present in
a list of stop words or tokens that do not contain
at least an alphabet character (such as numbers and
punctuation symbols) are removed. All remaining
tokens from all training contexts provided for are
gathered. Each remaining token contributes one
feature. In a training (or test) example, the feature
corresponding to is set to 1 iff the context of in
that training (or test) example contains .
For example, if is the word bars and the set
of selected unigrams is chocolate, iron, beer , the
feature vector for the sentence “Reid saw me look-
ing at the iron bars .” is 0, 1, 0 .
</bodyText>
<subsectionHeader confidence="0.99273">
3.3 Local Collocations
</subsectionHeader>
<bodyText confidence="0.997714">
A local collocation refers to the ordered se-
quence of tokens in the local, narrow context of .
Offsets and denote the starting and ending posi-
tion (relative to ) of the sequence, where a neg-
ative (positive) offset refers to a token to its left
(right). For example, let be the word bars in
the sentence “Reid saw me looking at the iron bars
.” Then is the iron and is iron. ,
where denotes a null token. Like POS, a colloca-
tion does not cross sentence boundary. To represent
this knowledge source of local collocations, we ex-
tracted 11 features corresponding to the following
collocations: , , , ,,
, , , , ,and . This
set of 11 features is the union of the collocation fea-
tures used in Ng and Lee (1996) and Ng (1997).
Note that each collocation is represented by
one feature that can have many possible feature val-
ues (the local collocation strings), whereas each dis-
tinct surrounding word is represented by one feature
that takes binary values (indicating presence or ab-
sence of that word). For example, if is the word
bars and suppose the set of collocations for
is a chocolate, the wine, the iron , then the fea-
ture value for collocation in the sentence
“Reid saw me looking at the iron bars .” is the iron.
</bodyText>
<subsectionHeader confidence="0.959011">
3.4 Syntactic Relations
</subsectionHeader>
<bodyText confidence="0.999928461538462">
We first parse the sentence containing with a sta-
tistical parser (Charniak, 2000). The constituent
tree structure generated by Charniak&apos;s parser is then
converted into a dependency tree in which every
word points to a parent headword. For example,
in the sentence “Reid saw me looking at the iron
bars .”, the word Reid points to the parent headword
saw. Similarly, the word me also points to the parent
headword saw.
We use different types of syntactic relations, de-
pending on the POS of . If is a noun, we use four
features: its parent headword , the POS of , the
voice of (active, passive, or if is not a verb),
</bodyText>
<listItem confidence="0.404375">
, where ( )
</listItem>
<tableCaption confidence="0.994725">
Table 1: Examples of syntactic relations
</tableCaption>
<bodyText confidence="0.999546294117647">
and the relative position of from (whether is
to the left or right of ). If is a verb, we use six
features: the nearest word to the left of such that
is the parent headword of , the nearest word to
the right of such that is the parent headword of
, the POS of , the POS of , the POS of , and the
voice of . If is an adjective, we use two features:
its parent headword and the POS of .
Headwords are obtained from a parse tree with
the script used for the CoNLL-2000 shared task
(Tjong Kim Sang and Buchholz, 2000).1
Some examples are shown in Table 1. Each POS
noun, verb, or adjective is illustrated by one exam-
ple. For each example, (a) shows and its POS; (b)
shows the sentence where occurs; and (c) shows
the feature vector corresponding to syntactic rela-
tions.
</bodyText>
<subsectionHeader confidence="0.974781">
3.5 Source Language (English) Sense
</subsectionHeader>
<bodyText confidence="0.999807933333334">
For the translation and sense subtask of the multilin-
gual lexical sample task, the sense of an ambiguous
word in the source language (English) is provided
for most of the training and test examples. An ex-
ample with unknown English sense is denoted with
question mark (“?”) in the corpus. We treat “?” as
another “sense” of (just like any other valid sense
of ).
We compile the set of English senses of a word
encountered in the whole training corpus. For
each sense in this set, a binary feature is generated
for each training and test example. If an example
has as the English sense of , this binary feature
(corresponding to ) is set to 1, otherwise it is set to
0.
</bodyText>
<sectionHeader confidence="0.998864" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.99875175">
Since our WSD system always outputs exactly one
prediction for each test example, its recall is always
the same as precision. We report below the micro-
averaged recall over all test words.
</bodyText>
<footnote confidence="0.848753">
&apos;Available athttp://ilk.uvt.nl/ sabine/chunklink/chunklink 2-
2-2000 for conll.pl
</footnote>
<table confidence="0.99839275">
Evaluation data Recall
SE-2 0.656
SE-1 (with dictionary examples) 0.796
SE-1 (without dictionary examples) 0.776
</table>
<tableCaption confidence="0.995775">
Table 2: Micro-averaged, fine-grained recall on
SENSEVAL-2 and SENSEVAL-1 test data
</tableCaption>
<table confidence="0.9984326">
System Recall
nusels 0.724 (fine-grained)
0.788 (coarse-grained)
nusmlst 0.634
nusmlsts 0.673
</table>
<tableCaption confidence="0.993323">
Table 3: Micro-averaged recall on SENSEVAL-3
test data
</tableCaption>
<subsectionHeader confidence="0.964713">
4.1 Evaluation on SENSEVAL-2 and
SENSEVAL-1 Data
</subsectionHeader>
<bodyText confidence="0.9999606">
Before participating in SENSEVAL-3, we evaluated
our WSD system on the English lexical sample task
of SENSEVAL-2 and SENSEVAL-1. The micro-
averaged, fine-grained recall over all SENSEVAL-
2 test words and all SENSEVAL-1 test words are
given in Table 2.
In SENSEVAL-1, some example sentences are
provided with the dictionary entries of the words
used in the evaluation. We provide the recall on
SENSEVAL-1 test data with and without the use of
such additional dictionary examples in training.
On both SENSEVAL-2 and SENSEVAL-1 test
data, the accuracy figures we obtained, as reported
in Table 2, are higher than the best official test
scores reported on both evaluation data sets.
</bodyText>
<subsectionHeader confidence="0.998413">
4.2 Official SENSEVAL-3 Scores
</subsectionHeader>
<bodyText confidence="0.999671705882353">
We participated in the SENSEVAL-3 English lexi-
cal sample task, and both subtasks of the multilin-
gual lexical sample task. The official SENSEVAL-
3 scores are shown in Table 3. Each score is the
micro-averaged recall (which is the same as preci-
sion) over all test words.
According to the task organizers, the fine-grained
(coarse-grained) recall of the best participating sys-
tem in the English lexical sample task is 0.729
(0.795). As such, the performance of our system
nusels compares favorably with the best participat-
ing system.
We are not able to fully assess the performance
of our multilingual lexical sample task systems
nusmlst and nusmlsts at the time of writing this
paper, since performance figures of the best partici-
pating system in this task have not been released by
</bodyText>
<figure confidence="0.9655297">
3(a) green (adj)
3(b) The modern tram is a green machine.
3(c) machine, NN
1(a) attention (noun)
1(b) He turned his attention to the workbench.
1(c) turned, VBD, active, left
2(a) turned (verb)
2(b) He turned his attention to the workbench.
2(c) he, attention, PRP, NN, VBD, active
the task organizers.
</figure>
<subsectionHeader confidence="0.992569">
4.3 Utility of English Sense as an Additional
Knowledge Source
</subsectionHeader>
<bodyText confidence="0.999973375">
To determine if using the English sense as an addi-
tional knowledge source improved the accuracy of
the translation and sense subtask, we conducted a
five-fold cross validation experiment. We randomly
divided the training data of the translation and sense
subtask for each word into 5 portions, using 4 por-
tions for training and 1 portion for test. We then re-
peated the process by selecting a different portion as
the test data each time and training on the remaining
portions.
Our investigation revealed that adding the En-
glish sense to the four existing knowledge sources
improved the micro-averaged recall from 0.628 to
0.638 on the training data. As such, we decided to
use the English sense as an additional knowledge
source for our system nusmlsts.
After the official SENSEVAL-3 evaluation
ended, we evaluated a variant of our system nusml-
sts without using the English sense as an additional
knowledge source. Based on the official test keys
released, the micro-averaged recall drops to 0.643,
which seems to suggest that the English sense is
a helpful knowledge source for the translation and
sense subtask.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999955714285714">
In this paper, we described our participating systems
in the SENSEVAL-3 English lexical sample task
and multilingual lexical sample task. Our WSD sys-
tems used SVM learning and multiple knowledge
sources. Evaluation results on the English lexical
sample task indicate that our method achieves good
accuracy on this task.
</bodyText>
<sectionHeader confidence="0.999059" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.925737333333333">
This research is partially supported by a research
grant R252-000-125-112 from National University
of Singapore Academic Research Fund.
</bodyText>
<sectionHeader confidence="0.99869" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999852102040816">
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings ofthe 1st Meeting
of the North American Chapter ofthe Association
for Computational Linguistics, pages 132–139.
Radu Florian, Silviu Cucerzan, Charles Schafer, and
David Yarowsky. 2002. Combining classifiers
for word sense disambiguation. Natural Lan-
guage Engineering, 8(4):327–341.
Yoong Keok Lee and Hwee Tou Ng. 2002. An
empirical evaluation of knowledge sources and
learning algorithms for word sense disambigua-
tion. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 41–48.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrat-
ing multiple knowledge sources to disambiguate
word sense: An exemplar-based approach. In
Proceedings of the 34th Annual Meeting of the
Association for ComputationalLinguistics, pages
40–47.
Hwee Tou Ng. 1997. Exemplar-based word sense
disambiguation: Some recent improvements. In
Proceedings of the Second Conference on Em-
pirical Methods in Natural Language Processing,
pages 208–213.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 133–142.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997.
A maximum entropy approach to identifying sen-
tence boundaries. In Proceedings of the Fifth
Conference on Applied Natural Language Pro-
cessing, pages 16–19.
Mark Stevenson and Yorick Wilks. 2001. The
interaction of knowledge sources in word
sense disambiguation. Computational Linguis-
tics, 27(3):321–349.
Erik F. Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 shared
task: Chunking. In Proceedings of the CoNLL-
2000 and LLL-2000, pages 127–132.
Vladimir N. Vapnik. 1995. The Nature of Sta-
tistical Learning Theory. Springer-Verlag, New
York.
Ian H. Witten and Eibe Frank. 2000. Data Min-
ing: Practical Machine Learning Tools and
Techniques with Java Implementations. Morgan
Kaufmann, San Francisco.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.163516">
<note confidence="0.7619215">SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July 2004</note>
<title confidence="0.757975666666667">Association for Computational Linguistics Supervised Word Sense Disambiguation with Support Vector Machines and Multiple Knowledge Sources</title>
<author confidence="0.999469">Keok Lee Tou Ng Kiah</author>
<affiliation confidence="0.9995585">Department of Computer National University of</affiliation>
<address confidence="0.51256">3 Science Drive 2, Singapore</address>
<email confidence="0.989142">chiateek@comp.nus.edu.sg</email>
<abstract confidence="0.996728631578947">We participated in the SENSEVAL-3 English lexical sample task and multilingual lexical sample task. We adopted a supervised learning approach with Support Vector Machines, using only the official training data provided. No other external resources were used. The knowledge sources used were partof-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations. For the translation and sense subtask of the multilingual lexical sample task, the English sense given for the target word was also used as an additional knowledge source. For the English lexical sample task, we obtained fine-grained and coarse-grained score (for both recall and precision) of 0.724 and 0.788 respectively. For the multilingual lexical sample task, we obtained recall (and precision) of 0.634 for the translation subtask, and 0.673 for the translation and sense subtask.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings ofthe 1st Meeting of the North American Chapter ofthe Association for Computational Linguistics,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="8604" citStr="Charniak, 2000" startWordPosition="1424" endWordPosition="1425">) and Ng (1997). Note that each collocation is represented by one feature that can have many possible feature values (the local collocation strings), whereas each distinct surrounding word is represented by one feature that takes binary values (indicating presence or absence of that word). For example, if is the word bars and suppose the set of collocations for is a chocolate, the wine, the iron , then the feature value for collocation in the sentence “Reid saw me looking at the iron bars .” is the iron. 3.4 Syntactic Relations We first parse the sentence containing with a statistical parser (Charniak, 2000). The constituent tree structure generated by Charniak&apos;s parser is then converted into a dependency tree in which every word points to a parent headword. For example, in the sentence “Reid saw me looking at the iron bars .”, the word Reid points to the parent headword saw. Similarly, the word me also points to the parent headword saw. We use different types of syntactic relations, depending on the POS of . If is a noun, we use four features: its parent headword , the POS of , the voice of (active, passive, or if is not a verb), , where ( ) Table 1: Examples of syntactic relations and the relat</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proceedings ofthe 1st Meeting of the North American Chapter ofthe Association for Computational Linguistics, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
<author>Silviu Cucerzan</author>
<author>Charles Schafer</author>
<author>David Yarowsky</author>
</authors>
<title>Combining classifiers for word sense disambiguation.</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="4626" citStr="Florian et al., 2002" startWordPosition="727" endWordPosition="730"> used the implementation of SVM in WEKA (Witten and Frank, 2000), where each nominal feature with possible values is converted into binary (0 or 1) features. If a nominal feature takes the th value, then the th binary feature is set to 1 and all the other binary features are set to 0. The default linear kernel is used. Since SVM only handles binary (2-class) classification, we built one binary classifier for each sense class. Note that our supervised learning approach made use of a single learning algorithm, without combining multiple learning algorithms as adopted in other research (such as (Florian et al., 2002)). 3 Multiple Knowledge Sources To disambiguate a word occurrence , systems nusels and nusmlst used the first four knowledge sources listed below. System nusmlsts used the English sense given for the target ambiguous word as an additional knowledge source. Previous research (Ng and Lee, 1996; Stevenson and Wilks, 2001; Florian et al., 2002; Lee and Ng, 2002) has shown that a combination of knowledge sources improves WSD accuracy. Our experiments on the provided training data of the SENSEVAL-3 translation and sense subtask also indicated that the additional knowledge source of the English sense</context>
</contexts>
<marker>Florian, Cucerzan, Schafer, Yarowsky, 2002</marker>
<rawString>Radu Florian, Silviu Cucerzan, Charles Schafer, and David Yarowsky. 2002. Combining classifiers for word sense disambiguation. Natural Language Engineering, 8(4):327–341.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Hwee Tou Ng</author>
</authors>
<title>An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>41--48</pages>
<contexts>
<context position="2451" citStr="Lee and Ng (2002)" startWordPosition="366" endWordPosition="369">ce language) word . The multilingual lexical sample task is further subdivided into two subtasks: the translation subtask, as well as the translation and sense subtask. The distinction is that for the translation and sense subtask, the English sense of the target ambiguous word is also provided (for both training and test data). In all, we submitted 3 systems: system nusels for the English lexical sample task, system nusmlst for the translation subtask, and system nusmlsts for the translation and sense subtask. All systems were based on the supervised word sense disambiguation (WSD) system of Lee and Ng (2002), and used Support Vector Machines (SVM) learning. Only the training examples provided in the official training corpus were used to train the systems, and no other external resources were used. In particular, we did not use any external dictionary or the sample sentences in the provided dictionary. The knowledge sources used included part-ofspeech (POS) of neighboring words, single words in the surrounding context, local collocations, and syntactic relations, as described in Lee and Ng (2002). For the translation and sense subtask of the multilingual lexical sample task, the English sense give</context>
<context position="4986" citStr="Lee and Ng, 2002" startWordPosition="785" endWordPosition="788">ssification, we built one binary classifier for each sense class. Note that our supervised learning approach made use of a single learning algorithm, without combining multiple learning algorithms as adopted in other research (such as (Florian et al., 2002)). 3 Multiple Knowledge Sources To disambiguate a word occurrence , systems nusels and nusmlst used the first four knowledge sources listed below. System nusmlsts used the English sense given for the target ambiguous word as an additional knowledge source. Previous research (Ng and Lee, 1996; Stevenson and Wilks, 2001; Florian et al., 2002; Lee and Ng, 2002) has shown that a combination of knowledge sources improves WSD accuracy. Our experiments on the provided training data of the SENSEVAL-3 translation and sense subtask also indicated that the additional knowledge source of the English sense of the target word further improved accuracy (See Section 4.3 for details). We did not attempt feature selection since our previous research (Lee and Ng, 2002) indicated that SVM performs better without feature selection. 3.1 Part-of-Speech (POS) of Neighboring Words We use 7 features to encode this knowledge source: is the POS of the th token to the left (</context>
</contexts>
<marker>Lee, Ng, 2002</marker>
<rawString>Yoong Keok Lee and Hwee Tou Ng. 2002. An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Hian Beng Lee</author>
</authors>
<title>Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for ComputationalLinguistics,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="4918" citStr="Ng and Lee, 1996" startWordPosition="773" endWordPosition="776"> linear kernel is used. Since SVM only handles binary (2-class) classification, we built one binary classifier for each sense class. Note that our supervised learning approach made use of a single learning algorithm, without combining multiple learning algorithms as adopted in other research (such as (Florian et al., 2002)). 3 Multiple Knowledge Sources To disambiguate a word occurrence , systems nusels and nusmlst used the first four knowledge sources listed below. System nusmlsts used the English sense given for the target ambiguous word as an additional knowledge source. Previous research (Ng and Lee, 1996; Stevenson and Wilks, 2001; Florian et al., 2002; Lee and Ng, 2002) has shown that a combination of knowledge sources improves WSD accuracy. Our experiments on the provided training data of the SENSEVAL-3 translation and sense subtask also indicated that the additional knowledge source of the English sense of the target word further improved accuracy (See Section 4.3 for details). We did not attempt feature selection since our previous research (Lee and Ng, 2002) indicated that SVM performs better without feature selection. 3.1 Part-of-Speech (POS) of Neighboring Words We use 7 features to en</context>
<context position="7990" citStr="Ng and Lee (1996)" startWordPosition="1315" endWordPosition="1318">xt of . Offsets and denote the starting and ending position (relative to ) of the sequence, where a negative (positive) offset refers to a token to its left (right). For example, let be the word bars in the sentence “Reid saw me looking at the iron bars .” Then is the iron and is iron. , where denotes a null token. Like POS, a collocation does not cross sentence boundary. To represent this knowledge source of local collocations, we extracted 11 features corresponding to the following collocations: , , , ,, , , , , ,and . This set of 11 features is the union of the collocation features used in Ng and Lee (1996) and Ng (1997). Note that each collocation is represented by one feature that can have many possible feature values (the local collocation strings), whereas each distinct surrounding word is represented by one feature that takes binary values (indicating presence or absence of that word). For example, if is the word bars and suppose the set of collocations for is a chocolate, the wine, the iron , then the feature value for collocation in the sentence “Reid saw me looking at the iron bars .” is the iron. 3.4 Syntactic Relations We first parse the sentence containing with a statistical parser (C</context>
</contexts>
<marker>Ng, Lee, 1996</marker>
<rawString>Hwee Tou Ng and Hian Beng Lee. 1996. Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach. In Proceedings of the 34th Annual Meeting of the Association for ComputationalLinguistics, pages 40–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
</authors>
<title>Exemplar-based word sense disambiguation: Some recent improvements.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>208--213</pages>
<contexts>
<context position="8004" citStr="Ng (1997)" startWordPosition="1320" endWordPosition="1321">note the starting and ending position (relative to ) of the sequence, where a negative (positive) offset refers to a token to its left (right). For example, let be the word bars in the sentence “Reid saw me looking at the iron bars .” Then is the iron and is iron. , where denotes a null token. Like POS, a collocation does not cross sentence boundary. To represent this knowledge source of local collocations, we extracted 11 features corresponding to the following collocations: , , , ,, , , , , ,and . This set of 11 features is the union of the collocation features used in Ng and Lee (1996) and Ng (1997). Note that each collocation is represented by one feature that can have many possible feature values (the local collocation strings), whereas each distinct surrounding word is represented by one feature that takes binary values (indicating presence or absence of that word). For example, if is the word bars and suppose the set of collocations for is a chocolate, the wine, the iron , then the feature value for collocation in the sentence “Reid saw me looking at the iron bars .” is the iron. 3.4 Syntactic Relations We first parse the sentence containing with a statistical parser (Charniak, 2000)</context>
</contexts>
<marker>Ng, 1997</marker>
<rawString>Hwee Tou Ng. 1997. Exemplar-based word sense disambiguation: Some recent improvements. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, pages 208–213.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--142</pages>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy approach to identifying sentence boundaries.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>16--19</pages>
<marker>Reynar, Ratnaparkhi, 1997</marker>
<rawString>Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A maximum entropy approach to identifying sentence boundaries. In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 16–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Stevenson</author>
<author>Yorick Wilks</author>
</authors>
<title>The interaction of knowledge sources in word sense disambiguation.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="4945" citStr="Stevenson and Wilks, 2001" startWordPosition="777" endWordPosition="780">used. Since SVM only handles binary (2-class) classification, we built one binary classifier for each sense class. Note that our supervised learning approach made use of a single learning algorithm, without combining multiple learning algorithms as adopted in other research (such as (Florian et al., 2002)). 3 Multiple Knowledge Sources To disambiguate a word occurrence , systems nusels and nusmlst used the first four knowledge sources listed below. System nusmlsts used the English sense given for the target ambiguous word as an additional knowledge source. Previous research (Ng and Lee, 1996; Stevenson and Wilks, 2001; Florian et al., 2002; Lee and Ng, 2002) has shown that a combination of knowledge sources improves WSD accuracy. Our experiments on the provided training data of the SENSEVAL-3 translation and sense subtask also indicated that the additional knowledge source of the English sense of the target word further improved accuracy (See Section 4.3 for details). We did not attempt feature selection since our previous research (Lee and Ng, 2002) indicated that SVM performs better without feature selection. 3.1 Part-of-Speech (POS) of Neighboring Words We use 7 features to encode this knowledge source:</context>
</contexts>
<marker>Stevenson, Wilks, 2001</marker>
<rawString>Mark Stevenson and Yorick Wilks. 2001. The interaction of knowledge sources in word sense disambiguation. Computational Linguistics, 27(3):321–349.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Sabine Buchholz</author>
</authors>
<title>Introduction to the CoNLL-2000 shared task: Chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of the CoNLL2000 and LLL-2000,</booktitle>
<pages>127--132</pages>
<contexts>
<context position="9704" citStr="Sang and Buchholz, 2000" startWordPosition="1639" endWordPosition="1642">of , the voice of (active, passive, or if is not a verb), , where ( ) Table 1: Examples of syntactic relations and the relative position of from (whether is to the left or right of ). If is a verb, we use six features: the nearest word to the left of such that is the parent headword of , the nearest word to the right of such that is the parent headword of , the POS of , the POS of , the POS of , and the voice of . If is an adjective, we use two features: its parent headword and the POS of . Headwords are obtained from a parse tree with the script used for the CoNLL-2000 shared task (Tjong Kim Sang and Buchholz, 2000).1 Some examples are shown in Table 1. Each POS noun, verb, or adjective is illustrated by one example. For each example, (a) shows and its POS; (b) shows the sentence where occurs; and (c) shows the feature vector corresponding to syntactic relations. 3.5 Source Language (English) Sense For the translation and sense subtask of the multilingual lexical sample task, the sense of an ambiguous word in the source language (English) is provided for most of the training and test examples. An example with unknown English sense is denoted with question mark (“?”) in the corpus. We treat “?” as another</context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduction to the CoNLL-2000 shared task: Chunking. In Proceedings of the CoNLL2000 and LLL-2000, pages 127–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer-Verlag,</publisher>
<location>New York.</location>
<contexts>
<context position="3397" citStr="Vapnik, 1995" startWordPosition="520" endWordPosition="521">ed part-ofspeech (POS) of neighboring words, single words in the surrounding context, local collocations, and syntactic relations, as described in Lee and Ng (2002). For the translation and sense subtask of the multilingual lexical sample task, the English sense given for the target word was also used as an additional knowledge source. All features encoding these knowledge sources were used, without any feature selection. We next describe SVM learning and the combined knowledge sources adopted. Much of the description follows that of Lee and Ng (2002). 2 Support Vector Machines (SVM) The SVM (Vapnik, 1995) performs optimization to find a hyperplane with the largest margin that separates training examples into two classes. A test example is classified depending on the side of the hyperplane it lies in. Input features can be mapped into high dimensional space before performing the optimization and classification. A kernel function can be used to reduce the computational cost of training and testing in high dimensional space. If the training examples are nonseparable, a regularization parameter ( by default) can be used to control the trade-off between achieving a large margin and a low training e</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer-Verlag, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<date>2000</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco.</location>
<contexts>
<context position="4069" citStr="Witten and Frank, 2000" startWordPosition="631" endWordPosition="634">the largest margin that separates training examples into two classes. A test example is classified depending on the side of the hyperplane it lies in. Input features can be mapped into high dimensional space before performing the optimization and classification. A kernel function can be used to reduce the computational cost of training and testing in high dimensional space. If the training examples are nonseparable, a regularization parameter ( by default) can be used to control the trade-off between achieving a large margin and a low training error. We used the implementation of SVM in WEKA (Witten and Frank, 2000), where each nominal feature with possible values is converted into binary (0 or 1) features. If a nominal feature takes the th value, then the th binary feature is set to 1 and all the other binary features are set to 0. The default linear kernel is used. Since SVM only handles binary (2-class) classification, we built one binary classifier for each sense class. Note that our supervised learning approach made use of a single learning algorithm, without combining multiple learning algorithms as adopted in other research (such as (Florian et al., 2002)). 3 Multiple Knowledge Sources To disambig</context>
</contexts>
<marker>Witten, Frank, 2000</marker>
<rawString>Ian H. Witten and Eibe Frank. 2000. Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations. Morgan Kaufmann, San Francisco.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>