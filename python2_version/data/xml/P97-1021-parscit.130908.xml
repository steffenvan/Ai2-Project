<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99574">
A DOP Model for Semantic Interpretation*
</title>
<author confidence="0.988151">
Remko Bonnema, Rens Bod and Remko Scha
</author>
<affiliation confidence="0.987707">
Institute for Logic, Language and Computation
University of Amsterdam
</affiliation>
<address confidence="0.7621955">
Spuistraat 134, 1012 VB Amsterdam
Bonnemaelmars.let.uva.n1
</address>
<email confidence="0.726306">
Rens.Bod@let.uva.n1
</email>
<note confidence="0.56923">
Remko.SchAlet.uva.n1
</note>
<sectionHeader confidence="0.982965" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999933952380953">
In data-oriented language processing, an
annotated language corpus is used as a
stochastic grammar. The most probable
analysis of a new sentence is constructed
by combining fragments from the corpus in
the most probable way. This approach has
been successfully used for syntactic anal-
ysis, using corpora with syntactic annota-
tions such as the Penn Tree-bank. If a cor-
pus with semantically annotated sentences
is used, the same approach can also gen-
erate the most probable semantic interpre-
tation of an input sentence. The present
paper explains this semantic interpretation
method. A data-oriented semantic inter-
pretation algorithm was tested on two se-
mantically annotated corpora: the English
ATIS corpus and the Dutch OVIS corpus.
Experiments show an increase in seman-
tic accuracy if larger corpus-fragments are
taken into consideration.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.982712117647059">
Data-oriented models of language processing em-
body the assumption that human language per-
ception and production works with representations
of concrete past language experiences, rather than
with abstract grammar rules. Such models therefore
maintain large corpora of linguistic representations
of previously occurring utterances. When processing
a new input utterance, analyses of this utterance are
constructed by combining fragments from the cor-
pus; the occurrence-frequencies of the fragments are
used to estimate which analysis is the most probable
one.
* This work was partially supported by NWO, the
Netherlands Organization for Scientific Research (Prior-
ity Programme Language and Speech Technology).
For the syntactic dimension of language, vari-
ous instantiations of this data-oriented processing
or &amp;quot;DOP&amp;quot; approach have been worked out (e.g.
Bod (1992-1995); Charniak (1996); Tugwell (1995);
Sima&apos;an et al. (1994); Sima&apos;an (1994; 1996a);
Goodman (1996); Rajman (1995ab); Kaplan (1996);
Sekine and Grishman (1995)). A method for ex-
tending it to the semantic domain was first intro-
duced by van den Berg et al. (1994). In the present
paper we discuss a computationally effective version
of that method, and an implemented system that
uses it. We first summarize the first fully instanti-
ated DOP model as presented in Bod (1992-1993).
Then we show how this method can be straightfor-
wardly extended into a semantic analysis method, if
corpora are created in which the trees are enriched
with semantic annotations. Finally, we discuss an
implementation and report on experiments with two
semantically analyzed corpora (ATIS and OVIS).
</bodyText>
<sectionHeader confidence="0.990066" genericHeader="method">
2 Data-Oriented Syntactic Analysis
</sectionHeader>
<bodyText confidence="0.9998856">
So far, the data-oriented processing method has
mainly been applied to corpora with simple syntac-
tic annotations, consisting of labelled trees. Let us
illustrate this with a very simple imaginary example.
Suppose that a corpus consists of only two trees:
</bodyText>
<figure confidence="0.453369">
NP VP NP VP
Det N sings Det N whistles
every woman man
</figure>
<figureCaption confidence="0.9994">
Figure 1: Imaginary corpus of two trees
</figureCaption>
<bodyText confidence="0.9981332">
We employ one operation for combining subtrees,
called composition, indicated as o; this operation
identifies the leftmost nonterminal leaf node of one
tree with the root node of a second tree (i.e., the
second tree is substituted on the leftmost nontermi-
</bodyText>
<page confidence="0.997964">
159
</page>
<bodyText confidence="0.991876666666667">
nal leaf node of the first tree). A new input sentence
like &amp;quot;A woman whistles&amp;quot; can now be parsed by com-
bining subtrees from this corpus. For instance:
</bodyText>
<figure confidence="0.965326">
o
NP VP woman NP VP
I I
Det N whistles Det N whistles
woman
</figure>
<figureCaption confidence="0.996854">
Figure 2: Derivation and parse for &amp;quot;A woman
</figureCaption>
<figure confidence="0.98636">
whistles&amp;quot;
Other derivations may yield the same parse tree;
for instancel:
NP Det VP
° o I
NP VP Det a whistles
woman
NP VP
Det N whistles
a woman
</figure>
<figureCaption confidence="0.810168">
Figure 3: Different derivation generating the same
parse for &amp;quot;A woman whistles&amp;quot;
</figureCaption>
<figure confidence="0.906045714285714">
or
o Det N S
NP VP ° I
I a woman NP VP
Det N whistles I
Det N whistles
al woman
</figure>
<figureCaption confidence="0.9682145">
Figure 4: Another derivation generating the same
parse for &amp;quot;A woman whistles&amp;quot;
</figureCaption>
<bodyText confidence="0.999757444444444">
Thus, a parse tree can have many derivations in-
volving different corpus-subtrees. DOP estimates
the probability of substituting a subtree t on a spe-
cific node as the probability of selecting t among all
subtrees in the corpus that could be substituted on
that node. This probability is equal to the number of
occurrences of a subtree t, divided by the total num-
ber of occurrences of subtrees t&apos; with the same root
node label as t : P(t) = Iti I E
</bodyText>
<equation confidence="0.78253">
:root(t1=root(t) ti.
</equation>
<bodyText confidence="0.99921575">
The probability of a derivation t1 • • o 4, can be
computed as the product of the probabilities of the
subtrees this derivation consists of: P(ti o• • •o tn)
ft P(t). The probability of a parse tree is equal to
</bodyText>
<footnote confidence="0.73964">
1Here touovow should be read as ((to u) o v) ow.
</footnote>
<bodyText confidence="0.999814529411764">
the probability that any of its distinct derivations is
generated, which is the sum of the probabilities of all
derivations of that parse tree. Let td be the i-th sub-
tree in the derivation d that yields tree T, then the
probability of T is given by: P(T) = Ed ni P(tsd).
The DOP method differs from other statisti-
cal approaches, such as Pereira and Schabes (1992),
Black et al. (1993) and Briscoe (1994), in that it
does not predefine or train a formal grammar; in-
stead it takes subtrees directly from annotated sen-
tences in a treebank with a probability propor-
tional to the number of occurrences of these sub-
trees in the treebank. Bod (1993b) shows that
DOP can be implemented using context-free pars-
ing techniques. To select the most probable parse,
Bod (1993a) gives a Monte Carlo approximation al-
gorithm. Sima&apos;an (1995) gives an efficient polyno-
mial algorithm for a sub-optimal solution.
The model was tested on the Air Travel In-
formation System (ATIS) corpus as analyzed in
the Penn Treebank (Marcus et al. (1993)), achiev-
ing better test results than other stochastic
grammars (cf. Bod (1996), Sima&apos;an (1996a),
Goodman (1996)). On Penn&apos;s Wall Street Jour-
nal corpus, the data-oriented processing approach
has been tested by Sekine and Grishman (1995) and
by Charniak (1996). Though Charniak only uses
corpus-subtrees smaller than depth 2 (which in our
experience constitutes a less-than-optimal version
of the data-oriented processing method), he reports
that it &amp;quot;outperforms all other non-word-based sta-
tistical parsers/grammars on this corpus&amp;quot;. For an
overview of data-oriented language processing, we
refer to (Bod and Scha, 1996).
</bodyText>
<sectionHeader confidence="0.987631" genericHeader="method">
3 Data-Oriented Semantic Analysis
</sectionHeader>
<bodyText confidence="0.999538333333333">
To use the DOP method not just for syntactic anal-
ysis, but also for semantic interpretation, four steps
must be taken:
</bodyText>
<listItem confidence="0.9733966">
1. decide on a formalism for representing the
meanings of sentences and surface-constituents.
2. annotate the corpus-sentences and their
surface-constituents with such semantic repre-
sentations.
3. establish a method for deriving the mean-
ing representations associated with arbitrary
corpus-subtrees and with compositions of such
subtrees.
4. reconsider the probability calculations.
</listItem>
<bodyText confidence="0.673042">
We now discuss these four steps.
</bodyText>
<subsectionHeader confidence="0.999029">
3.1 Semantic formalism
</subsectionHeader>
<bodyText confidence="0.9998215">
The decision about the representational formalism
is to some extent arbitrary, as long as it has a well-
</bodyText>
<page confidence="0.985343">
160
</page>
<figure confidence="0.9940165">
S:Vx(woman(x)--.sing(x))
NP: XYdx(woman(x)-.Y(x)) VP:sing
Det:XXXYVx(X(x)-.Y(x)) N:woman sings
every woman
S3x(man(x)Awhistle(x))
NP: Mex(man(x)AY(x)) VP:whistle
Det:XXXY3x/\(X(x)AY(x)) N:man whistles
a man
</figure>
<figureCaption confidence="0.991504">
Figure 5: Imaginary corpus of two trees with syntactic and semantic labels.
</figureCaption>
<figure confidence="0.9587981">
S:d1(d2)
NP:d1(d2) VP:sing
1
Det:XXXYchc(X(x)-.Y(x)) N:woman sings
1 1
every woman
S:d1(d2)
NP:d1(d2) VP:whistle
Det:XXXY3x/\(X(x)AY(x)) N:man whistles
a man
</figure>
<figureCaption confidence="0.999877">
Figure 6: Same imaginary corpus of two trees with syntactic and semantic labels using the daughter notation.
</figureCaption>
<bodyText confidence="0.999699133333333">
defined model-theory and is rich enough for repre-
senting the meanings of sentences and constituents
that are relevant for the intended application do-
main. For our exposition in this paper we will
use a wellknown standard formalism: extensional
type theory (see Gamut (1991)), i.e., a higher-order
logical language that combines lambda-abstraction
with connectives and quantifiers. The first imple-
mented system for data-oriented semantic interpre-
tation, presented in Bonnema (1996), used a differ-
ent logical language, however. And in many appli-
cation contexts it probably makes sense to use an
A.I.-style language which highlights domain struc-
ture (frames, slots, and fillers), while limiting the
use of quantification and negation (see section 5).
</bodyText>
<subsectionHeader confidence="0.999862">
3.2 Semantic annotation
</subsectionHeader>
<bodyText confidence="0.999841481481481">
We assume a corpus that is already syntactically
annotated as before: with labelled trees that indi-
cate surface constituent structure. Now the basic
idea, taken from van den Berg et al. (1994), is to
augment this syntactic annotation with a semantic
one: to every meaningful syntactic node, we add a
type-logical formula that expresses the meaning of
the corresponding surface-constituent. If we would
carry out this idea in a completely direct way, the
toy corpus of Figure 1 might, for instance, turn into
the toy corpus of Figure 5.
Van den Berg et al. indicate how a corpus of this
sort may be used for data-oriented semantic inter-
pretation. Their algorithm, however, requires a pro-
cedure which can inspect the semantic formula of a
node and determine the contribution of the seman-
tics of a lower node, in order to be able to &amp;quot;fac-
tor out&amp;quot; that contribution. The details of this pro-
cedure have not been specified. However, van den
Berg et al. also propose a simpler annotation con-
vention which avoids the need for this procedure,
and which is computationally more effective: an an-
notation convention which indicates explicitly how
the semantic formula for a node is built up on the
basis of the semantic formulas of its daughter nodes.
Using this convention, the semantic annotation of
the corpus trees is indicated as follows:
</bodyText>
<listItem confidence="0.949638285714286">
• For every meaningful lexical node a type logical
formula is specified that represents its meaning.
• For every meaningful non-lexical node a for-
mula schema is specified which indicates how
its meaning representation may be put together
out of the formulas assigned to its daughter
nodes.
</listItem>
<bodyText confidence="0.989498176470588">
In the examples below, these schemata use the vari-
able dl to indicate the meaning of the leftmost
daughter constituent, d2 to indicate the meaning
of the second daughter constituent, etc. Using this
notation, the semantically annotated version of the
toy corpus of Figure 1 is the toy corpus rendered in
Figure 6. This kind of semantic annotation is what
will be used in the construction of the corpora de-
scribed in section 5 of this paper. It may be noted
that the rather oblique description of the semantics
of the higher nodes in the tree would easily lead to
mistakes, if annotation would be carried out com-
pletely manually. An annotation tool that makes
the expanded versions of the formulas visible for the
annotator is obviously called for. Such a tool was
developed by Bonnema (1996), it will be briefly de-
scribed in section 5.
</bodyText>
<page confidence="0.99117">
161
</page>
<bodyText confidence="0.999132384615385">
This annotation convention obviously, assumes
that the meaning representation of a surface-
constituent can in fact always be composed out of
the meaning representations of its subconstituents.
This assumption is not unproblematic. To maintain
it in the face of phenomena such as non-standard
quantifier scope or discontinuous constituents cre-
ates complications in the syntactic or semantic anal-
yses assigned to certain sentences and their con-
stituents. It is therefore not clear yet whether
our current treatment ought to be viewed as com-
pletely general, or whether a treatment in the vein
of van den Berg et al. (1994) should be worked out.
</bodyText>
<subsectionHeader confidence="0.882237">
3.3 The meanings of subtrees and their
compositions
</subsectionHeader>
<bodyText confidence="0.999879571428571">
As in the purely syntactic version of DOP, we now
want to compute the probability of a (semantic)
analysis by considering the most probable way in
which it can be generated by combining subtrees
from the corpus. We can do this in virtually the
same way. The only novelty is a slight modification
in the process by which a corpus tree is decomposed
into subtrees, and a corresponding modification in
the composition operation which combines subtrees.
If we extract a subtree out of a tree, we replace the
semantics of the new leaf node with a unification
variable of the same type. Correspondingly, when
the composition operation substitutes a subtree at
this node, this unification variable is unified with
the semantic formula on the substituting tree. (It
is required that the semantic type of this formula
matches the semantic type of the unification vari-
able.)
A simple example will make this clear. First, let
us consider what subtrees the corpus makes avail-
able now. As an example, Figure 7 shows one of the
decompositions of the annotated corpus sentence &amp;quot;A
man whistles&amp;quot;. We see that by decomposing the tree
into two subtrees, the semantics at the breakpoint-
node N: man is replaced by a variable. Now an
analysis for the sentence &amp;quot;A woman whistles&amp;quot; can,
for instance, be generated in the way shown in Fig-
ure 8.
</bodyText>
<subsectionHeader confidence="0.9918745">
3.4 The Statistical Model of Data-Oriented
Semantic Interpretation
</subsectionHeader>
<bodyText confidence="0.999869142857143">
We now define the probability of an interpretation
of an input string.
Given a partially annotated corpus as defined
above, the multiset of corpus subtrees consists of
all subtrees with a well-defined top-node seman-
tics, that are generated by applying to the trees of
the corpus the decomposition mechanism described
</bodyText>
<figure confidence="0.9673281">
S:d1(d2)
NP:d1(d2) VP:whistle
I
Det: )30,Y3x(X(x)AY(x)) N:man whistles
1 I
a man
S:d1(d2)
NP:d1(d2) VP:whistle
I
Det: XXX,Y3x(X(x) AY(x)) N:U whistles
</figure>
<figureCaption confidence="0.990764">
Figure 7: Decomposing a tree into subtrees with uni-
fication variables.
</figureCaption>
<figure confidence="0.988526222222222">
S:d1(d2) N: woman
0 I =
NP:d 1 (d2) VP:whistle woman
Det: XXXY3x(X(x)AY(x)) N:U whistles
S:d1(d2)
a
NP:d1(d2) VP:whistle
Det: XXXY3x(X(x)AY(x)) N:woman whistles
a woman
</figure>
<figureCaption confidence="0.9602075">
Figure 8: Generating an analysis for &amp;quot;A woman
whistles&amp;quot;.
</figureCaption>
<bodyText confidence="0.998970142857143">
above. The probability of substituting a subtree t on
a specific node is the probability of selecting t among
all subtrees in the multiset that could be substituted
on that node. This probability is equal to the num-
ber of occurrences of a subtree t, divided by the total
number of occurrences of subtrees t&apos; with the same
root node label as t:
</bodyText>
<equation confidence="0.998401">
P(t) = It&apos; (1)
t&apos; :root(t9=root(t) Iti
</equation>
<bodyText confidence="0.9963595">
A derivation of a string is a tuple of subtrees, such
that their composition results in a tree whose yield is
the string. The probability of a derivation tlo • • .0 tn
is the product of the probabilities of these subtrees:
</bodyText>
<figure confidence="0.446014">
P(ti o • • • o tn) = 11 P(tt) (2)
A tree resulting from a derivation of a string is called
a parse of this string. The probability of a parse is
N:man
man
</figure>
<page confidence="0.978372">
162
</page>
<bodyText confidence="0.9991962">
the probability that any of its derivations occurs;
this is the sum of the probabilities of all its deriva-
tions. Let tid be the i-th subtree in the derivation d
that yields tree T, then the probability of T is given
by:
</bodyText>
<equation confidence="0.934291">
P(T) = Ellp(tia) (3)
d i
</equation>
<bodyText confidence="0.999982777777778">
An interpretation of a string is a formula which is
provably equivalent to the semantic annotation of
the top node of a parse of this string. The proba-
bility of an interpretation I of a string is the sum of
the probabilities of the parses of this string with a
top node annotated with a formula that is provably
equivalent to I. Let tidp be the i-th subtree in the
derivation d that yields parse p with interpretation
I, then the probability of I is given by:
</bodyText>
<equation confidence="0.9571465">
EEll P(td) (4)
p d i
</equation>
<bodyText confidence="0.999981384615385">
We choose the most probable interpretation I of a
string s as the most appropriate interpretation of s.
In Bonnema (1996) a semantic extension of the
DOP parser of Sima&apos;an (1996a) is given. But in-
stead of computing the most likely interpretation
of a string, it computes the interpretation of the
most likely combination of semantically annotated
subtrees. As was shown in Sima&apos;an (1996b), the
most likely interpretation of a string cannot be com-
puted in deterministic polynomial time. It is not yet
known how often the most likely interpretation and
the interpretation of the most likely combination of
semantically enriched subtrees do actually coincide.
</bodyText>
<sectionHeader confidence="0.999334" genericHeader="method">
4 Implementations
</sectionHeader>
<bodyText confidence="0.999973333333333">
The first implementation of a semantic DOP-model
yielded rather encouraging preliminary results on a
semantically enriched part of the ATIS-corpus. Im-
plementation details and experimental results can
be found in Bonnema (1996), and Bod et al. (1996).
We repeat the most important observations:
</bodyText>
<listItem confidence="0.9914098">
• Data-oriented semantic interpretation seems to
be robust; of the sentences that could be parsed,
a significantly higher percentage received a cor-
rect semantic interpretation (88%), than an ex-
actly correct syntactic analysis (62%).
• The coverage of the parser was rather low
(72%), because of the sheer number of differ-
ent semantic types and constructs in the trees.
• The parser was fast: on the average six times
as fast as a parser trained on syntax alone.
</listItem>
<bodyText confidence="0.999978588235294">
The current implementation is again an extension
of Sima&apos;an (1996a), by Bonnema2. In our experi-
ments, we notice a robustness and speed-up compa-
rable to our experience with the previous implemen-
tation. Besides that, we observe higher accuracy,
and higher coverage, due to a new method of orga-
nizing the information in the tree-bank before it is
used for building the actual parser.
A semantically enriched tree-bank will generally
contain a wealth of detail. This makes it hard for
a probabilistic model to estimate all parameters. In
sections 4.1 and 4.2, we discuss a way of generalizing
over semantic information in the tree-bank, before a
DOP-parser is trained on the material. We automat-
ically learn a simpler, less redundant representation
of the same information. The method is employed
in our current implementation.
</bodyText>
<subsectionHeader confidence="0.998047">
4.1 Simplifying the tree-bank
</subsectionHeader>
<bodyText confidence="0.983741857142857">
A tree-bank annotated in the manner described
above, consists of tree-structures with syntactic and
semantic attributes at every node. The semantic
attributes are rules that indicate how the meaning-
representation of the expression dominated by that
node is built-up out of its parts. Every instance of
a semantic rule at a node has a semantic type asso-
ciated with it. These types usually depend on the
lexical instantiations of a syntactic-semantic struc-
ture.
If we decide to view subtrees as identical if their
syntactic structure, the semantic rule at each node,
and the semantic type of each node is identical,
any fine-grained type-system will cause a huge in-
crease in different instantiations of subtrees. In the
two tree-banks we tested on, there are many sub-
trees that differ in semantic type, but otherwise
share the same syntactic/semantic structure. Disre-
garding the semantic types completely, on the other
hand, will cause syntactic constraints to govern both
syntactic substitution and semantic unification. The
semantic types of constituents often give rise to dif-
ferences in semantic structure. If this type informa-
tion is not available during parsing, important clues
will be missing, and loss of accuracy will result.
Apparently, we do need some of the information
present in the types of semantic expressions. Ignor-
ing semantic types will result in loss of accuracy, but
distinguishing all different semantic types will result
in loss of coverage and generalizing power. With
these observations in mind, we decided to group the
types, and relax the constraints on semantic unifi-
cation. In this approach, every semantic expression,
2With thanks to Khalil Sima&apos;an for fruitful discus-
sions, and for the use of his parser
</bodyText>
<page confidence="0.998298">
163
</page>
<bodyText confidence="0.999989433333333">
and every variable, has a set of types associated with
it. In our semantic DOP model, we modify the con-
straints on semantic unification as follows: A vari-
able can be unified with an expression, if the inter-
section of their respective sets of types is not empty.
The semantic types are classified into sets that
can be distinguished on the basis of their behavior
in the tree-bank. We let the tree-bank data decide
which types can be grouped together, and which
types should be distinguished. This way we can
generalize over semantic types, and exploit relevant
type-information in the parsing process at the same
time. In learning the optimal grouping of types, we
have two concerns: keeping the number of different
sets of types to a minimum, and increasing the se-
mantic determinacy of syntactic structures enhanced
with type-information. We say that a subtree T,
with type-information at every node, is semantically
determinate, if we can determine a unique, correct
semantic rule for every CFG rule R3 occurring in T.
Semantic determinacy is very attractive from a com-
putational point of view: if our processed tree-bank
has semantic determinacy, we do not need to involve
the semantic rules in the parsing process. Instead,
the parser yields parses containing information re-
garding syntax and semantic types, and the actual
semantic rules can be determined on the basis of
that information. In the next section we will elabo-
rate on how we learn the grouping of semantic types
from the data.
</bodyText>
<subsectionHeader confidence="0.999165">
4.2 Classification of semantic types
</subsectionHeader>
<bodyText confidence="0.813226476190476">
The algorithm presented in this section proceeds by
grouping semantic types occurring with the same
syntactic label into mutually exclusive sets, and as-
signing to every syntactic label an index that indi-
cates to which set of types its corresponding seman-
tic type belongs. It is an iterative, greedy algorithm.
In every iteration a tuple, consisting of a syntactic
category and a set of types, is selected. Distinguish-
ing this tuple in the tree bank, leads to the great-
est increase in semantic determinacy that could be
found. Iteration continues until the increase in se-
mantic determinacy is below a certain threshold.
Before giving the algorithm, we need some defini-
tions:
3By &amp;quot;CFG rule&amp;quot;, we mean a subtree of depth 1, with-
out a specified root-node semantics, but with the features
relevant for substitution, i.e. syntactic category and se-
mantic type. Since the subtree of depth 1 is the smallest
structural building block of our DOP model, semantic
determinacy of every CFG rule in a subtree, means the
whole subtree is semantically determinate.
</bodyText>
<equation confidence="0.76322">
tuples()
</equation>
<bodyText confidence="0.9985335">
tuples(T) is the set of all pairs (c, s) in a tree-
bank T, where c is a syntactic category, and s is
the set of all semantic types that a constituent
of category c in T can have.
</bodyText>
<equation confidence="0.791591">
apply()
</equation>
<bodyText confidence="0.932640833333333">
if c is a category, s is a set of types, and T is a
tree-bank
then apply ((c, s),T) yields a tree-bank T&apos;, by
indexing each instance of category c in T, such
that the c constituent is of semantic type t E s,
with a unique index i.
</bodyText>
<equation confidence="0.971782">
amb()
if T is a tree-bank
</equation>
<bodyText confidence="0.999543285714286">
then amb(T) yields an n E N, such that n is the
sum of the frequencies of all CFG rules R that
occur in T with more than one corresponding
semantic rule.
The algorithm starts with a tree-bank To; in To,
the cardinality of tuples(To) equals the number of
different syn. Irtacc
</bodyText>
<equation confidence="0.799561666666667">
i=tio categories in To.
l
repeat
</equation>
<listItem confidence="0.9872266">
2. D((c, s)) = amb(Ti)—
amb (apply ((c, s), Ti))
= {(c,s1)13(c, s) E
tuples(Ti)&amp; s&apos; E 2181}
= argmax D(7.1)
7&amp;quot; ET,
3. i := + 1
4. Ti apply (r, , T2_1)
until D(Ti_i)
5. Ti_1
</listItem>
<bodyText confidence="0.999805111111111">
2181 is the powerset of s. In the implementation,
a limit can be set to the cardinality of s&apos; E 2181, to
avoid excessively long processing time. Obviously,
the iteration will always end, if we require 6 to be
&gt; 0. When the algorithm finishes, To, , con-
tain the category/set-of-types pairs that took the
largest steps towards semantic determinacy, and are
therefore distinguished in the tree-bank. The se-
mantic types not occurring in any of these pairs are
grouped together, and treated as equivalent.
Note that the algorithm cannot be guaranteed to
achieve full semantic determinacy. The degree of se-
mantic determinacy reached, depends on the consis-
tency of annotation, annotation errors, the granular-
ity of the type system, peculiarities of the language,
in short: on the nature of the tree-bank. To force
semantic determinacy, we assign a unique index to
those rare instances of categories, i.e, left hand sides
</bodyText>
<figure confidence="0.915870466666667">
(5)
164
dl.d2
PER VP
user d]..d2
tit v MP
wants (d1;d2)
wi/ MP MP
(d1:[d2d3]) di.d2
MP CON MP P NP
Idld2] ! tomorrow destination.place (d1;d2)
I
ADV MP maar morgen naar NP NP
today
niet vandaag
</figure>
<figureCaption confidence="0.986557">
Figure 9: A tree from the OVIS tree-bank
</figureCaption>
<figure confidence="0.7518915">
5 Experiments on the OVIS
tree-bank
</figure>
<footnote confidence="0.993166">
4Netherlands Organization for Scientific Research
5Public Transport Information System
</footnote>
<bodyText confidence="0.992667916666667">
workbench SEMTAGS is used. It is a graphical inter-
face, written by Bonnema, offering all functionality
needed for examining, evaluating, and editing syn-
tactic and semantic analyses. SEMTAGS is mainly
used for correcting the output of the DOP-parser.
It incrementally builds a probabilistic model of cor-
rected annotations, allowing it to quickly suggest al-
ternative semantic analyses to the annotator. It took
approximately 600 hours to annotate these 10.000
utterances (supervision included).
Syntactic annotation of the tree-bank is conven-
tional. There are 40 different syntactic categories in
</bodyText>
<subsectionHeader confidence="0.991504">
5.1 The Semantic formalism
</subsectionHeader>
<bodyText confidence="0.997349645833333">
The semantic formalism used in the OVIS
tree-bank, is a frame semantics, defined in
Veldhuijzen van Zanten (1996). In this section, we
give a very short impression. The well-formedness
and validity of an expression is decided on the ba-
sis of a type-lattice, called a frame structure. The
interpretation of an utterance, is an update of an
information state. An information state is a repre-
sentation of objects and the relations between them,
that complies to the frame structure. For OVIS, the
various objects are related to concepts in the train
travel domain. In updating an information state,
the notion of a slot-value assignment is used. Every
object can be a slot or a value. The slot-value assign-
ments are defined in a way that corresponds closely
to the linguistic notion of a ground-focus structure.
The slot is part of the common ground, the value
of CFG-rules, that do not have any distinguishing
features to account for their differing semantic rule.
Now the resulting tree-bank embodies a function
from CFG rules to semantic rules. We store this
function in a table, and strip all semantic rules from
the trees. As the experimental results in the next
section show, using a tree-bank obtained in this way
for data oriented semantic interpretation, results in
high coverage, and good probability estimations.
The NWO4 Priority Programme &amp;quot;Language and
Speech Technology&amp;quot; is a five year research pro-
gramme aiming at the development of advanced
telephone-based information systems. Within this
programme, the OVIS5 tree-bank is created. Using
a pilot version of the OVIS system, a large number
of human-machine dialogs were collected and tran-
scribed. Currently, 10.000 user utterances have re-
ceived a full syntactic and semantic analysis. Re-
grettably, the tree-bank is not available (yet) to the
public. More information on the tree-bank can be
found on http://grid.let.rug.n1:4321/. These-
mantic domain of all dialogs, is the Dutch railways
schedule. The user utterances are mostly answers
to questions, like: &amp;quot;From where to where do you
want to travel?&amp;quot;, &amp;quot;At what time do you want to
arrive in Amsterdam?&amp;quot;, &amp;quot;Could you please repeat
your destination?&amp;quot;. The annotation method is ro-
bust and flexible, as we are dealing with real, spo-
ken data, containing a lot of clearly ungrammatical
utterances. For the annotation task, the annotation
town.almere suffix.buiten the OVIS tree-bank, that appear to cover the syn-
</bodyText>
<equation confidence="0.49732">
1 1 1 1
</equation>
<bodyText confidence="0.733917913043478">
tactic domain quite well. No grammar is used to
almere buiten
determine the correct annotation; there is a small
set of guidelines, that has the degree of detail nec-
essary to avoid an &amp;quot;anything goes&amp;quot;-attitude in the
annotator, but leaves room for his/her perception of
the structure of an utterance. There is no concep-
tual division in the tree-bank between POS-tags and
nonterminal categories.
Figure 9 shows an example tree from the tree-
bank. It is an analysis of the Dutch sentence: &amp;quot;Ik(/)
wil(want) niet(not) vandaag(today) maar( but) mor-
gen(tomorrow) naar(to) Almere Buiten(A/mere
Buiten)&amp;quot; . The analysis uses the formula schemata
discussed in section 3.2, but here the interpreta-
tions of daughter-nodes are so-called &amp;quot;update&amp;quot; ex-
pressions, conforming to a frame structure, that
are combined into an update of an information
state. The complete interpretation of this utterance
is: user.wants.(([#today];[!tomorrow]);destination.-
place.(town.almere;suffix.buiten)). The semantic for-
malism employed in the tree-bank is the topic of the
next section.
</bodyText>
<page confidence="0.990706">
165
</page>
<figure confidence="0.88667675">
Interpretation: Exact Match
90 87.18 88.21 90 76 92 31
81.54
80
171.27
70 &apos;
1000 25&apos;00 40 00 5500 7000 8500
Trainingset size
</figure>
<figureCaption confidence="0.974702">
Figure 12: Max. depth of subtrees = 4
</figureCaption>
<figure confidence="0.991702666666667">
Interpretation: Exact Match
95%
91.28 91.79 92&apos;31 91.28
90_89.23-
85 2 3 4 5
Max. subtree depth
</figure>
<figureCaption confidence="0.975957">
Figure 10: Size of training set: 8500
</figureCaption>
<figure confidence="0.9359314">
Sem./Synt. Analysis: Exact Match
90-
85.64
1 2 3 4 5
Max. subtree depth
</figure>
<figureCaption confidence="0.999895">
Figure 11: Size of training set: 8500
</figureCaption>
<bodyText confidence="0.999985076923077">
is new information. Added to the semantic formal-
ism are pragmatic operators, corresponding to de-
nial, confirmation , correction and assertion6 that
indicate the relation between the value in its scope,
and the information state.
An update expression is a set of paths through the
frame structure, enhanced with pragmatic operators
that have scope over a certain part of a path. For
the semantic DOP model, the semantic type of an
expression cb is a pair of types (ti, t2). Given the
type-lattice Tof the frame structure, t1 is the lowest
upper bound in T of the paths in cb, and t2 is the
greatest lower bound in Tof the paths in 0.
</bodyText>
<subsectionHeader confidence="0.988532">
5.2 Experimental results
</subsectionHeader>
<bodyText confidence="0.999989857142857">
We performed a number of experiments, using a ran-
dom division of the tree-bank data into test- and
training-set. No provisions were taken for unknown
words. The results reported here, are obtained by
randomly selecting 300 trees from the tree-bank. All
utterances of length greater than one in this selection
are used as testing material. We varied the size of
the training-set, and the maximal depth of the sub-
trees. The average length of the test-sentences was
4.74 words. There was a constraint on the extrac-
tion of subtrees from the training-set trees: subtrees
could have a maximum of two substitution-sites, and
no more than three contiguous lexical nodes (Expe-
rience has shown that such limitations improve prob-
</bodyText>
<footnote confidence="0.8663965">
61n the example in figure 9, the pragmatic opera-
tors #, denial, and !, correction, are used
</footnote>
<table confidence="0.625799777777778">
Sem./Synt. Analysis: Exact Match
90 87.18 88.21
85.13 85.64
79.49
80-
68.71
70
1000 2500 4000 5500 7000 8500
Trainingset size
</table>
<figureCaption confidence="0.997341">
Figure 13: Max. depth of subtrees = 4
</figureCaption>
<bodyText confidence="0.9999109">
ability estimations, while retaining the full power of
DOP). Figures 10 and 11 show results using a train-
ing set size of 8500 trees. The maximal depth of sub-
trees involved in the parsing process was varied from
1 to 5. Results in figure 11 concern a match with
the total analysis in the test-set, whereas Figure 10
shows success on just the resulting interpretation.
Only exact matches with the trees and interpreta-
tions in the test-set were counted as successes. The
experiments show that involving larger fragments in
the parsing process leads to higher accuracy. Appar-
ently, for this domain fragments of depth 5 are too
large, and deteriorate probability estimations7. The
results also confirm our earlier findings, that seman-
tic parsing is robust. Quite a few analysis trees that
did not exactly match with their counterparts in the
test-set, yielded a semantic interpretation that did
match. Finally, figures 12 and 13 show results for
differing training-set sizes, using subtrees of maxi-
mal depth 4.
</bodyText>
<sectionHeader confidence="0.990697" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.50536575">
M. van den Berg, R. Bod, and R. Scha. 1994.
A Corpus-Based Approach to Semantic Interpre-
tation. In Proceedings Ninth Amsterdam Collo-
quium. ILLC,University of Amsterdam.
</reference>
<footnote confidence="0.853474666666667">
7Experiments using fragments of maximal depth 6
and maximal depth 7 yielded the same results as maxi-
mal depth 5
</footnote>
<figure confidence="0.944117">
85-83.08
80
87.0 88.21
86.66
</figure>
<page confidence="0.986016">
166
</page>
<reference confidence="0.999235431372549">
E. Black, R. Garside, and G. Leech. 1993.
Statistically-Driven Computer Grammars of En-
glish: The IBM/Lancaster Approach. Rodopi,
Amsterdam-Atlanta.
R. Bod. 1992. A computational model of language
performance: Data Oriented Parsing. In Proceed-
ings COLING &apos;92, Nantes.
R. Bod. 1993a. Monte Carlo Parsing. In Proceedings
Third International Workshop on Parsing Tech-
nologies, Tilburg/Durbuy.
R. Bod. 1993b. Using an Annotated Corpus as a
Stochastic Grammar. In Proceedings EACL&apos;93,
Utrecht.
R. Bod. 1995. Enriching Linguistics with
Statistics: Performance models of Natural
Language. Phd-thesis, ILLC-dissertation
series 1995-14, University of Amsterdam.
ftp://ftp.fwi.uva.nl/pub/theory/illc/-
dissertations/DS-95-14.text.ps.gz
R Bod. 1996. Two Questions about Data-Oriented
Parsing. In Proceedings Fourth Workshop on Very
Large Corpora, Copenhagen, Denmark. (cmp-
1g/9606022).
R. Bod, R. Bonnema, and R. Scha. 1996. A data-
oriented approach to semantic interpretation. In
Proceedings Workshop on Corpus-Oriented Se-
mantic Analysis, ECAI-96, Budapest, Hungary.
(cmp-lg/9606024).
R. Bod and R. Scha. 1996. Data-oriented lan-
guage processing. an overview. Technical Re-
port LP-96-13, Institute for Logic, Language and
Computation, University of Amsterdam. (cmp-
1g/9611003).
R. Bonnema. 1996. Data oriented se-
mantics. Master&apos;s thesis, Department of
Computational Linguistics, University of Am-
sterdam. http://mars.let.uva.n1/reniko_b/-
dopsem/scriptie .html
T. Briscoe. 1994. Prospects for practical parsing of
unrestricted text: Robust statistical parsing tech-
niques. In N. Oostdijk and P de Haan, editors,
Corpus-based Research into Language. Rodopi,
Amsterdam.
E. Charniak. 1996. Tree-bank grammars. In Pro-
ceedings AAAI&apos;96, Portland, Oregon.
L. Gamut. 1991. Logic, Language and Meaning.
Chicago University Press.
J Goodman. 1996. Efficient Algorithms for Parsing
the DOP Model. In Proceedings Empirical Meth-
ods in Natural Language Processing, Philadelphia,
Pennsylvania.
R. Kaplan. 1996. A probabilistic approach
to Lexical-Functional Grammar. Keynote pa-
per held at the LFG-workshop 1996, Greno-
ble, France. ftp://ftp.parc.xerox.com/pub/-
nl/slides/grenoble96/kaplan-doptalk .ps.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Lin-
guistics, 19(2).
F. Pereira and Y. Schabes. 1992. Inside-outside
reestimation from partially bracketed corpora. In
Proceedings of the 30th Annual Meeting of the
ACL, Newark, De.
M. Rajman. 1995a. Apports d&apos;une approche a base
de corpus aux techniques de traitement automa-
tique du langage naturel. Ph.D. thesis, Ecole Na-
tionale Superieure des Telecommunications, Paris.
M. Rajman. 1995b. Approche probabiliste de
l&apos;analyse syntaxique. Traitement Automatique des
Langues, 36:1-2.
S. Sekine and R. Grishman. 1995. A corpus-
based probabilistic grammar with only two
non-terminals. In Proceedings Fourth Interna-
tional Workshop on Parsing Technologies, Prague,
Czech Republic.
K. Sima&apos;an, R. Bod, S. Krauwer, and R. Scha. 1994.
Efficient Disambiguation by means of Stochastic
Tree Substitution Grammars. In Proceedings In-
ternational Conference on New Methods in Lan-
guage Processing. CCL, UMIST, Manchester.
K. Sima&apos;an. 1995. An optimized algorithm for Data
Oriented Parsing. In Proceedings International
Conference on Recent Advances in Natural Lan-
guage Processing. Tzigov Chark, Bulgaria.
K. Sima&apos;an. 1996a. An optimized algorithm for
Data Oriented Parsing. In R. Mitkov and N. Ni-
colov, editors, Recent Advances in Natural Lan-
guage Processing 1995, volume 136 of Current Is-
sues in Linguistic Theory. John Benjamins, Ams-
terdam.
K. Sima&apos;an. 1996b. Computational Complexity of
Probabilistic Disambiguation by means of Tree-
Grammars. In Proceedings COLING &apos;96, Copen-
hagen, Denmark.
D. Tugwell. 1995. A state-transition grammar for
data-oriented parsing. In Proceedings European
Chapter of the ACL&apos;95, Dublin, Ireland.
G. Veldhuijzen van Zanten. 1996. Seman-
tics of update expressions. NWO priority
Programme Language and Speech Technology,
http://grid.let.rug.n1:4321/.
</reference>
<page confidence="0.997615">
167
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.514717">
<title confidence="0.994438">A DOP Model for Semantic Interpretation*</title>
<author confidence="0.887293">Rens Bod Scha Bonnema</author>
<affiliation confidence="0.9962905">Institute for Logic, Language and Computation University of Amsterdam</affiliation>
<address confidence="0.97078">Spuistraat 134, 1012 VB Amsterdam</address>
<note confidence="0.78466">Bonnemaelmars.let.uva.n1 Rens.Bod@let.uva.n1 Remko.SchAlet.uva.n1</note>
<abstract confidence="0.999215909090909">In data-oriented language processing, an annotated language corpus is used as a stochastic grammar. The most probable analysis of a new sentence is constructed by combining fragments from the corpus in the most probable way. This approach has been successfully used for syntactic analysis, using corpora with syntactic annotations such as the Penn Tree-bank. If a corpus with semantically annotated sentences is used, the same approach can also generate the most probable semantic interpretation of an input sentence. The present paper explains this semantic interpretation method. A data-oriented semantic interpretation algorithm was tested on two semantically annotated corpora: the English ATIS corpus and the Dutch OVIS corpus. Experiments show an increase in semantic accuracy if larger corpus-fragments are taken into consideration.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M van den Berg</author>
<author>R Bod</author>
<author>R Scha</author>
</authors>
<title>A Corpus-Based Approach to Semantic Interpretation.</title>
<date>1994</date>
<booktitle>In Proceedings Ninth Amsterdam Colloquium. ILLC,University of</booktitle>
<location>Amsterdam.</location>
<marker>van den Berg, Bod, Scha, 1994</marker>
<rawString>M. van den Berg, R. Bod, and R. Scha. 1994. A Corpus-Based Approach to Semantic Interpretation. In Proceedings Ninth Amsterdam Colloquium. ILLC,University of Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Black</author>
<author>R Garside</author>
<author>G Leech</author>
</authors>
<title>Statistically-Driven Computer Grammars of English: The IBM/Lancaster Approach.</title>
<date>1993</date>
<location>Rodopi, Amsterdam-Atlanta.</location>
<contexts>
<context position="5146" citStr="Black et al. (1993)" startWordPosition="846" endWordPosition="849">vation t1 • • o 4, can be computed as the product of the probabilities of the subtrees this derivation consists of: P(ti o• • •o tn) ft P(t). The probability of a parse tree is equal to 1Here touovow should be read as ((to u) o v) ow. the probability that any of its distinct derivations is generated, which is the sum of the probabilities of all derivations of that parse tree. Let td be the i-th subtree in the derivation d that yields tree T, then the probability of T is given by: P(T) = Ed ni P(tsd). The DOP method differs from other statistical approaches, such as Pereira and Schabes (1992), Black et al. (1993) and Briscoe (1994), in that it does not predefine or train a formal grammar; instead it takes subtrees directly from annotated sentences in a treebank with a probability proportional to the number of occurrences of these subtrees in the treebank. Bod (1993b) shows that DOP can be implemented using context-free parsing techniques. To select the most probable parse, Bod (1993a) gives a Monte Carlo approximation algorithm. Sima&apos;an (1995) gives an efficient polynomial algorithm for a sub-optimal solution. The model was tested on the Air Travel Information System (ATIS) corpus as analyzed in the P</context>
</contexts>
<marker>Black, Garside, Leech, 1993</marker>
<rawString>E. Black, R. Garside, and G. Leech. 1993. Statistically-Driven Computer Grammars of English: The IBM/Lancaster Approach. Rodopi, Amsterdam-Atlanta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bod</author>
</authors>
<title>A computational model of language performance: Data Oriented Parsing.</title>
<date>1992</date>
<booktitle>In Proceedings COLING &apos;92,</booktitle>
<location>Nantes.</location>
<contexts>
<context position="1973" citStr="Bod (1992" startWordPosition="282" endWordPosition="283">large corpora of linguistic representations of previously occurring utterances. When processing a new input utterance, analyses of this utterance are constructed by combining fragments from the corpus; the occurrence-frequencies of the fragments are used to estimate which analysis is the most probable one. * This work was partially supported by NWO, the Netherlands Organization for Scientific Research (Priority Programme Language and Speech Technology). For the syntactic dimension of language, various instantiations of this data-oriented processing or &amp;quot;DOP&amp;quot; approach have been worked out (e.g. Bod (1992-1995); Charniak (1996); Tugwell (1995); Sima&apos;an et al. (1994); Sima&apos;an (1994; 1996a); Goodman (1996); Rajman (1995ab); Kaplan (1996); Sekine and Grishman (1995)). A method for extending it to the semantic domain was first introduced by van den Berg et al. (1994). In the present paper we discuss a computationally effective version of that method, and an implemented system that uses it. We first summarize the first fully instantiated DOP model as presented in Bod (1992-1993). Then we show how this method can be straightforwardly extended into a semantic analysis method, if corpora are created i</context>
</contexts>
<marker>Bod, 1992</marker>
<rawString>R. Bod. 1992. A computational model of language performance: Data Oriented Parsing. In Proceedings COLING &apos;92, Nantes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bod</author>
</authors>
<title>Monte Carlo Parsing.</title>
<date>1993</date>
<booktitle>In Proceedings Third International Workshop on Parsing Technologies,</booktitle>
<location>Tilburg/Durbuy.</location>
<contexts>
<context position="5403" citStr="Bod (1993" startWordPosition="894" endWordPosition="895">distinct derivations is generated, which is the sum of the probabilities of all derivations of that parse tree. Let td be the i-th subtree in the derivation d that yields tree T, then the probability of T is given by: P(T) = Ed ni P(tsd). The DOP method differs from other statistical approaches, such as Pereira and Schabes (1992), Black et al. (1993) and Briscoe (1994), in that it does not predefine or train a formal grammar; instead it takes subtrees directly from annotated sentences in a treebank with a probability proportional to the number of occurrences of these subtrees in the treebank. Bod (1993b) shows that DOP can be implemented using context-free parsing techniques. To select the most probable parse, Bod (1993a) gives a Monte Carlo approximation algorithm. Sima&apos;an (1995) gives an efficient polynomial algorithm for a sub-optimal solution. The model was tested on the Air Travel Information System (ATIS) corpus as analyzed in the Penn Treebank (Marcus et al. (1993)), achieving better test results than other stochastic grammars (cf. Bod (1996), Sima&apos;an (1996a), Goodman (1996)). On Penn&apos;s Wall Street Journal corpus, the data-oriented processing approach has been tested by Sekine and Gr</context>
</contexts>
<marker>Bod, 1993</marker>
<rawString>R. Bod. 1993a. Monte Carlo Parsing. In Proceedings Third International Workshop on Parsing Technologies, Tilburg/Durbuy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bod</author>
</authors>
<title>Using an Annotated Corpus as a Stochastic Grammar.</title>
<date>1993</date>
<booktitle>In Proceedings EACL&apos;93,</booktitle>
<location>Utrecht.</location>
<contexts>
<context position="5403" citStr="Bod (1993" startWordPosition="894" endWordPosition="895">distinct derivations is generated, which is the sum of the probabilities of all derivations of that parse tree. Let td be the i-th subtree in the derivation d that yields tree T, then the probability of T is given by: P(T) = Ed ni P(tsd). The DOP method differs from other statistical approaches, such as Pereira and Schabes (1992), Black et al. (1993) and Briscoe (1994), in that it does not predefine or train a formal grammar; instead it takes subtrees directly from annotated sentences in a treebank with a probability proportional to the number of occurrences of these subtrees in the treebank. Bod (1993b) shows that DOP can be implemented using context-free parsing techniques. To select the most probable parse, Bod (1993a) gives a Monte Carlo approximation algorithm. Sima&apos;an (1995) gives an efficient polynomial algorithm for a sub-optimal solution. The model was tested on the Air Travel Information System (ATIS) corpus as analyzed in the Penn Treebank (Marcus et al. (1993)), achieving better test results than other stochastic grammars (cf. Bod (1996), Sima&apos;an (1996a), Goodman (1996)). On Penn&apos;s Wall Street Journal corpus, the data-oriented processing approach has been tested by Sekine and Gr</context>
</contexts>
<marker>Bod, 1993</marker>
<rawString>R. Bod. 1993b. Using an Annotated Corpus as a Stochastic Grammar. In Proceedings EACL&apos;93, Utrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bod</author>
</authors>
<title>Enriching Linguistics with Statistics: Performance models of Natural Language. Phd-thesis, ILLC-dissertation series 1995-14,</title>
<date>1995</date>
<pages>95--14</pages>
<institution>University of Amsterdam.</institution>
<marker>Bod, 1995</marker>
<rawString>R. Bod. 1995. Enriching Linguistics with Statistics: Performance models of Natural Language. Phd-thesis, ILLC-dissertation series 1995-14, University of Amsterdam. ftp://ftp.fwi.uva.nl/pub/theory/illc/-dissertations/DS-95-14.text.ps.gz</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bod</author>
</authors>
<title>Two Questions about Data-Oriented Parsing.</title>
<date>1996</date>
<booktitle>In Proceedings Fourth Workshop on Very Large Corpora,</booktitle>
<location>Copenhagen,</location>
<contexts>
<context position="5859" citStr="Bod (1996)" startWordPosition="967" endWordPosition="968"> directly from annotated sentences in a treebank with a probability proportional to the number of occurrences of these subtrees in the treebank. Bod (1993b) shows that DOP can be implemented using context-free parsing techniques. To select the most probable parse, Bod (1993a) gives a Monte Carlo approximation algorithm. Sima&apos;an (1995) gives an efficient polynomial algorithm for a sub-optimal solution. The model was tested on the Air Travel Information System (ATIS) corpus as analyzed in the Penn Treebank (Marcus et al. (1993)), achieving better test results than other stochastic grammars (cf. Bod (1996), Sima&apos;an (1996a), Goodman (1996)). On Penn&apos;s Wall Street Journal corpus, the data-oriented processing approach has been tested by Sekine and Grishman (1995) and by Charniak (1996). Though Charniak only uses corpus-subtrees smaller than depth 2 (which in our experience constitutes a less-than-optimal version of the data-oriented processing method), he reports that it &amp;quot;outperforms all other non-word-based statistical parsers/grammars on this corpus&amp;quot;. For an overview of data-oriented language processing, we refer to (Bod and Scha, 1996). 3 Data-Oriented Semantic Analysis To use the DOP method no</context>
</contexts>
<marker>Bod, 1996</marker>
<rawString>R Bod. 1996. Two Questions about Data-Oriented Parsing. In Proceedings Fourth Workshop on Very Large Corpora, Copenhagen, Denmark. (cmp1g/9606022).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bod</author>
<author>R Bonnema</author>
<author>R Scha</author>
</authors>
<title>A dataoriented approach to semantic interpretation.</title>
<date>1996</date>
<booktitle>In Proceedings Workshop on Corpus-Oriented Semantic Analysis, ECAI-96,</booktitle>
<location>Budapest,</location>
<contexts>
<context position="16142" citStr="Bod et al. (1996)" startWordPosition="2639" endWordPosition="2642">y combination of semantically annotated subtrees. As was shown in Sima&apos;an (1996b), the most likely interpretation of a string cannot be computed in deterministic polynomial time. It is not yet known how often the most likely interpretation and the interpretation of the most likely combination of semantically enriched subtrees do actually coincide. 4 Implementations The first implementation of a semantic DOP-model yielded rather encouraging preliminary results on a semantically enriched part of the ATIS-corpus. Implementation details and experimental results can be found in Bonnema (1996), and Bod et al. (1996). We repeat the most important observations: • Data-oriented semantic interpretation seems to be robust; of the sentences that could be parsed, a significantly higher percentage received a correct semantic interpretation (88%), than an exactly correct syntactic analysis (62%). • The coverage of the parser was rather low (72%), because of the sheer number of different semantic types and constructs in the trees. • The parser was fast: on the average six times as fast as a parser trained on syntax alone. The current implementation is again an extension of Sima&apos;an (1996a), by Bonnema2. In our expe</context>
</contexts>
<marker>Bod, Bonnema, Scha, 1996</marker>
<rawString>R. Bod, R. Bonnema, and R. Scha. 1996. A dataoriented approach to semantic interpretation. In Proceedings Workshop on Corpus-Oriented Semantic Analysis, ECAI-96, Budapest, Hungary. (cmp-lg/9606024).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bod</author>
<author>R Scha</author>
</authors>
<title>Data-oriented language processing. an overview.</title>
<date>1996</date>
<tech>Technical Report LP-96-13,</tech>
<institution>Institute for Logic, Language and Computation, University of Amsterdam.</institution>
<contexts>
<context position="6399" citStr="Bod and Scha, 1996" startWordPosition="1042" endWordPosition="1045">), achieving better test results than other stochastic grammars (cf. Bod (1996), Sima&apos;an (1996a), Goodman (1996)). On Penn&apos;s Wall Street Journal corpus, the data-oriented processing approach has been tested by Sekine and Grishman (1995) and by Charniak (1996). Though Charniak only uses corpus-subtrees smaller than depth 2 (which in our experience constitutes a less-than-optimal version of the data-oriented processing method), he reports that it &amp;quot;outperforms all other non-word-based statistical parsers/grammars on this corpus&amp;quot;. For an overview of data-oriented language processing, we refer to (Bod and Scha, 1996). 3 Data-Oriented Semantic Analysis To use the DOP method not just for syntactic analysis, but also for semantic interpretation, four steps must be taken: 1. decide on a formalism for representing the meanings of sentences and surface-constituents. 2. annotate the corpus-sentences and their surface-constituents with such semantic representations. 3. establish a method for deriving the meaning representations associated with arbitrary corpus-subtrees and with compositions of such subtrees. 4. reconsider the probability calculations. We now discuss these four steps. 3.1 Semantic formalism The de</context>
</contexts>
<marker>Bod, Scha, 1996</marker>
<rawString>R. Bod and R. Scha. 1996. Data-oriented language processing. an overview. Technical Report LP-96-13, Institute for Logic, Language and Computation, University of Amsterdam. (cmp1g/9611003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bonnema</author>
</authors>
<title>Data oriented semantics.</title>
<date>1996</date>
<tech>Master&apos;s thesis,</tech>
<institution>Department of Computational Linguistics, University of Amsterdam.</institution>
<note>http://mars.let.uva.n1/reniko_b/-dopsem/scriptie .html</note>
<contexts>
<context position="8130" citStr="Bonnema (1996)" startWordPosition="1282" endWordPosition="1283">x)) N:man whistles a man Figure 6: Same imaginary corpus of two trees with syntactic and semantic labels using the daughter notation. defined model-theory and is rich enough for representing the meanings of sentences and constituents that are relevant for the intended application domain. For our exposition in this paper we will use a wellknown standard formalism: extensional type theory (see Gamut (1991)), i.e., a higher-order logical language that combines lambda-abstraction with connectives and quantifiers. The first implemented system for data-oriented semantic interpretation, presented in Bonnema (1996), used a different logical language, however. And in many application contexts it probably makes sense to use an A.I.-style language which highlights domain structure (frames, slots, and fillers), while limiting the use of quantification and negation (see section 5). 3.2 Semantic annotation We assume a corpus that is already syntactically annotated as before: with labelled trees that indicate surface constituent structure. Now the basic idea, taken from van den Berg et al. (1994), is to augment this syntactic annotation with a semantic one: to every meaningful syntactic node, we add a type-log</context>
<context position="10834" citStr="Bonnema (1996)" startWordPosition="1737" endWordPosition="1738">uent, etc. Using this notation, the semantically annotated version of the toy corpus of Figure 1 is the toy corpus rendered in Figure 6. This kind of semantic annotation is what will be used in the construction of the corpora described in section 5 of this paper. It may be noted that the rather oblique description of the semantics of the higher nodes in the tree would easily lead to mistakes, if annotation would be carried out completely manually. An annotation tool that makes the expanded versions of the formulas visible for the annotator is obviously called for. Such a tool was developed by Bonnema (1996), it will be briefly described in section 5. 161 This annotation convention obviously, assumes that the meaning representation of a surfaceconstituent can in fact always be composed out of the meaning representations of its subconstituents. This assumption is not unproblematic. To maintain it in the face of phenomena such as non-standard quantifier scope or discontinuous constituents creates complications in the syntactic or semantic analyses assigned to certain sentences and their constituents. It is therefore not clear yet whether our current treatment ought to be viewed as completely genera</context>
<context position="15339" citStr="Bonnema (1996)" startWordPosition="2518" endWordPosition="2519">ia) (3) d i An interpretation of a string is a formula which is provably equivalent to the semantic annotation of the top node of a parse of this string. The probability of an interpretation I of a string is the sum of the probabilities of the parses of this string with a top node annotated with a formula that is provably equivalent to I. Let tidp be the i-th subtree in the derivation d that yields parse p with interpretation I, then the probability of I is given by: EEll P(td) (4) p d i We choose the most probable interpretation I of a string s as the most appropriate interpretation of s. In Bonnema (1996) a semantic extension of the DOP parser of Sima&apos;an (1996a) is given. But instead of computing the most likely interpretation of a string, it computes the interpretation of the most likely combination of semantically annotated subtrees. As was shown in Sima&apos;an (1996b), the most likely interpretation of a string cannot be computed in deterministic polynomial time. It is not yet known how often the most likely interpretation and the interpretation of the most likely combination of semantically enriched subtrees do actually coincide. 4 Implementations The first implementation of a semantic DOP-mod</context>
</contexts>
<marker>Bonnema, 1996</marker>
<rawString>R. Bonnema. 1996. Data oriented semantics. Master&apos;s thesis, Department of Computational Linguistics, University of Amsterdam. http://mars.let.uva.n1/reniko_b/-dopsem/scriptie .html</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Briscoe</author>
</authors>
<title>Prospects for practical parsing of unrestricted text: Robust statistical parsing techniques.</title>
<date>1994</date>
<booktitle>Corpus-based Research into Language. Rodopi,</booktitle>
<editor>In N. Oostdijk and P de Haan, editors,</editor>
<location>Amsterdam.</location>
<contexts>
<context position="5165" citStr="Briscoe (1994)" startWordPosition="851" endWordPosition="852">e computed as the product of the probabilities of the subtrees this derivation consists of: P(ti o• • •o tn) ft P(t). The probability of a parse tree is equal to 1Here touovow should be read as ((to u) o v) ow. the probability that any of its distinct derivations is generated, which is the sum of the probabilities of all derivations of that parse tree. Let td be the i-th subtree in the derivation d that yields tree T, then the probability of T is given by: P(T) = Ed ni P(tsd). The DOP method differs from other statistical approaches, such as Pereira and Schabes (1992), Black et al. (1993) and Briscoe (1994), in that it does not predefine or train a formal grammar; instead it takes subtrees directly from annotated sentences in a treebank with a probability proportional to the number of occurrences of these subtrees in the treebank. Bod (1993b) shows that DOP can be implemented using context-free parsing techniques. To select the most probable parse, Bod (1993a) gives a Monte Carlo approximation algorithm. Sima&apos;an (1995) gives an efficient polynomial algorithm for a sub-optimal solution. The model was tested on the Air Travel Information System (ATIS) corpus as analyzed in the Penn Treebank (Marcu</context>
</contexts>
<marker>Briscoe, 1994</marker>
<rawString>T. Briscoe. 1994. Prospects for practical parsing of unrestricted text: Robust statistical parsing techniques. In N. Oostdijk and P de Haan, editors, Corpus-based Research into Language. Rodopi, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Tree-bank grammars.</title>
<date>1996</date>
<booktitle>In Proceedings AAAI&apos;96,</booktitle>
<location>Portland, Oregon.</location>
<contexts>
<context position="1996" citStr="Charniak (1996)" startWordPosition="284" endWordPosition="285">linguistic representations of previously occurring utterances. When processing a new input utterance, analyses of this utterance are constructed by combining fragments from the corpus; the occurrence-frequencies of the fragments are used to estimate which analysis is the most probable one. * This work was partially supported by NWO, the Netherlands Organization for Scientific Research (Priority Programme Language and Speech Technology). For the syntactic dimension of language, various instantiations of this data-oriented processing or &amp;quot;DOP&amp;quot; approach have been worked out (e.g. Bod (1992-1995); Charniak (1996); Tugwell (1995); Sima&apos;an et al. (1994); Sima&apos;an (1994; 1996a); Goodman (1996); Rajman (1995ab); Kaplan (1996); Sekine and Grishman (1995)). A method for extending it to the semantic domain was first introduced by van den Berg et al. (1994). In the present paper we discuss a computationally effective version of that method, and an implemented system that uses it. We first summarize the first fully instantiated DOP model as presented in Bod (1992-1993). Then we show how this method can be straightforwardly extended into a semantic analysis method, if corpora are created in which the trees are e</context>
<context position="6039" citStr="Charniak (1996)" startWordPosition="994" endWordPosition="995"> be implemented using context-free parsing techniques. To select the most probable parse, Bod (1993a) gives a Monte Carlo approximation algorithm. Sima&apos;an (1995) gives an efficient polynomial algorithm for a sub-optimal solution. The model was tested on the Air Travel Information System (ATIS) corpus as analyzed in the Penn Treebank (Marcus et al. (1993)), achieving better test results than other stochastic grammars (cf. Bod (1996), Sima&apos;an (1996a), Goodman (1996)). On Penn&apos;s Wall Street Journal corpus, the data-oriented processing approach has been tested by Sekine and Grishman (1995) and by Charniak (1996). Though Charniak only uses corpus-subtrees smaller than depth 2 (which in our experience constitutes a less-than-optimal version of the data-oriented processing method), he reports that it &amp;quot;outperforms all other non-word-based statistical parsers/grammars on this corpus&amp;quot;. For an overview of data-oriented language processing, we refer to (Bod and Scha, 1996). 3 Data-Oriented Semantic Analysis To use the DOP method not just for syntactic analysis, but also for semantic interpretation, four steps must be taken: 1. decide on a formalism for representing the meanings of sentences and surface-const</context>
</contexts>
<marker>Charniak, 1996</marker>
<rawString>E. Charniak. 1996. Tree-bank grammars. In Proceedings AAAI&apos;96, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Gamut</author>
</authors>
<title>Logic, Language and Meaning.</title>
<date>1991</date>
<publisher>Chicago University Press.</publisher>
<contexts>
<context position="7923" citStr="Gamut (1991)" startWordPosition="1256" endWordPosition="1257">re 5: Imaginary corpus of two trees with syntactic and semantic labels. S:d1(d2) NP:d1(d2) VP:sing 1 Det:XXXYchc(X(x)-.Y(x)) N:woman sings 1 1 every woman S:d1(d2) NP:d1(d2) VP:whistle Det:XXXY3x/\(X(x)AY(x)) N:man whistles a man Figure 6: Same imaginary corpus of two trees with syntactic and semantic labels using the daughter notation. defined model-theory and is rich enough for representing the meanings of sentences and constituents that are relevant for the intended application domain. For our exposition in this paper we will use a wellknown standard formalism: extensional type theory (see Gamut (1991)), i.e., a higher-order logical language that combines lambda-abstraction with connectives and quantifiers. The first implemented system for data-oriented semantic interpretation, presented in Bonnema (1996), used a different logical language, however. And in many application contexts it probably makes sense to use an A.I.-style language which highlights domain structure (frames, slots, and fillers), while limiting the use of quantification and negation (see section 5). 3.2 Semantic annotation We assume a corpus that is already syntactically annotated as before: with labelled trees that indica</context>
</contexts>
<marker>Gamut, 1991</marker>
<rawString>L. Gamut. 1991. Logic, Language and Meaning. Chicago University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Efficient Algorithms for Parsing the DOP Model.</title>
<date>1996</date>
<booktitle>In Proceedings Empirical Methods in Natural Language Processing,</booktitle>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="2074" citStr="Goodman (1996)" startWordPosition="295" endWordPosition="296">a new input utterance, analyses of this utterance are constructed by combining fragments from the corpus; the occurrence-frequencies of the fragments are used to estimate which analysis is the most probable one. * This work was partially supported by NWO, the Netherlands Organization for Scientific Research (Priority Programme Language and Speech Technology). For the syntactic dimension of language, various instantiations of this data-oriented processing or &amp;quot;DOP&amp;quot; approach have been worked out (e.g. Bod (1992-1995); Charniak (1996); Tugwell (1995); Sima&apos;an et al. (1994); Sima&apos;an (1994; 1996a); Goodman (1996); Rajman (1995ab); Kaplan (1996); Sekine and Grishman (1995)). A method for extending it to the semantic domain was first introduced by van den Berg et al. (1994). In the present paper we discuss a computationally effective version of that method, and an implemented system that uses it. We first summarize the first fully instantiated DOP model as presented in Bod (1992-1993). Then we show how this method can be straightforwardly extended into a semantic analysis method, if corpora are created in which the trees are enriched with semantic annotations. Finally, we discuss an implementation and r</context>
<context position="5892" citStr="Goodman (1996)" startWordPosition="971" endWordPosition="972">ences in a treebank with a probability proportional to the number of occurrences of these subtrees in the treebank. Bod (1993b) shows that DOP can be implemented using context-free parsing techniques. To select the most probable parse, Bod (1993a) gives a Monte Carlo approximation algorithm. Sima&apos;an (1995) gives an efficient polynomial algorithm for a sub-optimal solution. The model was tested on the Air Travel Information System (ATIS) corpus as analyzed in the Penn Treebank (Marcus et al. (1993)), achieving better test results than other stochastic grammars (cf. Bod (1996), Sima&apos;an (1996a), Goodman (1996)). On Penn&apos;s Wall Street Journal corpus, the data-oriented processing approach has been tested by Sekine and Grishman (1995) and by Charniak (1996). Though Charniak only uses corpus-subtrees smaller than depth 2 (which in our experience constitutes a less-than-optimal version of the data-oriented processing method), he reports that it &amp;quot;outperforms all other non-word-based statistical parsers/grammars on this corpus&amp;quot;. For an overview of data-oriented language processing, we refer to (Bod and Scha, 1996). 3 Data-Oriented Semantic Analysis To use the DOP method not just for syntactic analysis, bu</context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>J Goodman. 1996. Efficient Algorithms for Parsing the DOP Model. In Proceedings Empirical Methods in Natural Language Processing, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kaplan</author>
</authors>
<title>A probabilistic approach to Lexical-Functional Grammar. Keynote paper held at the LFG-workshop</title>
<date>1996</date>
<location>Grenoble,</location>
<note>ftp://ftp.parc.xerox.com/pub/-nl/slides/grenoble96/kaplan-doptalk .ps.</note>
<contexts>
<context position="2106" citStr="Kaplan (1996)" startWordPosition="299" endWordPosition="300">f this utterance are constructed by combining fragments from the corpus; the occurrence-frequencies of the fragments are used to estimate which analysis is the most probable one. * This work was partially supported by NWO, the Netherlands Organization for Scientific Research (Priority Programme Language and Speech Technology). For the syntactic dimension of language, various instantiations of this data-oriented processing or &amp;quot;DOP&amp;quot; approach have been worked out (e.g. Bod (1992-1995); Charniak (1996); Tugwell (1995); Sima&apos;an et al. (1994); Sima&apos;an (1994; 1996a); Goodman (1996); Rajman (1995ab); Kaplan (1996); Sekine and Grishman (1995)). A method for extending it to the semantic domain was first introduced by van den Berg et al. (1994). In the present paper we discuss a computationally effective version of that method, and an implemented system that uses it. We first summarize the first fully instantiated DOP model as presented in Bod (1992-1993). Then we show how this method can be straightforwardly extended into a semantic analysis method, if corpora are created in which the trees are enriched with semantic annotations. Finally, we discuss an implementation and report on experiments with two se</context>
</contexts>
<marker>Kaplan, 1996</marker>
<rawString>R. Kaplan. 1996. A probabilistic approach to Lexical-Functional Grammar. Keynote paper held at the LFG-workshop 1996, Grenoble, France. ftp://ftp.parc.xerox.com/pub/-nl/slides/grenoble96/kaplan-doptalk .ps.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="5780" citStr="Marcus et al. (1993)" startWordPosition="953" endWordPosition="956">1994), in that it does not predefine or train a formal grammar; instead it takes subtrees directly from annotated sentences in a treebank with a probability proportional to the number of occurrences of these subtrees in the treebank. Bod (1993b) shows that DOP can be implemented using context-free parsing techniques. To select the most probable parse, Bod (1993a) gives a Monte Carlo approximation algorithm. Sima&apos;an (1995) gives an efficient polynomial algorithm for a sub-optimal solution. The model was tested on the Air Travel Information System (ATIS) corpus as analyzed in the Penn Treebank (Marcus et al. (1993)), achieving better test results than other stochastic grammars (cf. Bod (1996), Sima&apos;an (1996a), Goodman (1996)). On Penn&apos;s Wall Street Journal corpus, the data-oriented processing approach has been tested by Sekine and Grishman (1995) and by Charniak (1996). Though Charniak only uses corpus-subtrees smaller than depth 2 (which in our experience constitutes a less-than-optimal version of the data-oriented processing method), he reports that it &amp;quot;outperforms all other non-word-based statistical parsers/grammars on this corpus&amp;quot;. For an overview of data-oriented language processing, we refer to (</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>Y Schabes</author>
</authors>
<title>Inside-outside reestimation from partially bracketed corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th Annual Meeting of the ACL,</booktitle>
<location>Newark, De.</location>
<contexts>
<context position="5125" citStr="Pereira and Schabes (1992)" startWordPosition="842" endWordPosition="845">i. The probability of a derivation t1 • • o 4, can be computed as the product of the probabilities of the subtrees this derivation consists of: P(ti o• • •o tn) ft P(t). The probability of a parse tree is equal to 1Here touovow should be read as ((to u) o v) ow. the probability that any of its distinct derivations is generated, which is the sum of the probabilities of all derivations of that parse tree. Let td be the i-th subtree in the derivation d that yields tree T, then the probability of T is given by: P(T) = Ed ni P(tsd). The DOP method differs from other statistical approaches, such as Pereira and Schabes (1992), Black et al. (1993) and Briscoe (1994), in that it does not predefine or train a formal grammar; instead it takes subtrees directly from annotated sentences in a treebank with a probability proportional to the number of occurrences of these subtrees in the treebank. Bod (1993b) shows that DOP can be implemented using context-free parsing techniques. To select the most probable parse, Bod (1993a) gives a Monte Carlo approximation algorithm. Sima&apos;an (1995) gives an efficient polynomial algorithm for a sub-optimal solution. The model was tested on the Air Travel Information System (ATIS) corpus</context>
</contexts>
<marker>Pereira, Schabes, 1992</marker>
<rawString>F. Pereira and Y. Schabes. 1992. Inside-outside reestimation from partially bracketed corpora. In Proceedings of the 30th Annual Meeting of the ACL, Newark, De.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rajman</author>
</authors>
<title>Apports d&apos;une approche a base de corpus aux techniques de traitement automatique du langage naturel.</title>
<date>1995</date>
<booktitle>Ph.D. thesis, Ecole Nationale Superieure des Telecommunications,</booktitle>
<location>Paris.</location>
<contexts>
<context position="2088" citStr="Rajman (1995" startWordPosition="297" endWordPosition="298">rance, analyses of this utterance are constructed by combining fragments from the corpus; the occurrence-frequencies of the fragments are used to estimate which analysis is the most probable one. * This work was partially supported by NWO, the Netherlands Organization for Scientific Research (Priority Programme Language and Speech Technology). For the syntactic dimension of language, various instantiations of this data-oriented processing or &amp;quot;DOP&amp;quot; approach have been worked out (e.g. Bod (1992-1995); Charniak (1996); Tugwell (1995); Sima&apos;an et al. (1994); Sima&apos;an (1994; 1996a); Goodman (1996); Rajman (1995ab); Kaplan (1996); Sekine and Grishman (1995)). A method for extending it to the semantic domain was first introduced by van den Berg et al. (1994). In the present paper we discuss a computationally effective version of that method, and an implemented system that uses it. We first summarize the first fully instantiated DOP model as presented in Bod (1992-1993). Then we show how this method can be straightforwardly extended into a semantic analysis method, if corpora are created in which the trees are enriched with semantic annotations. Finally, we discuss an implementation and report on exper</context>
</contexts>
<marker>Rajman, 1995</marker>
<rawString>M. Rajman. 1995a. Apports d&apos;une approche a base de corpus aux techniques de traitement automatique du langage naturel. Ph.D. thesis, Ecole Nationale Superieure des Telecommunications, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rajman</author>
</authors>
<title>Approche probabiliste de l&apos;analyse syntaxique.</title>
<date>1995</date>
<booktitle>Traitement Automatique des Langues,</booktitle>
<pages>36--1</pages>
<contexts>
<context position="2088" citStr="Rajman (1995" startWordPosition="297" endWordPosition="298">rance, analyses of this utterance are constructed by combining fragments from the corpus; the occurrence-frequencies of the fragments are used to estimate which analysis is the most probable one. * This work was partially supported by NWO, the Netherlands Organization for Scientific Research (Priority Programme Language and Speech Technology). For the syntactic dimension of language, various instantiations of this data-oriented processing or &amp;quot;DOP&amp;quot; approach have been worked out (e.g. Bod (1992-1995); Charniak (1996); Tugwell (1995); Sima&apos;an et al. (1994); Sima&apos;an (1994; 1996a); Goodman (1996); Rajman (1995ab); Kaplan (1996); Sekine and Grishman (1995)). A method for extending it to the semantic domain was first introduced by van den Berg et al. (1994). In the present paper we discuss a computationally effective version of that method, and an implemented system that uses it. We first summarize the first fully instantiated DOP model as presented in Bod (1992-1993). Then we show how this method can be straightforwardly extended into a semantic analysis method, if corpora are created in which the trees are enriched with semantic annotations. Finally, we discuss an implementation and report on exper</context>
</contexts>
<marker>Rajman, 1995</marker>
<rawString>M. Rajman. 1995b. Approche probabiliste de l&apos;analyse syntaxique. Traitement Automatique des Langues, 36:1-2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sekine</author>
<author>R Grishman</author>
</authors>
<title>A corpusbased probabilistic grammar with only two non-terminals.</title>
<date>1995</date>
<booktitle>In Proceedings Fourth International Workshop on Parsing Technologies,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2134" citStr="Sekine and Grishman (1995)" startWordPosition="301" endWordPosition="304">e are constructed by combining fragments from the corpus; the occurrence-frequencies of the fragments are used to estimate which analysis is the most probable one. * This work was partially supported by NWO, the Netherlands Organization for Scientific Research (Priority Programme Language and Speech Technology). For the syntactic dimension of language, various instantiations of this data-oriented processing or &amp;quot;DOP&amp;quot; approach have been worked out (e.g. Bod (1992-1995); Charniak (1996); Tugwell (1995); Sima&apos;an et al. (1994); Sima&apos;an (1994; 1996a); Goodman (1996); Rajman (1995ab); Kaplan (1996); Sekine and Grishman (1995)). A method for extending it to the semantic domain was first introduced by van den Berg et al. (1994). In the present paper we discuss a computationally effective version of that method, and an implemented system that uses it. We first summarize the first fully instantiated DOP model as presented in Bod (1992-1993). Then we show how this method can be straightforwardly extended into a semantic analysis method, if corpora are created in which the trees are enriched with semantic annotations. Finally, we discuss an implementation and report on experiments with two semantically analyzed corpora </context>
<context position="6016" citStr="Sekine and Grishman (1995)" startWordPosition="988" endWordPosition="991">nk. Bod (1993b) shows that DOP can be implemented using context-free parsing techniques. To select the most probable parse, Bod (1993a) gives a Monte Carlo approximation algorithm. Sima&apos;an (1995) gives an efficient polynomial algorithm for a sub-optimal solution. The model was tested on the Air Travel Information System (ATIS) corpus as analyzed in the Penn Treebank (Marcus et al. (1993)), achieving better test results than other stochastic grammars (cf. Bod (1996), Sima&apos;an (1996a), Goodman (1996)). On Penn&apos;s Wall Street Journal corpus, the data-oriented processing approach has been tested by Sekine and Grishman (1995) and by Charniak (1996). Though Charniak only uses corpus-subtrees smaller than depth 2 (which in our experience constitutes a less-than-optimal version of the data-oriented processing method), he reports that it &amp;quot;outperforms all other non-word-based statistical parsers/grammars on this corpus&amp;quot;. For an overview of data-oriented language processing, we refer to (Bod and Scha, 1996). 3 Data-Oriented Semantic Analysis To use the DOP method not just for syntactic analysis, but also for semantic interpretation, four steps must be taken: 1. decide on a formalism for representing the meanings of sent</context>
</contexts>
<marker>Sekine, Grishman, 1995</marker>
<rawString>S. Sekine and R. Grishman. 1995. A corpusbased probabilistic grammar with only two non-terminals. In Proceedings Fourth International Workshop on Parsing Technologies, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sima&apos;an</author>
<author>R Bod</author>
<author>S Krauwer</author>
<author>R Scha</author>
</authors>
<title>Efficient Disambiguation by means of Stochastic Tree Substitution Grammars.</title>
<date>1994</date>
<booktitle>In Proceedings International Conference on New Methods in Language Processing. CCL, UMIST,</booktitle>
<location>Manchester.</location>
<contexts>
<context position="2035" citStr="Sima&apos;an et al. (1994)" startWordPosition="288" endWordPosition="291">viously occurring utterances. When processing a new input utterance, analyses of this utterance are constructed by combining fragments from the corpus; the occurrence-frequencies of the fragments are used to estimate which analysis is the most probable one. * This work was partially supported by NWO, the Netherlands Organization for Scientific Research (Priority Programme Language and Speech Technology). For the syntactic dimension of language, various instantiations of this data-oriented processing or &amp;quot;DOP&amp;quot; approach have been worked out (e.g. Bod (1992-1995); Charniak (1996); Tugwell (1995); Sima&apos;an et al. (1994); Sima&apos;an (1994; 1996a); Goodman (1996); Rajman (1995ab); Kaplan (1996); Sekine and Grishman (1995)). A method for extending it to the semantic domain was first introduced by van den Berg et al. (1994). In the present paper we discuss a computationally effective version of that method, and an implemented system that uses it. We first summarize the first fully instantiated DOP model as presented in Bod (1992-1993). Then we show how this method can be straightforwardly extended into a semantic analysis method, if corpora are created in which the trees are enriched with semantic annotations. Fina</context>
</contexts>
<marker>Sima&apos;an, Bod, Krauwer, Scha, 1994</marker>
<rawString>K. Sima&apos;an, R. Bod, S. Krauwer, and R. Scha. 1994. Efficient Disambiguation by means of Stochastic Tree Substitution Grammars. In Proceedings International Conference on New Methods in Language Processing. CCL, UMIST, Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sima&apos;an</author>
</authors>
<title>An optimized algorithm for Data Oriented Parsing.</title>
<date>1995</date>
<booktitle>In Proceedings International Conference on Recent Advances in Natural Language Processing. Tzigov Chark,</booktitle>
<contexts>
<context position="5585" citStr="Sima&apos;an (1995)" startWordPosition="922" endWordPosition="923">, then the probability of T is given by: P(T) = Ed ni P(tsd). The DOP method differs from other statistical approaches, such as Pereira and Schabes (1992), Black et al. (1993) and Briscoe (1994), in that it does not predefine or train a formal grammar; instead it takes subtrees directly from annotated sentences in a treebank with a probability proportional to the number of occurrences of these subtrees in the treebank. Bod (1993b) shows that DOP can be implemented using context-free parsing techniques. To select the most probable parse, Bod (1993a) gives a Monte Carlo approximation algorithm. Sima&apos;an (1995) gives an efficient polynomial algorithm for a sub-optimal solution. The model was tested on the Air Travel Information System (ATIS) corpus as analyzed in the Penn Treebank (Marcus et al. (1993)), achieving better test results than other stochastic grammars (cf. Bod (1996), Sima&apos;an (1996a), Goodman (1996)). On Penn&apos;s Wall Street Journal corpus, the data-oriented processing approach has been tested by Sekine and Grishman (1995) and by Charniak (1996). Though Charniak only uses corpus-subtrees smaller than depth 2 (which in our experience constitutes a less-than-optimal version of the data-orie</context>
</contexts>
<marker>Sima&apos;an, 1995</marker>
<rawString>K. Sima&apos;an. 1995. An optimized algorithm for Data Oriented Parsing. In Proceedings International Conference on Recent Advances in Natural Language Processing. Tzigov Chark, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sima&apos;an</author>
</authors>
<title>An optimized algorithm for Data Oriented Parsing.</title>
<date>1996</date>
<booktitle>Recent Advances in Natural Language Processing 1995, volume 136 of Current Issues in Linguistic Theory. John Benjamins,</booktitle>
<editor>In R. Mitkov and N. Nicolov, editors,</editor>
<location>Amsterdam.</location>
<contexts>
<context position="5874" citStr="Sima&apos;an (1996" startWordPosition="969" endWordPosition="970">om annotated sentences in a treebank with a probability proportional to the number of occurrences of these subtrees in the treebank. Bod (1993b) shows that DOP can be implemented using context-free parsing techniques. To select the most probable parse, Bod (1993a) gives a Monte Carlo approximation algorithm. Sima&apos;an (1995) gives an efficient polynomial algorithm for a sub-optimal solution. The model was tested on the Air Travel Information System (ATIS) corpus as analyzed in the Penn Treebank (Marcus et al. (1993)), achieving better test results than other stochastic grammars (cf. Bod (1996), Sima&apos;an (1996a), Goodman (1996)). On Penn&apos;s Wall Street Journal corpus, the data-oriented processing approach has been tested by Sekine and Grishman (1995) and by Charniak (1996). Though Charniak only uses corpus-subtrees smaller than depth 2 (which in our experience constitutes a less-than-optimal version of the data-oriented processing method), he reports that it &amp;quot;outperforms all other non-word-based statistical parsers/grammars on this corpus&amp;quot;. For an overview of data-oriented language processing, we refer to (Bod and Scha, 1996). 3 Data-Oriented Semantic Analysis To use the DOP method not just for synt</context>
<context position="15395" citStr="Sima&apos;an (1996" startWordPosition="2528" endWordPosition="2529">ich is provably equivalent to the semantic annotation of the top node of a parse of this string. The probability of an interpretation I of a string is the sum of the probabilities of the parses of this string with a top node annotated with a formula that is provably equivalent to I. Let tidp be the i-th subtree in the derivation d that yields parse p with interpretation I, then the probability of I is given by: EEll P(td) (4) p d i We choose the most probable interpretation I of a string s as the most appropriate interpretation of s. In Bonnema (1996) a semantic extension of the DOP parser of Sima&apos;an (1996a) is given. But instead of computing the most likely interpretation of a string, it computes the interpretation of the most likely combination of semantically annotated subtrees. As was shown in Sima&apos;an (1996b), the most likely interpretation of a string cannot be computed in deterministic polynomial time. It is not yet known how often the most likely interpretation and the interpretation of the most likely combination of semantically enriched subtrees do actually coincide. 4 Implementations The first implementation of a semantic DOP-model yielded rather encouraging preliminary results on a s</context>
<context position="16714" citStr="Sima&apos;an (1996" startWordPosition="2735" endWordPosition="2736"> in Bonnema (1996), and Bod et al. (1996). We repeat the most important observations: • Data-oriented semantic interpretation seems to be robust; of the sentences that could be parsed, a significantly higher percentage received a correct semantic interpretation (88%), than an exactly correct syntactic analysis (62%). • The coverage of the parser was rather low (72%), because of the sheer number of different semantic types and constructs in the trees. • The parser was fast: on the average six times as fast as a parser trained on syntax alone. The current implementation is again an extension of Sima&apos;an (1996a), by Bonnema2. In our experiments, we notice a robustness and speed-up comparable to our experience with the previous implementation. Besides that, we observe higher accuracy, and higher coverage, due to a new method of organizing the information in the tree-bank before it is used for building the actual parser. A semantically enriched tree-bank will generally contain a wealth of detail. This makes it hard for a probabilistic model to estimate all parameters. In sections 4.1 and 4.2, we discuss a way of generalizing over semantic information in the tree-bank, before a DOP-parser is trained o</context>
</contexts>
<marker>Sima&apos;an, 1996</marker>
<rawString>K. Sima&apos;an. 1996a. An optimized algorithm for Data Oriented Parsing. In R. Mitkov and N. Nicolov, editors, Recent Advances in Natural Language Processing 1995, volume 136 of Current Issues in Linguistic Theory. John Benjamins, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sima&apos;an</author>
</authors>
<title>Computational Complexity of Probabilistic Disambiguation by means of TreeGrammars.</title>
<date>1996</date>
<booktitle>In Proceedings COLING &apos;96,</booktitle>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="5874" citStr="Sima&apos;an (1996" startWordPosition="969" endWordPosition="970">om annotated sentences in a treebank with a probability proportional to the number of occurrences of these subtrees in the treebank. Bod (1993b) shows that DOP can be implemented using context-free parsing techniques. To select the most probable parse, Bod (1993a) gives a Monte Carlo approximation algorithm. Sima&apos;an (1995) gives an efficient polynomial algorithm for a sub-optimal solution. The model was tested on the Air Travel Information System (ATIS) corpus as analyzed in the Penn Treebank (Marcus et al. (1993)), achieving better test results than other stochastic grammars (cf. Bod (1996), Sima&apos;an (1996a), Goodman (1996)). On Penn&apos;s Wall Street Journal corpus, the data-oriented processing approach has been tested by Sekine and Grishman (1995) and by Charniak (1996). Though Charniak only uses corpus-subtrees smaller than depth 2 (which in our experience constitutes a less-than-optimal version of the data-oriented processing method), he reports that it &amp;quot;outperforms all other non-word-based statistical parsers/grammars on this corpus&amp;quot;. For an overview of data-oriented language processing, we refer to (Bod and Scha, 1996). 3 Data-Oriented Semantic Analysis To use the DOP method not just for synt</context>
<context position="15395" citStr="Sima&apos;an (1996" startWordPosition="2528" endWordPosition="2529">ich is provably equivalent to the semantic annotation of the top node of a parse of this string. The probability of an interpretation I of a string is the sum of the probabilities of the parses of this string with a top node annotated with a formula that is provably equivalent to I. Let tidp be the i-th subtree in the derivation d that yields parse p with interpretation I, then the probability of I is given by: EEll P(td) (4) p d i We choose the most probable interpretation I of a string s as the most appropriate interpretation of s. In Bonnema (1996) a semantic extension of the DOP parser of Sima&apos;an (1996a) is given. But instead of computing the most likely interpretation of a string, it computes the interpretation of the most likely combination of semantically annotated subtrees. As was shown in Sima&apos;an (1996b), the most likely interpretation of a string cannot be computed in deterministic polynomial time. It is not yet known how often the most likely interpretation and the interpretation of the most likely combination of semantically enriched subtrees do actually coincide. 4 Implementations The first implementation of a semantic DOP-model yielded rather encouraging preliminary results on a s</context>
<context position="16714" citStr="Sima&apos;an (1996" startWordPosition="2735" endWordPosition="2736"> in Bonnema (1996), and Bod et al. (1996). We repeat the most important observations: • Data-oriented semantic interpretation seems to be robust; of the sentences that could be parsed, a significantly higher percentage received a correct semantic interpretation (88%), than an exactly correct syntactic analysis (62%). • The coverage of the parser was rather low (72%), because of the sheer number of different semantic types and constructs in the trees. • The parser was fast: on the average six times as fast as a parser trained on syntax alone. The current implementation is again an extension of Sima&apos;an (1996a), by Bonnema2. In our experiments, we notice a robustness and speed-up comparable to our experience with the previous implementation. Besides that, we observe higher accuracy, and higher coverage, due to a new method of organizing the information in the tree-bank before it is used for building the actual parser. A semantically enriched tree-bank will generally contain a wealth of detail. This makes it hard for a probabilistic model to estimate all parameters. In sections 4.1 and 4.2, we discuss a way of generalizing over semantic information in the tree-bank, before a DOP-parser is trained o</context>
</contexts>
<marker>Sima&apos;an, 1996</marker>
<rawString>K. Sima&apos;an. 1996b. Computational Complexity of Probabilistic Disambiguation by means of TreeGrammars. In Proceedings COLING &apos;96, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Tugwell</author>
</authors>
<title>A state-transition grammar for data-oriented parsing. In</title>
<date>1995</date>
<booktitle>Proceedings European Chapter of the ACL&apos;95,</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="2012" citStr="Tugwell (1995)" startWordPosition="286" endWordPosition="287">entations of previously occurring utterances. When processing a new input utterance, analyses of this utterance are constructed by combining fragments from the corpus; the occurrence-frequencies of the fragments are used to estimate which analysis is the most probable one. * This work was partially supported by NWO, the Netherlands Organization for Scientific Research (Priority Programme Language and Speech Technology). For the syntactic dimension of language, various instantiations of this data-oriented processing or &amp;quot;DOP&amp;quot; approach have been worked out (e.g. Bod (1992-1995); Charniak (1996); Tugwell (1995); Sima&apos;an et al. (1994); Sima&apos;an (1994; 1996a); Goodman (1996); Rajman (1995ab); Kaplan (1996); Sekine and Grishman (1995)). A method for extending it to the semantic domain was first introduced by van den Berg et al. (1994). In the present paper we discuss a computationally effective version of that method, and an implemented system that uses it. We first summarize the first fully instantiated DOP model as presented in Bod (1992-1993). Then we show how this method can be straightforwardly extended into a semantic analysis method, if corpora are created in which the trees are enriched with sem</context>
</contexts>
<marker>Tugwell, 1995</marker>
<rawString>D. Tugwell. 1995. A state-transition grammar for data-oriented parsing. In Proceedings European Chapter of the ACL&apos;95, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Veldhuijzen van Zanten</author>
</authors>
<title>Semantics of update expressions.</title>
<date>1996</date>
<booktitle>NWO priority Programme Language and Speech Technology, http://grid.let.rug.n1:4321/.</booktitle>
<marker>van Zanten, 1996</marker>
<rawString>G. Veldhuijzen van Zanten. 1996. Semantics of update expressions. NWO priority Programme Language and Speech Technology, http://grid.let.rug.n1:4321/.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>