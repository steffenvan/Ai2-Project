<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.035242">
<title confidence="0.999327">
Stem Translation with Affix-Based Rule Selection
for Agglutinative Languages
</title>
<author confidence="0.999317">
Zhiyang Wang†, Yajuan L¨u†, Meng Sun†, Qun Liu$†
</author>
<affiliation confidence="0.997814">
†Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
</affiliation>
<address confidence="0.853741">
P.O. Box 2704, Beijing 100190, China
</address>
<email confidence="0.991794">
{wangzhiyang,lvyajuan,sunmeng,liuqun}@ict.ac.cn
</email>
<affiliation confidence="0.9774825">
‡Centre for Next Generation Localisation
Faculty of Engineering and Computing, Dublin City University
</affiliation>
<email confidence="0.986669">
qliu@computing.dcu.ie
</email>
<sectionHeader confidence="0.997267" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9991581">
Current translation models are mainly de-
signed for languages with limited mor-
phology, which are not readily applicable
to agglutinative languages as the differ-
ence in the way lexical forms are gener-
ated. In this paper, we propose a nov-
el approach for translating agglutinative
languages by treating stems and affixes
differently. We employ stem as the atomic
translation unit to alleviate data spare-
ness. In addition, we associate each stem-
granularity translation rule with a distri-
bution of related affixes, and select desir-
able rules according to the similarity of
their affix distributions with given spans to
be translated. Experimental results show
that our approach significantly improves
the translation performance on tasks of
translating from three Turkic languages to
Chinese.
</bodyText>
<sectionHeader confidence="0.999473" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999942">
Currently, most methods on statistical machine
translation (SMT) are developed for translation
of languages with limited morphology (e.g., En-
glish, Chinese). They assumed that word was the
atomic translation unit (ATU), always ignoring the
internal morphological structure of word. This
assumption can be traced back to the original
IBM word-based models (Brown et al., 1993) and
several significantly improved models, including
phrase-based (Och and Ney, 2004; Koehn et al.,
2003), hierarchical (Chiang, 2005) and syntac-
tic (Quirk et al., 2005; Galley et al., 2006; Liu et
al., 2006) models. These improved models worked
well for translating languages like English with
large scale parallel corpora available.
Different from languages with limited morphol-
ogy, words of agglutinative languages are formed
mainly by concatenation of stems and affixes.
Generally, a stem can attach with several affixes,
thus leading to tens of hundreds of possible inflect-
ed variants of lexicons for a single stem. Modeling
each lexical form as a separate word will generate
high out-of-vocabulary rate for SMT. Theoretical-
ly, ways like morphological analysis and increas-
ing bilingual corpora could alleviate the problem
of data sparsity, but most agglutinative languages
are less-studied and suffer from the problem of
resource-scarceness. Therefore, previous research
mainly focused on the different inflected variants
of the same stem and made various transformation
of input by morphological analysis, such as (Lee,
2004; Goldwater and McClosky, 2005; Yang and
Kirchhoff, 2006; Habash and Sadat, 2006; Bisazza
and Federico, 2009; Wang et al., 2011). These
work still assume that the atomic translation unit
is word, stem or morpheme, without considering
the difference between stems and affixes.
In agglutinative languages, stem is the base
part of word not including inflectional affixes.
Affix, especially inflectional affix, indicates dif-
ferent grammatical categories such as tense, per-
son, number and case, etc., which is useful for
translation rule disambiguation. Therefore, we
employ stem as the atomic translation unit and
use affix information to guide translation rule
selection. Stem-granularity translation rules have
much larger coverage and can lower the OOV
rate. Affix based rule selection takes advantage
of auxiliary syntactic roles of affixes to make a
better rule selection. In this way, we can achieve
a balance between rule coverage and matching
accuracy, and ultimately improve the translation
performance.
</bodyText>
<page confidence="0.986604">
364
</page>
<note confidence="0.4610215">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 364–369,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figure confidence="0.9997315">
(A) Instances of translation rule
(B)Translation rules with affix distribution
Original:zunyi yighin+i+gha
Meaning:of zunyi conference
i
/SUF
(1) (2)
gha
/SUF
zunyi
/STM
yighin
/STM
Original:zunyi yighin+i+da
Meaning:on zunyi conference
zunyi
/STM
yighin
/STM
i
/SUF
da
/SUF
Original:zunyi yighin+i+gha
Meaning:of zunyi conference
(3)
zunyi
/STM
yighin
/STM
i
/SUF
gha
/SUF
</figure>
<figureCaption confidence="0.9766375">
Figure 1: Translation rule extraction from Uyghur to Chinese. Here tag “/STM” represents stem and
“/SUF” means suffix.
</figureCaption>
<sectionHeader confidence="0.941048" genericHeader="introduction">
2 Affix Based Rule Selection Model
</sectionHeader>
<bodyText confidence="0.99939996">
Figure 1 (B) shows two translation rules along
with affix distributions. Here a translation rule
contains three parts: the source part (on stem lev-
el), the target part, and the related affix distribution
(represented as a vector). We can see that, al-
though the source part of the two translation rules
are identical, their affix distributions are quite
different. Affix “gha” in the first rule indicates
that something is affiliated to a subject, similar to
“of” in English. And “da” in second rule implies
location information. Therefore, given a span
“zunyi/STM yighin/STM+i/SUF+da/SUF+...” to
be translated, we hope to encourage our model to
select the second translation rule. We can achieve
this by calculating similarity between the affix
distributions of the translation rule and the span.
The affix distribution can be obtained by keep-
ing the related affixes for each rule instance during
translation rule extraction ((A) in Figure 1). After
extracting and scoring stem-granularity rules in a
traditional way, we extract stem-granularity rules
again by keeping affix information and compute
the affix distribution with tf-idf (Salton and Buck-
ley, 1987). Finally, the affix distribution will be
added to the previous stem-granularity rules.
</bodyText>
<subsectionHeader confidence="0.985771">
2.1 Affix Distribution Estimation
</subsectionHeader>
<bodyText confidence="0.996442384615384">
Formally, translation rule instances with the same
source part can be treated as a document collec-
tions, so each rule instance in the collection is
&apos;We employ concepts from text classification to illustrate
how to estimate affix distribution.
some kind of document. Our goal is to classify the
source parts into the target parts on the document
collection level with the help of affix distribu-
tion. Accordingly, we employ vector space model
(VSM) to represent affix distribution of each rule
instance. In this model, the feature weights are
represented by the classic tf-idf (Salton and Buck-
ley, 1987):
</bodyText>
<equation confidence="0.999489">
tfi,j = ni,j idfi,j = log , |D |(1)
∑k nk,j |J : ai E rj|
tfidfi,j = tfi,j x idfi,j
</equation>
<bodyText confidence="0.99990952631579">
where tfidfij is the weight of affix ai in transla-
tion rule instance rj. nij indicates the number of
occurrence of affix ai in rj. |D |is the number
of rule instance with the same source part, and
|j : ai ∈ rj |is the number of rule instance which
contains affix ai within |D|.
Let’s take the suffix “gha” from (A1) in Figure
1 as an example. We assume that there are only
three instances of translation rules extracted from
parallel corpus ((A) in Figure 1). We can see that
“gha” only appear once in (A1) and also appear
once in whole instances. Therefore, tfgha�(A1) is
0.5 and idfgha�(A1) is lo9(3/2). tfidfgha�(A1) is
the product of tfgha�(A1) and idfgha�(A1) which
is 0.09.
Given a set of N translation rule instances with
the same source and target part, we define the
centroid vector dr according to the centroid-based
classification algorithm (Han and Karypis, 2000),
</bodyText>
<equation confidence="0.573692">
1 ∑ di (2)
dr _ N
iEN
</equation>
<page confidence="0.92735">
365
</page>
<table confidence="0.999932636363636">
Data set #Sent. #Type #Token
word stem morph word stem morph
UY-CH-Train. 50K 69K 39K 42K 1.2M 1.2M 1.6M
UY-CH-Dev. 0.7K*4 5.9K 4.1K 4.6K 18K 18K 23.5K
UY-CH-Test. 0.7K*1 4.7K 3.3K 3.8K 14K 14K 17.8K
KA-CH-Train. 50K 62K 40K 42K 1.1M 1.1M 1.3M
KA-CH-Dev. 0.7K*4 5.3K 4.2K 4.5K 15K 15K 18K
KA-CH-Test. 0.2K*1 2.6K 2.0K 2.3K 8.6K 8.6K 10.8K
KI-CH-Train. 50K 53K 27K 31K 1.2M 1.2M 1.5M
KI-CH-Dev. 0.5K*4 4.1K 3.1K 3.5K 12K 12K 15K
KI-CH-Test. 0.2K*4 2.2K 1.8K 2.1K 4.7K 4.7K 5.8K
</table>
<tableCaption confidence="0.9445375">
Table 1: Statistics of data sets. ∗N means the number of reference, morph is short to morpheme. UY,
KA, KI, CH represent Uyghur, Kazakh, Kirghiz and Chinese respectively.
</tableCaption>
<bodyText confidence="0.995398">
dr is the final affix distribution.
By comparing the similarity of affix distribu-
tions, we are able to decide whether a translation
rule is suitable for a span to be translated. In
this work, similarity is measured using the cosine
distance similarity metric, given by
</bodyText>
<equation confidence="0.999441666666667">
d1 · d2
sim(d1, d2) = (3)
∥d1∥ × ∥d2∥
</equation>
<bodyText confidence="0.999955466666667">
where di corresponds to a vector indicating affix
distribution, and “·” denotes the inner product of
the two vectors.
Therefore, for a specific span to be translated,
we first analyze it to get the corresponding stem
sequence and related affix distribution represented
as a vector. Then the stem sequence is used to
search the translation rule table. If the source part
is matched, the similarity will be calculated for
each candidate translation rule by cosine similarity
(as in equation 3). Therefore, in addition to the
traditional translation features on stem level, our
model also adds the affix similarity score as a
dynamic feature into the log-linear model (Och
and Ney, 2002).
</bodyText>
<sectionHeader confidence="0.999982" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999973230769231">
Most previous work on agglutinative language
translation mainly focus on Turkish and Finnish.
Bisazza and Federico (2009) and Mermer and
Saraclar (2011) optimized morphological analysis
as a pre-processing step to improve the translation
between Turkish and English. Yeniterzi and Oflaz-
er (2010) mapped the syntax of the English side
to the morphology of the Turkish side with the
factored model (Koehn and Hoang, 2007). Yang
and Kirchhoff (2006) backed off surface form to
stem when translating OOV words of Finnish.
Luong and Kan (2010) and Luong et al. (2010)
focused on Finnish-English translation through
improving word alignment and enhancing phrase
table. These works still assumed that the atomic
translation unit is word, stem or morpheme, with-
out considering the difference between stems and
affixes.
There are also some work that employed the
context information to make a better choice of
translation rules (Carpuat and Wu, 2007; Chan et
al., 2007; He et al., 2008; Cui et al., 2010). all the
work employed rich context information, such as
POS, syntactic, etc., and experiments were mostly
done on less inflectional languages (i.e. Chinese,
English) and resourceful languages (i.e. Arabic).
</bodyText>
<sectionHeader confidence="0.999847" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999866714285714">
In this work, we conduct our experiments on
three different agglutinative languages, including
Uyghur, Kazakh and Kirghiz. All of them are
derived from Altaic language family, belonging to
Turkic languages, and mostly spoken by people in
Central Asia. There are about 24 million people
take these languages as mother tongue. All of
the tasks are derived from the evaluation of Chi-
na Workshop of Machine Translation (CWMT)2.
Table 1 shows the statistics of data sets.
For the language model, we use the SRI Lan-
guage Modeling Toolkit (Stolcke, 2002) to train
a 5-gram model with the target side of training
corpus. And phrase-based Moses3 is used as our
</bodyText>
<footnote confidence="0.999886">
2http://mt.xmu.edu.cn/cwmt2011/en/index.html.
3http://www.statmt.org/moses/
</footnote>
<page confidence="0.995069">
366
</page>
<table confidence="0.9987744">
UY-CH KA-CH KI-CH
word 31.74+0.0 28.64+0.0 35.05+0.0
stem 33.74+2.0 30.14+1.5 35.52+0.47
morph 32.69+0.95 29.21+0.57 34.97−0.08
affix 34.34+2.6 30.19+2.27 35.96+0.91
</table>
<tableCaption confidence="0.985781">
Table 2: Translation results from Turkic languages
</tableCaption>
<bodyText confidence="0.9527387">
to Chinese. word: ATU is surface form,
stem: ATU is represented stem, morph: ATU
denotes morpheme, affix: stem translation with
affix distribution similarity. BLEU scores in
bold means significantly better than the baseline
according to (Koehn, 2004) for p-value less than
0.01.
baseline SMT system. The decoding weights are
optimized with MERT (Och, 2003) to maximum
word-level BLEU scores (Papineni et al., 2002).
</bodyText>
<subsectionHeader confidence="0.944871">
4.1 Using Unsupervised Morphological
Analyzer
</subsectionHeader>
<bodyText confidence="0.999918035714286">
As most agglutinative languages are resource-
poor, we employ unsupervised learning method
to obtain the morphological structure. Follow-
ing the approach in (Virpioja et al., 2007), we
employ the Morfessor4 Categories-MAP algorith-
m (Creutz and Lagus, 2005). It applies a hierar-
chical model with three categories (prefix, stem,
and suffix) in an unsupervised way. From Table 1
we can see that vocabulary sizes of the three lan-
guages are reduced obviously after unsupervised
morphological analysis.
Table 2 shows the translation results. All the
three translation tasks achieve obvious improve-
ments with the proposed model, which always per-
forms better than only employ word, stem and
morph. For the Uyghur to Chinese translation
(UY-CH) task in Table 2, performances after unsu-
pervised morphological analysis are always better
than the baseline. And we gain up to +2.6 BLEU
points improvements with affix compared to the
baseline. For the Kazakh to Chinese translation
(KA-CH) task, the improvements are also signifi-
cant. We achieve +2.27 and +0.77 improvements
compared to the baseline and stem, respectively.
As for the Kirghiz to Chinese translation (KI-CH)
task, improvements seem relative small compared
to the other two language pairs. However, it also
gains +0.91 BLEU points over the baseline.
</bodyText>
<footnote confidence="0.954928">
4http://www.cis.hut.fi/projects/Morpho/
</footnote>
<table confidence="0.997857">
UY Unsup Sup
stem #Type 39K 21K
#Token 1.2M 1.2M
affix #Type 3.0K 0.3K
#Token 0.4M 0.7M
</table>
<tableCaption confidence="0.920037666666667">
Table 3: Statistics of training corpus after unsuper-
vised(Unsup) and supervised(Sup) morphological
analysis.
</tableCaption>
<figureCaption confidence="0.939553">
Figure 2: Uyghur to Chinese translation results
after unsupervised and supervised analysis.
</figureCaption>
<subsectionHeader confidence="0.994855">
4.2 Using Supervised Morphological
Analyzer
</subsectionHeader>
<bodyText confidence="0.908746235294118">
Taking it further, we also want to see the effect of
supervised analysis on our model. A generative
statistical model of morphological analysis for
Uyghur was developed according to (Mairehaba
et al., 2012). Table 3 shows the difference of
statistics of training corpus after supervised and
unsupervised analysis. Supervised method gen-
erates fewer type of stems and affixes than the
unsupervised approach. As we can see from
Figure 2, except for the morph method, stem
and affix based approaches perform better after
supervised analysis. The results show that our
approach can obtain even better translation per-
formance if better morphological analyzers are
available. Supervised morphological analysis gen-
erates more meaningful morphemes, which lead to
better disambiguation of translation rules.
</bodyText>
<sectionHeader confidence="0.994494" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.997167875">
In this paper we propose a novel framework for
agglutinative language translation by treating stem
and affix differently. We employ the stem se-
quence as the main part for training and decod-
ing. Besides, we associate each stem-granularity
translation rule with an affix distribution, which
could be used to make better translation decisions
by calculating the affix distribution similarity be-
</bodyText>
<figure confidence="0.997125571428571">
BLEU score(%)
36.5
35.5
34.5
33.5
32.5
31.5
36
35
34
33
32
Unsupervised
Supervised
</figure>
<page confidence="0.992521">
367
</page>
<bodyText confidence="0.999942833333333">
tween the rule and the instance to be translated.
We conduct our model on three different language
pairs, all of which substantially improved the
translation performance. The procedure is totally
language-independent, and we expect that other
language pairs could benefit from our approach.
</bodyText>
<sectionHeader confidence="0.986897" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997365">
The authors were supported by 863 State
Key Project (No. 2011AA01A207), and
National Key Technology R&amp;D Program (No.
2012BAH39B03), Key Project of Knowledge
Innovation Program of Chinese Academy of
Sciences (No. KGZD-EW-501). Qun Liu’s work
is partially supported by Science Foundation
Ireland (Grant No.07/CE/I1142) as part of the
CNGL at Dublin City University. We would
like to thank the anonymous reviewers for their
insightful comments and those who helped to
modify the paper.
</bodyText>
<sectionHeader confidence="0.998627" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99877493902439">
Arianna Bisazza and Marcello Federico. 2009. Mor-
phological pre-processing for Turkish to English
statistical machine translation. In Proceedings of
IWSLT, pages 129–135.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: pa-
rameter estimation. Comput. Linguist., 19(2):263–
311.
Marine Carpuat and Dekai Wu. 2007. Improving
statistical machine translation using word sense
disambiguation. In Proceedings of EMNLP-CoNLL,
pages 61–72.
Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word sense disambiguation improves
statistical machine translation. In Proceedings of
ACL, pages 33–40.
David Chiang. 2005. A hierarchical phrase-
based model for statistical machine translation. In
Proceedings of ACL, pages 263–270.
Mathias Creutz and Krista Lagus. 2005. Inducing the
morphological lexicon of a natural language from
unannotated text. In Proceedings of AKRR, pages
106–113.
Lei Cui, Dongdong Zhang, Mu Li, Ming Zhou,
and Tiejun Zhao. 2010. A joint rule selection
model for hierarchical phrase-based translation. In
Proceedings of ACL, Short Papers, pages 6–11.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training
of context-rich syntactic translation models. In
Proceedings of COLING/ACL, pages 961–968.
Sharon Goldwater and David McClosky. 2005.
Improving statistical MT through morphological
analysis. In Proceedings of HLT-EMNLP, pages
676–683.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation.
In Proceedings of NAACL, Short Papers, pages 49–
52.
Eui-Hong Sam Han and George Karypis. 2000.
Centroid-based document classification: analysis
experimental results. In Proceedings of PKDD,
pages 424–431.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008.
Improving statistical machine translation using
lexicalized rule selection. In Proceedings of
COLING, pages 321–328.
Philipp Koehn and Hieu Hoang. 2007. Factored
translation models. In Proceedings of EMNLP-
CoNLL, pages 868–876.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of NAACL, pages 48–54.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, pages 388–395.
Young-Suk Lee. 2004. Morphological analysis for
statistical machine translation. In Proceedings of
HLT-NAACL, Short Papers, pages 57–60.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of COLING-ACL, pages
609–616.
Minh-Thang Luong and Min-Yen Kan. 2010.
Enhancing morphological alignment for translating
highly inflected languages. In Proceedings of
COLING, pages 743–751.
Minh-Thang Luong, Preslav Nakov, and Min-Yen Kan.
2010. A hybrid morpheme-word representation
for machine translation of morphologically rich
languages. In Proceedings of EMNLP, pages 148–
157.
Aili Mairehaba, Wenbin Jiang, Zhiyang Wang, Yibu-
layin Tuergen, and Qun Liu. 2012. Directed graph
model of Uyghur morphological analysis. Journal
of Software, 23(12):3115–3129.
Coskun Mermer and Murat Saraclar. 2011. Un-
supervised Turkish morphological segmentation for
statistical machine translation. In Workshop of MT
and Morphologically-rich Languages.
</reference>
<page confidence="0.983532">
368
</page>
<reference confidence="0.99947656097561">
Franz Josef Och and Hermann Ney. 2002. Discrim-
inative training and maximum entropy models for
statistical machine translation. In Proceedings of
ACL, pages 295–302.
Franz Josef Och and Hermann Ney. 2004. The
alignment template approach to statistical machine
translation. Comput. Linguist., pages 417–449.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311–318.
Chris Quirk, Arul Menezes, and Colin Cherry.
2005. Dependency treelet translation: syntactically
informed phrasal SMT. In Proceedings of ACL,
pages 271–279.
Gerard Salton and Chris Buckley. 1987. Term
weighting approaches in automatic text retrieval.
Technical report.
Andreas Stolcke. 2002. SRILM - an extensible
language modeling toolkit. In Proceedings of
ICSLP, pages 311–318.
Sami Virpioja, Jaakko J. V¨ayrynen, Mathias Creutz,
and Markus Sadeniemi. 2007. Morphology-aware
statistical machine translation based on morphs
induced in an unsupervised manner. In Proceedings
of MT SUMMIT, pages 491–498.
Zhiyang Wang, Yajuan L¨u, and Qun Liu. 2011.
Multi-granularity word alignment and decoding for
agglutinative language translation. In Proceedings
of MT SUMMIT, pages 360–367.
Mei Yang and Katrin Kirchhoff. 2006. Phrase-based
backoff models for machine translation of highly
inflected languages. In Proceedings of EACL, pages
1017–1020.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-
to-morphology mapping in factored phrase-based
statistical machine translation from English to
Turkish. In Proceedings of ACL, pages 454–464.
</reference>
<page confidence="0.99916">
369
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.096269">
<title confidence="0.998789">Stem Translation with Affix-Based Rule for Agglutinative Languages</title>
<author confidence="0.996069">Yajuan Meng Qun</author>
<affiliation confidence="0.9660885">Laboratory of Intelligent Information Institute of Computing Technology, Chinese Academy of</affiliation>
<address confidence="0.696643">P.O. Box 2704, Beijing 100190,</address>
<title confidence="0.562179">for Next Generation</title>
<author confidence="0.314739">Faculty of Engineering</author>
<author confidence="0.314739">Dublin City Computing</author>
<email confidence="0.904656">qliu@computing.dcu.ie</email>
<abstract confidence="0.99915825">Current translation models are mainly designed for languages with limited morphology, which are not readily applicable to agglutinative languages as the difference in the way lexical forms are generated. In this paper, we propose a novel approach for translating agglutinative languages by treating stems and affixes differently. We employ stem as the atomic translation unit to alleviate data spareness. In addition, we associate each stemgranularity translation rule with a distribution of related affixes, and select desirable rules according to the similarity of their affix distributions with given spans to be translated. Experimental results show that our approach significantly improves the translation performance on tasks of translating from three Turkic languages to</abstract>
<intro confidence="0.430805">Chinese.</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>CNGL at Dublin City University. We would like to thank the anonymous reviewers for their insightful comments and those who helped to modify the paper.</title>
<marker></marker>
<rawString>CNGL at Dublin City University. We would like to thank the anonymous reviewers for their insightful comments and those who helped to modify the paper.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arianna Bisazza</author>
<author>Marcello Federico</author>
</authors>
<title>Morphological pre-processing for Turkish to English statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of IWSLT,</booktitle>
<pages>129--135</pages>
<contexts>
<context position="2877" citStr="Bisazza and Federico, 2009" startWordPosition="411" endWordPosition="414">le stem. Modeling each lexical form as a separate word will generate high out-of-vocabulary rate for SMT. Theoretically, ways like morphological analysis and increasing bilingual corpora could alleviate the problem of data sparsity, but most agglutinative languages are less-studied and suffer from the problem of resource-scarceness. Therefore, previous research mainly focused on the different inflected variants of the same stem and made various transformation of input by morphological analysis, such as (Lee, 2004; Goldwater and McClosky, 2005; Yang and Kirchhoff, 2006; Habash and Sadat, 2006; Bisazza and Federico, 2009; Wang et al., 2011). These work still assume that the atomic translation unit is word, stem or morpheme, without considering the difference between stems and affixes. In agglutinative languages, stem is the base part of word not including inflectional affixes. Affix, especially inflectional affix, indicates different grammatical categories such as tense, person, number and case, etc., which is useful for translation rule disambiguation. Therefore, we employ stem as the atomic translation unit and use affix information to guide translation rule selection. Stem-granularity translation rules hav</context>
<context position="9147" citStr="Bisazza and Federico (2009)" startWordPosition="1421" endWordPosition="1424">he corresponding stem sequence and related affix distribution represented as a vector. Then the stem sequence is used to search the translation rule table. If the source part is matched, the similarity will be calculated for each candidate translation rule by cosine similarity (as in equation 3). Therefore, in addition to the traditional translation features on stem level, our model also adds the affix similarity score as a dynamic feature into the log-linear model (Och and Ney, 2002). 3 Related Work Most previous work on agglutinative language translation mainly focus on Turkish and Finnish. Bisazza and Federico (2009) and Mermer and Saraclar (2011) optimized morphological analysis as a pre-processing step to improve the translation between Turkish and English. Yeniterzi and Oflazer (2010) mapped the syntax of the English side to the morphology of the Turkish side with the factored model (Koehn and Hoang, 2007). Yang and Kirchhoff (2006) backed off surface form to stem when translating OOV words of Finnish. Luong and Kan (2010) and Luong et al. (2010) focused on Finnish-English translation through improving word alignment and enhancing phrase table. These works still assumed that the atomic translation unit</context>
</contexts>
<marker>Bisazza, Federico, 2009</marker>
<rawString>Arianna Bisazza and Marcello Federico. 2009. Morphological pre-processing for Turkish to English statistical machine translation. In Proceedings of IWSLT, pages 129–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Comput. Linguist.,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>311</pages>
<contexts>
<context position="1638" citStr="Brown et al., 1993" startWordPosition="227" endWordPosition="230">arity of their affix distributions with given spans to be translated. Experimental results show that our approach significantly improves the translation performance on tasks of translating from three Turkic languages to Chinese. 1 Introduction Currently, most methods on statistical machine translation (SMT) are developed for translation of languages with limited morphology (e.g., English, Chinese). They assumed that word was the atomic translation unit (ATU), always ignoring the internal morphological structure of word. This assumption can be traced back to the original IBM word-based models (Brown et al., 1993) and several significantly improved models, including phrase-based (Och and Ney, 2004; Koehn et al., 2003), hierarchical (Chiang, 2005) and syntactic (Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006) models. These improved models worked well for translating languages like English with large scale parallel corpora available. Different from languages with limited morphology, words of agglutinative languages are formed mainly by concatenation of stems and affixes. Generally, a stem can attach with several affixes, thus leading to tens of hundreds of possible inflected variants of lexico</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Comput. Linguist., 19(2):263– 311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Improving statistical machine translation using word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>61--72</pages>
<contexts>
<context position="9966" citStr="Carpuat and Wu, 2007" startWordPosition="1550" endWordPosition="1553"> English side to the morphology of the Turkish side with the factored model (Koehn and Hoang, 2007). Yang and Kirchhoff (2006) backed off surface form to stem when translating OOV words of Finnish. Luong and Kan (2010) and Luong et al. (2010) focused on Finnish-English translation through improving word alignment and enhancing phrase table. These works still assumed that the atomic translation unit is word, stem or morpheme, without considering the difference between stems and affixes. There are also some work that employed the context information to make a better choice of translation rules (Carpuat and Wu, 2007; Chan et al., 2007; He et al., 2008; Cui et al., 2010). all the work employed rich context information, such as POS, syntactic, etc., and experiments were mostly done on less inflectional languages (i.e. Chinese, English) and resourceful languages (i.e. Arabic). 4 Experiments In this work, we conduct our experiments on three different agglutinative languages, including Uyghur, Kazakh and Kirghiz. All of them are derived from Altaic language family, belonging to Turkic languages, and mostly spoken by people in Central Asia. There are about 24 million people take these languages as mother tongu</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007. Improving statistical machine translation using word sense disambiguation. In Proceedings of EMNLP-CoNLL, pages 61–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
<author>David Chiang</author>
</authors>
<title>Word sense disambiguation improves statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="9985" citStr="Chan et al., 2007" startWordPosition="1554" endWordPosition="1557">orphology of the Turkish side with the factored model (Koehn and Hoang, 2007). Yang and Kirchhoff (2006) backed off surface form to stem when translating OOV words of Finnish. Luong and Kan (2010) and Luong et al. (2010) focused on Finnish-English translation through improving word alignment and enhancing phrase table. These works still assumed that the atomic translation unit is word, stem or morpheme, without considering the difference between stems and affixes. There are also some work that employed the context information to make a better choice of translation rules (Carpuat and Wu, 2007; Chan et al., 2007; He et al., 2008; Cui et al., 2010). all the work employed rich context information, such as POS, syntactic, etc., and experiments were mostly done on less inflectional languages (i.e. Chinese, English) and resourceful languages (i.e. Arabic). 4 Experiments In this work, we conduct our experiments on three different agglutinative languages, including Uyghur, Kazakh and Kirghiz. All of them are derived from Altaic language family, belonging to Turkic languages, and mostly spoken by people in Central Asia. There are about 24 million people take these languages as mother tongue. All of the tasks</context>
</contexts>
<marker>Chan, Ng, Chiang, 2007</marker>
<rawString>Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007. Word sense disambiguation improves statistical machine translation. In Proceedings of ACL, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrasebased model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1773" citStr="Chiang, 2005" startWordPosition="247" endWordPosition="248">translation performance on tasks of translating from three Turkic languages to Chinese. 1 Introduction Currently, most methods on statistical machine translation (SMT) are developed for translation of languages with limited morphology (e.g., English, Chinese). They assumed that word was the atomic translation unit (ATU), always ignoring the internal morphological structure of word. This assumption can be traced back to the original IBM word-based models (Brown et al., 1993) and several significantly improved models, including phrase-based (Och and Ney, 2004; Koehn et al., 2003), hierarchical (Chiang, 2005) and syntactic (Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006) models. These improved models worked well for translating languages like English with large scale parallel corpora available. Different from languages with limited morphology, words of agglutinative languages are formed mainly by concatenation of stems and affixes. Generally, a stem can attach with several affixes, thus leading to tens of hundreds of possible inflected variants of lexicons for a single stem. Modeling each lexical form as a separate word will generate high out-of-vocabulary rate for SMT. Theoretically, w</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrasebased model for statistical machine translation. In Proceedings of ACL, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Inducing the morphological lexicon of a natural language from unannotated text.</title>
<date>2005</date>
<booktitle>In Proceedings of AKRR,</booktitle>
<pages>106--113</pages>
<contexts>
<context position="11908" citStr="Creutz and Lagus, 2005" startWordPosition="1835" endWordPosition="1838">pheme, affix: stem translation with affix distribution similarity. BLEU scores in bold means significantly better than the baseline according to (Koehn, 2004) for p-value less than 0.01. baseline SMT system. The decoding weights are optimized with MERT (Och, 2003) to maximum word-level BLEU scores (Papineni et al., 2002). 4.1 Using Unsupervised Morphological Analyzer As most agglutinative languages are resourcepoor, we employ unsupervised learning method to obtain the morphological structure. Following the approach in (Virpioja et al., 2007), we employ the Morfessor4 Categories-MAP algorithm (Creutz and Lagus, 2005). It applies a hierarchical model with three categories (prefix, stem, and suffix) in an unsupervised way. From Table 1 we can see that vocabulary sizes of the three languages are reduced obviously after unsupervised morphological analysis. Table 2 shows the translation results. All the three translation tasks achieve obvious improvements with the proposed model, which always performs better than only employ word, stem and morph. For the Uyghur to Chinese translation (UY-CH) task in Table 2, performances after unsupervised morphological analysis are always better than the baseline. And we gain</context>
</contexts>
<marker>Creutz, Lagus, 2005</marker>
<rawString>Mathias Creutz and Krista Lagus. 2005. Inducing the morphological lexicon of a natural language from unannotated text. In Proceedings of AKRR, pages 106–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Cui</author>
<author>Dongdong Zhang</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Tiejun Zhao</author>
</authors>
<title>A joint rule selection model for hierarchical phrase-based translation.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL, Short Papers,</booktitle>
<pages>6--11</pages>
<contexts>
<context position="10021" citStr="Cui et al., 2010" startWordPosition="1562" endWordPosition="1565">he factored model (Koehn and Hoang, 2007). Yang and Kirchhoff (2006) backed off surface form to stem when translating OOV words of Finnish. Luong and Kan (2010) and Luong et al. (2010) focused on Finnish-English translation through improving word alignment and enhancing phrase table. These works still assumed that the atomic translation unit is word, stem or morpheme, without considering the difference between stems and affixes. There are also some work that employed the context information to make a better choice of translation rules (Carpuat and Wu, 2007; Chan et al., 2007; He et al., 2008; Cui et al., 2010). all the work employed rich context information, such as POS, syntactic, etc., and experiments were mostly done on less inflectional languages (i.e. Chinese, English) and resourceful languages (i.e. Arabic). 4 Experiments In this work, we conduct our experiments on three different agglutinative languages, including Uyghur, Kazakh and Kirghiz. All of them are derived from Altaic language family, belonging to Turkic languages, and mostly spoken by people in Central Asia. There are about 24 million people take these languages as mother tongue. All of the tasks are derived from the evaluation of </context>
</contexts>
<marker>Cui, Zhang, Li, Zhou, Zhao, 2010</marker>
<rawString>Lei Cui, Dongdong Zhang, Mu Li, Ming Zhou, and Tiejun Zhao. 2010. A joint rule selection model for hierarchical phrase-based translation. In Proceedings of ACL, Short Papers, pages 6–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL,</booktitle>
<pages>961--968</pages>
<contexts>
<context position="1828" citStr="Galley et al., 2006" startWordPosition="256" endWordPosition="259">rom three Turkic languages to Chinese. 1 Introduction Currently, most methods on statistical machine translation (SMT) are developed for translation of languages with limited morphology (e.g., English, Chinese). They assumed that word was the atomic translation unit (ATU), always ignoring the internal morphological structure of word. This assumption can be traced back to the original IBM word-based models (Brown et al., 1993) and several significantly improved models, including phrase-based (Och and Ney, 2004; Koehn et al., 2003), hierarchical (Chiang, 2005) and syntactic (Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006) models. These improved models worked well for translating languages like English with large scale parallel corpora available. Different from languages with limited morphology, words of agglutinative languages are formed mainly by concatenation of stems and affixes. Generally, a stem can attach with several affixes, thus leading to tens of hundreds of possible inflected variants of lexicons for a single stem. Modeling each lexical form as a separate word will generate high out-of-vocabulary rate for SMT. Theoretically, ways like morphological analysis and increasing bilingua</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of COLING/ACL, pages 961–968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>David McClosky</author>
</authors>
<title>Improving statistical MT through morphological analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP,</booktitle>
<pages>676--683</pages>
<contexts>
<context position="2799" citStr="Goldwater and McClosky, 2005" startWordPosition="399" endWordPosition="402">eading to tens of hundreds of possible inflected variants of lexicons for a single stem. Modeling each lexical form as a separate word will generate high out-of-vocabulary rate for SMT. Theoretically, ways like morphological analysis and increasing bilingual corpora could alleviate the problem of data sparsity, but most agglutinative languages are less-studied and suffer from the problem of resource-scarceness. Therefore, previous research mainly focused on the different inflected variants of the same stem and made various transformation of input by morphological analysis, such as (Lee, 2004; Goldwater and McClosky, 2005; Yang and Kirchhoff, 2006; Habash and Sadat, 2006; Bisazza and Federico, 2009; Wang et al., 2011). These work still assume that the atomic translation unit is word, stem or morpheme, without considering the difference between stems and affixes. In agglutinative languages, stem is the base part of word not including inflectional affixes. Affix, especially inflectional affix, indicates different grammatical categories such as tense, person, number and case, etc., which is useful for translation rule disambiguation. Therefore, we employ stem as the atomic translation unit and use affix informati</context>
</contexts>
<marker>Goldwater, McClosky, 2005</marker>
<rawString>Sharon Goldwater and David McClosky. 2005. Improving statistical MT through morphological analysis. In Proceedings of HLT-EMNLP, pages 676–683.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Fatiha Sadat</author>
</authors>
<title>Arabic preprocessing schemes for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL, Short Papers,</booktitle>
<pages>49--52</pages>
<contexts>
<context position="2849" citStr="Habash and Sadat, 2006" startWordPosition="407" endWordPosition="410">s of lexicons for a single stem. Modeling each lexical form as a separate word will generate high out-of-vocabulary rate for SMT. Theoretically, ways like morphological analysis and increasing bilingual corpora could alleviate the problem of data sparsity, but most agglutinative languages are less-studied and suffer from the problem of resource-scarceness. Therefore, previous research mainly focused on the different inflected variants of the same stem and made various transformation of input by morphological analysis, such as (Lee, 2004; Goldwater and McClosky, 2005; Yang and Kirchhoff, 2006; Habash and Sadat, 2006; Bisazza and Federico, 2009; Wang et al., 2011). These work still assume that the atomic translation unit is word, stem or morpheme, without considering the difference between stems and affixes. In agglutinative languages, stem is the base part of word not including inflectional affixes. Affix, especially inflectional affix, indicates different grammatical categories such as tense, person, number and case, etc., which is useful for translation rule disambiguation. Therefore, we employ stem as the atomic translation unit and use affix information to guide translation rule selection. Stem-granu</context>
</contexts>
<marker>Habash, Sadat, 2006</marker>
<rawString>Nizar Habash and Fatiha Sadat. 2006. Arabic preprocessing schemes for statistical machine translation. In Proceedings of NAACL, Short Papers, pages 49– 52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eui-Hong Sam Han</author>
<author>George Karypis</author>
</authors>
<title>Centroid-based document classification: analysis experimental results.</title>
<date>2000</date>
<booktitle>In Proceedings of PKDD,</booktitle>
<pages>424--431</pages>
<contexts>
<context position="7342" citStr="Han and Karypis, 2000" startWordPosition="1115" endWordPosition="1118">ins affix ai within |D|. Let’s take the suffix “gha” from (A1) in Figure 1 as an example. We assume that there are only three instances of translation rules extracted from parallel corpus ((A) in Figure 1). We can see that “gha” only appear once in (A1) and also appear once in whole instances. Therefore, tfgha�(A1) is 0.5 and idfgha�(A1) is lo9(3/2). tfidfgha�(A1) is the product of tfgha�(A1) and idfgha�(A1) which is 0.09. Given a set of N translation rule instances with the same source and target part, we define the centroid vector dr according to the centroid-based classification algorithm (Han and Karypis, 2000), 1 ∑ di (2) dr _ N iEN 365 Data set #Sent. #Type #Token word stem morph word stem morph UY-CH-Train. 50K 69K 39K 42K 1.2M 1.2M 1.6M UY-CH-Dev. 0.7K*4 5.9K 4.1K 4.6K 18K 18K 23.5K UY-CH-Test. 0.7K*1 4.7K 3.3K 3.8K 14K 14K 17.8K KA-CH-Train. 50K 62K 40K 42K 1.1M 1.1M 1.3M KA-CH-Dev. 0.7K*4 5.3K 4.2K 4.5K 15K 15K 18K KA-CH-Test. 0.2K*1 2.6K 2.0K 2.3K 8.6K 8.6K 10.8K KI-CH-Train. 50K 53K 27K 31K 1.2M 1.2M 1.5M KI-CH-Dev. 0.5K*4 4.1K 3.1K 3.5K 12K 12K 15K KI-CH-Test. 0.2K*4 2.2K 1.8K 2.1K 4.7K 4.7K 5.8K Table 1: Statistics of data sets. ∗N means the number of reference, morph is short to morpheme.</context>
</contexts>
<marker>Han, Karypis, 2000</marker>
<rawString>Eui-Hong Sam Han and George Karypis. 2000. Centroid-based document classification: analysis experimental results. In Proceedings of PKDD, pages 424–431.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongjun He</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Improving statistical machine translation using lexicalized rule selection.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>321--328</pages>
<contexts>
<context position="10002" citStr="He et al., 2008" startWordPosition="1558" endWordPosition="1561">rkish side with the factored model (Koehn and Hoang, 2007). Yang and Kirchhoff (2006) backed off surface form to stem when translating OOV words of Finnish. Luong and Kan (2010) and Luong et al. (2010) focused on Finnish-English translation through improving word alignment and enhancing phrase table. These works still assumed that the atomic translation unit is word, stem or morpheme, without considering the difference between stems and affixes. There are also some work that employed the context information to make a better choice of translation rules (Carpuat and Wu, 2007; Chan et al., 2007; He et al., 2008; Cui et al., 2010). all the work employed rich context information, such as POS, syntactic, etc., and experiments were mostly done on less inflectional languages (i.e. Chinese, English) and resourceful languages (i.e. Arabic). 4 Experiments In this work, we conduct our experiments on three different agglutinative languages, including Uyghur, Kazakh and Kirghiz. All of them are derived from Altaic language family, belonging to Turkic languages, and mostly spoken by people in Central Asia. There are about 24 million people take these languages as mother tongue. All of the tasks are derived from</context>
</contexts>
<marker>He, Liu, Lin, 2008</marker>
<rawString>Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Improving statistical machine translation using lexicalized rule selection. In Proceedings of COLING, pages 321–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
</authors>
<title>Factored translation models.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLPCoNLL,</booktitle>
<pages>868--876</pages>
<contexts>
<context position="9445" citStr="Koehn and Hoang, 2007" startWordPosition="1468" endWordPosition="1471">fore, in addition to the traditional translation features on stem level, our model also adds the affix similarity score as a dynamic feature into the log-linear model (Och and Ney, 2002). 3 Related Work Most previous work on agglutinative language translation mainly focus on Turkish and Finnish. Bisazza and Federico (2009) and Mermer and Saraclar (2011) optimized morphological analysis as a pre-processing step to improve the translation between Turkish and English. Yeniterzi and Oflazer (2010) mapped the syntax of the English side to the morphology of the Turkish side with the factored model (Koehn and Hoang, 2007). Yang and Kirchhoff (2006) backed off surface form to stem when translating OOV words of Finnish. Luong and Kan (2010) and Luong et al. (2010) focused on Finnish-English translation through improving word alignment and enhancing phrase table. These works still assumed that the atomic translation unit is word, stem or morpheme, without considering the difference between stems and affixes. There are also some work that employed the context information to make a better choice of translation rules (Carpuat and Wu, 2007; Chan et al., 2007; He et al., 2008; Cui et al., 2010). all the work employed </context>
</contexts>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Philipp Koehn and Hieu Hoang. 2007. Factored translation models. In Proceedings of EMNLPCoNLL, pages 868–876.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="1744" citStr="Koehn et al., 2003" startWordPosition="242" endWordPosition="245">pproach significantly improves the translation performance on tasks of translating from three Turkic languages to Chinese. 1 Introduction Currently, most methods on statistical machine translation (SMT) are developed for translation of languages with limited morphology (e.g., English, Chinese). They assumed that word was the atomic translation unit (ATU), always ignoring the internal morphological structure of word. This assumption can be traced back to the original IBM word-based models (Brown et al., 1993) and several significantly improved models, including phrase-based (Och and Ney, 2004; Koehn et al., 2003), hierarchical (Chiang, 2005) and syntactic (Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006) models. These improved models worked well for translating languages like English with large scale parallel corpora available. Different from languages with limited morphology, words of agglutinative languages are formed mainly by concatenation of stems and affixes. Generally, a stem can attach with several affixes, thus leading to tens of hundreds of possible inflected variants of lexicons for a single stem. Modeling each lexical form as a separate word will generate high out-of-vocabulary r</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of NAACL, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>388--395</pages>
<contexts>
<context position="11443" citStr="Koehn, 2004" startWordPosition="1769" endWordPosition="1770"> side of training corpus. And phrase-based Moses3 is used as our 2http://mt.xmu.edu.cn/cwmt2011/en/index.html. 3http://www.statmt.org/moses/ 366 UY-CH KA-CH KI-CH word 31.74+0.0 28.64+0.0 35.05+0.0 stem 33.74+2.0 30.14+1.5 35.52+0.47 morph 32.69+0.95 29.21+0.57 34.97−0.08 affix 34.34+2.6 30.19+2.27 35.96+0.91 Table 2: Translation results from Turkic languages to Chinese. word: ATU is surface form, stem: ATU is represented stem, morph: ATU denotes morpheme, affix: stem translation with affix distribution similarity. BLEU scores in bold means significantly better than the baseline according to (Koehn, 2004) for p-value less than 0.01. baseline SMT system. The decoding weights are optimized with MERT (Och, 2003) to maximum word-level BLEU scores (Papineni et al., 2002). 4.1 Using Unsupervised Morphological Analyzer As most agglutinative languages are resourcepoor, we employ unsupervised learning method to obtain the morphological structure. Following the approach in (Virpioja et al., 2007), we employ the Morfessor4 Categories-MAP algorithm (Creutz and Lagus, 2005). It applies a hierarchical model with three categories (prefix, stem, and suffix) in an unsupervised way. From Table 1 we can see that</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP, pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Suk Lee</author>
</authors>
<title>Morphological analysis for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL, Short Papers,</booktitle>
<pages>57--60</pages>
<contexts>
<context position="2769" citStr="Lee, 2004" startWordPosition="397" endWordPosition="398">xes, thus leading to tens of hundreds of possible inflected variants of lexicons for a single stem. Modeling each lexical form as a separate word will generate high out-of-vocabulary rate for SMT. Theoretically, ways like morphological analysis and increasing bilingual corpora could alleviate the problem of data sparsity, but most agglutinative languages are less-studied and suffer from the problem of resource-scarceness. Therefore, previous research mainly focused on the different inflected variants of the same stem and made various transformation of input by morphological analysis, such as (Lee, 2004; Goldwater and McClosky, 2005; Yang and Kirchhoff, 2006; Habash and Sadat, 2006; Bisazza and Federico, 2009; Wang et al., 2011). These work still assume that the atomic translation unit is word, stem or morpheme, without considering the difference between stems and affixes. In agglutinative languages, stem is the base part of word not including inflectional affixes. Affix, especially inflectional affix, indicates different grammatical categories such as tense, person, number and case, etc., which is useful for translation rule disambiguation. Therefore, we employ stem as the atomic translatio</context>
</contexts>
<marker>Lee, 2004</marker>
<rawString>Young-Suk Lee. 2004. Morphological analysis for statistical machine translation. In Proceedings of HLT-NAACL, Short Papers, pages 57–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-string alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>609--616</pages>
<contexts>
<context position="1847" citStr="Liu et al., 2006" startWordPosition="260" endWordPosition="263">uages to Chinese. 1 Introduction Currently, most methods on statistical machine translation (SMT) are developed for translation of languages with limited morphology (e.g., English, Chinese). They assumed that word was the atomic translation unit (ATU), always ignoring the internal morphological structure of word. This assumption can be traced back to the original IBM word-based models (Brown et al., 1993) and several significantly improved models, including phrase-based (Och and Ney, 2004; Koehn et al., 2003), hierarchical (Chiang, 2005) and syntactic (Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006) models. These improved models worked well for translating languages like English with large scale parallel corpora available. Different from languages with limited morphology, words of agglutinative languages are formed mainly by concatenation of stems and affixes. Generally, a stem can attach with several affixes, thus leading to tens of hundreds of possible inflected variants of lexicons for a single stem. Modeling each lexical form as a separate word will generate high out-of-vocabulary rate for SMT. Theoretically, ways like morphological analysis and increasing bilingual corpora could all</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Treeto-string alignment template for statistical machine translation. In Proceedings of COLING-ACL, pages 609–616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minh-Thang Luong</author>
<author>Min-Yen Kan</author>
</authors>
<title>Enhancing morphological alignment for translating highly inflected languages.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>743--751</pages>
<contexts>
<context position="9564" citStr="Luong and Kan (2010)" startWordPosition="1488" endWordPosition="1491">s a dynamic feature into the log-linear model (Och and Ney, 2002). 3 Related Work Most previous work on agglutinative language translation mainly focus on Turkish and Finnish. Bisazza and Federico (2009) and Mermer and Saraclar (2011) optimized morphological analysis as a pre-processing step to improve the translation between Turkish and English. Yeniterzi and Oflazer (2010) mapped the syntax of the English side to the morphology of the Turkish side with the factored model (Koehn and Hoang, 2007). Yang and Kirchhoff (2006) backed off surface form to stem when translating OOV words of Finnish. Luong and Kan (2010) and Luong et al. (2010) focused on Finnish-English translation through improving word alignment and enhancing phrase table. These works still assumed that the atomic translation unit is word, stem or morpheme, without considering the difference between stems and affixes. There are also some work that employed the context information to make a better choice of translation rules (Carpuat and Wu, 2007; Chan et al., 2007; He et al., 2008; Cui et al., 2010). all the work employed rich context information, such as POS, syntactic, etc., and experiments were mostly done on less inflectional languages</context>
</contexts>
<marker>Luong, Kan, 2010</marker>
<rawString>Minh-Thang Luong and Min-Yen Kan. 2010. Enhancing morphological alignment for translating highly inflected languages. In Proceedings of COLING, pages 743–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minh-Thang Luong</author>
<author>Preslav Nakov</author>
<author>Min-Yen Kan</author>
</authors>
<title>A hybrid morpheme-word representation for machine translation of morphologically rich languages.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>148--157</pages>
<contexts>
<context position="9588" citStr="Luong et al. (2010)" startWordPosition="1493" endWordPosition="1496">the log-linear model (Och and Ney, 2002). 3 Related Work Most previous work on agglutinative language translation mainly focus on Turkish and Finnish. Bisazza and Federico (2009) and Mermer and Saraclar (2011) optimized morphological analysis as a pre-processing step to improve the translation between Turkish and English. Yeniterzi and Oflazer (2010) mapped the syntax of the English side to the morphology of the Turkish side with the factored model (Koehn and Hoang, 2007). Yang and Kirchhoff (2006) backed off surface form to stem when translating OOV words of Finnish. Luong and Kan (2010) and Luong et al. (2010) focused on Finnish-English translation through improving word alignment and enhancing phrase table. These works still assumed that the atomic translation unit is word, stem or morpheme, without considering the difference between stems and affixes. There are also some work that employed the context information to make a better choice of translation rules (Carpuat and Wu, 2007; Chan et al., 2007; He et al., 2008; Cui et al., 2010). all the work employed rich context information, such as POS, syntactic, etc., and experiments were mostly done on less inflectional languages (i.e. Chinese, English)</context>
</contexts>
<marker>Luong, Nakov, Kan, 2010</marker>
<rawString>Minh-Thang Luong, Preslav Nakov, and Min-Yen Kan. 2010. A hybrid morpheme-word representation for machine translation of morphologically rich languages. In Proceedings of EMNLP, pages 148– 157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aili Mairehaba</author>
<author>Wenbin Jiang</author>
<author>Zhiyang Wang</author>
<author>Yibulayin Tuergen</author>
<author>Qun Liu</author>
</authors>
<title>Directed graph model of Uyghur morphological analysis.</title>
<date>2012</date>
<journal>Journal</journal>
<contexts>
<context position="13532" citStr="Mairehaba et al., 2012" startWordPosition="2080" endWordPosition="2083"> gains +0.91 BLEU points over the baseline. 4http://www.cis.hut.fi/projects/Morpho/ UY Unsup Sup stem #Type 39K 21K #Token 1.2M 1.2M affix #Type 3.0K 0.3K #Token 0.4M 0.7M Table 3: Statistics of training corpus after unsupervised(Unsup) and supervised(Sup) morphological analysis. Figure 2: Uyghur to Chinese translation results after unsupervised and supervised analysis. 4.2 Using Supervised Morphological Analyzer Taking it further, we also want to see the effect of supervised analysis on our model. A generative statistical model of morphological analysis for Uyghur was developed according to (Mairehaba et al., 2012). Table 3 shows the difference of statistics of training corpus after supervised and unsupervised analysis. Supervised method generates fewer type of stems and affixes than the unsupervised approach. As we can see from Figure 2, except for the morph method, stem and affix based approaches perform better after supervised analysis. The results show that our approach can obtain even better translation performance if better morphological analyzers are available. Supervised morphological analysis generates more meaningful morphemes, which lead to better disambiguation of translation rules. 5 Conclu</context>
</contexts>
<marker>Mairehaba, Jiang, Wang, Tuergen, Liu, 2012</marker>
<rawString>Aili Mairehaba, Wenbin Jiang, Zhiyang Wang, Yibulayin Tuergen, and Qun Liu. 2012. Directed graph model of Uyghur morphological analysis. Journal</rawString>
</citation>
<citation valid="true">
<authors>
<author>Coskun Mermer</author>
<author>Murat Saraclar</author>
</authors>
<title>Unsupervised Turkish morphological segmentation for statistical machine translation.</title>
<date>2011</date>
<booktitle>In Workshop of MT and Morphologically-rich Languages.</booktitle>
<contexts>
<context position="9178" citStr="Mermer and Saraclar (2011)" startWordPosition="1426" endWordPosition="1429">nd related affix distribution represented as a vector. Then the stem sequence is used to search the translation rule table. If the source part is matched, the similarity will be calculated for each candidate translation rule by cosine similarity (as in equation 3). Therefore, in addition to the traditional translation features on stem level, our model also adds the affix similarity score as a dynamic feature into the log-linear model (Och and Ney, 2002). 3 Related Work Most previous work on agglutinative language translation mainly focus on Turkish and Finnish. Bisazza and Federico (2009) and Mermer and Saraclar (2011) optimized morphological analysis as a pre-processing step to improve the translation between Turkish and English. Yeniterzi and Oflazer (2010) mapped the syntax of the English side to the morphology of the Turkish side with the factored model (Koehn and Hoang, 2007). Yang and Kirchhoff (2006) backed off surface form to stem when translating OOV words of Finnish. Luong and Kan (2010) and Luong et al. (2010) focused on Finnish-English translation through improving word alignment and enhancing phrase table. These works still assumed that the atomic translation unit is word, stem or morpheme, wit</context>
</contexts>
<marker>Mermer, Saraclar, 2011</marker>
<rawString>Coskun Mermer and Murat Saraclar. 2011. Unsupervised Turkish morphological segmentation for statistical machine translation. In Workshop of MT and Morphologically-rich Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>295--302</pages>
<contexts>
<context position="9009" citStr="Och and Ney, 2002" startWordPosition="1401" endWordPosition="1404">d “·” denotes the inner product of the two vectors. Therefore, for a specific span to be translated, we first analyze it to get the corresponding stem sequence and related affix distribution represented as a vector. Then the stem sequence is used to search the translation rule table. If the source part is matched, the similarity will be calculated for each candidate translation rule by cosine similarity (as in equation 3). Therefore, in addition to the traditional translation features on stem level, our model also adds the affix similarity score as a dynamic feature into the log-linear model (Och and Ney, 2002). 3 Related Work Most previous work on agglutinative language translation mainly focus on Turkish and Finnish. Bisazza and Federico (2009) and Mermer and Saraclar (2011) optimized morphological analysis as a pre-processing step to improve the translation between Turkish and English. Yeniterzi and Oflazer (2010) mapped the syntax of the English side to the morphology of the Turkish side with the factored model (Koehn and Hoang, 2007). Yang and Kirchhoff (2006) backed off surface form to stem when translating OOV words of Finnish. Luong and Kan (2010) and Luong et al. (2010) focused on Finnish-E</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of ACL, pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Comput. Linguist.,</journal>
<pages>417--449</pages>
<contexts>
<context position="1723" citStr="Och and Ney, 2004" startWordPosition="238" endWordPosition="241">lts show that our approach significantly improves the translation performance on tasks of translating from three Turkic languages to Chinese. 1 Introduction Currently, most methods on statistical machine translation (SMT) are developed for translation of languages with limited morphology (e.g., English, Chinese). They assumed that word was the atomic translation unit (ATU), always ignoring the internal morphological structure of word. This assumption can be traced back to the original IBM word-based models (Brown et al., 1993) and several significantly improved models, including phrase-based (Och and Ney, 2004; Koehn et al., 2003), hierarchical (Chiang, 2005) and syntactic (Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006) models. These improved models worked well for translating languages like English with large scale parallel corpora available. Different from languages with limited morphology, words of agglutinative languages are formed mainly by concatenation of stems and affixes. Generally, a stem can attach with several affixes, thus leading to tens of hundreds of possible inflected variants of lexicons for a single stem. Modeling each lexical form as a separate word will generate hig</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Comput. Linguist., pages 417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="11549" citStr="Och, 2003" startWordPosition="1786" endWordPosition="1787">l. 3http://www.statmt.org/moses/ 366 UY-CH KA-CH KI-CH word 31.74+0.0 28.64+0.0 35.05+0.0 stem 33.74+2.0 30.14+1.5 35.52+0.47 morph 32.69+0.95 29.21+0.57 34.97−0.08 affix 34.34+2.6 30.19+2.27 35.96+0.91 Table 2: Translation results from Turkic languages to Chinese. word: ATU is surface form, stem: ATU is represented stem, morph: ATU denotes morpheme, affix: stem translation with affix distribution similarity. BLEU scores in bold means significantly better than the baseline according to (Koehn, 2004) for p-value less than 0.01. baseline SMT system. The decoding weights are optimized with MERT (Och, 2003) to maximum word-level BLEU scores (Papineni et al., 2002). 4.1 Using Unsupervised Morphological Analyzer As most agglutinative languages are resourcepoor, we employ unsupervised learning method to obtain the morphological structure. Following the approach in (Virpioja et al., 2007), we employ the Morfessor4 Categories-MAP algorithm (Creutz and Lagus, 2005). It applies a hierarchical model with three categories (prefix, stem, and suffix) in an unsupervised way. From Table 1 we can see that vocabulary sizes of the three languages are reduced obviously after unsupervised morphological analysis. </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="11607" citStr="Papineni et al., 2002" startWordPosition="1793" endWordPosition="1796">CH KI-CH word 31.74+0.0 28.64+0.0 35.05+0.0 stem 33.74+2.0 30.14+1.5 35.52+0.47 morph 32.69+0.95 29.21+0.57 34.97−0.08 affix 34.34+2.6 30.19+2.27 35.96+0.91 Table 2: Translation results from Turkic languages to Chinese. word: ATU is surface form, stem: ATU is represented stem, morph: ATU denotes morpheme, affix: stem translation with affix distribution similarity. BLEU scores in bold means significantly better than the baseline according to (Koehn, 2004) for p-value less than 0.01. baseline SMT system. The decoding weights are optimized with MERT (Och, 2003) to maximum word-level BLEU scores (Papineni et al., 2002). 4.1 Using Unsupervised Morphological Analyzer As most agglutinative languages are resourcepoor, we employ unsupervised learning method to obtain the morphological structure. Following the approach in (Virpioja et al., 2007), we employ the Morfessor4 Categories-MAP algorithm (Creutz and Lagus, 2005). It applies a hierarchical model with three categories (prefix, stem, and suffix) in an unsupervised way. From Table 1 we can see that vocabulary sizes of the three languages are reduced obviously after unsupervised morphological analysis. Table 2 shows the translation results. All the three trans</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: syntactically informed phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>271--279</pages>
<contexts>
<context position="1807" citStr="Quirk et al., 2005" startWordPosition="252" endWordPosition="255">sks of translating from three Turkic languages to Chinese. 1 Introduction Currently, most methods on statistical machine translation (SMT) are developed for translation of languages with limited morphology (e.g., English, Chinese). They assumed that word was the atomic translation unit (ATU), always ignoring the internal morphological structure of word. This assumption can be traced back to the original IBM word-based models (Brown et al., 1993) and several significantly improved models, including phrase-based (Och and Ney, 2004; Koehn et al., 2003), hierarchical (Chiang, 2005) and syntactic (Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006) models. These improved models worked well for translating languages like English with large scale parallel corpora available. Different from languages with limited morphology, words of agglutinative languages are formed mainly by concatenation of stems and affixes. Generally, a stem can attach with several affixes, thus leading to tens of hundreds of possible inflected variants of lexicons for a single stem. Modeling each lexical form as a separate word will generate high out-of-vocabulary rate for SMT. Theoretically, ways like morphological analysis an</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: syntactically informed phrasal SMT. In Proceedings of ACL, pages 271–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Chris Buckley</author>
</authors>
<title>Term weighting approaches in automatic text retrieval.</title>
<date>1987</date>
<tech>Technical report.</tech>
<contexts>
<context position="5658" citStr="Salton and Buckley, 1987" startWordPosition="825" endWordPosition="829">ore, given a span “zunyi/STM yighin/STM+i/SUF+da/SUF+...” to be translated, we hope to encourage our model to select the second translation rule. We can achieve this by calculating similarity between the affix distributions of the translation rule and the span. The affix distribution can be obtained by keeping the related affixes for each rule instance during translation rule extraction ((A) in Figure 1). After extracting and scoring stem-granularity rules in a traditional way, we extract stem-granularity rules again by keeping affix information and compute the affix distribution with tf-idf (Salton and Buckley, 1987). Finally, the affix distribution will be added to the previous stem-granularity rules. 2.1 Affix Distribution Estimation Formally, translation rule instances with the same source part can be treated as a document collections, so each rule instance in the collection is &apos;We employ concepts from text classification to illustrate how to estimate affix distribution. some kind of document. Our goal is to classify the source parts into the target parts on the document collection level with the help of affix distribution. Accordingly, we employ vector space model (VSM) to represent affix distribution</context>
</contexts>
<marker>Salton, Buckley, 1987</marker>
<rawString>Gerard Salton and Chris Buckley. 1987. Term weighting approaches in automatic text retrieval. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of ICSLP,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="10791" citStr="Stolcke, 2002" startWordPosition="1684" endWordPosition="1685">e, English) and resourceful languages (i.e. Arabic). 4 Experiments In this work, we conduct our experiments on three different agglutinative languages, including Uyghur, Kazakh and Kirghiz. All of them are derived from Altaic language family, belonging to Turkic languages, and mostly spoken by people in Central Asia. There are about 24 million people take these languages as mother tongue. All of the tasks are derived from the evaluation of China Workshop of Machine Translation (CWMT)2. Table 1 shows the statistics of data sets. For the language model, we use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 5-gram model with the target side of training corpus. And phrase-based Moses3 is used as our 2http://mt.xmu.edu.cn/cwmt2011/en/index.html. 3http://www.statmt.org/moses/ 366 UY-CH KA-CH KI-CH word 31.74+0.0 28.64+0.0 35.05+0.0 stem 33.74+2.0 30.14+1.5 35.52+0.47 morph 32.69+0.95 29.21+0.57 34.97−0.08 affix 34.34+2.6 30.19+2.27 35.96+0.91 Table 2: Translation results from Turkic languages to Chinese. word: ATU is surface form, stem: ATU is represented stem, morph: ATU denotes morpheme, affix: stem translation with affix distribution similarity. BLEU scores in bold means significantly</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings of ICSLP, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sami Virpioja</author>
<author>Jaakko J V¨ayrynen</author>
<author>Mathias Creutz</author>
<author>Markus Sadeniemi</author>
</authors>
<title>Morphology-aware statistical machine translation based on morphs induced in an unsupervised manner.</title>
<date>2007</date>
<booktitle>In Proceedings of MT SUMMIT,</booktitle>
<pages>491--498</pages>
<marker>Virpioja, V¨ayrynen, Creutz, Sadeniemi, 2007</marker>
<rawString>Sami Virpioja, Jaakko J. V¨ayrynen, Mathias Creutz, and Markus Sadeniemi. 2007. Morphology-aware statistical machine translation based on morphs induced in an unsupervised manner. In Proceedings of MT SUMMIT, pages 491–498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyang Wang</author>
<author>Yajuan L¨u</author>
<author>Qun Liu</author>
</authors>
<title>Multi-granularity word alignment and decoding for agglutinative language translation.</title>
<date>2011</date>
<booktitle>In Proceedings of MT SUMMIT,</booktitle>
<pages>360--367</pages>
<marker>Wang, L¨u, Liu, 2011</marker>
<rawString>Zhiyang Wang, Yajuan L¨u, and Qun Liu. 2011. Multi-granularity word alignment and decoding for agglutinative language translation. In Proceedings of MT SUMMIT, pages 360–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mei Yang</author>
<author>Katrin Kirchhoff</author>
</authors>
<title>Phrase-based backoff models for machine translation of highly inflected languages.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>pages</pages>
<contexts>
<context position="2825" citStr="Yang and Kirchhoff, 2006" startWordPosition="403" endWordPosition="406">possible inflected variants of lexicons for a single stem. Modeling each lexical form as a separate word will generate high out-of-vocabulary rate for SMT. Theoretically, ways like morphological analysis and increasing bilingual corpora could alleviate the problem of data sparsity, but most agglutinative languages are less-studied and suffer from the problem of resource-scarceness. Therefore, previous research mainly focused on the different inflected variants of the same stem and made various transformation of input by morphological analysis, such as (Lee, 2004; Goldwater and McClosky, 2005; Yang and Kirchhoff, 2006; Habash and Sadat, 2006; Bisazza and Federico, 2009; Wang et al., 2011). These work still assume that the atomic translation unit is word, stem or morpheme, without considering the difference between stems and affixes. In agglutinative languages, stem is the base part of word not including inflectional affixes. Affix, especially inflectional affix, indicates different grammatical categories such as tense, person, number and case, etc., which is useful for translation rule disambiguation. Therefore, we employ stem as the atomic translation unit and use affix information to guide translation ru</context>
<context position="9472" citStr="Yang and Kirchhoff (2006)" startWordPosition="1472" endWordPosition="1475"> traditional translation features on stem level, our model also adds the affix similarity score as a dynamic feature into the log-linear model (Och and Ney, 2002). 3 Related Work Most previous work on agglutinative language translation mainly focus on Turkish and Finnish. Bisazza and Federico (2009) and Mermer and Saraclar (2011) optimized morphological analysis as a pre-processing step to improve the translation between Turkish and English. Yeniterzi and Oflazer (2010) mapped the syntax of the English side to the morphology of the Turkish side with the factored model (Koehn and Hoang, 2007). Yang and Kirchhoff (2006) backed off surface form to stem when translating OOV words of Finnish. Luong and Kan (2010) and Luong et al. (2010) focused on Finnish-English translation through improving word alignment and enhancing phrase table. These works still assumed that the atomic translation unit is word, stem or morpheme, without considering the difference between stems and affixes. There are also some work that employed the context information to make a better choice of translation rules (Carpuat and Wu, 2007; Chan et al., 2007; He et al., 2008; Cui et al., 2010). all the work employed rich context information, s</context>
</contexts>
<marker>Yang, Kirchhoff, 2006</marker>
<rawString>Mei Yang and Katrin Kirchhoff. 2006. Phrase-based backoff models for machine translation of highly inflected languages. In Proceedings of EACL, pages</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>