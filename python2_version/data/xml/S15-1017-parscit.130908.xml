<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000128">
<title confidence="0.999155">
Dissecting the Practical Lexical Function Model for
Compositional Distributional Semantics
</title>
<author confidence="0.953731">
Abhijeet Gupta, Jason Utt and Sebastian Pad´o
</author>
<affiliation confidence="0.6438935">
Institut f¨ur Maschinelle Sprachverarbeitung
Universit¨at Stuttgart
</affiliation>
<email confidence="0.987899">
[guptaat|uttjn|pado]@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.998526" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998256733333333">
The Practical Lexical Function model (PLF)
is a recently proposed compositional distribu-
tional semantic model which provides an el-
egant account of composition, striking a bal-
ance between expressiveness and robustness
and performing at the state-of-the-art. In this
paper, we identify an inconsistency in PLF be-
tween the objective function at training and the
prediction at testing which leads to an over-
counting of the predicate’s contribution to the
meaning of the phrase. We investigate two pos-
sible solutions of which one (the exclusion of
simple lexical vector at test time) improves per-
formance significantly on two out of the three
composition datasets.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99981672">
Compositional distributional semantic models
(CDSMs) make an important theoretical contribution,
explaining the meaning of a phrase by the meanings
of its parts. They have also found application
in psycholinguistics (Lenci, 2011), in sentiment
analysis (Socher et al., 2012), and in machine
translation (Kalchbrenner and Blunsom, 2013).
A first generation of CDSMs represented all words
as vectors and combined them by component-wise
operations (Mitchell and Lapata, 2010). Given the
conceptual limitations of this simple approach, nu-
merous models were subsequently proposed which
represent the meaning of predicates as higher-order
algebraic objects such as matrices and tensors (Ba-
roni and Zamparelli, 2010; Guevara, 2010; Coecke
et al., 2010). For example, one-place predicates such
as adjectives or intransitive verbs can be modeled as
matrices (order-2 tensors), and two-place predicates,
e.g., transitive verbs, as order-3 tensors, and so forth.
While such tensors enable mathematically elegant ac-
counts of composition, their large degrees of freedom
lead to severe sparsity issues when they are learned
from corpora.
The recently proposed Practical Lexical Function
model (PLF; Paperno et al., 2014) represents a com-
promise between these two extremes by restricting
itself to vectors and matrices, effectively reducing
sparsity while retaining state-of-the-art performance
across multiple datasets. It does away with tensors by
ignoring interactions among the arguments of predi-
cates p. Instead, each argument position arg is mod-
eled as a matrix p that is applied to a vector for the
argument’s meaning, -a-+. The meaning of the phrase
is then defined as the sum of the lexical meaning of
the predicate, p , and the contributions of each ar-
gument (see Fig. 1). The matrices can be learned in
a supervised manner with regression from pairs of
corpus-extracted vectors for arguments and phrases.
In this paper, we identify an inconsistency between
the training and testing phases of the PLF. More
specifically, we show that its composition procedure
leads to over-counting of the contribution of the pred-
icate. We propose two remedies to harmonize the
training and prediction phases – by excluding the
predicate meaning from either training or testing. In
an evaluation of the standard PLF and our variants
on three datasets, we find that modifying the training
phase fails, but that modifying testing phase improves
performance on two out of three datasets. We analyze
this effect in terms of a bias-variance tradeoff.
</bodyText>
<page confidence="0.989877">
153
</page>
<note confidence="0.6493825">
Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 153–158,
Denver, Colorado, June 4–5, 2015.
</note>
<table confidence="0.743133">
❑S ❑O −−−−−→ −−→
write x−−→ user + write x software + write
user {−−→ ❑S ❑O
−−→ write, write, write}
</table>
<figureCaption confidence="0.988196">
Figure 1: Practical Lexical Function model derivation for
the noun-verb-noun phrase “user writes software”.
</figureCaption>
<sectionHeader confidence="0.815041" genericHeader="method">
2 Debugging the PLF model
</sectionHeader>
<subsectionHeader confidence="0.997991">
2.1 An Inconsistency
</subsectionHeader>
<bodyText confidence="0.99453325">
We have identified an inconsistency in the PLF model
as a result of which the predicted vector for a phrase
systematically differs from the corpus-observed vec-
tor of the phrase. We will illustrate it on a minimal
example, the phrase “dogs sleep”.
Training Phase. The training of PLF creates three
representations: (1), a lexical vector for the noun
(−→n ); (2), the lexical vector for the verb (−→v ); and
(3), a matrix for the subject argument position of the
verb (v ). While (1) and (2) can be acquired directly
from the corpus, (3) involves optimization, since the
matrix (3) is supposed to account for the verb’s dis-
ambiguating effect on all its subjects. PLF proposes
to learn matrices via regression problems such as the
following (Guevara, 2010), where subj(v) comprises
the subjects seen with the verb v:1
</bodyText>
<equation confidence="0.997518">
❑S
v := argmin
M n∈subj(v)
� IIM x →−n − n−→vII2 (1)
</equation>
<bodyText confidence="0.999782285714286">
That is, the verb’s subject matrix is learned as the
matrix which, multiplied with a subject noun vector,
best predicts the noun-verb phrase vector. If we as-
sume that the verb of our example (sleep) is only seen
with a single noun in the corpus, namely its subject
dog, Eq. (1) has a particularly simple solution where
the matrix can perfectly predict the phrase vector:
</bodyText>
<equation confidence="0.9229345">
❑S
sleep x dog = dog sleep (2)
</equation>
<footnote confidence="0.895496">
1All matrices are learned using least-squares regression and,
for the sake of simplicity, we ignore regularization. Adjective
matrices are obtained in the same fashion.
</footnote>
<bodyText confidence="0.9820985">
Testing Phase. PLF predicts the phrase meaning P
for our example as predicate plus argument meaning:
</bodyText>
<equation confidence="0.8738145">
P(dog sleeps) = sleep+ sleep x
−−→ ❑S dog (3)
</equation>
<bodyText confidence="0.8543264">
Intuitively, what we would expect as the result of
−−−−−−→
this computation to be dog sleeps — the empirically
observed vector for the noun-verb phrase. However,
substituting Eq. (2) into Eq. (3), we instead obtain:
</bodyText>
<equation confidence="0.846322">
P(dog sleeps) = sleep + dog sleeps (4)
</equation>
<bodyText confidence="0.999982461538462">
The predicted phrase meaning does not correspond
to the empirical phrase vector because in PLF, the
verb contributes twice to the phrase meaning.
Discussion. This issue remains pertinent beyond
the minimal example presented above. The reason
is a discrepancy between the training and test se-
tups: The argument matrices in PLF are learned so
as to predict the complete phrase vector when mul-
tiplied with an argument (compare Eq. (1)).2 This
objective is inconsistent with the way phrase vectors
are predicted at test time. The addition of the pred-
icate’s lexical vector thus amounts to a systematic
over-counting of the predicate’s lexical contribution.
</bodyText>
<subsectionHeader confidence="0.989524">
2.2 Two Ways to Remedy the Inconsistency
</subsectionHeader>
<bodyText confidence="0.9999617">
The above description gives direct rise to two simple
strategies to harmonize training and test procedures.
Adapting the Training Phase. One strategy is to
adapt the training objective from Eq. (1). Recogniz-
ing that the predicate vector is added in by Eq. (3)
at test time, we can attempt to learn a matrix that
predicts not the phrase vector, but the difference be-
tween the phrase vector and the predicate vector. That
means, the matrices capture only the disambiguating
contribution of argument positions such as subject:
</bodyText>
<equation confidence="0.963166">
�IS v = argmin
M n∈subj(v) IIMx →−n − (−→ n v − →−v )II2 (5)
</equation>
<bodyText confidence="0.999851666666667">
Adapting the Testing Phase. Another strategy is
to adapt the phrase meaning prediction at test time by
simply leaving out the predicate vector. For subject-
</bodyText>
<subsectionHeader confidence="0.368856">
ElS
</subsectionHeader>
<bodyText confidence="0.930108">
verb combinations, we predict P(n v) =v x−→n .
</bodyText>
<footnote confidence="0.794021">
2A formal, more general argument can be made based on the
</footnote>
<page confidence="0.846315">
❑arg
</page>
<figure confidence="0.847587818181818">
error term e�=v ×
−−→
❑O
write}
{
❑S
write x user + write,
software
n v which is minimized in training.
−→
−→ n −
</figure>
<page confidence="0.989895">
154
</page>
<bodyText confidence="0.5330168">
verb in context landmark in context similarity
private landlord charge annual rent private landlord accuse annual rent low
private landlord charge annual rent private landlord bill annual rent high
armed police charge unemployed person armed police accuse unemployed person high
armed police charge unemployed person armed police bill unemployed person low
</bodyText>
<tableCaption confidence="0.996613">
Table 1: Example of experimental items in the ANVAN data sets (target verb: charge).
</tableCaption>
<bodyText confidence="0.950144">
For transitive sentences (cf. Figure 1), we predict
</bodyText>
<equation confidence="0.905602">
�S n + �O
P(n v n) =v x�� v xn (the sum of the subject
</equation>
<bodyText confidence="0.9886985">
and the object contributions), and analogously for
other constructions.
</bodyText>
<sectionHeader confidence="0.999566" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.990512862068966">
Evaluation Datasets. We evaluate the modifica-
tions from the last section on three standard bench-
marks for CDSMs: ANVAN-1 (Kartsaklis et al.,
2013), ANVAN-2 (Grefenstette, 2013) (Paperno et
al.’s term) and NVN (Grefenstette and Sadrzadeh,
2011) (our term).
As the abbreviations indicate, the two ANVAN
datasets contain transitive verbs whose NP arguments
are modified by arguments; the NVN dataset con-
tains only bare noun arguments. All three datasets
are built around ambiguous target verbs that are com-
bined with two disambiguating contexts (subjects
plus objects) and two landmark verbs in a balanced
design (cf. Table 1). Each context matches one of the
landmark verbs, but not the other. Annotators were
asked to rate the similarity between the target verb in
context and the landmark on a Likert scale.
Corpus and Co-Occurrences. We followed the
specifications by Paperno et al. (2014) as closely
as possible to replicate the original PLF results. As
corpora, we used ukWAC, English Wikipedia, and
the BNC. We extracted a square co-occurrence ma-
trix for the 30K most frequent content words using
a 3-word window and applied the PPMI transforma-
tion. Subsequently, the matrix was reduced to 300
dimensions with SVD. In the same manner, we built
a co-occurrence matrix for all corpus bigrams for
relevant adjectives and verbs from the experimental
materials, applying a frequency threshold of 5.
</bodyText>
<footnote confidence="0.72446">
Composition Models and Evaluation. We build
matrix representations for adjectives and subject and
</footnote>
<figureCaption confidence="0.975721">
Figure 2: PLF Derivation for ANVAN phrase “private
landlord charge yearly rent”.
</figureCaption>
<bodyText confidence="0.984660428571429">
object positions of verbs using the DISSECT toolkit
(Dinu et al., 2013). In addition to the standard
PLF model, which we see as a baseline, we imple-
ment both proposals from Section 2.2. On the NVN
dataset, both training and test modification can ap-
ply only to the verb (cf. Figure 1), which gives us
two conditions. On the ANVAN datasets (cf. Fig-
ure 2), the changes can be applied to the verb, to the
adjectives, or to both, for a total of six conditions.
Our evaluation measure is the nonparametric
Spearman correlations between each annotator’s sim-
ilarity rating and the cosine between the predicted
sentence vectors containing the ambiguous and land-
mark verb, respectively.
</bodyText>
<sectionHeader confidence="0.999438" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9995545">
Main Results. The main results are shown in Ta-
ble 2. Our PLF re-implementation in the first column
almost replicates the results reported by Paperno et
al. (2014) for ANVAN1 and ANVAN2 (20 and 36,
respectively). On NVN, no results for the PLF were
previously reported. Our result (35.4) is substantially
above the result of 21.0 reported by Greffenstette and
Sadrzadeh (2011) for their categorial model. This
supports our general focus on the PLF as an interest-
ing target for analysis.
</bodyText>
<figure confidence="0.99660245">
−−−→ ElO
°S −→ charge × p obj
charge + charge × np subj+
ElS −→
{ −−−→
charge + charge × np subj,
DO
charge} →− np obj:=
−−−→ ON
−−→
annual + annual ×rent
→− np subj:=
−−−→ ON
private + private ×
−−−→ ElS
charge, charge,
0O
charge}
{
landlord
</figure>
<page confidence="0.992043">
155
</page>
<table confidence="0.9128808">
Training phase modifications Test phase modifications
Dataset PLF Sub Adj Sub Verb Sub Both No Adj No Verb No Both
ANVAN1 20.6 18.7 -0.3 3.8 19.2 20.7 22.1*
ANVAN2 35.2 32.8 13.8 17.0 33.8 35.7 35.4
NVN 35.4 – 25.5 – – 40.6** –
</table>
<tableCaption confidence="0.9968485">
Table 2: Experimental results (Spearman’s p) on three dataset. Significant improvements over the PLF results are
indicated with stars (*: p&lt;0.05, **: p&lt;0.01 ), – denotes non-applicability of parameter.
</tableCaption>
<bodyText confidence="0.999909243243243">
The results for the training phase modification are
overwhelmingly negative. There is a minor degrada-
tion when the adjective is subtracted at training time,
and major degradation when the verb is subtracted.
We will come back to this result below.
In contrast, we obtain improvements when we
modify the test phase, when we either leave out the
verb or both the verb and the adjective in the composi-
tion. For two out of the three datasets, the respective
best models perform statistically significantly better
than the PLF as determined by a bootstrap resampling
test (Efron and Tibshirani, 1993): ANVAN1 (+1.5%,
p&lt;0.05) and NVN (+5.2%, p&lt;0.01). The improve-
ment for ANVAN2 (+0.5%) is not large enough to
reach significance.
Discussion. These results leave us with two main
questions: (a), why does the modification at training
time fail so completely; and (b), can we develop a
better understanding of the kind of improvement that
the modification at test time introduces?
Regarding question (a), we believe that the dif-
ference between the phrase vector and the predicate
vector that we are training the matrix to predict in
Eq. (5) is, in practice, a very brittle representation.
The reason is that typically the phrase nv is much
less frequent than v, and therefore n−→v − →−v ≈ −−→v
(cf. Figure 3). Consequently, the matrix attempts to
predict the verb vector from the noun – not only a
very hard problem, but one that does not help solve
the task at hand.
To answer question (b), we perform a mixed ef-
fects linear regression analysis (Hedeker, 2005) on
the three datasets, concentrating on a comparison of
the standard PLF and the best respective test phase
modification. We follow the intuition that the fre-
quency and ambiguity of the target verbs should in-
fluence the quality of the prediction both in the PLF
</bodyText>
<table confidence="0.998518833333333">
ANVAN1 ANVAN2 NVN
logf -359*** -182n.s. -96***
ambig 118*** 8 n.s. 6***
ModTest 438*** -2606*** -1413***
ModTest:logf -53** 165*** 94***
ModTest:ambig 20* 32*** 8***
</table>
<tableCaption confidence="0.979099">
Table 3: Coefficients of Linear Mixed Effects Model.
*: p&lt;0.05; **: p&lt;0.01; ***: p&lt;0.001. See text for details.
</tableCaption>
<bodyText confidence="0.999981185185185">
and in the modified model, and that it might be in-
formative to look at differences in these effects. To
this effect, we construct a mixed-effects model which
predicts, for each experimental item (cf. Table 1), the
absolute rank difference between the item’s rank in
the gold standard ratings and the item’s rank in the
model prediction. Thus, high values of the output
variable denote items which are difficult to predict,
while low values of the output variable denote items
which are easy to predict. As fixed effects, we include
the target verbs’ logarithmized corpus frequencies
(logf), their ambiguities, measured as the number of
WordNet top nodes subsuming their synsets (ambig),
the presence of the test phase modification (NoVerb
for ANVAN2 and NVN, NoBoth for ANVAN1; Mod-
Test) as well as interaction terms between ModTest
and the two other predictors. We also include the
identity of the target verb as random effect.
The results are shown in Table 3. There are consid-
erable differences between the datasets, but the over-
all patterns are nevertheless comparable. Notably,
frequency has a negative effect on rank difference. In
other words, more frequent verbs are easier to pre-
dict. Conversely, the ambiguity of the target verb has
a positive effect on rank difference, that is, higher
ambiguity makes predictions more difficult. Both of
these effects are very strong on ANVAN1 and NVN
</bodyText>
<page confidence="0.99447">
156
</page>
<figure confidence="0.629902">
Similarity to ModTrain−Phrase Vector
verb noun
</figure>
<figureCaption confidence="0.950743">
Figure 3: Similarities between the training-time modified
</figureCaption>
<bodyText confidence="0.980639131578947">
phrase vector (subject-verb &amp; verb-object) and the respec-
tive word vectors in the NVN dataset. The low values and
smaller variance in verb similarities shows the informa-
tion encoded by the modified phrase vector aligns better
with the verb’s (or predicate’s) information than that of
the noun (argument).
and not significant on ANVAN2, which appears to be
a more controlled dataset. Taken together, the models
still seem to struggle with ambiguous and infrequent
target verbs.
The coefficients that we obtain for ModTest look
puzzling at first glance: we obtain a negative coeffi-
cient (i.e., an overall improvement) only for AN-
VAN2 and NVN while the coefficient is positive
for ANVAN1. For ANVAN1, the improvement is
brought about by the interaction with the frequency
variable: when the test phase is modified, the (ben-
eficial) effect of frequency becomes much stronger,
that is, the predictions for high-frequency verbs im-
prove. In contrast, the effect of frequency becomes
weaker for the test phase modification on ANVAN2
and NVN. What is true for all three datasets is that the
effect of ambiguity gets stronger when the test phase
is modified: ambiguous verbs become significantly
more difficult to model.
On the basis of this analysis, we believe that this
difference between the standard PLF and our test
phase modification can be understood as a classical
bias-variance tradeoff: the addition of the predicate
meaning in the standard PLF reduces variance, ensur-
ing that the phrase meaning stays close to the predi-
cate meaning prior even for matrices that are difficult
to learn, e.g., due to sparse data or high ambiguity. At
the same time, this dilutes the disambiguating effect
of composition. In our modified scheme, the situa-
tion is reversed: the composed representations vary
more freely, which benefits well-learned matrices but
leads to worse predictions for poorly learned ones.
</bodyText>
<sectionHeader confidence="0.999262" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999946333333333">
In this paper, we have presented an analysis of the re-
cent Practical Lexical Function (PLF) model in com-
positional distributional semantics. We have shown
that the PLF contains an inconsistency between the
objective function at training time and the definition
of compositional phase construction at testing time.
We have argued that either training or testing needs
to be modified to harmonize the two. Our empiri-
cal evaluation found that testing phase modification
is indeed effective (by reducing bias in the predic-
tions), while the training phase modification is not
(by relying on brittle representations). In the spirit of
the bias-variance analysis, future work is to experi-
ment with weighting schemes to optimize the relative
contributions of predicate and arguments.
</bodyText>
<sectionHeader confidence="0.997297" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999496">
We gratefully acknowledge funding of our research
by the DFG (SFB 732, Project D10).
</bodyText>
<sectionHeader confidence="0.99865" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.982551642857143">
Marco Baroni and Roberto Zamparelli. 2010. Nouns are
vectors, adjectives are matrices: Representing adjective-
noun constructions in semantic space. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1183–1193.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.
2010. Mathematical foundations for a compositional
distributional model of meaning. Linguistic Analysis,
36:345–386.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013. DISSECT - DIStributional SEmantics Composi-
tion Toolkit. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics: Sys-
tem Demonstrations, pages 31–36, Sofia, Bulgaria.
</reference>
<figure confidence="0.9962905">
●
0.6
0.4
0.2
0.0
−0.2
−0.4
−0.6
</figure>
<page confidence="0.928091">
157
</page>
<reference confidence="0.999852019230769">
Bradley Efron and Robert J. Tibshirani. 1993. An In-
troduction to the Bootstrap. Chapman and Hall, New
York.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical compositional
distributional model of meaning. In Proceedings of
the 2011 Conference on Empirical Methods in Natural
Language Processing, pages 1394–1404, Edinburgh,
Scotland, UK.
Edward Grefenstette. 2013. Category-Theoretic Quanti-
tative Compositional Distributional Models of Natural
Language Semantics. Ph.D. thesis.
Emiliano Guevara. 2010. A regression model of adjective-
noun compositionality in distributional semantics. In
Proceedings of the 2010 Workshop on GEometrical
Models of Natural Language Semantics, pages 33–37,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Donald Hedeker. 2005. Generalized linear mixed models.
In Encyclopedia of Statistics in Behavioral Science.
Wiley, New York.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of the
Conference on Empirical Methods in Natural Language
Processing, pages 1700–1709, Melbourne, Australia.
Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen
Pulman. 2013. Separating disambiguation from com-
position in distributional semantics. In Proceedings of
the Seventeenth Conference on Computational Natural
Language Learning, pages 114–123, Sofia, Bulgaria.
Alessandro Lenci. 2011. Composing and updating
verb argument expectations: A distributional semantic
model. In Proceedings of the 2nd Workshop on Cog-
nitive Modeling and Computational Linguistics, pages
58–66, Portland, OR.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388–1429.
Denis Paperno, Nghia The Pham, and Marco Baroni. 2014.
A practical and linguistically-motivated approach to
compositional distributional semantics. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics, pages 90–99, Baltimore,
Maryland.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceedings
of the 2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational Nat-
ural Language Learning, EMNLP-CoNLL ’12, pages
1201–1211, Stroudsburg, PA, USA. Association for
Computational Linguistics.
</reference>
<page confidence="0.997087">
158
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.671036">
<title confidence="0.94535">Dissecting the Practical Lexical Function Model Compositional Distributional Semantics</title>
<author confidence="0.988265">Jason Utt Gupta</author>
<affiliation confidence="0.864784">Institut f¨ur Maschinelle Universit¨at</affiliation>
<abstract confidence="0.9995785">The Practical Lexical Function model (PLF) is a recently proposed compositional distributional semantic model which provides an elegant account of composition, striking a balance between expressiveness and robustness and performing at the state-of-the-art. In this paper, we identify an inconsistency in PLF between the objective function at training and the prediction at testing which leads to an overcounting of the predicate’s contribution to the meaning of the phrase. We investigate two possible solutions of which one (the exclusion of simple lexical vector at test time) improves performance significantly on two out of the three composition datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjectivenoun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1183--1193</pages>
<contexts>
<context position="1639" citStr="Baroni and Zamparelli, 2010" startWordPosition="225" endWordPosition="229">l contribution, explaining the meaning of a phrase by the meanings of its parts. They have also found application in psycholinguistics (Lenci, 2011), in sentiment analysis (Socher et al., 2012), and in machine translation (Kalchbrenner and Blunsom, 2013). A first generation of CDSMs represented all words as vectors and combined them by component-wise operations (Mitchell and Lapata, 2010). Given the conceptual limitations of this simple approach, numerous models were subsequently proposed which represent the meaning of predicates as higher-order algebraic objects such as matrices and tensors (Baroni and Zamparelli, 2010; Guevara, 2010; Coecke et al., 2010). For example, one-place predicates such as adjectives or intransitive verbs can be modeled as matrices (order-2 tensors), and two-place predicates, e.g., transitive verbs, as order-3 tensors, and so forth. While such tensors enable mathematically elegant accounts of composition, their large degrees of freedom lead to severe sparsity issues when they are learned from corpora. The recently proposed Practical Lexical Function model (PLF; Paperno et al., 2014) represents a compromise between these two extremes by restricting itself to vectors and matrices, eff</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjectivenoun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1183–1193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Clark</author>
</authors>
<title>Mathematical foundations for a compositional distributional model of meaning. Linguistic Analysis,</title>
<date>2010</date>
<pages>36--345</pages>
<contexts>
<context position="1676" citStr="Coecke et al., 2010" startWordPosition="232" endWordPosition="235">phrase by the meanings of its parts. They have also found application in psycholinguistics (Lenci, 2011), in sentiment analysis (Socher et al., 2012), and in machine translation (Kalchbrenner and Blunsom, 2013). A first generation of CDSMs represented all words as vectors and combined them by component-wise operations (Mitchell and Lapata, 2010). Given the conceptual limitations of this simple approach, numerous models were subsequently proposed which represent the meaning of predicates as higher-order algebraic objects such as matrices and tensors (Baroni and Zamparelli, 2010; Guevara, 2010; Coecke et al., 2010). For example, one-place predicates such as adjectives or intransitive verbs can be modeled as matrices (order-2 tensors), and two-place predicates, e.g., transitive verbs, as order-3 tensors, and so forth. While such tensors enable mathematically elegant accounts of composition, their large degrees of freedom lead to severe sparsity issues when they are learned from corpora. The recently proposed Practical Lexical Function model (PLF; Paperno et al., 2014) represents a compromise between these two extremes by restricting itself to vectors and matrices, effectively reducing sparsity while reta</context>
</contexts>
<marker>Coecke, Sadrzadeh, Clark, 2010</marker>
<rawString>Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. 2010. Mathematical foundations for a compositional distributional model of meaning. Linguistic Analysis, 36:345–386.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Nghia The Pham</author>
<author>Marco Baroni</author>
</authors>
<title>DISSECT - DIStributional SEmantics Composition Toolkit.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>31--36</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="9657" citStr="Dinu et al., 2013" startWordPosition="1540" endWordPosition="1543">are co-occurrence matrix for the 30K most frequent content words using a 3-word window and applied the PPMI transformation. Subsequently, the matrix was reduced to 300 dimensions with SVD. In the same manner, we built a co-occurrence matrix for all corpus bigrams for relevant adjectives and verbs from the experimental materials, applying a frequency threshold of 5. Composition Models and Evaluation. We build matrix representations for adjectives and subject and Figure 2: PLF Derivation for ANVAN phrase “private landlord charge yearly rent”. object positions of verbs using the DISSECT toolkit (Dinu et al., 2013). In addition to the standard PLF model, which we see as a baseline, we implement both proposals from Section 2.2. On the NVN dataset, both training and test modification can apply only to the verb (cf. Figure 1), which gives us two conditions. On the ANVAN datasets (cf. Figure 2), the changes can be applied to the verb, to the adjectives, or to both, for a total of six conditions. Our evaluation measure is the nonparametric Spearman correlations between each annotator’s similarity rating and the cosine between the predicted sentence vectors containing the ambiguous and landmark verb, respecti</context>
</contexts>
<marker>Dinu, Pham, Baroni, 2013</marker>
<rawString>Georgiana Dinu, Nghia The Pham, and Marco Baroni. 2013. DISSECT - DIStributional SEmantics Composition Toolkit. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 31–36, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Efron</author>
<author>Robert J Tibshirani</author>
</authors>
<title>An Introduction to the Bootstrap. Chapman and Hall,</title>
<date>1993</date>
<location>New York.</location>
<contexts>
<context position="12034" citStr="Efron and Tibshirani, 1993" startWordPosition="1951" endWordPosition="1954">non-applicability of parameter. The results for the training phase modification are overwhelmingly negative. There is a minor degradation when the adjective is subtracted at training time, and major degradation when the verb is subtracted. We will come back to this result below. In contrast, we obtain improvements when we modify the test phase, when we either leave out the verb or both the verb and the adjective in the composition. For two out of the three datasets, the respective best models perform statistically significantly better than the PLF as determined by a bootstrap resampling test (Efron and Tibshirani, 1993): ANVAN1 (+1.5%, p&lt;0.05) and NVN (+5.2%, p&lt;0.01). The improvement for ANVAN2 (+0.5%) is not large enough to reach significance. Discussion. These results leave us with two main questions: (a), why does the modification at training time fail so completely; and (b), can we develop a better understanding of the kind of improvement that the modification at test time introduces? Regarding question (a), we believe that the difference between the phrase vector and the predicate vector that we are training the matrix to predict in Eq. (5) is, in practice, a very brittle representation. The reason is t</context>
</contexts>
<marker>Efron, Tibshirani, 1993</marker>
<rawString>Bradley Efron and Robert J. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman and Hall, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimental support for a categorical compositional distributional model of meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1394--1404</pages>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="8252" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="1317" endWordPosition="1320">use unemployed person high armed police charge unemployed person armed police bill unemployed person low Table 1: Example of experimental items in the ANVAN data sets (target verb: charge). For transitive sentences (cf. Figure 1), we predict �S n + �O P(n v n) =v x�� v xn (the sum of the subject and the object contributions), and analogously for other constructions. 3 Experimental Setup Evaluation Datasets. We evaluate the modifications from the last section on three standard benchmarks for CDSMs: ANVAN-1 (Kartsaklis et al., 2013), ANVAN-2 (Grefenstette, 2013) (Paperno et al.’s term) and NVN (Grefenstette and Sadrzadeh, 2011) (our term). As the abbreviations indicate, the two ANVAN datasets contain transitive verbs whose NP arguments are modified by arguments; the NVN dataset contains only bare noun arguments. All three datasets are built around ambiguous target verbs that are combined with two disambiguating contexts (subjects plus objects) and two landmark verbs in a balanced design (cf. Table 1). Each context matches one of the landmark verbs, but not the other. Annotators were asked to rate the similarity between the target verb in context and the landmark on a Likert scale. Corpus and Co-Occurrences. We follo</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Experimental support for a categorical compositional distributional model of meaning. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1394–1404, Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
</authors>
<title>Category-Theoretic Quantitative Compositional Distributional Models of Natural Language Semantics.</title>
<date>2013</date>
<tech>Ph.D. thesis.</tech>
<contexts>
<context position="8185" citStr="Grefenstette, 2013" startWordPosition="1309" endWordPosition="1310">rmed police charge unemployed person armed police accuse unemployed person high armed police charge unemployed person armed police bill unemployed person low Table 1: Example of experimental items in the ANVAN data sets (target verb: charge). For transitive sentences (cf. Figure 1), we predict �S n + �O P(n v n) =v x�� v xn (the sum of the subject and the object contributions), and analogously for other constructions. 3 Experimental Setup Evaluation Datasets. We evaluate the modifications from the last section on three standard benchmarks for CDSMs: ANVAN-1 (Kartsaklis et al., 2013), ANVAN-2 (Grefenstette, 2013) (Paperno et al.’s term) and NVN (Grefenstette and Sadrzadeh, 2011) (our term). As the abbreviations indicate, the two ANVAN datasets contain transitive verbs whose NP arguments are modified by arguments; the NVN dataset contains only bare noun arguments. All three datasets are built around ambiguous target verbs that are combined with two disambiguating contexts (subjects plus objects) and two landmark verbs in a balanced design (cf. Table 1). Each context matches one of the landmark verbs, but not the other. Annotators were asked to rate the similarity between the target verb in context and </context>
</contexts>
<marker>Grefenstette, 2013</marker>
<rawString>Edward Grefenstette. 2013. Category-Theoretic Quantitative Compositional Distributional Models of Natural Language Semantics. Ph.D. thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiliano Guevara</author>
</authors>
<title>A regression model of adjectivenoun compositionality in distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics,</booktitle>
<pages>33--37</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="1654" citStr="Guevara, 2010" startWordPosition="230" endWordPosition="231">e meaning of a phrase by the meanings of its parts. They have also found application in psycholinguistics (Lenci, 2011), in sentiment analysis (Socher et al., 2012), and in machine translation (Kalchbrenner and Blunsom, 2013). A first generation of CDSMs represented all words as vectors and combined them by component-wise operations (Mitchell and Lapata, 2010). Given the conceptual limitations of this simple approach, numerous models were subsequently proposed which represent the meaning of predicates as higher-order algebraic objects such as matrices and tensors (Baroni and Zamparelli, 2010; Guevara, 2010; Coecke et al., 2010). For example, one-place predicates such as adjectives or intransitive verbs can be modeled as matrices (order-2 tensors), and two-place predicates, e.g., transitive verbs, as order-3 tensors, and so forth. While such tensors enable mathematically elegant accounts of composition, their large degrees of freedom lead to severe sparsity issues when they are learned from corpora. The recently proposed Practical Lexical Function model (PLF; Paperno et al., 2014) represents a compromise between these two extremes by restricting itself to vectors and matrices, effectively reduci</context>
<context position="4613" citStr="Guevara, 2010" startWordPosition="707" endWordPosition="708"> corpus-observed vector of the phrase. We will illustrate it on a minimal example, the phrase “dogs sleep”. Training Phase. The training of PLF creates three representations: (1), a lexical vector for the noun (−→n ); (2), the lexical vector for the verb (−→v ); and (3), a matrix for the subject argument position of the verb (v ). While (1) and (2) can be acquired directly from the corpus, (3) involves optimization, since the matrix (3) is supposed to account for the verb’s disambiguating effect on all its subjects. PLF proposes to learn matrices via regression problems such as the following (Guevara, 2010), where subj(v) comprises the subjects seen with the verb v:1 ❑S v := argmin M n∈subj(v) � IIM x →−n − n−→vII2 (1) That is, the verb’s subject matrix is learned as the matrix which, multiplied with a subject noun vector, best predicts the noun-verb phrase vector. If we assume that the verb of our example (sleep) is only seen with a single noun in the corpus, namely its subject dog, Eq. (1) has a particularly simple solution where the matrix can perfectly predict the phrase vector: ❑S sleep x dog = dog sleep (2) 1All matrices are learned using least-squares regression and, for the sake of simpl</context>
</contexts>
<marker>Guevara, 2010</marker>
<rawString>Emiliano Guevara. 2010. A regression model of adjectivenoun compositionality in distributional semantics. In Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, pages 33–37, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hedeker</author>
</authors>
<title>Generalized linear mixed models.</title>
<date>2005</date>
<booktitle>In Encyclopedia of Statistics in Behavioral Science.</booktitle>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="12991" citStr="Hedeker, 2005" startWordPosition="2117" endWordPosition="2118">fication at test time introduces? Regarding question (a), we believe that the difference between the phrase vector and the predicate vector that we are training the matrix to predict in Eq. (5) is, in practice, a very brittle representation. The reason is that typically the phrase nv is much less frequent than v, and therefore n−→v − →−v ≈ −−→v (cf. Figure 3). Consequently, the matrix attempts to predict the verb vector from the noun – not only a very hard problem, but one that does not help solve the task at hand. To answer question (b), we perform a mixed effects linear regression analysis (Hedeker, 2005) on the three datasets, concentrating on a comparison of the standard PLF and the best respective test phase modification. We follow the intuition that the frequency and ambiguity of the target verbs should influence the quality of the prediction both in the PLF ANVAN1 ANVAN2 NVN logf -359*** -182n.s. -96*** ambig 118*** 8 n.s. 6*** ModTest 438*** -2606*** -1413*** ModTest:logf -53** 165*** 94*** ModTest:ambig 20* 32*** 8*** Table 3: Coefficients of Linear Mixed Effects Model. *: p&lt;0.05; **: p&lt;0.01; ***: p&lt;0.001. See text for details. and in the modified model, and that it might be informative</context>
</contexts>
<marker>Hedeker, 2005</marker>
<rawString>Donald Hedeker. 2005. Generalized linear mixed models. In Encyclopedia of Statistics in Behavioral Science. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent continuous translation models.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1700--1709</pages>
<location>Melbourne, Australia.</location>
<contexts>
<context position="1266" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="172" endWordPosition="175">g which leads to an overcounting of the predicate’s contribution to the meaning of the phrase. We investigate two possible solutions of which one (the exclusion of simple lexical vector at test time) improves performance significantly on two out of the three composition datasets. 1 Introduction Compositional distributional semantic models (CDSMs) make an important theoretical contribution, explaining the meaning of a phrase by the meanings of its parts. They have also found application in psycholinguistics (Lenci, 2011), in sentiment analysis (Socher et al., 2012), and in machine translation (Kalchbrenner and Blunsom, 2013). A first generation of CDSMs represented all words as vectors and combined them by component-wise operations (Mitchell and Lapata, 2010). Given the conceptual limitations of this simple approach, numerous models were subsequently proposed which represent the meaning of predicates as higher-order algebraic objects such as matrices and tensors (Baroni and Zamparelli, 2010; Guevara, 2010; Coecke et al., 2010). For example, one-place predicates such as adjectives or intransitive verbs can be modeled as matrices (order-2 tensors), and two-place predicates, e.g., transitive verbs, as order-3 tensor</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1700–1709, Melbourne, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitri Kartsaklis</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Pulman</author>
</authors>
<title>Separating disambiguation from composition in distributional semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning,</booktitle>
<pages>114--123</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="8155" citStr="Kartsaklis et al., 2013" startWordPosition="1304" endWordPosition="1307">te landlord bill annual rent high armed police charge unemployed person armed police accuse unemployed person high armed police charge unemployed person armed police bill unemployed person low Table 1: Example of experimental items in the ANVAN data sets (target verb: charge). For transitive sentences (cf. Figure 1), we predict �S n + �O P(n v n) =v x�� v xn (the sum of the subject and the object contributions), and analogously for other constructions. 3 Experimental Setup Evaluation Datasets. We evaluate the modifications from the last section on three standard benchmarks for CDSMs: ANVAN-1 (Kartsaklis et al., 2013), ANVAN-2 (Grefenstette, 2013) (Paperno et al.’s term) and NVN (Grefenstette and Sadrzadeh, 2011) (our term). As the abbreviations indicate, the two ANVAN datasets contain transitive verbs whose NP arguments are modified by arguments; the NVN dataset contains only bare noun arguments. All three datasets are built around ambiguous target verbs that are combined with two disambiguating contexts (subjects plus objects) and two landmark verbs in a balanced design (cf. Table 1). Each context matches one of the landmark verbs, but not the other. Annotators were asked to rate the similarity between t</context>
</contexts>
<marker>Kartsaklis, Sadrzadeh, Pulman, 2013</marker>
<rawString>Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen Pulman. 2013. Separating disambiguation from composition in distributional semantics. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 114–123, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Lenci</author>
</authors>
<title>Composing and updating verb argument expectations: A distributional semantic model.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics,</booktitle>
<pages>58--66</pages>
<location>Portland, OR.</location>
<contexts>
<context position="1160" citStr="Lenci, 2011" startWordPosition="159" endWordPosition="160">sistency in PLF between the objective function at training and the prediction at testing which leads to an overcounting of the predicate’s contribution to the meaning of the phrase. We investigate two possible solutions of which one (the exclusion of simple lexical vector at test time) improves performance significantly on two out of the three composition datasets. 1 Introduction Compositional distributional semantic models (CDSMs) make an important theoretical contribution, explaining the meaning of a phrase by the meanings of its parts. They have also found application in psycholinguistics (Lenci, 2011), in sentiment analysis (Socher et al., 2012), and in machine translation (Kalchbrenner and Blunsom, 2013). A first generation of CDSMs represented all words as vectors and combined them by component-wise operations (Mitchell and Lapata, 2010). Given the conceptual limitations of this simple approach, numerous models were subsequently proposed which represent the meaning of predicates as higher-order algebraic objects such as matrices and tensors (Baroni and Zamparelli, 2010; Guevara, 2010; Coecke et al., 2010). For example, one-place predicates such as adjectives or intransitive verbs can be </context>
</contexts>
<marker>Lenci, 2011</marker>
<rawString>Alessandro Lenci. 2011. Composing and updating verb argument expectations: A distributional semantic model. In Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 58–66, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="1403" citStr="Mitchell and Lapata, 2010" startWordPosition="192" endWordPosition="195">one (the exclusion of simple lexical vector at test time) improves performance significantly on two out of the three composition datasets. 1 Introduction Compositional distributional semantic models (CDSMs) make an important theoretical contribution, explaining the meaning of a phrase by the meanings of its parts. They have also found application in psycholinguistics (Lenci, 2011), in sentiment analysis (Socher et al., 2012), and in machine translation (Kalchbrenner and Blunsom, 2013). A first generation of CDSMs represented all words as vectors and combined them by component-wise operations (Mitchell and Lapata, 2010). Given the conceptual limitations of this simple approach, numerous models were subsequently proposed which represent the meaning of predicates as higher-order algebraic objects such as matrices and tensors (Baroni and Zamparelli, 2010; Guevara, 2010; Coecke et al., 2010). For example, one-place predicates such as adjectives or intransitive verbs can be modeled as matrices (order-2 tensors), and two-place predicates, e.g., transitive verbs, as order-3 tensors, and so forth. While such tensors enable mathematically elegant accounts of composition, their large degrees of freedom lead to severe </context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denis Paperno</author>
<author>Nghia The Pham</author>
<author>Marco Baroni</author>
</authors>
<title>A practical and linguistically-motivated approach to compositional distributional semantics.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>90--99</pages>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="2137" citStr="Paperno et al., 2014" startWordPosition="299" endWordPosition="302">esent the meaning of predicates as higher-order algebraic objects such as matrices and tensors (Baroni and Zamparelli, 2010; Guevara, 2010; Coecke et al., 2010). For example, one-place predicates such as adjectives or intransitive verbs can be modeled as matrices (order-2 tensors), and two-place predicates, e.g., transitive verbs, as order-3 tensors, and so forth. While such tensors enable mathematically elegant accounts of composition, their large degrees of freedom lead to severe sparsity issues when they are learned from corpora. The recently proposed Practical Lexical Function model (PLF; Paperno et al., 2014) represents a compromise between these two extremes by restricting itself to vectors and matrices, effectively reducing sparsity while retaining state-of-the-art performance across multiple datasets. It does away with tensors by ignoring interactions among the arguments of predicates p. Instead, each argument position arg is modeled as a matrix p that is applied to a vector for the argument’s meaning, -a-+. The meaning of the phrase is then defined as the sum of the lexical meaning of the predicate, p , and the contributions of each argument (see Fig. 1). The matrices can be learned in a super</context>
<context position="8899" citStr="Paperno et al. (2014)" startWordPosition="1421" endWordPosition="1424">iations indicate, the two ANVAN datasets contain transitive verbs whose NP arguments are modified by arguments; the NVN dataset contains only bare noun arguments. All three datasets are built around ambiguous target verbs that are combined with two disambiguating contexts (subjects plus objects) and two landmark verbs in a balanced design (cf. Table 1). Each context matches one of the landmark verbs, but not the other. Annotators were asked to rate the similarity between the target verb in context and the landmark on a Likert scale. Corpus and Co-Occurrences. We followed the specifications by Paperno et al. (2014) as closely as possible to replicate the original PLF results. As corpora, we used ukWAC, English Wikipedia, and the BNC. We extracted a square co-occurrence matrix for the 30K most frequent content words using a 3-word window and applied the PPMI transformation. Subsequently, the matrix was reduced to 300 dimensions with SVD. In the same manner, we built a co-occurrence matrix for all corpus bigrams for relevant adjectives and verbs from the experimental materials, applying a frequency threshold of 5. Composition Models and Evaluation. We build matrix representations for adjectives and subjec</context>
<context position="10438" citStr="Paperno et al. (2014)" startWordPosition="1673" endWordPosition="1676">dification can apply only to the verb (cf. Figure 1), which gives us two conditions. On the ANVAN datasets (cf. Figure 2), the changes can be applied to the verb, to the adjectives, or to both, for a total of six conditions. Our evaluation measure is the nonparametric Spearman correlations between each annotator’s similarity rating and the cosine between the predicted sentence vectors containing the ambiguous and landmark verb, respectively. 4 Evaluation Main Results. The main results are shown in Table 2. Our PLF re-implementation in the first column almost replicates the results reported by Paperno et al. (2014) for ANVAN1 and ANVAN2 (20 and 36, respectively). On NVN, no results for the PLF were previously reported. Our result (35.4) is substantially above the result of 21.0 reported by Greffenstette and Sadrzadeh (2011) for their categorial model. This supports our general focus on the PLF as an interesting target for analysis. −−−→ ElO °S −→ charge × p obj charge + charge × np subj+ ElS −→ { −−−→ charge + charge × np subj, DO charge} →− np obj:= −−−→ ON −−→ annual + annual ×rent →− np subj:= −−−→ ON private + private × −−−→ ElS charge, charge, 0O charge} { landlord 155 Training phase modifications </context>
</contexts>
<marker>Paperno, Pham, Baroni, 2014</marker>
<rawString>Denis Paperno, Nghia The Pham, and Marco Baroni. 2014. A practical and linguistically-motivated approach to compositional distributional semantics. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 90–99, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12,</booktitle>
<pages>1201--1211</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1205" citStr="Socher et al., 2012" startWordPosition="164" endWordPosition="167"> function at training and the prediction at testing which leads to an overcounting of the predicate’s contribution to the meaning of the phrase. We investigate two possible solutions of which one (the exclusion of simple lexical vector at test time) improves performance significantly on two out of the three composition datasets. 1 Introduction Compositional distributional semantic models (CDSMs) make an important theoretical contribution, explaining the meaning of a phrase by the meanings of its parts. They have also found application in psycholinguistics (Lenci, 2011), in sentiment analysis (Socher et al., 2012), and in machine translation (Kalchbrenner and Blunsom, 2013). A first generation of CDSMs represented all words as vectors and combined them by component-wise operations (Mitchell and Lapata, 2010). Given the conceptual limitations of this simple approach, numerous models were subsequently proposed which represent the meaning of predicates as higher-order algebraic objects such as matrices and tensors (Baroni and Zamparelli, 2010; Guevara, 2010; Coecke et al., 2010). For example, one-place predicates such as adjectives or intransitive verbs can be modeled as matrices (order-2 tensors), and tw</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 1201–1211, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>