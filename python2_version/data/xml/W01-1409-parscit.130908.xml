<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.997367">
Building a Statistical Machine Translation System from Scratch:
How Much Bang for the Buck Can We Expect?
</title>
<author confidence="0.987121">
Ulrich Germann
</author>
<affiliation confidence="0.835913">
USC Information Sciences Institute
4676 Admiralty Way, Suite 1001
</affiliation>
<address confidence="0.609633">
Marina del Rey, CA 90292
</address>
<email confidence="0.998995">
germann@isi.edu
</email>
<sectionHeader confidence="0.993902" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997406">
We report on our experience with building a
statistical MT system from scratch, includ-
ing the creation of a small parallel Tamil-
English corpus, and the results of a task-
based pilot evaluation of statistical MT sys-
tems trained on sets of ca. 1300 and ca.
5000 parallel sentences of Tamil and English
data. Our results show that even with appar-
ently incomprehensible system output, hu-
mans without any knowledge of Tamil can
achieve performance rates as high as 86%
accuracy for topic identification, 93% recall
for document retrieval, and 64% recall on
question answering (plus an additional 14%
partially correct answers).
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995183625">
Crises and disasters frequently attract international at-
tention to regions of the world that have previously
been largely ignored by the international community.
While it is possible to stock up on emergency relief
supplies and, for the worst case, weapons, regardless
of where exactly they are eventually going to be used,
this cannot be done with multilingual information pro-
cessing technology. This technology will often have to
be developed after the fact in a quick response to the
given situation. Multilingual data resources for sta-
tistical approaches, such as parallel corpora, may not
always be available.
In the fall of 2000, we decided to put the current
state of the art to the test with respect to the rapid con-
struction of a machine translation system from scratch.
Within one month, we would
</bodyText>
<listItem confidence="0.99648175">
• hire translators;
• translate as much text as possible; and
• train a statistical MT system on the data thus cre-
ated.
</listItem>
<bodyText confidence="0.9999105">
The language of choice was Tamil, which is spoken
in Sri Lanka and in the southern part of India. Tamil is
a head-last language with a very rich morphology and
therefore quite different from English.
</bodyText>
<sectionHeader confidence="0.892172" genericHeader="method">
2 Data Collection and Preparation
</sectionHeader>
<subsectionHeader confidence="0.976056">
2.1 Obtaining Tamil Data
</subsectionHeader>
<bodyText confidence="0.999957928571429">
Tamil data is not very difficult to find on the web.
There are several Tamil newspapers and magazines
with online editions, and the large international Tamil
community fosters the use of the Internet for the dis-
semination of information. After initial investigation
of several web sites we decided to download our exper-
imental corpus from www.tamilnet.com, a news
site that provides local news on Sri Lanka in both
Tamil and English. The Tamil and English news texts
on this site do not seem to be translations of each other.
The availability of a fairly large in-domain corpus of
local news on Sri Lanka in English (over 2 million
words) allowed us to train an in-domain English lan-
guage model of Sri Lankan news.
</bodyText>
<subsectionHeader confidence="0.994306">
2.2 Encoding and Tokenization
</subsectionHeader>
<bodyText confidence="0.999998">
Tamil is written in a phonematic, non-Latin script.
Several encoding schemes exist in parallel. Even
though the Unicode standard includes a set of glyphs
for Tamil, it is not widely used in practice. Most web
sites that offer Tamil language material assume Latin-1
encoding and rely on special true type fonts, which of-
ten are also offered for free download at those sites.
Tamil text is therefore fairly easy to identify on web
sites via the face attribute of the HTML font tag. All
that is necessary is a list of Tamil font names used by
the different sites, and knowledge about which encod-
ings these fonts implement. While we could restrict
ourselves to one data source and encoding for our ex-
periment, any large-scale system would have to take
this into account. In order to make the source text
recognizable to humans who have no knowledge of
Tamil, we decided to work with transliterated text1.
</bodyText>
<subsectionHeader confidence="0.992117">
2.3 Translating the Corpus
</subsectionHeader>
<bodyText confidence="0.999330327586207">
Originally we hoped to be able to create a parallel cor-
pus of about 100,000 words on the Tamil side within
one month, using several translators. Professional
1Translations, however, were produced from the original
Tamil.
translation services in the US currently charge rates of
about 30 cents per English word for translations from
Tamil into English. Given that the English transla-
tion of a Tamil text usually contains about 1.2 times
as many words as the Tamil original, the translation of
a corpus of 100,000 Tamil words would cost approxi-
mately USD 36,000. This was far beyond our budget.
In India, by comparison, raw translations may cost
as little as one cent per Tamil word2. However, out-
sourcing the translation work abroad was not feasible
for us, since we had neither the administrative infras-
tructure nor the time to manage such an effort. Also,
working with partners so remote would have made it
very difficult to communicate our exact needs and to
implement proper quality control.
We finally decided to hire as translators four enter-
ing and second-year graduate students in the depart-
ment of engineering whose native language is Tamil
and who had responded to an ad posted in the local
mailing list for students from India.
In order to manage the corpus translation process,
we set up a web interface through which the transla-
tors could retrieve source texts and upload their trans-
lations, post-editors could post-edit text online, the
project progress could be monitored, and all incoming
text was available to other project members as soon as
it was submitted.
We originally assumed that translators would be
able to translate about 500 words per hour if we were
content with raw translations and hardly any format-
ting, and if we allowed them to skip difficult words
or sentences. This estimate was based on an inter-
nal evaluation, in which multilingual members of our
group translated sample documents from their native
language (Arabic, German, Romanian) into English
and kept track of the time they spent on this.
It turned out that our expectations were very much
exaggerated with both respect to translation speed and
the quality of translation. The actual translation speed
for Tamil varied between 156 and 247 words per hour
with an average of 170 words per hour. In 139 hours of
reported translation time (over a period of eventually 6
weeks), about 24,000 words / 1,300 sentences of Tamil
text were translated, at an effective cost of ca. 10.8
cents per Tamil word (translators’ compensation plus
administrative overhead). This figure does not include
the effort for manually post-editing the translations by
a native speaker of English (12-16 person hours).
The overall organization of the project (source data
retrieval, hiring and management of the translators,
design and implementation of the web interface for
managing the project via the Internet, development of
transliterator and stemmer, etc.) required an additional
</bodyText>
<footnote confidence="0.634728">
2Personal communications with Thomas Malten, University
of Cologne.
</footnote>
<bodyText confidence="0.992300666666667">
estimated 2.5 person months. However, a good part of
this effort led to resources that can also be used for
other purposes.
</bodyText>
<subsectionHeader confidence="0.999206">
2.4 Lessons Learned for Future Projects
</subsectionHeader>
<bodyText confidence="0.999961">
If we were to give advice for future, similar projects,
we would emphasize and recommend the following:
</bodyText>
<subsectionHeader confidence="0.812552">
2.4.1 Good translators are not easy to find
</subsectionHeader>
<bodyText confidence="0.999869">
It is difficult to find good translators for a short-
term commitment. Unless one is willing to pay a pre-
mium price, it is unlikely that one will find profes-
sional translators who are willing to commit much of
their time for a limited period of time and on short no-
tice.
</bodyText>
<subsectionHeader confidence="0.86222">
2.4.2 Make the translation job attractive
</subsectionHeader>
<bodyText confidence="0.999505333333333">
As foreign students, our translators would each have
been allowed to work up to twenty hours per week.
None of them did, because the work was frustrating
and boring, and because they found more attractive,
long term employment on campus. Our translators’
frustration may have been fostered by several factors:
</bodyText>
<listItem confidence="0.969912333333333">
• the differences between Sri Lankan Tamil (the va-
riety used in our corpus) and the Tamil spoken in
Southern India (the native language of our trans-
lators), which made translating, according to our
translators, very difficult;
• the lack of translation experience of our transla-
tors; and
• our high expectations. We originally told our
translators that since they were not working on
</listItem>
<bodyText confidence="0.9970625">
site, we would expect the translation of 500
words per hour reported. When we later switched
to hourly pay regardless of translation volume,
the translation volume picked up slightly.
</bodyText>
<subsectionHeader confidence="0.839401">
2.4.3 Be prepared to post-edit
</subsectionHeader>
<bodyText confidence="0.9832219375">
In professional translating, translators typically
translate into their native language only. One may not
be able to find translators with English as their na-
tive language for low density or “small” languages,
so it may be necessary to have the translations post-
edited by people with greater language proficiency in
English.
2.4.4 Have translators and post-editors work on
site
It is better to have translators and post-editors work
on site and ideally as teams, so that they can resolve
ambiguities and misunderstandings immediately with-
out the delays of communicating indirectly, be it by
email or other means. A post-editor who does not
know the source language may misinterpret the trans-
lator, as the following case from our corpus illustrates:
</bodyText>
<figure confidence="0.99722565">
text coverage (in %)
text coverage (in %)
100
80
60
40
20
0
0 20 40 60 80 100 120
corpus size (in thousand tokens)
English
100
80
60
40
20
0
0 20 40 60 80 100 120
corpus size (in thousand tokens)
Tamil
</figure>
<figureCaption confidence="0.857291333333333">
Figure 1: Text coverage on previously unseen text for English (left) and Tamil (right). The upper line in each
graph shows the coverage by tokens that have been seen at least once, the lower line shows the coverage by tokens
that have been seen at least 5 times. The error bars indicate standard deviation.
</figureCaption>
<listItem confidence="0.8172815">
Raw translation: Information about the schools in
which people who migrated to Kudaanadu are staying
is being gathered.
Post-edited version: Information about the schools in
(sic!) which immigrants to Kudaanadu are attending
is being gathered.
</listItem>
<bodyText confidence="0.999952666666667">
In this case, the post-editor clearly misinterpreted the
translator. What the translator meant to and actually
did say is that information was being gathered about
the schools in which migrants/war refugees who had
arrived in Kudaanadu had found shelter. However,
the post-editor interpreted the phrase people who mi-
grated to Kudaana as describing immigrants and as-
sumed that information was being gathered about their
education rather than their housing.
</bodyText>
<sectionHeader confidence="0.984727" genericHeader="method">
3 Evaluation Experiments
</sectionHeader>
<subsectionHeader confidence="0.995954">
3.1 A Priori Considerations
</subsectionHeader>
<bodyText confidence="0.999947551020408">
The richer the morphology of a language is, the greater
is the total number of distinct word forms that a given
corpus consists of, and the smaller is the probability
that a certain word form actually occurs in any given
text segment. Figure 1 shows the percentage of word
forms in unseen text that have occurred in previously
seen text as a function of the amount of previously
seen text. The graph on the left shows the curves for
English, the one on the right the curves for Sri Lankan
Tamil. The graphs show the averages of 100 runs on
different text fragments; the error bars indicate stan-
dard deviation.
The numbers were computed in the following man-
ner: A corpus of 120,000 tokens was split into seg-
ments of 1000 tokens each. For each segment nk,
we computed how many of the tokens had been pre-
viously seen in the segments n1 . . . nk-1. The upper
line in the graphs shows the percentage of tokens in nk
that had occurred at least once before in the segments
n1 . . . nk-1, the lower line shows the percentage of to-
kens that had been seen at least five times before.
For the purpose of statistical NLP, it seems reason-
able to assume that the lower curve gives a better indi-
cation of how many percent of previously unseen text
we can expect to be “known” to a statistical model
trained on a corpus of m tokens.
At a corpus size of 24,000 tokens, which is approx-
imately the size of the parallel corpus we were able to
create during our experiment, about 28% of all word
forms in previously unseen Sri Lankan Tamil text can-
not be found in the corpus, and 50% have been seen
less than 5 times. In other words, if we train a system
on this data, we can expect it to stumble over every
other word! At a corpus size of 100,000 tokens, the
numbers are 17% and 33%.
For English, the numbers are 9%/23% for a corpus
of 24K tokens and 0%/8% for a corpus of 100K to-
kens.
In order to boost the text coverage we built a simple
text stemmer for Tamil, based on the Tamil inflection
tables in Steever (1990) and some additional inspec-
tion of our parallel corpus. The stemmer uses regular
expression matching to cut off inflectional endings and
introduce some extra tokens for negation and certain
case markings (such as locative and genitive), which
are all marked morphologically in Tamil. It should be
noted that the stemmer is far from perfect and was only
intended to be an interim solution. The performance
increases are displayed in Figure 2. For a corpus size
</bodyText>
<figure confidence="0.973346">
0 20 40 60 80 100 120
corpus size (in thousand tokens)
</figure>
<figureCaption confidence="0.8262852">
Figure 2: Text coverage increase by stemming for
Tamil. The solid lines indicate text coverage for un-
stemmed data (seen at least once and at least five times,
respectively), the dashed lines the text coverage for
stemmed data.
</figureCaption>
<bodyText confidence="0.9851754">
of 24K tokens, the percentages of unknown items drop
to 19% (from 28%; never seen before) and 36% (from
50%; seen less than 5 times). For a training corpus
of 100K tokens, the numbers are 12% and 23% (from
17%/33%).
</bodyText>
<subsectionHeader confidence="0.997221">
3.2 Task-Based Pilot Evaluation
</subsectionHeader>
<bodyText confidence="0.999953764705882">
Given these numbers, it is obvious that one cannot ex-
pect much performance from a system that relies on
models trained on only 24K tokens of data. As a mat-
ter of fact, it is close to impossible to make any sense
whatsoever of the output of such a system (cf. Fig. 3).
To get an estimate of the performance with more
training data, we augmented our corpus with a paral-
lel corpus of international news texts in Southern In-
dian Tamil which was made available to us by Fred
Gey of the University of California at Berkeley (hence-
forth: Berkeley corpus). This corpus contains ca.
3,800 sentence pairs with 75,800 Tamil tokens after
stemming (before stemming: 60,000; the difference
is due to the introduction of additional markers dur-
ing stemming). Some of the parallel data was with-
held for system evaluation; the augmented training
corpus (Berkeley and TamilNet corpus; short B+TN)
had a size of 85K tokens on the Tamil side. The aug-
mented training corpus had a text coverage of 81%
(seen at least once; 75% without augmentation), and
67% (seen at least 5 times; 60% without augmenta-
tion), respectively, for Sri Lankan Tamil. We trained
IBM Translation Model 4 (Brown et al., 1993) both on
our corpus alone and on the augmented corpus, using
the EGYPT toolkit (Knight et al., 1999; Al-Onaizan et
al., 1999), and then translated a number of texts us-
ing different translation models and different transfer
methods, namely glossing (replacing each Tamil word
by the most likely candidate from the translation tables
created with the EGYPT toolkit) and Model 4 decoding
(Brown et al., 1995; Germann et al., 2001).
Figure 3 shows the output of the different systems
in comparison with the human translation.
We then conducted the following experiments.
</bodyText>
<subsectionHeader confidence="0.572076">
3.2.1 Document Classification Task
</subsectionHeader>
<bodyText confidence="0.99989575">
Seven human subjects without any knowledge of
Tamil were given translations of a set of 15 texts (all
from the Berkeley corpus) and asked to categorize
them according to the following topic hierarchy:
</bodyText>
<listItem confidence="0.999949409090909">
• News about Sri Lanka
• Reports about clashes between the Sri
Lankan army and the Liberation Tigers
• Sri Lankan security-related news (arrests,
arms deals, etc.)
• Sri Lankan political news (strikes, transport,
telecom)
• concerns Sri Lanka but doesn’t fit any of the
above
• News about Pakistan/India
• Nuclear tests in Pakistan and India, includ-
ing their aftermath (international reactions,
etc.)
• Corruption investigation against Benazir
Buto
• News about Pakistan/India but none of the
above
• International news
• Disasters, accidents
• Nelson Mandela’s birthday
• Other international news
• Impossible to tell
</listItem>
<bodyText confidence="0.999802944444444">
Except for one duplicate set, each subject received a
different set of translations. The sets differed in train-
ing parameters and the translation method used. Ta-
ble 1 shows the results of this evaluation. The dif-
ference between the subjects 5a and 5b, who received
the same set of translations, suggests that the individ-
ual classifiers’ accuracy influences the results so much
as to blur the effect of the other parameters. There
seems to be a tendency for glossing to work better than
Model 4 decoding. Glossing, in our system, is a simple
base line algorithm that provides the most likely word
translation for each word of input. Translation can-
didates and their probabilities are retrieved from the
translation table, which is part of the translation model
trained on the parallel corpus.
The document classification test is foremost and
above all a measure of the quality of the translation ta-
ble for frequently occurring words. In practice, actual
</bodyText>
<figure confidence="0.845793545454546">
text coverage (in %) 100
80
60
40
20
0
Small Corpus Augmented Corpus
24K tokens (Tamil) of training data 85K tokens (Tamil) of training data
Human translation Gloss Model 4 decoding&apos; Gloss Model 4 decoding
Government - United vanni united national children united government united the government with
National Party party kalloya national party national party meet united national party
</figure>
<bodyText confidence="0.920817263157894">
meeting will not take tomorrow tomorrow . day meets day .
place tomorrow. naTaipeRamaaTTa naTaipeRamaaTTa
The proposed friday progressive it on friday health throughout throughout the
meeting tomorrow, and united national mothers united progressive government and the
Thursday, between same kaTcikkumiTai national tomorrow on government the united national party
the Peoples Front tomorrow thursday thursday new united national in . day thursday leno
Government and the naTaipeRaviru new political found on kaTcikkumiTai day new political found
United National political vaappu information . thursday leno new meet movies.
Party regarding the information kalloya political vaappu
New Political piRitoru found meet piRitoru
Document was tin2attiRkupa tin2attiRkupa
announced as pin2pooTappaTT- pin2pooTappaTT-
postponed to a later uLLataaka found uLLataaka movies
date. is .
Presidential of lankan to and ’s lanka to advisor of lankan president the sri lankan forces
Secretariat sources freedom tamilian freedom of the chandrika sankari of president
say that this meeting now tamilian team to freedom now chandrika sankari
was postponed as the veLinaaTTukkuc secretariat is called veLinaaTTukkuc freedom presently
Sri Lankan President cen2Riruppat minister. cen2Riruppat secretariat circles
</bodyText>
<figureCaption confidence="0.520086888888889">
Chandrika iccantippu iccantippu reported.
Bandaranayakka has piRpooTappaTT- piRpooTappaTT-
gone to a foreign uLLataaka to uLLataaka president
country. secretariat in * is secretariat circles
reported .
At the same time it is that about returning her about the military he countries his country and the
to be noted that the and iccantippu and they were returning the returning to increase
meeting of the sub naTaipeRumen2a announced . iccantippu the radio .
committee and ivvaTTaaram naTaipeRumen2a the
examining the more and * is ivvaTTaaram more
political document the reported .
between the United
National Party and
the Government was
held yesterday
Tuesday evening
&apos;Brown et al. (1993); Brown et al. (1995)
Figure 3: Sample output of various systems
</figureCaption>
<tableCaption confidence="0.945262">
Table 1: Results of the Document Classification Task. Test subjects were asked to classify the translations of 15
documents into 4 major and 11 minor categories.
</tableCaption>
<table confidence="0.9449245">
input pegginga? transfer partially incorrect
correct correctb
1 raw no M4 decodingc 7 4 4
2 stemmed yes M4 decoding 8 3 4
3 stemmed no M4 decoding 13 2 0
4 raw no gloss 13 1 1
</table>
<footnote confidence="0.585367">
5a stemmed yes gloss 8 3 4
5b stemmed yes gloss 12 2 1
6 stemmed no gloss 11 2 2
</footnote>
<bodyText confidence="0.9649949375">
apegging causes the training algorithm to consider a larger search space
bcorrect top level category but incorrect sub-category
ctranslation by maximizing the IBM Model 4 probability of the source/translation pair (Brown
et al., 1993; Brown et al., 1995)
classification might be performed by automatic pro-
cedures rather than humans. If we dare to accept the
top performances of our human subjects as the ten-
tative upper bound of what can be achieved with the
current system using a translation model trained on
85K tokens of Tamil text and the corresponding En-
glish translations, we can conclude that the classifica-
tion accuracy can exceed 86% (13/15) for fine-grained
classification and reach 100% for coarse-grained clas-
sification. However, given the extremely small sample
size in this evaluation, the evidence should not be con-
sidered conclusive.
</bodyText>
<subsectionHeader confidence="0.937869">
3.2.2 Document Retrieval Task
</subsectionHeader>
<bodyText confidence="0.99996725">
The document retrieval task and the question an-
swering task (see below) were combined into one task.
The subjects received 14 texts (from the TamilNet cor-
pus) and 15 lead questions plus 13 additional follow-
up questions. Their task was to identify the docu-
ment(s) that contain(s) the answer to the question and
to answer the questions asked. Typical lead questions
were questions such as What is the security situation
in Trinconmalee?, or Who is S. Thivakarasa?; typi-
cal follow-up questions were Who is in control of the
situation?, What happened to him/her?, or How did
(other) people react to what happened?. As in the pre-
vious experiment, each subject received the output of
a different system.
Table 2 shows the result of the document retrieval
task. Again, the sample size was too small to draw
any final conclusions, but our results seem to suggest
the following. Firstly, the test subject in the group
dealing with output of systems trained on the bigger
training set tend to perform better than the ones deal-
ing with the results of training on less data. This sug-
gests that the jump from 24K to 85K tokens of train-
ing data might improve system performance in a sig-
nificant manner. We were surprised that even with
the poor translation performance of our system, re-
calls as high as 93% at a precision of 88% could be
achieved. Secondly, the data shows that gaps are not
randomly distributed over the data, but that some ques-
tions clearly seem to have been more difficult than oth-
ers. One of the particular difficult aspects of the task
was the spelling of names. Question 11, for exam-
ple, asked What happened to Chandra Kumar Abayas-
ingh?. In the translations, however, it was rendered in
simple transliteration: cantirakumaara apayacingka.
It requires a considerable degree of tenacity and imag-
ination to find this connection.
</bodyText>
<subsectionHeader confidence="0.84888">
3.2.3 Question Answering Task
</subsectionHeader>
<bodyText confidence="0.96288256">
In order to measure the performance in the ques-
tion answering part of this evaluation, we considered
only questions relevant to the documents that the test
subjects had identified correctly. Because of the diffi-
culty of the task, we were lenient to some degree in the
evaluation. For example, if the correct answer was the
former president of the teacher’s union and the answer
given was an official of the teacher’s union, we still
counted this as “close enough” and therefore correct.
In addition, we also allowed partially correct answers,
that is, answers that went into the right direction but
were not quite correct. For example, if the correct an-
swer was The army imposed a curfew on fishing, we
counted the answer the army is stopping fishing boats
as partially correct. All in all, it was very difficult to
evaluate this section of the task, because it was often
close to impossible to determine whether the answer
was just an educated guess or actually based on the
text. There were some cases where answers were par-
tially or even fully correct even though the correct doc-
ument had not been identified. In retrospect we con-
clude that it would have been better to have the test
Table 2: Recall and precision on the document retrieval task. Test subjects were asked to identify the document(s)
containing the answers to 15 lead questions. Black dots indicate successful identification of at least one document
containing the answer.
</bodyText>
<table confidence="0.997766071428571">
trainingcorpus transl. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 recall precision
method
1 TNa glossing • • • • • • • • • • 67% 79%
2 TN M4dec.b • • • • • • • • 53% 80%
3 TN bothc • • • • • • • • • 60% 48%
4 TNd both • • • • • • • • • • 67% 79%
5 B+TN.1 glossing • • • • • • • • • • • • 80% 87%
6 B+TN.1 M4dec. • • • • • • • • • • 67% 86%
7 B+TN.1 both • • • • • • • • • • • • 80% 86%
8 B+TN.1f both • • • • • • • • 60% 73%
9 B+TN.1g both • • • • • • • • • • • • • 93% 88%
•
10 B+TN.2h both • • • • • • • • • • • • • 87% 75%
Human Translations • • • • • • • • • • • • • • • 100% 100%
</table>
<tableCaption confidence="0.7413685">
aTamilNet corpus only; stemmed; 1291 aligned text chunks; 23,359 tokens on Tamil side; 1000 training iterations.
bIBM Model 4 decoding.
</tableCaption>
<bodyText confidence="0.991158617647059">
cBoth glossing and IBM Model 4 decoding were available to the test subject.
dsame as above, but trained with pegging option (more thorough search during training); 10 training iterations.
eBerkeley and TamilNet corpora; 5069 aligned text chunks; 85421 tokens on Tamil side; 100 training iterations.
fsame as above; 10 training iterations.
gsame as above, trained with pegging option; 10 training iterations.
hBerkeley and TamilNet Corpora, raw (unstemmed); 64439 tokens on Tamil side, 50 training iterations.
subjects mark up those text passages in the text that
justify their answers.
Again, the data suggests that the difference in train-
ing corpus size does affect the amount of information
that is available from the system output. Subjects us-
ing output of a system based on a translation model
that was trained on only the TamilNet data tend to per-
form worse than subjects using output from a system
based on a translation model trained on the larger cor-
pus. The poor performance on test set No. 6 may
suggest that for this task and at this level of transla-
tion quality, glossing provides more informative out-
put than Model 4 decoding. This result is not partic-
ularly surprising, since we noticed that Model 4 de-
coding tends to leave out more words than acceptable.
Clearly, this is one area where the translation model
has to be improved.
Test set 10 is the only set produced by a system
using a translation model trained on raw, unstemmed
data. It is unclear whether the poor performance on
question answering for this test set is due to a princi-
pally worse translation quality or the (lack of) tenacity
and willingness of the test subject to work her way
through the system output.
All in all, we were astonished by the amount of in-
formation that our test subjects were able to retrieve
from the material they received (the top recall for the
question answering task is 64%, plus an additional
</bodyText>
<tableCaption confidence="0.9581785">
Table 3: Accuracy on question answering. The test
sets are the same as in Table 2. Only questions con-
cerning documents that were identified correctly were
considered in this evaluation.
</tableCaption>
<table confidence="0.996505">
test training No. of rele- correct partially
set corpus vant questions correct
1 TN 17 2 = 12% 0 = 0%
2 TN 16 3 = 19% 4 = 25%
3 TN 18 2 = 11% 0 = 0%
4 TN 17 7 = 41% 5 = 19%
4 B+TN.1 23 14 = 61% 6 = 26%
6 B+TN.1 19 5 = 26% 3 = 16%
7 B+TN.1 22 14 = 64% 3 = 14%
8 B+TN.1 19 12 = 63% 1 = 5%
9 B+TN.1 26 14 = 54% 5 = 19%
10 B+TN.2 24 8 = 33% 9 = 38%
human 28 24 = 86% 2 = 7%
</table>
<bodyText confidence="0.998094">
14% partially correct answers!). However, using a
system such as the one discussed in the paper is not
an option for actual information processing. Espe-
cially those subjects that had to deal with the output
of systems trained on the smaller corpus experienced
the task as utterly frustrating and would not want to do
it again.
</bodyText>
<sectionHeader confidence="0.999401" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.945484921875">
We have reported on our experience with rapidly
building a statistical MT system from scratch. Within
ca. 140 translator hours, we were able to create a par-
allel corpus of about 1300 sentence pairs with 24,000
tokens on the Tamil side, at an average translation rate
of approximately 170 Tamil words per hour.
Very clearly, the effort needed to create parallel data
is one of the biggest obstacles to the rapid development
of statistical MT systems for new languages.
With the output of a system which uses a translation
model trained on the small amount of parallel data that
we created during the course of our experiment, hu-
man test subjects achieved a recall of over 50% on
the document retrieval task but generally performed
poorly on question answering (less than 20%).
The addition of an additional corpus of 3,800 sen-
tence pairs allowed us to estimate the benefits of in-
creasing the overall corpus size by roughly 300%.
Based on our experience with translating the TamilNet
corpus, this additional effort would require an addi-
tional 450 translator and 36 to 48 post-editor hours.
With the additional training data, we were able to
produce output that increased the performance on our
evaluation tasks (document retrieval and question an-
swering) to up to 93% for document retrieval and 64%
for question answering.
With respect to the scenario of “MT in a month”,
we can now make the following calculation: If we as-
sume that the average translator translates at a rate of
170 words/hour and is able to spend 6-7 hours per day
on actual translations, then a translator can translate
about 1000-1200 words per day. In order to translate a
corpus of 100,000 words within one month (assuming
a five-day work week), we therefore need four to five
full time translators. For this effort, we can expect
a translation system whose performance resembles the
one shown in our evaluation.
This, of course, raises the following questions,
which we are only able to ask but not to answer at this
point.
• Can the translation model and the algorithms for
statistical training be improved so that they re-
quire less data to produce acceptable results?
• Are there more efficient uses of scarce resources
(such as language experts and translators) for
building a statistical (or any other) MT system
quickly, for example the creation of less but more
informative data, e.g. a parallel corpus with
alignments on the word level, or the compilation
of a glossary/dictionary of the most frequently
used terms?
• How do the various approaches compare with re-
spect to the ratio of construction effort versus
performance improvement when the MT systems
are scaled up? One approach may show rapid
improvements initially but also reach a plateau
quickly, whereas another may show slow but
steady improvements.
• Is there any potential for bootstrapping the re-
source creation process by using knowledge that
can be extracted from little and poor data to speed
up the creation of more and better data?
These are some of the the questions that will need
to be addressed in future research on Quick MT.
</bodyText>
<sectionHeader confidence="0.999432" genericHeader="acknowledgments">
5 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999709625">
This research has been funded by the DARPA TIDES
program under grant No. N66001-00-1-8914. We
would like to thank our translators as well as Fred
Gey of the University of California at Berkeley and
Thomas Malten of the University of Cologne for their
kind support of our investigations into Tamil, and our
test subjects for their tenacity and patience during this
experiment.
</bodyText>
<sectionHeader confidence="0.999105" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999743129032258">
Yaser Al-Onaizan, David Purdy, Jan Curin, Michael
Jahr, Kevin Knight, John Lafferty, Dan Melamed,
Noah A. Smith, Franz Josef Och, and David
Yarowsky. 1999. Statistical machine translation.
Final report, Center for Language and Speech Pro-
cessing, John Hopkins University.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263–311.
Peter Brown, John Cocke, Stephen Della Pietra, Vin-
cent Della Pietra, Frederick Jelinek, Jennifer Lai,
and Robert Mercer. 1995. Method and system for
natural language translation. U.S. Patent 5,477,451,
Dec 19.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding
and optimal decoding for MT. In 39th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL-2001).
Kevin Knight, Yaser Al-Onaizan, David Purdy, Jan
Curin, Michael Jahr, John Lafferty, Dan Melamed,
Noah Smith, Franz Josef Och, and David Yarowsky.
1999. EGYPT: a statistical machine translation
toolkit. http://www.clsp.jhu.edu/ws99/
projects/mt/.
Sanford B. Steever. 1990. Tamil and the Dravidian
languages. In Bernard Comrie, editor, The World’s
Major Languages, pages 725–746. Oxford Univer-
sity Press, New York, Oxford.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.508302">
<title confidence="0.994004">Building a Statistical Machine Translation System from How Much Bang for the Buck Can We Expect?</title>
<author confidence="0.959799">Ulrich</author>
<affiliation confidence="0.997395">USC Information Sciences</affiliation>
<address confidence="0.991799">4676 Admiralty Way, Suite</address>
<author confidence="0.91734">Marina del Rey</author>
<author confidence="0.91734">CA</author>
<email confidence="0.999346">germann@isi.edu</email>
<abstract confidence="0.9619093125">We report on our experience with building a statistical MT system from scratch, including the creation of a small parallel Tamil- English corpus, and the results of a taskbased pilot evaluation of statistical MT systems trained on sets of ca. 1300 and ca. 5000 parallel sentences of Tamil and English data. Our results show that even with apparently incomprehensible system output, humans without any knowledge of Tamil can achieve performance rates as high as 86% accuracy for topic identification, 93% recall for document retrieval, and 64% recall on question answering (plus an additional 14% partially correct answers).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>David Purdy</author>
<author>Jan Curin</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>John Lafferty</author>
<author>Dan Melamed</author>
<author>Noah A Smith</author>
<author>Franz Josef Och</author>
<author>David Yarowsky</author>
</authors>
<title>Statistical machine translation. Final report,</title>
<date>1999</date>
<institution>Center for Language and Speech Processing, John Hopkins University.</institution>
<contexts>
<context position="14586" citStr="Al-Onaizan et al., 1999" startWordPosition="2480" endWordPosition="2483"> due to the introduction of additional markers during stemming). Some of the parallel data was withheld for system evaluation; the augmented training corpus (Berkeley and TamilNet corpus; short B+TN) had a size of 85K tokens on the Tamil side. The augmented training corpus had a text coverage of 81% (seen at least once; 75% without augmentation), and 67% (seen at least 5 times; 60% without augmentation), respectively, for Sri Lankan Tamil. We trained IBM Translation Model 4 (Brown et al., 1993) both on our corpus alone and on the augmented corpus, using the EGYPT toolkit (Knight et al., 1999; Al-Onaizan et al., 1999), and then translated a number of texts using different translation models and different transfer methods, namely glossing (replacing each Tamil word by the most likely candidate from the translation tables created with the EGYPT toolkit) and Model 4 decoding (Brown et al., 1995; Germann et al., 2001). Figure 3 shows the output of the different systems in comparison with the human translation. We then conducted the following experiments. 3.2.1 Document Classification Task Seven human subjects without any knowledge of Tamil were given translations of a set of 15 texts (all from the Berkeley cor</context>
</contexts>
<marker>Al-Onaizan, Purdy, Curin, Jahr, Knight, Lafferty, Melamed, Smith, Och, Yarowsky, 1999</marker>
<rawString>Yaser Al-Onaizan, David Purdy, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Noah A. Smith, Franz Josef Och, and David Yarowsky. 1999. Statistical machine translation. Final report, Center for Language and Speech Processing, John Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="14461" citStr="Brown et al., 1993" startWordPosition="2458" endWordPosition="2461">us contains ca. 3,800 sentence pairs with 75,800 Tamil tokens after stemming (before stemming: 60,000; the difference is due to the introduction of additional markers during stemming). Some of the parallel data was withheld for system evaluation; the augmented training corpus (Berkeley and TamilNet corpus; short B+TN) had a size of 85K tokens on the Tamil side. The augmented training corpus had a text coverage of 81% (seen at least once; 75% without augmentation), and 67% (seen at least 5 times; 60% without augmentation), respectively, for Sri Lankan Tamil. We trained IBM Translation Model 4 (Brown et al., 1993) both on our corpus alone and on the augmented corpus, using the EGYPT toolkit (Knight et al., 1999; Al-Onaizan et al., 1999), and then translated a number of texts using different translation models and different transfer methods, namely glossing (replacing each Tamil word by the most likely candidate from the translation tables created with the EGYPT toolkit) and Model 4 decoding (Brown et al., 1995; Germann et al., 2001). Figure 3 shows the output of the different systems in comparison with the human translation. We then conducted the following experiments. 3.2.1 Document Classification Tas</context>
<context position="19199" citStr="Brown et al. (1993)" startWordPosition="3184" endWordPosition="3187">appaTT- piRpooTappaTTgone to a foreign uLLataaka to uLLataaka president country. secretariat in * is secretariat circles reported . At the same time it is that about returning her about the military he countries his country and the to be noted that the and iccantippu and they were returning the returning to increase meeting of the sub naTaipeRumen2a announced . iccantippu the radio . committee and ivvaTTaaram naTaipeRumen2a the examining the more and * is ivvaTTaaram more political document the reported . between the United National Party and the Government was held yesterday Tuesday evening &apos;Brown et al. (1993); Brown et al. (1995) Figure 3: Sample output of various systems Table 1: Results of the Document Classification Task. Test subjects were asked to classify the translations of 15 documents into 4 major and 11 minor categories. input pegginga? transfer partially incorrect correct correctb 1 raw no M4 decodingc 7 4 4 2 stemmed yes M4 decoding 8 3 4 3 stemmed no M4 decoding 13 2 0 4 raw no gloss 13 1 1 5a stemmed yes gloss 8 3 4 5b stemmed yes gloss 12 2 1 6 stemmed no gloss 11 2 2 apegging causes the training algorithm to consider a larger search space bcorrect top level category but incorrect s</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Brown</author>
<author>John Cocke</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>Frederick Jelinek</author>
<author>Jennifer Lai</author>
<author>Robert Mercer</author>
</authors>
<title>Method and system for natural language translation.</title>
<date>1995</date>
<tech>U.S. Patent 5,477,451,</tech>
<contexts>
<context position="14865" citStr="Brown et al., 1995" startWordPosition="2524" endWordPosition="2527">coverage of 81% (seen at least once; 75% without augmentation), and 67% (seen at least 5 times; 60% without augmentation), respectively, for Sri Lankan Tamil. We trained IBM Translation Model 4 (Brown et al., 1993) both on our corpus alone and on the augmented corpus, using the EGYPT toolkit (Knight et al., 1999; Al-Onaizan et al., 1999), and then translated a number of texts using different translation models and different transfer methods, namely glossing (replacing each Tamil word by the most likely candidate from the translation tables created with the EGYPT toolkit) and Model 4 decoding (Brown et al., 1995; Germann et al., 2001). Figure 3 shows the output of the different systems in comparison with the human translation. We then conducted the following experiments. 3.2.1 Document Classification Task Seven human subjects without any knowledge of Tamil were given translations of a set of 15 texts (all from the Berkeley corpus) and asked to categorize them according to the following topic hierarchy: • News about Sri Lanka • Reports about clashes between the Sri Lankan army and the Liberation Tigers • Sri Lankan security-related news (arrests, arms deals, etc.) • Sri Lankan political news (strikes,</context>
<context position="19220" citStr="Brown et al. (1995)" startWordPosition="3188" endWordPosition="3191">gone to a foreign uLLataaka to uLLataaka president country. secretariat in * is secretariat circles reported . At the same time it is that about returning her about the military he countries his country and the to be noted that the and iccantippu and they were returning the returning to increase meeting of the sub naTaipeRumen2a announced . iccantippu the radio . committee and ivvaTTaaram naTaipeRumen2a the examining the more and * is ivvaTTaaram more political document the reported . between the United National Party and the Government was held yesterday Tuesday evening &apos;Brown et al. (1993); Brown et al. (1995) Figure 3: Sample output of various systems Table 1: Results of the Document Classification Task. Test subjects were asked to classify the translations of 15 documents into 4 major and 11 minor categories. input pegginga? transfer partially incorrect correct correctb 1 raw no M4 decodingc 7 4 4 2 stemmed yes M4 decoding 8 3 4 3 stemmed no M4 decoding 13 2 0 4 raw no gloss 13 1 1 5a stemmed yes gloss 8 3 4 5b stemmed yes gloss 12 2 1 6 stemmed no gloss 11 2 2 apegging causes the training algorithm to consider a larger search space bcorrect top level category but incorrect sub-category ctranslat</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lai, Mercer, 1995</marker>
<rawString>Peter Brown, John Cocke, Stephen Della Pietra, Vincent Della Pietra, Frederick Jelinek, Jennifer Lai, and Robert Mercer. 1995. Method and system for natural language translation. U.S. Patent 5,477,451, Dec 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Kenji Yamada</author>
</authors>
<title>Fast decoding and optimal decoding for MT.</title>
<date>2001</date>
<booktitle>In 39th Annual Meeting of the Association for Computational Linguistics (ACL-2001).</booktitle>
<contexts>
<context position="14888" citStr="Germann et al., 2001" startWordPosition="2528" endWordPosition="2531">n at least once; 75% without augmentation), and 67% (seen at least 5 times; 60% without augmentation), respectively, for Sri Lankan Tamil. We trained IBM Translation Model 4 (Brown et al., 1993) both on our corpus alone and on the augmented corpus, using the EGYPT toolkit (Knight et al., 1999; Al-Onaizan et al., 1999), and then translated a number of texts using different translation models and different transfer methods, namely glossing (replacing each Tamil word by the most likely candidate from the translation tables created with the EGYPT toolkit) and Model 4 decoding (Brown et al., 1995; Germann et al., 2001). Figure 3 shows the output of the different systems in comparison with the human translation. We then conducted the following experiments. 3.2.1 Document Classification Task Seven human subjects without any knowledge of Tamil were given translations of a set of 15 texts (all from the Berkeley corpus) and asked to categorize them according to the following topic hierarchy: • News about Sri Lanka • Reports about clashes between the Sri Lankan army and the Liberation Tigers • Sri Lankan security-related news (arrests, arms deals, etc.) • Sri Lankan political news (strikes, transport, telecom) • </context>
</contexts>
<marker>Germann, Jahr, Knight, Marcu, Yamada, 2001</marker>
<rawString>Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu, and Kenji Yamada. 2001. Fast decoding and optimal decoding for MT. In 39th Annual Meeting of the Association for Computational Linguistics (ACL-2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Yaser Al-Onaizan</author>
<author>David Purdy</author>
<author>Jan Curin</author>
<author>Michael Jahr</author>
<author>John Lafferty</author>
<author>Dan Melamed</author>
<author>Noah Smith</author>
<author>Franz Josef Och</author>
<author>David Yarowsky</author>
</authors>
<title>EGYPT: a statistical machine translation toolkit.</title>
<date>1999</date>
<note>http://www.clsp.jhu.edu/ws99/ projects/mt/.</note>
<contexts>
<context position="14560" citStr="Knight et al., 1999" startWordPosition="2476" endWordPosition="2479">00; the difference is due to the introduction of additional markers during stemming). Some of the parallel data was withheld for system evaluation; the augmented training corpus (Berkeley and TamilNet corpus; short B+TN) had a size of 85K tokens on the Tamil side. The augmented training corpus had a text coverage of 81% (seen at least once; 75% without augmentation), and 67% (seen at least 5 times; 60% without augmentation), respectively, for Sri Lankan Tamil. We trained IBM Translation Model 4 (Brown et al., 1993) both on our corpus alone and on the augmented corpus, using the EGYPT toolkit (Knight et al., 1999; Al-Onaizan et al., 1999), and then translated a number of texts using different translation models and different transfer methods, namely glossing (replacing each Tamil word by the most likely candidate from the translation tables created with the EGYPT toolkit) and Model 4 decoding (Brown et al., 1995; Germann et al., 2001). Figure 3 shows the output of the different systems in comparison with the human translation. We then conducted the following experiments. 3.2.1 Document Classification Task Seven human subjects without any knowledge of Tamil were given translations of a set of 15 texts </context>
</contexts>
<marker>Knight, Al-Onaizan, Purdy, Curin, Jahr, Lafferty, Melamed, Smith, Och, Yarowsky, 1999</marker>
<rawString>Kevin Knight, Yaser Al-Onaizan, David Purdy, Jan Curin, Michael Jahr, John Lafferty, Dan Melamed, Noah Smith, Franz Josef Och, and David Yarowsky. 1999. EGYPT: a statistical machine translation toolkit. http://www.clsp.jhu.edu/ws99/ projects/mt/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanford B Steever</author>
</authors>
<title>Tamil and the Dravidian languages.</title>
<date>1990</date>
<booktitle>The World’s Major Languages,</booktitle>
<pages>725--746</pages>
<editor>In Bernard Comrie, editor,</editor>
<publisher>Oxford University Press,</publisher>
<location>New York, Oxford.</location>
<contexts>
<context position="12294" citStr="Steever (1990)" startWordPosition="2085" endWordPosition="2086">of the parallel corpus we were able to create during our experiment, about 28% of all word forms in previously unseen Sri Lankan Tamil text cannot be found in the corpus, and 50% have been seen less than 5 times. In other words, if we train a system on this data, we can expect it to stumble over every other word! At a corpus size of 100,000 tokens, the numbers are 17% and 33%. For English, the numbers are 9%/23% for a corpus of 24K tokens and 0%/8% for a corpus of 100K tokens. In order to boost the text coverage we built a simple text stemmer for Tamil, based on the Tamil inflection tables in Steever (1990) and some additional inspection of our parallel corpus. The stemmer uses regular expression matching to cut off inflectional endings and introduce some extra tokens for negation and certain case markings (such as locative and genitive), which are all marked morphologically in Tamil. It should be noted that the stemmer is far from perfect and was only intended to be an interim solution. The performance increases are displayed in Figure 2. For a corpus size 0 20 40 60 80 100 120 corpus size (in thousand tokens) Figure 2: Text coverage increase by stemming for Tamil. The solid lines indicate text</context>
</contexts>
<marker>Steever, 1990</marker>
<rawString>Sanford B. Steever. 1990. Tamil and the Dravidian languages. In Bernard Comrie, editor, The World’s Major Languages, pages 725–746. Oxford University Press, New York, Oxford.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>