<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.238458">
<sectionHeader confidence="0.720796" genericHeader="abstract">
WHEN IS THE NEXT ALPAC REPORT DUE?
</sectionHeader>
<bodyText confidence="0.988893708737865">
Margaret KING
Dalle Molle Institute for Semantic and Cognitive Studies
University of Geneva
Switzerland
Machine translation has a someOhat checquered
history. There were already proposals for automatic
translation systems in the 30&apos;s, but it was not
until after the second world war that real enthu-
siasm led to heavy funding and unrealistic expec-
tations. Traditionally, the start of intensive
work on machine translation is taken as being a
memorandum of Warren Weaver, then Director of the
Natural Sciences Division of the Rockefeller
Foundation, in 1949. In this memorandum, called
&apos;Translation&apos;, Weaver took stock of earlier work
done by Booth and Richens. He likened the problem
of machine translation to the problem of code
breaking, for which digital computers had been
used with considerable success : &amp;quot;It is very
tempting to say that a book written in Chinese
is simply a bock written in English which was
coded into the &apos;Chinese code&apos;. If we have useful
methods for solving almost any cryptographic pro-
blem, may it not be that with proper interpreta-
tion we already have useful methods for transla-
tion?&amp;quot; (Weaver, 1949).
Weaver&apos;s memorandum led to a great deal of
activity in research on machine translation, and
eventually to the first conference on the topic,
organised by Bar-Hillel in 1952. At this confe-
rence, optimism reigned. Afterwards, teams in a
number of American universities pursued research
along the general lines agreed at the conference
to be fruitful. At Georgetown University, L.E.
Dostert started up a machine translation project
with the declared aim of building a pilot system
to convince potential funding agencies of the
feasibility and the practicability of machine
translation. This led in 1954 to the famous
Georgetown experiment, a pilot system translating
from Russian to English, which was hailed as an
unqualified success: during the next ten years
over 20 million dollars were invested in machine
translation by various US government agencies.
An idea of the amount of research between
1956 and 1959 can be gained by considering that in
those years no fewer than twelve research groups
were established in the US, a number of groups
in the USSR came into existence, most within the
Academy of Sciences in Moscow, and two British
Universities were carrying on research.
Most of the systems developed were basPd on
what Buchmann (1984) has called a &apos;brute force&apos;
approach: Syntactic analysis was only done at
a local word-centred level, both so-called syntax
and dictionary compilation were very narrowly
corpus based, and thus almost totally empirical.
Indeed, the problem of machine translation was
perceived as being an engineering problem requir-
ing clever programming rather than linguistic
insight.
By the late 1960.s, workers in machine trans-
lation themselves had begun to see that-the empi-
rical.approach was unsatisfactory. The European
projects begun in the early 1960&apos;s at Grenoble
and Milan reflect this, as does the work of the
group set up in Montreal in 1962. These groups
based their work from the start on clear theore-
tical foundations (dependency theory in Grenoble,
correlational grammar in Milan, transformational
theory in Montreal).
However, the growing perception that brute
force was not enough came too late to save re-
search in the US. In 1964, the US National Academy
of Sciences set up an investigatory committee,
the Automatic Language Processing Advisory Com-
mittee (ALPAC), with the task of investigating
the results so far obtained and advising on fur-
ther funding. The committee, in setting pp a frame-
work for assessing machine translation, considered
such questions as quality and effectiveness of
human translation, the time and money required
for scientists to learn Russian, amounts spent
for translation within the US government and the
need for translations and translators. Based on
such criteria, the cannittee came to a strong
negative conclusion &apos;... we do not have useful
machine translation. Further, there is no imme-
diate or predictable prospect of useful machine
translation&apos;.
The ALPAC report effectively killed machine
translation research in the States, although some
European projects survived.
In the years since the ALPAC report, a number
of commercial systems has been developed, some of
them, ironically, based on the very system so
roundly condemned by the ALPAC committee. Two
trends can he distinguished: systems, such as
SYSTRAN, which still aim at no significant human
intervention during the translation process, but
accept pre- and/or post-editing, and interactive
systems which aim primarily at being translators&apos;
aids, such as Weidner or Alps.
</bodyText>
<page confidence="0.995927">
352
</page>
<bodyText confidence="0.9886855">
In recent years, partially because the deve-
lopment of commercial systems renewed faith in the
feasibility of maChine translation, partially
because of the results achieved by the surviving
research projects, above all because of the grow-
ing and pressing need for translation, research in
machine translation has begun to revive. At the
moment, the European Community is spcasozing a
large research and development programme, France
has a National Project on machine translation, a
very large number of projects are being funded in
Japan and a German Corporation is proposing cam-
mercial development of a system developed at the
University of Texas.
There are people who see strong parallels
between the present situation and that immediately
before the publication of the AIPAC report, fore-
seeing a second &apos;failure&apos; for machine translation
as a discipline. Others believe that advances in
linguistics and in computer science, together with
the results of the last twenty years, justify a
cautious optimism, especially when the more rea-
listic expectations of today&apos;s research workers
(and of their funding authorities) are taken into
account.
The panel discussion will aim at clarifying
similarities and differences in the two states
of the world, weighing both scientific conside-
rations and other relevant factors.
ACKNOWLEDGEMENTS
The availability of Buchmann (1984) greatly
facilitated the writing of the first part of this
panel paper. I would like to record my thanks to
its author.
</bodyText>
<sectionHeader confidence="0.927579" genericHeader="references">
REFERENCES
</sectionHeader>
<bodyText confidence="0.761030428571429">
ALPAC, 1966. language and Machines; Computers in
Translation and Liriguistics. Washington D.C.,
Publication 1416, National Academy of Sciences.
Budhmann, B. Early History of Machine Translation.
Paper prepared for the Lugano Tutorial on
Machine Translation, April 1984.
Weaver, W. Translation. New York, 1949. Mimeo.
</bodyText>
<page confidence="0.999026">
353
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.114380">
<title confidence="0.994941">WHEN IS THE NEXT ALPAC REPORT DUE?</title>
<author confidence="0.99948">Margaret KING</author>
<affiliation confidence="0.980958">Dalle Molle Institute for Semantic and Cognitive Studies University of Geneva</affiliation>
<address confidence="0.454326">Switzerland</address>
<abstract confidence="0.994946635036496">Machine translation has a someOhat checquered There were already proposals for translation systems in the 30&apos;s, but it was not until after the second world war that real enthusiasm led to heavy funding and unrealistic expectations. Traditionally, the start of intensive work on machine translation is taken as being a memorandum of Warren Weaver, then Director of the Natural Sciences Division of the Rockefeller Foundation, in 1949. In this memorandum, called &apos;Translation&apos;, Weaver took stock of earlier work done by Booth and Richens. He likened the problem of machine translation to the problem of code breaking, for which digital computers had been used with considerable success : &amp;quot;It is very tempting to say that a book written in Chinese is simply a bock written in English which was into the &apos;Chinese code&apos;. If we useful for solving almost any cryptographic promay be that with proper interpretation we already have useful methods for translation?&amp;quot; (Weaver, 1949). Weaver&apos;s memorandum led to a great deal of activity in research on machine translation, and eventually to the first conference on the topic, organised by Bar-Hillel in 1952. At this conference, optimism reigned. Afterwards, teams in a number of American universities pursued research along the general lines agreed at the conference to be fruitful. At Georgetown University, L.E. Dostert started up a machine translation project with the declared aim of building a pilot system to convince potential funding agencies of the feasibility and the practicability of machine translation. This led in 1954 to the famous Georgetown experiment, a pilot system translating from Russian to English, which was hailed as an unqualified success: during the next ten years over 20 million dollars were invested in machine translation by various US government agencies. An idea of the amount of research between and 1959 can gained by considering that in those years no fewer than twelve research groups were established in the US, a number of groups in the USSR came into existence, most within the Academy of Sciences in Moscow, and two British Universities were carrying on research. of the systems developed were on what Buchmann (1984) has called a &apos;brute force&apos; approach: Syntactic analysis was only done at a local word-centred level, both so-called syntax and dictionary compilation were very narrowly corpus based, and thus almost totally empirical. Indeed, the problem of machine translation was perceived as being an engineering problem requirclever rather than insight. the late workers in machine translation themselves had begun to see that-the empiwas unsatisfactory. The European projects begun in the early 1960&apos;s at Grenoble and Milan reflect this, as does the work of the group set up in Montreal in 1962. These groups based their work from the start on clear theoretical foundations (dependency theory in Grenoble, correlational grammar in Milan, transformational theory in Montreal). However, the growing perception that brute force was not enough came too late to save research in the US. In 1964, the US National Academy of Sciences set up an investigatory committee, the Automatic Language Processing Advisory Committee (ALPAC), with the task of investigating the results so far obtained and advising on further funding. The committee, in setting pp a framework for assessing machine translation, considered such questions as quality and effectiveness of human translation, the time and money required for scientists to learn Russian, amounts spent for translation within the US government and the need for translations and translators. Based on criteria, the to a strong negative conclusion &apos;... we do not have useful machine translation. Further, there is no immediate or predictable prospect of useful machine translation&apos;. The ALPAC report effectively killed machine translation research in the States, although some European projects survived. In the years since the ALPAC report, a number of commercial systems has been developed, some of them, ironically, based on the very system so roundly condemned by the ALPAC committee. Two trends can he distinguished: systems, such as SYSTRAN, which still aim at no significant human intervention during the translation process, but accept preand/or post-editing, and interactive which aim primarily at translators&apos; aids, such as Weidner or Alps. 352 In recent years, partially because the development of commercial systems renewed faith in the feasibility of maChine translation, partially because of the results achieved by the surviving research projects, above all because of the growing and pressing need for translation, research in machine translation has begun to revive. At the moment, the European Community is spcasozing a large research and development programme, France has a National Project on machine translation, a very large number of projects are being funded in Japan and a German Corporation is proposing cammercial development of a system developed at the University of Texas. There are people who see strong parallels between the present situation and that immediately before the publication of the AIPAC report, foreseeing a second &apos;failure&apos; for machine translation as a discipline. Others believe that advances in linguistics and in computer science, together with the results of the last twenty years, justify a cautious optimism, especially when the more realistic expectations of today&apos;s research workers (and of their funding authorities) are taken into account. The panel discussion will aim at clarifying similarities and differences in the two states of the world, weighing both scientific considerations and other relevant factors. ACKNOWLEDGEMENTS The availability of Buchmann (1984) greatly facilitated the writing of the first part of this panel paper. I would like to record my thanks to its author. REFERENCES 1966. and Machines; Computers in and Liriguistics.Washington D.C.,</abstract>
<note confidence="0.867199333333333">Publication 1416, National Academy of Sciences. B. History of Machine Translation. Paper prepared for the Lugano Tutorial on Machine Translation, April 1984. Translation.New York, 1949. Mimeo. 353</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>