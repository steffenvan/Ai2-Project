<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.98406">
France Telecom R&amp;D Beijing Word Segmenter
for Sighan Bakeoff 2006
</title>
<author confidence="0.440031333333333">
Wu Liu
France Telecom R&amp;D
Beijing
</author>
<affiliation confidence="0.280105">
wu.liu@franceteleco
</affiliation>
<email confidence="0.667504">
m.com
</email>
<author confidence="0.985767">
Nan He
</author>
<affiliation confidence="0.967071">
Beijing University of Posts
</affiliation>
<address confidence="0.429215">
and Telecommunications
</address>
<email confidence="0.890162">
hn.ft.pris@gmail.co
</email>
<note confidence="0.3959326">
m
Heng Li
France Telecom R&amp;D Bei-
jing
heng.li@francetele
</note>
<email confidence="0.399084">
com.com
</email>
<author confidence="0.978334">
Haitao Luo
</author>
<affiliation confidence="0.776632">
Northeastern University of
China
</affiliation>
<email confidence="0.887455">
luoht@ics.neu.edu.
</email>
<note confidence="0.235941">
cn
</note>
<author confidence="0.914044">
Yuan Dong
</author>
<affiliation confidence="0.893912">
Beijing University of
</affiliation>
<sectionHeader confidence="0.286067" genericHeader="abstract">
Posts and Telecommunications
</sectionHeader>
<email confidence="0.975771">
yuandong@bupt.edu.cn
</email>
<author confidence="0.659941">
Haila Wang
France Telecom R&amp;D Beijing
</author>
<affiliation confidence="0.445751">
haila.wang@francetele
com.com
</affiliation>
<sectionHeader confidence="0.990152" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.999965166666667">
This paper presents two word segmenta-
tion (WS) systems and a named entity
recognition (NER) system in France
Telecom R&amp;D Beijing. The one system
of WS is for open tracks based on n-
gram language model and another one is
for closed tracks based on maximum en-
tropy approach. The NER system uses a
hybrid algorithm based on Class-based
language model and rule-based knowl-
edge. These systems are all augmented
with a set of post-processors.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999795428571429">
The FTRD team participated in MSRA Open,
MSRA Closed and CityU Closed tracks of the
WS bakeoff and MSRA Open track of the NER
bakeoff, and achieved the state-of-the-art per-
formance in these tracks. Analysis of the results
shows that each component of these systems
contributed to the scores.
</bodyText>
<sectionHeader confidence="0.996809" genericHeader="method">
2 System Description
</sectionHeader>
<subsectionHeader confidence="0.995419">
2.1 MSRA Open track of WS
</subsectionHeader>
<bodyText confidence="0.970000333333333">
The system used in open track of WS is based on
the system (Li 2005) participated in the second
international WS bakeoff. We mainly modify the
factoid detection rules and add the GKB (The
Grammatical Knowledge-base of Contemporary
Chinese) dictionary. The system also has a few
postprocessors. The main postprocessors include
named entity recognizers and TBL (Transforma-
tion-Based Learning) component.
2.1.1 Basic system
In our basic system, Chinese words can be cate-
gorized into one of the following types: lexicon
words, morphological words, factoids, name en-
tities. These types of words were processed in
different ways in our system, and were incorpo-
rated into a unified statistical framework of the
trigram language model. The details about the
basic system are reported in (Li 2005).
</bodyText>
<subsubsectionHeader confidence="0.508267">
2.1.2 Factoid detection
</subsubsectionHeader>
<bodyText confidence="0.999682285714286">
The factoid rules used in the basic system were
summarized according to the MSRA training
data. The Tokenization Guidelines of Chinese
Text (V5.0) was provided by MSRA in this
bakeoff. We used the Guidelines to rewrite the
factoid rules, and the performance had the dis-
tinct improvement.
</bodyText>
<page confidence="0.969503">
122
</page>
<bodyText confidence="0.757179">
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 122–125,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</bodyText>
<subsectionHeader confidence="0.530926">
2.1.3 Named entity identification
</subsectionHeader>
<bodyText confidence="0.999978">
The named entity recognizer is the one partici-
pated in the NER bakeoff, as shown in figure 1.
In the section 2.3, we will describe in detail.
</bodyText>
<subsectionHeader confidence="0.999618">
2.2 System Used in Close tracks
</subsectionHeader>
<bodyText confidence="0.9980366">
The system used in closed tracks of WS is based
on maximum entropy approach. The system also
has a few postprocessors. The main postproces-
sors include combining the separated words and
TBL component.
</bodyText>
<subsectionHeader confidence="0.687396">
2.2.1 Basic system
</subsectionHeader>
<bodyText confidence="0.999861">
The basic system is similar to (Ng and Low,
2004). We used the Tsujii laboratory maximum
entropy package v2.0 (http://www-tsujii.is.s.u-
tokyo.ac.jp/—tsuruoka/maxent/) to train our mod-
els. For CityU closed track, the basic features are
the same as (Ng and Low, 2004). For MSRA
closed track, we used two sets of basic features.
The one is similar to (Ng and Low, 2004) and we
change the window size of another one from 2 to
3, so we trained two models for MSRA closed
track and submitted two results.
</bodyText>
<subsubsectionHeader confidence="0.705888">
2.2.2 Post processing
</subsubsectionHeader>
<bodyText confidence="0.9998598">
Firstly, we extracted one lexicon from each train-
ing data. For MSRA closed track, the postpro-
cessor only combined the words which appeared
in the lexicon but were separated in the test result.
For CityU closed track, we firstly used the fac-
toid tool provided by the open system of WS to
combine the separated factoid words, and then
we used the lexicon to combine the separated
words, at last the TBL was applied to the test
result.
</bodyText>
<subsectionHeader confidence="0.986941">
2.3 MSRA Open track of NER
</subsectionHeader>
<bodyText confidence="0.9999716">
The system used a hybrid algorithm which can
combine a class-based statistical model (Gao
2004) with various types of rule-based knowl-
edge very well. All the words were categorized
into three types: Lexicon words (LWs), Factoid
words (FTs), Named Entity (NEs). Accordingly,
three main components were included to identify
each kind of named entities: basic word candi-
dates, NE combination and Viterbi search, as
shown in Figure 1.
</bodyText>
<figureCaption confidence="0.922709">
Figure 1 FTRD NE Recognizer
</figureCaption>
<bodyText confidence="0.99952825">
The recognizer was applied to open track of WS
and we used it to participate in the MSRA open
track of NER. The system also had a TBL post-
processor.
</bodyText>
<subsectionHeader confidence="0.938533">
2.4 TBL
</subsectionHeader>
<bodyText confidence="0.999985380952381">
In our system, the open source toolkit fnTBL
(http://nlp.cs.jhu.edu/—rflorian/fntbl/index.html)
is chosen. Coping with word segmentation task,
we utilized a method called &amp;quot;LMR&amp;quot; tagging
which was the same as (Nianwen Xue and Libin
Shen 2003). Two rule template sets were used in
our system. The complicated one had 40 tem-
plates, which covered various kinds of words
position and tag position occurrence, i.e., consid-
ering contextual information of words and tags.
For example, rule &amp;quot;pos_0 word-0 word-1
word_2 =&gt; pos&amp;quot; could generate rules containing
information about current word, current word&apos;s
tag, the next word and the word after next. The
other rule template neglected tag information, it
took only contextual word information into ac-
count. For an instance, &amp;quot;word—0 word—1 word—2
=&gt; pos&amp;quot;. The task of WS applied the two rule
template sets, and the task of NER only applied
the complicated one. In the Section 3, we will
compare the two rule template sets.
</bodyText>
<sectionHeader confidence="0.998959" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.981992">
3.1 Open tracks
</subsectionHeader>
<bodyText confidence="0.951822833333333">
3.1.1 MSRA Open track of WS
In this open track, we used one lexicon of
294,382 entries, which included the entries of
42,430 MDWs (Morphological Derived Words)
generated from the GKB dictionary, 12,487 PNs,
22,907 LNs and 29,032 ONs, 10,414 four-
character idioms, plus the word lists generated
from the training data provided by the second
international Chinese Word Segmentation bake-
off and 80114 GKB words. We also used the
training data provided by the last bakeoff for
training our trigram word-based language model.
</bodyText>
<page confidence="0.995315">
123
</page>
<bodyText confidence="0.9985415">
Table 1 presents the results of this track. For
comparison, we also include in the table (Row 1)
the results of basic system. From Row 2 to Row
11, it shows the relative contribution of each
component and resource to the overall word
segmentation performance. The second column
shows the recall, the third column the precision,
and the fourth column F-score. The last two col-
umns present the recall of the OOV words and
the recall of IV words, respectively.
</bodyText>
<table confidence="0.999326">
(�) R P F Roov Riv
1.basic 0.971 0.958 0.964 0.590 0.984
system
2.1+new 0.966 0.958 0.962 0.642 0.978
factoid
3.1+GK 0.975 0.966 0.971 0.716 0.984
B lexicon
4.3+new 0.971 0.967 0.969 0.768 0.978
factoid
4+NE 0.971 0.973 0.972 0.838 0.975
5+TBL 0.977 0.976 0.977 0.840 0.982
7.5+new 0.980 0.978 0.979 0.839 0.985
TBL
8. 0.977 0.970 0.974 0.769 0.984
4+TBL
9.4+new 0.980 0.971 0.975 0.769 0.987
TBL
8+NE 0.977 0.976 0.977 0.840 0.982
9+NE 0.979 0.978 0.979 0.841 0.984
</table>
<bodyText confidence="0.992652338709678">
Table1: Our system results on Open tracks
From Table 1 we can find that, in Row 1, the
basic system participated in the last bakeoff al-
ready achieves quite good recall, but the recall of
OOV is not very good because it cannot correctly
identify unknown words that are not in the lexi-
con such as factoids and name entities (espe-
cially the nested named entity) and new words
(except factoids, named entities and words ab-
stracted from training data). In Row 2, we only
rewrite the factoid rules according to the MSRA
Guidelines, and the recall of OOV improves sig-
nificantly while the recall of IV falls slightly. It
shows that the factoid detection affects the recall
of IV. As shown in Table 1, the GKB lexicon has
made significant and persistent progress in all
performance because the GKB lexicon is refined
and the words are conformed to the MSRA stan-
dard. We also find that the NE postprocessor can
improve the recall of OOV but affects slightly
the recall of IV in all experiments. It shows that
our named entity recognition has make im-
provement compared with that of last year. As
shown in Table 1, TBL has made slightly but
persistent progress in all steps it applies to. After
TBL adaptation OOV recall stays almost un-
changed, for the rules are derived from training
corpus, and no OOV words would meet the con-
dition of applying them in theory, but IV recall
improves, which compensates the loss of IV re-
call caused by NE post-process and the factoid
detection. It is interesting comparing the per-
formance of two TBL template sets, the first
template set is simple and the threshold for gen-
erating rules is 3 by default (called TBL in Table
1), and the second is more complicated with a
&amp;quot;0&amp;quot; threshold (called New TBL in Table 1). The
number of rules generated is 1061 and 12135
respectively. Our experiments demonstrate that
more precise rule template set with low threshold
always leads to better performance, for they
could cover more situations, although a simple
rule template set with high threshold does better
in OOV word recognition.
3.1.2 MSRA Open track of NER
In the track, we used People&apos;s Daily 2000 corpus
(Yu, 2003) for building our lexicon and training
our model.
Considering that organization names are ir-
regular in their forms compared with person
names and location names, and there are many
abbreviations and anaphora, TBL adaptation may
degrade the performance of organization, we
submitted two results, as shown in Table 2.
1+TBL1 means that TBL only adapt person and
location results of basic system, the organization
performance of basic system and 1+TBL1 would
be identical. 1+TBL2 means TBL adapt all three
types of NE. For comparison, we list (Column 2)
the results of basic system. The Row 2 to Row
13 shows the recall, the precision, and the F-
score of PN, LN, ON and total.
</bodyText>
<table confidence="0.999821538461539">
(�) 1.basic 1+TBL1 1+TBL2
PN R 87.28 91.43 91.74
P 90.63 92.56 92.77
F 88.92 91.99 92.25
LN R 80.18 87.39 89.74
P 81.68 87.51 89.77
F 80.92 87.45 89.76
ON R 65.59 65.59 76.48
P 73.80 73.80 75.44
F 69.45 69.45 76.11
Total R 79.31 83.99 87.53
P 82.98 86.45 87.67
F 81.10 85.20 87.60
</table>
<page confidence="0.959463">
124
</page>
<tableCaption confidence="0.996052">
Table 2: MSRA Open track of NER
</tableCaption>
<bodyText confidence="0.996605833333333">
To our surprise, performance listed in Table 2
demonstrates that applying TBL causes a dra-
matic improvement in all three types of NE, es-
pecially organization performance. The great
similarity between training corpus and test cor-
pus of MSRA may explain this. For the inconsis-
tency of standard between MSRA and PKU, the
recall, especially of the ONs, is not very good.
We did some effort in the standard adaptation,
such as constraint the length and type of candi-
date words in combining the named entities, but
the result is not very good.
</bodyText>
<subsectionHeader confidence="0.996843">
3.2 Closed tracks
</subsectionHeader>
<bodyText confidence="0.9183185">
The Table 3 and Table 4 present the results of
MSRA and CityU closed tracks respectively.
</bodyText>
<table confidence="0.998037909090909">
(%) R P F Roov Riv
1.basic 0.924 0.877 0.900 0.575 0.936
system(2)
2.1+traini 0.955 0.953 0.954 0.575 0.969
ng lexicon
3.2+TBL 0.960 0.955 0.958 0.575 0.973
4.basic 0.919 0.880 0.899 0.602 0.930
system(3)
5.4+traini 0.950 0.954 0.952 0.602 0.962
ng lexicon
6.5+TBL 0.954 0.955 0.955 0.603 0.966
</table>
<tableCaption confidence="0.995297">
Table 3: Our system results on MSRA Closed
</tableCaption>
<table confidence="0.99960775">
(%) R P F Roov Riv
1.basic 0.947 0.916 0.931 0.716 0.957
system
2.1+traini 0.959 0.960 0.959 0.716 0.969
ng lexicon
3.2+TBL 0.969 0.964 0.967 0.716 0.980
4.1+factoi 0.946 0.915 0.931 0.713 0.956
d tool
5.4+traini 0.958 0.959 0.959 0.713 0.968
ng lexicon
6.5+TBL 0.969 0.964 0.966 0.712 0.980
6&apos; 0.962 0.962 0.962 0.722 0.972
</table>
<tableCaption confidence="0.986816">
Table 4: Our system results on CityU Closed
</tableCaption>
<bodyText confidence="0.999899263157895">
In Table 3, the basic system (2) shows the win-
dow size of the template is 2 and the basic sys-
tem (3) is 3. As is shown in the table, except the
precision and the recall of OOV, the performance
of window size with 2 outperforms that of win-
dow size with 3.
In Table 4, the system 6&apos; is the one we submit-
ted in this closed CityU track, but the system 6 is
better than the system 6&apos;. In TBL training, we
made a mistake that the training data weren&apos;t
processed by factoid tool and lexicon combining.
We also can find that the factoid tool doesn&apos;t im-
prove the performance. The system 6 isn&apos;t the
best one (system 3).
Combining the separated words according to
training lexicon improved the performance of
both MSRA and CITYU closed track. In the
meantime, TBL worked considerably well in all
closed tracks.
</bodyText>
<sectionHeader confidence="0.99979" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999655333333333">
The evaluation results show that the performance
of NER need be improved in abbreviations rec-
ognition and anaphora resolution.
</bodyText>
<sectionHeader confidence="0.998145" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.900505">
The work reported here was a team effort. We
thank Yonggang Xue, Duo Ji, Haitao Luo, Nan
He and Xinnian Mao for their help in the ex-
perimentation and evaluation of the system. We
also thank Prof. Shiwen Yu for the People&apos;s
Daily 2000 corpus (Yu 2003) and GKB (Yu
2002) lexicon.
</bodyText>
<sectionHeader confidence="0.990667" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999761083333334">
Heng Li, etc. 2005. Chinese Word Segmentation in
FTRD Beijing. Proceedings of the Fourth SIGHAN
workshop on Chinese Language Processing.
Pages:150-154
Hwee Tou Ng, Jin Kiat Low. 2004. Chiense part-of-
speech tagging: One-at-a-time or all-at-once?
Word-based or character-based?. Proceedings of
the 2004 conference on Empirical Methods in
Natural Language Processing. Pages:277-284
Jianfeng Gao, Mu Li, Andi Wu and Chang-Ning
Huang. 2004a. Chinese word segmentation: a
pragmatic approach. Microsoft Research Technical
Report, MSR-TR-2004-123.
Nianwen Xue, Libin Shen. July 2003. Chinese word
segmentation as LMR tagging. Proceedings of the
Second SIGHAN workshop on Chinese Language
Processing. Pages:176-179.
Shiwen Yu, etc. 2003. Specification for Corpus Proc-
essing at Peking University:Word Segmentation,
POS Tagging and Phonetic Notation. Journal of
Chinese Language and Computing, 13(2) 121-158.
Shiwen Yu, etc. 2002. The Grammatical Knowledge-
base of Contemporary Chinese --- A Complete
Specification. Tsinghua University Press.
</reference>
<page confidence="0.998501">
125
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.008135">
<title confidence="0.872163">France Telecom R&amp;D Beijing Word Segmenter for Sighan Bakeoff 2006</title>
<author confidence="0.974146">Wu Liu</author>
<affiliation confidence="0.950715">France Telecom</affiliation>
<address confidence="0.84518">Beijing</address>
<email confidence="0.9815905">wu.liu@francetelecom.com</email>
<author confidence="0.839909">Nan</author>
<affiliation confidence="0.9135475">Beijing University of and</affiliation>
<email confidence="0.698573">m</email>
<author confidence="0.7115765">Heng Li Telecom R&amp;D Bei-</author>
<email confidence="0.7862205">jingcom.com</email>
<author confidence="0.596493">Haitao</author>
<affiliation confidence="0.998736">Northeastern University</affiliation>
<email confidence="0.7719685">luoht@ics.neu.edu.cn</email>
<author confidence="0.909875">Yuan</author>
<affiliation confidence="0.95275">Beijing University of</affiliation>
<title confidence="0.84666">Posts and Telecommunications yuandong@bupt.edu.cn</title>
<author confidence="0.546261">Haila France Telecom R&amp;D</author>
<email confidence="0.997508">com.com</email>
<abstract confidence="0.999012230769231">This paper presents two word segmentation (WS) systems and a named entity recognition (NER) system in France Telecom R&amp;D Beijing. The one system of WS is for open tracks based on ngram language model and another one is for closed tracks based on maximum entropy approach. The NER system uses a hybrid algorithm based on Class-based language model and rule-based knowledge. These systems are all augmented with a set of post-processors.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Heng Li</author>
<author>etc</author>
</authors>
<title>Chinese Word Segmentation in FTRD Beijing.</title>
<date>2005</date>
<booktitle>Proceedings of the Fourth SIGHAN workshop on Chinese Language Processing.</booktitle>
<pages>150--154</pages>
<marker>Li, etc, 2005</marker>
<rawString>Heng Li, etc. 2005. Chinese Word Segmentation in FTRD Beijing. Proceedings of the Fourth SIGHAN workshop on Chinese Language Processing. Pages:150-154</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Jin Kiat Low</author>
</authors>
<title>Chiense part-ofspeech tagging: One-at-a-time or all-at-once? Word-based or character-based?.</title>
<date>2004</date>
<booktitle>Proceedings of the 2004 conference on Empirical Methods in Natural Language Processing.</booktitle>
<pages>277--284</pages>
<contexts>
<context position="3021" citStr="Ng and Low, 2004" startWordPosition="472" endWordPosition="475">the Fifth SIGHAN Workshop on Chinese Language Processing, pages 122–125, Sydney, July 2006. c�2006 Association for Computational Linguistics 2.1.3 Named entity identification The named entity recognizer is the one participated in the NER bakeoff, as shown in figure 1. In the section 2.3, we will describe in detail. 2.2 System Used in Close tracks The system used in closed tracks of WS is based on maximum entropy approach. The system also has a few postprocessors. The main postprocessors include combining the separated words and TBL component. 2.2.1 Basic system The basic system is similar to (Ng and Low, 2004). We used the Tsujii laboratory maximum entropy package v2.0 (http://www-tsujii.is.s.utokyo.ac.jp/—tsuruoka/maxent/) to train our models. For CityU closed track, the basic features are the same as (Ng and Low, 2004). For MSRA closed track, we used two sets of basic features. The one is similar to (Ng and Low, 2004) and we change the window size of another one from 2 to 3, so we trained two models for MSRA closed track and submitted two results. 2.2.2 Post processing Firstly, we extracted one lexicon from each training data. For MSRA closed track, the postprocessor only combined the words which</context>
</contexts>
<marker>Ng, Low, 2004</marker>
<rawString>Hwee Tou Ng, Jin Kiat Low. 2004. Chiense part-ofspeech tagging: One-at-a-time or all-at-once? Word-based or character-based?. Proceedings of the 2004 conference on Empirical Methods in Natural Language Processing. Pages:277-284</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Mu Li</author>
<author>Andi Wu</author>
<author>Chang-Ning Huang</author>
</authors>
<title>Chinese word segmentation: a pragmatic approach.</title>
<date>2004</date>
<tech>Microsoft Research Technical Report, MSR-TR-2004-123.</tech>
<marker>Gao, Li, Wu, Huang, 2004</marker>
<rawString>Jianfeng Gao, Mu Li, Andi Wu and Chang-Ning Huang. 2004a. Chinese word segmentation: a pragmatic approach. Microsoft Research Technical Report, MSR-TR-2004-123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Libin Shen</author>
</authors>
<title>Chinese word segmentation as LMR tagging.</title>
<date>2003</date>
<booktitle>Proceedings of the Second SIGHAN workshop on Chinese Language Processing.</booktitle>
<pages>176--179</pages>
<marker>Xue, Shen, 2003</marker>
<rawString>Nianwen Xue, Libin Shen. July 2003. Chinese word segmentation as LMR tagging. Proceedings of the Second SIGHAN workshop on Chinese Language Processing. Pages:176-179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiwen Yu</author>
<author>etc</author>
</authors>
<title>Specification for Corpus Processing at Peking University:Word Segmentation, POS Tagging and Phonetic Notation.</title>
<date>2003</date>
<journal>Journal of Chinese Language and Computing,</journal>
<volume>13</volume>
<issue>2</issue>
<pages>121--158</pages>
<marker>Yu, etc, 2003</marker>
<rawString>Shiwen Yu, etc. 2003. Specification for Corpus Processing at Peking University:Word Segmentation, POS Tagging and Phonetic Notation. Journal of Chinese Language and Computing, 13(2) 121-158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiwen Yu</author>
<author>etc</author>
</authors>
<title>The Grammatical Knowledgebase of Contemporary Chinese --- A Complete Specification.</title>
<date>2002</date>
<publisher>Tsinghua University Press.</publisher>
<marker>Yu, etc, 2002</marker>
<rawString>Shiwen Yu, etc. 2002. The Grammatical Knowledgebase of Contemporary Chinese --- A Complete Specification. Tsinghua University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>