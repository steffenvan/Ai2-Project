<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.842731">
Linguistic Structured Sparsity in Text Categorization
</title>
<author confidence="0.996696">
Dani Yogatama
</author>
<affiliation confidence="0.948878">
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.997089">
dyogatama@cs.cmu.edu
</email>
<sectionHeader confidence="0.994773" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999868625">
We introduce three linguistically moti-
vated structured regularizers based on
parse trees, topics, and hierarchical word
clusters for text categorization. These
regularizers impose linguistic bias in fea-
ture weights, enabling us to incorporate
prior knowledge into conventional bag-
of-words models. We show that our
structured regularizers consistently im-
prove classification accuracies compared
to standard regularizers that penalize fea-
tures in isolation (such as lasso, ridge,
and elastic net regularizers) on a range of
datasets for various text prediction prob-
lems: topic classification, sentiment anal-
ysis, and forecasting.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999932863636364">
What is the best way to exploit linguistic infor-
mation in statistical text processing models? For
tasks like text classification, sentiment analysis,
and text-driven forecasting, this is an open ques-
tion, as cheap “bag-of-words” models often per-
form well. Much recent work in NLP has fo-
cused on linguistic feature engineering (Joshi et
al., 2010) or representation learning (Glorot et al.,
2011; Socher et al., 2013).
In this paper, we propose a radical alternative.
We embrace the conventional bag-of-words repre-
sentation of text, instead bringing linguistic bias
to bear on regularization. Since the seminal work
of Chen and Rosenfeld (2000), the importance of
regularization in discriminative models of text—
including language modeling, structured predic-
tion, and classification—has been widely recog-
nized. The emphasis, however, has largely been
on one specific kind of inductive bias: avoiding
large weights (i.e., coefficients in a linear model).
Recently, structured (or composite) regulariza-
tion has been introduced; simply put, it reasons
</bodyText>
<author confidence="0.417372">
Noah A. Smith
</author>
<affiliation confidence="0.8862465">
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.983649">
nasmith@cs.cmu.edu
</email>
<bodyText confidence="0.999585652173913">
about different weights jointly. The most widely
explored variant, group lasso (Yuan and Lin, 2006)
seeks to avoid large E2 norms for groups of
weights. Group lasso has been shown useful in
a range of applications, including computational
biology (Kim and Xing, 2008), signal processing
(Lv et al., 2011), and NLP (Eisenstein et al., 2011;
Martins et al., 2011; Nelakanti et al., 2013). For
text categorization problems, Yogatama and Smith
(2014) proposed groups based on sentences, an
idea generalized here to take advantage of richer
linguistic information.
In this paper, we show how linguistic informa-
tion of various kinds—parse trees, thematic topics,
and hierarchical word clusterings—can be used to
construct group lasso variants that impose linguis-
tic bias without introducing any new features. Our
experiments demonstrate that structured regulariz-
ers can squeeze higher performance out of conven-
tional bag-of-words models on seven out of eight
of text categorization tasks tested, in six cases with
more compact models than the best-performing
unstructured-regularized model.
</bodyText>
<sectionHeader confidence="0.976177" genericHeader="introduction">
2 Notation
</sectionHeader>
<bodyText confidence="0.9999761">
We represent each document as a feature vector
x E RV , where V is the vocabulary size. xv is the
frequency of the with word (i.e., this is a “bag of
words” model).
Consider a linear model that predicts a binary
response y E {−1, +1} given x and weight vector
w E RV . We denote our training data of D doc-
uments in the corpus by {xd, yd}Dd=1. The goal of
the learning procedure is to estimate w by mini-
mizing the regularized training data loss:
</bodyText>
<equation confidence="0.980083">
wˆ = arg min Q(w) + EDd=1 f-(xd, w, yd),
W
</equation>
<bodyText confidence="0.98279275">
where Z(x, w, y) is the loss function for docu-
ment d and Q(w) is the regularizer.
In this work, we use the log loss:
Z(xd, w, yd) = − log(1 + exp(−ydwTxd)),
</bodyText>
<page confidence="0.970166">
786
</page>
<note confidence="0.8312615">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 786–796,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9987338">
Other loss functions (e.g., hinge loss, squared loss)
can also be used with any of the regularizers dis-
cussed in this paper.
Our focus is on the regularizer, Ω(w). For high
dimensional data such as text, regularization is
crucial to avoid overfitting.1
The usual starting points for regularization are
the “lasso” (Tibshirani, 1996) and the “ridge” (Ho-
erl and Kennard, 1970), based respectively on the
E1 and squared E2 norms:
</bodyText>
<equation confidence="0.9999675">
Ωlas(w) = Alaskwk1 = AEj |wj|
Ωrid(w) = Aridkwk22 = A Ej w2j
</equation>
<bodyText confidence="0.999723">
Both methods disprefer weights of large magni-
tude; smaller (relative) magnitude means a feature
(here, a word) has a smaller effect on the predic-
tion, and zero means a feature has no effect.2 The
hyperparameter A in each case is typically tuned
on a development dataset. A linear combination
of ridge and lasso is known as the elastic net (Zou
and Hastie, 2005). The lasso, ridge, and elastic net
are three strong baselines in our experiments.
</bodyText>
<sectionHeader confidence="0.995084" genericHeader="method">
3 Group Lasso
</sectionHeader>
<bodyText confidence="0.999947636363636">
Structured regularizers penalize estimates of w in
which collections of weights are penalized jointly.
For example, in the group lasso (Yuan and Lin,
2006), predefined groups of weights (subvectors
of w) are encouraged to either go to zero (as
a group) or not (as a group)—this is known as
“group sparsity.”3
The variant of group lasso we explore here uses
an E1,2 norm. Let g index the G predefined groups
of weights and wg denote the subvector of w con-
taining weights for group g:
</bodyText>
<equation confidence="0.4607565">
EG
Ωglas(w) =Aglas g=1 Agkwgk2,
</equation>
<bodyText confidence="0.970120888888889">
1A Bayesian interpretation of regularization is as a prior
on the weight vector w; in many cases Ω can be under-
stood as a log-prior representing beliefs about the model held
before exposure to data. For lasso regression, the prior is
a zero-mean Laplace distribution, whereas for ridge regres-
sion the prior is a zero-mean Gaussian distribution. For non-
overlapping group lasso, the prior is a two-level hierarchical
Bayes model (Figueiredo, 2002). The Bayesian interpretation
of overlapping group lasso is not yet well understood.
</bodyText>
<footnote confidence="0.996810777777778">
2The lasso leads to strongly sparse solutions, in which
many elements of the estimated w are actually zero. This
is an attractive property for efficiency and (perhaps) inter-
pretability. The ridge encourages weights to go toward zero,
but usually not all the way to zero; for this reason its solutions
are known as “weakly” sparse.
3Other structured regularizers include the fused lasso
(Tibshirani et al., 2005) and the elitist lasso (Kowalski and
Torresani, 2009).
</footnote>
<bodyText confidence="0.999855166666667">
where Aglas is a hyperparameter tuned on a devel-
opment data, and Ag is a group specific weight.
Typically the groups are non-overlapping, which
offers computational advantages, but this need not
be the case (Jacob et al., 2009; Jenatton et al.,
2011).
</bodyText>
<sectionHeader confidence="0.96399" genericHeader="method">
4 Structured Regularizers for Text
</sectionHeader>
<bodyText confidence="0.9999865">
Past work applying the group lasso to NLP prob-
lems has considered four ways of defining the
groups. Eisenstein et al. (2011) defined groups
of coefficients corresponding to the same inde-
pendent variable applied to different (continuous)
output variables in multi-output regression. Mar-
tins et al. (2011) defined groups based on fea-
ture templates used in chunking and parsing tasks.
Nelakanti et al. (2013) defined groups based on n-
gram histories for language modeling. In each of
these cases, the groups were defined based on in-
formation from feature types alone; given the fea-
tures to be used, the groups were known.
Here we build on a fourth approach that exploits
structure in the data.4 Yogatama and Smith (2014)
introduced the sentence regularizer, which uses
patterns of word cooccurrence in the training data
to define groups. We review this method, then ap-
ply the idea to three more linguistically informed
structure in text data.
</bodyText>
<subsectionHeader confidence="0.999642">
4.1 Sentence Regularizer
</subsectionHeader>
<bodyText confidence="0.9998646">
The sentence regularizer exploits sentence bound-
aries in each training document. The idea is to
define a group gd,s for every sentence s in every
training document d. The group contains coeffi-
cients for words that occur in its sentence. This
means that a word is a member of one group for
every distinct (training) sentence it occurs in, and
that the regularizer is based on word tokens, not
types as in the approach of Martins et al. (2011)
and Nelakanti et al. (2013). The regularizer is:
</bodyText>
<equation confidence="0.9984845">
Ω _ �d1 �s d 1 �d,s I I wd,s I l2 ,
3e� (w)
</equation>
<bodyText confidence="0.999913833333333">
where Sd is the number of sentences in document
d. This regularizer results in tens of thousands
to millions of heavily overlapping groups, since
a standard corpus typically contains thousands to
millions of sentences and many words that appear
in more than one sentence.
</bodyText>
<footnote confidence="0.99929225">
4This provides a compelling reason not to view such
methods in a Bayesian framework: if the regularizer is in-
formed by the data, then it does not truly correspond to a
prior.
</footnote>
<page confidence="0.993596">
787
</page>
<figure confidence="0.964866">
c0,++
c1 c4,+
c2 c3 c5,++ c8
are fantastic
</figure>
<figureCaption confidence="0.9112616">
Figure 1: An example of a parse tree from the Stanford sen-
timent treebank, which annotates sentiment at the level of
every constituent (indicated here by + and ++; no mark-
ing indicates neutral sentiment). The sentence is The ac-
tors are fantastic. Our regularizer constructs nine groups for
</figureCaption>
<bodyText confidence="0.979024136363636">
this sentence, corresponding to c0, c1, ... , ca. gc0 consists of
5 weights—(wthe, wactors, ware, wfantastic, w.), exactly the
same as the group in the sentence regularizer—gc1 consists
of 2 words, gc4 of 3 words, etc. Notice that c2, c3, c6, c7,
and ca each consist of only 1 word. The Stanford sentiment
treebank has an annotation of sentiments at the constituent
level. As in this example, most constituents are annotated as
neutral.
If the norm of wgd,s is driven to zero, then the
learner has deemed the corresponding sentence ir-
relevant to the prediction. It is important to point
out that, while the regularizer prefers to zero out
the weights for all words in irrelevant sentences, it
also prefers not to zero out weights for words in
relevant sentences. Since the groups overlap and
may work against each other, the regularizer may
not be able to drive many weights to zero on its
own. Yogatama and Smith (2014) used a linear
combination of the sentence regularizer and the
lasso (a kind of sparse group lasso; Friedman et
al., 2010) to also encourage weights of irrelevant
word types to go to zero.5
</bodyText>
<subsectionHeader confidence="0.998232">
4.2 Parse Tree Regularizer
</subsectionHeader>
<bodyText confidence="0.992596611111111">
Sentence boundaries are a rather superficial kind
of linguistic structure; syntactic parse trees pro-
vide more fine-grained information. We introduce
a new regularizer, the parse tree regularizer, in
which groups are defined for every constituent in
every parse of a training data sentence.
Figure 1 illustrates the group structures derived
from an example sentence from the Stanford sen-
timent treebank (Socher et al., 2013). This regu-
larizer captures the idea that phrases might be se-
lected as relevant or (in most cases) irrelevant to
a task, and is expected to be especially useful in
sentence-level prediction tasks.
The parse-tree regularizer (omitting the group
5Formally, this is equivalent to including one additional
group for each word type.
coefficients and A) for one sentence with the parse
tree shown in Figure 1 is:
</bodyText>
<equation confidence="0.9999395">
Qtree(W) =
&apos;✓|wthe|2 + |wactors|2 + |ware|2 + |wfantastic|2 + |w.|2
&apos;✓+ |ware|2 + |wfantastic|2 + |w2.|
&apos;✓ &apos;✓
+ |wthe|2 + |wactors|2 + |ware|2 + |wfantastic|2
+ |wthe |+ |wactors |+ |ware |+ |wfantastic |+ |w.|
</equation>
<bodyText confidence="0.996263363636364">
The groups have a tree structure, in that assign-
ing zero values to the weights in a group corre-
sponding to a higher-level constituent implies the
same for those constituents that are dominated by
it. This resembles the tree-guided group lasso in
Kim and Xing (2008), although the leaf nodes in
their tree represent tasks in multi-task regression.
Of course, in a corpus there are many parse trees
(one per sentence, so the number of parse trees is
the number of sentences). The parse-tree regular-
izer is:
</bodyText>
<equation confidence="0.988628">
Ωtree(w) = Ed=1 �s=1 �c=Sd 1 Ad,s,ck wd,s,ck2
</equation>
<bodyText confidence="0.999969433333333">
where Ad,s,c = Aglas × v1size(gd,s,c), d ranges
over (training) documents and c ranges over con-
stituents in the parse of sentence s in docu-
ment d. Similar to the sentence regularizer,
the parse-tree regularizer operates on word to-
kens. Note that, since each word token is it-
self a constituent, the parse tree regularizer in-
cludes terms just like the lasso naturally, penal-
izing the absolute value of each word’s weight
in isolation. For the lasso-like penalty on each
word, instead of defining the group weights to be
1 × the number of tokens for each word type, we
tune one group weight for all word types on a de-
velopment data. As a result, besides Aglas, we have
an additional hyperparameter, denoted by Alas.
To gain an intuition for this regularizer, consider
the case where we apply the penalty only for a sin-
gle tree (sentence), which for ease of exposition is
assumed not to use the same word more than once
(i.e., kxk∞ = 1). Because it instantiates the tree-
structured group lasso, the regularizer will require
bigger constituents to be “included” (i.e., their
words given nonzero weight) before smaller con-
stituents can be included. The result is that some
words may not be included. Of course, in some
sentences, some words will occur more than once,
and the parse tree regularizer instantiates groups
for constituents in every sentence in the training
corpus, and these groups may work against each
other. The parse tree regularizer should therefore
</bodyText>
<figure confidence="0.43690675">
The
actors
c6 c7,+
.
</figure>
<page confidence="0.976294">
788
</page>
<bodyText confidence="0.999865769230769">
be understood as encouraging group behavior of
syntactically grouped words, or sharing of infor-
mation by syntactic neighbors.
In sentence level prediction tasks, such as
sentence-level sentiment analysis, it is known that
most constituents (especially those that corre-
spond to shorter phrases) in a parse tree are un-
informative (neutral sentiment). This was verified
by Socher et al. (2013) when annotating phrases
in a sentence for building the Stanford sentiment
treebank. Our regularizer incorporates our prior
expectation that most constituents should have no
effect on prediction.
</bodyText>
<subsectionHeader confidence="0.999224">
4.3 LDA Regularizer
</subsectionHeader>
<bodyText confidence="0.999983176470588">
Another type of structure to consider is topics.
For example, if we want to predict whether a pa-
per will be cited or not (Yogatama et al., 2011),
the model can perform better if it knows before-
hand the collections of words that represent certain
themes (e.g., in ACL papers, these might include
machine translation, parsing, etc.). As a result,
the model can focus on which topics will increase
the probability of getting citations, and penalize
weights for words in the same topic together, in-
stead of treating each word separately.
We do this by inferring topics in the training
corpus by estimating the latent Dirichlet alloca-
tion (LDA) model (Blei et al., 2003)). Note that
LDA is an unsupervised method, so we can in-
fer topical structures from any collection of docu-
ments that are considered related to the target cor-
pus (e.g., training documents, text from the web,
etc.). This contrasts with typical semi-supervised
learning methods for text categorization that com-
bine unlabeled and labeled data within a genera-
tive model, such as multinomial naive Bayes, via
expectation-maximization (Nigam et al., 2000) or
semi-supervised frequency estimation (Su et al.,
2011). Our method does not use unlabeled data
to obtain more training documents or estimate the
joint distributions of words better, but it allows the
use of unlabeled data to induce topics. We leave
comparison with other semi-supervised methods
for future work.
There are many ways to associate inferred top-
ics with group structure. In our experiments, we
choose the R most probable words given a topic
and create a group for them.6 The LDA regular-
</bodyText>
<footnote confidence="0.994828666666667">
6Another possibility is to group the smallest set of words
whose total probability given a topic amounts to P (e.g.,
0.99). mass of a topic. Preliminary experiments found this
</footnote>
<equation confidence="0.956097">
izer can be written as:
Qlda(w) = EKk=1 Aklwk�2,
</equation>
<bodyText confidence="0.9999805">
where k ranges over the K topics. Similar to our
earlier notations, wk corresponds to the subvec-
tor of w such that the corresponding features are
present in topic k. Note that in this case we can
also have overlapping groups, since words can ap-
pear in the top R of many topics.
</bodyText>
<table confidence="0.650788333333333">
k = 1 k = 2 k = 3 k = 4
soccer injury physics monday
striker knee gravity tuesday
midfielder ligament moon april
goal shoulder sun june
defender cruciate relativity sunday
</table>
<tableCaption confidence="0.972378">
Table 1: A toy example of K = 4 topics. The top R = 5
</tableCaption>
<bodyText confidence="0.963944352941176">
words in each topics are displayed. The LDA regularizer
will construct four groups from these topics. The first group
is (wsoccer, wstriker, wmidfielder, wgoal, wdefender), the sec-
ond group is (winjury, wknee, wligament, wshoulder, wcruciate),
etc. In this example, there are no words occurring in the top
R of more than one topic, but that need not be the case in
general.
To gain an intuition for this regularizer, consider
the toy example in Table 1. the case where we
have K = 4 topics and we select R = 5 top words
from each topic. Supposed that we want to clas-
sify whether an article is a sports article or a sci-
ence article. The regularizer might encourage the
weights for the fourth topics’ words toward zero,
since they are less useful for the task. Addition-
ally, the regularizer will penalize words in each of
the other three groups collectively. Therefore, if
(for example) ligament is deemed a useful feature
for classifying an article to be about sports, then
the other words in that topic will have a smaller ef-
fective penalty for getting nonzero weights—even
weights of the opposite sign as wligament. It is im-
portant to distinguish this from unstructured reg-
ularizers such as the lasso, which penalize each
word’s weight on its own without regard for re-
lated word types.
Unlike the parse tree regularizer, the LDA regu-
larizer is not tree structured. Since the lasso-like
penalty does not occur naturally in a non tree-
structured regularizer, we add an additional lasso
penalty for each word type (with hyperparameter
Alas) to also encourage weights of irrelevant words
to go to zero. Our LDA regularizer is an instance
of sparse group lasso (Friedman et al., 2010).
</bodyText>
<footnote confidence="0.431505">
not to work well.
</footnote>
<page confidence="0.98552">
789
</page>
<note confidence="0.531185">
v0
</note>
<subsectionHeader confidence="0.991955">
4.4 Brown Cluster Regularizer
</subsectionHeader>
<bodyText confidence="0.997706642857143">
Brown clustering is a commonly used unsuper-
vised method for grouping words into a hierarchy
of clusters (Brown et al., 1992). Because it uses
local information, it tends to discover words with
similar syntactic behavior, though semantic group-
ings are often evident, especially at the more fine-
grained end of the hierarchy.
We incorporate Brown clusters into a regular-
izer in a similar way to the topical word groups
inferred using LDA in §4.3, but here we make use
of the hierarchy. Specifically, we construct tree-
structured groups, one per cluster (i.e., one per
node in the hierarchy). The Brown cluster regu-
larizer is:
</bodyText>
<equation confidence="0.98669">
Ωbrown(w) = ENv=1 Avkwvk2,
</equation>
<bodyText confidence="0.999791615384616">
where v ranges over the N nodes in the Brown
cluster tree. As a tree structured regularizer, this
regularizer enforces constraints that a node v’s
group is given nonzero weights only if those nodes
that dominate v (i.e., are on a path from v to the
root) have their groups selected.
Consider a similar toy example to the LDA reg-
ularizer (sports vs. science) and the hierarchical
clustering of words in Figure 2. In this case, the
Brown cluster regularizer will create 17 groups,
one for every node in the clustering tree. The regu-
larizer for this tree (omitting the group coefficients
and A) is:
</bodyText>
<equation confidence="0.99761">
Ωbrown(w) = E7i=0 kwvik2 + |wgoal |+ |wstriker|
+ |wmidfielder |+ |wknee |+ |winjury|
+ |wgravity |+ |wmoon |+ |wsun|
</equation>
<bodyText confidence="0.9997260625">
The regularizer penalizes words in a cluster to-
gether, exploiting discovered syntactic related-
ness. Additionally, the regularizer can zero out
weights of words corresponding to any of the in-
ternal nodes, such as v7 if the words monday and
sunday are deemed irrelevant to prediction.
Note that the regularizer already includes terms
like the lasso naturally. Similar to the parse
tree regularizer, for the lasso-like penalty on each
word, we tune one group weight for all word types
on a development data with a hyperparameter Alas.
A key difference between the Brown cluster
regularizer and the parse tree regularizer is that
there is only one tree for Brown cluster regularizer,
whereas the parse tree regularizer can have mil-
lions (one per sentence in the training data). The
</bodyText>
<note confidence="0.747444">
v1 v5
</note>
<figureCaption confidence="0.7460806">
Figure 2: An illustrative example of Brown clusters for N =
9. The Brown cluster regularizer constructs 17 groups, one
per node in for this tree, v0, v1, ... , v16. v0 contains 8 words,
v1 contains 5, etc. Note that the leaves, vs, v9, ... ,v16, each
contain one word.
</figureCaption>
<bodyText confidence="0.997056857142857">
LDA and Brown cluster regularizers offer ways to
incorporate unlabeled data, if we believe that the
unlabeled data can help us infer better topics or
clusters. Note that the processes of learning topics
or clusters, or parsing training data sentences, are
a separate stage that precedes learning our predic-
tive model.
</bodyText>
<sectionHeader confidence="0.993042" genericHeader="method">
5 Learning
</sectionHeader>
<bodyText confidence="0.999823142857143">
There are many optimization methods for learn-
ing models with structured regularizers, particu-
lary group lasso (Jacob et al., 2009; Jenatton et al.,
2011; Chen et al., 2011; Qin and Goldfarb, 2012;
Yuan et al., 2013). We choose the optimization
method of Yogatama and Smith (2014) since it
handles millions of overlapping groups effectively.
The method is based on the alternating directions
method of multipliers (ADMM; Hestenes, 1969;
Powell, 1969). We review it here in brief, for com-
pleteness, and show how it can be applied to tree-
structured regularizers (such as the parse tree and
Brown cluster regularizers in §4) in particular.
Our learning problem is, generically:
</bodyText>
<equation confidence="0.959816">
Ω(w) + EDd=1 L(xd, w, yd).
</equation>
<bodyText confidence="0.999775666666667">
Separating the lasso-like penalty for each word
type from our group regularizers, we can rewrite
this problem as:
</bodyText>
<equation confidence="0.895645">
min Ωlas(w) + Ωglas(v) + ED d=1 L(xd, w, yd)
W,v
s.t. v = Mw
</equation>
<bodyText confidence="0.999869666666667">
where v consists of copies of the elements of
w. Notice that we work directly on w instead
of the copies for the lasso-like penalty, since it
does not have overlaps and has its own hyper-
parameters Alas. For the remaining groups with
size greater than one, we create copies v of size
</bodyText>
<figure confidence="0.584881142857143">
min
W
v2 v4 v6 v7
v3 v10 v11 v12 v13 v14 v15 v16
v8 v9 midfielder knee injury moon sun monday sunday
goal
striker
</figure>
<page confidence="0.921089">
790
</page>
<bodyText confidence="0.9866126">
L = EGg=1 size(g). M E 10,11LxV is a ma-
trix whose 1s link elements of w to their copies.7
We now have a constrained optimization prob-
lem, from which we can create an augmented La-
grangian problem; let u be the Lagrange variables:
</bodyText>
<equation confidence="0.9937765">
Qlas(w) + Qglas(v) + L(w)
+ uT(v − Mw) + ρ211v − Mw1122
</equation>
<bodyText confidence="0.997948">
ADMM proceeds by iteratively updating each
of w, v, and u, amounting to the following sub-
problems:
</bodyText>
<equation confidence="0.99987275">
Ωlas(w) +,C(w) − u�Mw + p2kv − Mwk22 (1)
Ωglas(v) + uTv + p2 kv − Mwk2 (2)
2
u = u + p(v − Mw) (3)
</equation>
<bodyText confidence="0.998761076923077">
Yogatama and Smith (2014) show that Eq. 1
can be rewritten in a form quite similar to E2-
regularized loss minimization.8
Eq. 2 is the proximal operator of 1 ρQglas ap-
plied to Mw − uρ. As such, it depends on the
form of M. Note that when applied to the col-
lection of “copies” of the parameters, v, Qglas no
longer has overlapping groups. Defined Mg as
the rows of M corresponding to weight copies as-
signed to group g. Let zg °= Mgw − ugρ . De-
note Ag = Aglas V/size(g). The problem can be
solved by applying the proximal operator used in
non-overlapping group lasso to each subvector:
</bodyText>
<equation confidence="0.9971848">
vg = prox λg(zg)
Qglas, ρ
0 if 11zg112 &lt; λgρ
llzgll2− ρg z otherwise.
llzgll2 g
</equation>
<bodyText confidence="0.9978175">
For a tree structured regularizer, we can get
speedups by working from the root node towards
the leaf nodes when applying the proximal oper-
ator in the second step. If g is a node in a tree
which is driven to zero, all of its children h that
has Ah &lt; Ag will also be driven to zero.
Eq. 3 is a simple update of the dual variable u.
Algorithm 1 summarizes our learning procedure.9
</bodyText>
<footnote confidence="0.941544833333333">
7For the parse tree regularizer, L is the sum, over all
training-data word tokens t, of the number of constituents t
belongs to. For the LDA regularizer, L = R × K. For the
Brown cluster regularizer, L = V − 1.
8The difference lies in that the squared f2 norm in the
penalty penalizes the difference between w and a vector that
depends on the current values of u and v. This does not affect
the algorithm or its convergence in any substantive way.
9We use relative changes in the f2 norm of the parameter
vector w as our convergence criterion (threshold of 10−3),
and set the maximum number of iterations to 100. Other cri-
teria can also be used.
</footnote>
<construct confidence="0.302166">
Algorithm 1 ADMM for overlapping group lasso
</construct>
<bodyText confidence="0.589414666666667">
Input: augmented Lagrangian variable p, regularization
strengths Aglas and Alas
while stopping criterion not met do
</bodyText>
<equation confidence="0.80003975">
Ωlas(w)+,G(w)+p PV i=1 Ni(wi−µi)2
2
for g = 1 to G do
vg = prox λg (zg)
Oglas , ρ
end for
u = u + p(v − Mw)
end while
</equation>
<sectionHeader confidence="0.973774" genericHeader="method">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.939614">
6.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999875363636364">
We use publicly available datasets to evaluate our
model described in more detail below.
Topic classification. We consider four binary
categorization tasks from the 20 Newsgroups
dataset.10 Each task involves categorizing a
document according to two related categories:
comp.sys: ibm.pc.hardware vs. mac.hardware;
rec.sport: baseball vs. hockey; sci: med vs. space;
and alt.atheism vs. soc.religion.christian.
Sentiment analysis. One task in sentiment anal-
ysis is predicting the polarity of a piece of text, i.e.,
whether the author is favorably inclined toward a
(usually known) subject of discussion or proposi-
tion (Pang and Lee, 2008). Sentiment analysis,
even at the coarse level of polarity we consider
here, can be confused by negation, stylistic use of
irony, and other linguistic phenomena. Our sen-
timent analysis datasets consist of movie reviews
from the Stanford sentiment treebank (Socher et
al., 2013),11 and floor speeches by U.S. Congress-
men alongside “yea”/“nay” votes on the bill under
discussion (Thomas et al., 2006).12 For the Stan-
ford sentiment treebank, we only predict binary
classifications (positive or negative) and exclude
neutral reviews.
Text-driven forecasting. Forecasting from text
requires identifying textual correlates of a re-
sponse variable revealed in the future, most of
which will be weak and many of which will be
spurious (Kogan et al., 2009). We consider two
such problems. The first one is predicting whether
a scientific paper will be cited or not within three
years of its publication (Yogatama et al., 2011);
</bodyText>
<footnote confidence="0.997203666666667">
10http://qwone.com/-jason/20Newsgroups
11http://nlp.stanford.edu/sentiment/
12http://www.cs.cornell.edu/-ainur/data.html
</footnote>
<figure confidence="0.7276764">
min
W
min
V
⎧
⎨
⎩
=
w = arg min
W
</figure>
<page confidence="0.987833">
791
</page>
<table confidence="0.999325444444445">
Dataset D # Dev. # Test V
Fore. Sent. 20N science 952 235 790 30,154
sports 958 239 796 20,832
relig. 870 209 717 24,528
comp. 929 239 777 20,868
movie 6,920 872 1,821 17,576
vote 1,175 257 860 24,508
science 3,207 280 539 42,702
bill 37,850 7,341 6,571 10,001
</table>
<tableCaption confidence="0.999512">
Table 2: Descriptive statistics about the datasets.
</tableCaption>
<bodyText confidence="0.994005555555556">
the dataset comes from the ACL Anthology and
consists of research papers from the Association
for Computational Linguistics and citation data
(Radev et al., 2009). The second task is predicting
whether a legislative bill will be recommended by
a Congressional committee (Yano et al., 2012).13
Table 2 summarizes statistics about the datasets
used in our experiments. In total, we evaluate our
method on eight binary classification tasks.
</bodyText>
<subsectionHeader confidence="0.99642">
6.2 Setup
</subsectionHeader>
<bodyText confidence="0.982374129032258">
In all our experiments, we use unigram features
plus an additional bias term which is not regu-
larized. We compare our new regularizers with
state-of-the-art methods for document classifica-
tion: lasso, ridge, and elastic net regularization, as
well as the sentence regularizer discussed in §4.1
(Yogatama and Smith, 2014).14
We parsed all corpora using the Berkeley parser
(Petrov and Klein, 2007).15 For the LDA regular-
izers, we ran LDA16 on training documents with
K = 1, 000 and R = 10. For the Brown cluster
regularizers, we ran Brown clustering17 on train-
ing documents with 5, 000 clusters for the topic
classification and sentiment analysis datasets, and
1, 000 for the larger text forecasting datasets (since
they are bigger datasets that took more time).
13http://www.ark.cs.cmu.edu/bills
14Hyperparameters are tuned on a separate develop-
ment dataset, using accuracy as the evaluation crite-
rion. For lasso and ridge models, we choose A from
{10−2, 10−1, 1, 10, 102, 103}. For elastic net, we perform
grid search on the same set of values as ridge and lasso
experiments for Arid and Alas. For the sentence, Brown
cluster, and LDA regularizers, we perform grid search on
the same set of values as ridge and lasso experiments for
p, Aglas, Alas. For the parse tree regularizer, because there
are many more groups than other regularizers, we choose
Aglas from {10−4, 10−3, 10−2, 10−1, 10}, p and Alas from
the same set of values as ridge and lasso experiments. If there
is a tie on development data we choose the model with the
smallest number of nonzero weights.
</bodyText>
<footnote confidence="0.999886666666667">
15https://code.google.com/p/berkeleyparser/
16http://www.cs.princeton.edu/˜blei/lda-c/
17https://github.com/percyliang/brown-cluster
</footnote>
<subsectionHeader confidence="0.95088">
6.3 Results
</subsectionHeader>
<bodyText confidence="0.999949363636364">
Table 3 shows the results of our experiments on
the eight datasets. The results demonstrate the su-
periority of structured regularizers. One of them
achieved the best result on all but one dataset.18 It
is also worth noting that in most cases all variants
of the structured regularizers outperformed lasso,
ridge, and elastic net. In four cases, the new regu-
larizers in this paper outperform the sentence reg-
ularizer.
We can see that the parse tree regularizer per-
formed the best for the movie review dataset. The
task is to predict sentence-level sentiment, so each
training example is a sentence. Since constituent-
level annotations are available for this dataset, we
only constructed groups for neutral constituents
(i.e., we drive neutral constituents to zero during
training). It has been shown that syntactic in-
formation is helpful for sentence-level predictions
(Socher et al., 2013), so the parse tree regularizer
is naturally suitable for this task.
The Brown cluster and LDA regularizers per-
formed best for the forecasting scientific articles
dataset. The task is to predict whether an article
will be cited or not within three years after publi-
cation. Regularizers that exploit the knowledge of
semantic relations (e.g., topical categories), such
as the Brown cluster and LDA regularizers, are
therefore suitable for this type of prediction.
Table 4 shows model sizes obtained by each
of the regularizers for each dataset. While lasso
prunes more aggressively, it almost always per-
forms worse. Our structured regularizers were
able to obtain a significantly smaller model (27%,
34%, 19% as large on average for parse tree,
Brown, and LDA regularizers respectively) com-
pared to the ridge model.
Topic and cluster features. Another way to in-
corporate LDA topics and Brown clusters into a
linear model is by adding them as additional fea-
tures. For the 20N datasets, we also ran lasso,
ridge, and elastic net with additional LDA topic
and Brown cluster features.19 Note that these new
baselines use more features than our model. We
can also add these additional features to our model
</bodyText>
<footnote confidence="0.923632571428571">
18This “bill” dataset, where they offered no improvement,
is the largest by far (37,850 documents), and therefore the
one where regularizers should matter the least. Note that the
differences are small across regularizers for this dataset.
19For LDA, we took the top 10 words in a topic as a feature.
For Brown clusters, we add a cluster as an additional feature
if its size is less than 50.
</footnote>
<page confidence="0.985999">
792
</page>
<tableCaption confidence="0.961989">
Table 3:
</tableCaption>
<table confidence="0.96561018367347">
Classification
accuracies on
various datasets.
“m.f.c.” is the
most frequent
class baseline.
Boldface shows
best results.
Task Dataset Accuracy (%)
m.f.c. lasso ridge elastic sentence parse Brown LDA
20N science 50.13 90.63 91.90 91.65 96.20 92.66 93.04 93.67
sports 50.13 91.08 93.34 93.71 95.10 93.09 93.71 94.97
religion 55.51 90.52 92.47 92.47 92.75 94.98 92.89 93.03
computer 50.45 85.84 86.74 87.13 90.86 89.45 86.36 88.42
Sentiment movie 50.08 78.03 80.45 80.40 80.72 81.55 80.34 78.36
vote 58.37 73.14 72.79 72.79 73.95 73.72 66.86 73.14
Forecasting science 50.28 64.00 66.79 66.23 67.71 66.42 69.02 69.39
bill 87.40 88.36 87.70 88.48 88.11 87.98 88.20 88.27
Task Dataset Model size (%)
m.f.c. lasso ridge elastic sentence parse Brown LDA
20N science - 1 100 34 12 2 42 9
sports - 2 100 15 3 3 16 9
religion - 0.3 100 48 94 72 41 15
computer - 2 100 24 10 5 24 8
Sentiment movie - 10 100 54 83 87 59 12
vote - 2 100 44 6 2 30 4
Forecasting science - 31 100 43 99 9 50 90
bill - 7 100 7 8 37 7 7
Table 4: Model
sizes (percentages
of nonzero
features in the
resulting models)
on various
datasets.
Dataset + LDA features LDA
reg.
lasso ridge elastic
science 90.63 91.90 91.90 93.67
sports 91.33 93.47 93.84 94.97
religion 91.35 92.47 91.35 93.03
computer 85.20 86.87 86.35 88.42
Dataset + Brown features Brown
reg.
lasso ridge elastic
science 86.96 90.51 91.14 93.04
sports 82.66 88.94 85.43 93.71
religion 94.98 96.93 96.93 92.89
computer 55.72 96.65 67.57 86.36
</table>
<tableCaption confidence="0.9280535">
Table 5: Classification accuracies on the 20N datasets for
lasso, ridge, and elastic net models with additional LDA fea-
tures (top) and Brown cluster features (bottom). The last col-
umn shows structured regularized models from Table 3.
</tableCaption>
<bodyText confidence="0.999945933333333">
and treat them as regular features (i.e., they do
not belong to any groups and are regularized with
standard regularizer such as the lasso penalty).
The results in Table 5 show that for these datasets,
models that incorporate this information through
structured regularizers outperformed models that
encode this information as additional features in
4 out 4 of cases (LDA) and 2 out of 4 cases
(Brown). Sparse models with Brown clusters ap-
pear to overfit badly; recall that the clusters were
learned on only the training data—clusters from
a larger dataset would likely give stronger re-
sults. Of course, better performance might also
be achieved by incorporating new features as well
as using structured regularizers.
</bodyText>
<subsectionHeader confidence="0.986064">
6.4 Examples
</subsectionHeader>
<bodyText confidence="0.999971736842105">
To gain an insight into the models, we inspect
group sparsity patterns in the learned models by
looking at the parameter copies v. This lets us see
which groups are considered important (i.e., “se-
lected” vs. “removed”). For each of the proposed
regularizers, we inspect the model a task in which
it performed well.
For the parse tree regularizer, we inspect the
model for the 20N:religion task. We observed that
the model included most of the sentences (root
node groups), but in some cases removed phrases
from the parse trees, such as ozzy osbourne in the
sentence ozzy osbourne , ex-singer and main char-
acter of the black sabbath of good ole days past,
is and always was a devout catholic.
For the LDA regularizer, we inspect zero and
nonzero groups (topics) in the forecasting scien-
tific articles task. In this task, we observed that
642 out of 1,000 topics are driven to zero by
our model. Table 6 shows examples of zero and
nonzero topics for the dev.-tuned hyperparameter
values. We can see that in this particular case, the
model kept meaningful topics such as parsing and
speech processing, and discarded general topics
that are not correlated with the content of the pa-
pers (e.g., acknowledgment, document metadata,
equation, etc.). Note that most weights for non-
selected groups, even in w, are near zero.
For the Brown cluster regularizer, we inspect
the model from the 20N:science task. 771 out
of 5,775 groups were driven to zero for the best
model tuned on the development set. Examples
of zero and nonzero groups are shown in Ta-
ble 7. Similar to the LDA example, the groups
that were driven to zero tend to contain generic
words that are not relevant to the predictions. We
can also see the tree structure effect in the regu-
larizer. The group {underwater, industrial} was
</bodyText>
<page confidence="0.998123">
793
</page>
<tableCaption confidence="0.8711125">
“acknowledgment”: workshop arpa program session darpa research papers spoken technology systems
“document metadata”: university references proceedings abstract work introduction new been research both
“equation”: pr w h probability wi gram context z probabilities complete
“translation”: translation target source german english length alignment hypothesis translations position
“translation”: korean translation english rules sentences parsing input evaluation machine verb
“speech processing”: speaker identification topic recognition recognizer models acoustic test vocabulary independent
“parsing”: parser parsing probabilistic prediction parse pearl edges chart phase theory
“classification”: documents learning accuracy bayes classification wt document naive method selection
Table 6: Examples of LDA regularizer-removed and -selected groups (in v) in the forecasting scientific articles dataset. Words
with weights (in w) of magnitude greater than 10−3 are highlighted in red (not cited) and blue (cited).
</tableCaption>
<figure confidence="0.633394714285714">
= 0
=�0
underwater industrial
spotted hit reaped rejuvenated destroyed stretched undertake shake run
seeing developing tingles diminishing launching finding investigating receiving
maintaining
adds engage explains builds
failure reproductive ignition reproduction
cyanamid planetary nikola fertility astronomical geophysical # lunar cometary
=�0 supplying astronautical
magnetic atmospheric
std underwater hpr wordscan exclusively aneutronic industrial peoples obsessive
congenital rare simple bowel hereditary breast
= 0
</figure>
<figureCaption confidence="0.984265444444445">
Table 7: Examples of Brown
regularizer-removed and
-selected groups (in v) in the
20N:science task. # denotes
any numeral. Words with
weights (in w) of magnitude
greater than 10−3 are
highlighted in red (space) and
blue (medical).
</figureCaption>
<bodyText confidence="0.999652857142857">
driven to zero, but not once it combined with other
words such as hpr, std, obsessive. Note that we
ran Brown clustering only on the training docu-
ments; running it on a larger collection of (unla-
beled) documents relevant to the prediction task
(i.e., semi-supervised learning) is worth exploring
in future work.
</bodyText>
<sectionHeader confidence="0.998925" genericHeader="evaluation">
7 Related and Future Work
</sectionHeader>
<bodyText confidence="0.99989775">
Overall, our results demonstrate that linguistic
structure in the data can be used to improve bag-
of-words models, through structured regulariza-
tion. State-of-the-art approaches to some of these
problems have used additional features and repre-
sentations (Yessenalina et al., 2010; Socher et al.,
2013). For example, for the vote sentiment analy-
sis datasets, latent variable models of Yessenalina
et al. (2010) achieved a superior result of 77.67%.
To do so, they sacrificed convexity and had to rely
on side information for initialization. Our exper-
imental focus has been on a controlled compari-
son between regularizers for a fixed model family
(the simplest available, linear with bag-of-words
features). However, the improvements offered by
our regularization methods can be applied in fu-
ture work to other model families with more care-
fully engineered features, metadata features (espe-
cially important in forecasting), latent variables,
etc. In particular, note that other kinds of weights
(e.g., metadata) can be penalized conventionally,
or incorporated into the structured regularization
where it makes sense to do so (e.g., n-grams, as in
Nelakanti et al., 2013).
</bodyText>
<sectionHeader confidence="0.998455" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.9999396">
We introduced three data-driven, linguistically
informed structured regularizers based on parse
trees, topics, and hierarchical word clusters. We
empirically showed that models regularized us-
ing our methods consistently outperformed stan-
dard regularizers that penalize features in isolation
such as lasso, ridge, and elastic net on a range
of datasets for various text prediction problems:
topic classification, sentiment analysis, and fore-
casting.
</bodyText>
<sectionHeader confidence="0.99756" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999836555555556">
The authors thank Brendan O’Connor for help
with visualization and three anonymous review-
ers for helpful feedback on an earlier draft of this
paper. This research was supported in part by
computing resources provided by a grant from the
Pittsburgh Supercomputing Center, a Google re-
search award, and the Intelligence Advanced Re-
search Projects Activity via Department of In-
terior National Business Center contract number
D12PC00347. The U.S. Government is authorized
to reproduce and distribute reprints for Govern-
mental purposes notwithstanding any copyright
annotation thereon. The views and conclusions
contained herein are those of the authors and
should not be interpreted as necessarily represent-
ing the official policies or endorsements, either ex-
pressed or implied, of IARPA, DoI/NBC, or the
U.S. Government.
</bodyText>
<page confidence="0.993191">
794
</page>
<bodyText confidence="0.955163">
Matthieu Kowalski and Bruno Torresani. 2009. Spar-
sity and persistence: mixed norms provide simple
signal models with dependent coefficients. Signal,
Image and Video Processing, 3(3):251–0264.
</bodyText>
<sectionHeader confidence="0.933993" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999463908163266">
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18:467–479.
Stanley F. Chen and Ronald Rosenfeld. 2000. A
survey of smoothing techniques for me models.
IEEE Transactions on Speech and Audio Process-
ing, 8(1):37–50.
Xi Chen, Qihang Lin, Seyoung Kim, Jaime G. Car-
bonell, and Eric P. Xing. 2011. Smoothing prox-
imal gradient method for general structured sparse
learning. In Proc. of UAI.
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.
2011. Discovering sociolinguistic associations with
structured sparsity. In Proc. of ACL.
Mario A. T. Figueiredo. 2002. Adaptive sparseness
using Jeffreys’ prior. In Proc. of NIPS.
Jerome Friedman, Trevor Hastie, and Robert Tibshiran.
2010. A note on the group lasso and a sparse group
lasso. Technical report, Stanford University.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Proc. of
ICML.
Magnus R. Hestenes. 1969. Multiplier and gradient
methods. Journal of Optimization Theory and Ap-
plications, 4:303–320.
Arthur E. Hoerl and Robert W. Kennard. 1970. Ridge
regression: Biased estimation for nonorthogonal
problems. Technometrics, 12(1):55–67.
Laurent Jacob, Guillaume Obozinski, and Jean-
Philippe Vert. 2009. Group lasso with overlap and
graph lasso. In Proc. of ICML.
Rodolphe Jenatton, Jean-Yves Audibert, and Fran-
cis Bach. 2011. Structured variable selection
with sparsity-inducing norms. Journal of Machine
Learning Research, 12:2777–2824.
Mahesh Joshi, Dipanjan Das, Kevin Gimpel, and
Noah A. Smith. 2010. Movie reviews and rev-
enues: An experiment in text regression. In Proc.
of NAACL.
Seyoung Kim and Eric P. Xing. 2008. Feature selec-
tion via block-regularized regression. In Proc. of
UAI.
Shimon Kogan, Dimitry Levin, Bryan R. Routledge,
Jacob S. Sagi, and Noah A. Smith. 2009. Predicting
risk from financial reports with regression. In Proc.
of HLT-NAACL.
Xiaolei Lv, Guoan Bi, and Chunru Wan. 2011. The
group lasso for stable recovery of block-sparse sig-
nal representations. IEEE Transactions on Signal
Processing, 59(4):1371–1382.
Andre F. T. Martins, Noah A. Smith, Pedro M. Q.
Aguiar, and Mario A. T. Figueiredo. 2011. Struc-
tured sparsity in structured prediction. In Proc. of
EMNLP.
Anil Nelakanti, Cedric Archambeau, Julien Mairal,
Francis Bach, and Guillaume Bouchard. 2013.
Structured penalties for log-linear language models.
In Proc. of EMNLP.
Kamal Nigam, Andrew McCallum, Sebastian Thrun,
and Tom Mitchell. 2000. Text classification from la-
beled and unlabeled documents using em. Machine
Learning, 39(2-3):103–134.
Bo Pang and Lilian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1–2):1–135.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proc. of HLT-NAACL.
M. J. D. Powell. 1969. A method for nonlinear con-
straints in minimization problems. In R. Fletcher,
editor, Optimization, pages 283–298. Academic
Press.
Zhiwei (Tony) Qin and Donald Goldfarb. 2012. Struc-
tured sparsity via alternating direction methods.
Journal of Machine Learning Research, 13:1435–
1468.
Dragomir R. Radev, Pradeep Muthukrishnan, and Va-
hed Qazvinian. 2009. The ACL anthology net-
work corpus. In Proc. of ACL Workshop on Natural
Language Processing and Information Retrieval for
Digital Libraries.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Chris Manning, Andrew Ng, and Chris
Potts. 2013. Recursive deep models for semantic
compositionality over a sentiment treebank. In Proc.
of EMNLP.
Jiang Su, Jelber Sayyad-Shirabad, and Stan Matwin.
2011. Large scale text classication using semi-
supervised multinomial naive Bayes. In Proc. of
ICML.
Matt Thomas, Bo Pang, and Lilian Lee. 2006. Get out
the vote: Determining support or opposition from
congressional floor-debate transcripts. In Proc. of
EMNLP.
</reference>
<page confidence="0.980854">
795
</page>
<reference confidence="0.99893234375">
Robert Tibshirani, Michael Saunders, Saharon Ros-
set, Ji Zhu, and Keith Knight. 2005. Sparsity and
smoothness via the fused lasso. Journal of Royal
Statistical Society B, 67(1):91–108.
Robert Tibshirani. 1996. Regression shrinkage and
selection via the lasso. Journal of Royal Statistical
Society B, 58(1):267–288.
Tae Yano, Noah A. Smith, and John D. Wilkerson.
2012. Textual predictors of bill survival in congres-
sional committees. In Proc. of NAACL.
Ainur Yessenalina, Yisong Yue, and Claire Cardie.
2010. Multi-level structured models for document
sentiment classification. In Proc. of EMNLP.
Dani Yogatama and Noah A. Smith. 2014. Making the
most of bag of words: Sentence regularization with
alternating direction method of multipliers. In Proc.
of ICML.
Dani Yogatama, Michael Heilman, Brendan O’Connor,
Chris Dyer, Bryan R. Routledge, and Noah A.
Smith. 2011. Predicting a scientific community’s
response to an article. In Proc. of EMNLP.
Ming Yuan and Yi Lin. 2006. Model selection
and estimation in regression with grouped variables.
Journal of the Royal Statistical Society, Series B,
68(1):49–67.
Lei Yuan, Jun Liu, and Jieping Ye. 2013. Efficient
methods for overlapping group lasso. IEEE Trans-
actions on Pattern Analysis and Machine Intelli-
gence, 35(9):2104–2116.
Hui Zou and Trevor Hastie. 2005. Regularization and
variable selection via the elastic net. Journal of the
Royal Statistical Society, Series B, 67:301–320.
</reference>
<page confidence="0.998514">
796
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.625200">
<title confidence="0.999897">Linguistic Structured Sparsity in Text Categorization</title>
<author confidence="0.995176">Dani</author>
<affiliation confidence="0.96369">Language Technologies School of Computer Carnegie Mellon</affiliation>
<address confidence="0.979361">Pittsburgh, PA 15213,</address>
<email confidence="0.999264">dyogatama@cs.cmu.edu</email>
<abstract confidence="0.981638411764706">We introduce three linguistically motivated structured regularizers based on parse trees, topics, and hierarchical word clusters for text categorization. These regularizers impose linguistic bias in feature weights, enabling us to incorporate prior knowledge into conventional bagof-words models. We show that our structured regularizers consistently improve classification accuracies compared to standard regularizers that penalize features in isolation (such as lasso, ridge, and elastic net regularizers) on a range of datasets for various text prediction problems: topic classification, sentiment analysis, and forecasting.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="14501" citStr="Blei et al., 2003" startWordPosition="2376" endWordPosition="2379">. For example, if we want to predict whether a paper will be cited or not (Yogatama et al., 2011), the model can perform better if it knows beforehand the collections of words that represent certain themes (e.g., in ACL papers, these might include machine translation, parsing, etc.). As a result, the model can focus on which topics will increase the probability of getting citations, and penalize weights for words in the same topic together, instead of treating each word separately. We do this by inferring topics in the training corpus by estimating the latent Dirichlet allocation (LDA) model (Blei et al., 2003)). Note that LDA is an unsupervised method, so we can infer topical structures from any collection of documents that are considered related to the target corpus (e.g., training documents, text from the web, etc.). This contrasts with typical semi-supervised learning methods for text categorization that combine unlabeled and labeled data within a generative model, such as multinomial naive Bayes, via expectation-maximization (Nigam et al., 2000) or semi-supervised frequency estimation (Su et al., 2011). Our method does not use unlabeled data to obtain more training documents or estimate the joi</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--467</pages>
<contexts>
<context position="18040" citStr="Brown et al., 1992" startWordPosition="2987" endWordPosition="2990">ts own without regard for related word types. Unlike the parse tree regularizer, the LDA regularizer is not tree structured. Since the lasso-like penalty does not occur naturally in a non treestructured regularizer, we add an additional lasso penalty for each word type (with hyperparameter Alas) to also encourage weights of irrelevant words to go to zero. Our LDA regularizer is an instance of sparse group lasso (Friedman et al., 2010). not to work well. 789 v0 4.4 Brown Cluster Regularizer Brown clustering is a commonly used unsupervised method for grouping words into a hierarchy of clusters (Brown et al., 1992). Because it uses local information, it tends to discover words with similar syntactic behavior, though semantic groupings are often evident, especially at the more finegrained end of the hierarchy. We incorporate Brown clusters into a regularizer in a similar way to the topical word groups inferred using LDA in §4.3, but here we make use of the hierarchy. Specifically, we construct treestructured groups, one per cluster (i.e., one per node in the hierarchy). The Brown cluster regularizer is: Ωbrown(w) = ENv=1 Avkwvk2, where v ranges over the N nodes in the Brown cluster tree. As a tree struct</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18:467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A survey of smoothing techniques for me models.</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="1496" citStr="Chen and Rosenfeld (2000)" startWordPosition="209" endWordPosition="212">ay to exploit linguistic information in statistical text processing models? For tasks like text classification, sentiment analysis, and text-driven forecasting, this is an open question, as cheap “bag-of-words” models often perform well. Much recent work in NLP has focused on linguistic feature engineering (Joshi et al., 2010) or representation learning (Glorot et al., 2011; Socher et al., 2013). In this paper, we propose a radical alternative. We embrace the conventional bag-of-words representation of text, instead bringing linguistic bias to bear on regularization. Since the seminal work of Chen and Rosenfeld (2000), the importance of regularization in discriminative models of text— including language modeling, structured prediction, and classification—has been widely recognized. The emphasis, however, has largely been on one specific kind of inductive bias: avoiding large weights (i.e., coefficients in a linear model). Recently, structured (or composite) regularization has been introduced; simply put, it reasons Noah A. Smith Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA nasmith@cs.cmu.edu about different weights jointly. The most widely </context>
</contexts>
<marker>Chen, Rosenfeld, 2000</marker>
<rawString>Stanley F. Chen and Ronald Rosenfeld. 2000. A survey of smoothing techniques for me models. IEEE Transactions on Speech and Audio Processing, 8(1):37–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xi Chen</author>
<author>Qihang Lin</author>
<author>Seyoung Kim</author>
<author>Jaime G Carbonell</author>
<author>Eric P Xing</author>
</authors>
<title>Smoothing proximal gradient method for general structured sparse learning.</title>
<date>2011</date>
<booktitle>In Proc. of UAI.</booktitle>
<contexts>
<context position="20832" citStr="Chen et al., 2011" startWordPosition="3460" endWordPosition="3463">, ... , v16. v0 contains 8 words, v1 contains 5, etc. Note that the leaves, vs, v9, ... ,v16, each contain one word. LDA and Brown cluster regularizers offer ways to incorporate unlabeled data, if we believe that the unlabeled data can help us infer better topics or clusters. Note that the processes of learning topics or clusters, or parsing training data sentences, are a separate stage that precedes learning our predictive model. 5 Learning There are many optimization methods for learning models with structured regularizers, particulary group lasso (Jacob et al., 2009; Jenatton et al., 2011; Chen et al., 2011; Qin and Goldfarb, 2012; Yuan et al., 2013). We choose the optimization method of Yogatama and Smith (2014) since it handles millions of overlapping groups effectively. The method is based on the alternating directions method of multipliers (ADMM; Hestenes, 1969; Powell, 1969). We review it here in brief, for completeness, and show how it can be applied to treestructured regularizers (such as the parse tree and Brown cluster regularizers in §4) in particular. Our learning problem is, generically: Ω(w) + EDd=1 L(xd, w, yd). Separating the lasso-like penalty for each word type from our group re</context>
</contexts>
<marker>Chen, Lin, Kim, Carbonell, Xing, 2011</marker>
<rawString>Xi Chen, Qihang Lin, Seyoung Kim, Jaime G. Carbonell, and Eric P. Xing. 2011. Smoothing proximal gradient method for general structured sparse learning. In Proc. of UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Discovering sociolinguistic associations with structured sparsity.</title>
<date>2011</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="2385" citStr="Eisenstein et al., 2011" startWordPosition="338" endWordPosition="341">ghts (i.e., coefficients in a linear model). Recently, structured (or composite) regularization has been introduced; simply put, it reasons Noah A. Smith Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA nasmith@cs.cmu.edu about different weights jointly. The most widely explored variant, group lasso (Yuan and Lin, 2006) seeks to avoid large E2 norms for groups of weights. Group lasso has been shown useful in a range of applications, including computational biology (Kim and Xing, 2008), signal processing (Lv et al., 2011), and NLP (Eisenstein et al., 2011; Martins et al., 2011; Nelakanti et al., 2013). For text categorization problems, Yogatama and Smith (2014) proposed groups based on sentences, an idea generalized here to take advantage of richer linguistic information. In this paper, we show how linguistic information of various kinds—parse trees, thematic topics, and hierarchical word clusterings—can be used to construct group lasso variants that impose linguistic bias without introducing any new features. Our experiments demonstrate that structured regularizers can squeeze higher performance out of conventional bag-of-words models on seve</context>
<context position="6856" citStr="Eisenstein et al. (2011)" startWordPosition="1085" endWordPosition="1088"> the way to zero; for this reason its solutions are known as “weakly” sparse. 3Other structured regularizers include the fused lasso (Tibshirani et al., 2005) and the elitist lasso (Kowalski and Torresani, 2009). where Aglas is a hyperparameter tuned on a development data, and Ag is a group specific weight. Typically the groups are non-overlapping, which offers computational advantages, but this need not be the case (Jacob et al., 2009; Jenatton et al., 2011). 4 Structured Regularizers for Text Past work applying the group lasso to NLP problems has considered four ways of defining the groups. Eisenstein et al. (2011) defined groups of coefficients corresponding to the same independent variable applied to different (continuous) output variables in multi-output regression. Martins et al. (2011) defined groups based on feature templates used in chunking and parsing tasks. Nelakanti et al. (2013) defined groups based on ngram histories for language modeling. In each of these cases, the groups were defined based on information from feature types alone; given the features to be used, the groups were known. Here we build on a fourth approach that exploits structure in the data.4 Yogatama and Smith (2014) introdu</context>
</contexts>
<marker>Eisenstein, Smith, Xing, 2011</marker>
<rawString>Jacob Eisenstein, Noah A. Smith, and Eric P. Xing. 2011. Discovering sociolinguistic associations with structured sparsity. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mario A T Figueiredo</author>
</authors>
<title>Adaptive sparseness using Jeffreys’ prior.</title>
<date>2002</date>
<booktitle>In Proc. of NIPS.</booktitle>
<contexts>
<context position="5894" citStr="Figueiredo, 2002" startWordPosition="932" endWordPosition="933">ore here uses an E1,2 norm. Let g index the G predefined groups of weights and wg denote the subvector of w containing weights for group g: EG Ωglas(w) =Aglas g=1 Agkwgk2, 1A Bayesian interpretation of regularization is as a prior on the weight vector w; in many cases Ω can be understood as a log-prior representing beliefs about the model held before exposure to data. For lasso regression, the prior is a zero-mean Laplace distribution, whereas for ridge regression the prior is a zero-mean Gaussian distribution. For nonoverlapping group lasso, the prior is a two-level hierarchical Bayes model (Figueiredo, 2002). The Bayesian interpretation of overlapping group lasso is not yet well understood. 2The lasso leads to strongly sparse solutions, in which many elements of the estimated w are actually zero. This is an attractive property for efficiency and (perhaps) interpretability. The ridge encourages weights to go toward zero, but usually not all the way to zero; for this reason its solutions are known as “weakly” sparse. 3Other structured regularizers include the fused lasso (Tibshirani et al., 2005) and the elitist lasso (Kowalski and Torresani, 2009). where Aglas is a hyperparameter tuned on a develo</context>
</contexts>
<marker>Figueiredo, 2002</marker>
<rawString>Mario A. T. Figueiredo. 2002. Adaptive sparseness using Jeffreys’ prior. In Proc. of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome Friedman</author>
<author>Trevor Hastie</author>
<author>Robert Tibshiran</author>
</authors>
<title>A note on the group lasso and a sparse group lasso.</title>
<date>2010</date>
<tech>Technical report,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="10057" citStr="Friedman et al., 2010" startWordPosition="1638" endWordPosition="1641">ated as neutral. If the norm of wgd,s is driven to zero, then the learner has deemed the corresponding sentence irrelevant to the prediction. It is important to point out that, while the regularizer prefers to zero out the weights for all words in irrelevant sentences, it also prefers not to zero out weights for words in relevant sentences. Since the groups overlap and may work against each other, the regularizer may not be able to drive many weights to zero on its own. Yogatama and Smith (2014) used a linear combination of the sentence regularizer and the lasso (a kind of sparse group lasso; Friedman et al., 2010) to also encourage weights of irrelevant word types to go to zero.5 4.2 Parse Tree Regularizer Sentence boundaries are a rather superficial kind of linguistic structure; syntactic parse trees provide more fine-grained information. We introduce a new regularizer, the parse tree regularizer, in which groups are defined for every constituent in every parse of a training data sentence. Figure 1 illustrates the group structures derived from an example sentence from the Stanford sentiment treebank (Socher et al., 2013). This regularizer captures the idea that phrases might be selected as relevant or</context>
<context position="17859" citStr="Friedman et al., 2010" startWordPosition="2956" endWordPosition="2959">ero weights—even weights of the opposite sign as wligament. It is important to distinguish this from unstructured regularizers such as the lasso, which penalize each word’s weight on its own without regard for related word types. Unlike the parse tree regularizer, the LDA regularizer is not tree structured. Since the lasso-like penalty does not occur naturally in a non treestructured regularizer, we add an additional lasso penalty for each word type (with hyperparameter Alas) to also encourage weights of irrelevant words to go to zero. Our LDA regularizer is an instance of sparse group lasso (Friedman et al., 2010). not to work well. 789 v0 4.4 Brown Cluster Regularizer Brown clustering is a commonly used unsupervised method for grouping words into a hierarchy of clusters (Brown et al., 1992). Because it uses local information, it tends to discover words with similar syntactic behavior, though semantic groupings are often evident, especially at the more finegrained end of the hierarchy. We incorporate Brown clusters into a regularizer in a similar way to the topical word groups inferred using LDA in §4.3, but here we make use of the hierarchy. Specifically, we construct treestructured groups, one per cl</context>
</contexts>
<marker>Friedman, Hastie, Tibshiran, 2010</marker>
<rawString>Jerome Friedman, Trevor Hastie, and Robert Tibshiran. 2010. A note on the group lasso and a sparse group lasso. Technical report, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Glorot</author>
<author>Antoine Bordes</author>
<author>Yoshua Bengio</author>
</authors>
<title>Domain adaptation for large-scale sentiment classification: A deep learning approach.</title>
<date>2011</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="1247" citStr="Glorot et al., 2011" startWordPosition="171" endWordPosition="174"> that penalize features in isolation (such as lasso, ridge, and elastic net regularizers) on a range of datasets for various text prediction problems: topic classification, sentiment analysis, and forecasting. 1 Introduction What is the best way to exploit linguistic information in statistical text processing models? For tasks like text classification, sentiment analysis, and text-driven forecasting, this is an open question, as cheap “bag-of-words” models often perform well. Much recent work in NLP has focused on linguistic feature engineering (Joshi et al., 2010) or representation learning (Glorot et al., 2011; Socher et al., 2013). In this paper, we propose a radical alternative. We embrace the conventional bag-of-words representation of text, instead bringing linguistic bias to bear on regularization. Since the seminal work of Chen and Rosenfeld (2000), the importance of regularization in discriminative models of text— including language modeling, structured prediction, and classification—has been widely recognized. The emphasis, however, has largely been on one specific kind of inductive bias: avoiding large weights (i.e., coefficients in a linear model). Recently, structured (or composite) regu</context>
</contexts>
<marker>Glorot, Bordes, Bengio, 2011</marker>
<rawString>Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Domain adaptation for large-scale sentiment classification: A deep learning approach. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus R Hestenes</author>
</authors>
<title>Multiplier and gradient methods.</title>
<date>1969</date>
<journal>Journal of Optimization Theory and Applications,</journal>
<pages>4--303</pages>
<contexts>
<context position="21095" citStr="Hestenes, 1969" startWordPosition="3502" endWordPosition="3503"> clusters. Note that the processes of learning topics or clusters, or parsing training data sentences, are a separate stage that precedes learning our predictive model. 5 Learning There are many optimization methods for learning models with structured regularizers, particulary group lasso (Jacob et al., 2009; Jenatton et al., 2011; Chen et al., 2011; Qin and Goldfarb, 2012; Yuan et al., 2013). We choose the optimization method of Yogatama and Smith (2014) since it handles millions of overlapping groups effectively. The method is based on the alternating directions method of multipliers (ADMM; Hestenes, 1969; Powell, 1969). We review it here in brief, for completeness, and show how it can be applied to treestructured regularizers (such as the parse tree and Brown cluster regularizers in §4) in particular. Our learning problem is, generically: Ω(w) + EDd=1 L(xd, w, yd). Separating the lasso-like penalty for each word type from our group regularizers, we can rewrite this problem as: min Ωlas(w) + Ωglas(v) + ED d=1 L(xd, w, yd) W,v s.t. v = Mw where v consists of copies of the elements of w. Notice that we work directly on w instead of the copies for the lasso-like penalty, since it does not have ov</context>
</contexts>
<marker>Hestenes, 1969</marker>
<rawString>Magnus R. Hestenes. 1969. Multiplier and gradient methods. Journal of Optimization Theory and Applications, 4:303–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur E Hoerl</author>
<author>Robert W Kennard</author>
</authors>
<title>Ridge regression: Biased estimation for nonorthogonal problems.</title>
<date>1970</date>
<journal>Technometrics,</journal>
<volume>12</volume>
<issue>1</issue>
<contexts>
<context position="4362" citStr="Hoerl and Kennard, 1970" startWordPosition="666" endWordPosition="670">log loss: Z(xd, w, yd) = − log(1 + exp(−ydwTxd)), 786 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 786–796, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Other loss functions (e.g., hinge loss, squared loss) can also be used with any of the regularizers discussed in this paper. Our focus is on the regularizer, Ω(w). For high dimensional data such as text, regularization is crucial to avoid overfitting.1 The usual starting points for regularization are the “lasso” (Tibshirani, 1996) and the “ridge” (Hoerl and Kennard, 1970), based respectively on the E1 and squared E2 norms: Ωlas(w) = Alaskwk1 = AEj |wj| Ωrid(w) = Aridkwk22 = A Ej w2j Both methods disprefer weights of large magnitude; smaller (relative) magnitude means a feature (here, a word) has a smaller effect on the prediction, and zero means a feature has no effect.2 The hyperparameter A in each case is typically tuned on a development dataset. A linear combination of ridge and lasso is known as the elastic net (Zou and Hastie, 2005). The lasso, ridge, and elastic net are three strong baselines in our experiments. 3 Group Lasso Structured regularizers pena</context>
</contexts>
<marker>Hoerl, Kennard, 1970</marker>
<rawString>Arthur E. Hoerl and Robert W. Kennard. 1970. Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12(1):55–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurent Jacob</author>
<author>Guillaume Obozinski</author>
<author>JeanPhilippe Vert</author>
</authors>
<title>Group lasso with overlap and graph lasso.</title>
<date>2009</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="6671" citStr="Jacob et al., 2009" startWordPosition="1054" endWordPosition="1057">he estimated w are actually zero. This is an attractive property for efficiency and (perhaps) interpretability. The ridge encourages weights to go toward zero, but usually not all the way to zero; for this reason its solutions are known as “weakly” sparse. 3Other structured regularizers include the fused lasso (Tibshirani et al., 2005) and the elitist lasso (Kowalski and Torresani, 2009). where Aglas is a hyperparameter tuned on a development data, and Ag is a group specific weight. Typically the groups are non-overlapping, which offers computational advantages, but this need not be the case (Jacob et al., 2009; Jenatton et al., 2011). 4 Structured Regularizers for Text Past work applying the group lasso to NLP problems has considered four ways of defining the groups. Eisenstein et al. (2011) defined groups of coefficients corresponding to the same independent variable applied to different (continuous) output variables in multi-output regression. Martins et al. (2011) defined groups based on feature templates used in chunking and parsing tasks. Nelakanti et al. (2013) defined groups based on ngram histories for language modeling. In each of these cases, the groups were defined based on information f</context>
<context position="20790" citStr="Jacob et al., 2009" startWordPosition="3452" endWordPosition="3455">oups, one per node in for this tree, v0, v1, ... , v16. v0 contains 8 words, v1 contains 5, etc. Note that the leaves, vs, v9, ... ,v16, each contain one word. LDA and Brown cluster regularizers offer ways to incorporate unlabeled data, if we believe that the unlabeled data can help us infer better topics or clusters. Note that the processes of learning topics or clusters, or parsing training data sentences, are a separate stage that precedes learning our predictive model. 5 Learning There are many optimization methods for learning models with structured regularizers, particulary group lasso (Jacob et al., 2009; Jenatton et al., 2011; Chen et al., 2011; Qin and Goldfarb, 2012; Yuan et al., 2013). We choose the optimization method of Yogatama and Smith (2014) since it handles millions of overlapping groups effectively. The method is based on the alternating directions method of multipliers (ADMM; Hestenes, 1969; Powell, 1969). We review it here in brief, for completeness, and show how it can be applied to treestructured regularizers (such as the parse tree and Brown cluster regularizers in §4) in particular. Our learning problem is, generically: Ω(w) + EDd=1 L(xd, w, yd). Separating the lasso-like pe</context>
</contexts>
<marker>Jacob, Obozinski, Vert, 2009</marker>
<rawString>Laurent Jacob, Guillaume Obozinski, and JeanPhilippe Vert. 2009. Group lasso with overlap and graph lasso. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodolphe Jenatton</author>
<author>Jean-Yves Audibert</author>
<author>Francis Bach</author>
</authors>
<title>Structured variable selection with sparsity-inducing norms.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2777</pages>
<contexts>
<context position="6695" citStr="Jenatton et al., 2011" startWordPosition="1058" endWordPosition="1061">ctually zero. This is an attractive property for efficiency and (perhaps) interpretability. The ridge encourages weights to go toward zero, but usually not all the way to zero; for this reason its solutions are known as “weakly” sparse. 3Other structured regularizers include the fused lasso (Tibshirani et al., 2005) and the elitist lasso (Kowalski and Torresani, 2009). where Aglas is a hyperparameter tuned on a development data, and Ag is a group specific weight. Typically the groups are non-overlapping, which offers computational advantages, but this need not be the case (Jacob et al., 2009; Jenatton et al., 2011). 4 Structured Regularizers for Text Past work applying the group lasso to NLP problems has considered four ways of defining the groups. Eisenstein et al. (2011) defined groups of coefficients corresponding to the same independent variable applied to different (continuous) output variables in multi-output regression. Martins et al. (2011) defined groups based on feature templates used in chunking and parsing tasks. Nelakanti et al. (2013) defined groups based on ngram histories for language modeling. In each of these cases, the groups were defined based on information from feature types alone;</context>
<context position="20813" citStr="Jenatton et al., 2011" startWordPosition="3456" endWordPosition="3459">n for this tree, v0, v1, ... , v16. v0 contains 8 words, v1 contains 5, etc. Note that the leaves, vs, v9, ... ,v16, each contain one word. LDA and Brown cluster regularizers offer ways to incorporate unlabeled data, if we believe that the unlabeled data can help us infer better topics or clusters. Note that the processes of learning topics or clusters, or parsing training data sentences, are a separate stage that precedes learning our predictive model. 5 Learning There are many optimization methods for learning models with structured regularizers, particulary group lasso (Jacob et al., 2009; Jenatton et al., 2011; Chen et al., 2011; Qin and Goldfarb, 2012; Yuan et al., 2013). We choose the optimization method of Yogatama and Smith (2014) since it handles millions of overlapping groups effectively. The method is based on the alternating directions method of multipliers (ADMM; Hestenes, 1969; Powell, 1969). We review it here in brief, for completeness, and show how it can be applied to treestructured regularizers (such as the parse tree and Brown cluster regularizers in §4) in particular. Our learning problem is, generically: Ω(w) + EDd=1 L(xd, w, yd). Separating the lasso-like penalty for each word typ</context>
</contexts>
<marker>Jenatton, Audibert, Bach, 2011</marker>
<rawString>Rodolphe Jenatton, Jean-Yves Audibert, and Francis Bach. 2011. Structured variable selection with sparsity-inducing norms. Journal of Machine Learning Research, 12:2777–2824.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mahesh Joshi</author>
<author>Dipanjan Das</author>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Movie reviews and revenues: An experiment in text regression.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="1199" citStr="Joshi et al., 2010" startWordPosition="164" endWordPosition="167">ion accuracies compared to standard regularizers that penalize features in isolation (such as lasso, ridge, and elastic net regularizers) on a range of datasets for various text prediction problems: topic classification, sentiment analysis, and forecasting. 1 Introduction What is the best way to exploit linguistic information in statistical text processing models? For tasks like text classification, sentiment analysis, and text-driven forecasting, this is an open question, as cheap “bag-of-words” models often perform well. Much recent work in NLP has focused on linguistic feature engineering (Joshi et al., 2010) or representation learning (Glorot et al., 2011; Socher et al., 2013). In this paper, we propose a radical alternative. We embrace the conventional bag-of-words representation of text, instead bringing linguistic bias to bear on regularization. Since the seminal work of Chen and Rosenfeld (2000), the importance of regularization in discriminative models of text— including language modeling, structured prediction, and classification—has been widely recognized. The emphasis, however, has largely been on one specific kind of inductive bias: avoiding large weights (i.e., coefficients in a linear </context>
</contexts>
<marker>Joshi, Das, Gimpel, Smith, 2010</marker>
<rawString>Mahesh Joshi, Dipanjan Das, Kevin Gimpel, and Noah A. Smith. 2010. Movie reviews and revenues: An experiment in text regression. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seyoung Kim</author>
<author>Eric P Xing</author>
</authors>
<title>Feature selection via block-regularized regression.</title>
<date>2008</date>
<booktitle>In Proc. of UAI.</booktitle>
<contexts>
<context position="2314" citStr="Kim and Xing, 2008" startWordPosition="326" endWordPosition="329">ely been on one specific kind of inductive bias: avoiding large weights (i.e., coefficients in a linear model). Recently, structured (or composite) regularization has been introduced; simply put, it reasons Noah A. Smith Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA nasmith@cs.cmu.edu about different weights jointly. The most widely explored variant, group lasso (Yuan and Lin, 2006) seeks to avoid large E2 norms for groups of weights. Group lasso has been shown useful in a range of applications, including computational biology (Kim and Xing, 2008), signal processing (Lv et al., 2011), and NLP (Eisenstein et al., 2011; Martins et al., 2011; Nelakanti et al., 2013). For text categorization problems, Yogatama and Smith (2014) proposed groups based on sentences, an idea generalized here to take advantage of richer linguistic information. In this paper, we show how linguistic information of various kinds—parse trees, thematic topics, and hierarchical word clusterings—can be used to construct group lasso variants that impose linguistic bias without introducing any new features. Our experiments demonstrate that structured regularizers can squ</context>
<context position="11458" citStr="Kim and Xing (2008)" startWordPosition="1868" endWordPosition="1871">quivalent to including one additional group for each word type. coefficients and A) for one sentence with the parse tree shown in Figure 1 is: Qtree(W) = &apos;✓|wthe|2 + |wactors|2 + |ware|2 + |wfantastic|2 + |w.|2 &apos;✓+ |ware|2 + |wfantastic|2 + |w2.| &apos;✓ &apos;✓ + |wthe|2 + |wactors|2 + |ware|2 + |wfantastic|2 + |wthe |+ |wactors |+ |ware |+ |wfantastic |+ |w.| The groups have a tree structure, in that assigning zero values to the weights in a group corresponding to a higher-level constituent implies the same for those constituents that are dominated by it. This resembles the tree-guided group lasso in Kim and Xing (2008), although the leaf nodes in their tree represent tasks in multi-task regression. Of course, in a corpus there are many parse trees (one per sentence, so the number of parse trees is the number of sentences). The parse-tree regularizer is: Ωtree(w) = Ed=1 �s=1 �c=Sd 1 Ad,s,ck wd,s,ck2 where Ad,s,c = Aglas × v1size(gd,s,c), d ranges over (training) documents and c ranges over constituents in the parse of sentence s in document d. Similar to the sentence regularizer, the parse-tree regularizer operates on word tokens. Note that, since each word token is itself a constituent, the parse tree regul</context>
</contexts>
<marker>Kim, Xing, 2008</marker>
<rawString>Seyoung Kim and Eric P. Xing. 2008. Feature selection via block-regularized regression. In Proc. of UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shimon Kogan</author>
<author>Dimitry Levin</author>
<author>Bryan R Routledge</author>
<author>Jacob S Sagi</author>
<author>Noah A Smith</author>
</authors>
<title>Predicting risk from financial reports with regression.</title>
<date>2009</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="25799" citStr="Kogan et al., 2009" startWordPosition="4349" endWordPosition="4352">y, and other linguistic phenomena. Our sentiment analysis datasets consist of movie reviews from the Stanford sentiment treebank (Socher et al., 2013),11 and floor speeches by U.S. Congressmen alongside “yea”/“nay” votes on the bill under discussion (Thomas et al., 2006).12 For the Stanford sentiment treebank, we only predict binary classifications (positive or negative) and exclude neutral reviews. Text-driven forecasting. Forecasting from text requires identifying textual correlates of a response variable revealed in the future, most of which will be weak and many of which will be spurious (Kogan et al., 2009). We consider two such problems. The first one is predicting whether a scientific paper will be cited or not within three years of its publication (Yogatama et al., 2011); 10http://qwone.com/-jason/20Newsgroups 11http://nlp.stanford.edu/sentiment/ 12http://www.cs.cornell.edu/-ainur/data.html min W min V ⎧ ⎨ ⎩ = w = arg min W 791 Dataset D # Dev. # Test V Fore. Sent. 20N science 952 235 790 30,154 sports 958 239 796 20,832 relig. 870 209 717 24,528 comp. 929 239 777 20,868 movie 6,920 872 1,821 17,576 vote 1,175 257 860 24,508 science 3,207 280 539 42,702 bill 37,850 7,341 6,571 10,001 Table 2:</context>
</contexts>
<marker>Kogan, Levin, Routledge, Sagi, Smith, 2009</marker>
<rawString>Shimon Kogan, Dimitry Levin, Bryan R. Routledge, Jacob S. Sagi, and Noah A. Smith. 2009. Predicting risk from financial reports with regression. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaolei Lv</author>
<author>Guoan Bi</author>
<author>Chunru Wan</author>
</authors>
<title>The group lasso for stable recovery of block-sparse signal representations.</title>
<date>2011</date>
<journal>IEEE Transactions on Signal Processing,</journal>
<volume>59</volume>
<issue>4</issue>
<contexts>
<context position="2351" citStr="Lv et al., 2011" startWordPosition="332" endWordPosition="335">ve bias: avoiding large weights (i.e., coefficients in a linear model). Recently, structured (or composite) regularization has been introduced; simply put, it reasons Noah A. Smith Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA nasmith@cs.cmu.edu about different weights jointly. The most widely explored variant, group lasso (Yuan and Lin, 2006) seeks to avoid large E2 norms for groups of weights. Group lasso has been shown useful in a range of applications, including computational biology (Kim and Xing, 2008), signal processing (Lv et al., 2011), and NLP (Eisenstein et al., 2011; Martins et al., 2011; Nelakanti et al., 2013). For text categorization problems, Yogatama and Smith (2014) proposed groups based on sentences, an idea generalized here to take advantage of richer linguistic information. In this paper, we show how linguistic information of various kinds—parse trees, thematic topics, and hierarchical word clusterings—can be used to construct group lasso variants that impose linguistic bias without introducing any new features. Our experiments demonstrate that structured regularizers can squeeze higher performance out of conven</context>
</contexts>
<marker>Lv, Bi, Wan, 2011</marker>
<rawString>Xiaolei Lv, Guoan Bi, and Chunru Wan. 2011. The group lasso for stable recovery of block-sparse signal representations. IEEE Transactions on Signal Processing, 59(4):1371–1382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre F T Martins</author>
<author>Noah A Smith</author>
<author>Pedro M Q Aguiar</author>
<author>Mario A T Figueiredo</author>
</authors>
<title>Structured sparsity in structured prediction.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2407" citStr="Martins et al., 2011" startWordPosition="342" endWordPosition="345">in a linear model). Recently, structured (or composite) regularization has been introduced; simply put, it reasons Noah A. Smith Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA nasmith@cs.cmu.edu about different weights jointly. The most widely explored variant, group lasso (Yuan and Lin, 2006) seeks to avoid large E2 norms for groups of weights. Group lasso has been shown useful in a range of applications, including computational biology (Kim and Xing, 2008), signal processing (Lv et al., 2011), and NLP (Eisenstein et al., 2011; Martins et al., 2011; Nelakanti et al., 2013). For text categorization problems, Yogatama and Smith (2014) proposed groups based on sentences, an idea generalized here to take advantage of richer linguistic information. In this paper, we show how linguistic information of various kinds—parse trees, thematic topics, and hierarchical word clusterings—can be used to construct group lasso variants that impose linguistic bias without introducing any new features. Our experiments demonstrate that structured regularizers can squeeze higher performance out of conventional bag-of-words models on seven out of eight of text</context>
<context position="7035" citStr="Martins et al. (2011)" startWordPosition="1109" endWordPosition="1113">Kowalski and Torresani, 2009). where Aglas is a hyperparameter tuned on a development data, and Ag is a group specific weight. Typically the groups are non-overlapping, which offers computational advantages, but this need not be the case (Jacob et al., 2009; Jenatton et al., 2011). 4 Structured Regularizers for Text Past work applying the group lasso to NLP problems has considered four ways of defining the groups. Eisenstein et al. (2011) defined groups of coefficients corresponding to the same independent variable applied to different (continuous) output variables in multi-output regression. Martins et al. (2011) defined groups based on feature templates used in chunking and parsing tasks. Nelakanti et al. (2013) defined groups based on ngram histories for language modeling. In each of these cases, the groups were defined based on information from feature types alone; given the features to be used, the groups were known. Here we build on a fourth approach that exploits structure in the data.4 Yogatama and Smith (2014) introduced the sentence regularizer, which uses patterns of word cooccurrence in the training data to define groups. We review this method, then apply the idea to three more linguistical</context>
</contexts>
<marker>Martins, Smith, Aguiar, Figueiredo, 2011</marker>
<rawString>Andre F. T. Martins, Noah A. Smith, Pedro M. Q. Aguiar, and Mario A. T. Figueiredo. 2011. Structured sparsity in structured prediction. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anil Nelakanti</author>
<author>Cedric Archambeau</author>
<author>Julien Mairal</author>
<author>Francis Bach</author>
<author>Guillaume Bouchard</author>
</authors>
<title>Structured penalties for log-linear language models.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2432" citStr="Nelakanti et al., 2013" startWordPosition="346" endWordPosition="349">cently, structured (or composite) regularization has been introduced; simply put, it reasons Noah A. Smith Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA nasmith@cs.cmu.edu about different weights jointly. The most widely explored variant, group lasso (Yuan and Lin, 2006) seeks to avoid large E2 norms for groups of weights. Group lasso has been shown useful in a range of applications, including computational biology (Kim and Xing, 2008), signal processing (Lv et al., 2011), and NLP (Eisenstein et al., 2011; Martins et al., 2011; Nelakanti et al., 2013). For text categorization problems, Yogatama and Smith (2014) proposed groups based on sentences, an idea generalized here to take advantage of richer linguistic information. In this paper, we show how linguistic information of various kinds—parse trees, thematic topics, and hierarchical word clusterings—can be used to construct group lasso variants that impose linguistic bias without introducing any new features. Our experiments demonstrate that structured regularizers can squeeze higher performance out of conventional bag-of-words models on seven out of eight of text categorization tasks tes</context>
<context position="7137" citStr="Nelakanti et al. (2013)" startWordPosition="1127" endWordPosition="1130">s a group specific weight. Typically the groups are non-overlapping, which offers computational advantages, but this need not be the case (Jacob et al., 2009; Jenatton et al., 2011). 4 Structured Regularizers for Text Past work applying the group lasso to NLP problems has considered four ways of defining the groups. Eisenstein et al. (2011) defined groups of coefficients corresponding to the same independent variable applied to different (continuous) output variables in multi-output regression. Martins et al. (2011) defined groups based on feature templates used in chunking and parsing tasks. Nelakanti et al. (2013) defined groups based on ngram histories for language modeling. In each of these cases, the groups were defined based on information from feature types alone; given the features to be used, the groups were known. Here we build on a fourth approach that exploits structure in the data.4 Yogatama and Smith (2014) introduced the sentence regularizer, which uses patterns of word cooccurrence in the training data to define groups. We review this method, then apply the idea to three more linguistically informed structure in text data. 4.1 Sentence Regularizer The sentence regularizer exploits sentenc</context>
<context position="38594" citStr="Nelakanti et al., 2013" startWordPosition="6383" endWordPosition="6386">perimental focus has been on a controlled comparison between regularizers for a fixed model family (the simplest available, linear with bag-of-words features). However, the improvements offered by our regularization methods can be applied in future work to other model families with more carefully engineered features, metadata features (especially important in forecasting), latent variables, etc. In particular, note that other kinds of weights (e.g., metadata) can be penalized conventionally, or incorporated into the structured regularization where it makes sense to do so (e.g., n-grams, as in Nelakanti et al., 2013). 8 Conclusion We introduced three data-driven, linguistically informed structured regularizers based on parse trees, topics, and hierarchical word clusters. We empirically showed that models regularized using our methods consistently outperformed standard regularizers that penalize features in isolation such as lasso, ridge, and elastic net on a range of datasets for various text prediction problems: topic classification, sentiment analysis, and forecasting. Acknowledgments The authors thank Brendan O’Connor for help with visualization and three anonymous reviewers for helpful feedback on an </context>
</contexts>
<marker>Nelakanti, Archambeau, Mairal, Bach, Bouchard, 2013</marker>
<rawString>Anil Nelakanti, Cedric Archambeau, Julien Mairal, Francis Bach, and Guillaume Bouchard. 2013. Structured penalties for log-linear language models. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Andrew McCallum</author>
<author>Sebastian Thrun</author>
<author>Tom Mitchell</author>
</authors>
<title>Text classification from labeled and unlabeled documents using em.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="14949" citStr="Nigam et al., 2000" startWordPosition="2446" endWordPosition="2449">stead of treating each word separately. We do this by inferring topics in the training corpus by estimating the latent Dirichlet allocation (LDA) model (Blei et al., 2003)). Note that LDA is an unsupervised method, so we can infer topical structures from any collection of documents that are considered related to the target corpus (e.g., training documents, text from the web, etc.). This contrasts with typical semi-supervised learning methods for text categorization that combine unlabeled and labeled data within a generative model, such as multinomial naive Bayes, via expectation-maximization (Nigam et al., 2000) or semi-supervised frequency estimation (Su et al., 2011). Our method does not use unlabeled data to obtain more training documents or estimate the joint distributions of words better, but it allows the use of unlabeled data to induce topics. We leave comparison with other semi-supervised methods for future work. There are many ways to associate inferred topics with group structure. In our experiments, we choose the R most probable words given a topic and create a group for them.6 The LDA regular6Another possibility is to group the smallest set of words whose total probability given a topic a</context>
</contexts>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>Kamal Nigam, Andrew McCallum, Sebastian Thrun, and Tom Mitchell. 2000. Text classification from labeled and unlabeled documents using em. Machine Learning, 39(2-3):103–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lilian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="25053" citStr="Pang and Lee, 2008" startWordPosition="4234" endWordPosition="4237">ailable datasets to evaluate our model described in more detail below. Topic classification. We consider four binary categorization tasks from the 20 Newsgroups dataset.10 Each task involves categorizing a document according to two related categories: comp.sys: ibm.pc.hardware vs. mac.hardware; rec.sport: baseball vs. hockey; sci: med vs. space; and alt.atheism vs. soc.religion.christian. Sentiment analysis. One task in sentiment analysis is predicting the polarity of a piece of text, i.e., whether the author is favorably inclined toward a (usually known) subject of discussion or proposition (Pang and Lee, 2008). Sentiment analysis, even at the coarse level of polarity we consider here, can be confused by negation, stylistic use of irony, and other linguistic phenomena. Our sentiment analysis datasets consist of movie reviews from the Stanford sentiment treebank (Socher et al., 2013),11 and floor speeches by U.S. Congressmen alongside “yea”/“nay” votes on the bill under discussion (Thomas et al., 2006).12 For the Stanford sentiment treebank, we only predict binary classifications (positive or negative) and exclude neutral reviews. Text-driven forecasting. Forecasting from text requires identifying te</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lilian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1–2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="27287" citStr="Petrov and Klein, 2007" startWordPosition="4582" endWordPosition="4585">will be recommended by a Congressional committee (Yano et al., 2012).13 Table 2 summarizes statistics about the datasets used in our experiments. In total, we evaluate our method on eight binary classification tasks. 6.2 Setup In all our experiments, we use unigram features plus an additional bias term which is not regularized. We compare our new regularizers with state-of-the-art methods for document classification: lasso, ridge, and elastic net regularization, as well as the sentence regularizer discussed in §4.1 (Yogatama and Smith, 2014).14 We parsed all corpora using the Berkeley parser (Petrov and Klein, 2007).15 For the LDA regularizers, we ran LDA16 on training documents with K = 1, 000 and R = 10. For the Brown cluster regularizers, we ran Brown clustering17 on training documents with 5, 000 clusters for the topic classification and sentiment analysis datasets, and 1, 000 for the larger text forecasting datasets (since they are bigger datasets that took more time). 13http://www.ark.cs.cmu.edu/bills 14Hyperparameters are tuned on a separate development dataset, using accuracy as the evaluation criterion. For lasso and ridge models, we choose A from {10−2, 10−1, 1, 10, 102, 103}. For elastic net, </context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J D Powell</author>
</authors>
<title>A method for nonlinear constraints in minimization problems.</title>
<date>1969</date>
<booktitle>Optimization,</booktitle>
<pages>283--298</pages>
<editor>In R. Fletcher, editor,</editor>
<publisher>Academic Press.</publisher>
<contexts>
<context position="21110" citStr="Powell, 1969" startWordPosition="3504" endWordPosition="3505">that the processes of learning topics or clusters, or parsing training data sentences, are a separate stage that precedes learning our predictive model. 5 Learning There are many optimization methods for learning models with structured regularizers, particulary group lasso (Jacob et al., 2009; Jenatton et al., 2011; Chen et al., 2011; Qin and Goldfarb, 2012; Yuan et al., 2013). We choose the optimization method of Yogatama and Smith (2014) since it handles millions of overlapping groups effectively. The method is based on the alternating directions method of multipliers (ADMM; Hestenes, 1969; Powell, 1969). We review it here in brief, for completeness, and show how it can be applied to treestructured regularizers (such as the parse tree and Brown cluster regularizers in §4) in particular. Our learning problem is, generically: Ω(w) + EDd=1 L(xd, w, yd). Separating the lasso-like penalty for each word type from our group regularizers, we can rewrite this problem as: min Ωlas(w) + Ωglas(v) + ED d=1 L(xd, w, yd) W,v s.t. v = Mw where v consists of copies of the elements of w. Notice that we work directly on w instead of the copies for the lasso-like penalty, since it does not have overlaps and has </context>
</contexts>
<marker>Powell, 1969</marker>
<rawString>M. J. D. Powell. 1969. A method for nonlinear constraints in minimization problems. In R. Fletcher, editor, Optimization, pages 283–298. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiwei Qin</author>
<author>Donald Goldfarb</author>
</authors>
<title>Structured sparsity via alternating direction methods.</title>
<date>2012</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>13</volume>
<pages>1468</pages>
<contexts>
<context position="20856" citStr="Qin and Goldfarb, 2012" startWordPosition="3464" endWordPosition="3467">tains 8 words, v1 contains 5, etc. Note that the leaves, vs, v9, ... ,v16, each contain one word. LDA and Brown cluster regularizers offer ways to incorporate unlabeled data, if we believe that the unlabeled data can help us infer better topics or clusters. Note that the processes of learning topics or clusters, or parsing training data sentences, are a separate stage that precedes learning our predictive model. 5 Learning There are many optimization methods for learning models with structured regularizers, particulary group lasso (Jacob et al., 2009; Jenatton et al., 2011; Chen et al., 2011; Qin and Goldfarb, 2012; Yuan et al., 2013). We choose the optimization method of Yogatama and Smith (2014) since it handles millions of overlapping groups effectively. The method is based on the alternating directions method of multipliers (ADMM; Hestenes, 1969; Powell, 1969). We review it here in brief, for completeness, and show how it can be applied to treestructured regularizers (such as the parse tree and Brown cluster regularizers in §4) in particular. Our learning problem is, generically: Ω(w) + EDd=1 L(xd, w, yd). Separating the lasso-like penalty for each word type from our group regularizers, we can rewri</context>
</contexts>
<marker>Qin, Goldfarb, 2012</marker>
<rawString>Zhiwei (Tony) Qin and Donald Goldfarb. 2012. Structured sparsity via alternating direction methods. Journal of Machine Learning Research, 13:1435– 1468.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Pradeep Muthukrishnan</author>
<author>Vahed Qazvinian</author>
</authors>
<title>The ACL anthology network corpus.</title>
<date>2009</date>
<booktitle>In Proc. of ACL Workshop on Natural Language Processing and Information Retrieval for Digital Libraries.</booktitle>
<contexts>
<context position="26605" citStr="Radev et al., 2009" startWordPosition="4477" endWordPosition="4480">com/-jason/20Newsgroups 11http://nlp.stanford.edu/sentiment/ 12http://www.cs.cornell.edu/-ainur/data.html min W min V ⎧ ⎨ ⎩ = w = arg min W 791 Dataset D # Dev. # Test V Fore. Sent. 20N science 952 235 790 30,154 sports 958 239 796 20,832 relig. 870 209 717 24,528 comp. 929 239 777 20,868 movie 6,920 872 1,821 17,576 vote 1,175 257 860 24,508 science 3,207 280 539 42,702 bill 37,850 7,341 6,571 10,001 Table 2: Descriptive statistics about the datasets. the dataset comes from the ACL Anthology and consists of research papers from the Association for Computational Linguistics and citation data (Radev et al., 2009). The second task is predicting whether a legislative bill will be recommended by a Congressional committee (Yano et al., 2012).13 Table 2 summarizes statistics about the datasets used in our experiments. In total, we evaluate our method on eight binary classification tasks. 6.2 Setup In all our experiments, we use unigram features plus an additional bias term which is not regularized. We compare our new regularizers with state-of-the-art methods for document classification: lasso, ridge, and elastic net regularization, as well as the sentence regularizer discussed in §4.1 (Yogatama and Smith,</context>
</contexts>
<marker>Radev, Muthukrishnan, Qazvinian, 2009</marker>
<rawString>Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed Qazvinian. 2009. The ACL anthology network corpus. In Proc. of ACL Workshop on Natural Language Processing and Information Retrieval for Digital Libraries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Wu</author>
<author>Jason Chuang</author>
<author>Chris Manning</author>
<author>Andrew Ng</author>
<author>Chris Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1269" citStr="Socher et al., 2013" startWordPosition="175" endWordPosition="178">es in isolation (such as lasso, ridge, and elastic net regularizers) on a range of datasets for various text prediction problems: topic classification, sentiment analysis, and forecasting. 1 Introduction What is the best way to exploit linguistic information in statistical text processing models? For tasks like text classification, sentiment analysis, and text-driven forecasting, this is an open question, as cheap “bag-of-words” models often perform well. Much recent work in NLP has focused on linguistic feature engineering (Joshi et al., 2010) or representation learning (Glorot et al., 2011; Socher et al., 2013). In this paper, we propose a radical alternative. We embrace the conventional bag-of-words representation of text, instead bringing linguistic bias to bear on regularization. Since the seminal work of Chen and Rosenfeld (2000), the importance of regularization in discriminative models of text— including language modeling, structured prediction, and classification—has been widely recognized. The emphasis, however, has largely been on one specific kind of inductive bias: avoiding large weights (i.e., coefficients in a linear model). Recently, structured (or composite) regularization has been in</context>
<context position="10575" citStr="Socher et al., 2013" startWordPosition="1718" endWordPosition="1721">nation of the sentence regularizer and the lasso (a kind of sparse group lasso; Friedman et al., 2010) to also encourage weights of irrelevant word types to go to zero.5 4.2 Parse Tree Regularizer Sentence boundaries are a rather superficial kind of linguistic structure; syntactic parse trees provide more fine-grained information. We introduce a new regularizer, the parse tree regularizer, in which groups are defined for every constituent in every parse of a training data sentence. Figure 1 illustrates the group structures derived from an example sentence from the Stanford sentiment treebank (Socher et al., 2013). This regularizer captures the idea that phrases might be selected as relevant or (in most cases) irrelevant to a task, and is expected to be especially useful in sentence-level prediction tasks. The parse-tree regularizer (omitting the group 5Formally, this is equivalent to including one additional group for each word type. coefficients and A) for one sentence with the parse tree shown in Figure 1 is: Qtree(W) = &apos;✓|wthe|2 + |wactors|2 + |ware|2 + |wfantastic|2 + |w.|2 &apos;✓+ |ware|2 + |wfantastic|2 + |w2.| &apos;✓ &apos;✓ + |wthe|2 + |wactors|2 + |ware|2 + |wfantastic|2 + |wthe |+ |wactors |+ |ware |+ |w</context>
<context position="13620" citStr="Socher et al. (2013)" startWordPosition="2232" endWordPosition="2235">nce, and the parse tree regularizer instantiates groups for constituents in every sentence in the training corpus, and these groups may work against each other. The parse tree regularizer should therefore The actors c6 c7,+ . 788 be understood as encouraging group behavior of syntactically grouped words, or sharing of information by syntactic neighbors. In sentence level prediction tasks, such as sentence-level sentiment analysis, it is known that most constituents (especially those that correspond to shorter phrases) in a parse tree are uninformative (neutral sentiment). This was verified by Socher et al. (2013) when annotating phrases in a sentence for building the Stanford sentiment treebank. Our regularizer incorporates our prior expectation that most constituents should have no effect on prediction. 4.3 LDA Regularizer Another type of structure to consider is topics. For example, if we want to predict whether a paper will be cited or not (Yogatama et al., 2011), the model can perform better if it knows beforehand the collections of words that represent certain themes (e.g., in ACL papers, these might include machine translation, parsing, etc.). As a result, the model can focus on which topics wil</context>
<context position="25330" citStr="Socher et al., 2013" startWordPosition="4277" endWordPosition="4280">e vs. mac.hardware; rec.sport: baseball vs. hockey; sci: med vs. space; and alt.atheism vs. soc.religion.christian. Sentiment analysis. One task in sentiment analysis is predicting the polarity of a piece of text, i.e., whether the author is favorably inclined toward a (usually known) subject of discussion or proposition (Pang and Lee, 2008). Sentiment analysis, even at the coarse level of polarity we consider here, can be confused by negation, stylistic use of irony, and other linguistic phenomena. Our sentiment analysis datasets consist of movie reviews from the Stanford sentiment treebank (Socher et al., 2013),11 and floor speeches by U.S. Congressmen alongside “yea”/“nay” votes on the bill under discussion (Thomas et al., 2006).12 For the Stanford sentiment treebank, we only predict binary classifications (positive or negative) and exclude neutral reviews. Text-driven forecasting. Forecasting from text requires identifying textual correlates of a response variable revealed in the future, most of which will be weak and many of which will be spurious (Kogan et al., 2009). We consider two such problems. The first one is predicting whether a scientific paper will be cited or not within three years of </context>
<context position="29491" citStr="Socher et al., 2013" startWordPosition="4929" endWordPosition="4932">ured regularizers outperformed lasso, ridge, and elastic net. In four cases, the new regularizers in this paper outperform the sentence regularizer. We can see that the parse tree regularizer performed the best for the movie review dataset. The task is to predict sentence-level sentiment, so each training example is a sentence. Since constituentlevel annotations are available for this dataset, we only constructed groups for neutral constituents (i.e., we drive neutral constituents to zero during training). It has been shown that syntactic information is helpful for sentence-level predictions (Socher et al., 2013), so the parse tree regularizer is naturally suitable for this task. The Brown cluster and LDA regularizers performed best for the forecasting scientific articles dataset. The task is to predict whether an article will be cited or not within three years after publication. Regularizers that exploit the knowledge of semantic relations (e.g., topical categories), such as the Brown cluster and LDA regularizers, are therefore suitable for this type of prediction. Table 4 shows model sizes obtained by each of the regularizers for each dataset. While lasso prunes more aggressively, it almost always p</context>
<context position="37726" citStr="Socher et al., 2013" startWordPosition="6250" endWordPosition="6253">t not once it combined with other words such as hpr, std, obsessive. Note that we ran Brown clustering only on the training documents; running it on a larger collection of (unlabeled) documents relevant to the prediction task (i.e., semi-supervised learning) is worth exploring in future work. 7 Related and Future Work Overall, our results demonstrate that linguistic structure in the data can be used to improve bagof-words models, through structured regularization. State-of-the-art approaches to some of these problems have used additional features and representations (Yessenalina et al., 2010; Socher et al., 2013). For example, for the vote sentiment analysis datasets, latent variable models of Yessenalina et al. (2010) achieved a superior result of 77.67%. To do so, they sacrificed convexity and had to rely on side information for initialization. Our experimental focus has been on a controlled comparison between regularizers for a fixed model family (the simplest available, linear with bag-of-words features). However, the improvements offered by our regularization methods can be applied in future work to other model families with more carefully engineered features, metadata features (especially import</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Chris Manning, Andrew Ng, and Chris Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Su</author>
<author>Jelber Sayyad-Shirabad</author>
<author>Stan Matwin</author>
</authors>
<title>Large scale text classication using semisupervised multinomial naive Bayes.</title>
<date>2011</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="15007" citStr="Su et al., 2011" startWordPosition="2454" endWordPosition="2457">ng topics in the training corpus by estimating the latent Dirichlet allocation (LDA) model (Blei et al., 2003)). Note that LDA is an unsupervised method, so we can infer topical structures from any collection of documents that are considered related to the target corpus (e.g., training documents, text from the web, etc.). This contrasts with typical semi-supervised learning methods for text categorization that combine unlabeled and labeled data within a generative model, such as multinomial naive Bayes, via expectation-maximization (Nigam et al., 2000) or semi-supervised frequency estimation (Su et al., 2011). Our method does not use unlabeled data to obtain more training documents or estimate the joint distributions of words better, but it allows the use of unlabeled data to induce topics. We leave comparison with other semi-supervised methods for future work. There are many ways to associate inferred topics with group structure. In our experiments, we choose the R most probable words given a topic and create a group for them.6 The LDA regular6Another possibility is to group the smallest set of words whose total probability given a topic amounts to P (e.g., 0.99). mass of a topic. Preliminary exp</context>
</contexts>
<marker>Su, Sayyad-Shirabad, Matwin, 2011</marker>
<rawString>Jiang Su, Jelber Sayyad-Shirabad, and Stan Matwin. 2011. Large scale text classication using semisupervised multinomial naive Bayes. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Thomas</author>
<author>Bo Pang</author>
<author>Lilian Lee</author>
</authors>
<title>Get out the vote: Determining support or opposition from congressional floor-debate transcripts.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="25451" citStr="Thomas et al., 2006" startWordPosition="4296" endWordPosition="4299">ment analysis. One task in sentiment analysis is predicting the polarity of a piece of text, i.e., whether the author is favorably inclined toward a (usually known) subject of discussion or proposition (Pang and Lee, 2008). Sentiment analysis, even at the coarse level of polarity we consider here, can be confused by negation, stylistic use of irony, and other linguistic phenomena. Our sentiment analysis datasets consist of movie reviews from the Stanford sentiment treebank (Socher et al., 2013),11 and floor speeches by U.S. Congressmen alongside “yea”/“nay” votes on the bill under discussion (Thomas et al., 2006).12 For the Stanford sentiment treebank, we only predict binary classifications (positive or negative) and exclude neutral reviews. Text-driven forecasting. Forecasting from text requires identifying textual correlates of a response variable revealed in the future, most of which will be weak and many of which will be spurious (Kogan et al., 2009). We consider two such problems. The first one is predicting whether a scientific paper will be cited or not within three years of its publication (Yogatama et al., 2011); 10http://qwone.com/-jason/20Newsgroups 11http://nlp.stanford.edu/sentiment/ 12ht</context>
</contexts>
<marker>Thomas, Pang, Lee, 2006</marker>
<rawString>Matt Thomas, Bo Pang, and Lilian Lee. 2006. Get out the vote: Determining support or opposition from congressional floor-debate transcripts. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Tibshirani</author>
<author>Michael Saunders</author>
<author>Saharon Rosset</author>
<author>Ji Zhu</author>
<author>Keith Knight</author>
</authors>
<title>Sparsity and smoothness via the fused lasso.</title>
<date>2005</date>
<journal>Journal of Royal Statistical Society B,</journal>
<volume>67</volume>
<issue>1</issue>
<contexts>
<context position="6390" citStr="Tibshirani et al., 2005" startWordPosition="1008" endWordPosition="1011">mean Gaussian distribution. For nonoverlapping group lasso, the prior is a two-level hierarchical Bayes model (Figueiredo, 2002). The Bayesian interpretation of overlapping group lasso is not yet well understood. 2The lasso leads to strongly sparse solutions, in which many elements of the estimated w are actually zero. This is an attractive property for efficiency and (perhaps) interpretability. The ridge encourages weights to go toward zero, but usually not all the way to zero; for this reason its solutions are known as “weakly” sparse. 3Other structured regularizers include the fused lasso (Tibshirani et al., 2005) and the elitist lasso (Kowalski and Torresani, 2009). where Aglas is a hyperparameter tuned on a development data, and Ag is a group specific weight. Typically the groups are non-overlapping, which offers computational advantages, but this need not be the case (Jacob et al., 2009; Jenatton et al., 2011). 4 Structured Regularizers for Text Past work applying the group lasso to NLP problems has considered four ways of defining the groups. Eisenstein et al. (2011) defined groups of coefficients corresponding to the same independent variable applied to different (continuous) output variables in m</context>
</contexts>
<marker>Tibshirani, Saunders, Rosset, Zhu, Knight, 2005</marker>
<rawString>Robert Tibshirani, Michael Saunders, Saharon Rosset, Ji Zhu, and Keith Knight. 2005. Sparsity and smoothness via the fused lasso. Journal of Royal Statistical Society B, 67(1):91–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Tibshirani</author>
</authors>
<title>Regression shrinkage and selection via the lasso.</title>
<date>1996</date>
<journal>Journal of Royal Statistical Society B,</journal>
<volume>58</volume>
<issue>1</issue>
<contexts>
<context position="4320" citStr="Tibshirani, 1996" startWordPosition="661" endWordPosition="662">ularizer. In this work, we use the log loss: Z(xd, w, yd) = − log(1 + exp(−ydwTxd)), 786 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 786–796, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Other loss functions (e.g., hinge loss, squared loss) can also be used with any of the regularizers discussed in this paper. Our focus is on the regularizer, Ω(w). For high dimensional data such as text, regularization is crucial to avoid overfitting.1 The usual starting points for regularization are the “lasso” (Tibshirani, 1996) and the “ridge” (Hoerl and Kennard, 1970), based respectively on the E1 and squared E2 norms: Ωlas(w) = Alaskwk1 = AEj |wj| Ωrid(w) = Aridkwk22 = A Ej w2j Both methods disprefer weights of large magnitude; smaller (relative) magnitude means a feature (here, a word) has a smaller effect on the prediction, and zero means a feature has no effect.2 The hyperparameter A in each case is typically tuned on a development dataset. A linear combination of ridge and lasso is known as the elastic net (Zou and Hastie, 2005). The lasso, ridge, and elastic net are three strong baselines in our experiments. </context>
</contexts>
<marker>Tibshirani, 1996</marker>
<rawString>Robert Tibshirani. 1996. Regression shrinkage and selection via the lasso. Journal of Royal Statistical Society B, 58(1):267–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tae Yano</author>
<author>Noah A Smith</author>
<author>John D Wilkerson</author>
</authors>
<title>Textual predictors of bill survival in congressional committees.</title>
<date>2012</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="26732" citStr="Yano et al., 2012" startWordPosition="4497" endWordPosition="4500">= arg min W 791 Dataset D # Dev. # Test V Fore. Sent. 20N science 952 235 790 30,154 sports 958 239 796 20,832 relig. 870 209 717 24,528 comp. 929 239 777 20,868 movie 6,920 872 1,821 17,576 vote 1,175 257 860 24,508 science 3,207 280 539 42,702 bill 37,850 7,341 6,571 10,001 Table 2: Descriptive statistics about the datasets. the dataset comes from the ACL Anthology and consists of research papers from the Association for Computational Linguistics and citation data (Radev et al., 2009). The second task is predicting whether a legislative bill will be recommended by a Congressional committee (Yano et al., 2012).13 Table 2 summarizes statistics about the datasets used in our experiments. In total, we evaluate our method on eight binary classification tasks. 6.2 Setup In all our experiments, we use unigram features plus an additional bias term which is not regularized. We compare our new regularizers with state-of-the-art methods for document classification: lasso, ridge, and elastic net regularization, as well as the sentence regularizer discussed in §4.1 (Yogatama and Smith, 2014).14 We parsed all corpora using the Berkeley parser (Petrov and Klein, 2007).15 For the LDA regularizers, we ran LDA16 on</context>
</contexts>
<marker>Yano, Smith, Wilkerson, 2012</marker>
<rawString>Tae Yano, Noah A. Smith, and John D. Wilkerson. 2012. Textual predictors of bill survival in congressional committees. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ainur Yessenalina</author>
<author>Yisong Yue</author>
<author>Claire Cardie</author>
</authors>
<title>Multi-level structured models for document sentiment classification.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="37704" citStr="Yessenalina et al., 2010" startWordPosition="6246" endWordPosition="6249">dical). driven to zero, but not once it combined with other words such as hpr, std, obsessive. Note that we ran Brown clustering only on the training documents; running it on a larger collection of (unlabeled) documents relevant to the prediction task (i.e., semi-supervised learning) is worth exploring in future work. 7 Related and Future Work Overall, our results demonstrate that linguistic structure in the data can be used to improve bagof-words models, through structured regularization. State-of-the-art approaches to some of these problems have used additional features and representations (Yessenalina et al., 2010; Socher et al., 2013). For example, for the vote sentiment analysis datasets, latent variable models of Yessenalina et al. (2010) achieved a superior result of 77.67%. To do so, they sacrificed convexity and had to rely on side information for initialization. Our experimental focus has been on a controlled comparison between regularizers for a fixed model family (the simplest available, linear with bag-of-words features). However, the improvements offered by our regularization methods can be applied in future work to other model families with more carefully engineered features, metadata featu</context>
</contexts>
<marker>Yessenalina, Yue, Cardie, 2010</marker>
<rawString>Ainur Yessenalina, Yisong Yue, and Claire Cardie. 2010. Multi-level structured models for document sentiment classification. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dani Yogatama</author>
<author>Noah A Smith</author>
</authors>
<title>Making the most of bag of words: Sentence regularization with alternating direction method of multipliers.</title>
<date>2014</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="2493" citStr="Yogatama and Smith (2014)" startWordPosition="354" endWordPosition="357">ntroduced; simply put, it reasons Noah A. Smith Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA nasmith@cs.cmu.edu about different weights jointly. The most widely explored variant, group lasso (Yuan and Lin, 2006) seeks to avoid large E2 norms for groups of weights. Group lasso has been shown useful in a range of applications, including computational biology (Kim and Xing, 2008), signal processing (Lv et al., 2011), and NLP (Eisenstein et al., 2011; Martins et al., 2011; Nelakanti et al., 2013). For text categorization problems, Yogatama and Smith (2014) proposed groups based on sentences, an idea generalized here to take advantage of richer linguistic information. In this paper, we show how linguistic information of various kinds—parse trees, thematic topics, and hierarchical word clusterings—can be used to construct group lasso variants that impose linguistic bias without introducing any new features. Our experiments demonstrate that structured regularizers can squeeze higher performance out of conventional bag-of-words models on seven out of eight of text categorization tasks tested, in six cases with more compact models than the best-perf</context>
<context position="7448" citStr="Yogatama and Smith (2014)" startWordPosition="1182" endWordPosition="1185"> groups. Eisenstein et al. (2011) defined groups of coefficients corresponding to the same independent variable applied to different (continuous) output variables in multi-output regression. Martins et al. (2011) defined groups based on feature templates used in chunking and parsing tasks. Nelakanti et al. (2013) defined groups based on ngram histories for language modeling. In each of these cases, the groups were defined based on information from feature types alone; given the features to be used, the groups were known. Here we build on a fourth approach that exploits structure in the data.4 Yogatama and Smith (2014) introduced the sentence regularizer, which uses patterns of word cooccurrence in the training data to define groups. We review this method, then apply the idea to three more linguistically informed structure in text data. 4.1 Sentence Regularizer The sentence regularizer exploits sentence boundaries in each training document. The idea is to define a group gd,s for every sentence s in every training document d. The group contains coefficients for words that occur in its sentence. This means that a word is a member of one group for every distinct (training) sentence it occurs in, and that the r</context>
<context position="9935" citStr="Yogatama and Smith (2014)" startWordPosition="1617" endWordPosition="1620"> sentiment treebank has an annotation of sentiments at the constituent level. As in this example, most constituents are annotated as neutral. If the norm of wgd,s is driven to zero, then the learner has deemed the corresponding sentence irrelevant to the prediction. It is important to point out that, while the regularizer prefers to zero out the weights for all words in irrelevant sentences, it also prefers not to zero out weights for words in relevant sentences. Since the groups overlap and may work against each other, the regularizer may not be able to drive many weights to zero on its own. Yogatama and Smith (2014) used a linear combination of the sentence regularizer and the lasso (a kind of sparse group lasso; Friedman et al., 2010) to also encourage weights of irrelevant word types to go to zero.5 4.2 Parse Tree Regularizer Sentence boundaries are a rather superficial kind of linguistic structure; syntactic parse trees provide more fine-grained information. We introduce a new regularizer, the parse tree regularizer, in which groups are defined for every constituent in every parse of a training data sentence. Figure 1 illustrates the group structures derived from an example sentence from the Stanford </context>
<context position="20940" citStr="Yogatama and Smith (2014)" startWordPosition="3478" endWordPosition="3481">ontain one word. LDA and Brown cluster regularizers offer ways to incorporate unlabeled data, if we believe that the unlabeled data can help us infer better topics or clusters. Note that the processes of learning topics or clusters, or parsing training data sentences, are a separate stage that precedes learning our predictive model. 5 Learning There are many optimization methods for learning models with structured regularizers, particulary group lasso (Jacob et al., 2009; Jenatton et al., 2011; Chen et al., 2011; Qin and Goldfarb, 2012; Yuan et al., 2013). We choose the optimization method of Yogatama and Smith (2014) since it handles millions of overlapping groups effectively. The method is based on the alternating directions method of multipliers (ADMM; Hestenes, 1969; Powell, 1969). We review it here in brief, for completeness, and show how it can be applied to treestructured regularizers (such as the parse tree and Brown cluster regularizers in §4) in particular. Our learning problem is, generically: Ω(w) + EDd=1 L(xd, w, yd). Separating the lasso-like penalty for each word type from our group regularizers, we can rewrite this problem as: min Ωlas(w) + Ωglas(v) + ED d=1 L(xd, w, yd) W,v s.t. v = Mw whe</context>
<context position="22446" citStr="Yogatama and Smith (2014)" startWordPosition="3763" endWordPosition="3766">in W v2 v4 v6 v7 v3 v10 v11 v12 v13 v14 v15 v16 v8 v9 midfielder knee injury moon sun monday sunday goal striker 790 L = EGg=1 size(g). M E 10,11LxV is a matrix whose 1s link elements of w to their copies.7 We now have a constrained optimization problem, from which we can create an augmented Lagrangian problem; let u be the Lagrange variables: Qlas(w) + Qglas(v) + L(w) + uT(v − Mw) + ρ211v − Mw1122 ADMM proceeds by iteratively updating each of w, v, and u, amounting to the following subproblems: Ωlas(w) +,C(w) − u�Mw + p2kv − Mwk22 (1) Ωglas(v) + uTv + p2 kv − Mwk2 (2) 2 u = u + p(v − Mw) (3) Yogatama and Smith (2014) show that Eq. 1 can be rewritten in a form quite similar to E2- regularized loss minimization.8 Eq. 2 is the proximal operator of 1 ρQglas applied to Mw − uρ. As such, it depends on the form of M. Note that when applied to the collection of “copies” of the parameters, v, Qglas no longer has overlapping groups. Defined Mg as the rows of M corresponding to weight copies assigned to group g. Let zg °= Mgw − ugρ . Denote Ag = Aglas V/size(g). The problem can be solved by applying the proximal operator used in non-overlapping group lasso to each subvector: vg = prox λg(zg) Qglas, ρ 0 if 11zg112 &lt; </context>
<context position="27211" citStr="Yogatama and Smith, 2014" startWordPosition="4570" endWordPosition="4573">Radev et al., 2009). The second task is predicting whether a legislative bill will be recommended by a Congressional committee (Yano et al., 2012).13 Table 2 summarizes statistics about the datasets used in our experiments. In total, we evaluate our method on eight binary classification tasks. 6.2 Setup In all our experiments, we use unigram features plus an additional bias term which is not regularized. We compare our new regularizers with state-of-the-art methods for document classification: lasso, ridge, and elastic net regularization, as well as the sentence regularizer discussed in §4.1 (Yogatama and Smith, 2014).14 We parsed all corpora using the Berkeley parser (Petrov and Klein, 2007).15 For the LDA regularizers, we ran LDA16 on training documents with K = 1, 000 and R = 10. For the Brown cluster regularizers, we ran Brown clustering17 on training documents with 5, 000 clusters for the topic classification and sentiment analysis datasets, and 1, 000 for the larger text forecasting datasets (since they are bigger datasets that took more time). 13http://www.ark.cs.cmu.edu/bills 14Hyperparameters are tuned on a separate development dataset, using accuracy as the evaluation criterion. For lasso and rid</context>
</contexts>
<marker>Yogatama, Smith, 2014</marker>
<rawString>Dani Yogatama and Noah A. Smith. 2014. Making the most of bag of words: Sentence regularization with alternating direction method of multipliers. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dani Yogatama</author>
<author>Michael Heilman</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Bryan R Routledge</author>
<author>Noah A Smith</author>
</authors>
<title>Predicting a scientific community’s response to an article.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<marker>Yogatama, Heilman, O’Connor, Dyer, Routledge, Smith, 2011</marker>
<rawString>Dani Yogatama, Michael Heilman, Brendan O’Connor, Chris Dyer, Bryan R. Routledge, and Noah A. Smith. 2011. Predicting a scientific community’s response to an article. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming Yuan</author>
<author>Yi Lin</author>
</authors>
<title>Model selection and estimation in regression with grouped variables.</title>
<date>2006</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<volume>68</volume>
<issue>1</issue>
<contexts>
<context position="2146" citStr="Yuan and Lin, 2006" startWordPosition="298" endWordPosition="301">ion in discriminative models of text— including language modeling, structured prediction, and classification—has been widely recognized. The emphasis, however, has largely been on one specific kind of inductive bias: avoiding large weights (i.e., coefficients in a linear model). Recently, structured (or composite) regularization has been introduced; simply put, it reasons Noah A. Smith Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA nasmith@cs.cmu.edu about different weights jointly. The most widely explored variant, group lasso (Yuan and Lin, 2006) seeks to avoid large E2 norms for groups of weights. Group lasso has been shown useful in a range of applications, including computational biology (Kim and Xing, 2008), signal processing (Lv et al., 2011), and NLP (Eisenstein et al., 2011; Martins et al., 2011; Nelakanti et al., 2013). For text categorization problems, Yogatama and Smith (2014) proposed groups based on sentences, an idea generalized here to take advantage of richer linguistic information. In this paper, we show how linguistic information of various kinds—parse trees, thematic topics, and hierarchical word clusterings—can be u</context>
<context position="5089" citStr="Yuan and Lin, 2006" startWordPosition="792" endWordPosition="795">j w2j Both methods disprefer weights of large magnitude; smaller (relative) magnitude means a feature (here, a word) has a smaller effect on the prediction, and zero means a feature has no effect.2 The hyperparameter A in each case is typically tuned on a development dataset. A linear combination of ridge and lasso is known as the elastic net (Zou and Hastie, 2005). The lasso, ridge, and elastic net are three strong baselines in our experiments. 3 Group Lasso Structured regularizers penalize estimates of w in which collections of weights are penalized jointly. For example, in the group lasso (Yuan and Lin, 2006), predefined groups of weights (subvectors of w) are encouraged to either go to zero (as a group) or not (as a group)—this is known as “group sparsity.”3 The variant of group lasso we explore here uses an E1,2 norm. Let g index the G predefined groups of weights and wg denote the subvector of w containing weights for group g: EG Ωglas(w) =Aglas g=1 Agkwgk2, 1A Bayesian interpretation of regularization is as a prior on the weight vector w; in many cases Ω can be understood as a log-prior representing beliefs about the model held before exposure to data. For lasso regression, the prior is a zero</context>
</contexts>
<marker>Yuan, Lin, 2006</marker>
<rawString>Ming Yuan and Yi Lin. 2006. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society, Series B, 68(1):49–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Yuan</author>
<author>Jun Liu</author>
<author>Jieping Ye</author>
</authors>
<title>Efficient methods for overlapping group lasso.</title>
<date>2013</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>35</volume>
<issue>9</issue>
<contexts>
<context position="20876" citStr="Yuan et al., 2013" startWordPosition="3468" endWordPosition="3471">ns 5, etc. Note that the leaves, vs, v9, ... ,v16, each contain one word. LDA and Brown cluster regularizers offer ways to incorporate unlabeled data, if we believe that the unlabeled data can help us infer better topics or clusters. Note that the processes of learning topics or clusters, or parsing training data sentences, are a separate stage that precedes learning our predictive model. 5 Learning There are many optimization methods for learning models with structured regularizers, particulary group lasso (Jacob et al., 2009; Jenatton et al., 2011; Chen et al., 2011; Qin and Goldfarb, 2012; Yuan et al., 2013). We choose the optimization method of Yogatama and Smith (2014) since it handles millions of overlapping groups effectively. The method is based on the alternating directions method of multipliers (ADMM; Hestenes, 1969; Powell, 1969). We review it here in brief, for completeness, and show how it can be applied to treestructured regularizers (such as the parse tree and Brown cluster regularizers in §4) in particular. Our learning problem is, generically: Ω(w) + EDd=1 L(xd, w, yd). Separating the lasso-like penalty for each word type from our group regularizers, we can rewrite this problem as: </context>
</contexts>
<marker>Yuan, Liu, Ye, 2013</marker>
<rawString>Lei Yuan, Jun Liu, and Jieping Ye. 2013. Efficient methods for overlapping group lasso. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(9):2104–2116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Zou</author>
<author>Trevor Hastie</author>
</authors>
<title>Regularization and variable selection via the elastic net.</title>
<date>2005</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<pages>67--301</pages>
<contexts>
<context position="4837" citStr="Zou and Hastie, 2005" startWordPosition="752" endWordPosition="755">l to avoid overfitting.1 The usual starting points for regularization are the “lasso” (Tibshirani, 1996) and the “ridge” (Hoerl and Kennard, 1970), based respectively on the E1 and squared E2 norms: Ωlas(w) = Alaskwk1 = AEj |wj| Ωrid(w) = Aridkwk22 = A Ej w2j Both methods disprefer weights of large magnitude; smaller (relative) magnitude means a feature (here, a word) has a smaller effect on the prediction, and zero means a feature has no effect.2 The hyperparameter A in each case is typically tuned on a development dataset. A linear combination of ridge and lasso is known as the elastic net (Zou and Hastie, 2005). The lasso, ridge, and elastic net are three strong baselines in our experiments. 3 Group Lasso Structured regularizers penalize estimates of w in which collections of weights are penalized jointly. For example, in the group lasso (Yuan and Lin, 2006), predefined groups of weights (subvectors of w) are encouraged to either go to zero (as a group) or not (as a group)—this is known as “group sparsity.”3 The variant of group lasso we explore here uses an E1,2 norm. Let g index the G predefined groups of weights and wg denote the subvector of w containing weights for group g: EG Ωglas(w) =Aglas g</context>
</contexts>
<marker>Zou, Hastie, 2005</marker>
<rawString>Hui Zou and Trevor Hastie. 2005. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society, Series B, 67:301–320.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>