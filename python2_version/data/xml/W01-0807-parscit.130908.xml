<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.951224">
On Using a Parallel Graph Rewriting Formalism in Generation
</title>
<author confidence="0.928449">
Bernd Bohnet and Leo Wanner
</author>
<affiliation confidence="0.8513955">
CS Department, Intelligent Systems Group
University of Stuttgart
</affiliation>
<address confidence="0.753955">
Breitwiesenstr. 20 - 22, 70565 Stuttgart, Germany
</address>
<email confidence="0.85975">
{bohnetlwannerAinformatik.uni-stuttgart.de}
</email>
<sectionHeader confidence="0.970183" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999917916666667">
In this paper, we present a paral-
lel context sensitive graph rewriting
formalism for a dependency-oriented
generation grammar. The paral-
lel processing of the input structure
makes an explicit presentation of all
alternative options for its mapping
onto the output structure possible.
This allows for the selection of the
linguistic realization that suits best
the communicative and contextual
criteria available.
</bodyText>
<sectionHeader confidence="0.993812" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9948421">
Graph-rewriting formalisms received a con-
siderable attention in generation grammar
implementations and in the area of transfer in
machine translation. A graph-rewriting for-
malism is either sequential or parallel (Rozen-
berg, 1997). A sequential graph-rewriting for-
malism gradually transforms an input struc-
ture specified in the formal language Li into
an output structure, which is specified in the
formal language L2, by using explicitly or
implicitly defined rewriting rules.&apos; Explicit
rewriting rules may have the format of clas-
sic rewriting rules, as, e.g., in (Frank, 1999)
or of bidirectional rules that establish a cor-
respondence relation between minimal struc-
tures of Li and L2, as, e.g., in (Iordanskaja
et al., 1988; Lavoie and Rainbow, 1997). Im-
plicit rewriting rules are encoded in terms of
LI-constraints that are associated with struc-
ture chunks and lexical items of L2; see, e.g.,
</bodyText>
<note confidence="0.5150645">
&apos;Note that Li and £2 may be identical, but do not
need to be so.
</note>
<bodyText confidence="0.998438068965517">
(Nicolov et al., 1996; Beale et al., 1998; Stede,
1999.).
A parallel graph-rewriting formalism maps
a given input structure to an output struc-
ture instead of transforming the former into
the latter. Although parallel graph-rewriting
shows several advantages when compared to
sequential graph rewriting (see Section 3), se-
quential graph-rewriting formalisms are much
more common.
In this paper, we present the implementa-
tion of a parallel graph rewriting formalism
for the grammar of the Meaning-Text The-
ory (1017) (Mel&apos;Cuk, 1981; Mel&apos;Cuk, 1988).
The focus of the presentation is on one of the
major stages of the algorithm: the spelling
out which rules are to be applied to which
fragments of the input structure in order to
achieve its most optimal coverage. This is a
search problem.
In the next section, a brief introduction to
MTT and its formal basics is given. In Sec-
tion 3 we present the stages of processing in
parallel graph rewriting. Section 4 explains
the search algorithm in detail and presents an
example of how the search algorithm works in
practice. Section 5 discusses some of the re-
lated work in this area. In Section 6, finally,
a summary and some conclusions are given.
</bodyText>
<sectionHeader confidence="0.9104075" genericHeader="method">
2 The Meaning-Text Theory
2.1 Linguistic Foundations
</sectionHeader>
<bodyText confidence="0.954514">
The Meaning-Text Theory is a multistratal
dependency theory. Five of its strata are im-
mediately relevant for generation: (1) the se-
mantic stratum, (2) the deep-syntactic stra-
tum, (3) the surface-syntactic-stratum, (4)
</bodyText>
<figure confidence="0.989032619047619">
assembly drink
Socrates cup_of hemlock
force
\III
Socrat,
force
subjective dobjective
assembly Socrates
determinative
the
Lobjective
prep infinite
circumstantial
dobjective
in--
1
prepositional
dawn
determinative
S the
cup_of hemlock
</figure>
<bodyText confidence="0.998769933333333">
the deep-morphological stratum, and (5) the
surface-morphological stratum, which is the
linearized surface structure. Linguistic struc-
tures at the semantic stratum are predicate-
argument structures, i.e., directed acyclic
graphs in which nodes stand for predicates
and objects, and edges establish relations be-
tween predicates and their arguments (with
each edge being labelled by the number of
the respective argument). Linguistic struc-
tures at both syntactic strata are dependency
trees with lexemes being represented as nodes
and syntactic relations as edges. At the
deep-syntactic stratum, the encoded syntac-
tic relations are actant or participant rela-
tions. The actant relations are not named
(as, e.g., in the systemic grammar), but sim-
ply numbered by I, II, III, ... ). As (gram-
matical) functions in the f-structure in LEG,
actant relations are assumed to be univer-
sal. At the surface-syntactic stratum, the en-
coded syntactic relations are language-specific
grammatical functions (such as subject, di-
rect object, etc.). Linguistic structures at the
morphological strata are (ordered) sequences
of word forms. Figure 1 shows the deep-
syntactic structure and the surface-syntactic
structure for the sentence The assembly forced
Socrates to drink the cup of hemlock in the
dawn. In the deep-syntactic structure, the
dashed line represents the referential link be-
tween the two &apos;Socrates&apos; nodes.
At all levels of representation, the nodes
of linguistic structures are, in fact, fea-
ture structures. They are defined in
terms of attribute-value pairs (such as, e.g.,
&apos;lex = Socrates&apos;, &apos;cat = verb&apos;, &apos;voice =
passive&apos;, etc.). For instance, the node
force carries the attribute-value pairs cat =
verb, form = finite, tense = past, voice
= active. In the graphic representation, the
attribute-value pairs of a node are shown only
upon request. Both the nodes and the at-
tributes are typed.&apos;
The grammar in MTT is a priori an equative
</bodyText>
<footnote confidence="0.9874405">
2As a matter of fact, the linguistic structures and
the rules in MTT can be represented in terms of typed
feature structures; see (Mebeuk and Wanner, forth-
coming).
</footnote>
<figureCaption confidence="0.89138925">
Figure 1: Deep-syntactic and surface-
syntactic structures of the sentence The as-
sembly forced Socrates to drink the cup of
hemlock in the dawn.
</figureCaption>
<bodyText confidence="0.997356052631579">
device (Kahane, forthcoming). It consists of a
set of rules that establish the correspondence
between minimal structures at two adjacent
strata with a minimal structure being a fea-
ture of a node, a node, a relation between two
nodes, or a configuration of relations. Fig-
ure 2 shows a sample grammar rule as imple-
mented in MATE (Bohnet et al., 2000). This
rule maps the deep-syntactic relation II (the
second actant) onto the surface-syntactic re-
lation dobjective (i.e., direct object). The
rule applies if there is the first actant avail-
able (i.e.., the relation I is specified in the in-
put structure), the verbal head of the struc-
ture contains the attribute-feature pair &apos;cat
= verb&apos;, and no attribute-value pair &apos;voice =
passive&apos;.3 The relation I is specified as being
in the context. That is, it is not &amp;quot;consumed&amp;quot;
by the rule; it rather serves as a constraint for
</bodyText>
<footnote confidence="0.9933876">
3Note that if there would be no actant I available,
in order to get a grammatical sentence, the second
actant would have to be realized as subjective. In
other words, in such a case, passivization would take
place.
</footnote>
<table confidence="0.784925142857143">
context: ?Xds -I-&gt; ?Zds
left-h.s.: ?Xds -II-&gt; ?Yds
conditions: ?Xds.cat = verb
NOT ?Xds.voice = passive
right-h.s.: ?Xss -dobjective-&gt; ?Yss
correspondences :?Xss &lt;=&gt;?Xds
?Yss &lt;=&gt;?Yds
</table>
<figureCaption confidence="0.998697">
Figure 2: A sample grammar rule.
</figureCaption>
<bodyText confidence="0.999533769230769">
the application of the rule.
In the process of generation, a compiler ap-
plies grammar rules to an input structure. If
a fragment of the structure matches with the
left-hand side of a rule, and the constraints
specified in context and in the condition slots
of this rule are met, the fragment is mapped
onto the substructure specified at the right-
hand side of the rule.
Our grammar formalism is intended to be
bidirectional,&apos; i.e., to be applicable for gener-
ation and for parsing. Therefore (and because
of the general policy adopted in MTT), in
grammar rules, only purely linguistic criteria
are specified as conditions. No criteria, e.g.,
from the situational context are considered.
As a result, the grammar might well produce
several output structures. It is left to spe-
cific submodules of sentence planning (such
as lexicalization, syntacticticization, etc.) to
further restrict and monitor the realization of
an input structure by the grammar. How-
ever, we do not discuss the interaction of these
modules with the grammar in what follows.
Rather, we restrict ourselves to the presenta-
tion of the grammar formalism.
</bodyText>
<subsectionHeader confidence="0.991459">
2.2 Formal Description
</subsectionHeader>
<bodyText confidence="0.915221375">
From the formal viewpoint, linguistic struc-
tures can be considered as attributed graphs
with a variant degree of freedom.
Definition 1 (Attributed Graph) Let
Ee, Ea and Eva11,-,n be sets with Ee being
the set of edge labels, Ea the set of attributes
used in node descriptions, and Eva11,-,n the
set of possible attribute values.
</bodyText>
<footnote confidence="0.9732198">
4When a generation rule as shown in Figure 2 is
reversed, the conditions in the conditions-slot become
right-hand side statements. The context information
does not need to be modified since the formalism al-
lows for &apos;right-hand side contexts&apos;.
</footnote>
<bodyText confidence="0.4652815">
Then a directed attributed graph is a triple
= E, where N is a finite set of
nodes, E is a subset of Ee x N x N, A
is a subset of ((al x
</bodyText>
<equation confidence="0.8010795">
x ) U U
(an x
)) and aj E Ea , and i E
{Sem,DSynt,SSynt,DMorph,SMorph}.
</equation>
<bodyText confidence="0.91958625">
In (e, nl, n2) E E, n1 is the source node and
n2 is the target node of the edge e.
In this context, an MTT- grammar rule is a
graph rule of the following kind:
</bodyText>
<construct confidence="0.8815979">
Definition 2 (Graph Rule) A graph rule
is a quintuple GR = (G1,Gr,C,R,O. GI is
the left-hand side (connected) graph and Cr is
the right-hand side graph as defined in Defini-
tion 1. C is the set of conditions which must
hold in order for the rule to be applicable. R
is the relation between parts of GI and Cr. c
is a function that is defined for each node, and
edge: c(x) = {g1(x,y) E C0} with C, =N x
{context consume} UE x {context consume}.
</construct>
<bodyText confidence="0.9975155">
R is a subset of N x N; it is what in graph
grammar literature is called &amp;quot;embedding&amp;quot;. In
the most simple case, R holds between nodes
of GI and Cr.
Due to space restrictions, we don&apos;t intro-
duce the definition of the conditions here.
The interested reader can consult the MATE-
Manual (Bohnet et al., 2001a).
A graph grammar GG consists thus of a
set of static rules of the above kind. A graph
system compiles then a given &amp;quot;source&amp;quot; graph
using GG into a &amp;quot;destination&amp;quot; graph in our
scenario a structure at a given stratum into a
structure at the stratum adjacent to the for-
mer. It can thus be defined as outlined in the
next section.
</bodyText>
<sectionHeader confidence="0.983125" genericHeader="method">
3 Graph Systems
</sectionHeader>
<subsectionHeader confidence="0.997922">
3.1 Basic Approaches
</subsectionHeader>
<bodyText confidence="0.99855432038835">
Definition 3 (Graph System) A graph
system is a triple G = (GG,,GG,GG,). GG,
is a set of graphs at a given stratum; GG is
the graph grammar applicable to g E GG,,
and GG, is the set of graphs resulting from
the application of GG to g E GG,.
The problem one faces when using a Graph
System is to find the optimal strategy for
matching the Gis (see the Definition 2 above)
of the rules with fragments of GG,. As men-
tioned above, there are two different basic ap-
proaches for how to proceed: (i) sequential
graph rewriting and (ii) parallel graph rewrit-
ing (Rozenberg, 1997).
Sequential graph rewriting systems iden-
tify fragments of the source graph that match
with the left-hand side of one of the given
graph rules and replace these fragments with
the right-hand side of the rule in question.
By a successive application of the rules to the
source graph, the latter is rewritten. In the
course of the process, we have thus an in-
termediate graph that consists partly of the
vocabulary of the source side language and
partly of the vocabulary of the target side
language. The process terminates if there are
no more rules applicable to the intermediate
graph. The main problem one faces when
following the sequential graph rewriting ap-
proach is thus to figure out how to embed a
new chunk gained from the application of a
rule into the intermediate graph produced so
far.
The sequential graph rewriting approach
bears some disadvantages when applied to
generation. For instance, in order to achieve a
predictable resulting structure, the rules must
be ordered before hand. However, a prede-
fined ordering of rules is linguistically not jus-
tified. Furthermore, in generators that sepa-
rate the task of grammar processing from the
tasks of sentence planning (as is the case in
our generator), it must be possible to exam-
ine which alternative structures are possible
in the given situation context so as to invoke
the generation of the most appropriate one.
In a sequential graph rewriting approach, this
requires a non-trivial book keeping overhead
for backtracking or alternative structure pro-
cessing.
Parallel graph rewriting systems identify
parts of the source structure that correspond
to the left-hand side of one of the available
rules in the same way sequential graph rewrit-
ing systems do. However, unlike in a sequen-
tial system where the rules are applied in se-
quence to intermediate graph structure, in a
parallel system, first a &amp;quot;rule binding map&amp;quot; (or
&amp;quot;lock map&amp;quot;; see below) of the source graph is
created. In this map, it is indicated which
rules are applicable to which fragments of the
graph. This allows for the determination of
an optimal &amp;quot;coverage&amp;quot; of the source graph by
the available rules before the rules are actu-
ally executed &amp;quot;in one shot&amp;quot;. That is, no inter-
mediate graph structure is produced and no
unmodified parts of the source graph appear
in the resulting graph. The main problem one
faces when following the parallel graph rewrit-
ing approach is thus to find optimal strategies
for binding rules to the source graph and for
unifying the resulting fragments.
We chose the parallel approach because of
four reasons. First MTT defines the corre-
spondence of meaning and text in terms of
equative rules. This view is supported by the
parallel approach.5 Second, the parallel ap-
proach allows for a more powerful concept of
a context sensitive graph rule. That is, a par-
allel graph rewriting rule can contain a dec-
laration of a context a chunk of the source
graph which must be available for the rule to
be applicable, but which is not &amp;quot;consumed&amp;quot;,
i.e. mapped onto the target side, when the
rule is executed. Contexts provide an indis-
pensable means for making rules as specific
as necessary and as elementary as desired.
This is possible only because the source graph
does not change in the course of the process.
Third, in the parallel approach, the grammar-
ian does not need to take care of the order
in which the rules should be applied. Fur-
thermore, if the same grammatical resources
are to be made available for generation and
parsing, no hard wired order of rule execution
can be accepted. Fourth, the parallel graph
rewriting approach allows for (but does not
enforce) the generation of alternative result
structures. This is useful, e.g., for grammar
maintenance, and for advanced sentence plan-
ning strategies.
5Although, to our knowledge, all so far existing
mTT-based generators use the sequential approach.
</bodyText>
<subsectionHeader confidence="0.975338">
3.2 Implementing a Parallel Graph
System for Generation
</subsectionHeader>
<bodyText confidence="0.996885315789474">
A parallel graph system cycle. In the re-
alization of a parallel graph system for MTT-
grammars, the mapping between the graphs
of two adjacent strata Si and Si+j is per-
formed in cycles. A cycle consists of five
stages: (1) binding, (2) evaluation, (3) clus-
tering, (4) application and (5) unification.
In what follows, we briefly introduce (1) to
(5). Since the binding stage (in combination
with the evaluation of simple conditions) is
the most difficult (and the most interesting)
part, we discuss it in Section 4 in more detail.
Binding. In the source graph g.EG, all
parts that match the left-hand side d E
of one or several rules that are available for
5i&lt;=&gt;-5i+1 are identified and bound. Obviously,
a rule may match more than one part in the
source graph. To increase efficiency one-node
(= simple) conditions such as
</bodyText>
<equation confidence="0.928664333333333">
Lexicon::(?Xdsyn.lex).cat = noun
?Ydsynt.theme = yes
NOT ?Ydsynt.perspective = background
</equation>
<bodyText confidence="0.997315363636364">
are evaluated already during the binding
stage.
Evaluation of complex conditions. Af-
ter the binding stage, the evaluation of com-
plex conditions takes place. Unlike simple
conditions, complex conditions draw on sev-
eral nodes in the input structure; cf., e.g.:
?Xds.form = finite AND ?Yds.form =
infinite
The result of the evaluation stage are sets
of instances of applicable rules.
Clustering. During the clustering stage,
rules that are applicable together to the input
structure in question without contradicting
each other are grouped or &amp;quot;clustered&amp;quot;. Two
rules contradict if they apply to the same frag-
ment of the input structure.
The clusters are retrieved from the &amp;quot;lock
map&amp;quot;, which contains the association of rules
to fragments of the input structure.
Figure 3 shows a screen shot of the lock
map as presented in the inspector of MATE
</bodyText>
<table confidence="0.994970928571428">
nodes and relations f ins nit1 5 subie 3 stand
drinkl (m288e0) xl
in.1 (m289e1)
dawnd (m290e2)
Socrates:1 (m291 e3)
cup_of_hemlock.1 (m292e4)
forcel (m293e5)
assembly:1 (m294e6)
force.1 (rn293e5)-Ike drink 1 (... [129]
drinkl (m2e8e0)-ATTR-. ind xl
drinkl (m288e0)-l-e Socrate.. [124] xl
drink.1 (rn288e0)-Ike cup of xl
ind (m28991) -ll- e dawnd (m
force:1 (m293e5)-l-. assemb. xl
</table>
<figureCaption confidence="0.998429">
Figure 3: The lock map.
</figureCaption>
<bodyText confidence="0.987552289473684">
while processing the deep-syntactic structure
in Figure 1. The first column contains the
names of instantiated nodes and relations of
the input structure (in parentheses, the num-
bers of instances are given that are used by
the compiler for book keeping). The first row
contains the names of rules that apply to the
given input structure. The application of a
rule to a fragment of the input structure is
marked by an `x1&apos; in the respective slot of the
lock map matrix. `x1&apos; stands for &amp;quot;exclusive
lock&amp;quot;. That is, only one rule is allowed to ap-
ply to a fragment. If two locks occur in one
row, there is a contradiction and two clusters
are built. The numbers in the slots of the
lock map matrix are numbers of rule instances
that lock non-exclusively the respective parts
of the input structure as context.
Application. During the application
stage, the rule clusters specified in the lock
map are applied to the respective parts of the
input structure and thus fragments of the out-
put structure (as specified in the right-hand
sides of the rules) are generated. As Figure 4
shows, the result of this stage are isolated el-
ementary structures that are similar to ele-
mentary trees of a TAG and segments of the
Segment Grammar.
Unification. During the last stage, the
stage of unification, the elementary structures
are &amp;quot;glued&amp;quot; together. That is, nodes, which
correspond to the same source node are uni-
fied. For each cluster a result structure is gen-
erated. Operations at this stage are equiva-
lent to substitution in a TAG.
Cycle repetition. A repetition of the pro-
cessing cycle as sketched above becomes nec-
essary if one or several rules contain in the
</bodyText>
<figure confidence="0.994251631578947">
prepmffnite
drink drink
circumstantial cp*lr ink
dobjective
in cup_of hemlock
prepositional
dawn I? dawn
determinative
V
J the
force force
subjective Mf objective
3,4!
assembly
assembly
Socrates
&amp;quot;native
detem
the
</figure>
<bodyText confidence="0.982650333333333">
hand sides of ri E R (with R being the set of
rules available for the the mapping between
the Strata Si and Si+i ) and (2) binding (after
the first cycle) the target structure generated
so far with the contexts of ri E R. The output
of the algorithm are instances of applicable
rules. Each instance contains a copy of the
rule in question and a copy of the fragment of
the input structure this rule applies to.
</bodyText>
<figureCaption confidence="0.96123">
Figure 5 shows the binding of the rule
Figure 4: Elementary structures as produced
at the stage oft rule application.
</figureCaption>
<bodyText confidence="0.999183214285714">
context slot a target substructure (i.e., a
structure produced by the preceding rules).
In this case, the stages (I) to (5) are repeated
with rules which access the target structure
and rules that compete with target structure
accessing rules (i.e., rules that apply to the
same parts of the source structure).
The termination of cycle repetition is en-
sured since no correct grammar rule accesses
only the target structure. In other words,
each rule consumes some source structure in-
formation. The algorithm terminates when
the entire source structure has been &amp;quot;con-
sumed&amp;quot;.
</bodyText>
<sectionHeader confidence="0.988497" genericHeader="method">
4 Binding
</sectionHeader>
<bodyText confidence="0.9999954">
Above, we introduced as the first stage of
graph processing the binding of the left-hand
side in a source graph and the binding of the
context of a rule that contains a fragment
of the target structure. In this section, we
present the internals of the binding algorithm.
In order to keep the presentation as simple
as possible, we dispense with the discussion
of some advanced features of our approach.
This is for example the use of a rule hierachy,
the reuse of rule instances, and an optimized
strategy for rule evaluation; cf (Bohnet et al.,
2000).
In what follows, the algorithm is presented
and illustrated by an example.
</bodyText>
<subsectionHeader confidence="0.976115">
4.1 Basic Algorithm
</subsectionHeader>
<bodyText confidence="0.9969795">
The binding procedure consists of two stages:
(I) binding the source structure with the left-
</bodyText>
<figure confidence="0.9830819">
left-h.s.: ?Xds -I-&gt; ?Yds
conditions: NOT ?Xds.voice = passive
?Xds.form = finite
right-h.s.: ?Xss -subject ive-&gt; ?Yss
correspondences: ?Xss &lt;=&gt;?Xds
?Yss &lt;=&gt;?Yds
to the input substructure drink —I—&gt;
Socrates.
BMW ?Xds
k ?Xss
</figure>
<figureCaption confidence="0.999994">
Figure 5: Example of a rule binding.
</figureCaption>
<bodyText confidence="0.999776375">
(I) and (2) can be divided into (i) search-
ing for the initial node within the input
structure from which the matching procedure
starts; (ii) identify relations that match the
rule structure and the source/target structure
(starting with the initial node found before);
(iii) the actual binding of the nodes and rela-
tions.
</bodyText>
<subsubsectionHeader confidence="0.570914">
4.1.1 Searching for an Entry Node
</subsubsectionHeader>
<bodyText confidence="0.999808875">
The entry node search function loops over
all nodes of the input graph (-; and over all
candidate rules. If C is a predicate-argument
structure (i.e., a semantic net), the search
starts from any arbitrary node of (7; if (7 is a
tree structure, from the root node.
In pseudocode, the search function looks as
follows:
</bodyText>
<figure confidence="0.9213616875">
searchNodes (G , R)
Ires.u.0
for ni E G do
for rj E R do
•
cup_of hemlock
u1,3 getRuleNode (u„ r3)
flag evalSimpleCondit ions (u, 0 ,r3)
if flag == T
i createInstance (u„ u1,3 , r3)
I {i}
active-edges getEdges (u1,3)
for e, E active-edges do
I searchEdges (e3 , I, r3, G)
Irestat Irestat U
return
</figure>
<bodyText confidence="0.998891695652174">
In the inner for-loop, we first pick a node
nr/ j in rj. For efficiency, this is always the
node with the highest number of simple condi-
tions: compared to graph traversal, the eval-
uation of conditions is a &amp;quot;cheap&amp;quot; operation.
Then, we evaluate whether the graph node
under consideration ni matches the conditions
of the node picked, i.e. of ni . If the condi-
tions match, nr/ j is associated with (&amp;quot;bound&amp;quot;
to) n, and the triple (nri j,ni,rj is kept in the
set of bound node instances I. Otherwise, we
loop over the nodes in rj until either a bound-
ing node is found or one of the conditions of all
nodes in rj has been evaluated to F(alse). In
the first case, the incoming and the outgoing
edges of nr/ j must be further matched against
the incoming and the outgoing edges of n.
This is done in the function searchEdges af-
ter the function getEdges retrieved all edges
of which nr/ j is either the tail or the head node.
In the second case, rj is rejected.
The bound nodes and edges are kept in the
global variable /r„„it.
</bodyText>
<subsubsectionHeader confidence="0.510667">
4.1.2 Searching for Edges to Match
</subsubsectionHeader>
<bodyText confidence="0.9695975">
Once an edge eirj in a rule rj has been se-
lected for matching, the task is to identify
edges in the graph to which 4c an be bound.
A rule edge is defined as bound if both its tail
node and its head node are bound. Above,
the node bounding information has been kept
in I (in terms of triples (nri j,ni,rj). There-
fore, the function searchEdges checks first if
I contains instances of both nodes of eirj. If
yes, ei . is immediately added to the set of
r,
bound edges. Otherwise, we proceed with its
bound node nr/ j (recall that searchEdges is
invoked after the entry node binding proce-
dure has been performed). If nr/ j is the tail
node of eirj, all edges for which the graph
node ni is the tail node are plausible bind-
ing candidates. If nri, is the head node of eir,,
all edges for which ni is the head node are
plausible binding candidates. searchEdges
retrieves the graph edges accordingly and in-
vokes the function bindEdges.
</bodyText>
<equation confidence="0.551235071428571">
searchEdges (e, I,r,,G)
Irestat {}
for i E I do
if boundP (tail (e) ,i) &amp; boundP (head (e) ,i)
Irestat Irestat u fil
else
r/1 = getBoundNode (e ,i)
if tai1P(nl,e)
E get Out (getCorrespondence (r/l, i, G))
else
E get In (getCorrespondence (r/l, i, G))
bindEdges (E, r3, G)
return
4.1.3 Bind Edges
</equation>
<bodyText confidence="0.999873454545454">
The function searchEdges thus identifies a
set of edges E in G that potentially match
with an edge e&amp;quot; of the rule rj and calls
bindEdges. The function bindEdges does the
actual evaluation and binding. If the name of
an ei E E matches with the name of e&amp;quot; and the
not yet bound node of ei &apos;Wei fulfills the con-
ditions of the not yet bound node of e&amp;quot; ,
n&apos;e, is bound to n&apos;ei. The instance of which
ei is part is copied, and the triple We- ,niej,rj
is added to the copy i„:„ before the set of in-
stances dealt with I is initialized with ic„.
If this was the last unbound edge of the
rule rj, the complex conditions of the rule are
checked. If they are fulfilled, the rule and
the fragment of G it applies to are introduced
into the lock map. If e&amp;quot; was not the last un-
bound edge, the function searchEdges is in-
voked with each incoming and each outgoing
edge of n&apos;e.
In pseudocode, the function bindEdges
reads as follows:
</bodyText>
<equation confidence="0.767929833333334">
bindEdges (E, i, r 3 , G)
for ei E E do
if (name(ei) narrte(e&apos;))
next ei
end if
= gat,&apos; nbound(e&apos; )
= gat,&apos; nbound(e
if condit ,r, ) t
next ei
end if
copy (i)
U (r/1,?3- ,r4„r3)
/
active-edges getEdges (4,-)
for e„ E active-edges
/ searchEdges(e„, /, r3, G)
Irestat Irestat U
return
</equation>
<subsectionHeader confidence="0.924454">
4.2 Example
</subsectionHeader>
<bodyText confidence="0.995649555555556">
This section illustrates how the algorithm
that has been presented above functions in
practice. It shows the application of the
rule introduced in Figure 2, which maps
the second syntactic actant onto the surface-
syntactic relation dobjective.
al. Searching for an entry node.
As pointed out above, in tree structures,
the seach of an entry node starts with the
root. In our sample structure, this is the node
force. In the rule, the node with highest
number of simple conditions is ?Xds. force
meets the conditions specified for ?Xds: its
cat feature is set to verb and its voice fea-
ture is set to active (i.e., not passive).
Therefore, force is bound to ?Xds.
?Xds has two outgoing edges:
?Xds-I-&gt;?Zds and ?Xds-II-&gt;?Yds (with
?Xds-I-&gt;?Zds being in the context). For
both the function searchEdges is invoked.
bl. Searching for edges to match.
In ?Xds-I-&gt;?Zds, the ?Xds node is bound,
while ?Zds is not. Therefore, we get all edges
in G in which the node to which ?Xds is bound
(= force) is the tail. These are the edges
force-I-&gt;assembly, force-II-&gt;Socrates,
and force-III-&gt;drink.
</bodyText>
<subsectionHeader confidence="0.93476">
cl. Binding edges.
</subsectionHeader>
<bodyText confidence="0.9999853">
In the function bindEdges, the relation I
in force-I-&gt;assembly matches with the re-
lation I in ?Xds-I-&gt;?Zds, and assembly ful-
fills the conditions specified for ?Zds. There-
fore, ?YZds is bound to assembly and, subse-
quently, the edge ?Xds-I-&gt;?Zds is bound to
the edge force-I-&gt;assembly.
The node assembly has no other incoming
and outgoing edges. The recursion stops thus
at this point.
</bodyText>
<subsectionHeader confidence="0.859607">
c2./c3. Binding edges.
</subsectionHeader>
<bodyText confidence="0.980849">
The relation II in force-II-&gt;Socrates
and the relation III in force-III-&gt;drink
do not match with the relation I in
?Xds-I-&gt;?Zds and are thus both rejected.
b2. Searching for edges to match.
As in ?Xds-I-&gt;?Zds, in ?Xds-II-&gt;?Yds,
the tail node is bound while the head node
is not. The edges we get at this point
for processing are the same as above for
?Xds-I-&gt;?Zds.
</bodyText>
<subsectionHeader confidence="0.896516">
c4./c5./c6 Binding edges.
</subsectionHeader>
<bodyText confidence="0.999987866666667">
From the three edges evaluated, one,
namely f orce-I I-&gt; Socrat es , is found
to match the rule edge ?Xds-II-&gt;?Yds.
?Xds-II-&gt;?Yds is thus bound to it. This is
the last edge of the rule under examination to
be bound. That is, the rule can be applied.
The bounding information is introduced
into the lock map (an exclusive lock for
the left-hand side edge and a rule instance
reference for the context edge).
The other nodes of the input structure are
examined along these lines and another frag-
ment to which the rule in question can be
applied is identified: Socrates &lt;-1- drink
-II-&gt; cup_of_hemlock.
</bodyText>
<subsectionHeader confidence="0.9988">
4.3 Some Complexity Considerations
</subsectionHeader>
<bodyText confidence="0.9901856">
To estimate the complexity of the binding al-
gorithm, we count the binding attempts for
both nodes and edges.
For nodal rules, the cost is IGI x IRI, where
I Cl stands for the number of the nodes in the
graph IGI and IRI is the number of rules. The
number of rules can be considered as constant.
That is, we get the complexity of 0(n) (with
n =
For one-edge rules, we get in the worst case
a cost of ICI x - I)
I RI . Since, again,
the number of rules can be considered as be-
ing constant, we arrive at 0(n2). Given that
the number of types of outgoing and incoming
edges is very restricted. Thus, at the deep-
syntactic stratum there are only nine6 (I-VI,
ATTR, COORD, and APPEND), and most
of the rules contain nodal conditions, which
are evaluated first, in practice, the complexity
</bodyText>
<footnote confidence="0.595099666666667">
6 We did not introduce all of these relations because
they were not important for the understanding of the
approach.
</footnote>
<bodyText confidence="0.99142716">
is near 0(n). However, obviously, the com-
plexity rises with the number of edges in the
rules. Especially in cases where more than
five edges of the same type appear in rules
we run into a combinatorial explosion. The
nature of the binding problem, remains, after
all, NP complete. But such rules appear if
at all very seldom; the overwhelming ma-
jority of the rules contains no more than four
edges, rather less.
The complexity of clustering depends on
the number of alternative rules for the same
chunk of the input structure. In the (hypo-
thetical) worst case, where all rules in the
grammar are alternative, it is thus again NP
complete. However, this case never occurs:
the number of alternative rules is strictly con-
strained.
In applications, the run time of the algo-
rithm is acceptable: AutoText-UIS(Bohnet et
al., 2001b)—a text generator which uses a
release of our formalism implementation gen-
erates 60 air pollution reports with five com-
plex sentences each, in about three minutes
on a Pentium III PC with 800 MHz.
</bodyText>
<sectionHeader confidence="0.999941" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.997572166666666">
Tree rewriting which is the more constrained
version of graph rewriting has been central in
transfer-oriented MT for a long time. In the
last few years, there has been increasing in-
terest in MT in graph rewriting (Einele and
Dorna, 1998; Frank, 1999; Dymetman and
Tendeau, 2000).
Tree rewriting is also used in generation
for instance, by the 101-r-based generator Re-
alPro (Lavoie and Rainbow, 1997; Lavoie et
al., 2000). Other well-known 101-r-based gen-
erators such as GOSSIP (Iordanskaja et al.,
1988) use a sequential graph rewriting formal-
ism.
Apart from 101-r-oriented approaches, there
are several other approaches in generation
that are related to our work. In what follows,
we would like to mention two of them. The
first is (Nicolov et al., 1996)&apos;s work. The dif-
ference between Nicolov et al.&apos;s approach and
ours is threefold. First, Nicolov et al. use a
graph rewriting grammar formalism, while we
use a parallel graph rewriting formalism. Sec-
ond, we strictly separate between grammati-
cal processing and tasks of sentence planning.
Our grammar rules thus do not contain any
but linguistic conditions, while Nicolov et al.&apos;s
rules may also contain pragmatic and situa-
tional conditions. And third, finally, Nicolov
et al. use complex rules which cover whole
fragments of the input structure. As a result,
it may well occur that with the rules chosen
not all of the input structure is rendered into
wording. This makes it necessary to evaluate
the rules at disposal with respect to their po-
tential to (i) cover best the remaining parts of
the input structure and (ii) to be compatible
with the rules already applied. In our ap-
proach, with most of the rules covering only
one edge or node (or even a feature of a node),
the probability of this problem is reduced to
nearly zero. Furthermore, the lock map pro-
vides a full picture of how the different rules
cover the input structure. This allows for an
optimal mapping of the input structure onto
the output structure.
Our approach also resembles Beale&apos;s
constraint-satisfaction based Hunter and
Gatherer-strategy (Beale, 1997; Beale et al.,
1998) in that Hunter and Gatherer is a par-
allel graph rewriting formalism. However,
Beale&apos;s approach is an integrated approach.
It contains all information necessary for gen-
eration in the lexicon (including discourse in-
formation) in terms of structures that can be
interpreted as complex mapping rules. Also,
the binding strategy in Hunter and Gatherer
is different: before the actual binding stage
takes place, chunks of the input structure
that possess a minimal number of relations
to other chunks are recursively identified.
The transformation of chunks with a mini-
mal number of connections to other chunks
reduces the number of conflicting cases dur-
ing the stage of gluing together the resulting
substructures.
</bodyText>
<sectionHeader confidence="0.998642" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9985558">
In this paper, we presented a parallel graph
rewriting formalism and illustrated how this
formalism can be used to implement a gram-
mar for generation. Although the implemen-
tation is for MTT, the algorithm is per se the-
ory independent.
An optimization of the rewriting proce-
dure can be achieved by making use of a
rule generalization hierarchy. Then, the state-
ments that are located higher in the hierarchy
are evaluated first, which means that whole
classes of rules can be excluded from evalu-
ation very early. See (Wanner and Bohnet,
forthcoming) for details.
Unlike in many generators, we consider the
grammar to be a resource (in the same vein as
a knowledge base and a lexicon are resources)
rather than a generation module. This re-
source is used by different sentence planning
modules to render a semantic structure into a
wording according to communicative and con-
textual criteria. However, it is beyond the
scope of this paper to describe how the sen-
tence planning mechanisms make use of the
grammatical resource.
</bodyText>
<sectionHeader confidence="0.978925" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997405">
Many thanks to the two anonymous reviewers
for helpful comments and suggestions.
</bodyText>
<sectionHeader confidence="0.999255" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999808868421053">
S. Beale, S. Nirenburg, E. Viegas, and L. Wan-
ner. 1998. De-Constraining Text Generation.
In Proceedings of the International Workshop
on Natural Language Generation, Niagara-on-
the-Lake, ON, Canada.
S. Beale. 1997. HUNTER-GATHERER: Apply-
ing Constraint Satisfaction, Branch-and-Bound
and Solution Synthesis to Computational Se-
mantics. Ph.D. thesis, School of Computer Sci-
ence, Carnegie Mellon University.
B. Bohnet, A. Langjahr, and L. Wanner. 2000. A
Development Environment for an MTT-Based
Sentence Generator. In Proceedings of the
First International Natural Language Genera-
tion Conference.
B. Bohnet, A. Langjahr, and L. Wanner. 2001a.
MATE-Manual. University Stuttgart.
B. Bohnet, L. Wanner, and et al. 2001b.
Autotext-UIS — Automatische Erstellung von
Ozonkurzberichten im Rahmen des Umwelt-
informationssystems Baden-Wiirttemberg. In
Workshop Hypermedia im Umweltschutz.
M. Dymetman and F. Tendeau. 2000. Context-
Free Grammar Rewriting and the Transfer of
Packed Linguistic Representations. In COL-
ING 2000.
M. Emele and M. Dorna. 1998. Ambiguity
Preserving Machine Translation Using Packed
Representations. In COLING 1998, pages 365-
371.
A. Frank. 1999. From Parallel Grammar Develop-
ment towards Machine Translation — A Project
Overview —. In MT-Summit VII. MT in the
Great Translation Era, pages 134-142.
L. N. Iordanskaja, R. Kittredge, and A. Polguere.
1988. Implementing a Meaning-Text Model for
Language Generation. In COLING 1988.
S. Kahane. forthcoming. Transductive Genera-
tive and Equative Grammars. In Polguere A.
&amp; Wanner L., editor, Selected Topics in Depen-
dency Grammar. Benj amins, Amsterdam.
B. Lavoie and 0. Rambow. 1997. A Fast and
Portable Realizer for Text Generation Systems.
In Proceedings of the ANLP Conference.
B. Lavoie, R. Kittredge, T. Korelsky, and 0. Ram-
bow. 2000. A Framework for MT and Multilin-
gual NLG Systems Based on Uniform Lexico-
Structural Processing. In Proceedings of the
ANLP/NAACL Conference.
I.A. Mel&apos;Cuk and L. Wanner. forthcoming. To-
wards a Lexicographic Approach to Lexical
Transfer in Machine Translation (Illustrated by
the German-Russian Language Pair). Machine
Translation Journal.
I.A. Mel&apos;Cuk. 1981. &amp;quot;Meaning-Text Models: A
Recent Trend in Soviet Linguistics&amp;quot;. Annual
Review of Anthropology, 10:27-62.
I.A. Mel&apos;Cuk. 1988. Dependency Syntax: The-
ory and Practice. State University of New York
Press, Albany.
N. Nicolov, C. Mellish, and G. Richie. 1996.
Approximate Generation from Non-hierarchical
Representations. In Proceedings of the 8th
International Workshop on Natural Language
Generation, Herstmonceux.
G. Rozenberg, editor. 1997. Handbook of Graph
Grammars and Computing by Graph Transfor-
mation. World Scientific, Singapore, New Jer-
sey, London, Hong Kong.
M. Stede. 1999. Lexical Semantics and Knowl-
edge Representation in Multilingual Text Gen-
eration. Kluwer Academic Publishers Group.
L. Wanner and B. Bohnet. forthcoming. Inher-
itance in a MTT Grammar. In Polgu&apos;ere A.
&amp; Wanner L., editor, Selected Topics in Depen-
dency Grammar. Benj amins, Amsterdam.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.919557">
<title confidence="0.999976">On Using a Parallel Graph Rewriting Formalism in Generation</title>
<author confidence="0.988865">Bernd Bohnet</author>
<author confidence="0.988865">Leo</author>
<affiliation confidence="0.998346">CS Department, Intelligent Systems University of</affiliation>
<address confidence="0.965573">Breitwiesenstr. 20 - 22, 70565 Stuttgart,</address>
<email confidence="0.993246">{bohnetlwannerAinformatik.uni-stuttgart.de}</email>
<abstract confidence="0.997009153846154">In this paper, we present a parallel context sensitive graph rewriting formalism for a dependency-oriented generation grammar. The parallel processing of the input structure makes an explicit presentation of all alternative options for its mapping onto the output structure possible. This allows for the selection of the linguistic realization that suits best the communicative and contextual criteria available.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Beale</author>
<author>S Nirenburg</author>
<author>E Viegas</author>
<author>L Wanner</author>
</authors>
<title>De-Constraining Text Generation.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Workshop on Natural Language Generation,</booktitle>
<location>Niagara-onthe-Lake, ON,</location>
<contexts>
<context position="1685" citStr="Beale et al., 1998" startWordPosition="251" endWordPosition="254">ecified in the formal language L2, by using explicitly or implicitly defined rewriting rules.&apos; Explicit rewriting rules may have the format of classic rewriting rules, as, e.g., in (Frank, 1999) or of bidirectional rules that establish a correspondence relation between minimal structures of Li and L2, as, e.g., in (Iordanskaja et al., 1988; Lavoie and Rainbow, 1997). Implicit rewriting rules are encoded in terms of LI-constraints that are associated with structure chunks and lexical items of L2; see, e.g., &apos;Note that Li and £2 may be identical, but do not need to be so. (Nicolov et al., 1996; Beale et al., 1998; Stede, 1999.). A parallel graph-rewriting formalism maps a given input structure to an output structure instead of transforming the former into the latter. Although parallel graph-rewriting shows several advantages when compared to sequential graph rewriting (see Section 3), sequential graph-rewriting formalisms are much more common. In this paper, we present the implementation of a parallel graph rewriting formalism for the grammar of the Meaning-Text Theory (1017) (Mel&apos;Cuk, 1981; Mel&apos;Cuk, 1988). The focus of the presentation is on one of the major stages of the algorithm: the spelling out </context>
<context position="31714" citStr="Beale et al., 1998" startWordPosition="5411" endWordPosition="5414">ct to their potential to (i) cover best the remaining parts of the input structure and (ii) to be compatible with the rules already applied. In our approach, with most of the rules covering only one edge or node (or even a feature of a node), the probability of this problem is reduced to nearly zero. Furthermore, the lock map provides a full picture of how the different rules cover the input structure. This allows for an optimal mapping of the input structure onto the output structure. Our approach also resembles Beale&apos;s constraint-satisfaction based Hunter and Gatherer-strategy (Beale, 1997; Beale et al., 1998) in that Hunter and Gatherer is a parallel graph rewriting formalism. However, Beale&apos;s approach is an integrated approach. It contains all information necessary for generation in the lexicon (including discourse information) in terms of structures that can be interpreted as complex mapping rules. Also, the binding strategy in Hunter and Gatherer is different: before the actual binding stage takes place, chunks of the input structure that possess a minimal number of relations to other chunks are recursively identified. The transformation of chunks with a minimal number of connections to other c</context>
</contexts>
<marker>Beale, Nirenburg, Viegas, Wanner, 1998</marker>
<rawString>S. Beale, S. Nirenburg, E. Viegas, and L. Wanner. 1998. De-Constraining Text Generation. In Proceedings of the International Workshop on Natural Language Generation, Niagara-onthe-Lake, ON, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Beale</author>
</authors>
<title>HUNTER-GATHERER: Applying Constraint Satisfaction, Branch-and-Bound and Solution Synthesis to Computational Semantics.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>School of Computer Science, Carnegie Mellon University.</institution>
<contexts>
<context position="31693" citStr="Beale, 1997" startWordPosition="5409" endWordPosition="5410">al with respect to their potential to (i) cover best the remaining parts of the input structure and (ii) to be compatible with the rules already applied. In our approach, with most of the rules covering only one edge or node (or even a feature of a node), the probability of this problem is reduced to nearly zero. Furthermore, the lock map provides a full picture of how the different rules cover the input structure. This allows for an optimal mapping of the input structure onto the output structure. Our approach also resembles Beale&apos;s constraint-satisfaction based Hunter and Gatherer-strategy (Beale, 1997; Beale et al., 1998) in that Hunter and Gatherer is a parallel graph rewriting formalism. However, Beale&apos;s approach is an integrated approach. It contains all information necessary for generation in the lexicon (including discourse information) in terms of structures that can be interpreted as complex mapping rules. Also, the binding strategy in Hunter and Gatherer is different: before the actual binding stage takes place, chunks of the input structure that possess a minimal number of relations to other chunks are recursively identified. The transformation of chunks with a minimal number of c</context>
</contexts>
<marker>Beale, 1997</marker>
<rawString>S. Beale. 1997. HUNTER-GATHERER: Applying Constraint Satisfaction, Branch-and-Bound and Solution Synthesis to Computational Semantics. Ph.D. thesis, School of Computer Science, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Bohnet</author>
<author>A Langjahr</author>
<author>L Wanner</author>
</authors>
<title>A Development Environment for an MTT-Based Sentence Generator.</title>
<date>2000</date>
<booktitle>In Proceedings of the First International Natural Language Generation Conference.</booktitle>
<contexts>
<context position="5922" citStr="Bohnet et al., 2000" startWordPosition="923" endWordPosition="926">c structures and the rules in MTT can be represented in terms of typed feature structures; see (Mebeuk and Wanner, forthcoming). Figure 1: Deep-syntactic and surfacesyntactic structures of the sentence The assembly forced Socrates to drink the cup of hemlock in the dawn. device (Kahane, forthcoming). It consists of a set of rules that establish the correspondence between minimal structures at two adjacent strata with a minimal structure being a feature of a node, a node, a relation between two nodes, or a configuration of relations. Figure 2 shows a sample grammar rule as implemented in MATE (Bohnet et al., 2000). This rule maps the deep-syntactic relation II (the second actant) onto the surface-syntactic relation dobjective (i.e., direct object). The rule applies if there is the first actant available (i.e.., the relation I is specified in the input structure), the verbal head of the structure contains the attribute-feature pair &apos;cat = verb&apos;, and no attribute-value pair &apos;voice = passive&apos;.3 The relation I is specified as being in the context. That is, it is not &amp;quot;consumed&amp;quot; by the rule; it rather serves as a constraint for 3Note that if there would be no actant I available, in order to get a grammatical</context>
<context position="20233" citStr="Bohnet et al., 2000" startWordPosition="3377" endWordPosition="3380">ates when the entire source structure has been &amp;quot;consumed&amp;quot;. 4 Binding Above, we introduced as the first stage of graph processing the binding of the left-hand side in a source graph and the binding of the context of a rule that contains a fragment of the target structure. In this section, we present the internals of the binding algorithm. In order to keep the presentation as simple as possible, we dispense with the discussion of some advanced features of our approach. This is for example the use of a rule hierachy, the reuse of rule instances, and an optimized strategy for rule evaluation; cf (Bohnet et al., 2000). In what follows, the algorithm is presented and illustrated by an example. 4.1 Basic Algorithm The binding procedure consists of two stages: (I) binding the source structure with the leftleft-h.s.: ?Xds -I-&gt; ?Yds conditions: NOT ?Xds.voice = passive ?Xds.form = finite right-h.s.: ?Xss -subject ive-&gt; ?Yss correspondences: ?Xss &lt;=&gt;?Xds ?Yss &lt;=&gt;?Yds to the input substructure drink —I—&gt; Socrates. BMW ?Xds k ?Xss Figure 5: Example of a rule binding. (I) and (2) can be divided into (i) searching for the initial node within the input structure from which the matching procedure starts; (ii) identify</context>
</contexts>
<marker>Bohnet, Langjahr, Wanner, 2000</marker>
<rawString>B. Bohnet, A. Langjahr, and L. Wanner. 2000. A Development Environment for an MTT-Based Sentence Generator. In Proceedings of the First International Natural Language Generation Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Bohnet</author>
<author>A Langjahr</author>
<author>L Wanner</author>
</authors>
<date>2001</date>
<institution>MATE-Manual. University</institution>
<contexts>
<context position="9699" citStr="Bohnet et al., 2001" startWordPosition="1590" endWordPosition="1593"> the right-hand side graph as defined in Definition 1. C is the set of conditions which must hold in order for the rule to be applicable. R is the relation between parts of GI and Cr. c is a function that is defined for each node, and edge: c(x) = {g1(x,y) E C0} with C, =N x {context consume} UE x {context consume}. R is a subset of N x N; it is what in graph grammar literature is called &amp;quot;embedding&amp;quot;. In the most simple case, R holds between nodes of GI and Cr. Due to space restrictions, we don&apos;t introduce the definition of the conditions here. The interested reader can consult the MATEManual (Bohnet et al., 2001a). A graph grammar GG consists thus of a set of static rules of the above kind. A graph system compiles then a given &amp;quot;source&amp;quot; graph using GG into a &amp;quot;destination&amp;quot; graph in our scenario a structure at a given stratum into a structure at the stratum adjacent to the former. It can thus be defined as outlined in the next section. 3 Graph Systems 3.1 Basic Approaches Definition 3 (Graph System) A graph system is a triple G = (GG,,GG,GG,). GG, is a set of graphs at a given stratum; GG is the graph grammar applicable to g E GG,, and GG, is the set of graphs resulting from the application of GG to g E</context>
<context position="29398" citStr="Bohnet et al., 2001" startWordPosition="5025" endWordPosition="5028">plosion. The nature of the binding problem, remains, after all, NP complete. But such rules appear if at all very seldom; the overwhelming majority of the rules contains no more than four edges, rather less. The complexity of clustering depends on the number of alternative rules for the same chunk of the input structure. In the (hypothetical) worst case, where all rules in the grammar are alternative, it is thus again NP complete. However, this case never occurs: the number of alternative rules is strictly constrained. In applications, the run time of the algorithm is acceptable: AutoText-UIS(Bohnet et al., 2001b)—a text generator which uses a release of our formalism implementation generates 60 air pollution reports with five complex sentences each, in about three minutes on a Pentium III PC with 800 MHz. 5 Related Work Tree rewriting which is the more constrained version of graph rewriting has been central in transfer-oriented MT for a long time. In the last few years, there has been increasing interest in MT in graph rewriting (Einele and Dorna, 1998; Frank, 1999; Dymetman and Tendeau, 2000). Tree rewriting is also used in generation for instance, by the 101-r-based generator RealPro (Lavoie and R</context>
</contexts>
<marker>Bohnet, Langjahr, Wanner, 2001</marker>
<rawString>B. Bohnet, A. Langjahr, and L. Wanner. 2001a. MATE-Manual. University Stuttgart. B. Bohnet, L. Wanner, and et al. 2001b.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Autotext-UIS</author>
</authors>
<title>Automatische Erstellung von Ozonkurzberichten im Rahmen des Umweltinformationssystems Baden-Wiirttemberg.</title>
<booktitle>In Workshop Hypermedia im Umweltschutz.</booktitle>
<marker>Autotext-UIS, </marker>
<rawString>Autotext-UIS — Automatische Erstellung von Ozonkurzberichten im Rahmen des Umweltinformationssystems Baden-Wiirttemberg. In Workshop Hypermedia im Umweltschutz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dymetman</author>
<author>F Tendeau</author>
</authors>
<title>ContextFree Grammar Rewriting and the Transfer of Packed Linguistic Representations. In</title>
<date>2000</date>
<booktitle>COLING</booktitle>
<contexts>
<context position="29890" citStr="Dymetman and Tendeau, 2000" startWordPosition="5109" endWordPosition="5112">ternative rules is strictly constrained. In applications, the run time of the algorithm is acceptable: AutoText-UIS(Bohnet et al., 2001b)—a text generator which uses a release of our formalism implementation generates 60 air pollution reports with five complex sentences each, in about three minutes on a Pentium III PC with 800 MHz. 5 Related Work Tree rewriting which is the more constrained version of graph rewriting has been central in transfer-oriented MT for a long time. In the last few years, there has been increasing interest in MT in graph rewriting (Einele and Dorna, 1998; Frank, 1999; Dymetman and Tendeau, 2000). Tree rewriting is also used in generation for instance, by the 101-r-based generator RealPro (Lavoie and Rainbow, 1997; Lavoie et al., 2000). Other well-known 101-r-based generators such as GOSSIP (Iordanskaja et al., 1988) use a sequential graph rewriting formalism. Apart from 101-r-oriented approaches, there are several other approaches in generation that are related to our work. In what follows, we would like to mention two of them. The first is (Nicolov et al., 1996)&apos;s work. The difference between Nicolov et al.&apos;s approach and ours is threefold. First, Nicolov et al. use a graph rewritin</context>
</contexts>
<marker>Dymetman, Tendeau, 2000</marker>
<rawString>M. Dymetman and F. Tendeau. 2000. ContextFree Grammar Rewriting and the Transfer of Packed Linguistic Representations. In COLING 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Emele</author>
<author>M Dorna</author>
</authors>
<title>Ambiguity Preserving Machine Translation Using Packed Representations.</title>
<date>1998</date>
<booktitle>In COLING</booktitle>
<pages>365--371</pages>
<marker>Emele, Dorna, 1998</marker>
<rawString>M. Emele and M. Dorna. 1998. Ambiguity Preserving Machine Translation Using Packed Representations. In COLING 1998, pages 365-371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Frank</author>
</authors>
<title>From Parallel Grammar Development towards Machine Translation — A Project Overview —.</title>
<date>1999</date>
<booktitle>In MT-Summit VII. MT in the Great Translation Era,</booktitle>
<pages>134--142</pages>
<contexts>
<context position="1261" citStr="Frank, 1999" startWordPosition="177" endWordPosition="178">criteria available. 1 Introduction Graph-rewriting formalisms received a considerable attention in generation grammar implementations and in the area of transfer in machine translation. A graph-rewriting formalism is either sequential or parallel (Rozenberg, 1997). A sequential graph-rewriting formalism gradually transforms an input structure specified in the formal language Li into an output structure, which is specified in the formal language L2, by using explicitly or implicitly defined rewriting rules.&apos; Explicit rewriting rules may have the format of classic rewriting rules, as, e.g., in (Frank, 1999) or of bidirectional rules that establish a correspondence relation between minimal structures of Li and L2, as, e.g., in (Iordanskaja et al., 1988; Lavoie and Rainbow, 1997). Implicit rewriting rules are encoded in terms of LI-constraints that are associated with structure chunks and lexical items of L2; see, e.g., &apos;Note that Li and £2 may be identical, but do not need to be so. (Nicolov et al., 1996; Beale et al., 1998; Stede, 1999.). A parallel graph-rewriting formalism maps a given input structure to an output structure instead of transforming the former into the latter. Although parallel </context>
<context position="29861" citStr="Frank, 1999" startWordPosition="5107" endWordPosition="5108"> number of alternative rules is strictly constrained. In applications, the run time of the algorithm is acceptable: AutoText-UIS(Bohnet et al., 2001b)—a text generator which uses a release of our formalism implementation generates 60 air pollution reports with five complex sentences each, in about three minutes on a Pentium III PC with 800 MHz. 5 Related Work Tree rewriting which is the more constrained version of graph rewriting has been central in transfer-oriented MT for a long time. In the last few years, there has been increasing interest in MT in graph rewriting (Einele and Dorna, 1998; Frank, 1999; Dymetman and Tendeau, 2000). Tree rewriting is also used in generation for instance, by the 101-r-based generator RealPro (Lavoie and Rainbow, 1997; Lavoie et al., 2000). Other well-known 101-r-based generators such as GOSSIP (Iordanskaja et al., 1988) use a sequential graph rewriting formalism. Apart from 101-r-oriented approaches, there are several other approaches in generation that are related to our work. In what follows, we would like to mention two of them. The first is (Nicolov et al., 1996)&apos;s work. The difference between Nicolov et al.&apos;s approach and ours is threefold. First, Nicolo</context>
</contexts>
<marker>Frank, 1999</marker>
<rawString>A. Frank. 1999. From Parallel Grammar Development towards Machine Translation — A Project Overview —. In MT-Summit VII. MT in the Great Translation Era, pages 134-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L N Iordanskaja</author>
<author>R Kittredge</author>
<author>A Polguere</author>
</authors>
<title>Implementing a Meaning-Text Model for Language Generation. In</title>
<date>1988</date>
<booktitle>COLING</booktitle>
<contexts>
<context position="1408" citStr="Iordanskaja et al., 1988" startWordPosition="200" endWordPosition="203">nd in the area of transfer in machine translation. A graph-rewriting formalism is either sequential or parallel (Rozenberg, 1997). A sequential graph-rewriting formalism gradually transforms an input structure specified in the formal language Li into an output structure, which is specified in the formal language L2, by using explicitly or implicitly defined rewriting rules.&apos; Explicit rewriting rules may have the format of classic rewriting rules, as, e.g., in (Frank, 1999) or of bidirectional rules that establish a correspondence relation between minimal structures of Li and L2, as, e.g., in (Iordanskaja et al., 1988; Lavoie and Rainbow, 1997). Implicit rewriting rules are encoded in terms of LI-constraints that are associated with structure chunks and lexical items of L2; see, e.g., &apos;Note that Li and £2 may be identical, but do not need to be so. (Nicolov et al., 1996; Beale et al., 1998; Stede, 1999.). A parallel graph-rewriting formalism maps a given input structure to an output structure instead of transforming the former into the latter. Although parallel graph-rewriting shows several advantages when compared to sequential graph rewriting (see Section 3), sequential graph-rewriting formalisms are muc</context>
<context position="30115" citStr="Iordanskaja et al., 1988" startWordPosition="5144" endWordPosition="5147">llution reports with five complex sentences each, in about three minutes on a Pentium III PC with 800 MHz. 5 Related Work Tree rewriting which is the more constrained version of graph rewriting has been central in transfer-oriented MT for a long time. In the last few years, there has been increasing interest in MT in graph rewriting (Einele and Dorna, 1998; Frank, 1999; Dymetman and Tendeau, 2000). Tree rewriting is also used in generation for instance, by the 101-r-based generator RealPro (Lavoie and Rainbow, 1997; Lavoie et al., 2000). Other well-known 101-r-based generators such as GOSSIP (Iordanskaja et al., 1988) use a sequential graph rewriting formalism. Apart from 101-r-oriented approaches, there are several other approaches in generation that are related to our work. In what follows, we would like to mention two of them. The first is (Nicolov et al., 1996)&apos;s work. The difference between Nicolov et al.&apos;s approach and ours is threefold. First, Nicolov et al. use a graph rewriting grammar formalism, while we use a parallel graph rewriting formalism. Second, we strictly separate between grammatical processing and tasks of sentence planning. Our grammar rules thus do not contain any but linguistic cond</context>
</contexts>
<marker>Iordanskaja, Kittredge, Polguere, 1988</marker>
<rawString>L. N. Iordanskaja, R. Kittredge, and A. Polguere. 1988. Implementing a Meaning-Text Model for Language Generation. In COLING 1988.</rawString>
</citation>
<citation valid="false">
<authors>
<author>forthcoming</author>
</authors>
<title>Transductive Generative and Equative Grammars.</title>
<booktitle>Selected Topics in Dependency Grammar. Benj amins,</booktitle>
<editor>In Polguere A. &amp; Wanner L., editor,</editor>
<location>Amsterdam.</location>
<marker>forthcoming, </marker>
<rawString>S. Kahane. forthcoming. Transductive Generative and Equative Grammars. In Polguere A. &amp; Wanner L., editor, Selected Topics in Dependency Grammar. Benj amins, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lavoie</author>
</authors>
<title>A Fast and Portable Realizer for Text Generation Systems.</title>
<date>1997</date>
<booktitle>In Proceedings of the ANLP Conference.</booktitle>
<marker>Lavoie, 1997</marker>
<rawString>B. Lavoie and 0. Rambow. 1997. A Fast and Portable Realizer for Text Generation Systems. In Proceedings of the ANLP Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lavoie</author>
<author>R Kittredge</author>
<author>T Korelsky</author>
</authors>
<date>2000</date>
<booktitle>A Framework for MT and Multilingual NLG Systems Based on Uniform LexicoStructural Processing. In Proceedings of the ANLP/NAACL Conference.</booktitle>
<contexts>
<context position="30032" citStr="Lavoie et al., 2000" startWordPosition="5132" endWordPosition="5135">rator which uses a release of our formalism implementation generates 60 air pollution reports with five complex sentences each, in about three minutes on a Pentium III PC with 800 MHz. 5 Related Work Tree rewriting which is the more constrained version of graph rewriting has been central in transfer-oriented MT for a long time. In the last few years, there has been increasing interest in MT in graph rewriting (Einele and Dorna, 1998; Frank, 1999; Dymetman and Tendeau, 2000). Tree rewriting is also used in generation for instance, by the 101-r-based generator RealPro (Lavoie and Rainbow, 1997; Lavoie et al., 2000). Other well-known 101-r-based generators such as GOSSIP (Iordanskaja et al., 1988) use a sequential graph rewriting formalism. Apart from 101-r-oriented approaches, there are several other approaches in generation that are related to our work. In what follows, we would like to mention two of them. The first is (Nicolov et al., 1996)&apos;s work. The difference between Nicolov et al.&apos;s approach and ours is threefold. First, Nicolov et al. use a graph rewriting grammar formalism, while we use a parallel graph rewriting formalism. Second, we strictly separate between grammatical processing and tasks </context>
</contexts>
<marker>Lavoie, Kittredge, Korelsky, 2000</marker>
<rawString>B. Lavoie, R. Kittredge, T. Korelsky, and 0. Rambow. 2000. A Framework for MT and Multilingual NLG Systems Based on Uniform LexicoStructural Processing. In Proceedings of the ANLP/NAACL Conference.</rawString>
</citation>
<citation valid="false">
<authors>
<author>forthcoming</author>
</authors>
<title>Towards a Lexicographic Approach to Lexical Transfer in Machine Translation (Illustrated by the German-Russian Language Pair).</title>
<journal>Machine Translation Journal.</journal>
<marker>forthcoming, </marker>
<rawString>I.A. Mel&apos;Cuk and L. Wanner. forthcoming. Towards a Lexicographic Approach to Lexical Transfer in Machine Translation (Illustrated by the German-Russian Language Pair). Machine Translation Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I A Mel&apos;Cuk</author>
</authors>
<title>Meaning-Text Models: A Recent Trend in Soviet Linguistics&amp;quot;. Annual Review of Anthropology,</title>
<date>1981</date>
<pages>10--27</pages>
<contexts>
<context position="2172" citStr="Mel&apos;Cuk, 1981" startWordPosition="325" endWordPosition="326">s of L2; see, e.g., &apos;Note that Li and £2 may be identical, but do not need to be so. (Nicolov et al., 1996; Beale et al., 1998; Stede, 1999.). A parallel graph-rewriting formalism maps a given input structure to an output structure instead of transforming the former into the latter. Although parallel graph-rewriting shows several advantages when compared to sequential graph rewriting (see Section 3), sequential graph-rewriting formalisms are much more common. In this paper, we present the implementation of a parallel graph rewriting formalism for the grammar of the Meaning-Text Theory (1017) (Mel&apos;Cuk, 1981; Mel&apos;Cuk, 1988). The focus of the presentation is on one of the major stages of the algorithm: the spelling out which rules are to be applied to which fragments of the input structure in order to achieve its most optimal coverage. This is a search problem. In the next section, a brief introduction to MTT and its formal basics is given. In Section 3 we present the stages of processing in parallel graph rewriting. Section 4 explains the search algorithm in detail and presents an example of how the search algorithm works in practice. Section 5 discusses some of the related work in this area. In </context>
</contexts>
<marker>Mel&apos;Cuk, 1981</marker>
<rawString>I.A. Mel&apos;Cuk. 1981. &amp;quot;Meaning-Text Models: A Recent Trend in Soviet Linguistics&amp;quot;. Annual Review of Anthropology, 10:27-62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I A Mel&apos;Cuk</author>
</authors>
<title>Dependency Syntax: Theory and Practice.</title>
<date>1988</date>
<publisher>State University of New York Press,</publisher>
<location>Albany.</location>
<contexts>
<context position="2188" citStr="Mel&apos;Cuk, 1988" startWordPosition="327" endWordPosition="328">.g., &apos;Note that Li and £2 may be identical, but do not need to be so. (Nicolov et al., 1996; Beale et al., 1998; Stede, 1999.). A parallel graph-rewriting formalism maps a given input structure to an output structure instead of transforming the former into the latter. Although parallel graph-rewriting shows several advantages when compared to sequential graph rewriting (see Section 3), sequential graph-rewriting formalisms are much more common. In this paper, we present the implementation of a parallel graph rewriting formalism for the grammar of the Meaning-Text Theory (1017) (Mel&apos;Cuk, 1981; Mel&apos;Cuk, 1988). The focus of the presentation is on one of the major stages of the algorithm: the spelling out which rules are to be applied to which fragments of the input structure in order to achieve its most optimal coverage. This is a search problem. In the next section, a brief introduction to MTT and its formal basics is given. In Section 3 we present the stages of processing in parallel graph rewriting. Section 4 explains the search algorithm in detail and presents an example of how the search algorithm works in practice. Section 5 discusses some of the related work in this area. In Section 6, final</context>
</contexts>
<marker>Mel&apos;Cuk, 1988</marker>
<rawString>I.A. Mel&apos;Cuk. 1988. Dependency Syntax: Theory and Practice. State University of New York Press, Albany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Nicolov</author>
<author>C Mellish</author>
<author>G Richie</author>
</authors>
<title>Approximate Generation from Non-hierarchical Representations.</title>
<date>1996</date>
<booktitle>In Proceedings of the 8th International Workshop on Natural Language Generation,</booktitle>
<location>Herstmonceux.</location>
<contexts>
<context position="1665" citStr="Nicolov et al., 1996" startWordPosition="247" endWordPosition="250">structure, which is specified in the formal language L2, by using explicitly or implicitly defined rewriting rules.&apos; Explicit rewriting rules may have the format of classic rewriting rules, as, e.g., in (Frank, 1999) or of bidirectional rules that establish a correspondence relation between minimal structures of Li and L2, as, e.g., in (Iordanskaja et al., 1988; Lavoie and Rainbow, 1997). Implicit rewriting rules are encoded in terms of LI-constraints that are associated with structure chunks and lexical items of L2; see, e.g., &apos;Note that Li and £2 may be identical, but do not need to be so. (Nicolov et al., 1996; Beale et al., 1998; Stede, 1999.). A parallel graph-rewriting formalism maps a given input structure to an output structure instead of transforming the former into the latter. Although parallel graph-rewriting shows several advantages when compared to sequential graph rewriting (see Section 3), sequential graph-rewriting formalisms are much more common. In this paper, we present the implementation of a parallel graph rewriting formalism for the grammar of the Meaning-Text Theory (1017) (Mel&apos;Cuk, 1981; Mel&apos;Cuk, 1988). The focus of the presentation is on one of the major stages of the algorith</context>
<context position="30367" citStr="Nicolov et al., 1996" startWordPosition="5186" endWordPosition="5189">he last few years, there has been increasing interest in MT in graph rewriting (Einele and Dorna, 1998; Frank, 1999; Dymetman and Tendeau, 2000). Tree rewriting is also used in generation for instance, by the 101-r-based generator RealPro (Lavoie and Rainbow, 1997; Lavoie et al., 2000). Other well-known 101-r-based generators such as GOSSIP (Iordanskaja et al., 1988) use a sequential graph rewriting formalism. Apart from 101-r-oriented approaches, there are several other approaches in generation that are related to our work. In what follows, we would like to mention two of them. The first is (Nicolov et al., 1996)&apos;s work. The difference between Nicolov et al.&apos;s approach and ours is threefold. First, Nicolov et al. use a graph rewriting grammar formalism, while we use a parallel graph rewriting formalism. Second, we strictly separate between grammatical processing and tasks of sentence planning. Our grammar rules thus do not contain any but linguistic conditions, while Nicolov et al.&apos;s rules may also contain pragmatic and situational conditions. And third, finally, Nicolov et al. use complex rules which cover whole fragments of the input structure. As a result, it may well occur that with the rules chos</context>
</contexts>
<marker>Nicolov, Mellish, Richie, 1996</marker>
<rawString>N. Nicolov, C. Mellish, and G. Richie. 1996. Approximate Generation from Non-hierarchical Representations. In Proceedings of the 8th International Workshop on Natural Language Generation, Herstmonceux.</rawString>
</citation>
<citation valid="true">
<date>1997</date>
<booktitle>Handbook of Graph Grammars and Computing by Graph Transformation. World Scientific,</booktitle>
<editor>G. Rozenberg, editor.</editor>
<location>Singapore, New Jersey, London, Hong Kong.</location>
<marker>1997</marker>
<rawString>G. Rozenberg, editor. 1997. Handbook of Graph Grammars and Computing by Graph Transformation. World Scientific, Singapore, New Jersey, London, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stede</author>
</authors>
<title>Lexical Semantics and Knowledge Representation in Multilingual Text Generation.</title>
<date>1999</date>
<publisher>Kluwer Academic Publishers Group.</publisher>
<contexts>
<context position="1698" citStr="Stede, 1999" startWordPosition="255" endWordPosition="256">l language L2, by using explicitly or implicitly defined rewriting rules.&apos; Explicit rewriting rules may have the format of classic rewriting rules, as, e.g., in (Frank, 1999) or of bidirectional rules that establish a correspondence relation between minimal structures of Li and L2, as, e.g., in (Iordanskaja et al., 1988; Lavoie and Rainbow, 1997). Implicit rewriting rules are encoded in terms of LI-constraints that are associated with structure chunks and lexical items of L2; see, e.g., &apos;Note that Li and £2 may be identical, but do not need to be so. (Nicolov et al., 1996; Beale et al., 1998; Stede, 1999.). A parallel graph-rewriting formalism maps a given input structure to an output structure instead of transforming the former into the latter. Although parallel graph-rewriting shows several advantages when compared to sequential graph rewriting (see Section 3), sequential graph-rewriting formalisms are much more common. In this paper, we present the implementation of a parallel graph rewriting formalism for the grammar of the Meaning-Text Theory (1017) (Mel&apos;Cuk, 1981; Mel&apos;Cuk, 1988). The focus of the presentation is on one of the major stages of the algorithm: the spelling out which rules a</context>
</contexts>
<marker>Stede, 1999</marker>
<rawString>M. Stede. 1999. Lexical Semantics and Knowledge Representation in Multilingual Text Generation. Kluwer Academic Publishers Group.</rawString>
</citation>
<citation valid="false">
<authors>
<author>forthcoming</author>
</authors>
<title>Inheritance</title>
<booktitle>Selected Topics in Dependency Grammar. Benj amins,</booktitle>
<editor>in a MTT Grammar. In Polgu&apos;ere A. &amp; Wanner L., editor,</editor>
<location>Amsterdam.</location>
<marker>forthcoming, </marker>
<rawString>L. Wanner and B. Bohnet. forthcoming. Inheritance in a MTT Grammar. In Polgu&apos;ere A. &amp; Wanner L., editor, Selected Topics in Dependency Grammar. Benj amins, Amsterdam.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>