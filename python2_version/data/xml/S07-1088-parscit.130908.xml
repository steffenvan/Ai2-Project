<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006743">
<title confidence="0.92636">
UNIBA: JIGSAW algorithm for Word Sense Disambiguation
</title>
<author confidence="0.969639">
P. Basile and M. de Gemmis and A.L. Gentile and P. Lops and G. Semeraro
</author>
<affiliation confidence="0.995878">
Department of Computer Science - University of Bari - Via E. Orabona, 4 70125 Bari ITALY
</affiliation>
<email confidence="0.942731">
{basilepp, degemmis, al.gentile, lops, semeraro}@di.uniba.it
</email>
<sectionHeader confidence="0.980533" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997699166666667">
Word Sense Disambiguation (WSD) is tra-
ditionally considered an AI-hard problem.
A breakthrough in this field would have a
significant impact on many relevant web-
based applications, such as information re-
trieval and information extraction. This pa-
per describes JIGSAW, a knowledge-based
WSD system that attemps to disambiguate
all words in a text by exploiting WordNet1
senses. The main assumption is that a spe-
cific strategy for each Part-Of-Speech (POS)
is better than a single strategy. We evalu-
ated the accuracy of JIGSAW on SemEval-
2007 task 1 competition2. This task is an
application-driven one, where the applica-
tion is a fixed cross-lingual information re-
trieval system. Participants disambiguate
text by assigning WordNet synsets, then the
system has to do the expansion to other lan-
guages, index the expanded documents and
run the retrieval for all the languages in
batch. The retrieval results are taken as a
measure for the effectiveness of the disam-
biguation.
</bodyText>
<sectionHeader confidence="0.981732" genericHeader="method">
1 The JIGSAW algorithm
</sectionHeader>
<bodyText confidence="0.999933076923077">
The goal of a WSD algorithm consists in assigning
a word wi occurring in a document d with its appro-
priate meaning or sense s, by exploiting the context
C in where wi is found. The context C for wi is de-
fined as a set of words that precede and follow wi.
The sense s is selected from a predefined set of pos-
sibilities, usually known as sense inventory. In the
proposed algorithm, the sense inventory is obtained
from WordNet 1.6, according to SemEval-2007 task
1 instructions. JIGSAW is a WSD algorithm based
on the idea of combining three different strategies to
disambiguate nouns, verbs, adjectives and adverbs.
The main motivation behind our approach is that
</bodyText>
<footnote confidence="0.999785">
1http://wordnet.princeton.edu/
2http://www.senseval.org/
</footnote>
<bodyText confidence="0.9998295">
the effectiveness of a WSD algorithm is strongly
influenced by the POS tag of the target word. An
adaptation of Lesk dictionary-based WSD algorithm
has been used to disambiguate adjectives and ad-
verbs (Banerjee and Pedersen, 2002), an adaptation
of the Resnik algorithm has been used to disam-
biguate nouns (Resnik, 1995), while the algorithm
we developed for disambiguating verbs exploits the
nouns in the context of the verb as well as the nouns
both in the glosses and in the phrases that WordNet
utilizes to describe the usage of a verb. JIGSAW
takes as input a document d = {w1, w2, ... , wh} and
returns a list of WordNet synsets X = {s1, s2, ... ,
sk} in which each element si is obtained by disam-
biguating the target word wi based on the informa-
tion obtained from WordNet about a few immedi-
ately surrounding words. We define the context C of
the target word to be a window of n words to the left
and another n words to the right, for a total of 2n
surrounding words. The algorithm is based on three
different procedures for nouns, verbs, adverbs and
adjectives, called JIGSAWnouns, JIGSAWverbs,
JIGSAWothers, respectively. More details for each
one of the above mentioned procedures follow.
</bodyText>
<subsectionHeader confidence="0.989395">
1.1 JIGSAWnouns
</subsectionHeader>
<bodyText confidence="0.998110470588235">
The procedure is obtained by making some varia-
tions to the algorithm designed by Resnik (1995) for
disambiguating noun groups. Given a set of nouns
W = {w1, w2, ... , wn}, obtained from document
d, with each wi having an associated sense inven-
tory Si = {si1, si2, ..., sik} of possible senses, the
goal is assigning each wi with the most appropri-
ate sense sih E Si, according to the similarity of
wi with the other words in W (the context for wi).
The idea is to define a function ϕ(wi, sig), wi E W,
sig E Si, that computes a value in [0, 1] representing
the confidence with which word wi can be assigned
with sense sig. The intuition behind this algorithm
is essentially the same exploited by Lesk (1986) and
other authors: The most plausible assignment of
senses to multiple co-occurring words is the one that
maximizes relatedness of meanings among the cho-
</bodyText>
<page confidence="0.981258">
398
</page>
<bodyText confidence="0.988364807692308">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 398–401,
Prague, June 2007. c�2007 Association for Computational Linguistics
sen senses. JIGSAWnouns differs from the original
algorithm by Resnik (1995) in the similarity mea-
sure used to compute relatedness of two senses. We
adopted the Leacock-Chodorow measure (Leacock
and Chodorow, 1998), which is based on the length
of the path between concepts in an IS-A hierarchy.
The idea behind this measure is that similarity be-
tween two synsets, s1 and s2, is inversely propor-
tional to their distance in the WordNet IS-A hierar-
chy. The distance is computed by finding the most
specific subsumer (MSS) between s1 and s2 (each
ancestor of both s1 and s2 in the WordNet hierar-
chy is a subsumer, the MSS is the one at the lowest
level) and counting the number of nodes in the path
between s1 and s2 that traverse their MSS. We ex-
tended this measure by introducing a parameter k
that limits the search for the MSS to k ancestors (i.e.
that climbs the WordNet IS-A hierarchy until either
it finds the MSS or k + 1 ancestors of both s1 and
s2 have been explored). This guarantees that “too
abstract” (i.e. “less informative”) MSSs will be ig-
nored. In addition to the semantic similarity func-
tion, JIGSAWnouns differs from the Resnik algo-
rithm in the use of:
</bodyText>
<listItem confidence="0.969208285714286">
1. a Gaussian factor G, which takes into account the dis-
tance between the words in the text to be disambiguated;
2. a factor R, which gives more importance to the synsets
that are more common than others, according to the fre-
quency score in WordNet;
3. a parametrized search for the MSS between two concepts
(the search is limited to a certain number of ancestors).
</listItem>
<bodyText confidence="0.998955166666667">
Algorithm 1 describes the complete procedure for
the disambiguation of nouns. This algorithm consid-
ers the words in W pairwise. For each pair (wi,wj),
the most specific subsumer MSSij is identified, by
reducing the search to depth1 ancestors at most.
Then, the similarity sim(wi, wj, depth2) between
the two words is computed, by reducing the search
for the MSS to depth2 ancestors at most. MSSij is
considered as supporting evidence for those synsets
sik in Si and sjh in Sj that are descendants of
MSSij. The MSS search is computed choosing the
nearest MSS in all pairs of synsets sik,sjh. Like-
wise, the similarity for (wi,wj) is the max similarity
computed in all pairs of sik,sjh and is weighted by
a gaussian factor that takes into account the posi-
tion of wi and wj in W (the shorter is the distance
Algorithm 1 The procedure for disambiguating
nouns derived from the algorithm by Resnik
</bodyText>
<listItem confidence="0.928396">
1: procedure JIGSAWnouns(W, depth1, depth2) �
finds the proper synset for each polysemous noun in the set
W = {w1, w2, ... , wn}, depth1 and depth2 are used in
the computation of MSS
2: for all wi, wj ∈ W do
3: if i &lt; j then
4: sim ← sim(wi, wj, depth1) ∗
</listItem>
<bodyText confidence="0.833957333333333">
G(pos(wi), pos(wj)) &gt; G(x, y) is a Gaussian
function which takes into account the difference between
the positions of wi and wj
</bodyText>
<listItem confidence="0.702165357142857">
5: MSSij ← MSS(wi, wj, depth2) �
MSSij is the most specific subsumer between wi and wj,
search for MSS restricted to depth2 ancestors
6: for all sik ∈ Si do
7: if is-ancestor(MSSij,sik) then &gt; if
MSSij is an ancestor of sik
8: supik ← supik + sim
9: end if
10: end for
11: for all sjh ∈ Sj do
12: if is-ancestor(MSSij,sjh) then
13: supjh ← supjh + sim
14: end if
15: end for
16: normi ← normi + sim
17: normj ← normj + sim
18: end if
19: end for
20: for all wi ∈ W do
21: for all sik ∈ Si do
22: if normi &gt; 0 then
23: ϕ(i, k) ← α ∗ supik/normi + β ∗ R(k)
24: else
25: ϕ(i, k) ← α/|Si |+ β ∗ R(k)
26: end if
27: end for
28: end for
29: end procedure
</listItem>
<bodyText confidence="0.999828111111111">
between the words, the higher is the weight). The
value ϕ(i, k) assigned to each candidate synset sik
for the word wi is the sum of two elements. The
first one is the proportion of support it received, out
of the support possible, computed as supik/normi
in Algorithm 1. The other element that contributes
to ϕ(i, k) is a factor R(k) that takes into account
the rank of sik in WordNet, i.e. how common is the
sense sik for the word wi. R(k) is computed as:
</bodyText>
<equation confidence="0.9831535">
k
R(k) = 1 − 0.8 ∗
(1)
n − 1
</equation>
<bodyText confidence="0.99952075">
where n is the cardinality of the sense inventory Si
for wi, and k is the rank of sik in Si, starting from 0.
Finally, both elements are weighted by two pa-
rameters: α, which controls the contribution given
</bodyText>
<page confidence="0.996003">
399
</page>
<bodyText confidence="0.999193545454545">
to ϕ(i, k) by the normalized support, and Q, which
controls the contribution given by the rank of sik.
We set α = 0.7 and Q = 0.3. The synset assigned
to each word in W is the one with the highest ϕ
value. Notice that we used two different parameters,
depth1 and depth2 for setting the maximum depth
for the search of the MSS: depth1 limits the search
for the MSS computed in the similarity function,
while depth2 limits the computation of the MSS
used for assigning support to candidate synsets. We
set depth1 = 6 and depth2 = 3.
</bodyText>
<subsectionHeader confidence="0.894081">
1.2 JIGSAWverbs
</subsectionHeader>
<bodyText confidence="0.9999637">
Before describing the JIGSAWverbs procedure, the
description of a synset must be defined. It is the
string obtained by concatenating the gloss and the
sentences that WordNet uses to explain the usage
of a synset. First, JIGSAWverbs includes, in the
context C for the target verb wi, all the nouns in
the window of 2n words surrounding wi. For each
candidate synset sik of wi, the algorithm computes
nouns(i, k), that is the set of nouns in the descrip-
tion for sik.
</bodyText>
<equation confidence="0.991393">
maxjk = maxwl∈nouns(i,k) {sim(wj,wl,depth)} (2)
</equation>
<bodyText confidence="0.999305166666667">
where sim(wj,wl,depth) is defined as in
JIGSAWnouns. In other words, maxjk is the
highest similarity value for wj wrt the nouns related
to the k-th sense for wi. Finally, an overall simi-
larity score among sik and the whole context C is
computed:
</bodyText>
<equation confidence="0.995919333333333">
Pw.EaG(pos(wi), pos(wj)) · maxjk
ϕ(i, k) = R(k)· P (3)
hG(pos(wi), pos(wh))
</equation>
<bodyText confidence="0.999990857142857">
where R(k) is defined as in Equation 1 with a differ-
ent constant factor (0.9) and G(pos(wi), pos(wj)) is
the same Gaussian factor used in JIGSAWnouns,
that gives a higher weight to words closer to the tar-
get word. The synset assigned to wi is the one with
the highest ϕ value. Algorithm 2 provides a detailed
description of the procedure.
</bodyText>
<subsectionHeader confidence="0.931819">
1.3 JIGSAWothers
</subsectionHeader>
<bodyText confidence="0.9850396">
This procedure is based on the WSD algorithm pro-
posed by Banerjee and Pedersen (2002). The idea is
to compare the glosses of each candidate sense for
Algorithm 2 The procedure for the disambiguation
of verbs
</bodyText>
<listItem confidence="0.970346571428572">
1: procedure JIGSAWverbs(wi, d, depth) &gt; finds the
proper synset of a polysemous verb wi in document d
2: C ← {w1, ..., wn} &gt; C is
the context for wi. For example, C = {w1, w2, w4, w5},
if the sequence of words {w1, w2, w3, w4, w5} occurs in d,
w3 being the target verb, wj being nouns, j =~ 3
3: Si ← {si1, ...sim} &gt; Si
is the sense inventory for wi, that is the set of all candidate
synsets for wi returned by WordNet
4: s ← null &gt; s is the synset to be returned
5: score ← −MAXDOUBLE &gt; score is the
similarity score assigned to s
6: p ← 1 &gt; p is the position of the synsets for wi
7: for all sik ∈ Si do
8: max ← {max1k, ..., maxnk}
9: nouns(i, k) ← {noun1, ..., nounz} &gt;
nouns(i, k) is the set of all nouns in the description of sik
10: sumGauss ← 0
11: sumTot ← 0
12: for all wj ∈ C do &gt; computation of the similarity
between C and sik
13: maxjk ← 0 &gt; maxjk is the highest similarity
value for wj, wrt the nouns related to the k-th sense for wi.
14: sumGauss ← G(pos(wi), pos(wj)) &gt;
Gaussian function which takes into account the difference
between the positions of the nouns in d
15: for all nounl ∈ nouns(i, k) do
16: sim ← sim(wj, nounl, depth) &gt; sim is
the similarity between the j-th noun in C and l-th noun in
nouns(i, k)
17: if sim &gt; maxjk then
18: maxjk ← sim
19: end if
20: end for
21: end for
22: for all wj ∈ C do
23: sumTot ← sumTot+G(pos(wi), pos(wj))∗
maxjk
24: end for
25: sumTot ← sumTot/sumGauss
26: ϕ(i, k) ← R(k) ∗ sumTot &gt; R(k) is defined as in
JIGSAWnouns
27: if ϕ(i, k) &gt; score then
28: score ← ϕ(i, k)
29: p ← k
30: end if
31: end for
32: s ← sip
33: return s
</listItem>
<subsectionHeader confidence="0.84994">
34: end procedure
</subsectionHeader>
<bodyText confidence="0.999959428571428">
the target word to the glosses of all the words in its
context. Let Wi be the sense inventory for the tar-
get word wi. For each sik ∈ Wi, JIGSAWothers
computes the string targetGlossik that contains the
words in the gloss of sik. Then, the procedure
computes the string contextGlossi, which contains
the words in the glosses of all the synsets corre-
</bodyText>
<page confidence="0.98845">
400
</page>
<bodyText confidence="0.999910375">
sponding to each word in the context for wi. Fi-
nally, the procedure computes the overlap between
contextGlossi and targetGlossik, and assigns the
synset with the highest overlap score to wi. This
score is computed by counting the words that occur
both in targetGlossik and in contextGlossi. If ties
occur, the most common synset in WordNet is cho-
sen.
</bodyText>
<sectionHeader confidence="0.997044" genericHeader="method">
2 Experiment
</sectionHeader>
<bodyText confidence="0.999885733333333">
We performed the experiment following the instruc-
tions for SemEval-2007 task 1 (Agirre et al., 2007).
JIGSAW is implemented in JAVA, by using JWNL
library3 in order to access WordNet 1.6 dictionary.
We ran the experiment on a Linux-based PC with
Intel Pentium D processor having a speed of 3 GHz
and 2 GB of RAM. The dataset consists of 29,681
documents, including 300 topics. Results are re-
ported in Table 1. Only two systems (PART-A and
PART-B) partecipated to the competition, thus the
organizers decided to add a third system (ORGA-
NIZERS) developed by themselves. The systems
were scored according to standard IR/CLIR mea-
sures as implemented in the TREC evaluation pack-
age4. Our system is labelled as PART-A.
</bodyText>
<table confidence="0.971745285714286">
system IR documents IR topics CLIR
no expansion 0.3599 0.1446
full expansion 0.1610 0.1410 0.2676
1st sense 0.2862 0.1172 0.2637
ORGANIZERS 0.2886 0.1587 0.2664
PART-A 0.3030 0.1521 0.1373
PART-B 0.3036 0.1482 0.1734
</table>
<tableCaption confidence="0.99964">
Table 1: SemEval-2007 task 1 Results
</tableCaption>
<bodyText confidence="0.999825875">
All systems show similar results in IR tasks, while
their behaviour is extremely different on CLIR task.
WSD results are reported in Table 2. These re-
sults are encouraging as regard precision, consid-
ering that our system exploits only WordNet as
kwnoledge-base, while ORGANIZERS uses a su-
pervised method that exploits SemCor to train a
kNN classifier.
</bodyText>
<sectionHeader confidence="0.999439" genericHeader="conclusions">
3 Conclusions
</sectionHeader>
<bodyText confidence="0.793682">
In this paper we have presented a WSD algorithm
that exploits WordNet as knowledge-base and uses
</bodyText>
<footnote confidence="0.999849">
3http://sourceforge.net/projects/jwordnet
4http://trec.nist.gov/
</footnote>
<table confidence="0.998880333333333">
system precision recall attempted
SENSEVAL-2
ORGANIZERS 0.584 0.577 93.61%
PART-A 0.498 0.375 75.39%
PART-B 0.388 0.240 61.92%
SENSEVAL-3
ORGANIZERS 0.591 0.566 95.76%
PART-A 0.484 0.338 69.98%
PART-B 0.334 0.186 55.68%
</table>
<tableCaption confidence="0.998692">
Table 2: WSD results on all-words task
</tableCaption>
<bodyText confidence="0.99978875">
three different methods for each part-of-speech. The
algorithm has been evaluated by SemEval-2007 task
1. The system shows a good performance in all
tasks, but low precision in CLIR evaluation. Prob-
ably, the negative result in CLIR task depends on
complex interaction of WSD, expansion and index-
ing. Contrarily to other tasks, organizers do not plan
to provide a ranking of systems on SemEval-2007
task 1. As a consequence, the goal of this task - what
is the best WSD system in the context of a CLIR
system? - is still open. This is why the organizers
stressed in the call that this was ”a first try”.
</bodyText>
<sectionHeader confidence="0.999116" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9999035">
E. Agirre, B. Magnini, o. Lopez de Lacalle, A. Otegi,
G. Rigau, and Vossen. 2007. Semeval-2007 task
1: Evaluating wsd on cross-language information re-
trieval. In Proceedings of SemEval-2007. Association
for Computational Linguistics.
S. Banerjee and T. Pedersen. 2002. An adapted lesk
algorithm for word sense disambiguation using word-
net. In CICLing’02: Proc. 3rd Int’l Conf. on Com-
putational Linguistics and Intelligent Text Processing,
pages 136–145, London, UK. Springer-Verlag.
C. Leacock and M. Chodorow. 1998. Combining local
context and wordnet similarity for word sense identifi-
cation. In C. Fellbaum (Ed.), WordNet: An Electronic
Lexical Database, pages 305–332. MIT Press.
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: how to tell a pine cone
from an ice cream cone. In Proceedings of the 1986
SIGDOC Conference, pages 20–29. ACM Press.
P. Resnik. 1995. Disambiguating noun groupings with
respect to WordNet senses. In Proceedings of the
Third Workshop on Very Large Corpora, pages 54–68.
Association for Computational Linguistics.
</reference>
<page confidence="0.998646">
401
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.095469">
<title confidence="0.999772">UNIBA: JIGSAW algorithm for Word Sense Disambiguation</title>
<author confidence="0.983444">Basile de_Gemmis Gentile Lops Semeraro</author>
<affiliation confidence="0.990144">Department of Computer Science - University of Bari - Via E. Orabona, 4 70125 Bari ITALY</affiliation>
<email confidence="0.978029">degemmis,al.gentile,lops,</email>
<abstract confidence="0.986225907356948">Word Sense Disambiguation (WSD) is traditionally considered an AI-hard problem. A breakthrough in this field would have a significant impact on many relevant webbased applications, such as information retrieval and information extraction. This paper describes JIGSAW, a knowledge-based WSD system that attemps to disambiguate words in a text by exploiting senses. The main assumption is that a specific strategy for each Part-Of-Speech (POS) is better than a single strategy. We evaluated the accuracy of JIGSAW on SemEvaltask 1 This task is an application-driven one, where the application is a fixed cross-lingual information retrieval system. Participants disambiguate text by assigning WordNet synsets, then the system has to do the expansion to other languages, index the expanded documents and run the retrieval for all the languages in batch. The retrieval results are taken as a measure for the effectiveness of the disambiguation. 1 The JIGSAW algorithm The goal of a WSD algorithm consists in assigning word in a document its appromeaning or sense by exploiting the where found. The context deas a set of words that precede and follow sense selected from a predefined set of posusually known as In the proposed algorithm, the sense inventory is obtained from WordNet 1.6, according to SemEval-2007 task 1 instructions. JIGSAW is a WSD algorithm based on the idea of combining three different strategies to disambiguate nouns, verbs, adjectives and adverbs. The main motivation behind our approach is that the effectiveness of a WSD algorithm is strongly influenced by the POS tag of the target word. An adaptation of Lesk dictionary-based WSD algorithm has been used to disambiguate adjectives and adverbs (Banerjee and Pedersen, 2002), an adaptation of the Resnik algorithm has been used to disambiguate nouns (Resnik, 1995), while the algorithm we developed for disambiguating verbs exploits the in the the verb as well as the nouns both in the glosses and in the phrases that WordNet utilizes to describe the usage of a verb. JIGSAW as input a document ... , a list of WordNet synsets ... , which each element obtained by disamthe word on the information obtained from WordNet about a few immedisurrounding words. We define the target word to be a window of to the left another to the right, for a total of surrounding words. The algorithm is based on three different procedures for nouns, verbs, adverbs and called respectively. More details for each one of the above mentioned procedures follow. The procedure is obtained by making some variations to the algorithm designed by Resnik (1995) for disambiguating noun groups. Given a set of nouns ... , obtained from document with each an associated sense inven- ..., possible senses, the is assigning each the most approprisense according to the the other words in context for idea is to define a function that computes a value in confidence with which word be assigned sense The intuition behind this algorithm is essentially the same exploited by Lesk (1986) and other authors: The most plausible assignment of senses to multiple co-occurring words is the one that meanings among the cho- 398 of the 4th International Workshop on Semantic Evaluations pages 398–401, June 2007. Association for Computational Linguistics senses. from the original algorithm by Resnik (1995) in the similarity measure used to compute relatedness of two senses. We adopted the Leacock-Chodorow measure (Leacock and Chodorow, 1998), which is based on the length the path between concepts in an The idea behind this measure is that similarity betwo synsets, is inversely proporto their distance in the WordNet hierar- The distance is computed by finding the subsumer between of both the WordNet hierarchy is a subsumer, the MSS is the one at the lowest level) and counting the number of nodes in the path traverse their MSS. We exthis measure by introducing a parameter limits the search for the MSS to (i.e. climbs the WordNet until either finds the MSS or 1 of both been explored). This guarantees that “too abstract” (i.e. “less informative”) MSSs will be ignored. In addition to the semantic similarity funcfrom the Resnik algorithm in the use of: a Gaussian factor which takes into account the distance between the words in the text to be disambiguated; a factor which gives more importance to the synsets that are more common than others, according to the frequency score in WordNet; a for the MSS between two concepts (the search is limited to a certain number of ancestors). Algorithm 1 describes the complete procedure for the disambiguation of nouns. This algorithm considthe words in For each pair most specific subsumer identified, by the search to at most. the similarity the two words is computed, by reducing the search the MSS to at most. supporting evidence those synsets are descendants of The MSS search is computed choosing the MSS in all pairs of synsets Likethe similarity for is the max similarity in all pairs of is weighted by a gaussian factor that takes into account the posiof shorter is the distance 1 procedure for disambiguating nouns derived from the algorithm by Resnik procedure � finds the proper synset for each polysemous noun in the set ... , used in the computation of MSS for all W if &lt; j sim ← a Gaussian function which takes into account the difference between positions of 5: � the most specific subsumer between for MSS restricted to for all if if an ancestor of 8: 9: end if 10: end for for all if 13: 14: end if 15: end for 16: 17: 18: end if 19: end for for all W for all if 23: α ∗ ∗ 24: else 25: ∗ 26: end if 27: end for 28: end for 29: end procedure between the words, the higher is the weight). The to each candidate synset the word the sum of two elements. The first one is the proportion of support it received, out the support possible, computed as in Algorithm 1. The other element that contributes a factor takes into account rank of WordNet, i.e. how common is the the word computed as: = 1 (1) − the cardinality of the sense inventory and the rank of starting from Finally, both elements are weighted by two pawhich controls the contribution given 399 the normalized support, and which the contribution given by the rank of set 0.7 The synset assigned each word in the one with the highest value. Notice that we used two different parameters, setting the maximum depth the search of the MSS: the search for the MSS computed in the similarity function, the computation of the MSS used for assigning support to candidate synsets. We = 6 = describing the the a synset must be defined. It is the string obtained by concatenating the gloss and the sentences that WordNet uses to explain the usage a synset. First, in the the target verb all the nouns in window of surrounding For each synset the algorithm computes that is the set of nouns in the descripfor defined as in In other words, the similarity value for the nouns related the sense for Finally, an overall simiscore among the whole context computed: = P defined as in Equation 1 with a differconstant factor (0.9) and same Gaussian factor used in that gives a higher weight to words closer to the tarword. The synset assigned to the one with highest Algorithm 2 provides a detailed description of the procedure. This procedure is based on the WSD algorithm proposed by Banerjee and Pedersen (2002). The idea is to compare the glosses of each candidate sense for 2 procedure for the disambiguation of verbs procedure d, the synset of a polysemous verb document C ..., C is context for For example, the sequence of words in the target verb, nouns, 3: the sense inventory for that is the set of all candidate for by WordNet s &gt; s the synset to be returned score &gt; score the score assigned to p p the position of the synsets for for all max ..., 9: ..., &gt; the set of all nouns in the description of sumGauss sumTot for all of the similarity 13: the highest similarity for wrt the nouns related to the sense for sumGauss &gt; Gaussian function which takes into account the difference the positions of the nouns in for all sim sim is similarity between the noun in noun in if &gt; 18: 19: end if 20: end for 21: end for for all sumTot 24: end for sumTot 26: &gt; defined as in if score score p 30: end if 31: end for s return 34: end procedure the target word to the glosses of all the words in its Let the sense inventory for the tarword For each the string contains the in the gloss of Then, the procedure the string which contains words in the glosses of all the synsets corre- 400 to each word in the context for Fithe procedure computes the and assigns the with the highest overlap score to This score is computed by counting the words that occur in in If ties occur, the most common synset in WordNet is chosen. 2 Experiment We performed the experiment following the instructions for SemEval-2007 task 1 (Agirre et al., 2007). implemented in JAVA, by using JWNL in order to access WordNet 1.6 dictionary. We ran the experiment on a Linux-based PC with Intel Pentium D processor having a speed of 3 GHz and 2 GB of RAM. The dataset consists of 29,681 documents, including 300 topics. Results are reported in Table 1. Only two systems (PART-A and PART-B) partecipated to the competition, thus the organizers decided to add a third system (ORGA- NIZERS) developed by themselves. The systems were scored according to standard IR/CLIR measures as implemented in the TREC evaluation pack- Our system is labelled as PART-A. system IR documents IR topics CLIR no expansion 0.3599 0.1446 full expansion 0.1610 0.1410 0.2676 1st sense 0.2862 0.1172 0.2637 ORGANIZERS 0.2886 0.1587 0.2664 PART-A 0.3030 0.1521 0.1373 PART-B 0.3036 0.1482 0.1734 Table 1: SemEval-2007 task 1 Results All systems show similar results in IR tasks, while their behaviour is extremely different on CLIR task. WSD results are reported in Table 2. These results are encouraging as regard precision, considering that our system exploits only WordNet as kwnoledge-base, while ORGANIZERS uses a supervised method that exploits SemCor to train a kNN classifier. 3 Conclusions In this paper we have presented a WSD algorithm that exploits WordNet as knowledge-base and uses system precision recall attempted SENSEVAL-2 ORGANIZERS 0.584 0.577 93.61% PART-A 0.498 0.375 75.39% PART-B 0.388 0.240 61.92% SENSEVAL-3 ORGANIZERS 0.591 0.566 95.76% PART-A 0.484 0.338 69.98% PART-B 0.334 0.186 55.68% Table 2: WSD results on all-words task three different methods for each part-of-speech. The algorithm has been evaluated by SemEval-2007 task 1. The system shows a good performance in all tasks, but low precision in CLIR evaluation. Probably, the negative result in CLIR task depends on complex interaction of WSD, expansion and indexing. Contrarily to other tasks, organizers do not plan to provide a ranking of systems on SemEval-2007 task 1. As a consequence, the goal of this task what is the best WSD system in the context of a CLIR system? is still open. This is why the organizers in the call that this was first References E. Agirre, B. Magnini, o. Lopez de Lacalle, A. Otegi, G. Rigau, and Vossen. 2007. Semeval-2007 task 1: Evaluating wsd on cross-language information re- In of Association for Computational Linguistics. S. Banerjee and T. Pedersen. 2002. An adapted lesk algorithm for word sense disambiguation using word- In Proc. 3rd Int’l Conf. on Com- Linguistics and Intelligent Text pages 136–145, London, UK. Springer-Verlag. C. Leacock and M. Chodorow. 1998. Combining local context and wordnet similarity for word sense identifi- In Fellbaum (Ed.), WordNet: An Electronic pages 305–332. MIT Press. M. Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone an ice cream cone. In of the 1986 pages 20–29. ACM Press. P. Resnik. 1995. Disambiguating noun groupings with to WordNet senses. In of the</abstract>
<note confidence="0.906753666666667">Workshop on Very Large pages 54–68. Association for Computational Linguistics. 401</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>B Magnini</author>
<author>o Lopez de Lacalle</author>
<author>A Otegi</author>
<author>G Rigau</author>
<author>Vossen</author>
</authors>
<title>Semeval-2007 task 1: Evaluating wsd on cross-language information retrieval.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval-2007. Association for Computational Linguistics.</booktitle>
<marker>Agirre, Magnini, de Lacalle, Otegi, Rigau, Vossen, 2007</marker>
<rawString>E. Agirre, B. Magnini, o. Lopez de Lacalle, A. Otegi, G. Rigau, and Vossen. 2007. Semeval-2007 task 1: Evaluating wsd on cross-language information retrieval. In Proceedings of SemEval-2007. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>T Pedersen</author>
</authors>
<title>An adapted lesk algorithm for word sense disambiguation using wordnet.</title>
<date>2002</date>
<booktitle>In CICLing’02: Proc. 3rd Int’l Conf. on Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>136--145</pages>
<publisher>Springer-Verlag.</publisher>
<location>London, UK.</location>
<contexts>
<context position="2230" citStr="Banerjee and Pedersen, 2002" startWordPosition="355" endWordPosition="358">ally known as sense inventory. In the proposed algorithm, the sense inventory is obtained from WordNet 1.6, according to SemEval-2007 task 1 instructions. JIGSAW is a WSD algorithm based on the idea of combining three different strategies to disambiguate nouns, verbs, adjectives and adverbs. The main motivation behind our approach is that 1http://wordnet.princeton.edu/ 2http://www.senseval.org/ the effectiveness of a WSD algorithm is strongly influenced by the POS tag of the target word. An adaptation of Lesk dictionary-based WSD algorithm has been used to disambiguate adjectives and adverbs (Banerjee and Pedersen, 2002), an adaptation of the Resnik algorithm has been used to disambiguate nouns (Resnik, 1995), while the algorithm we developed for disambiguating verbs exploits the nouns in the context of the verb as well as the nouns both in the glosses and in the phrases that WordNet utilizes to describe the usage of a verb. JIGSAW takes as input a document d = {w1, w2, ... , wh} and returns a list of WordNet synsets X = {s1, s2, ... , sk} in which each element si is obtained by disambiguating the target word wi based on the information obtained from WordNet about a few immediately surrounding words. We defin</context>
<context position="10228" citStr="Banerjee and Pedersen (2002)" startWordPosition="1825" endWordPosition="1828"> to the k-th sense for wi. Finally, an overall similarity score among sik and the whole context C is computed: Pw.EaG(pos(wi), pos(wj)) · maxjk ϕ(i, k) = R(k)· P (3) hG(pos(wi), pos(wh)) where R(k) is defined as in Equation 1 with a different constant factor (0.9) and G(pos(wi), pos(wj)) is the same Gaussian factor used in JIGSAWnouns, that gives a higher weight to words closer to the target word. The synset assigned to wi is the one with the highest ϕ value. Algorithm 2 provides a detailed description of the procedure. 1.3 JIGSAWothers This procedure is based on the WSD algorithm proposed by Banerjee and Pedersen (2002). The idea is to compare the glosses of each candidate sense for Algorithm 2 The procedure for the disambiguation of verbs 1: procedure JIGSAWverbs(wi, d, depth) &gt; finds the proper synset of a polysemous verb wi in document d 2: C ← {w1, ..., wn} &gt; C is the context for wi. For example, C = {w1, w2, w4, w5}, if the sequence of words {w1, w2, w3, w4, w5} occurs in d, w3 being the target verb, wj being nouns, j =~ 3 3: Si ← {si1, ...sim} &gt; Si is the sense inventory for wi, that is the set of all candidate synsets for wi returned by WordNet 4: s ← null &gt; s is the synset to be returned 5: score ← −</context>
</contexts>
<marker>Banerjee, Pedersen, 2002</marker>
<rawString>S. Banerjee and T. Pedersen. 2002. An adapted lesk algorithm for word sense disambiguation using wordnet. In CICLing’02: Proc. 3rd Int’l Conf. on Computational Linguistics and Intelligent Text Processing, pages 136–145, London, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
</authors>
<title>Combining local context and wordnet similarity for word sense identification.</title>
<date>1998</date>
<booktitle>In C. Fellbaum (Ed.), WordNet: An Electronic Lexical Database,</booktitle>
<pages>305--332</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4461" citStr="Leacock and Chodorow, 1998" startWordPosition="741" endWordPosition="744">. The intuition behind this algorithm is essentially the same exploited by Lesk (1986) and other authors: The most plausible assignment of senses to multiple co-occurring words is the one that maximizes relatedness of meanings among the cho398 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 398–401, Prague, June 2007. c�2007 Association for Computational Linguistics sen senses. JIGSAWnouns differs from the original algorithm by Resnik (1995) in the similarity measure used to compute relatedness of two senses. We adopted the Leacock-Chodorow measure (Leacock and Chodorow, 1998), which is based on the length of the path between concepts in an IS-A hierarchy. The idea behind this measure is that similarity between two synsets, s1 and s2, is inversely proportional to their distance in the WordNet IS-A hierarchy. The distance is computed by finding the most specific subsumer (MSS) between s1 and s2 (each ancestor of both s1 and s2 in the WordNet hierarchy is a subsumer, the MSS is the one at the lowest level) and counting the number of nodes in the path between s1 and s2 that traverse their MSS. We extended this measure by introducing a parameter k that limits the searc</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>C. Leacock and M. Chodorow. 1998. Combining local context and wordnet similarity for word sense identification. In C. Fellbaum (Ed.), WordNet: An Electronic Lexical Database, pages 305–332. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the 1986 SIGDOC Conference,</booktitle>
<pages>pages</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="3920" citStr="Lesk (1986)" startWordPosition="666" endWordPosition="667">for disambiguating noun groups. Given a set of nouns W = {w1, w2, ... , wn}, obtained from document d, with each wi having an associated sense inventory Si = {si1, si2, ..., sik} of possible senses, the goal is assigning each wi with the most appropriate sense sih E Si, according to the similarity of wi with the other words in W (the context for wi). The idea is to define a function ϕ(wi, sig), wi E W, sig E Si, that computes a value in [0, 1] representing the confidence with which word wi can be assigned with sense sig. The intuition behind this algorithm is essentially the same exploited by Lesk (1986) and other authors: The most plausible assignment of senses to multiple co-occurring words is the one that maximizes relatedness of meanings among the cho398 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 398–401, Prague, June 2007. c�2007 Association for Computational Linguistics sen senses. JIGSAWnouns differs from the original algorithm by Resnik (1995) in the similarity measure used to compute relatedness of two senses. We adopted the Leacock-Chodorow measure (Leacock and Chodorow, 1998), which is based on the length of the path between concepts</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>M. Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In Proceedings of the 1986 SIGDOC Conference, pages 20–29. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Disambiguating noun groupings with respect to WordNet senses.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Workshop on Very Large Corpora,</booktitle>
<pages>54--68</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2320" citStr="Resnik, 1995" startWordPosition="372" endWordPosition="373">6, according to SemEval-2007 task 1 instructions. JIGSAW is a WSD algorithm based on the idea of combining three different strategies to disambiguate nouns, verbs, adjectives and adverbs. The main motivation behind our approach is that 1http://wordnet.princeton.edu/ 2http://www.senseval.org/ the effectiveness of a WSD algorithm is strongly influenced by the POS tag of the target word. An adaptation of Lesk dictionary-based WSD algorithm has been used to disambiguate adjectives and adverbs (Banerjee and Pedersen, 2002), an adaptation of the Resnik algorithm has been used to disambiguate nouns (Resnik, 1995), while the algorithm we developed for disambiguating verbs exploits the nouns in the context of the verb as well as the nouns both in the glosses and in the phrases that WordNet utilizes to describe the usage of a verb. JIGSAW takes as input a document d = {w1, w2, ... , wh} and returns a list of WordNet synsets X = {s1, s2, ... , sk} in which each element si is obtained by disambiguating the target word wi based on the information obtained from WordNet about a few immediately surrounding words. We define the context C of the target word to be a window of n words to the left and another n wor</context>
<context position="4323" citStr="Resnik (1995)" startWordPosition="722" endWordPosition="723">E W, sig E Si, that computes a value in [0, 1] representing the confidence with which word wi can be assigned with sense sig. The intuition behind this algorithm is essentially the same exploited by Lesk (1986) and other authors: The most plausible assignment of senses to multiple co-occurring words is the one that maximizes relatedness of meanings among the cho398 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 398–401, Prague, June 2007. c�2007 Association for Computational Linguistics sen senses. JIGSAWnouns differs from the original algorithm by Resnik (1995) in the similarity measure used to compute relatedness of two senses. We adopted the Leacock-Chodorow measure (Leacock and Chodorow, 1998), which is based on the length of the path between concepts in an IS-A hierarchy. The idea behind this measure is that similarity between two synsets, s1 and s2, is inversely proportional to their distance in the WordNet IS-A hierarchy. The distance is computed by finding the most specific subsumer (MSS) between s1 and s2 (each ancestor of both s1 and s2 in the WordNet hierarchy is a subsumer, the MSS is the one at the lowest level) and counting the number o</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>P. Resnik. 1995. Disambiguating noun groupings with respect to WordNet senses. In Proceedings of the Third Workshop on Very Large Corpora, pages 54–68. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>