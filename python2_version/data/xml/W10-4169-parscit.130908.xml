<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002853">
<title confidence="0.973694">
Chinese Word Sense Induction based on Hierarchical Clustering
Algorithm
</title>
<author confidence="0.929026">
Ke Cai, Xiaodong Shi, Yidong Chen,Zhehuang Huang, Yan Gao
</author>
<affiliation confidence="0.9229">
Cognitive Science Department, Xiamen University, Xiamen, 361005, China
</affiliation>
<sectionHeader confidence="0.97842" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995376">
Sense induction seeks to automatically identify word senses of polysemous words
encountered in a corpus. Unsupervised word sense induction can be viewed as a clustering
problem. In this paper, we used the Hierarchical Clustering Algorithm as the classifier for
word sense induction. Experiments show the system can achieve 72% F-score about
train-corpus and 65% F-score about test-corpus.
</bodyText>
<sectionHeader confidence="0.998635" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.993687766666667">
Word sense induction is a central problem in many natural language processing tasks such
as information extraction, information retrieval, and machine translation [Vickrey et al.,
2005].
Clp 2010 launches totally 4 tasks for evaluation exercise, these are: Chinese word
segmentation, Chinese parsing, Chinese Personal Name disambiguation and Chinese Word
Sense Induction. We participated in task 4, which is Chinese Word Sense Induction..
Because the contents surround an ambiguous word is related to its meaning, we solve the
sense problem by grouping the instances of the target word into the supposed number of
clusters according to the similarity of contexts of the instance. In this paper we used the
hierarchical clustering algorithm to accomplish the problem.
The task can be defined as two stage process: Feature selection and word clustering.
Researchers have proposed much approach to the sense induction task which involved the use
of basic word co-occurrence features and application of classical clustering algorithms.
Because the meanings of unknown words can be inferred from the contexts in which they
appear, Pantel and Lin (2002) map the senses to WordNet. More recently, the mapping has
been used to test the system on publicly available benchmarks (Purandare and Pedersen, 2004;
Niu et al., 2005).
However, this approach does not generalize to multiple-sense words. Each sense of a
polysemous word can appear in a different context, there have been many attempts in recent
years to apply classical clustering algorithms to this problem.
Clustering algorithms have been employed ranging from k-means (Purandare and Pedersen,
2004), to agglomerative clustering (Sch¨utze, 1998), and the Information Bottleneck (Niu et
al., 2007). Senses are induced by identifying highly dense subgraphs (hubs) in the
co-occurrence graph (V´eronis, 2004).The sIB algorithm was used to estimate cluster
structure, which measures the similarity of contexts of instances according to the similarity of
their feature conditional distribution(Slonim, et al.,2002). Each algorithm treats words as
feature vectors, using the same similarity function based on context information.
The remainder of this paper is organized as follows. In section 2 the Featured set and
word similarity definition is introduced. The hierarchical clustering algorithm is presented in
section 3. Section 4 provides the experimental results and conclusion is drawn in section 5.
</bodyText>
<page confidence="0.955249">
1
</page>
<sectionHeader confidence="0.577373" genericHeader="method">
2. Feature Selection and Word Similarity Definition
</sectionHeader>
<subsectionHeader confidence="0.87156">
2.1 Feature Selection
</subsectionHeader>
<bodyText confidence="0.957418272727273">
A feature set is used designed to capture both immediate local context in our
experiment, wider context and syntactic context. Specifically, we experimented with several
feature categories: ±5-word window (5w), ±3-word window (3w), part-of speech n-grams and
dependency relations. These features have been widely adopted in various word sense
induction algorithms. The overall best scores are achieved with local (5 words) context
windows.
. 2.2 Similarity Definition
We treat the context words as feature vectors, using the same similarity function.
Suppose Ci (wi1, wi2  win) is the contexts set of sentence Si , and Cj (wj1, wj2  wjn) is the
contexts set of sentence Sj .
Then we defined sim(Si, Sj) = 1: wklsim(wik, w jl) , here wkl is variable weight,
</bodyText>
<equation confidence="0.904222">
WjkE Ci
Wjl E Cj
sim(wik, wjl) =
,  is an adjustable parameter with a value of 1.2, and
Where
dis(wik , wjl ) +
</equation>
<bodyText confidence="0.5962415">
Dis(wik, wjl) is the path length between wik and wjl based on the semantic tree structure
used for TongYiCi CiLin (同义词词林).
</bodyText>
<sectionHeader confidence="0.93728" genericHeader="method">
3. The Hierarchical Clustering Algorithm Used In Word Sense Induction
</sectionHeader>
<bodyText confidence="0.99965775">
Sense induction is viewed as an unsupervised clustering problem where to group a
word’s contexts into different classes, each representing a word sense. In this paper, we use
the bottom-up clumping approach, which begin with n singleton clusters and successively
merge clusters to produce the other ones.
</bodyText>
<listItem confidence="0.9626978">
Table1: Hierarchical Clustering Algorithm:
1. initialize number of senses n 、 number of clusters m
and clusters Ci (wi1, wi2 ), i =1,2  m
2. Set k = n
3. Set k =k-1
</listItem>
<page confidence="0.581242">
2
</page>
<listItem confidence="0.924672333333333">
4. Find the nearest clusters Ci and Cj , Merge Ci and Cj
5. If k  m , go to step 3, otherwise go to step 6;
6. return m clusters
</listItem>
<bodyText confidence="0.996249666666667">
The merging of the two clusters in step 4 simply corresponds to adding an edge between
the nearest pair of nodes in Ci and Cj .To find the nearest clusters, the following clustering
similarity function is used:
</bodyText>
<equation confidence="0.985069666666667">
(Si,Sj)   wklsim(wik,wjl)
Wjk Ci .
Wjl  Cj
</equation>
<bodyText confidence="0.859422666666667">
Our model incorporates features based on lexical information and parts of speech.
So we propose a improved hierarchical clustering algorithm based on parts of speech.
Table2: improved algorithm based on parts of speech.
</bodyText>
<listItem confidence="0.984324181818182">
1. initialize number of senses n 、 number of clusters m
and clusters Ci(wi1,wi2•••),i1,2•••m
2. Part of Speech Tagging on the corpus
3. Divided n senses into nn classes base on the information of parts of
speech.
4. If nn  m , return m clusters
5. If nn m , invoke hierarchical clustering algorithm in different
classes,merge clusters into m cluster.
6.if nn  m , invoke hierarchical clustering algorithm in different
tagging, merge clusters into m cluster.
7. return m clusters
</listItem>
<sectionHeader confidence="0.982347" genericHeader="evaluation">
4. Experimental Results
</sectionHeader>
<bodyText confidence="0.9027495">
The test data includes totally 100 ambiguous Chinese words, every word have 50
sim
</bodyText>
<page confidence="0.996922">
3
</page>
<tableCaption confidence="0.4780715">
untagged instances. Table3 show the best/worst/average F-Score of our system about
train-corpus and test-corpus.
</tableCaption>
<table confidence="0.998564666666667">
Best word Worst word All words
Train-corpus 0.98 0.5 0.73
Test-corpus 0.65
</table>
<tableCaption confidence="0.950968666666667">
Table 3 Model performance with deferent corpus
Table 4 shows the performance of our model about train-corpus when using 3w and 5w
word windows, which represent more immediate, local context.
</tableCaption>
<table confidence="0.9996824">
Best word Worst word All words
3w(±3-word 0.98 0.5 0.73
window)
5w(±5-word 0.92 0.52 0.72
window)
</table>
<tableCaption confidence="0.918547666666667">
Table 4 Model performance with deferent windows
Table 5 summarizes the F-score in our system about train-corpus when using deferent
similarity definition.
</tableCaption>
<table confidence="0.999049333333333">
Best word Worst word All words
This article 0.98 0.5 0.73
Qun LIU 0.99 0.59 0.78
</table>
<tableCaption confidence="0.997935">
Table 5 Model performance with deferent similarity definition
</tableCaption>
<bodyText confidence="0.99941225">
Experimental results show that the Hierarchical Clustering Algorithm can be applied to
sense induction. Considering words to be feature vectors and applying clustering algorithm
can improve accuracy of the task. A significant gap still exists between the results of these
techniques and the gold standard of manually compiled word sense dictionaries.
</bodyText>
<sectionHeader confidence="0.999165" genericHeader="conclusions">
5. Conclusions
</sectionHeader>
<bodyText confidence="0.999938833333333">
Sense induction is treated as an unsupervised clustering problem. In this paper we adopt
hierarchical clustering algorithm to accomplish the problem. Generate context words
according to this distribution of key words and formalize the induction problem in a
generative mode. Experiments show the system can achieved 72% F-score about train-corpus
and 65% F-score about test-corpus. The basic cluster algorithm can sorts the word sense into
clusters corresponding to the context.
</bodyText>
<sectionHeader confidence="0.999142" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.989015666666667">
Boyd-Graber, Jordan, David Blei, and Xiaojin Zhu. 2007.A topic model for word sense
disambiguation. In Proceedings of the EMNLP-CoNLL. Prague, Czech Republic,pages
1024–1033.
David Vickrey, Luke Biewald, Marc Teyssler, and Daphne Koller. Word-sense
disambiguation for machine translation. In Proceedings of the conference on Human
Language Technology and Empirical Methods in Natural Language Processing, page
</reference>
<page confidence="0.976187">
4
</page>
<reference confidence="0.996268">
771-778, 2005.
Qun LIU , Sujian LI. Word Similarity Computing Based on How-net. Computational
Linguistics and Chinese Language Processing
Niu, Zheng-Yu, Dong-Hong Ji, and Chew-Lim Tan. 2007. I2r: Three systems for word
sense discrimination, chineseword sense disambiguation, and english word sense
disambiguation. In Proceedings of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007). Association for Computational Linguistics, Prague, Czech
Republic, pages 177–182.
Niu, Z.Y., Ji, D.H., &amp; Tan, C.L. 2005. Word Sense Disambiguation Using Label
Propagation Based Semi-Supervised Learning. Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics.
Pantel, Patrick and Dekang Lin. 2002. Discovering word senses from text. In Proceedings
of the 8th KDD. New York, NY, pages 613–619.
Pedersen, Ted. 2007. Umnd2 : Senseclusters applied to the sense induction task of
senseval-4. In Proceedings of SemEval-2007. Prague, Czech Republic, pages 394–397.
Purandare, Amruta and Ted Pedersen. 2004. Word sense discrimination by clustering
contexts in vector and similarity spaces. In Proceedings of the CoNLL. Boston, MA, pages
41–48
V´eronis, Jean. 2004. Hyperlex: lexical cartography for information retrieval. Computer
Speech &amp; Language. 18(3):223–252.
</reference>
<page confidence="0.994336">
5
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.388231">
<title confidence="0.970578">Chinese Word Sense Induction based on Hierarchical Clustering Algorithm</title>
<author confidence="0.928868">Xiaodong Shi Cai</author>
<author confidence="0.928868">Yidong Huang</author>
<author confidence="0.928868">Yan Gao</author>
<address confidence="0.455329">Cognitive Science Department, Xiamen University, Xiamen, 361005, China</address>
<abstract confidence="0.996574">Sense induction seeks to automatically identify word senses of polysemous words encountered in a corpus. Unsupervised word sense induction can be viewed as a clustering problem. In this paper, we used the Hierarchical Clustering Algorithm as the classifier for word sense induction. Experiments show the system can achieve 72% F-score about train-corpus and 65% F-score about test-corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>David Blei</author>
<author>Xiaojin Zhu</author>
</authors>
<title>topic model for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the EMNLP-CoNLL. Prague, Czech Republic,pages</booktitle>
<pages>1024--1033</pages>
<marker>Boyd-Graber, Blei, Zhu, 2007</marker>
<rawString>Boyd-Graber, Jordan, David Blei, and Xiaojin Zhu. 2007.A topic model for word sense disambiguation. In Proceedings of the EMNLP-CoNLL. Prague, Czech Republic,pages 1024–1033.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vickrey</author>
<author>Luke Biewald</author>
<author>Marc Teyssler</author>
<author>Daphne Koller</author>
</authors>
<title>Word-sense disambiguation for machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>771--778</pages>
<contexts>
<context position="799" citStr="Vickrey et al., 2005" startWordPosition="108" endWordPosition="111">Xiamen, 361005, China Abstract Sense induction seeks to automatically identify word senses of polysemous words encountered in a corpus. Unsupervised word sense induction can be viewed as a clustering problem. In this paper, we used the Hierarchical Clustering Algorithm as the classifier for word sense induction. Experiments show the system can achieve 72% F-score about train-corpus and 65% F-score about test-corpus. 1. Introduction Word sense induction is a central problem in many natural language processing tasks such as information extraction, information retrieval, and machine translation [Vickrey et al., 2005]. Clp 2010 launches totally 4 tasks for evaluation exercise, these are: Chinese word segmentation, Chinese parsing, Chinese Personal Name disambiguation and Chinese Word Sense Induction. We participated in task 4, which is Chinese Word Sense Induction.. Because the contents surround an ambiguous word is related to its meaning, we solve the sense problem by grouping the instances of the target word into the supposed number of clusters according to the similarity of contexts of the instance. In this paper we used the hierarchical clustering algorithm to accomplish the problem. The task can be d</context>
</contexts>
<marker>Vickrey, Biewald, Teyssler, Koller, 2005</marker>
<rawString>David Vickrey, Luke Biewald, Marc Teyssler, and Daphne Koller. Word-sense disambiguation for machine translation. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, page 771-778, 2005.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Qun LIU</author>
</authors>
<booktitle>Word Similarity Computing Based on How-net. Computational Linguistics and Chinese Language Processing</booktitle>
<marker>LIU, </marker>
<rawString>Qun LIU , Sujian LI. Word Similarity Computing Based on How-net. Computational Linguistics and Chinese Language Processing</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng-Yu Niu</author>
<author>Dong-Hong Ji</author>
<author>Chew-Lim Tan</author>
</authors>
<title>I2r: Three systems for word sense discrimination, chineseword sense disambiguation, and english word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007). Association for Computational Linguistics,</booktitle>
<pages>177--182</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2361" citStr="Niu et al., 2007" startWordPosition="346" endWordPosition="349">tel and Lin (2002) map the senses to WordNet. More recently, the mapping has been used to test the system on publicly available benchmarks (Purandare and Pedersen, 2004; Niu et al., 2005). However, this approach does not generalize to multiple-sense words. Each sense of a polysemous word can appear in a different context, there have been many attempts in recent years to apply classical clustering algorithms to this problem. Clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 2004), to agglomerative clustering (Sch¨utze, 1998), and the Information Bottleneck (Niu et al., 2007). Senses are induced by identifying highly dense subgraphs (hubs) in the co-occurrence graph (V´eronis, 2004).The sIB algorithm was used to estimate cluster structure, which measures the similarity of contexts of instances according to the similarity of their feature conditional distribution(Slonim, et al.,2002). Each algorithm treats words as feature vectors, using the same similarity function based on context information. The remainder of this paper is organized as follows. In section 2 the Featured set and word similarity definition is introduced. The hierarchical clustering algorithm is pr</context>
</contexts>
<marker>Niu, Ji, Tan, 2007</marker>
<rawString>Niu, Zheng-Yu, Dong-Hong Ji, and Chew-Lim Tan. 2007. I2r: Three systems for word sense discrimination, chineseword sense disambiguation, and english word sense disambiguation. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007). Association for Computational Linguistics, Prague, Czech Republic, pages 177–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Y Niu</author>
<author>D H Ji</author>
<author>C L Tan</author>
</authors>
<title>Word Sense Disambiguation Using Label Propagation Based Semi-Supervised Learning.</title>
<date>2005</date>
<booktitle>Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1931" citStr="Niu et al., 2005" startWordPosition="284" endWordPosition="287">ed the hierarchical clustering algorithm to accomplish the problem. The task can be defined as two stage process: Feature selection and word clustering. Researchers have proposed much approach to the sense induction task which involved the use of basic word co-occurrence features and application of classical clustering algorithms. Because the meanings of unknown words can be inferred from the contexts in which they appear, Pantel and Lin (2002) map the senses to WordNet. More recently, the mapping has been used to test the system on publicly available benchmarks (Purandare and Pedersen, 2004; Niu et al., 2005). However, this approach does not generalize to multiple-sense words. Each sense of a polysemous word can appear in a different context, there have been many attempts in recent years to apply classical clustering algorithms to this problem. Clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 2004), to agglomerative clustering (Sch¨utze, 1998), and the Information Bottleneck (Niu et al., 2007). Senses are induced by identifying highly dense subgraphs (hubs) in the co-occurrence graph (V´eronis, 2004).The sIB algorithm was used to estimate cluster structure, wh</context>
</contexts>
<marker>Niu, Ji, Tan, 2005</marker>
<rawString>Niu, Z.Y., Ji, D.H., &amp; Tan, C.L. 2005. Word Sense Disambiguation Using Label Propagation Based Semi-Supervised Learning. Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<booktitle>In Proceedings of the 8th KDD.</booktitle>
<pages>613--619</pages>
<location>New York, NY,</location>
<contexts>
<context position="1762" citStr="Pantel and Lin (2002)" startWordPosition="256" endWordPosition="259"> sense problem by grouping the instances of the target word into the supposed number of clusters according to the similarity of contexts of the instance. In this paper we used the hierarchical clustering algorithm to accomplish the problem. The task can be defined as two stage process: Feature selection and word clustering. Researchers have proposed much approach to the sense induction task which involved the use of basic word co-occurrence features and application of classical clustering algorithms. Because the meanings of unknown words can be inferred from the contexts in which they appear, Pantel and Lin (2002) map the senses to WordNet. More recently, the mapping has been used to test the system on publicly available benchmarks (Purandare and Pedersen, 2004; Niu et al., 2005). However, this approach does not generalize to multiple-sense words. Each sense of a polysemous word can appear in a different context, there have been many attempts in recent years to apply classical clustering algorithms to this problem. Clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 2004), to agglomerative clustering (Sch¨utze, 1998), and the Information Bottleneck (Niu et al., 2007).</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Pantel, Patrick and Dekang Lin. 2002. Discovering word senses from text. In Proceedings of the 8th KDD. New York, NY, pages 613–619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>Umnd2 : Senseclusters applied to the sense induction task of senseval-4.</title>
<date>2007</date>
<journal>Purandare, Amruta</journal>
<booktitle>In Proceedings of SemEval-2007.</booktitle>
<pages>394--397</pages>
<location>Prague, Czech Republic,</location>
<marker>Pedersen, 2007</marker>
<rawString>Pedersen, Ted. 2007. Umnd2 : Senseclusters applied to the sense induction task of senseval-4. In Proceedings of SemEval-2007. Prague, Czech Republic, pages 394–397. Purandare, Amruta and Ted Pedersen. 2004. Word sense discrimination by clustering contexts in vector and similarity spaces. In Proceedings of the CoNLL. Boston, MA, pages 41–48</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean V´eronis</author>
</authors>
<title>Hyperlex: lexical cartography for information retrieval.</title>
<date>2004</date>
<journal>Computer Speech &amp; Language.</journal>
<volume>18</volume>
<issue>3</issue>
<marker>V´eronis, 2004</marker>
<rawString>V´eronis, Jean. 2004. Hyperlex: lexical cartography for information retrieval. Computer Speech &amp; Language. 18(3):223–252.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>