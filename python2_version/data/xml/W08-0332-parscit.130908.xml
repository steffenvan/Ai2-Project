<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010925">
<title confidence="0.995411">
A Smorgasbord of Features for Automatic MT Evaluation
</title>
<author confidence="0.97878">
Jes´us Gim´enez and Lluis M`arquez
</author>
<affiliation confidence="0.947814">
TALP Research Center, LSI Department
</affiliation>
<address confidence="0.722182">
Universitat Polit`ecnica de Catalunya
Jordi Girona Salgado 1–3, E-08034, Barcelona
</address>
<email confidence="0.99969">
{jgimenez,lluism}@lsi.upc.edu
</email>
<sectionHeader confidence="0.998598" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9967368">
This document describes the approach by the
NLP Group at the Technical University of Cat-
alonia (UPC-LSI), for the shared task on Au-
tomatic Evaluation of Machine Translation at
the ACL 2008 Third SMT Workshop.
</bodyText>
<sectionHeader confidence="0.999388" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999845944444444">
Our proposal is based on a rich set of individual
metrics operating at different linguistic levels: lex-
ical (i.e., on word forms), shallow-syntactic (e.g., on
word lemmas, part-of-speech tags, and base phrase
chunks), syntactic (e.g., on dependency and con-
stituency trees), shallow-semantic (e.g., on named
entities and semantic roles), and semantic (e.g., on
discourse representations). Although from differ-
ent viewpoints, and based on different similarity as-
sumptions, in all cases, translation quality is mea-
sured by comparing automatic translations against
human references. Extensive details on the met-
ric set may be found in the IQMT technical manual
(Gim´enez, 2007).
Apart from individual metrics, we have also
applied a simple integration scheme based on
uniformly-averaged linear metric combinations
(Gim´enez and M`arquez, 2008a).
</bodyText>
<sectionHeader confidence="0.945175" genericHeader="introduction">
2 What is new?
</sectionHeader>
<bodyText confidence="0.942235888888889">
The main novelty, with respect to the set of metrics
presented last year (Gim´enez and M`arquez, 2007),
is the incorporation of a novel family of metrics
at the properly semantic level. DR metrics ana-
lyze similarities between automatic and reference
translations by comparing their respective discourse
representation structures (DRS), as provided by the
the C&amp;C Tools (Clark and Curran, 2004). DRS are
essentially a variation of first-order predicate calcu-
lus which can be seen as semantic trees. We use
three different kinds of metrics:
DR-STM Semantic Tree Matching, a la Liu and
Gildea (2005), but over DRS instead of over
constituency trees.
DR-Or-⋆ Lexical overlapping over DRS.
DR-Orp-⋆ Morphosyntactic overlapping on DRS.
Further details on DR metrics can be found in
(Gim´enez and M`arquez, 2008b).
</bodyText>
<subsectionHeader confidence="0.989886">
2.1 Improved Sentence Level Behavior
</subsectionHeader>
<bodyText confidence="0.8507191875">
Metrics based on deep linguistic analysis rely on
automatic processors trained on out-domain data,
which may be, thus, prone to error. Indeed, we found
out that in many cases, metrics are unable to pro-
duce a result due to the lack of linguistic analysis.
For instance, in our experiments, for SR metrics, we
found that the semantic role labeler was unable to
parse 14% of the sentences. In order to improve the
recall of these metrics, we have designed two simple
variants. Given a linguistic metric x, we define:
• xb --+ by backing off to lexical overlapping,
Ol, only when the linguistic processor is not
able to produce a linguistic analysis. Other-
wise, x score is returned. Lexical scores are
conveniently scaled so that they are in a similar
range to scores of x. Specifically, we multiply
</bodyText>
<page confidence="0.982549">
195
</page>
<note confidence="0.40039">
Proceedings of the Third Workshop on Statistical Machine Translation, pages 195–198,
</note>
<page confidence="0.479382">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.998995">
them by the average x score attained over all
other test cases for which the parser succeeded.
</bodyText>
<listItem confidence="0.9150434">
• xi --+ by linearly interpolating x and Ol scores
for all test cases, via the arithmetic mean.
In both cases, system scores are calculated by av-
eraging over all sentence scores. Currently, these
variants are applied only to SR and DR metrics.
</listItem>
<subsectionHeader confidence="0.999569">
2.2 Uniform Linear Metric Combinations
</subsectionHeader>
<bodyText confidence="0.999892625">
We have simulated a non-parametric combination
scheme based on human acceptability by working
on uniformly averaged linear combinations (ULC)
of metrics (Gim´enez and M`arquez, 2008a). Our ap-
proach is similar to that of Liu and Gildea (2007)
except that in our case the contribution of each met-
ric to the overall score is not adjusted.
Optimal metric sets are determined by maximiz-
ing the correlation with human assessments, either
at the document or sentence level. However, because
exploring all possible combinations was not viable,
we have used a simple algorithm which performs an
approximate search. First, metrics are ranked ac-
cording to their individual quality. Then, following
that order, metrics are added to the optimal set only
if in doing so the global quality increases.
</bodyText>
<sectionHeader confidence="0.999771" genericHeader="related work">
3 Experimental Work
</sectionHeader>
<bodyText confidence="0.913581777777778">
We use all into-English test beds from the 2006
and 2007 editions of the SMT workshop (Koehn
and Monz, 2006; Callison-Burch et al., 2007).
These include the translation of three differ-
ent language-pairs: German-to-English (de-en),
Spanish-to-English (es-en), and French-to-English
(fr-en), over two different scenarios: in-domain (Eu-
ropean Parliament Proceedings) and out-of-domain
(News Commentary Corpus)1. In all cases, a single
reference translation is available. In addition, hu-
man assessments on adequacy and fluency are avail-
able for a subset of systems and sentences. Each
sentence has been evaluated at least by two different
judges. A brief numerical description of these test
beds is available in Table 1.
1We have not used the out-of-domain Czech-to-English test
bed from the 2007 shared task because it includes only 4 sys-
tems, and only 3 of them count on human assessments.
</bodyText>
<table confidence="0.997503071428572">
WMT 2006
in-domain out-of-domain
2,000 cases 1,064 cases
#snt #sys #snt #sys
de-en 2,281 10/12 1,444 10/12
es-en 1,852 11/15 1,008 11/15
fr-en 2,268 11/14 1,281 11/14
WMT 2007
in-domain out-of-domain
2,000 cases 2,007 cases
#snt #sys #snt #sys
de-en 956 7/8 947 5/6
es-en 812 8/10 675 7/9
fr-en 624 7/8 741 7/7
</table>
<tableCaption confidence="0.9997">
Table 1: Test bed description. ‘#snt’ columns show the
</tableCaption>
<bodyText confidence="0.9886912">
number of sentences assessed (considering all systems).
‘#sys’ columns shows the number of systems counting
on human assessments with respect to the total number
of systems which participated in each task.
Metrics are evaluated in terms of human accept-
ability, i.e., according to their ability to capture
the degree of acceptability to humans of automatic
translations. We measure human acceptability by
computing Pearson correlation coefficients between
automatic metric scores and human assessments of
translation quality both at document and sentence
level. We use the sum of adequacy and fluency to
simulate a global assessment of quality. Assess-
ments from different judges over the same test case
are averaged into a single score.
</bodyText>
<subsectionHeader confidence="0.993745">
3.1 Individual Performance
</subsectionHeader>
<bodyText confidence="0.999967846153846">
In first place, we study the behavior of individual
metrics. Table 2 shows meta-evaluation results, over
into-English WMT 2007 test beds, in-domain and
out-of-domain, both at the system and sentence lev-
els, for a set of selected representatives from several
linguistic levels.
At the system level (columns 1-6), corroborating
previous findings by Gim´enez and M`arquez (2007),
highest levels of correlation are attained by met-
rics based on deep linguistic analysis (either syn-
tactic or semantic). In particular, two kinds of met-
rics, respectively based on head-word chain match-
ing over grammatical categories and relations (‘DP-
</bodyText>
<page confidence="0.988234">
196
</page>
<table confidence="0.999923615384615">
Level Metric System Level Sentence Level
de-en es-en fr-en de-en es-en fr-en
in out in out in out in out in out in out
1-TER 0.64 0.41 0.83 0.58 0.72 0.47 0.43 0.29 0.23 0.23 0.29 0.20
BLEU 0.87 0.76 0.88 0.70 0.74 0.54 0.46 0.27 0.33 0.20 0.20 0.12
Lexical GTM (e = 2) 0.82 0.69 0.93 0.71 0.76 0.60 0.56 0.36 0.43 0.33 0.27 0.18
ROUGEW 0.87 0.91 0.96 0.78 0.85 0.83 0.58 0.40 0.43 0.35 0.30 0.31
METEORwn 0.83 0.92 0.96 0.74 0.91 0.86 0.53 0.41 0.35 0.28 0.33 0.32
Ol 0.79 0.75 0.91 0.55 0.81 0.66 0.48 0.33 0.35 0.30 0.30 0.21
CP-Oc-* 0.84 0.88 0.95 0.62 0.84 0.76 0.49 0.37 0.38 0.33 0.32 0.25
DP-HWCw-4 0.85 0.93 0.96 0.68 0.84 0.80 0.31 0.26 0.33 0.07 0.10 0.14
Syntactic DP-HWCc-4 0.91 0.98 0.96 0.90 0.98 0.95 0.30 0.25 0.23 0.06 0.13 0.12
DP-HWCr-4 0.89 0.97 0.97 0.92 0.97 0.95 0.33 0.28 0.29 0.08 0.16 0.16
DP-Or-* 0.88 0.96 0.97 0.84 0.89 0.89 0.57 0.41 0.44 0.36 0.33 0.30
CP-STM-4 0.88 0.97 0.97 0.79 0.89 0.89 0.49 0.39 0.40 0.37 0.32 0.26
NE-Me-* -0.13 0.79 0.95 0.68 0.87 0.92 -0.03 0.07 0.07 -0.05 0.05 0.06
NE-Oe-** -0.18 0.78 0.95 0.58 0.81 0.71 0.32 0.26 0.37 0.26 0.31 0.20
SR-Or-* 0.55 0.96 0.94 0.69 0.89 0.85 0.26 0.14 0.30 0.11 0.08 0.19
SR-Or-*b 0.24 0.98 0.94 0.68 0.92 0.87 0.33 0.21 0.35 0.15 0.18 0.24
Shallow SR-Or-*i 0.51 0.95 0.93 0.67 0.88 0.83 0.37 0.26 0.38 0.19 0.24 0.27
Semantic SR-Mr-* 0.38 0.95 0.96 0.83 0.79 0.75 0.32 0.18 0.28 0.18 0.08 0.14
SR-Mr-*b 0.14 0.98 0.97 0.82 0.84 0.79 0.37 0.23 0.32 0.21 0.15 0.17
SR-Mr-*i 0.38 0.94 0.96 0.80 0.79 0.74 0.40 0.27 0.36 0.24 0.20 0.20
SR-Or 0.73 0.99 0.94 0.66 0.97 0.93 0.12 0.09 0.16 0.07 -0.04 0.17
SR-Ori 0.66 0.99 0.94 0.64 0.95 0.89 0.29 0.25 0.29 0.19 0.15 0.28
DR-Or-* 0.87 0.89 0.96 0.71 0.78 0.75 0.50 0.40 0.37 0.35 0.27 0.28
DR-Or-*b 0.91 0.93 0.97 0.72 0.83 0.80 0.52 0.41 0.38 0.34 0.28 0.27
DR-Or-*i 0.87 0.87 0.96 0.68 0.79 0.74 0.53 0.42 0.39 0.35 0.30 0.28
DR-Orp-* 0.92 0.98 0.99 0.81 0.91 0.89 0.42 0.32 0.29 0.25 0.21 0.30
Semantic DR-Orp-*b 0.93 0.98 0.99 0.81 0.94 0.91 0.45 0.34 0.32 0.22 0.22 0.30
DR-Orp-*i 0.91 0.95 0.98 0.75 0.89 0.85 0.50 0.38 0.36 0.28 0.27 0.33
DR-STM-4 0.89 0.95 0.98 0.79 0.85 0.87 0.28 0.29 0.25 0.21 0.15 0.22
DR-STM-4b 0.92 0.97 0.98 0.80 0.90 0.91 0.36 0.31 0.29 0.21 0.19 0.23
DR-STM-4i 0.91 0.94 0.97 0.74 0.87 0.86 0.43 0.35 0.34 0.26 0.24 0.27
Optimal07 0.93 1.00 0.99 0.92 0.98 0.95 0.60 0.46 0.47 0.42 0.36 0.39
Optimal06 0.01 0.95 0.96 0.75 0.97 0.87 0.50 0.41 0.40 0.20 0.27 0.30
ULC Optimal*07 0.93 0.98 0.99 0.81 0.94 0.91 0.58 0.45 0.46 0.39 0.35 0.34
Optimal*06 0.34 0.96 0.98 0.82 0.92 0.93 0.54 0.41 0.42 0.32 0.32 0.34
Optimalh 0.87 0.98 0.97 0.79 0.91 0.89 0.56 0.44 0.43 0.32 0.31 0.35
</table>
<tableCaption confidence="0.999415">
Table 2: Meta-evaluation results based on human acceptability for the WMT 2007 into-English translation tasks
</tableCaption>
<bodyText confidence="0.999875346153846">
HWC,-4’, ‘DP-HWCT-4’), and morphosyntactic over-
lapping over discourse representations (‘DR-OTp-*’),
are consistently among the top-scoring in all test
beds. At the lexical level, variants of ROUGE and
METEOR attain the best results, close to the perfor-
mance of syntactic and semantic features. It can also
be observed that metrics based on semantic roles
and named entities have serious troubles with the
German-to-English in-domain test bed (column 1).
At the sentence level, the highest levels of corre-
lation are attained by metrics based on lexical simi-
larity alone, only rivaled by lexical overlapping over
dependency relations (‘DP-OT-*’) and discourse rep-
resentations (‘DR-OT-*’). We speculate the underly-
ing cause might be on the side of parsing errors. In
that respect, lexical back-off strategies report in all
cases a significant improvement.
It can also be observed that, over these test beds,
metrics based on named entities are completely use-
less at the sentence level, at least in isolation. The
reason is that they capture a very partial aspect of
quality which may be not relevant in many cases.
This has been verified by computing the ‘NE-Oe-
**’ variant which considers also lexical overlapping
over regular items. Observe how this metric attains
a much higher correlation with human assessments.
</bodyText>
<page confidence="0.997005">
197
</page>
<subsectionHeader confidence="0.985535">
3.2 Metric Combinations
</subsectionHeader>
<bodyText confidence="0.996013729166667">
For future work, we plan to apply parametric
combination schemes based on human likeness clas-
sifiers, as suggested by Kulesza and Shieber (2004).
We must also further investigate the impact of pars-
ing errors on the performance of linguistic metrics.
We also study the behavior of metric combinations
under the ULC scheme. Last 5 rows in Table 2
shows meta-evaluation results following 3 different
optimization strategies:
Optimal: the metric set is optimized for each test
bed (language-pair and domain) individually.
Optimal⋆: the metric set is optimized over the
union of all test beds.
Optimalh: the metric set is heuristically defined
so as to include several of the top-scoring
representatives from each level: Optimalh =
{ ROUGEW, METEOR,,,,,,3y,,,, DP-HWC,-4, DP-
HWCr-4, DP-Or-*, CP-STM-4, SR-Mr-*i, SR-
Or-*i, SR-Ori, DR-Or-*i, DR-Orp-*b }.
We present results optimizing over the 2006 and
2007 data sets. Let us provide, as an illustration,
Optimal*07 sets. For instance, at the system level,
no combination improved the isolated global perfor-
mance of the ‘DR-Orp-*b’ metric (R=0.94). In con-
trast, at the sentence level, the optimal metric set
contains several metrics from each linguistic level:
Optimal*07 = { ROUGEW, DP-Or-*, CP-STM-4, SR-
Or-*i, SR-Mr-*i, DR-Or-*i }. A similar pattern is
observed for all test beds, both at the system and
sentence levels, although with different metrics.
The behavior of optimal metric sets is in general
quite stable, except for the German-to-English in-
domain test bed which presents an anomalous be-
havior when meta-evaluating WMT 2006 optimal
metric sets at the system level. The reason for this
anomaly is in the ‘NE-Me-*’ metric, which is in-
cluded in the 2006 optimal set: { ‘NE-Me-*’, ‘SR-
Ori’ }. ‘NE-Me-*’ is based on lexical matching over
named entities, and attains in the 2006 German-to-
English in-domain test bed a very high correlation
of 0.95 with human assessments. This partial aspect
of quality seems to be of marginal importance in the
2007 test bed. We have verified this hypothesis by
computing optimal metrics sets without considering
NE variants. Correlation increases to more reason-
able values (e.g., from 0.01 to 0.66 and from 0.34
to 0.91. This result suggests that more robust metric
combination schemes should be pursued.
</bodyText>
<sectionHeader confidence="0.997714" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999031">
This research has been funded by the Spanish Min-
istry of Education and Science (OpenMT, TIN2006-
15307-C03-02). Our group is recognized by DURSI
as a Quality Research Group (2005 SGR-00130).
</bodyText>
<sectionHeader confidence="0.998968" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999933157894737">
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-)
Evaluation of Machine Translation. In Proceedings of
the ACL Second SMT Workshop, pages 136–158.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and Log-Linear Models. In Proceed-
ings of the 42nd Annual Meeting of the Association for
Computational Linguistics (ACL), pages 104–111.
Jes´us Gim´enez and Lluis M`arquez. 2007. Linguistic
Features for Automatic Evaluation of Heterogeneous
MT Systems. In Proceedings of the ACL Second SMT
Workshop, pages 256–264.
Jes´us Gim´enez and Lluis M`arquez. 2008a. Hetero-
geneous Automatic MT Evaluation Through Non-
Parametric Metric Combinations. In Proceedings of
IJCNLP, pages 319–326.
Jes´us Gim´enez and Lluis M`arquez. 2008b. On the
Robustness of Linguistic Features for Automatic MT
Evaluation. To be published.
Jes´us Gim´enez. 2007. IQMT v 2.1. Tech-
nical Manual (LSI-07-29-R). Technical re-
port, TALP Research Center. LSI Department.
http://www.lsi.upc.edu/ nlp/IQMT/IQMT.v2.1.pdf.
Philipp Koehn and Christof Monz. 2006. Manual and
Automatic Evaluation of Machine Translation between
European Languages. In Proceedings of the Workshop
on Statistical Machine Translation, pages 102–121.
Alex Kulesza and Stuart M. Shieber. 2004. A learning
approach to improving sentence-level MT evaluation.
In Proceedings of the 10th TMI, pages 75–84.
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Proceed-
ings ofACL Workshop on Intrinsic and Extrinsic Eval-
uation Measures for MT and/or Summarization.
Ding Liu and Daniel Gildea. 2007. Source-Language
Features and Maximum Correlation Training for Ma-
chine Translation Evaluation. In Proceedings of
NAACL, pages 41–48.
</reference>
<page confidence="0.997719">
198
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.184313">
<title confidence="0.999839">A Smorgasbord of Features for Automatic MT Evaluation</title>
<author confidence="0.976697">Gim´enez</author>
<affiliation confidence="0.975153">TALP Research Center, LSI Universitat Polit`ecnica de</affiliation>
<note confidence="0.344083">Jordi Girona Salgado 1–3, E-08034,</note>
<abstract confidence="0.9757158">This document describes the approach by the NLP Group at the Technical University of Catalonia (UPC-LSI), for the shared task on Automatic Evaluation of Machine Translation at</abstract>
<note confidence="0.648506">the ACL 2008 Third SMT Workshop.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>(Meta-) Evaluation of Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL Second SMT Workshop,</booktitle>
<pages>136--158</pages>
<contexts>
<context position="4461" citStr="Callison-Burch et al., 2007" startWordPosition="695" endWordPosition="698">ore is not adjusted. Optimal metric sets are determined by maximizing the correlation with human assessments, either at the document or sentence level. However, because exploring all possible combinations was not viable, we have used a simple algorithm which performs an approximate search. First, metrics are ranked according to their individual quality. Then, following that order, metrics are added to the optimal set only if in doing so the global quality increases. 3 Experimental Work We use all into-English test beds from the 2006 and 2007 editions of the SMT workshop (Koehn and Monz, 2006; Callison-Burch et al., 2007). These include the translation of three different language-pairs: German-to-English (de-en), Spanish-to-English (es-en), and French-to-English (fr-en), over two different scenarios: in-domain (European Parliament Proceedings) and out-of-domain (News Commentary Corpus)1. In all cases, a single reference translation is available. In addition, human assessments on adequacy and fluency are available for a subset of systems and sentences. Each sentence has been evaluated at least by two different judges. A brief numerical description of these test beds is available in Table 1. 1We have not used th</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2007</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2007. (Meta-) Evaluation of Machine Translation. In Proceedings of the ACL Second SMT Workshop, pages 136–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Parsing the WSJ using CCG and Log-Linear Models.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>104--111</pages>
<contexts>
<context position="1722" citStr="Clark and Curran, 2004" startWordPosition="248" endWordPosition="251"> in the IQMT technical manual (Gim´enez, 2007). Apart from individual metrics, we have also applied a simple integration scheme based on uniformly-averaged linear metric combinations (Gim´enez and M`arquez, 2008a). 2 What is new? The main novelty, with respect to the set of metrics presented last year (Gim´enez and M`arquez, 2007), is the incorporation of a novel family of metrics at the properly semantic level. DR metrics analyze similarities between automatic and reference translations by comparing their respective discourse representation structures (DRS), as provided by the the C&amp;C Tools (Clark and Curran, 2004). DRS are essentially a variation of first-order predicate calculus which can be seen as semantic trees. We use three different kinds of metrics: DR-STM Semantic Tree Matching, a la Liu and Gildea (2005), but over DRS instead of over constituency trees. DR-Or-⋆ Lexical overlapping over DRS. DR-Orp-⋆ Morphosyntactic overlapping on DRS. Further details on DR metrics can be found in (Gim´enez and M`arquez, 2008b). 2.1 Improved Sentence Level Behavior Metrics based on deep linguistic analysis rely on automatic processors trained on out-domain data, which may be, thus, prone to error. Indeed, we fo</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004. Parsing the WSJ using CCG and Log-Linear Models. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL), pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Lluis M`arquez</author>
</authors>
<title>Linguistic Features for Automatic Evaluation of Heterogeneous MT Systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL Second SMT Workshop,</booktitle>
<pages>256--264</pages>
<marker>Gim´enez, M`arquez, 2007</marker>
<rawString>Jes´us Gim´enez and Lluis M`arquez. 2007. Linguistic Features for Automatic Evaluation of Heterogeneous MT Systems. In Proceedings of the ACL Second SMT Workshop, pages 256–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Lluis M`arquez</author>
</authors>
<title>Heterogeneous Automatic MT Evaluation Through NonParametric Metric Combinations.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<pages>319--326</pages>
<marker>Gim´enez, M`arquez, 2008</marker>
<rawString>Jes´us Gim´enez and Lluis M`arquez. 2008a. Heterogeneous Automatic MT Evaluation Through NonParametric Metric Combinations. In Proceedings of IJCNLP, pages 319–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Lluis M`arquez</author>
</authors>
<title>On the Robustness of Linguistic Features for Automatic MT Evaluation.</title>
<date>2008</date>
<note>To be published.</note>
<marker>Gim´enez, M`arquez, 2008</marker>
<rawString>Jes´us Gim´enez and Lluis M`arquez. 2008b. On the Robustness of Linguistic Features for Automatic MT Evaluation. To be published.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
</authors>
<title>IQMT v 2.1.</title>
<date>2007</date>
<tech>Technical Manual (LSI-07-29-R). Technical report,</tech>
<institution>TALP Research Center. LSI Department.</institution>
<note>http://www.lsi.upc.edu/ nlp/IQMT/IQMT.v2.1.pdf.</note>
<marker>Gim´enez, 2007</marker>
<rawString>Jes´us Gim´enez. 2007. IQMT v 2.1. Technical Manual (LSI-07-29-R). Technical report, TALP Research Center. LSI Department. http://www.lsi.upc.edu/ nlp/IQMT/IQMT.v2.1.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
</authors>
<title>Manual and Automatic Evaluation of Machine Translation between European Languages.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation,</booktitle>
<pages>102--121</pages>
<contexts>
<context position="4431" citStr="Koehn and Monz, 2006" startWordPosition="691" endWordPosition="694">tric to the overall score is not adjusted. Optimal metric sets are determined by maximizing the correlation with human assessments, either at the document or sentence level. However, because exploring all possible combinations was not viable, we have used a simple algorithm which performs an approximate search. First, metrics are ranked according to their individual quality. Then, following that order, metrics are added to the optimal set only if in doing so the global quality increases. 3 Experimental Work We use all into-English test beds from the 2006 and 2007 editions of the SMT workshop (Koehn and Monz, 2006; Callison-Burch et al., 2007). These include the translation of three different language-pairs: German-to-English (de-en), Spanish-to-English (es-en), and French-to-English (fr-en), over two different scenarios: in-domain (European Parliament Proceedings) and out-of-domain (News Commentary Corpus)1. In all cases, a single reference translation is available. In addition, human assessments on adequacy and fluency are available for a subset of systems and sentences. Each sentence has been evaluated at least by two different judges. A brief numerical description of these test beds is available in</context>
</contexts>
<marker>Koehn, Monz, 2006</marker>
<rawString>Philipp Koehn and Christof Monz. 2006. Manual and Automatic Evaluation of Machine Translation between European Languages. In Proceedings of the Workshop on Statistical Machine Translation, pages 102–121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Kulesza</author>
<author>Stuart M Shieber</author>
</authors>
<title>A learning approach to improving sentence-level MT evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 10th TMI,</booktitle>
<pages>75--84</pages>
<contexts>
<context position="11213" citStr="Kulesza and Shieber (2004)" startWordPosition="1829" endWordPosition="1832"> also be observed that, over these test beds, metrics based on named entities are completely useless at the sentence level, at least in isolation. The reason is that they capture a very partial aspect of quality which may be not relevant in many cases. This has been verified by computing the ‘NE-Oe**’ variant which considers also lexical overlapping over regular items. Observe how this metric attains a much higher correlation with human assessments. 197 3.2 Metric Combinations For future work, we plan to apply parametric combination schemes based on human likeness classifiers, as suggested by Kulesza and Shieber (2004). We must also further investigate the impact of parsing errors on the performance of linguistic metrics. We also study the behavior of metric combinations under the ULC scheme. Last 5 rows in Table 2 shows meta-evaluation results following 3 different optimization strategies: Optimal: the metric set is optimized for each test bed (language-pair and domain) individually. Optimal⋆: the metric set is optimized over the union of all test beds. Optimalh: the metric set is heuristically defined so as to include several of the top-scoring representatives from each level: Optimalh = { ROUGEW, METEOR,</context>
</contexts>
<marker>Kulesza, Shieber, 2004</marker>
<rawString>Alex Kulesza and Stuart M. Shieber. 2004. A learning approach to improving sentence-level MT evaluation. In Proceedings of the 10th TMI, pages 75–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Syntactic Features for Evaluation of Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.</booktitle>
<contexts>
<context position="1925" citStr="Liu and Gildea (2005)" startWordPosition="282" endWordPosition="285">, 2008a). 2 What is new? The main novelty, with respect to the set of metrics presented last year (Gim´enez and M`arquez, 2007), is the incorporation of a novel family of metrics at the properly semantic level. DR metrics analyze similarities between automatic and reference translations by comparing their respective discourse representation structures (DRS), as provided by the the C&amp;C Tools (Clark and Curran, 2004). DRS are essentially a variation of first-order predicate calculus which can be seen as semantic trees. We use three different kinds of metrics: DR-STM Semantic Tree Matching, a la Liu and Gildea (2005), but over DRS instead of over constituency trees. DR-Or-⋆ Lexical overlapping over DRS. DR-Orp-⋆ Morphosyntactic overlapping on DRS. Further details on DR metrics can be found in (Gim´enez and M`arquez, 2008b). 2.1 Improved Sentence Level Behavior Metrics based on deep linguistic analysis rely on automatic processors trained on out-domain data, which may be, thus, prone to error. Indeed, we found out that in many cases, metrics are unable to produce a result due to the lack of linguistic analysis. For instance, in our experiments, for SR metrics, we found that the semantic role labeler was un</context>
</contexts>
<marker>Liu, Gildea, 2005</marker>
<rawString>Ding Liu and Daniel Gildea. 2005. Syntactic Features for Evaluation of Machine Translation. In Proceedings ofACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Source-Language Features and Maximum Correlation Training for Machine Translation Evaluation.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="3759" citStr="Liu and Gildea (2007)" startWordPosition="580" endWordPosition="583">s them by the average x score attained over all other test cases for which the parser succeeded. • xi --+ by linearly interpolating x and Ol scores for all test cases, via the arithmetic mean. In both cases, system scores are calculated by averaging over all sentence scores. Currently, these variants are applied only to SR and DR metrics. 2.2 Uniform Linear Metric Combinations We have simulated a non-parametric combination scheme based on human acceptability by working on uniformly averaged linear combinations (ULC) of metrics (Gim´enez and M`arquez, 2008a). Our approach is similar to that of Liu and Gildea (2007) except that in our case the contribution of each metric to the overall score is not adjusted. Optimal metric sets are determined by maximizing the correlation with human assessments, either at the document or sentence level. However, because exploring all possible combinations was not viable, we have used a simple algorithm which performs an approximate search. First, metrics are ranked according to their individual quality. Then, following that order, metrics are added to the optimal set only if in doing so the global quality increases. 3 Experimental Work We use all into-English test beds f</context>
</contexts>
<marker>Liu, Gildea, 2007</marker>
<rawString>Ding Liu and Daniel Gildea. 2007. Source-Language Features and Maximum Correlation Training for Machine Translation Evaluation. In Proceedings of NAACL, pages 41–48.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>