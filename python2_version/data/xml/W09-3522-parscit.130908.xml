<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.053516">
<title confidence="0.99207">
Transliteration System using pair HMM with weighted FSTs
</title>
<author confidence="0.818163">
Peter Nabende
Alfa Informatica, CLCG,
</author>
<affiliation confidence="0.984649">
University of Groningen, Netherlands
</affiliation>
<email confidence="0.997495">
p.nabende@rug.nl
</email>
<sectionHeader confidence="0.993568" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.903302533333333">
This paper presents a transliteration system
based on pair Hidden Markov Model (pair
HMM) training and Weighted Finite State
Transducer (WFST) techniques. Parameters
used by WFSTs for transliteration generation
are learned from a pair HMM. Parameters
from pair-HMM training on English-Russian
data sets are found to give better transliteration
quality than parameters trained for WFSTs for
corresponding structures. Training a pair
HMM on English vowel bigrams and standard
bigrams for Cyrillic Romanization, and using
a few transformation rules on generated Rus-
sian transliterations to test for context im-
proves the system’s transliteration quality.
</bodyText>
<sectionHeader confidence="0.998131" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.984684423076923">
Machine transliteration is the automatic trans-
formation of a word in a source language to a
phonetically equivalent word in a target language
that uses a different writing system. Translitera-
tion is important for various Natural Language
Processing (NLP) applications including: Cross
Lingual Information Retrieval (CLIR), and Ma-
chine Translation (MT). This paper introduces a
system that utilizes parameters learned for a pair
Hidden Markov Model (pair HMM) in a shared
transliteration generation task1. The pair HMM
has been used before (Mackay and Kondrak,
2005; Wieling et al., 2007) for string similarity
estimation, and is based on the notion of string
Edit Distance (ED). String ED is defined here as
the total edit cost incurred in transforming a
source language string (S) to a target language
string (T) through a sequence of edit operations.
The edit operations include: (M)atching an ele-
ment in S with an element in T; (I)nserting an
element into T, and (D)eleting an element in S.
1 The generation task is part of the NEWS 2009 machine
transliteration shared task (Li et al., 2009)
Based on all representative symbols used for
each of the two languages, emission costs for
each of the edit operations and transition parame-
ters can be estimated and used in measuring the
similarity between two strings. To generate
transliterations using pair HMM parameters,
WFST (Graehl, 1997) techniques are adopted.
Transliteration training is based mainly on the
initial orthographic representation and no explicit
phonetic scheme is used. Instead, transliteration
quality is tested for different bigram combina-
tions including all English vowel bigram combi-
nations and n-gram combinations specified for
Cyrillic Romanization by the US Board on Geo-
graphic Names and British Permanent Commit-
tee on Geographic Names (BGN/PCGN). How-
ever, transliteration parameters can still be esti-
mated for a pair HMM when a particular phonet-
ic representation scheme is used.
The quality of transliterations generated using
pair HMM parameters is evaluated against trans-
literations generated from training WFSTs and
transliterations generated using a Phrase-based
Statistical Machine Translation (PBSMT) sys-
tem. Section 2 describes the components of the
transliteration system that uses pair HMM para-
meters; section 3 gives the experimental set up
and results associated with the transliterations
generated; and section 4 concludes the paper.
</bodyText>
<sectionHeader confidence="0.957915" genericHeader="method">
2 Machine Transliteration System
</sectionHeader>
<bodyText confidence="0.997264125">
The transliteration system comprises of a training
and generation components (Figure 1). In the
training component, the Baum-Welch Expecta-
tion Maximization (EM) algorithm (Baum et al.,
1970) is used to learn the parameters of a pair
HMM. In the generation component, WFST
techniques (Graehl, 1997) model the learned pair
HMM parameters for generating transliterations.
</bodyText>
<subsectionHeader confidence="0.979124">
2.1 Parameter Estimation for a pair-HMM
</subsectionHeader>
<bodyText confidence="0.9985375">
A pair HMM has two output observations (Fig-
ure 2) that are aligned through the hidden states,
</bodyText>
<page confidence="0.90744">
100
</page>
<note confidence="0.9935885">
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 100–103,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<figureCaption confidence="0.958557666666667">
Figure 1: Machine Transliteration system
Figure 2: pair-HMM alignment for converting an
English string “Peter” to a Russian string “Help”
</figureCaption>
<bodyText confidence="0.993781166666667">
unlike the classic HMMs that have only one ob-
servation sequence. The pair HMM structure dif-
fers from that of WFSTs in that in WFSTs the
input and output symbols and associated weights
occur on a transition arc while for the pair HMM,
the input and output symbols and associated edit
costs are encoded in a node. Two main sets of
parameters are learned for the pair HMM: transi-
tion parameters (S, c, X, TM, TDI) as shown in Fig-
ure 3 for different state transitions; and emission
parameters in the (M)atch state and the other two
gap states (D and I). si in Figure 3 is the ith sym-
bol in the source language string S while tj is the
jth symbol in T.
si
Pair HMM Emission parameters are stored in
matrix form in three tables associated with the
edit operations; transition parameters are also
stored in matrix form in a table. The emission
parameters are (n × m) + n + m in total; n and m
are the numbers of symbols in the pair HMM
source language alphabet (VS) and target lan-
guage alphabet (VT) respectively. The parameters
of starting in a given edit operation state are de-
rived from the parameters of transiting from the
match state (M) to either D or I or back to M.
Although pair HMM training is evaluated
against WFST training, there is no major differ-
ence in the training approach used in both cases;
a forward-backward EM algorithm is used in
each case. The main difference is in the struc-
ture; for the pair-HMM, the state transition pa-
rameter is also incorporated into the weight that
measures the level of relationship between the
input and output symbol when transformed to a
WFST arc.
</bodyText>
<subsectionHeader confidence="0.999552">
2.2 Generating Transliterations in WFSTs
</subsectionHeader>
<bodyText confidence="0.9999640625">
A Weighted Finite State Transducer is a finite
automaton whose state transitions are labeled
with input and output elements and weights that
express the level of relationship between the in-
put and output elements. Although the frame-
work of WFSTs has mostly been applied in
representing various models for speech recogni-
tion (Mohri et al., 2008) including HMMs,
WFSTs have as well been used for transliteration
(Knight and Graehl, 1998), and are the most suit-
able for modeling pair HMM constraints for ge-
nerating transliterations. In the WFST frame-
work, it is possible to specify various configura-
tions associated with constraints inherent in a
particular model. Figure 4 shows a WFST that
precisely corresponds to the structure of the pair
</bodyText>
<figure confidence="0.994762813953488">
Pairs of correct
transliterations
Transliteration parame-
ter estimation for pair
HMM
Transliteration gen-
eration using
Weighted Finite State
Transducers
Target
name
Source
name
Estimated
parameters
P : 11
M
e : e
M M D M End
t : T e : _ r : p
si:e
s
1- s- X- TDI
D
D
TDI
e:e
e:e
8
si:e
1-25- TM
X
si:e
si:tj si:tj
~M
M end
e:e
start M end
X
si
tj
1- s- X- TDI
s
</figure>
<figureCaption confidence="0.9824075">
Figure 3: Pair Hidden Markov Model [Adapted from
Mackay and Kondrak, 2005]
</figureCaption>
<figure confidence="0.9463895">
e:e
si:tj
e:tj
e:e e:e
I
e:tj
</figure>
<figureCaption confidence="0.9965865">
Figure 4: Finite State Transducer corresponding to the
pair HMM.
</figureCaption>
<figure confidence="0.9906498">
S
tj
I
TDI
e:tj
</figure>
<page confidence="0.989777">
101
</page>
<bodyText confidence="0.99980975">
HMM considering the constraints specified for
the pair HMM. In Figure 4, e is an empty symbol
while si and sj are as defined for the pair HMM in
Figure 3. Note that, in Figure 4, a start state is
needed to model pair HMM parameter con-
straints for starting in any of the three edit states.
However, it is possible to specify a WFST cor-
responding to the pair HMM with no start state.
Various WFST configurations that do not con-
form to the bias corresponding to the pair HMM
constraints had low transliteration quality and for
space limitations, are not reported in this paper.
</bodyText>
<subsectionHeader confidence="0.999">
2.3 Transformation Rules
</subsectionHeader>
<bodyText confidence="0.999979947368421">
A look into the transliterations generated using
pair HMM parameters on English-Russian de-
velopment data showed consistent mistranslitera-
tions mainly due to lack of contextual modeling
in the generated transliterations. For example in
all cases where the Russian character n ‘l’ pre-
cedes the Russian soft sign b ‘ &apos; ’, the Russian
soft sign was missing, resulting into a loss of
transliteration accuracy. Two examples of mi-
stransliterations that do not include the Russian
soft sign b are: rcpecpe.nu instead of rcpecpe.nbu
‘krefeld’, and 6m.n6ao instead of 6m.nb6ao
‘bilbao’. For such cases, simple transformation
rules, such as “ ~
” were defined on the out-
put transliterations in a post processing step. 25
transformation rules were specified for some of
the mistransliterations to test the effect of model-
ing context.
</bodyText>
<subsectionHeader confidence="0.997245">
2.4 Transliteration using PSMT system
</subsectionHeader>
<bodyText confidence="0.999964166666667">
Transliterations generated using pair HMM pa-
rameters and WFSTs are evaluated against those
generated from a state of the art Phrase-based
Statistical Machine Translation system called
Moses. Moses has been used before for machine
transliteration (Matthews, 2007) and performed
way better than a baseline system that was asso-
ciated with finding the most frequent mappings
between source and target transliteration units in
the training data. In the PBSMT system, bilin-
gual phrase-tables are used and several compo-
nents are combined in a log-linear model (trans-
lation models, reverse translation model, word
and phrase penalties, language models, distortion
parameters, etc.) with weights optimized using
minimum error rate training. For machine transli-
teration: characters are aligned instead of words,
phrases refer to character n-grams instead of
word n-grams, and language models are defined
over character sequences instead of word se-
quences. A major advantage of the PBSMT sys-
tem over the pair HMM and a WFST models is
that the phrase tables (character n-grams) cover a
lot of contextual dependencies found in the data.
</bodyText>
<sectionHeader confidence="0.997248" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999256">
3.1 Data Setup
</subsectionHeader>
<bodyText confidence="0.999883695652174">
The data used is divided according to the expe-
rimental runs that were specified for the NEWS
2009 shared transliteration task (Li et al., 2009):
a standard run and non-standard runs. The stan-
dard run involved using the transliteration system
described above that uses pair HMM parameters
combined with transformation rules. The Eng-
lish-Russian datasets used here were provided for
the NEWS 2009 shared transliteration task (Ku-
maran and Kellner, 2009): 5977 pairs of names
for training, 943 pairs for development, and 1000
for testing. For the non-standard runs, an addi-
tional English-Russian dataset extracted from the
Geonames data dump was merged with the
shared transliteration task data above to form
10481 pairs for training and development. For a
second set of experiments (Table 2), a different
set of test data (1000 pairs) extracted from the
Geonames data dump was used. For the system
used in the standard run, the training data was
preprocessed to include representation of bi-
grams associated with Cyrillic Romanization and
all English vowel bigram combinations.
</bodyText>
<subsectionHeader confidence="0.95711">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.999895909090909">
Six measures were used for evaluating system
transliteration quality. These include (Li et al.,
2009): Accuracy (ACC), Fuzziness in Top-1
(Mean F Score), Mean Reciprocal Rank (MRR),
Mean Average Precision for reference translite-
rations (MAP_R), Mean Average Precision in 10
best candidate transliterations (MAP_10), Mean
Average Precision for the system (MAP_sys).
Table 1 shows the results obtained using only the
data sets provided for the shared transliteration
task. The system used for the standard run is
“phmm_rules” described in section 2 to sub sec-
tion 2.3. “phmm_basic” is the system in which
pair HMM parameters are used for transliteration
generation but there is no representation for bi-
grams as described for the system used in the
standard run. Table 2 shows the results obtained
when additional data from Geonames data dump
was used for training and development. In Table
2, “WFST_basic” and “WFST_rules” are sys-
tems associated with training WFSTs for the
“phmm_basic” and “phmm_rules” systems
</bodyText>
<page confidence="0.991417">
102
</page>
<table confidence="0.9968242">
metrics ACC Mean F MRR
models Score
phmm_basic 0.293 0.845 0.325
Moses_PSMT 0.509 0.908 0.619
phmm_rules 0.354 0.869 0.394
metrics MAP_R MAP_10 MAP_sys
models
phmm_basic 0.293 0.099 0.099
Moses_PSMT 0.509 0.282 0.282
phmm_rules 0.354 0.134 0.134
</table>
<tableCaption confidence="0.9819855">
Table 1 Results from data sets for shared transli-
teration task.
</tableCaption>
<table confidence="0.999524">
metrics ACC Mean F MRR
models Score
phmm_basic 0.341 0.776 0.368
phmm_rules 0.515 0.821 0.571
WFST_basic 0.321 0.768 0.403
WFST_rules 0.466 0.808 0.525
Moses_PSMT 0.612 0.845 0.660
metrics MAP_R MAP_10 MAP_sys
models
phmm_basic 0.341 0.111 0.111
phmm_rules 0.515 0.174 0.174
WFST_basic 0.321 0.128 0.128
WFST_rules 0.466 0.175 0.175
Moses_PSMT 0.612 0.364 0.364
</table>
<tableCaption confidence="0.7527515">
Table 2 Results from additional Geonames data
sets.
</tableCaption>
<bodyText confidence="0.999866444444445">
respectively. Moses_PSMT is the phrase-based
statistical machine translation system. The results
in both tables show that the systems using pair
HMM parameters perform relatively better than
the systems trained on WFSTs but not better than
Moses. The low transliteration quality in the pair
HMM and WFST systems as compared to Moses
can be attributed to lack of modeling contextual
dependencies unlike the case in PBSMT.
</bodyText>
<sectionHeader confidence="0.99933" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9997590625">
A Transliteration system using pair HMM para-
meters has been presented. Although its perfor-
mance is better than that of systems based on
only WFSTs, its transliteration quality is lower
than the PBSMT system. On seeing that the pair
HMM generated consistent mistransliterations,
manual specification of a few contextual rules
resulted in improved performance. As part of
future work, we expect a technique that automat-
ically identifies the mistransliterations would
lead to improved transliteration quality. A more
general framework, in which we intend to inves-
tigate contextual issues in addition to other fac-
tors such as position in source and target strings
and edit operation memory in transliteration, is
that of Dynamic Bayesian Networks (DBNs).
</bodyText>
<sectionHeader confidence="0.996683" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.96409825">
Funds associated with this work are from a second
NPT Uganda project. I also thank Jörg Tiedemann for
helping with experimental runs for the Moses PBSMT
system.
</bodyText>
<sectionHeader confidence="0.996653" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999214853658537">
A. Kumaran and Tobias Kellner. 2007. A Generic
Framework for Machine Transliteration. Pro-
ceedings of the 30th SIGIR.
David Matthews. 2007. Machine Transliteration of
Proper Names. Master’s Thesis. School of Infor-
matics. University of Edinburgh.
Jonathan Graehl. 1997. Carmel Finite-state Toolkit.
http://www.isi.edu/licensed-sw/carmel/.
Haizhou Li, A. Kumaran, Min Zhang, Vladimir Per-
vouchine. 2009. Whitepaper of NEWS 2009 Ma-
chine Transliteration Shared Task. Proceedings of
ACL-IJCNLP 2009 Named Entities Workshop
(NEWS 2009), Singapore.
Kevin Knight and Jonathan Graehl. 1998. Machine
Transliteration. Computational Linguistics, 24 (4):
599-612, MIT Press Cambridge, MA, USA.
Leonard E. Baum, Ted Petrie, George Soules, and
Norman Weiss. 1970. A Maximization Technique
Occurring in the Statistical Analysis of Probabilis-
tic Functions of Markov Chains. The Annals of
Mathematical Statistics, 41(1):164-171.
Martijn Wieling, Therese Leinonen and John Ner-
bonne. 2007. Inducing Sound Segment Differences
using Pair Hidden Markov Models. In John Ner-
bonne, Mark Ellison and Grzegorz Kondrak (eds.)
Computing Historical Phonology: 9th Meeting of
the ACL Special Interest Group for Computational
Morphology and Phonology Workshop, pp. 48-56,
Prague.
Mehryar Mohri, Fernando C.N. Pereira, and Michael
Riley. 2008. Speech Recognition with Weighte Fi-
nite State Transducers. In Larry Rabiner and Fred
Juang, editors, Handbook on Speech Processing
and Speech Communication, Part E: Speech Rec-
ognition. Springer-Verlag, Heidelberg, Germany.
Wesley Mackay and Grzegorz Kondrak. 2005. Com-
puting Word Similarity and Identifying Cognates
with Pair Hidden Markov Models. Proceedings of
the Ninth Conference on Computational Natural
Language Learning (CoNLL 2005), pp. 40-47,
Ann-Arbor, Michigan.
</reference>
<page confidence="0.999297">
103
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.726834">
<title confidence="0.999813">Transliteration System using pair HMM with weighted FSTs</title>
<author confidence="0.998896">Peter Nabende</author>
<affiliation confidence="0.866752">Alfa Informatica, CLCG, University of Groningen, Netherlands</affiliation>
<email confidence="0.993178">p.nabende@rug.nl</email>
<abstract confidence="0.9994405625">This paper presents a transliteration system based on pair Hidden Markov Model (pair HMM) training and Weighted Finite State Transducer (WFST) techniques. Parameters used by WFSTs for transliteration generation are learned from a pair HMM. Parameters from pair-HMM training on English-Russian data sets are found to give better transliteration quality than parameters trained for WFSTs for corresponding structures. Training a pair HMM on English vowel bigrams and standard bigrams for Cyrillic Romanization, and using a few transformation rules on generated Russian transliterations to test for context improves the system’s transliteration quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Kumaran</author>
<author>Tobias Kellner</author>
</authors>
<title>A Generic Framework for Machine Transliteration.</title>
<date>2007</date>
<booktitle>Proceedings of the 30th SIGIR.</booktitle>
<marker>Kumaran, Kellner, 2007</marker>
<rawString>A. Kumaran and Tobias Kellner. 2007. A Generic Framework for Machine Transliteration. Proceedings of the 30th SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Matthews</author>
</authors>
<title>Machine Transliteration of Proper Names. Master’s Thesis.</title>
<date>2007</date>
<institution>School of Informatics. University of Edinburgh.</institution>
<contexts>
<context position="8709" citStr="Matthews, 2007" startWordPosition="1422" endWordPosition="1423">d of rcpecpe.nbu ‘krefeld’, and 6m.n6ao instead of 6m.nb6ao ‘bilbao’. For such cases, simple transformation rules, such as “ ~ ” were defined on the output transliterations in a post processing step. 25 transformation rules were specified for some of the mistransliterations to test the effect of modeling context. 2.4 Transliteration using PSMT system Transliterations generated using pair HMM parameters and WFSTs are evaluated against those generated from a state of the art Phrase-based Statistical Machine Translation system called Moses. Moses has been used before for machine transliteration (Matthews, 2007) and performed way better than a baseline system that was associated with finding the most frequent mappings between source and target transliteration units in the training data. In the PBSMT system, bilingual phrase-tables are used and several components are combined in a log-linear model (translation models, reverse translation model, word and phrase penalties, language models, distortion parameters, etc.) with weights optimized using minimum error rate training. For machine transliteration: characters are aligned instead of words, phrases refer to character n-grams instead of word n-grams, </context>
</contexts>
<marker>Matthews, 2007</marker>
<rawString>David Matthews. 2007. Machine Transliteration of Proper Names. Master’s Thesis. School of Informatics. University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Graehl</author>
</authors>
<date>1997</date>
<note>Carmel Finite-state Toolkit. http://www.isi.edu/licensed-sw/carmel/.</note>
<contexts>
<context position="2214" citStr="Graehl, 1997" startWordPosition="336" endWordPosition="337"> (S) to a target language string (T) through a sequence of edit operations. The edit operations include: (M)atching an element in S with an element in T; (I)nserting an element into T, and (D)eleting an element in S. 1 The generation task is part of the NEWS 2009 machine transliteration shared task (Li et al., 2009) Based on all representative symbols used for each of the two languages, emission costs for each of the edit operations and transition parameters can be estimated and used in measuring the similarity between two strings. To generate transliterations using pair HMM parameters, WFST (Graehl, 1997) techniques are adopted. Transliteration training is based mainly on the initial orthographic representation and no explicit phonetic scheme is used. Instead, transliteration quality is tested for different bigram combinations including all English vowel bigram combinations and n-gram combinations specified for Cyrillic Romanization by the US Board on Geographic Names and British Permanent Committee on Geographic Names (BGN/PCGN). However, transliteration parameters can still be estimated for a pair HMM when a particular phonetic representation scheme is used. The quality of transliterations g</context>
<context position="3578" citStr="Graehl, 1997" startWordPosition="533" endWordPosition="534">sed Statistical Machine Translation (PBSMT) system. Section 2 describes the components of the transliteration system that uses pair HMM parameters; section 3 gives the experimental set up and results associated with the transliterations generated; and section 4 concludes the paper. 2 Machine Transliteration System The transliteration system comprises of a training and generation components (Figure 1). In the training component, the Baum-Welch Expectation Maximization (EM) algorithm (Baum et al., 1970) is used to learn the parameters of a pair HMM. In the generation component, WFST techniques (Graehl, 1997) model the learned pair HMM parameters for generating transliterations. 2.1 Parameter Estimation for a pair-HMM A pair HMM has two output observations (Figure 2) that are aligned through the hidden states, 100 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 100–103, Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP Figure 1: Machine Transliteration system Figure 2: pair-HMM alignment for converting an English string “Peter” to a Russian string “Help” unlike the classic HMMs that have only one observation sequence. The pair HMM structure differs from that of WFSTs i</context>
</contexts>
<marker>Graehl, 1997</marker>
<rawString>Jonathan Graehl. 1997. Carmel Finite-state Toolkit. http://www.isi.edu/licensed-sw/carmel/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>A Kumaran</author>
<author>Min Zhang</author>
<author>Vladimir Pervouchine</author>
</authors>
<date>2009</date>
<booktitle>Whitepaper of NEWS 2009 Machine Transliteration Shared Task. Proceedings of ACL-IJCNLP 2009 Named Entities Workshop (NEWS</booktitle>
<contexts>
<context position="1918" citStr="Li et al., 2009" startWordPosition="288" endWordPosition="291">teration generation task1. The pair HMM has been used before (Mackay and Kondrak, 2005; Wieling et al., 2007) for string similarity estimation, and is based on the notion of string Edit Distance (ED). String ED is defined here as the total edit cost incurred in transforming a source language string (S) to a target language string (T) through a sequence of edit operations. The edit operations include: (M)atching an element in S with an element in T; (I)nserting an element into T, and (D)eleting an element in S. 1 The generation task is part of the NEWS 2009 machine transliteration shared task (Li et al., 2009) Based on all representative symbols used for each of the two languages, emission costs for each of the edit operations and transition parameters can be estimated and used in measuring the similarity between two strings. To generate transliterations using pair HMM parameters, WFST (Graehl, 1997) techniques are adopted. Transliteration training is based mainly on the initial orthographic representation and no explicit phonetic scheme is used. Instead, transliteration quality is tested for different bigram combinations including all English vowel bigram combinations and n-gram combinations speci</context>
<context position="9743" citStr="Li et al., 2009" startWordPosition="1585" endWordPosition="1588">eights optimized using minimum error rate training. For machine transliteration: characters are aligned instead of words, phrases refer to character n-grams instead of word n-grams, and language models are defined over character sequences instead of word sequences. A major advantage of the PBSMT system over the pair HMM and a WFST models is that the phrase tables (character n-grams) cover a lot of contextual dependencies found in the data. 3 Experiments 3.1 Data Setup The data used is divided according to the experimental runs that were specified for the NEWS 2009 shared transliteration task (Li et al., 2009): a standard run and non-standard runs. The standard run involved using the transliteration system described above that uses pair HMM parameters combined with transformation rules. The English-Russian datasets used here were provided for the NEWS 2009 shared transliteration task (Kumaran and Kellner, 2009): 5977 pairs of names for training, 943 pairs for development, and 1000 for testing. For the non-standard runs, an additional English-Russian dataset extracted from the Geonames data dump was merged with the shared transliteration task data above to form 10481 pairs for training and developme</context>
</contexts>
<marker>Li, Kumaran, Zhang, Pervouchine, 2009</marker>
<rawString>Haizhou Li, A. Kumaran, Min Zhang, Vladimir Pervouchine. 2009. Whitepaper of NEWS 2009 Machine Transliteration Shared Task. Proceedings of ACL-IJCNLP 2009 Named Entities Workshop (NEWS 2009), Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<date>1998</date>
<journal>Machine Transliteration. Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<pages>599--612</pages>
<publisher>MIT Press</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="6132" citStr="Knight and Graehl, 1998" startWordPosition="973" endWordPosition="976">er is also incorporated into the weight that measures the level of relationship between the input and output symbol when transformed to a WFST arc. 2.2 Generating Transliterations in WFSTs A Weighted Finite State Transducer is a finite automaton whose state transitions are labeled with input and output elements and weights that express the level of relationship between the input and output elements. Although the framework of WFSTs has mostly been applied in representing various models for speech recognition (Mohri et al., 2008) including HMMs, WFSTs have as well been used for transliteration (Knight and Graehl, 1998), and are the most suitable for modeling pair HMM constraints for generating transliterations. In the WFST framework, it is possible to specify various configurations associated with constraints inherent in a particular model. Figure 4 shows a WFST that precisely corresponds to the structure of the pair Pairs of correct transliterations Transliteration parameter estimation for pair HMM Transliteration generation using Weighted Finite State Transducers Target name Source name Estimated parameters P : 11 M e : e M M D M End t : T e : _ r : p si:e s 1- s- X- TDI D D TDI e:e e:e 8 si:e 1-25- TM X </context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>Kevin Knight and Jonathan Graehl. 1998. Machine Transliteration. Computational Linguistics, 24 (4): 599-612, MIT Press Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonard E Baum</author>
<author>Ted Petrie</author>
<author>George Soules</author>
<author>Norman Weiss</author>
</authors>
<title>A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains. The Annals of Mathematical Statistics,</title>
<date>1970</date>
<pages>41--1</pages>
<contexts>
<context position="3471" citStr="Baum et al., 1970" startWordPosition="513" endWordPosition="516">valuated against transliterations generated from training WFSTs and transliterations generated using a Phrase-based Statistical Machine Translation (PBSMT) system. Section 2 describes the components of the transliteration system that uses pair HMM parameters; section 3 gives the experimental set up and results associated with the transliterations generated; and section 4 concludes the paper. 2 Machine Transliteration System The transliteration system comprises of a training and generation components (Figure 1). In the training component, the Baum-Welch Expectation Maximization (EM) algorithm (Baum et al., 1970) is used to learn the parameters of a pair HMM. In the generation component, WFST techniques (Graehl, 1997) model the learned pair HMM parameters for generating transliterations. 2.1 Parameter Estimation for a pair-HMM A pair HMM has two output observations (Figure 2) that are aligned through the hidden states, 100 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 100–103, Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP Figure 1: Machine Transliteration system Figure 2: pair-HMM alignment for converting an English string “Peter” to a Russian string “Help” unlike th</context>
</contexts>
<marker>Baum, Petrie, Soules, Weiss, 1970</marker>
<rawString>Leonard E. Baum, Ted Petrie, George Soules, and Norman Weiss. 1970. A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains. The Annals of Mathematical Statistics, 41(1):164-171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martijn Wieling</author>
<author>Therese Leinonen</author>
<author>John Nerbonne</author>
</authors>
<title>Inducing Sound Segment Differences using Pair Hidden Markov Models.</title>
<date>2007</date>
<booktitle>In John Nerbonne, Mark Ellison and Grzegorz Kondrak (eds.) Computing Historical Phonology: 9th Meeting of the ACL Special Interest Group for Computational Morphology and Phonology Workshop,</booktitle>
<pages>48--56</pages>
<location>Prague.</location>
<contexts>
<context position="1411" citStr="Wieling et al., 2007" startWordPosition="199" endWordPosition="202">iteration quality. 1 Introduction Machine transliteration is the automatic transformation of a word in a source language to a phonetically equivalent word in a target language that uses a different writing system. Transliteration is important for various Natural Language Processing (NLP) applications including: Cross Lingual Information Retrieval (CLIR), and Machine Translation (MT). This paper introduces a system that utilizes parameters learned for a pair Hidden Markov Model (pair HMM) in a shared transliteration generation task1. The pair HMM has been used before (Mackay and Kondrak, 2005; Wieling et al., 2007) for string similarity estimation, and is based on the notion of string Edit Distance (ED). String ED is defined here as the total edit cost incurred in transforming a source language string (S) to a target language string (T) through a sequence of edit operations. The edit operations include: (M)atching an element in S with an element in T; (I)nserting an element into T, and (D)eleting an element in S. 1 The generation task is part of the NEWS 2009 machine transliteration shared task (Li et al., 2009) Based on all representative symbols used for each of the two languages, emission costs for e</context>
</contexts>
<marker>Wieling, Leinonen, Nerbonne, 2007</marker>
<rawString>Martijn Wieling, Therese Leinonen and John Nerbonne. 2007. Inducing Sound Segment Differences using Pair Hidden Markov Models. In John Nerbonne, Mark Ellison and Grzegorz Kondrak (eds.) Computing Historical Phonology: 9th Meeting of the ACL Special Interest Group for Computational Morphology and Phonology Workshop, pp. 48-56, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando C N Pereira</author>
<author>Michael Riley</author>
</authors>
<title>Speech Recognition with Weighte Finite State Transducers.</title>
<date>2008</date>
<booktitle>Handbook on Speech Processing and Speech Communication, Part E: Speech Recognition.</booktitle>
<editor>In Larry Rabiner and Fred Juang, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Heidelberg, Germany.</location>
<contexts>
<context position="6041" citStr="Mohri et al., 2008" startWordPosition="959" endWordPosition="962">he main difference is in the structure; for the pair-HMM, the state transition parameter is also incorporated into the weight that measures the level of relationship between the input and output symbol when transformed to a WFST arc. 2.2 Generating Transliterations in WFSTs A Weighted Finite State Transducer is a finite automaton whose state transitions are labeled with input and output elements and weights that express the level of relationship between the input and output elements. Although the framework of WFSTs has mostly been applied in representing various models for speech recognition (Mohri et al., 2008) including HMMs, WFSTs have as well been used for transliteration (Knight and Graehl, 1998), and are the most suitable for modeling pair HMM constraints for generating transliterations. In the WFST framework, it is possible to specify various configurations associated with constraints inherent in a particular model. Figure 4 shows a WFST that precisely corresponds to the structure of the pair Pairs of correct transliterations Transliteration parameter estimation for pair HMM Transliteration generation using Weighted Finite State Transducers Target name Source name Estimated parameters P : 11 M</context>
</contexts>
<marker>Mohri, Pereira, Riley, 2008</marker>
<rawString>Mehryar Mohri, Fernando C.N. Pereira, and Michael Riley. 2008. Speech Recognition with Weighte Finite State Transducers. In Larry Rabiner and Fred Juang, editors, Handbook on Speech Processing and Speech Communication, Part E: Speech Recognition. Springer-Verlag, Heidelberg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wesley Mackay</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Computing Word Similarity and Identifying Cognates with Pair Hidden Markov Models.</title>
<date>2005</date>
<booktitle>Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL</booktitle>
<pages>40--47</pages>
<location>Ann-Arbor, Michigan.</location>
<contexts>
<context position="1388" citStr="Mackay and Kondrak, 2005" startWordPosition="195" endWordPosition="198">proves the system’s transliteration quality. 1 Introduction Machine transliteration is the automatic transformation of a word in a source language to a phonetically equivalent word in a target language that uses a different writing system. Transliteration is important for various Natural Language Processing (NLP) applications including: Cross Lingual Information Retrieval (CLIR), and Machine Translation (MT). This paper introduces a system that utilizes parameters learned for a pair Hidden Markov Model (pair HMM) in a shared transliteration generation task1. The pair HMM has been used before (Mackay and Kondrak, 2005; Wieling et al., 2007) for string similarity estimation, and is based on the notion of string Edit Distance (ED). String ED is defined here as the total edit cost incurred in transforming a source language string (S) to a target language string (T) through a sequence of edit operations. The edit operations include: (M)atching an element in S with an element in T; (I)nserting an element into T, and (D)eleting an element in S. 1 The generation task is part of the NEWS 2009 machine transliteration shared task (Li et al., 2009) Based on all representative symbols used for each of the two language</context>
<context position="6870" citStr="Mackay and Kondrak, 2005" startWordPosition="1115" endWordPosition="1118">rk, it is possible to specify various configurations associated with constraints inherent in a particular model. Figure 4 shows a WFST that precisely corresponds to the structure of the pair Pairs of correct transliterations Transliteration parameter estimation for pair HMM Transliteration generation using Weighted Finite State Transducers Target name Source name Estimated parameters P : 11 M e : e M M D M End t : T e : _ r : p si:e s 1- s- X- TDI D D TDI e:e e:e 8 si:e 1-25- TM X si:e si:tj si:tj ~M M end e:e start M end X si tj 1- s- X- TDI s Figure 3: Pair Hidden Markov Model [Adapted from Mackay and Kondrak, 2005] e:e si:tj e:tj e:e e:e I e:tj Figure 4: Finite State Transducer corresponding to the pair HMM. S tj I TDI e:tj 101 HMM considering the constraints specified for the pair HMM. In Figure 4, e is an empty symbol while si and sj are as defined for the pair HMM in Figure 3. Note that, in Figure 4, a start state is needed to model pair HMM parameter constraints for starting in any of the three edit states. However, it is possible to specify a WFST corresponding to the pair HMM with no start state. Various WFST configurations that do not conform to the bias corresponding to the pair HMM constraints</context>
</contexts>
<marker>Mackay, Kondrak, 2005</marker>
<rawString>Wesley Mackay and Grzegorz Kondrak. 2005. Computing Word Similarity and Identifying Cognates with Pair Hidden Markov Models. Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL 2005), pp. 40-47, Ann-Arbor, Michigan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>