<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.027453">
<title confidence="0.980908">
Analysis of Listening-oriented Dialogue for Building Listening Agents
</title>
<author confidence="0.960428">
Toyomi Meguro, Ryuichiro Higashinaka, Kohji Dohsaka, Yasuhiro Minami, Hideki Isozaki
</author>
<affiliation confidence="0.887544">
NTT Communication Science Laboratories, NTT Corporation
</affiliation>
<address confidence="0.921372">
2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237, Japan
</address>
<email confidence="0.998438">
{meguro,rh,dohsaka,minami,isozaki}@cslab.kecl.ntt.co.jp
</email>
<sectionHeader confidence="0.997382" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999978368421053">
Our aim is to build listening agents that
can attentively listen to the user and sat-
isfy his/her desire to speak and have him-
self/herself heard. This paper investigates
the characteristics of such listening-oriented
dialogues so that such a listening process
can be achieved by automated dialogue sys-
tems. We collected both listening-oriented
dialogues and casual conversation, and ana-
lyzed them by comparing the frequency of
dialogue acts, as well as the dialogue flows
using Hidden Markov Models (HMMs). The
analysis revealed that listening-oriented dia-
logues and casual conversation have charac-
teristically different dialogue flows and that
it is important for listening agents to self-
disclose before asking questions and to utter
more questions and acknowledgment than in
casual conversation to be good listeners.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.974841772727273">
Although task-oriented dialogue systems have been
actively researched over the years (Walker et al.,
2001), systems that perform more flexible (less task-
oriented) dialogues such as chats are beginning to be
actively investigated from their social and entertain-
ment aspects (Bickmore and Cassell, 2001; Higuchi
et al., 2008).
This paper deals with dialogues in which one con-
versational participant attentively listens to the other
(hereafter, listening-oriented dialogue). Our aim is
to build listening agents that can implement such a
listening process so that a user can satisfy his/her
desire to speak and have him/herself heard. Such
agents would lead the user’s state of mind for the
better as in a therapy session, although we want our
listening agents to help users mentally in everyday
conversation. It should also be noted that the pur-
pose of the listening-oriented dialogue is to simply
listen to users, not to elicit information as in inter-
views.
L: The topic is “travel”, so did you (QUESTION)
travel during summer vacation?
</bodyText>
<figure confidence="0.986825352941176">
S: I like traveling. (SELF-DISCLOSURE)
L: Oh! I see! (SYMPATHY)
Why do you like to travel? (QUESTION)
S: This summer, I just went back (SELF-DISCLOSURE)
to my hometown.
I was busy at work, but I’m (SELF-DISCLOSURE)
planning to go to Kawaguchi
Lake this weekend.
I like traveling because it is (SELF-DISCLOSURE)
stimulating.
L: Going to unusual places (SYMPATHY)
changes one’s perspective,
doesn’t it?
You said you’re going to go to (QUESTION)
Kawaguchi Lake this weekend.
Is this travel?
Will you go by car or train? (QUESTION)
</figure>
<figureCaption confidence="0.909927">
Figure 1: Excerpt of a typical listening-oriented di-
alogue. Dialogue acts corresponding to utterances
</figureCaption>
<bodyText confidence="0.982602421052632">
are shown in parentheses (See Section 3.1 for their
meanings). The dialogue was originally in Japanese
and was translated by the authors.
There has been little research on listening agents.
One exception is (Maatman et al., 2005), which
showed that systems can make the user have the
sense of being heard by using gestures, such as nod-
ding and shaking of the head. Although our work is
similar to theirs, the difference is that we focus more
on verbal communication instead of non-verbal one.
For the purpose of gaining insight into how to
build our listening agents, we collected listening-
oriented dialogues as well as casual conversation,
and compared them in order to reveal the charac-
teristics of the listening-oriented dialogue. Figure 1
shows an example of a typical listening-oriented di-
alogue. In the figure, the conversational participants
talk about travel with the listener (L), repeatedly ask-
ing the speaker (S) to make self-disclosure.
</bodyText>
<sectionHeader confidence="0.991511" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.9998728">
We analyze the characteristics of listening-oriented
dialogues by comparing them with casual conversa-
tion. Here, casual conversation means a dialogue
where conversational participants have no prede-
fined roles (i.e., listeners and speakers). In this
</bodyText>
<subsubsectionHeader confidence="0.642402">
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 124–127,
</subsubsectionHeader>
<affiliation confidence="0.938291">
Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics
</affiliation>
<page confidence="0.999628">
124
</page>
<bodyText confidence="0.999927269230769">
study, we collect dialogues in texts because we want
to avoid the particular problems of voice, such as
filled pauses and interruptions, although we plan to
deal with speech input in the future.
As a procedure, we first collect listening-oriented
dialogues and casual conversation using human sub-
jects. Then, we label the collected dialogues with
dialogue act tags (see Section 3.1 for details of the
tags) to facilitate the analysis of the data. In the anal-
ysis, we examine the frequency of the tags in each
type of dialogue. We also look into the difference of
dialogue flows by modeling each type of dialogue by
Hidden Markov Models (HMMs) and comparing the
obtained models. We employ HMMs because they
are useful for modeling sequential data especially
when the number of states is unknown. We check
whether the HMMs for the listening-oriented dia-
logue and casual conversation can be successfully
distinguished from each other to see if the listen-
ing process can be successfully modeled. We also
analyze the transitions between states in the created
HMMs to examine the dialogue flows. We note that
HMMs have been used to model task-oriented dia-
logues (Shirai, 1996) and casual conversation (Iso-
mura et al., 2006). In this study, we use HMMs to
model and analyze listening-oriented dialogues.
</bodyText>
<sectionHeader confidence="0.983648" genericHeader="method">
3 Data collection
</sectionHeader>
<bodyText confidence="0.999927304347826">
We recruited 16 participants. Eight participated as
listeners and the other eight as speakers. The male-
to-female ratio was even. The participants were 21
to 29 years old. Each participant engaged in four di-
alogues: two casual conversations followed by two
listening-oriented dialogues with a fixed role of lis-
tener/speaker. In listening-oriented dialogue, the lis-
teners were instructed to make it easy for the speak-
ers to say what they wanted to say. When col-
lecting the casual conversation, listeners were not
aware that they would be listeners afterwards. Lis-
teners had never met nor talked to the speakers prior
to the data collection. The listeners and speakers
talked over Microsoft Live MessengerTMin different
rooms; therefore, they could not see each other.
In each conversation, participants chatted for 30
minutes about their favorite topic that they selected
from the topic list we prepared. The topics were
food, travel, movies, music, entertainers, sports,
health, housework and childcare, personal comput-
ers and the Internet, animals, fashion and games. Ta-
ble 1 shows the number of collected dialogues, utter-
ances and words in each utterance of listeners and
</bodyText>
<table confidence="0.997897">
Listening Casual
# dialogues 16 16
# utterances 850 720
# words Listener 20.60 17.92
per utt.
Speaker 26.46 21.44
</table>
<tableCaption confidence="0.999885">
Table 1: Statistics of collected dialogues.
</tableCaption>
<bodyText confidence="0.999942842105263">
speakers. Generally, utterances in listening-oriented
dialogue were longer than those in casual conversa-
tion, probably because the subjects explained them-
selves in detail to make themselves better under-
stood.
At the end of each dialogue, the participants
filled out questionnaires that asked for their sat-
isfaction levels of dialogue, as well as how well
they could talk about themselves to their conver-
sational partners on the 10-point Likert scale. The
analysis of the questionnaire results showed that, in
listening-oriented dialogue, speakers were having a
better sense of making themselves heard than in ca-
sual conversation (Welch’s pairwise t-test; p=0.016)
without any degradation in the satisfaction level of
dialogue. This indicates that the subjects were suc-
cessfully performing attentive listening and that it is
meaningful to investigate the characteristics of the
collected listening-oriented dialogues.
</bodyText>
<subsectionHeader confidence="0.998868">
3.1 Dialogue act
</subsectionHeader>
<bodyText confidence="0.999949">
We labeled the collected dialogues using the dia-
logue act tag set: (1) SELF-DISCLOSURE (disclo-
sure of one’s preferences and feelings), (2) INFOR-
MATION (delivery of objective information), (3) AC-
KNOWLEDGMENT (encourages the conversational
partner to speak), (4) QUESTION (utterances that ex-
pect answers), (5) SYMPATHY (sympathetic utter-
ances and praises) and, (6) GREETING (social cues
to begin/end a dialogue).
We selected these tags from the DAMSL tag set
(Jurafsky et al., 1997) that deals with general con-
versation and also from those used to label therapy
conversation (Ivey and Ivey, 2002). Since our work
is still preliminary, we selected only a small num-
ber of labels that we thought were important for
modeling utterances in our collected dialogues, al-
though we plan to incorporate other tags in the fu-
ture. We expected that self-disclosure would occur
quite often in our data because the subjects were to
talk about their favorite topics and the participants
would be willing to communicate about their expe-
riences and feelings. We also expected that the lis-
teners would sympathize often to make others talk
with ease. Note that sympathy has been found useful
to increase closeness between conversational partic-
</bodyText>
<page confidence="0.983768">
125
</page>
<table confidence="0.999816">
Listener Speaker
Casual Listening Casual Listening
DISC 66.6% 44.5% 53.3% 57.3%
INFO 6.5% 1.4% 5.6% 5.2%
ACK 8.0% 12.3% 6.6% 6.9%
QUES 4.1% 25.8% 21.3% 14.0%
SYM 2.6% 3.7% 3.2% 3.3%
GR 10.9% 9.8% 7.2% 9.6%
OTHER 1.3% 2.5% 2.9% 3.7%
</table>
<tableCaption confidence="0.850528">
Table 2: Rates of dialogue act tags.
</tableCaption>
<table confidence="0.999627333333333">
DISC INFO ACK QUES SYM GR
Increase 0 0 8 8 5 4
Decrease 8 8 0 0 3 4
</table>
<tableCaption confidence="0.981136">
Table 3: Number of listeners whose tags in-
creased/decreased in listening-oriented dialogue.
</tableCaption>
<bodyText confidence="0.983695222222222">
ipants (Reis and Shaver, 1998).
A single annotator, who is not one of the authors,
labeled each utterance using the seven tags (six di-
alogue act tags plus OTHER). As a result, 1,177
tags were labeled to the utterances in the listening-
oriented dialogues and 1,312 tags to those in casual
conversation. The numbers of tags and utterances do
not match because, in text dialogue, an utterance can
be long and may be annotated with several tags.
</bodyText>
<sectionHeader confidence="0.99918" genericHeader="method">
4 Analysis
</sectionHeader>
<subsectionHeader confidence="0.999528">
4.1 Comparing the frequency of dialogue acts
</subsectionHeader>
<bodyText confidence="0.997826">
We compared the frequency of the dialogue act tags
in listening-oriented dialogues and casual conversa-
tion. Table 2 shows the rates of the tags in each type
of dialogue. In the table, OTHER means the expres-
sions that did not fall into any of our six dialogue
acts, such as facial expressions and mistypes. Table
3 shows the number of listeners whose rates of tags
increased or decreased from casual conversation to
listening-oriented dialogue.
Compared to casual conversation, the rates of
SELF-DISCLOSURE and INFORMATION decreased
in the listening-oriented dialogue. On the other
hand, the rates of ACKNOWLEDGMENT and QUES-
TION increased. This means that the listeners tended
to hold the transmission of information and focused
on letting speakers self-disclose or deliver informa-
tion. It can also be seen that the speakers decreased
QUESTION to increase self-disclosure.
</bodyText>
<subsectionHeader confidence="0.997953">
4.2 Modeling dialogue act sequences by HMM
</subsectionHeader>
<bodyText confidence="0.9991416">
We analyzed the flow of listening-oriented dialogue
and casual conversation by modeling their dialogue
act sequences using HMMs. We defined 14 obser-
vation symbols, corresponding to the seven tags for
a listener and the same number of tags for a speaker.
</bodyText>
<figureCaption confidence="0.973221">
Figure 2: Ergodic HMM for listening-oriented dia-
logue. Circled numbers represent state IDs.
</figureCaption>
<bodyText confidence="0.988993352941177">
We trained the following two types of HMMs for
each type of dialogue.
Ergodic HMM: Each state emits all 14 observation
symbols. All states are connected to each other.
Speaker HMM: Half the states in this HMM only
emit one speaker’s dialogue acts and the other
half emit other speaker’s dialogue acts. All
states are connected to each other.
The EM algorithm was used to train the HMMs.
To find the best fitting HMM with minimal states,
we trained 1,000 HMMs for each type of HMM by
increasing the number of states from one to ten and
training 100 HMMs for each number of states. This
was necessary because the HMMs severely depend
on the initial probabilities. From the 1,000 HMMs,
we chose the most fitting model using the MDL
(Minimum Description Length) criterion.
</bodyText>
<subsectionHeader confidence="0.801464">
4.2.1 Distinguishing Dialogue Types
</subsectionHeader>
<bodyText confidence="0.999983571428572">
We performed an experiment to examine whether
the trained HMMs can distinguish listening-oriented
dialogues and casual conversation. For this exper-
iment, we used eight listening-oriented dialogues
and eight casual conversations to train HMMs and
made them classify the remaining 16 dialogues. We
found that Ergodic HMM can distinguish the dia-
logues with an accuracy of 87.5%, and the Speaker
HMM achieved 100% accuracy. This indicates that
we can successfully train HMMs for each type of
dialogue and that investigating the trained HMMs
would show the characteristics of each type of di-
alogue. In the following sections, we analyze the
HMMs trained using all 16 dialogues of each type.
</bodyText>
<subsectionHeader confidence="0.820389">
4.2.2 Analysis of Ergodic HMM
</subsectionHeader>
<bodyText confidence="0.4730705">
Figure 2 shows the Ergodic HMM for listening-
oriented dialogue. It can be seen that the major flow
</bodyText>
<figure confidence="0.9980009375">
0.38
0.41
0.58
0.13
T1 L:Greeting:0.483
S:Greeting:0.39
0.55
0.1
L:Self-disclosure:0.107 0.83
② L:Question:0.456 ③
S:Ack:0.224
S:Self-disclosure:0.828
0.51
L:Self-disclosure:0.579
④
L:Ack:0.132
</figure>
<page confidence="0.887088">
126
</page>
<figureCaption confidence="0.999554">
Figure 4: Speaker HMM for casual conversation.
</figureCaption>
<bodyText confidence="0.9998915">
of dialogue acts are: Q2 L’s question -* 3) S’s self-
disclosure -* ®L’s self-disclosure -* Q L’s ques-
tion. This flow indicates that listeners tend to self-
disclose before the next question, showing the cycle
of reciprocal self-disclosure. This indicates that lis-
tening agents would need to have the capability of
self-disclosure in order to become human-like lis-
teners.
</bodyText>
<subsectionHeader confidence="0.997384">
4.2.3 Analysis of Speaker HMM
</subsectionHeader>
<bodyText confidence="0.999981">
Figures 3 and 4 show the Speaker HMMs for
listening-oriented dialogue and casual conversation,
respectively. Here, L and S correspond to S1 and
S2. It can be clearly seen that the two HMMs
have very similar structures. From the probabili-
ties, states with the same IDs seem to correspond to
each other. When we compare state IDs 3 and 5, it
can be seen that, when speakers take the role of lis-
teners, they reduce self-disclosure while increasing
questions and acknowledgment. Questions seem to
have more importance in listening-oriented dialogue
than in casual conversation, indicating that listening
agents need to have a good capability of generating
questions. The agents would also need to explicitly
increase acknowledgment in their utterances. Note
that, compared to spoken dialogue, acknowledgment
has to be performed consciously in text-based dia-
logue. When we compare state ID 4, we see that
the speaker starts questioning in casual conversation,
whereas the speaker only self-discloses in listening-
oriented dialogue. This shows that, in our data, the
speakers are successfully concentrating on making
self-disclosure in listening-oriented dialogue.
</bodyText>
<sectionHeader confidence="0.993926" genericHeader="conclusions">
5 Conclusion and Future work
</sectionHeader>
<bodyText confidence="0.999896714285714">
We collected listening-oriented dialogue and ca-
sual conversation, and compared them to find the
characteristics of listening-oriented dialogues that
are useful for building automated listening agents.
Our analysis found that it is important for listen-
ing agents to self-disclose before asking questions
and that it is necessary to utter more questions and
acknowledgment than in casual conversation to be
good listeners. As future work, we plan to use a
more elaborate tag set to further analyze the dia-
logue flows. We also plan to extend the HMMs
to Partially Observable Markov Decision Processes
(POMDPs) (Williams and Young, 2007) to achieve
dialogue management of listening agents from data.
</bodyText>
<sectionHeader confidence="0.998519" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99421403030303">
Timothy Bickmore and Justine Cassell. 2001. Relational
agents: A model and implementation of building user trust.
In Proc. ACM CHI, pages 396–403.
Shinsuke Higuchi, Rafal Rzepka, and Kenji Araki. 2008. A
casual conversation system using modality and word associ-
ations retrieved from the web”. In EMNLP, pages 382–390.
Naoki Isomura, Fujio Toriumi, and Kenichiro Ishii. 2006.
Evaluation method of non-task-oriented dialogue system by
HMM. In Proc. the 4th Symposium on Intelligent Media In-
tegration for Social Information Infrastructure, pages 149–
152.
Allen E. Ivey and Mary Bradford Ivey. 2002. Intentional Inter-
viewing and Counseling: Facilitating Client Development in
a Multicultural Society. Brooks/Cole Publishing Company.
Dan Jurafsky, Liz Shriberg, and Debra Biasca, 1997. Switch-
board SWBD-DAMSL Shallow-Discourse-Function Annota-
tion Coders Manual.
Martijn Maatman, Jonathan Gratch, and Stacy Marsella. 2005.
Natural behavior of a listening agent. Lecture Notes in Com-
puter Science, 3661:25–36.
Harry T. Reis and Phillip Shaver. 1998. Intimacy as an inter-
personal process. In S. Duck, editor, Handbook of personal
relationships, pages 367–398. John Wiley &amp; Sons Ltd.
Katsuhiko Shirai. 1996. Modeling of spoken dialogue with and
without visual information. In Proc. ICSLP, volume 1, pages
188–191.
Marilyn A. Walker, Rebecca Passonneau, and Julie E. Boland.
2001. Quantitative and qualitative evaluation of darpa com-
municator spoken dialogue systems. In Proc. ACL, pages
515–522.
Jason D. Williams and Steve Young. 2007. Partially observ-
able Markov decision processes for spoken dialog systems.
Computer Speech and Language, 21(2):393–422.
</reference>
<figure confidence="0.999158095238095">
0.42 0.37
0.43
(1) L:Greeting:0.888 �
S:Greeting:0.98
0.47
0.11
0.51
L:Self-disclosure:0.445 0.56
O
® S:Self-disclosure:0.835
L:Question:0.492
0.18
0.38
0.63
0.92
L:Self-disclosure:0.556
© �
L:Ack:0.27
S:Self-disclosure:0.125
S:Ack:0.661
0.25
</figure>
<figureCaption confidence="0.95618">
Figure 3: Speaker HMM for listening-oriented dia-
logue.
</figureCaption>
<figure confidence="0.99976247826087">
0.1
③ ④
S1:Question:0.26
S1:Self-disclosure:0.662
⑤ ⑥
S1:Ack:0.135 0.12
S1:Self-disclosure:0.644
0.51
0.32 0.42
①S1:Greeting:0.848 ②
0.16
0.45 0.45 0.35
0.74
0.45
0.76
0.15
S2:Greeting:0.775
S2:Self-disclosure:0.523
S2:Question:0.414
S2:Self-disclosure:0.629
S2:Ack:0.12
0.11
0.43
</figure>
<page confidence="0.883359">
127
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.883097">
<title confidence="0.999357">Analysis of Listening-oriented Dialogue for Building Listening Agents</title>
<author confidence="0.993929">Toyomi Meguro</author>
<author confidence="0.993929">Ryuichiro Higashinaka</author>
<author confidence="0.993929">Kohji Dohsaka</author>
<author confidence="0.993929">Yasuhiro Minami</author>
<author confidence="0.993929">Hideki</author>
<affiliation confidence="0.986033">NTT Communication Science Laboratories, NTT</affiliation>
<address confidence="0.919814">2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237,</address>
<abstract confidence="0.99849895">Our aim is to build listening agents that can attentively listen to the user and satisfy his/her desire to speak and have himself/herself heard. This paper investigates the characteristics of such listening-oriented dialogues so that such a listening process can be achieved by automated dialogue systems. We collected both listening-oriented dialogues and casual conversation, and analyzed them by comparing the frequency of dialogue acts, as well as the dialogue flows using Hidden Markov Models (HMMs). The analysis revealed that listening-oriented dialogues and casual conversation have characteristically different dialogue flows and that it is important for listening agents to selfdisclose before asking questions and to utter more questions and acknowledgment than in casual conversation to be good listeners.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Timothy Bickmore</author>
<author>Justine Cassell</author>
</authors>
<title>Relational agents: A model and implementation of building user trust.</title>
<date>2001</date>
<booktitle>In Proc. ACM CHI,</booktitle>
<pages>396--403</pages>
<contexts>
<context position="1470" citStr="Bickmore and Cassell, 2001" startWordPosition="199" endWordPosition="202">he analysis revealed that listening-oriented dialogues and casual conversation have characteristically different dialogue flows and that it is important for listening agents to selfdisclose before asking questions and to utter more questions and acknowledgment than in casual conversation to be good listeners. 1 Introduction Although task-oriented dialogue systems have been actively researched over the years (Walker et al., 2001), systems that perform more flexible (less taskoriented) dialogues such as chats are beginning to be actively investigated from their social and entertainment aspects (Bickmore and Cassell, 2001; Higuchi et al., 2008). This paper deals with dialogues in which one conversational participant attentively listens to the other (hereafter, listening-oriented dialogue). Our aim is to build listening agents that can implement such a listening process so that a user can satisfy his/her desire to speak and have him/herself heard. Such agents would lead the user’s state of mind for the better as in a therapy session, although we want our listening agents to help users mentally in everyday conversation. It should also be noted that the purpose of the listening-oriented dialogue is to simply list</context>
</contexts>
<marker>Bickmore, Cassell, 2001</marker>
<rawString>Timothy Bickmore and Justine Cassell. 2001. Relational agents: A model and implementation of building user trust. In Proc. ACM CHI, pages 396–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shinsuke Higuchi</author>
<author>Rafal Rzepka</author>
<author>Kenji Araki</author>
</authors>
<title>A casual conversation system using modality and word associations retrieved from the web”. In</title>
<date>2008</date>
<booktitle>EMNLP,</booktitle>
<pages>382--390</pages>
<contexts>
<context position="1493" citStr="Higuchi et al., 2008" startWordPosition="203" endWordPosition="206">stening-oriented dialogues and casual conversation have characteristically different dialogue flows and that it is important for listening agents to selfdisclose before asking questions and to utter more questions and acknowledgment than in casual conversation to be good listeners. 1 Introduction Although task-oriented dialogue systems have been actively researched over the years (Walker et al., 2001), systems that perform more flexible (less taskoriented) dialogues such as chats are beginning to be actively investigated from their social and entertainment aspects (Bickmore and Cassell, 2001; Higuchi et al., 2008). This paper deals with dialogues in which one conversational participant attentively listens to the other (hereafter, listening-oriented dialogue). Our aim is to build listening agents that can implement such a listening process so that a user can satisfy his/her desire to speak and have him/herself heard. Such agents would lead the user’s state of mind for the better as in a therapy session, although we want our listening agents to help users mentally in everyday conversation. It should also be noted that the purpose of the listening-oriented dialogue is to simply listen to users, not to eli</context>
</contexts>
<marker>Higuchi, Rzepka, Araki, 2008</marker>
<rawString>Shinsuke Higuchi, Rafal Rzepka, and Kenji Araki. 2008. A casual conversation system using modality and word associations retrieved from the web”. In EMNLP, pages 382–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naoki Isomura</author>
<author>Fujio Toriumi</author>
<author>Kenichiro Ishii</author>
</authors>
<title>Evaluation method of non-task-oriented dialogue system by HMM.</title>
<date>2006</date>
<booktitle>In Proc. the 4th Symposium on Intelligent Media Integration for Social Information Infrastructure,</booktitle>
<pages>149--152</pages>
<contexts>
<context position="5489" citStr="Isomura et al., 2006" startWordPosition="843" endWordPosition="847">ype of dialogue by Hidden Markov Models (HMMs) and comparing the obtained models. We employ HMMs because they are useful for modeling sequential data especially when the number of states is unknown. We check whether the HMMs for the listening-oriented dialogue and casual conversation can be successfully distinguished from each other to see if the listening process can be successfully modeled. We also analyze the transitions between states in the created HMMs to examine the dialogue flows. We note that HMMs have been used to model task-oriented dialogues (Shirai, 1996) and casual conversation (Isomura et al., 2006). In this study, we use HMMs to model and analyze listening-oriented dialogues. 3 Data collection We recruited 16 participants. Eight participated as listeners and the other eight as speakers. The maleto-female ratio was even. The participants were 21 to 29 years old. Each participant engaged in four dialogues: two casual conversations followed by two listening-oriented dialogues with a fixed role of listener/speaker. In listening-oriented dialogue, the listeners were instructed to make it easy for the speakers to say what they wanted to say. When collecting the casual conversation, listeners </context>
</contexts>
<marker>Isomura, Toriumi, Ishii, 2006</marker>
<rawString>Naoki Isomura, Fujio Toriumi, and Kenichiro Ishii. 2006. Evaluation method of non-task-oriented dialogue system by HMM. In Proc. the 4th Symposium on Intelligent Media Integration for Social Information Infrastructure, pages 149– 152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Allen E Ivey</author>
<author>Mary Bradford Ivey</author>
</authors>
<title>Intentional Interviewing and Counseling: Facilitating Client Development in a Multicultural Society.</title>
<date>2002</date>
<publisher>Brooks/Cole Publishing Company.</publisher>
<contexts>
<context position="8446" citStr="Ivey and Ivey, 2002" startWordPosition="1298" endWordPosition="1301">alogues. 3.1 Dialogue act We labeled the collected dialogues using the dialogue act tag set: (1) SELF-DISCLOSURE (disclosure of one’s preferences and feelings), (2) INFORMATION (delivery of objective information), (3) ACKNOWLEDGMENT (encourages the conversational partner to speak), (4) QUESTION (utterances that expect answers), (5) SYMPATHY (sympathetic utterances and praises) and, (6) GREETING (social cues to begin/end a dialogue). We selected these tags from the DAMSL tag set (Jurafsky et al., 1997) that deals with general conversation and also from those used to label therapy conversation (Ivey and Ivey, 2002). Since our work is still preliminary, we selected only a small number of labels that we thought were important for modeling utterances in our collected dialogues, although we plan to incorporate other tags in the future. We expected that self-disclosure would occur quite often in our data because the subjects were to talk about their favorite topics and the participants would be willing to communicate about their experiences and feelings. We also expected that the listeners would sympathize often to make others talk with ease. Note that sympathy has been found useful to increase closeness bet</context>
</contexts>
<marker>Ivey, Ivey, 2002</marker>
<rawString>Allen E. Ivey and Mary Bradford Ivey. 2002. Intentional Interviewing and Counseling: Facilitating Client Development in a Multicultural Society. Brooks/Cole Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Jurafsky</author>
<author>Liz Shriberg</author>
<author>Debra Biasca</author>
</authors>
<title>Switchboard SWBD-DAMSL Shallow-Discourse-Function Annotation Coders Manual.</title>
<date>1997</date>
<contexts>
<context position="8332" citStr="Jurafsky et al., 1997" startWordPosition="1279" endWordPosition="1282">entive listening and that it is meaningful to investigate the characteristics of the collected listening-oriented dialogues. 3.1 Dialogue act We labeled the collected dialogues using the dialogue act tag set: (1) SELF-DISCLOSURE (disclosure of one’s preferences and feelings), (2) INFORMATION (delivery of objective information), (3) ACKNOWLEDGMENT (encourages the conversational partner to speak), (4) QUESTION (utterances that expect answers), (5) SYMPATHY (sympathetic utterances and praises) and, (6) GREETING (social cues to begin/end a dialogue). We selected these tags from the DAMSL tag set (Jurafsky et al., 1997) that deals with general conversation and also from those used to label therapy conversation (Ivey and Ivey, 2002). Since our work is still preliminary, we selected only a small number of labels that we thought were important for modeling utterances in our collected dialogues, although we plan to incorporate other tags in the future. We expected that self-disclosure would occur quite often in our data because the subjects were to talk about their favorite topics and the participants would be willing to communicate about their experiences and feelings. We also expected that the listeners would </context>
</contexts>
<marker>Jurafsky, Shriberg, Biasca, 1997</marker>
<rawString>Dan Jurafsky, Liz Shriberg, and Debra Biasca, 1997. Switchboard SWBD-DAMSL Shallow-Discourse-Function Annotation Coders Manual.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martijn Maatman</author>
<author>Jonathan Gratch</author>
<author>Stacy Marsella</author>
</authors>
<title>Natural behavior of a listening agent.</title>
<date>2005</date>
<journal>Lecture Notes in Computer Science,</journal>
<pages>3661--25</pages>
<contexts>
<context position="3065" citStr="Maatman et al., 2005" startWordPosition="457" endWordPosition="460">uchi Lake this weekend. I like traveling because it is (SELF-DISCLOSURE) stimulating. L: Going to unusual places (SYMPATHY) changes one’s perspective, doesn’t it? You said you’re going to go to (QUESTION) Kawaguchi Lake this weekend. Is this travel? Will you go by car or train? (QUESTION) Figure 1: Excerpt of a typical listening-oriented dialogue. Dialogue acts corresponding to utterances are shown in parentheses (See Section 3.1 for their meanings). The dialogue was originally in Japanese and was translated by the authors. There has been little research on listening agents. One exception is (Maatman et al., 2005), which showed that systems can make the user have the sense of being heard by using gestures, such as nodding and shaking of the head. Although our work is similar to theirs, the difference is that we focus more on verbal communication instead of non-verbal one. For the purpose of gaining insight into how to build our listening agents, we collected listeningoriented dialogues as well as casual conversation, and compared them in order to reveal the characteristics of the listening-oriented dialogue. Figure 1 shows an example of a typical listening-oriented dialogue. In the figure, the conversa</context>
</contexts>
<marker>Maatman, Gratch, Marsella, 2005</marker>
<rawString>Martijn Maatman, Jonathan Gratch, and Stacy Marsella. 2005. Natural behavior of a listening agent. Lecture Notes in Computer Science, 3661:25–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harry T Reis</author>
<author>Phillip Shaver</author>
</authors>
<title>Intimacy as an interpersonal process.</title>
<date>1998</date>
<booktitle>Handbook of personal relationships,</booktitle>
<pages>367--398</pages>
<editor>In S. Duck, editor,</editor>
<publisher>John Wiley &amp; Sons Ltd.</publisher>
<contexts>
<context position="9535" citStr="Reis and Shaver, 1998" startWordPosition="1487" endWordPosition="1490">he listeners would sympathize often to make others talk with ease. Note that sympathy has been found useful to increase closeness between conversational partic125 Listener Speaker Casual Listening Casual Listening DISC 66.6% 44.5% 53.3% 57.3% INFO 6.5% 1.4% 5.6% 5.2% ACK 8.0% 12.3% 6.6% 6.9% QUES 4.1% 25.8% 21.3% 14.0% SYM 2.6% 3.7% 3.2% 3.3% GR 10.9% 9.8% 7.2% 9.6% OTHER 1.3% 2.5% 2.9% 3.7% Table 2: Rates of dialogue act tags. DISC INFO ACK QUES SYM GR Increase 0 0 8 8 5 4 Decrease 8 8 0 0 3 4 Table 3: Number of listeners whose tags increased/decreased in listening-oriented dialogue. ipants (Reis and Shaver, 1998). A single annotator, who is not one of the authors, labeled each utterance using the seven tags (six dialogue act tags plus OTHER). As a result, 1,177 tags were labeled to the utterances in the listeningoriented dialogues and 1,312 tags to those in casual conversation. The numbers of tags and utterances do not match because, in text dialogue, an utterance can be long and may be annotated with several tags. 4 Analysis 4.1 Comparing the frequency of dialogue acts We compared the frequency of the dialogue act tags in listening-oriented dialogues and casual conversation. Table 2 shows the rates o</context>
</contexts>
<marker>Reis, Shaver, 1998</marker>
<rawString>Harry T. Reis and Phillip Shaver. 1998. Intimacy as an interpersonal process. In S. Duck, editor, Handbook of personal relationships, pages 367–398. John Wiley &amp; Sons Ltd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katsuhiko Shirai</author>
</authors>
<title>Modeling of spoken dialogue with and without visual information.</title>
<date>1996</date>
<booktitle>In Proc. ICSLP,</booktitle>
<volume>1</volume>
<pages>188--191</pages>
<contexts>
<context position="5442" citStr="Shirai, 1996" startWordPosition="838" endWordPosition="839">ce of dialogue flows by modeling each type of dialogue by Hidden Markov Models (HMMs) and comparing the obtained models. We employ HMMs because they are useful for modeling sequential data especially when the number of states is unknown. We check whether the HMMs for the listening-oriented dialogue and casual conversation can be successfully distinguished from each other to see if the listening process can be successfully modeled. We also analyze the transitions between states in the created HMMs to examine the dialogue flows. We note that HMMs have been used to model task-oriented dialogues (Shirai, 1996) and casual conversation (Isomura et al., 2006). In this study, we use HMMs to model and analyze listening-oriented dialogues. 3 Data collection We recruited 16 participants. Eight participated as listeners and the other eight as speakers. The maleto-female ratio was even. The participants were 21 to 29 years old. Each participant engaged in four dialogues: two casual conversations followed by two listening-oriented dialogues with a fixed role of listener/speaker. In listening-oriented dialogue, the listeners were instructed to make it easy for the speakers to say what they wanted to say. When</context>
</contexts>
<marker>Shirai, 1996</marker>
<rawString>Katsuhiko Shirai. 1996. Modeling of spoken dialogue with and without visual information. In Proc. ICSLP, volume 1, pages 188–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Rebecca Passonneau</author>
<author>Julie E Boland</author>
</authors>
<title>Quantitative and qualitative evaluation of darpa communicator spoken dialogue systems.</title>
<date>2001</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>515--522</pages>
<contexts>
<context position="1276" citStr="Walker et al., 2001" startWordPosition="170" endWordPosition="173">both listening-oriented dialogues and casual conversation, and analyzed them by comparing the frequency of dialogue acts, as well as the dialogue flows using Hidden Markov Models (HMMs). The analysis revealed that listening-oriented dialogues and casual conversation have characteristically different dialogue flows and that it is important for listening agents to selfdisclose before asking questions and to utter more questions and acknowledgment than in casual conversation to be good listeners. 1 Introduction Although task-oriented dialogue systems have been actively researched over the years (Walker et al., 2001), systems that perform more flexible (less taskoriented) dialogues such as chats are beginning to be actively investigated from their social and entertainment aspects (Bickmore and Cassell, 2001; Higuchi et al., 2008). This paper deals with dialogues in which one conversational participant attentively listens to the other (hereafter, listening-oriented dialogue). Our aim is to build listening agents that can implement such a listening process so that a user can satisfy his/her desire to speak and have him/herself heard. Such agents would lead the user’s state of mind for the better as in a the</context>
</contexts>
<marker>Walker, Passonneau, Boland, 2001</marker>
<rawString>Marilyn A. Walker, Rebecca Passonneau, and Julie E. Boland. 2001. Quantitative and qualitative evaluation of darpa communicator spoken dialogue systems. In Proc. ACL, pages 515–522.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D Williams</author>
<author>Steve Young</author>
</authors>
<title>Partially observable Markov decision processes for spoken dialog systems.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<issue>2</issue>
<marker>Williams, Young, 2007</marker>
<rawString>Jason D. Williams and Steve Young. 2007. Partially observable Markov decision processes for spoken dialog systems. Computer Speech and Language, 21(2):393–422.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>