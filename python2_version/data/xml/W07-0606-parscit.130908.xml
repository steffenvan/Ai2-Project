<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004267">
<title confidence="0.9996025">
A Cognitive Model for the Representation and Acquisition
of Verb Selectional Preferences
</title>
<author confidence="0.984238">
Afra Alishahi
</author>
<affiliation confidence="0.999295">
Department of Computer Science
University of Toronto
</affiliation>
<email confidence="0.99252">
afra@cs.toronto.edu
</email>
<author confidence="0.987985">
Suzanne Stevenson
</author>
<affiliation confidence="0.9993225">
Department of Computer Science
University of Toronto
</affiliation>
<email confidence="0.996805">
suzanne@cs.toronto.edu
</email>
<sectionHeader confidence="0.998592" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999951857142857">
We present a cognitive model of inducing
verb selectional preferences from individ-
ual verb usages. The selectional preferences
for each verb argument are represented as
a probability distribution over the set of
semantic properties that the argument can
possess—a semantic profile. The seman-
tic profiles yield verb-specific conceptual-
izations of the arguments associated with a
syntactic position. The proposed model can
learn appropriate verb profiles from a small
set of noisy training data, and can use them
in simulating human plausibility judgments
and analyzing implicit object alternation.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998404375">
Verbs have preferences for the semantic properties
of the arguments filling a particular role. For ex-
ample, the verb eat expects that the object receiving
its theme role will have the property of being edi-
ble, among others. Learning verb selectional pref-
erences is an important aspect of human language
acquisition, and the acquired preferences have been
shown to guide children’s expectations about miss-
ing or upcoming arguments in language comprehen-
sion (Nation et al., 2003).
Resnik (1996) introduced a statistical approach
to learning and use of verb selectional preferences.
In this framework, a semantic class hierarchy for
words is used, together with statistical tools, to in-
duce a verb’s selectional preferences for a particu-
lar argument position in the form of a distribution
</bodyText>
<page confidence="0.543803">
41
</page>
<bodyText confidence="0.999908882352941">
over all the classes that can occur in that position.
Resnik’s model was proposed as a model of human
learning of selectional preferences that made min-
imal representational assumptions; it showed how
such preferences could be acquired from usage data
and an existing conceptual hierarchy. However, his
and later computational models (see Section 2) have
properties that do not match with certain cognitive
plausibility criteria for a child language acquisition
model. All these models use the training data in
“batch mode”, and most of them use information
theoretic measures that rely on total counts from a
corpus. Therefore, it is not clear how the representa-
tion of selectional preferences could be updated in-
crementally in these models as the person receives
more data. Moreover, the assumption that children
have access to a full hierarchical representation of
semantic classes may be too strict. We propose an
alternative view in this paper which is more plausi-
ble in the context of child language acquisition.
In previous work (Alishahi and Stevenson, 2005),
we have proposed a usage-based computational
model of early verb learning that uses Bayesian clus-
tering and prediction to model language acquisition
and use. Individual verb usages are incrementally
grouped to form emergent classes of linguistic con-
structions that share semantic and syntactic proper-
ties. We have shown that our Bayesian model can
incrementally acquire a general conception of the
semantic roles of predicates based only on expo-
sure to individual verb usages (Alishahi and Steven-
son, 2007). The model forms probabilistic associa-
tions between the semantic properties of arguments,
their syntactic positions, and the semantic primitives
</bodyText>
<note confidence="0.875802">
Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 41–48,
Prague, Czech Republic, June 2007 c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999959087719298">
of verbs. Our previous experiments demonstrated
that, initially, this probability distribution for an ar-
gument position yields verb-specific conceptualiza-
tions of the role associated with that position. As the
model is exposed to more input, the verb-based roles
gradually transform into more abstract representa-
tions that reflect the general properties of arguments
across the observed verbs.
A shortcoming of the model was that, because
the prediction of the semantic roles was based only
on the groupings of verbs, it could not make use of
verb-specific knowledge in generating expectations
about a particular verb’s arguments. That is, once
it was exposed to a range of verbs, it no longer had
access to the verb-specific information, only to gen-
eralizations over clusters of verbs.
In this paper, we propose a new version of our
model that, in addition to learning general seman-
tic roles for constructions, can use its verb-specific
knowledge to predict intuitive selectional prefer-
ences for each verb argument position. We introduce
a new notion, a verb semantic profile, as a prob-
ability distribution over the semantic properties of
an argument for each verb. A verb semantic pro-
file is predicted from both the verb-based and the
construction-based knowledge that the model has
learned through clustering, and reflects the prop-
erties of the arguments that are observed for that
verb. Our proposed prediction model makes appro-
priate generalizations over the observed properties,
and captures expectations about previously unseen
arguments.
As in other work on selectional preferences, the
semantic properties that we use in our representa-
tion of arguments are drawn from a standard lex-
ical ontology (WordNet; Miller, 1990), but we do
not require knowledge of the hierarchical structure
of the WordNet concepts. From the computational
point of view, this makes use of an available re-
source, while from the cognitive view, this avoids
ad hoc assumptions about the representation of a
conceptual hierarchy. However, we do require some
properties to be more general (i.e., shared by more
words) than others, which eventually enables the
model to make appropriate generalizations. Other-
wise, the selected semantic properties are not fun-
damental to the model, and could in the future be
replaced with an approach that is deemed more ap-
propriate to child language acquisition. Each argu-
ment contributes to the semantic profile of the verb
through its (potentially large) set of semantic prop-
erties instead of its membership in a single class. As
input to our model, we use an automatically parsed
corpus, which is very noisy. However, as a result of
our novel representation, the model can induce and
use selectional preferences using a relatively small
set of noisy training data.
</bodyText>
<sectionHeader confidence="0.998621" genericHeader="introduction">
2 Related Computational Models
</sectionHeader>
<bodyText confidence="0.999931111111111">
A variety of computational models for verb selec-
tional preferences have been proposed, which use
different statistical models to induce the preferences
of each verb from corpus data. Most of these
models, however, use the same representation for
verb selectional preferences: the preference can be
thought of as a mapping, with respect to an argument
position for a verb, of each class to a real number
(Light and Greiff, 2002). The induction of a verb’s
preferences is, therefore, modeled as using a set of
training data to estimate that number.
Resnik (1996) defines the selectional preference
strength of a verb as the divergence between two
probability distributions: the prior probabilities of
the classes, and the posterior probabilities of the
classes given that verb. The selectional association
of a verb with a class is also defined as the contribu-
tion of that class to the total selectional preference
strength. Resnik estimates the prior and posterior
probabilities based on the frequencies of each verb
and its relevant argument in a corpus.
Li and Abe (1998) model selectional preferences
of a verb (for an argument position) as a set of nodes
in the semantic class hierarchy with a probability
distribution over them. They use the Minimum De-
scription Length (MDL) principle to find the best set
for each verb and argument based on the usages of
that verb in the training data. Clark and Weir (2002)
also find an appropriate set of concept nodes to rep-
resent the selectional preferences for a verb, but do
so using a x2 test over corpus frequencies mapped
to concepts to determine when to generalize from a
node to its parent. Ciaramita and Johnson (2000)
use a Bayesian network with the same topology as
WordNet to estimate the probability distribution of
the relevant set of nodes in the hierarchy. Abney
</bodyText>
<page confidence="0.988532">
42
</page>
<bodyText confidence="0.8540272">
and Light (1999) use a different representational ap- Sense 1
proach: they train a separate hidden Markov model dinner
for each verb, and the selectional preference is rep- =&gt; meal, repast
resented as a probability distribution over words in- =&gt; nutriment, nourishment, nutrition, sustenance,
stead of semantic classes. aliment, alimentation, victuals
</bodyText>
<table confidence="0.974632727272727">
=&gt; food, nutrient
=&gt; substance, matter
=&gt; entity
Sense 2
dinner, dinner party
=&gt; party
=&gt; social gathering, social affair
=&gt; gathering, assemblage
=&gt; social group
=&gt; group, grouping
3 The Bayesian Verb-Learning Model
</table>
<subsectionHeader confidence="0.529526">
3.1 Overview of the Model
</subsectionHeader>
<bodyText confidence="0.993449821917809">
Our model learns the set of argument structure dinner: {meal, repast, nutriment, nourishment, nutrition, substance, aliment, alimentation,
frames for each verb, and their grouping across verbs victuals, food, nutrient, substance, matter, entity, party, social gathering,
into constructions. An argument structure frame is social affair, gathering, assemblage, social group, group, grouping }
a set of features of a verb usage that are both syn-
tactic (the number of arguments, the syntactic pat-
tern of the usage) and semantic (the semantic prop-
erties of the verb, the semantic properties of each
argument). The syntactic pattern indicates the word
order of the verb and arguments. A construction is
a grouping of individual frames which probabilisti-
cally share syntactic and semantic features, and form
probabilistic associations across verb semantic prop-
erties, argument semantic properties, and syntactic
pattern. These groupings typically correspond to
general constructions in the language such as tran-
sitive, intransitive, and ditransitive.
For each verb, the model associates an argument
position with a probability distribution over a set of
semantic properties—a semantic profile. In doing
so, the model uses the knowledge that it has learned
for that verb, as well as the grouping of frames for
that verb into constructions.
The semantic properties of words are taken from
WordNet (version 2.0) as follows. We extract all the
hypernyms (ancestors) for all the senses of the word,
and add all the words in the hypernym synsets to the
list of the semantic properties. Figure 1 shows an ex-
ample of the hypernyms for dinner, and its resulting
set of semantic properties.1
The following sections review basic properties
of the model from Alishahi and Stevenson (2005,
2007), and introduce extensions that give the model
its ability to make verb-based predictions.
Figure 1: Semantic properties for dinner from Word-
Net
process. This process groups the new frame together
with an existing group of frames—a construction—
that probabilistically has the most similar semantic
and syntactic properties to it. If no construction has
sufficiently high probability for the new frame, then
a new construction is created for it. We use the prob-
abilistic model of Alishahi and Stevenson (2007) for
learning constructions, which is itself an adaptation
of a Bayesian model of human categorization pro-
posed by Anderson (1991). It is important to note
that the categories (i.e., constructions) are not prede-
fined, but rather are created according to the patterns
of similarity over observed frames.
Grouping a frame F with other frames participat-
ing in construction k is formulated as finding the k
with the maximum probability given F:
BestConstruction(F) = argmax P(k|F) (1)
k
where k ranges over the indices of all constructions,
with index 0 representing recognition of a new con-
struction.
Using Bayes rule, and dropping P(F) which is
constant for all k:
P(k|F) = P( k)P(  |k) a P(k)P(F|k) (2)
P(FThe prior probability, P(k), indicates the degree of
entrenchment of construction k, and is given by the
relative frequency of its frames over all observed
frames. The posterior probability of a frame F is
expressed in terms of the individual probabilities of
its features, which we assume are independent, thus
yielding a simple product of feature probabilities:
3.2 Learning as Bayesian Clustering
Each argument structure frame for an observed verb
usage is input to an incremental Bayesian clustering
1We do not remove alternate spellings of a term in WordNet;
this will be seen in the profiles in the results section.
43
and normalize the resulting probability over all pos-
</bodyText>
<equation confidence="0.9469935">
P(F|k) = � Pi(j|k) (3) sible sets of semantic properties in our lexicon.
iEFrameFeatures
</equation>
<bodyText confidence="0.9999938">
where j is the value of the ith feature of F, and
Pi(j|k) is the probability of displaying value j on
feature i within construction k. Given the focus here
on semantic profiles, we next focus on the calcula-
tion of the probabilities of semantic properties.
</bodyText>
<subsectionHeader confidence="0.99988">
3.3 Probabilities of Semantic Properties
</subsectionHeader>
<bodyText confidence="0.999943666666667">
The probability in equation (3) of value j for feature
i in construction k is estimated using a smoothed
version of this maximum likelihood formula:
</bodyText>
<equation confidence="0.945755">
countk i (j)
Pi(j|k) =
nk
</equation>
<bodyText confidence="0.9996883125">
where nk is the number of frames participating in
construction k, and countki (j) is the number of
those with value j for feature i.
For most features, countki (j) is calculated by
simply counting those members of construction k
whose value for feature i exactly matches j. How-
ever, for the semantic properties of words, counting
only the number of exact matches between the sets
is too strict, since even highly similar words very
rarely have the exact same set of properties. We
instead use the following Jaccard similarity score
to measure the overlap between the set of semantic
properties, SF, of a particular argument in the frame
to be clustered, and the set of semantic properties,
Sk, of the same argument in a member frame of a
construction:
</bodyText>
<equation confidence="0.9950935">
sem score(SF, Sk) = |SF � Sk|
|SF � Sk |(5)
</equation>
<bodyText confidence="0.973517428571429">
For example, assume that the new frame F repre-
sents a usage of John ate cake. In the construction
that we are considering for inclusion of F, one of
the member frames represents a usage of Mom got
water. We must compare the semantic properties of
the corresponding arguments cake and water:
cake: {baked goods,food,solid,substance,matter,entity}
water: {liquid,fluid,food,nutrient,substance,matter,entity}
The intersection of the two sets is {food, substance,
matter, entity}, yielding a sem score of 9.
In general, to calculate the conditional probability
for the set of semantic properties, we set countki (j)
in equation (4) to the sum of the sem score’s for
the new frame and every member of construction k,
</bodyText>
<subsectionHeader confidence="0.998795">
3.4 Predicting Semantic Profiles for Verbs
</subsectionHeader>
<bodyText confidence="0.999945333333333">
We represent the selectional preferences of a verb
for an argument position as a semantic profile, which
is a probability distribution over all the semantic
properties. To predict the profile of a verb v for
an argument position arg, we need to estimate the
probability of each semantic property j separately:
</bodyText>
<equation confidence="0.9978195">
Parg(j|v) = � Parg(j, k|v) (6)
k
�a P(k, v)Parg (j|k, v)
k
</equation>
<bodyText confidence="0.999974272727273">
Here, j ranges over all the possible semantic proper-
ties that an argument can have, and k ranges over all
constructions. The prior probability of having verb v
in construction k, or P(k, v), takes into account two
important factors: the relative entrenchment of the
construction k, and the (smoothed) frequency with
which v participates in k.
The posterior probability Parg(j|k, v) is calcu-
lated analogously to Pi(j|k) in equation (4), but lim-
iting the count of matching features to those frames
in k that contain v:
</bodyText>
<equation confidence="0.78695">
Parg(j|k,v) = verb countkarg(j,v)
nkv
</equation>
<bodyText confidence="0.999805714285714">
where nkv is the number of frames for v participat-
ing in construction k, and verb countkarg(j, v) is
the number of those with semantic property j for
argument arg. We use a smoothed version of the
above formula, where the relative frequency of each
property j among all nouns is used as the smoothing
factor.
</bodyText>
<subsectionHeader confidence="0.860389">
3.5 Verb-Argument Compatibility
</subsectionHeader>
<bodyText confidence="0.999962857142857">
In one of our experiments, we need to measure the
compatibility of a particular noun n for an argument
position arg of some verb v. That is, we need to es-
timate how much the semantic properties of n con-
form to the acquired semantic profile of v for arg.
We formulate the compatibility as the conditional
probability of observing n as an argument arg of v:
</bodyText>
<equation confidence="0.989742333333333">
compatibility(v, n) = log(Parg(jn|v)) (8)
(4)
(7)
</equation>
<page confidence="0.967022">
44
</page>
<bodyText confidence="0.9757448">
where jn is the set of the semantic properties for
word n, and Parg(jn|v) is estimated as in equa-
tion (7). However, since jn here is a set of prop-
erties (as opposed to j in equation (7) being a
single property), verb count�a in equation (7)
rg
should be modified as described in Section 3.3:
we set verb countarg(jn,v) to the sum of the
sem score’s (equation (5)) for jn and every frame
of v that participates in construction k.
</bodyText>
<sectionHeader confidence="0.998264" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.9999554">
In the following sections, we first describe the train-
ing data for our model. In accordance with other
computational models, we focus here on the verb
preferences for the direct object position.2 Next, we
provide a qualitative analysis of our model through
examination of the semantic profiles for a number
of verbs. We then evaluate our model through two
tasks of simulating verb-argument plausibility judg-
ment, and analyzing the implicit object alternation,
following Resnik (1996).3
</bodyText>
<subsectionHeader confidence="0.997267">
4.1 The Training Data
</subsectionHeader>
<bodyText confidence="0.9999051875">
In earlier work (Alishahi and Stevenson, 2005,
2007), we used a method to automatically generate
training data with the same distributional properties
as the input children receive. However, this relies on
manually-compiled data about verbs and their argu-
ment structure frames from the CHILDES database
(MacWhinney, 1995). To evaluate the new version
of our model for the task of learning selectional pref-
erences, we need a wide selection of verbs and their
arguments that is impractical to compile by hand.
The training data for our experiments here are
generated as follows. We use 20,000 sentences
randomly selected from the British National Cor-
pus (BNC),4 automatically parsed using the Collins
parser (Collins, 1999), and further processed with
TGrep2,5 and an NP-head extraction software.6 For
</bodyText>
<footnote confidence="0.998004090909091">
2To our knowledge, the only work that considers selectional
preferences of subjects and prepositional phrases as well as di-
rect objects is Brockmann and Lapata (2003).
3Computational models of verb selectional preference have
been evaluated through disambiguation tasks (Li and Abe,
1998; Abney and Light, 1999; Ciaramita and Johnson, 2000;
Clark and Weir, 2002), but for to evaluate our cognitive model,
the experiments from Resnik (1996) are the most interesting.
4http://www.natcorp.ox.ac.uk
5http://tedlab.mit.edu/—dr/Tgrep2
6The software was provided to us by Eric Joanis, and Af-
</footnote>
<bodyText confidence="0.999676545454545">
each verb usage in a sentence, we construct a frame
by recording the verb in root form, the number of
the arguments for that verb, and the syntactic pattern
of the verb usage (i.e., the word order of the verb
and the arguments). We also record in the frame the
semantic properties of the verb and each of the ar-
gument heads (each noun is also converted to root
form); these properties are extracted from WordNet
(as discussed in Section 3.1 and illustrated in Fig-
ure 1). This process results in 16,300 frames which
serve as input data to our learning model.
</bodyText>
<subsectionHeader confidence="0.99916">
4.2 Formation of Semantic Profiles for Verbs
</subsectionHeader>
<bodyText confidence="0.9849336">
After training our model on the above data, we use
equation (7) to predict the semantic profile of the di-
rect object position for a range of verbs. Some of
these verbs, such as write and sing, have strong se-
lectional preferences, whereas others, such as want
and put, can take a wide range of nouns as direct
object (as confirmed by Resnik’s (1996) estimated
strength of selectional preference for these verbs).
The semantic profiles for write and sing are dis-
played in Figure 2, and the profiles for want and put
are displayed in Figure 3. (Due to limited space, we
only include the 25 properties that have the highest
probability in each profile.)
Because we extract the semantic properties of
words from WordNet, which has a hierarchical
structure, the properties that come from nodes in
the higher levels of the hierarchy (such as entity and
abstraction) appear as the semantic property for a
very large set of words, whereas the properties that
come from the leaves in the hierarchy are specific to
a small set of words. Therefore, the general prop-
erties are more likely to be associated with a higher
probability in the semantic profiles for most verbs.
In fact, a closer look at the semantic profiles for want
and put reveals that the top portion of the semantic
profile for these verbs consists solely of such gen-
eral properties that are shared among a large group
of words. However, this is not the case for the more
restrictive verbs. The semantic profiles for write and
sing show that the specific properties that these verbs
demand from their direct object appear amongst the
highest-ranked properties, even though only a small
set of words share these properties (e.g., content,
saneh Fazly helped us in using the above-mentioned tools for
generating our input corpora.
</bodyText>
<page confidence="0.99572">
45
</page>
<table confidence="0.996437068965517">
write sing
(0.024) abstraction (0.020) abstraction
(0.022) entity (0.015) relation
(0.021) location (0.015) communication
(0.020) substance (0.015) social relation
(0.019) destination (0.013) act
(0.018) relation (0.013) human action
(0.015) communication (0.013) human activity
(0.015) social relation (0.013) auditory
(0.013) content communication
(0.011) message (0.012) music
(0.011) subject matter (0.010) entity
(0.011) written (0.010) piece
communication (0.009) composition
(0.011) written (0.009) musical
language composition
(0.010) object (0.009) opus
(0.010) physical object (0.009) piece of music
(0.010) writing (0.009) psychological
(0.010) goal feature
(0.010) unit (0.008) cognition
(0.009) whole (0.008) knowledge
(0.009) whole thing (0.008) noesis
(0.009) artifact (0.008) activity
(0.009) artefact (0.008) content
(0.009) state (0.008) grouping
(0.009) amount (0.008) group
(0.009) measure (0.008) amount
(0.008) measure
</table>
<figureCaption confidence="0.98456">
Figure 2: Semantic profiles of write and sing for the
direct object position.
</figureCaption>
<bodyText confidence="0.999047476190476">
message, written communication, written language,
... for write, and auditory communication, music,
musical composition, opus, ... for sing).
The examination of the semantic profiles for fairly
frequent verbs in the training data shows that our
model can use the verb usages to predict an appro-
priate semantic profile for each verb. When pre-
sented with a novel verb (for which no verb-based
information is available), equation (7) predicts a se-
mantic profile which reflects the relative frequencies
of the semantic properties among all words (due to
the smoothing factor added to equation (7)), modu-
lated by the prior probability of each construction.
The predicted profile is displayed in Figure 4. It
shows similarities with the profiles for want and put
in Figure 3, but the general properties in this profile
have an even higher probability. Since the profile for
the novel verb is predicted in the absence of any evi-
dence (i.e., verb usage) in the training data, we later
use it as the base for estimating other verbs’ strength
of selectional preference.
</bodyText>
<table confidence="0.999224384615385">
want put
(0.016) entity (0.015) entity
(0.015) object (0.015) object
(0.015) physical object (0.013) physical object
(0.014) abstraction (0.013) abstraction
(0.013) act (0.011) unit
(0.012) human action (0.011) whole
(0.012) human activity (0.011) whole thing
(0.012) relation (0.011) artifact
(0.011) unit (0.011) artefact
(0.011) whole (0.010) act
(0.011) whole thing (0.009) relation
(0.011) artifact (0.008) human action
(0.011) artefact (0.008) human activity
(0.008) communication (0.008) communication
(0.008) social relation (0.008) social relation
(0.008) activity (0.007) substance
(0.007) cause (0.007) content
(0.007) state (0.007) instrumentality
(0.007) instrumentality (0.007) instrumentation
(0.007) instrumentation (0.007) measure
(0.007) event (0.006) amount
(0.006) being (0.006) quantity
(0.006) living thing (0.006) cause
(0.006) animate thing (0.006) causal agent
(0.006) organism (0.006) causalagency
</table>
<figureCaption confidence="0.953077">
Figure 3: Semantic profiles of want and put for the
direct object position.
</figureCaption>
<subsectionHeader confidence="0.999134">
4.3 Verb-Argument Plausibility Judgments
</subsectionHeader>
<bodyText confidence="0.999973391304348">
Holmes et al. (1989) evaluate verb argument plau-
sibility by asking human subjects to rate sentences
like The mechanic warned the driver and The me-
chanic warned the engine. Resnik (1996) used this
data to assess the performance of his model by com-
paring its judgments of selectional fit against the
plausibility ratings elicited from human subjects. He
showed that his selectional association measure for
a verb and its direct object can be used to select the
more plausible verb-noun pair among the two (e.g.,
&lt;warn,driver&gt; vs. &lt;warn,engine&gt; in the previous
example). That is, a higher selectional association
between the verb and one of the nouns compared to
the other noun indicates that the former is the more
plausible pair. Resnik (1996) used the Brown corpus
as training data, and showed that his model arrives
at the correct ordering of more and less plausible ar-
guments in 11 of the 16 cases.
We repeated this experiment, using the same 16
pairs of verb-noun combinations. For each pair of
&lt;v, n1&gt; and &lt;v, n2&gt;, we calculate the compati-
bility measure using equation (8); these values are
shown in Figure 5. (Note that because these are
</bodyText>
<page confidence="0.998502">
46
</page>
<bodyText confidence="0.7345">
A novel verb
</bodyText>
<listItem confidence="0.99476352">
(0.021) entity
(0.017) object
(0.017) physical object
(0.015) abstraction
(0.010) act
(0.010) human action
(0.010) human activity
(0.010) unit
(0.009) whole
(0.009) whole thing
(0.009) artifact
(0.009) artefact
(0.009) being
(0.009) living thing
(0.009) animate thing
(0.009) organism
(0.008) cause
(0.008) causal agent
(0.008) causal agency
(0.008) relation
(0.008) person
(0.008) individual
(0.008) someone
(0.008) somebody
(0.008) mortal
</listItem>
<figureCaption confidence="0.9705305">
Figure 4: Semantic profile of a novel verb for the
direct object position.
</figureCaption>
<bodyText confidence="0.998416823529412">
log-probabilities and therefore negative numbers,
a lower absolute value of compatibility(v, n)
shows a better compatibility between the verb v
and the argument n.) For example, &lt;see,friend&gt;
has a higher compatibility score (-30.50) than
&lt;see,method&gt; (-32.14). Similar to Resnik, our
model detects 11 plausible pairs out of 16. How-
ever, these results are reached with a much smaller
training corpus (around 500,000 words), compared
to the Brown corpus used by Resnik (1996) which
contains one million words. Moreover, whereas the
Brown corpus is tagged and parsed manually, the
portion of the BNC that we use is parsed automat-
ically, and as a result our training data is very noisy.
Nonetheless, the model achieves the same level of
accuracy in distinguishing plausible verb-argument
pairs from implausible ones.
</bodyText>
<subsectionHeader confidence="0.961759">
4.4 Implicit Object Alternations
</subsectionHeader>
<bodyText confidence="0.9994098">
In English, some inherently transitive verbs can ap-
pear with or without their direct objects (e.g., John
ate his dinner as well as John ate), but others can-
not (e.g., Mary made a cake but not *Mary made).
It is argued that implicit object alternations involve a
</bodyText>
<table confidence="0.999477529411765">
Verb Plausible Implausible
see friend -30.50 method -32.14
read article -32.76 fashion -33.33
find label -32.05 fever -33.30
hear story -32.11 issue -32.40
write letter -31.37 market -32.46
urge daughter -36.73 contrast -35.64
warn driver -33.68 engine -34.42
judge contest -39.05 climate -38.23
teach language -45.64 distance -45.11
show sample -31.75 travel -31.42
expect visit -33.88 mouth -32.87
answer request -31.89 tragedy -33.95
recognize author -32.53 pocket -32.62
repeat comment -33.80 journal -33.97
understand concept -32.25 session -32.93
remember reply -33.79 smoke -34.29
</table>
<figureCaption confidence="0.988374">
Figure 5: Compatibility scores for plausible vs. im-
plausible verb-noun pairs.
</figureCaption>
<bodyText confidence="0.999592870967742">
particular relationship between the verb and its argu-
ment. In particular, for verbs that participate in the
implicit object alternation, the omitted object must
be in some sense inferable or typical for that verb
(Levin, 1993, among others).
Resnik (1996) used his model of selectional pref-
erences to analyze implicit object alternations, and
showed a relationship between his measure of se-
lectional preference strength and the notion of typ-
icality of an object. He calculated this measure
for two groups of Alternating and Non-alternating
verbs, and showed that, on average, the Alternating
verbs have a higher strength of selectional prefer-
ence for the direct object than the Non-alternating
verbs. However, there was no threshold separating
the two groups of verbs.
To repeat Resnik’s experiment, we need a mea-
sure of how “strongly constraining” a semantic pro-
file is. We can do this by measuring the similarity
between the semantic profile we generate for the ob-
ject of a particular verb and some “default” notion of
the argument for that position across all verbs. We
use the semantic profile predicted for the object po-
sition of a novel verb, shown earlier in Figure 4, as
the default profile for that argument position. Be-
cause this profile is predicted in the absence of any
evidence in the training data, it makes the minimum
assumptions about the properties of the argument
and thus serves as a suitable default. We then assume
that verbs with weaker selectional preferences have
semantic profiles more similar to the default profile
</bodyText>
<page confidence="0.996916">
47
</page>
<table confidence="0.999623">
Alternating verbs Non-alternating verbs
write 0.61 hang 0.56
sing 0.67 wear 0.71
drink 0.67 say 0.75
eat 0.74 catch 0.76
play 0.74 show 0.77
pour 0.76 make 0.78
watch 0.77 hit 0.78
pack 0.78 open 0.81
steal 0.80 take 0.83
push 0.80 see 0.87
call 0.80 like 0.87
pull 0.80 get 0.87
explain 0.81 find 0.87
read 0.82 give 0.88
hear 0.87 bring 0.89
want 0.89
put 0.90
Mean: 0.76 Mean: 0.81
</table>
<figureCaption confidence="0.9350325">
Figure 6: Similarity with the base profile for Alter-
nating and Non-alternating verbs.
</figureCaption>
<bodyText confidence="0.958369666666667">
than verbs with stronger preferences. We use the
cosine measure to estimate the similarity between
two profiles p and q:
</bodyText>
<equation confidence="0.988558">
cosine(p, q) = p × q(9)
||p ||× ||q||
</equation>
<bodyText confidence="0.999988909090909">
The similarity values for the Alternating and Non-
alternating verbs are shown in Figure 6. The larger
values represent more similarity with the base pro-
file, which means a weaker selectional preference.
The means for the Alternating and Non-alternating
verbs were respectively 0.76 and 0.81, which con-
firm the hypothesis that verbs participating in im-
plicit object alternations select more strongly for the
direct objects than verbs that do not. However, like
Resnik (1996), we find that it is not possible to set a
threshold that will distinguish the two sets of verbs.
</bodyText>
<sectionHeader confidence="0.999735" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999986">
We have proposed a cognitively plausible model for
learning selectional preferences from instances of
verb usage. The model represents verb selectional
preferences as a semantic profile, which is a prob-
ability distribution over the semantic properties that
an argument can take. One of the strengths of our
model is the incremental nature of its learning mech-
anism, in contrast to other approaches which learn
selectional preferences in batch mode. Here we have
only reported the results for the final stage of learn-
ing, but the model allows us to monitor the semantic
profiles during the course of learning, and compare
it with child data for different age groups, as we do
with semantic roles (Alishahi and Stevenson, 2007).
We have shown that the model can predict appropri-
ate semantic profiles for a variety of verbs, and use
these profiles to simulate human judgments of verb-
argument plausibility, using a small and highly noisy
set of training data. The model can also use the pro-
files to measure verb-argument compatibility, which
was used in analyzing the implicit object alternation.
</bodyText>
<sectionHeader confidence="0.999431" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999700022727273">
Abney, S. and Light, M. (1999). Hiding a semantic hierarchy
in a Markov model. In Proc. of the ACL Workshop on Unsu-
pervised Learning in Natural Language Processing.
Alishahi, A. and Stevenson, S. (2005). A probabilistic model of
early argument structure acquisition. In Proc. of the CogSci
2005.
Alishahi, A. and Stevenson, S. (2007). A computational usage-
based model for learning general properties of semantic
roles. In Proc. of the EuroCogSci 2007.
Anderson, J. R. (1991). The adaptive nature of human catego-
rization. Psychological Review, 98(3):409–429.
Brockmann, C. and Lapata, M. (2003). Evaluating and com-
bining approaches to selectional preference acquisition. In
Proc. of the EACL 2003.
Ciaramita, M. and Johnson, M. (2000). Explaining away am-
biguity: Learning verb selectional preference with Bayesian
networks. In Proc. of the COLING 2000.
Clark, S. and Weir, D. (2002). Class-based probability estima-
tion using a semantic hierarchy. Computational Linguistics,
28(2):187–206.
Collins, M. (1999). Head-Driven Statistical Models for Natural
Language Parsing. PhD thesis, University of Pennsylvania.
Holmes, V. M., Stowe, L., and Cupples, L. (1989). Lexical
expectations in parsing complement-verb sentences. Journal
ofMemory and Language, 28:668–689.
Levin, B. (1993). English verb classes and alternations: A pre-
liminary investigation. The University of Chicago Press.
Li, H. and Abe, N. (1998). Generalizing case frames using a
thesaurus and the MDL principle. Computational Linguis-
tics, 24(2):217–244.
Light, M. and Greiff, W. (2002). Statistical models for the in-
duction and use of selectional preferences. Cognitive Sci-
ence, 26(3):269–281.
MacWhinney, B. (1995). The CHILDESproject: Tools for an-
alyzing talk. Lawrence Erlbaum.
Miller, G. (1990). WordNet: An on-line lexical database. Inter-
national Journal ofLexicography, 17(3).
Nation, K., Marshall, C. M., and Altmann, G. T. M. (2003). In-
vestigating individual differences in children’s real-time sen-
tence comprehension using language-mediated eye move-
ments. J. of Experimental Child Psych., 86:314–329.
Resnik, P. (1996). Selectional constraints: An information-
theoretic model and its computational realization. Cognition,
61:127–199.
</reference>
<page confidence="0.999353">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.813759">
<title confidence="0.9957005">A Cognitive Model for the Representation and of Verb Selectional Preferences</title>
<author confidence="0.902567">Afra</author>
<affiliation confidence="0.999748">Department of Computer University of</affiliation>
<email confidence="0.995241">afra@cs.toronto.edu</email>
<author confidence="0.919066">Suzanne</author>
<affiliation confidence="0.999774">Department of Computer University of</affiliation>
<email confidence="0.99871">suzanne@cs.toronto.edu</email>
<abstract confidence="0.999352">We present a cognitive model of inducing verb selectional preferences from individual verb usages. The selectional preferences for each verb argument are represented as a probability distribution over the set of semantic properties that the argument can The semantic profiles yield verb-specific conceptualizations of the arguments associated with a syntactic position. The proposed model can learn appropriate verb profiles from a small set of noisy training data, and can use them in simulating human plausibility judgments and analyzing implicit object alternation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
<author>M Light</author>
</authors>
<title>Hiding a semantic hierarchy in a Markov model.</title>
<date>1999</date>
<booktitle>In Proc. of the ACL Workshop on Unsupervised Learning in Natural Language Processing.</booktitle>
<contexts>
<context position="18336" citStr="Abney and Light, 1999" startWordPosition="2924" endWordPosition="2927">o compile by hand. The training data for our experiments here are generated as follows. We use 20,000 sentences randomly selected from the British National Corpus (BNC),4 automatically parsed using the Collins parser (Collins, 1999), and further processed with TGrep2,5 and an NP-head extraction software.6 For 2To our knowledge, the only work that considers selectional preferences of subjects and prepositional phrases as well as direct objects is Brockmann and Lapata (2003). 3Computational models of verb selectional preference have been evaluated through disambiguation tasks (Li and Abe, 1998; Abney and Light, 1999; Ciaramita and Johnson, 2000; Clark and Weir, 2002), but for to evaluate our cognitive model, the experiments from Resnik (1996) are the most interesting. 4http://www.natcorp.ox.ac.uk 5http://tedlab.mit.edu/—dr/Tgrep2 6The software was provided to us by Eric Joanis, and Afeach verb usage in a sentence, we construct a frame by recording the verb in root form, the number of the arguments for that verb, and the syntactic pattern of the verb usage (i.e., the word order of the verb and the arguments). We also record in the frame the semantic properties of the verb and each of the argument heads (e</context>
</contexts>
<marker>Abney, Light, 1999</marker>
<rawString>Abney, S. and Light, M. (1999). Hiding a semantic hierarchy in a Markov model. In Proc. of the ACL Workshop on Unsupervised Learning in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Alishahi</author>
<author>S Stevenson</author>
</authors>
<title>A probabilistic model of early argument structure acquisition.</title>
<date>2005</date>
<booktitle>In Proc. of the CogSci</booktitle>
<contexts>
<context position="2745" citStr="Alishahi and Stevenson, 2005" startWordPosition="412" endWordPosition="415">ild language acquisition model. All these models use the training data in “batch mode”, and most of them use information theoretic measures that rely on total counts from a corpus. Therefore, it is not clear how the representation of selectional preferences could be updated incrementally in these models as the person receives more data. Moreover, the assumption that children have access to a full hierarchical representation of semantic classes may be too strict. We propose an alternative view in this paper which is more plausible in the context of child language acquisition. In previous work (Alishahi and Stevenson, 2005), we have proposed a usage-based computational model of early verb learning that uses Bayesian clustering and prediction to model language acquisition and use. Individual verb usages are incrementally grouped to form emergent classes of linguistic constructions that share semantic and syntactic properties. We have shown that our Bayesian model can incrementally acquire a general conception of the semantic roles of predicates based only on exposure to individual verb usages (Alishahi and Stevenson, 2007). The model forms probabilistic associations between the semantic properties of arguments, t</context>
<context position="10583" citStr="Alishahi and Stevenson (2005" startWordPosition="1643" endWordPosition="1646">set of semantic properties—a semantic profile. In doing so, the model uses the knowledge that it has learned for that verb, as well as the grouping of frames for that verb into constructions. The semantic properties of words are taken from WordNet (version 2.0) as follows. We extract all the hypernyms (ancestors) for all the senses of the word, and add all the words in the hypernym synsets to the list of the semantic properties. Figure 1 shows an example of the hypernyms for dinner, and its resulting set of semantic properties.1 The following sections review basic properties of the model from Alishahi and Stevenson (2005, 2007), and introduce extensions that give the model its ability to make verb-based predictions. Figure 1: Semantic properties for dinner from WordNet process. This process groups the new frame together with an existing group of frames—a construction— that probabilistically has the most similar semantic and syntactic properties to it. If no construction has sufficiently high probability for the new frame, then a new construction is created for it. We use the probabilistic model of Alishahi and Stevenson (2007) for learning constructions, which is itself an adaptation of a Bayesian model of hu</context>
<context position="17271" citStr="Alishahi and Stevenson, 2005" startWordPosition="2762" endWordPosition="2765">ery frame of v that participates in construction k. 4 Experimental Results In the following sections, we first describe the training data for our model. In accordance with other computational models, we focus here on the verb preferences for the direct object position.2 Next, we provide a qualitative analysis of our model through examination of the semantic profiles for a number of verbs. We then evaluate our model through two tasks of simulating verb-argument plausibility judgment, and analyzing the implicit object alternation, following Resnik (1996).3 4.1 The Training Data In earlier work (Alishahi and Stevenson, 2005, 2007), we used a method to automatically generate training data with the same distributional properties as the input children receive. However, this relies on manually-compiled data about verbs and their argument structure frames from the CHILDES database (MacWhinney, 1995). To evaluate the new version of our model for the task of learning selectional preferences, we need a wide selection of verbs and their arguments that is impractical to compile by hand. The training data for our experiments here are generated as follows. We use 20,000 sentences randomly selected from the British National </context>
</contexts>
<marker>Alishahi, Stevenson, 2005</marker>
<rawString>Alishahi, A. and Stevenson, S. (2005). A probabilistic model of early argument structure acquisition. In Proc. of the CogSci 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Alishahi</author>
<author>S Stevenson</author>
</authors>
<title>A computational usagebased model for learning general properties of semantic roles.</title>
<date>2007</date>
<booktitle>In Proc. of the EuroCogSci</booktitle>
<contexts>
<context position="3253" citStr="Alishahi and Stevenson, 2007" startWordPosition="489" endWordPosition="493">per which is more plausible in the context of child language acquisition. In previous work (Alishahi and Stevenson, 2005), we have proposed a usage-based computational model of early verb learning that uses Bayesian clustering and prediction to model language acquisition and use. Individual verb usages are incrementally grouped to form emergent classes of linguistic constructions that share semantic and syntactic properties. We have shown that our Bayesian model can incrementally acquire a general conception of the semantic roles of predicates based only on exposure to individual verb usages (Alishahi and Stevenson, 2007). The model forms probabilistic associations between the semantic properties of arguments, their syntactic positions, and the semantic primitives Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 41–48, Prague, Czech Republic, June 2007 c�2007 Association for Computational Linguistics of verbs. Our previous experiments demonstrated that, initially, this probability distribution for an argument position yields verb-specific conceptualizations of the role associated with that position. As the model is exposed to more input, the verb-based roles gradual</context>
<context position="11099" citStr="Alishahi and Stevenson (2007)" startWordPosition="1723" endWordPosition="1726">mantic properties.1 The following sections review basic properties of the model from Alishahi and Stevenson (2005, 2007), and introduce extensions that give the model its ability to make verb-based predictions. Figure 1: Semantic properties for dinner from WordNet process. This process groups the new frame together with an existing group of frames—a construction— that probabilistically has the most similar semantic and syntactic properties to it. If no construction has sufficiently high probability for the new frame, then a new construction is created for it. We use the probabilistic model of Alishahi and Stevenson (2007) for learning constructions, which is itself an adaptation of a Bayesian model of human categorization proposed by Anderson (1991). It is important to note that the categories (i.e., constructions) are not predefined, but rather are created according to the patterns of similarity over observed frames. Grouping a frame F with other frames participating in construction k is formulated as finding the k with the maximum probability given F: BestConstruction(F) = argmax P(k|F) (1) k where k ranges over the indices of all constructions, with index 0 representing recognition of a new construction. Us</context>
</contexts>
<marker>Alishahi, Stevenson, 2007</marker>
<rawString>Alishahi, A. and Stevenson, S. (2007). A computational usagebased model for learning general properties of semantic roles. In Proc. of the EuroCogSci 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Anderson</author>
</authors>
<title>The adaptive nature of human categorization.</title>
<date>1991</date>
<journal>Psychological Review,</journal>
<volume>98</volume>
<issue>3</issue>
<contexts>
<context position="11229" citStr="Anderson (1991)" startWordPosition="1745" endWordPosition="1746">ns that give the model its ability to make verb-based predictions. Figure 1: Semantic properties for dinner from WordNet process. This process groups the new frame together with an existing group of frames—a construction— that probabilistically has the most similar semantic and syntactic properties to it. If no construction has sufficiently high probability for the new frame, then a new construction is created for it. We use the probabilistic model of Alishahi and Stevenson (2007) for learning constructions, which is itself an adaptation of a Bayesian model of human categorization proposed by Anderson (1991). It is important to note that the categories (i.e., constructions) are not predefined, but rather are created according to the patterns of similarity over observed frames. Grouping a frame F with other frames participating in construction k is formulated as finding the k with the maximum probability given F: BestConstruction(F) = argmax P(k|F) (1) k where k ranges over the indices of all constructions, with index 0 representing recognition of a new construction. Using Bayes rule, and dropping P(F) which is constant for all k: P(k|F) = P( k)P( |k) a P(k)P(F|k) (2) P(FThe prior probability, P(k</context>
</contexts>
<marker>Anderson, 1991</marker>
<rawString>Anderson, J. R. (1991). The adaptive nature of human categorization. Psychological Review, 98(3):409–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Brockmann</author>
<author>M Lapata</author>
</authors>
<title>Evaluating and combining approaches to selectional preference acquisition.</title>
<date>2003</date>
<booktitle>In Proc. of the EACL</booktitle>
<contexts>
<context position="18192" citStr="Brockmann and Lapata (2003)" startWordPosition="2904" endWordPosition="2907">new version of our model for the task of learning selectional preferences, we need a wide selection of verbs and their arguments that is impractical to compile by hand. The training data for our experiments here are generated as follows. We use 20,000 sentences randomly selected from the British National Corpus (BNC),4 automatically parsed using the Collins parser (Collins, 1999), and further processed with TGrep2,5 and an NP-head extraction software.6 For 2To our knowledge, the only work that considers selectional preferences of subjects and prepositional phrases as well as direct objects is Brockmann and Lapata (2003). 3Computational models of verb selectional preference have been evaluated through disambiguation tasks (Li and Abe, 1998; Abney and Light, 1999; Ciaramita and Johnson, 2000; Clark and Weir, 2002), but for to evaluate our cognitive model, the experiments from Resnik (1996) are the most interesting. 4http://www.natcorp.ox.ac.uk 5http://tedlab.mit.edu/—dr/Tgrep2 6The software was provided to us by Eric Joanis, and Afeach verb usage in a sentence, we construct a frame by recording the verb in root form, the number of the arguments for that verb, and the syntactic pattern of the verb usage (i.e., </context>
</contexts>
<marker>Brockmann, Lapata, 2003</marker>
<rawString>Brockmann, C. and Lapata, M. (2003). Evaluating and combining approaches to selectional preference acquisition. In Proc. of the EACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ciaramita</author>
<author>M Johnson</author>
</authors>
<title>Explaining away ambiguity: Learning verb selectional preference with Bayesian networks.</title>
<date>2000</date>
<booktitle>In Proc. of the COLING</booktitle>
<contexts>
<context position="8064" citStr="Ciaramita and Johnson (2000)" startWordPosition="1255" endWordPosition="1258"> in a corpus. Li and Abe (1998) model selectional preferences of a verb (for an argument position) as a set of nodes in the semantic class hierarchy with a probability distribution over them. They use the Minimum Description Length (MDL) principle to find the best set for each verb and argument based on the usages of that verb in the training data. Clark and Weir (2002) also find an appropriate set of concept nodes to represent the selectional preferences for a verb, but do so using a x2 test over corpus frequencies mapped to concepts to determine when to generalize from a node to its parent. Ciaramita and Johnson (2000) use a Bayesian network with the same topology as WordNet to estimate the probability distribution of the relevant set of nodes in the hierarchy. Abney 42 and Light (1999) use a different representational ap- Sense 1 proach: they train a separate hidden Markov model dinner for each verb, and the selectional preference is rep- =&gt; meal, repast resented as a probability distribution over words in- =&gt; nutriment, nourishment, nutrition, sustenance, stead of semantic classes. aliment, alimentation, victuals =&gt; food, nutrient =&gt; substance, matter =&gt; entity Sense 2 dinner, dinner party =&gt; party =&gt; soc</context>
<context position="18365" citStr="Ciaramita and Johnson, 2000" startWordPosition="2928" endWordPosition="2931">training data for our experiments here are generated as follows. We use 20,000 sentences randomly selected from the British National Corpus (BNC),4 automatically parsed using the Collins parser (Collins, 1999), and further processed with TGrep2,5 and an NP-head extraction software.6 For 2To our knowledge, the only work that considers selectional preferences of subjects and prepositional phrases as well as direct objects is Brockmann and Lapata (2003). 3Computational models of verb selectional preference have been evaluated through disambiguation tasks (Li and Abe, 1998; Abney and Light, 1999; Ciaramita and Johnson, 2000; Clark and Weir, 2002), but for to evaluate our cognitive model, the experiments from Resnik (1996) are the most interesting. 4http://www.natcorp.ox.ac.uk 5http://tedlab.mit.edu/—dr/Tgrep2 6The software was provided to us by Eric Joanis, and Afeach verb usage in a sentence, we construct a frame by recording the verb in root form, the number of the arguments for that verb, and the syntactic pattern of the verb usage (i.e., the word order of the verb and the arguments). We also record in the frame the semantic properties of the verb and each of the argument heads (each noun is also converted to</context>
</contexts>
<marker>Ciaramita, Johnson, 2000</marker>
<rawString>Ciaramita, M. and Johnson, M. (2000). Explaining away ambiguity: Learning verb selectional preference with Bayesian networks. In Proc. of the COLING 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>D Weir</author>
</authors>
<title>Class-based probability estimation using a semantic hierarchy.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>2</issue>
<contexts>
<context position="7808" citStr="Clark and Weir (2002)" startWordPosition="1210" endWordPosition="1213">association of a verb with a class is also defined as the contribution of that class to the total selectional preference strength. Resnik estimates the prior and posterior probabilities based on the frequencies of each verb and its relevant argument in a corpus. Li and Abe (1998) model selectional preferences of a verb (for an argument position) as a set of nodes in the semantic class hierarchy with a probability distribution over them. They use the Minimum Description Length (MDL) principle to find the best set for each verb and argument based on the usages of that verb in the training data. Clark and Weir (2002) also find an appropriate set of concept nodes to represent the selectional preferences for a verb, but do so using a x2 test over corpus frequencies mapped to concepts to determine when to generalize from a node to its parent. Ciaramita and Johnson (2000) use a Bayesian network with the same topology as WordNet to estimate the probability distribution of the relevant set of nodes in the hierarchy. Abney 42 and Light (1999) use a different representational ap- Sense 1 proach: they train a separate hidden Markov model dinner for each verb, and the selectional preference is rep- =&gt; meal, repast </context>
<context position="18388" citStr="Clark and Weir, 2002" startWordPosition="2932" endWordPosition="2935">ents here are generated as follows. We use 20,000 sentences randomly selected from the British National Corpus (BNC),4 automatically parsed using the Collins parser (Collins, 1999), and further processed with TGrep2,5 and an NP-head extraction software.6 For 2To our knowledge, the only work that considers selectional preferences of subjects and prepositional phrases as well as direct objects is Brockmann and Lapata (2003). 3Computational models of verb selectional preference have been evaluated through disambiguation tasks (Li and Abe, 1998; Abney and Light, 1999; Ciaramita and Johnson, 2000; Clark and Weir, 2002), but for to evaluate our cognitive model, the experiments from Resnik (1996) are the most interesting. 4http://www.natcorp.ox.ac.uk 5http://tedlab.mit.edu/—dr/Tgrep2 6The software was provided to us by Eric Joanis, and Afeach verb usage in a sentence, we construct a frame by recording the verb in root form, the number of the arguments for that verb, and the syntactic pattern of the verb usage (i.e., the word order of the verb and the arguments). We also record in the frame the semantic properties of the verb and each of the argument heads (each noun is also converted to root form); these prop</context>
</contexts>
<marker>Clark, Weir, 2002</marker>
<rawString>Clark, S. and Weir, D. (2002). Class-based probability estimation using a semantic hierarchy. Computational Linguistics, 28(2):187–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>PhD thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="17947" citStr="Collins, 1999" startWordPosition="2869" endWordPosition="2870">ata with the same distributional properties as the input children receive. However, this relies on manually-compiled data about verbs and their argument structure frames from the CHILDES database (MacWhinney, 1995). To evaluate the new version of our model for the task of learning selectional preferences, we need a wide selection of verbs and their arguments that is impractical to compile by hand. The training data for our experiments here are generated as follows. We use 20,000 sentences randomly selected from the British National Corpus (BNC),4 automatically parsed using the Collins parser (Collins, 1999), and further processed with TGrep2,5 and an NP-head extraction software.6 For 2To our knowledge, the only work that considers selectional preferences of subjects and prepositional phrases as well as direct objects is Brockmann and Lapata (2003). 3Computational models of verb selectional preference have been evaluated through disambiguation tasks (Li and Abe, 1998; Abney and Light, 1999; Ciaramita and Johnson, 2000; Clark and Weir, 2002), but for to evaluate our cognitive model, the experiments from Resnik (1996) are the most interesting. 4http://www.natcorp.ox.ac.uk 5http://tedlab.mit.edu/—dr</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Collins, M. (1999). Head-Driven Statistical Models for Natural Language Parsing. PhD thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V M Holmes</author>
<author>L Stowe</author>
<author>L Cupples</author>
</authors>
<title>Lexical expectations in parsing complement-verb sentences.</title>
<date>1989</date>
<journal>Journal ofMemory and Language,</journal>
<pages>28--668</pages>
<contexts>
<context position="24139" citStr="Holmes et al. (1989)" startWordPosition="3814" endWordPosition="3817"> (0.008) human activity (0.008) communication (0.008) communication (0.008) social relation (0.008) social relation (0.008) activity (0.007) substance (0.007) cause (0.007) content (0.007) state (0.007) instrumentality (0.007) instrumentality (0.007) instrumentation (0.007) instrumentation (0.007) measure (0.007) event (0.006) amount (0.006) being (0.006) quantity (0.006) living thing (0.006) cause (0.006) animate thing (0.006) causal agent (0.006) organism (0.006) causalagency Figure 3: Semantic profiles of want and put for the direct object position. 4.3 Verb-Argument Plausibility Judgments Holmes et al. (1989) evaluate verb argument plausibility by asking human subjects to rate sentences like The mechanic warned the driver and The mechanic warned the engine. Resnik (1996) used this data to assess the performance of his model by comparing its judgments of selectional fit against the plausibility ratings elicited from human subjects. He showed that his selectional association measure for a verb and its direct object can be used to select the more plausible verb-noun pair among the two (e.g., &lt;warn,driver&gt; vs. &lt;warn,engine&gt; in the previous example). That is, a higher selectional association between th</context>
</contexts>
<marker>Holmes, Stowe, Cupples, 1989</marker>
<rawString>Holmes, V. M., Stowe, L., and Cupples, L. (1989). Lexical expectations in parsing complement-verb sentences. Journal ofMemory and Language, 28:668–689.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
</authors>
<title>English verb classes and alternations: A preliminary investigation.</title>
<date>1993</date>
<publisher>The University of Chicago Press.</publisher>
<contexts>
<context position="27793" citStr="Levin, 1993" startWordPosition="4388" endWordPosition="4389"> climate -38.23 teach language -45.64 distance -45.11 show sample -31.75 travel -31.42 expect visit -33.88 mouth -32.87 answer request -31.89 tragedy -33.95 recognize author -32.53 pocket -32.62 repeat comment -33.80 journal -33.97 understand concept -32.25 session -32.93 remember reply -33.79 smoke -34.29 Figure 5: Compatibility scores for plausible vs. implausible verb-noun pairs. particular relationship between the verb and its argument. In particular, for verbs that participate in the implicit object alternation, the omitted object must be in some sense inferable or typical for that verb (Levin, 1993, among others). Resnik (1996) used his model of selectional preferences to analyze implicit object alternations, and showed a relationship between his measure of selectional preference strength and the notion of typicality of an object. He calculated this measure for two groups of Alternating and Non-alternating verbs, and showed that, on average, the Alternating verbs have a higher strength of selectional preference for the direct object than the Non-alternating verbs. However, there was no threshold separating the two groups of verbs. To repeat Resnik’s experiment, we need a measure of how </context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Levin, B. (1993). English verb classes and alternations: A preliminary investigation. The University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Li</author>
<author>N Abe</author>
</authors>
<title>Generalizing case frames using a thesaurus and the MDL principle.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="7467" citStr="Li and Abe (1998)" startWordPosition="1150" endWordPosition="1153">preferences is, therefore, modeled as using a set of training data to estimate that number. Resnik (1996) defines the selectional preference strength of a verb as the divergence between two probability distributions: the prior probabilities of the classes, and the posterior probabilities of the classes given that verb. The selectional association of a verb with a class is also defined as the contribution of that class to the total selectional preference strength. Resnik estimates the prior and posterior probabilities based on the frequencies of each verb and its relevant argument in a corpus. Li and Abe (1998) model selectional preferences of a verb (for an argument position) as a set of nodes in the semantic class hierarchy with a probability distribution over them. They use the Minimum Description Length (MDL) principle to find the best set for each verb and argument based on the usages of that verb in the training data. Clark and Weir (2002) also find an appropriate set of concept nodes to represent the selectional preferences for a verb, but do so using a x2 test over corpus frequencies mapped to concepts to determine when to generalize from a node to its parent. Ciaramita and Johnson (2000) us</context>
<context position="18313" citStr="Li and Abe, 1998" startWordPosition="2920" endWordPosition="2923">t is impractical to compile by hand. The training data for our experiments here are generated as follows. We use 20,000 sentences randomly selected from the British National Corpus (BNC),4 automatically parsed using the Collins parser (Collins, 1999), and further processed with TGrep2,5 and an NP-head extraction software.6 For 2To our knowledge, the only work that considers selectional preferences of subjects and prepositional phrases as well as direct objects is Brockmann and Lapata (2003). 3Computational models of verb selectional preference have been evaluated through disambiguation tasks (Li and Abe, 1998; Abney and Light, 1999; Ciaramita and Johnson, 2000; Clark and Weir, 2002), but for to evaluate our cognitive model, the experiments from Resnik (1996) are the most interesting. 4http://www.natcorp.ox.ac.uk 5http://tedlab.mit.edu/—dr/Tgrep2 6The software was provided to us by Eric Joanis, and Afeach verb usage in a sentence, we construct a frame by recording the verb in root form, the number of the arguments for that verb, and the syntactic pattern of the verb usage (i.e., the word order of the verb and the arguments). We also record in the frame the semantic properties of the verb and each o</context>
</contexts>
<marker>Li, Abe, 1998</marker>
<rawString>Li, H. and Abe, N. (1998). Generalizing case frames using a thesaurus and the MDL principle. Computational Linguistics, 24(2):217–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Light</author>
<author>W Greiff</author>
</authors>
<title>Statistical models for the induction and use of selectional preferences.</title>
<date>2002</date>
<journal>Cognitive Science,</journal>
<volume>26</volume>
<issue>3</issue>
<contexts>
<context position="6822" citStr="Light and Greiff, 2002" startWordPosition="1047" endWordPosition="1050">y noisy. However, as a result of our novel representation, the model can induce and use selectional preferences using a relatively small set of noisy training data. 2 Related Computational Models A variety of computational models for verb selectional preferences have been proposed, which use different statistical models to induce the preferences of each verb from corpus data. Most of these models, however, use the same representation for verb selectional preferences: the preference can be thought of as a mapping, with respect to an argument position for a verb, of each class to a real number (Light and Greiff, 2002). The induction of a verb’s preferences is, therefore, modeled as using a set of training data to estimate that number. Resnik (1996) defines the selectional preference strength of a verb as the divergence between two probability distributions: the prior probabilities of the classes, and the posterior probabilities of the classes given that verb. The selectional association of a verb with a class is also defined as the contribution of that class to the total selectional preference strength. Resnik estimates the prior and posterior probabilities based on the frequencies of each verb and its rel</context>
</contexts>
<marker>Light, Greiff, 2002</marker>
<rawString>Light, M. and Greiff, W. (2002). Statistical models for the induction and use of selectional preferences. Cognitive Science, 26(3):269–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacWhinney</author>
</authors>
<title>The CHILDESproject: Tools for analyzing talk. Lawrence Erlbaum.</title>
<date>1995</date>
<contexts>
<context position="17547" citStr="MacWhinney, 1995" startWordPosition="2804" endWordPosition="2805"> a qualitative analysis of our model through examination of the semantic profiles for a number of verbs. We then evaluate our model through two tasks of simulating verb-argument plausibility judgment, and analyzing the implicit object alternation, following Resnik (1996).3 4.1 The Training Data In earlier work (Alishahi and Stevenson, 2005, 2007), we used a method to automatically generate training data with the same distributional properties as the input children receive. However, this relies on manually-compiled data about verbs and their argument structure frames from the CHILDES database (MacWhinney, 1995). To evaluate the new version of our model for the task of learning selectional preferences, we need a wide selection of verbs and their arguments that is impractical to compile by hand. The training data for our experiments here are generated as follows. We use 20,000 sentences randomly selected from the British National Corpus (BNC),4 automatically parsed using the Collins parser (Collins, 1999), and further processed with TGrep2,5 and an NP-head extraction software.6 For 2To our knowledge, the only work that considers selectional preferences of subjects and prepositional phrases as well as </context>
</contexts>
<marker>MacWhinney, 1995</marker>
<rawString>MacWhinney, B. (1995). The CHILDESproject: Tools for analyzing talk. Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal ofLexicography,</journal>
<volume>17</volume>
<issue>3</issue>
<contexts>
<context position="5311" citStr="Miller, 1990" startWordPosition="803" endWordPosition="804">e semantic properties of an argument for each verb. A verb semantic profile is predicted from both the verb-based and the construction-based knowledge that the model has learned through clustering, and reflects the properties of the arguments that are observed for that verb. Our proposed prediction model makes appropriate generalizations over the observed properties, and captures expectations about previously unseen arguments. As in other work on selectional preferences, the semantic properties that we use in our representation of arguments are drawn from a standard lexical ontology (WordNet; Miller, 1990), but we do not require knowledge of the hierarchical structure of the WordNet concepts. From the computational point of view, this makes use of an available resource, while from the cognitive view, this avoids ad hoc assumptions about the representation of a conceptual hierarchy. However, we do require some properties to be more general (i.e., shared by more words) than others, which eventually enables the model to make appropriate generalizations. Otherwise, the selected semantic properties are not fundamental to the model, and could in the future be replaced with an approach that is deemed </context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>Miller, G. (1990). WordNet: An on-line lexical database. International Journal ofLexicography, 17(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Nation</author>
<author>C M Marshall</author>
<author>G T M Altmann</author>
</authors>
<title>Investigating individual differences in children’s real-time sentence comprehension using language-mediated eye movements.</title>
<date>2003</date>
<journal>J. of Experimental Child Psych.,</journal>
<pages>86--314</pages>
<contexts>
<context position="1368" citStr="Nation et al., 2003" startWordPosition="194" endWordPosition="197"> of noisy training data, and can use them in simulating human plausibility judgments and analyzing implicit object alternation. 1 Introduction Verbs have preferences for the semantic properties of the arguments filling a particular role. For example, the verb eat expects that the object receiving its theme role will have the property of being edible, among others. Learning verb selectional preferences is an important aspect of human language acquisition, and the acquired preferences have been shown to guide children’s expectations about missing or upcoming arguments in language comprehension (Nation et al., 2003). Resnik (1996) introduced a statistical approach to learning and use of verb selectional preferences. In this framework, a semantic class hierarchy for words is used, together with statistical tools, to induce a verb’s selectional preferences for a particular argument position in the form of a distribution 41 over all the classes that can occur in that position. Resnik’s model was proposed as a model of human learning of selectional preferences that made minimal representational assumptions; it showed how such preferences could be acquired from usage data and an existing conceptual hierarchy.</context>
</contexts>
<marker>Nation, Marshall, Altmann, 2003</marker>
<rawString>Nation, K., Marshall, C. M., and Altmann, G. T. M. (2003). Investigating individual differences in children’s real-time sentence comprehension using language-mediated eye movements. J. of Experimental Child Psych., 86:314–329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Selectional constraints: An informationtheoretic model and its computational realization.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--127</pages>
<contexts>
<context position="1383" citStr="Resnik (1996)" startWordPosition="198" endWordPosition="199">a, and can use them in simulating human plausibility judgments and analyzing implicit object alternation. 1 Introduction Verbs have preferences for the semantic properties of the arguments filling a particular role. For example, the verb eat expects that the object receiving its theme role will have the property of being edible, among others. Learning verb selectional preferences is an important aspect of human language acquisition, and the acquired preferences have been shown to guide children’s expectations about missing or upcoming arguments in language comprehension (Nation et al., 2003). Resnik (1996) introduced a statistical approach to learning and use of verb selectional preferences. In this framework, a semantic class hierarchy for words is used, together with statistical tools, to induce a verb’s selectional preferences for a particular argument position in the form of a distribution 41 over all the classes that can occur in that position. Resnik’s model was proposed as a model of human learning of selectional preferences that made minimal representational assumptions; it showed how such preferences could be acquired from usage data and an existing conceptual hierarchy. However, his a</context>
<context position="6955" citStr="Resnik (1996)" startWordPosition="1071" endWordPosition="1072"> noisy training data. 2 Related Computational Models A variety of computational models for verb selectional preferences have been proposed, which use different statistical models to induce the preferences of each verb from corpus data. Most of these models, however, use the same representation for verb selectional preferences: the preference can be thought of as a mapping, with respect to an argument position for a verb, of each class to a real number (Light and Greiff, 2002). The induction of a verb’s preferences is, therefore, modeled as using a set of training data to estimate that number. Resnik (1996) defines the selectional preference strength of a verb as the divergence between two probability distributions: the prior probabilities of the classes, and the posterior probabilities of the classes given that verb. The selectional association of a verb with a class is also defined as the contribution of that class to the total selectional preference strength. Resnik estimates the prior and posterior probabilities based on the frequencies of each verb and its relevant argument in a corpus. Li and Abe (1998) model selectional preferences of a verb (for an argument position) as a set of nodes in</context>
<context position="17201" citStr="Resnik (1996)" startWordPosition="2753" endWordPosition="2754">the sum of the sem score’s (equation (5)) for jn and every frame of v that participates in construction k. 4 Experimental Results In the following sections, we first describe the training data for our model. In accordance with other computational models, we focus here on the verb preferences for the direct object position.2 Next, we provide a qualitative analysis of our model through examination of the semantic profiles for a number of verbs. We then evaluate our model through two tasks of simulating verb-argument plausibility judgment, and analyzing the implicit object alternation, following Resnik (1996).3 4.1 The Training Data In earlier work (Alishahi and Stevenson, 2005, 2007), we used a method to automatically generate training data with the same distributional properties as the input children receive. However, this relies on manually-compiled data about verbs and their argument structure frames from the CHILDES database (MacWhinney, 1995). To evaluate the new version of our model for the task of learning selectional preferences, we need a wide selection of verbs and their arguments that is impractical to compile by hand. The training data for our experiments here are generated as follows</context>
<context position="18465" citStr="Resnik (1996)" startWordPosition="2946" endWordPosition="2947">e British National Corpus (BNC),4 automatically parsed using the Collins parser (Collins, 1999), and further processed with TGrep2,5 and an NP-head extraction software.6 For 2To our knowledge, the only work that considers selectional preferences of subjects and prepositional phrases as well as direct objects is Brockmann and Lapata (2003). 3Computational models of verb selectional preference have been evaluated through disambiguation tasks (Li and Abe, 1998; Abney and Light, 1999; Ciaramita and Johnson, 2000; Clark and Weir, 2002), but for to evaluate our cognitive model, the experiments from Resnik (1996) are the most interesting. 4http://www.natcorp.ox.ac.uk 5http://tedlab.mit.edu/—dr/Tgrep2 6The software was provided to us by Eric Joanis, and Afeach verb usage in a sentence, we construct a frame by recording the verb in root form, the number of the arguments for that verb, and the syntactic pattern of the verb usage (i.e., the word order of the verb and the arguments). We also record in the frame the semantic properties of the verb and each of the argument heads (each noun is also converted to root form); these properties are extracted from WordNet (as discussed in Section 3.1 and illustrate</context>
<context position="24304" citStr="Resnik (1996)" startWordPosition="3843" endWordPosition="3844"> content (0.007) state (0.007) instrumentality (0.007) instrumentality (0.007) instrumentation (0.007) instrumentation (0.007) measure (0.007) event (0.006) amount (0.006) being (0.006) quantity (0.006) living thing (0.006) cause (0.006) animate thing (0.006) causal agent (0.006) organism (0.006) causalagency Figure 3: Semantic profiles of want and put for the direct object position. 4.3 Verb-Argument Plausibility Judgments Holmes et al. (1989) evaluate verb argument plausibility by asking human subjects to rate sentences like The mechanic warned the driver and The mechanic warned the engine. Resnik (1996) used this data to assess the performance of his model by comparing its judgments of selectional fit against the plausibility ratings elicited from human subjects. He showed that his selectional association measure for a verb and its direct object can be used to select the more plausible verb-noun pair among the two (e.g., &lt;warn,driver&gt; vs. &lt;warn,engine&gt; in the previous example). That is, a higher selectional association between the verb and one of the nouns compared to the other noun indicates that the former is the more plausible pair. Resnik (1996) used the Brown corpus as training data, an</context>
<context position="26267" citStr="Resnik (1996)" startWordPosition="4153" endWordPosition="4154">.008) individual (0.008) someone (0.008) somebody (0.008) mortal Figure 4: Semantic profile of a novel verb for the direct object position. log-probabilities and therefore negative numbers, a lower absolute value of compatibility(v, n) shows a better compatibility between the verb v and the argument n.) For example, &lt;see,friend&gt; has a higher compatibility score (-30.50) than &lt;see,method&gt; (-32.14). Similar to Resnik, our model detects 11 plausible pairs out of 16. However, these results are reached with a much smaller training corpus (around 500,000 words), compared to the Brown corpus used by Resnik (1996) which contains one million words. Moreover, whereas the Brown corpus is tagged and parsed manually, the portion of the BNC that we use is parsed automatically, and as a result our training data is very noisy. Nonetheless, the model achieves the same level of accuracy in distinguishing plausible verb-argument pairs from implausible ones. 4.4 Implicit Object Alternations In English, some inherently transitive verbs can appear with or without their direct objects (e.g., John ate his dinner as well as John ate), but others cannot (e.g., Mary made a cake but not *Mary made). It is argued that impl</context>
<context position="27823" citStr="Resnik (1996)" startWordPosition="4392" endWordPosition="4393">ge -45.64 distance -45.11 show sample -31.75 travel -31.42 expect visit -33.88 mouth -32.87 answer request -31.89 tragedy -33.95 recognize author -32.53 pocket -32.62 repeat comment -33.80 journal -33.97 understand concept -32.25 session -32.93 remember reply -33.79 smoke -34.29 Figure 5: Compatibility scores for plausible vs. implausible verb-noun pairs. particular relationship between the verb and its argument. In particular, for verbs that participate in the implicit object alternation, the omitted object must be in some sense inferable or typical for that verb (Levin, 1993, among others). Resnik (1996) used his model of selectional preferences to analyze implicit object alternations, and showed a relationship between his measure of selectional preference strength and the notion of typicality of an object. He calculated this measure for two groups of Alternating and Non-alternating verbs, and showed that, on average, the Alternating verbs have a higher strength of selectional preference for the direct object than the Non-alternating verbs. However, there was no threshold separating the two groups of verbs. To repeat Resnik’s experiment, we need a measure of how “strongly constraining” a sema</context>
<context position="30217" citStr="Resnik (1996)" startWordPosition="4794" endWordPosition="4795">ronger preferences. We use the cosine measure to estimate the similarity between two profiles p and q: cosine(p, q) = p × q(9) ||p ||× ||q|| The similarity values for the Alternating and Nonalternating verbs are shown in Figure 6. The larger values represent more similarity with the base profile, which means a weaker selectional preference. The means for the Alternating and Non-alternating verbs were respectively 0.76 and 0.81, which confirm the hypothesis that verbs participating in implicit object alternations select more strongly for the direct objects than verbs that do not. However, like Resnik (1996), we find that it is not possible to set a threshold that will distinguish the two sets of verbs. 5 Conclusions We have proposed a cognitively plausible model for learning selectional preferences from instances of verb usage. The model represents verb selectional preferences as a semantic profile, which is a probability distribution over the semantic properties that an argument can take. One of the strengths of our model is the incremental nature of its learning mechanism, in contrast to other approaches which learn selectional preferences in batch mode. Here we have only reported the results </context>
</contexts>
<marker>Resnik, 1996</marker>
<rawString>Resnik, P. (1996). Selectional constraints: An informationtheoretic model and its computational realization. Cognition, 61:127–199.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>