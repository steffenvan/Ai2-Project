<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012151">
<title confidence="0.988253">
Can Selectional Preferences Help Automatic Semantic Role Labeling?
</title>
<author confidence="0.99897">
Shumin Wu
</author>
<affiliation confidence="0.999873">
Department of Computer Science
University of Colorado Boulder
</affiliation>
<email confidence="0.996852">
shumin@colorado.edu
</email>
<author confidence="0.985611">
Martha Palmer
</author>
<affiliation confidence="0.9992935">
Department of Linguistics
University of Colorado Boulder
</affiliation>
<email confidence="0.998714">
mpalmer@colorado.edu
</email>
<sectionHeader confidence="0.995646" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999852">
We describe a topic model based approach for
selectional preference. Using the topic fea-
tures generated by an LDA model on the ex-
tracted predicate-arguments over the Chinese
Gigaword corpus, we show improvement to
our state-of-the-art Chinese SRL system by
2.34 F1 points on arguments of nominal pred-
icates, 0.40 F1 point on arguments of verb
predicates, and 0.66 F1 point overall. More
over, similar gains were achieved on out-of-
genre test data, as well as on English SRL us-
ing the same technique.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995042657894737">
It’s long been theorized that selectional preferences
(SP)/semantic constraints can improve automatic se-
mantic role labeling (SRL). And while there have
been several publications showing positive effects of
SP, the evaluations have been dominated by pseudo-
disambiguation. Zapirain et al. (2013) demonstrated
end-to-end SRL improvement on arguments of En-
glish verb predicates by using a combination of lex-
ical resources and distributional similarity based SP.
However, the margin of improvement is a modest
0.4 F1 point (on WSJ) over a baseline system with
performance over 4 F1 points lower than the top sys-
tem in CoNLL-2005 (Carreras and M`arquez, 2005).
These results may not be convincing enough to mo-
tivate the incorporation of SP when building an SRL
system. One reason for the small improvement may
be that arguments of a verb predicate are highly con-
strained by the underlying syntactic parse, and SP
features that could disambiguate between role types
222
are often negated by parse errors. With the recent
extension of PropBank SRL to nominal and adjec-
tive predicates, preposition relationships, light-verb
constructions, and abstract meaning representation
(Bonial et al., 2014; Banarescu et al., 2013), it may
be time to revisit SP for SRL. We hypothesize that
SP will provide a greater benefit to nominal SRL, es-
pecially on a language with lower parsing accuracy.
In this paper, we apply SP to Chinese SRL (which
has few morphological clues that impacts parsing
accuracy) for arguments of both verb and nominal
predicates using Chinese Gigaword. Our hypothe-
sis, that SP will provide a greater benefit for nomi-
nal predicates than for verbal predicates, is verified
by our results. We achieve a 2.34 F1 point improve-
ment to our Chinese SRL system on arguments of
nominal predicates, 0.40 F1 point on arguments of
verb predicates, and 0.66 F1 point overall.
</bodyText>
<sectionHeader confidence="0.941372" genericHeader="introduction">
2 Previous Work on Selectional Preference
</sectionHeader>
<bodyText confidence="0.976012384615385">
Inducing selectional preferences from corpus data
was first proposed by Resnik (1997) for sense dis-
ambiguation. He generalized seen words using the
WordNet (Fellbaum, 1998) hierarchy. Gildea and
Jurafsky (2002) applied SP to automatic SRL by
clustering extracted verb-direct object pairs, result-
ing in modest improvements. This syntactic signa-
ture based selectional preference technique has also
been successfully extended and applied to unsuper-
vised SRL by Lang and Lapata (2011) (using split-
merge role clustering), as well as Titov and Kle-
mentiev (2012) (using a distance-dependent Chinese
Restaurant Process prior for role clustering). Zapi-
rain et al. (2013) improved the end-to-end perfor-
Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 222–227,
Denver, Colorado, June 4–5, 2015.
mance of an English PropBank SRL system by 0.4
F1 points using a variety of word similarity mea-
sures, from WordNet hierarchy distance to distribu-
tional similarity measures.
Ritter and Etzioni (2010) reasoned that the set of
hidden variables modeled by latent Dirichlet alloca-
tion (LDA) naturally represents the semantic struc-
ture of a document collection, and the topics gener-
ated can be viewed as the latent set of classes that
store preferences. The work utilizes LinkLDA, a
variant of the standard LDA that models two sets of
distributions for each topic simultaneously, with the
resulting topics encoding the mutual constraints of a
pair of arguments for the same predicate. S´eaghdha
and Korhonen (2014) also proposed SP w/ the LDA
variants ROOTH-LDA and LEX-LDA.
There has also been work on Chinese selec-
tional preferences, both lexical resource (HowNet)
based and corpus based (Jia et al., 2011; Jia et al.,
2013). The authors found the LDA corpus based
SP improved over the HowNet based SP on pseudo-
disambiguation. All of these results encouraged us
to also attempt an LDA based approach to SP.
</bodyText>
<sectionHeader confidence="0.963252" genericHeader="method">
3 Selectional Preference for SRL
</sectionHeader>
<subsectionHeader confidence="0.995734">
3.1 SP Representation
</subsectionHeader>
<bodyText confidence="0.9997695">
Some of the most discriminative SP models used
by Zapirain et al. (2013) relied on distributional
similarity computed over dependency relationships
(provided by Lin (1998)). For example, in “John
lent Mary the book.”, we would extract John-nsubj,
Mary-iobj, book-dobj for the predicate lend. While
this has proven to be of higher quality than pure
word co-occurrence based similarity, it may not be
optimal for semantic-based processing. With nom-
inal SRL, a large portion of the arguments (around
50% in Chinese PropBank) are not the direct syntac-
tic dependents of the predicate: in figure 1, because
of a light verb-like construction, all the arguments
of &amp;quot;Î/welcome are the syntactic dependents of A
T/express. To address this, we directly extract SP
of the predicates by running our SRL system over
the unannotated corpus. For our example, we would
extract John-Arg0, Mary-Arg2, book-Arg1 for lend.
</bodyText>
<subsectionHeader confidence="0.999801">
3.2 SP with LDA-based Topic Model
</subsectionHeader>
<bodyText confidence="0.99999465">
Our approach to modeling selectional preferences
(SP) follows a relatively straightforward application
of LDA to a set of predicate-argument instances de-
rived from a corpus. In the standard LDA model, a
document d is represented by a bag of words and is
drawn from a multi-nominal Dirichlet Bd over top-
ics. The resulting model is a probability distribution
of each word amongst the topics.
For the SRL application, we treat each extracted
argument (represented by the (label, headword)
pair) as a “word”, and the collection of arguments
for all instances of a particular predicate as a “doc-
ument”. The generated topics would then contain
arguments sharing a similar set of predicates. With
this definition, we allow different role labels to share
the same topic (though it does not encode role con-
straints quite like LinkLDA, ROOTH-LDA, etc).
For prepositional phrases, we used the dependent of
the preposition as the head word since the preposi-
tion can often be omitted in Chinese.
</bodyText>
<subsectionHeader confidence="0.998996">
3.3 SRL Filtering
</subsectionHeader>
<bodyText confidence="0.999960764705882">
Building selectional preferences by means of using
the output of an SRL system is unlikely to improve
the same SRL system unless one filters out the lower
quality labels (in earlier experiments where we per-
formed no filtering, this was indeed the case). We
ran SRL on the unannotated corpus using a logistic
regression model and filtered out the low probability
output. To balance between precision and recall, we
set a hard 0.5 probability cutoff and discounted the
occurrences of the rest using the label probability.
Since we can extract higher quality SP from the
output of a better performing SRL system, we can
iteratively improve our SRL system by re-extracting
SP using a retrained (SP enhanced) SRL system. We
arrived at diminishing returns after one additional it-
eration (of training SRL, extracting SP, and retrain-
ing SRL w/ new SP).
</bodyText>
<sectionHeader confidence="0.997538" genericHeader="method">
4 SRL Implementation
</sectionHeader>
<bodyText confidence="0.999313">
Our Chinese SRL system follows the standard (En-
glish) approach where the SRL task is posed as
a multi-class classification problem requiring the
identification of argument candidates for each pred-
icate and their argument types using a set of lexical
</bodyText>
<page confidence="0.989735">
223
</page>
<figure confidence="0.82953">
A0 AM-tmp A1 Sup V
[�� �� [��] [� �� ��� �� � �� ��] [��]Hong Kong official Dong Jianhua today toward US foundation post economic report express welcome
[AM-tmp Today], [A0 Hong Kong official Dong Jianhua] [V welcomed] [A1 the economic report released by the US foundation].
</figure>
<figureCaption confidence="0.99967">
Figure 1: Chinese nominal predicate translated to English verb predicate
</figureCaption>
<bodyText confidence="0.999879666666667">
and syntactic features (predicate word, constituent
head, path, syntactic frame, etc). While the top SRL
systems from CoNLL-20051 and some subsequent
systems use multiple parses for structural inference,
we instead implement a 2-stage argument label clas-
sification system on a single input parse: the argu-
ment set found by the first classifier is used as an
additional feature for the second classifier (to iden-
tify missing or duplicate argument label types).
</bodyText>
<subsectionHeader confidence="0.997215">
4.1 Selectional Preference
</subsectionHeader>
<bodyText confidence="0.978198909090909">
The LDA topic model produces a probability dis-
tribution of words (represented here by the (label,
headword) pair) over topics. For the SRL task, ar-
gument candidates with topic distributions similar to
those of the arguments found in the training set are
likely to be permissible. Ideally, we would use these
distributions directly. Since our SRL system was de-
signed to accept lexical (binary) features only (for
training/decoding performance), we pared the distri-
bution down to at most 3 topics for each label type
and excluded words that do not have high affinity
to a few topics (sum of the probability of the top 3
topics &lt; 50%) to prevent diluting the discriminative
power of the topic feature. We used the resulting
list of (label, topic id) pairs for each word as the
selectional preference feature for each encountered
constituent in the Chinese SRL system.
During the normal LDA inference stage, using
the learned topic model, a predicate instance (“doc-
ument”) will be assigned a probability distribution
over topics based on its arguments, and each argu-
ment will be assigned a specific topic (or topic distri-
bution). This could further constrain an argument’s
selectional preference within the context of the pred-
icate instance and other arguments. For our system,
we experimented with performing inference on the
argument label set extracted from the first stage clas-
sifier and using the constrained argument topic dis-
1We use CoNLL-2005 instead of CoNLL-2009 for compar-
ison because our SRL system is based on constituent parses.
tribution for the second stage classifier. However,
we observed no improvement, likely because there
are only a few arguments for each predicate instance.
</bodyText>
<sectionHeader confidence="0.998479" genericHeader="evaluation">
5 Experiment
</sectionHeader>
<subsectionHeader confidence="0.942338">
5.1 Setup
</subsectionHeader>
<bodyText confidence="0.999635894736842">
Our Chinese SRL system is trained on Chinese Tree-
Bank 5.1 and Chinese PropBank 1.0. We used the
standard: sections 81-885 for training, sections 41-
80 for development, and sections 1-40, 900-931 for
testing. We generated the training parses (with 10
fold cross-validation) and the test parses using the
Berkeley parser2 (5 split-merge cycles). The parser
F1 score on the test sections is 82.73 as measured by
ParseEval (Black et al., 1991).
We prepared the Chinese Gigaword3 corpus with
the Stanford Chinese Word Segmenter4. We per-
formed LDA topic modeling using PLDA+ (Liu et
al., 2011) and the recommended α = 50/topic cnt,
Q = 0.01 values. We chose 2000 topics (tuned on
the SRL performance of the development set rather
than any topic based metrics). Table 1 lists some of
the found topics (with the most frequent, relatively
interesting, and least frequent headword, label pairs)
using Chinese Gigaword.
</bodyText>
<subsectionHeader confidence="0.991578">
5.2 Performance
</subsectionHeader>
<bodyText confidence="0.999713375">
As table 2 shows, the addition of the SP feature im-
proved nominal SRL by 2.34 F1 points. Verb SRL
improved by 0.40 F1 point and overall SRL im-
proved by 0.66 F1 point. These F1 differences were
all found to be statistically significant5 (p ≤ 0.05).
We also tested the system on Sinorama magazine
and other out-of-genre sections (broadcast conver-
sation, broadcast news, web blog) in Chinese Prop-
</bodyText>
<footnote confidence="0.9522376">
2code.google.com/p/berkeleyparser/
3LDC2011T13
4nlp.stanford.edu/software/segmenter.shtml
5SIGF (www.nlpado.de/%7esebastian/software/sigf.shtml),
using stratified approximate randomization test (Yeh, 2000)
</footnote>
<page confidence="0.988242">
224
</page>
<table confidence="0.999862055555556">
topic headword:argument label pairs
emergency 169,/damage:Arg1 PJIi:/stop:Arg1 6 /fabricate:Arg1 4Wsearch:Arg1 Ny;/suicide:Arg1
response ... Jk/extinguish:Arg1 MiWblackmail:Arg1 *1/break free:Arg1 �LU4-4Wcomeback:Arg1
government � �/custom:Arg0 0 � /union:Arg0 � è/work department:Arg0 � 8
agency P/travel department:Arg0 � it P/census:Arg0 ... è /ministries:Arg0 1 �
k&amp;/checkpoint:Arg0 #61A/finance bureau:Arg0
law &amp; � �/police:Arg0 � �/suspect:Arg1 7 �/male:Arg1 0 �/court appearance:Arg1 �
order �/public safety:Arg0 ... �/alley:Argm-loc � � �/Chiayi City:Argm-loc � f � �
,/Columbian:Arg1
path S M/road:Arg1 M/path:Arg1 )Q S/avenue:Arg1 ... YZ �t ?A/red carpet:Arg1 C
ft/steel wire:Arg1 ì*e/plank bridge:Arg1 ... Ag/maze:Arg1 1Rj Jè/side entrance:Arg1
f4/risky move:Arg1
competition }t /competition:Arg1 W /final:Arg1 0 /league comp:Arg1 ... 5� �/exam:Arg1 )Q
lc/election:Arg1 t SE /world pingpong match:Arg1 ... ft /playoff:Arg1 5t Q/sub-
group:Arg0
moral &amp; � �/spirit:Arg1 �/tradition:Arg1 � �/style:Arg1 X �/civil:Arg1 ... �
ethics �/school spirit:Arg1 � � � 9F/share hard time:Arg1 ... * � �/happy outlook:Arg1 �
1/universal love:Arg1
</table>
<tableCaption confidence="0.999634">
Table 1: Topics in Chinese Gigaword
</tableCaption>
<table confidence="0.99626825">
system nominal verb all
p r f1 f1 f1
baseline 64.71 48.20 55.25 75.53 72.08
SPLDA 65.70 51.27 57.59 75.93 72.74
</table>
<tableCaption confidence="0.877977">
Table 2: Chinese PropBank 1.0 results
</tableCaption>
<table confidence="0.998617571428571">
sections system p r f1
Sinorama baseline 37.58 25.10 30.10
nominal SPLDA 39.72 27.36 32.40
verb baseline 67.13 50.37 57.55
SPLDA 67.56 50.59 57.86
4051- baseline 62.01 50.74 55.81
4411 (verb) SPLDA 62.70 51.03 56.27
</table>
<tableCaption confidence="0.999477">
Table 3: Chinese PropBank 3.0 out-of-genre results
</tableCaption>
<bodyText confidence="0.999410285714286">
Bank 3.0. Only Sinorama has nominal SRL anno-
tations. As table 3 shows, even though the absolute
performance is much lower, SP improved the preci-
sion and recall in all cases, the nominal SRL score
on Sinorama by 2.30 F1 points, and verb SRL score
by 0.31-0.46 F1 point. Again, these F1 differences
were statistically significant.
</bodyText>
<subsectionHeader confidence="0.798309">
5.2.1 Comparison
</subsectionHeader>
<bodyText confidence="0.999979666666667">
Direct performance comparison with previous
Chinese SRL systems is a bit difficult: Xue (2008),
Zhuang and Zong (2010) trained the syntactic
parsers with an additional 250K word broadcast
news corpus found in Chinese TreeBank 6.0, while
Sun (2010) only reported results using gold POS
tags but no additional gold parses. However, as ta-
ble 4 shows, for verb predicates, our system bests
Xue’s (2008) system by 4-7 F1 points with less
parser training data and when tested with (but was
not retrained to take full advantage of) gold POS tags
besting Sun’s (2010) system by 0.53 F1 point. For
nominal predicates, our system bests Xue’s (2008)
system, by 1.9 F1 points on arguments of nominal
predicates (since we have an integrated SRL sys-
tem, the results are obtained by training both verb
and nominal predicates, then using only the nominal
classifier to classify the nominal predicates).
</bodyText>
<subsectionHeader confidence="0.654817">
5.2.2 English SRL
</subsectionHeader>
<bodyText confidence="0.998359666666667">
We applied the same techniques to English SRL
using the English Gigaword7 corpus. We used 800
topics (w/ lemmatized headwords) tuning on the
</bodyText>
<footnote confidence="0.464466">
6Verb results are from SRL systems trained on verbs only.
Table 2 results are from SRL systems trained on all predicates.
7LDC2003T05
</footnote>
<page confidence="0.972463">
225
</page>
<table confidence="0.999673">
type system p r f1
Xue 2008 76.8 62.5 68.9
w/ gold POS 79.5 65.6 71.9
verb Sun 2010 81.03 72.38 76.46
(gold POS)
SPLDA 82.74 70.96 76.40
w/ gold POS 82.81 71.93 76.99
nominal Xue 2008 62.9 53.1 57.6
SPLDA 67.30 53.31 59.50
</table>
<tableCaption confidence="0.964768">
Table 4: Chinese SRL comparison6
</tableCaption>
<table confidence="0.9994378">
system p r f1 errorΔ
SwiRL 79.7 70.9 75.0
Zapirain 2013 80.0 71.3 75.4 −1.60%
baseline 82.59 77.27 79.84
SPLDA 82.96 77.52 80.15 −1.54%
</table>
<tableCaption confidence="0.999753">
Table 5: English SRL comparison (CoNLL-2005 WSJ)
</tableCaption>
<bodyText confidence="0.999110125">
CoNLL-2005 development set. Compared to Zapi-
rain et al. (2013) (table 5), our SP approach had a
smaller (but still statistically significant) absolute F1
gain, with most of the gain coming from core argu-
ment type improvements. But with a much higher
performing baseline system (one of the highest re-
ported results using a single input parse per sen-
tence), the error reduction rate is comparable.
</bodyText>
<sectionHeader confidence="0.996155" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999953115384616">
We presented a LDA topic model based selectional
preference approach to improving automatic SRL.
Using SP extracted from a 63.6M sentence Chinese
Gigaword corpus, we were able to improve on the
results of an already competitive Chinese SRL sys-
tem by 2.34 F1 points on nominal predicates, 0.40
F1 point on verb predicates, and 0.66 F1 point on the
standard test set. More over, we obtained compara-
ble improvement on out-of-genre data and demon-
strated our technique is also applicable to English
SRL. Given the margin of improvement on nomi-
nal SRL, which is not as well constrained by syntax
as verb SRL, there are reasons to speculate the pro-
posed technique could be applicable to other predi-
cate type extensions of PropBank SRL.
As our first attempt at automatically deriving Chi-
nese selectional preference, there is a lot of room
for future improvement. Notably, these include
techniques used for English SP such as computing
similarity based on lexical resources (for Chinese
- HowNet (Dong et al., 2010)), distributional sim-
ilarity, latent word language model (Deschacht and
Moens, 2009), different variants of LDA topic mod-
els, as well as taking advantages of argument con-
straints in parallel corpora to extract higher quality
SP.
</bodyText>
<sectionHeader confidence="0.734975" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999453352941177">
We gratefully acknowledge the support of the
National Science Foundation CISE-IISRI-0910992,
Richer Representations for Machine Translation,
DARPA FA8750-09-C-0179 (via BBN) Machine
Reading: Ontology Induction: Semlink+, and
DARPA HR0011-11-C-0145 (via LDC) BOLT. This
work utilized the Janus supercomputer, which is
supported by the National Science Foundation
(award number CNS-0821794) and the University
of Colorado Boulder. The Janus supercomputer is
a joint effort of the University of Colorado Boulder,
the University of Colorado Denver and the National
Center for Atmospheric Research. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the National
Science Foundation.
</bodyText>
<sectionHeader confidence="0.998573" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999392789473684">
Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider, 2013. Proceedings of the 7th Linguistic
Annotation Workshop and Interoperability with Dis-
course, chapter Abstract Meaning Representation for
Sembanking, pages 178–186. Association for Com-
putational Linguistics.
E. Black, S. Abney, S. Flickenger, C. Gdaniec, C. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. Procedure
for quantitatively comparing the syntactic coverage of
english grammars. In Proceedings of the Workshop on
Speech and Natural Language, HLT ’91, pages 306–
311, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Claire Bonial, Julia Bonn, Kathryn Conger, Jena D.
Hwang, and Martha Palmer. 2014. Propbank: Se-
</reference>
<page confidence="0.993619">
226
</page>
<reference confidence="0.992880822222222">
mantics of new predicate types. In Proceedings of
the Ninth International Conference on Language Re-
sources and Evaluation (LREC’14), Reykjavik, Ice-
land, may.
Xavier Carreras and Llu´ıs M`arquez. 2005. Introduc-
tion to the conll-2005 shared task: Semantic role la-
beling. In Proceedings of the Ninth Conference on
Computational Natural Language Learning, CONLL
’05, pages 152–164, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Koen Deschacht and Marie-Francine Moens. 2009.
Semi-supervised semantic role labeling using the la-
tent words language model. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing: Volume 1 - Volume 1, EMNLP
’09, pages 21–29, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Zhendong Dong, Qiang Dong, and Changling Hao. 2010.
Hownet and its computation of meaning. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics: Demonstrations, COLING ’10,
pages 53–56, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245–288.
Yuxiang Jia, Hongying Zan, and Ming Fan. 2011. Induc-
ing chinese selectional preference based on hownet. In
Proceedings of the Seventh International Conference
on Computational Intelligence and Security, CIS2011,
pages 1146–1149.
Yuxiang Jia, Hongying Zan, Ming Fan, , Shiwen Yu, and
Zhimin Wang. 2013. Computational models for chi-
nese selectional preferences induction. International
Journal of Advanced Intelligence, 5(1):110–119, July.
Joel Lang and Mirella Lapata. 2011. Unsupervised se-
mantic role induction via split-merge clustering. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of ACL 1998, ACL
’98, pages 768–774, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Zhiyuan Liu, Yuzhou Zhang, Edward Y. Chang, and
Maosong Sun. 2011. Plda+: Parallel latent dirich-
let allocation with data placement and pipeline pro-
cessing. ACM Trans. Intell. Syst. Technol., 2(3):26:1–
26:18, May.
Philip Resnik. 1997. Selectional preference and sense
disambiguation. In Proceedings of ACL SIGLEX
Workshop on Tagging Text with Lexical Semantics,
pages 52–57, Washington, D.C. ACL.
Alan Ritter and Oren Etzioni. 2010. A latent dirichlet
allocation method for selectional preferences. In In
Proceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL).
Diarmuid S´eaghdha and Anna Korhonen. 2014. Prob-
abilistic distributional semantics with latent variable
models. Computational Linguistics, 40(3):587–631,
September.
Weiwei Sun. 2010. Semantics-driven shallow parsing for
chinese semantic role labeling. In Proceedings of the
ACL 2010 Conference Short Papers, ACLShort ’10,
pages 103–108, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ivan Titov and Alexandre Klementiev. 2012. A Bayesian
approach to unsupervised semantic role induction. In
Proceedings of the Conference of the European Chap-
ter of the Association for Computational Linguistics,
Avignon, France, April.
Nianwen Xue. 2008. Labeling chinese predicates
with semantic roles. Computational Linguistics,
34(2):225–255.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of the 18th Conference on Computational Linguistics -
Volume 2, COLING ’00, pages 947–953, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Be˜nat Zapirain, Eneko Agirre, Llu´ıs M`arquez, and Mihai
Surdeanu. 2013. Selectional preferences for seman-
tic role classification. In Computational Linguistics,
pages 631–663.
Tao Zhuang and Chengqing Zong. 2010. Joint inference
for bilingual semantic role labeling. In Proceedings of
EMNLP 2010, pages 304–314, Cambridge, MA, Oc-
tober. Association for Computational Linguistics.
</reference>
<page confidence="0.997942">
227
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.874946">
<title confidence="0.996953">Can Selectional Preferences Help Automatic Semantic Role Labeling?</title>
<author confidence="0.892922">Shumin</author>
<affiliation confidence="0.999921">Department of Computer University of Colorado</affiliation>
<email confidence="0.996807">shumin@colorado.edu</email>
<author confidence="0.993826">Martha</author>
<affiliation confidence="0.999714">Department of University of Colorado</affiliation>
<email confidence="0.999739">mpalmer@colorado.edu</email>
<abstract confidence="0.999278384615385">We describe a topic model based approach for selectional preference. Using the topic features generated by an LDA model on the extracted predicate-arguments over the Chinese Gigaword corpus, we show improvement to our state-of-the-art Chinese SRL system by 2.34 F1 points on arguments of nominal predicates, 0.40 F1 point on arguments of verb predicates, and 0.66 F1 point overall. More over, similar gains were achieved on out-ofgenre test data, as well as on English SRL using the same technique.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Laura Banarescu</author>
<author>Claire Bonial</author>
<author>Shu Cai</author>
<author>Madalina Georgescu</author>
<author>Kira Griffitt</author>
<author>Ulf Hermjakob</author>
<author>Kevin Knight</author>
<author>Philipp Koehn</author>
<author>Martha Palmer</author>
<author>Nathan Schneider</author>
</authors>
<date>2013</date>
<booktitle>Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, chapter Abstract Meaning Representation for Sembanking,</booktitle>
<pages>178--186</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1986" citStr="Banarescu et al., 2013" startWordPosition="299" endWordPosition="302">r than the top system in CoNLL-2005 (Carreras and M`arquez, 2005). These results may not be convincing enough to motivate the incorporation of SP when building an SRL system. One reason for the small improvement may be that arguments of a verb predicate are highly constrained by the underlying syntactic parse, and SP features that could disambiguate between role types 222 are often negated by parse errors. With the recent extension of PropBank SRL to nominal and adjective predicates, preposition relationships, light-verb constructions, and abstract meaning representation (Bonial et al., 2014; Banarescu et al., 2013), it may be time to revisit SP for SRL. We hypothesize that SP will provide a greater benefit to nominal SRL, especially on a language with lower parsing accuracy. In this paper, we apply SP to Chinese SRL (which has few morphological clues that impacts parsing accuracy) for arguments of both verb and nominal predicates using Chinese Gigaword. Our hypothesis, that SP will provide a greater benefit for nominal predicates than for verbal predicates, is verified by our results. We achieve a 2.34 F1 point improvement to our Chinese SRL system on arguments of nominal predicates, 0.40 F1 point on ar</context>
</contexts>
<marker>Banarescu, Bonial, Cai, Georgescu, Griffitt, Hermjakob, Knight, Koehn, Palmer, Schneider, 2013</marker>
<rawString>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider, 2013. Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, chapter Abstract Meaning Representation for Sembanking, pages 178–186. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>E Black</author>
<author>S Abney</author>
<author>S Flickenger</author>
<author>C Gdaniec</author>
<author>C Grishman</author>
<author>P Harrison</author>
<author>D Hindle</author>
<author>R Ingria</author>
<author>F Jelinek</author>
<author>J Klavans</author>
<author>M Liberman</author>
<author>M Marcus</author>
<author>S Roukos</author>
<author>B Santorini</author>
<author>T Strzalkowski</author>
</authors>
<title>Procedure for quantitatively comparing the syntactic coverage of english grammars.</title>
<date>1991</date>
<booktitle>In Proceedings of the Workshop on Speech and Natural Language, HLT ’91,</booktitle>
<pages>306--311</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="10726" citStr="Black et al., 1991" startWordPosition="1709" endWordPosition="1712"> tribution for the second stage classifier. However, we observed no improvement, likely because there are only a few arguments for each predicate instance. 5 Experiment 5.1 Setup Our Chinese SRL system is trained on Chinese TreeBank 5.1 and Chinese PropBank 1.0. We used the standard: sections 81-885 for training, sections 41- 80 for development, and sections 1-40, 900-931 for testing. We generated the training parses (with 10 fold cross-validation) and the test parses using the Berkeley parser2 (5 split-merge cycles). The parser F1 score on the test sections is 82.73 as measured by ParseEval (Black et al., 1991). We prepared the Chinese Gigaword3 corpus with the Stanford Chinese Word Segmenter4. We performed LDA topic modeling using PLDA+ (Liu et al., 2011) and the recommended α = 50/topic cnt, Q = 0.01 values. We chose 2000 topics (tuned on the SRL performance of the development set rather than any topic based metrics). Table 1 lists some of the found topics (with the most frequent, relatively interesting, and least frequent headword, label pairs) using Chinese Gigaword. 5.2 Performance As table 2 shows, the addition of the SP feature improved nominal SRL by 2.34 F1 points. Verb SRL improved by 0.40</context>
</contexts>
<marker>Black, Abney, Flickenger, Gdaniec, Grishman, Harrison, Hindle, Ingria, Jelinek, Klavans, Liberman, Marcus, Roukos, Santorini, Strzalkowski, 1991</marker>
<rawString>E. Black, S. Abney, S. Flickenger, C. Gdaniec, C. Grishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Klavans, M. Liberman, M. Marcus, S. Roukos, B. Santorini, and T. Strzalkowski. 1991. Procedure for quantitatively comparing the syntactic coverage of english grammars. In Proceedings of the Workshop on Speech and Natural Language, HLT ’91, pages 306– 311, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Bonial</author>
<author>Julia Bonn</author>
<author>Kathryn Conger</author>
<author>Jena D Hwang</author>
<author>Martha Palmer</author>
</authors>
<title>Propbank: Semantics of new predicate types.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14),</booktitle>
<location>Reykjavik, Iceland,</location>
<contexts>
<context position="1961" citStr="Bonial et al., 2014" startWordPosition="295" endWordPosition="298">over 4 F1 points lower than the top system in CoNLL-2005 (Carreras and M`arquez, 2005). These results may not be convincing enough to motivate the incorporation of SP when building an SRL system. One reason for the small improvement may be that arguments of a verb predicate are highly constrained by the underlying syntactic parse, and SP features that could disambiguate between role types 222 are often negated by parse errors. With the recent extension of PropBank SRL to nominal and adjective predicates, preposition relationships, light-verb constructions, and abstract meaning representation (Bonial et al., 2014; Banarescu et al., 2013), it may be time to revisit SP for SRL. We hypothesize that SP will provide a greater benefit to nominal SRL, especially on a language with lower parsing accuracy. In this paper, we apply SP to Chinese SRL (which has few morphological clues that impacts parsing accuracy) for arguments of both verb and nominal predicates using Chinese Gigaword. Our hypothesis, that SP will provide a greater benefit for nominal predicates than for verbal predicates, is verified by our results. We achieve a 2.34 F1 point improvement to our Chinese SRL system on arguments of nominal predic</context>
</contexts>
<marker>Bonial, Bonn, Conger, Hwang, Palmer, 2014</marker>
<rawString>Claire Bonial, Julia Bonn, Kathryn Conger, Jena D. Hwang, and Martha Palmer. 2014. Propbank: Semantics of new predicate types. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), Reykjavik, Iceland, may.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Introduction to the conll-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Computational Natural Language Learning, CONLL ’05,</booktitle>
<pages>152--164</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>Xavier Carreras and Llu´ıs M`arquez. 2005. Introduction to the conll-2005 shared task: Semantic role labeling. In Proceedings of the Ninth Conference on Computational Natural Language Learning, CONLL ’05, pages 152–164, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koen Deschacht</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Semi-supervised semantic role labeling using the latent words language model.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP ’09,</booktitle>
<pages>21--29</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="16879" citStr="Deschacht and Moens, 2009" startWordPosition="2669" endWordPosition="2672">is also applicable to English SRL. Given the margin of improvement on nominal SRL, which is not as well constrained by syntax as verb SRL, there are reasons to speculate the proposed technique could be applicable to other predicate type extensions of PropBank SRL. As our first attempt at automatically deriving Chinese selectional preference, there is a lot of room for future improvement. Notably, these include techniques used for English SP such as computing similarity based on lexical resources (for Chinese - HowNet (Dong et al., 2010)), distributional similarity, latent word language model (Deschacht and Moens, 2009), different variants of LDA topic models, as well as taking advantages of argument constraints in parallel corpora to extract higher quality SP. Acknowledgement We gratefully acknowledge the support of the National Science Foundation CISE-IISRI-0910992, Richer Representations for Machine Translation, DARPA FA8750-09-C-0179 (via BBN) Machine Reading: Ontology Induction: Semlink+, and DARPA HR0011-11-C-0145 (via LDC) BOLT. This work utilized the Janus supercomputer, which is supported by the National Science Foundation (award number CNS-0821794) and the University of Colorado Boulder. The Janus </context>
</contexts>
<marker>Deschacht, Moens, 2009</marker>
<rawString>Koen Deschacht and Marie-Francine Moens. 2009. Semi-supervised semantic role labeling using the latent words language model. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP ’09, pages 21–29, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhendong Dong</author>
<author>Qiang Dong</author>
<author>Changling Hao</author>
</authors>
<title>Hownet and its computation of meaning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Demonstrations, COLING ’10,</booktitle>
<pages>53--56</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="16795" citStr="Dong et al., 2010" startWordPosition="2658" endWordPosition="2661"> comparable improvement on out-of-genre data and demonstrated our technique is also applicable to English SRL. Given the margin of improvement on nominal SRL, which is not as well constrained by syntax as verb SRL, there are reasons to speculate the proposed technique could be applicable to other predicate type extensions of PropBank SRL. As our first attempt at automatically deriving Chinese selectional preference, there is a lot of room for future improvement. Notably, these include techniques used for English SP such as computing similarity based on lexical resources (for Chinese - HowNet (Dong et al., 2010)), distributional similarity, latent word language model (Deschacht and Moens, 2009), different variants of LDA topic models, as well as taking advantages of argument constraints in parallel corpora to extract higher quality SP. Acknowledgement We gratefully acknowledge the support of the National Science Foundation CISE-IISRI-0910992, Richer Representations for Machine Translation, DARPA FA8750-09-C-0179 (via BBN) Machine Reading: Ontology Induction: Semlink+, and DARPA HR0011-11-C-0145 (via LDC) BOLT. This work utilized the Janus supercomputer, which is supported by the National Science Foun</context>
</contexts>
<marker>Dong, Dong, Hao, 2010</marker>
<rawString>Zhendong Dong, Qiang Dong, and Changling Hao. 2010. Hownet and its computation of meaning. In Proceedings of the 23rd International Conference on Computational Linguistics: Demonstrations, COLING ’10, pages 53–56, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2855" citStr="Fellbaum, 1998" startWordPosition="445" endWordPosition="446">parsing accuracy) for arguments of both verb and nominal predicates using Chinese Gigaword. Our hypothesis, that SP will provide a greater benefit for nominal predicates than for verbal predicates, is verified by our results. We achieve a 2.34 F1 point improvement to our Chinese SRL system on arguments of nominal predicates, 0.40 F1 point on arguments of verb predicates, and 0.66 F1 point overall. 2 Previous Work on Selectional Preference Inducing selectional preferences from corpus data was first proposed by Resnik (1997) for sense disambiguation. He generalized seen words using the WordNet (Fellbaum, 1998) hierarchy. Gildea and Jurafsky (2002) applied SP to automatic SRL by clustering extracted verb-direct object pairs, resulting in modest improvements. This syntactic signature based selectional preference technique has also been successfully extended and applied to unsupervised SRL by Lang and Lapata (2011) (using splitmerge role clustering), as well as Titov and Klementiev (2012) (using a distance-dependent Chinese Restaurant Process prior for role clustering). Zapirain et al. (2013) improved the end-to-end perforProceedings of the Fourth Joint Conference on Lexical and Computational Semantic</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="2893" citStr="Gildea and Jurafsky (2002)" startWordPosition="448" endWordPosition="451">ents of both verb and nominal predicates using Chinese Gigaword. Our hypothesis, that SP will provide a greater benefit for nominal predicates than for verbal predicates, is verified by our results. We achieve a 2.34 F1 point improvement to our Chinese SRL system on arguments of nominal predicates, 0.40 F1 point on arguments of verb predicates, and 0.66 F1 point overall. 2 Previous Work on Selectional Preference Inducing selectional preferences from corpus data was first proposed by Resnik (1997) for sense disambiguation. He generalized seen words using the WordNet (Fellbaum, 1998) hierarchy. Gildea and Jurafsky (2002) applied SP to automatic SRL by clustering extracted verb-direct object pairs, resulting in modest improvements. This syntactic signature based selectional preference technique has also been successfully extended and applied to unsupervised SRL by Lang and Lapata (2011) (using splitmerge role clustering), as well as Titov and Klementiev (2012) (using a distance-dependent Chinese Restaurant Process prior for role clustering). Zapirain et al. (2013) improved the end-to-end perforProceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 222–227, Denver, </context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuxiang Jia</author>
<author>Hongying Zan</author>
<author>Ming Fan</author>
</authors>
<title>Inducing chinese selectional preference based on hownet.</title>
<date>2011</date>
<booktitle>In Proceedings of the Seventh International Conference on Computational Intelligence and Security, CIS2011,</booktitle>
<pages>1146--1149</pages>
<contexts>
<context position="4420" citStr="Jia et al., 2011" startWordPosition="686" endWordPosition="689"> naturally represents the semantic structure of a document collection, and the topics generated can be viewed as the latent set of classes that store preferences. The work utilizes LinkLDA, a variant of the standard LDA that models two sets of distributions for each topic simultaneously, with the resulting topics encoding the mutual constraints of a pair of arguments for the same predicate. S´eaghdha and Korhonen (2014) also proposed SP w/ the LDA variants ROOTH-LDA and LEX-LDA. There has also been work on Chinese selectional preferences, both lexical resource (HowNet) based and corpus based (Jia et al., 2011; Jia et al., 2013). The authors found the LDA corpus based SP improved over the HowNet based SP on pseudodisambiguation. All of these results encouraged us to also attempt an LDA based approach to SP. 3 Selectional Preference for SRL 3.1 SP Representation Some of the most discriminative SP models used by Zapirain et al. (2013) relied on distributional similarity computed over dependency relationships (provided by Lin (1998)). For example, in “John lent Mary the book.”, we would extract John-nsubj, Mary-iobj, book-dobj for the predicate lend. While this has proven to be of higher quality than </context>
</contexts>
<marker>Jia, Zan, Fan, 2011</marker>
<rawString>Yuxiang Jia, Hongying Zan, and Ming Fan. 2011. Inducing chinese selectional preference based on hownet. In Proceedings of the Seventh International Conference on Computational Intelligence and Security, CIS2011, pages 1146–1149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuxiang Jia</author>
<author>Hongying Zan</author>
<author>Ming Fan</author>
</authors>
<title>Computational models for chinese selectional preferences induction.</title>
<date>2013</date>
<journal>International Journal of Advanced Intelligence,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="4439" citStr="Jia et al., 2013" startWordPosition="690" endWordPosition="693">nts the semantic structure of a document collection, and the topics generated can be viewed as the latent set of classes that store preferences. The work utilizes LinkLDA, a variant of the standard LDA that models two sets of distributions for each topic simultaneously, with the resulting topics encoding the mutual constraints of a pair of arguments for the same predicate. S´eaghdha and Korhonen (2014) also proposed SP w/ the LDA variants ROOTH-LDA and LEX-LDA. There has also been work on Chinese selectional preferences, both lexical resource (HowNet) based and corpus based (Jia et al., 2011; Jia et al., 2013). The authors found the LDA corpus based SP improved over the HowNet based SP on pseudodisambiguation. All of these results encouraged us to also attempt an LDA based approach to SP. 3 Selectional Preference for SRL 3.1 SP Representation Some of the most discriminative SP models used by Zapirain et al. (2013) relied on distributional similarity computed over dependency relationships (provided by Lin (1998)). For example, in “John lent Mary the book.”, we would extract John-nsubj, Mary-iobj, book-dobj for the predicate lend. While this has proven to be of higher quality than pure word co-occurr</context>
</contexts>
<marker>Jia, Zan, Fan, 2013</marker>
<rawString>Yuxiang Jia, Hongying Zan, Ming Fan, , Shiwen Yu, and Zhimin Wang. 2013. Computational models for chinese selectional preferences induction. International Journal of Advanced Intelligence, 5(1):110–119, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Lang</author>
<author>Mirella Lapata</author>
</authors>
<title>Unsupervised semantic role induction via split-merge clustering.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="3163" citStr="Lang and Lapata (2011)" startWordPosition="488" endWordPosition="491">ts of nominal predicates, 0.40 F1 point on arguments of verb predicates, and 0.66 F1 point overall. 2 Previous Work on Selectional Preference Inducing selectional preferences from corpus data was first proposed by Resnik (1997) for sense disambiguation. He generalized seen words using the WordNet (Fellbaum, 1998) hierarchy. Gildea and Jurafsky (2002) applied SP to automatic SRL by clustering extracted verb-direct object pairs, resulting in modest improvements. This syntactic signature based selectional preference technique has also been successfully extended and applied to unsupervised SRL by Lang and Lapata (2011) (using splitmerge role clustering), as well as Titov and Klementiev (2012) (using a distance-dependent Chinese Restaurant Process prior for role clustering). Zapirain et al. (2013) improved the end-to-end perforProceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 222–227, Denver, Colorado, June 4–5, 2015. mance of an English PropBank SRL system by 0.4 F1 points using a variety of word similarity measures, from WordNet hierarchy distance to distributional similarity measures. Ritter and Etzioni (2010) reasoned that the set of hidden variables mod</context>
</contexts>
<marker>Lang, Lapata, 2011</marker>
<rawString>Joel Lang and Mirella Lapata. 2011. Unsupervised semantic role induction via split-merge clustering. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<journal>ACL</journal>
<booktitle>In Proceedings of ACL</booktitle>
<volume>98</volume>
<pages>768--774</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4848" citStr="Lin (1998)" startWordPosition="757" endWordPosition="758">sed SP w/ the LDA variants ROOTH-LDA and LEX-LDA. There has also been work on Chinese selectional preferences, both lexical resource (HowNet) based and corpus based (Jia et al., 2011; Jia et al., 2013). The authors found the LDA corpus based SP improved over the HowNet based SP on pseudodisambiguation. All of these results encouraged us to also attempt an LDA based approach to SP. 3 Selectional Preference for SRL 3.1 SP Representation Some of the most discriminative SP models used by Zapirain et al. (2013) relied on distributional similarity computed over dependency relationships (provided by Lin (1998)). For example, in “John lent Mary the book.”, we would extract John-nsubj, Mary-iobj, book-dobj for the predicate lend. While this has proven to be of higher quality than pure word co-occurrence based similarity, it may not be optimal for semantic-based processing. With nominal SRL, a large portion of the arguments (around 50% in Chinese PropBank) are not the direct syntactic dependents of the predicate: in figure 1, because of a light verb-like construction, all the arguments of &amp;quot;Î/welcome are the syntactic dependents of A T/express. To address this, we directly extract SP of the predicates </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of ACL 1998, ACL ’98, pages 768–774, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyuan Liu</author>
<author>Yuzhou Zhang</author>
<author>Edward Y Chang</author>
<author>Maosong Sun</author>
</authors>
<title>Plda+: Parallel latent dirichlet allocation with data placement and pipeline processing.</title>
<date>2011</date>
<journal>ACM Trans. Intell. Syst. Technol.,</journal>
<volume>2</volume>
<issue>3</issue>
<pages>26--18</pages>
<contexts>
<context position="10874" citStr="Liu et al., 2011" startWordPosition="1733" endWordPosition="1736">ance. 5 Experiment 5.1 Setup Our Chinese SRL system is trained on Chinese TreeBank 5.1 and Chinese PropBank 1.0. We used the standard: sections 81-885 for training, sections 41- 80 for development, and sections 1-40, 900-931 for testing. We generated the training parses (with 10 fold cross-validation) and the test parses using the Berkeley parser2 (5 split-merge cycles). The parser F1 score on the test sections is 82.73 as measured by ParseEval (Black et al., 1991). We prepared the Chinese Gigaword3 corpus with the Stanford Chinese Word Segmenter4. We performed LDA topic modeling using PLDA+ (Liu et al., 2011) and the recommended α = 50/topic cnt, Q = 0.01 values. We chose 2000 topics (tuned on the SRL performance of the development set rather than any topic based metrics). Table 1 lists some of the found topics (with the most frequent, relatively interesting, and least frequent headword, label pairs) using Chinese Gigaword. 5.2 Performance As table 2 shows, the addition of the SP feature improved nominal SRL by 2.34 F1 points. Verb SRL improved by 0.40 F1 point and overall SRL improved by 0.66 F1 point. These F1 differences were all found to be statistically significant5 (p ≤ 0.05). We also tested</context>
</contexts>
<marker>Liu, Zhang, Chang, Sun, 2011</marker>
<rawString>Zhiyuan Liu, Yuzhou Zhang, Edward Y. Chang, and Maosong Sun. 2011. Plda+: Parallel latent dirichlet allocation with data placement and pipeline processing. ACM Trans. Intell. Syst. Technol., 2(3):26:1– 26:18, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selectional preference and sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL SIGLEX Workshop on Tagging Text with Lexical Semantics,</booktitle>
<pages>52--57</pages>
<publisher>ACL.</publisher>
<location>Washington, D.C.</location>
<contexts>
<context position="2768" citStr="Resnik (1997)" startWordPosition="432" endWordPosition="433">is paper, we apply SP to Chinese SRL (which has few morphological clues that impacts parsing accuracy) for arguments of both verb and nominal predicates using Chinese Gigaword. Our hypothesis, that SP will provide a greater benefit for nominal predicates than for verbal predicates, is verified by our results. We achieve a 2.34 F1 point improvement to our Chinese SRL system on arguments of nominal predicates, 0.40 F1 point on arguments of verb predicates, and 0.66 F1 point overall. 2 Previous Work on Selectional Preference Inducing selectional preferences from corpus data was first proposed by Resnik (1997) for sense disambiguation. He generalized seen words using the WordNet (Fellbaum, 1998) hierarchy. Gildea and Jurafsky (2002) applied SP to automatic SRL by clustering extracted verb-direct object pairs, resulting in modest improvements. This syntactic signature based selectional preference technique has also been successfully extended and applied to unsupervised SRL by Lang and Lapata (2011) (using splitmerge role clustering), as well as Titov and Klementiev (2012) (using a distance-dependent Chinese Restaurant Process prior for role clustering). Zapirain et al. (2013) improved the end-to-end</context>
</contexts>
<marker>Resnik, 1997</marker>
<rawString>Philip Resnik. 1997. Selectional preference and sense disambiguation. In Proceedings of ACL SIGLEX Workshop on Tagging Text with Lexical Semantics, pages 52–57, Washington, D.C. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Oren Etzioni</author>
</authors>
<title>A latent dirichlet allocation method for selectional preferences. In</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="3717" citStr="Ritter and Etzioni (2010)" startWordPosition="572" endWordPosition="575">ully extended and applied to unsupervised SRL by Lang and Lapata (2011) (using splitmerge role clustering), as well as Titov and Klementiev (2012) (using a distance-dependent Chinese Restaurant Process prior for role clustering). Zapirain et al. (2013) improved the end-to-end perforProceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 222–227, Denver, Colorado, June 4–5, 2015. mance of an English PropBank SRL system by 0.4 F1 points using a variety of word similarity measures, from WordNet hierarchy distance to distributional similarity measures. Ritter and Etzioni (2010) reasoned that the set of hidden variables modeled by latent Dirichlet allocation (LDA) naturally represents the semantic structure of a document collection, and the topics generated can be viewed as the latent set of classes that store preferences. The work utilizes LinkLDA, a variant of the standard LDA that models two sets of distributions for each topic simultaneously, with the resulting topics encoding the mutual constraints of a pair of arguments for the same predicate. S´eaghdha and Korhonen (2014) also proposed SP w/ the LDA variants ROOTH-LDA and LEX-LDA. There has also been work on C</context>
</contexts>
<marker>Ritter, Etzioni, 2010</marker>
<rawString>Alan Ritter and Oren Etzioni. 2010. A latent dirichlet allocation method for selectional preferences. In In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid S´eaghdha</author>
<author>Anna Korhonen</author>
</authors>
<title>Probabilistic distributional semantics with latent variable models.</title>
<date>2014</date>
<journal>Computational Linguistics,</journal>
<volume>40</volume>
<issue>3</issue>
<marker>S´eaghdha, Korhonen, 2014</marker>
<rawString>Diarmuid S´eaghdha and Anna Korhonen. 2014. Probabilistic distributional semantics with latent variable models. Computational Linguistics, 40(3):587–631, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
</authors>
<title>Semantics-driven shallow parsing for chinese semantic role labeling.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ’10,</booktitle>
<pages>103--108</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="14002" citStr="Sun (2010)" startWordPosition="2187" endWordPosition="2188">of-genre results Bank 3.0. Only Sinorama has nominal SRL annotations. As table 3 shows, even though the absolute performance is much lower, SP improved the precision and recall in all cases, the nominal SRL score on Sinorama by 2.30 F1 points, and verb SRL score by 0.31-0.46 F1 point. Again, these F1 differences were statistically significant. 5.2.1 Comparison Direct performance comparison with previous Chinese SRL systems is a bit difficult: Xue (2008), Zhuang and Zong (2010) trained the syntactic parsers with an additional 250K word broadcast news corpus found in Chinese TreeBank 6.0, while Sun (2010) only reported results using gold POS tags but no additional gold parses. However, as table 4 shows, for verb predicates, our system bests Xue’s (2008) system by 4-7 F1 points with less parser training data and when tested with (but was not retrained to take full advantage of) gold POS tags besting Sun’s (2010) system by 0.53 F1 point. For nominal predicates, our system bests Xue’s (2008) system, by 1.9 F1 points on arguments of nominal predicates (since we have an integrated SRL system, the results are obtained by training both verb and nominal predicates, then using only the nominal classifi</context>
</contexts>
<marker>Sun, 2010</marker>
<rawString>Weiwei Sun. 2010. Semantics-driven shallow parsing for chinese semantic role labeling. In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ’10, pages 103–108, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Alexandre Klementiev</author>
</authors>
<title>A Bayesian approach to unsupervised semantic role induction.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Avignon, France,</location>
<contexts>
<context position="3238" citStr="Titov and Klementiev (2012)" startWordPosition="500" endWordPosition="504">s, and 0.66 F1 point overall. 2 Previous Work on Selectional Preference Inducing selectional preferences from corpus data was first proposed by Resnik (1997) for sense disambiguation. He generalized seen words using the WordNet (Fellbaum, 1998) hierarchy. Gildea and Jurafsky (2002) applied SP to automatic SRL by clustering extracted verb-direct object pairs, resulting in modest improvements. This syntactic signature based selectional preference technique has also been successfully extended and applied to unsupervised SRL by Lang and Lapata (2011) (using splitmerge role clustering), as well as Titov and Klementiev (2012) (using a distance-dependent Chinese Restaurant Process prior for role clustering). Zapirain et al. (2013) improved the end-to-end perforProceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 222–227, Denver, Colorado, June 4–5, 2015. mance of an English PropBank SRL system by 0.4 F1 points using a variety of word similarity measures, from WordNet hierarchy distance to distributional similarity measures. Ritter and Etzioni (2010) reasoned that the set of hidden variables modeled by latent Dirichlet allocation (LDA) naturally represents the semantic</context>
</contexts>
<marker>Titov, Klementiev, 2012</marker>
<rawString>Ivan Titov and Alexandre Klementiev. 2012. A Bayesian approach to unsupervised semantic role induction. In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics, Avignon, France, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Labeling chinese predicates with semantic roles.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="13849" citStr="Xue (2008)" startWordPosition="2163" endWordPosition="2164">aseline 67.13 50.37 57.55 SPLDA 67.56 50.59 57.86 4051- baseline 62.01 50.74 55.81 4411 (verb) SPLDA 62.70 51.03 56.27 Table 3: Chinese PropBank 3.0 out-of-genre results Bank 3.0. Only Sinorama has nominal SRL annotations. As table 3 shows, even though the absolute performance is much lower, SP improved the precision and recall in all cases, the nominal SRL score on Sinorama by 2.30 F1 points, and verb SRL score by 0.31-0.46 F1 point. Again, these F1 differences were statistically significant. 5.2.1 Comparison Direct performance comparison with previous Chinese SRL systems is a bit difficult: Xue (2008), Zhuang and Zong (2010) trained the syntactic parsers with an additional 250K word broadcast news corpus found in Chinese TreeBank 6.0, while Sun (2010) only reported results using gold POS tags but no additional gold parses. However, as table 4 shows, for verb predicates, our system bests Xue’s (2008) system by 4-7 F1 points with less parser training data and when tested with (but was not retrained to take full advantage of) gold POS tags besting Sun’s (2010) system by 0.53 F1 point. For nominal predicates, our system bests Xue’s (2008) system, by 1.9 F1 points on arguments of nominal predic</context>
<context position="15122" citStr="Xue 2008" startWordPosition="2381" endWordPosition="2382">ained by training both verb and nominal predicates, then using only the nominal classifier to classify the nominal predicates). 5.2.2 English SRL We applied the same techniques to English SRL using the English Gigaword7 corpus. We used 800 topics (w/ lemmatized headwords) tuning on the 6Verb results are from SRL systems trained on verbs only. Table 2 results are from SRL systems trained on all predicates. 7LDC2003T05 225 type system p r f1 Xue 2008 76.8 62.5 68.9 w/ gold POS 79.5 65.6 71.9 verb Sun 2010 81.03 72.38 76.46 (gold POS) SPLDA 82.74 70.96 76.40 w/ gold POS 82.81 71.93 76.99 nominal Xue 2008 62.9 53.1 57.6 SPLDA 67.30 53.31 59.50 Table 4: Chinese SRL comparison6 system p r f1 errorΔ SwiRL 79.7 70.9 75.0 Zapirain 2013 80.0 71.3 75.4 −1.60% baseline 82.59 77.27 79.84 SPLDA 82.96 77.52 80.15 −1.54% Table 5: English SRL comparison (CoNLL-2005 WSJ) CoNLL-2005 development set. Compared to Zapirain et al. (2013) (table 5), our SP approach had a smaller (but still statistically significant) absolute F1 gain, with most of the gain coming from core argument type improvements. But with a much higher performing baseline system (one of the highest reported results using a single input parse p</context>
</contexts>
<marker>Xue, 2008</marker>
<rawString>Nianwen Xue. 2008. Labeling chinese predicates with semantic roles. Computational Linguistics, 34(2):225–255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th Conference on Computational Linguistics -Volume 2, COLING ’00,</booktitle>
<pages>947--953</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="11810" citStr="Yeh, 2000" startWordPosition="1871" endWordPosition="1872">Performance As table 2 shows, the addition of the SP feature improved nominal SRL by 2.34 F1 points. Verb SRL improved by 0.40 F1 point and overall SRL improved by 0.66 F1 point. These F1 differences were all found to be statistically significant5 (p ≤ 0.05). We also tested the system on Sinorama magazine and other out-of-genre sections (broadcast conversation, broadcast news, web blog) in Chinese Prop2code.google.com/p/berkeleyparser/ 3LDC2011T13 4nlp.stanford.edu/software/segmenter.shtml 5SIGF (www.nlpado.de/%7esebastian/software/sigf.shtml), using stratified approximate randomization test (Yeh, 2000) 224 topic headword:argument label pairs emergency 169,/damage:Arg1 PJIi:/stop:Arg1 6 /fabricate:Arg1 4Wsearch:Arg1 Ny;/suicide:Arg1 response ... Jk/extinguish:Arg1 MiWblackmail:Arg1 *1/break free:Arg1 �LU4-4Wcomeback:Arg1 government � �/custom:Arg0 0 � /union:Arg0 � è/work department:Arg0 � 8 agency P/travel department:Arg0 � it P/census:Arg0 ... è /ministries:Arg0 1 � k&amp;/checkpoint:Arg0 #61A/finance bureau:Arg0 law &amp; � �/police:Arg0 � �/suspect:Arg1 7 �/male:Arg1 0 �/court appearance:Arg1 � order �/public safety:Arg0 ... �/alley:Argm-loc � � �/Chiayi City:Argm-loc � f � � ,/Columbian:Arg1 pa</context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>Alexander Yeh. 2000. More accurate tests for the statistical significance of result differences. In Proceedings of the 18th Conference on Computational Linguistics -Volume 2, COLING ’00, pages 947–953, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Be˜nat Zapirain</author>
</authors>
<title>Eneko Agirre, Llu´ıs M`arquez, and Mihai Surdeanu.</title>
<date>2013</date>
<booktitle>In Computational Linguistics,</booktitle>
<pages>631--663</pages>
<contexts>
<context position="15250" citStr="Zapirain 2013" startWordPosition="2404" endWordPosition="2405">es). 5.2.2 English SRL We applied the same techniques to English SRL using the English Gigaword7 corpus. We used 800 topics (w/ lemmatized headwords) tuning on the 6Verb results are from SRL systems trained on verbs only. Table 2 results are from SRL systems trained on all predicates. 7LDC2003T05 225 type system p r f1 Xue 2008 76.8 62.5 68.9 w/ gold POS 79.5 65.6 71.9 verb Sun 2010 81.03 72.38 76.46 (gold POS) SPLDA 82.74 70.96 76.40 w/ gold POS 82.81 71.93 76.99 nominal Xue 2008 62.9 53.1 57.6 SPLDA 67.30 53.31 59.50 Table 4: Chinese SRL comparison6 system p r f1 errorΔ SwiRL 79.7 70.9 75.0 Zapirain 2013 80.0 71.3 75.4 −1.60% baseline 82.59 77.27 79.84 SPLDA 82.96 77.52 80.15 −1.54% Table 5: English SRL comparison (CoNLL-2005 WSJ) CoNLL-2005 development set. Compared to Zapirain et al. (2013) (table 5), our SP approach had a smaller (but still statistically significant) absolute F1 gain, with most of the gain coming from core argument type improvements. But with a much higher performing baseline system (one of the highest reported results using a single input parse per sentence), the error reduction rate is comparable. 6 Conclusion We presented a LDA topic model based selectional preference a</context>
</contexts>
<marker>Zapirain, 2013</marker>
<rawString>Be˜nat Zapirain, Eneko Agirre, Llu´ıs M`arquez, and Mihai Surdeanu. 2013. Selectional preferences for semantic role classification. In Computational Linguistics, pages 631–663.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Zhuang</author>
<author>Chengqing Zong</author>
</authors>
<title>Joint inference for bilingual semantic role labeling.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP 2010,</booktitle>
<pages>304--314</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="13873" citStr="Zhuang and Zong (2010)" startWordPosition="2165" endWordPosition="2168">3 50.37 57.55 SPLDA 67.56 50.59 57.86 4051- baseline 62.01 50.74 55.81 4411 (verb) SPLDA 62.70 51.03 56.27 Table 3: Chinese PropBank 3.0 out-of-genre results Bank 3.0. Only Sinorama has nominal SRL annotations. As table 3 shows, even though the absolute performance is much lower, SP improved the precision and recall in all cases, the nominal SRL score on Sinorama by 2.30 F1 points, and verb SRL score by 0.31-0.46 F1 point. Again, these F1 differences were statistically significant. 5.2.1 Comparison Direct performance comparison with previous Chinese SRL systems is a bit difficult: Xue (2008), Zhuang and Zong (2010) trained the syntactic parsers with an additional 250K word broadcast news corpus found in Chinese TreeBank 6.0, while Sun (2010) only reported results using gold POS tags but no additional gold parses. However, as table 4 shows, for verb predicates, our system bests Xue’s (2008) system by 4-7 F1 points with less parser training data and when tested with (but was not retrained to take full advantage of) gold POS tags besting Sun’s (2010) system by 0.53 F1 point. For nominal predicates, our system bests Xue’s (2008) system, by 1.9 F1 points on arguments of nominal predicates (since we have an i</context>
</contexts>
<marker>Zhuang, Zong, 2010</marker>
<rawString>Tao Zhuang and Chengqing Zong. 2010. Joint inference for bilingual semantic role labeling. In Proceedings of EMNLP 2010, pages 304–314, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>