<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.9979015">
Probing the space of grammatical variation:
induction of cross-lingual grammatical constraints from treebanks
</title>
<author confidence="0.936452">
Felice Dell’Orletta
</author>
<affiliation confidence="0.83333">
Università di Pisa, Dipartimento di
Informatica - Largo B. Pontecorvo 3
</affiliation>
<address confidence="0.6660615">
ILC-CNR - via G. Moruzzi 1
56100 Pisa, Italy
</address>
<email confidence="0.997652">
felice.dellorletta@ilc.cnr.it
</email>
<author confidence="0.920371">
Simonetta Montemagni
</author>
<affiliation confidence="0.6420715">
ILC-CNR - via G. Moruzzi 1
56100 Pisa, Italy
</affiliation>
<email confidence="0.984488">
simonetta.montemagni@ilc.cnr.it
</email>
<sectionHeader confidence="0.995419" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999736">
The paper reports on a detailed
quantitative analysis of distributional
language data of both Italian and Czech,
highlighting the relative contribution of a
number of distributed grammatical
factors to sentence-based identification of
subjects and direct objects. The work
uses a Maximum Entropy model of
stochastic resolution of conflicting
grammatical constraints and is
demonstrably capable of putting
explanatory theoretical accounts to the
test of usage-based empirical verification.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999921764705882">
The paper illustrates the application of a
Maximum Entropy (henceforth MaxEnt) model
(Ratnaparkhi 1998) to the processing of subjects
and direct objects in Italian and Czech. The
model makes use of richly annotated Treebanks
to determine the types of linguistic factors
involved in the task and weigh up their relative
salience. In doing so, we set ourselves a two-
fold goal. On the one hand, we intend to discuss
the use of Treebanks to discover typologically
relevant and linguistically motivated factors and
assess the relative contribution of the latter to
cross-linguistic parsing issues. On the other
hand, we are interested in testing the empirical
plausibility of constraint-resolution models of
language processing (see infra) when confronted
with real language data.
</bodyText>
<note confidence="0.410224">
Alessandro Lenci
</note>
<address confidence="0.512077333333333">
Università di Pisa, Dipartimento di
Linguistica - Via Santa Maria 36
56100 Pisa, Italy
</address>
<email confidence="0.991765">
alessandro.lenci@ilc.cnr.it
</email>
<author confidence="0.788744">
Vito Pirrelli
</author>
<affiliation confidence="0.322049">
ILC-CNR - via G. Moruzzi 1
56100 Pisa, Italy
</affiliation>
<email confidence="0.776022">
vito.pirrelli@ilc.cnr.it
</email>
<bodyText confidence="0.999856083333333">
Current research in natural language learning
and processing supports the view that
grammatical competence consists in mastering
and integrating multiple, parallel constraints
(Seidenberg and MacDonald 1999, MacWhinney
2004). Moreover, there is growing consensus on
two major properties of grammatical constraints:
i.) they are probabilistic “soft constraints”
(Bresnan et al. 2001), and ii.) they have an
inherently functional nature, involving different
types of linguistic (and non linguistic)
information (syntactic, semantic, etc.). These
features emerge particularly clearly in dealing
with one of the core aspects of grammar
learning: the ability to identify syntactic relations
in text. Psycholinguistic evidence shows that
speakers learn to identify sentence subjects and
direct objects by combining various types of
probabilistic, functional cues, such as word
order, noun animacy, definiteness, agreement,
etc. An important observation is that the relative
prominence of each such cue can considerably
vary cross-linguistically. Bates et al. (1984), for
example, argue that while, in English, word order
is the most effective cue for Subject-Object
Identification (henceforth SOI) both in syntactic
processing and during the child’s syntactic
development, the same cue plays second fiddle in
relatively free phrase-order languages such as
Italian or German.
If grammatical constraints are inherently
probabilistic (Manning 2003), the path through
which adult grammar competence is acquired can
be viewed as the process of building a stochastic
model out of the linguistic input. In
computational linguistics, MaxEnt models have
</bodyText>
<page confidence="0.991209">
21
</page>
<note confidence="0.6954525">
Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 21–28,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999385125">
proven to be robust statistical learning algorithms
that perform well in a number of processing
tasks. Being supervised learning models, they
require richly annotated data as training input.
Before we turn to the use of Treebanks for
training a MaxEnt model for SOI, we first
analyse the range of linguistic factors that are
taken to play a significant role in the task.
</bodyText>
<sectionHeader confidence="0.9553735" genericHeader="introduction">
2 Subjects and objects in Czech and
Italian
</sectionHeader>
<bodyText confidence="0.999884744897959">
Grammatical relations - such as subject (S) and
direct object (O) - are variously encoded in
languages, the two most widespread strategies
being: i) structural encoding through word order,
and ii) morpho-syntactic marking. In turn,
morpho-syntactic marking can apply either on
the noun head only, in the form of case
inflections, or on both the noun and the verb, in
the form of agreement marking (Croft 2003).
Besides formal coding, the distribution of
subjects and object is also governed by semantic
and pragmatic factors, such as noun animacy,
definiteness, topicality, etc. As a result, there
exists a variety of linguistic clues jointly co-
operating in making a particular noun phrase the
subject or direct object of a sentence. Crucially
for our present purposes, cross-linguistic
variation does not only concern the particular
strategy used to encode S and O, but also the
relative strength that each factor plays in a given
language. For instance, while English word order
is by and large the dominant clue to identify S
and O, in other languages the presence of a rich
morphological system allows word order to have
a much looser connection with the coding of
grammatical relations, thus playing a secondary
role in their identification. Moreover, there are
languages where semantic and pragmatic
constraints such as animacy and/or definiteness
play a predominant role in the processing of
grammatical relations. A large spectrum of
variations exists, ranging from languages where
S must have a higher degree of animacy and/or
definiteness relative to O, to languages where
this constraint only takes the form of a softer
statistical preference (cf. Bresnan et al. 2001).
The goal of this paper is to explore the area of
this complex space of grammar variation through
careful assessment of the distribution of S and O
tokens in Italian and Czech. For our present
analysis, we have used a MaxEnt statistical
model trained on data extracted from two
syntactically annotated corpora: the Prague
Dependency Treebank (PDT, Bohmova et al.
2003) for Czech, and the Italian Syntactic
Semantic Treebank (ISST, Montemagni et al.
2003) for Italian. These corpora have been
chosen not only because they are the largest
syntactically annotated resources for the two
languages, but also because of their high degree
of comparability, since they both adopt a
dependency-based annotation scheme.
Czech and Italian provide an interesting
vantage point for the cross-lingual analysis of
grammatical variation. They are both Indo-
European languages, but they do not belong to
the same family: Czech is a West Slavonic
language, while Italian is a Romance language.
For our present concerns, they appear to share
two crucial features: i) the free order of
grammatical relations with respect to the verb; ii)
the possible absence of an overt subject.
Nevertheless, they also greatly differ due to: the
virtual non-existence of case marking in Italian
(with the only marginal exception of personal
pronouns), and the degree of phrase-order
freedom in the two languages. Empirical
evidence supporting the latter claim is provided
in Table 1, which reports data extracted from
PDT and ISST. Notice that although in both
languages S and O can occur either pre-verbally
or post-verbally, Czech and Italian greatly differ
in their propensity to depart from the (unmarked)
SVO order. While in Italian preverbal O is
highly infrequent (1.90%), in Czech more than
30% of O tokens occur before the verb. The
situation is similar but somewhat more balanced
in the case of S, which occurs post-verbally in
22.21% of the Italian cases, and in 40% of
Czech ones. For sure, one can argue that, in
spoken Italian, the number of pre-verbal objects
is actually higher, because of the greater number
of left dislocations and topicalizations occurring
in informal speech. However reasonable, the
observation does not explain away the
distributional differences in the two corpora,
since both PDT and ISST contain written
language only. We thus suggest that there is clear
empirical evidence in favour of a systematic,
higher phrase-order freedom in Czech, arguably
related to the well-known correlation of Czech
constituent placement with sentence information
structure, with the element carrying new
information showing a tendency to occur
sentence-finally (Stone 1990). For our present
concerns, however, aspects of information
structure, albeit central in Czech grammar, were
not taken into account, as they happen not to be
</bodyText>
<page confidence="0.997482">
22
</page>
<table confidence="0.998646888888889">
Czech
Subj Obj
Nominative 53.83% 0.65%
Accusative 0.15% 28.30%
Dative 0.16% 9.54%
Genitive 0.22% 2.03%
Instrumental 0.01% 3.40%
Ambiguous 45.63% 56.08%
All 100.00% 100.00%
</table>
<tableCaption confidence="0.739081">
Table 2 - Distribution of Czech S and O
</tableCaption>
<table confidence="0.999161583333333">
wrt case
Czech Italian
Subj Obj Subj Obj
Pos Pre 59.82% 30.27% 77.79% 1.90%
Post 40.18% 69.73% 22.21% 98.10%
All 100.00% 100.00% 100.00% 100.00%
Agr Agr 98.50% 56.54% 97.73% 58.33%
NoAgr 1.50% 43.46% 2.27% 41.67%
All 100.00% 100.00% 100.00% 100.00%
Anim Anim 34.10% 15.42% 50.18% 10.67%
NoAnim 65.90% 84.58% 49.82% 89.33%
All 100.00% 100.00% 100.00% 100.00%
</table>
<tableCaption confidence="0.8915655">
Table 1 –Distribution of Czech and Italian S and O wrt word order,
agreement and noun animacy
</tableCaption>
<bodyText confidence="0.999789638888889">
marked-up in the Italian corpus.
According to the data reported in Table 1,
Czech and Italian show similar correlation
patterns between animacy and grammatical
relations. S and O in ISST were automatically
annotated for animacy using the SIMPLE Italian
computational lexicon (Lenci et al. 2000) as a
background semantic resource. The annotation
was then checked manually. Czech S and O were
annotated for animacy using Czech WordNet
(Pala and Smrz 2004); it is worth remarking that
in Czech animacy annotation was done only
automatically, without any manual revision.
Italian shows a prominent asymmetry in the
distribution of animate nouns in subject and
object roles: over 50% of ISST subjects are
animate, while only 10% of the objects are
animate. Such a trend is also confirmed in Czech
– although to a lesser extent - with 34.10% of
animate subjects vs. 15.42% of objects.1 Such an
overwhelming preference for animate subjects in
corpus data suggests that animacy may play a
very important role for S and O identification in
both languages.
Corpus data also provide interesting evidence
concerning the actual role of morpho-syntactic
constraints in the distribution of grammatical
relations. Prima facie, agreement and case are
the strongest and most directly accessible clues
for S/O processing, as they are marked both
overtly and locally. This is also confirmed by
psycholinguistic evidence, showing that subjects
tend to rely on these clues to identify S/O.
However, it should be observed that agreement
can be relied upon conclusively in S/O
processing only when a nominal constituent and
</bodyText>
<footnote confidence="0.9313815">
1 In fact, the considerable difference in animacy distribution
between the two languages might only be an artefact of the
way we annotated Czech nouns semantically, on the basis of
their context-free classification in the Czech WordNet.
</footnote>
<bodyText confidence="0.99984115">
a verb do not agree in number and/or person (as
in leggono il libro ‘(they) read the book’).
Conversely, when N and V share the same
person and number, no conclusion can be drawn,
as trivially shown by a sentence like il bambino
legge il libro ‘the child reads the book’. In ISST,
more than 58% of O tokens agree with their
governing V, thus being formally
indistinguishable from S on the basis of
agreement features. PDT also exhibits a similar
ratio, with 56% of O tokens agreeing with their
verb head. Analogous considerations apply to
case marking, whose perceptual reliability is
undermined by morphological syncretism,
whereby different cases are realized through the
same marker. Czech data reveal the massive
extent of this phenomenon and its impact on SOI.
As reported in Table 2, more than 56% of O
tokens extracted from PDT are formally
indistinguishable from S in case ending.
Similarly, 45% of S tokens are formally
indistinguishable from O uses on the same
ground. All in all, this means that in 50% of the
cases a Czech noun can not be understood as the
S/O of a sentence by relying on overt case
marking only.
To sum up, corpus data lend support to the
idea that in both Italian and in Czech SOI is
governed by a complex interplay of probabilistic
constraints of a different nature (morpho-
syntactic, semantic, word order, etc.) as the latter
are neither singly necessary nor jointly sufficient
to attack the processing task at hand. It is
tempting to hypothesize that the joint distribution
of these data can provide a statistically reliable
basis upon which relevant probabilistic
constraints are bootstrapped and combined
consistently. This should be possible due to i) the
different degrees of clue salience in the two
languages and ii) the functional need to minimize
</bodyText>
<page confidence="0.988174">
23
</page>
<bodyText confidence="0.999915909090909">
processing ambiguity in ordinary communicative
exchanges. With reference to the latter point, for
example, we may surmise that a speaker will be
more inclined to violate one constraint on S/O
distribution (e.g. word order) when another clue
is available (e.g. animacy) that strongly supports
the intended interpretation only. The following
section illustrates how a MaxEnt model can be
used to model these intuitions by bootstrapping
constraints and their interaction from language
data.
</bodyText>
<sectionHeader confidence="0.993017" genericHeader="method">
3 Maximum Entropy modelling
</sectionHeader>
<bodyText confidence="0.999983142857143">
The MaxEnt framework offers a mathematically
sound way to build a probabilistic model for SOI,
which combines different linguistic cues. Given
a linguistic context c and an outcome aEA that
depends on c, in the MaxEnt framework the
conditional probability distribution p(a|c) is
estimated on the basis of the assumption that no
a priori constraints must be met other than those
related to a set of features fj(a,c) of c, whose
distribution is derived from the training data. It
can be proven that the probability distribution p
satisfying the above assumption is the one with
the highest entropy, is unique and has the
following exponential form (Berger et al. 1996):
</bodyText>
<equation confidence="0.9935135">
(1)
1
p a c
(  |) =
Z c
( ) ∏=
</equation>
<bodyText confidence="0.997127121212121">
j 1
where Z(c) is a normalization factor, fj(a,c) are
the values of k features of the pair (a,c) and
correspond to the linguistic cues of c that are
relevant to predict the outcome a. Features are
extracted from the training data and define the
constraints that the probabilistic model p must
satisfy. The parameters of the distribution a1, ...,
ak correspond to weights associated with the
features, and determine the relevance of each
feature in the overall model. In the experiments
reported below feature weights have been
estimated with the Generative Iterative Scaling
(GIS) algorithm implemented in the AMIS
software (Miyao and Tsujii 2002).
We model SOI as the task of predicting the
correct syntactic function (p E {subject, object}
of a noun occurring in a given syntactic context
u. This is equivalent to building the conditional
probability distribution p((p|u) of having a
syntactic function (p in a syntactic context u.
Adopting the MaxEnt approach, the distribution
p can be rewritten in the parametric form of (1),
with features corresponding to the linguistic
contextual cues relevant to SOI. The context u is
a pair &lt;vu, nu&gt;, where vu is the verbal head and nu
its nominal dependent in u. This notion of u
departs from more traditional ways of describing
an SOI context as a triple of one verb and two
nouns in a certain syntactic configuration (e.g,
SOV or VOS, etc.). In fact, we assume that SOI
can be stated in terms of the more local task of
establishing the grammatical function of a noun
n observed in a verb-noun pair. This simplifying
assumption is consistent with the claim in
MacWhinney et al. (1984) that SVO word order
is actually derivative from SV and VO local
patterns and downplays the role of the transitive
complex construction in sentence processing.
Evidence in favour of this hypothesis also comes
from corpus data: for instance, in ISST complete
subject-verb-object configurations represent only
26% of the cases, a small percentage if compared
to the 74% of verb tokens appearing with either a
subject or an object only; a similar situation can
be observed in PDT where complete subject-
verb-object configurations occur in only 20% of
the cases. Due to the comparative sparseness of
canonical SVO constructions in Czech and
Italian, it seems more reasonable to assume that
children should pay a great deal of attention to
both SV and VO units as cues in sentence
perception (Matthews et al. in press).
Reconstruction of the whole lexical SVO pattern
can accordingly be seen as the end point of an
acquisition process whereby smaller units are re-
analyzed as being part of more comprehensive
constructions. This hypothesis is more in line
with a distributed view of canonical
constructions as derivative of more basic local
positional patterns, working together to yield
more complex and abstract constructions. Last
but not least, assuming verb-noun pairs as the
relevant context for SOI allows us to
simultaneously model the interaction of word
order variation with pro-drop.
</bodyText>
<sectionHeader confidence="0.995052" genericHeader="method">
4 Feature selection
</sectionHeader>
<bodyText confidence="0.9995965">
The most important part of any MaxEnt model is
the selection of the context features whose
weights are to be estimated from data
distributions. Our feature selection strategy is
grounded on the main assumption that features
should correspond to theoretically and
typologically well-motivated contextual cues.
This allows us to evaluate the probabilistic
model also with respect to its consistency with
current linguistic generalizations. In turn, the
model can be used as a probe into the
correspondence between theoretically motivated
</bodyText>
<figure confidence="0.76189975">
k
(a, c)
fj
j
</figure>
<page confidence="0.99473">
24
</page>
<bodyText confidence="0.798200421052632">
generalizations and usage-based empirical
evidence.
Features are binary functions fki,φ (φ,σ), which
test whether a certain cue ki for the feature φ
occurs in the context σ. For our MaxEnt model,
we have selected different features types that test
morpho-syntactic, syntactic, and semantic key
dimensions in determining the distribution of S
and O.
Morpho-syntactic features. These include N-V
agreement, for Italian and Czech, and case, only
for Czech. The combined use of such features
allow us not only to test the impact of morpho-
syntactic information on SOI, but also to analyze
patterns of cross-lingual variation stemming
from language specific morphological
differences, e.g. lack of case marking in Italian.
Word order. This feature essentially test the
position of the noun wrt the verb, for instance:
</bodyText>
<equation confidence="0.7991385">
(2) fpost,subj (subj,6) = r1 if noun6 .pos = post ⎩
0 otherwise
</equation>
<bodyText confidence="0.999009615384615">
Animacy. This is the main semantic feature,
which tests whether the noun in σ is animate or
inanimate (cf. section 2). The centrality of this
cue for grammatical relation assignment is
widely supported by typological evidence (cf.
Aissen 2003, Croft 2003). The Animacy
Markedness Hierarchy - representing the relative
markedness of the associations between
grammatical functions and animacy degrees – is
actually assigned the role of a functional
universal principle in grammar. The hierarchy is
reported below, with each item in these scales
been less marked than the elements to its right:
</bodyText>
<subsectionHeader confidence="0.360117">
Animacy Markedness Hierarchy
</subsectionHeader>
<bodyText confidence="0.9483862">
Subj/Human &gt; Subj/Animate &gt; Subj/Inanimate
Obj/Inanimate &gt; Obj/Animate &gt; Obj/Human
Markedness hierarchies have also been
interpreted as probabilistic constraints estimated
from corpus data (Bresnan et al. 2001). In our
MaxEnt model we have used a reduced version
of the animacy markedness hierarchy in which
human and animate nouns have been both
subsumed under the general class animate.
Definiteness tests the degree of “referentiality” of
the noun in a context pair σ. Like for animacy,
definiteness has been claimed to be associated
with grammatical functions, giving rise to the
following universal markedness hierarchy Aissen
(2003):
</bodyText>
<equation confidence="0.475408">
Definiteness Markedness Hierarchy
Subj/Pro &gt; Subj/Name &gt; Subj/Def &gt; Subj/Indef
Obj/Indef &gt; Obj/Def &gt; Obj/Name &gt; Obj/Pro
</equation>
<bodyText confidence="0.999399166666667">
According to this hierarchy, subjects with a low
degree of definiteness are more marked than
subjects with a high degree of definiteness (for
objects the reverse pattern holds). Given the
importance assigned to the definiteness
markedness hierarchy in current linguistic
research, we have included the definiteness cue
in the MaxEnt model. In our experiments, for
Italian we have used a compact version of the
definiteness scale: the definiteness cue tests
whether the noun in the context pair i) is a name
or a pronoun ii) has a definite article iii), has an
indefinite article or iv) is a bare noun (i.e. with
no article). It is worth saying that bare nouns are
usually placed at the bottom end of the
definiteness scale. Since in Czech there is no
article, we only make a distinction between
proper names and common nouns.
</bodyText>
<sectionHeader confidence="0.950952" genericHeader="method">
5 Testing the model
</sectionHeader>
<bodyText confidence="0.999964777777778">
The Italian MaxEnt model was trained on 14,643
verb-subject/object pairs extracted from ISST.
For Czech, we used a training corpus of 37,947
verb-subject/object pairs extracted from PDT. In
both cases, the training set was obtained by
extracting all verb-subject and verb-object
dependencies headed by an active verb, with the
exclusion of all cases where the position of the
nominal constituent was grammatically
determined (e.g. clitic objects, relative clauses).
It is interesting to note that in both training sets
the proportion of subjects and objects relations is
nearly the same: 63.06%-65.93% verb-subject
pairs and 36.94%-34.07% verb-object pairs for
Italian and Czech respectively.
The test corpus consists of a set of verb-noun
pairs randomly extracted from the reference
Treebanks: 1,000 pairs for Italian and 1,373 for
Czech. For Italian, 559 pairs contained a subject
and 441 contained an object; for Czech, 905
pairs contained a subject and 468 an object.
Evaluation was carried out by calculating the
percentage of correctly assigned relations over
the total number of test pairs (accuracy). As our
model always assigns one syntactic relation to
each test pair, accuracy equals both standard
precision and recall.
</bodyText>
<page confidence="0.996252">
25
</page>
<table confidence="0.9997572">
Czech Italian
Subj Obj Subj Obj
Preverb 1.99% 19.40% 0.00% 6.90%
Postverb 71.14% 7.46% 71.55% 21.55%
Anim 0.50% 3.98% 6.90% 21.55%
Inanim 72.64% 22.89% 64.66% 6.90%
Nomin 0.00% 1.00%
Genitive 0.50% 0.00%
Dative 1.99% 0.00% Na
Accus 0.00% 0.00%
Instrum 0.00% 0.00%
Ambig 70.65% 25.87%
Agr 70.15% 25.87% 61.21% 12.07%
NoAgr 2.99% 0.50% 7.76% 1.72%
NAAgr 0.00% 0.50% 2.59% 14.66%
</table>
<tableCaption confidence="0.666183">
Table 3 – Types of errors for Czech and Italian
</tableCaption>
<table confidence="0.999933">
Czech Italian
Subj Obj Subj Obj
Preverb 1.24E+00 5.40E-01 1.31E+00 2.11E-02
Postverb 8.77E-01 1.17E+00 5.39E-01 1.38E+00
Anim 1.16E+00 6.63E-01 1.28E+00 3.17E-01
Inanim 1.03E+00 9.63E-01 8.16E-01 1.23E+00
PronName 1.13E+00 7.72E-01 1.13E+00 8.05E-01
DefArt 1.05E+00 9.31E-01 1.01E+00 1.02E+00
1.26E+00
1.02E+00
IndefArt 6.82E-01
NoArticle 9.91E-01
Nomin 1.23E+00 2.22E-02 Na
Genitive 2.94E-01 1.51E+00
Dative 2.85E-02 1.49E+00
Accus 8.06E-03 1.39E+00
Instrum 3.80E-03 1.39E+00
Agr 1.18E+00 6.67E-01 1.28E+00 4.67E-01
NoAgr 7.71E-02 1.50E+00 1.52E-01 1.58E+00
1.84E+00
NAAgr 3.75E-01 1.53E+00 2.61E-01
</table>
<tableCaption confidence="0.999262">
Table 4 - Feature value weights in NLC for Czech and
</tableCaption>
<subsubsectionHeader confidence="0.571979">
Italian
</subsubsectionHeader>
<bodyText confidence="0.994991263888889">
We have assumed a baseline score of 56% for
Italian and of 66% for Czech, corresponding to
the result yielded by a naive model assigning
to each test pair the most frequent relation
in the training corpus, i.e. subject. Experiments
were carried out with the general features
illustrated in section 4: verb agreement, case (for
Czech only), word order, noun animacy and
noun definiteness.
Accuracy on the test corpus is 88.4% for
Italian and 85.4% for Czech. A detailed error
analysis for the two languages is reported in
Table 3, showing that in both languages subject
identification appears to be particularly
problematic. In Czech, it appears that the
prototypically mistaken subjects are post-verbal
(71.14%), inanimate (72.64%), ambiguously
case-marked (70.65%) and agreeing with the
verb (70.15%), where reported percentages refer
to the whole error set. Likewise, Italian mistaken
subjects can be described thus: they typically
occur in post-verbal position (71.55%), are
mostly inanimate (64.66%) and agree with the
verb (61.21%). Interestingly, in both languages,
the highest number of errors occurs when a) N
has the least prototypical syntactic and semantic
properties for O or S (relative to word order and
noun animacy) and b) morpho-syntactic features
such as agreement and case are neutralised. This
shows that MaxEnt is able to home in on the core
linguistic properties that govern the distribution
of S and O in Italian and Czech, while remaining
uncertain in the face of somewhat peripheral and
occasional cases.
A further way to evaluate the goodness of fit
of our model is by inspecting the weights
associated with feature values for the two
languages. They are reported in Table 4, where
grey cells highlight the preference of each
feature value for either subject or object
identification. In both languages agreement with
the verb strongly relates to the subject relation.
For Czech, nominative case is strongly
associated with subjects while the other cases
with objects. Moreover, in both languages
preverbal subjects are strongly preferred over
preverbal objects; animate subjects are preferred
over animate objects; pronouns and proper
names are typically subjects.
Let us now try to relate these feature values to
the Markedness Hierarchies reported in section
4. Interestingly enough, if we rank the Italian
Anim and Inanim values for subjects and objects,
we observe that they distribute consistently with
the Animacy Markedness Hierarchy: Subj/Anim
&gt; Subj/Inanim and Obj/Inanim &gt; Obj/Anim. This
is confirmed by the Czech results. Similarly, by
ranking the Italian values for the definiteness
features in the Subj column by decreasing weight
values we obtain the following ordering:
PronName &gt; DefArt &gt; IndefArt &gt; NoArt, which
nicely fits in with the Definiteness Markedness
Hierarchy in section 4. The so-called
“markedness reversal” is replicated with a good
degree of approximation, if we focus on the
values for the same features in the Obj column:
the PronName feature represents the most
marked option, followed by IndefArt, DefArt and
NoArt (the latter two showing the same feature
value). The exception here is represented by the
relative ordering of IndefArt and DefArt which
however show very close values. The same
</bodyText>
<page confidence="0.983731">
26
</page>
<bodyText confidence="0.999642">
seems to hold for Czech, where the feature
ordering for Subj is PronName &gt;
DefArt/IndefArt/NoArt and the reverse is
observed for Obj.
</bodyText>
<subsectionHeader confidence="0.990677">
5.1 Evaluating comparative feature salience
</subsectionHeader>
<bodyText confidence="0.999993780487805">
The relative salience of the different constraints
acting on SOI can be inferred by comparing the
weights associated with individual feature
values. For instance, Goldwater and Johnson
(2003) show that MaxEnt can successfully be
applied to learn constraint rankings in Optimality
Theory, by assuming the parameter weights &lt;α1,
..., αk&gt; as the ranking values of the constraints.
Table 5 illustrates the constraint ranking for
the two languages, ordered by decreasing weight
values for both S and O. Note that, although not
all constraints are applicable in both languages,
the weights associated with applicable
constraints exhibit the same relative salience in
Czech and Italian. This seems to suggest the
existence of a rather dominant (if not universal)
salience scale of S and O processing constraints,
in spite of the considerable difference in the
marking strategies adopted by the two languages.
As the relative weight of each constraint
crucially depends on its overall interaction with
other constraints on a given processing task,
absolute weight values can considerably vary
from language to language, with a resulting
impact on the distribution of S and O
constructions. For example, the possibility of
overtly and unambiguously marking a direct
object with case inflection makes wider room for
preverbal use of objects in Czech. Conversely,
lack of case marking in Italian considerably
limits the preverbal distribution of direct objects.
This evidence, however, appears to be an
epiphenomenon of the interaction of fairly stable
and invariant preferences, reflecting common
functional tendencies in language processing. As
shown in Table 5, if constraint ranking largely
confirms the interplay between animacy and
word order in Italian, Czech does not contradict
it but rather re-modulate it somewhat, due to the
“perturbation” factors introduced by its richer
battery of case markers.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999642">
Probabilistic language models, machine language
learning algorithms and linguistic theorizing all
appear to support a view of language processing
as a process of dynamic, on-line resolution of
conflicting grammatical constraints. We begin to
gain considerable insights into the complex
process of bootstrapping nature and behaviour of
these constraints upon observing their actual
distribution in perceptually salient contexts. In
our view of things, this trend outlines a
promising framework providing fresh support to
usage-based models of language acquisition
through mathematical and computational
simulations. Moreover, it allows scholars to
investigate patterns of cross-linguistic
typological variation that crucially depend on the
appropriate setting of model parameters. Finally,
it promises to solve, on a principled basis,
traditional performance-oriented cruces of
grammar theorizing such as degrees of human
acceptability of ill-formed grammatical
constructions (Hayes 2000) and the inherently
graded compositionality of linguistic
constructions such as morpheme-based words
and word-based phrases (Bybee 2002, Hay and
Baayen 2005).
We argue that the current availability of
comparable, richly annotated corpora and of
mathematical tools and models for corpus
exploration make time ripe for probing the space
of grammatical variation, both intra- and inter-
linguistically, on unprecedented levels of
sophistication and granularity. All in all, we
anticipate that such a convergence is likely to
have a twofold impact: it is bound to shed light
on the integration of performance and
competence factors in language study; it will
make mathematical models of language
increasingly able to accommodate richer and
richer language evidence, thus putting
explanatory theoretical accounts to the test of a
usage-based empirical verification.
In the near future, we intend to pursue two
parallel lines of development. First we would
like to increase the context-sensitiveness of our
processing task by integrating binary
grammatical constraints into the broader context
of multiply conflicting grammar relations. This
way, we will be in a position to capture the
constraint that a (transitive) verb has at most one
subject and one object, thus avoiding multiple
assignment of subject (object) relations in the
same context. Suppose, for example, that both
nouns in a noun-noun-verb triple are amenable to
a subject interpretation, but that one of them is a
more likely subject than the other. Then, it is
reasonable to expect the model to process the
less likely subject candidate as the object of the
verb in the triple. Another promising line of
development is based on the observation that the
</bodyText>
<page confidence="0.990986">
27
</page>
<bodyText confidence="0.999965785714286">
order in which verb arguments appear in context
is also lexically governed: in Italian, for
example, report verbs show a strong tendency to
select subjects post-verbally. Dell’Orletta et al.
(2005) report a substantial improvement on the
model performance on Italian SOI when lexical
information is taken into account, as a lexicalized
MaxEnt model appears to integrate general
constructional and semantic biases with
lexically-specific preferences. In a cross-lingual
perspective, comparable evidence of lexical
constraints on word order would allow us to
discover language-wide invariants in the lexicon-
grammar interplay.
</bodyText>
<sectionHeader confidence="0.999439" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9991347">
Bates E., MacWhinney B., Caselli C., Devescovi A.,
Natale F., Venza V. 1984. A crosslinguistic study
of the development of sentence interpretation
strategies. Child Development, 55: 341-354.
Bohmova A., Hajic J., Hajicova E., Hladka B. 2003.
The Prague Dependency Treebank: Three-Level
Annotation Scenario, in A. Abeille (ed.)
Treebanks: Building and Using Syntactically
Annotated Corpora, Kluwer Academic Publishers,
pp. 103-128.
Bybee J. 2002. Sequentiality as the basis of
constituent structure. in T. Givón and B. Malle
(eds.) The Evolution of Language out of Pre-
Language, Amsterdam: John Benjamins. 107-132.
Croft W. 2003. Typology and Universals. Second
Edition, Cambridge University Press, Cambridge.
Bresnan J., Dingare D., Manning C. D. 2001. Soft
constraints mirror hard constraints: voice and
person in English and Lummi. Proceedings of the
LFG01 Conference, Hong Kong: 13-32.
</reference>
<table confidence="0.864500076923077">
Constraints for S
Feature Italian Czech
Preverbal 1.31E+00 1.24E+00
Nomin na 1.23E+00
Agr 1.28E+00 1.18E+00
Anim 1.28E+00 1.16E+00
Inanim 8.16E-01 1.03E+00
Postverbal 5.39E-01 8.77E-01
Genitive na 2.94E-01
NoAgr 1.52E-01 7.71E-02
Dative na 2.85E-02
Accus na 8.06E-03
Instrum na 3.80E-03
</table>
<reference confidence="0.998376885714286">
Dell’Orletta F., Lenci A., Montemagni S., Pirrelli V.
2005. Climbing the path to grammar: a maximum
entropy model of subject/object learning.
Proceedings of the ACL-2005 Workshop
“Psychocomputational Models of Human
Language Acquisition”, University of Michigan,
Ann Arbour (USA), 29-30 June 2005.
Hay J., Baayen R.H. 2005. Shifting paradigms:
gradient structure in morphology, Trends in
Cognitive Sciences, 9(7): 342-348.
Hayes B. 2000. Gradient Well-Formedness in
Optimality Theory, in Joost Dekkers, Frank van
der Leeuw and Jeroen van de Weijer (eds.)
Optimality Theory: Phonology, Syntax, and
Acquisition, Oxford University Press, pp. 88-120.
Lenci A. et al. 2000. SIMPLE: A General Framework
for the Development of Multilingual Lexicons.
International Journal of Lexicography, 13 (4):
249-263.
MacWhinney B. 2004. A unified model of language
acquisition. In J. Kroll &amp; A. De Groot (eds.),
Handbook of bilingualism: Psycholinguistic
approaches, Oxford University Press, Oxford.
Manning C. D. 2003. Probabilistic syntax. In R. Bod,
J. Hay, S. Jannedy (eds), Probabilistic Linguistics,
MIT Press, Cambridge MA: 289-341.
Miyao Y., Tsujii J. 2002. Maximum entropy
estimation for feature forests. Proc. HLT2002.
Montemagni S. et al. 2003. Building the Italian
syntactic-semantic treebank. In Abeillé A. (ed.)
Treebanks. Building and Using Parsed Corpora,
Kluwer, Dordrecht: 189-210.
Ratnaparkhi A. 1998. Maximum Entropy Models for
Natural Language Ambiguity Resolution. Ph.D.
Dissertation, University of Pennsylvania.
</reference>
<table confidence="0.966066076923077">
Constraints for O
Feature Italian Czech
Genitive na 1.51E+00
NoAgr 1.58E+00 1.50E+00
Dative na 1.49E+00
Accus na 1.39E+00
Instrum na 1.39E+00
Postverbal 1.38E+00 1.17E+00
Inanim 1.23E+00 9.63E-01
Agr 4.67E-01 6.67E-01
Anim 3.17E-01 6.63E-01
Preverbal 2.11E-02 5.40E-01
Nomin na 2.22E-02
</table>
<tableCaption confidence="0.999064">
Table 5 – Ranked constraints for S and O in Czech and Italian
</tableCaption>
<page confidence="0.998181">
28
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.192138">
<title confidence="0.9993315">Probing the space of grammatical variation: induction of cross-lingual grammatical constraints from treebanks</title>
<author confidence="0.974405">Felice Dell’Orletta</author>
<affiliation confidence="0.875371">Università di Pisa, Dipartimento</affiliation>
<author confidence="0.5900015">Largo B Pontecorvo ILC-CNR via G Moruzzi</author>
<address confidence="0.999618">56100 Pisa, Italy</address>
<email confidence="0.998365">felice.dellorletta@ilc.cnr.it</email>
<author confidence="0.877912">Simonetta</author>
<affiliation confidence="0.557006">ILC-CNR via G. Moruzzi</affiliation>
<address confidence="0.992662">56100 Pisa,</address>
<email confidence="0.999358">simonetta.montemagni@ilc.cnr.it</email>
<abstract confidence="0.999332571428572">The paper reports on a detailed quantitative analysis of distributional language data of both Italian and Czech, highlighting the relative contribution of a number of distributed grammatical factors to sentence-based identification of subjects and direct objects. The work uses a Maximum Entropy model of stochastic resolution of conflicting grammatical constraints and is demonstrably capable of putting explanatory theoretical accounts to the test of usage-based empirical verification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Bates</author>
<author>B MacWhinney</author>
<author>C Caselli</author>
<author>A Devescovi</author>
<author>F Natale</author>
<author>V Venza</author>
</authors>
<title>A crosslinguistic study of the development of sentence interpretation strategies.</title>
<date>1984</date>
<journal>Child Development,</journal>
<volume>55</volume>
<pages>341--354</pages>
<contexts>
<context position="2939" citStr="Bates et al. (1984)" startWordPosition="405" endWordPosition="408">lving different types of linguistic (and non linguistic) information (syntactic, semantic, etc.). These features emerge particularly clearly in dealing with one of the core aspects of grammar learning: the ability to identify syntactic relations in text. Psycholinguistic evidence shows that speakers learn to identify sentence subjects and direct objects by combining various types of probabilistic, functional cues, such as word order, noun animacy, definiteness, agreement, etc. An important observation is that the relative prominence of each such cue can considerably vary cross-linguistically. Bates et al. (1984), for example, argue that while, in English, word order is the most effective cue for Subject-Object Identification (henceforth SOI) both in syntactic processing and during the child’s syntactic development, the same cue plays second fiddle in relatively free phrase-order languages such as Italian or German. If grammatical constraints are inherently probabilistic (Manning 2003), the path through which adult grammar competence is acquired can be viewed as the process of building a stochastic model out of the linguistic input. In computational linguistics, MaxEnt models have 21 Proceedings of th</context>
</contexts>
<marker>Bates, MacWhinney, Caselli, Devescovi, Natale, Venza, 1984</marker>
<rawString>Bates E., MacWhinney B., Caselli C., Devescovi A., Natale F., Venza V. 1984. A crosslinguistic study of the development of sentence interpretation strategies. Child Development, 55: 341-354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bohmova</author>
<author>J Hajic</author>
<author>E Hajicova</author>
<author>B Hladka</author>
</authors>
<title>The Prague Dependency Treebank: Three-Level Annotation Scenario,</title>
<date>2003</date>
<pages>103--128</pages>
<editor>in A. Abeille (ed.)</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<contexts>
<context position="6149" citStr="Bohmova et al. 2003" startWordPosition="910" endWordPosition="913">e spectrum of variations exists, ranging from languages where S must have a higher degree of animacy and/or definiteness relative to O, to languages where this constraint only takes the form of a softer statistical preference (cf. Bresnan et al. 2001). The goal of this paper is to explore the area of this complex space of grammar variation through careful assessment of the distribution of S and O tokens in Italian and Czech. For our present analysis, we have used a MaxEnt statistical model trained on data extracted from two syntactically annotated corpora: the Prague Dependency Treebank (PDT, Bohmova et al. 2003) for Czech, and the Italian Syntactic Semantic Treebank (ISST, Montemagni et al. 2003) for Italian. These corpora have been chosen not only because they are the largest syntactically annotated resources for the two languages, but also because of their high degree of comparability, since they both adopt a dependency-based annotation scheme. Czech and Italian provide an interesting vantage point for the cross-lingual analysis of grammatical variation. They are both IndoEuropean languages, but they do not belong to the same family: Czech is a West Slavonic language, while Italian is a Romance lan</context>
</contexts>
<marker>Bohmova, Hajic, Hajicova, Hladka, 2003</marker>
<rawString>Bohmova A., Hajic J., Hajicova E., Hladka B. 2003. The Prague Dependency Treebank: Three-Level Annotation Scenario, in A. Abeille (ed.) Treebanks: Building and Using Syntactically Annotated Corpora, Kluwer Academic Publishers, pp. 103-128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bybee</author>
</authors>
<title>Sequentiality as the basis of constituent structure.</title>
<date>2002</date>
<booktitle>The Evolution of Language out of PreLanguage,</booktitle>
<pages>107--132</pages>
<editor>in T. Givón and B. Malle (eds.)</editor>
<publisher>John Benjamins.</publisher>
<location>Amsterdam:</location>
<contexts>
<context position="29591" citStr="Bybee 2002" startWordPosition="4584" endWordPosition="4585">port to usage-based models of language acquisition through mathematical and computational simulations. Moreover, it allows scholars to investigate patterns of cross-linguistic typological variation that crucially depend on the appropriate setting of model parameters. Finally, it promises to solve, on a principled basis, traditional performance-oriented cruces of grammar theorizing such as degrees of human acceptability of ill-formed grammatical constructions (Hayes 2000) and the inherently graded compositionality of linguistic constructions such as morpheme-based words and word-based phrases (Bybee 2002, Hay and Baayen 2005). We argue that the current availability of comparable, richly annotated corpora and of mathematical tools and models for corpus exploration make time ripe for probing the space of grammatical variation, both intra- and interlinguistically, on unprecedented levels of sophistication and granularity. All in all, we anticipate that such a convergence is likely to have a twofold impact: it is bound to shed light on the integration of performance and competence factors in language study; it will make mathematical models of language increasingly able to accommodate richer and r</context>
</contexts>
<marker>Bybee, 2002</marker>
<rawString>Bybee J. 2002. Sequentiality as the basis of constituent structure. in T. Givón and B. Malle (eds.) The Evolution of Language out of PreLanguage, Amsterdam: John Benjamins. 107-132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Croft</author>
</authors>
<title>Typology and Universals. Second Edition,</title>
<date>2003</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="4510" citStr="Croft 2003" startWordPosition="650" endWordPosition="651">use of Treebanks for training a MaxEnt model for SOI, we first analyse the range of linguistic factors that are taken to play a significant role in the task. 2 Subjects and objects in Czech and Italian Grammatical relations - such as subject (S) and direct object (O) - are variously encoded in languages, the two most widespread strategies being: i) structural encoding through word order, and ii) morpho-syntactic marking. In turn, morpho-syntactic marking can apply either on the noun head only, in the form of case inflections, or on both the noun and the verb, in the form of agreement marking (Croft 2003). Besides formal coding, the distribution of subjects and object is also governed by semantic and pragmatic factors, such as noun animacy, definiteness, topicality, etc. As a result, there exists a variety of linguistic clues jointly cooperating in making a particular noun phrase the subject or direct object of a sentence. Crucially for our present purposes, cross-linguistic variation does not only concern the particular strategy used to encode S and O, but also the relative strength that each factor plays in a given language. For instance, while English word order is by and large the dominant</context>
<context position="18827" citStr="Croft 2003" startWordPosition="2958" endWordPosition="2959">pact of morphosyntactic information on SOI, but also to analyze patterns of cross-lingual variation stemming from language specific morphological differences, e.g. lack of case marking in Italian. Word order. This feature essentially test the position of the noun wrt the verb, for instance: (2) fpost,subj (subj,6) = r1 if noun6 .pos = post ⎩ 0 otherwise Animacy. This is the main semantic feature, which tests whether the noun in σ is animate or inanimate (cf. section 2). The centrality of this cue for grammatical relation assignment is widely supported by typological evidence (cf. Aissen 2003, Croft 2003). The Animacy Markedness Hierarchy - representing the relative markedness of the associations between grammatical functions and animacy degrees – is actually assigned the role of a functional universal principle in grammar. The hierarchy is reported below, with each item in these scales been less marked than the elements to its right: Animacy Markedness Hierarchy Subj/Human &gt; Subj/Animate &gt; Subj/Inanimate Obj/Inanimate &gt; Obj/Animate &gt; Obj/Human Markedness hierarchies have also been interpreted as probabilistic constraints estimated from corpus data (Bresnan et al. 2001). In our MaxEnt model we</context>
</contexts>
<marker>Croft, 2003</marker>
<rawString>Croft W. 2003. Typology and Universals. Second Edition, Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bresnan</author>
<author>D Dingare</author>
<author>C D Manning</author>
</authors>
<title>Soft constraints mirror hard constraints: voice and person</title>
<date>2001</date>
<booktitle>in English and Lummi. Proceedings of the LFG01 Conference, Hong Kong:</booktitle>
<pages>13--32</pages>
<contexts>
<context position="2262" citStr="Bresnan et al. 2001" startWordPosition="311" endWordPosition="314">h real language data. Alessandro Lenci Università di Pisa, Dipartimento di Linguistica - Via Santa Maria 36 56100 Pisa, Italy alessandro.lenci@ilc.cnr.it Vito Pirrelli ILC-CNR - via G. Moruzzi 1 56100 Pisa, Italy vito.pirrelli@ilc.cnr.it Current research in natural language learning and processing supports the view that grammatical competence consists in mastering and integrating multiple, parallel constraints (Seidenberg and MacDonald 1999, MacWhinney 2004). Moreover, there is growing consensus on two major properties of grammatical constraints: i.) they are probabilistic “soft constraints” (Bresnan et al. 2001), and ii.) they have an inherently functional nature, involving different types of linguistic (and non linguistic) information (syntactic, semantic, etc.). These features emerge particularly clearly in dealing with one of the core aspects of grammar learning: the ability to identify syntactic relations in text. Psycholinguistic evidence shows that speakers learn to identify sentence subjects and direct objects by combining various types of probabilistic, functional cues, such as word order, noun animacy, definiteness, agreement, etc. An important observation is that the relative prominence of </context>
<context position="5780" citStr="Bresnan et al. 2001" startWordPosition="849" endWordPosition="852">presence of a rich morphological system allows word order to have a much looser connection with the coding of grammatical relations, thus playing a secondary role in their identification. Moreover, there are languages where semantic and pragmatic constraints such as animacy and/or definiteness play a predominant role in the processing of grammatical relations. A large spectrum of variations exists, ranging from languages where S must have a higher degree of animacy and/or definiteness relative to O, to languages where this constraint only takes the form of a softer statistical preference (cf. Bresnan et al. 2001). The goal of this paper is to explore the area of this complex space of grammar variation through careful assessment of the distribution of S and O tokens in Italian and Czech. For our present analysis, we have used a MaxEnt statistical model trained on data extracted from two syntactically annotated corpora: the Prague Dependency Treebank (PDT, Bohmova et al. 2003) for Czech, and the Italian Syntactic Semantic Treebank (ISST, Montemagni et al. 2003) for Italian. These corpora have been chosen not only because they are the largest syntactically annotated resources for the two languages, but a</context>
<context position="19403" citStr="Bresnan et al. 2001" startWordPosition="3037" endWordPosition="3040">gical evidence (cf. Aissen 2003, Croft 2003). The Animacy Markedness Hierarchy - representing the relative markedness of the associations between grammatical functions and animacy degrees – is actually assigned the role of a functional universal principle in grammar. The hierarchy is reported below, with each item in these scales been less marked than the elements to its right: Animacy Markedness Hierarchy Subj/Human &gt; Subj/Animate &gt; Subj/Inanimate Obj/Inanimate &gt; Obj/Animate &gt; Obj/Human Markedness hierarchies have also been interpreted as probabilistic constraints estimated from corpus data (Bresnan et al. 2001). In our MaxEnt model we have used a reduced version of the animacy markedness hierarchy in which human and animate nouns have been both subsumed under the general class animate. Definiteness tests the degree of “referentiality” of the noun in a context pair σ. Like for animacy, definiteness has been claimed to be associated with grammatical functions, giving rise to the following universal markedness hierarchy Aissen (2003): Definiteness Markedness Hierarchy Subj/Pro &gt; Subj/Name &gt; Subj/Def &gt; Subj/Indef Obj/Indef &gt; Obj/Def &gt; Obj/Name &gt; Obj/Pro According to this hierarchy, subjects with a low d</context>
</contexts>
<marker>Bresnan, Dingare, Manning, 2001</marker>
<rawString>Bresnan J., Dingare D., Manning C. D. 2001. Soft constraints mirror hard constraints: voice and person in English and Lummi. Proceedings of the LFG01 Conference, Hong Kong: 13-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Dell’Orletta</author>
<author>A Lenci</author>
<author>S Montemagni</author>
<author>V Pirrelli</author>
</authors>
<title>Climbing the path to grammar: a maximum entropy model of subject/object learning.</title>
<date>2005</date>
<booktitle>Proceedings of the ACL-2005 Workshop “Psychocomputational Models of</booktitle>
<institution>Human Language Acquisition”, University of Michigan,</institution>
<location>Ann Arbour (USA),</location>
<marker>Dell’Orletta, Lenci, Montemagni, Pirrelli, 2005</marker>
<rawString>Dell’Orletta F., Lenci A., Montemagni S., Pirrelli V. 2005. Climbing the path to grammar: a maximum entropy model of subject/object learning. Proceedings of the ACL-2005 Workshop “Psychocomputational Models of Human Language Acquisition”, University of Michigan, Ann Arbour (USA), 29-30 June 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hay</author>
<author>R H Baayen</author>
</authors>
<title>Shifting paradigms: gradient structure in morphology,</title>
<date>2005</date>
<journal>Trends in Cognitive Sciences,</journal>
<volume>9</volume>
<issue>7</issue>
<pages>342--348</pages>
<contexts>
<context position="29613" citStr="Hay and Baayen 2005" startWordPosition="4586" endWordPosition="4589">e-based models of language acquisition through mathematical and computational simulations. Moreover, it allows scholars to investigate patterns of cross-linguistic typological variation that crucially depend on the appropriate setting of model parameters. Finally, it promises to solve, on a principled basis, traditional performance-oriented cruces of grammar theorizing such as degrees of human acceptability of ill-formed grammatical constructions (Hayes 2000) and the inherently graded compositionality of linguistic constructions such as morpheme-based words and word-based phrases (Bybee 2002, Hay and Baayen 2005). We argue that the current availability of comparable, richly annotated corpora and of mathematical tools and models for corpus exploration make time ripe for probing the space of grammatical variation, both intra- and interlinguistically, on unprecedented levels of sophistication and granularity. All in all, we anticipate that such a convergence is likely to have a twofold impact: it is bound to shed light on the integration of performance and competence factors in language study; it will make mathematical models of language increasingly able to accommodate richer and richer language evidenc</context>
</contexts>
<marker>Hay, Baayen, 2005</marker>
<rawString>Hay J., Baayen R.H. 2005. Shifting paradigms: gradient structure in morphology, Trends in Cognitive Sciences, 9(7): 342-348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Hayes</author>
</authors>
<title>Gradient Well-Formedness in Optimality Theory,</title>
<date>2000</date>
<booktitle>in Joost Dekkers, Frank van der Leeuw and Jeroen van de Weijer (eds.) Optimality Theory: Phonology, Syntax, and Acquisition,</booktitle>
<pages>88--120</pages>
<publisher>University Press,</publisher>
<location>Oxford</location>
<contexts>
<context position="29456" citStr="Hayes 2000" startWordPosition="4567" endWordPosition="4568">tual distribution in perceptually salient contexts. In our view of things, this trend outlines a promising framework providing fresh support to usage-based models of language acquisition through mathematical and computational simulations. Moreover, it allows scholars to investigate patterns of cross-linguistic typological variation that crucially depend on the appropriate setting of model parameters. Finally, it promises to solve, on a principled basis, traditional performance-oriented cruces of grammar theorizing such as degrees of human acceptability of ill-formed grammatical constructions (Hayes 2000) and the inherently graded compositionality of linguistic constructions such as morpheme-based words and word-based phrases (Bybee 2002, Hay and Baayen 2005). We argue that the current availability of comparable, richly annotated corpora and of mathematical tools and models for corpus exploration make time ripe for probing the space of grammatical variation, both intra- and interlinguistically, on unprecedented levels of sophistication and granularity. All in all, we anticipate that such a convergence is likely to have a twofold impact: it is bound to shed light on the integration of performan</context>
</contexts>
<marker>Hayes, 2000</marker>
<rawString>Hayes B. 2000. Gradient Well-Formedness in Optimality Theory, in Joost Dekkers, Frank van der Leeuw and Jeroen van de Weijer (eds.) Optimality Theory: Phonology, Syntax, and Acquisition, Oxford University Press, pp. 88-120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lenci</author>
</authors>
<title>SIMPLE: A General Framework for the Development of Multilingual Lexicons.</title>
<date>2000</date>
<journal>International Journal of Lexicography,</journal>
<volume>13</volume>
<issue>4</issue>
<pages>249--263</pages>
<marker>Lenci, 2000</marker>
<rawString>Lenci A. et al. 2000. SIMPLE: A General Framework for the Development of Multilingual Lexicons. International Journal of Lexicography, 13 (4): 249-263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacWhinney</author>
</authors>
<title>A unified model of language acquisition.</title>
<date>2004</date>
<booktitle>Handbook of bilingualism: Psycholinguistic approaches,</booktitle>
<editor>In J. Kroll &amp; A. De Groot (eds.),</editor>
<publisher>University Press,</publisher>
<location>Oxford</location>
<contexts>
<context position="2104" citStr="MacWhinney 2004" startWordPosition="291" endWordPosition="292">other hand, we are interested in testing the empirical plausibility of constraint-resolution models of language processing (see infra) when confronted with real language data. Alessandro Lenci Università di Pisa, Dipartimento di Linguistica - Via Santa Maria 36 56100 Pisa, Italy alessandro.lenci@ilc.cnr.it Vito Pirrelli ILC-CNR - via G. Moruzzi 1 56100 Pisa, Italy vito.pirrelli@ilc.cnr.it Current research in natural language learning and processing supports the view that grammatical competence consists in mastering and integrating multiple, parallel constraints (Seidenberg and MacDonald 1999, MacWhinney 2004). Moreover, there is growing consensus on two major properties of grammatical constraints: i.) they are probabilistic “soft constraints” (Bresnan et al. 2001), and ii.) they have an inherently functional nature, involving different types of linguistic (and non linguistic) information (syntactic, semantic, etc.). These features emerge particularly clearly in dealing with one of the core aspects of grammar learning: the ability to identify syntactic relations in text. Psycholinguistic evidence shows that speakers learn to identify sentence subjects and direct objects by combining various types o</context>
</contexts>
<marker>MacWhinney, 2004</marker>
<rawString>MacWhinney B. 2004. A unified model of language acquisition. In J. Kroll &amp; A. De Groot (eds.), Handbook of bilingualism: Psycholinguistic approaches, Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
</authors>
<title>Probabilistic syntax. In</title>
<date>2003</date>
<pages>289--341</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge MA:</location>
<contexts>
<context position="3319" citStr="Manning 2003" startWordPosition="460" endWordPosition="461">probabilistic, functional cues, such as word order, noun animacy, definiteness, agreement, etc. An important observation is that the relative prominence of each such cue can considerably vary cross-linguistically. Bates et al. (1984), for example, argue that while, in English, word order is the most effective cue for Subject-Object Identification (henceforth SOI) both in syntactic processing and during the child’s syntactic development, the same cue plays second fiddle in relatively free phrase-order languages such as Italian or German. If grammatical constraints are inherently probabilistic (Manning 2003), the path through which adult grammar competence is acquired can be viewed as the process of building a stochastic model out of the linguistic input. In computational linguistics, MaxEnt models have 21 Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 21–28, Sydney, July 2006. c�2006 Association for Computational Linguistics proven to be robust statistical learning algorithms that perform well in a number of processing tasks. Being supervised learning models, they require richly annotated data as training input. Before we turn to the use of Treebanks for</context>
</contexts>
<marker>Manning, 2003</marker>
<rawString>Manning C. D. 2003. Probabilistic syntax. In R. Bod, J. Hay, S. Jannedy (eds), Probabilistic Linguistics, MIT Press, Cambridge MA: 289-341.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Maximum entropy estimation for feature forests.</title>
<date>2002</date>
<booktitle>Proc. HLT2002.</booktitle>
<contexts>
<context position="14753" citStr="Miyao and Tsujii 2002" startWordPosition="2297" endWordPosition="2300">lization factor, fj(a,c) are the values of k features of the pair (a,c) and correspond to the linguistic cues of c that are relevant to predict the outcome a. Features are extracted from the training data and define the constraints that the probabilistic model p must satisfy. The parameters of the distribution a1, ..., ak correspond to weights associated with the features, and determine the relevance of each feature in the overall model. In the experiments reported below feature weights have been estimated with the Generative Iterative Scaling (GIS) algorithm implemented in the AMIS software (Miyao and Tsujii 2002). We model SOI as the task of predicting the correct syntactic function (p E {subject, object} of a noun occurring in a given syntactic context u. This is equivalent to building the conditional probability distribution p((p|u) of having a syntactic function (p in a syntactic context u. Adopting the MaxEnt approach, the distribution p can be rewritten in the parametric form of (1), with features corresponding to the linguistic contextual cues relevant to SOI. The context u is a pair &lt;vu, nu&gt;, where vu is the verbal head and nu its nominal dependent in u. This notion of u departs from more tradi</context>
</contexts>
<marker>Miyao, Tsujii, 2002</marker>
<rawString>Miyao Y., Tsujii J. 2002. Maximum entropy estimation for feature forests. Proc. HLT2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Montemagni</author>
</authors>
<title>Building the Italian syntactic-semantic treebank.</title>
<date>2003</date>
<booktitle>In Abeillé A. (ed.) Treebanks. Building and Using Parsed Corpora,</booktitle>
<pages>189--210</pages>
<publisher>Kluwer,</publisher>
<location>Dordrecht:</location>
<marker>Montemagni, 2003</marker>
<rawString>Montemagni S. et al. 2003. Building the Italian syntactic-semantic treebank. In Abeillé A. (ed.) Treebanks. Building and Using Parsed Corpora, Kluwer, Dordrecht: 189-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>Maximum Entropy Models for Natural Language Ambiguity Resolution.</title>
<date>1998</date>
<tech>Ph.D. Dissertation,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="991" citStr="Ratnaparkhi 1998" startWordPosition="129" endWordPosition="130"> Abstract The paper reports on a detailed quantitative analysis of distributional language data of both Italian and Czech, highlighting the relative contribution of a number of distributed grammatical factors to sentence-based identification of subjects and direct objects. The work uses a Maximum Entropy model of stochastic resolution of conflicting grammatical constraints and is demonstrably capable of putting explanatory theoretical accounts to the test of usage-based empirical verification. 1 Introduction The paper illustrates the application of a Maximum Entropy (henceforth MaxEnt) model (Ratnaparkhi 1998) to the processing of subjects and direct objects in Italian and Czech. The model makes use of richly annotated Treebanks to determine the types of linguistic factors involved in the task and weigh up their relative salience. In doing so, we set ourselves a twofold goal. On the one hand, we intend to discuss the use of Treebanks to discover typologically relevant and linguistically motivated factors and assess the relative contribution of the latter to cross-linguistic parsing issues. On the other hand, we are interested in testing the empirical plausibility of constraint-resolution models of </context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>Ratnaparkhi A. 1998. Maximum Entropy Models for Natural Language Ambiguity Resolution. Ph.D. Dissertation, University of Pennsylvania.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>