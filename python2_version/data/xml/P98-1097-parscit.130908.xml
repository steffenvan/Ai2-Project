<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.9979735">
Improving Automatic Indexing through Concept Combination
and Term Enrichment
</title>
<author confidence="0.806896">
Christian Jacquemin*
</author>
<note confidence="0.9244785">
LIMSI-CNRS
BP 133, F-91403 ORSAY Cedex, FRANCE
</note>
<email confidence="0.996482">
jacquemin@limsi.fr
</email>
<sectionHeader confidence="0.993837" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999854909090909">
Although indexes may overlap, the output of
an automatic indexer is generally presented as
a flat and unstructured list of terms. Our pur-
pose is to exploit term overlap and embed-
ding so as to yield a substantial qualitative
and quantitative improvement in automatic in-
dexing through concept combination. The in-
crease in the volume of indexing is 10.5% for
free indexing and 52.3% for controlled indexing.
The resulting structure of the indexed corpus is
a partial conceptual analysis.
</bodyText>
<sectionHeader confidence="0.994267" genericHeader="keywords">
1 Overview
</sectionHeader>
<bodyText confidence="0.990762971428572">
The method, proposed here for improving au-
tomatic indexing, builds partial syntactic stru-
ctures by combining overlapping indexes. It is
complemented by a method for term acquisition
which is described in (Jacquemin, 1996). The
text, thus structured, is reindexed; new indexes
are produced and new candidates are discove-
red.
Most NLP approaches to automatic indexing
concern free indexing and rely on large-scale
shallow parsers with a particular concern for
dependency relations (Strzalkowski, 1996). For
the purpose of controlled indexing, we exploit
the output of a NLP-based indexer and the stru-
ctural relations between terms and variants in
order to (1) enhance the coverage of the in-
dexes, (2) incrementally build an a posteriori
conceptual analysis of the document, and, (3)
interweave controlled indexing, free indexing,
and thesaurus acquisition. These 3 goals are
achieved by CONPARS (CONceptual PARSer),
presented in this paper and illustrated by Fi-
gure 1. CONPARS is based on the output of
&amp;quot; We thank INIST-CNRS for providing us with thesauri
and corpora in the agricultural domain and AFIRST for
supporting this research through the SKETCHI project.
a part-of-speech tagger for French described in
(Tzoukermann and Radev, 1997) and FASTR,
a controlled indexer (Jacquemin et al., 1997).
All the experiments reported in this paper are
performed on data in the agricultural domain:
[AGRIC] a 1.18-million word corpus, [AGRO-
VOC] a 10,570-term controlled vocabulary, and
[AGR-CAND] a 15,875-term list acquired by
ACABIT (Daille, 1997) from [AGRIC].
</bodyText>
<table confidence="0.954398904761905">
Bell Labs Part-of-speech tagger
Tagged corpus Morpholog.
families
Variations
FASTR: controlled indexer Increm, ntally
indexed
corpus
Indexed corpus
Terms
Term enricher
CONPARS:
Indexed corpus
Acquisition
patterns
CONPARS: Phrase builder
Yes
Local New indexes
grammars or new terms
or new phrases?
No
Augmented indexing
</table>
<figureCaption confidence="0.990955">
Figure 1: Overall Architecture of CONPARS
</figureCaption>
<sectionHeader confidence="0.680759" genericHeader="introduction">
2 Basic Controlled Indexing
</sectionHeader>
<bodyText confidence="0.999611666666667">
The preprocessing of the corpus by the tag-
ger yields a morphologically analyzed text,
with unambiguous syntactic categories. Then,
the tagged corpus is automatically indexed by
FASTR which retrieves occurrences of multi-
word terms or variants (see Table 1).
</bodyText>
<figure confidence="0.979209333333333">
Linguistic
rules and
dictionary
</figure>
<page confidence="0.997389">
595
</page>
<tableCaption confidence="0.999685">
Table 1: Indexing of a Sample Sentence
</tableCaption>
<bodyText confidence="0.968278">
La variation mensuelle de la respiration du sol et
ses rapports avec l&apos;humidite et la temperature du
sol ont ete analysees dans le sol superficiel d&apos;une
foret tropicale. (The monthly variation of the respi-
ration of the soil and its connections with the mois-
ture and the temperature of the soil have been ana-
lyzed in the surface soil of a tropical forest.)
</bodyText>
<equation confidence="0.714102">
007019 Respiration du sol Occurrence
respiration du sol (respiration of the soil)
002904 Sol de foret Embedding2
sol superficiel d&apos;une foret (surf. soil of a forest)
i3 012670 Humidite du sol Coordination&apos;
humidite et la temperature du sol
</equation>
<bodyText confidence="0.713783266666667">
(moisture and the temperature of the soil)
i4 007034 Temperature du sol Occurrence
temperature du sol (temperature of the soil)
i5 007035 Analyse de sol VerbTra nsfi
analysees dans le sol (analyzed in the soil)
i6 007809 Foret tropicale Occurrence
foret tropicale (tropical forest)
Each variant is obtained by generating term
variations through local transformations com-
posed of an input lexico-syntactic structure
and a corresponding output transformed struc-
ture. Thus, VerbTra nsfi is a verbalization which
transforms a Noun-Preposition-Noun term into
a verb phrase represented by the variation pat-
tern V4 (Adv? (Prep? Art I Prep) A?) N3:1
</bodyText>
<equation confidence="0.997933333333333">
VerbTransfi( N1 PreP2 N3 ) (1)
-= V4 (Adv? (Prep? Art I Prep) A?) N3
{MorphFamily(Ni) = MorphFamily(V4)}
</equation>
<bodyText confidence="0.99437875">
The constraint following the output structure
states that V4 belongs to the same morphologi-
cal family as NI, the head noun of the term.
VerbTra nsfi recognizes analyseesivj dansiprepi
le(Art] sol[N] (analyzed in the soil) as a variant
of analysem deiprepi sol[N] (soil analysis).
Six families of term variations are accounted
for by our implementation for French: coordina-
tion, compounding/decompounding, term em-
bedding, verbalization (of nouns or adjectives),
nominalization (of nouns, adjectives, or verbs),
and adjectivization (of nouns, adjectives, or
verbs). Each index in Table 1 corresponds to
&apos;The following abbreviations are used for the catego-
ries: V = verb, N = noun, Art = article, Adv = adverb,
Conj = conjunction, Prep = preposition, Punc = punc-
tuation.
a unique term; it is referenced by its identifier,
its string, and a unique variation of one of the
aforementioned types (or a plain occurrence).
</bodyText>
<sectionHeader confidence="0.98004" genericHeader="method">
3 Conceptual Phrase Building
</sectionHeader>
<bodyText confidence="0.999970818181818">
The indexes extracted at the preceding step are
text chunks which generally build up a correct
syntactic structure: verb phrases for verbaliza-
tions and, otherwise, noun phrases. When over-
lapping, these indexes can be combined and re-
placed by their head words so as to condense
and structure the documents. This process is
the reverse operation of the noun phrase decom-
position described in (Habert et al., 1996).
The purpose of automatic indexing entails the
following characteristics of indexes:
</bodyText>
<listItem confidence="0.963310714285714">
• frequently, indexes overlap or are embed-
ded one in another (with [AGR-CAND],
35% of the indexes overlap with another
one and 37% of the indexes are embed-
ded in another one; with [AGROVOC], the
rates are respectively 13% and 5%),
• generally, indexes cover only a small fra-
ction of the parsed sentence (with [AGR-
CANDI, the indexes cover, on average, 15%
of the surface; with [AGROVOC], the ave-
rage coverage is 3%),
• generally, indexes do not correspond to
maximal structures and only include part
of the arguments of their head word.
</listItem>
<bodyText confidence="0.99290225">
Because of these characteristics, the construc-
tion of a syntactic structure from indexes is like
solving a puzzle with only part of the clues, and
with a certain overlap between these clues.
</bodyText>
<subsectionHeader confidence="0.995525">
Text Structuring
</subsectionHeader>
<bodyText confidence="0.989786166666666">
The construction of the structure consists of the
following 3 steps:
Step 1. The syntactic head of terms is deter-
mined by a simple noun phrase grammar of the
language under study. For French, the following
regular expression covers 98% of the term struc-
tures in the database [AGROVOC] (Mod is any
adjectival modifier and the syntactic head is the
noun in bold face):
Mod* N N? (Mod (Prep Art? Mod* N N? Mod*))*
The second source of knowledge about synta-
ctic heads is embodied in transformations. For
</bodyText>
<page confidence="0.99365">
596
</page>
<bodyText confidence="0.99849">
instance, the syntactic head of the verbalization
in (1) is the verb in bold typeface.
Step 2. A partial relation between the indexes
of a sentence is now defined in order to rank
in priority the indexes that should be grouped
first into structures (the most deeply embedded
ones). This definition relies on the relative spa-
tial positions of two indexes i and j and their
syntactic heads H(i) and H(j):
Definition 3.1 (Index priority) Let i and j
be two indexes in the same sentence. The rela-
tive priority ranking of i and j is:
</bodyText>
<equation confidence="0.963816">
i R. j &lt;#. (i = j) v (H(i) = H(j) A i g j)
v (H(i) 0 H (j) A H(i) Ej A H (j) fl i)
</equation>
<bodyText confidence="0.987528571428571">
This relation is obviously reflexive. It is nei-
ther transitive nor antisymmetric. It can, howe-
ver, be shown that this relation is not cyclic for
3 elements: iRj A j &apos;R. k --.(kR.i). (This
property is not demonstrated here, due to the
lack of space.)
The linguistic motivations of Definition 3.1
are linked to the composite structure built at
Step 3 according to the relative priorities stated
by R. We now examine, in turn, the 4 cases of
term overlap:
1. Head embedding: 2 indexes i and j, with
a common head word and such that i is
embedded into j, build a 2-level structure:
</bodyText>
<equation confidence="0.940738333333333">
+
H(i)
H(i)
</equation>
<bodyText confidence="0.920844571428571">
This structuring is illustrated by nappe
d&apos;eau (sheet of water) which combines
with nappe d&apos;eau souterraine (underground
sheet of water) and produces the 2-level
structure [[nappe d&apos;eau] souterraine] ([un-
derground Fii-ei of water]]). (Head words
are underlined.) In this case, i has a higher
priority than j; it corresponds to (H(i) =
H(j) A i C j) in Definition 3.1.
2. Argument embedding: 2 indexes i and j,
with different head words and such that the
head word of i belongs to j and the head
word of j does not belong to i, combine as
follows:
</bodyText>
<equation confidence="0.715894333333333">
ilk +
H(j)
11(i)
</equation>
<bodyText confidence="0.999368052631579">
This structuring is illustrated by nappe
d&apos;eau which combines with eau souter-
raine (underground water) and produces
the structure [nappe dleau souterraind
([sheet of [underground water][). Here, i
has a higher priority than j; it corresponds
to (H(i) 0 H(j) A H(i) E j A H(j) cl i)
in Definition 3.1.
3. Head overlap: 2 indexes i and j, with
a common head word and such that i
and j partially overlap, are also combi-
ned at Step 3 by making j a substructure
of i. This combination is, however, non-
deterministic since no priority ordering is
defined between these 2 indexes. There-
fore, it does not correspond to a condition
in Definition 3.1.
In our experiments, this structure cor-
responds to only one situation: a head
word with pre- and post-modifiers such
as importante activite (intense activity)
and activivte de degradation metaboligue
(activity of metabolic degradation).
With [AGR-CAND], this configuration
is encountered only 27 times (.1% of
the index overlaps) because premodifiers
rarely build correct term occurrences in
French. Premodifiers generally correspond
to occasional characteristics such as size,
height, rank, etc.
4. The remaining case of overlapping indexes
with different head words and reciprocal in-
clusions of head words is never encounte-
red. Its presence would undeniably denote
a flaw in the calculus of head words.
Step 3. A bottom-up structure of the sentences
is incrementally built by replacing indexes by
trees. The indexes which are highest ranked by
</bodyText>
<figure confidence="0.928181333333333">
+ N.D.
H(i) H(i)
11(i)
</figure>
<page confidence="0.961996">
597
</page>
<bodyText confidence="0.682374">
the Step 2 are processed first according to the
following bottom-up algorithm:
</bodyText>
<listItem confidence="0.999124461538461">
1. build a depth-1 tree whose daughter nodes
are all the words in the current sentence
and whose head node is S,
2. for all the indexes i in the current sentence,
selected by decreasing order of priority,
(a) mark all the the depth-1 nodes which
are a lexical leaf of i or which are the
head node of a tree with at least one
leaf in i,
(b) replace all the marked nodes by a
unique tree whose head features are
the features of H(i), and whose depth-
1 leaves are all the marked nodes.
</listItem>
<bodyText confidence="0.9629349">
When considering the sentence given in
Table 1, the ordering of the indexes after Step 2
is the following: i2 &gt; i5, i6 &gt; i2, and i4 &gt; i3.
(They all result from the argument embedding
relation.) The algorithm yields the following
structure of the sample sentence:
...la respiration et ses rapports avec rhumidite on: ete anal sees
respiration du sol humidite et la temperature analysies dans le sol
temperature du sol sol superficiel dune Ls@
fore: tropicale
</bodyText>
<subsectionHeader confidence="0.993633">
Text Condensation
</subsectionHeader>
<bodyText confidence="0.972946388888889">
The text structure resulting from this algorithm
condenses the text and brings closer words that
would otherwise remain separated by a large
number of arguments or modifiers. Because of
this condensation, a reindexing of the structu-
red text yields new indexes which are not ex-
tracted at the first step.
Let us illustrate the gains from reindexing
on a sample utterance: l&apos;evolution au cours du
temps du sol et des rendements (temporal evo-
lution of soils and productivity). At the first
step of indexing, evolution au cours du temps
(lit, evolution over time) is recognized as a va-
riant of evolution dans le temps (lit, evolution
with time). At the second step of indexing, the
daughter nodes of the top-most tree build the
condensed text: l&apos;evolution du sol et des rende-
ments (evolution of soils and productivity):
</bodyText>
<figure confidence="0.7307486">
1st step
I evolution au cours du temps du sol et des rendements
2nd step
revolution du sol et des rendements
I evolution au cours du temps
</figure>
<bodyText confidence="0.99883347368421">
This condensed text allows for another index ex-
traction: evolution du sol et des rendements, a
Coordination variant of evolution du rendement
(evolution of productivity). This index was not
visible at the first step because of the additional
modifier au cours du temps (temporal). (Reite-
rated indexing is preferable to too unconstrai-
ned transformations which burden the system
with spurious indexes.)
Both processes—text structuring, presented
here, and term acquisition, described in (Jac-
quemin, 1996)—reinforce each other. On the
one hand, acquisition of new terms increases the
volume of indexes and thereby improves text
structuring by decreasing the non-conceptual
surface of the text. On the other hand, text
condensation triggers the extraction of new in-
dexes, and thereby furnishes new possibilities
for the acquisition of terms.
</bodyText>
<sectionHeader confidence="0.998052" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999690888888889">
Qualitative evaluation: The volume of in-
dexing is characterized by the surface of the
text occupied by terms or their combinations—
we call it the conceptual surface. Figure 2
shows the distribution of the sentences in re-
lation to their conceptual surface. For instance,
in 8,449 sentences among the 62,460 sentences
of [AGRIC], the indexes occupy from 20 to 30%
of the surface (3rd column).
This figure indicates that the structures built
from free indexing are significantly richer than
those obtained from controlled indexing. The
number of sentences is a decreasing exponen-
tial function of their conceptual surface (a linear
function with a log scale on the y axis).
Figure 3 illustrates how the successive steps
of the algorithm contribute to the final size of
the incremental indexing. For each mode of
</bodyText>
<page confidence="0.989274">
598
</page>
<tableCaption confidence="0.504853">
# sentences (log scale)
Table 3: Incremental Structure Building
</tableCaption>
<figure confidence="0.994145833333333">
Total
embedding embedding
Head Argument
10&amp;quot;
0 10 20 30 40 50 60 70 80 90 100
% of conceptual suface
</figure>
<figureCaption confidence="0.998734">
Figure 2: Conceptual Surface of Sentences
</figureCaption>
<tableCaption confidence="0.996478">
Table 2: Increase in the volume of indexing
</tableCaption>
<sectionHeader confidence="0.598823" genericHeader="method">
Acquisition Condensation Total
</sectionHeader>
<bodyText confidence="0.959238333333334">
Controlled 49.3% 3.0% 52.3%
Free 5.8% 4.7% 10.5%
indexing two curves are plotted: the phrases
resulting from initial indexing and from rein-
dexing due to text condensation (circles) and
the phrases due to term acquisition (asterisks).
For instance, at step3, free indexing yields 309
indexes and reindexing 645. The corresponding
percentages are reported in Table 2.
The indexing with the poorest initial volume
(controlled indexing) is the one that benefits
best from term acquisition. Thus, concept com-
bination and term enrichment tend to compen-
sate the deficiencies of the initial term list by
extracting more knowledge from the corpus.
</bodyText>
<page confidence="0.370441">
103
</page>
<figure confidence="0.6111375">
&apos;a1-*.) 104
&apos;8&apos; 103
rn
4) 102
rn
ct$
fa. 10&apos;
10&amp;quot;
1 2 3 4 5 6 7 8
# step
</figure>
<figureCaption confidence="0.999574">
Figure 3: Step-by-step Number of Phrases
</figureCaption>
<bodyText confidence="0.99818375">
Qualitative evaluation: Table 3 indicates the
number of overlapping indexes in relation to
their type. It provides, for each type, the rate of
success of the structuring algorithm. This eva-
</bodyText>
<table confidence="0.965709666666667">
Distribution 27.0% 73.0% 100%
# correct 128 346 474
Precision 79.0% 91.1% 87.5%
</table>
<bodyText confidence="0.611676">
luation results from a human scanning of 542
randomly chosen structures.
</bodyText>
<sectionHeader confidence="0.995484" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999921833333333">
This study has presented CONPARS, a tool
for enhancing the output of an automatic in-
dexer through index combination and term en-
richment. Ongoing work intends to improve the
interaction of indexing and acquisition through
self-indexing of automatically acquired terms.
</bodyText>
<sectionHeader confidence="0.998059" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999156">
Beatrice Daille. 1997. Study and implementa-
tion of combined techniques for automatic ex-
traction of terminology. In J. L. Klavans and
P. Resnik, ed., The Balancing Act: Combi-
ning Symbolic and Statistical Approaches to
Language, p. 49-66. MIT Press, Cambridge.
Benoit Habert, Elie Naulleau, and Adeline Na-
zarenko. 1996. Symbolic word clustering for
medium size corpora. In Proceedings of CO-
LING&apos;96, p. 490-495, Copenhagen.
Christian Jacquemin, Judith L. Klavans, and
Evelyne Tzoukermann. 1997. Expansion of
multi-word terms for indexing and retrieval
using morphology and syntax. In Proceedings
of ACL-EACL&apos;97, p.24-31.
Christian Jacquemin. 1996. A symbolic and
surgical acquisition of terms through varia-
tion. In S. Wermter, E. Riloff, and G. Sche-
ler, ed., Connectionist, Statistical and Symbo-
lic Approaches to Learning for NLP, p. 425-
438. Springer, Heidelberg.
Tomek Strzalkowski. 1996. Natural language
information retrieval. Information Processing
CY Management, 31(3):397-417.
Evelyne Tzoukermann and Dragomir R. Radev.
1997. Use of weighted finite state transducers
in part of speech tagging. In A. Kornai, ed.,
Extended Finite State Models of Language.
Cambridge University Press.
</reference>
<figure confidence="0.867712666666667">
105
104
103
102
10&apos;
— Free indexing
Controlled indexing
Free indexing
Free acquisition
Controlled indexing
Controlled acquisition
•
</figure>
<page confidence="0.97201">
599
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.9949315">Improving Automatic Indexing through Concept Combination and Term Enrichment</title>
<author confidence="0.999983">Christian Jacquemin</author>
<affiliation confidence="0.966142">LIMSI-CNRS</affiliation>
<address confidence="0.996892">BP 133, F-91403 ORSAY Cedex, FRANCE</address>
<email confidence="0.999113">jacquemin@limsi.fr</email>
<abstract confidence="0.998774333333333">Although indexes may overlap, the output of an automatic indexer is generally presented as a flat and unstructured list of terms. Our purpose is to exploit term overlap and embedding so as to yield a substantial qualitative and quantitative improvement in automatic indexing through concept combination. The increase in the volume of indexing is 10.5% for free indexing and 52.3% for controlled indexing. The resulting structure of the indexed corpus is a partial conceptual analysis. 1 Overview The method, proposed here for improving automatic indexing, builds partial syntactic structures by combining overlapping indexes. It is complemented by a method for term acquisition which is described in (Jacquemin, 1996). The text, thus structured, is reindexed; new indexes are produced and new candidates are discovered. Most NLP approaches to automatic indexing concern free indexing and rely on large-scale shallow parsers with a particular concern for dependency relations (Strzalkowski, 1996). For the purpose of controlled indexing, we exploit the output of a NLP-based indexer and the structural relations between terms and variants in order to (1) enhance the coverage of the in- (2) incrementally build an a conceptual analysis of the document, and, (3) interweave controlled indexing, free indexing, and thesaurus acquisition. These 3 goals are achieved by CONPARS (CONceptual PARSer), presented in this paper and illustrated by Figure 1. CONPARS is based on the output of &amp;quot; We thank INIST-CNRS for providing us with thesauri and corpora in the agricultural domain and AFIRST for supporting this research through the SKETCHI project.</abstract>
<note confidence="0.784770388888889">a part-of-speech tagger for French described in (Tzoukermann and Radev, 1997) and FASTR, a controlled indexer (Jacquemin et al., 1997). All the experiments reported in this paper are performed on data in the agricultural domain: [AGRIC] a 1.18-million word corpus, [AGRO- VOC] a 10,570-term controlled vocabulary, and [AGR-CAND] a 15,875-term list acquired by ACABIT (Daille, 1997) from [AGRIC]. Bell Labs Part-of-speech tagger Tagged corpus Morpholog. families Variations FASTR: controlled indexer Increm, ntally indexed corpus Indexed corpus Terms Term enricher CONPARS: Indexed corpus</note>
<abstract confidence="0.977718040935672">Acquisition patterns CONPARS: Phrase builder Yes Local New indexes or new terms or new phrases? grammars No Augmented indexing Figure 1: Overall Architecture of CONPARS 2 Basic Controlled Indexing The preprocessing of the corpus by the tagger yields a morphologically analyzed text, with unambiguous syntactic categories. Then, the tagged corpus is automatically indexed by FASTR which retrieves occurrences of multiword terms or variants (see Table 1). Linguistic rules and dictionary 595 of a Sample Sentence La variation mensuelle de la respiration du sol et ses rapports avec l&apos;humidite et la temperature du sol ont ete analysees dans le sol superficiel d&apos;une tropicale. monthly variation of the respiration of the soil and its connections with the moisture and the temperature of the soil have been analyzed in the surface soil of a tropical forest.) du sol respirationdu sol of the soil) de foret superficiel d&apos;une foret soil of a forest) 012670 du sol humiditeet la temperature du sol (moisture and the temperature of the soil) 007034 du sol temperaturedu sol of the soil) 007035 de sol analyseesdans le sol in the soil) tropicale tropicale forest) variant obtained by generating term variations through local transformations composed of an input lexico-syntactic structure and a corresponding output transformed struc- Thus, VerbTra is a verbalization which transforms a Noun-Preposition-Noun term into a verb phrase represented by the variation pat- Art I Prep) N1 PreP2 ) (1) V4 Art I Prep) N3 {MorphFamily(Ni) = MorphFamily(V4)} following the output structure that to the same morphologifamily as head noun of the term. recognizes sol[N] in the soil) as a variant sol[N] analysis). Six families of term variations are accounted for by our implementation for French: coordination, compounding/decompounding, term embedding, verbalization (of nouns or adjectives), nominalization (of nouns, adjectives, or verbs), and adjectivization (of nouns, adjectives, or verbs). Each index in Table 1 corresponds to &apos;The following abbreviations are used for the catego- V = verb, = Art = article, Adv = adverb, Conj = conjunction, Prep = preposition, Punc = punctuation. a unique term; it is referenced by its identifier, its string, and a unique variation of one of the aforementioned types (or a plain occurrence). 3 Conceptual Phrase Building The indexes extracted at the preceding step are text chunks which generally build up a correct syntactic structure: verb phrases for verbalizations and, otherwise, noun phrases. When overlapping, these indexes can be combined and replaced by their head words so as to condense and structure the documents. This process is the reverse operation of the noun phrase decomposition described in (Habert et al., 1996). The purpose of automatic indexing entails the following characteristics of indexes: • frequently, indexes overlap or are embedded one in another (with [AGR-CAND], 35% of the indexes overlap with another one and 37% of the indexes are embedded in another one; with [AGROVOC], the rates are respectively 13% and 5%), • generally, indexes cover only a small fraction of the parsed sentence (with [AGR- CANDI, the indexes cover, on average, 15% of the surface; with [AGROVOC], the average coverage is 3%), • generally, indexes do not correspond to maximal structures and only include part of the arguments of their head word. Because of these characteristics, the construction of a syntactic structure from indexes is like solving a puzzle with only part of the clues, and with a certain overlap between these clues. Text Structuring The construction of the structure consists of the following 3 steps: 1. syntactic head of terms is determined by a simple noun phrase grammar of the language under study. For French, the following regular expression covers 98% of the term structures in the database [AGROVOC] (Mod is any adjectival modifier and the syntactic head is the noun in bold face): (Mod (Prep Mod* N Mod*))* The second source of knowledge about syntactic heads is embodied in transformations. For 596 instance, the syntactic head of the verbalization in (1) is the verb in bold typeface. Step 2. A partial relation between the indexes of a sentence is now defined in order to rank in priority the indexes that should be grouped first into structures (the most deeply embedded ones). This definition relies on the relative spapositions of two indexes i and their heads 3.1 (Index priority) i and j be two indexes in the same sentence. The relative priority ranking of i and j is: R. j &lt;#. (i = j) v (H(i) = H(j) i (H(i) 0 H (j) A H(i) (j) fl i) This relation is obviously reflexive. It is neither transitive nor antisymmetric. It can, however, be shown that this relation is not cyclic for elements: --.(kR.i). (This property is not demonstrated here, due to the lack of space.) The linguistic motivations of Definition 3.1 are linked to the composite structure built at Step 3 according to the relative priorities stated by R. We now examine, in turn, the 4 cases of term overlap: Head embedding: 2 indexes i and a common head word and such that i is into a 2-level structure: + H(i) H(i) structuring is illustrated by (sheetof water) which combines napped&apos;eau souterraine sheet of water) and produces the 2-level d&apos;eau] souterraine] ([unof water]]). (Head words are underlined.) In this case, i has a higher than corresponds to i C Definition 3.1. Argument embedding: 2 indexes i and with different head words and such that the word of i belongs to the head of not belong to i, combine as follows: ilk + H(j) 11(i) structuring is illustrated by combines with souterraine (underground water) and produces structure [nappedleau souterraind ([sheetof [underground water][).Here, i a higher priority than corresponds cl in Definition 3.1. Head overlap: 2 indexes i and common head word and such that overlap, are also combiat Step 3 by making substructure of i. This combination is, however, nondeterministic since no priority ordering is defined between these 2 indexes. Therefore, it does not correspond to a condition in Definition 3.1. our experiments, this structure coronly one situation: a head word with preand post-modifiers such activite(intense activity) activivtede degradation metaboligue (activityof metabolic degradation). With [AGR-CAND], this configuration is encountered only 27 times (.1% of the index overlaps) because premodifiers rarely build correct term occurrences in French. Premodifiers generally correspond to occasional characteristics such as size, height, rank, etc. 4. The remaining case of overlapping indexes with different head words and reciprocal inclusions of head words is never encountered. Its presence would undeniably denote a flaw in the calculus of head words. 3. bottom-up structure of the sentences is incrementally built by replacing indexes by trees. The indexes which are highest ranked by + N.D. H(i) H(i) 11(i) 597 the Step 2 are processed first according to the following bottom-up algorithm: 1. build a depth-1 tree whose daughter nodes are all the words in the current sentence and whose head node is S, for all the indexes the current sentence, selected by decreasing order of priority, (a) mark all the the depth-1 nodes which a lexical leaf of which are the head node of a tree with at least one leaf in i, (b) replace all the marked nodes by a unique tree whose head features are features of whose depth- 1 leaves are all the marked nodes. When considering the sentence given in Table 1, the ordering of the indexes after Step 2 the following: &gt; i5, i6 &gt; i2, and i4 &gt; (They all result from the argument embedding relation.) The algorithm yields the following structure of the sample sentence: respirationet ses rapports avec rhumiditeon: ete anal sees du sol humidite et la temperatureanalysies dans le sol temperature du sol sol superficiel dune Ls@ fore: tropicale Text Condensation The text structure resulting from this algorithm condenses the text and brings closer words that would otherwise remain separated by a large number of arguments or modifiers. Because of this condensation, a reindexing of the structured text yields new indexes which are not extracted at the first step. Let us illustrate the gains from reindexing a sample utterance: au cours du du sol et des rendements evolution of soils and productivity). At the first of indexing, au cours du temps (lit, evolution over time) is recognized as a vaof dans le temps evolution with time). At the second step of indexing, the daughter nodes of the top-most tree build the text: du sol et des rendeof soils and productivity): 1st step I evolution au cours du temps du sol et des rendements 2nd step revolutiondu sol et des rendements I evolution au cours du temps This condensed text allows for another index exdu sol et des rendements, variant of du rendement (evolution of productivity). This index was not visible at the first step because of the additional au du temps (Reiterated indexing is preferable to too unconstrained transformations which burden the system with spurious indexes.) Both processes—text structuring, presented here, and term acquisition, described in (Jacquemin, 1996)—reinforce each other. On the one hand, acquisition of new terms increases the volume of indexes and thereby improves text structuring by decreasing the non-conceptual surface of the text. On the other hand, text condensation triggers the extraction of new indexes, and thereby furnishes new possibilities for the acquisition of terms. 4 Evaluation evaluation: volume of indexing is characterized by the surface of the text occupied by terms or their combinations— call it the surface. 2 shows the distribution of the sentences in relation to their conceptual surface. For instance, in 8,449 sentences among the 62,460 sentences of [AGRIC], the indexes occupy from 20 to 30% of the surface (3rd column). This figure indicates that the structures built from free indexing are significantly richer than those obtained from controlled indexing. The number of sentences is a decreasing exponential function of their conceptual surface (a linear with a log scale on the Figure 3 illustrates how the successive steps of the algorithm contribute to the final size of the incremental indexing. For each mode of 598 Table 3: Incremental Structure Building Total embedding embedding Head Argument 10&amp;quot; 0 10 20 30 40 50 60 70 80 90 100 % of conceptual suface Figure 2: Conceptual Surface of Sentences Table 2: Increase in the volume of indexing Acquisition Condensation Total Controlled 49.3% 3.0% 52.3% 4.7% 10.5% indexing two curves are plotted: the phrases resulting from initial indexing and from reindexing due to text condensation (circles) and the phrases due to term acquisition (asterisks). For instance, at step3, free indexing yields 309 indexes and reindexing 645. The corresponding percentages are reported in Table 2. The indexing with the poorest initial volume (controlled indexing) is the one that benefits best from term acquisition. Thus, concept combination and term enrichment tend to compensate the deficiencies of the initial term list by extracting more knowledge from the corpus. rn 4) rn ct$ 10&amp;quot; 1 2 3 4 5 6 7 8 Figure 3: Step-by-step Number of Phrases Qualitative evaluation: Table 3 indicates the number of overlapping indexes in relation to their type. It provides, for each type, the rate of of the structuring algorithm. This eva- Distribution 27.0% 73.0% 100% Precision 79.0% 91.1% 87.5% luation results from a human scanning of 542 randomly chosen structures. 5 Conclusion This study has presented CONPARS, a tool for enhancing the output of an automatic indexer through index combination and term enrichment. Ongoing work intends to improve the interaction of indexing and acquisition through self-indexing of automatically acquired terms. References Daille. Study implementation of combined techniques for automatic extraction of terminology. In J. L. Klavans and</abstract>
<note confidence="0.512375222222222">Resnik, ed., Balancing Act: Combining Symbolic and Statistical Approaches to 49-66. MIT Press, Cambridge. Benoit Habert, Elie Naulleau, and Adeline Nazarenko. 1996. Symbolic word clustering for size corpora. In of CO- 490-495, Copenhagen. Christian Jacquemin, Judith L. Klavans, and Evelyne Tzoukermann. 1997. Expansion of multi-word terms for indexing and retrieval morphology and syntax. In ACL-EACL&apos;97, Christian Jacquemin. 1996. A symbolic and surgical acquisition of terms through variation. In S. Wermter, E. Riloff, and G. Scheed., Statistical and Symbo- Approaches to Learning for NLP, 425- 438. Springer, Heidelberg. Tomek Strzalkowski. 1996. Natural language retrieval. Processing Management, Evelyne Tzoukermann and Dragomir R. Radev. 1997. Use of weighted finite state transducers in part of speech tagging. In A. Kornai, ed., Extended Finite State Models of Language. Cambridge University Press. 10&apos;</note>
<title confidence="0.8887982">Free indexing Controlled indexing Free indexing Free acquisition Controlled indexing</title>
<note confidence="0.422673333333333">Controlled acquisition • 599</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Beatrice Daille</author>
</authors>
<title>Study and implementation of combined techniques for automatic extraction of terminology.</title>
<date>1997</date>
<booktitle>The Balancing Act: Combining Symbolic and Statistical Approaches to Language,</booktitle>
<pages>49--66</pages>
<editor>In J. L. Klavans and P. Resnik, ed.,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="2210" citStr="Daille, 1997" startWordPosition="334" endWordPosition="335">his paper and illustrated by Figure 1. CONPARS is based on the output of &amp;quot; We thank INIST-CNRS for providing us with thesauri and corpora in the agricultural domain and AFIRST for supporting this research through the SKETCHI project. a part-of-speech tagger for French described in (Tzoukermann and Radev, 1997) and FASTR, a controlled indexer (Jacquemin et al., 1997). All the experiments reported in this paper are performed on data in the agricultural domain: [AGRIC] a 1.18-million word corpus, [AGROVOC] a 10,570-term controlled vocabulary, and [AGR-CAND] a 15,875-term list acquired by ACABIT (Daille, 1997) from [AGRIC]. Bell Labs Part-of-speech tagger Tagged corpus Morpholog. families Variations FASTR: controlled indexer Increm, ntally indexed corpus Indexed corpus Terms Term enricher CONPARS: Indexed corpus Acquisition patterns CONPARS: Phrase builder Yes Local New indexes grammars or new terms or new phrases? No Augmented indexing Figure 1: Overall Architecture of CONPARS 2 Basic Controlled Indexing The preprocessing of the corpus by the tagger yields a morphologically analyzed text, with unambiguous syntactic categories. Then, the tagged corpus is automatically indexed by FASTR which retriev</context>
</contexts>
<marker>Daille, 1997</marker>
<rawString>Beatrice Daille. 1997. Study and implementation of combined techniques for automatic extraction of terminology. In J. L. Klavans and P. Resnik, ed., The Balancing Act: Combining Symbolic and Statistical Approaches to Language, p. 49-66. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Habert</author>
<author>Elie Naulleau</author>
<author>Adeline Nazarenko</author>
</authors>
<title>Symbolic word clustering for medium size corpora.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING&apos;96,</booktitle>
<pages>490--495</pages>
<location>Copenhagen.</location>
<contexts>
<context position="5643" citStr="Habert et al., 1996" startWordPosition="871" endWordPosition="874"> preposition, Punc = punctuation. a unique term; it is referenced by its identifier, its string, and a unique variation of one of the aforementioned types (or a plain occurrence). 3 Conceptual Phrase Building The indexes extracted at the preceding step are text chunks which generally build up a correct syntactic structure: verb phrases for verbalizations and, otherwise, noun phrases. When overlapping, these indexes can be combined and replaced by their head words so as to condense and structure the documents. This process is the reverse operation of the noun phrase decomposition described in (Habert et al., 1996). The purpose of automatic indexing entails the following characteristics of indexes: • frequently, indexes overlap or are embedded one in another (with [AGR-CAND], 35% of the indexes overlap with another one and 37% of the indexes are embedded in another one; with [AGROVOC], the rates are respectively 13% and 5%), • generally, indexes cover only a small fraction of the parsed sentence (with [AGRCANDI, the indexes cover, on average, 15% of the surface; with [AGROVOC], the average coverage is 3%), • generally, indexes do not correspond to maximal structures and only include part of the argument</context>
</contexts>
<marker>Habert, Naulleau, Nazarenko, 1996</marker>
<rawString>Benoit Habert, Elie Naulleau, and Adeline Nazarenko. 1996. Symbolic word clustering for medium size corpora. In Proceedings of COLING&apos;96, p. 490-495, Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Jacquemin</author>
<author>Judith L Klavans</author>
<author>Evelyne Tzoukermann</author>
</authors>
<title>Expansion of multi-word terms for indexing and retrieval using morphology and syntax.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL-EACL&apos;97,</booktitle>
<pages>24--31</pages>
<contexts>
<context position="1965" citStr="Jacquemin et al., 1997" startWordPosition="296" endWordPosition="299">erage of the indexes, (2) incrementally build an a posteriori conceptual analysis of the document, and, (3) interweave controlled indexing, free indexing, and thesaurus acquisition. These 3 goals are achieved by CONPARS (CONceptual PARSer), presented in this paper and illustrated by Figure 1. CONPARS is based on the output of &amp;quot; We thank INIST-CNRS for providing us with thesauri and corpora in the agricultural domain and AFIRST for supporting this research through the SKETCHI project. a part-of-speech tagger for French described in (Tzoukermann and Radev, 1997) and FASTR, a controlled indexer (Jacquemin et al., 1997). All the experiments reported in this paper are performed on data in the agricultural domain: [AGRIC] a 1.18-million word corpus, [AGROVOC] a 10,570-term controlled vocabulary, and [AGR-CAND] a 15,875-term list acquired by ACABIT (Daille, 1997) from [AGRIC]. Bell Labs Part-of-speech tagger Tagged corpus Morpholog. families Variations FASTR: controlled indexer Increm, ntally indexed corpus Indexed corpus Terms Term enricher CONPARS: Indexed corpus Acquisition patterns CONPARS: Phrase builder Yes Local New indexes grammars or new terms or new phrases? No Augmented indexing Figure 1: Overall Arc</context>
</contexts>
<marker>Jacquemin, Klavans, Tzoukermann, 1997</marker>
<rawString>Christian Jacquemin, Judith L. Klavans, and Evelyne Tzoukermann. 1997. Expansion of multi-word terms for indexing and retrieval using morphology and syntax. In Proceedings of ACL-EACL&apos;97, p.24-31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Jacquemin</author>
</authors>
<title>A symbolic and surgical acquisition of terms through variation.</title>
<date>1996</date>
<booktitle>Connectionist, Statistical and Symbolic Approaches to Learning for NLP,</booktitle>
<pages>425--438</pages>
<editor>In S. Wermter, E. Riloff, and G. Scheler, ed.,</editor>
<publisher>Springer,</publisher>
<location>Heidelberg.</location>
<contexts>
<context position="890" citStr="Jacquemin, 1996" startWordPosition="132" endWordPosition="133"> unstructured list of terms. Our purpose is to exploit term overlap and embedding so as to yield a substantial qualitative and quantitative improvement in automatic indexing through concept combination. The increase in the volume of indexing is 10.5% for free indexing and 52.3% for controlled indexing. The resulting structure of the indexed corpus is a partial conceptual analysis. 1 Overview The method, proposed here for improving automatic indexing, builds partial syntactic structures by combining overlapping indexes. It is complemented by a method for term acquisition which is described in (Jacquemin, 1996). The text, thus structured, is reindexed; new indexes are produced and new candidates are discovered. Most NLP approaches to automatic indexing concern free indexing and rely on large-scale shallow parsers with a particular concern for dependency relations (Strzalkowski, 1996). For the purpose of controlled indexing, we exploit the output of a NLP-based indexer and the structural relations between terms and variants in order to (1) enhance the coverage of the indexes, (2) incrementally build an a posteriori conceptual analysis of the document, and, (3) interweave controlled indexing, free ind</context>
<context position="12749" citStr="Jacquemin, 1996" startWordPosition="2105" endWordPosition="2107">ours du temps du sol et des rendements 2nd step revolution du sol et des rendements I evolution au cours du temps This condensed text allows for another index extraction: evolution du sol et des rendements, a Coordination variant of evolution du rendement (evolution of productivity). This index was not visible at the first step because of the additional modifier au cours du temps (temporal). (Reiterated indexing is preferable to too unconstrained transformations which burden the system with spurious indexes.) Both processes—text structuring, presented here, and term acquisition, described in (Jacquemin, 1996)—reinforce each other. On the one hand, acquisition of new terms increases the volume of indexes and thereby improves text structuring by decreasing the non-conceptual surface of the text. On the other hand, text condensation triggers the extraction of new indexes, and thereby furnishes new possibilities for the acquisition of terms. 4 Evaluation Qualitative evaluation: The volume of indexing is characterized by the surface of the text occupied by terms or their combinations— we call it the conceptual surface. Figure 2 shows the distribution of the sentences in relation to their conceptual sur</context>
</contexts>
<marker>Jacquemin, 1996</marker>
<rawString>Christian Jacquemin. 1996. A symbolic and surgical acquisition of terms through variation. In S. Wermter, E. Riloff, and G. Scheler, ed., Connectionist, Statistical and Symbolic Approaches to Learning for NLP, p. 425-438. Springer, Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomek Strzalkowski</author>
</authors>
<title>Natural language information retrieval.</title>
<date>1996</date>
<booktitle>Information Processing CY Management,</booktitle>
<pages>31--3</pages>
<contexts>
<context position="1168" citStr="Strzalkowski, 1996" startWordPosition="172" endWordPosition="173">.3% for controlled indexing. The resulting structure of the indexed corpus is a partial conceptual analysis. 1 Overview The method, proposed here for improving automatic indexing, builds partial syntactic structures by combining overlapping indexes. It is complemented by a method for term acquisition which is described in (Jacquemin, 1996). The text, thus structured, is reindexed; new indexes are produced and new candidates are discovered. Most NLP approaches to automatic indexing concern free indexing and rely on large-scale shallow parsers with a particular concern for dependency relations (Strzalkowski, 1996). For the purpose of controlled indexing, we exploit the output of a NLP-based indexer and the structural relations between terms and variants in order to (1) enhance the coverage of the indexes, (2) incrementally build an a posteriori conceptual analysis of the document, and, (3) interweave controlled indexing, free indexing, and thesaurus acquisition. These 3 goals are achieved by CONPARS (CONceptual PARSer), presented in this paper and illustrated by Figure 1. CONPARS is based on the output of &amp;quot; We thank INIST-CNRS for providing us with thesauri and corpora in the agricultural domain and AF</context>
</contexts>
<marker>Strzalkowski, 1996</marker>
<rawString>Tomek Strzalkowski. 1996. Natural language information retrieval. Information Processing CY Management, 31(3):397-417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evelyne Tzoukermann</author>
<author>Dragomir R Radev</author>
</authors>
<title>Use of weighted finite state transducers in part of speech tagging.</title>
<date>1997</date>
<editor>In A. Kornai, ed.,</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1908" citStr="Tzoukermann and Radev, 1997" startWordPosition="287" endWordPosition="290">ons between terms and variants in order to (1) enhance the coverage of the indexes, (2) incrementally build an a posteriori conceptual analysis of the document, and, (3) interweave controlled indexing, free indexing, and thesaurus acquisition. These 3 goals are achieved by CONPARS (CONceptual PARSer), presented in this paper and illustrated by Figure 1. CONPARS is based on the output of &amp;quot; We thank INIST-CNRS for providing us with thesauri and corpora in the agricultural domain and AFIRST for supporting this research through the SKETCHI project. a part-of-speech tagger for French described in (Tzoukermann and Radev, 1997) and FASTR, a controlled indexer (Jacquemin et al., 1997). All the experiments reported in this paper are performed on data in the agricultural domain: [AGRIC] a 1.18-million word corpus, [AGROVOC] a 10,570-term controlled vocabulary, and [AGR-CAND] a 15,875-term list acquired by ACABIT (Daille, 1997) from [AGRIC]. Bell Labs Part-of-speech tagger Tagged corpus Morpholog. families Variations FASTR: controlled indexer Increm, ntally indexed corpus Indexed corpus Terms Term enricher CONPARS: Indexed corpus Acquisition patterns CONPARS: Phrase builder Yes Local New indexes grammars or new terms or</context>
</contexts>
<marker>Tzoukermann, Radev, 1997</marker>
<rawString>Evelyne Tzoukermann and Dragomir R. Radev. 1997. Use of weighted finite state transducers in part of speech tagging. In A. Kornai, ed., Extended Finite State Models of Language. Cambridge University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>