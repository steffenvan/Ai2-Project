<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000382">
<title confidence="0.985748">
The information-processing difficulty of incremental parsing
</title>
<author confidence="0.997102">
John Hale
</author>
<affiliation confidence="0.9956215">
Department of Linguistics and Languages
Michigan State University
</affiliation>
<address confidence="0.940952">
East Lansing, MI 48824-1027
</address>
<email confidence="0.999482">
jthale@msu.edu
</email>
<sectionHeader confidence="0.993909" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999667545454546">
When an incremental parser gets the next word,
its expectations about upcoming grammatical struc-
tures can change. When a word greatly constrains
these grammatical expectations, uncertainty is re-
duced. This elimination of possibilities constitutes
information processing work. Formalizing this no-
tion of information processing work yields a com-
plexity metric that predicts human repetition ac-
curacy scores across a systematic class of linguis-
tic phenomena, the Accessibility Hierarchy of rela-
tivizable grammatical relations.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990640377049181">
An attractive hypothesis in psycholinguistics, dat-
ing back at least to the 1950s, has been that
the degree of predictability of words in sentences
is somehow related to understandability (Taylor,
1953), production difficulty (Goldman-Eisler, 1958)
or, more recently, eye-movements (McDonald and
Shillcock, 2003). However, since the 1950s, inte-
grating this hypothesis with realistic models of lin-
guistic structure has remained a challenge.
Lounsbury (1954) appreciated the formal char-
acter of the problem. He defined a finite, artificial
language, endowed with a rudimentary phonology,
morphology and syntax, and showed that a word’s
informational contribution could be formally de-
fined as the entropy reduction brought about by its
addition to the end of a sentence fragment. He qual-
ified the significance of his achievement, saying
An entropy reduction analysis presupposes
that the number of possible messages is finite,
and that the probabilities of each of the mes-
sages is known....Thus it appears that the en-
tropy reduction analysis could be applied only
to limited classes of natural language mes-
sages since the number of messages in nearly
all languages is indefinitely large
(Lounsbury, 1954, 108)
A fuller presentation of this work can be found in
Hale (forthcoming).
The present paper extends Lounsbury’s original
idea to infinite languages, by applying two classi-
cal ideas in (probabilistic) formal language theory:
Grenander’s (1967) closed-form solution for the en-
tropy of a nonterminal in a probabilistic context-free
phrase structure grammar, and Lang’s (1974; 1988)
insight that an intermediate parser state is itself a
specification of a grammar.
This extension permits the psycholinguistic hy-
pothesis ERH to be examined.
Entropy Reduction Hypothesis (ERH) a per-
son’s processing difficulty at a word in a
sentence is directly related to the number of
bits signaled to the person by that word with
respect to a probabilistic grammar the person
knows.
In section 2 a method for calculating the entropy
reduction of a word in a sentence generated by a
probabilistic grammar is presented. Section 3 de-
scribes the empirical domain of interest, the Acces-
sibility Hierarchy (Keenan and Comrie, 1977). Sec-
tion 4 goes on to describe two probabilistic gram-
mars in the class of mildly context-sensitive Min-
imalist Grammars (Stabler, 1997). One expresses
the “promotion analysis” (Kayne, 1994) of relative
clauses while the other expresses the more standard
“adjunction analysis” (Chomsky, 1977). The pre-
dictions of these grammars through the lens of the
ERH are considered in sections 5 through 7, where
it is shown that predictions derived from the pro-
motion analysis match human repetition accuracy
scores better than predictions derived from the ad-
junction analysis. Section 8 concludes.
</bodyText>
<sectionHeader confidence="0.982424" genericHeader="introduction">
2 Entropy Reduction
</sectionHeader>
<bodyText confidence="0.9998345">
The idea of the entropy reduction of a word is that
uncertainty about grammatical continuations fluctu-
ates as new words come in. The ERH is the pro-
posal that fluctuations in this value be taken as psy-
cholinguistic predictions. This proposal is founded
on the possibility of viewing nonterminal symbols
the vector of the h(ξi) and A is a matrix whose
(i, j)th component gives the expected number of
nonterminals of type j resulting from nonterminals
of type i.
in probabilistic grammars as random variables. For
instance, in the rules given below,
</bodyText>
<note confidence="0.951493">
0.87 NP the boy
0.13 NP the tall boy
</note>
<bodyText confidence="0.9935635">
the nonterminal NP can be viewed as a random
variable that has two alternative outcomes. Indeed,
nonterminals generally in probabilistic context-free
phrase structure grammars (PCFGs) can be viewed
this way. Since their outcomes are discrete, their
entropy H is easily calculated
</bodyText>
<equation confidence="0.9979152">
H(X) = − � p(x) lo92 p(x) (1)
xEX
H(NP) = − [(0.87 x lo92 0.87)
+(0.13 x lo92 0.13)]
� 0.56 bits
</equation>
<bodyText confidence="0.999546777777778">
There is just over half a bit of uncertainty about
how NP is going to rewrite, because the outcome
is so heavily weighted towards the first alternative.
In this simple example there is no recursion, so the
generated language is finite. To obtain the uncer-
tainty about infinite PCFG languages, a recursive
relation due to Grenander (1967) can be used to cal-
culate the entropy of the start symbol S which be-
gins all derivations.
</bodyText>
<subsectionHeader confidence="0.998189">
2.1 Entropy of nonterminals in a PCFG
</subsectionHeader>
<bodyText confidence="0.985449833333333">
Grenander’s theorem is a recurrence relation that
gives the entropy of each nonterminal in a PCFG G
as the sum of two terms. Let the set of production
rules in G be H and the subset rewriting nontermi-
nal ξ be H(ξ). Denote by pr the probability of a rule
r having daughters ξj1, ξj2,.... Then
</bodyText>
<equation confidence="0.996922166666667">
h(ξi) = − � pr lo92 pr
rEH(ξi)
H(ξi) = h(ξi) + � pr [H(ξj1)
rEH(ξi)
+H(ξj2) + ···]
(Grenander, 1967, 19)
</equation>
<bodyText confidence="0.999648625">
the first term, lowercase h, is simply the definition
of entropy for a discrete random variable. The sec-
ond term, uppercase H, is the recurrence. It ex-
presses the intuition that derivational uncertainty is
propagated from children to parents.
For PCFGs that define a probability distribution,
the solution to this recurrence can be written as a
matrix equation where I is the identity matrix, h�
</bodyText>
<equation confidence="0.995044">
H = (I − A)−1h (2)
</equation>
<subsectionHeader confidence="0.994415">
2.2 Incomplete sentences
</subsectionHeader>
<bodyText confidence="0.99993712">
Grenander’s theorem supplies the entropy for any
PCFG nonterminal in one step by inverting a ma-
trix. To determine the contribution of a particu-
lar word, one would like to be able to look at the
change in uncertainty about compatible derivations
as a given prefix string is lengthened. When this set,
the set of derivations generating a given string w =
w0w1 ... wn as a left prefix, is finite, it can be ex-
pressed as a list. In the case of a recursive grammar
this set is not finite and some other representation is
necessary.
Lang and Billot observe (1974; 1988; 1989) that
the incremental state of a parser can be described
by another, related grammar. They view parsing as
the intersection of a grammar with a regular lan-
guage, of which ordinary strings are but the simplest
examples. This perspective readily accommodates
incomplete sentences as regular languages whose
members all have the same initial n words but con-
tinue with all possible words of the terminal vocabu-
lary, for all possible lengths. If L(G) is the language
of the grammar G, parsing an initial substring w is
the intersection depicted in 3 where the period de-
notes any terminal symbol of G and the Kleene star
indicates any number of repetitions.
</bodyText>
<equation confidence="0.969885">
w(.)∗ n L(G) (3)
</equation>
<bodyText confidence="0.96044047368421">
The result of this intersection is a new context-
free grammar describing just the derivations whose
yield begins with the string w. By generalizing the
input from a single string to a regular set of strings,
the grammatical continuations can be captured in
the new, output grammar. These grammars are eas-
ily read off of chart parsers’ internal data structures
by attaching position indices to nonterminal names,
thus distinguishing recognized constituents in dif-
ferent positions.
The uncertainty associated with the the start sym-
bol of this new, resultant grammar is the conditional
entropy H(S|w1, w2, · · · wn). The entropy reduc-
tion of word wn+1 then is the downward change in
this value as the string w is made one word longer.
The proposal of the ERH is that these changes mea-
sure the disambiguation work the comprehender has
performed by ruling out possible syntactic analyses.
SUBJECT ⊃ DIR. OBJECT ⊃ INDIR. OBJECT ⊃ OBLIQUE ⊃ GENITIVE ⊃ OCOMP
</bodyText>
<figureCaption confidence="0.99599">
Figure 1: The Accessibility Hierarchy of relativizable grammatical relations
</figureCaption>
<sectionHeader confidence="0.943421" genericHeader="method">
3 The Accessibility Hierarchy
</sectionHeader>
<bodyText confidence="0.997617488372093">
This paper examines the processing predictions of
the ERH on a systematic class of relative clause
types, the Accessibility Hierarchy (AH) shown in
figure 1. The AH is an implicational markedness
hierarchy of grammatical relations discovered by
Keenan and Comrie in (1977). The implication is
that if a language has a relative-clause formation
rule applicable to grammatical relations at some
point x on the AH, then it can also form relative
clauses on grammatical relations listed at all points
before x.
This hierarchy shows up in a variety of mod-
ern syntactic theories that have been influenced by
Relational Grammar (Perlmutter and Postal, 1974).
In Head-driven Phrase Structure Grammar (Pollard
and Sag, 1994) the hierarchy corresponds to the
order of elements on the SUBCAT list, and inter-
acts with other principles in explanations of bind-
ing facts. The hierarchy also figures in Lexical-
Functional Grammar (Bresnan, 1982) where it is
known as Syntactic Rank.
Keenan and Comrie speculated that their typo-
logical generalization might have a basis in per-
formance factors. This idea was examined in
a repetition-accuracy experiment carried out in
1974 but not published until 1987. Subjects in this
study repeated back stimulus sentences after a delay
while under the additional memory load of a digit-
memory task. Stimuli were subject-modifying rel-
ative clauses embedded in one of four carrier sen-
tence frames, exemplified in figure 2.
subject extracted they had forgotten that the boy who
told the story was so young
direct object extracted the fact that the cat which
David showed to the man likes eggs is strange
indirect object extracted I know that the man who
Stephen explained the accident to is kind
oblique extracted he remembered that the food which
Chris paid the bill for was cheap
genitive subject extracted they had forgotten that the
girl whose friend bought the cake was waiting
genitive object extracted the fact that the sailor whose
ship Jim took had one leg is important
</bodyText>
<figureCaption confidence="0.952939">
Figure 2: Relative clauses in each of four carrier
sentence types
</figureCaption>
<bodyText confidence="0.6566205">
The results of the human study, given in figure 3,
SU DO IO OBL GenS GenO
</bodyText>
<figureCaption confidence="0.7348705">
406 364 342 279 167 171
Figure 3: results from Keenan &amp; Hawkins (1987)
</figureCaption>
<bodyText confidence="0.999978545454546">
show that repetition accuracy1 declines across the
AH. Keenan and Hawkins (1987) note however that
“It remains unexplained just why RCs should be
more difficult to comprehend-produce as they are
formed on positions lower on the AH.”
The ERH, if correct, would offer just such an ex-
planation. If a person’s difficulty on each word of
a sentence is related to derivational information sig-
naled by that word, then the total difficulty reading
a sentence ought to be the sum of the difficulty on
each word2.
</bodyText>
<sectionHeader confidence="0.970885" genericHeader="method">
4 Minimalist Grammars
</sectionHeader>
<bodyText confidence="0.9999472">
If correct, the ERH would explain the increasing
difficulty across the AH in terms of greater or lesser
uncertainty about intermediate parser states. To cal-
culate these predictions, some assumption must be
made about what those structures are.
</bodyText>
<subsectionHeader confidence="0.999057">
4.1 Two analyses of relativization
</subsectionHeader>
<bodyText confidence="0.999982153846154">
Toward this end, two grammars covering the
Keenan and Hawkins stimuli were written in the
Minimalist Grammars (Stabler, 1997) formalism.
These grammars were exactly the same except for
their treatment of relative clauses.
One grammar expresses the usual analysis of rel-
ative clauses as right-adjoined modifiers (Chomsky,
1977). The other expresses the promotion analysis
of relative clause. The analysis, which dates back to
the 1960s, is revived in Kayne (1994). For reasons
having to do with Kayne’s general theory of phrase
structure, he proposes that, in a sentence like 1, the
underlying form of the subject is akin to 2.
</bodyText>
<footnote confidence="0.940106111111111">
1Each response was coded for accuracy on a 0-2 scale where
2 means perfect repetition and 1 suggests minor, grammatical
errors. A score of 0 was assigned when the response did not
include a relative clause of the indicated grammatical function.
Cf.Keenan and Hawkins (1987)
2Summation naturally extends the word-by-word complex-
ity metric ERH to the sentence level. In word-by-word self-
paced reading, evidence for the Accessibility Hierarchy is lim-
ited (cf. chapter 5 of Hale (2003)).
</footnote>
<bodyText confidence="0.9200985">
repetition
accuracy
</bodyText>
<listItem confidence="0.9896135">
(1) the boy who the father explained the answer
to was honest
(2) [IP the father explained the answer to
[DP[+wh] who boy[+f] ] ]
</listItem>
<bodyText confidence="0.999278571428571">
According to Kayne, at an early stage (2) of syn-
tactic derivation, the determiner phrase (DP) “who
boy” occupies what will eventually be the gap posi-
tion. This DP moves to a specifier position of the en-
closing, empty-headed (C0) complementizer phrase
(CP), thereby checking a feature +wh as indicated
in 3.
</bodyText>
<listItem confidence="0.879404">
(3) [CP [DP who boy[+f] ]i C0 [IP the father ex-
plained the answer to ti ] ]
</listItem>
<bodyText confidence="0.97982">
In a second movement, “boy” evacuates from DP,
moving to another specifier (perhaps that of the
silent agreement morpheme, Agr) as in 4 – checking
a different feature, +f.
(4) [AgrP boyj Agr [CP [DP who tj ]i C0 [IP the
father explained the answer to ti ] ] ]
The entire structure becomes a complement of a de-
terminer to yield a larger DP in 5.
is that MGs are equivalent to Multiple context-
free grammars (Seki et al., 1991). Multiple context-
free grammars generalize standard context-free
grammars by allowing the string yields of daugh-
ter categories to be manipulated by a function other
than simple concatenation. As in Tree Adjoin-
ing Grammar (Joshi et al., 1975) a record of these
manipulations is kept at each node of an MG deriva-
tion tree, while a picture of the result is manifested
in derived trees such as the ones in figures 4 and 5.
The derivation tree on the promotion grammar is
shown4 in figure 6 for the substring “the boy who
the father explained the answer to.”
</bodyText>
<figure confidence="0.9505365">
d -case
::=c_rel d -case c_rel
+wh_rel c_rel,-wh_rel
::=t +wh_rel c_rel t,-wh_rel
+case t,-case,-wh_rel
::=&gt;little_v +case t little_v,-case,-wh_rel
=d little_v,-wh_rel d -case
(5) [DP the [AgrP boyj Agr [CP [DP who tj ]i C0
[IP the father explained the answer to ti ] ] ]
]
::=&gt;v =d little_v v,-wh_rel ::=Num d -case Num
+case v,-case,-wh_rel ::=n Num ::n
</figure>
<bodyText confidence="0.999184777777778">
No adjunction is used in this derivation, and, un-
conventionally, the leftmost “the” and “boy” do not
share an exclusive common constituent. Nor is the
wh-word “who” co-indexed with anything. Struc-
tural descriptions involving both the Kaynian anal-
ysis and the more standard adjunction analysis are
shown in figures 4 and 5 respectively3. The other
linguistic assumptions suggested by these diagrams
are discussed in chapter 4 of Hale (2003).
</bodyText>
<subsectionHeader confidence="0.944582">
4.2 Formal grammars of relativization
</subsectionHeader>
<bodyText confidence="0.999468833333333">
The Minimalist Grammars (MG) formalism (cf.
Stabler and Keenan (2003) for a systematic pre-
sentation) facilitates the relatively transparent im-
plementation of ideas like movement and fea-
ture checking that figure prominently in the two
analyses of relativization discussed in the previ-
ous subsection. MGs define a set of sentences
by closing the structure-building functions merge
and move on a finite set of lexical entries; how-
ever, this does not mean that parsing must happen
bottom-up. A fundamental result, obtained inde-
pendently by Harkema (2001) and Michaelis (2001)
</bodyText>
<footnote confidence="0.8956895">
3The X-bar structures depicted in figures 4 and 5 are drawn
using tools developed by Edward Stabler and colleagues.
</footnote>
<table confidence="0.872411375">
=d +case v,-wh_rel d -case
::=p_to =d +case v p_to,-wh_rel
::=&gt;Pto p_to Pto,-wh_rel
+case Pto,-case -wh_rel
::=d +case Pto d -case -wh_rel
+f d -case -wh_rel,-f
::=Num +f d -case -wh_rel Num,-f
::=n Num ::n -f
</table>
<figureCaption confidence="0.998055">
Figure 6: Derivation tree on promotion grammar.
</figureCaption>
<bodyText confidence="0.9959422">
The derivation trees encode everything there is to
know about MG derivations, and can be parsed in
a variety of orders. Most importantly, if equipped
with weights on their branches, they can be gener-
ated by probabilistic context-free grammars.
</bodyText>
<footnote confidence="0.931361">
4These derivation trees are drawn using tools developed by
Maxime Amblard.
</footnote>
<figure confidence="0.998837">
::=Num d -case Num
::=n Num ::n
cP
✭✭✭✭✭✭c’ ❤❤❤❤❤❤
c tP
’
✘ P
dP lit✏✏tlePPv’
t(3) little v ✏✏ vPPP
t dP(2) v’
d’ d✧❜
v
t
answer
eP
Be’
Be
t
t’
✥✥✥ ❵❵❵
tlittle vP
p toP
✏✏p to’PP
p to PtoP
✟ ❍
dP(1) Pto’
t(1)
t(1)
✥✥✥✥
the dP(1)
nP(0)
n’
n
d
who
d’
NumP
Num’
✥✥✥ ❵ ❵ ❵
c rel tP
❵❵❵❵
relc ’
✭
dP(3)
d’
✦✦ t’ ❛❛B
t
Be
be
t
-ed
d
the
NumnP
n’
NumPt(2)
Num’
t(4)
aP
dP
n
a’
a AP
A’
A
Pto
to
p to
v
t
Pto
dP
boy
NumnP
t(0)
d
the
NumP
Num’
✦✦ ❛❛t
little v
-ed
v
little v
NumnP
n’
n
father
explain
dP(4)
✘✘✘d’ ❳❳❳
d c relP
</figure>
<figureCaption confidence="0.99689">
Figure 4: Kaynian promotion analysis
</figureCaption>
<figure confidence="0.900134172413793">
cP
c ’
✭✭✭✭✭✭ ❤❤❤❤❤❤
c tP
✦✦ t’❛❛B
t
✘✘✘ dP(3)
❳❳❳
dP ✘✘✘ c relP ❳❳❳
d’ dP(0) c rel❵’
✥✥✥ ❵ ❵
c rel t P
aP
d’
d
NumP
Num’
d
the
✭✭ ✭ ✭✭dP(2)
t
eP
Be’
Be
t dP
t(3)
d’
who
t
-ed
Be
be
a AP
A
’
’
❳
lit✏✏tlePPv’
little v ✏✏vP PP
t dP(1) v’
d’ d✧❜
honest
v
NumnP
n’
n
boy
✥✥ ❳❳❳
t little vP
✦✦ ❛❛
little v t
-ed
v little v
NumP
Num’
d
the
t(2)
explain
’
NumnP
n’
n
father
v
t
d
the
NumPt(1)
Num’
NumnP
n’
p toP
✏✏p to’PP
p to PtoP
P o’
p to
Pto
to
n
Pto
dP
✟ ❍
dP(0)
t(0)
t(0)
answer t
</figure>
<figureCaption confidence="0.997533">
Figure 5: more standard adjunction analysis
</figureCaption>
<sectionHeader confidence="0.983906" genericHeader="method">
5 Procedure
</sectionHeader>
<bodyText confidence="0.999872">
Derivation trees on both grammars were obtained5
for each of Keenan and Hawkins’ (1987) twenty-
four stimulus sentences6. Branches of these deriva-
tion trees were viewed as PCFG rules with probabil-
ities set according to the usual relative-frequency es-
timation technique (Chi, 1999). However, because
the stimuli were intentionally constructed to have
</bodyText>
<footnote confidence="0.958707">
5Derivations were obtained using a parser described in Ap-
pendix A of Hale (2003)
6To eliminate number agreement as a source of derivational
uncertainty, the results were calculated using a modified stim-
ulus set in which four noun phrases were changed from plural
to singular.
</footnote>
<bodyText confidence="0.9939325">
exactly four examples of each structure, these sen-
tences were weighted in accordance with a corpus
study (Keenan, 1975) to make their relative frequen-
cies more realistic.
</bodyText>
<figure confidence="0.956222">
honest
a
A
</figure>
<sectionHeader confidence="0.995697" genericHeader="method">
6 Results
</sectionHeader>
<bodyText confidence="0.99996125">
The summed entropy reductions exhibit a signifi-
cant correlation with the repetition accuracy scores
collected by Keenan and Hawkins (1987).
The correlation in figure 7(a) obtains only on the
grammar expressing the Kaynian promotion anal-
ysis, and not on the grammar expressing the stan-
dard adjunction analysis (figure 7(b)). Nor do log-
probabilitiesfor stimulus sentences on the grammar
</bodyText>
<figure confidence="0.801273545454545">
Accessibility Hierarchy
total promotion grammar
bits reduced r2=0.45, p&lt;0.001
250 300 350 400 450 500
Accessibility Hierarchy
total adjunction grammar
bits reduced r2=0.02, n.s.
250 300 350 400 450 500
55
50
45
40
35
30
75
70
65
60
55
50
error score
error score
</figure>
<figureCaption confidence="0.999996">
Figure 7: Predictions of two probabilistic Minimalist Grammars through the lens of the ERH
</figureCaption>
<bodyText confidence="0.982724">
exhibit a significant correlation with repetition ac-
curacy scores.
</bodyText>
<sectionHeader confidence="0.999678" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999957785714286">
From the perspective of the ERH, the difference be-
tween the promotion and adjunction grammars re-
sides in the uncertainty of particular states an incre-
mental parser would pass through on the way to a
complete analysis.
On the Keenan and Hawkins’ (1987) stimuli,
these grammars specify incremental parser states
that support explanations for some of the observed
repetition accuracy asymmetries, abbreviated &lt;.
SU &lt; IO subject extracted relatives are easier than
indirect object extracted relatives, because a
left-to-right incremental parser evades, in just
subject extracted relatives, the uncertainty as-
sociated with questions like
</bodyText>
<listItem confidence="0.9976445">
• which internal argument is the gap?
• did dative shift happen?
</listItem>
<bodyText confidence="0.999978454545454">
These questions are defined by alterna-
tive derivation-subtrees associated with the
verb phrase. For the DO stimuli that use poten-
tially ditransitive embedded verbs the same ex-
planation is available, however only two out of
four items in the Keenan and Hawkins (1987)
set qualify.
IO &lt; OBL there is only one type of extraction
from indirect object, whereas on these gram-
mars, the head of the oblique phrase (“for”
“with” “on” or “in”) signals which of four cat-
egorically separate kinds of extraction has oc-
curred. These alternatives correspond to four
different derivation-nonterminals.
OBL &lt; GEN both grammars analyze “whose” as
taking a common noun argument, for example
“whose ship.” But in just the promotion gram-
mar, “whose” is further analyzed as the ordi-
nary “who” morpheme plus a complex pos-
sessive phrase headed by “-s” (McDaniel et
al., 1998). Because of the recursive charac-
ter of this possessor category, the structure of
“whose’s” common noun argument introduces
additional uncertainty not present in the indi-
rect object extracted relatives.
Strikingly, the two grammars disagree on six out-
liers in figure 7(b) where just the adjunction gram-
mar predicts very great difficulty in conjunction
with the ERH. These outlier predictions are made on
just the sentences that use the nominal carrier frame
beginning with “the fact that...” Because the adjunc-
tion grammar analyzes relative clauses with an MG
rule analogous to the phrase structure rule (4),
</bodyText>
<equation confidence="0.837259">
DP → DP CPrel. (4)
</equation>
<bodyText confidence="0.9999767">
all DPs are available for modification by any num-
ber of stacked relative clauses. The nominal frame
introduces an additional DP, not present in the other
stimuli, that can be modified in this way.
By contrast, the promotion grammar does not in-
clude a +f promotion feature on any lexical en-
try for “fact,” precluding the possibility of such
modification. Moreover, even with such a feature,
the promotion grammar assigns different categories
to the outermost versus successive relative clause
modifiers. Because only one relative clause is ever
stacked in the Keenan and Hawkins (1987) stimulus
set, the relevant recursion is not attested, yielding a
category of caseless subject DP that is more certain
than it is in the adjunction grammar.
An ERH account that avoids predicting these out-
liers on the Keenan and Hawkins (1987) stimuli
seems to require a grammar where the probability
of 2nd and subsequent stacked relative clause modi-
fiers is closer to 0 (its value on the trained promotion
grammar) than to 0.31 (its value on the trained ad-
junction grammar). Beyond these particular stimuli,
this modeling motivates a general question about
the scale of structural expectations in human sen-
tence processing. Does disconfirmation of a more
complicated structural alternative (such as stacked
relative clauses) induce greater processing difficulty
than disconfirmation of a simpler one? Such em-
pirical issues go beyond the scope of this paper but
suggest particular kinds of future work.
</bodyText>
<sectionHeader confidence="0.997471" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999977333333333">
By extending Lounsbury’s (1954) entropy reduction
idea to infinite languages, it has become possible
to relate predictability and processing difficulty in a
way that takes into account linguistic structures de-
fined by one kind of mildly context-sensitive gram-
mar formalism. This relation is the linking hypoth-
esis ERH.
On this linking hypothesis, a grammar express-
ing the promotion analysis of relative clauses yields
whole-sentence predictions more closely approxi-
mating human repetition accuracy results than does
a grammar expressing the standard adjunction anal-
ysis.
If the ERH is true, this result suggests that one
grammar carries a kind of greater psychological va-
lidity than the other. On the other hand, to the
extent that the promotion grammar correctly char-
acterizes human linguistic competence, this con-
firms the ERH as a linking hypothesis. In any case,
the information-processing difficulty of incremental
parsing can now be given a more specific definition.
</bodyText>
<sectionHeader confidence="0.998361" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999244">
The author wishes to thank Paul Smolensky, Ed Sta-
bler and Ted Gibson.
</bodyText>
<sectionHeader confidence="0.999271" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997878674157303">
Sylvie Billot and Bernard Lang. 1989. The struc-
ture of shared forests in ambiguous parsing. In
Proceedings of the 1989 Meeting of the Associa-
tion for Computational Linguistics.
Joan Bresnan, editor. 1982. The Mental Repre-
sentation of Grammatical Relations. MIT Press,
Cambridge, MA.
Zhiyi Chi. 1999. Statistical properties of prob-
abilistic context-free grammars. Computa-
tional Linguistics, 25(1):131–160.
Noam Chomsky. 1977. On Wh-Movement. In Pe-
ter Culicover, Thomas Wasow, and Adrian Ak-
majian, editors, Formal Syntax, pages 71–132.
Academic Press, New York.
Frieda Goldman-Eisler. 1958. Speech produc-
tion and the predictability of words in context.
Quarterly Journal of Experimental Psychology,
10:96–106.
Ulf Grenander. 1967. Syntax-controlled probabili-
ties. Technical report, Brown University Division
of Applied Mathematics, Providence, RI.
John Hale. 2003. Grammar, uncertainty and sen-
tence processing. Ph.D. thesis, Johns Hopkins
University, Baltimore, Maryland.
Henk Harkema. 2001. Parsing Minimalist Gram-
mars. Ph.D. thesis, UCLA.
Aravind K. Joshi, Leon S. Levy, and Masako Taka-
hashi. 1975. Tree adjunct grammars. Journal of
Computer and System Sciences, 10:136–163.
Richard S. Kayne. 1994. The Antisymmetry of Syn-
tax. MIT Press.
Edward L. Keenan and Bernard Comrie. 1977.
Noun phrase accessibility and universal grammar.
Linguistic Inquiry, 8(1):63–99.
Edward L. Keenan and Sarah Hawkins. 1987. The
psychological validity of the Accessibility Hi-
erarchy. In Edward L. Keenan, editor, Univer-
sal Grammar: 15 Essays, pages 60–85, London.
Croom Helm.
Edward L. Keenan. 1975. Variation in universal
grammar. In R.W. Shuy and R.W. Fasold, edi-
tors, Analyzing Variation in Language. George-
town University Press.
Bernard Lang. 1974. Deterministic techniques for
efficient non-deterministic parsers. In J. Loeckx,
editor, Proceedings of the 2nd Colloquium on
Automata, Languages and Programming, num-
ber 14 in Springer Lecture Notes in Computer
Science, pages 255–269, Saarbru¨ucken.
Bernard Lang. 1988. Parsing incomplete sentences.
In Proceedings of the 12th International Confer-
ence on Computational Linguistics, pages 365–
371.
Floyd G. Lounsbury. 1954. Transitional proba-
bility, linguistic structure and systems of habit-
family hierarchies. In C. E. Osgood and T. A.
Sebeok, editors, Psycholinguistics: a survey of
theory and research. Indiana University Press.
Dana McDaniel, Cecile McKee, and Judy B. Bern-
stein. 1998. How children’s relatives solve a
problem for minimalism. Language, pages 308–
334.
Scott A. McDonald and Richard C. Shillcock. 2003.
Eye movements reveal the on-line computation of
lexical probabilities during reading. Psychologi-
cal Science, 14:648–652.
Jens Michaelis. 2001. On Formal Properties of
Minimalist Grammars. Ph.D. thesis, Potsdam
University.
David Perlmutter and Paul Postal. 1974. Lectures
on Relational Grammar. LSA Linguistic Insti-
tute, UMass Amherst.
Carl Pollard and Ivan A. Sag. 1994. Head-
driven Phrase Structure Grammar. University of
Chicago Press.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii,
and Tadao Kasami. 1991. On multiple context-
free grammars. Theoretical Computer Science,
88:191–229.
Edward Stabler and Edward Keenan. 2003. Struc-
tural similarity. Theoretical Computer Science,
293:345–363.
Edward P. Stabler. 1997. Derivational minimal-
ism. In Christian Retor´e, editor, Logical As-
pects of Computational Linguistics, pages 68–95.
Springer.
Wilson Taylor. 1953. Cloze procedure: a new tool
for measuring readability. Journalism Quarterly,
30:415–433.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.445219">
<title confidence="0.999312">The information-processing difficulty of incremental parsing</title>
<author confidence="0.993285">John</author>
<affiliation confidence="0.8977915">Department of Linguistics and Michigan State</affiliation>
<address confidence="0.487758">East Lansing, MI</address>
<email confidence="0.999461">jthale@msu.edu</email>
<abstract confidence="0.99717475">When an incremental parser gets the next word, its expectations about upcoming grammatical structures can change. When a word greatly constrains these grammatical expectations, uncertainty is reduced. This elimination of possibilities constitutes information processing work. Formalizing this notion of information processing work yields a complexity metric that predicts human repetition accuracy scores across a systematic class of linguistic phenomena, the Accessibility Hierarchy of relativizable grammatical relations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sylvie Billot</author>
<author>Bernard Lang</author>
</authors>
<title>The structure of shared forests in ambiguous parsing.</title>
<date>1989</date>
<booktitle>In Proceedings of the 1989 Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Billot, Lang, 1989</marker>
<rawString>Sylvie Billot and Bernard Lang. 1989. The structure of shared forests in ambiguous parsing. In Proceedings of the 1989 Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<title>The Mental Representation of Grammatical Relations.</title>
<date>1982</date>
<editor>Joan Bresnan, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1982</marker>
<rawString>Joan Bresnan, editor. 1982. The Mental Representation of Grammatical Relations. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyi Chi</author>
</authors>
<title>Statistical properties of probabilistic context-free grammars.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="17203" citStr="Chi, 1999" startWordPosition="2923" endWordPosition="2924">v’ little v ✏✏vP PP t dP(1) v’ d’ d✧❜ honest v NumnP n’ n boy ✥✥ ❳❳❳ t little vP ✦✦ ❛❛ little v t -ed v little v NumP Num’ d the t(2) explain ’ NumnP n’ n father v t d the NumPt(1) Num’ NumnP n’ p toP ✏✏p to’PP p to PtoP P o’ p to Pto to n Pto dP ✟ ❍ dP(0) t(0) t(0) answer t Figure 5: more standard adjunction analysis 5 Procedure Derivation trees on both grammars were obtained5 for each of Keenan and Hawkins’ (1987) twentyfour stimulus sentences6. Branches of these derivation trees were viewed as PCFG rules with probabilities set according to the usual relative-frequency estimation technique (Chi, 1999). However, because the stimuli were intentionally constructed to have 5Derivations were obtained using a parser described in Appendix A of Hale (2003) 6To eliminate number agreement as a source of derivational uncertainty, the results were calculated using a modified stimulus set in which four noun phrases were changed from plural to singular. exactly four examples of each structure, these sentences were weighted in accordance with a corpus study (Keenan, 1975) to make their relative frequencies more realistic. honest a A 6 Results The summed entropy reductions exhibit a significant correlatio</context>
</contexts>
<marker>Chi, 1999</marker>
<rawString>Zhiyi Chi. 1999. Statistical properties of probabilistic context-free grammars. Computational Linguistics, 25(1):131–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>On Wh-Movement.</title>
<date>1977</date>
<booktitle>Formal Syntax,</booktitle>
<pages>71--132</pages>
<editor>In Peter Culicover, Thomas Wasow, and Adrian Akmajian, editors,</editor>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="3228" citStr="Chomsky, 1977" startWordPosition="479" endWordPosition="480">he person by that word with respect to a probabilistic grammar the person knows. In section 2 a method for calculating the entropy reduction of a word in a sentence generated by a probabilistic grammar is presented. Section 3 describes the empirical domain of interest, the Accessibility Hierarchy (Keenan and Comrie, 1977). Section 4 goes on to describe two probabilistic grammars in the class of mildly context-sensitive Minimalist Grammars (Stabler, 1997). One expresses the “promotion analysis” (Kayne, 1994) of relative clauses while the other expresses the more standard “adjunction analysis” (Chomsky, 1977). The predictions of these grammars through the lens of the ERH are considered in sections 5 through 7, where it is shown that predictions derived from the promotion analysis match human repetition accuracy scores better than predictions derived from the adjunction analysis. Section 8 concludes. 2 Entropy Reduction The idea of the entropy reduction of a word is that uncertainty about grammatical continuations fluctuates as new words come in. The ERH is the proposal that fluctuations in this value be taken as psycholinguistic predictions. This proposal is founded on the possibility of viewing n</context>
<context position="11439" citStr="Chomsky, 1977" startWordPosition="1867" endWordPosition="1868">Grammars If correct, the ERH would explain the increasing difficulty across the AH in terms of greater or lesser uncertainty about intermediate parser states. To calculate these predictions, some assumption must be made about what those structures are. 4.1 Two analyses of relativization Toward this end, two grammars covering the Keenan and Hawkins stimuli were written in the Minimalist Grammars (Stabler, 1997) formalism. These grammars were exactly the same except for their treatment of relative clauses. One grammar expresses the usual analysis of relative clauses as right-adjoined modifiers (Chomsky, 1977). The other expresses the promotion analysis of relative clause. The analysis, which dates back to the 1960s, is revived in Kayne (1994). For reasons having to do with Kayne’s general theory of phrase structure, he proposes that, in a sentence like 1, the underlying form of the subject is akin to 2. 1Each response was coded for accuracy on a 0-2 scale where 2 means perfect repetition and 1 suggests minor, grammatical errors. A score of 0 was assigned when the response did not include a relative clause of the indicated grammatical function. Cf.Keenan and Hawkins (1987) 2Summation naturally exte</context>
</contexts>
<marker>Chomsky, 1977</marker>
<rawString>Noam Chomsky. 1977. On Wh-Movement. In Peter Culicover, Thomas Wasow, and Adrian Akmajian, editors, Formal Syntax, pages 71–132. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frieda Goldman-Eisler</author>
</authors>
<title>Speech production and the predictability of words in context.</title>
<date>1958</date>
<journal>Quarterly Journal of Experimental Psychology,</journal>
<pages>10--96</pages>
<contexts>
<context position="975" citStr="Goldman-Eisler, 1958" startWordPosition="130" endWordPosition="131">pectations, uncertainty is reduced. This elimination of possibilities constitutes information processing work. Formalizing this notion of information processing work yields a complexity metric that predicts human repetition accuracy scores across a systematic class of linguistic phenomena, the Accessibility Hierarchy of relativizable grammatical relations. 1 Introduction An attractive hypothesis in psycholinguistics, dating back at least to the 1950s, has been that the degree of predictability of words in sentences is somehow related to understandability (Taylor, 1953), production difficulty (Goldman-Eisler, 1958) or, more recently, eye-movements (McDonald and Shillcock, 2003). However, since the 1950s, integrating this hypothesis with realistic models of linguistic structure has remained a challenge. Lounsbury (1954) appreciated the formal character of the problem. He defined a finite, artificial language, endowed with a rudimentary phonology, morphology and syntax, and showed that a word’s informational contribution could be formally defined as the entropy reduction brought about by its addition to the end of a sentence fragment. He qualified the significance of his achievement, saying An entropy red</context>
</contexts>
<marker>Goldman-Eisler, 1958</marker>
<rawString>Frieda Goldman-Eisler. 1958. Speech production and the predictability of words in context. Quarterly Journal of Experimental Psychology, 10:96–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulf Grenander</author>
</authors>
<title>Syntax-controlled probabilities.</title>
<date>1967</date>
<tech>Technical report,</tech>
<institution>Brown University Division of Applied Mathematics,</institution>
<location>Providence, RI.</location>
<contexts>
<context position="4843" citStr="Grenander (1967)" startWordPosition="752" endWordPosition="753">onterminals generally in probabilistic context-free phrase structure grammars (PCFGs) can be viewed this way. Since their outcomes are discrete, their entropy H is easily calculated H(X) = − � p(x) lo92 p(x) (1) xEX H(NP) = − [(0.87 x lo92 0.87) +(0.13 x lo92 0.13)] � 0.56 bits There is just over half a bit of uncertainty about how NP is going to rewrite, because the outcome is so heavily weighted towards the first alternative. In this simple example there is no recursion, so the generated language is finite. To obtain the uncertainty about infinite PCFG languages, a recursive relation due to Grenander (1967) can be used to calculate the entropy of the start symbol S which begins all derivations. 2.1 Entropy of nonterminals in a PCFG Grenander’s theorem is a recurrence relation that gives the entropy of each nonterminal in a PCFG G as the sum of two terms. Let the set of production rules in G be H and the subset rewriting nonterminal ξ be H(ξ). Denote by pr the probability of a rule r having daughters ξj1, ξj2,.... Then h(ξi) = − � pr lo92 pr rEH(ξi) H(ξi) = h(ξi) + � pr [H(ξj1) rEH(ξi) +H(ξj2) + ···] (Grenander, 1967, 19) the first term, lowercase h, is simply the definition of entropy for a disc</context>
</contexts>
<marker>Grenander, 1967</marker>
<rawString>Ulf Grenander. 1967. Syntax-controlled probabilities. Technical report, Brown University Division of Applied Mathematics, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hale</author>
</authors>
<title>Grammar, uncertainty and sentence processing.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Johns Hopkins University,</institution>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="12221" citStr="Hale (2003)" startWordPosition="1996" endWordPosition="1997">ne’s general theory of phrase structure, he proposes that, in a sentence like 1, the underlying form of the subject is akin to 2. 1Each response was coded for accuracy on a 0-2 scale where 2 means perfect repetition and 1 suggests minor, grammatical errors. A score of 0 was assigned when the response did not include a relative clause of the indicated grammatical function. Cf.Keenan and Hawkins (1987) 2Summation naturally extends the word-by-word complexity metric ERH to the sentence level. In word-by-word selfpaced reading, evidence for the Accessibility Hierarchy is limited (cf. chapter 5 of Hale (2003)). repetition accuracy (1) the boy who the father explained the answer to was honest (2) [IP the father explained the answer to [DP[+wh] who boy[+f] ] ] According to Kayne, at an early stage (2) of syntactic derivation, the determiner phrase (DP) “who boy” occupies what will eventually be the gap position. This DP moves to a specifier position of the enclosing, empty-headed (C0) complementizer phrase (CP), thereby checking a feature +wh as indicated in 3. (3) [CP [DP who boy[+f] ]i C0 [IP the father explained the answer to ti ] ] In a second movement, “boy” evacuates from DP, moving to another</context>
<context position="14530" citStr="Hale (2003)" startWordPosition="2399" endWordPosition="2400">the [AgrP boyj Agr [CP [DP who tj ]i C0 [IP the father explained the answer to ti ] ] ] ] ::=&gt;v =d little_v v,-wh_rel ::=Num d -case Num +case v,-case,-wh_rel ::=n Num ::n No adjunction is used in this derivation, and, unconventionally, the leftmost “the” and “boy” do not share an exclusive common constituent. Nor is the wh-word “who” co-indexed with anything. Structural descriptions involving both the Kaynian analysis and the more standard adjunction analysis are shown in figures 4 and 5 respectively3. The other linguistic assumptions suggested by these diagrams are discussed in chapter 4 of Hale (2003). 4.2 Formal grammars of relativization The Minimalist Grammars (MG) formalism (cf. Stabler and Keenan (2003) for a systematic presentation) facilitates the relatively transparent implementation of ideas like movement and feature checking that figure prominently in the two analyses of relativization discussed in the previous subsection. MGs define a set of sentences by closing the structure-building functions merge and move on a finite set of lexical entries; however, this does not mean that parsing must happen bottom-up. A fundamental result, obtained independently by Harkema (2001) and Micha</context>
<context position="17353" citStr="Hale (2003)" startWordPosition="2946" endWordPosition="2947"> n’ n father v t d the NumPt(1) Num’ NumnP n’ p toP ✏✏p to’PP p to PtoP P o’ p to Pto to n Pto dP ✟ ❍ dP(0) t(0) t(0) answer t Figure 5: more standard adjunction analysis 5 Procedure Derivation trees on both grammars were obtained5 for each of Keenan and Hawkins’ (1987) twentyfour stimulus sentences6. Branches of these derivation trees were viewed as PCFG rules with probabilities set according to the usual relative-frequency estimation technique (Chi, 1999). However, because the stimuli were intentionally constructed to have 5Derivations were obtained using a parser described in Appendix A of Hale (2003) 6To eliminate number agreement as a source of derivational uncertainty, the results were calculated using a modified stimulus set in which four noun phrases were changed from plural to singular. exactly four examples of each structure, these sentences were weighted in accordance with a corpus study (Keenan, 1975) to make their relative frequencies more realistic. honest a A 6 Results The summed entropy reductions exhibit a significant correlation with the repetition accuracy scores collected by Keenan and Hawkins (1987). The correlation in figure 7(a) obtains only on the grammar expressing th</context>
</contexts>
<marker>Hale, 2003</marker>
<rawString>John Hale. 2003. Grammar, uncertainty and sentence processing. Ph.D. thesis, Johns Hopkins University, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henk Harkema</author>
</authors>
<title>Parsing Minimalist Grammars.</title>
<date>2001</date>
<tech>Ph.D. thesis, UCLA.</tech>
<contexts>
<context position="15120" citStr="Harkema (2001)" startWordPosition="2489" endWordPosition="2490">chapter 4 of Hale (2003). 4.2 Formal grammars of relativization The Minimalist Grammars (MG) formalism (cf. Stabler and Keenan (2003) for a systematic presentation) facilitates the relatively transparent implementation of ideas like movement and feature checking that figure prominently in the two analyses of relativization discussed in the previous subsection. MGs define a set of sentences by closing the structure-building functions merge and move on a finite set of lexical entries; however, this does not mean that parsing must happen bottom-up. A fundamental result, obtained independently by Harkema (2001) and Michaelis (2001) 3The X-bar structures depicted in figures 4 and 5 are drawn using tools developed by Edward Stabler and colleagues. =d +case v,-wh_rel d -case ::=p_to =d +case v p_to,-wh_rel ::=&gt;Pto p_to Pto,-wh_rel +case Pto,-case -wh_rel ::=d +case Pto d -case -wh_rel +f d -case -wh_rel,-f ::=Num +f d -case -wh_rel Num,-f ::=n Num ::n -f Figure 6: Derivation tree on promotion grammar. The derivation trees encode everything there is to know about MG derivations, and can be parsed in a variety of orders. Most importantly, if equipped with weights on their branches, they can be generated </context>
</contexts>
<marker>Harkema, 2001</marker>
<rawString>Henk Harkema. 2001. Parsing Minimalist Grammars. Ph.D. thesis, UCLA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Leon S Levy</author>
<author>Masako Takahashi</author>
</authors>
<title>Tree adjunct grammars.</title>
<date>1975</date>
<journal>Journal of Computer and System Sciences,</journal>
<pages>10--136</pages>
<contexts>
<context position="13422" citStr="Joshi et al., 1975" startWordPosition="2209" endWordPosition="2212"> moving to another specifier (perhaps that of the silent agreement morpheme, Agr) as in 4 – checking a different feature, +f. (4) [AgrP boyj Agr [CP [DP who tj ]i C0 [IP the father explained the answer to ti ] ] ] The entire structure becomes a complement of a determiner to yield a larger DP in 5. is that MGs are equivalent to Multiple contextfree grammars (Seki et al., 1991). Multiple contextfree grammars generalize standard context-free grammars by allowing the string yields of daughter categories to be manipulated by a function other than simple concatenation. As in Tree Adjoining Grammar (Joshi et al., 1975) a record of these manipulations is kept at each node of an MG derivation tree, while a picture of the result is manifested in derived trees such as the ones in figures 4 and 5. The derivation tree on the promotion grammar is shown4 in figure 6 for the substring “the boy who the father explained the answer to.” d -case ::=c_rel d -case c_rel +wh_rel c_rel,-wh_rel ::=t +wh_rel c_rel t,-wh_rel +case t,-case,-wh_rel ::=&gt;little_v +case t little_v,-case,-wh_rel =d little_v,-wh_rel d -case (5) [DP the [AgrP boyj Agr [CP [DP who tj ]i C0 [IP the father explained the answer to ti ] ] ] ] ::=&gt;v =d litt</context>
</contexts>
<marker>Joshi, Levy, Takahashi, 1975</marker>
<rawString>Aravind K. Joshi, Leon S. Levy, and Masako Takahashi. 1975. Tree adjunct grammars. Journal of Computer and System Sciences, 10:136–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard S Kayne</author>
</authors>
<title>The Antisymmetry of Syntax.</title>
<date>1994</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3126" citStr="Kayne, 1994" startWordPosition="465" endWordPosition="466">rocessing difficulty at a word in a sentence is directly related to the number of bits signaled to the person by that word with respect to a probabilistic grammar the person knows. In section 2 a method for calculating the entropy reduction of a word in a sentence generated by a probabilistic grammar is presented. Section 3 describes the empirical domain of interest, the Accessibility Hierarchy (Keenan and Comrie, 1977). Section 4 goes on to describe two probabilistic grammars in the class of mildly context-sensitive Minimalist Grammars (Stabler, 1997). One expresses the “promotion analysis” (Kayne, 1994) of relative clauses while the other expresses the more standard “adjunction analysis” (Chomsky, 1977). The predictions of these grammars through the lens of the ERH are considered in sections 5 through 7, where it is shown that predictions derived from the promotion analysis match human repetition accuracy scores better than predictions derived from the adjunction analysis. Section 8 concludes. 2 Entropy Reduction The idea of the entropy reduction of a word is that uncertainty about grammatical continuations fluctuates as new words come in. The ERH is the proposal that fluctuations in this va</context>
<context position="11575" citStr="Kayne (1994)" startWordPosition="1889" endWordPosition="1890">diate parser states. To calculate these predictions, some assumption must be made about what those structures are. 4.1 Two analyses of relativization Toward this end, two grammars covering the Keenan and Hawkins stimuli were written in the Minimalist Grammars (Stabler, 1997) formalism. These grammars were exactly the same except for their treatment of relative clauses. One grammar expresses the usual analysis of relative clauses as right-adjoined modifiers (Chomsky, 1977). The other expresses the promotion analysis of relative clause. The analysis, which dates back to the 1960s, is revived in Kayne (1994). For reasons having to do with Kayne’s general theory of phrase structure, he proposes that, in a sentence like 1, the underlying form of the subject is akin to 2. 1Each response was coded for accuracy on a 0-2 scale where 2 means perfect repetition and 1 suggests minor, grammatical errors. A score of 0 was assigned when the response did not include a relative clause of the indicated grammatical function. Cf.Keenan and Hawkins (1987) 2Summation naturally extends the word-by-word complexity metric ERH to the sentence level. In word-by-word selfpaced reading, evidence for the Accessibility Hier</context>
</contexts>
<marker>Kayne, 1994</marker>
<rawString>Richard S. Kayne. 1994. The Antisymmetry of Syntax. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward L Keenan</author>
<author>Bernard Comrie</author>
</authors>
<title>Noun phrase accessibility and universal grammar.</title>
<date>1977</date>
<journal>Linguistic Inquiry,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="2937" citStr="Keenan and Comrie, 1977" startWordPosition="434" endWordPosition="437">ight that an intermediate parser state is itself a specification of a grammar. This extension permits the psycholinguistic hypothesis ERH to be examined. Entropy Reduction Hypothesis (ERH) a person’s processing difficulty at a word in a sentence is directly related to the number of bits signaled to the person by that word with respect to a probabilistic grammar the person knows. In section 2 a method for calculating the entropy reduction of a word in a sentence generated by a probabilistic grammar is presented. Section 3 describes the empirical domain of interest, the Accessibility Hierarchy (Keenan and Comrie, 1977). Section 4 goes on to describe two probabilistic grammars in the class of mildly context-sensitive Minimalist Grammars (Stabler, 1997). One expresses the “promotion analysis” (Kayne, 1994) of relative clauses while the other expresses the more standard “adjunction analysis” (Chomsky, 1977). The predictions of these grammars through the lens of the ERH are considered in sections 5 through 7, where it is shown that predictions derived from the promotion analysis match human repetition accuracy scores better than predictions derived from the adjunction analysis. Section 8 concludes. 2 Entropy Re</context>
</contexts>
<marker>Keenan, Comrie, 1977</marker>
<rawString>Edward L. Keenan and Bernard Comrie. 1977. Noun phrase accessibility and universal grammar. Linguistic Inquiry, 8(1):63–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward L Keenan</author>
<author>Sarah Hawkins</author>
</authors>
<title>The psychological validity of the Accessibility Hierarchy.</title>
<date>1987</date>
<booktitle>Universal Grammar: 15 Essays,</booktitle>
<pages>60--85</pages>
<editor>In Edward L. Keenan, editor,</editor>
<location>London. Croom Helm.</location>
<contexts>
<context position="10388" citStr="Keenan and Hawkins (1987)" startWordPosition="1697" endWordPosition="1700"> man who Stephen explained the accident to is kind oblique extracted he remembered that the food which Chris paid the bill for was cheap genitive subject extracted they had forgotten that the girl whose friend bought the cake was waiting genitive object extracted the fact that the sailor whose ship Jim took had one leg is important Figure 2: Relative clauses in each of four carrier sentence types The results of the human study, given in figure 3, SU DO IO OBL GenS GenO 406 364 342 279 167 171 Figure 3: results from Keenan &amp; Hawkins (1987) show that repetition accuracy1 declines across the AH. Keenan and Hawkins (1987) note however that “It remains unexplained just why RCs should be more difficult to comprehend-produce as they are formed on positions lower on the AH.” The ERH, if correct, would offer just such an explanation. If a person’s difficulty on each word of a sentence is related to derivational information signaled by that word, then the total difficulty reading a sentence ought to be the sum of the difficulty on each word2. 4 Minimalist Grammars If correct, the ERH would explain the increasing difficulty across the AH in terms of greater or lesser uncertainty about intermediate parser states. To c</context>
<context position="12013" citStr="Keenan and Hawkins (1987)" startWordPosition="1962" endWordPosition="1965">clauses as right-adjoined modifiers (Chomsky, 1977). The other expresses the promotion analysis of relative clause. The analysis, which dates back to the 1960s, is revived in Kayne (1994). For reasons having to do with Kayne’s general theory of phrase structure, he proposes that, in a sentence like 1, the underlying form of the subject is akin to 2. 1Each response was coded for accuracy on a 0-2 scale where 2 means perfect repetition and 1 suggests minor, grammatical errors. A score of 0 was assigned when the response did not include a relative clause of the indicated grammatical function. Cf.Keenan and Hawkins (1987) 2Summation naturally extends the word-by-word complexity metric ERH to the sentence level. In word-by-word selfpaced reading, evidence for the Accessibility Hierarchy is limited (cf. chapter 5 of Hale (2003)). repetition accuracy (1) the boy who the father explained the answer to was honest (2) [IP the father explained the answer to [DP[+wh] who boy[+f] ] ] According to Kayne, at an early stage (2) of syntactic derivation, the determiner phrase (DP) “who boy” occupies what will eventually be the gap position. This DP moves to a specifier position of the enclosing, empty-headed (C0) complement</context>
<context position="17879" citStr="Keenan and Hawkins (1987)" startWordPosition="3027" endWordPosition="3030">onstructed to have 5Derivations were obtained using a parser described in Appendix A of Hale (2003) 6To eliminate number agreement as a source of derivational uncertainty, the results were calculated using a modified stimulus set in which four noun phrases were changed from plural to singular. exactly four examples of each structure, these sentences were weighted in accordance with a corpus study (Keenan, 1975) to make their relative frequencies more realistic. honest a A 6 Results The summed entropy reductions exhibit a significant correlation with the repetition accuracy scores collected by Keenan and Hawkins (1987). The correlation in figure 7(a) obtains only on the grammar expressing the Kaynian promotion analysis, and not on the grammar expressing the standard adjunction analysis (figure 7(b)). Nor do logprobabilitiesfor stimulus sentences on the grammar Accessibility Hierarchy total promotion grammar bits reduced r2=0.45, p&lt;0.001 250 300 350 400 450 500 Accessibility Hierarchy total adjunction grammar bits reduced r2=0.02, n.s. 250 300 350 400 450 500 55 50 45 40 35 30 75 70 65 60 55 50 error score error score Figure 7: Predictions of two probabilistic Minimalist Grammars through the lens of the ERH </context>
<context position="19523" citStr="Keenan and Hawkins (1987)" startWordPosition="3285" endWordPosition="3288"> some of the observed repetition accuracy asymmetries, abbreviated &lt;. SU &lt; IO subject extracted relatives are easier than indirect object extracted relatives, because a left-to-right incremental parser evades, in just subject extracted relatives, the uncertainty associated with questions like • which internal argument is the gap? • did dative shift happen? These questions are defined by alternative derivation-subtrees associated with the verb phrase. For the DO stimuli that use potentially ditransitive embedded verbs the same explanation is available, however only two out of four items in the Keenan and Hawkins (1987) set qualify. IO &lt; OBL there is only one type of extraction from indirect object, whereas on these grammars, the head of the oblique phrase (“for” “with” “on” or “in”) signals which of four categorically separate kinds of extraction has occurred. These alternatives correspond to four different derivation-nonterminals. OBL &lt; GEN both grammars analyze “whose” as taking a common noun argument, for example “whose ship.” But in just the promotion grammar, “whose” is further analyzed as the ordinary “who” morpheme plus a complex possessive phrase headed by “-s” (McDaniel et al., 1998). Because of th</context>
<context position="21320" citStr="Keenan and Hawkins (1987)" startWordPosition="3577" endWordPosition="3580">e structure rule (4), DP → DP CPrel. (4) all DPs are available for modification by any number of stacked relative clauses. The nominal frame introduces an additional DP, not present in the other stimuli, that can be modified in this way. By contrast, the promotion grammar does not include a +f promotion feature on any lexical entry for “fact,” precluding the possibility of such modification. Moreover, even with such a feature, the promotion grammar assigns different categories to the outermost versus successive relative clause modifiers. Because only one relative clause is ever stacked in the Keenan and Hawkins (1987) stimulus set, the relevant recursion is not attested, yielding a category of caseless subject DP that is more certain than it is in the adjunction grammar. An ERH account that avoids predicting these outliers on the Keenan and Hawkins (1987) stimuli seems to require a grammar where the probability of 2nd and subsequent stacked relative clause modifiers is closer to 0 (its value on the trained promotion grammar) than to 0.31 (its value on the trained adjunction grammar). Beyond these particular stimuli, this modeling motivates a general question about the scale of structural expectations in hu</context>
<context position="10307" citStr="Keenan &amp; Hawkins (1987)" startWordPosition="1685" endWordPosition="1688">owed to the man likes eggs is strange indirect object extracted I know that the man who Stephen explained the accident to is kind oblique extracted he remembered that the food which Chris paid the bill for was cheap genitive subject extracted they had forgotten that the girl whose friend bought the cake was waiting genitive object extracted the fact that the sailor whose ship Jim took had one leg is important Figure 2: Relative clauses in each of four carrier sentence types The results of the human study, given in figure 3, SU DO IO OBL GenS GenO 406 364 342 279 167 171 Figure 3: results from Keenan &amp; Hawkins (1987) show that repetition accuracy1 declines across the AH. Keenan and Hawkins (1987) note however that “It remains unexplained just why RCs should be more difficult to comprehend-produce as they are formed on positions lower on the AH.” The ERH, if correct, would offer just such an explanation. If a person’s difficulty on each word of a sentence is related to derivational information signaled by that word, then the total difficulty reading a sentence ought to be the sum of the difficulty on each word2. 4 Minimalist Grammars If correct, the ERH would explain the increasing difficulty across the AH</context>
</contexts>
<marker>Keenan, Hawkins, 1987</marker>
<rawString>Edward L. Keenan and Sarah Hawkins. 1987. The psychological validity of the Accessibility Hierarchy. In Edward L. Keenan, editor, Universal Grammar: 15 Essays, pages 60–85, London. Croom Helm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward L Keenan</author>
</authors>
<title>Variation in universal grammar.</title>
<date>1975</date>
<booktitle>Analyzing Variation in Language.</booktitle>
<editor>In R.W. Shuy and R.W. Fasold, editors,</editor>
<publisher>Georgetown University Press.</publisher>
<contexts>
<context position="17668" citStr="Keenan, 1975" startWordPosition="2996" endWordPosition="2997">f these derivation trees were viewed as PCFG rules with probabilities set according to the usual relative-frequency estimation technique (Chi, 1999). However, because the stimuli were intentionally constructed to have 5Derivations were obtained using a parser described in Appendix A of Hale (2003) 6To eliminate number agreement as a source of derivational uncertainty, the results were calculated using a modified stimulus set in which four noun phrases were changed from plural to singular. exactly four examples of each structure, these sentences were weighted in accordance with a corpus study (Keenan, 1975) to make their relative frequencies more realistic. honest a A 6 Results The summed entropy reductions exhibit a significant correlation with the repetition accuracy scores collected by Keenan and Hawkins (1987). The correlation in figure 7(a) obtains only on the grammar expressing the Kaynian promotion analysis, and not on the grammar expressing the standard adjunction analysis (figure 7(b)). Nor do logprobabilitiesfor stimulus sentences on the grammar Accessibility Hierarchy total promotion grammar bits reduced r2=0.45, p&lt;0.001 250 300 350 400 450 500 Accessibility Hierarchy total adjunction</context>
</contexts>
<marker>Keenan, 1975</marker>
<rawString>Edward L. Keenan. 1975. Variation in universal grammar. In R.W. Shuy and R.W. Fasold, editors, Analyzing Variation in Language. Georgetown University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Lang</author>
</authors>
<title>Deterministic techniques for efficient non-deterministic parsers.</title>
<date>1974</date>
<booktitle>Proceedings of the 2nd Colloquium on Automata, Languages and Programming, number 14 in Springer Lecture Notes in Computer Science,</booktitle>
<pages>255--269</pages>
<editor>In J. Loeckx, editor,</editor>
<marker>Lang, 1974</marker>
<rawString>Bernard Lang. 1974. Deterministic techniques for efficient non-deterministic parsers. In J. Loeckx, editor, Proceedings of the 2nd Colloquium on Automata, Languages and Programming, number 14 in Springer Lecture Notes in Computer Science, pages 255–269, Saarbru¨ucken.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Lang</author>
</authors>
<title>Parsing incomplete sentences.</title>
<date>1988</date>
<booktitle>In Proceedings of the 12th International Conference on Computational Linguistics,</booktitle>
<pages>365--371</pages>
<marker>Lang, 1988</marker>
<rawString>Bernard Lang. 1988. Parsing incomplete sentences. In Proceedings of the 12th International Conference on Computational Linguistics, pages 365– 371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Floyd G Lounsbury</author>
</authors>
<title>Transitional probability, linguistic structure and systems of habitfamily hierarchies.</title>
<date>1954</date>
<editor>In C. E. Osgood and T. A. Sebeok, editors,</editor>
<publisher>Indiana University Press.</publisher>
<contexts>
<context position="1183" citStr="Lounsbury (1954)" startWordPosition="159" endWordPosition="160">repetition accuracy scores across a systematic class of linguistic phenomena, the Accessibility Hierarchy of relativizable grammatical relations. 1 Introduction An attractive hypothesis in psycholinguistics, dating back at least to the 1950s, has been that the degree of predictability of words in sentences is somehow related to understandability (Taylor, 1953), production difficulty (Goldman-Eisler, 1958) or, more recently, eye-movements (McDonald and Shillcock, 2003). However, since the 1950s, integrating this hypothesis with realistic models of linguistic structure has remained a challenge. Lounsbury (1954) appreciated the formal character of the problem. He defined a finite, artificial language, endowed with a rudimentary phonology, morphology and syntax, and showed that a word’s informational contribution could be formally defined as the entropy reduction brought about by its addition to the end of a sentence fragment. He qualified the significance of his achievement, saying An entropy reduction analysis presupposes that the number of possible messages is finite, and that the probabilities of each of the messages is known....Thus it appears that the entropy reduction analysis could be applied </context>
</contexts>
<marker>Lounsbury, 1954</marker>
<rawString>Floyd G. Lounsbury. 1954. Transitional probability, linguistic structure and systems of habitfamily hierarchies. In C. E. Osgood and T. A. Sebeok, editors, Psycholinguistics: a survey of theory and research. Indiana University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dana McDaniel</author>
<author>Cecile McKee</author>
<author>Judy B Bernstein</author>
</authors>
<title>How children’s relatives solve a problem for minimalism.</title>
<date>1998</date>
<journal>Language,</journal>
<pages>308--334</pages>
<contexts>
<context position="20108" citStr="McDaniel et al., 1998" startWordPosition="3383" endWordPosition="3386">tems in the Keenan and Hawkins (1987) set qualify. IO &lt; OBL there is only one type of extraction from indirect object, whereas on these grammars, the head of the oblique phrase (“for” “with” “on” or “in”) signals which of four categorically separate kinds of extraction has occurred. These alternatives correspond to four different derivation-nonterminals. OBL &lt; GEN both grammars analyze “whose” as taking a common noun argument, for example “whose ship.” But in just the promotion grammar, “whose” is further analyzed as the ordinary “who” morpheme plus a complex possessive phrase headed by “-s” (McDaniel et al., 1998). Because of the recursive character of this possessor category, the structure of “whose’s” common noun argument introduces additional uncertainty not present in the indirect object extracted relatives. Strikingly, the two grammars disagree on six outliers in figure 7(b) where just the adjunction grammar predicts very great difficulty in conjunction with the ERH. These outlier predictions are made on just the sentences that use the nominal carrier frame beginning with “the fact that...” Because the adjunction grammar analyzes relative clauses with an MG rule analogous to the phrase structure r</context>
</contexts>
<marker>McDaniel, McKee, Bernstein, 1998</marker>
<rawString>Dana McDaniel, Cecile McKee, and Judy B. Bernstein. 1998. How children’s relatives solve a problem for minimalism. Language, pages 308– 334.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott A McDonald</author>
<author>Richard C Shillcock</author>
</authors>
<title>Eye movements reveal the on-line computation of lexical probabilities during reading.</title>
<date>2003</date>
<journal>Psychological Science,</journal>
<pages>14--648</pages>
<contexts>
<context position="1039" citStr="McDonald and Shillcock, 2003" startWordPosition="136" endWordPosition="139">possibilities constitutes information processing work. Formalizing this notion of information processing work yields a complexity metric that predicts human repetition accuracy scores across a systematic class of linguistic phenomena, the Accessibility Hierarchy of relativizable grammatical relations. 1 Introduction An attractive hypothesis in psycholinguistics, dating back at least to the 1950s, has been that the degree of predictability of words in sentences is somehow related to understandability (Taylor, 1953), production difficulty (Goldman-Eisler, 1958) or, more recently, eye-movements (McDonald and Shillcock, 2003). However, since the 1950s, integrating this hypothesis with realistic models of linguistic structure has remained a challenge. Lounsbury (1954) appreciated the formal character of the problem. He defined a finite, artificial language, endowed with a rudimentary phonology, morphology and syntax, and showed that a word’s informational contribution could be formally defined as the entropy reduction brought about by its addition to the end of a sentence fragment. He qualified the significance of his achievement, saying An entropy reduction analysis presupposes that the number of possible messages</context>
</contexts>
<marker>McDonald, Shillcock, 2003</marker>
<rawString>Scott A. McDonald and Richard C. Shillcock. 2003. Eye movements reveal the on-line computation of lexical probabilities during reading. Psychological Science, 14:648–652.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jens Michaelis</author>
</authors>
<title>On Formal Properties of Minimalist Grammars.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>Potsdam University.</institution>
<contexts>
<context position="15141" citStr="Michaelis (2001)" startWordPosition="2492" endWordPosition="2493">2003). 4.2 Formal grammars of relativization The Minimalist Grammars (MG) formalism (cf. Stabler and Keenan (2003) for a systematic presentation) facilitates the relatively transparent implementation of ideas like movement and feature checking that figure prominently in the two analyses of relativization discussed in the previous subsection. MGs define a set of sentences by closing the structure-building functions merge and move on a finite set of lexical entries; however, this does not mean that parsing must happen bottom-up. A fundamental result, obtained independently by Harkema (2001) and Michaelis (2001) 3The X-bar structures depicted in figures 4 and 5 are drawn using tools developed by Edward Stabler and colleagues. =d +case v,-wh_rel d -case ::=p_to =d +case v p_to,-wh_rel ::=&gt;Pto p_to Pto,-wh_rel +case Pto,-case -wh_rel ::=d +case Pto d -case -wh_rel +f d -case -wh_rel,-f ::=Num +f d -case -wh_rel Num,-f ::=n Num ::n -f Figure 6: Derivation tree on promotion grammar. The derivation trees encode everything there is to know about MG derivations, and can be parsed in a variety of orders. Most importantly, if equipped with weights on their branches, they can be generated by probabilistic cont</context>
</contexts>
<marker>Michaelis, 2001</marker>
<rawString>Jens Michaelis. 2001. On Formal Properties of Minimalist Grammars. Ph.D. thesis, Potsdam University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Perlmutter</author>
<author>Paul Postal</author>
</authors>
<date>1974</date>
<booktitle>Lectures on Relational Grammar. LSA Linguistic Institute, UMass Amherst.</booktitle>
<contexts>
<context position="8753" citStr="Perlmutter and Postal, 1974" startWordPosition="1422" endWordPosition="1425">e processing predictions of the ERH on a systematic class of relative clause types, the Accessibility Hierarchy (AH) shown in figure 1. The AH is an implicational markedness hierarchy of grammatical relations discovered by Keenan and Comrie in (1977). The implication is that if a language has a relative-clause formation rule applicable to grammatical relations at some point x on the AH, then it can also form relative clauses on grammatical relations listed at all points before x. This hierarchy shows up in a variety of modern syntactic theories that have been influenced by Relational Grammar (Perlmutter and Postal, 1974). In Head-driven Phrase Structure Grammar (Pollard and Sag, 1994) the hierarchy corresponds to the order of elements on the SUBCAT list, and interacts with other principles in explanations of binding facts. The hierarchy also figures in LexicalFunctional Grammar (Bresnan, 1982) where it is known as Syntactic Rank. Keenan and Comrie speculated that their typological generalization might have a basis in performance factors. This idea was examined in a repetition-accuracy experiment carried out in 1974 but not published until 1987. Subjects in this study repeated back stimulus sentences after a d</context>
</contexts>
<marker>Perlmutter, Postal, 1974</marker>
<rawString>David Perlmutter and Paul Postal. 1974. Lectures on Relational Grammar. LSA Linguistic Institute, UMass Amherst.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Headdriven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="8818" citStr="Pollard and Sag, 1994" startWordPosition="1431" endWordPosition="1434">lause types, the Accessibility Hierarchy (AH) shown in figure 1. The AH is an implicational markedness hierarchy of grammatical relations discovered by Keenan and Comrie in (1977). The implication is that if a language has a relative-clause formation rule applicable to grammatical relations at some point x on the AH, then it can also form relative clauses on grammatical relations listed at all points before x. This hierarchy shows up in a variety of modern syntactic theories that have been influenced by Relational Grammar (Perlmutter and Postal, 1974). In Head-driven Phrase Structure Grammar (Pollard and Sag, 1994) the hierarchy corresponds to the order of elements on the SUBCAT list, and interacts with other principles in explanations of binding facts. The hierarchy also figures in LexicalFunctional Grammar (Bresnan, 1982) where it is known as Syntactic Rank. Keenan and Comrie speculated that their typological generalization might have a basis in performance factors. This idea was examined in a repetition-accuracy experiment carried out in 1974 but not published until 1987. Subjects in this study repeated back stimulus sentences after a delay while under the additional memory load of a digitmemory task</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag. 1994. Headdriven Phrase Structure Grammar. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Seki</author>
<author>Takashi Matsumura</author>
<author>Mamoru Fujii</author>
<author>Tadao Kasami</author>
</authors>
<title>On multiple contextfree grammars.</title>
<date>1991</date>
<journal>Theoretical Computer Science,</journal>
<pages>88--191</pages>
<contexts>
<context position="13181" citStr="Seki et al., 1991" startWordPosition="2172" endWordPosition="2175">ion of the enclosing, empty-headed (C0) complementizer phrase (CP), thereby checking a feature +wh as indicated in 3. (3) [CP [DP who boy[+f] ]i C0 [IP the father explained the answer to ti ] ] In a second movement, “boy” evacuates from DP, moving to another specifier (perhaps that of the silent agreement morpheme, Agr) as in 4 – checking a different feature, +f. (4) [AgrP boyj Agr [CP [DP who tj ]i C0 [IP the father explained the answer to ti ] ] ] The entire structure becomes a complement of a determiner to yield a larger DP in 5. is that MGs are equivalent to Multiple contextfree grammars (Seki et al., 1991). Multiple contextfree grammars generalize standard context-free grammars by allowing the string yields of daughter categories to be manipulated by a function other than simple concatenation. As in Tree Adjoining Grammar (Joshi et al., 1975) a record of these manipulations is kept at each node of an MG derivation tree, while a picture of the result is manifested in derived trees such as the ones in figures 4 and 5. The derivation tree on the promotion grammar is shown4 in figure 6 for the substring “the boy who the father explained the answer to.” d -case ::=c_rel d -case c_rel +wh_rel c_rel,-</context>
</contexts>
<marker>Seki, Matsumura, Fujii, Kasami, 1991</marker>
<rawString>Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, and Tadao Kasami. 1991. On multiple contextfree grammars. Theoretical Computer Science, 88:191–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Stabler</author>
<author>Edward Keenan</author>
</authors>
<date>2003</date>
<booktitle>Structural similarity. Theoretical Computer Science,</booktitle>
<pages>293--345</pages>
<contexts>
<context position="14639" citStr="Stabler and Keenan (2003)" startWordPosition="2412" endWordPosition="2415"> =d little_v v,-wh_rel ::=Num d -case Num +case v,-case,-wh_rel ::=n Num ::n No adjunction is used in this derivation, and, unconventionally, the leftmost “the” and “boy” do not share an exclusive common constituent. Nor is the wh-word “who” co-indexed with anything. Structural descriptions involving both the Kaynian analysis and the more standard adjunction analysis are shown in figures 4 and 5 respectively3. The other linguistic assumptions suggested by these diagrams are discussed in chapter 4 of Hale (2003). 4.2 Formal grammars of relativization The Minimalist Grammars (MG) formalism (cf. Stabler and Keenan (2003) for a systematic presentation) facilitates the relatively transparent implementation of ideas like movement and feature checking that figure prominently in the two analyses of relativization discussed in the previous subsection. MGs define a set of sentences by closing the structure-building functions merge and move on a finite set of lexical entries; however, this does not mean that parsing must happen bottom-up. A fundamental result, obtained independently by Harkema (2001) and Michaelis (2001) 3The X-bar structures depicted in figures 4 and 5 are drawn using tools developed by Edward Stabl</context>
</contexts>
<marker>Stabler, Keenan, 2003</marker>
<rawString>Edward Stabler and Edward Keenan. 2003. Structural similarity. Theoretical Computer Science, 293:345–363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward P Stabler</author>
</authors>
<title>Derivational minimalism.</title>
<date>1997</date>
<booktitle>Logical Aspects of Computational Linguistics,</booktitle>
<pages>68--95</pages>
<editor>In Christian Retor´e, editor,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="3072" citStr="Stabler, 1997" startWordPosition="458" endWordPosition="459">xamined. Entropy Reduction Hypothesis (ERH) a person’s processing difficulty at a word in a sentence is directly related to the number of bits signaled to the person by that word with respect to a probabilistic grammar the person knows. In section 2 a method for calculating the entropy reduction of a word in a sentence generated by a probabilistic grammar is presented. Section 3 describes the empirical domain of interest, the Accessibility Hierarchy (Keenan and Comrie, 1977). Section 4 goes on to describe two probabilistic grammars in the class of mildly context-sensitive Minimalist Grammars (Stabler, 1997). One expresses the “promotion analysis” (Kayne, 1994) of relative clauses while the other expresses the more standard “adjunction analysis” (Chomsky, 1977). The predictions of these grammars through the lens of the ERH are considered in sections 5 through 7, where it is shown that predictions derived from the promotion analysis match human repetition accuracy scores better than predictions derived from the adjunction analysis. Section 8 concludes. 2 Entropy Reduction The idea of the entropy reduction of a word is that uncertainty about grammatical continuations fluctuates as new words come in</context>
<context position="11238" citStr="Stabler, 1997" startWordPosition="1838" endWordPosition="1839"> on each word of a sentence is related to derivational information signaled by that word, then the total difficulty reading a sentence ought to be the sum of the difficulty on each word2. 4 Minimalist Grammars If correct, the ERH would explain the increasing difficulty across the AH in terms of greater or lesser uncertainty about intermediate parser states. To calculate these predictions, some assumption must be made about what those structures are. 4.1 Two analyses of relativization Toward this end, two grammars covering the Keenan and Hawkins stimuli were written in the Minimalist Grammars (Stabler, 1997) formalism. These grammars were exactly the same except for their treatment of relative clauses. One grammar expresses the usual analysis of relative clauses as right-adjoined modifiers (Chomsky, 1977). The other expresses the promotion analysis of relative clause. The analysis, which dates back to the 1960s, is revived in Kayne (1994). For reasons having to do with Kayne’s general theory of phrase structure, he proposes that, in a sentence like 1, the underlying form of the subject is akin to 2. 1Each response was coded for accuracy on a 0-2 scale where 2 means perfect repetition and 1 sugges</context>
</contexts>
<marker>Stabler, 1997</marker>
<rawString>Edward P. Stabler. 1997. Derivational minimalism. In Christian Retor´e, editor, Logical Aspects of Computational Linguistics, pages 68–95. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wilson Taylor</author>
</authors>
<title>Cloze procedure: a new tool for measuring readability.</title>
<date>1953</date>
<journal>Journalism Quarterly,</journal>
<pages>30--415</pages>
<contexts>
<context position="929" citStr="Taylor, 1953" startWordPosition="126" endWordPosition="127">reatly constrains these grammatical expectations, uncertainty is reduced. This elimination of possibilities constitutes information processing work. Formalizing this notion of information processing work yields a complexity metric that predicts human repetition accuracy scores across a systematic class of linguistic phenomena, the Accessibility Hierarchy of relativizable grammatical relations. 1 Introduction An attractive hypothesis in psycholinguistics, dating back at least to the 1950s, has been that the degree of predictability of words in sentences is somehow related to understandability (Taylor, 1953), production difficulty (Goldman-Eisler, 1958) or, more recently, eye-movements (McDonald and Shillcock, 2003). However, since the 1950s, integrating this hypothesis with realistic models of linguistic structure has remained a challenge. Lounsbury (1954) appreciated the formal character of the problem. He defined a finite, artificial language, endowed with a rudimentary phonology, morphology and syntax, and showed that a word’s informational contribution could be formally defined as the entropy reduction brought about by its addition to the end of a sentence fragment. He qualified the signific</context>
</contexts>
<marker>Taylor, 1953</marker>
<rawString>Wilson Taylor. 1953. Cloze procedure: a new tool for measuring readability. Journalism Quarterly, 30:415–433.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>