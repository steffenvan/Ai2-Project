<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000212">
<title confidence="0.9993805">
Predicting the Compositionality of Multiword Expressions
Using Translations in Multiple Languages
</title>
<author confidence="0.960427">
Bahar Salehi&apos;6° and Paul Cook°
</author>
<affiliation confidence="0.801873666666667">
A NICTA Victoria Research Laboratory
Q Department of Computing and Information Systems
The University of Melbourne
</affiliation>
<address confidence="0.685598">
Victoria 3010, Australia
</address>
<email confidence="0.998193">
bsalehi@student.unimelb.edu.au, paulcook@unimelb.edu.au
</email>
<sectionHeader confidence="0.99564" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998670153846154">
In this paper, we propose a simple, language-
independent and highly effective method for
predicting the degree of compositionality of
multiword expressions (MWEs). We compare
the translations of an MWE with the trans-
lations of its components, using a range of
different languages and string similarity mea-
sures. We demonstrate the effectiveness of
the method on two types of English MWEs:
noun compounds and verb particle construc-
tions. The results show that our approach is
competitive with or superior to state-of-the-art
methods over standard datasets.
</bodyText>
<sectionHeader confidence="0.714454" genericHeader="categories and subject descriptors">
1 Compositionality of MWEs
</sectionHeader>
<bodyText confidence="0.9996513125">
A multiword expression (MWE) is any combina-
tion of words with lexical, syntactic or semantic
idiosyncrasy (Sag et al., 2002; Baldwin and Kim,
2009), in that the properties of the MWE are not
predictable from the component words. For exam-
ple, with ad hoc, the fact that neither ad nor hoc are
standalone English words, makes ad hoc a lexically-
idiosyncratic MWE; with shoot the breeze, on the
other hand, we have semantic idiosyncrasy, as the
meaning of “to chat” in usages such as It was good
to shoot the breeze with you1 cannot be predicted
from the meanings of the component words shoot
and breeze.
Semantic idiosyncrasy has been of particular in-
terest to NLP researchers, with research on bi-
nary compositional/non-compositional MWE clas-
</bodyText>
<footnote confidence="0.9916825">
1The example is taken from http://www.
thefreedictionary.com
</footnote>
<bodyText confidence="0.998800848484849">
sification (Lin, 1999; Baldwin et al., 2003), or
a three-way compositional/semi-compositional/non-
compositional distinction (Fazly and Stevenson,
2007). There has also been research to suggest that
MWEs span the entire continuum from full compo-
sitionality to full non-compositionality (McCarthy et
al., 2003; Reddy et al., 2011).
Investigating the degree of MWE compositional-
ity has been shown to have applications in informa-
tion retrieval and machine translation (Acosta et al.,
2011; Venkatapathy and Joshi, 2006). As an exam-
ple of an information retrieval system, if we were
looking for documents relating to rat race (mean-
ing “an exhausting routine that leaves no time for
relaxation”2), we would not be interested in docu-
ments on rodents. These results underline the need
for methods for broad-coverage MWE composition-
ality prediction.
In this research, we investigate the possibility of
using an MWE’s translations in multiple languages
to measure the degree of the MWE’s compositional-
ity, and investigate how literal the semantics of each
component is within the MWE. We use Panlex to
translate the MWE and its components, and compare
the translations of the MWE with the translations
of its components using string similarity measures.
The greater the string similarity, the more composi-
tional the MWE is.
Whereas past research on MWE compositionality
has tended to be tailored to a specific MWE type
(McCarthy et al., 2007; Kim and Baldwin, 2007;
Fazly et al., 2009), our method is applicable to
any MWE type in any language. Our experiments
</bodyText>
<footnote confidence="0.858822">
2This definition is from WordNet 3.1.
</footnote>
<page confidence="0.933641">
266
</page>
<note confidence="0.9018225">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 266–275, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.995082333333333">
over two English MWE types demonstrate that our
method is competitive with state-of-the-art methods
over standard datasets.
</bodyText>
<sectionHeader confidence="0.999292" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999884252873563">
Most previous work on measuring MWE composi-
tionality makes use of lexical, syntactic or semantic
properties of the MWE. One early study on MWE
compositionality was Lin (1999), who claimed that
the distribution of non-compositional MWEs (e.g.
shoot the breeze) differs significantly from the dis-
tribution of expressions formed by substituting one
of the components with a semantically similar word
(e.g. shoot the wind). Unfortunately, the method
tends to fall down in cases of high statistical id-
iosyncrasy (or “institutionalization”): consider fry-
ing pan which is compositional but distributionally
very different to phrases produced through word-
substitution such as sauteing pan or frying plate.
Some research has investigated the syntactic
properties of MWEs, to detect their composition-
ality (Fazly et al., 2009; McCarthy et al., 2007).
The assumption behind these methods is that non-
compositional MWEs are more syntactically fixed
than compositional MWEs. For example, make a de-
cision can be passivised, but shoot the breeze cannot.
One serious problem with syntax-based methods is
their lack of generalization: each type of MWE has
its own characteristics, and these characteristics dif-
fer from one language to another. Moreover, some
MWEs (such as noun compounds) are not flexible
syntactically, no matter whether they are composi-
tional or non-compositional (Reddy et al., 2011).
Much of the recent work on MWEs focuses on
their semantic properties, measuring the semantic
similarity between the MWE and its components us-
ing different resources, such as WordNet (Kim and
Baldwin, 2007) or distributional similarity relative
to a corpus (e.g. based on Latent Semantic Analysis:
Schone and Jurafsky (2001), Bannard et al. (2003),
Reddy et al. (2011)). The size of the corpus is im-
portant in methods based on distributional similarity.
Unfortunately, however, large corpora are not avail-
able for all languages.
Reddy et al. (2011) hypothesize that the num-
ber of common co-occurrences between a given
MWE and its component words indicates the de-
gree of compositionality of that MWE. First, the co-
occurrences of a given MWE/word are considered
as the values of a vector. They then measure the
Cosine similarity between the vectors of the MWE
and its components. Bannard et al. (2003) presented
four methods to measure the compositionality of En-
glish verb particle constructions. Their best result
is based on the previously-discussed method of Lin
(1999) for measuring compositionality, but uses a
more-general distributional similarity model to iden-
tify synonyms.
Recently, a few studies have investigated using
parallel corpora to detect the degree of composi-
tionality (Melamed, 1997; Moir´on and Tiedemann,
2006; de Caseli et al., 2010; Salehi et al., 2012).
The general approach is to word-align the source
and target language sentences and analyse align-
ment patterns for MWEs (e.g. if the MWE is al-
ways aligned as a single “phrase”, then it is a strong
indicator of non-compositionality). de Caseli et
al. (2010) consider non-compositional MWEs to be
those candidates that align to the same target lan-
guage unit, without decomposition into word align-
ments. Melamed (1997) suggests using mutual in-
formation to investigate how well the translation
model predicts the distribution of words in the tar-
get text given the distribution of words in the source
text. Moir´on and Tiedemann (2006) show that en-
tropy is a good indicator of compositionality, be-
cause word alignment models are often confused by
non-compositional MWEs. However, this assump-
tion does not always hold, especially when deal-
ing with high-frequency non-compositional MWEs.
Salehi et al. (2012) tried to solve this problem with
high frequency MWEs by using word alignment in
both directions.3 They computed backward and for-
ward entropy to try to remedy the problem with es-
pecially high-frequency phrases. However, their as-
sumptions were not easily generalisable across lan-
guages, e.g., they assume that the relative frequency
of a specific type of MWE (light verb constructions)
in Persian is much greater than in English.
Although methods using bilingual corpora are in-
tuitively appealing, they have a number of draw-
backs. The first and the most important problem
</bodyText>
<footnote confidence="0.99379">
3The IBM models (Brown et al., 1993), e.g., are not bidi-
rectional, which means that the alignments are affected by the
alignment direction.
</footnote>
<page confidence="0.996859">
267
</page>
<bodyText confidence="0.999312111111111">
is data: they need large-scale parallel bilingual cor-
pora, which are available for relatively few language
pairs. Second, since they use statistical measures,
they are not suitable for measuring the composition-
ality of MWEs with low frequency. And finally,
most experiments have been carried out on English
paired with other European languages, and it is not
clear whether the results translate across to other
language pairs.
</bodyText>
<sectionHeader confidence="0.998775" genericHeader="method">
3 Resources
</sectionHeader>
<bodyText confidence="0.999963181818182">
In this research, we use the translations of MWEs
and their components to estimate the relative de-
gree of compositionality of a MWE. There are
several resources available to translate words into
various languages such as Babelnet (Navigli and
Ponzetto, 2010),4 Wiktionary,5 Panlex (Baldwin et
al., 2010) and Google Translate.6 As we are ide-
ally after broad coverage over multiple languages
and MWEs/component words in a given language,
we exclude Babelnet and Wiktionary from our cur-
rent research. Babelnet covers only six languages
at the time of writing this paper, and in Wiktionary,
because it is constantly being updated, words and
MWEs do not have translations into the same lan-
guages. This leaves translation resources such as
Panlex and Google Translate. However, after man-
ually analysing the two resources for a range of
MWEs, we decided not to use Google Translate for
two reasons: (1) we consider the MWE out of con-
text (i.e., we are working at the type level and do not
consider the usage of the MWE in a particular sen-
tence), and Google Translate tends to generate com-
positional translations of MWEs out of context; and
(2) Google Translate provides only one translation
for each component word/MWE. This left Panlex.
Panlex is an online translation database that is
freely available. It contains lemmatized words and
MWEs in a large variety of languages, with lemma-
based (and less frequently sense-based) links be-
tween them. The database covers more than 1353
languages, and is made up of 12M lemmas and ex-
pressions. The translations are sourced from hand-
made electronic dictionaries, making it more accu-
</bodyText>
<footnote confidence="0.999878666666667">
4http://lcl.uniroma1.it/babelnet/
5http://www.wiktionary.org/
6http://translate.google.com/
</footnote>
<bodyText confidence="0.999898785714286">
rate than translation dictionaries generated automat-
ically, e.g. through word alignment. Usually there
are several direct translations for a word/MWE
from one language to another, as in translations
which were extracted from electronic dictionaries. If
there is no direct translation for a word/MWE in the
database, we can translate indirectly via one or more
pivot languages (indirect translation: Soderland et
al. (2010)). For example, English ivory tower has
direct translations in only 13 languages in Panlex,
including French (tour d’ivoire) but not Esperanto.
There is, however, a translation of tour d’ivoire into
Esperanto (ebura turo), allowing us to infer an indi-
rect translation between ivory tower and ebura turo.
</bodyText>
<sectionHeader confidence="0.998707" genericHeader="method">
4 Dataset
</sectionHeader>
<bodyText confidence="0.999793533333333">
We evaluate our method over two datasets, as de-
scribed below.
REDDY (Reddy et al., 2011): 90 English (binary)
noun compounds (NCs), where the overall NC and
each component word has been annotated for com-
positionality on a scale from 0 (non-compositional)
to 5 (compositional). In order to avoid issues
with polysemy, the annotators were presented with
each NC in a sentential context. The authors tried
to achieve a balance of compositional and non-
compositional NCs: based on a threshold of 2.5, the
dataset consists of 43 (48%) compositional NCs, 46
(51%) NCs with a compositional usage of the first
component, and 54 (60%) NCs with a compositional
usage of the second component.
BANNARD (Bannard, 2006): 160 English verb
particle constructions (VPCs) were annotated for
compositionality relative to each of the two compo-
nent words (the verb and the particle). Each annota-
tor was asked to annotate each of the verb and parti-
cle as yes, no or don’t know. Based on the ma-
jority annotation, among the 160 VPCs, 122 (76%)
are verb-compositional and 76 (48%) are particle-
compositional.
We compute the proportion of yes tags to get the
compositionality score. This dataset, unlike REDDY,
does not include annotations for the compositional-
ity of the whole VPC, and is also less balanced, con-
taining more VPCs which are verb-compositional
than verb-non-compositional.
</bodyText>
<page confidence="0.995444">
268
</page>
<figureCaption confidence="0.99983">
Figure 1: Schematic of our proposed method
</figureCaption>
<sectionHeader confidence="0.961375" genericHeader="method">
5 Method
</sectionHeader>
<bodyText confidence="0.999974952380952">
To predict the degree of compositionality of an
MWE, we require a way to measure the semantic
similarity of the MWE with its components. Our
hypothesis is that compositional MWEs are more
likely to be word-for-word translations in a given
language than non-compositional MWEs. Hence, if
we can locate the translations of the components in
the translation of the MWE, we can deduce that it
is compositional. Our second hypothesis is that the
more languages we use as the basis for determin-
ing translation similarity between the MWE and its
component words, the more accurately we will be
able to estimate compositionality. Thus, rather than
using just one translation language, we experiment
with as many languages as possible.
Figure 1 provides a schematic outline of our
method. The MWE and its components are trans-
lated using Panlex. Then, we compare the transla-
tion of the MWE with the translations of its compo-
nents. In order to locate the translation of each com-
ponent in the MWE translation, we use string simi-
</bodyText>
<figure confidence="0.97967775">
English Persian Translation
kick the bucket mord
kick zad
the –
bucket satl
make a decision tasmim gereft
make sakht
a yek
decision tasmim
public service khadamaat omumi
public omumi
service khedmat
</figure>
<tableCaption confidence="0.991132">
Table 1: English MWEs and their components with their
translation in Persian. Direct matches between the trans-
lation of a MWE and its components are shown in bold;
partial matches are underlined.
</tableCaption>
<bodyText confidence="0.99828337037037">
larity measures. The score shown in Figure 1 is de-
rived from a given language. In Section 6, we show
how to combine scores across multiple languages.
As an example of our method, consider the
English-to-Persian translation of kick the bucket as
a non-compositional MWE and make a decision as
a semi-compositional MWE (Table 1).7 By locating
the translation of decision (tasmim) in the translation
of make a decision (tasmim gereftan), we can deduce
that it is semi-compositional. However, we cannot
locate any of the component translations in the trans-
lation of kick the bucket. Therefore, we conclude
that it is non-compositional. Note that in this simple
example, the match is word-level, but that due to the
effects of morphophonology, the more likely situa-
tion is that the components don’t match exactly (as
we observe in the case of khadamaat and khedmat
for the public service example), which motivates our
use of string similarity measures which can capture
partial matches.
We consider the following string similarity mea-
sures to compare the translations. In each case,
we normalize the output value to the range [0, 1],
where 1 indicates identical strings and 0 indicates
completely different strings. We will indicate the
translation of the MWE in a particular language t as
MWEt, and the translation of a given component in
</bodyText>
<footnote confidence="0.893257">
7Note that the Persian words are transliterated into English
for ease of understanding.
</footnote>
<figure confidence="0.99774275">
MWE Components
...
Panlex
...
Translations
Compare
Translate
Score
</figure>
<page confidence="0.974657">
269
</page>
<bodyText confidence="0.984655571428571">
language t as components.
Longest common substring (LCS): The LCS
measure finds the longest common substring be-
tween two strings. For example, the LCS between
ABABC and BABCAB is BABC. We calculate a nor-
malized similarity value based on the length of the
LCS as follows:
</bodyText>
<equation confidence="0.8945245">
LongestCommonString(MWEs, components)
min(len(MWEs), len(components))
</equation>
<bodyText confidence="0.949682">
Levenshtein (LEV1): The Levenshtein distance
calculates for the number of basic edit operations re-
quired to transpose one word into the other. Edits
consist of single-letter insertions, deletions or sub-
stitutions. We normalize LEV1 as follows:
</bodyText>
<equation confidence="0.9587595">
LEV1(MWEs, components)
max(len(MWEs), len(components))
</equation>
<bodyText confidence="0.9957745">
Levenshtein with substitution penalty (LEV2):
One well-documented feature of Levenshtein dis-
tance (Baldwin, 2009) is that substitutions are in fact
the combination of an addition and a deletion, and as
such can be considered to be two edits. Based on this
observation, we experiment with a variant of LEV1
with this penalty applied for substitutions. Similarly
to LEV1, we normalize as follows:
</bodyText>
<equation confidence="0.988288">
LEV2(MWEs, components)
len(MWEs) + len(components)
</equation>
<bodyText confidence="0.999647111111111">
Smith Waterman (SW) This method is based on
the Needleman-Wunsch algorithm,8 and was devel-
oped to locally-align two protein sequences (Smith
and Waterman, 1981). It finds the optimal simi-
lar regions by maximizing the number of matches
and minimizing the number of gaps necessary to
align the two sequences. For example, the opti-
mal local sequence for the two sequences below is
AT−−ATCC, in which “-” indicates a gap:
</bodyText>
<footnote confidence="0.559881777777778">
8The Needleman-Wunsch (NW) algorithm, was designed to
align two sequences of amino-acids (Needleman and Wunsch,
1970). The algorithm looks for the sequence alignment which
maximizes the similarity. As with the LEV score, NW min-
imizes edit distance, but also takes into account character-to-
character similarity based on the relative distance between char-
acters on the keyboard. We exclude this score, because it is
highly similar to the LEV scores, and we did not obtain encour-
aging results using NW in our preliminary experiments.
</footnote>
<note confidence="0.464775">
Seq1: ATGCATCCCATGAC
</note>
<sectionHeader confidence="0.361876" genericHeader="method">
Seq2: TCTATATCCGT
</sectionHeader>
<bodyText confidence="0.999877625">
As the example shows, it looks for the longest com-
mon string but has an in-built mechanism for includ-
ing gaps in the alignment (with penalty). This char-
acteristic of SW might be helpful in our task, be-
cause there may be morphophonological variations
between the MWE and component translations (as
seen above in the public service example). We nor-
malize SW similarly to LCS:
</bodyText>
<equation confidence="0.917826">
len(alignedSequence)
min(len(MWEs), len(components))
</equation>
<sectionHeader confidence="0.996483" genericHeader="method">
6 Computational Model
</sectionHeader>
<bodyText confidence="0.999976333333333">
Given the scores calculated by the aforementioned
string similarity measures between the translations
for a given component word and the MWE, we need
some way of combining scores across component
words.9 First, we measure the compositionality of
each component within the MWE (s1 and s2):
</bodyText>
<equation confidence="0.999231">
s1 = f1(sim1(w1, MWE), ..., simi(w1, MWE))
s2 = f1(sim1(w2, MWE), ..., simi(w2, MWE))
</equation>
<bodyText confidence="0.9999388">
where sim is a string similarity measure, simi indi-
cates that the calculation is based on translations in
language i, and f1 is a score combination function.
Then, we compute the overall compositionality of
the MWE (s3) from s1 and s2 using f2:
</bodyText>
<equation confidence="0.989416">
s3 = f2(s1, s2)
</equation>
<bodyText confidence="0.998304461538462">
Since we often have multiple translations for a given
component word/MWE in Panlex, we exhaustively
compute the similarity between each MWE transla-
tion and component translation, and use the highest
similarity as the result of simi. If an instance does
not have a direct/indirect translation in Panlex, we
assign a default value, which is the mean of the high-
est and lowest annotation score (2.5 for REDDY and
0.5 for BANNARD). Note that word order is not an
issue in our method, as we calculate the similarity
independently for each MWE component.
In this research, we consider simple functions for
f1 such as mean, median, product, min and max. f2
</bodyText>
<footnote confidence="0.9645195">
9Note that in all experiments we only combine scores given
by the same string similarity measure.
</footnote>
<figure confidence="0.4211605">
1
1
</figure>
<page confidence="0.774432">
270
</page>
<table confidence="0.99300925">
NC
Language Frequency Family
Czech 100 Slavic
Norwegian 100 Germanic
Portuguese 100 Romance
Thai 99 Kam-thai
French 95 Romance
Chinese 94 Chinese
Dutch 93 Germanic
Romanian 91 Romance
Hindi 67 Indic
Russian 43 Slavic
</table>
<tableCaption confidence="0.999874">
Table 2: The 10 best languages for REDDY using LCS.
</tableCaption>
<bodyText confidence="0.99894825">
was selected to be the same as f1 in all situations,
except when we use mean for f1. Here, following
Reddy et al. (2011), we experimented with weighted
mean:
</bodyText>
<equation confidence="0.998691">
f2(s1,s2) = αs1 + (1 − α)s2
</equation>
<bodyText confidence="0.998879909090909">
Based on 3-fold cross validation, we chose α = 0.7
for REDDY.10
Since we do not have judgements for the com-
positionality of the full VPC in BANNARD (we in-
stead have separate judgements for the verb and
particle), we cannot use f2 for this dataset. Ban-
nard et al. (2003) observed that nearly all of the
verb-compositional instances were also annotated as
particle-compositional by the annotators. In line
with this observation, we use s1 (based on the verb)
as the compositionality score for the full VPC.
</bodyText>
<sectionHeader confidence="0.971385" genericHeader="method">
7 Language Selection
</sectionHeader>
<bodyText confidence="0.999685625">
Our method is based on the translation of an MWE
into many languages. In the first stage, we chose 54
languages for which relatively large corpora were
available.11 The coverage, or the number of in-
stances which have direct/indirect translations in
Panlex, varies from one language to another. In
preliminary experiments, we noticed that there is
a high correlation (about 0.50 for BANNARD and
</bodyText>
<footnote confidence="0.855024">
10We considered values of α from 0 to 1, incremented by 0.1.
11In future work, we intend to look at the distribution of trans-
</footnote>
<figureCaption confidence="0.521985666666667">
lations of the given MWE and its components in corpora for
many languages. The present method does not rely on the avail-
ability of large corpora.
</figureCaption>
<table confidence="0.979073583333333">
VPC:verb
Language Frequency Family
Basque 100 Basque
Lithuanian 100 Baltic
Slovenian 100 Slavic
Hebrew 99 Semitic
Arabic 98 Semitic
Czech 95 Slavic
Slovak 92 Slavic
Latin 79 Italic
Tagalog 74 Austronesian
Polish 44 Slavic
</table>
<tableCaption confidence="0.9690215">
Table 3: The 10 best languages for the verb component
of BANNARD using LCS.
</tableCaption>
<table confidence="0.99382125">
VPC:particle
Language Frequency Family
French 100 Romance
Icelandic 100 Germanic
Thai 100 Kam-thai
Indonesian 92 Indonesian
Spanish 90 Romance
Tamil 87 Dravidian
Turkish 83 Turkic
Catalan 79 Romance
Occitan 76 Romance
Romanian 69 Romance
</table>
<tableCaption confidence="0.979327">
Table 4: The 10 best languages for the particle compo-
nent of BANNARD using LCS.
</tableCaption>
<bodyText confidence="0.998497">
about 0.80 for REDDY) between the usefulness of
a language and its translation coverage on MWEs.
Therefore, we excluded languages with MWE trans-
lation coverage of less than 50%. Based on nested
10-fold cross validation in our experiments, we se-
lect the 10 most useful languages for each cross-
validation training partition, based on the Pearson
correlation between the given scores in that language
and human judgements.12 The 10 best languages
are selected based only on the training set for each
fold. (The languages selected for each fold will later
be used to predict the compositionality of the items
in the testing portion for that fold.) In Tables 2, 3
12Note that for VPCs, we calculate the compositionality of
only the verb part, because we don’t have the human judge-
ments for the whole VPC.
</bodyText>
<page confidence="0.994497">
271
</page>
<table confidence="0.999934380952381">
f1 sim() N1 N2 NC
Mean SW 0.541 0.396 0.637
LCS 0.525 0.431 0.649
LEV1 0.405 0.200 0.523
LEV2 0.481 0.263 0.577
Prod SW 0.451 0.287 0.410
LCS 0.430 0.233 0.434
LEV1 0.299 0.128 0.311
LEV2 0.294 0.188 0.364
Median SW 0.443 0.334 0.544
LCS 0.408 0.365 0.553
LEV1 0.315 0.054 0.376
LEV2 0.404 0.134 0.523
Min SW 0.420 0.176 0.312
LCS 0.347 0.225 0.307
LEV1 0.362 0.310 0.248
LEV2 0.386 0.345 0.338
Max SW 0.371 0.408 0.345
LCS 0.406 0.430 0.335
LEV1 0.279 0.362 0.403
LEV2 0.380 0.349 0.406
</table>
<tableCaption confidence="0.964447">
Table 5: Correlation on REDDY (NCs). N1, N2 and NC,
are the first component of the noun compound, its second
component, and the noun compound itself, respectively.
</tableCaption>
<bodyText confidence="0.999898416666667">
and 4, we show how often each language was se-
lected in the top-10 languages over the combined
100 (10x10) folds of nested 10-fold cross valida-
tion, based on LCS.13 The tables show that the se-
lected languages were mostly consistent over the
folds. The languages are a mixture of Romance,
Germanic and languages from other families (based
on Voegelin and Voegelin (1977)), with no standout
language which performs well in all cases (indeed,
no language occurs in all three tables). Additionally,
there is nothing in common between the verb and the
particle top-10 languages.
</bodyText>
<sectionHeader confidence="0.999684" genericHeader="evaluation">
8 Results
</sectionHeader>
<bodyText confidence="0.933652285714286">
As mentioned before, we perform nested 10-fold
cross-validation to select the 10 best languages on
the training data for each fold. The selected lan-
guages for a given fold are then used to compute s1
13Since our later results show that LCS and SW have higher
results, we only show the best languages using LCS. These
largely coincide with those for SW.
</bodyText>
<table confidence="0.999744952380952">
f1 sim() Verb Particle
Mean SW 0.369 0.510
LCS 0.406 0.509
LEV1 0.335 0.454
LEV2 0.340 0.460
Prod SW 0.315 0.316
LCS 0.339 0.299
LEV1 0.322 0.280
LEV2 0.342 0.284
Median SW 0.316 0.409
LCS 0.352 0.423
LEV1 0.295 0.387
LEV2 0.309 0.368
Min SW 0.262 0.210
LCS 0.329 0.251
LEV1 0.307 0.278
LEV2 0.310 0.281
Max SW 0.141 0.288
LCS 0.268 0.299
LEV1 0.145 0.450
LEV2 0.170 0.398
</table>
<tableCaption confidence="0.99318">
Table 6: Correlation on BANNARD (VPC), based on the
best-10 languages for the verb and particle individually
</tableCaption>
<bodyText confidence="0.999898545454545">
and s2 (and s3 for NCs) for each instance in the test
set for that fold. The scores are compared with hu-
man judgements using Pearson’s correlation. The
results are shown in Tables 5 and 6. Among the five
functions we experimented with for f1, Mean per-
forms much more consistently than the others. Me-
dian is less prone to noise, and therefore performs
better than Prod, Max and Min, but it is still worse
than Mean.
For the most part, LCS and SW perform better
than the other measures. There is little to separate
these two methods, partly because they both look for
a sequence of similar characters, unlike LEV1 and
LEV2 which do not consider contiguity of match.
The results support our hypothesis that using mul-
tiple target languages rather than one, results in a
more accurate prediction of MWE compositionality.
Our best result using the 10 selected languages on
REDDY is 0.649, as compared to the best single-
language correlation of 0.497 for Portuguese. On
BANNARD, the best LCS result for the verb com-
ponent is 0.406, as compared to the best single-
</bodyText>
<page confidence="0.991407">
272
</page>
<bodyText confidence="0.99988965">
language correlation of 0.350 for Lithuanian.
Reddy et al. (2011) reported a correlation of 0.714
on REDDY. Our best correlation is 0.649. Note that
Reddy et al. (2011) base their method on identifi-
cation of MWEs in a corpus, thus requiring MWE-
specific identification. Given that this has been
shown to be difficult for MWE types including En-
glish VPCs (McCarthy et al., 2003; Baldwin, 2005),
the fact that our method is as competitive as this is
highly encouraging, especially when you consider
that it can equally be applied to different types of
MWEs in other languages. Moreover, the computa-
tional processing required by methods based on dis-
tributional similarity is greater than our method, as
it does not require processing a large corpus.
Finally, we experimented with combining our
method (STRINGSIMMEAN) with a reimplementation
of the method of Reddy et al. (2011), based on sim-
ple averaging, as detailed in Table 7. The results are
higher than both component methods and the state-
of-the-art for REDDY, demonstrating the comple-
mentarity between our proposed method and meth-
ods based on distributional similarity.
In Table 8, we compare our results
(STRINGSIMMEAN) with those of Bannard et
al. (2003), who interpreted the dataset as a binary
classification task. The dataset used in their study
is a subset of BANNARD, containing 40 VPCs, of
which 29 (72%) were verb compositional and 23
(57%) were particle compositional. By applying a
threshold of 0.5 over the output of our regression
model, we binarize the VPCs into the compositional
and non-compositional classes. According to the
results shown in Table 6, LCS is a better similarity
measure for this task. Our proposed method has
higher results than the best results of Bannard et
al. (2003), in part due to their reliance on VPC
identification, and the low recall on the task, as
reported in the paper. Our proposed method does
not rely on a corpus or MWE identification.
</bodyText>
<sectionHeader confidence="0.998842" genericHeader="evaluation">
9 Error Analysis
</sectionHeader>
<bodyText confidence="0.99997472">
We analyse items in REDDY which have a high dif-
ference (more than 2.5) between the human anno-
tation and our scores (using LCS and Mean). The
words are cutting edge, melting pot, gold mine and
ivory tower, which are non-compositional accord-
ing to REDDY. After investigating their translations,
we came to the conclusion that the first three MWEs
have word-for-word translations in most languages.
Hence, they disagree with our hypothesis that word-
for-word translation is a strong indicator of compo-
sitionality. The word-for-word translations might be
because of the fact that they have both compositional
and non-compositional senses, or because they are
calques (loan translations). However, we have tried
to avoid such problems with calques by using trans-
lations into several languages.
For ivory tower (“a state of mind that is discussed
as if it were a place”)14 we noticed that we have a di-
rect translation into 13 languages. Other languages
have indirect translations. By checking the direct
translations, we noticed that, in French, the MWE is
translated to tour and tour d’ivoire. A noisy (wrong)
translation of tour “tower” resulted in wrong indirect
translations for ivory tower and an inflated estimate
of compositionality.
</bodyText>
<sectionHeader confidence="0.976803" genericHeader="conclusions">
10 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999967631578947">
In this study, we proposed a method to predict MWE
compositionality based on the translation of the
MWE and its component words into multiple lan-
guages. We used string similarity measures between
the translations of the MWE and each of its compo-
nents to predict the relative degree of composition-
ality. Among the four similarity measures that we
experimented with, LCS and SW were found to be
superior to edit distance-based methods. Our best re-
sults were found to be competitive with state-of-the-
art results using vector-based approaches, and were
also shown to complement state-of-the-art methods.
In future work, we are interested in investigating
whether alternative ways of combining our proposed
method with vector-based models can lead to fur-
ther enhancements in results. These models could
be especially effective when comparing translations
which are roughly synonymous but not string-wise
similar.
</bodyText>
<sectionHeader confidence="0.998295" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999797">
We would like to thank Timothy Baldwin, Su Nam
Kim, and the anonymous reviewers for their valu-
able comments and suggestions.
</bodyText>
<footnote confidence="0.628136">
14This definition is from Wordnet 3.1.
</footnote>
<page confidence="0.993004">
273
</page>
<table confidence="0.943276">
3iMO STRINGSIMMEAN STRINGSIMMEAN + Reddy et al.
SW 0.637 0.735
LCS 0.649 0.742
LEV1 0.523 0.724
LEV2 0.577 0.726
</table>
<tableCaption confidence="0.9559235">
Table 7: Correlation after combining Reddy et al.’s method and our method with Mean for f, (STRINGSIMMEAN). The
correlation using Reddy et al.’s method is 0.714.
</tableCaption>
<table confidence="0.999812">
Method Precision Recall F-score (Q = 1) Accuracy
Bannard et al. (2003) 0.608 0.666 0.636 0.600
STRINGSIMMEAN 0.862 0.718 0.774 0.693
</table>
<tableCaption confidence="0.998402">
Table 8: Results for the classification task. STRINGSIMMEAN is our method using Mean for f,
</tableCaption>
<bodyText confidence="0.9997158">
NICTA is funded by the Australian Government
as represented by the Department of Broadband,
Communications and the Digital Economy and the
Australian Research Council through the ICT Cen-
tre of Excellence program.
</bodyText>
<sectionHeader confidence="0.997982" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999365123076923">
Otavio Costa Acosta, Aline Villavicencio, and Viviane P
Moreira. 2011. Identification and treatment of multi-
word expressions applied to information retrieval. In
Proceedings of the ALC Workshop on MWEs: from
Parsing and Generation to the Real World (MWE
2011), pages 101–109.
Timothy Baldwin and Su Nam Kim. 2009. Multiword
expressions. In Nitin Indurkhya and Fred J. Damerau,
editors, Handbook of Natural Language Processing.
CRC Press, Boca Raton, USA, 2nd edition.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model of
multiword expression decomposability. In Proceed-
ings of the ACL-2003 Workshop on Multiword Expres-
sions: Analysis, Acquisition and Treatment, pages 89–
96, Sapporo, Japan.
Timothy Baldwin, Jonathan Pool, and Susan M Colow-
ick. 2010. Panlex and lextract: Translating all words
of all languages of the world. In Proceedings of the
23rd International Conference on Computational Lin-
guistics: Demonstrations, pages 37–40.
Timothy Baldwin. 2005. The deep lexical acquisition of
English verb-particle constructions. Computer Speech
and Language, Special Issue on Multiword Expres-
sions, 19(4):398–414.
Timothy Baldwin. 2009. The hare and the tortoise:
Speed and reliability in translation retrieval. Machine
Translation, 23(4):195–240.
Colin Bannard, Timothy Baldwin, and Alex Lascarides.
2003. A statistical approach to the semantics of verb-
particles. In Proceedings of the ACL 2003 workshop
on Multiword expressions: analysis, acquisition and
treatment-Volume 18, pages 65–72.
Colin James Bannard. 2006. Acquiring Phrasal Lexicons
from Corpora. Ph.D. thesis, University of Edinburgh.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263–311.
Helena Medeiros de Caseli, Carlos Ramisch, Maria das
Grac¸as Volpe Nunes, and Aline Villavicencio. 2010.
Alignment-based extraction of multiword expressions.
Language Resources and Evaluation, 44(1):59–77.
Afsaneh Fazly and Suzanne Stevenson. 2007. Dis-
tinguishing subtypes of multiword expressions using
linguistically-motivated statistical measures. In Pro-
ceedings of the ACL 2007 Workshop on A Broader Per-
spective on Multiword Expressions, pages 9–16.
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identification
of idiomatic expressions. Computational Linguistics,
35(1):61–103.
Su Nam Kim and Timothy Baldwin. 2007. Detecting
compositionality of english verb-particle constructions
using semantic similarity. In Proceedings of the 7th
Meeting of the Pacific Association for Computational
Linguistics (PACLING 2007), pages 40–48.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of the 37th
annual meeting of the Association for Computational
Linguistics on Computational Linguistics, pages 317–
324.
Diana McCarthy, Bill Keller, and John Carroll. 2003.
Detecting a continuum of compositionality in phrasal
verbs. In Proceedings of the ACL 2003 workshop
</reference>
<page confidence="0.976184">
274
</page>
<reference confidence="0.999303619047619">
on Multiword expressions: analysis, acquisition and
treatment-Volume 18, pages 73–80.
Diana McCarthy, Sriram Venkatapathy, and Aravind K
Joshi. 2007. Detecting compositionality of verb-
object combinations using selectional preferences. In
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 369–379.
I. Dan Melamed. 1997. Automatic discovery of non-
compositional compounds in parallel data. In Pro-
ceedings of the Fifth Workshop on Very Large Cor-
pora. EMNLP.
Begona Villada Moir´on and J¨org Tiedemann. 2006.
Identifying idiomatic expressions using automatic
word-alignment. In Proceedings of the EACL 2006
Workshop on Multi-wordexpressions in a multilingual
context, pages 33–40.
Roberto Navigli and Simone Paolo Ponzetto. 2010. Ba-
belnet: Building a very large multilingual semantic
network. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 216–225, Uppsala, Sweden.
Saul B Needleman and Christian D Wunsch. 1970. A
general method applicable to the search for similarities
in the amino acid sequence of two proteins. Journal of
molecular biology, 48(3):443–453.
Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011. An empirical study on compositionality in com-
pound nouns. In Proceedings of IJCNLP, pages 210–
218.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for nlp. In Proceed-
ings of the 3rd International Conference on Intelligent
Text Processing Computational Linguistics (CICLing-
2002), pages 189–206. Springer.
Bahar Salehi, Narjes Askarian, and Afsaneh Fazly. 2012.
Automatic identification of Persian light verb con-
structions. In Proceedings of the 13th International
Conference on Intelligent Text Processing Computa-
tional Linguistics (CICLing-2012), pages 201–210.
Patrick Schone and Dan Jurafsky. 2001. Is knowledge-
free induction of multiword unit dictionary headwords
a solved problem. In Proceedings of the 6th Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2001), pages 100–108.
TF Smith and MS Waterman. 1981. Identification of
common molecular subsequences. Molecular Biology,
147:195–197.
Stephen Soderland, Oren Etzioni, Daniel S Weld, Kobi
Reiter, Michael Skinner, Marcus Sammer, Jeff Bilmes,
et al. 2010. Panlingual lexical translation via proba-
bilistic inference. Artificial Intelligence, 174(9):619–
637.
Sriram Venkatapathy and Aravind K Joshi. 2006. Us-
ing information about multi-word expressions for the
word-alignment task. In Proceedings of the Workshop
on Multiword Expressions: Identifying and Exploiting
Underlying Properties, pages 20–27.
Charles Frederick Voegelin and Florence Marie Voegelin.
1977. Classification and index of the world’s lan-
guages, volume 4. Elsevier Science Ltd.
</reference>
<page confidence="0.998429">
275
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99901">Predicting the Compositionality of Multiword Using Translations in Multiple Languages</title>
<author confidence="0.806975">Paul</author>
<affiliation confidence="0.862741666666667">Victoria Research of Computing and Information The University of</affiliation>
<address confidence="0.917119">Victoria 3010,</address>
<email confidence="0.99566">bsalehi@student.unimelb.edu.au,paulcook@unimelb.edu.au</email>
<abstract confidence="0.978958712121212">In this paper, we propose a simple, languageindependent and highly effective method for predicting the degree of compositionality of multiword expressions (MWEs). We compare the translations of an MWE with the translations of its components, using a range of different languages and string similarity measures. We demonstrate the effectiveness of the method on two types of English MWEs: noun compounds and verb particle constructions. The results show that our approach is competitive with or superior to state-of-the-art methods over standard datasets. 1 Compositionality of MWEs A multiword expression (MWE) is any combination of words with lexical, syntactic or semantic idiosyncrasy (Sag et al., 2002; Baldwin and Kim, 2009), in that the properties of the MWE are not predictable from the component words. For examwith the fact that neither English words, makes hoc lexically- MWE; with the on the other hand, we have semantic idiosyncrasy, as the of “to chat” in usages such as was good shoot the breeze with cannot be predicted the meanings of the component words Semantic idiosyncrasy has been of particular interest to NLP researchers, with research on bicompositional/non-compositional MWE clasexample is taken from thefreedictionary.com sification (Lin, 1999; Baldwin et al., 2003), or a three-way compositional/semi-compositional/noncompositional distinction (Fazly and Stevenson, 2007). There has also been research to suggest that MWEs span the entire continuum from full compositionality to full non-compositionality (McCarthy et al., 2003; Reddy et al., 2011). Investigating the degree of MWE compositionality has been shown to have applications in information retrieval and machine translation (Acosta et al., 2011; Venkatapathy and Joshi, 2006). As an example of an information retrieval system, if we were for documents relating to race (meaning “an exhausting routine that leaves no time for we would not be interested in documents on rodents. These results underline the need for methods for broad-coverage MWE compositionality prediction. In this research, we investigate the possibility of using an MWE’s translations in multiple languages to measure the degree of the MWE’s compositionality, and investigate how literal the semantics of each component is within the MWE. We use Panlex to translate the MWE and its components, and compare the translations of the MWE with the translations of its components using string similarity measures. The greater the string similarity, the more compositional the MWE is. Whereas past research on MWE compositionality has tended to be tailored to a specific MWE type (McCarthy et al., 2007; Kim and Baldwin, 2007; Fazly et al., 2009), our method is applicable to any MWE type in any language. Our experiments definition is from WordNet 3.1.</abstract>
<note confidence="0.858350666666667">266 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference the Shared pages 266–275, Atlanta, Georgia, June 13-14, 2013. Association for Computational Linguistics</note>
<abstract confidence="0.990795608108108">over two English MWE types demonstrate that our method is competitive with state-of-the-art methods over standard datasets. 2 Related Work Most previous work on measuring MWE compositionality makes use of lexical, syntactic or semantic properties of the MWE. One early study on MWE compositionality was Lin (1999), who claimed that the distribution of non-compositional MWEs (e.g. the differs significantly from the distribution of expressions formed by substituting one of the components with a semantically similar word the Unfortunately, the method tends to fall down in cases of high statistical id- (or “institutionalization”): consider frypan is compositional but distributionally very different to phrases produced through wordsuch as pan Some research has investigated the syntactic properties of MWEs, to detect their compositionality (Fazly et al., 2009; McCarthy et al., 2007). The assumption behind these methods is that noncompositional MWEs are more syntactically fixed compositional MWEs. For example, a debe passivised, but the breeze One serious problem with syntax-based methods is their lack of generalization: each type of MWE has its own characteristics, and these characteristics differ from one language to another. Moreover, some MWEs (such as noun compounds) are not flexible syntactically, no matter whether they are compositional or non-compositional (Reddy et al., 2011). Much of the recent work on MWEs focuses on their semantic properties, measuring the semantic similarity between the MWE and its components using different resources, such as WordNet (Kim and Baldwin, 2007) or distributional similarity relative to a corpus (e.g. based on Latent Semantic Analysis: Schone and Jurafsky (2001), Bannard et al. (2003), Reddy et al. (2011)). The size of the corpus is important in methods based on distributional similarity. Unfortunately, however, large corpora are not available for all languages. Reddy et al. (2011) hypothesize that the number of common co-occurrences between a given MWE and its component words indicates the degree of compositionality of that MWE. First, the cooccurrences of a given MWE/word are considered as the values of a vector. They then measure the Cosine similarity between the vectors of the MWE and its components. Bannard et al. (2003) presented four methods to measure the compositionality of English verb particle constructions. Their best result is based on the previously-discussed method of Lin (1999) for measuring compositionality, but uses a more-general distributional similarity model to identify synonyms. Recently, a few studies have investigated using parallel corpora to detect the degree of compositionality (Melamed, 1997; Moir´on and Tiedemann, 2006; de Caseli et al., 2010; Salehi et al., 2012). The general approach is to word-align the source and target language sentences and analyse alignment patterns for MWEs (e.g. if the MWE is always aligned as a single “phrase”, then it is a strong indicator of non-compositionality). de Caseli et al. (2010) consider non-compositional MWEs to be those candidates that align to the same target language unit, without decomposition into word alignments. Melamed (1997) suggests using mutual information to investigate how well the translation model predicts the distribution of words in the target text given the distribution of words in the source text. Moir´on and Tiedemann (2006) show that entropy is a good indicator of compositionality, because word alignment models are often confused by non-compositional MWEs. However, this assumption does not always hold, especially when dealing with high-frequency non-compositional MWEs. Salehi et al. (2012) tried to solve this problem with high frequency MWEs by using word alignment in They computed backward and forward entropy to try to remedy the problem with especially high-frequency phrases. However, their assumptions were not easily generalisable across languages, e.g., they assume that the relative frequency of a specific type of MWE (light verb constructions) in Persian is much greater than in English. Although methods using bilingual corpora are intuitively appealing, they have a number of drawbacks. The first and the most important problem IBM models (Brown et al., 1993), e.g., are not bidirectional, which means that the alignments are affected by the alignment direction. 267 is data: they need large-scale parallel bilingual corpora, which are available for relatively few language pairs. Second, since they use statistical measures, they are not suitable for measuring the compositionality of MWEs with low frequency. And finally, most experiments have been carried out on English paired with other European languages, and it is not clear whether the results translate across to other language pairs. 3 Resources In this research, we use the translations of MWEs and their components to estimate the relative degree of compositionality of a MWE. There are several resources available to translate words into various languages such as Babelnet (Navigli and Panlex (Baldwin et 2010) and Google As we are ideally after broad coverage over multiple languages and MWEs/component words in a given language, we exclude Babelnet and Wiktionary from our current research. Babelnet covers only six languages at the time of writing this paper, and in Wiktionary, because it is constantly being updated, words and MWEs do not have translations into the same languages. This leaves translation resources such as Panlex and Google Translate. However, after manually analysing the two resources for a range of MWEs, we decided not to use Google Translate for two reasons: (1) we consider the MWE out of context (i.e., we are working at the type level and do not consider the usage of the MWE in a particular sentence), and Google Translate tends to generate compositional translations of MWEs out of context; and (2) Google Translate provides only one translation for each component word/MWE. This left Panlex. Panlex is an online translation database that is freely available. It contains lemmatized words and MWEs in a large variety of languages, with lemmabased (and less frequently sense-based) links between them. The database covers more than 1353 languages, and is made up of 12M lemmas and expressions. The translations are sourced from handelectronic dictionaries, making it more accurate than translation dictionaries generated automatically, e.g. through word alignment. Usually there several translations a word/MWE from one language to another, as in translations which were extracted from electronic dictionaries. If there is no direct translation for a word/MWE in the database, we can translate indirectly via one or more languages Soderland et (2010)). For example, English tower direct translations in only 13 languages in Panlex, French but not Esperanto. is, however, a translation of d’ivoire allowing us to infer an inditranslation between tower 4 Dataset We evaluate our method over two datasets, as described below. et al., 2011): 90 English (binary) noun compounds (NCs), where the overall NC and each component word has been annotated for compositionality on a scale from 0 (non-compositional) to 5 (compositional). In order to avoid issues with polysemy, the annotators were presented with each NC in a sentential context. The authors tried to achieve a balance of compositional and noncompositional NCs: based on a threshold of 2.5, the dataset consists of 43 (48%) compositional NCs, 46 (51%) NCs with a compositional usage of the first component, and 54 (60%) NCs with a compositional usage of the second component. 2006): 160 English verb particle constructions (VPCs) were annotated for compositionality relative to each of the two component words (the verb and the particle). Each annotator was asked to annotate each of the verb and partias Based on the majority annotation, among the 160 VPCs, 122 (76%) are verb-compositional and 76 (48%) are particlecompositional. compute the proportion of to get the score. This dataset, unlike does not include annotations for the compositionality of the whole VPC, and is also less balanced, containing more VPCs which are verb-compositional than verb-non-compositional. 268 Figure 1: Schematic of our proposed method 5 Method To predict the degree of compositionality of an MWE, we require a way to measure the semantic similarity of the MWE with its components. Our hypothesis is that compositional MWEs are more likely to be word-for-word translations in a given language than non-compositional MWEs. Hence, if we can locate the translations of the components in the translation of the MWE, we can deduce that it is compositional. Our second hypothesis is that the more languages we use as the basis for determining translation similarity between the MWE and its component words, the more accurately we will be able to estimate compositionality. Thus, rather than using just one translation language, we experiment with as many languages as possible. Figure 1 provides a schematic outline of our method. The MWE and its components are translated using Panlex. Then, we compare the translation of the MWE with the translations of its components. In order to locate the translation of each comin the MWE translation, we use string simi- English Persian Translation kick the bucket mord kick zad the – bucket satl a decision make sakht a yek service khadamaatomumi Table 1: English MWEs and their components with their translation in Persian. Direct matches between the transof a MWE and its components are shown in matches are larity measures. The score shown in Figure 1 is derived from a given language. In Section 6, we show how to combine scores across multiple languages. As an example of our method, consider the translation of the bucket non-compositional MWE and a decision semi-compositional MWE (Table By locating translation of in the translation a decision we can deduce that it is semi-compositional. However, we cannot locate any of the component translations in the transof the Therefore, we conclude that it is non-compositional. Note that in this simple example, the match is word-level, but that due to the effects of morphophonology, the more likely situation is that the components don’t match exactly (as observe in the case of the service which motivates our use of string similarity measures which can capture partial matches. We consider the following string similarity measures to compare the translations. In each case, normalize the output value to the range where 1 indicates identical strings and 0 indicates completely different strings. We will indicate the of the MWE in a particular language and the translation of a given component in that the Persian words are transliterated into English for ease of understanding. MWE Components ... Panlex ... Translations Compare Translate Score 269 common substring (LCS): LCS measure finds the longest common substring between two strings. For example, the LCS between We calculate a normalized similarity value based on the length of the LCS as follows: (LEV1): Levenshtein distance calculates for the number of basic edit operations required to transpose one word into the other. Edits consist of single-letter insertions, deletions or substitutions. We normalize LEV1 as follows: Levenshtein with substitution penalty (LEV2): One well-documented feature of Levenshtein distance (Baldwin, 2009) is that substitutions are in fact the combination of an addition and a deletion, and as such can be considered to be two edits. Based on this observation, we experiment with a variant of LEV1 with this penalty applied for substitutions. Similarly to LEV1, we normalize as follows: + Waterman (SW) method is based on Needleman-Wunsch and was developed to locally-align two protein sequences (Smith and Waterman, 1981). It finds the optimal similar regions by maximizing the number of matches and minimizing the number of gaps necessary to align the two sequences. For example, the optimal local sequence for the two sequences below is in which indicates a gap: Needleman-Wunsch (NW) algorithm, was designed to align two sequences of amino-acids (Needleman and Wunsch, 1970). The algorithm looks for the sequence alignment which maximizes the similarity. As with the LEV score, NW minimizes edit distance, but also takes into account character-tocharacter similarity based on the relative distance between characters on the keyboard. We exclude this score, because it is highly similar to the LEV scores, and we did not obtain encouraging results using NW in our preliminary experiments.</abstract>
<note confidence="0.9565695">Seq1: Seq2:</note>
<abstract confidence="0.99060972972973">As the example shows, it looks for the longest common string but has an in-built mechanism for including gaps in the alignment (with penalty). This characteristic of SW might be helpful in our task, because there may be morphophonological variations between the MWE and component translations (as above in the service We normalize SW similarly to LCS: 6 Computational Model Given the scores calculated by the aforementioned string similarity measures between the translations for a given component word and the MWE, we need some way of combining scores across component First, we measure the compositionality of component within the MWE ..., ..., a string similarity measure, indicates that the calculation is based on translations in and a score combination function. Then, we compute the overall compositionality of MWE from Since we often have multiple translations for a given component word/MWE in Panlex, we exhaustively compute the similarity between each MWE translation and component translation, and use the highest as the result of If an instance does not have a direct/indirect translation in Panlex, we assign a default value, which is the mean of the highand lowest annotation score (2.5 for for Note that word order is not an issue in our method, as we calculate the similarity independently for each MWE component. In this research, we consider simple functions for as mean, median, product, min and max. that in all experiments we only combine scores given by the same string similarity measure.</abstract>
<note confidence="0.744365533333333">1 1 270 NC Language Frequency Family Czech 100 Slavic Norwegian 100 Germanic Portuguese 100 Romance Thai 99 Kam-thai French 95 Romance Chinese 94 Chinese Dutch 93 Germanic Romanian 91 Romance Hindi 67 Indic Russian 43 Slavic</note>
<abstract confidence="0.949302">2: The 10 best languages for LCS. selected to be the same as all situations, when we use mean for Here, following Reddy et al. (2011), we experimented with weighted mean: = (1 on 3-fold cross validation, we chose 0.7 Since we do not have judgements for the comof the full VPC in instead have separate judgements for the verb and we cannot use this dataset. Bannard et al. (2003) observed that nearly all of the verb-compositional instances were also annotated as particle-compositional by the annotators. In line this observation, we use on the verb) as the compositionality score for the full VPC. 7 Language Selection Our method is based on the translation of an MWE into many languages. In the first stage, we chose 54 languages for which relatively large corpora were The coverage, or the number of instances which have direct/indirect translations in Panlex, varies from one language to another. In preliminary experiments, we noticed that there is high correlation (about 0.50 for considered values of 0 to 1, incremented by 0.1. future work, we intend to look at the distribution of translations of the given MWE and its components in corpora for many languages. The present method does not rely on the availability of large corpora. VPC:verb Language Frequency Family</abstract>
<note confidence="0.892139590909091">Basque 100 Basque Lithuanian 100 Baltic Slovenian 100 Slavic Hebrew 99 Semitic Arabic 98 Semitic Czech 95 Slavic Slovak 92 Slavic Latin 79 Italic Tagalog 74 Austronesian Polish 44 Slavic Table 3: The 10 best languages for the verb component LCS. VPC:particle Language Frequency Family French 100 Romance Icelandic 100 Germanic Thai 100 Kam-thai Indonesian 92 Indonesian Spanish 90 Romance Tamil 87 Dravidian Turkish 83 Turkic Catalan 79 Romance</note>
<abstract confidence="0.92801325">Occitan 76 Romance Romanian 69 Romance Table 4: The 10 best languages for the particle compoof LCS. 0.80 for between the usefulness of a language and its translation coverage on MWEs. Therefore, we excluded languages with MWE translation coverage of less than 50%. Based on nested 10-fold cross validation in our experiments, we select the 10 most useful languages for each crossvalidation training partition, based on the Pearson correlation between the given scores in that language human The 10 best languages are selected based only on the training set for each fold. (The languages selected for each fold will later be used to predict the compositionality of the items in the testing portion for that fold.) In Tables 2, 3 that for VPCs, we calculate the compositionality of only the verb part, because we don’t have the human judgements for the whole VPC.</abstract>
<note confidence="0.9094499375">271 N1 N2 NC Mean LCS 0.541 0.525 0.405 0.481 0.396 0.431 0.200 0.263 0.637 0.649 0.523 0.577 LEV1 LEV2 Prod LCS 0.451 0.430 0.299 0.294 0.287 0.233 0.128 0.188 0.410 0.434 0.311 0.364 LEV1 LEV2 Median LCS 0.443 0.408 0.315 0.404 0.334 0.365 0.054 0.134 0.544 0.553 0.376 0.523 LEV1 LEV2 Min LCS 0.420 0.347 0.362 0.386 0.176 0.225 0.310 0.345 0.312 0.307 0.248 0.338 LEV1 LEV2 Max LCS 0.371 0.406 0.279 0.380 0.408 0.430 0.362 0.349 0.345 0.335 0.403 0.406 LEV1</note>
<abstract confidence="0.962708916666667">LEV2 5: Correlation on N1, N2 and NC, are the first component of the noun compound, its second component, and the noun compound itself, respectively. and 4, we show how often each language was selected in the top-10 languages over the combined folds of nested 10-fold cross validabased on The tables show that the selected languages were mostly consistent over the folds. The languages are a mixture of Romance, Germanic and languages from other families (based on Voegelin and Voegelin (1977)), with no standout language which performs well in all cases (indeed, no language occurs in all three tables). Additionally, there is nothing in common between the verb and the particle top-10 languages. 8 Results As mentioned before, we perform nested 10-fold cross-validation to select the 10 best languages on the training data for each fold. The selected lanfor a given fold are then used to compute our later results show that LCS and SW have higher results, we only show the best languages using LCS. These largely coincide with those for SW.</abstract>
<note confidence="0.897404333333333">Verb Particle Mean LCS 0.369 0.406 0.335 0.340 0.510 0.509 0.454 0.460 LEV1 LEV2 Prod LCS 0.315 0.339 0.322 0.342 0.316 0.299 0.280 0.284 LEV1 LEV2 Median LCS 0.316 0.352 0.295 0.309 0.409 0.423 0.387 0.368 LEV1 LEV2 Min LCS 0.262 0.329 0.307 0.310 0.210 0.251 0.278 0.281 LEV1 LEV2 Max LCS 0.141 0.268 0.145 0.170 0.288 0.299 0.450 0.398 LEV1</note>
<abstract confidence="0.906275194029851">LEV2 6: Correlation on based on the best-10 languages for the verb and particle individually NCs) for each instance in the test set for that fold. The scores are compared with human judgements using Pearson’s correlation. The results are shown in Tables 5 and 6. Among the five we experimented with for Mean performs much more consistently than the others. Median is less prone to noise, and therefore performs better than Prod, Max and Min, but it is still worse than Mean. For the most part, LCS and SW perform better than the other measures. There is little to separate these two methods, partly because they both look for a sequence of similar characters, unlike LEV1 and LEV2 which do not consider contiguity of match. The results support our hypothesis that using multiple target languages rather than one, results in a more accurate prediction of MWE compositionality. Our best result using the 10 selected languages on 0.649, as compared to the best singlelanguage correlation of 0.497 for Portuguese. On the best LCS result for the verb comis 0.406, as compared to the best single- 272 language correlation of 0.350 for Lithuanian. Reddy et al. (2011) reported a correlation of 0.714 Our best correlation is 0.649. Note that Reddy et al. (2011) base their method on identification of MWEs in a corpus, thus requiring MWEspecific identification. Given that this has been shown to be difficult for MWE types including English VPCs (McCarthy et al., 2003; Baldwin, 2005), the fact that our method is as competitive as this is highly encouraging, especially when you consider that it can equally be applied to different types of MWEs in other languages. Moreover, the computational processing required by methods based on distributional similarity is greater than our method, as it does not require processing a large corpus. Finally, we experimented with combining our with a reimplementation of the method of Reddy et al. (2011), based on simple averaging, as detailed in Table 7. The results are higher than both component methods and the statefor demonstrating the complementarity between our proposed method and methods based on distributional similarity. In Table 8, we compare our results with those of Bannard et al. (2003), who interpreted the dataset as a binary classification task. The dataset used in their study a subset of containing 40 VPCs, of which 29 (72%) were verb compositional and 23 (57%) were particle compositional. By applying a threshold of 0.5 over the output of our regression model, we binarize the VPCs into the compositional and non-compositional classes. According to the results shown in Table 6, LCS is a better similarity measure for this task. Our proposed method has higher results than the best results of Bannard et al. (2003), in part due to their reliance on VPC identification, and the low recall on the task, as reported in the paper. Our proposed method does not rely on a corpus or MWE identification. 9 Error Analysis analyse items in have a high difference (more than 2.5) between the human annotation and our scores (using LCS and Mean). The are mine which are non-compositional accordto After investigating their translations, we came to the conclusion that the first three MWEs have word-for-word translations in most languages. Hence, they disagree with our hypothesis that wordfor-word translation is a strong indicator of compositionality. The word-for-word translations might be because of the fact that they have both compositional and non-compositional senses, or because they are calques (loan translations). However, we have tried to avoid such problems with calques by using translations into several languages. tower state of mind that is discussed if it were a we noticed that we have a direct translation into 13 languages. Other languages have indirect translations. By checking the direct translations, we noticed that, in French, the MWE is to A noisy (wrong) of resulted in wrong indirect for tower an inflated estimate of compositionality. 10 Conclusion and Future Work In this study, we proposed a method to predict MWE compositionality based on the translation of the MWE and its component words into multiple languages. We used string similarity measures between the translations of the MWE and each of its components to predict the relative degree of compositionality. Among the four similarity measures that we experimented with, LCS and SW were found to be superior to edit distance-based methods. Our best results were found to be competitive with state-of-theart results using vector-based approaches, and were also shown to complement state-of-the-art methods. In future work, we are interested in investigating whether alternative ways of combining our proposed method with vector-based models can lead to further enhancements in results. These models could be especially effective when comparing translations which are roughly synonymous but not string-wise similar. Acknowledgments We would like to thank Timothy Baldwin, Su Nam Kim, and the anonymous reviewers for their valuable comments and suggestions. definition is from Wordnet 3.1. 273 + Reddy et al. SW 0.637 0.735 0.742 LEV1 0.523 0.724 LEV2 0.577 0.726 7: Correlation after combining Reddy et al.’s method and our method with Mean for The correlation using Reddy et al.’s method is 0.714. Method Precision Recall Accuracy Bannard et al. (2003) 0.608 0.666 0.636 0.600 0.862 0.718 0.774 0.693 8: Results for the classification task. is our method using Mean for NICTA is funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program.</abstract>
<note confidence="0.899003375">References Otavio Costa Acosta, Aline Villavicencio, and Viviane P Moreira. 2011. Identification and treatment of multiword expressions applied to information retrieval. In Proceedings of the ALC Workshop on MWEs: from Parsing and Generation to the Real World (MWE pages 101–109. Timothy Baldwin and Su Nam Kim. 2009. Multiword expressions. In Nitin Indurkhya and Fred J. Damerau, of Natural Language CRC Press, Boca Raton, USA, 2nd edition. Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and Dominic Widdows. 2003. An empirical model of expression decomposability. In Proceedings of the ACL-2003 Workshop on Multiword Expres- Analysis, Acquisition and pages 89–</note>
<address confidence="0.43938">96, Sapporo, Japan.</address>
<author confidence="0.48184">Timothy Baldwin</author>
<author confidence="0.48184">Jonathan Pool</author>
<author confidence="0.48184">Susan M Colow-</author>
<abstract confidence="0.903109266666667">ick. 2010. Panlex and lextract: Translating all words all languages of the world. In of the 23rd International Conference on Computational Linpages 37–40. Timothy Baldwin. 2005. The deep lexical acquisition of verb-particle constructions. Speech and Language, Special Issue on Multiword Expres- 19(4):398–414. Timothy Baldwin. 2009. The hare and the tortoise: and reliability in translation retrieval. 23(4):195–240. Colin Bannard, Timothy Baldwin, and Alex Lascarides. A statistical approach to the semantics of verb- In of the ACL 2003 workshop on Multiword expressions: analysis, acquisition and</abstract>
<note confidence="0.834915916666667">pages 65–72. James Bannard. 2006. Phrasal Lexicons Ph.D. thesis, University of Edinburgh. Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estima- 19(2):263–311. Helena Medeiros de Caseli, Carlos Ramisch, Maria das Volpe Nunes, and Aline Villavicencio. 2010. Alignment-based extraction of multiword expressions. Resources and 44(1):59–77. Afsaneh Fazly and Suzanne Stevenson. 2007. Dis-</note>
<abstract confidence="0.695102086956522">tinguishing subtypes of multiword expressions using statistical measures. In Proceedings of the ACL 2007 Workshop on A Broader Peron Multiword pages 9–16. Afsaneh Fazly, Paul Cook, and Suzanne Stevenson. 2009. Unsupervised type and token identification idiomatic expressions. 35(1):61–103. Su Nam Kim and Timothy Baldwin. 2007. Detecting compositionality of english verb-particle constructions semantic similarity. In of the 7th Meeting of the Pacific Association for Computational (PACLING pages 40–48. Dekang Lin. 1999. Automatic identification of nonphrases. In of the 37th annual meeting of the Association for Computational on Computational pages 317– 324. Diana McCarthy, Bill Keller, and John Carroll. 2003. Detecting a continuum of compositionality in phrasal In of the ACL 2003 workshop 274 on Multiword expressions: analysis, acquisition and</abstract>
<note confidence="0.918736352941176">pages 73–80. Diana McCarthy, Sriram Venkatapathy, and Aravind K Joshi. 2007. Detecting compositionality of verbobject combinations using selectional preferences. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPpages 369–379. I. Dan Melamed. 1997. Automatic discovery of noncompounds in parallel data. In Proceedings of the Fifth Workshop on Very Large Cor- EMNLP. Begona Villada Moir´on and J¨org Tiedemann. 2006. Identifying idiomatic expressions using automatic In of the EACL 2006 Workshop on Multi-wordexpressions in a multilingual pages 33–40.</note>
<author confidence="0.444837">Ba-</author>
<email confidence="0.542904">belnet:Buildingaverylargemultilingualsemantic</email>
<note confidence="0.933497695652174">In of the 48th Annual Meetof the Association for Computational pages 216–225, Uppsala, Sweden. Saul B Needleman and Christian D Wunsch. 1970. A general method applicable to the search for similarities the amino acid sequence of two proteins. of 48(3):443–453. Siva Reddy, Diana McCarthy, and Suresh Manandhar. 2011. An empirical study on compositionality in comnouns. In of pages 210– 218. Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copestake, and Dan Flickinger. 2002. Multiword ex- A pain in the neck for nlp. In Proceedings of the 3rd International Conference on Intelligent Text Processing Computational Linguistics (CICLingpages 189–206. Springer. Bahar Salehi, Narjes Askarian, and Afsaneh Fazly. 2012. Automatic identification of Persian light verb con- In of the 13th International Conference on Intelligent Text Processing Computa- Linguistics pages 201–210. Patrick Schone and Dan Jurafsky. 2001. Is knowledge-</note>
<abstract confidence="0.570663142857143">free induction of multiword unit dictionary headwords solved problem. In of the 6th Conference on Empirical Methods in Natural Language Pro- (EMNLP pages 100–108. TF Smith and MS Waterman. 1981. Identification of molecular subsequences. 147:195–197.</abstract>
<author confidence="0.7518075">Stephen Soderland</author>
<author confidence="0.7518075">Oren Etzioni</author>
<author confidence="0.7518075">Daniel S Weld</author>
<author confidence="0.7518075">Kobi Reiter</author>
<author confidence="0.7518075">Michael Skinner</author>
<author confidence="0.7518075">Marcus Sammer</author>
<author confidence="0.7518075">Jeff Bilmes</author>
<abstract confidence="0.885158909090909">et al. 2010. Panlingual lexical translation via probainference. 174(9):619– 637. Sriram Venkatapathy and Aravind K Joshi. 2006. Using information about multi-word expressions for the task. In of the Workshop on Multiword Expressions: Identifying and Exploiting pages 20–27. Charles Frederick Voegelin and Florence Marie Voegelin. and index of the world’s lanvolume 4. Elsevier Science Ltd.</abstract>
<intro confidence="0.405953">275</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Otavio Costa Acosta</author>
<author>Aline Villavicencio</author>
<author>Viviane P Moreira</author>
</authors>
<title>Identification and treatment of multiword expressions applied to information retrieval.</title>
<date>2011</date>
<booktitle>In Proceedings of the ALC Workshop on MWEs: from Parsing and Generation to the Real World (MWE</booktitle>
<pages>101--109</pages>
<contexts>
<context position="2199" citStr="Acosta et al., 2011" startWordPosition="324" endWordPosition="327">searchers, with research on binary compositional/non-compositional MWE clas1The example is taken from http://www. thefreedictionary.com sification (Lin, 1999; Baldwin et al., 2003), or a three-way compositional/semi-compositional/noncompositional distinction (Fazly and Stevenson, 2007). There has also been research to suggest that MWEs span the entire continuum from full compositionality to full non-compositionality (McCarthy et al., 2003; Reddy et al., 2011). Investigating the degree of MWE compositionality has been shown to have applications in information retrieval and machine translation (Acosta et al., 2011; Venkatapathy and Joshi, 2006). As an example of an information retrieval system, if we were looking for documents relating to rat race (meaning “an exhausting routine that leaves no time for relaxation”2), we would not be interested in documents on rodents. These results underline the need for methods for broad-coverage MWE compositionality prediction. In this research, we investigate the possibility of using an MWE’s translations in multiple languages to measure the degree of the MWE’s compositionality, and investigate how literal the semantics of each component is within the MWE. We use Pa</context>
</contexts>
<marker>Acosta, Villavicencio, Moreira, 2011</marker>
<rawString>Otavio Costa Acosta, Aline Villavicencio, and Viviane P Moreira. 2011. Identification and treatment of multiword expressions applied to information retrieval. In Proceedings of the ALC Workshop on MWEs: from Parsing and Generation to the Real World (MWE 2011), pages 101–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Su Nam Kim</author>
</authors>
<title>Multiword expressions.</title>
<date>2009</date>
<booktitle>In Nitin Indurkhya</booktitle>
<editor>and Fred J. Damerau, editors,</editor>
<publisher>CRC Press,</publisher>
<location>Boca Raton, USA,</location>
<note>2nd edition.</note>
<contexts>
<context position="1063" citStr="Baldwin and Kim, 2009" startWordPosition="147" endWordPosition="150">ee of compositionality of multiword expressions (MWEs). We compare the translations of an MWE with the translations of its components, using a range of different languages and string similarity measures. We demonstrate the effectiveness of the method on two types of English MWEs: noun compounds and verb particle constructions. The results show that our approach is competitive with or superior to state-of-the-art methods over standard datasets. 1 Compositionality of MWEs A multiword expression (MWE) is any combination of words with lexical, syntactic or semantic idiosyncrasy (Sag et al., 2002; Baldwin and Kim, 2009), in that the properties of the MWE are not predictable from the component words. For example, with ad hoc, the fact that neither ad nor hoc are standalone English words, makes ad hoc a lexicallyidiosyncratic MWE; with shoot the breeze, on the other hand, we have semantic idiosyncrasy, as the meaning of “to chat” in usages such as It was good to shoot the breeze with you1 cannot be predicted from the meanings of the component words shoot and breeze. Semantic idiosyncrasy has been of particular interest to NLP researchers, with research on binary compositional/non-compositional MWE clas1The exa</context>
</contexts>
<marker>Baldwin, Kim, 2009</marker>
<rawString>Timothy Baldwin and Su Nam Kim. 2009. Multiword expressions. In Nitin Indurkhya and Fred J. Damerau, editors, Handbook of Natural Language Processing. CRC Press, Boca Raton, USA, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Colin Bannard</author>
<author>Takaaki Tanaka</author>
<author>Dominic Widdows</author>
</authors>
<title>An empirical model of multiword expression decomposability.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL-2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment,</booktitle>
<pages>89--96</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1760" citStr="Baldwin et al., 2003" startWordPosition="262" endWordPosition="265">ords. For example, with ad hoc, the fact that neither ad nor hoc are standalone English words, makes ad hoc a lexicallyidiosyncratic MWE; with shoot the breeze, on the other hand, we have semantic idiosyncrasy, as the meaning of “to chat” in usages such as It was good to shoot the breeze with you1 cannot be predicted from the meanings of the component words shoot and breeze. Semantic idiosyncrasy has been of particular interest to NLP researchers, with research on binary compositional/non-compositional MWE clas1The example is taken from http://www. thefreedictionary.com sification (Lin, 1999; Baldwin et al., 2003), or a three-way compositional/semi-compositional/noncompositional distinction (Fazly and Stevenson, 2007). There has also been research to suggest that MWEs span the entire continuum from full compositionality to full non-compositionality (McCarthy et al., 2003; Reddy et al., 2011). Investigating the degree of MWE compositionality has been shown to have applications in information retrieval and machine translation (Acosta et al., 2011; Venkatapathy and Joshi, 2006). As an example of an information retrieval system, if we were looking for documents relating to rat race (meaning “an exhausting </context>
</contexts>
<marker>Baldwin, Bannard, Tanaka, Widdows, 2003</marker>
<rawString>Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and Dominic Widdows. 2003. An empirical model of multiword expression decomposability. In Proceedings of the ACL-2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment, pages 89– 96, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Jonathan Pool</author>
<author>Susan M Colowick</author>
</authors>
<title>Panlex and lextract: Translating all words of all languages of the world.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Demonstrations,</booktitle>
<pages>37--40</pages>
<contexts>
<context position="8805" citStr="Baldwin et al., 2010" startWordPosition="1359" endWordPosition="1362"> since they use statistical measures, they are not suitable for measuring the compositionality of MWEs with low frequency. And finally, most experiments have been carried out on English paired with other European languages, and it is not clear whether the results translate across to other language pairs. 3 Resources In this research, we use the translations of MWEs and their components to estimate the relative degree of compositionality of a MWE. There are several resources available to translate words into various languages such as Babelnet (Navigli and Ponzetto, 2010),4 Wiktionary,5 Panlex (Baldwin et al., 2010) and Google Translate.6 As we are ideally after broad coverage over multiple languages and MWEs/component words in a given language, we exclude Babelnet and Wiktionary from our current research. Babelnet covers only six languages at the time of writing this paper, and in Wiktionary, because it is constantly being updated, words and MWEs do not have translations into the same languages. This leaves translation resources such as Panlex and Google Translate. However, after manually analysing the two resources for a range of MWEs, we decided not to use Google Translate for two reasons: (1) we cons</context>
</contexts>
<marker>Baldwin, Pool, Colowick, 2010</marker>
<rawString>Timothy Baldwin, Jonathan Pool, and Susan M Colowick. 2010. Panlex and lextract: Translating all words of all languages of the world. In Proceedings of the 23rd International Conference on Computational Linguistics: Demonstrations, pages 37–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
</authors>
<title>The deep lexical acquisition of English verb-particle constructions.</title>
<date>2005</date>
<journal>Computer Speech and Language, Special Issue on Multiword Expressions,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="25770" citStr="Baldwin, 2005" startWordPosition="4155" endWordPosition="4156">ng the 10 selected languages on REDDY is 0.649, as compared to the best singlelanguage correlation of 0.497 for Portuguese. On BANNARD, the best LCS result for the verb component is 0.406, as compared to the best single272 language correlation of 0.350 for Lithuanian. Reddy et al. (2011) reported a correlation of 0.714 on REDDY. Our best correlation is 0.649. Note that Reddy et al. (2011) base their method on identification of MWEs in a corpus, thus requiring MWEspecific identification. Given that this has been shown to be difficult for MWE types including English VPCs (McCarthy et al., 2003; Baldwin, 2005), the fact that our method is as competitive as this is highly encouraging, especially when you consider that it can equally be applied to different types of MWEs in other languages. Moreover, the computational processing required by methods based on distributional similarity is greater than our method, as it does not require processing a large corpus. Finally, we experimented with combining our method (STRINGSIMMEAN) with a reimplementation of the method of Reddy et al. (2011), based on simple averaging, as detailed in Table 7. The results are higher than both component methods and the stateo</context>
</contexts>
<marker>Baldwin, 2005</marker>
<rawString>Timothy Baldwin. 2005. The deep lexical acquisition of English verb-particle constructions. Computer Speech and Language, Special Issue on Multiword Expressions, 19(4):398–414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
</authors>
<title>The hare and the tortoise: Speed and reliability in translation retrieval.</title>
<date>2009</date>
<journal>Machine Translation,</journal>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="16023" citStr="Baldwin, 2009" startWordPosition="2508" endWordPosition="2509">le, the LCS between ABABC and BABCAB is BABC. We calculate a normalized similarity value based on the length of the LCS as follows: LongestCommonString(MWEs, components) min(len(MWEs), len(components)) Levenshtein (LEV1): The Levenshtein distance calculates for the number of basic edit operations required to transpose one word into the other. Edits consist of single-letter insertions, deletions or substitutions. We normalize LEV1 as follows: LEV1(MWEs, components) max(len(MWEs), len(components)) Levenshtein with substitution penalty (LEV2): One well-documented feature of Levenshtein distance (Baldwin, 2009) is that substitutions are in fact the combination of an addition and a deletion, and as such can be considered to be two edits. Based on this observation, we experiment with a variant of LEV1 with this penalty applied for substitutions. Similarly to LEV1, we normalize as follows: LEV2(MWEs, components) len(MWEs) + len(components) Smith Waterman (SW) This method is based on the Needleman-Wunsch algorithm,8 and was developed to locally-align two protein sequences (Smith and Waterman, 1981). It finds the optimal similar regions by maximizing the number of matches and minimizing the number of gap</context>
</contexts>
<marker>Baldwin, 2009</marker>
<rawString>Timothy Baldwin. 2009. The hare and the tortoise: Speed and reliability in translation retrieval. Machine Translation, 23(4):195–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Timothy Baldwin</author>
<author>Alex Lascarides</author>
</authors>
<title>A statistical approach to the semantics of verbparticles.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL</booktitle>
<pages>65--72</pages>
<contexts>
<context position="5417" citStr="Bannard et al. (2003)" startWordPosition="821" endWordPosition="824">ion: each type of MWE has its own characteristics, and these characteristics differ from one language to another. Moreover, some MWEs (such as noun compounds) are not flexible syntactically, no matter whether they are compositional or non-compositional (Reddy et al., 2011). Much of the recent work on MWEs focuses on their semantic properties, measuring the semantic similarity between the MWE and its components using different resources, such as WordNet (Kim and Baldwin, 2007) or distributional similarity relative to a corpus (e.g. based on Latent Semantic Analysis: Schone and Jurafsky (2001), Bannard et al. (2003), Reddy et al. (2011)). The size of the corpus is important in methods based on distributional similarity. Unfortunately, however, large corpora are not available for all languages. Reddy et al. (2011) hypothesize that the number of common co-occurrences between a given MWE and its component words indicates the degree of compositionality of that MWE. First, the cooccurrences of a given MWE/word are considered as the values of a vector. They then measure the Cosine similarity between the vectors of the MWE and its components. Bannard et al. (2003) presented four methods to measure the compositi</context>
<context position="19909" citStr="Bannard et al. (2003)" startWordPosition="3151" endWordPosition="3155">-thai French 95 Romance Chinese 94 Chinese Dutch 93 Germanic Romanian 91 Romance Hindi 67 Indic Russian 43 Slavic Table 2: The 10 best languages for REDDY using LCS. was selected to be the same as f1 in all situations, except when we use mean for f1. Here, following Reddy et al. (2011), we experimented with weighted mean: f2(s1,s2) = αs1 + (1 − α)s2 Based on 3-fold cross validation, we chose α = 0.7 for REDDY.10 Since we do not have judgements for the compositionality of the full VPC in BANNARD (we instead have separate judgements for the verb and particle), we cannot use f2 for this dataset. Bannard et al. (2003) observed that nearly all of the verb-compositional instances were also annotated as particle-compositional by the annotators. In line with this observation, we use s1 (based on the verb) as the compositionality score for the full VPC. 7 Language Selection Our method is based on the translation of an MWE into many languages. In the first stage, we chose 54 languages for which relatively large corpora were available.11 The coverage, or the number of instances which have direct/indirect translations in Panlex, varies from one language to another. In preliminary experiments, we noticed that there</context>
<context position="26587" citStr="Bannard et al. (2003)" startWordPosition="4284" endWordPosition="4287"> computational processing required by methods based on distributional similarity is greater than our method, as it does not require processing a large corpus. Finally, we experimented with combining our method (STRINGSIMMEAN) with a reimplementation of the method of Reddy et al. (2011), based on simple averaging, as detailed in Table 7. The results are higher than both component methods and the stateof-the-art for REDDY, demonstrating the complementarity between our proposed method and methods based on distributional similarity. In Table 8, we compare our results (STRINGSIMMEAN) with those of Bannard et al. (2003), who interpreted the dataset as a binary classification task. The dataset used in their study is a subset of BANNARD, containing 40 VPCs, of which 29 (72%) were verb compositional and 23 (57%) were particle compositional. By applying a threshold of 0.5 over the output of our regression model, we binarize the VPCs into the compositional and non-compositional classes. According to the results shown in Table 6, LCS is a better similarity measure for this task. Our proposed method has higher results than the best results of Bannard et al. (2003), in part due to their reliance on VPC identificatio</context>
</contexts>
<marker>Bannard, Baldwin, Lascarides, 2003</marker>
<rawString>Colin Bannard, Timothy Baldwin, and Alex Lascarides. 2003. A statistical approach to the semantics of verbparticles. In Proceedings of the ACL 2003 workshop on Multiword expressions: analysis, acquisition and treatment-Volume 18, pages 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin James Bannard</author>
</authors>
<title>Acquiring Phrasal Lexicons from Corpora.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="11652" citStr="Bannard, 2006" startWordPosition="1811" endWordPosition="1812">90 English (binary) noun compounds (NCs), where the overall NC and each component word has been annotated for compositionality on a scale from 0 (non-compositional) to 5 (compositional). In order to avoid issues with polysemy, the annotators were presented with each NC in a sentential context. The authors tried to achieve a balance of compositional and noncompositional NCs: based on a threshold of 2.5, the dataset consists of 43 (48%) compositional NCs, 46 (51%) NCs with a compositional usage of the first component, and 54 (60%) NCs with a compositional usage of the second component. BANNARD (Bannard, 2006): 160 English verb particle constructions (VPCs) were annotated for compositionality relative to each of the two component words (the verb and the particle). Each annotator was asked to annotate each of the verb and particle as yes, no or don’t know. Based on the majority annotation, among the 160 VPCs, 122 (76%) are verb-compositional and 76 (48%) are particlecompositional. We compute the proportion of yes tags to get the compositionality score. This dataset, unlike REDDY, does not include annotations for the compositionality of the whole VPC, and is also less balanced, containing more VPCs w</context>
</contexts>
<marker>Bannard, 2006</marker>
<rawString>Colin James Bannard. 2006. Acquiring Phrasal Lexicons from Corpora. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="7955" citStr="Brown et al., 1993" startWordPosition="1227" endWordPosition="1230">t al. (2012) tried to solve this problem with high frequency MWEs by using word alignment in both directions.3 They computed backward and forward entropy to try to remedy the problem with especially high-frequency phrases. However, their assumptions were not easily generalisable across languages, e.g., they assume that the relative frequency of a specific type of MWE (light verb constructions) in Persian is much greater than in English. Although methods using bilingual corpora are intuitively appealing, they have a number of drawbacks. The first and the most important problem 3The IBM models (Brown et al., 1993), e.g., are not bidirectional, which means that the alignments are affected by the alignment direction. 267 is data: they need large-scale parallel bilingual corpora, which are available for relatively few language pairs. Second, since they use statistical measures, they are not suitable for measuring the compositionality of MWEs with low frequency. And finally, most experiments have been carried out on English paired with other European languages, and it is not clear whether the results translate across to other language pairs. 3 Resources In this research, we use the translations of MWEs and</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helena Medeiros de Caseli</author>
</authors>
<title>Carlos Ramisch, Maria das Grac¸as Volpe Nunes, and Aline Villavicencio.</title>
<date>2010</date>
<journal>Language Resources and Evaluation,</journal>
<volume>44</volume>
<issue>1</issue>
<marker>de Caseli, 2010</marker>
<rawString>Helena Medeiros de Caseli, Carlos Ramisch, Maria das Grac¸as Volpe Nunes, and Aline Villavicencio. 2010. Alignment-based extraction of multiword expressions. Language Resources and Evaluation, 44(1):59–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Afsaneh Fazly</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Distinguishing subtypes of multiword expressions using linguistically-motivated statistical measures.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL 2007 Workshop on A Broader Perspective on Multiword Expressions,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="1866" citStr="Fazly and Stevenson, 2007" startWordPosition="272" endWordPosition="275">ad hoc a lexicallyidiosyncratic MWE; with shoot the breeze, on the other hand, we have semantic idiosyncrasy, as the meaning of “to chat” in usages such as It was good to shoot the breeze with you1 cannot be predicted from the meanings of the component words shoot and breeze. Semantic idiosyncrasy has been of particular interest to NLP researchers, with research on binary compositional/non-compositional MWE clas1The example is taken from http://www. thefreedictionary.com sification (Lin, 1999; Baldwin et al., 2003), or a three-way compositional/semi-compositional/noncompositional distinction (Fazly and Stevenson, 2007). There has also been research to suggest that MWEs span the entire continuum from full compositionality to full non-compositionality (McCarthy et al., 2003; Reddy et al., 2011). Investigating the degree of MWE compositionality has been shown to have applications in information retrieval and machine translation (Acosta et al., 2011; Venkatapathy and Joshi, 2006). As an example of an information retrieval system, if we were looking for documents relating to rat race (meaning “an exhausting routine that leaves no time for relaxation”2), we would not be interested in documents on rodents. These r</context>
</contexts>
<marker>Fazly, Stevenson, 2007</marker>
<rawString>Afsaneh Fazly and Suzanne Stevenson. 2007. Distinguishing subtypes of multiword expressions using linguistically-motivated statistical measures. In Proceedings of the ACL 2007 Workshop on A Broader Perspective on Multiword Expressions, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Afsaneh Fazly</author>
<author>Paul Cook</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Unsupervised type and token identification of idiomatic expressions.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>1</issue>
<contexts>
<context position="3190" citStr="Fazly et al., 2009" startWordPosition="485" endWordPosition="488">h, we investigate the possibility of using an MWE’s translations in multiple languages to measure the degree of the MWE’s compositionality, and investigate how literal the semantics of each component is within the MWE. We use Panlex to translate the MWE and its components, and compare the translations of the MWE with the translations of its components using string similarity measures. The greater the string similarity, the more compositional the MWE is. Whereas past research on MWE compositionality has tended to be tailored to a specific MWE type (McCarthy et al., 2007; Kim and Baldwin, 2007; Fazly et al., 2009), our method is applicable to any MWE type in any language. Our experiments 2This definition is from WordNet 3.1. 266 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 266–275, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics over two English MWE types demonstrate that our method is competitive with state-of-the-art methods over standard datasets. 2 Related Work Most previous work on measuring MWE compositionality makes use of lexical, syntactic or semantic propertie</context>
<context position="4499" citStr="Fazly et al., 2009" startWordPosition="679" endWordPosition="682">distribution of non-compositional MWEs (e.g. shoot the breeze) differs significantly from the distribution of expressions formed by substituting one of the components with a semantically similar word (e.g. shoot the wind). Unfortunately, the method tends to fall down in cases of high statistical idiosyncrasy (or “institutionalization”): consider frying pan which is compositional but distributionally very different to phrases produced through wordsubstitution such as sauteing pan or frying plate. Some research has investigated the syntactic properties of MWEs, to detect their compositionality (Fazly et al., 2009; McCarthy et al., 2007). The assumption behind these methods is that noncompositional MWEs are more syntactically fixed than compositional MWEs. For example, make a decision can be passivised, but shoot the breeze cannot. One serious problem with syntax-based methods is their lack of generalization: each type of MWE has its own characteristics, and these characteristics differ from one language to another. Moreover, some MWEs (such as noun compounds) are not flexible syntactically, no matter whether they are compositional or non-compositional (Reddy et al., 2011). Much of the recent work on M</context>
</contexts>
<marker>Fazly, Cook, Stevenson, 2009</marker>
<rawString>Afsaneh Fazly, Paul Cook, and Suzanne Stevenson. 2009. Unsupervised type and token identification of idiomatic expressions. Computational Linguistics, 35(1):61–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Timothy Baldwin</author>
</authors>
<title>Detecting compositionality of english verb-particle constructions using semantic similarity.</title>
<date>2007</date>
<booktitle>In Proceedings of the 7th Meeting of the Pacific Association for Computational Linguistics (PACLING</booktitle>
<pages>40--48</pages>
<contexts>
<context position="3169" citStr="Kim and Baldwin, 2007" startWordPosition="481" endWordPosition="484">iction. In this research, we investigate the possibility of using an MWE’s translations in multiple languages to measure the degree of the MWE’s compositionality, and investigate how literal the semantics of each component is within the MWE. We use Panlex to translate the MWE and its components, and compare the translations of the MWE with the translations of its components using string similarity measures. The greater the string similarity, the more compositional the MWE is. Whereas past research on MWE compositionality has tended to be tailored to a specific MWE type (McCarthy et al., 2007; Kim and Baldwin, 2007; Fazly et al., 2009), our method is applicable to any MWE type in any language. Our experiments 2This definition is from WordNet 3.1. 266 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 266–275, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics over two English MWE types demonstrate that our method is competitive with state-of-the-art methods over standard datasets. 2 Related Work Most previous work on measuring MWE compositionality makes use of lexical, syntactic </context>
<context position="5276" citStr="Kim and Baldwin, 2007" startWordPosition="800" endWordPosition="803">le, make a decision can be passivised, but shoot the breeze cannot. One serious problem with syntax-based methods is their lack of generalization: each type of MWE has its own characteristics, and these characteristics differ from one language to another. Moreover, some MWEs (such as noun compounds) are not flexible syntactically, no matter whether they are compositional or non-compositional (Reddy et al., 2011). Much of the recent work on MWEs focuses on their semantic properties, measuring the semantic similarity between the MWE and its components using different resources, such as WordNet (Kim and Baldwin, 2007) or distributional similarity relative to a corpus (e.g. based on Latent Semantic Analysis: Schone and Jurafsky (2001), Bannard et al. (2003), Reddy et al. (2011)). The size of the corpus is important in methods based on distributional similarity. Unfortunately, however, large corpora are not available for all languages. Reddy et al. (2011) hypothesize that the number of common co-occurrences between a given MWE and its component words indicates the degree of compositionality of that MWE. First, the cooccurrences of a given MWE/word are considered as the values of a vector. They then measure t</context>
</contexts>
<marker>Kim, Baldwin, 2007</marker>
<rawString>Su Nam Kim and Timothy Baldwin. 2007. Detecting compositionality of english verb-particle constructions using semantic similarity. In Proceedings of the 7th Meeting of the Pacific Association for Computational Linguistics (PACLING 2007), pages 40–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic identification of noncompositional phrases.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics,</booktitle>
<pages>317--324</pages>
<contexts>
<context position="1737" citStr="Lin, 1999" startWordPosition="260" endWordPosition="261">component words. For example, with ad hoc, the fact that neither ad nor hoc are standalone English words, makes ad hoc a lexicallyidiosyncratic MWE; with shoot the breeze, on the other hand, we have semantic idiosyncrasy, as the meaning of “to chat” in usages such as It was good to shoot the breeze with you1 cannot be predicted from the meanings of the component words shoot and breeze. Semantic idiosyncrasy has been of particular interest to NLP researchers, with research on binary compositional/non-compositional MWE clas1The example is taken from http://www. thefreedictionary.com sification (Lin, 1999; Baldwin et al., 2003), or a three-way compositional/semi-compositional/noncompositional distinction (Fazly and Stevenson, 2007). There has also been research to suggest that MWEs span the entire continuum from full compositionality to full non-compositionality (McCarthy et al., 2003; Reddy et al., 2011). Investigating the degree of MWE compositionality has been shown to have applications in information retrieval and machine translation (Acosta et al., 2011; Venkatapathy and Joshi, 2006). As an example of an information retrieval system, if we were looking for documents relating to rat race (</context>
<context position="3858" citStr="Lin (1999)" startWordPosition="587" endWordPosition="588">. Our experiments 2This definition is from WordNet 3.1. 266 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 266–275, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics over two English MWE types demonstrate that our method is competitive with state-of-the-art methods over standard datasets. 2 Related Work Most previous work on measuring MWE compositionality makes use of lexical, syntactic or semantic properties of the MWE. One early study on MWE compositionality was Lin (1999), who claimed that the distribution of non-compositional MWEs (e.g. shoot the breeze) differs significantly from the distribution of expressions formed by substituting one of the components with a semantically similar word (e.g. shoot the wind). Unfortunately, the method tends to fall down in cases of high statistical idiosyncrasy (or “institutionalization”): consider frying pan which is compositional but distributionally very different to phrases produced through wordsubstitution such as sauteing pan or frying plate. Some research has investigated the syntactic properties of MWEs, to detect t</context>
<context position="6140" citStr="Lin (1999)" startWordPosition="941" endWordPosition="942">unately, however, large corpora are not available for all languages. Reddy et al. (2011) hypothesize that the number of common co-occurrences between a given MWE and its component words indicates the degree of compositionality of that MWE. First, the cooccurrences of a given MWE/word are considered as the values of a vector. They then measure the Cosine similarity between the vectors of the MWE and its components. Bannard et al. (2003) presented four methods to measure the compositionality of English verb particle constructions. Their best result is based on the previously-discussed method of Lin (1999) for measuring compositionality, but uses a more-general distributional similarity model to identify synonyms. Recently, a few studies have investigated using parallel corpora to detect the degree of compositionality (Melamed, 1997; Moir´on and Tiedemann, 2006; de Caseli et al., 2010; Salehi et al., 2012). The general approach is to word-align the source and target language sentences and analyse alignment patterns for MWEs (e.g. if the MWE is always aligned as a single “phrase”, then it is a strong indicator of non-compositionality). de Caseli et al. (2010) consider non-compositional MWEs to b</context>
</contexts>
<marker>Lin, 1999</marker>
<rawString>Dekang Lin. 1999. Automatic identification of noncompositional phrases. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 317– 324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Bill Keller</author>
<author>John Carroll</author>
</authors>
<title>Detecting a continuum of compositionality in phrasal verbs.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL</booktitle>
<pages>73--80</pages>
<contexts>
<context position="2022" citStr="McCarthy et al., 2003" startWordPosition="296" endWordPosition="299">s good to shoot the breeze with you1 cannot be predicted from the meanings of the component words shoot and breeze. Semantic idiosyncrasy has been of particular interest to NLP researchers, with research on binary compositional/non-compositional MWE clas1The example is taken from http://www. thefreedictionary.com sification (Lin, 1999; Baldwin et al., 2003), or a three-way compositional/semi-compositional/noncompositional distinction (Fazly and Stevenson, 2007). There has also been research to suggest that MWEs span the entire continuum from full compositionality to full non-compositionality (McCarthy et al., 2003; Reddy et al., 2011). Investigating the degree of MWE compositionality has been shown to have applications in information retrieval and machine translation (Acosta et al., 2011; Venkatapathy and Joshi, 2006). As an example of an information retrieval system, if we were looking for documents relating to rat race (meaning “an exhausting routine that leaves no time for relaxation”2), we would not be interested in documents on rodents. These results underline the need for methods for broad-coverage MWE compositionality prediction. In this research, we investigate the possibility of using an MWE’s</context>
<context position="25754" citStr="McCarthy et al., 2003" startWordPosition="4151" endWordPosition="4154">ty. Our best result using the 10 selected languages on REDDY is 0.649, as compared to the best singlelanguage correlation of 0.497 for Portuguese. On BANNARD, the best LCS result for the verb component is 0.406, as compared to the best single272 language correlation of 0.350 for Lithuanian. Reddy et al. (2011) reported a correlation of 0.714 on REDDY. Our best correlation is 0.649. Note that Reddy et al. (2011) base their method on identification of MWEs in a corpus, thus requiring MWEspecific identification. Given that this has been shown to be difficult for MWE types including English VPCs (McCarthy et al., 2003; Baldwin, 2005), the fact that our method is as competitive as this is highly encouraging, especially when you consider that it can equally be applied to different types of MWEs in other languages. Moreover, the computational processing required by methods based on distributional similarity is greater than our method, as it does not require processing a large corpus. Finally, we experimented with combining our method (STRINGSIMMEAN) with a reimplementation of the method of Reddy et al. (2011), based on simple averaging, as detailed in Table 7. The results are higher than both component method</context>
</contexts>
<marker>McCarthy, Keller, Carroll, 2003</marker>
<rawString>Diana McCarthy, Bill Keller, and John Carroll. 2003. Detecting a continuum of compositionality in phrasal verbs. In Proceedings of the ACL 2003 workshop on Multiword expressions: analysis, acquisition and treatment-Volume 18, pages 73–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Sriram Venkatapathy</author>
<author>Aravind K Joshi</author>
</authors>
<title>Detecting compositionality of verbobject combinations using selectional preferences.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>369--379</pages>
<contexts>
<context position="3146" citStr="McCarthy et al., 2007" startWordPosition="477" endWordPosition="480">E compositionality prediction. In this research, we investigate the possibility of using an MWE’s translations in multiple languages to measure the degree of the MWE’s compositionality, and investigate how literal the semantics of each component is within the MWE. We use Panlex to translate the MWE and its components, and compare the translations of the MWE with the translations of its components using string similarity measures. The greater the string similarity, the more compositional the MWE is. Whereas past research on MWE compositionality has tended to be tailored to a specific MWE type (McCarthy et al., 2007; Kim and Baldwin, 2007; Fazly et al., 2009), our method is applicable to any MWE type in any language. Our experiments 2This definition is from WordNet 3.1. 266 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 266–275, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics over two English MWE types demonstrate that our method is competitive with state-of-the-art methods over standard datasets. 2 Related Work Most previous work on measuring MWE compositionality makes use</context>
<context position="4523" citStr="McCarthy et al., 2007" startWordPosition="683" endWordPosition="686">compositional MWEs (e.g. shoot the breeze) differs significantly from the distribution of expressions formed by substituting one of the components with a semantically similar word (e.g. shoot the wind). Unfortunately, the method tends to fall down in cases of high statistical idiosyncrasy (or “institutionalization”): consider frying pan which is compositional but distributionally very different to phrases produced through wordsubstitution such as sauteing pan or frying plate. Some research has investigated the syntactic properties of MWEs, to detect their compositionality (Fazly et al., 2009; McCarthy et al., 2007). The assumption behind these methods is that noncompositional MWEs are more syntactically fixed than compositional MWEs. For example, make a decision can be passivised, but shoot the breeze cannot. One serious problem with syntax-based methods is their lack of generalization: each type of MWE has its own characteristics, and these characteristics differ from one language to another. Moreover, some MWEs (such as noun compounds) are not flexible syntactically, no matter whether they are compositional or non-compositional (Reddy et al., 2011). Much of the recent work on MWEs focuses on their sem</context>
</contexts>
<marker>McCarthy, Venkatapathy, Joshi, 2007</marker>
<rawString>Diana McCarthy, Sriram Venkatapathy, and Aravind K Joshi. 2007. Detecting compositionality of verbobject combinations using selectional preferences. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 369–379.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Automatic discovery of noncompositional compounds in parallel data.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Workshop on Very Large Corpora. EMNLP.</booktitle>
<contexts>
<context position="6371" citStr="Melamed, 1997" startWordPosition="973" endWordPosition="974">f that MWE. First, the cooccurrences of a given MWE/word are considered as the values of a vector. They then measure the Cosine similarity between the vectors of the MWE and its components. Bannard et al. (2003) presented four methods to measure the compositionality of English verb particle constructions. Their best result is based on the previously-discussed method of Lin (1999) for measuring compositionality, but uses a more-general distributional similarity model to identify synonyms. Recently, a few studies have investigated using parallel corpora to detect the degree of compositionality (Melamed, 1997; Moir´on and Tiedemann, 2006; de Caseli et al., 2010; Salehi et al., 2012). The general approach is to word-align the source and target language sentences and analyse alignment patterns for MWEs (e.g. if the MWE is always aligned as a single “phrase”, then it is a strong indicator of non-compositionality). de Caseli et al. (2010) consider non-compositional MWEs to be those candidates that align to the same target language unit, without decomposition into word alignments. Melamed (1997) suggests using mutual information to investigate how well the translation model predicts the distribution of</context>
</contexts>
<marker>Melamed, 1997</marker>
<rawString>I. Dan Melamed. 1997. Automatic discovery of noncompositional compounds in parallel data. In Proceedings of the Fifth Workshop on Very Large Corpora. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Begona Villada Moir´on</author>
<author>J¨org Tiedemann</author>
</authors>
<title>Identifying idiomatic expressions using automatic word-alignment.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL 2006 Workshop on Multi-wordexpressions in a multilingual context,</booktitle>
<pages>33--40</pages>
<marker>Moir´on, Tiedemann, 2006</marker>
<rawString>Begona Villada Moir´on and J¨org Tiedemann. 2006. Identifying idiomatic expressions using automatic word-alignment. In Proceedings of the EACL 2006 Workshop on Multi-wordexpressions in a multilingual context, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>Babelnet: Building a very large multilingual semantic network.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>216--225</pages>
<location>Uppsala,</location>
<contexts>
<context position="8760" citStr="Navigli and Ponzetto, 2010" startWordPosition="1353" endWordPosition="1356">vailable for relatively few language pairs. Second, since they use statistical measures, they are not suitable for measuring the compositionality of MWEs with low frequency. And finally, most experiments have been carried out on English paired with other European languages, and it is not clear whether the results translate across to other language pairs. 3 Resources In this research, we use the translations of MWEs and their components to estimate the relative degree of compositionality of a MWE. There are several resources available to translate words into various languages such as Babelnet (Navigli and Ponzetto, 2010),4 Wiktionary,5 Panlex (Baldwin et al., 2010) and Google Translate.6 As we are ideally after broad coverage over multiple languages and MWEs/component words in a given language, we exclude Babelnet and Wiktionary from our current research. Babelnet covers only six languages at the time of writing this paper, and in Wiktionary, because it is constantly being updated, words and MWEs do not have translations into the same languages. This leaves translation resources such as Panlex and Google Translate. However, after manually analysing the two resources for a range of MWEs, we decided not to use </context>
</contexts>
<marker>Navigli, Ponzetto, 2010</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2010. Babelnet: Building a very large multilingual semantic network. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 216–225, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saul B Needleman</author>
<author>Christian D Wunsch</author>
</authors>
<title>A general method applicable to the search for similarities in the amino acid sequence of two proteins.</title>
<date>1970</date>
<journal>Journal of molecular biology,</journal>
<volume>48</volume>
<issue>3</issue>
<contexts>
<context position="16891" citStr="Needleman and Wunsch, 1970" startWordPosition="2644" endWordPosition="2647">ilarly to LEV1, we normalize as follows: LEV2(MWEs, components) len(MWEs) + len(components) Smith Waterman (SW) This method is based on the Needleman-Wunsch algorithm,8 and was developed to locally-align two protein sequences (Smith and Waterman, 1981). It finds the optimal similar regions by maximizing the number of matches and minimizing the number of gaps necessary to align the two sequences. For example, the optimal local sequence for the two sequences below is AT−−ATCC, in which “-” indicates a gap: 8The Needleman-Wunsch (NW) algorithm, was designed to align two sequences of amino-acids (Needleman and Wunsch, 1970). The algorithm looks for the sequence alignment which maximizes the similarity. As with the LEV score, NW minimizes edit distance, but also takes into account character-tocharacter similarity based on the relative distance between characters on the keyboard. We exclude this score, because it is highly similar to the LEV scores, and we did not obtain encouraging results using NW in our preliminary experiments. Seq1: ATGCATCCCATGAC Seq2: TCTATATCCGT As the example shows, it looks for the longest common string but has an in-built mechanism for including gaps in the alignment (with penalty). This</context>
</contexts>
<marker>Needleman, Wunsch, 1970</marker>
<rawString>Saul B Needleman and Christian D Wunsch. 1970. A general method applicable to the search for similarities in the amino acid sequence of two proteins. Journal of molecular biology, 48(3):443–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siva Reddy</author>
<author>Diana McCarthy</author>
<author>Suresh Manandhar</author>
</authors>
<title>An empirical study on compositionality in compound nouns.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<pages>210--218</pages>
<contexts>
<context position="2043" citStr="Reddy et al., 2011" startWordPosition="300" endWordPosition="303">eze with you1 cannot be predicted from the meanings of the component words shoot and breeze. Semantic idiosyncrasy has been of particular interest to NLP researchers, with research on binary compositional/non-compositional MWE clas1The example is taken from http://www. thefreedictionary.com sification (Lin, 1999; Baldwin et al., 2003), or a three-way compositional/semi-compositional/noncompositional distinction (Fazly and Stevenson, 2007). There has also been research to suggest that MWEs span the entire continuum from full compositionality to full non-compositionality (McCarthy et al., 2003; Reddy et al., 2011). Investigating the degree of MWE compositionality has been shown to have applications in information retrieval and machine translation (Acosta et al., 2011; Venkatapathy and Joshi, 2006). As an example of an information retrieval system, if we were looking for documents relating to rat race (meaning “an exhausting routine that leaves no time for relaxation”2), we would not be interested in documents on rodents. These results underline the need for methods for broad-coverage MWE compositionality prediction. In this research, we investigate the possibility of using an MWE’s translations in mult</context>
<context position="5069" citStr="Reddy et al., 2011" startWordPosition="767" endWordPosition="770">detect their compositionality (Fazly et al., 2009; McCarthy et al., 2007). The assumption behind these methods is that noncompositional MWEs are more syntactically fixed than compositional MWEs. For example, make a decision can be passivised, but shoot the breeze cannot. One serious problem with syntax-based methods is their lack of generalization: each type of MWE has its own characteristics, and these characteristics differ from one language to another. Moreover, some MWEs (such as noun compounds) are not flexible syntactically, no matter whether they are compositional or non-compositional (Reddy et al., 2011). Much of the recent work on MWEs focuses on their semantic properties, measuring the semantic similarity between the MWE and its components using different resources, such as WordNet (Kim and Baldwin, 2007) or distributional similarity relative to a corpus (e.g. based on Latent Semantic Analysis: Schone and Jurafsky (2001), Bannard et al. (2003), Reddy et al. (2011)). The size of the corpus is important in methods based on distributional similarity. Unfortunately, however, large corpora are not available for all languages. Reddy et al. (2011) hypothesize that the number of common co-occurrenc</context>
<context position="11036" citStr="Reddy et al., 2011" startWordPosition="1709" endWordPosition="1712">were extracted from electronic dictionaries. If there is no direct translation for a word/MWE in the database, we can translate indirectly via one or more pivot languages (indirect translation: Soderland et al. (2010)). For example, English ivory tower has direct translations in only 13 languages in Panlex, including French (tour d’ivoire) but not Esperanto. There is, however, a translation of tour d’ivoire into Esperanto (ebura turo), allowing us to infer an indirect translation between ivory tower and ebura turo. 4 Dataset We evaluate our method over two datasets, as described below. REDDY (Reddy et al., 2011): 90 English (binary) noun compounds (NCs), where the overall NC and each component word has been annotated for compositionality on a scale from 0 (non-compositional) to 5 (compositional). In order to avoid issues with polysemy, the annotators were presented with each NC in a sentential context. The authors tried to achieve a balance of compositional and noncompositional NCs: based on a threshold of 2.5, the dataset consists of 43 (48%) compositional NCs, 46 (51%) NCs with a compositional usage of the first component, and 54 (60%) NCs with a compositional usage of the second component. BANNARD</context>
<context position="19574" citStr="Reddy et al. (2011)" startWordPosition="3089" endWordPosition="3092">or each MWE component. In this research, we consider simple functions for f1 such as mean, median, product, min and max. f2 9Note that in all experiments we only combine scores given by the same string similarity measure. 1 1 270 NC Language Frequency Family Czech 100 Slavic Norwegian 100 Germanic Portuguese 100 Romance Thai 99 Kam-thai French 95 Romance Chinese 94 Chinese Dutch 93 Germanic Romanian 91 Romance Hindi 67 Indic Russian 43 Slavic Table 2: The 10 best languages for REDDY using LCS. was selected to be the same as f1 in all situations, except when we use mean for f1. Here, following Reddy et al. (2011), we experimented with weighted mean: f2(s1,s2) = αs1 + (1 − α)s2 Based on 3-fold cross validation, we chose α = 0.7 for REDDY.10 Since we do not have judgements for the compositionality of the full VPC in BANNARD (we instead have separate judgements for the verb and particle), we cannot use f2 for this dataset. Bannard et al. (2003) observed that nearly all of the verb-compositional instances were also annotated as particle-compositional by the annotators. In line with this observation, we use s1 (based on the verb) as the compositionality score for the full VPC. 7 Language Selection Our meth</context>
<context position="25444" citStr="Reddy et al. (2011)" startWordPosition="4097" endWordPosition="4100">separate these two methods, partly because they both look for a sequence of similar characters, unlike LEV1 and LEV2 which do not consider contiguity of match. The results support our hypothesis that using multiple target languages rather than one, results in a more accurate prediction of MWE compositionality. Our best result using the 10 selected languages on REDDY is 0.649, as compared to the best singlelanguage correlation of 0.497 for Portuguese. On BANNARD, the best LCS result for the verb component is 0.406, as compared to the best single272 language correlation of 0.350 for Lithuanian. Reddy et al. (2011) reported a correlation of 0.714 on REDDY. Our best correlation is 0.649. Note that Reddy et al. (2011) base their method on identification of MWEs in a corpus, thus requiring MWEspecific identification. Given that this has been shown to be difficult for MWE types including English VPCs (McCarthy et al., 2003; Baldwin, 2005), the fact that our method is as competitive as this is highly encouraging, especially when you consider that it can equally be applied to different types of MWEs in other languages. Moreover, the computational processing required by methods based on distributional similari</context>
</contexts>
<marker>Reddy, McCarthy, Manandhar, 2011</marker>
<rawString>Siva Reddy, Diana McCarthy, and Suresh Manandhar. 2011. An empirical study on compositionality in compound nouns. In Proceedings of IJCNLP, pages 210– 218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Sag</author>
<author>Timothy Baldwin</author>
<author>Francis Bond</author>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
</authors>
<title>Multiword expressions: A pain in the neck for nlp.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Intelligent Text Processing Computational Linguistics (CICLing2002),</booktitle>
<pages>189--206</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1039" citStr="Sag et al., 2002" startWordPosition="143" endWordPosition="146">redicting the degree of compositionality of multiword expressions (MWEs). We compare the translations of an MWE with the translations of its components, using a range of different languages and string similarity measures. We demonstrate the effectiveness of the method on two types of English MWEs: noun compounds and verb particle constructions. The results show that our approach is competitive with or superior to state-of-the-art methods over standard datasets. 1 Compositionality of MWEs A multiword expression (MWE) is any combination of words with lexical, syntactic or semantic idiosyncrasy (Sag et al., 2002; Baldwin and Kim, 2009), in that the properties of the MWE are not predictable from the component words. For example, with ad hoc, the fact that neither ad nor hoc are standalone English words, makes ad hoc a lexicallyidiosyncratic MWE; with shoot the breeze, on the other hand, we have semantic idiosyncrasy, as the meaning of “to chat” in usages such as It was good to shoot the breeze with you1 cannot be predicted from the meanings of the component words shoot and breeze. Semantic idiosyncrasy has been of particular interest to NLP researchers, with research on binary compositional/non-compos</context>
</contexts>
<marker>Sag, Baldwin, Bond, Copestake, Flickinger, 2002</marker>
<rawString>Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copestake, and Dan Flickinger. 2002. Multiword expressions: A pain in the neck for nlp. In Proceedings of the 3rd International Conference on Intelligent Text Processing Computational Linguistics (CICLing2002), pages 189–206. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bahar Salehi</author>
<author>Narjes Askarian</author>
<author>Afsaneh Fazly</author>
</authors>
<title>Automatic identification of Persian light verb constructions.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th International Conference on Intelligent Text Processing Computational Linguistics (CICLing-2012),</booktitle>
<pages>201--210</pages>
<contexts>
<context position="6446" citStr="Salehi et al., 2012" startWordPosition="984" endWordPosition="987">red as the values of a vector. They then measure the Cosine similarity between the vectors of the MWE and its components. Bannard et al. (2003) presented four methods to measure the compositionality of English verb particle constructions. Their best result is based on the previously-discussed method of Lin (1999) for measuring compositionality, but uses a more-general distributional similarity model to identify synonyms. Recently, a few studies have investigated using parallel corpora to detect the degree of compositionality (Melamed, 1997; Moir´on and Tiedemann, 2006; de Caseli et al., 2010; Salehi et al., 2012). The general approach is to word-align the source and target language sentences and analyse alignment patterns for MWEs (e.g. if the MWE is always aligned as a single “phrase”, then it is a strong indicator of non-compositionality). de Caseli et al. (2010) consider non-compositional MWEs to be those candidates that align to the same target language unit, without decomposition into word alignments. Melamed (1997) suggests using mutual information to investigate how well the translation model predicts the distribution of words in the target text given the distribution of words in the source tex</context>
</contexts>
<marker>Salehi, Askarian, Fazly, 2012</marker>
<rawString>Bahar Salehi, Narjes Askarian, and Afsaneh Fazly. 2012. Automatic identification of Persian light verb constructions. In Proceedings of the 13th International Conference on Intelligent Text Processing Computational Linguistics (CICLing-2012), pages 201–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Schone</author>
<author>Dan Jurafsky</author>
</authors>
<title>Is knowledgefree induction of multiword unit dictionary headwords a solved problem.</title>
<date>2001</date>
<booktitle>In Proceedings of the 6th Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>100--108</pages>
<contexts>
<context position="5394" citStr="Schone and Jurafsky (2001)" startWordPosition="817" endWordPosition="820">is their lack of generalization: each type of MWE has its own characteristics, and these characteristics differ from one language to another. Moreover, some MWEs (such as noun compounds) are not flexible syntactically, no matter whether they are compositional or non-compositional (Reddy et al., 2011). Much of the recent work on MWEs focuses on their semantic properties, measuring the semantic similarity between the MWE and its components using different resources, such as WordNet (Kim and Baldwin, 2007) or distributional similarity relative to a corpus (e.g. based on Latent Semantic Analysis: Schone and Jurafsky (2001), Bannard et al. (2003), Reddy et al. (2011)). The size of the corpus is important in methods based on distributional similarity. Unfortunately, however, large corpora are not available for all languages. Reddy et al. (2011) hypothesize that the number of common co-occurrences between a given MWE and its component words indicates the degree of compositionality of that MWE. First, the cooccurrences of a given MWE/word are considered as the values of a vector. They then measure the Cosine similarity between the vectors of the MWE and its components. Bannard et al. (2003) presented four methods t</context>
</contexts>
<marker>Schone, Jurafsky, 2001</marker>
<rawString>Patrick Schone and Dan Jurafsky. 2001. Is knowledgefree induction of multiword unit dictionary headwords a solved problem. In Proceedings of the 6th Conference on Empirical Methods in Natural Language Processing (EMNLP 2001), pages 100–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TF Smith</author>
<author>MS Waterman</author>
</authors>
<title>Identification of common molecular subsequences. Molecular Biology,</title>
<date>1981</date>
<pages>147--195</pages>
<contexts>
<context position="16516" citStr="Smith and Waterman, 1981" startWordPosition="2583" endWordPosition="2586">, len(components)) Levenshtein with substitution penalty (LEV2): One well-documented feature of Levenshtein distance (Baldwin, 2009) is that substitutions are in fact the combination of an addition and a deletion, and as such can be considered to be two edits. Based on this observation, we experiment with a variant of LEV1 with this penalty applied for substitutions. Similarly to LEV1, we normalize as follows: LEV2(MWEs, components) len(MWEs) + len(components) Smith Waterman (SW) This method is based on the Needleman-Wunsch algorithm,8 and was developed to locally-align two protein sequences (Smith and Waterman, 1981). It finds the optimal similar regions by maximizing the number of matches and minimizing the number of gaps necessary to align the two sequences. For example, the optimal local sequence for the two sequences below is AT−−ATCC, in which “-” indicates a gap: 8The Needleman-Wunsch (NW) algorithm, was designed to align two sequences of amino-acids (Needleman and Wunsch, 1970). The algorithm looks for the sequence alignment which maximizes the similarity. As with the LEV score, NW minimizes edit distance, but also takes into account character-tocharacter similarity based on the relative distance b</context>
</contexts>
<marker>Smith, Waterman, 1981</marker>
<rawString>TF Smith and MS Waterman. 1981. Identification of common molecular subsequences. Molecular Biology, 147:195–197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
<author>Daniel S Weld</author>
<author>Kobi Reiter</author>
<author>Michael Skinner</author>
<author>Marcus Sammer</author>
<author>Jeff Bilmes</author>
</authors>
<title>Panlingual lexical translation via probabilistic inference.</title>
<date>2010</date>
<journal>Artificial Intelligence,</journal>
<volume>174</volume>
<issue>9</issue>
<pages>637</pages>
<contexts>
<context position="10634" citStr="Soderland et al. (2010)" startWordPosition="1644" endWordPosition="1647">mmas and expressions. The translations are sourced from handmade electronic dictionaries, making it more accu4http://lcl.uniroma1.it/babelnet/ 5http://www.wiktionary.org/ 6http://translate.google.com/ rate than translation dictionaries generated automatically, e.g. through word alignment. Usually there are several direct translations for a word/MWE from one language to another, as in translations which were extracted from electronic dictionaries. If there is no direct translation for a word/MWE in the database, we can translate indirectly via one or more pivot languages (indirect translation: Soderland et al. (2010)). For example, English ivory tower has direct translations in only 13 languages in Panlex, including French (tour d’ivoire) but not Esperanto. There is, however, a translation of tour d’ivoire into Esperanto (ebura turo), allowing us to infer an indirect translation between ivory tower and ebura turo. 4 Dataset We evaluate our method over two datasets, as described below. REDDY (Reddy et al., 2011): 90 English (binary) noun compounds (NCs), where the overall NC and each component word has been annotated for compositionality on a scale from 0 (non-compositional) to 5 (compositional). In order </context>
</contexts>
<marker>Soderland, Etzioni, Weld, Reiter, Skinner, Sammer, Bilmes, 2010</marker>
<rawString>Stephen Soderland, Oren Etzioni, Daniel S Weld, Kobi Reiter, Michael Skinner, Marcus Sammer, Jeff Bilmes, et al. 2010. Panlingual lexical translation via probabilistic inference. Artificial Intelligence, 174(9):619– 637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sriram Venkatapathy</author>
<author>Aravind K Joshi</author>
</authors>
<title>Using information about multi-word expressions for the word-alignment task.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties,</booktitle>
<pages>20--27</pages>
<contexts>
<context position="2230" citStr="Venkatapathy and Joshi, 2006" startWordPosition="328" endWordPosition="331">rch on binary compositional/non-compositional MWE clas1The example is taken from http://www. thefreedictionary.com sification (Lin, 1999; Baldwin et al., 2003), or a three-way compositional/semi-compositional/noncompositional distinction (Fazly and Stevenson, 2007). There has also been research to suggest that MWEs span the entire continuum from full compositionality to full non-compositionality (McCarthy et al., 2003; Reddy et al., 2011). Investigating the degree of MWE compositionality has been shown to have applications in information retrieval and machine translation (Acosta et al., 2011; Venkatapathy and Joshi, 2006). As an example of an information retrieval system, if we were looking for documents relating to rat race (meaning “an exhausting routine that leaves no time for relaxation”2), we would not be interested in documents on rodents. These results underline the need for methods for broad-coverage MWE compositionality prediction. In this research, we investigate the possibility of using an MWE’s translations in multiple languages to measure the degree of the MWE’s compositionality, and investigate how literal the semantics of each component is within the MWE. We use Panlex to translate the MWE and i</context>
</contexts>
<marker>Venkatapathy, Joshi, 2006</marker>
<rawString>Sriram Venkatapathy and Aravind K Joshi. 2006. Using information about multi-word expressions for the word-alignment task. In Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 20–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Frederick Voegelin</author>
<author>Florence Marie Voegelin</author>
</authors>
<title>Classification and index of the world’s languages, volume 4.</title>
<date>1977</date>
<publisher>Elsevier Science Ltd.</publisher>
<contexts>
<context position="23271" citStr="Voegelin and Voegelin (1977)" startWordPosition="3718" endWordPosition="3721">38 Max SW 0.371 0.408 0.345 LCS 0.406 0.430 0.335 LEV1 0.279 0.362 0.403 LEV2 0.380 0.349 0.406 Table 5: Correlation on REDDY (NCs). N1, N2 and NC, are the first component of the noun compound, its second component, and the noun compound itself, respectively. and 4, we show how often each language was selected in the top-10 languages over the combined 100 (10x10) folds of nested 10-fold cross validation, based on LCS.13 The tables show that the selected languages were mostly consistent over the folds. The languages are a mixture of Romance, Germanic and languages from other families (based on Voegelin and Voegelin (1977)), with no standout language which performs well in all cases (indeed, no language occurs in all three tables). Additionally, there is nothing in common between the verb and the particle top-10 languages. 8 Results As mentioned before, we perform nested 10-fold cross-validation to select the 10 best languages on the training data for each fold. The selected languages for a given fold are then used to compute s1 13Since our later results show that LCS and SW have higher results, we only show the best languages using LCS. These largely coincide with those for SW. f1 sim() Verb Particle Mean SW 0</context>
</contexts>
<marker>Voegelin, Voegelin, 1977</marker>
<rawString>Charles Frederick Voegelin and Florence Marie Voegelin. 1977. Classification and index of the world’s languages, volume 4. Elsevier Science Ltd.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>