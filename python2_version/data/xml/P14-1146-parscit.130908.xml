<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000093">
<title confidence="0.9980015">
Learning Sentiment-Specific Word Embedding
for Twitter Sentiment Classification∗
</title>
<author confidence="0.994348">
Duyu Tang†, Furu Wei$ , Nan Yangb, Ming Zhou$, Ting Liu†, Bing Qin††Research Center for Social Computing and Information Retrieval
</author>
<affiliation confidence="0.98012">
Harbin Institute of Technology, China
$Microsoft Research, Beijing, China
bUniversity of Science and Technology of China, Hefei, China
</affiliation>
<email confidence="0.949569">
{dytang, tliu, qinb}@ir.hit.edu.cn
{fuwei, v-nayang, mingzhou}@microsoft.com
</email>
<sectionHeader confidence="0.997344" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999630580645161">
We present a method that learns word em-
bedding for Twitter sentiment classifica-
tion in this paper. Most existing algorithm-
s for learning continuous word represen-
tations typically only model the syntactic
context of words but ignore the sentimen-
t of text. This is problematic for senti-
ment analysis as they usually map word-
s with similar syntactic context but oppo-
site sentiment polarity, such as good and
bad, to neighboring word vectors. We
address this issue by learning sentiment-
specific word embedding (SSWE), which
encodes sentiment information in the con-
tinuous representation of words. Specif-
ically, we develop three neural networks
to effectively incorporate the supervision
from sentiment polarity of text (e.g. sen-
tences or tweets) in their loss function-
s. To obtain large scale training corpora,
we learn the sentiment-specific word em-
bedding from massive distant-supervised
tweets collected by positive and negative
emoticons. Experiments on applying SS-
WE to a benchmark Twitter sentimen-
t classification dataset in SemEval 2013
show that (1) the SSWE feature performs
comparably with hand-crafted features in
the top-performed system; (2) the perfor-
mance is further improved by concatenat-
ing SSWE with existing feature set.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.987567680851064">
Twitter sentiment classification has attracted in-
creasing research interest in recent years (Jiang et
al., 2011; Hu et al., 2013). The objective is to clas-
sify the sentiment polarity of a tweet as positive,
∗ This work was done when the first and third authors
were visiting Microsoft Research Asia.
negative or neutral. The majority of existing ap-
proaches follow Pang et al. (2002) and employ ma-
chine learning algorithms to build classifiers from
tweets with manually annotated sentiment polar-
ity. Under this direction, most studies focus on
designing effective features to obtain better clas-
sification performance. For example, Mohammad
et al. (2013) build the top-performed system in the
Twitter sentiment classification track of SemEval
2013 (Nakov et al., 2013), using diverse sentiment
lexicons and a variety of hand-crafted features.
Feature engineering is important but labor-
intensive. It is therefore desirable to discover ex-
planatory factors from the data and make the learn-
ing algorithms less dependent on extensive fea-
ture engineering (Bengio, 2013). For the task of
sentiment classification, an effective feature learn-
ing method is to compose the representation of a
sentence (or document) from the representation-
s of the words or phrases it contains (Socher et
al., 2013b; Yessenalina and Cardie, 2011). Ac-
cordingly, it is a crucial step to learn the word
representation (or word embedding), which is a
dense, low-dimensional and real-valued vector for
a word. Although existing word embedding learn-
ing algorithms (Collobert et al., 2011; Mikolov et
al., 2013) are intuitive choices, they are not effec-
tive enough if directly used for sentiment classi-
fication. The most serious problem is that tradi-
tional methods typically model the syntactic con-
text of words but ignore the sentiment information
of text. As a result, words with opposite polari-
ty, such as good and bad, are mapped into close
vectors. It is meaningful for some tasks such as
pos-tagging (Zheng et al., 2013) as the two words
have similar usages and grammatical roles, but it
becomes a disaster for sentiment analysis as they
have the opposite sentiment polarity.
In this paper, we propose learning sentiment-
specific word embedding (SSWE) for sentiment
analysis. We encode the sentiment information in-
</bodyText>
<page confidence="0.935824">
1555
</page>
<note confidence="0.8289135">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1555–1565,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999926555555555">
to the continuous representation of words, so that
it is able to separate good and bad to opposite ends
of the spectrum. To this end, we extend the ex-
isting word embedding learning algorithm (Col-
lobert et al., 2011) and develop three neural net-
works to effectively incorporate the supervision
from sentiment polarity of text (e.g. sentences
or tweets) in their loss functions. We learn the
sentiment-specific word embedding from tweet-
s, leveraging massive tweets with emoticons as
distant-supervised corpora without any manual an-
notations. These automatically collected tweet-
s contain noises so they cannot be directly used
as gold training data to build sentiment classifier-
s, but they are effective enough to provide weak-
ly supervised signals for training the sentiment-
specific word embedding.
We apply SSWE as features in a supervised
learning framework for Twitter sentiment classi-
fication, and evaluate it on the benchmark dataset
in SemEval 2013. In the task of predicting posi-
tive/negative polarity of tweets, our method yields
84.89% in macro-F1 by only using SSWE as fea-
ture, which is comparable to the top-performed
system based on hand-crafted features (84.70%).
After concatenating the SSWE feature with ex-
isting feature set, we push the state-of-the-art to
86.58% in macro-F1. The quality of SSWE is al-
so directly evaluated by measuring the word sim-
ilarity in the embedding space for sentiment lexi-
cons. In the accuracy of polarity consistency be-
tween each sentiment word and its top N closest
words, SSWE outperforms existing word embed-
ding learning algorithms.
The major contributions of the work presented
in this paper are as follows.
</bodyText>
<listItem confidence="0.9976815">
• We develop three neural networks to learn
sentiment-specific word embedding (SSWE)
from massive distant-supervised tweets with-
out any manual annotations;
• To our knowledge, this is the first work that
exploits word embedding for Twitter senti-
ment classification. We report the results that
the SSWE feature performs comparably with
hand-crafted features in the top-performed
system in SemEval 2013;
• We release the sentiment-specific word em-
bedding learned from 10 million tweets,
which can be adopted off-the-shell in other
sentiment analysis tasks.
</listItem>
<sectionHeader confidence="0.999081" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99984725">
In this section, we present a brief review of the
related work from two perspectives, Twitter senti-
ment classification and learning continuous repre-
sentations for sentiment classification.
</bodyText>
<subsectionHeader confidence="0.985787">
2.1 Twitter Sentiment Classification
</subsectionHeader>
<bodyText confidence="0.999953162790698">
Twitter sentiment classification, which identifies
the sentiment polarity of short, informal tweets,
has attracted increasing research interest (Jiang et
al., 2011; Hu et al., 2013) in recent years. Gen-
erally, the methods employed in Twitter sentiment
classification follow traditional sentiment classifi-
cation approaches. The lexicon-based approaches
(Turney, 2002; Ding et al., 2008; Taboada et al.,
2011; Thelwall et al., 2012) mostly use a dictio-
nary of sentiment words with their associated sen-
timent polarity, and incorporate negation and in-
tensification to compute the sentiment polarity for
each sentence (or document).
The learning based methods for Twitter sen-
timent classification follow Pang et al. (2002)’s
work, which treat sentiment classification of texts
as a special case of text categorization issue. Many
studies on Twitter sentiment classification (Pak
and Paroubek, 2010; Davidov et al., 2010; Bar-
bosa and Feng, 2010; Kouloumpis et al., 2011;
Zhao et al., 2012) leverage massive noisy-labeled
tweets selected by positive and negative emoticon-
s as training set and build sentiment classifiers di-
rectly, which is called distant supervision (Go et
al., 2009). Instead of directly using the distant-
supervised data as training set, Liu et al. (2012)
adopt the tweets with emoticons to smooth the lan-
guage model and Hu et al. (2013) incorporate the
emotional signals into an unsupervised learning
framework for Twitter sentiment classification.
Many existing learning based methods on Twit-
ter sentiment classification focus on feature engi-
neering. The reason is that the performance of sen-
timent classifier being heavily dependent on the
choice of feature representation of tweets. The
most representative system is introduced by Mo-
hammad et al. (2013), which is the state-of-the-
art system (the top-performed system in SemEval
2013 Twitter Sentiment Classification Track) by
implementing a number of hand-crafted features.
Unlike the previous studies, we focus on learning
discriminative features automatically from mas-
sive distant-supervised tweets.
</bodyText>
<page confidence="0.98809">
1556
</page>
<subsectionHeader confidence="0.999429">
2.2 Learning Continuous Representations for
Sentiment Classification
</subsectionHeader>
<bodyText confidence="0.9998555">
Pang et al. (2002) pioneer this field by using bag-
of-word representation, representing each word as
a one-hot vector. It has the same length as the size
of the vocabulary, and only one dimension is 1,
with all others being 0. Under this assumption,
many feature learning algorithms are proposed to
obtain better classification performance (Pang and
Lee, 2008; Liu, 2012; Feldman, 2013). However,
the one-hot word representation cannot sufficient-
ly capture the complex linguistic characteristics of
words.
With the revival of interest in deep learn-
ing (Bengio et al., 2013), incorporating the con-
tinuous representation of a word as features has
been proving effective in a variety of NLP tasks,
such as parsing (Socher et al., 2013a), language
modeling (Bengio et al., 2003; Mnih and Hin-
ton, 2009) and NER (Turian et al., 2010). In the
field of sentiment analysis, Bespalov et al. (2011;
2012) initialize the word embedding by Laten-
t Semantic Analysis and further represent each
document as the linear weighted of ngram vec-
tors for sentiment classification. Yessenalina and
Cardie (2011) model each word as a matrix and
combine words using iterated matrix multiplica-
tion. Glorot et al. (2011) explore Stacked Denois-
ing Autoencoders for domain adaptation in sen-
timent classification. Socher et al. propose Re-
cursive Neural Network (RNN) (2011b), matrix-
vector RNN (2012) and Recursive Neural Tensor
Network (RNTN) (2013b) to learn the composi-
tionality of phrases of any length based on the
representation of each pair of children recursively.
Hermann et al. (2013) present Combinatory Cate-
gorial Autoencoders to learn the compositionality
of sentence, which marries the Combinatory Cat-
egorial Grammar with Recursive Autoencoder.
The representation of words heavily relies on
the applications or tasks in which it is used (Lab-
utov and Lipson, 2013). This paper focuses
on learning sentiment-specific word embedding,
which is tailored for sentiment analysis. Un-
like Maas et al. (2011) that follow the proba-
bilistic document model (Blei et al., 2003) and
give an sentiment predictor function to each word,
we develop neural networks and map each n-
gram to the sentiment polarity of sentence. Un-
like Socher et al. (2011c) that utilize manually
labeled texts to learn the meaning of phrase (or
sentence) through compositionality, we focus on
learning the meaning of word, namely word em-
bedding, from massive distant-supervised tweets.
Unlike Labutov and Lipson (2013) that produce
task-specific embedding from an existing word
embedding, we learn sentiment-specific word em-
bedding from scratch.
</bodyText>
<sectionHeader confidence="0.937459" genericHeader="method">
3 Sentiment-Specific Word Embedding
</sectionHeader>
<subsectionHeader confidence="0.575366">
for Twitter Sentiment Classification
</subsectionHeader>
<bodyText confidence="0.999849615384615">
In this section, we present the details of learn-
ing sentiment-specific word embedding (SSWE)
for Twitter sentiment classification. We pro-
pose incorporating the sentiment information of
sentences to learn continuous representations for
words and phrases. We extend the existing word
embedding learning algorithm (Collobert et al.,
2011) and develop three neural networks to learn
SSWE. In the following sections, we introduce the
traditional method before presenting the details of
SSWE learning algorithms. We then describe the
use of SSWE in a supervised learning framework
for Twitter sentiment classification.
</bodyText>
<subsectionHeader confidence="0.84564">
3.1 C&amp;W Model
</subsectionHeader>
<bodyText confidence="0.9999773">
Collobert et al. (2011) introduce C&amp;W model to
learn word embedding based on the syntactic con-
texts of words. Given an ngram “cat chills on a
mat”, C&amp;W replaces the center word with a ran-
dom word wr and derives a corrupted ngram “cat
chills wr a mat”. The training objective is that the
original ngram is expected to obtain a higher lan-
guage model score than the corrupted ngram by a
margin of 1. The ranking objective function can
be optimized by a hinge loss,
</bodyText>
<equation confidence="0.982536">
losscw(t,tr) = max(0, 1 − fcw(t) + fcw(tr))
(1)
</equation>
<bodyText confidence="0.998939923076923">
where t is the original ngram, tr is the corrupted
ngram, fcw(·) is a one-dimensional scalar repre-
senting the language model score of the input n-
gram. Figure 1(a) illustrates the neural architec-
ture of C&amp;W, which consists of four layers, name-
ly lookup → linear → hTanh → linear (from
bottom to top). The original and corrupted ngram-
s are treated as inputs of the feed-forward neural
network, respectively. The output fcw is the lan-
guage model score of the input, which is calculat-
ed as given in Equation 2, where L is the lookup
table of word embedding, w1, w2, b1, b2 are the pa-
rameters of linear layers.
</bodyText>
<equation confidence="0.994826">
fcw(t) = w2(a) + b2 (2)
</equation>
<page confidence="0.978545">
1557
</page>
<figure confidence="0.9989032">
lookup
softmax
linear
linear
hTanh
so cooooL :D
(a) C&amp;W
so cooooL :D
(b) SSWEh
so cooooL :D
(c) SSWEu
syntactic
sentiment
positive
negative
</figure>
<figureCaption confidence="0.9906245">
Figure 1: The traditional C&amp;W model and our neural networks (SSWEh and SSWEu) for learning
sentiment-specific word embedding.
</figureCaption>
<equation confidence="0.998337">
a = hTanh(w1Lt + b1) (3)
hTanh(x) = { −1 if x &lt; −1 (4)
x if − 1 &lt; x &lt; 1
1 if x &gt; 1
</equation>
<subsectionHeader confidence="0.999323">
3.2 Sentiment-Specific Word Embedding
</subsectionHeader>
<bodyText confidence="0.999990371428571">
Following the traditional C&amp;W model (Collobert
et al., 2011), we incorporate the sentiment infor-
mation into the neural network to learn sentiment-
specific word embedding. We develop three neural
networks with different strategies to integrate the
sentiment information of tweets.
Basic Model 1 (SSWEh). As an unsupervised
approach, C&amp;W model does not explicitly capture
the sentiment information of texts. An intuitive
solution to integrate the sentiment information is
predicting the sentiment distribution of text based
on input ngram. We do not utilize the entire sen-
tence as input because the length of different sen-
tences might be variant. We therefore slide the
window of ngram across a sentence, and then pre-
dict the sentiment polarity based on each ngram
with a shared neural network. In the neural net-
work, the distributed representation of higher lay-
er are interpreted as features describing the input.
Thus, we utilize the continuous vector of top layer
to predict the sentiment distribution of text.
Assuming there are K labels, we modify the di-
mension of top layer in C&amp;W model as K and
add a softmax layer upon the top layer. The
neural network (SSWEh) is given in Figure 1(b).
Softmax layer is suitable for this scenario be-
cause its outputs are interpreted as conditional
probabilities. Unlike C&amp;W, SSWEh does not gen-
erate any corrupted ngram. Let fg(t), where K
denotes the number of sentiment polarity label-
s, be the gold K-dimensional multinomial distri-
bution of input t and Ek fgk (t) = 1. For pos-
itive/negative classification, the distribution is of
the form [1,0] for positive and [0,1] for negative.
The cross-entropy error of the softmax layer is :
</bodyText>
<equation confidence="0.992116">
�lossh(t) = − fgk (t) · log(fhk (t)) (5)
k={0,1}
</equation>
<bodyText confidence="0.999985">
where fg(t) is the gold sentiment distribution and
fh(t) is the predicted sentiment distribution.
Basic Model 2 (SSWEr). SSWEh is trained by
predicting the positive ngram as [1,0] and the neg-
ative ngram as [0,1]. However, the constraint of
SSWEh is too strict. The distribution of [0.7,0.3]
can also be interpreted as a positive label because
the positive score is larger than the negative s-
core. Similarly, the distribution of [0.2,0.8] indi-
cates negative polarity. Based on the above obser-
vation, the hard constraints in SSWEh should be
relaxed. If the sentiment polarity of a tweet is pos-
itive, the predicted positive score is expected to be
larger than the predicted negative score, and the
exact reverse if the tweet has negative polarity.
We model the relaxed constraint with a rank-
ing objective function and borrow the bottom four
layers from SSWEh, namely lookup —* linear —*
hTanh —* linear in Figure 1(b), to build the re-
laxed neural network (SSWEr). Compared with
SSWEh, the softmax layer is removed because
SSWEr does not require probabilistic interpreta-
tion. The hinge loss of SSWEr is modeled as de-
</bodyText>
<page confidence="0.968552">
1558
</page>
<bodyText confidence="0.9735648">
scribed below.
where fr0 is the predicted positive score, fr1 is
the predicted negative score, δs(t) is an indicator
function reflecting the sentiment polarity of a sen-
tence,
</bodyText>
<equation confidence="0.895726333333333">
�
1 if fg(t) = [1, 0]
δs(t) = −1 if fg(t) = [0, 1] (7)
</equation>
<bodyText confidence="0.999000410714286">
Similar with SSWEh, SSWEr also does not gen-
erate the corrupted ngram.
Unified Model (SSWEu). The C&amp;W model
learns word embedding by modeling syntactic
contexts of words but ignoring sentiment infor-
mation. By contrast, SSWEh and SSWEr learn
sentiment-specific word embedding by integrating
the sentiment polarity of sentences but leaving out
the syntactic contexts of words. We develop a uni-
fied model (SSWE.) in this part, which captures
the sentiment information of sentences as well as
the syntactic contexts of words. SSWEu is illus-
trated in Figure 1(c).
Given an original (or corrupted) ngram and
the sentiment polarity of a sentence as the in-
put, SSWEu predicts a two-dimensional vector for
each input ngram. The two scalars (fu0 , fu1 ) s-
tand for language model score and sentiment s-
core of the input ngram, respectively. The training
objectives of SSWEu are that (1) the original n-
gram should obtain a higher language model score
fu0 (t) than the corrupted ngram fu0 (tr), and (2) the
sentiment score of original ngram fu1 (t) should be
more consistent with the gold polarity annotation
of sentence than corrupted ngram fu1 (tr). The loss
function of SSWEu is the linear combination of t-
wo hinge losses,
where losscw(t, tr) is the syntactic loss as given
in Equation 1, lossus(t, tr) is the sentiment loss
as described in Equation 9. The hyper-parameter
α weighs the two parts.
Model Training. We train sentiment-specific
word embedding from massive distant-supervised
tweets collected with positive and negative emoti-
cons1. We crawl tweets from April 1st, 2013 to
April 30th, 2013 with TwitterAPI. We tokenize
each tweet with TwitterNLP (Gimpel et al., 2011),
remove the @user and URLs of each tweet, and fil-
ter the tweets that are too short (&lt; 7 words). Final-
ly, we collect 10M tweets, selected by 5M tweets
with positive emoticons and 5M tweets with nega-
tive emoticons.
We train SSWEh, SSWEr and SSWEu by
taking the derivative of the loss through back-
propagation with respect to the whole set of pa-
rameters (Collobert et al., 2011), and use Ada-
Grad (Duchi et al., 2011) to update the parame-
ters. We empirically set the window size as 3, the
embedding length as 50, the length of hidden lay-
er as 20 and the learning rate of AdaGrad as 0.1
for all baseline and our models. We learn embed-
ding for unigrams, bigrams and trigrams separate-
ly with same neural network and same parameter
setting. The contexts of unigram (bigram/trigram)
are the surrounding unigrams (bigrams/trigrams),
respectively.
</bodyText>
<subsectionHeader confidence="0.993481">
3.3 Twitter Sentiment Classification
</subsectionHeader>
<bodyText confidence="0.999899">
We apply sentiment-specific word embedding for
Twitter sentiment classification under a supervised
learning framework as in previous work (Pang et
al., 2002). Instead of hand-crafting features, we
incorporate the continuous representation of word-
s and phrases as the feature of a tweet. The senti-
ment classifier is built from tweets with manually
annotated sentiment polarity.
We explore min, average and max convolu-
tional layers (Collobert et al., 2011; Socher et
al., 2011a), which have been used as simple and
effective methods for compositionality learning
in vector-based semantics (Mitchell and Lapata,
2010), to obtain the tweet representation. The re-
sult is the concatenation of vectors derived from
different convolutional layers.
</bodyText>
<equation confidence="0.913136">
z(tw) = [zmax(tw), zmin(tw), zaverage(tw)]
</equation>
<bodyText confidence="0.989645666666667">
where z(tw) is the representation of tweet tw and
zx(tw) is the results of the convolutional layer x E
{min, max, average}. Each convolutional layer
</bodyText>
<footnote confidence="0.863007">
1We use the emoticons selected by Hu et al. (2013). The
positive emoticons are :) : ) :-) :D =), and the negative emoti-
cons are :( : ( :-( .
</footnote>
<equation confidence="0.999890333333333">
lossr(t) = max(0,1 − δs(t)fr0 (t)
+ δs(t)fr 1 (t) ) (6)
lossu(t, tr) = α · losscw(t, tr)+
(1 − α) · lossus(t, tr) (8)
lossus(t, tr) = max(0,1 − δs(t)fu1 (t)
+ δs(t)fu1 (tr) ) (9)
</equation>
<page confidence="0.95697">
1559
</page>
<bodyText confidence="0.999612">
zx employs the embedding of unigrams, bigrams
and trigrams separately and conducts the matrix-
vector operation of x on the sequence represented
by columns in each lookup table. The output of
zx is the concatenation of results obtained from
different lookup tables.
</bodyText>
<equation confidence="0.932027">
zx(tw) = [wx(Luni)tw,wx(Lbi)tw,wx(Ltri)twl
</equation>
<bodyText confidence="0.9995162">
where wx is the convolutional function of zx,
(L)tw is the concatenated column vectors of the
words in the tweet. Luni, Lbi and Ltri are the
lookup tables of the unigram, bigram and trigram
embedding, respectively.
</bodyText>
<sectionHeader confidence="0.999715" genericHeader="method">
4 Experiment
</sectionHeader>
<bodyText confidence="0.999988833333333">
We conduct experiments to evaluate SSWE by in-
corporating it into a supervised learning frame-
work for Twitter sentiment classification. We also
directly evaluate the effectiveness of the SSWE by
measuring the word similarity in the embedding
space for sentiment lexicons.
</bodyText>
<subsectionHeader confidence="0.977556">
4.1 Twitter Sentiment Classification
</subsectionHeader>
<bodyText confidence="0.998137066666667">
Experiment Setup and Datasets. We conduct
experiments on the latest Twitter sentiment clas-
sification benchmark dataset in SemEval 2013
(Nakov et al., 2013). The training and develop-
ment sets were completely in full to task partici-
pants. However, we were unable to download all
the training and development sets because some
tweets were deleted or not available due to mod-
ified authorization status. The test set is directly
provided to the participants. The distribution of
our dataset is given in Table 1. We train sentiment
classifier with LibLinear (Fan et al., 2008) on the
training set, tune parameter −c on the dev set and
evaluate on the test set. Evaluation metric is the
Macro-F1 of positive and negative categories 2.
</bodyText>
<table confidence="0.99979725">
Positive Negative Neutral Total
Train 2,642 994 3,436 7,072
Dev 408 219 493 1,120
Test 1,570 601 1,639 3,810
</table>
<tableCaption confidence="0.985768">
Table 1: Statistics of the SemEval 2013 Twitter
sentiment classification dataset.
</tableCaption>
<table confidence="0.651490666666667">
2We investigate 2-class Twitter sentiment classifica-
tion (positive/negative) instead of 3-class Twitter sentiment
classification (positive/negative/neutral) in SemEval2013.
</table>
<listItem confidence="0.926219318181818">
Baseline Methods. We compare our method
with the following sentiment classification algo-
rithms:
(1) DistSuper: We use the 10 million tweets se-
lected by positive and negative emoticons as train-
ing data, and build sentiment classifier with Lib-
Linear and ngram features (Go et al., 2009).
(2) SVM: The ngram features and Support Vec-
tor Machine are widely used baseline methods to
build sentiment classifiers (Pang et al., 2002). Li-
bLinear is used to train the SVM classifier.
(3) NBSVM: NBSVM (Wang and Manning,
2012) is a state-of-the-art performer on many sen-
timent classification datasets, which trades-off be-
tween Naive Bayes and NB-enhanced SVM.
(4) RAE: Recursive Autoencoder (Socher et al.,
2011c) has been proven effective in many senti-
ment analysis tasks by learning compositionality
automatically. We run RAE with randomly initial-
ized word embedding.
(5) NRC: NRC builds the top-performed system
in SemEval 2013 Twitter sentiment classification
</listItem>
<bodyText confidence="0.953747384615385">
track which incorporates diverse sentiment lexi-
cons and many manually designed features. We
re-implement this system because the codes are
not publicly available 3. NRC-ngram refers to the
feature set of NRC leaving out ngram features.
Except for DistSuper, other baseline method-
s are conducted in a supervised manner. We do
not compare with RNTN (Socher et al., 2013b) be-
cause we cannot efficiently train the RNTN model.
The reason lies in that the tweets in our dataset do
not have accurately parsed results or fine grained
sentiment labels for phrases. Another reason is
that the RNTN model trained on movie reviews
cannot be directly applied on tweets due to the d-
ifferences between domains (Blitzer et al., 2007).
Results and Analysis. Table 2 shows the macro-
F1 of the baseline systems as well as the SSWE-
based methods on positive/negative sentimen-
t classification of tweets. Distant supervision is
relatively weak because the noisy-labeled tweet-
s are treated as the gold standard, which affects
the performance of classifier. The results of bag-
of-ngram (uni/bi/tri-gram) features are not satis-
fied because the one-hot word representation can-
not capture the latent connections between words.
NBSVM and RAE perform comparably and have
</bodyText>
<footnote confidence="0.983828333333333">
3For 3-class sentiment classification in SemEval 2013,
our re-implementation of NRC achieved 68.3%, 0.7% low-
er than NRC (69%) due to less training data.
</footnote>
<page confidence="0.942418">
1560
</page>
<table confidence="0.999489833333334">
Method Macro-F1
DistSuper + unigram 61.74
DistSuper + uni/bi/tri-gram 63.84
SVM + unigram 74.50
SVM + uni/bi/tri-gram 75.06
NBSVM 75.28
RAE 75.12
NRC (Top System in SemEval) 84.73
NRC - ngram 84.17
SSWE,, 84.98
SSWE,,+NRC 86.58
SSWE,,+NRC-ngram 86.48
</table>
<tableCaption confidence="0.9565965">
Table 2: Macro-F1 on positive/negative classifica-
tion of tweets.
</tableCaption>
<bodyText confidence="0.987231654545455">
a big gap in comparison with the NRC and SSWE-
based methods. The reason is that RAE and NB-
SVM learn the representation of tweets from the
small-scale manually annotated training set, which
cannot well capture the comprehensive linguistic
phenomenons of words.
NRC implements a variety of features and
reaches 84.73% in macro-F1, verifying the impor-
tance of a better feature representation for Twit-
ter sentiment classification. We achieve 84.98%
by using only SSWE,, as features without borrow-
ing any sentiment lexicons or hand-crafted rules.
The results indicate that SSWE,, automatically
learns discriminative features from massive tweets
and performs comparable with the state-of-the-art
manually designed features. After concatenating
SSWE,, with the feature set of NRC, the perfor-
mance is further improved to 86.58%. We also
compare SSWE,, with the ngram feature by inte-
grating SSWE into NRC-ngram. The concatenated
features SSWE,,+NRC-ngram (86.48%) outperfor-
m the original feature set of NRC (84.73%).
As a reference, we apply SSWE,, on subjec-
tive classification of tweets, and obtain 72.17% in
macro-F1 by using only SSWE,, as feature. Af-
ter combining SSWE,, with the feature set of NR-
C, we improve NRC from 74.86% to 75.39% for
subjective classification.
Comparision between Different Word Embed-
ding. We compare sentiment-specific word em-
bedding (SSWEh, SSWE,, SSWE,,) with base-
line embedding learning algorithms by only us-
ing word embedding as features for Twitter sen-
timent classification. We use the embedding of u-
nigrams, bigrams and trigrams in the experimen-
t. The embeddings of C&amp;W (Collobert et al.,
2011), word2vec4, WVSA (Maas et al., 2011) and
our models are trained with the same dataset and
same parameter setting. We compare with C&amp;W
and word2vec as they have been proved effective
in many NLP tasks. The trade-off parameter of
ReEmb (Labutov and Lipson, 2013) is tuned on
the development set of SemEval 2013.
Table 3 shows the performance on the pos-
itive/negative classification of tweets5. ReEm-
b(C&amp;W) and ReEmb(w2v) stand for the use
of embeddings learned from 10 million distant-
supervised tweets with C&amp;W and word2vec, re-
spectively. Each row of Table 3 represents a word
embedding learning algorithm. Each column s-
tands for a type of embedding used to compose
features of tweets. The column uni+bi denotes the
use of unigram and bigram embedding, and the
column uni+bi+tri indicates the use of unigram,
bigram and trigram embedding.
</bodyText>
<table confidence="0.999588666666667">
Embedding unigram uni+bi uni+bi+tri
C&amp;W 74.89 75.24 75.89
Word2vec 73.21 75.07 76.31
ReEmb(C&amp;W) 75.87 – –
ReEmb(w2v) 75.21 – –
WVSA 77.04 – –
SSWEh 81.33 83.16 83.37
SSWE, 80.45 81.52 82.60
SSWE,, 83.70 84.70 84.98
</table>
<tableCaption confidence="0.8111495">
Table 3: Macro-F1 on positive/negative classifica-
tion of tweets with different word embeddings.
</tableCaption>
<bodyText confidence="0.999739076923077">
From the first column of Table 3, we can see that
the performance of C&amp;W and word2vec are obvi-
ously lower than sentiment-specific word embed-
dings by only using unigram embedding as fea-
tures. The reason is that C&amp;W and word2vec do
not explicitly exploit the sentiment information of
the text, resulting in that the words with oppo-
site polarity such as good and bad are mapped
to close word vectors. When such word embed-
dings are fed as features to a Twitter sentimen-
t classifier, the discriminative ability of sentiment
words are weakened thus the classification perfor-
mance is affected. Sentiment-specific word em-
</bodyText>
<footnote confidence="0.995400666666667">
4Available at https://code.google.com/p/word2vec/. We
utilize the Skip-gram model because it performs better than
CBOW in our experiments.
5MVSA and ReEmb are not suitable for learning bigram
and trigram embedding because their sentiment predictor
functions only utilize the unigram embedding.
</footnote>
<page confidence="0.995636">
1561
</page>
<bodyText confidence="0.992509647058823">
beddings (SSWEh, SSWE,, SSWE,,) effectively
distinguish words with opposite sentiment polarity
and perform best in three settings. SSWE outper-
forms MVSA by exploiting more contextual infor-
mation in the sentiment predictor function. SSWE
outperforms ReEmb by leveraging more senti-
ment information from massive distant-supervised
tweets. Among three sentiment-specific word em-
beddings, SSWE,, captures more context informa-
tion and yields best performance. SSWEh and
SSWE, obtain comparative results.
From each row of Table 3, we can see that the
bigram and trigram embeddings consistently im-
prove the performance of Twitter sentiment classi-
fication. The underlying reason is that a phrase,
which cannot be accurately represented by uni-
gram embedding, is directly encoded into the n-
gram embedding as an idiomatic unit. A typical
case in sentiment analysis is that the composed
phrase and multiword expression may have a dif-
ferent sentiment polarity than the individual word-
s it contains, such as not [bad] and [great] deal
of (the word in the bracket has different sentiment
polarity with the ngram). A very recent study by
Mikolov et al. (2013) also verified the effective-
ness of phrase embedding for analogically reason-
ing phrases.
Effect of α in SSWE,, We tune the hyper-
parameter α of SSWE,, on the development set by
using unigram embedding as features. As given
in Equation 8, α is the weighting score of syntac-
tic loss of SSWE,, and trades-off the syntactic and
sentiment losses. SSWE,, is trained from 10 mil-
lion distant-supervised tweets.
</bodyText>
<figureCaption confidence="0.7758086">
Figure 2: Macro-F1 of SSWE,, on the develop-
ment set of SemEval 2013 with different α.
Figure 2 shows the macro-F1 of SSWE,, on pos-
itive/negative classification of tweets with differ-
ent α on our development set. We can see that
</figureCaption>
<bodyText confidence="0.999955">
SSWE,, performs better when α is in the range
of [0.5, 0.6], which balances the syntactic context
and sentiment information. The model with α=1
stands for C&amp;W model, which only encodes the
syntactic contexts of words. The sharp decline at
α=1 reflects the importance of sentiment informa-
tion in learning word embedding for Twitter senti-
ment classification.
</bodyText>
<subsectionHeader confidence="0.980263">
Effect of Distant-supervised Data in SSWE,,
</subsectionHeader>
<bodyText confidence="0.998985">
We investigate how the size of the distant-
supervised data affects the performance of SSWE,,
feature for Twitter sentiment classification. We
vary the number of distant-supervised tweets from
1 million to 12 million, increased by 1 million.
We set the α of SSWE,, as 0.5, according to the
experiments shown in Figure 2. Results of posi-
tive/negative classification of tweets on our devel-
opment set are given in Figure 3.
</bodyText>
<figureCaption confidence="0.8774695">
Figure 3: Macro-F1 of SSWE,, with different size
of distant-supervised data on our development set.
</figureCaption>
<bodyText confidence="0.999515076923077">
We can see that when more distant-supervised
tweets are added, the accuracy of SSWE,, con-
sistently improves. The underlying reason is that
when more tweets are incorporated, the word em-
bedding is better estimated as the vocabulary size
is larger and the context and sentiment informa-
tion are richer. When we have 10 million distant-
supervised tweets, the SSWE,, feature increases
the macro-F1 of positive/negative classification of
tweets to 82.94% on our development set. When
we have more than 10 million tweets, the per-
formance remains stable as the contexts of words
have been mostly covered.
</bodyText>
<subsectionHeader confidence="0.998565">
4.2 Word Similarity of Sentiment Lexicons
</subsectionHeader>
<bodyText confidence="0.99934025">
The quality of SSWE has been implicitly evaluat-
ed when applied in Twitter sentiment classification
in the previous subsection. We explicitly evaluate
it in this section through word similarity in the em-
</bodyText>
<figure confidence="0.99724444">
0.84
0.83
0.82
Macro−F1
0.81
0.8
0.79
0.78
SSWEu
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
α
0.77
# of distant−supervised tweets
x 106
0.84
SSWEu
0.83
0.82
1 2 3 4 5 6 7 8 9 10 11 12
0.77
M..−F1
0.81
0.8
0.79
0.78
</figure>
<page confidence="0.992552">
1562
</page>
<bodyText confidence="0.996149">
bedding space for sentiment lexicons. The evalua-
tion metric is the accuracy of polarity consistency
between each sentiment word and its top N closest
words in the sentiment lexicon,
</bodyText>
<figure confidence="0.628811">
#Lex N
Accuracy Ei=1 Ej=1 Q(wi, cij) 10
y #Lex × N ( )
</figure>
<bodyText confidence="0.982504894736842">
where #Lex is the number of words in the senti-
ment lexicon, wi is the i-th word in the lexicon, cij
is the j-th closest word to wi in the lexicon with co-
sine similarity, Q(wi, cij) is an indicator function
that is equal to 1 if wi and cij have the same sen-
timent polarity and 0 for the opposite case. The
higher accuracy refers to a better polarity consis-
tency of words in the sentiment lexicon. We set N
as 100 in our experiment.
Experiment Setup and Datasets We utilize
the widely-used sentiment lexicons, namely M-
PQA (Wilson et al., 2005) and HL (Hu and Liu,
2004), to evaluate the quality of word embedding.
For each lexicon, we remove the words that do
not appear in the lookup table of word embedding.
We only use unigram embedding in this section
because these sentiment lexicons do not contain
phrases. The distribution of the lexicons used in
this paper is listed in Table 4.
</bodyText>
<table confidence="0.999658">
Lexicon Positive Negative Total
HL 1,331 2,647 3,978
MPQA 1,932 2,817 4,749
Joint 1,051 2,024 3,075
</table>
<tableCaption confidence="0.986455">
Table 4: Statistics of the sentiment lexicons. Join-
</tableCaption>
<bodyText confidence="0.980968411764706">
t stands for the words that occur in both HL and
MPQA with the same sentiment polarity.
Results. Table 5 shows our results com-
pared to other word embedding learning al-
gorithms. The accuracy of random result is
50% as positive and negative words are ran-
domly occurred in the nearest neighbors of
each word. Sentiment-specific word embed-
dings (SSWEh, SSWEr, SSWEu) outperform ex-
isting neural models (C&amp;W, word2vec) by large
margins. SSWEu performs best in three lexicon-
s. SSWEh and SSWEr have comparable perfor-
mances. Experimental results further demonstrate
that sentiment-specific word embeddings are able
to capture the sentiment information of texts and
distinguish words with opposite sentiment polari-
ty, which are not well solved in traditional neural
</bodyText>
<table confidence="0.9998151">
Embedding HL MPQA Joint
Random 50.00 50.00 50.00
C&amp;W 63.10 58.13 62.58
Word2vec 66.22 60.72 65.59
ReEmb(C&amp;W) 64.81 59.76 64.09
ReEmb(w2v) 67.16 61.81 66.39
WVSA 68.14 64.07 67.12
SSWEh 74.17 68.36 74.03
SSWEr 73.65 68.02 73.14
SSWEu 77.30 71.74 77.33
</table>
<tableCaption confidence="0.9916795">
Table 5: Accuracy of the polarity consistency of
words in different sentiment lexicons.
</tableCaption>
<bodyText confidence="0.9962505">
models like C&amp;W and word2vec. SSWE outper-
forms MVSA and ReEmb by exploiting more con-
text information of words and sentiment informa-
tion of sentences, respectively.
</bodyText>
<sectionHeader confidence="0.999641" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999969217391305">
In this paper, we propose learning continuous
word representations as features for Twitter sen-
timent classification under a supervised learning
framework. We show that the word embedding
learned by traditional neural networks are not ef-
fective enough for Twitter sentiment classification.
These methods typically only model the contex-
t information of words so that they cannot dis-
tinguish words with similar context but opposite
sentiment polarity (e.g. good and bad). We learn
sentiment-specific word embedding (SSWE) by
integrating the sentiment information into the loss
functions of three neural networks. We train SS-
WE with massive distant-supervised tweets select-
ed by positive and negative emoticons. The ef-
fectiveness of SSWE has been implicitly evaluat-
ed by using it as features in sentiment classifica-
tion on the benchmark dataset in SemEval 2013,
and explicitly verified by measuring word similar-
ity in the embedding space for sentiment lexicon-
s. Our unified model combining syntactic context
of words and sentiment information of sentences
yields the best performance in both experiments.
</bodyText>
<sectionHeader confidence="0.998813" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9976036">
We thank Yajuan Duan, Shujie Liu, Zhenghua Li,
Li Dong, Hong Sun and Lanjun Zhou for their
great help. This research was partly supported
by National Natural Science Foundation of China
(No.61133012, No.61273321, No.61300113).
</bodyText>
<page confidence="0.96371">
1563
</page>
<sectionHeader confidence="0.99607" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999131657142857">
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy da-
ta. In Proceedings of International Conference on
Computational Linguistics, pages 36–44.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155.
Yoshua Bengio, Aaron Courville, and Pascal Vincent.
2013. Representation learning: A review and new
perspectives. IEEE Trans. Pattern Analysis and Ma-
chine Intelligence.
Yoshua Bengio. 2013. Deep learning of represen-
tations: Looking forward. arXiv preprint arX-
iv:1305.0445.
Dmitriy Bespalov, Bing Bai, Yanjun Qi, and Ali Shok-
oufandeh. 2011. Sentiment classification based on
supervised latent n-gram analysis. In Proceedings
of the Conference on Information and Knowledge
Management, pages 375–382.
Dmitriy Bespalov, Yanjun Qi, Bing Bai, and Ali Shok-
oufandeh. 2012. Sentiment classification with su-
pervised sequence embedding. In Machine Learn-
ing and Knowledge Discovery in Databases, pages
159–174. Springer.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993–1022.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Annual Meeting of the Association for
Computational Linguistics, volume 7.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493–2537.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of International Con-
ference on Computational Linguistics, pages 241–
249.
Xiaowen Ding, Bing Liu, and Philip S Yu. 2008. A
holistic lexicon-based approach to opinion mining.
In Proceedings of the International Conference on
Web Search and Data Mining, pages 231–240.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, pages 2121–2159.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871–1874.
Ronen Feldman. 2013. Techniques and application-
s for sentiment analysis. Communications of the
ACM, 56(4):82–89.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics, pages 42–47.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. Proceed-
ings of International Conference on Machine Learn-
ing.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, pages 1–12.
Karl Moritz Hermann and Phil Blunsom. 2013. The
role of syntax in vector space models of compo-
sitional semantics. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics, pages 894–904.
Ming Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, pages 168–177.
Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu.
2013. Unsupervised sentiment analysis with emo-
tional signals. In Proceedings of the International
World Wide Web Conference, pages 607–618.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. The Proceeding of Annual
Meeting of the Association for Computational Lin-
guistics, 1:151–160.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg! In The International AAAI
Conference on Weblogs and Social Media.
Igor Labutov and Hod Lipson. 2013. Re-embedding
words. In Annual Meeting of the Association for
Computational Linguistics.
Kun-Lin Liu, Wu-Jun Li, and Minyi Guo. 2012. E-
moticon smoothed language models for twitter sen-
timent analysis. In The Association for the Advance-
ment of Artificial Intelligence.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1–167.
</reference>
<page confidence="0.873022">
1564
</page>
<reference confidence="0.999887459459459">
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corra-
do, and Jeffrey Dean. 2013. Distributed representa-
tions of words and phrases and their compositionali-
ty. The Conference on Neural Information Process-
ing Systems.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388–1429.
Andriy Mnih and Geoffrey E Hinton. 2009. A s-
calable hierarchical distributed language model. In
Advances in neural information processing systems,
pages 1081–1088.
Saif M Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-of-
the-art in sentiment analysis of tweets. Proceedings
of the International Workshop on Semantic Evalua-
tion.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Proceedings of the International Work-
shop on Semantic Evaluation, volume 13.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In Proceedings of Language Resources and Evalua-
tion Conference, volume 2010.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1–135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 79–86.
Richard Socher, Eric H Huang, Jeffrey Pennington,
Andrew Y Ng, and Christopher D Manning. 2011a.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. The Conference
on Neural Information Processing Systems, 24:801–
809.
Richard Socher, Cliff C Lin, Andrew Ng, and Chris
Manning. 2011b. Parsing natural scenes and nat-
ural language with recursive neural networks. In
Proceedings of the International Conference on Ma-
chine Learning, pages 129–136.
Richard Socher, J. Pennington, E.H. Huang, A.Y. Ng,
and C.D. Manning. 2011c. Semi-supervised recur-
sive autoencoders for predicting sentiment distribu-
tions. In Conference on Empirical Methods in Nat-
ural Language Processing, pages 151–161.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Com-
positionality Through Recursive Matrix-Vector S-
paces. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013a. Parsing with composi-
tional vector grammars. In Annual Meeting of the
Association for Computational Linguistics.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013b. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Conference on Empirical Methods in Nat-
ural Language Processing, pages 1631–1642.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional linguistics, 37(2):267–307.
Mike Thelwall, Kevan Buckley, and Georgios Pal-
toglou. 2012. Sentiment strength detection for the
social web. Journal of the American Society for In-
formation Science and Technology, 63(1):163–173.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. Annual Meeting of the
Association for Computational Linguistics.
Peter D Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 417–424.
Sida Wang and Christopher D Manning. 2012. Base-
lines and bigrams: Simple, good sentiment and topic
classification. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 90–94.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 347–354.
Ainur Yessenalina and Claire Cardie. 2011. Compo-
sitional matrix-space models for sentiment analysis.
In Proceedings of Conference on Empirical Methods
in Natural Language Processing, pages 172–182.
Jichang Zhao, Li Dong, Junjie Wu, and Ke Xu. 2012.
Moodlens: an emoticon-based sentiment analysis
system for chinese tweets. In Proceedings of the
18th ACM SIGKDD international conference on
Knowledge discovery and data mining.
Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu.
2013. Deep learning for chinese word segmenta-
tion and pos tagging. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 647–657.
</reference>
<page confidence="0.992518">
1565
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.291345">
<title confidence="0.733216666666667">Learning Sentiment-Specific Word Twitter Sentiment Furu Nan Ming Ting Bing Center for Social Computing and Information</title>
<affiliation confidence="0.999171">Harbin Institute of Technology,</affiliation>
<address confidence="0.838182">Research, Beijing, bUniversity of Science and Technology of China, Hefei,</address>
<email confidence="0.9405325">tliu,v-nayang,</email>
<abstract confidence="0.99723228125">We present a method that learns word embedding for Twitter sentiment classificain this paper. Most existing algorithms for learning continuous word representations typically only model the syntactic of words but ignore the sentiment of text. This is problematic for sentianalysis as they usually map words with similar syntactic context but opposentiment polarity, such as to neighboring word vectors. We address this issue by learning sentimentword embedding which encodes sentiment information in the continuous representation of words. Specifically, we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g. sentences or tweets) in their loss functions. To obtain large scale training corpora, we learn the sentiment-specific word embedding from massive distant-supervised tweets collected by positive and negative emoticons. Experiments on applying SSto a benchmark Twitter sentiment classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenating SSWE with existing feature set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Luciano Barbosa</author>
<author>Junlan Feng</author>
</authors>
<title>Robust sentiment detection on twitter from biased and noisy data. In</title>
<date>2010</date>
<booktitle>Proceedings of International Conference on Computational Linguistics,</booktitle>
<pages>36--44</pages>
<contexts>
<context position="7555" citStr="Barbosa and Feng, 2010" startWordPosition="1147" endWordPosition="1151">aches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision (Go et al., 2009). Instead of directly using the distantsupervised data as training set, Liu et al. (2012) adopt the tweets with emoticons to smooth the language model and Hu et al. (2013) incorporate the emotional signals into an unsupervised learning framework for Twitter sentiment classification. Many existing learning based methods on Twitter sentiment classification focu</context>
</contexts>
<marker>Barbosa, Feng, 2010</marker>
<rawString>Luciano Barbosa and Junlan Feng. 2010. Robust sentiment detection on twitter from biased and noisy data. In Proceedings of International Conference on Computational Linguistics, pages 36–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="9530" citStr="Bengio et al., 2003" startWordPosition="1448" endWordPosition="1451">of the vocabulary, and only one dimension is 1, with all others being 0. Under this assumption, many feature learning algorithms are proposed to obtain better classification performance (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). However, the one-hot word representation cannot sufficiently capture the complex linguistic characteristics of words. With the revival of interest in deep learning (Bengio et al., 2013), incorporating the continuous representation of a word as features has been proving effective in a variety of NLP tasks, such as parsing (Socher et al., 2013a), language modeling (Bengio et al., 2003; Mnih and Hinton, 2009) and NER (Turian et al., 2010). In the field of sentiment analysis, Bespalov et al. (2011; 2012) initialize the word embedding by Latent Semantic Analysis and further represent each document as the linear weighted of ngram vectors for sentiment classification. Yessenalina and Cardie (2011) model each word as a matrix and combine words using iterated matrix multiplication. Glorot et al. (2011) explore Stacked Denoising Autoencoders for domain adaptation in sentiment classification. Socher et al. propose Recursive Neural Network (RNN) (2011b), matrixvector RNN (2012) and </context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Aaron Courville</author>
<author>Pascal Vincent</author>
</authors>
<title>Representation learning: A review and new perspectives.</title>
<date>2013</date>
<journal>IEEE Trans. Pattern Analysis and Machine Intelligence.</journal>
<contexts>
<context position="9330" citStr="Bengio et al., 2013" startWordPosition="1415" endWordPosition="1418">us Representations for Sentiment Classification Pang et al. (2002) pioneer this field by using bagof-word representation, representing each word as a one-hot vector. It has the same length as the size of the vocabulary, and only one dimension is 1, with all others being 0. Under this assumption, many feature learning algorithms are proposed to obtain better classification performance (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). However, the one-hot word representation cannot sufficiently capture the complex linguistic characteristics of words. With the revival of interest in deep learning (Bengio et al., 2013), incorporating the continuous representation of a word as features has been proving effective in a variety of NLP tasks, such as parsing (Socher et al., 2013a), language modeling (Bengio et al., 2003; Mnih and Hinton, 2009) and NER (Turian et al., 2010). In the field of sentiment analysis, Bespalov et al. (2011; 2012) initialize the word embedding by Latent Semantic Analysis and further represent each document as the linear weighted of ngram vectors for sentiment classification. Yessenalina and Cardie (2011) model each word as a matrix and combine words using iterated matrix multiplication. G</context>
</contexts>
<marker>Bengio, Courville, Vincent, 2013</marker>
<rawString>Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation learning: A review and new perspectives. IEEE Trans. Pattern Analysis and Machine Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
</authors>
<title>Deep learning of representations: Looking forward. arXiv preprint arXiv:1305.0445.</title>
<date>2013</date>
<contexts>
<context position="2745" citStr="Bengio, 2013" startWordPosition="408" endWordPosition="409">tweets with manually annotated sentiment polarity. Under this direction, most studies focus on designing effective features to obtain better classification performance. For example, Mohammad et al. (2013) build the top-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011). Accordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word. Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment cl</context>
</contexts>
<marker>Bengio, 2013</marker>
<rawString>Yoshua Bengio. 2013. Deep learning of representations: Looking forward. arXiv preprint arXiv:1305.0445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Bespalov</author>
<author>Bing Bai</author>
<author>Yanjun Qi</author>
<author>Ali Shokoufandeh</author>
</authors>
<title>Sentiment classification based on supervised latent n-gram analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Information and Knowledge Management,</booktitle>
<pages>375--382</pages>
<contexts>
<context position="9643" citStr="Bespalov et al. (2011" startWordPosition="1469" endWordPosition="1472">arning algorithms are proposed to obtain better classification performance (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). However, the one-hot word representation cannot sufficiently capture the complex linguistic characteristics of words. With the revival of interest in deep learning (Bengio et al., 2013), incorporating the continuous representation of a word as features has been proving effective in a variety of NLP tasks, such as parsing (Socher et al., 2013a), language modeling (Bengio et al., 2003; Mnih and Hinton, 2009) and NER (Turian et al., 2010). In the field of sentiment analysis, Bespalov et al. (2011; 2012) initialize the word embedding by Latent Semantic Analysis and further represent each document as the linear weighted of ngram vectors for sentiment classification. Yessenalina and Cardie (2011) model each word as a matrix and combine words using iterated matrix multiplication. Glorot et al. (2011) explore Stacked Denoising Autoencoders for domain adaptation in sentiment classification. Socher et al. propose Recursive Neural Network (RNN) (2011b), matrixvector RNN (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the compositionality of phrases of any length based on th</context>
</contexts>
<marker>Bespalov, Bai, Qi, Shokoufandeh, 2011</marker>
<rawString>Dmitriy Bespalov, Bing Bai, Yanjun Qi, and Ali Shokoufandeh. 2011. Sentiment classification based on supervised latent n-gram analysis. In Proceedings of the Conference on Information and Knowledge Management, pages 375–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Bespalov</author>
<author>Yanjun Qi</author>
<author>Bing Bai</author>
<author>Ali Shokoufandeh</author>
</authors>
<title>Sentiment classification with supervised sequence embedding.</title>
<date>2012</date>
<booktitle>In Machine Learning and Knowledge Discovery in Databases,</booktitle>
<pages>159--174</pages>
<publisher>Springer.</publisher>
<marker>Bespalov, Qi, Bai, Shokoufandeh, 2012</marker>
<rawString>Dmitriy Bespalov, Yanjun Qi, Bing Bai, and Ali Shokoufandeh. 2012. Sentiment classification with supervised sequence embedding. In Machine Learning and Knowledge Discovery in Databases, pages 159–174. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>the Journal of machine Learning research,</journal>
<pages>3--993</pages>
<contexts>
<context position="10802" citStr="Blei et al., 2003" startWordPosition="1647" endWordPosition="1650">n the compositionality of phrases of any length based on the representation of each pair of children recursively. Hermann et al. (2013) present Combinatory Categorial Autoencoders to learn the compositionality of sentence, which marries the Combinatory Categorial Grammar with Recursive Autoencoder. The representation of words heavily relies on the applications or tasks in which it is used (Labutov and Lipson, 2013). This paper focuses on learning sentiment-specific word embedding, which is tailored for sentiment analysis. Unlike Maas et al. (2011) that follow the probabilistic document model (Blei et al., 2003) and give an sentiment predictor function to each word, we develop neural networks and map each ngram to the sentiment polarity of sentence. Unlike Socher et al. (2011c) that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word embedding, from massive distant-supervised tweets. Unlike Labutov and Lipson (2013) that produce task-specific embedding from an existing word embedding, we learn sentiment-specific word embedding from scratch. 3 Sentiment-Specific Word Embedding for Twitter Sentiment </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. the Journal of machine Learning research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>7</volume>
<contexts>
<context position="24044" citStr="Blitzer et al., 2007" startWordPosition="3828" endWordPosition="3831">mplement this system because the codes are not publicly available 3. NRC-ngram refers to the feature set of NRC leaving out ngram features. Except for DistSuper, other baseline methods are conducted in a supervised manner. We do not compare with RNTN (Socher et al., 2013b) because we cannot efficiently train the RNTN model. The reason lies in that the tweets in our dataset do not have accurately parsed results or fine grained sentiment labels for phrases. Another reason is that the RNTN model trained on movie reviews cannot be directly applied on tweets due to the differences between domains (Blitzer et al., 2007). Results and Analysis. Table 2 shows the macroF1 of the baseline systems as well as the SSWEbased methods on positive/negative sentiment classification of tweets. Distant supervision is relatively weak because the noisy-labeled tweets are treated as the gold standard, which affects the performance of classifier. The results of bagof-ngram (uni/bi/tri-gram) features are not satisfied because the one-hot word representation cannot capture the latent connections between words. NBSVM and RAE perform comparably and have 3For 3-class sentiment classification in SemEval 2013, our re-implementation o</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Annual Meeting of the Association for Computational Linguistics, volume 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="3235" citStr="Collobert et al., 2011" startWordPosition="484" endWordPosition="487">er explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011). Accordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word. Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classification. The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text. As a result, words with opposite polarity, such as good and bad, are mapped into close vectors. It is meaningful for some tasks such as pos-tagging (Zheng et al., 2013) as the two words have similar usages and grammatical roles, but it becomes a disaster for sentiment analysis as they have the opposite sentiment polarity. In thi</context>
<context position="11752" citStr="Collobert et al., 2011" startWordPosition="1785" endWordPosition="1788">ord embedding, from massive distant-supervised tweets. Unlike Labutov and Lipson (2013) that produce task-specific embedding from an existing word embedding, we learn sentiment-specific word embedding from scratch. 3 Sentiment-Specific Word Embedding for Twitter Sentiment Classification In this section, we present the details of learning sentiment-specific word embedding (SSWE) for Twitter sentiment classification. We propose incorporating the sentiment information of sentences to learn continuous representations for words and phrases. We extend the existing word embedding learning algorithm (Collobert et al., 2011) and develop three neural networks to learn SSWE. In the following sections, we introduce the traditional method before presenting the details of SSWE learning algorithms. We then describe the use of SSWE in a supervised learning framework for Twitter sentiment classification. 3.1 C&amp;W Model Collobert et al. (2011) introduce C&amp;W model to learn word embedding based on the syntactic contexts of words. Given an ngram “cat chills on a mat”, C&amp;W replaces the center word with a random word wr and derives a corrupted ngram “cat chills wr a mat”. The training objective is that the original ngram is exp</context>
<context position="13636" citStr="Collobert et al., 2011" startWordPosition="2125" endWordPosition="2128">ut, which is calculated as given in Equation 2, where L is the lookup table of word embedding, w1, w2, b1, b2 are the parameters of linear layers. fcw(t) = w2(a) + b2 (2) 1557 lookup softmax linear linear hTanh so cooooL :D (a) C&amp;W so cooooL :D (b) SSWEh so cooooL :D (c) SSWEu syntactic sentiment positive negative Figure 1: The traditional C&amp;W model and our neural networks (SSWEh and SSWEu) for learning sentiment-specific word embedding. a = hTanh(w1Lt + b1) (3) hTanh(x) = { −1 if x &lt; −1 (4) x if − 1 &lt; x &lt; 1 1 if x &gt; 1 3.2 Sentiment-Specific Word Embedding Following the traditional C&amp;W model (Collobert et al., 2011), we incorporate the sentiment information into the neural network to learn sentimentspecific word embedding. We develop three neural networks with different strategies to integrate the sentiment information of tweets. Basic Model 1 (SSWEh). As an unsupervised approach, C&amp;W model does not explicitly capture the sentiment information of texts. An intuitive solution to integrate the sentiment information is predicting the sentiment distribution of text based on input ngram. We do not utilize the entire sentence as input because the length of different sentences might be variant. We therefore sli</context>
<context position="18684" citStr="Collobert et al., 2011" startWordPosition="2975" endWordPosition="2978">t-specific word embedding from massive distant-supervised tweets collected with positive and negative emoticons1. We crawl tweets from April 1st, 2013 to April 30th, 2013 with TwitterAPI. We tokenize each tweet with TwitterNLP (Gimpel et al., 2011), remove the @user and URLs of each tweet, and filter the tweets that are too short (&lt; 7 words). Finally, we collect 10M tweets, selected by 5M tweets with positive emoticons and 5M tweets with negative emoticons. We train SSWEh, SSWEr and SSWEu by taking the derivative of the loss through backpropagation with respect to the whole set of parameters (Collobert et al., 2011), and use AdaGrad (Duchi et al., 2011) to update the parameters. We empirically set the window size as 3, the embedding length as 50, the length of hidden layer as 20 and the learning rate of AdaGrad as 0.1 for all baseline and our models. We learn embedding for unigrams, bigrams and trigrams separately with same neural network and same parameter setting. The contexts of unigram (bigram/trigram) are the surrounding unigrams (bigrams/trigrams), respectively. 3.3 Twitter Sentiment Classification We apply sentiment-specific word embedding for Twitter sentiment classification under a supervised le</context>
<context position="26659" citStr="Collobert et al., 2011" startWordPosition="4231" endWordPosition="4234"> (84.73%). As a reference, we apply SSWE,, on subjective classification of tweets, and obtain 72.17% in macro-F1 by using only SSWE,, as feature. After combining SSWE,, with the feature set of NRC, we improve NRC from 74.86% to 75.39% for subjective classification. Comparision between Different Word Embedding. We compare sentiment-specific word embedding (SSWEh, SSWE,, SSWE,,) with baseline embedding learning algorithms by only using word embedding as features for Twitter sentiment classification. We use the embedding of unigrams, bigrams and trigrams in the experiment. The embeddings of C&amp;W (Collobert et al., 2011), word2vec4, WVSA (Maas et al., 2011) and our models are trained with the same dataset and same parameter setting. We compare with C&amp;W and word2vec as they have been proved effective in many NLP tasks. The trade-off parameter of ReEmb (Labutov and Lipson, 2013) is tuned on the development set of SemEval 2013. Table 3 shows the performance on the positive/negative classification of tweets5. ReEmb(C&amp;W) and ReEmb(w2v) stand for the use of embeddings learned from 10 million distantsupervised tweets with C&amp;W and word2vec, respectively. Each row of Table 3 represents a word embedding learning algori</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Enhanced sentiment learning using twitter hashtags and smileys.</title>
<date>2010</date>
<booktitle>In Proceedings of International Conference on Computational Linguistics,</booktitle>
<pages>241--249</pages>
<contexts>
<context position="7531" citStr="Davidov et al., 2010" startWordPosition="1143" endWordPosition="1146">t classification approaches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision (Go et al., 2009). Instead of directly using the distantsupervised data as training set, Liu et al. (2012) adopt the tweets with emoticons to smooth the language model and Hu et al. (2013) incorporate the emotional signals into an unsupervised learning framework for Twitter sentiment classification. Many existing learning based methods on Twitter senti</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Enhanced sentiment learning using twitter hashtags and smileys. In Proceedings of International Conference on Computational Linguistics, pages 241– 249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaowen Ding</author>
<author>Bing Liu</author>
<author>Philip S Yu</author>
</authors>
<title>A holistic lexicon-based approach to opinion mining.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Web Search and Data Mining,</booktitle>
<pages>231--240</pages>
<contexts>
<context position="7001" citStr="Ding et al., 2008" startWordPosition="1062" endWordPosition="1065">d Work In this section, we present a brief review of the related work from two perspectives, Twitter sentiment classification and learning continuous representations for sentiment classification. 2.1 Twitter Sentiment Classification Twitter sentiment classification, which identifies the sentiment polarity of short, informal tweets, has attracted increasing research interest (Jiang et al., 2011; Hu et al., 2013) in recent years. Generally, the methods employed in Twitter sentiment classification follow traditional sentiment classification approaches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) </context>
</contexts>
<marker>Ding, Liu, Yu, 2008</marker>
<rawString>Xiaowen Ding, Bing Liu, and Philip S Yu. 2008. A holistic lexicon-based approach to opinion mining. In Proceedings of the International Conference on Web Search and Data Mining, pages 231–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>2121--2159</pages>
<contexts>
<context position="18722" citStr="Duchi et al., 2011" startWordPosition="2983" endWordPosition="2986">tant-supervised tweets collected with positive and negative emoticons1. We crawl tweets from April 1st, 2013 to April 30th, 2013 with TwitterAPI. We tokenize each tweet with TwitterNLP (Gimpel et al., 2011), remove the @user and URLs of each tweet, and filter the tweets that are too short (&lt; 7 words). Finally, we collect 10M tweets, selected by 5M tweets with positive emoticons and 5M tweets with negative emoticons. We train SSWEh, SSWEr and SSWEu by taking the derivative of the loss through backpropagation with respect to the whole set of parameters (Collobert et al., 2011), and use AdaGrad (Duchi et al., 2011) to update the parameters. We empirically set the window size as 3, the embedding length as 50, the length of hidden layer as 20 and the learning rate of AdaGrad as 0.1 for all baseline and our models. We learn embedding for unigrams, bigrams and trigrams separately with same neural network and same parameter setting. The contexts of unigram (bigram/trigram) are the surrounding unigrams (bigrams/trigrams), respectively. 3.3 Twitter Sentiment Classification We apply sentiment-specific word embedding for Twitter sentiment classification under a supervised learning framework as in previous work (</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, pages 2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="21853" citStr="Fan et al., 2008" startWordPosition="3486" endWordPosition="3489">lexicons. 4.1 Twitter Sentiment Classification Experiment Setup and Datasets. We conduct experiments on the latest Twitter sentiment classification benchmark dataset in SemEval 2013 (Nakov et al., 2013). The training and development sets were completely in full to task participants. However, we were unable to download all the training and development sets because some tweets were deleted or not available due to modified authorization status. The test set is directly provided to the participants. The distribution of our dataset is given in Table 1. We train sentiment classifier with LibLinear (Fan et al., 2008) on the training set, tune parameter −c on the dev set and evaluate on the test set. Evaluation metric is the Macro-F1 of positive and negative categories 2. Positive Negative Neutral Total Train 2,642 994 3,436 7,072 Dev 408 219 493 1,120 Test 1,570 601 1,639 3,810 Table 1: Statistics of the SemEval 2013 Twitter sentiment classification dataset. 2We investigate 2-class Twitter sentiment classification (positive/negative) instead of 3-class Twitter sentiment classification (positive/negative/neutral) in SemEval2013. Baseline Methods. We compare our method with the following sentiment classific</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. The Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronen Feldman</author>
</authors>
<title>Techniques and applications for sentiment analysis.</title>
<date>2013</date>
<journal>Communications of the ACM,</journal>
<volume>56</volume>
<issue>4</issue>
<contexts>
<context position="9143" citStr="Feldman, 2013" startWordPosition="1389" endWordPosition="1390"> of hand-crafted features. Unlike the previous studies, we focus on learning discriminative features automatically from massive distant-supervised tweets. 1556 2.2 Learning Continuous Representations for Sentiment Classification Pang et al. (2002) pioneer this field by using bagof-word representation, representing each word as a one-hot vector. It has the same length as the size of the vocabulary, and only one dimension is 1, with all others being 0. Under this assumption, many feature learning algorithms are proposed to obtain better classification performance (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). However, the one-hot word representation cannot sufficiently capture the complex linguistic characteristics of words. With the revival of interest in deep learning (Bengio et al., 2013), incorporating the continuous representation of a word as features has been proving effective in a variety of NLP tasks, such as parsing (Socher et al., 2013a), language modeling (Bengio et al., 2003; Mnih and Hinton, 2009) and NER (Turian et al., 2010). In the field of sentiment analysis, Bespalov et al. (2011; 2012) initialize the word embedding by Latent Semantic Analysis and further represent each documen</context>
</contexts>
<marker>Feldman, 2013</marker>
<rawString>Ronen Feldman. 2013. Techniques and applications for sentiment analysis. Communications of the ACM, 56(4):82–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for twitter: Annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>42--47</pages>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for twitter: Annotation, features, and experiments. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 42–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Glorot</author>
<author>Antoine Bordes</author>
<author>Yoshua Bengio</author>
</authors>
<title>Domain adaptation for large-scale sentiment classification: A deep learning approach.</title>
<date>2011</date>
<booktitle>Proceedings of International Conference on Machine Learning.</booktitle>
<contexts>
<context position="9949" citStr="Glorot et al. (2011)" startWordPosition="1517" endWordPosition="1520">), incorporating the continuous representation of a word as features has been proving effective in a variety of NLP tasks, such as parsing (Socher et al., 2013a), language modeling (Bengio et al., 2003; Mnih and Hinton, 2009) and NER (Turian et al., 2010). In the field of sentiment analysis, Bespalov et al. (2011; 2012) initialize the word embedding by Latent Semantic Analysis and further represent each document as the linear weighted of ngram vectors for sentiment classification. Yessenalina and Cardie (2011) model each word as a matrix and combine words using iterated matrix multiplication. Glorot et al. (2011) explore Stacked Denoising Autoencoders for domain adaptation in sentiment classification. Socher et al. propose Recursive Neural Network (RNN) (2011b), matrixvector RNN (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the compositionality of phrases of any length based on the representation of each pair of children recursively. Hermann et al. (2013) present Combinatory Categorial Autoencoders to learn the compositionality of sentence, which marries the Combinatory Categorial Grammar with Recursive Autoencoder. The representation of words heavily relies on the applications or</context>
</contexts>
<marker>Glorot, Bordes, Bengio, 2011</marker>
<rawString>Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Domain adaptation for large-scale sentiment classification: A deep learning approach. Proceedings of International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alec Go</author>
<author>Richa Bhayani</author>
<author>Lei Huang</author>
</authors>
<title>Twitter sentiment classification using distant supervision. CS224N Project Report,</title>
<date>2009</date>
<pages>1--12</pages>
<location>Stanford,</location>
<contexts>
<context position="7794" citStr="Go et al., 2009" startWordPosition="1185" endWordPosition="1188">to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision (Go et al., 2009). Instead of directly using the distantsupervised data as training set, Liu et al. (2012) adopt the tweets with emoticons to smooth the language model and Hu et al. (2013) incorporate the emotional signals into an unsupervised learning framework for Twitter sentiment classification. Many existing learning based methods on Twitter sentiment classification focus on feature engineering. The reason is that the performance of sentiment classifier being heavily dependent on the choice of feature representation of tweets. The most representative system is introduced by Mohammad et al. (2013), which i</context>
<context position="22659" citStr="Go et al., 2009" startWordPosition="3607" endWordPosition="3610">Train 2,642 994 3,436 7,072 Dev 408 219 493 1,120 Test 1,570 601 1,639 3,810 Table 1: Statistics of the SemEval 2013 Twitter sentiment classification dataset. 2We investigate 2-class Twitter sentiment classification (positive/negative) instead of 3-class Twitter sentiment classification (positive/negative/neutral) in SemEval2013. Baseline Methods. We compare our method with the following sentiment classification algorithms: (1) DistSuper: We use the 10 million tweets selected by positive and negative emoticons as training data, and build sentiment classifier with LibLinear and ngram features (Go et al., 2009). (2) SVM: The ngram features and Support Vector Machine are widely used baseline methods to build sentiment classifiers (Pang et al., 2002). LibLinear is used to train the SVM classifier. (3) NBSVM: NBSVM (Wang and Manning, 2012) is a state-of-the-art performer on many sentiment classification datasets, which trades-off between Naive Bayes and NB-enhanced SVM. (4) RAE: Recursive Autoencoder (Socher et al., 2011c) has been proven effective in many sentiment analysis tasks by learning compositionality automatically. We run RAE with randomly initialized word embedding. (5) NRC: NRC builds the to</context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>The role of syntax in vector space models of compositional semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>894--904</pages>
<marker>Hermann, Blunsom, 2013</marker>
<rawString>Karl Moritz Hermann and Phil Blunsom. 2013. The role of syntax in vector space models of compositional semantics. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 894–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the tenth ACM SIGKDD Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>168--177</pages>
<contexts>
<context position="33271" citStr="Hu and Liu, 2004" startWordPosition="5333" endWordPosition="5336">N Accuracy Ei=1 Ej=1 Q(wi, cij) 10 y #Lex × N ( ) where #Lex is the number of words in the sentiment lexicon, wi is the i-th word in the lexicon, cij is the j-th closest word to wi in the lexicon with cosine similarity, Q(wi, cij) is an indicator function that is equal to 1 if wi and cij have the same sentiment polarity and 0 for the opposite case. The higher accuracy refers to a better polarity consistency of words in the sentiment lexicon. We set N as 100 in our experiment. Experiment Setup and Datasets We utilize the widely-used sentiment lexicons, namely MPQA (Wilson et al., 2005) and HL (Hu and Liu, 2004), to evaluate the quality of word embedding. For each lexicon, we remove the words that do not appear in the lookup table of word embedding. We only use unigram embedding in this section because these sentiment lexicons do not contain phrases. The distribution of the lexicons used in this paper is listed in Table 4. Lexicon Positive Negative Total HL 1,331 2,647 3,978 MPQA 1,932 2,817 4,749 Joint 1,051 2,024 3,075 Table 4: Statistics of the sentiment lexicons. Joint stands for the words that occur in both HL and MPQA with the same sentiment polarity. Results. Table 5 shows our results compared</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Ming Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 168–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xia Hu</author>
<author>Jiliang Tang</author>
<author>Huiji Gao</author>
<author>Huan Liu</author>
</authors>
<title>Unsupervised sentiment analysis with emotional signals.</title>
<date>2013</date>
<booktitle>In Proceedings of the International World Wide Web Conference,</booktitle>
<pages>607--618</pages>
<contexts>
<context position="1813" citStr="Hu et al., 2013" startWordPosition="262" endWordPosition="265">s. To obtain large scale training corpora, we learn the sentiment-specific word embedding from massive distant-supervised tweets collected by positive and negative emoticons. Experiments on applying SSWE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenating SSWE with existing feature set. 1 Introduction Twitter sentiment classification has attracted increasing research interest in recent years (Jiang et al., 2011; Hu et al., 2013). The objective is to classify the sentiment polarity of a tweet as positive, ∗ This work was done when the first and third authors were visiting Microsoft Research Asia. negative or neutral. The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on designing effective features to obtain better classification performance. For example, Mohammad et al. (2013) build the top-performed system in the Twitter sentiment classification track</context>
<context position="6798" citStr="Hu et al., 2013" startWordPosition="1035" endWordPosition="1038">he top-performed system in SemEval 2013; • We release the sentiment-specific word embedding learned from 10 million tweets, which can be adopted off-the-shell in other sentiment analysis tasks. 2 Related Work In this section, we present a brief review of the related work from two perspectives, Twitter sentiment classification and learning continuous representations for sentiment classification. 2.1 Twitter Sentiment Classification Twitter sentiment classification, which identifies the sentiment polarity of short, informal tweets, has attracted increasing research interest (Jiang et al., 2011; Hu et al., 2013) in recent years. Generally, the methods employed in Twitter sentiment classification follow traditional sentiment classification approaches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a spec</context>
<context position="20165" citStr="Hu et al. (2013)" startWordPosition="3206" endWordPosition="3209"> polarity. We explore min, average and max convolutional layers (Collobert et al., 2011; Socher et al., 2011a), which have been used as simple and effective methods for compositionality learning in vector-based semantics (Mitchell and Lapata, 2010), to obtain the tweet representation. The result is the concatenation of vectors derived from different convolutional layers. z(tw) = [zmax(tw), zmin(tw), zaverage(tw)] where z(tw) is the representation of tweet tw and zx(tw) is the results of the convolutional layer x E {min, max, average}. Each convolutional layer 1We use the emoticons selected by Hu et al. (2013). The positive emoticons are :) : ) :-) :D =), and the negative emoticons are :( : ( :-( . lossr(t) = max(0,1 − δs(t)fr0 (t) + δs(t)fr 1 (t) ) (6) lossu(t, tr) = α · losscw(t, tr)+ (1 − α) · lossus(t, tr) (8) lossus(t, tr) = max(0,1 − δs(t)fu1 (t) + δs(t)fu1 (tr) ) (9) 1559 zx employs the embedding of unigrams, bigrams and trigrams separately and conducts the matrixvector operation of x on the sequence represented by columns in each lookup table. The output of zx is the concatenation of results obtained from different lookup tables. zx(tw) = [wx(Luni)tw,wx(Lbi)tw,wx(Ltri)twl where wx is the co</context>
</contexts>
<marker>Hu, Tang, Gao, Liu, 2013</marker>
<rawString>Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu. 2013. Unsupervised sentiment analysis with emotional signals. In Proceedings of the International World Wide Web Conference, pages 607–618.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Jiang</author>
<author>Mo Yu</author>
<author>Ming Zhou</author>
<author>Xiaohua Liu</author>
<author>Tiejun Zhao</author>
</authors>
<title>Target-dependent twitter sentiment classification.</title>
<date>2011</date>
<booktitle>The Proceeding of Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--151</pages>
<contexts>
<context position="1795" citStr="Jiang et al., 2011" startWordPosition="258" endWordPosition="261"> their loss functions. To obtain large scale training corpora, we learn the sentiment-specific word embedding from massive distant-supervised tweets collected by positive and negative emoticons. Experiments on applying SSWE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenating SSWE with existing feature set. 1 Introduction Twitter sentiment classification has attracted increasing research interest in recent years (Jiang et al., 2011; Hu et al., 2013). The objective is to classify the sentiment polarity of a tweet as positive, ∗ This work was done when the first and third authors were visiting Microsoft Research Asia. negative or neutral. The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on designing effective features to obtain better classification performance. For example, Mohammad et al. (2013) build the top-performed system in the Twitter sentiment cl</context>
<context position="6780" citStr="Jiang et al., 2011" startWordPosition="1031" endWordPosition="1034">rafted features in the top-performed system in SemEval 2013; • We release the sentiment-specific word embedding learned from 10 million tweets, which can be adopted off-the-shell in other sentiment analysis tasks. 2 Related Work In this section, we present a brief review of the related work from two perspectives, Twitter sentiment classification and learning continuous representations for sentiment classification. 2.1 Twitter Sentiment Classification Twitter sentiment classification, which identifies the sentiment polarity of short, informal tweets, has attracted increasing research interest (Jiang et al., 2011; Hu et al., 2013) in recent years. Generally, the methods employed in Twitter sentiment classification follow traditional sentiment classification approaches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification </context>
</contexts>
<marker>Jiang, Yu, Zhou, Liu, Zhao, 2011</marker>
<rawString>Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent twitter sentiment classification. The Proceeding of Annual Meeting of the Association for Computational Linguistics, 1:151–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efthymios Kouloumpis</author>
<author>Theresa Wilson</author>
<author>Johanna Moore</author>
</authors>
<title>Twitter sentiment analysis: The good the bad and the omg!</title>
<date>2011</date>
<booktitle>In The International AAAI Conference on Weblogs and Social Media.</booktitle>
<contexts>
<context position="7580" citStr="Kouloumpis et al., 2011" startWordPosition="1152" endWordPosition="1155"> approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision (Go et al., 2009). Instead of directly using the distantsupervised data as training set, Liu et al. (2012) adopt the tweets with emoticons to smooth the language model and Hu et al. (2013) incorporate the emotional signals into an unsupervised learning framework for Twitter sentiment classification. Many existing learning based methods on Twitter sentiment classification focus on feature engineering.</context>
</contexts>
<marker>Kouloumpis, Wilson, Moore, 2011</marker>
<rawString>Efthymios Kouloumpis, Theresa Wilson, and Johanna Moore. 2011. Twitter sentiment analysis: The good the bad and the omg! In The International AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Labutov</author>
<author>Hod Lipson</author>
</authors>
<title>Re-embedding words.</title>
<date>2013</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="10602" citStr="Labutov and Lipson, 2013" startWordPosition="1615" endWordPosition="1619">utoencoders for domain adaptation in sentiment classification. Socher et al. propose Recursive Neural Network (RNN) (2011b), matrixvector RNN (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the compositionality of phrases of any length based on the representation of each pair of children recursively. Hermann et al. (2013) present Combinatory Categorial Autoencoders to learn the compositionality of sentence, which marries the Combinatory Categorial Grammar with Recursive Autoencoder. The representation of words heavily relies on the applications or tasks in which it is used (Labutov and Lipson, 2013). This paper focuses on learning sentiment-specific word embedding, which is tailored for sentiment analysis. Unlike Maas et al. (2011) that follow the probabilistic document model (Blei et al., 2003) and give an sentiment predictor function to each word, we develop neural networks and map each ngram to the sentiment polarity of sentence. Unlike Socher et al. (2011c) that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word embedding, from massive distant-supervised tweets. Unlike Labutov and</context>
<context position="26920" citStr="Labutov and Lipson, 2013" startWordPosition="4275" endWordPosition="4278">ation. Comparision between Different Word Embedding. We compare sentiment-specific word embedding (SSWEh, SSWE,, SSWE,,) with baseline embedding learning algorithms by only using word embedding as features for Twitter sentiment classification. We use the embedding of unigrams, bigrams and trigrams in the experiment. The embeddings of C&amp;W (Collobert et al., 2011), word2vec4, WVSA (Maas et al., 2011) and our models are trained with the same dataset and same parameter setting. We compare with C&amp;W and word2vec as they have been proved effective in many NLP tasks. The trade-off parameter of ReEmb (Labutov and Lipson, 2013) is tuned on the development set of SemEval 2013. Table 3 shows the performance on the positive/negative classification of tweets5. ReEmb(C&amp;W) and ReEmb(w2v) stand for the use of embeddings learned from 10 million distantsupervised tweets with C&amp;W and word2vec, respectively. Each row of Table 3 represents a word embedding learning algorithm. Each column stands for a type of embedding used to compose features of tweets. The column uni+bi denotes the use of unigram and bigram embedding, and the column uni+bi+tri indicates the use of unigram, bigram and trigram embedding. Embedding unigram uni+bi</context>
</contexts>
<marker>Labutov, Lipson, 2013</marker>
<rawString>Igor Labutov and Hod Lipson. 2013. Re-embedding words. In Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kun-Lin Liu</author>
<author>Wu-Jun Li</author>
<author>Minyi Guo</author>
</authors>
<title>Emoticon smoothed language models for twitter sentiment analysis.</title>
<date>2012</date>
<booktitle>In The Association for the Advancement of Artificial Intelligence.</booktitle>
<contexts>
<context position="7883" citStr="Liu et al. (2012)" startWordPosition="1200" endWordPosition="1203">thods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision (Go et al., 2009). Instead of directly using the distantsupervised data as training set, Liu et al. (2012) adopt the tweets with emoticons to smooth the language model and Hu et al. (2013) incorporate the emotional signals into an unsupervised learning framework for Twitter sentiment classification. Many existing learning based methods on Twitter sentiment classification focus on feature engineering. The reason is that the performance of sentiment classifier being heavily dependent on the choice of feature representation of tweets. The most representative system is introduced by Mohammad et al. (2013), which is the state-of-theart system (the top-performed system in SemEval 2013 Twitter Sentiment </context>
</contexts>
<marker>Liu, Li, Guo, 2012</marker>
<rawString>Kun-Lin Liu, Wu-Jun Li, and Minyi Guo. 2012. Emoticon smoothed language models for twitter sentiment analysis. In The Association for the Advancement of Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment analysis and opinion mining.</title>
<date>2012</date>
<journal>Synthesis Lectures on Human Language Technologies,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="9127" citStr="Liu, 2012" startWordPosition="1387" endWordPosition="1388">ng a number of hand-crafted features. Unlike the previous studies, we focus on learning discriminative features automatically from massive distant-supervised tweets. 1556 2.2 Learning Continuous Representations for Sentiment Classification Pang et al. (2002) pioneer this field by using bagof-word representation, representing each word as a one-hot vector. It has the same length as the size of the vocabulary, and only one dimension is 1, with all others being 0. Under this assumption, many feature learning algorithms are proposed to obtain better classification performance (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). However, the one-hot word representation cannot sufficiently capture the complex linguistic characteristics of words. With the revival of interest in deep learning (Bengio et al., 2013), incorporating the continuous representation of a word as features has been proving effective in a variety of NLP tasks, such as parsing (Socher et al., 2013a), language modeling (Bengio et al., 2003; Mnih and Hinton, 2009) and NER (Turian et al., 2010). In the field of sentiment analysis, Bespalov et al. (2011; 2012) initialize the word embedding by Latent Semantic Analysis and further repres</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment analysis and opinion mining. Synthesis Lectures on Human Language Technologies, 5(1):1–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew L Maas</author>
<author>Raymond E Daly</author>
<author>Peter T Pham</author>
<author>Dan Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Learning word vectors for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="10737" citStr="Maas et al. (2011)" startWordPosition="1636" endWordPosition="1639">(2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the compositionality of phrases of any length based on the representation of each pair of children recursively. Hermann et al. (2013) present Combinatory Categorial Autoencoders to learn the compositionality of sentence, which marries the Combinatory Categorial Grammar with Recursive Autoencoder. The representation of words heavily relies on the applications or tasks in which it is used (Labutov and Lipson, 2013). This paper focuses on learning sentiment-specific word embedding, which is tailored for sentiment analysis. Unlike Maas et al. (2011) that follow the probabilistic document model (Blei et al., 2003) and give an sentiment predictor function to each word, we develop neural networks and map each ngram to the sentiment polarity of sentence. Unlike Socher et al. (2011c) that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word embedding, from massive distant-supervised tweets. Unlike Labutov and Lipson (2013) that produce task-specific embedding from an existing word embedding, we learn sentiment-specific word embedding from sc</context>
<context position="26696" citStr="Maas et al., 2011" startWordPosition="4237" endWordPosition="4240"> on subjective classification of tweets, and obtain 72.17% in macro-F1 by using only SSWE,, as feature. After combining SSWE,, with the feature set of NRC, we improve NRC from 74.86% to 75.39% for subjective classification. Comparision between Different Word Embedding. We compare sentiment-specific word embedding (SSWEh, SSWE,, SSWE,,) with baseline embedding learning algorithms by only using word embedding as features for Twitter sentiment classification. We use the embedding of unigrams, bigrams and trigrams in the experiment. The embeddings of C&amp;W (Collobert et al., 2011), word2vec4, WVSA (Maas et al., 2011) and our models are trained with the same dataset and same parameter setting. We compare with C&amp;W and word2vec as they have been proved effective in many NLP tasks. The trade-off parameter of ReEmb (Labutov and Lipson, 2013) is tuned on the development set of SemEval 2013. Table 3 shows the performance on the positive/negative classification of tweets5. ReEmb(C&amp;W) and ReEmb(w2v) stand for the use of embeddings learned from 10 million distantsupervised tweets with C&amp;W and word2vec, respectively. Each row of Table 3 represents a word embedding learning algorithm. Each column stands for a type of</context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>The Conference on Neural Information Processing Systems.</booktitle>
<contexts>
<context position="3258" citStr="Mikolov et al., 2013" startWordPosition="488" endWordPosition="491">rom the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011). Accordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word. Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classification. The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text. As a result, words with opposite polarity, such as good and bad, are mapped into close vectors. It is meaningful for some tasks such as pos-tagging (Zheng et al., 2013) as the two words have similar usages and grammatical roles, but it becomes a disaster for sentiment analysis as they have the opposite sentiment polarity. In this paper, we propose lea</context>
<context position="29861" citStr="Mikolov et al. (2013)" startWordPosition="4739" endWordPosition="4742">3, we can see that the bigram and trigram embeddings consistently improve the performance of Twitter sentiment classification. The underlying reason is that a phrase, which cannot be accurately represented by unigram embedding, is directly encoded into the ngram embedding as an idiomatic unit. A typical case in sentiment analysis is that the composed phrase and multiword expression may have a different sentiment polarity than the individual words it contains, such as not [bad] and [great] deal of (the word in the bracket has different sentiment polarity with the ngram). A very recent study by Mikolov et al. (2013) also verified the effectiveness of phrase embedding for analogically reasoning phrases. Effect of α in SSWE,, We tune the hyperparameter α of SSWE,, on the development set by using unigram embedding as features. As given in Equation 8, α is the weighting score of syntactic loss of SSWE,, and trades-off the syntactic and sentiment losses. SSWE,, is trained from 10 million distant-supervised tweets. Figure 2: Macro-F1 of SSWE,, on the development set of SemEval 2013 with different α. Figure 2 shows the macro-F1 of SSWE,, on positive/negative classification of tweets with different α on our deve</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. The Conference on Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="19797" citStr="Mitchell and Lapata, 2010" startWordPosition="3148" endWordPosition="3151">ssification We apply sentiment-specific word embedding for Twitter sentiment classification under a supervised learning framework as in previous work (Pang et al., 2002). Instead of hand-crafting features, we incorporate the continuous representation of words and phrases as the feature of a tweet. The sentiment classifier is built from tweets with manually annotated sentiment polarity. We explore min, average and max convolutional layers (Collobert et al., 2011; Socher et al., 2011a), which have been used as simple and effective methods for compositionality learning in vector-based semantics (Mitchell and Lapata, 2010), to obtain the tweet representation. The result is the concatenation of vectors derived from different convolutional layers. z(tw) = [zmax(tw), zmin(tw), zaverage(tw)] where z(tw) is the representation of tweet tw and zx(tw) is the results of the convolutional layer x E {min, max, average}. Each convolutional layer 1We use the emoticons selected by Hu et al. (2013). The positive emoticons are :) : ) :-) :D =), and the negative emoticons are :( : ( :-( . lossr(t) = max(0,1 − δs(t)fr0 (t) + δs(t)fr 1 (t) ) (6) lossu(t, tr) = α · losscw(t, tr)+ (1 − α) · lossus(t, tr) (8) lossus(t, tr) = max(0,1</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model.</title>
<date>2009</date>
<booktitle>In Advances in neural information processing systems,</booktitle>
<pages>1081--1088</pages>
<contexts>
<context position="9554" citStr="Mnih and Hinton, 2009" startWordPosition="1452" endWordPosition="1456">d only one dimension is 1, with all others being 0. Under this assumption, many feature learning algorithms are proposed to obtain better classification performance (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). However, the one-hot word representation cannot sufficiently capture the complex linguistic characteristics of words. With the revival of interest in deep learning (Bengio et al., 2013), incorporating the continuous representation of a word as features has been proving effective in a variety of NLP tasks, such as parsing (Socher et al., 2013a), language modeling (Bengio et al., 2003; Mnih and Hinton, 2009) and NER (Turian et al., 2010). In the field of sentiment analysis, Bespalov et al. (2011; 2012) initialize the word embedding by Latent Semantic Analysis and further represent each document as the linear weighted of ngram vectors for sentiment classification. Yessenalina and Cardie (2011) model each word as a matrix and combine words using iterated matrix multiplication. Glorot et al. (2011) explore Stacked Denoising Autoencoders for domain adaptation in sentiment classification. Socher et al. propose Recursive Neural Network (RNN) (2011b), matrixvector RNN (2012) and Recursive Neural Tensor </context>
</contexts>
<marker>Mnih, Hinton, 2009</marker>
<rawString>Andriy Mnih and Geoffrey E Hinton. 2009. A scalable hierarchical distributed language model. In Advances in neural information processing systems, pages 1081–1088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>Nrc-canada: Building the state-ofthe-art in sentiment analysis of tweets.</title>
<date>2013</date>
<booktitle>Proceedings of the International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="2336" citStr="Mohammad et al. (2013)" startWordPosition="345" endWordPosition="348">on has attracted increasing research interest in recent years (Jiang et al., 2011; Hu et al., 2013). The objective is to classify the sentiment polarity of a tweet as positive, ∗ This work was done when the first and third authors were visiting Microsoft Research Asia. negative or neutral. The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on designing effective features to obtain better classification performance. For example, Mohammad et al. (2013) build the top-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases </context>
<context position="8385" citStr="Mohammad et al. (2013)" startWordPosition="1276" endWordPosition="1280">t supervision (Go et al., 2009). Instead of directly using the distantsupervised data as training set, Liu et al. (2012) adopt the tweets with emoticons to smooth the language model and Hu et al. (2013) incorporate the emotional signals into an unsupervised learning framework for Twitter sentiment classification. Many existing learning based methods on Twitter sentiment classification focus on feature engineering. The reason is that the performance of sentiment classifier being heavily dependent on the choice of feature representation of tweets. The most representative system is introduced by Mohammad et al. (2013), which is the state-of-theart system (the top-performed system in SemEval 2013 Twitter Sentiment Classification Track) by implementing a number of hand-crafted features. Unlike the previous studies, we focus on learning discriminative features automatically from massive distant-supervised tweets. 1556 2.2 Learning Continuous Representations for Sentiment Classification Pang et al. (2002) pioneer this field by using bagof-word representation, representing each word as a one-hot vector. It has the same length as the size of the vocabulary, and only one dimension is 1, with all others being 0. U</context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif M Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. Nrc-canada: Building the state-ofthe-art in sentiment analysis of tweets. Proceedings of the International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Zornitsa Kozareva</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
<author>Theresa Wilson</author>
</authors>
<title>Semeval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation,</booktitle>
<volume>13</volume>
<contexts>
<context position="2450" citStr="Nakov et al., 2013" startWordPosition="362" endWordPosition="365"> to classify the sentiment polarity of a tweet as positive, ∗ This work was done when the first and third authors were visiting Microsoft Research Asia. negative or neutral. The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on designing effective features to obtain better classification performance. For example, Mohammad et al. (2013) build the top-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011). Accordingly, it is a crucial step to learn the w</context>
<context position="21438" citStr="Nakov et al., 2013" startWordPosition="3417" endWordPosition="3420">column vectors of the words in the tweet. Luni, Lbi and Ltri are the lookup tables of the unigram, bigram and trigram embedding, respectively. 4 Experiment We conduct experiments to evaluate SSWE by incorporating it into a supervised learning framework for Twitter sentiment classification. We also directly evaluate the effectiveness of the SSWE by measuring the word similarity in the embedding space for sentiment lexicons. 4.1 Twitter Sentiment Classification Experiment Setup and Datasets. We conduct experiments on the latest Twitter sentiment classification benchmark dataset in SemEval 2013 (Nakov et al., 2013). The training and development sets were completely in full to task participants. However, we were unable to download all the training and development sets because some tweets were deleted or not available due to modified authorization status. The test set is directly provided to the participants. The distribution of our dataset is given in Table 1. We train sentiment classifier with LibLinear (Fan et al., 2008) on the training set, tune parameter −c on the dev set and evaluate on the test set. Evaluation metric is the Macro-F1 of positive and negative categories 2. Positive Negative Neutral T</context>
</contexts>
<marker>Nakov, Rosenthal, Kozareva, Stoyanov, Ritter, Wilson, 2013</marker>
<rawString>Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, and Theresa Wilson. 2013. Semeval-2013 task 2: Sentiment analysis in twitter. In Proceedings of the International Workshop on Semantic Evaluation, volume 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Pak</author>
<author>Patrick Paroubek</author>
</authors>
<title>Twitter as a corpus for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings of Language Resources and Evaluation Conference,</booktitle>
<volume>volume</volume>
<contexts>
<context position="7509" citStr="Pak and Paroubek, 2010" startWordPosition="1139" endWordPosition="1142">low traditional sentiment classification approaches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision (Go et al., 2009). Instead of directly using the distantsupervised data as training set, Liu et al. (2012) adopt the tweets with emoticons to smooth the language model and Hu et al. (2013) incorporate the emotional signals into an unsupervised learning framework for Twitter sentiment classification. Many existing learning based me</context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Alexander Pak and Patrick Paroubek. 2010. Twitter as a corpus for sentiment analysis and opinion mining. In Proceedings of Language Resources and Evaluation Conference, volume 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis. Foundations and trends in information retrieval,</title>
<date>2008</date>
<pages>2--1</pages>
<contexts>
<context position="9116" citStr="Pang and Lee, 2008" startWordPosition="1383" endWordPosition="1386">Track) by implementing a number of hand-crafted features. Unlike the previous studies, we focus on learning discriminative features automatically from massive distant-supervised tweets. 1556 2.2 Learning Continuous Representations for Sentiment Classification Pang et al. (2002) pioneer this field by using bagof-word representation, representing each word as a one-hot vector. It has the same length as the size of the vocabulary, and only one dimension is 1, with all others being 0. Under this assumption, many feature learning algorithms are proposed to obtain better classification performance (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). However, the one-hot word representation cannot sufficiently capture the complex linguistic characteristics of words. With the revival of interest in deep learning (Bengio et al., 2013), incorporating the continuous representation of a word as features has been proving effective in a variety of NLP tasks, such as parsing (Socher et al., 2013a), language modeling (Bengio et al., 2003; Mnih and Hinton, 2009) and NER (Turian et al., 2010). In the field of sentiment analysis, Bespalov et al. (2011; 2012) initialize the word embedding by Latent Semantic Analysis and fur</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and trends in information retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="2066" citStr="Pang et al. (2002)" startWordPosition="306" endWordPosition="309"> dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenating SSWE with existing feature set. 1 Introduction Twitter sentiment classification has attracted increasing research interest in recent years (Jiang et al., 2011; Hu et al., 2013). The objective is to classify the sentiment polarity of a tweet as positive, ∗ This work was done when the first and third authors were visiting Microsoft Research Asia. negative or neutral. The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on designing effective features to obtain better classification performance. For example, Mohammad et al. (2013) build the top-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the lea</context>
<context position="7334" citStr="Pang et al. (2002)" startWordPosition="1114" endWordPosition="1117">, has attracted increasing research interest (Jiang et al., 2011; Hu et al., 2013) in recent years. Generally, the methods employed in Twitter sentiment classification follow traditional sentiment classification approaches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision (Go et al., 2009). Instead of directly using the distantsupervised data as training set, Liu et al. (2012) adopt the tweets with emoticons to smooth the lang</context>
<context position="8776" citStr="Pang et al. (2002)" startWordPosition="1328" endWordPosition="1331">n feature engineering. The reason is that the performance of sentiment classifier being heavily dependent on the choice of feature representation of tweets. The most representative system is introduced by Mohammad et al. (2013), which is the state-of-theart system (the top-performed system in SemEval 2013 Twitter Sentiment Classification Track) by implementing a number of hand-crafted features. Unlike the previous studies, we focus on learning discriminative features automatically from massive distant-supervised tweets. 1556 2.2 Learning Continuous Representations for Sentiment Classification Pang et al. (2002) pioneer this field by using bagof-word representation, representing each word as a one-hot vector. It has the same length as the size of the vocabulary, and only one dimension is 1, with all others being 0. Under this assumption, many feature learning algorithms are proposed to obtain better classification performance (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). However, the one-hot word representation cannot sufficiently capture the complex linguistic characteristics of words. With the revival of interest in deep learning (Bengio et al., 2013), incorporating the continuous representation </context>
<context position="19340" citStr="Pang et al., 2002" startWordPosition="3079" endWordPosition="3082"> to update the parameters. We empirically set the window size as 3, the embedding length as 50, the length of hidden layer as 20 and the learning rate of AdaGrad as 0.1 for all baseline and our models. We learn embedding for unigrams, bigrams and trigrams separately with same neural network and same parameter setting. The contexts of unigram (bigram/trigram) are the surrounding unigrams (bigrams/trigrams), respectively. 3.3 Twitter Sentiment Classification We apply sentiment-specific word embedding for Twitter sentiment classification under a supervised learning framework as in previous work (Pang et al., 2002). Instead of hand-crafting features, we incorporate the continuous representation of words and phrases as the feature of a tweet. The sentiment classifier is built from tweets with manually annotated sentiment polarity. We explore min, average and max convolutional layers (Collobert et al., 2011; Socher et al., 2011a), which have been used as simple and effective methods for compositionality learning in vector-based semantics (Mitchell and Lapata, 2010), to obtain the tweet representation. The result is the concatenation of vectors derived from different convolutional layers. z(tw) = [zmax(tw)</context>
<context position="22799" citStr="Pang et al., 2002" startWordPosition="3630" endWordPosition="3633">ssification dataset. 2We investigate 2-class Twitter sentiment classification (positive/negative) instead of 3-class Twitter sentiment classification (positive/negative/neutral) in SemEval2013. Baseline Methods. We compare our method with the following sentiment classification algorithms: (1) DistSuper: We use the 10 million tweets selected by positive and negative emoticons as training data, and build sentiment classifier with LibLinear and ngram features (Go et al., 2009). (2) SVM: The ngram features and Support Vector Machine are widely used baseline methods to build sentiment classifiers (Pang et al., 2002). LibLinear is used to train the SVM classifier. (3) NBSVM: NBSVM (Wang and Manning, 2012) is a state-of-the-art performer on many sentiment classification datasets, which trades-off between Naive Bayes and NB-enhanced SVM. (4) RAE: Recursive Autoencoder (Socher et al., 2011c) has been proven effective in many sentiment analysis tasks by learning compositionality automatically. We run RAE with randomly initialized word embedding. (5) NRC: NRC builds the top-performed system in SemEval 2013 Twitter sentiment classification track which incorporates diverse sentiment lexicons and many manually de</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennington</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>The Conference on Neural Information Processing Systems,</booktitle>
<pages>24--801</pages>
<contexts>
<context position="10969" citStr="Socher et al. (2011" startWordPosition="1677" endWordPosition="1680"> Autoencoders to learn the compositionality of sentence, which marries the Combinatory Categorial Grammar with Recursive Autoencoder. The representation of words heavily relies on the applications or tasks in which it is used (Labutov and Lipson, 2013). This paper focuses on learning sentiment-specific word embedding, which is tailored for sentiment analysis. Unlike Maas et al. (2011) that follow the probabilistic document model (Blei et al., 2003) and give an sentiment predictor function to each word, we develop neural networks and map each ngram to the sentiment polarity of sentence. Unlike Socher et al. (2011c) that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word embedding, from massive distant-supervised tweets. Unlike Labutov and Lipson (2013) that produce task-specific embedding from an existing word embedding, we learn sentiment-specific word embedding from scratch. 3 Sentiment-Specific Word Embedding for Twitter Sentiment Classification In this section, we present the details of learning sentiment-specific word embedding (SSWE) for Twitter sentiment classification. We propose incorporat</context>
<context position="19657" citStr="Socher et al., 2011" startWordPosition="3129" endWordPosition="3132">ng. The contexts of unigram (bigram/trigram) are the surrounding unigrams (bigrams/trigrams), respectively. 3.3 Twitter Sentiment Classification We apply sentiment-specific word embedding for Twitter sentiment classification under a supervised learning framework as in previous work (Pang et al., 2002). Instead of hand-crafting features, we incorporate the continuous representation of words and phrases as the feature of a tweet. The sentiment classifier is built from tweets with manually annotated sentiment polarity. We explore min, average and max convolutional layers (Collobert et al., 2011; Socher et al., 2011a), which have been used as simple and effective methods for compositionality learning in vector-based semantics (Mitchell and Lapata, 2010), to obtain the tweet representation. The result is the concatenation of vectors derived from different convolutional layers. z(tw) = [zmax(tw), zmin(tw), zaverage(tw)] where z(tw) is the representation of tweet tw and zx(tw) is the results of the convolutional layer x E {min, max, average}. Each convolutional layer 1We use the emoticons selected by Hu et al. (2013). The positive emoticons are :) : ) :-) :D =), and the negative emoticons are :( : ( :-( . l</context>
<context position="23074" citStr="Socher et al., 2011" startWordPosition="3673" endWordPosition="3676">ion algorithms: (1) DistSuper: We use the 10 million tweets selected by positive and negative emoticons as training data, and build sentiment classifier with LibLinear and ngram features (Go et al., 2009). (2) SVM: The ngram features and Support Vector Machine are widely used baseline methods to build sentiment classifiers (Pang et al., 2002). LibLinear is used to train the SVM classifier. (3) NBSVM: NBSVM (Wang and Manning, 2012) is a state-of-the-art performer on many sentiment classification datasets, which trades-off between Naive Bayes and NB-enhanced SVM. (4) RAE: Recursive Autoencoder (Socher et al., 2011c) has been proven effective in many sentiment analysis tasks by learning compositionality automatically. We run RAE with randomly initialized word embedding. (5) NRC: NRC builds the top-performed system in SemEval 2013 Twitter sentiment classification track which incorporates diverse sentiment lexicons and many manually designed features. We re-implement this system because the codes are not publicly available 3. NRC-ngram refers to the feature set of NRC leaving out ngram features. Except for DistSuper, other baseline methods are conducted in a supervised manner. We do not compare with RNTN </context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric H Huang, Jeffrey Pennington, Andrew Y Ng, and Christopher D Manning. 2011a. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. The Conference on Neural Information Processing Systems, 24:801– 809.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff C Lin</author>
<author>Andrew Ng</author>
<author>Chris Manning</author>
</authors>
<title>Parsing natural scenes and natural language with recursive neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Conference on Machine Learning,</booktitle>
<pages>129--136</pages>
<contexts>
<context position="10969" citStr="Socher et al. (2011" startWordPosition="1677" endWordPosition="1680"> Autoencoders to learn the compositionality of sentence, which marries the Combinatory Categorial Grammar with Recursive Autoencoder. The representation of words heavily relies on the applications or tasks in which it is used (Labutov and Lipson, 2013). This paper focuses on learning sentiment-specific word embedding, which is tailored for sentiment analysis. Unlike Maas et al. (2011) that follow the probabilistic document model (Blei et al., 2003) and give an sentiment predictor function to each word, we develop neural networks and map each ngram to the sentiment polarity of sentence. Unlike Socher et al. (2011c) that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word embedding, from massive distant-supervised tweets. Unlike Labutov and Lipson (2013) that produce task-specific embedding from an existing word embedding, we learn sentiment-specific word embedding from scratch. 3 Sentiment-Specific Word Embedding for Twitter Sentiment Classification In this section, we present the details of learning sentiment-specific word embedding (SSWE) for Twitter sentiment classification. We propose incorporat</context>
<context position="19657" citStr="Socher et al., 2011" startWordPosition="3129" endWordPosition="3132">ng. The contexts of unigram (bigram/trigram) are the surrounding unigrams (bigrams/trigrams), respectively. 3.3 Twitter Sentiment Classification We apply sentiment-specific word embedding for Twitter sentiment classification under a supervised learning framework as in previous work (Pang et al., 2002). Instead of hand-crafting features, we incorporate the continuous representation of words and phrases as the feature of a tweet. The sentiment classifier is built from tweets with manually annotated sentiment polarity. We explore min, average and max convolutional layers (Collobert et al., 2011; Socher et al., 2011a), which have been used as simple and effective methods for compositionality learning in vector-based semantics (Mitchell and Lapata, 2010), to obtain the tweet representation. The result is the concatenation of vectors derived from different convolutional layers. z(tw) = [zmax(tw), zmin(tw), zaverage(tw)] where z(tw) is the representation of tweet tw and zx(tw) is the results of the convolutional layer x E {min, max, average}. Each convolutional layer 1We use the emoticons selected by Hu et al. (2013). The positive emoticons are :) : ) :-) :D =), and the negative emoticons are :( : ( :-( . l</context>
<context position="23074" citStr="Socher et al., 2011" startWordPosition="3673" endWordPosition="3676">ion algorithms: (1) DistSuper: We use the 10 million tweets selected by positive and negative emoticons as training data, and build sentiment classifier with LibLinear and ngram features (Go et al., 2009). (2) SVM: The ngram features and Support Vector Machine are widely used baseline methods to build sentiment classifiers (Pang et al., 2002). LibLinear is used to train the SVM classifier. (3) NBSVM: NBSVM (Wang and Manning, 2012) is a state-of-the-art performer on many sentiment classification datasets, which trades-off between Naive Bayes and NB-enhanced SVM. (4) RAE: Recursive Autoencoder (Socher et al., 2011c) has been proven effective in many sentiment analysis tasks by learning compositionality automatically. We run RAE with randomly initialized word embedding. (5) NRC: NRC builds the top-performed system in SemEval 2013 Twitter sentiment classification track which incorporates diverse sentiment lexicons and many manually designed features. We re-implement this system because the codes are not publicly available 3. NRC-ngram refers to the feature set of NRC leaving out ngram features. Except for DistSuper, other baseline methods are conducted in a supervised manner. We do not compare with RNTN </context>
</contexts>
<marker>Socher, Lin, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Cliff C Lin, Andrew Ng, and Chris Manning. 2011b. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the International Conference on Machine Learning, pages 129–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>J Pennington</author>
<author>E H Huang</author>
<author>A Y Ng</author>
<author>C D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<contexts>
<context position="10969" citStr="Socher et al. (2011" startWordPosition="1677" endWordPosition="1680"> Autoencoders to learn the compositionality of sentence, which marries the Combinatory Categorial Grammar with Recursive Autoencoder. The representation of words heavily relies on the applications or tasks in which it is used (Labutov and Lipson, 2013). This paper focuses on learning sentiment-specific word embedding, which is tailored for sentiment analysis. Unlike Maas et al. (2011) that follow the probabilistic document model (Blei et al., 2003) and give an sentiment predictor function to each word, we develop neural networks and map each ngram to the sentiment polarity of sentence. Unlike Socher et al. (2011c) that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word embedding, from massive distant-supervised tweets. Unlike Labutov and Lipson (2013) that produce task-specific embedding from an existing word embedding, we learn sentiment-specific word embedding from scratch. 3 Sentiment-Specific Word Embedding for Twitter Sentiment Classification In this section, we present the details of learning sentiment-specific word embedding (SSWE) for Twitter sentiment classification. We propose incorporat</context>
<context position="19657" citStr="Socher et al., 2011" startWordPosition="3129" endWordPosition="3132">ng. The contexts of unigram (bigram/trigram) are the surrounding unigrams (bigrams/trigrams), respectively. 3.3 Twitter Sentiment Classification We apply sentiment-specific word embedding for Twitter sentiment classification under a supervised learning framework as in previous work (Pang et al., 2002). Instead of hand-crafting features, we incorporate the continuous representation of words and phrases as the feature of a tweet. The sentiment classifier is built from tweets with manually annotated sentiment polarity. We explore min, average and max convolutional layers (Collobert et al., 2011; Socher et al., 2011a), which have been used as simple and effective methods for compositionality learning in vector-based semantics (Mitchell and Lapata, 2010), to obtain the tweet representation. The result is the concatenation of vectors derived from different convolutional layers. z(tw) = [zmax(tw), zmin(tw), zaverage(tw)] where z(tw) is the representation of tweet tw and zx(tw) is the results of the convolutional layer x E {min, max, average}. Each convolutional layer 1We use the emoticons selected by Hu et al. (2013). The positive emoticons are :) : ) :-) :D =), and the negative emoticons are :( : ( :-( . l</context>
<context position="23074" citStr="Socher et al., 2011" startWordPosition="3673" endWordPosition="3676">ion algorithms: (1) DistSuper: We use the 10 million tweets selected by positive and negative emoticons as training data, and build sentiment classifier with LibLinear and ngram features (Go et al., 2009). (2) SVM: The ngram features and Support Vector Machine are widely used baseline methods to build sentiment classifiers (Pang et al., 2002). LibLinear is used to train the SVM classifier. (3) NBSVM: NBSVM (Wang and Manning, 2012) is a state-of-the-art performer on many sentiment classification datasets, which trades-off between Naive Bayes and NB-enhanced SVM. (4) RAE: Recursive Autoencoder (Socher et al., 2011c) has been proven effective in many sentiment analysis tasks by learning compositionality automatically. We run RAE with randomly initialized word embedding. (5) NRC: NRC builds the top-performed system in SemEval 2013 Twitter sentiment classification track which incorporates diverse sentiment lexicons and many manually designed features. We re-implement this system because the codes are not publicly available 3. NRC-ngram refers to the feature set of NRC leaving out ngram features. Except for DistSuper, other baseline methods are conducted in a supervised manner. We do not compare with RNTN </context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, J. Pennington, E.H. Huang, A.Y. Ng, and C.D. Manning. 2011c. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Conference on Empirical Methods in Natural Language Processing, pages 151–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic Compositionality Through Recursive Matrix-Vector Spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic Compositionality Through Recursive Matrix-Vector Spaces. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing with compositional vector grammars.</title>
<date>2013</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2968" citStr="Socher et al., 2013" startWordPosition="443" endWordPosition="446">p-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011). Accordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word. Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classification. The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text. As a result, words with opposite polarity, such as good and bad</context>
<context position="9488" citStr="Socher et al., 2013" startWordPosition="1442" endWordPosition="1445">ector. It has the same length as the size of the vocabulary, and only one dimension is 1, with all others being 0. Under this assumption, many feature learning algorithms are proposed to obtain better classification performance (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). However, the one-hot word representation cannot sufficiently capture the complex linguistic characteristics of words. With the revival of interest in deep learning (Bengio et al., 2013), incorporating the continuous representation of a word as features has been proving effective in a variety of NLP tasks, such as parsing (Socher et al., 2013a), language modeling (Bengio et al., 2003; Mnih and Hinton, 2009) and NER (Turian et al., 2010). In the field of sentiment analysis, Bespalov et al. (2011; 2012) initialize the word embedding by Latent Semantic Analysis and further represent each document as the linear weighted of ngram vectors for sentiment classification. Yessenalina and Cardie (2011) model each word as a matrix and combine words using iterated matrix multiplication. Glorot et al. (2011) explore Stacked Denoising Autoencoders for domain adaptation in sentiment classification. Socher et al. propose Recursive Neural Network (</context>
<context position="23694" citStr="Socher et al., 2013" startWordPosition="3768" endWordPosition="3771">) has been proven effective in many sentiment analysis tasks by learning compositionality automatically. We run RAE with randomly initialized word embedding. (5) NRC: NRC builds the top-performed system in SemEval 2013 Twitter sentiment classification track which incorporates diverse sentiment lexicons and many manually designed features. We re-implement this system because the codes are not publicly available 3. NRC-ngram refers to the feature set of NRC leaving out ngram features. Except for DistSuper, other baseline methods are conducted in a supervised manner. We do not compare with RNTN (Socher et al., 2013b) because we cannot efficiently train the RNTN model. The reason lies in that the tweets in our dataset do not have accurately parsed results or fine grained sentiment labels for phrases. Another reason is that the RNTN model trained on movie reviews cannot be directly applied on tweets due to the differences between domains (Blitzer et al., 2007). Results and Analysis. Table 2 shows the macroF1 of the baseline systems as well as the SSWEbased methods on positive/negative sentiment classification of tweets. Distant supervision is relatively weak because the noisy-labeled tweets are treated as</context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. 2013a. Parsing with compositional vector grammars. In Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1631--1642</pages>
<contexts>
<context position="2968" citStr="Socher et al., 2013" startWordPosition="443" endWordPosition="446">p-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011). Accordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word. Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classification. The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text. As a result, words with opposite polarity, such as good and bad</context>
<context position="9488" citStr="Socher et al., 2013" startWordPosition="1442" endWordPosition="1445">ector. It has the same length as the size of the vocabulary, and only one dimension is 1, with all others being 0. Under this assumption, many feature learning algorithms are proposed to obtain better classification performance (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). However, the one-hot word representation cannot sufficiently capture the complex linguistic characteristics of words. With the revival of interest in deep learning (Bengio et al., 2013), incorporating the continuous representation of a word as features has been proving effective in a variety of NLP tasks, such as parsing (Socher et al., 2013a), language modeling (Bengio et al., 2003; Mnih and Hinton, 2009) and NER (Turian et al., 2010). In the field of sentiment analysis, Bespalov et al. (2011; 2012) initialize the word embedding by Latent Semantic Analysis and further represent each document as the linear weighted of ngram vectors for sentiment classification. Yessenalina and Cardie (2011) model each word as a matrix and combine words using iterated matrix multiplication. Glorot et al. (2011) explore Stacked Denoising Autoencoders for domain adaptation in sentiment classification. Socher et al. propose Recursive Neural Network (</context>
<context position="23694" citStr="Socher et al., 2013" startWordPosition="3768" endWordPosition="3771">) has been proven effective in many sentiment analysis tasks by learning compositionality automatically. We run RAE with randomly initialized word embedding. (5) NRC: NRC builds the top-performed system in SemEval 2013 Twitter sentiment classification track which incorporates diverse sentiment lexicons and many manually designed features. We re-implement this system because the codes are not publicly available 3. NRC-ngram refers to the feature set of NRC leaving out ngram features. Except for DistSuper, other baseline methods are conducted in a supervised manner. We do not compare with RNTN (Socher et al., 2013b) because we cannot efficiently train the RNTN model. The reason lies in that the tweets in our dataset do not have accurately parsed results or fine grained sentiment labels for phrases. Another reason is that the RNTN model trained on movie reviews cannot be directly applied on tweets due to the differences between domains (Blitzer et al., 2007). Results and Analysis. Table 2 shows the macroF1 of the baseline systems as well as the SSWEbased methods on positive/negative sentiment classification of tweets. Distant supervision is relatively weak because the noisy-labeled tweets are treated as</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013b. Recursive deep models for semantic compositionality over a sentiment treebank. In Conference on Empirical Methods in Natural Language Processing, pages 1631–1642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>Julian Brooke</author>
<author>Milan Tofiloski</author>
<author>Kimberly Voll</author>
<author>Manfred Stede</author>
</authors>
<title>Lexiconbased methods for sentiment analysis.</title>
<date>2011</date>
<journal>Computational linguistics,</journal>
<pages>37--2</pages>
<contexts>
<context position="7023" citStr="Taboada et al., 2011" startWordPosition="1066" endWordPosition="1069">ion, we present a brief review of the related work from two perspectives, Twitter sentiment classification and learning continuous representations for sentiment classification. 2.1 Twitter Sentiment Classification Twitter sentiment classification, which identifies the sentiment polarity of short, informal tweets, has attracted increasing research interest (Jiang et al., 2011; Hu et al., 2013) in recent years. Generally, the methods employed in Twitter sentiment classification follow traditional sentiment classification approaches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy</context>
</contexts>
<marker>Taboada, Brooke, Tofiloski, Voll, Stede, 2011</marker>
<rawString>Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly Voll, and Manfred Stede. 2011. Lexiconbased methods for sentiment analysis. Computational linguistics, 37(2):267–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Thelwall</author>
<author>Kevan Buckley</author>
<author>Georgios Paltoglou</author>
</authors>
<title>Sentiment strength detection for the social web.</title>
<date>2012</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>63</volume>
<issue>1</issue>
<contexts>
<context position="7047" citStr="Thelwall et al., 2012" startWordPosition="1070" endWordPosition="1073">f review of the related work from two perspectives, Twitter sentiment classification and learning continuous representations for sentiment classification. 2.1 Twitter Sentiment Classification Twitter sentiment classification, which identifies the sentiment polarity of short, informal tweets, has attracted increasing research interest (Jiang et al., 2011; Hu et al., 2013) in recent years. Generally, the methods employed in Twitter sentiment classification follow traditional sentiment classification approaches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy-labeled tweets selected</context>
</contexts>
<marker>Thelwall, Buckley, Paltoglou, 2012</marker>
<rawString>Mike Thelwall, Kevan Buckley, and Georgios Paltoglou. 2012. Sentiment strength detection for the social web. Journal of the American Society for Information Science and Technology, 63(1):163–173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning. Annual Meeting of the Association for Computational Linguistics.</title>
<date>2010</date>
<contexts>
<context position="9584" citStr="Turian et al., 2010" startWordPosition="1459" endWordPosition="1462">all others being 0. Under this assumption, many feature learning algorithms are proposed to obtain better classification performance (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). However, the one-hot word representation cannot sufficiently capture the complex linguistic characteristics of words. With the revival of interest in deep learning (Bengio et al., 2013), incorporating the continuous representation of a word as features has been proving effective in a variety of NLP tasks, such as parsing (Socher et al., 2013a), language modeling (Bengio et al., 2003; Mnih and Hinton, 2009) and NER (Turian et al., 2010). In the field of sentiment analysis, Bespalov et al. (2011; 2012) initialize the word embedding by Latent Semantic Analysis and further represent each document as the linear weighted of ngram vectors for sentiment classification. Yessenalina and Cardie (2011) model each word as a matrix and combine words using iterated matrix multiplication. Glorot et al. (2011) explore Stacked Denoising Autoencoders for domain adaptation in sentiment classification. Socher et al. propose Recursive Neural Network (RNN) (2011b), matrixvector RNN (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to lear</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>417--424</pages>
<contexts>
<context position="6982" citStr="Turney, 2002" startWordPosition="1060" endWordPosition="1061">asks. 2 Related Work In this section, we present a brief review of the related work from two perspectives, Twitter sentiment classification and learning continuous representations for sentiment classification. 2.1 Twitter Sentiment Classification Twitter sentiment classification, which identifies the sentiment polarity of short, informal tweets, has attracted increasing research interest (Jiang et al., 2011; Hu et al., 2013) in recent years. Generally, the methods employed in Twitter sentiment classification follow traditional sentiment classification approaches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; </context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D Turney. 2002. Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews. In Proceedings of Annual Meeting of the Association for Computational Linguistics, pages 417–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sida Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>Baselines and bigrams: Simple, good sentiment and topic classification.</title>
<date>2012</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>90--94</pages>
<contexts>
<context position="22889" citStr="Wang and Manning, 2012" startWordPosition="3646" endWordPosition="3649">ve/negative) instead of 3-class Twitter sentiment classification (positive/negative/neutral) in SemEval2013. Baseline Methods. We compare our method with the following sentiment classification algorithms: (1) DistSuper: We use the 10 million tweets selected by positive and negative emoticons as training data, and build sentiment classifier with LibLinear and ngram features (Go et al., 2009). (2) SVM: The ngram features and Support Vector Machine are widely used baseline methods to build sentiment classifiers (Pang et al., 2002). LibLinear is used to train the SVM classifier. (3) NBSVM: NBSVM (Wang and Manning, 2012) is a state-of-the-art performer on many sentiment classification datasets, which trades-off between Naive Bayes and NB-enhanced SVM. (4) RAE: Recursive Autoencoder (Socher et al., 2011c) has been proven effective in many sentiment analysis tasks by learning compositionality automatically. We run RAE with randomly initialized word embedding. (5) NRC: NRC builds the top-performed system in SemEval 2013 Twitter sentiment classification track which incorporates diverse sentiment lexicons and many manually designed features. We re-implement this system because the codes are not publicly available </context>
</contexts>
<marker>Wang, Manning, 2012</marker>
<rawString>Sida Wang and Christopher D Manning. 2012. Baselines and bigrams: Simple, good sentiment and topic classification. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 90–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="33245" citStr="Wilson et al., 2005" startWordPosition="5327" endWordPosition="5330"> the sentiment lexicon, #Lex N Accuracy Ei=1 Ej=1 Q(wi, cij) 10 y #Lex × N ( ) where #Lex is the number of words in the sentiment lexicon, wi is the i-th word in the lexicon, cij is the j-th closest word to wi in the lexicon with cosine similarity, Q(wi, cij) is an indicator function that is equal to 1 if wi and cij have the same sentiment polarity and 0 for the opposite case. The higher accuracy refers to a better polarity consistency of words in the sentiment lexicon. We set N as 100 in our experiment. Experiment Setup and Datasets We utilize the widely-used sentiment lexicons, namely MPQA (Wilson et al., 2005) and HL (Hu and Liu, 2004), to evaluate the quality of word embedding. For each lexicon, we remove the words that do not appear in the lookup table of word embedding. We only use unigram embedding in this section because these sentiment lexicons do not contain phrases. The distribution of the lexicons used in this paper is listed in Table 4. Lexicon Positive Negative Total HL 1,331 2,647 3,978 MPQA 1,932 2,817 4,749 Joint 1,051 2,024 3,075 Table 4: Statistics of the sentiment lexicons. Joint stands for the words that occur in both HL and MPQA with the same sentiment polarity. Results. Table 5 </context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 347–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ainur Yessenalina</author>
<author>Claire Cardie</author>
</authors>
<title>Compositional matrix-space models for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>172--182</pages>
<contexts>
<context position="3000" citStr="Yessenalina and Cardie, 2011" startWordPosition="447" endWordPosition="450">the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011). Accordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word. Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classification. The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text. As a result, words with opposite polarity, such as good and bad, are mapped into close vectors.</context>
<context position="9844" citStr="Yessenalina and Cardie (2011)" startWordPosition="1499" endWordPosition="1502">he complex linguistic characteristics of words. With the revival of interest in deep learning (Bengio et al., 2013), incorporating the continuous representation of a word as features has been proving effective in a variety of NLP tasks, such as parsing (Socher et al., 2013a), language modeling (Bengio et al., 2003; Mnih and Hinton, 2009) and NER (Turian et al., 2010). In the field of sentiment analysis, Bespalov et al. (2011; 2012) initialize the word embedding by Latent Semantic Analysis and further represent each document as the linear weighted of ngram vectors for sentiment classification. Yessenalina and Cardie (2011) model each word as a matrix and combine words using iterated matrix multiplication. Glorot et al. (2011) explore Stacked Denoising Autoencoders for domain adaptation in sentiment classification. Socher et al. propose Recursive Neural Network (RNN) (2011b), matrixvector RNN (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the compositionality of phrases of any length based on the representation of each pair of children recursively. Hermann et al. (2013) present Combinatory Categorial Autoencoders to learn the compositionality of sentence, which marries the Combinatory Categor</context>
</contexts>
<marker>Yessenalina, Cardie, 2011</marker>
<rawString>Ainur Yessenalina and Claire Cardie. 2011. Compositional matrix-space models for sentiment analysis. In Proceedings of Conference on Empirical Methods in Natural Language Processing, pages 172–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jichang Zhao</author>
<author>Li Dong</author>
<author>Junjie Wu</author>
<author>Ke Xu</author>
</authors>
<title>Moodlens: an emoticon-based sentiment analysis system for chinese tweets.</title>
<date>2012</date>
<booktitle>In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining.</booktitle>
<contexts>
<context position="7600" citStr="Zhao et al., 2012" startWordPosition="1156" endWordPosition="1159">; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision (Go et al., 2009). Instead of directly using the distantsupervised data as training set, Liu et al. (2012) adopt the tweets with emoticons to smooth the language model and Hu et al. (2013) incorporate the emotional signals into an unsupervised learning framework for Twitter sentiment classification. Many existing learning based methods on Twitter sentiment classification focus on feature engineering. The reason is that </context>
</contexts>
<marker>Zhao, Dong, Wu, Xu, 2012</marker>
<rawString>Jichang Zhao, Li Dong, Junjie Wu, and Ke Xu. 2012. Moodlens: an emoticon-based sentiment analysis system for chinese tweets. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqing Zheng</author>
<author>Hanyang Chen</author>
<author>Tianyu Xu</author>
</authors>
<title>Deep learning for chinese word segmentation and pos tagging.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>647--657</pages>
<contexts>
<context position="3673" citStr="Zheng et al., 2013" startWordPosition="559" endWordPosition="562">representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word. Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classification. The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text. As a result, words with opposite polarity, such as good and bad, are mapped into close vectors. It is meaningful for some tasks such as pos-tagging (Zheng et al., 2013) as the two words have similar usages and grammatical roles, but it becomes a disaster for sentiment analysis as they have the opposite sentiment polarity. In this paper, we propose learning sentimentspecific word embedding (SSWE) for sentiment analysis. We encode the sentiment information in1555 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1555–1565, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics to the continuous representation of words, so that it is able to separate good and bad to opposite ends </context>
</contexts>
<marker>Zheng, Chen, Xu, 2013</marker>
<rawString>Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu. 2013. Deep learning for chinese word segmentation and pos tagging. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 647–657.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>