<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009680">
<title confidence="0.8878495">
Improving Multi-Modal Representations Using Image Dispersion:
Why Less is Sometimes More
</title>
<author confidence="0.996298">
Douwe Kiela*, Felix Hill*, Anna Korhonen and Stephen Clark
</author>
<affiliation confidence="0.9754525">
University of Cambridge
Computer Laboratory
</affiliation>
<email confidence="0.998797">
{douwe.kiela|felix.hill|anna.korhonen|stephen.clark}@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.99739" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99982175">
Models that learn semantic representations
from both linguistic and perceptual in-
put outperform text-only models in many
contexts and better reflect human concept
acquisition. However, experiments sug-
gest that while the inclusion of perceptual
input improves representations of certain
concepts, it degrades the representations
of others. We propose an unsupervised
method to determine whether to include
perceptual input for a concept, and show
that it significantly improves the ability of
multi-modal models to learn and represent
word meanings. The method relies solely
on image data, and can be applied to a va-
riety of other NLP tasks.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999912483870968">
Multi-modal models that learn semantic concept
representations from both linguistic and percep-
tual input were originally motivated by parallels
with human concept acquisition, and evidence that
many concepts are grounded in the perceptual sys-
tem (Barsalou et al., 2003). Such models extract
information about the perceptible characteristics
of words from data collected in property norming
experiments (Roller and Schulte im Walde, 2013;
Silberer and Lapata, 2012) or directly from ‘raw’
data sources such as images (Feng and Lapata,
2010; Bruni et al., 2012). This input is combined
with information from linguistic corpora to pro-
duce enhanced representations of concept mean-
ing. Multi-modal models outperform language-
only models on a range of tasks, including mod-
elling conceptual association and predicting com-
positionality (Bruni et al., 2012; Silberer and Lap-
ata, 2012; Roller and Schulte im Walde, 2013).
Despite these results, the advantage of multi-
modal over linguistic-only models has only been
demonstrated on concrete concepts, such as
chocolate or cheeseburger, as opposed to abstract
concepts such as such as guilt or obesity. Indeed,
experiments indicate that while the addition of
perceptual input is generally beneficial for repre-
sentations of concrete concepts (Hill et al., 2013a;
Bruni et al., 2014), it can in fact be detrimental
to representations of abstract concepts (Hill et al.,
2013a). Further, while the theoretical importance
of the perceptual modalities to concrete represen-
tations is well known, evidence suggests this is not
the case for more abstract concepts (Paivio, 1990;
Hill et al., 2013b). Indeed, perhaps the most influ-
ential characterization of the abstract/concrete dis-
tinction, the Dual Coding Theory (Paivio, 1990),
posits that concrete representations are encoded
in both the linguistic and perceptual modalities
whereas abstract concepts are encoded only in the
linguistic modality.
Existing multi-modal architectures generally
extract and process all the information from their
specified sources of perceptual input. Since per-
ceptual data sources typically contain information
about both abstract and concrete concepts, such in-
formation is included for both concept types. The
potential effect of this design decision on perfor-
mance is significant because the vast majority of
meaning-bearing words in everyday language cor-
respond to abstract concepts. For instance, 72% of
word tokens in the British National Corpus (Leech
et al., 1994) were rated by contributors to the Uni-
versity of South Florida dataset (USF) (Nelson et
al., 2004) as more abstract than the noun war, a
concept that many would consider quite abstract.
In light of these considerations, we propose
a novel algorithm for approximating conceptual
concreteness. Multi-modal models in which per-
ceptual input is filtered according to our algorithm
learn higher-quality semantic representations than
previous approaches, resulting in a significant per-
formance improvement of up to 17% in captur-
</bodyText>
<page confidence="0.979673">
835
</page>
<bodyText confidence="0.967344545454546">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 835–841,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
ing the semantic similarity of concepts. Further,
our algorithm constitutes the first means of quan-
tifying conceptual concreteness that does not rely
on labor-intensive experimental studies or annota-
tors. Finally, we demonstrate the application of
this unsupervised concreteness metric to the se-
mantic classification of adjective-noun pairs, an
existing NLP task to which concreteness data has
proved valuable previously.
</bodyText>
<sectionHeader confidence="0.979461" genericHeader="method">
2 Experimental Approach
</sectionHeader>
<bodyText confidence="0.9999252">
Our experiments focus on multi-modal models
that extract their perceptual input automatically
from images. Image-based models more natu-
rally mirror the process of human concept acquisi-
tion than those whose input derives from exper-
imental datasets or expert annotation. They are
also more scalable since high-quality tagged im-
ages are freely available in several web-scale im-
age datasets.
We use Google Images as our image source,
and extract the first n image results for each con-
cept word. It has been shown that images from
Google yield higher-quality representations than
comparable sources such as Flickr (Bergsma and
Goebel, 2011). Other potential sources, such as
ImageNet (Deng et al., 2009) or the ESP Game
Dataset (Von Ahn and Dabbish, 2004), either do
not contain images for abstract concepts or do not
contain sufficient images for the concepts in our
evaluation sets.
</bodyText>
<subsectionHeader confidence="0.995658">
2.1 Image Dispersion-Based Filtering
</subsectionHeader>
<bodyText confidence="0.999682666666666">
Following the motivation outlined in Section 1, we
aim to distinguish visual input corresponding to
concrete concepts from visual input correspond-
ing to abstract concepts. Our algorithm is moti-
vated by the intuition that the diversity of images
returned for a particular concept depends on its
concreteness (see Figure 1). Specifically, we an-
ticipate greater congruence or similarity among a
set of images for, say, elephant than among im-
ages for happiness. By exploiting this connection,
the method approximates the concreteness of con-
cepts, and provides a basis to filter the correspond-
ing perceptual information.
Formally, we propose a measure, image disper-
sion d of a concept word w, defined as the aver-
age pairwise cosine distance between all the image
representations { w1 ... wn} in the set of images
for that concept:
</bodyText>
<figureCaption confidence="0.83551825">
Figure 1: Example images for a concrete (elephant
– little diversity, low dispersion) and an abstract
concept (happiness – greater diversity, high dis-
persion).
Figure 2: Computation of PHOW descriptors us-
ing dense SIFT for levels l = 0 to l = 2 and the
corresponding histogram representations (Bosch
et al., 2007).
</figureCaption>
<equation confidence="0.981234">
1 wi · wj
d(w) = 1 −  |�wi ||�wj |(1)
2n(n − 1) i&lt;j≤n
</equation>
<bodyText confidence="0.99989525">
We use an average pairwise distance-based met-
ric because this emphasizes the total variation
more than e.g. the mean distance from the cen-
troid. In all experiments we set n = 50.
</bodyText>
<subsectionHeader confidence="0.897763">
Generating Visual Representations Visual
</subsectionHeader>
<bodyText confidence="0.9995785">
vector representations for each image were ob-
tained using the well-known bag of visual words
(BoVW) approach (Sivic and Zisserman, 2003).
BoVW obtains a vector representation for an
</bodyText>
<page confidence="0.991855">
836
</page>
<bodyText confidence="0.99998108">
image by mapping each of its local descriptors
to a cluster histogram using a standard clustering
algorithm such as k-means.
Previous NLP-related work uses SIFT (Feng
and Lapata, 2010; Bruni et al., 2012) or SURF
(Roller and Schulte im Walde, 2013) descriptors
for identifying points of interest in an image,
quantified by 128-dimensional local descriptors.
We apply Pyramid Histogram Of visual Words
(PHOW) descriptors, which are particularly well-
suited for object categorization, a key component
of image similarity and thus dispersion (Bosch et
al., 2007). PHOW is roughly equivalent to run-
ning SIFT on a dense grid of locations at a fixed
scale and orientation and at multiple scales (see
Fig 2), but is both more efficient and more accu-
rate than regular (dense) SIFT approaches (Bosch
et al., 2007). We resize the images in our dataset
to 100x100 pixels and compute PHOW descriptors
using VLFeat (Vedaldi and Fulkerson, 2008).
The descriptors for the images were subse-
quently clustered using mini-batch k-means (Scul-
ley, 2010) with k = 50 to obtain histograms of
visual words, yielding 50-dimensional visual vec-
tors for each of the images.
</bodyText>
<subsectionHeader confidence="0.663993">
Generating Linguistic Representations We
</subsectionHeader>
<bodyText confidence="0.999689090909091">
extract continuous vector representations (also of
50 dimensions) for concepts using the continu-
ous log-linear skipgram model of Mikolov et al.
(2013a), trained on the 100M word British Na-
tional Corpus (Leech et al., 1994). This model
learns high quality lexical semantic representa-
tions based on the distributional properties of
words in text, and has been shown to outperform
simple distributional models on applications such
as semantic composition and analogical mapping
(Mikolov et al., 2013b).
</bodyText>
<subsectionHeader confidence="0.992582">
2.2 Evaluation Gold-standards
</subsectionHeader>
<bodyText confidence="0.995229463414634">
We evaluate models by measuring the Spearman
correlation of model output with two well-known
gold-standards reflecting semantic proximity – a
standard measure for evaluating the quality of rep-
resentations (see e.g. Agirre et al. (2009)).
To test the ability of our model to capture
concept similarity, we measure correlations with
WordSim353 (Finkelstein et al., 2001), a selec-
tion of 353 concept pairs together with a similar-
ity rating provided by human annotators. Word-
Sim has been used as a benchmark for distribu-
tional semantic models in numerous studies (see
e.g. (Huang et al., 2012; Bruni et al., 2012)).
As a complementary gold-standard, we use the
University of South Florida Norms (USF) (Nelson
et al., 2004). This dataset contains scores for free
association, an experimental measure of cognitive
association, between over 40,000 concept pairs.
The USF norms have been used in many previous
studies to evaluate semantic representations (An-
drews et al., 2009; Feng and Lapata, 2010; Sil-
berer and Lapata, 2012; Roller and Schulte im
Walde, 2013). The USF evaluation set is partic-
ularly appropriate in the present context because
concepts in the dataset are also rated for concep-
tual concreteness by at least 10 human annotators.
We create a representative evaluation set of USF
pairs as follows. We randomly sample 100 con-
cepts from the upper quartile and 100 concepts
from the lower quartile of a list of all USF con-
cepts ranked by concreteness. We denote these
sets C, for concrete, and A for abstract respec-
tively. We then extract all pairs (w1, w2) in the
USF dataset such that both w1 and w2 are in A∪C.
This yields an evaluation set of 903 pairs, of which
304 are such that w1, w2 ∈ C and 317 are such
that w1, w2 ∈ A.
The images used in our experiments and
the evaluation gold-standards can be down-
loaded from http://www.cl.cam.ac.uk/
˜dk427/dispersion.html.
</bodyText>
<sectionHeader confidence="0.997504" genericHeader="method">
3 Improving Multi-Modal
Representations
</sectionHeader>
<bodyText confidence="0.999866111111111">
We apply image dispersion-based filtering as fol-
lows: if both concepts in an evaluation pair have
an image dispersion below a given threshold, both
the linguistic and the visual representations are in-
cluded. If not, in accordance with the Dual Cod-
ing Theory of human concept processing (Paivio,
1990), only the linguistic representation is used.
For both datasets, we set the threshold as the
median image dispersion, although performance
could in principle be improved by adjusting this
parameter. We compare dispersion filtered rep-
resentations with linguistic, perceptual and stan-
dard multi-modal representations (concatenated
linguistic and perceptual representations). Sim-
ilarity between concept pairs is calculated using
cosine similarity.
As Figure 3 shows, dispersion-filtered multi-
modal representations significantly outperform
</bodyText>
<page confidence="0.969034">
837
</page>
<figure confidence="0.816461">
Similarity − WordSim 353 Free association − USF (903)
Evaluation Set
</figure>
<figureCaption confidence="0.981102">
Figure 3: Performance of conventional multi-
</figureCaption>
<bodyText confidence="0.922412">
modal (visual input included for all concepts) vs.
image dispersion-based filtering models (visual in-
put only for concepts classified as concrete) on the
two evaluation gold-standards.
</bodyText>
<figure confidence="0.938507">
&apos;concrete&apos; pairs (304) &apos;abstract&apos; pairs (317)
Concept Type
</figure>
<figureCaption confidence="0.771548">
Figure 4: Visual input is valuable for representing
concepts that are classified as concrete by the im-
age dispersion algorithm, but not so for concepts
classified as abstract. All correlations are with the
USF gold-standard.
</figureCaption>
<figure confidence="0.99939244117647">
0.532
0.145
0.477
0.542
0.229
0.189
Model Representations
Linguistic only
Image only
Standard multi−modal
Dispersion filtered
0.203
0.247
0.257
0.184
0.29
0.189
Representation Modality
Linguistic
Visual
Linguistic+Visual
0.054
0.167
Correlation 0.4
0.3
0.2
0.1
0.0
Correlation 0.5
0.4
0.3
0.2
0.1
0.0
</figure>
<bodyText confidence="0.984555833333333">
standard multi-modal representations on both
evaluation datasets. We observe a 17% increase in
Spearman correlation on WordSim353 and a 22%
increase on the USF norms. Based on the corre-
lation comparison method of Steiger (1980), both
represent significant improvements (WordSim353,
t = 2.42, p &lt; 0.05; USF, t = 1.86, p &lt; 0.1). In
both cases, models with the dispersion-based filter
also outperform the purely linguistic model, which
is not the case for other multi-modal approaches
that evaluate on WordSim353 (e.g. Bruni et al.
(2012)).
</bodyText>
<sectionHeader confidence="0.987856" genericHeader="evaluation">
4 Concreteness and Image Dispersion
</sectionHeader>
<bodyText confidence="0.999896">
The filtering approach described thus far improves
multi-modal representations because image dis-
persion provides a means to distinguish concrete
concepts from more abstract concepts. Since re-
search has demonstrated the applicability of con-
creteness to a range of other NLP tasks (Turney et
al., 2011; Kwong, 2008), it is important to exam-
ine the connection between image dispersion and
concreteness in more detail.
</bodyText>
<subsectionHeader confidence="0.996446">
4.1 Quantifying Concreteness
</subsectionHeader>
<bodyText confidence="0.999806764705882">
To evaluate the effectiveness of image dispersion
as a proxy for concreteness we evaluated our al-
gorithm on a binary classification task based on
the set of 100 concrete and 100 abstract concepts
A∪C introduced in Section 2. By classifying con-
cepts with image dispersion below the median as
concrete and concepts above this threshold as ab-
stract we achieved an abstract-concrete prediction
accuracy of 81%.
While well-understood intuitively, concreteness is
not a formally defined notion. Quantities such as
the USF concreteness score depend on the sub-
jective judgement of raters and the particular an-
notation guidelines. According to the Dual Cod-
ing Theory, however, concrete concepts are pre-
cisely those with a salient perceptual representa-
tion. As illustrated in Figure 4, our binary clas-
sification conforms to this characterization. The
importance of the visual modality is significantly
greater when evaluating on pairs for which both
concepts are classified as concrete than on pairs of
two abstract concepts.
Image dispersion is also an effective predic-
tor of concreteness on samples for which the ab-
stract/concrete distinction is less clear. On a differ-
ent set of 200 concepts extracted by random sam-
pling from the USF dataset stratified by concrete-
ness rating (including concepts across the con-
creteness spectrum), we observed a high correla-
tion between abstractness and dispersion (Spear-
man ρ = 0.61, p &lt; 0.001). On this more diverse
sample, which reflects the range of concepts typi-
cally found in linguistic corpora, image dispersion
is a particularly useful diagnostic for identifying
</bodyText>
<page confidence="0.999263">
838
</page>
<tableCaption confidence="0.6105228">
Table 1: Concepts with highest and lowest image
dispersion scores in our evaluation set, and con-
creteness ratings from the USF dataset.
the very abstract or very concrete concepts. As
Table 1 illustrates, the concepts with the lowest
</tableCaption>
<bodyText confidence="0.99776147826087">
dispersion in this sample are, without exception,
highly concrete, and the concepts of highest dis-
persion are clearly very abstract.
It should be noted that all previous approaches
to the automatic measurement of concreteness rely
on annotator ratings, dictionaries or manually-
constructed resources. Kwong (2008) proposes
a method based on the presence of hard-coded
phrasal features in dictionary entries correspond-
ing to each concept. By contrast, S´anchez et al.
(2011) present an approach based on the position
of word senses corresponding to each concept in
the WordNet ontology (Fellbaum, 1999). Turney
et al. (2011) propose a method that extends a large
set of concreteness ratings similar to those in the
USF dataset. The Turney et al. algorithm quanti-
fies the concreteness of concepts that lack such a
rating based on their proximity to rated concepts
in a semantic vector space. In contrast to each of
these approaches, the image dispersion approach
requires no hand-coded resources. It is therefore
more scalable, and instantly applicable to a wide
range of languages.
</bodyText>
<subsectionHeader confidence="0.999656">
4.2 Classifying Adjective-Noun Pairs
</subsectionHeader>
<bodyText confidence="0.9999818125">
Finally, we explored whether image dispersion
can be applied to specific NLP tasks as an effec-
tive proxy for concreteness. Turney et al. (2011)
showed that concreteness is applicable to the clas-
sification of adjective-noun modification as either
literal or non-literal. By applying a logistic regres-
sion with noun concreteness as the predictor vari-
able, Turney et al. achieved a classification accu-
racy of 79% on this task. This model relies on sig-
nificant supervision in the form of over 4,000 hu-
man lexical concreteness ratings.1 Applying im-
age dispersion in place of concreteness in an iden-
tical classifier on the same dataset, our entirely un-
supervised approach achieves an accuracy of 63%.
This is a notable improvement on the largest-class
baseline of 55%.
</bodyText>
<sectionHeader confidence="0.996388" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999986366666667">
We presented a novel method, image dispersion-
based filtering, that improves multi-modal repre-
sentations by approximating conceptual concrete-
ness from images and filtering model input. The
results clearly show that including more percep-
tual input in multi-modal models is not always bet-
ter. Motivated by this fact, our approach provides
an intuitive and straightforward metric to deter-
mine whether or not to include such information.
In addition to improving multi-modal represen-
tations, we have shown the applicability of the im-
age dispersion metric to several other tasks. To
our knowledge, our algorithm constitutes the first
unsupervised method for quantifying conceptual
concreteness as applied to NLP, although it does,
of course, rely on the Google Images retrieval al-
gorithm. Moreover, we presented a method to
classify adjective-noun pairs according to modi-
fication type that exploits the link between image
dispersion and concreteness. It is striking that this
apparently linguistic problem can be addressed
solely using the raw data encoded in images.
In future work, we will investigate the precise
quantity of perceptual information to be included
for best performance, as well as the optimal filter-
ing threshold. In addition, we will explore whether
the application of image data, and the interaction
between images and language, can yield improve-
ments on other tasks in semantic processing and
representation.
</bodyText>
<sectionHeader confidence="0.987248" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.8803075">
DK is supported by EPSRC grant EP/I037512/1.
FH is supported by St John’s College, Cambridge.
AK is supported by The Royal Society. SC is sup-
ported by ERC Starting Grant DisCoTex (306920)
and EPSRC grant EP/I037512/1. We thank the
anonymous reviewers for their helpful comments.
</bodyText>
<footnote confidence="0.980915">
1The MRC Psycholinguistics concreteness ratings (Colt-
heart, 1981) used by Turney et al. (2011) are a subset of those
included in the USF dataset.
</footnote>
<table confidence="0.999422909090909">
Concept Image Dispersion Conc. (USF)
shirt .488 6.05
bed .495 5.91
knife .560 6.08
dress .578 6.59
car .580 6.35
ego 1.000 1.93
nonsense .999 1.90
memory .999 1.78
potential .997 1.90
know .996 2.70
</table>
<page confidence="0.99655">
839
</page>
<sectionHeader confidence="0.995851" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998251308411215">
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
NAACL ’09, pages 19–27, Boulder, Colorado.
Mark Andrews, Gabriella Vigliocco, and David Vin-
son. 2009. Integrating experiential and distribu-
tional data to learn semantic representations. Psy-
chological review, 116(3):463.
Lawrence W Barsalou, W Kyle Simmons, Aron K Bar-
bey, and Christine D Wilson. 2003. Grounding
conceptual knowledge in modality-specific systems.
Trends in cognitive sciences, 7(2):84–91.
Shane Bergsma and Randy Goebel. 2011. Using vi-
sual information to predict lexical preference. In
RANLP, pages 399–405.
Anna Bosch, Andrew Zisserman, and Xavier Munoz.
2007. Image classification using random forests and
ferns. In Proceedings of ICCV.
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 136–145. Asso-
ciation for Computational Linguistics.
Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research, 49:1–47.
Max Coltheart. 1981. The MRC psycholinguistic
database. The Quarterly Journal of Experimental
Psychology, 33(4):497–505.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. Imagenet: A large-scale hi-
erarchical image database. In Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE Con-
ference on, pages 248–255. IEEE.
Christiane Fellbaum. 1999. WordNet. Wiley Online
Library.
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 91–99. Asso-
ciation for Computational Linguistics.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. In Proceedings of the 10th inter-
national conference on World Wide Web, pages 406–
414. ACM.
Felix Hill, Douwe Kiela, and Anna Korhonen. 2013a.
Concreteness and corpora: A theoretical and practi-
cal analysis. CMCL 2013.
Felix Hill, Anna Korhonen, and Christian Bentz.
2013b. A quantitative empirical analysis of the ab-
stract/concrete distinction. Cognitive science, 38(1).
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 873–882. Asso-
ciation for Computational Linguistics.
Oi Yee Kwong. 2008. A preliminary study on the im-
pact of lexical concreteness on word sense disam-
biguation. In PACLIC, pages 235–244.
Geoffrey Leech, Roger Garside, and Michael Bryant.
1994. Claws4: the tagging of the british national
corpus. In Proceedings of the 15th conference
on Computational linguistics-Volume 1, pages 622–
628. Association for Computational Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word repre-
sentations in vector space. In Proceedings of Inter-
national Conference of Learning Representations,
Scottsdale, Arizona, USA.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.
Douglas L Nelson, Cathy L McEvoy, and Thomas A
Schreiber. 2004. The University of South Florida
free association, rhyme, and word fragment norms.
Behavior Research Methods, Instruments, &amp; Com-
puters, 36(3):402–407.
Allan Paivio. 1990. Mental representations: A dual
coding approach. Oxford University Press.
Stephen Roller and Sabine Schulte im Walde. 2013.
A multimodal LDA model integrating textual, cog-
nitive and visual modalities. In Proceedings of the
2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1146–1157, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
David S´anchez, Montserrat Batet, and David Isern.
2011. Ontology-based information content compu-
tation. Knowledge-Based Systems, 24(2):297–303.
D Sculley. 2010. Web-scale k-means clustering. In
Proceedings of the 19th international conference on
World wide web, pages 1177–1178. ACM.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
</reference>
<page confidence="0.972425">
840
</page>
<reference confidence="0.999179041666667">
in Natural Language Processing and Computational
Natural Language Learning, pages 1423–1433. As-
sociation for Computational Linguistics.
J. Sivic and A. Zisserman. 2003. Video Google: a text
retrieval approach to object matching in videos. In
Proceedings of the Ninth IEEE International Con-
ference on Computer Vision, volume 2, pages 1470–
1477, Oct.
James H Steiger. 1980. Tests for comparing ele-
ments of a correlation matrix. Psychological Bul-
letin, 87(2):245.
Peter D Turney, Yair Neuman, Dan Assaf, and Yohai
Cohen. 2011. Literal and metaphorical sense iden-
tification through concrete and abstract context. In
Proceedings of the 2011 Conference on the Empiri-
cal Methods in Natural Language Processing, pages
680–690.
A. Vedaldi and B. Fulkerson. 2008. VLFeat: An open
and portable library of computer vision algorithms.
http://www.vlfeat.org/.
Luis Von Ahn and Laura Dabbish. 2004. Labeling
images with a computer game. In Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 319–326. ACM.
</reference>
<page confidence="0.998372">
841
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.336540">
<title confidence="0.999706">Improving Multi-Modal Representations Using Image</title>
<author confidence="0.7806325">Why Less is Sometimes More Douwe Kiela</author>
<author confidence="0.7806325">Felix Hill</author>
<author confidence="0.7806325">Anna Korhonen</author>
<author confidence="0.7806325">Stephen</author>
<affiliation confidence="0.8023325">University of Computer</affiliation>
<abstract confidence="0.998369705882353">Models that learn semantic representations from both linguistic and perceptual input outperform text-only models in many contexts and better reflect human concept acquisition. However, experiments suggest that while the inclusion of perceptual input improves representations of certain concepts, it degrades the representations of others. We propose an unsupervised method to determine whether to include perceptual input for a concept, and show that it significantly improves the ability of multi-modal models to learn and represent word meanings. The method relies solely on image data, and can be applied to a variety of other NLP tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pas¸ca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and wordnet-based approaches.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09,</booktitle>
<pages>pages</pages>
<location>Boulder, Colorado.</location>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pas¸ca, Soroa, 2009</marker>
<rawString>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and wordnet-based approaches. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 19–27, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Andrews</author>
<author>Gabriella Vigliocco</author>
<author>David Vinson</author>
</authors>
<title>Integrating experiential and distributional data to learn semantic representations. Psychological review,</title>
<date>2009</date>
<pages>116--3</pages>
<contexts>
<context position="9789" citStr="Andrews et al., 2009" startWordPosition="1488" endWordPosition="1492">lstein et al., 2001), a selection of 353 concept pairs together with a similarity rating provided by human annotators. WordSim has been used as a benchmark for distributional semantic models in numerous studies (see e.g. (Huang et al., 2012; Bruni et al., 2012)). As a complementary gold-standard, we use the University of South Florida Norms (USF) (Nelson et al., 2004). This dataset contains scores for free association, an experimental measure of cognitive association, between over 40,000 concept pairs. The USF norms have been used in many previous studies to evaluate semantic representations (Andrews et al., 2009; Feng and Lapata, 2010; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). The USF evaluation set is particularly appropriate in the present context because concepts in the dataset are also rated for conceptual concreteness by at least 10 human annotators. We create a representative evaluation set of USF pairs as follows. We randomly sample 100 concepts from the upper quartile and 100 concepts from the lower quartile of a list of all USF concepts ranked by concreteness. We denote these sets C, for concrete, and A for abstract respectively. We then extract all pairs (w1, w2) in the</context>
</contexts>
<marker>Andrews, Vigliocco, Vinson, 2009</marker>
<rawString>Mark Andrews, Gabriella Vigliocco, and David Vinson. 2009. Integrating experiential and distributional data to learn semantic representations. Psychological review, 116(3):463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence W Barsalou</author>
<author>W Kyle Simmons</author>
<author>Aron K Barbey</author>
<author>Christine D Wilson</author>
</authors>
<title>Grounding conceptual knowledge in modality-specific systems. Trends in cognitive sciences,</title>
<date>2003</date>
<pages>7--2</pages>
<contexts>
<context position="1192" citStr="Barsalou et al., 2003" startWordPosition="161" endWordPosition="164">ades the representations of others. We propose an unsupervised method to determine whether to include perceptual input for a concept, and show that it significantly improves the ability of multi-modal models to learn and represent word meanings. The method relies solely on image data, and can be applied to a variety of other NLP tasks. 1 Introduction Multi-modal models that learn semantic concept representations from both linguistic and perceptual input were originally motivated by parallels with human concept acquisition, and evidence that many concepts are grounded in the perceptual system (Barsalou et al., 2003). Such models extract information about the perceptible characteristics of words from data collected in property norming experiments (Roller and Schulte im Walde, 2013; Silberer and Lapata, 2012) or directly from ‘raw’ data sources such as images (Feng and Lapata, 2010; Bruni et al., 2012). This input is combined with information from linguistic corpora to produce enhanced representations of concept meaning. Multi-modal models outperform languageonly models on a range of tasks, including modelling conceptual association and predicting compositionality (Bruni et al., 2012; Silberer and Lapata, </context>
</contexts>
<marker>Barsalou, Simmons, Barbey, Wilson, 2003</marker>
<rawString>Lawrence W Barsalou, W Kyle Simmons, Aron K Barbey, and Christine D Wilson. 2003. Grounding conceptual knowledge in modality-specific systems. Trends in cognitive sciences, 7(2):84–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Randy Goebel</author>
</authors>
<title>Using visual information to predict lexical preference.</title>
<date>2011</date>
<booktitle>In RANLP,</booktitle>
<pages>399--405</pages>
<contexts>
<context position="5223" citStr="Bergsma and Goebel, 2011" startWordPosition="759" endWordPosition="762">ments focus on multi-modal models that extract their perceptual input automatically from images. Image-based models more naturally mirror the process of human concept acquisition than those whose input derives from experimental datasets or expert annotation. They are also more scalable since high-quality tagged images are freely available in several web-scale image datasets. We use Google Images as our image source, and extract the first n image results for each concept word. It has been shown that images from Google yield higher-quality representations than comparable sources such as Flickr (Bergsma and Goebel, 2011). Other potential sources, such as ImageNet (Deng et al., 2009) or the ESP Game Dataset (Von Ahn and Dabbish, 2004), either do not contain images for abstract concepts or do not contain sufficient images for the concepts in our evaluation sets. 2.1 Image Dispersion-Based Filtering Following the motivation outlined in Section 1, we aim to distinguish visual input corresponding to concrete concepts from visual input corresponding to abstract concepts. Our algorithm is motivated by the intuition that the diversity of images returned for a particular concept depends on its concreteness (see Figure</context>
</contexts>
<marker>Bergsma, Goebel, 2011</marker>
<rawString>Shane Bergsma and Randy Goebel. 2011. Using visual information to predict lexical preference. In RANLP, pages 399–405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Bosch</author>
<author>Andrew Zisserman</author>
<author>Xavier Munoz</author>
</authors>
<title>Image classification using random forests and ferns.</title>
<date>2007</date>
<booktitle>In Proceedings of ICCV.</booktitle>
<contexts>
<context position="6644" citStr="Bosch et al., 2007" startWordPosition="990" endWordPosition="993">creteness of concepts, and provides a basis to filter the corresponding perceptual information. Formally, we propose a measure, image dispersion d of a concept word w, defined as the average pairwise cosine distance between all the image representations { w1 ... wn} in the set of images for that concept: Figure 1: Example images for a concrete (elephant – little diversity, low dispersion) and an abstract concept (happiness – greater diversity, high dispersion). Figure 2: Computation of PHOW descriptors using dense SIFT for levels l = 0 to l = 2 and the corresponding histogram representations (Bosch et al., 2007). 1 wi · wj d(w) = 1 − |�wi ||�wj |(1) 2n(n − 1) i&lt;j≤n We use an average pairwise distance-based metric because this emphasizes the total variation more than e.g. the mean distance from the centroid. In all experiments we set n = 50. Generating Visual Representations Visual vector representations for each image were obtained using the well-known bag of visual words (BoVW) approach (Sivic and Zisserman, 2003). BoVW obtains a vector representation for an 836 image by mapping each of its local descriptors to a cluster histogram using a standard clustering algorithm such as k-means. Previous NLP-r</context>
<context position="7908" citStr="Bosch et al., 2007" startWordPosition="1200" endWordPosition="1203"> Bruni et al., 2012) or SURF (Roller and Schulte im Walde, 2013) descriptors for identifying points of interest in an image, quantified by 128-dimensional local descriptors. We apply Pyramid Histogram Of visual Words (PHOW) descriptors, which are particularly wellsuited for object categorization, a key component of image similarity and thus dispersion (Bosch et al., 2007). PHOW is roughly equivalent to running SIFT on a dense grid of locations at a fixed scale and orientation and at multiple scales (see Fig 2), but is both more efficient and more accurate than regular (dense) SIFT approaches (Bosch et al., 2007). We resize the images in our dataset to 100x100 pixels and compute PHOW descriptors using VLFeat (Vedaldi and Fulkerson, 2008). The descriptors for the images were subsequently clustered using mini-batch k-means (Sculley, 2010) with k = 50 to obtain histograms of visual words, yielding 50-dimensional visual vectors for each of the images. Generating Linguistic Representations We extract continuous vector representations (also of 50 dimensions) for concepts using the continuous log-linear skipgram model of Mikolov et al. (2013a), trained on the 100M word British National Corpus (Leech et al., </context>
</contexts>
<marker>Bosch, Zisserman, Munoz, 2007</marker>
<rawString>Anna Bosch, Andrew Zisserman, and Xavier Munoz. 2007. Image classification using random forests and ferns. In Proceedings of ICCV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Gemma Boleda</author>
<author>Marco Baroni</author>
<author>NamKhanh Tran</author>
</authors>
<title>Distributional semantics in technicolor.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>136--145</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1482" citStr="Bruni et al., 2012" startWordPosition="205" endWordPosition="208"> be applied to a variety of other NLP tasks. 1 Introduction Multi-modal models that learn semantic concept representations from both linguistic and perceptual input were originally motivated by parallels with human concept acquisition, and evidence that many concepts are grounded in the perceptual system (Barsalou et al., 2003). Such models extract information about the perceptible characteristics of words from data collected in property norming experiments (Roller and Schulte im Walde, 2013; Silberer and Lapata, 2012) or directly from ‘raw’ data sources such as images (Feng and Lapata, 2010; Bruni et al., 2012). This input is combined with information from linguistic corpora to produce enhanced representations of concept meaning. Multi-modal models outperform languageonly models on a range of tasks, including modelling conceptual association and predicting compositionality (Bruni et al., 2012; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). Despite these results, the advantage of multimodal over linguistic-only models has only been demonstrated on concrete concepts, such as chocolate or cheeseburger, as opposed to abstract concepts such as such as guilt or obesity. Indeed, experiments</context>
<context position="7309" citStr="Bruni et al., 2012" startWordPosition="1103" endWordPosition="1106">1) i&lt;j≤n We use an average pairwise distance-based metric because this emphasizes the total variation more than e.g. the mean distance from the centroid. In all experiments we set n = 50. Generating Visual Representations Visual vector representations for each image were obtained using the well-known bag of visual words (BoVW) approach (Sivic and Zisserman, 2003). BoVW obtains a vector representation for an 836 image by mapping each of its local descriptors to a cluster histogram using a standard clustering algorithm such as k-means. Previous NLP-related work uses SIFT (Feng and Lapata, 2010; Bruni et al., 2012) or SURF (Roller and Schulte im Walde, 2013) descriptors for identifying points of interest in an image, quantified by 128-dimensional local descriptors. We apply Pyramid Histogram Of visual Words (PHOW) descriptors, which are particularly wellsuited for object categorization, a key component of image similarity and thus dispersion (Bosch et al., 2007). PHOW is roughly equivalent to running SIFT on a dense grid of locations at a fixed scale and orientation and at multiple scales (see Fig 2), but is both more efficient and more accurate than regular (dense) SIFT approaches (Bosch et al., 2007).</context>
<context position="9430" citStr="Bruni et al., 2012" startWordPosition="1435" endWordPosition="1438">standards We evaluate models by measuring the Spearman correlation of model output with two well-known gold-standards reflecting semantic proximity – a standard measure for evaluating the quality of representations (see e.g. Agirre et al. (2009)). To test the ability of our model to capture concept similarity, we measure correlations with WordSim353 (Finkelstein et al., 2001), a selection of 353 concept pairs together with a similarity rating provided by human annotators. WordSim has been used as a benchmark for distributional semantic models in numerous studies (see e.g. (Huang et al., 2012; Bruni et al., 2012)). As a complementary gold-standard, we use the University of South Florida Norms (USF) (Nelson et al., 2004). This dataset contains scores for free association, an experimental measure of cognitive association, between over 40,000 concept pairs. The USF norms have been used in many previous studies to evaluate semantic representations (Andrews et al., 2009; Feng and Lapata, 2010; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). The USF evaluation set is particularly appropriate in the present context because concepts in the dataset are also rated for conceptual concreteness by a</context>
<context position="12993" citStr="Bruni et al. (2012)" startWordPosition="1985" endWordPosition="1988">ual 0.054 0.167 Correlation 0.4 0.3 0.2 0.1 0.0 Correlation 0.5 0.4 0.3 0.2 0.1 0.0 standard multi-modal representations on both evaluation datasets. We observe a 17% increase in Spearman correlation on WordSim353 and a 22% increase on the USF norms. Based on the correlation comparison method of Steiger (1980), both represent significant improvements (WordSim353, t = 2.42, p &lt; 0.05; USF, t = 1.86, p &lt; 0.1). In both cases, models with the dispersion-based filter also outperform the purely linguistic model, which is not the case for other multi-modal approaches that evaluate on WordSim353 (e.g. Bruni et al. (2012)). 4 Concreteness and Image Dispersion The filtering approach described thus far improves multi-modal representations because image dispersion provides a means to distinguish concrete concepts from more abstract concepts. Since research has demonstrated the applicability of concreteness to a range of other NLP tasks (Turney et al., 2011; Kwong, 2008), it is important to examine the connection between image dispersion and concreteness in more detail. 4.1 Quantifying Concreteness To evaluate the effectiveness of image dispersion as a proxy for concreteness we evaluated our algorithm on a binary </context>
</contexts>
<marker>Bruni, Boleda, Baroni, Tran, 2012</marker>
<rawString>Elia Bruni, Gemma Boleda, Marco Baroni, and NamKhanh Tran. 2012. Distributional semantics in technicolor. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 136–145. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Nam Khanh Tran</author>
<author>Marco Baroni</author>
</authors>
<title>Multimodal distributional semantics.</title>
<date>2014</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>49--1</pages>
<contexts>
<context position="2241" citStr="Bruni et al., 2014" startWordPosition="320" endWordPosition="323"> outperform languageonly models on a range of tasks, including modelling conceptual association and predicting compositionality (Bruni et al., 2012; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). Despite these results, the advantage of multimodal over linguistic-only models has only been demonstrated on concrete concepts, such as chocolate or cheeseburger, as opposed to abstract concepts such as such as guilt or obesity. Indeed, experiments indicate that while the addition of perceptual input is generally beneficial for representations of concrete concepts (Hill et al., 2013a; Bruni et al., 2014), it can in fact be detrimental to representations of abstract concepts (Hill et al., 2013a). Further, while the theoretical importance of the perceptual modalities to concrete representations is well known, evidence suggests this is not the case for more abstract concepts (Paivio, 1990; Hill et al., 2013b). Indeed, perhaps the most influential characterization of the abstract/concrete distinction, the Dual Coding Theory (Paivio, 1990), posits that concrete representations are encoded in both the linguistic and perceptual modalities whereas abstract concepts are encoded only in the linguistic </context>
</contexts>
<marker>Bruni, Tran, Baroni, 2014</marker>
<rawString>Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014. Multimodal distributional semantics. Journal of Artificial Intelligence Research, 49:1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Max Coltheart</author>
</authors>
<title>The MRC psycholinguistic database.</title>
<date>1981</date>
<journal>The Quarterly Journal of Experimental Psychology,</journal>
<volume>33</volume>
<issue>4</issue>
<marker>Coltheart, 1981</marker>
<rawString>Max Coltheart. 1981. The MRC psycholinguistic database. The Quarterly Journal of Experimental Psychology, 33(4):497–505.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Deng</author>
<author>Wei Dong</author>
<author>Richard Socher</author>
<author>Li-Jia Li</author>
<author>Kai Li</author>
<author>Li Fei-Fei</author>
</authors>
<title>Imagenet: A large-scale hierarchical image database.</title>
<date>2009</date>
<booktitle>In Computer Vision and Pattern Recognition,</booktitle>
<pages>248--255</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="5286" citStr="Deng et al., 2009" startWordPosition="769" endWordPosition="772">automatically from images. Image-based models more naturally mirror the process of human concept acquisition than those whose input derives from experimental datasets or expert annotation. They are also more scalable since high-quality tagged images are freely available in several web-scale image datasets. We use Google Images as our image source, and extract the first n image results for each concept word. It has been shown that images from Google yield higher-quality representations than comparable sources such as Flickr (Bergsma and Goebel, 2011). Other potential sources, such as ImageNet (Deng et al., 2009) or the ESP Game Dataset (Von Ahn and Dabbish, 2004), either do not contain images for abstract concepts or do not contain sufficient images for the concepts in our evaluation sets. 2.1 Image Dispersion-Based Filtering Following the motivation outlined in Section 1, we aim to distinguish visual input corresponding to concrete concepts from visual input corresponding to abstract concepts. Our algorithm is motivated by the intuition that the diversity of images returned for a particular concept depends on its concreteness (see Figure 1). Specifically, we anticipate greater congruence or similari</context>
</contexts>
<marker>Deng, Dong, Socher, Li, Li, Fei-Fei, 2009</marker>
<rawString>Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248–255. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<date>1999</date>
<publisher>WordNet. Wiley Online Library.</publisher>
<contexts>
<context position="15911" citStr="Fellbaum, 1999" startWordPosition="2439" endWordPosition="2440">ith the lowest dispersion in this sample are, without exception, highly concrete, and the concepts of highest dispersion are clearly very abstract. It should be noted that all previous approaches to the automatic measurement of concreteness rely on annotator ratings, dictionaries or manuallyconstructed resources. Kwong (2008) proposes a method based on the presence of hard-coded phrasal features in dictionary entries corresponding to each concept. By contrast, S´anchez et al. (2011) present an approach based on the position of word senses corresponding to each concept in the WordNet ontology (Fellbaum, 1999). Turney et al. (2011) propose a method that extends a large set of concreteness ratings similar to those in the USF dataset. The Turney et al. algorithm quantifies the concreteness of concepts that lack such a rating based on their proximity to rated concepts in a semantic vector space. In contrast to each of these approaches, the image dispersion approach requires no hand-coded resources. It is therefore more scalable, and instantly applicable to a wide range of languages. 4.2 Classifying Adjective-Noun Pairs Finally, we explored whether image dispersion can be applied to specific NLP tasks </context>
</contexts>
<marker>Fellbaum, 1999</marker>
<rawString>Christiane Fellbaum. 1999. WordNet. Wiley Online Library.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Visual information in semantic representation.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>91--99</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1461" citStr="Feng and Lapata, 2010" startWordPosition="201" endWordPosition="204"> on image data, and can be applied to a variety of other NLP tasks. 1 Introduction Multi-modal models that learn semantic concept representations from both linguistic and perceptual input were originally motivated by parallels with human concept acquisition, and evidence that many concepts are grounded in the perceptual system (Barsalou et al., 2003). Such models extract information about the perceptible characteristics of words from data collected in property norming experiments (Roller and Schulte im Walde, 2013; Silberer and Lapata, 2012) or directly from ‘raw’ data sources such as images (Feng and Lapata, 2010; Bruni et al., 2012). This input is combined with information from linguistic corpora to produce enhanced representations of concept meaning. Multi-modal models outperform languageonly models on a range of tasks, including modelling conceptual association and predicting compositionality (Bruni et al., 2012; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). Despite these results, the advantage of multimodal over linguistic-only models has only been demonstrated on concrete concepts, such as chocolate or cheeseburger, as opposed to abstract concepts such as such as guilt or obesity</context>
<context position="7288" citStr="Feng and Lapata, 2010" startWordPosition="1099" endWordPosition="1102">|�wi ||�wj |(1) 2n(n − 1) i&lt;j≤n We use an average pairwise distance-based metric because this emphasizes the total variation more than e.g. the mean distance from the centroid. In all experiments we set n = 50. Generating Visual Representations Visual vector representations for each image were obtained using the well-known bag of visual words (BoVW) approach (Sivic and Zisserman, 2003). BoVW obtains a vector representation for an 836 image by mapping each of its local descriptors to a cluster histogram using a standard clustering algorithm such as k-means. Previous NLP-related work uses SIFT (Feng and Lapata, 2010; Bruni et al., 2012) or SURF (Roller and Schulte im Walde, 2013) descriptors for identifying points of interest in an image, quantified by 128-dimensional local descriptors. We apply Pyramid Histogram Of visual Words (PHOW) descriptors, which are particularly wellsuited for object categorization, a key component of image similarity and thus dispersion (Bosch et al., 2007). PHOW is roughly equivalent to running SIFT on a dense grid of locations at a fixed scale and orientation and at multiple scales (see Fig 2), but is both more efficient and more accurate than regular (dense) SIFT approaches </context>
<context position="9812" citStr="Feng and Lapata, 2010" startWordPosition="1493" endWordPosition="1496">a selection of 353 concept pairs together with a similarity rating provided by human annotators. WordSim has been used as a benchmark for distributional semantic models in numerous studies (see e.g. (Huang et al., 2012; Bruni et al., 2012)). As a complementary gold-standard, we use the University of South Florida Norms (USF) (Nelson et al., 2004). This dataset contains scores for free association, an experimental measure of cognitive association, between over 40,000 concept pairs. The USF norms have been used in many previous studies to evaluate semantic representations (Andrews et al., 2009; Feng and Lapata, 2010; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). The USF evaluation set is particularly appropriate in the present context because concepts in the dataset are also rated for conceptual concreteness by at least 10 human annotators. We create a representative evaluation set of USF pairs as follows. We randomly sample 100 concepts from the upper quartile and 100 concepts from the lower quartile of a list of all USF concepts ranked by concreteness. We denote these sets C, for concrete, and A for abstract respectively. We then extract all pairs (w1, w2) in the USF dataset such that </context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010. Visual information in semantic representation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 91–99. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2001</date>
<booktitle>In Proceedings of the 10th international conference on World Wide Web,</booktitle>
<pages>406--414</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="9189" citStr="Finkelstein et al., 2001" startWordPosition="1392" endWordPosition="1395">epresentations based on the distributional properties of words in text, and has been shown to outperform simple distributional models on applications such as semantic composition and analogical mapping (Mikolov et al., 2013b). 2.2 Evaluation Gold-standards We evaluate models by measuring the Spearman correlation of model output with two well-known gold-standards reflecting semantic proximity – a standard measure for evaluating the quality of representations (see e.g. Agirre et al. (2009)). To test the ability of our model to capture concept similarity, we measure correlations with WordSim353 (Finkelstein et al., 2001), a selection of 353 concept pairs together with a similarity rating provided by human annotators. WordSim has been used as a benchmark for distributional semantic models in numerous studies (see e.g. (Huang et al., 2012; Bruni et al., 2012)). As a complementary gold-standard, we use the University of South Florida Norms (USF) (Nelson et al., 2004). This dataset contains scores for free association, an experimental measure of cognitive association, between over 40,000 concept pairs. The USF norms have been used in many previous studies to evaluate semantic representations (Andrews et al., 2009</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2001</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: The concept revisited. In Proceedings of the 10th international conference on World Wide Web, pages 406– 414. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>Douwe Kiela</author>
<author>Anna Korhonen</author>
</authors>
<title>Concreteness and corpora: A theoretical and practical analysis. CMCL</title>
<date>2013</date>
<contexts>
<context position="2219" citStr="Hill et al., 2013" startWordPosition="316" endWordPosition="319">. Multi-modal models outperform languageonly models on a range of tasks, including modelling conceptual association and predicting compositionality (Bruni et al., 2012; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). Despite these results, the advantage of multimodal over linguistic-only models has only been demonstrated on concrete concepts, such as chocolate or cheeseburger, as opposed to abstract concepts such as such as guilt or obesity. Indeed, experiments indicate that while the addition of perceptual input is generally beneficial for representations of concrete concepts (Hill et al., 2013a; Bruni et al., 2014), it can in fact be detrimental to representations of abstract concepts (Hill et al., 2013a). Further, while the theoretical importance of the perceptual modalities to concrete representations is well known, evidence suggests this is not the case for more abstract concepts (Paivio, 1990; Hill et al., 2013b). Indeed, perhaps the most influential characterization of the abstract/concrete distinction, the Dual Coding Theory (Paivio, 1990), posits that concrete representations are encoded in both the linguistic and perceptual modalities whereas abstract concepts are encoded o</context>
</contexts>
<marker>Hill, Kiela, Korhonen, 2013</marker>
<rawString>Felix Hill, Douwe Kiela, and Anna Korhonen. 2013a. Concreteness and corpora: A theoretical and practical analysis. CMCL 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>Anna Korhonen</author>
<author>Christian Bentz</author>
</authors>
<title>A quantitative empirical analysis of the abstract/concrete distinction.</title>
<date>2013</date>
<journal>Cognitive science,</journal>
<volume>38</volume>
<issue>1</issue>
<contexts>
<context position="2219" citStr="Hill et al., 2013" startWordPosition="316" endWordPosition="319">. Multi-modal models outperform languageonly models on a range of tasks, including modelling conceptual association and predicting compositionality (Bruni et al., 2012; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). Despite these results, the advantage of multimodal over linguistic-only models has only been demonstrated on concrete concepts, such as chocolate or cheeseburger, as opposed to abstract concepts such as such as guilt or obesity. Indeed, experiments indicate that while the addition of perceptual input is generally beneficial for representations of concrete concepts (Hill et al., 2013a; Bruni et al., 2014), it can in fact be detrimental to representations of abstract concepts (Hill et al., 2013a). Further, while the theoretical importance of the perceptual modalities to concrete representations is well known, evidence suggests this is not the case for more abstract concepts (Paivio, 1990; Hill et al., 2013b). Indeed, perhaps the most influential characterization of the abstract/concrete distinction, the Dual Coding Theory (Paivio, 1990), posits that concrete representations are encoded in both the linguistic and perceptual modalities whereas abstract concepts are encoded o</context>
</contexts>
<marker>Hill, Korhonen, Bentz, 2013</marker>
<rawString>Felix Hill, Anna Korhonen, and Christian Bentz. 2013b. A quantitative empirical analysis of the abstract/concrete distinction. Cognitive science, 38(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>873--882</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9409" citStr="Huang et al., 2012" startWordPosition="1431" endWordPosition="1434">2.2 Evaluation Gold-standards We evaluate models by measuring the Spearman correlation of model output with two well-known gold-standards reflecting semantic proximity – a standard measure for evaluating the quality of representations (see e.g. Agirre et al. (2009)). To test the ability of our model to capture concept similarity, we measure correlations with WordSim353 (Finkelstein et al., 2001), a selection of 353 concept pairs together with a similarity rating provided by human annotators. WordSim has been used as a benchmark for distributional semantic models in numerous studies (see e.g. (Huang et al., 2012; Bruni et al., 2012)). As a complementary gold-standard, we use the University of South Florida Norms (USF) (Nelson et al., 2004). This dataset contains scores for free association, an experimental measure of cognitive association, between over 40,000 concept pairs. The USF norms have been used in many previous studies to evaluate semantic representations (Andrews et al., 2009; Feng and Lapata, 2010; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). The USF evaluation set is particularly appropriate in the present context because concepts in the dataset are also rated for concept</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 873–882. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oi Yee Kwong</author>
</authors>
<title>A preliminary study on the impact of lexical concreteness on word sense disambiguation.</title>
<date>2008</date>
<booktitle>In PACLIC,</booktitle>
<pages>235--244</pages>
<contexts>
<context position="13345" citStr="Kwong, 2008" startWordPosition="2039" endWordPosition="2040">im353, t = 2.42, p &lt; 0.05; USF, t = 1.86, p &lt; 0.1). In both cases, models with the dispersion-based filter also outperform the purely linguistic model, which is not the case for other multi-modal approaches that evaluate on WordSim353 (e.g. Bruni et al. (2012)). 4 Concreteness and Image Dispersion The filtering approach described thus far improves multi-modal representations because image dispersion provides a means to distinguish concrete concepts from more abstract concepts. Since research has demonstrated the applicability of concreteness to a range of other NLP tasks (Turney et al., 2011; Kwong, 2008), it is important to examine the connection between image dispersion and concreteness in more detail. 4.1 Quantifying Concreteness To evaluate the effectiveness of image dispersion as a proxy for concreteness we evaluated our algorithm on a binary classification task based on the set of 100 concrete and 100 abstract concepts A∪C introduced in Section 2. By classifying concepts with image dispersion below the median as concrete and concepts above this threshold as abstract we achieved an abstract-concrete prediction accuracy of 81%. While well-understood intuitively, concreteness is not a forma</context>
<context position="15623" citStr="Kwong (2008)" startWordPosition="2394" endWordPosition="2395">ispersion is a particularly useful diagnostic for identifying 838 Table 1: Concepts with highest and lowest image dispersion scores in our evaluation set, and concreteness ratings from the USF dataset. the very abstract or very concrete concepts. As Table 1 illustrates, the concepts with the lowest dispersion in this sample are, without exception, highly concrete, and the concepts of highest dispersion are clearly very abstract. It should be noted that all previous approaches to the automatic measurement of concreteness rely on annotator ratings, dictionaries or manuallyconstructed resources. Kwong (2008) proposes a method based on the presence of hard-coded phrasal features in dictionary entries corresponding to each concept. By contrast, S´anchez et al. (2011) present an approach based on the position of word senses corresponding to each concept in the WordNet ontology (Fellbaum, 1999). Turney et al. (2011) propose a method that extends a large set of concreteness ratings similar to those in the USF dataset. The Turney et al. algorithm quantifies the concreteness of concepts that lack such a rating based on their proximity to rated concepts in a semantic vector space. In contrast to each of </context>
</contexts>
<marker>Kwong, 2008</marker>
<rawString>Oi Yee Kwong. 2008. A preliminary study on the impact of lexical concreteness on word sense disambiguation. In PACLIC, pages 235–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Leech</author>
<author>Roger Garside</author>
<author>Michael Bryant</author>
</authors>
<title>Claws4: the tagging of the british national corpus.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th conference on Computational linguistics-Volume 1,</booktitle>
<pages>622--628</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3406" citStr="Leech et al., 1994" startWordPosition="491" endWordPosition="494"> abstract concepts are encoded only in the linguistic modality. Existing multi-modal architectures generally extract and process all the information from their specified sources of perceptual input. Since perceptual data sources typically contain information about both abstract and concrete concepts, such information is included for both concept types. The potential effect of this design decision on performance is significant because the vast majority of meaning-bearing words in everyday language correspond to abstract concepts. For instance, 72% of word tokens in the British National Corpus (Leech et al., 1994) were rated by contributors to the University of South Florida dataset (USF) (Nelson et al., 2004) as more abstract than the noun war, a concept that many would consider quite abstract. In light of these considerations, we propose a novel algorithm for approximating conceptual concreteness. Multi-modal models in which perceptual input is filtered according to our algorithm learn higher-quality semantic representations than previous approaches, resulting in a significant performance improvement of up to 17% in captur835 Proceedings of the 52nd Annual Meeting of the Association for Computational</context>
<context position="8513" citStr="Leech et al., 1994" startWordPosition="1294" endWordPosition="1297"> et al., 2007). We resize the images in our dataset to 100x100 pixels and compute PHOW descriptors using VLFeat (Vedaldi and Fulkerson, 2008). The descriptors for the images were subsequently clustered using mini-batch k-means (Sculley, 2010) with k = 50 to obtain histograms of visual words, yielding 50-dimensional visual vectors for each of the images. Generating Linguistic Representations We extract continuous vector representations (also of 50 dimensions) for concepts using the continuous log-linear skipgram model of Mikolov et al. (2013a), trained on the 100M word British National Corpus (Leech et al., 1994). This model learns high quality lexical semantic representations based on the distributional properties of words in text, and has been shown to outperform simple distributional models on applications such as semantic composition and analogical mapping (Mikolov et al., 2013b). 2.2 Evaluation Gold-standards We evaluate models by measuring the Spearman correlation of model output with two well-known gold-standards reflecting semantic proximity – a standard measure for evaluating the quality of representations (see e.g. Agirre et al. (2009)). To test the ability of our model to capture concept si</context>
</contexts>
<marker>Leech, Garside, Bryant, 1994</marker>
<rawString>Geoffrey Leech, Roger Garside, and Michael Bryant. 1994. Claws4: the tagging of the british national corpus. In Proceedings of the 15th conference on Computational linguistics-Volume 1, pages 622– 628. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proceedings of International Conference of Learning Representations,</booktitle>
<location>Scottsdale, Arizona, USA.</location>
<contexts>
<context position="8440" citStr="Mikolov et al. (2013" startWordPosition="1281" endWordPosition="1284">re efficient and more accurate than regular (dense) SIFT approaches (Bosch et al., 2007). We resize the images in our dataset to 100x100 pixels and compute PHOW descriptors using VLFeat (Vedaldi and Fulkerson, 2008). The descriptors for the images were subsequently clustered using mini-batch k-means (Sculley, 2010) with k = 50 to obtain histograms of visual words, yielding 50-dimensional visual vectors for each of the images. Generating Linguistic Representations We extract continuous vector representations (also of 50 dimensions) for concepts using the continuous log-linear skipgram model of Mikolov et al. (2013a), trained on the 100M word British National Corpus (Leech et al., 1994). This model learns high quality lexical semantic representations based on the distributional properties of words in text, and has been shown to outperform simple distributional models on applications such as semantic composition and analogical mapping (Mikolov et al., 2013b). 2.2 Evaluation Gold-standards We evaluate models by measuring the Spearman correlation of model output with two well-known gold-standards reflecting semantic proximity – a standard measure for evaluating the quality of representations (see e.g. Agir</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. In Proceedings of International Conference of Learning Representations, Scottsdale, Arizona, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="8440" citStr="Mikolov et al. (2013" startWordPosition="1281" endWordPosition="1284">re efficient and more accurate than regular (dense) SIFT approaches (Bosch et al., 2007). We resize the images in our dataset to 100x100 pixels and compute PHOW descriptors using VLFeat (Vedaldi and Fulkerson, 2008). The descriptors for the images were subsequently clustered using mini-batch k-means (Sculley, 2010) with k = 50 to obtain histograms of visual words, yielding 50-dimensional visual vectors for each of the images. Generating Linguistic Representations We extract continuous vector representations (also of 50 dimensions) for concepts using the continuous log-linear skipgram model of Mikolov et al. (2013a), trained on the 100M word British National Corpus (Leech et al., 1994). This model learns high quality lexical semantic representations based on the distributional properties of words in text, and has been shown to outperform simple distributional models on applications such as semantic composition and analogical mapping (Mikolov et al., 2013b). 2.2 Evaluation Gold-standards We evaluate models by measuring the Spearman correlation of model output with two well-known gold-standards reflecting semantic proximity – a standard measure for evaluating the quality of representations (see e.g. Agir</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas L Nelson</author>
<author>Cathy L McEvoy</author>
<author>Thomas A Schreiber</author>
</authors>
<title>The University of South Florida free association, rhyme, and word fragment norms.</title>
<date>2004</date>
<journal>Behavior Research Methods, Instruments, &amp; Computers,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="3504" citStr="Nelson et al., 2004" startWordPosition="508" endWordPosition="511">s generally extract and process all the information from their specified sources of perceptual input. Since perceptual data sources typically contain information about both abstract and concrete concepts, such information is included for both concept types. The potential effect of this design decision on performance is significant because the vast majority of meaning-bearing words in everyday language correspond to abstract concepts. For instance, 72% of word tokens in the British National Corpus (Leech et al., 1994) were rated by contributors to the University of South Florida dataset (USF) (Nelson et al., 2004) as more abstract than the noun war, a concept that many would consider quite abstract. In light of these considerations, we propose a novel algorithm for approximating conceptual concreteness. Multi-modal models in which perceptual input is filtered according to our algorithm learn higher-quality semantic representations than previous approaches, resulting in a significant performance improvement of up to 17% in captur835 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 835–841, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Asso</context>
<context position="9539" citStr="Nelson et al., 2004" startWordPosition="1452" endWordPosition="1455">standards reflecting semantic proximity – a standard measure for evaluating the quality of representations (see e.g. Agirre et al. (2009)). To test the ability of our model to capture concept similarity, we measure correlations with WordSim353 (Finkelstein et al., 2001), a selection of 353 concept pairs together with a similarity rating provided by human annotators. WordSim has been used as a benchmark for distributional semantic models in numerous studies (see e.g. (Huang et al., 2012; Bruni et al., 2012)). As a complementary gold-standard, we use the University of South Florida Norms (USF) (Nelson et al., 2004). This dataset contains scores for free association, an experimental measure of cognitive association, between over 40,000 concept pairs. The USF norms have been used in many previous studies to evaluate semantic representations (Andrews et al., 2009; Feng and Lapata, 2010; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). The USF evaluation set is particularly appropriate in the present context because concepts in the dataset are also rated for conceptual concreteness by at least 10 human annotators. We create a representative evaluation set of USF pairs as follows. We randomly s</context>
</contexts>
<marker>Nelson, McEvoy, Schreiber, 2004</marker>
<rawString>Douglas L Nelson, Cathy L McEvoy, and Thomas A Schreiber. 2004. The University of South Florida free association, rhyme, and word fragment norms. Behavior Research Methods, Instruments, &amp; Computers, 36(3):402–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Allan Paivio</author>
</authors>
<title>Mental representations: A dual coding approach.</title>
<date>1990</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="2528" citStr="Paivio, 1990" startWordPosition="366" endWordPosition="367"> only been demonstrated on concrete concepts, such as chocolate or cheeseburger, as opposed to abstract concepts such as such as guilt or obesity. Indeed, experiments indicate that while the addition of perceptual input is generally beneficial for representations of concrete concepts (Hill et al., 2013a; Bruni et al., 2014), it can in fact be detrimental to representations of abstract concepts (Hill et al., 2013a). Further, while the theoretical importance of the perceptual modalities to concrete representations is well known, evidence suggests this is not the case for more abstract concepts (Paivio, 1990; Hill et al., 2013b). Indeed, perhaps the most influential characterization of the abstract/concrete distinction, the Dual Coding Theory (Paivio, 1990), posits that concrete representations are encoded in both the linguistic and perceptual modalities whereas abstract concepts are encoded only in the linguistic modality. Existing multi-modal architectures generally extract and process all the information from their specified sources of perceptual input. Since perceptual data sources typically contain information about both abstract and concrete concepts, such information is included for both c</context>
<context position="11036" citStr="Paivio, 1990" startWordPosition="1705" endWordPosition="1706"> w2 are in A∪C. This yields an evaluation set of 903 pairs, of which 304 are such that w1, w2 ∈ C and 317 are such that w1, w2 ∈ A. The images used in our experiments and the evaluation gold-standards can be downloaded from http://www.cl.cam.ac.uk/ ˜dk427/dispersion.html. 3 Improving Multi-Modal Representations We apply image dispersion-based filtering as follows: if both concepts in an evaluation pair have an image dispersion below a given threshold, both the linguistic and the visual representations are included. If not, in accordance with the Dual Coding Theory of human concept processing (Paivio, 1990), only the linguistic representation is used. For both datasets, we set the threshold as the median image dispersion, although performance could in principle be improved by adjusting this parameter. We compare dispersion filtered representations with linguistic, perceptual and standard multi-modal representations (concatenated linguistic and perceptual representations). Similarity between concept pairs is calculated using cosine similarity. As Figure 3 shows, dispersion-filtered multimodal representations significantly outperform 837 Similarity − WordSim 353 Free association − USF (903) Evalua</context>
</contexts>
<marker>Paivio, 1990</marker>
<rawString>Allan Paivio. 1990. Mental representations: A dual coding approach. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Roller</author>
<author>Sabine Schulte im Walde</author>
</authors>
<title>A multimodal LDA model integrating textual, cognitive and visual modalities.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1146--1157</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<marker>Roller, Walde, 2013</marker>
<rawString>Stephen Roller and Sabine Schulte im Walde. 2013. A multimodal LDA model integrating textual, cognitive and visual modalities. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1146–1157, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David S´anchez</author>
<author>Montserrat Batet</author>
<author>David Isern</author>
</authors>
<title>Ontology-based information content computation.</title>
<date>2011</date>
<journal>Knowledge-Based Systems,</journal>
<volume>24</volume>
<issue>2</issue>
<marker>S´anchez, Batet, Isern, 2011</marker>
<rawString>David S´anchez, Montserrat Batet, and David Isern. 2011. Ontology-based information content computation. Knowledge-Based Systems, 24(2):297–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Sculley</author>
</authors>
<title>Web-scale k-means clustering.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th international conference on World wide web,</booktitle>
<pages>1177--1178</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="8136" citStr="Sculley, 2010" startWordPosition="1236" endWordPosition="1238">rs, which are particularly wellsuited for object categorization, a key component of image similarity and thus dispersion (Bosch et al., 2007). PHOW is roughly equivalent to running SIFT on a dense grid of locations at a fixed scale and orientation and at multiple scales (see Fig 2), but is both more efficient and more accurate than regular (dense) SIFT approaches (Bosch et al., 2007). We resize the images in our dataset to 100x100 pixels and compute PHOW descriptors using VLFeat (Vedaldi and Fulkerson, 2008). The descriptors for the images were subsequently clustered using mini-batch k-means (Sculley, 2010) with k = 50 to obtain histograms of visual words, yielding 50-dimensional visual vectors for each of the images. Generating Linguistic Representations We extract continuous vector representations (also of 50 dimensions) for concepts using the continuous log-linear skipgram model of Mikolov et al. (2013a), trained on the 100M word British National Corpus (Leech et al., 1994). This model learns high quality lexical semantic representations based on the distributional properties of words in text, and has been shown to outperform simple distributional models on applications such as semantic compo</context>
</contexts>
<marker>Sculley, 2010</marker>
<rawString>D Sculley. 2010. Web-scale k-means clustering. In Proceedings of the 19th international conference on World wide web, pages 1177–1178. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Mirella Lapata</author>
</authors>
<title>Grounded models of semantic representation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1423--1433</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1387" citStr="Silberer and Lapata, 2012" startWordPosition="188" endWordPosition="191">lti-modal models to learn and represent word meanings. The method relies solely on image data, and can be applied to a variety of other NLP tasks. 1 Introduction Multi-modal models that learn semantic concept representations from both linguistic and perceptual input were originally motivated by parallels with human concept acquisition, and evidence that many concepts are grounded in the perceptual system (Barsalou et al., 2003). Such models extract information about the perceptible characteristics of words from data collected in property norming experiments (Roller and Schulte im Walde, 2013; Silberer and Lapata, 2012) or directly from ‘raw’ data sources such as images (Feng and Lapata, 2010; Bruni et al., 2012). This input is combined with information from linguistic corpora to produce enhanced representations of concept meaning. Multi-modal models outperform languageonly models on a range of tasks, including modelling conceptual association and predicting compositionality (Bruni et al., 2012; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). Despite these results, the advantage of multimodal over linguistic-only models has only been demonstrated on concrete concepts, such as chocolate or chee</context>
<context position="9839" citStr="Silberer and Lapata, 2012" startWordPosition="1497" endWordPosition="1501">ept pairs together with a similarity rating provided by human annotators. WordSim has been used as a benchmark for distributional semantic models in numerous studies (see e.g. (Huang et al., 2012; Bruni et al., 2012)). As a complementary gold-standard, we use the University of South Florida Norms (USF) (Nelson et al., 2004). This dataset contains scores for free association, an experimental measure of cognitive association, between over 40,000 concept pairs. The USF norms have been used in many previous studies to evaluate semantic representations (Andrews et al., 2009; Feng and Lapata, 2010; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). The USF evaluation set is particularly appropriate in the present context because concepts in the dataset are also rated for conceptual concreteness by at least 10 human annotators. We create a representative evaluation set of USF pairs as follows. We randomly sample 100 concepts from the upper quartile and 100 concepts from the lower quartile of a list of all USF concepts ranked by concreteness. We denote these sets C, for concrete, and A for abstract respectively. We then extract all pairs (w1, w2) in the USF dataset such that both w1 and w2 are in A∪C. </context>
</contexts>
<marker>Silberer, Lapata, 2012</marker>
<rawString>Carina Silberer and Mirella Lapata. 2012. Grounded models of semantic representation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1423–1433. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sivic</author>
<author>A Zisserman</author>
</authors>
<title>Video Google: a text retrieval approach to object matching in videos.</title>
<date>2003</date>
<booktitle>In Proceedings of the Ninth IEEE International Conference on Computer Vision,</booktitle>
<volume>2</volume>
<pages>1470--1477</pages>
<contexts>
<context position="7055" citStr="Sivic and Zisserman, 2003" startWordPosition="1062" endWordPosition="1065">act concept (happiness – greater diversity, high dispersion). Figure 2: Computation of PHOW descriptors using dense SIFT for levels l = 0 to l = 2 and the corresponding histogram representations (Bosch et al., 2007). 1 wi · wj d(w) = 1 − |�wi ||�wj |(1) 2n(n − 1) i&lt;j≤n We use an average pairwise distance-based metric because this emphasizes the total variation more than e.g. the mean distance from the centroid. In all experiments we set n = 50. Generating Visual Representations Visual vector representations for each image were obtained using the well-known bag of visual words (BoVW) approach (Sivic and Zisserman, 2003). BoVW obtains a vector representation for an 836 image by mapping each of its local descriptors to a cluster histogram using a standard clustering algorithm such as k-means. Previous NLP-related work uses SIFT (Feng and Lapata, 2010; Bruni et al., 2012) or SURF (Roller and Schulte im Walde, 2013) descriptors for identifying points of interest in an image, quantified by 128-dimensional local descriptors. We apply Pyramid Histogram Of visual Words (PHOW) descriptors, which are particularly wellsuited for object categorization, a key component of image similarity and thus dispersion (Bosch et al</context>
</contexts>
<marker>Sivic, Zisserman, 2003</marker>
<rawString>J. Sivic and A. Zisserman. 2003. Video Google: a text retrieval approach to object matching in videos. In Proceedings of the Ninth IEEE International Conference on Computer Vision, volume 2, pages 1470– 1477, Oct.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James H Steiger</author>
</authors>
<title>Tests for comparing elements of a correlation matrix.</title>
<date>1980</date>
<journal>Psychological Bulletin,</journal>
<volume>87</volume>
<issue>2</issue>
<contexts>
<context position="12685" citStr="Steiger (1980)" startWordPosition="1937" endWordPosition="1938">for concepts classified as abstract. All correlations are with the USF gold-standard. 0.532 0.145 0.477 0.542 0.229 0.189 Model Representations Linguistic only Image only Standard multi−modal Dispersion filtered 0.203 0.247 0.257 0.184 0.29 0.189 Representation Modality Linguistic Visual Linguistic+Visual 0.054 0.167 Correlation 0.4 0.3 0.2 0.1 0.0 Correlation 0.5 0.4 0.3 0.2 0.1 0.0 standard multi-modal representations on both evaluation datasets. We observe a 17% increase in Spearman correlation on WordSim353 and a 22% increase on the USF norms. Based on the correlation comparison method of Steiger (1980), both represent significant improvements (WordSim353, t = 2.42, p &lt; 0.05; USF, t = 1.86, p &lt; 0.1). In both cases, models with the dispersion-based filter also outperform the purely linguistic model, which is not the case for other multi-modal approaches that evaluate on WordSim353 (e.g. Bruni et al. (2012)). 4 Concreteness and Image Dispersion The filtering approach described thus far improves multi-modal representations because image dispersion provides a means to distinguish concrete concepts from more abstract concepts. Since research has demonstrated the applicability of concreteness to a</context>
</contexts>
<marker>Steiger, 1980</marker>
<rawString>James H Steiger. 1980. Tests for comparing elements of a correlation matrix. Psychological Bulletin, 87(2):245.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Yair Neuman</author>
<author>Dan Assaf</author>
<author>Yohai Cohen</author>
</authors>
<title>Literal and metaphorical sense identification through concrete and abstract context.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on the Empirical Methods in Natural Language Processing,</booktitle>
<pages>680--690</pages>
<contexts>
<context position="13331" citStr="Turney et al., 2011" startWordPosition="2035" endWordPosition="2038">t improvements (WordSim353, t = 2.42, p &lt; 0.05; USF, t = 1.86, p &lt; 0.1). In both cases, models with the dispersion-based filter also outperform the purely linguistic model, which is not the case for other multi-modal approaches that evaluate on WordSim353 (e.g. Bruni et al. (2012)). 4 Concreteness and Image Dispersion The filtering approach described thus far improves multi-modal representations because image dispersion provides a means to distinguish concrete concepts from more abstract concepts. Since research has demonstrated the applicability of concreteness to a range of other NLP tasks (Turney et al., 2011; Kwong, 2008), it is important to examine the connection between image dispersion and concreteness in more detail. 4.1 Quantifying Concreteness To evaluate the effectiveness of image dispersion as a proxy for concreteness we evaluated our algorithm on a binary classification task based on the set of 100 concrete and 100 abstract concepts A∪C introduced in Section 2. By classifying concepts with image dispersion below the median as concrete and concepts above this threshold as abstract we achieved an abstract-concrete prediction accuracy of 81%. While well-understood intuitively, concreteness </context>
<context position="15933" citStr="Turney et al. (2011)" startWordPosition="2441" endWordPosition="2444">spersion in this sample are, without exception, highly concrete, and the concepts of highest dispersion are clearly very abstract. It should be noted that all previous approaches to the automatic measurement of concreteness rely on annotator ratings, dictionaries or manuallyconstructed resources. Kwong (2008) proposes a method based on the presence of hard-coded phrasal features in dictionary entries corresponding to each concept. By contrast, S´anchez et al. (2011) present an approach based on the position of word senses corresponding to each concept in the WordNet ontology (Fellbaum, 1999). Turney et al. (2011) propose a method that extends a large set of concreteness ratings similar to those in the USF dataset. The Turney et al. algorithm quantifies the concreteness of concepts that lack such a rating based on their proximity to rated concepts in a semantic vector space. In contrast to each of these approaches, the image dispersion approach requires no hand-coded resources. It is therefore more scalable, and instantly applicable to a wide range of languages. 4.2 Classifying Adjective-Noun Pairs Finally, we explored whether image dispersion can be applied to specific NLP tasks as an effective proxy </context>
</contexts>
<marker>Turney, Neuman, Assaf, Cohen, 2011</marker>
<rawString>Peter D Turney, Yair Neuman, Dan Assaf, and Yohai Cohen. 2011. Literal and metaphorical sense identification through concrete and abstract context. In Proceedings of the 2011 Conference on the Empirical Methods in Natural Language Processing, pages 680–690.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Vedaldi</author>
<author>B Fulkerson</author>
</authors>
<title>VLFeat: An open and portable library of computer vision algorithms.</title>
<date>2008</date>
<note>http://www.vlfeat.org/.</note>
<contexts>
<context position="8035" citStr="Vedaldi and Fulkerson, 2008" startWordPosition="1220" endWordPosition="1223">image, quantified by 128-dimensional local descriptors. We apply Pyramid Histogram Of visual Words (PHOW) descriptors, which are particularly wellsuited for object categorization, a key component of image similarity and thus dispersion (Bosch et al., 2007). PHOW is roughly equivalent to running SIFT on a dense grid of locations at a fixed scale and orientation and at multiple scales (see Fig 2), but is both more efficient and more accurate than regular (dense) SIFT approaches (Bosch et al., 2007). We resize the images in our dataset to 100x100 pixels and compute PHOW descriptors using VLFeat (Vedaldi and Fulkerson, 2008). The descriptors for the images were subsequently clustered using mini-batch k-means (Sculley, 2010) with k = 50 to obtain histograms of visual words, yielding 50-dimensional visual vectors for each of the images. Generating Linguistic Representations We extract continuous vector representations (also of 50 dimensions) for concepts using the continuous log-linear skipgram model of Mikolov et al. (2013a), trained on the 100M word British National Corpus (Leech et al., 1994). This model learns high quality lexical semantic representations based on the distributional properties of words in text,</context>
</contexts>
<marker>Vedaldi, Fulkerson, 2008</marker>
<rawString>A. Vedaldi and B. Fulkerson. 2008. VLFeat: An open and portable library of computer vision algorithms. http://www.vlfeat.org/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis Von Ahn</author>
<author>Laura Dabbish</author>
</authors>
<title>Labeling images with a computer game.</title>
<date>2004</date>
<booktitle>In Proceedings of the SIGCHI conference on Human factors in computing systems,</booktitle>
<pages>319--326</pages>
<publisher>ACM.</publisher>
<marker>Von Ahn, Dabbish, 2004</marker>
<rawString>Luis Von Ahn and Laura Dabbish. 2004. Labeling images with a computer game. In Proceedings of the SIGCHI conference on Human factors in computing systems, pages 319–326. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>