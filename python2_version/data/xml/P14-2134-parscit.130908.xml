<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007474">
<title confidence="0.972849">
Distributed Representations of Geographically Situated Language
</title>
<author confidence="0.99806">
David Bamman Chris Dyer Noah A. Smith
</author>
<affiliation confidence="0.88684">
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.998763">
{dbamman,cdyer,nasmith}@cs.cmu.edu
</email>
<sectionHeader confidence="0.993894" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999808888888889">
We introduce a model for incorporating
contextual information (such as geogra-
phy) in learning vector-space representa-
tions of situated language. In contrast to
approaches to multimodal representation
learning that have used properties of the
object being described (such as its color),
our model includes information about the
subject (i.e., the speaker), allowing us to
learn the contours of a word’s meaning
that are shaped by the context in which
it is uttered. In a quantitative evaluation
on the task of judging geographically in-
formed semantic similarity between repre-
sentations learned from 1.1 billion words
of geo-located tweets, our joint model out-
performs comparable independent models
that learn meaning in isolation.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999963508196722">
The vast textual resources used in NLP –
newswire, web text, parliamentary proceedings –
can encourage a view of language as a disembod-
ied phenomenon. The rise of social media, how-
ever, with its large volume of text paired with in-
formation about its author and social context, re-
minds us that each word is uttered by a particular
person at a particular place and time. In short: lan-
guage is situated.
The coupling of text with demographic infor-
mation has enabled computational modeling of
linguistic variation, including uncovering words
and topics that are characteristic of geographical
regions (Eisenstein et al., 2010; O’Connor et al.,
2010; Hong et al., 2012; Doyle, 2014), learning
correlations between words and socioeconomic
variables (Rao et al., 2010; Eisenstein et al., 2011;
Pennacchiotti and Popescu, 2011; Bamman et al.,
2014); and charting how new terms spread geo-
graphically (Eisenstein et al., 2012). These models
can tell us that hella was (at one time) used most
often by a particular demographic group in north-
ern California, echoing earlier linguistic studies
(Bucholtz, 2006), and that wicked is used most
often in New England (Ravindranath, 2011); and
they have practical applications, facilitating tasks
like text-based geolocation (Wing and Baldridge,
2011; Roller et al., 2012; Ikawa et al., 2012).
One desideratum that remains, however, is how the
meaning of these terms is shaped by geographical
influences – while wicked is used throughout the
United States to mean bad or evil (“he is a wicked
man”), in New England it is used as an adverbial
intensifier (“my boy’s wicked smart”). In lever-
aging grounded social media to uncover linguistic
variation, what we want to learn is how a word’s
meaning is shaped by its geography.
In this paper, we introduce a method that ex-
tends vector-space lexical semantic models to
learn representations of geographically situated
language. Vector-space models of lexical seman-
tics have been a popular and effective approach
to learning representations of word meaning (Lin,
1998; Turney and Pantel, 2010; Reisinger and
Mooney, 2010; Socher et al., 2013; Mikolov et al.,
2013, inter alia). In bringing in extra-linguistic in-
formation to learn word representations, our work
falls into the general domain of multimodal learn-
ing; while other work has used visual informa-
tion to improve distributed representations (An-
drews et al., 2009; Feng and Lapata, 2010; Bruni
et al., 2011; Bruni et al., 2012a; Bruni et al.,
2012b; Roller and im Walde, 2013), this work
generally exploits information about the object be-
ing described (e.g., strawberry and a picture of a
strawberry); in contrast, we use information about
the speaker to learn representations that vary ac-
cording to contextual variables from the speaker’s
perspective. Unlike classic multimodal systems
that incorporate multiple active modalities (such
as gesture) from a user (Oviatt, 2003; Yu and
</bodyText>
<page confidence="0.96418">
828
</page>
<bodyText confidence="0.293332">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 828–834,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</bodyText>
<figure confidence="0.996936444444444">
Main
h
Alabama
W
Alaska
Arizona
Arkansas
X
o
</figure>
<figureCaption confidence="0.9945115">
Figure 1: Model. Illustrated are the input dimensions that fire for a single sample, reflecting a particular word (vocabulary item
#2) spoken in Alaska, along with a single output. Parameter matrix W consists of the learned low-dimensional embeddings.
</figureCaption>
<bodyText confidence="0.9999235">
Ballard, 2004), our primary input is textual data,
supplemented with metadata about the author and
the moment of authorship. This information en-
ables learning models of word meaning that are
sensitive to such factors, allowing us to distin-
guish, for example, between the usage of wicked
in Massachusetts from the usage of that word else-
where, and letting us better associate geographi-
cally grounded named entities (e.g, Boston) with
their hypernyms (city) in their respective regions.
</bodyText>
<sectionHeader confidence="0.977009" genericHeader="introduction">
2 Model
</sectionHeader>
<bodyText confidence="0.9997866">
The model we introduce is grounded in the distri-
butional hypothesis (Harris, 1954), that two words
are similar by appearing in the same kinds of con-
texts (where “context” itself can be variously de-
fined as the bag or sequence of tokens around a tar-
get word, either by linear distance or dependency
path). We can invoke the distributional hypothe-
sis for many instances of regional variation by ob-
serving that such variants often appear in similar
contexts. For example:
</bodyText>
<listItem confidence="0.999690333333333">
• my boy’s wicked smart
• my boy’s hella smart
• my boy’s very smart
</listItem>
<bodyText confidence="0.998145787234042">
Here, all three variants can often be seen in an im-
mediately pre-adjectival position (as is common
with intensifying adverbs).
Given the empirical success of vector-space rep-
resentations in capturing semantic properties and
their success at a variety of NLP tasks (Turian et
al., 2010; Socher et al., 2011; Collobert et al.,
2011; Socher et al., 2013), we use a simple, but
state-of-the-art neural architecture (Mikolov et al.,
2013) to learn low-dimensional real-valued repre-
sentations of words. The graphical form of this
model is illustrated in figure 1.
This model corresponds to an extension of
the “skip-gram” language model (Mikolov et al.,
2013) (hereafter SGLM). Given an input sentence
s and a context window of size t, each word si is
conditioned on in turn to predict the identities of
all of the tokens within t words around it. For a
vocabulary V , each input word si is represented
as a one-hot vector wi of length |V |. The SGLM
has two sets of parameters. The first is the rep-
resentation matrix W E R|V |×k, which encodes
the real-valued embeddings for each word in the
vocabulary. A matrix multiply h = w&gt;W, E Rk
serves to index the particular embedding for word
w, which constitutes the model’s hidden layer. To
predict the value of the context word y (again, a
one-hot vector of dimensionality |V |), this hidden
representation h is then multiplied by a second pa-
rameter matrix X E R|V |×k. The final prediction
over the output vocabulary is then found by pass-
ing this resulting vector through the softmax func-
tion o = softmax(Xh), giving a vector in the |V |-
dimensional unit simplex. Backpropagation using
(input x, output y) word tuples learns the values
of W (the embeddings) and X (the output param-
eter matrix) that maximize the likelihood of y (i.e.,
the context words) conditioned on x (i.e., the si’s).
During backpropagation, the errors propagated are
the difference between o (a probability distribu-
tion with k outcomes) and the true (one-hot) out-
put y.
Let us define a set of contextual variables
C; in the experiments that follow, C is com-
prised solely of geographical state Cstate =
{AK, AL, ... , WY}) but could in principle in-
clude any number of features, such as calendar
</bodyText>
<page confidence="0.992071">
829
</page>
<bodyText confidence="0.999873267857143">
month, day of week, or other demographic vari-
ables of the speaker. Let |C |denote the sum of the
cardinalities of all variables in C (i.e., 51 states,
including the District of Columbia). Rather than
using a single embedding matrix W that contains
low-dimensional representations for every word in
the vocabulary, we define a global embedding ma-
trix Wmain E R|V |xk and an additional |C |such
matrices (each again of size |V  |× k, which cap-
ture the effect that each variable value has on each
word in the vocabulary. Given an input word w
and set of active variable values A (e.g., A =
{state = MA}), we calculate the hidden layer
h as the sum of these independent embeddings:
h = wTWmain + EaEA wTWa. While the word
wicked has a common low-dimensional represen-
tation in Wmain,wicked that is invoked for every
instance of its use (regardless of the place), the
corresponding vector WMA,wicked indicates how
that common representation should shift in k-
dimensional space when used in Massachusetts.
Backpropagation functions as in standard SGLM,
with gradient updates for each training example
{x, y} touching not only Wmain (as in SGLM), but
all active WA as well.
The additional W embeddings we add lead to
an increase in the number of total parameters by
a factor of |C|. To control for the extra degrees
of freedom this entails, we add squared E2 regu-
larization to all parameters, using stochastic gra-
dient descent for backpropagation with minibatch
updates for the regularization term. As in Mikolov
et al. (2013), we speed up computation using the
hierarchical softmax (Morin and Bengio, 2005) on
the output matrix X.
This model defines a joint parameterization over
all variable values in the data, where information
from data originating in California, for instance,
can influence the representations learned for Wis-
consin; a naive alternative would be to simply train
individual models on each variable value (a “Cal-
ifornia” model using data only from California,
etc.). A joint model has three a priori advantages
over independent models: (i) sharing data across
variable values encourages representations across
those values to be similar; e.g., while city may be
closer to Boston in Massachusetts and Chicago in
Illinois, in both places it still generally connotes
a municipality; (ii) such sharing can mitigate data
sparseness for less-witnessed areas; and (iii) with
a joint model, all representations are guaranteed to
be in the same vector space and can therefore be
compared to each other; with individual models
(each with different initializations), word vectors
across different states may not be directly com-
pared.
</bodyText>
<sectionHeader confidence="0.998412" genericHeader="method">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.99749544">
We evaluate our model by confirming its face
validity in a qualitative analysis and estimating
its accuracy at the quantitative task of judging
geographically-informed semantic similarity. We
use 1.1 billion tokens from 93 million geolocated
tweets gathered between September 1, 2011 and
August 30, 2013 (approximately 127,000 tweets
per day evenly sampled over those two years).
This data only includes tweets that have been ge-
olocated to state-level granularity in the United
States using high-precision pattern matching on
the user-specified location field (e.g., “new york
ny” → NY, “chicago” → IL, etc.). As a pre-
processing step, we identify a set of target mul-
tiword expressions in this corpus as the maximal
sequence of adjectives + nouns with the highest
pointwise mutual information; in all experiments
described below, we define the vocabulary V as
the most frequent 100,000 terms (either unigrams
or multiword expressions) in the total data, and set
the dimensionality of the embedding k = 100. In
all experiments, the contextual variable is the ob-
served US state (including DC), so that |C |= 51;
the vector space representation of word w in state
s is wTWmain + wTWs.
</bodyText>
<subsectionHeader confidence="0.997342">
3.1 Qualitative Evaluation
</subsectionHeader>
<bodyText confidence="0.999758176470588">
To illustrate how the model described above can
learn geographically-informed semantic represen-
tations of words, table 1 displays the terms with
the highest cosine similarity to wicked in Kansas
and Massachusetts after running our joint model
on the full 1.1 billion words of Twitter data; while
wicked in Kansas is close to other evaluative terms
like evil and pure and religious terms like gods and
spirit, in Massachusetts it is most similar to other
intensifiers like super, ridiculously and insanely.
Table 2 likewise presents the terms with the
highest cosine similarity to city in both Califor-
nia and New York; while the terms most evoked
by city in California include regional locations
like Chinatown, Los Angeles’ South Bay and San
Francisco’s East Bay, in New York the most sim-
ilar terms include hamptons, upstate and borough
</bodyText>
<page confidence="0.986717">
830
</page>
<table confidence="0.999776666666667">
Kansas Massachusetts
term cosine term cosine
wicked 1.000 wicked 1.000
evil 0.884 super 0.855
pure 0.841 ridiculously 0.851
gods 0.841 insanely 0.820
mystery 0.830 extremely 0.793
spirit 0.830 goddamn 0.781
king 0.828 surprisingly 0.774
above 0.825 kinda 0.772
righteous 0.823 #sarcasm 0.772
magic 0.822 sooooooo 0.770
</table>
<tableCaption confidence="0.9929765">
Table 1: Terms with the highest cosine similarity to wicked
in Kansas and Massachusetts.
</tableCaption>
<table confidence="0.99952875">
California New York
term cosine term cosine
city 1.000 city 1.000
valley 0.880 suburbs 0.866
bay 0.874 town 0.855
downtown 0.873 hamptons 0.852
chinatown 0.854 big city 0.842
south bay 0.854 borough 0.837
area 0.851 neighborhood 0.835
east bay 0.845 downtown 0.827
neighborhood 0.843 upstate 0.826
peninsula 0.840 big apple 0.825
</table>
<tableCaption confidence="0.981145">
Table 2: Terms with the highest cosine similarity to city in
California and New York.
</tableCaption>
<subsectionHeader confidence="0.7194185">
(New York City’s term of administrative division).
3.2 Quantitative Evaluation
</subsectionHeader>
<bodyText confidence="0.999967666666667">
As a quantitative measure of our model’s perfor-
mance, we consider the task of judging semantic
similarity among words whose meanings are likely
to evoke strong geographical correlations. In the
absence of a sizable number of linguistically in-
teresting terms (like wicked) that are known to be
geographically variable, we consider the proxy of
estimating the named entities evoked by specific
terms in different geographical regions. As noted
above, geographic terms like city provide one such
example: in Massachusetts we expect the term city
to be more strongly connected to grounded named
entities like Boston than to other US cities. We
consider seven categories for which we can rea-
sonably expect the connotations of each term to
vary by geography; in each case, we calculate the
distance between two terms x and y using repre-
sentations learned for a given state (Sstate(x, y)).
</bodyText>
<listItem confidence="0.966944173913044">
1. city. For each state, we measure the distance
between the word city and the state’s most
populous city; e.g., SAZ(city, phoenix).
2. state. For each state, the distance between
the word state and the state’s name; e.g.,
SWI(state, wisconsin).
3. football. For all NFL teams, the distance be-
tween the word football and the team name;
e.g., SIL(football, bears).
4. basketball. For all NBA teams from
a US state, the distance between the
word basketball and the team name; e.g.,
SFL(basketball, heat).
5. baseball. For all MLB teams from a US
state, the distance between the word baseball
and the team name; e.g., SIL(baseball, cubs),
SIL(baseball, white sox).
6. hockey. For all NHL teams from a US state,
the distance between the word hockey and the
team name; e.g., SPA(hockey, penguins).
7. park. For all US national parks, the distance
between the word park and the park name;
e.g., SAK(park, denali).
</listItem>
<bodyText confidence="0.999897818181818">
Each of these questions asks the following:
what words are evoked for a given target word
(like football)? While football may everywhere
evoke similar sports like baseball or soccer or
more specific football-related terms like touch-
down or field goal, we expect that particular sports
teams will be evoked more strongly by the word
football in their particular geographical region: in
Wisconsin, football should evoke packers, while
in Pennsylvania, football evokes steelers. Note
that this is not the same as simply asking which
sports team is most frequently (or most character-
istically) mentioned in a given area; by measuring
the distance to a target word (football), we are at-
tempting to estimate the varying strengths of asso-
ciation between concepts in different regions.
For each category, we measure similarity as the
average cosine similarity between the vector for
the target word for that category (e.g., city) and the
corresponding vector for each state-specific an-
swer (e.g., chicago for IL; boston for MA). We
compare three different models:
</bodyText>
<listItem confidence="0.732484">
1. JOINT. The full model described in section
2, in which we learn a global representation
</listItem>
<bodyText confidence="0.964410428571429">
for each word along with deviations from that
common representation for each state.
2. INDIVIDUAL. For comparison, we also parti-
tion the data among all 51 states, and train a
single model for each state using only data
from that state. In this model, there is no
sharing among states; California has the most
</bodyText>
<page confidence="0.9948">
831
</page>
<figure confidence="0.391849">
city state baseball basketball football hockey park
</figure>
<figureCaption confidence="0.983647">
Figure 2: Average cosine similarity for all models across all categories, with 95% confidence intervals on the mean.
</figureCaption>
<figure confidence="0.99936075">
Model
Joint
Individual
–Geo
similarity 0.75
0.50
0.25
0.00
</figure>
<bodyText confidence="0.99225378125">
data with 11,604,637 tweets; Wyoming has
the least with 47,503 tweets.
3. –GEO. We also train a single model on all of
the training data, but ignore any state meta-
data. In this case the distance S between two
terms is their overall distance within the en-
tire United States.
As one concrete example of these differences
between individual data points, the cosine similar-
ity between city and seattle in the –GEO model
is 0.728 (seattle is ranked as the 188th most sim-
ilar term to city overall); in the INDIVIDUAL
model using only tweets from Washington state,
SWA(city, seattle) = 0.780 (rank #32); and in
the JOINT model, using information from the en-
tire United States with deviations for Washington,
SWA(city, seattle) = 0.858 (rank #6). The over-
all similarity for the city category of each model is
the average of 51 such tests (one for each city).
Figure 2 present the results of the full evalua-
tion, including 95% confidence intervals for each
mean. While the two models that include ge-
ographical information naturally outperform the
model that does not, the JOINT model generally
far outperforms the INDIVIDUAL models trained
on state-specific subsets of the data.1 A model that
can exploit all of the information in the data, learn-
ing core vector-space representations for all words
along with deviations for each contextual variable,
is able to learn more geographically-informed rep-
resentations for this task than strict geographical
models alone.
</bodyText>
<footnote confidence="0.99449175">
1This result is robust to the choice of distance metric; an
evaluation measuring the Euclidean distance between vectors
shows the JOINT model to outperform the INDIVIDUAL and
–GEO models across all seven categories.
</footnote>
<sectionHeader confidence="0.994574" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9999895">
We introduced a model for leveraging situational
information in learning vector-space representa-
tions of words that are sensitive to the speaker’s
social context. While our results use geographical
information in learning low-dimensional represen-
tations, other contextual variables are straightfor-
ward to include as well; incorporating effects for
time – such as time of day, month of year and ab-
solute year – may be a powerful tool for reveal-
ing periodic and historical influences on lexical se-
mantics.
Our approach explores the degree to which ge-
ography, and other contextual factors, influence
word meaning in addition to frequency of usage.
By allowing all words in different regions (or more
generally, with different metadata factors) to ex-
ist in the same vector space, we are able com-
pare different points in that space – for example,
to ask what terms used in Chicago are most simi-
lar to hot dog in New York, or what word groups
shift together in the same region in comparison
to the background (indicating the shift of an en-
tire semantic field). All datasets and software to
support these geographically-informed represen-
tations can be found at: http://www.ark.
cs.cmu.edu/geoSGLM.
</bodyText>
<sectionHeader confidence="0.999435" genericHeader="acknowledgments">
5 Acknowledgments
</sectionHeader>
<bodyText confidence="0.99838">
The research reported in this article was supported
by US NSF grants IIS-1251131 and CAREER IIS-
1054319, and by an ARCS scholarship to D.B.
This work was made possible through the use of
computing resources made available by the Open
Cloud Consortium, Yahoo and the Pittsburgh Su-
percomputing Center.
</bodyText>
<page confidence="0.994978">
832
</page>
<sectionHeader confidence="0.990186" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999817582524272">
Mark Andrews, Gabriella Vigliocco, and David Vin-
son. 2009. Integrating experiential and distribu-
tional data to learn semantic representations. Psy-
chological Review, 116(3):463–498.
David Bamman, Jacob Eisenstein, and Tyler Schnoe-
belen. 2014. Gender identity and lexical variation
in social media. Journal of Sociolinguistics, 18(2).
Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011.
Distributional semantics from text and images. In
Proc. of the Workshop on Geometrical Models of
Natural Language Semantics.
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012a. Distributional semantics in
technicolor. In Proc. ofACL.
Elia Bruni, Jasper Uijlings, Marco Baroni, and Nicu
Sebe. 2012b. Distributional semantics with eyes:
Using image analysis to improve computational rep-
resentations of word meaning. In Proc. of the ACM
International Conference on Multimedia.
Mary Bucholtz. 2006. Word up: Social meanings of
slang in California youth culture. In Jane Goodman
and Leila Monaghan, editors, A Cultural Approach
to Interpersonal Communication: Essential Read-
ings, Malden, MA. Blackwell.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493–2537.
Gabriel Doyle. 2014. Mapping dialectal variation by
querying social media. In Proc. of EACL.
Jacob Eisenstein, Brendan O’Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for
geographic lexical variation. In Proc. of EMNLP.
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.
2011. Discovering sociolinguistic associations with
structured sparsity. In Proc. of ACL.
Jacob Eisenstein, Brendan O’Connor, Noah A. Smith,
and Eric P. Xing. 2012. Mapping the geographical
diffusion of new words. arXiv, abs/1210.5268.
Yansong Feng and Mirella Lapata. 2010. Visual in-
formation in semantic representation. In Proc. of
NAACL.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146–162.
Liangjie Hong, Amr Ahmed, Siva Gurumurthy,
Alexander J. Smola, and Kostas Tsioutsiouliklis.
2012. Discovering geographical topics in the Twit-
ter stream. In Proc. of WWW.
Yohei Ikawa, Miki Enoki, and Michiaki Tatsubori.
2012. Location inference using microblog mes-
sages. In Proc. of WWW.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proc. of COLING-ACL.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Proc. of ICLR.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Robert G. Cowell and Zoubin Ghahramani, editors,
Proc. of AISTATS.
Brendan O’Connor, Jacob Eisenstein, Eric P. Xing, and
Noah A. Smith. 2010. Discovering demographic
language variation. In NIPS Workshop on Machine
Learning and Social Computing.
Sharon Oviatt. 2003. Multimodal interfaces.
In Julie A. Jacko and Andrew Sears, editors,
The Human-computer Interaction Handbook, pages
286–304, Hillsdale, NJ, USA. L. Erlbaum Asso-
ciates Inc.
Marco Pennacchiotti and Ana-Maria Popescu. 2011.
Democrats, Republicans and Starbucks afficionados:
User classification in Twitter. In Proc. of KDD.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In Proc. of the Workshop on
Search and Mining User-generated Contents.
Maya Ravindranath. 2011. A wicked good reason to
study intensifiers in New Hampshire. In NWAV 40.
Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Proc. of NAACL.
Stephen Roller and Sabine Schulte im Walde. 2013. A
multimodal LDA model integrating textual, cogni-
tive and visual modalities. In Proc. of EMNLP.
Stephen Roller, Michael Speriosu, Sarat Rallapalli,
Benjamin Wing, and Jason Baldridge. 2012. Super-
vised text-based geolocation using language models
on an adaptive grid. In Proc. of EMNLP-CoNLL.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proc. of EMNLP.
Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013. Parsing with composi-
tional vector grammars. In Proc. of ACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proc. of ACL.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37(1):141–188, January.
</reference>
<page confidence="0.989778">
833
</page>
<reference confidence="0.995561142857143">
Benjamin P. Wing and Jason Baldridge. 2011. Sim-
ple supervised document geolocation with geodesic
grids. In Proc. of ACL.
Chen Yu and Dana H. Ballard. 2004. A multimodal
learning interface for grounding spoken language in
sensory perceptions. ACM Transactions on Applied
Perception, 1(1):57–80.
</reference>
<page confidence="0.998706">
834
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.987770">
<title confidence="0.999785">Distributed Representations of Geographically Situated Language</title>
<author confidence="0.999996">David Bamman Chris Dyer Noah A Smith</author>
<affiliation confidence="0.9999575">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.999497">Pittsburgh, PA 15213, USA</address>
<abstract confidence="0.99936652631579">We introduce a model for incorporating contextual information (such as geography) in learning vector-space representaof In contrast to approaches to multimodal representation learning that have used properties of the described (such as its color), our model includes information about the the speaker), allowing us to learn the contours of a word’s meaning that are shaped by the context in which it is uttered. In a quantitative evaluation on the task of judging geographically informed semantic similarity between representations learned from 1.1 billion words of geo-located tweets, our joint model outperforms comparable independent models that learn meaning in isolation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mark Andrews</author>
<author>Gabriella Vigliocco</author>
<author>David Vinson</author>
</authors>
<title>Integrating experiential and distributional data to learn semantic representations.</title>
<date>2009</date>
<journal>Psychological Review,</journal>
<volume>116</volume>
<issue>3</issue>
<contexts>
<context position="3367" citStr="Andrews et al., 2009" startWordPosition="516" endWordPosition="520">roduce a method that extends vector-space lexical semantic models to learn representations of geographically situated language. Vector-space models of lexical semantics have been a popular and effective approach to learning representations of word meaning (Lin, 1998; Turney and Pantel, 2010; Reisinger and Mooney, 2010; Socher et al., 2013; Mikolov et al., 2013, inter alia). In bringing in extra-linguistic information to learn word representations, our work falls into the general domain of multimodal learning; while other work has used visual information to improve distributed representations (Andrews et al., 2009; Feng and Lapata, 2010; Bruni et al., 2011; Bruni et al., 2012a; Bruni et al., 2012b; Roller and im Walde, 2013), this work generally exploits information about the object being described (e.g., strawberry and a picture of a strawberry); in contrast, we use information about the speaker to learn representations that vary according to contextual variables from the speaker’s perspective. Unlike classic multimodal systems that incorporate multiple active modalities (such as gesture) from a user (Oviatt, 2003; Yu and 828 Proceedings of the 52nd Annual Meeting of the Association for Computational </context>
</contexts>
<marker>Andrews, Vigliocco, Vinson, 2009</marker>
<rawString>Mark Andrews, Gabriella Vigliocco, and David Vinson. 2009. Integrating experiential and distributional data to learn semantic representations. Psychological Review, 116(3):463–498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Bamman</author>
<author>Jacob Eisenstein</author>
<author>Tyler Schnoebelen</author>
</authors>
<title>Gender identity and lexical variation in social media.</title>
<date>2014</date>
<journal>Journal of Sociolinguistics,</journal>
<volume>18</volume>
<issue>2</issue>
<contexts>
<context position="1811" citStr="Bamman et al., 2014" startWordPosition="269" endWordPosition="272">ed with information about its author and social context, reminds us that each word is uttered by a particular person at a particular place and time. In short: language is situated. The coupling of text with demographic information has enabled computational modeling of linguistic variation, including uncovering words and topics that are characteristic of geographical regions (Eisenstein et al., 2010; O’Connor et al., 2010; Hong et al., 2012; Doyle, 2014), learning correlations between words and socioeconomic variables (Rao et al., 2010; Eisenstein et al., 2011; Pennacchiotti and Popescu, 2011; Bamman et al., 2014); and charting how new terms spread geographically (Eisenstein et al., 2012). These models can tell us that hella was (at one time) used most often by a particular demographic group in northern California, echoing earlier linguistic studies (Bucholtz, 2006), and that wicked is used most often in New England (Ravindranath, 2011); and they have practical applications, facilitating tasks like text-based geolocation (Wing and Baldridge, 2011; Roller et al., 2012; Ikawa et al., 2012). One desideratum that remains, however, is how the meaning of these terms is shaped by geographical influences – whi</context>
</contexts>
<marker>Bamman, Eisenstein, Schnoebelen, 2014</marker>
<rawString>David Bamman, Jacob Eisenstein, and Tyler Schnoebelen. 2014. Gender identity and lexical variation in social media. Journal of Sociolinguistics, 18(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Giang Binh Tran</author>
<author>Marco Baroni</author>
</authors>
<title>Distributional semantics from text and images.</title>
<date>2011</date>
<booktitle>In Proc. of the Workshop on Geometrical Models of Natural Language Semantics.</booktitle>
<contexts>
<context position="3410" citStr="Bruni et al., 2011" startWordPosition="525" endWordPosition="528">ical semantic models to learn representations of geographically situated language. Vector-space models of lexical semantics have been a popular and effective approach to learning representations of word meaning (Lin, 1998; Turney and Pantel, 2010; Reisinger and Mooney, 2010; Socher et al., 2013; Mikolov et al., 2013, inter alia). In bringing in extra-linguistic information to learn word representations, our work falls into the general domain of multimodal learning; while other work has used visual information to improve distributed representations (Andrews et al., 2009; Feng and Lapata, 2010; Bruni et al., 2011; Bruni et al., 2012a; Bruni et al., 2012b; Roller and im Walde, 2013), this work generally exploits information about the object being described (e.g., strawberry and a picture of a strawberry); in contrast, we use information about the speaker to learn representations that vary according to contextual variables from the speaker’s perspective. Unlike classic multimodal systems that incorporate multiple active modalities (such as gesture) from a user (Oviatt, 2003; Yu and 828 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 828–834, </context>
</contexts>
<marker>Bruni, Tran, Baroni, 2011</marker>
<rawString>Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011. Distributional semantics from text and images. In Proc. of the Workshop on Geometrical Models of Natural Language Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Gemma Boleda</author>
<author>Marco Baroni</author>
<author>NamKhanh Tran</author>
</authors>
<title>Distributional semantics in technicolor.</title>
<date>2012</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="3430" citStr="Bruni et al., 2012" startWordPosition="529" endWordPosition="532"> to learn representations of geographically situated language. Vector-space models of lexical semantics have been a popular and effective approach to learning representations of word meaning (Lin, 1998; Turney and Pantel, 2010; Reisinger and Mooney, 2010; Socher et al., 2013; Mikolov et al., 2013, inter alia). In bringing in extra-linguistic information to learn word representations, our work falls into the general domain of multimodal learning; while other work has used visual information to improve distributed representations (Andrews et al., 2009; Feng and Lapata, 2010; Bruni et al., 2011; Bruni et al., 2012a; Bruni et al., 2012b; Roller and im Walde, 2013), this work generally exploits information about the object being described (e.g., strawberry and a picture of a strawberry); in contrast, we use information about the speaker to learn representations that vary according to contextual variables from the speaker’s perspective. Unlike classic multimodal systems that incorporate multiple active modalities (such as gesture) from a user (Oviatt, 2003; Yu and 828 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 828–834, Baltimore, Maryland,</context>
</contexts>
<marker>Bruni, Boleda, Baroni, Tran, 2012</marker>
<rawString>Elia Bruni, Gemma Boleda, Marco Baroni, and NamKhanh Tran. 2012a. Distributional semantics in technicolor. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Jasper Uijlings</author>
<author>Marco Baroni</author>
<author>Nicu Sebe</author>
</authors>
<title>Distributional semantics with eyes: Using image analysis to improve computational representations of word meaning.</title>
<date>2012</date>
<booktitle>In Proc. of the ACM International Conference on Multimedia.</booktitle>
<contexts>
<context position="3430" citStr="Bruni et al., 2012" startWordPosition="529" endWordPosition="532"> to learn representations of geographically situated language. Vector-space models of lexical semantics have been a popular and effective approach to learning representations of word meaning (Lin, 1998; Turney and Pantel, 2010; Reisinger and Mooney, 2010; Socher et al., 2013; Mikolov et al., 2013, inter alia). In bringing in extra-linguistic information to learn word representations, our work falls into the general domain of multimodal learning; while other work has used visual information to improve distributed representations (Andrews et al., 2009; Feng and Lapata, 2010; Bruni et al., 2011; Bruni et al., 2012a; Bruni et al., 2012b; Roller and im Walde, 2013), this work generally exploits information about the object being described (e.g., strawberry and a picture of a strawberry); in contrast, we use information about the speaker to learn representations that vary according to contextual variables from the speaker’s perspective. Unlike classic multimodal systems that incorporate multiple active modalities (such as gesture) from a user (Oviatt, 2003; Yu and 828 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 828–834, Baltimore, Maryland,</context>
</contexts>
<marker>Bruni, Uijlings, Baroni, Sebe, 2012</marker>
<rawString>Elia Bruni, Jasper Uijlings, Marco Baroni, and Nicu Sebe. 2012b. Distributional semantics with eyes: Using image analysis to improve computational representations of word meaning. In Proc. of the ACM International Conference on Multimedia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Bucholtz</author>
</authors>
<title>Word up: Social meanings of slang in California youth culture.</title>
<date>2006</date>
<booktitle>In Jane Goodman and Leila Monaghan, editors, A Cultural Approach to Interpersonal Communication: Essential Readings,</booktitle>
<publisher>Blackwell.</publisher>
<location>Malden, MA.</location>
<contexts>
<context position="2068" citStr="Bucholtz, 2006" startWordPosition="312" endWordPosition="313">ing of linguistic variation, including uncovering words and topics that are characteristic of geographical regions (Eisenstein et al., 2010; O’Connor et al., 2010; Hong et al., 2012; Doyle, 2014), learning correlations between words and socioeconomic variables (Rao et al., 2010; Eisenstein et al., 2011; Pennacchiotti and Popescu, 2011; Bamman et al., 2014); and charting how new terms spread geographically (Eisenstein et al., 2012). These models can tell us that hella was (at one time) used most often by a particular demographic group in northern California, echoing earlier linguistic studies (Bucholtz, 2006), and that wicked is used most often in New England (Ravindranath, 2011); and they have practical applications, facilitating tasks like text-based geolocation (Wing and Baldridge, 2011; Roller et al., 2012; Ikawa et al., 2012). One desideratum that remains, however, is how the meaning of these terms is shaped by geographical influences – while wicked is used throughout the United States to mean bad or evil (“he is a wicked man”), in New England it is used as an adverbial intensifier (“my boy’s wicked smart”). In leveraging grounded social media to uncover linguistic variation, what we want to </context>
</contexts>
<marker>Bucholtz, 2006</marker>
<rawString>Mary Bucholtz. 2006. Word up: Social meanings of slang in California youth culture. In Jane Goodman and Leila Monaghan, editors, A Cultural Approach to Interpersonal Communication: Essential Readings, Malden, MA. Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="5759" citStr="Collobert et al., 2011" startWordPosition="899" endWordPosition="902">target word, either by linear distance or dependency path). We can invoke the distributional hypothesis for many instances of regional variation by observing that such variants often appear in similar contexts. For example: • my boy’s wicked smart • my boy’s hella smart • my boy’s very smart Here, all three variants can often be seen in an immediately pre-adjectival position (as is common with intensifying adverbs). Given the empirical success of vector-space representations in capturing semantic properties and their success at a variety of NLP tasks (Turian et al., 2010; Socher et al., 2011; Collobert et al., 2011; Socher et al., 2013), we use a simple, but state-of-the-art neural architecture (Mikolov et al., 2013) to learn low-dimensional real-valued representations of words. The graphical form of this model is illustrated in figure 1. This model corresponds to an extension of the “skip-gram” language model (Mikolov et al., 2013) (hereafter SGLM). Given an input sentence s and a context window of size t, each word si is conditioned on in turn to predict the identities of all of the tokens within t words around it. For a vocabulary V , each input word si is represented as a one-hot vector wi of length</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Doyle</author>
</authors>
<title>Mapping dialectal variation by querying social media.</title>
<date>2014</date>
<booktitle>In Proc. of EACL.</booktitle>
<contexts>
<context position="1648" citStr="Doyle, 2014" startWordPosition="248" endWordPosition="249">iamentary proceedings – can encourage a view of language as a disembodied phenomenon. The rise of social media, however, with its large volume of text paired with information about its author and social context, reminds us that each word is uttered by a particular person at a particular place and time. In short: language is situated. The coupling of text with demographic information has enabled computational modeling of linguistic variation, including uncovering words and topics that are characteristic of geographical regions (Eisenstein et al., 2010; O’Connor et al., 2010; Hong et al., 2012; Doyle, 2014), learning correlations between words and socioeconomic variables (Rao et al., 2010; Eisenstein et al., 2011; Pennacchiotti and Popescu, 2011; Bamman et al., 2014); and charting how new terms spread geographically (Eisenstein et al., 2012). These models can tell us that hella was (at one time) used most often by a particular demographic group in northern California, echoing earlier linguistic studies (Bucholtz, 2006), and that wicked is used most often in New England (Ravindranath, 2011); and they have practical applications, facilitating tasks like text-based geolocation (Wing and Baldridge, </context>
</contexts>
<marker>Doyle, 2014</marker>
<rawString>Gabriel Doyle. 2014. Mapping dialectal variation by querying social media. In Proc. of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Brendan O’Connor</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>A latent variable model for geographic lexical variation.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<marker>Eisenstein, O’Connor, Smith, Xing, 2010</marker>
<rawString>Jacob Eisenstein, Brendan O’Connor, Noah A. Smith, and Eric P. Xing. 2010. A latent variable model for geographic lexical variation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Discovering sociolinguistic associations with structured sparsity.</title>
<date>2011</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1756" citStr="Eisenstein et al., 2011" startWordPosition="261" endWordPosition="264"> social media, however, with its large volume of text paired with information about its author and social context, reminds us that each word is uttered by a particular person at a particular place and time. In short: language is situated. The coupling of text with demographic information has enabled computational modeling of linguistic variation, including uncovering words and topics that are characteristic of geographical regions (Eisenstein et al., 2010; O’Connor et al., 2010; Hong et al., 2012; Doyle, 2014), learning correlations between words and socioeconomic variables (Rao et al., 2010; Eisenstein et al., 2011; Pennacchiotti and Popescu, 2011; Bamman et al., 2014); and charting how new terms spread geographically (Eisenstein et al., 2012). These models can tell us that hella was (at one time) used most often by a particular demographic group in northern California, echoing earlier linguistic studies (Bucholtz, 2006), and that wicked is used most often in New England (Ravindranath, 2011); and they have practical applications, facilitating tasks like text-based geolocation (Wing and Baldridge, 2011; Roller et al., 2012; Ikawa et al., 2012). One desideratum that remains, however, is how the meaning of</context>
</contexts>
<marker>Eisenstein, Smith, Xing, 2011</marker>
<rawString>Jacob Eisenstein, Noah A. Smith, and Eric P. Xing. 2011. Discovering sociolinguistic associations with structured sparsity. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Brendan O’Connor</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Mapping the geographical diffusion of new words. arXiv,</title>
<date>2012</date>
<pages>1210--5268</pages>
<marker>Eisenstein, O’Connor, Smith, Xing, 2012</marker>
<rawString>Jacob Eisenstein, Brendan O’Connor, Noah A. Smith, and Eric P. Xing. 2012. Mapping the geographical diffusion of new words. arXiv, abs/1210.5268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Visual information in semantic representation.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="3390" citStr="Feng and Lapata, 2010" startWordPosition="521" endWordPosition="524">xtends vector-space lexical semantic models to learn representations of geographically situated language. Vector-space models of lexical semantics have been a popular and effective approach to learning representations of word meaning (Lin, 1998; Turney and Pantel, 2010; Reisinger and Mooney, 2010; Socher et al., 2013; Mikolov et al., 2013, inter alia). In bringing in extra-linguistic information to learn word representations, our work falls into the general domain of multimodal learning; while other work has used visual information to improve distributed representations (Andrews et al., 2009; Feng and Lapata, 2010; Bruni et al., 2011; Bruni et al., 2012a; Bruni et al., 2012b; Roller and im Walde, 2013), this work generally exploits information about the object being described (e.g., strawberry and a picture of a strawberry); in contrast, we use information about the speaker to learn representations that vary according to contextual variables from the speaker’s perspective. Unlike classic multimodal systems that incorporate multiple active modalities (such as gesture) from a user (Oviatt, 2003; Yu and 828 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Pape</context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010. Visual information in semantic representation. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1954</date>
<journal>Distributional structure. Word,</journal>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="4974" citStr="Harris, 1954" startWordPosition="766" endWordPosition="767">e learned low-dimensional embeddings. Ballard, 2004), our primary input is textual data, supplemented with metadata about the author and the moment of authorship. This information enables learning models of word meaning that are sensitive to such factors, allowing us to distinguish, for example, between the usage of wicked in Massachusetts from the usage of that word elsewhere, and letting us better associate geographically grounded named entities (e.g, Boston) with their hypernyms (city) in their respective regions. 2 Model The model we introduce is grounded in the distributional hypothesis (Harris, 1954), that two words are similar by appearing in the same kinds of contexts (where “context” itself can be variously defined as the bag or sequence of tokens around a target word, either by linear distance or dependency path). We can invoke the distributional hypothesis for many instances of regional variation by observing that such variants often appear in similar contexts. For example: • my boy’s wicked smart • my boy’s hella smart • my boy’s very smart Here, all three variants can often be seen in an immediately pre-adjectival position (as is common with intensifying adverbs). Given the empiric</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig Harris. 1954. Distributional structure. Word, 10(23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liangjie Hong</author>
<author>Amr Ahmed</author>
<author>Siva Gurumurthy</author>
<author>Alexander J Smola</author>
</authors>
<title>and Kostas Tsioutsiouliklis.</title>
<date>2012</date>
<booktitle>In Proc. of WWW.</booktitle>
<contexts>
<context position="1634" citStr="Hong et al., 2012" startWordPosition="244" endWordPosition="247">ire, web text, parliamentary proceedings – can encourage a view of language as a disembodied phenomenon. The rise of social media, however, with its large volume of text paired with information about its author and social context, reminds us that each word is uttered by a particular person at a particular place and time. In short: language is situated. The coupling of text with demographic information has enabled computational modeling of linguistic variation, including uncovering words and topics that are characteristic of geographical regions (Eisenstein et al., 2010; O’Connor et al., 2010; Hong et al., 2012; Doyle, 2014), learning correlations between words and socioeconomic variables (Rao et al., 2010; Eisenstein et al., 2011; Pennacchiotti and Popescu, 2011; Bamman et al., 2014); and charting how new terms spread geographically (Eisenstein et al., 2012). These models can tell us that hella was (at one time) used most often by a particular demographic group in northern California, echoing earlier linguistic studies (Bucholtz, 2006), and that wicked is used most often in New England (Ravindranath, 2011); and they have practical applications, facilitating tasks like text-based geolocation (Wing a</context>
</contexts>
<marker>Hong, Ahmed, Gurumurthy, Smola, 2012</marker>
<rawString>Liangjie Hong, Amr Ahmed, Siva Gurumurthy, Alexander J. Smola, and Kostas Tsioutsiouliklis. 2012. Discovering geographical topics in the Twitter stream. In Proc. of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohei Ikawa</author>
<author>Miki Enoki</author>
<author>Michiaki Tatsubori</author>
</authors>
<title>Location inference using microblog messages.</title>
<date>2012</date>
<booktitle>In Proc. of WWW.</booktitle>
<contexts>
<context position="2294" citStr="Ikawa et al., 2012" startWordPosition="344" endWordPosition="347">ween words and socioeconomic variables (Rao et al., 2010; Eisenstein et al., 2011; Pennacchiotti and Popescu, 2011; Bamman et al., 2014); and charting how new terms spread geographically (Eisenstein et al., 2012). These models can tell us that hella was (at one time) used most often by a particular demographic group in northern California, echoing earlier linguistic studies (Bucholtz, 2006), and that wicked is used most often in New England (Ravindranath, 2011); and they have practical applications, facilitating tasks like text-based geolocation (Wing and Baldridge, 2011; Roller et al., 2012; Ikawa et al., 2012). One desideratum that remains, however, is how the meaning of these terms is shaped by geographical influences – while wicked is used throughout the United States to mean bad or evil (“he is a wicked man”), in New England it is used as an adverbial intensifier (“my boy’s wicked smart”). In leveraging grounded social media to uncover linguistic variation, what we want to learn is how a word’s meaning is shaped by its geography. In this paper, we introduce a method that extends vector-space lexical semantic models to learn representations of geographically situated language. Vector-space models</context>
</contexts>
<marker>Ikawa, Enoki, Tatsubori, 2012</marker>
<rawString>Yohei Ikawa, Miki Enoki, and Michiaki Tatsubori. 2012. Location inference using microblog messages. In Proc. of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proc. of COLING-ACL.</booktitle>
<contexts>
<context position="3013" citStr="Lin, 1998" startWordPosition="463" endWordPosition="464">s – while wicked is used throughout the United States to mean bad or evil (“he is a wicked man”), in New England it is used as an adverbial intensifier (“my boy’s wicked smart”). In leveraging grounded social media to uncover linguistic variation, what we want to learn is how a word’s meaning is shaped by its geography. In this paper, we introduce a method that extends vector-space lexical semantic models to learn representations of geographically situated language. Vector-space models of lexical semantics have been a popular and effective approach to learning representations of word meaning (Lin, 1998; Turney and Pantel, 2010; Reisinger and Mooney, 2010; Socher et al., 2013; Mikolov et al., 2013, inter alia). In bringing in extra-linguistic information to learn word representations, our work falls into the general domain of multimodal learning; while other work has used visual information to improve distributed representations (Andrews et al., 2009; Feng and Lapata, 2010; Bruni et al., 2011; Bruni et al., 2012a; Bruni et al., 2012b; Roller and im Walde, 2013), this work generally exploits information about the object being described (e.g., strawberry and a picture of a strawberry); in cont</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proc. of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proc. of ICLR.</booktitle>
<contexts>
<context position="3109" citStr="Mikolov et al., 2013" startWordPosition="477" endWordPosition="480">icked man”), in New England it is used as an adverbial intensifier (“my boy’s wicked smart”). In leveraging grounded social media to uncover linguistic variation, what we want to learn is how a word’s meaning is shaped by its geography. In this paper, we introduce a method that extends vector-space lexical semantic models to learn representations of geographically situated language. Vector-space models of lexical semantics have been a popular and effective approach to learning representations of word meaning (Lin, 1998; Turney and Pantel, 2010; Reisinger and Mooney, 2010; Socher et al., 2013; Mikolov et al., 2013, inter alia). In bringing in extra-linguistic information to learn word representations, our work falls into the general domain of multimodal learning; while other work has used visual information to improve distributed representations (Andrews et al., 2009; Feng and Lapata, 2010; Bruni et al., 2011; Bruni et al., 2012a; Bruni et al., 2012b; Roller and im Walde, 2013), this work generally exploits information about the object being described (e.g., strawberry and a picture of a strawberry); in contrast, we use information about the speaker to learn representations that vary according to conte</context>
<context position="5863" citStr="Mikolov et al., 2013" startWordPosition="915" endWordPosition="918">or many instances of regional variation by observing that such variants often appear in similar contexts. For example: • my boy’s wicked smart • my boy’s hella smart • my boy’s very smart Here, all three variants can often be seen in an immediately pre-adjectival position (as is common with intensifying adverbs). Given the empirical success of vector-space representations in capturing semantic properties and their success at a variety of NLP tasks (Turian et al., 2010; Socher et al., 2011; Collobert et al., 2011; Socher et al., 2013), we use a simple, but state-of-the-art neural architecture (Mikolov et al., 2013) to learn low-dimensional real-valued representations of words. The graphical form of this model is illustrated in figure 1. This model corresponds to an extension of the “skip-gram” language model (Mikolov et al., 2013) (hereafter SGLM). Given an input sentence s and a context window of size t, each word si is conditioned on in turn to predict the identities of all of the tokens within t words around it. For a vocabulary V , each input word si is represented as a one-hot vector wi of length |V |. The SGLM has two sets of parameters. The first is the representation matrix W E R|V |×k, which en</context>
<context position="9159" citStr="Mikolov et al. (2013)" startWordPosition="1490" endWordPosition="1493">dicates how that common representation should shift in kdimensional space when used in Massachusetts. Backpropagation functions as in standard SGLM, with gradient updates for each training example {x, y} touching not only Wmain (as in SGLM), but all active WA as well. The additional W embeddings we add lead to an increase in the number of total parameters by a factor of |C|. To control for the extra degrees of freedom this entails, we add squared E2 regularization to all parameters, using stochastic gradient descent for backpropagation with minibatch updates for the regularization term. As in Mikolov et al. (2013), we speed up computation using the hierarchical softmax (Morin and Bengio, 2005) on the output matrix X. This model defines a joint parameterization over all variable values in the data, where information from data originating in California, for instance, can influence the representations learned for Wisconsin; a naive alternative would be to simply train individual models on each variable value (a “California” model using data only from California, etc.). A joint model has three a priori advantages over independent models: (i) sharing data across variable values encourages representations ac</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. In Proc. of ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederic Morin</author>
<author>Yoshua Bengio</author>
</authors>
<title>Hierarchical probabilistic neural network language model.</title>
<date>2005</date>
<booktitle>Proc. of AISTATS.</booktitle>
<editor>In Robert G. Cowell and Zoubin Ghahramani, editors,</editor>
<contexts>
<context position="9240" citStr="Morin and Bengio, 2005" startWordPosition="1502" endWordPosition="1505">used in Massachusetts. Backpropagation functions as in standard SGLM, with gradient updates for each training example {x, y} touching not only Wmain (as in SGLM), but all active WA as well. The additional W embeddings we add lead to an increase in the number of total parameters by a factor of |C|. To control for the extra degrees of freedom this entails, we add squared E2 regularization to all parameters, using stochastic gradient descent for backpropagation with minibatch updates for the regularization term. As in Mikolov et al. (2013), we speed up computation using the hierarchical softmax (Morin and Bengio, 2005) on the output matrix X. This model defines a joint parameterization over all variable values in the data, where information from data originating in California, for instance, can influence the representations learned for Wisconsin; a naive alternative would be to simply train individual models on each variable value (a “California” model using data only from California, etc.). A joint model has three a priori advantages over independent models: (i) sharing data across variable values encourages representations across those values to be similar; e.g., while city may be closer to Boston in Mass</context>
</contexts>
<marker>Morin, Bengio, 2005</marker>
<rawString>Frederic Morin and Yoshua Bengio. 2005. Hierarchical probabilistic neural network language model. In Robert G. Cowell and Zoubin Ghahramani, editors, Proc. of AISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Jacob Eisenstein</author>
<author>Eric P Xing</author>
<author>Noah A Smith</author>
</authors>
<title>Discovering demographic language variation.</title>
<date>2010</date>
<booktitle>In NIPS Workshop on Machine Learning and Social Computing.</booktitle>
<marker>O’Connor, Eisenstein, Xing, Smith, 2010</marker>
<rawString>Brendan O’Connor, Jacob Eisenstein, Eric P. Xing, and Noah A. Smith. 2010. Discovering demographic language variation. In NIPS Workshop on Machine Learning and Social Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Oviatt</author>
</authors>
<title>Multimodal interfaces.</title>
<date>2003</date>
<booktitle>The Human-computer Interaction Handbook,</booktitle>
<pages>286--304</pages>
<editor>In Julie A. Jacko and Andrew Sears, editors,</editor>
<publisher>Associates Inc.</publisher>
<location>Hillsdale, NJ, USA.</location>
<contexts>
<context position="3878" citStr="Oviatt, 2003" startWordPosition="598" endWordPosition="599">e other work has used visual information to improve distributed representations (Andrews et al., 2009; Feng and Lapata, 2010; Bruni et al., 2011; Bruni et al., 2012a; Bruni et al., 2012b; Roller and im Walde, 2013), this work generally exploits information about the object being described (e.g., strawberry and a picture of a strawberry); in contrast, we use information about the speaker to learn representations that vary according to contextual variables from the speaker’s perspective. Unlike classic multimodal systems that incorporate multiple active modalities (such as gesture) from a user (Oviatt, 2003; Yu and 828 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 828–834, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Main h Alabama W Alaska Arizona Arkansas X o Figure 1: Model. Illustrated are the input dimensions that fire for a single sample, reflecting a particular word (vocabulary item #2) spoken in Alaska, along with a single output. Parameter matrix W consists of the learned low-dimensional embeddings. Ballard, 2004), our primary input is textual data, supplemented with metadata a</context>
</contexts>
<marker>Oviatt, 2003</marker>
<rawString>Sharon Oviatt. 2003. Multimodal interfaces. In Julie A. Jacko and Andrew Sears, editors, The Human-computer Interaction Handbook, pages 286–304, Hillsdale, NJ, USA. L. Erlbaum Associates Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Pennacchiotti</author>
<author>Ana-Maria Popescu</author>
</authors>
<title>Democrats, Republicans and Starbucks afficionados: User classification in Twitter.</title>
<date>2011</date>
<booktitle>In Proc. of KDD.</booktitle>
<contexts>
<context position="1789" citStr="Pennacchiotti and Popescu, 2011" startWordPosition="265" endWordPosition="268">ith its large volume of text paired with information about its author and social context, reminds us that each word is uttered by a particular person at a particular place and time. In short: language is situated. The coupling of text with demographic information has enabled computational modeling of linguistic variation, including uncovering words and topics that are characteristic of geographical regions (Eisenstein et al., 2010; O’Connor et al., 2010; Hong et al., 2012; Doyle, 2014), learning correlations between words and socioeconomic variables (Rao et al., 2010; Eisenstein et al., 2011; Pennacchiotti and Popescu, 2011; Bamman et al., 2014); and charting how new terms spread geographically (Eisenstein et al., 2012). These models can tell us that hella was (at one time) used most often by a particular demographic group in northern California, echoing earlier linguistic studies (Bucholtz, 2006), and that wicked is used most often in New England (Ravindranath, 2011); and they have practical applications, facilitating tasks like text-based geolocation (Wing and Baldridge, 2011; Roller et al., 2012; Ikawa et al., 2012). One desideratum that remains, however, is how the meaning of these terms is shaped by geograp</context>
</contexts>
<marker>Pennacchiotti, Popescu, 2011</marker>
<rawString>Marco Pennacchiotti and Ana-Maria Popescu. 2011. Democrats, Republicans and Starbucks afficionados: User classification in Twitter. In Proc. of KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>David Yarowsky</author>
<author>Abhishek Shreevats</author>
<author>Manaswi Gupta</author>
</authors>
<title>Classifying latent user attributes in Twitter.</title>
<date>2010</date>
<booktitle>In Proc. of the Workshop on Search and Mining User-generated Contents.</booktitle>
<contexts>
<context position="1731" citStr="Rao et al., 2010" startWordPosition="257" endWordPosition="260">menon. The rise of social media, however, with its large volume of text paired with information about its author and social context, reminds us that each word is uttered by a particular person at a particular place and time. In short: language is situated. The coupling of text with demographic information has enabled computational modeling of linguistic variation, including uncovering words and topics that are characteristic of geographical regions (Eisenstein et al., 2010; O’Connor et al., 2010; Hong et al., 2012; Doyle, 2014), learning correlations between words and socioeconomic variables (Rao et al., 2010; Eisenstein et al., 2011; Pennacchiotti and Popescu, 2011; Bamman et al., 2014); and charting how new terms spread geographically (Eisenstein et al., 2012). These models can tell us that hella was (at one time) used most often by a particular demographic group in northern California, echoing earlier linguistic studies (Bucholtz, 2006), and that wicked is used most often in New England (Ravindranath, 2011); and they have practical applications, facilitating tasks like text-based geolocation (Wing and Baldridge, 2011; Roller et al., 2012; Ikawa et al., 2012). One desideratum that remains, howev</context>
</contexts>
<marker>Rao, Yarowsky, Shreevats, Gupta, 2010</marker>
<rawString>Delip Rao, David Yarowsky, Abhishek Shreevats, and Manaswi Gupta. 2010. Classifying latent user attributes in Twitter. In Proc. of the Workshop on Search and Mining User-generated Contents.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maya Ravindranath</author>
</authors>
<title>A wicked good reason to study intensifiers in New Hampshire.</title>
<date>2011</date>
<booktitle>In NWAV 40.</booktitle>
<contexts>
<context position="2140" citStr="Ravindranath, 2011" startWordPosition="324" endWordPosition="325">hat are characteristic of geographical regions (Eisenstein et al., 2010; O’Connor et al., 2010; Hong et al., 2012; Doyle, 2014), learning correlations between words and socioeconomic variables (Rao et al., 2010; Eisenstein et al., 2011; Pennacchiotti and Popescu, 2011; Bamman et al., 2014); and charting how new terms spread geographically (Eisenstein et al., 2012). These models can tell us that hella was (at one time) used most often by a particular demographic group in northern California, echoing earlier linguistic studies (Bucholtz, 2006), and that wicked is used most often in New England (Ravindranath, 2011); and they have practical applications, facilitating tasks like text-based geolocation (Wing and Baldridge, 2011; Roller et al., 2012; Ikawa et al., 2012). One desideratum that remains, however, is how the meaning of these terms is shaped by geographical influences – while wicked is used throughout the United States to mean bad or evil (“he is a wicked man”), in New England it is used as an adverbial intensifier (“my boy’s wicked smart”). In leveraging grounded social media to uncover linguistic variation, what we want to learn is how a word’s meaning is shaped by its geography. In this paper,</context>
</contexts>
<marker>Ravindranath, 2011</marker>
<rawString>Maya Ravindranath. 2011. A wicked good reason to study intensifiers in New Hampshire. In NWAV 40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Raymond J Mooney</author>
</authors>
<title>Multi-prototype vector-space models of word meaning.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="3066" citStr="Reisinger and Mooney, 2010" startWordPosition="469" endWordPosition="472">the United States to mean bad or evil (“he is a wicked man”), in New England it is used as an adverbial intensifier (“my boy’s wicked smart”). In leveraging grounded social media to uncover linguistic variation, what we want to learn is how a word’s meaning is shaped by its geography. In this paper, we introduce a method that extends vector-space lexical semantic models to learn representations of geographically situated language. Vector-space models of lexical semantics have been a popular and effective approach to learning representations of word meaning (Lin, 1998; Turney and Pantel, 2010; Reisinger and Mooney, 2010; Socher et al., 2013; Mikolov et al., 2013, inter alia). In bringing in extra-linguistic information to learn word representations, our work falls into the general domain of multimodal learning; while other work has used visual information to improve distributed representations (Andrews et al., 2009; Feng and Lapata, 2010; Bruni et al., 2011; Bruni et al., 2012a; Bruni et al., 2012b; Roller and im Walde, 2013), this work generally exploits information about the object being described (e.g., strawberry and a picture of a strawberry); in contrast, we use information about the speaker to learn r</context>
</contexts>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>Joseph Reisinger and Raymond J. Mooney. 2010. Multi-prototype vector-space models of word meaning. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Roller</author>
<author>Sabine Schulte im Walde</author>
</authors>
<title>A multimodal LDA model integrating textual, cognitive and visual modalities.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<marker>Roller, Walde, 2013</marker>
<rawString>Stephen Roller and Sabine Schulte im Walde. 2013. A multimodal LDA model integrating textual, cognitive and visual modalities. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Roller</author>
<author>Michael Speriosu</author>
<author>Sarat Rallapalli</author>
<author>Benjamin Wing</author>
<author>Jason Baldridge</author>
</authors>
<title>Supervised text-based geolocation using language models on an adaptive grid.</title>
<date>2012</date>
<booktitle>In Proc. of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="2273" citStr="Roller et al., 2012" startWordPosition="340" endWordPosition="343">ning correlations between words and socioeconomic variables (Rao et al., 2010; Eisenstein et al., 2011; Pennacchiotti and Popescu, 2011; Bamman et al., 2014); and charting how new terms spread geographically (Eisenstein et al., 2012). These models can tell us that hella was (at one time) used most often by a particular demographic group in northern California, echoing earlier linguistic studies (Bucholtz, 2006), and that wicked is used most often in New England (Ravindranath, 2011); and they have practical applications, facilitating tasks like text-based geolocation (Wing and Baldridge, 2011; Roller et al., 2012; Ikawa et al., 2012). One desideratum that remains, however, is how the meaning of these terms is shaped by geographical influences – while wicked is used throughout the United States to mean bad or evil (“he is a wicked man”), in New England it is used as an adverbial intensifier (“my boy’s wicked smart”). In leveraging grounded social media to uncover linguistic variation, what we want to learn is how a word’s meaning is shaped by its geography. In this paper, we introduce a method that extends vector-space lexical semantic models to learn representations of geographically situated language</context>
</contexts>
<marker>Roller, Speriosu, Rallapalli, Wing, Baldridge, 2012</marker>
<rawString>Stephen Roller, Michael Speriosu, Sarat Rallapalli, Benjamin Wing, and Jason Baldridge. 2012. Supervised text-based geolocation using language models on an adaptive grid. In Proc. of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="5735" citStr="Socher et al., 2011" startWordPosition="895" endWordPosition="898">e of tokens around a target word, either by linear distance or dependency path). We can invoke the distributional hypothesis for many instances of regional variation by observing that such variants often appear in similar contexts. For example: • my boy’s wicked smart • my boy’s hella smart • my boy’s very smart Here, all three variants can often be seen in an immediately pre-adjectival position (as is common with intensifying adverbs). Given the empirical success of vector-space representations in capturing semantic properties and their success at a variety of NLP tasks (Turian et al., 2010; Socher et al., 2011; Collobert et al., 2011; Socher et al., 2013), we use a simple, but state-of-the-art neural architecture (Mikolov et al., 2013) to learn low-dimensional real-valued representations of words. The graphical form of this model is illustrated in figure 1. This model corresponds to an extension of the “skip-gram” language model (Mikolov et al., 2013) (hereafter SGLM). Given an input sentence s and a context window of size t, each word si is conditioned on in turn to predict the identities of all of the tokens within t words around it. For a vocabulary V , each input word si is represented as a one</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Ng Andrew Y</author>
</authors>
<title>Parsing with compositional vector grammars.</title>
<date>2013</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="3087" citStr="Socher et al., 2013" startWordPosition="473" endWordPosition="476">d or evil (“he is a wicked man”), in New England it is used as an adverbial intensifier (“my boy’s wicked smart”). In leveraging grounded social media to uncover linguistic variation, what we want to learn is how a word’s meaning is shaped by its geography. In this paper, we introduce a method that extends vector-space lexical semantic models to learn representations of geographically situated language. Vector-space models of lexical semantics have been a popular and effective approach to learning representations of word meaning (Lin, 1998; Turney and Pantel, 2010; Reisinger and Mooney, 2010; Socher et al., 2013; Mikolov et al., 2013, inter alia). In bringing in extra-linguistic information to learn word representations, our work falls into the general domain of multimodal learning; while other work has used visual information to improve distributed representations (Andrews et al., 2009; Feng and Lapata, 2010; Bruni et al., 2011; Bruni et al., 2012a; Bruni et al., 2012b; Roller and im Walde, 2013), this work generally exploits information about the object being described (e.g., strawberry and a picture of a strawberry); in contrast, we use information about the speaker to learn representations that v</context>
<context position="5781" citStr="Socher et al., 2013" startWordPosition="903" endWordPosition="906">inear distance or dependency path). We can invoke the distributional hypothesis for many instances of regional variation by observing that such variants often appear in similar contexts. For example: • my boy’s wicked smart • my boy’s hella smart • my boy’s very smart Here, all three variants can often be seen in an immediately pre-adjectival position (as is common with intensifying adverbs). Given the empirical success of vector-space representations in capturing semantic properties and their success at a variety of NLP tasks (Turian et al., 2010; Socher et al., 2011; Collobert et al., 2011; Socher et al., 2013), we use a simple, but state-of-the-art neural architecture (Mikolov et al., 2013) to learn low-dimensional real-valued representations of words. The graphical form of this model is illustrated in figure 1. This model corresponds to an extension of the “skip-gram” language model (Mikolov et al., 2013) (hereafter SGLM). Given an input sentence s and a context window of size t, each word si is conditioned on in turn to predict the identities of all of the tokens within t words around it. For a vocabulary V , each input word si is represented as a one-hot vector wi of length |V |. The SGLM has tw</context>
</contexts>
<marker>Socher, Bauer, Manning, Y, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D. Manning, and Ng Andrew Y. 2013. Parsing with compositional vector grammars. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="5714" citStr="Turian et al., 2010" startWordPosition="891" endWordPosition="894">as the bag or sequence of tokens around a target word, either by linear distance or dependency path). We can invoke the distributional hypothesis for many instances of regional variation by observing that such variants often appear in similar contexts. For example: • my boy’s wicked smart • my boy’s hella smart • my boy’s very smart Here, all three variants can often be seen in an immediately pre-adjectival position (as is common with intensifying adverbs). Given the empirical success of vector-space representations in capturing semantic properties and their success at a variety of NLP tasks (Turian et al., 2010; Socher et al., 2011; Collobert et al., 2011; Socher et al., 2013), we use a simple, but state-of-the-art neural architecture (Mikolov et al., 2013) to learn low-dimensional real-valued representations of words. The graphical form of this model is illustrated in figure 1. This model corresponds to an extension of the “skip-gram” language model (Mikolov et al., 2013) (hereafter SGLM). Given an input sentence s and a context window of size t, each word si is conditioned on in turn to predict the identities of all of the tokens within t words around it. For a vocabulary V , each input word si is</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="3038" citStr="Turney and Pantel, 2010" startWordPosition="465" endWordPosition="468">icked is used throughout the United States to mean bad or evil (“he is a wicked man”), in New England it is used as an adverbial intensifier (“my boy’s wicked smart”). In leveraging grounded social media to uncover linguistic variation, what we want to learn is how a word’s meaning is shaped by its geography. In this paper, we introduce a method that extends vector-space lexical semantic models to learn representations of geographically situated language. Vector-space models of lexical semantics have been a popular and effective approach to learning representations of word meaning (Lin, 1998; Turney and Pantel, 2010; Reisinger and Mooney, 2010; Socher et al., 2013; Mikolov et al., 2013, inter alia). In bringing in extra-linguistic information to learn word representations, our work falls into the general domain of multimodal learning; while other work has used visual information to improve distributed representations (Andrews et al., 2009; Feng and Lapata, 2010; Bruni et al., 2011; Bruni et al., 2012a; Bruni et al., 2012b; Roller and im Walde, 2013), this work generally exploits information about the object being described (e.g., strawberry and a picture of a strawberry); in contrast, we use information </context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37(1):141–188, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin P Wing</author>
<author>Jason Baldridge</author>
</authors>
<title>Simple supervised document geolocation with geodesic grids.</title>
<date>2011</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="2252" citStr="Wing and Baldridge, 2011" startWordPosition="336" endWordPosition="339">, 2012; Doyle, 2014), learning correlations between words and socioeconomic variables (Rao et al., 2010; Eisenstein et al., 2011; Pennacchiotti and Popescu, 2011; Bamman et al., 2014); and charting how new terms spread geographically (Eisenstein et al., 2012). These models can tell us that hella was (at one time) used most often by a particular demographic group in northern California, echoing earlier linguistic studies (Bucholtz, 2006), and that wicked is used most often in New England (Ravindranath, 2011); and they have practical applications, facilitating tasks like text-based geolocation (Wing and Baldridge, 2011; Roller et al., 2012; Ikawa et al., 2012). One desideratum that remains, however, is how the meaning of these terms is shaped by geographical influences – while wicked is used throughout the United States to mean bad or evil (“he is a wicked man”), in New England it is used as an adverbial intensifier (“my boy’s wicked smart”). In leveraging grounded social media to uncover linguistic variation, what we want to learn is how a word’s meaning is shaped by its geography. In this paper, we introduce a method that extends vector-space lexical semantic models to learn representations of geographica</context>
</contexts>
<marker>Wing, Baldridge, 2011</marker>
<rawString>Benjamin P. Wing and Jason Baldridge. 2011. Simple supervised document geolocation with geodesic grids. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Yu</author>
<author>Dana H Ballard</author>
</authors>
<title>A multimodal learning interface for grounding spoken language in sensory perceptions.</title>
<date>2004</date>
<journal>ACM Transactions on Applied Perception,</journal>
<volume>1</volume>
<issue>1</issue>
<marker>Yu, Ballard, 2004</marker>
<rawString>Chen Yu and Dana H. Ballard. 2004. A multimodal learning interface for grounding spoken language in sensory perceptions. ACM Transactions on Applied Perception, 1(1):57–80.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>