<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000084">
<title confidence="0.9817165">
Generation of single-sentence paraphrases from
predicate/argument structure using lexico-grammatical resources
</title>
<author confidence="0.999573">
Raymond Kozlowski, Kathleen F. McCoy, and K. Vijay-Shanker
</author>
<affiliation confidence="0.996653">
Department of Computer and Information Sciences
University of Delaware
</affiliation>
<address confidence="0.751463">
Newark, DE 19716, USA
</address>
<email confidence="0.998448">
kozlowsk,mccoy,vijay@cis.udel.edu
</email>
<sectionHeader confidence="0.983164" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999870714285714">
Paraphrases, which stem from the va-
riety of lexical and grammatical means
of expressing meaning available in a
language, pose challenges for a sen-
tence generation system. In this
paper, we discuss the generation of
paraphrases from predicate/argument
structure using a simple, uniform gen-
eration methodology. Central to our
approach are lexico-grammatical re-
sources which pair elementary seman-
tic structures with their syntactic re-
alization and a simple but powerful
mechanism for combining resources.
</bodyText>
<sectionHeader confidence="0.997335" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999869119047619">
In natural language generation, producing some
realization of the input semantics is not the only
goal. The same meaning can often be expressed
in various ways using different lexical and syn-
tactic means. These different realizations, called
paraphrases, vary considerably in appropriate-
ness based on pragmatic factors and commu-
nicative goals. If a generator is to come up with
the most appropriate realization, it must be ca-
pable of generating all paraphrases that realize
the input semantics. Even if it makes choices on
pragmatic grounds during generation and pro-
duces a single realization, the ability to generate
them all must still exist.
Variety of lexical and grammatical forms
of expression pose challenges to a generator
((Stede, 1999); (Elhadad et al., 1997); (Nicolov
et al., 1995)). In this paper, we discuss the gen-
eration of single-sentence paraphrases realizing
the same semantics in a uniform fashion using a
simple sentence generation architecture.
In order to handle the various ways of realiz-
ing meaning in a simple manner, we believe that
the generation architecture should not be aware
of the variety and not have any special mech-
anisms to handle the different types of realiza-
tions&apos;. Instead, we want all lexical and gram-
matical variety to follow automatically from the
variety of the elementary building blocks of gen-
eration, lexico-grammatical resources.
We have developed a fully-operational proto-
type of our generation system capable of gen-
erating the examples presented in this paper,
which illustrate a wide range of paraphrases.
As we shall see, the paraphrases that are pro-
duced by the system depend entirely on the
actual lexicon used in the particular applica-
tion. Determining the range of alternate forms
that constitute paraphrases is not the focus of
this work. Instead, we describe a framework in
which lexico-grammatical resources, if properly
defined, can be used to generate paraphrases.
</bodyText>
<sectionHeader confidence="0.58635" genericHeader="method">
2 Typical generation methodology
</sectionHeader>
<bodyText confidence="0.966403833333333">
Sentence generation takes as input some seman-
tic representation of the meaning to be conveyed
in a sentence. We make the assumption that
&apos;Ability to handle variety in a uniform manner is also
important in multilingual generation as some forms avail-
able in one language may not be available in another.
</bodyText>
<note confidence="0.698343">
ENJOY
</note>
<figureCaption confidence="0.999778">
Figure 1: The semantics underlying (2a-2b)
</figureCaption>
<bodyText confidence="0.999615555555556">
the input is a hierarchical predicate/argument
structure such as that shown in Fig. 1. The
output of this process should be a set of gram-
matical sentences whose meaning matches the
original semantic input.
One standard approach to sentence genera-
tion from predicate/argument structure (like the
semantic-head-driven generation in (Shieber et
al., 1990)) involves a simple algorithm.
</bodyText>
<listItem confidence="0.825002875">
1. decompose the input into the top predicate
(to be realized by a (single) lexical item that
serves as the syntactic head) and identify
the arguments and modifiers
2. recursively realize arguments, then modi-
fiers
3. combine the realizations in step 2 with the
head in step 1
</listItem>
<bodyText confidence="0.999982214285714">
In realizing the input in Fig. 1, the input can be
decomposed into the top predicate which can be
realized by a syntactic head (a transitive verb)
and its two arguments, the experiencer and the
theme. Suppose that the verb enjoy is chosen
to realize the top predicate. The two arguments
can then be independently realized as Amy and
the interaction. Finally, the realization of the
experiencer, Amy, can be placed in the subject
position and that of the theme, the interaction,
in the complement position, yielding (2a).
Our architecture is very similar but we argue
for a more central role of lexico-grammatical re-
sources driving the realization process.
</bodyText>
<sectionHeader confidence="0.986099" genericHeader="method">
3 Challenges in generating
paraphrases
</sectionHeader>
<bodyText confidence="0.9960398">
Paraphrases come from various sources. In this
section, we give examples of some types of para-
phrases we handle and discuss the challenges
they pose to other generators. We also identify
types of paraphrases we do not consider.
</bodyText>
<subsectionHeader confidence="0.998186">
3.1 Paraphrases we handle
</subsectionHeader>
<bodyText confidence="0.9993092">
Simple synonymy The simplest source of
paraphrases is simple synonymy. We take sim-
ple synonyms to be different words that have
the same meaning and are of the same syntactic
category and set up the same syntactic context.
</bodyText>
<listItem confidence="0.5656485">
(1a) Booth killed Lincoln.
(1b) Booth assassinated Lincoln.
</listItem>
<bodyText confidence="0.980799846153846">
A generation system must be able to allow
the same semantic input to be realized in dif-
ferent ways. Notice that the words kill and as-
sassinate are not always interchangeable, e.g.,
assassinate is only appropriate when the victim
is a famous person. Such constraints need to be
captured with selectional restrictions lest inap-
propriate realizations be produced.
Different placement of argument realiza-
tions Sometimes different synonyms, like the
verbs enjoy and please, place argument realiza-
tions differently with respect to the head, as il-
lustrated in (2a-2b).
</bodyText>
<listItem confidence="0.9357705">
(2a) Amy enjoyed the interaction.
(2b) The interaction pleased Amy.
</listItem>
<bodyText confidence="0.9934328125">
To handle this variety, a uniform generation
methodology should not assume a fixed map-
ping between thematic and syntactic roles but
let each lexical item determine the placement of
argument realizations. Generation systems that
use such a fixed mapping must override it for
the divergent cases (e.g., (Dorr, 1993)).
Words with overlapping meaning There
are often cases of different words that realize dif-
ferent but overlapping semantic pieces. The eas-
iest way to see this is in what has been termed
incorporation, where a word not only realizes a
predicate but also one or more of its arguments.
Different words may incorporate different argu-
ments or none at all, which may lead to para-
phrases, as illustrated in (3a-3c).
</bodyText>
<listItem confidence="0.996684333333333">
(3a) Charles flew across the ocean.
(3b) Charles crossed the ocean by plane.
(3c) Charles went across the ocean by plane.
</listItem>
<bodyText confidence="0.9994695">
Notice that the verb fly realizes not only go-
ing but also the mode of transportation being a
plane, the verb cross with its complement real-
ize going whose path is across the object realized
</bodyText>
<sectionHeader confidence="0.811788" genericHeader="method">
AMY INTERACTION
EXPERIENCER THEME
</sectionHeader>
<bodyText confidence="0.989605470588235">
by the complement, and the verb go only real-
izes going. For all of these verbs, the remaining
arguments are realized by modifiers.
Incorporation shows that a uniform genera-
tor should use the word choices to determine 1)
what portion of the semantics they realize, 2)
what portions are to be realized as arguments
of the realized semantics, and 3) what portions
remain to be realized and attached as modifiers.
Generation systems that assume a one-to-one
mapping between semantic and syntactic units
(e.g., (Dorr, 1993)) must use special processing
for cases of overlapping semantics.
Different syntactic categories Predicates
can often be realized by words of different syn-
tactic categories, e.g., the verb found and the
noun founding, as in (4a-4b).
</bodyText>
<equation confidence="0.9752845">
(4a) I know that Olds founded GM.
(4b) I know about the founding of GM by Olds.
</equation>
<bodyText confidence="0.99874205882353">
Words of different syntactic categories usu-
ally have different syntactic consequences. One
such consequence is the presence of additional
syntactic material. Notice that (4b) contains
the prepositions of and by while (4a) does not.
These prepositions might be considered a syn-
tactic consequence of the use of the noun found-
ing in this configuration. Another syntactic con-
sequence is a different placement of argument re-
alizations. The realization of the founder is the
subject of the verb found in (4a) while in (4b)
the use of founding leads to its placement in the
object position of the preposition by.
Grammatical alternations Words can be
put in a variety of grammatical alternations such
as the active and passive voice, as in (5a-5b), the
topicalized form, the it-cleft form, etc.
</bodyText>
<listItem confidence="0.907531">
(5a) Oswald killed Kennedy.
(5b) Kennedy was killed by Oswald.
</listItem>
<bodyText confidence="0.999632666666667">
The choice of different grammatical alterna-
tions has different syntactic consequences which
must be enforced in generation, such as the pres-
ence or absence of the copula and the different
placement of argument realizations. In some
systems such as ones based on Tree-Adjoining
Grammars (TAG), including ours, these con-
sequences are encapsulated within elementary
structures of the grammar. Thus, such systems
do not have to specifically reason about these
consequences, as do some other systems.
More complex alternations The same con-
tent of excelling at an activity can be realized by
the verb excel, the adverb well, and the adjective
good, as illustrated in (6a-6c).
</bodyText>
<listItem confidence="0.965691666666667">
(6a) Barbara excels at teaching.
(6b) Barbara teaches well.
(6c) Barbara is a good teacher.
</listItem>
<bodyText confidence="0.998552">
This variety of expression, often called head
switching, poses a considerable difficulty for
most existing sentence generators. The diffi-
culty stems from the fact that the realization
of a phrase (sentence) typically starts with the
syntactic head (verb) which sets up a syntactic
context into which other constituents are fit. If
the top predicate is the excelling, we have to be
able to start generation not only with the verb
excel but also with the adverb well and the ad-
jective good, typically not seen as setting up an
appropriate syntactic context into which the re-
maining arguments can be fit. Existing genera-
tion systems that handle this variety do so using
special assumptions or exceptional processing,
all in order to start the generation of a phrase
with the syntactic head (e.g., (Stede, 1999), (El-
hadad et al., 1997), (Nicolov et al., 1995), (Dorr,
1993)). Our system does not require that the se-
mantic head map to the syntactic head.
Different grammatical forms realizing se-
mantic content Finally, we consider a case,
which to our knowledge is not handled by other
generation systems, where grammatical forms
realize content independently of the lexical item
on which they act, as in (7a-7b).
</bodyText>
<listItem confidence="0.910935">
(7a) Who rules Jordan?
(7b) Identify the ruler of Jordan!
</listItem>
<bodyText confidence="0.999987833333333">
The wh-question form, as used in (7a), real-
izes a request for identification by the listener
(in this case, the ruler of Jordan). Likewise, the
imperative structure (used in (7b)) realizes a re-
quest or a command to the listener (in this case,
to identify the ruler of Jordan).
</bodyText>
<subsectionHeader confidence="0.994638">
3.2 Paraphrases we do not consider
</subsectionHeader>
<bodyText confidence="0.9995282">
Since our focus is on sentence generation and not
sentence planning, we only consider the genera-
tion of single-sentence paraphrases. Hence, we
do not have the ability to generate (8a-8b) from
the same input.
</bodyText>
<equation confidence="0.824586">
(8a) CSI has a programming lab.
(8b) CSI has a lab. It involves programming.
</equation>
<bodyText confidence="0.999456666666667">
Since we do not reason about the semantic
input, including deriving entailment relations,
we cannot generate (9a-9b) from the same input.
</bodyText>
<listItem confidence="0.4879015">
(9a) Oslo is the capital of Norway.
(9b) Oslo is located in Norway.
</listItem>
<sectionHeader confidence="0.729599" genericHeader="method">
4 Our generation methodology
</sectionHeader>
<bodyText confidence="0.999975793103448">
Generation in our system is driven by the
semantic input, realized by selecting lexico-
grammatical resources matching pieces of it,
starting with the top predicate. The realization
of a piece containing the top predicate provides
the syntactic context into which the realizations
of the remaining pieces can be fit (their place-
ment being determined by the resource).
The key to our ability to handle paraphrases
in a uniform manner is that our processing is
driven by our lexicon and thus we do not make
any a priori assumptions about 1) the amount
of the input realized by a lexical unit, 2) the re-
lationship between semantic and syntactic types
(and thus the syntactic rank or category of the
realization of the top piece), 3) the nature of
the mapping between thematic roles and syn-
tactic positions, and 4) the grammatical alter-
nation (e.g., there are different resources for the
same verb in different alternations: the active,
passive, topicalized, etc.). Because this informa-
tion is contained in each lexico-grammatical re-
source, generation can proceed no matter what
choices are specified about these in each indi-
vidual resource. Our approach is fundamen-
tally different from systems that reason directly
about syntax and build realizations by syntactic
rank ((Bateman, 1997), (Elhadad et al., 1997);
(Nicolov et al., 1995); (Stone and Doran, 1997)).
</bodyText>
<subsectionHeader confidence="0.968982">
4.1 Our algorithm
</subsectionHeader>
<bodyText confidence="0.813092611111111">
Our generation algorithm is a simple, recursive,
semantic-head-driven generation process, con-
sistent with the approach described in section 2,
but one driven by the semantic input and the
lexico-grammatical resources.
1. given an unrealized input, find a lexico-
grammatical resource that matches a por-
tion including the top predicate and satis-
fies any selectional restrictions
2. recursively realize arguments, then modi-
fiers
3. combine the realizations in step 2 with the
resource in step 1, as determined by the re-
source in step 1
Notice the prominence of lexico-grammatical re-
sources in steps 1 and 3 of this algorithm. The
standard approach in section 2 need not be
driven by resources.
</bodyText>
<subsectionHeader confidence="0.908367">
4.2 Lexico-grammatical resources
</subsectionHeader>
<bodyText confidence="0.9995095">
The key to the simplicity of our algorithm lies in
the lexico-grammatical resources, which encap-
sulate information necessary to carry through
generation. These consist of three parts:
</bodyText>
<listItem confidence="0.965998272727273">
• the semantic side: the portion of seman-
tics realized by the resource (including the
predicate and any arguments; this part is
matched against the input semantics)
• the syntactic side: either word(s) in a syn-
tactic configuration or a grammatical form
without words, and syntactic consequences
• a mapping between semantic and syntactic
constituents indicating which constituent
on the semantic side is realized by which
constituent on the syntactic side
</listItem>
<bodyText confidence="0.999564941176471">
Consider the resources for the verbs enjoy and
please in Fig. 2. The semantic sides indicate
that these resources realize the predicate ENJOY
and the thematic roles EXPERIENCER and THEME.
The arguments filling those roles (which must be
realized separately, as indicated by dashed out-
lines) appear as variables X and Y which will be
matched against actual arguments. The syntac-
tic sides contain the verbs enjoy and please in
the active voice configuration. The mappings
include links between ENJOY and its realization
as well as links between the unrealized agent (X)
or theme (Y) and the subject or the complement.
Our mapping between semantic and syntactic
constituents bears resemblance to the pairings in
Synchronous TAG (Shieber and Schabes, 1990).
Just like in Synchronous TAG, the mapping is
</bodyText>
<figureCaption confidence="0.986647">
Figure 2: Two different resources for ENJOY
</figureCaption>
<bodyText confidence="0.999883111111111">
critical for combining realizations (in step 3 of
our algorithm in section 4.1). There are, how-
ever, advantages that our approach has. For
one, we are not constrained by the isomorphism
requirement in a Synchronous TAG derivation.
Also, the DSG formalism that we use affords
greater flexibility, significant in our approach, as
discussed later in this paper (and in more detail
in (Kozlowski, 2002b)).
</bodyText>
<subsectionHeader confidence="0.981696">
4.3 The grammatical formalism
</subsectionHeader>
<bodyText confidence="0.999726052631579">
Both step 3 of our algorithm (putting re-
alizations together) and the needs of lexico-
grammatical resources (the encapsulation of
syntactic consequences such as the position
of argument realizations) place significant de-
mands on the grammatical formalism to be used
in the implementation of the architecture. One
grammatical formalism that is well-suited for
our purposes is the D-Tree Substitution Gram-
mars (DSG, (Rambow et al., 2001)), a variant
of Tree-Adjoining Grammars (TAG). This for-
malism features an extended domain of locality
and flexibility in encapsulation of syntactic con-
sequences, crucial in our architecture.
Consider the elementary DSG structures on
the right-hand-side of the resources for enjoy
and please in Fig. 2. Note that nodes marked
with , are substitution nodes corresponding to
syntactic positions into which the realizations of
</bodyText>
<figureCaption confidence="0.7427155">
Figure 3: Combining argument realizations with
the resources for enjoy and please
</figureCaption>
<bodyText confidence="0.995945375">
arguments will be substituted. The positions of
both the subject and the complement are en-
capsulated in these elementary structures. This
allows the mapping between semantic and syn-
tactic constituents to be defined locally within
the resources. Dotted lines indicate domination
of length zero or more where syntactic material
(e.g., modifiers) may end up.
</bodyText>
<subsectionHeader confidence="0.998434">
4.4 Using resources in our algorithm
</subsectionHeader>
<bodyText confidence="0.99463347826087">
Step 1 of our algorithm requires matching the se-
mantic side of a resource against the top of the
input and testing selectional restrictions. A se-
mantic side matches if it can be overlaid against
the input. Details of this process are given
in (Kozlowski, 2002a). Selectional restrictions
(type restrictions on arguments) are associated
with nodes on the semantic side of resources.
In their evaluation, the appropriate knowledge
base instance is accessed and its type is tested.
More details about using selectional restrictions
in generation and in our architecture are given
in (Kozlowski et al., 2002).
Resources for enjoy and please which match
the top of the input in Fig. 1 are shown in
Fig. 2. In doing the matching, the arguments
AMY and INTERACTION are unified with X and
Y. The dashed outlines around X and Y indicate
that the resource does not realize them. Our al-
gorithm calls for the independent recursive real-
ization of these arguments and then putting to-
gether those realizations with the syntactic side
of the resource, as indicated by the mapping.
</bodyText>
<figure confidence="0.999753857142857">
ENJOY
NP VP
0 0
EXPERIENCER THEME
VP1
ENJOY
NP VP
0 0
EXPERIENCER THEME
VP1
1
1
S
X Y
NP
V◆
enjoy
S
X Y
NP
V◆
please
VP
0 0
NP
NP
VP
0 0
VP1
VP1
S
S
Amy
the interaction
NP
1
V◆
enjoy
NP
1
please
V◆
S S
S
AGENT PATH
X
GO
ACROSS
THEME
Y
NP VP
0 0
V◆
cross
VP1
NP
</figure>
<figureCaption confidence="0.955754">
Figure 4: Two different resources for GO
</figureCaption>
<figure confidence="0.999688476190476">
NP VP
0 0
AGENT
THEME
VP1
FOUND
X
Y
NP
V◆
found
1
GO
NP VP
0 0
VP1
X
PLANE
AGENT MODE
V◆
fly
FOUND
D ◆ N’0
the
PP
N’2
by
X Y
founding
NP
N’
1
P ◆ NP
N◆ PP
P ◆ NP2
of
AGENT THEME
1
1
GO Figure 6: Two different resources for FOUND
THEME
OCEAN
</figure>
<figureCaption confidence="0.999377">
Figure 5: The semantics underlying (3a-3c) with
</figureCaption>
<bodyText confidence="0.939822833333333">
portion realized by cross in bold
This is shown in Fig. 3. The argument realiza-
tions, Amy and the interaction, are placed in the
subject and complement positions of enjoy and
please, according to the mapping in the corre-
sponding resources.
</bodyText>
<subsectionHeader confidence="0.973955">
4.5 Driving decomposition by resources
</subsectionHeader>
<bodyText confidence="0.998215222222222">
The semantic side of a resource determines
which arguments, if any, are realized by the re-
source, while the matching done in step 1 of our
algorithm determines the portions that must be
realized by modifiers. This is always done the
same way regardless of the resources selected
and how much of the input they realize, such
as the two resources realizing the predicate GO
shown in Fig. 4, one for fly which incorporates
MODE PLANE and another for cross which incor-
porates PATH ACROSS.
Suppose the semantic input underlying (3a-
3c) is as given in Fig. 5. The portion shown
in bold is realized by the resource for cross in
Fig. 4. The agent of GO and the theme of ACROSS
are to be realized as arguments. The remaining
thematic role MODE with the argument PLANE fill-
ing it, is to be realized by a modifier.
</bodyText>
<subsectionHeader confidence="0.9819145">
4.6 Encapsulation of syntactic
consequences
</subsectionHeader>
<bodyText confidence="0.999931176470588">
All syntactic information should be encapsu-
lated within resources and transparent to the
algorithm. This includes the identification of ar-
guments, including their placement with respect
to the realization. Another example of a syn-
tactic consequence is the presence of additional
syntactic material required by the lexical item in
the particular syntactic configuration. The verb
found in the active configuration, as in (4a), does
not require any additional syntactic material.
On the other hand, the noun founding in the
configuration with prepositional phrases headed
by of and by, as in (4b), may be said to require
the use of the prepositions. The resources for
found and founding are shown in Fig. 6. Encap-
sulation of such consequences allows us to avoid
special mechanisms to keep track of and enforce
</bodyText>
<figure confidence="0.998314">
CHARLES
AGENT
ACROSS
PATH MODE
PLANE
</figure>
<figureCaption confidence="0.999872">
Figure 7: Two different resources for EXCEL
</figureCaption>
<bodyText confidence="0.855088">
them for individual resources.
</bodyText>
<subsectionHeader confidence="0.995934">
4.7 Syntactic rank and category
</subsectionHeader>
<bodyText confidence="0.99743628">
No assumptions are made about the realization
of a piece of input semantics, including its syn-
tactic rank and category. For instance, the pred-
icate EXCEL can be realized by the verb excel,
the adverb well, and the adjective good, as illus-
trated in (6a-6c). The processing is the same:
a resource is selected and any argument realiza-
tions are attached to the resource.
Fig. 7 shows a resource for the predicate
EXCEL realized by the verb excel. What is in-
teresting about this case is that the DSG for-
malism we chose allows us to encapsulate the
PRO in the subject position of the complement
as a syntactic consequence of the verb excel in
this configuration. The other resource for EXCEL
shown in Fig. 7 is unusual in that the predicate
is realized by an adverb, well. Note the link be-
tween the uninstantiated theme on the semantic
side and the position for its corresponding syn-
tactic realization, the substitution node VP12.
Suppose the semantic input underlying (6a-
Also notice that the experiencer of EXCEL is consid-
ered realized by the well resource and coindexed with the
agent of the theme of EXCEL, to be realized by a separate
resource.
</bodyText>
<figure confidence="0.8806554">
EXCEL [1]: BARBARA
EXPERIENCER THEME
[1] TEACH
AGENT
[1]
</figure>
<figureCaption confidence="0.997913666666667">
Figure 8: The semantics underlying (6a-6c)
6c) is as given in Fig. 8 and the well resource in
Fig. 7 is selected to realize the top of the seman-
</figureCaption>
<bodyText confidence="0.9685594375">
tics. The matching in step 1 of our algorithm
determines that the subtree of the input rooted
at TEACH must be recursively realized. The re-
alization of this subtree yields Barbara teaches.
Because of the link between the theme of EXCEL
and the VP1 node of well, the realization Bar-
bara teaches is substituted to the VP1 node of
well. This is a more complex substitution than
in regular TAG (where the substitution node is
identified with the root of the argument realiza-
tion), and is equivalent to the adjunction of well
to Barbara teaches. In DSG, we are able to treat
structures such as the well structure as initial
and not auxiliary, as TAG would. Thus, argu-
ment realizations are combined with all struc-
tures in a uniform fashion.
</bodyText>
<subsectionHeader confidence="0.936945">
4.8 Grammatical forms
</subsectionHeader>
<bodyText confidence="0.9999515">
As discussed before, grammatical forms them-
selves can realize a piece of semantics. For in-
stance, the imperative syntactic form realizes a
request or a command to the listener, as shown
in Fig. 9. Likewise, the wh-question form real-
izes a request to identify, also shown in Fig. 9.
In our system, whether the realization has any
lexical items is not relevant.
</bodyText>
<subsectionHeader confidence="0.972015">
4.9 The role of DSG
</subsectionHeader>
<bodyText confidence="0.999382125">
We believe that the choice of the DSG formal-
ism plays a crucial role in maintaining our sim-
ple methodology. Like TAG, DSG allows cap-
turing syntactic consequences in one elementary
structure. DSG, however, allows even greater
flexibility in what is included in an elementary
structure. Note that in DSG we may have non-
immediate domination links between nodes of
</bodyText>
<figure confidence="0.999717285714286">
S
EXPERIENCER
[0]:
EXPERIENCER
[0]:
EXCEL
EXCEL
[0]:
THEME
AGENT
[0]:
P
THEME
P
NP0
excel
VP
V◆
1
VP
VP0
VP1
at
P◆
0
AdvP
Adv’0
Advi
Adv
well
PP
◆
PRO
NP
S1
</figure>
<figureCaption confidence="0.999803">
Figure 9: Two different resources for REQUEST
</figureCaption>
<bodyText confidence="0.698746142857143">
different syntactic categories (e.g., between the S
and NP in Fig. 9 and also in the excel at structure
in Fig. 7). DSG also allows uniform treatment
of complementation and modification using the
operations of substitution (regardless of the re-
alization of the predicate, e.g., the structures in
Fig. 7) and adjunction, respectively.
</bodyText>
<sectionHeader confidence="0.999492" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999968764705882">
Although we only consider paraphrases with the
same semantics, there is still a wide variety of
expression which poses challenges to any genera-
tion system. In overcoming those challenges and
generating in a simple manner in our architec-
ture, our lexico-grammatical resources play an
important role in each phase of generation. En-
capsulation of syntactic consequences within ele-
mentary syntactic structures keeps our method-
ology modular. Whatever those consequences,
often very different for different paraphrases,
generation always proceeds in the same manner.
Both the algorithm and the constraints on
our lexico-grammatical resources place signif-
icant demands on the grammatical formalism
used for the architecture. We find that the DSG
formalism meets those demands well.
</bodyText>
<sectionHeader confidence="0.996917" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998426895833333">
John Bateman. 1997. Enabling technology for mul-
tilingual natural language generation: the KPML
development environment. Natural Language En-
gineering, 3(1):15-55.
Bonnie J. Dorr. 1993. Interlingual machine transla-
tion: a parametrized approach. Artificial Intelli-
gence, 63(1):429-492.
Michael Elhadad, Kathleen McKeown, and Jacques
Robin. 1997. Floating constraints in lexical
choice. Computational Intelligence, 23:195{239.
Raymond Kozlowski, Kathleen F. McCoy, and
K. Vijay-Shanker. 2002. Selectional restrictions
in natural language sentence generation. In Pro-
ceedings of the 6th World Multiconference on Sys-
temics, Cybernetics, and Informatics (SCI&apos;02).
Raymond Kozlowski. 2002a. Driving multilingual
sentence generation with lexico-grammatical re-
sources. In Proceedings of the Second Interna-
tional Natural Language Generation Conference
(INLG&apos;02) - Student Session.
Raymond Kozlowski. 2002b. DSG/TAG - An appro-
priate grammatical formalism for flexible sentence
generation. In Proceedings of the Student Research
Workshop at the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL&apos;02).
Nicolas Nicolov, Chris Mellish, and Graeme Ritchie.
1995. Sentence Generation from Conceptual
Graphs. In Proceedings of the 3rd International
Conference on Conceptual Structures (ICCS&apos;95).
Owen Rambow, K. Vijay-Shanker, and David Weir.
2001. D-Tree Substitution Grammars. Computa-
tional Linguistics, 27(1):87-122.
Stuart M. Shieber and Yves Schabes. 1990. Syn-
chronous Tree-Adjoining Grammars. In Proceed-
ings of the 13th International Conference on Com-
putational Linguistics.
Stuart M. Shieber, Gertjan van Noord, Fernando
C. N. Pereira, and Robert C. Moore. 1990.
Semantic-Head-Driven Generation. Computa-
tional Linguistics, 16(1):30-42.
Manfred Stede. 1999. Lexical semantics and knowl-
edge representation in multilingual text genera-
tion. Kluwer Academic Publishers, Boston.
Matthew Stone and Christine Doran. 1997. Sen-
tence Planning as Description Using Tree Adjoin-
ing Grammar. In Proceedings of the 35th Annual
Meeting of the Association for Computational Lin-
guistics (ACL&apos;9&apos;1).
</reference>
<figure confidence="0.99872692">
S[subj−empty:+]
P
YOU
REQUEST
ACTION
NP[empty:+]
(you)
S
YOU
AGENT
IDENTIFY
THEME SUCH−THAT
[0]:
THEME
SET−OF
[0]:
P
who
N◆
NP S1
NP
ε
[inv:+]
REQUEST
ACTION
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.526387">
<title confidence="0.993619">Generation of single-sentence paraphrases predicate/argument structure using lexico-grammatical resources</title>
<author confidence="0.996135">Raymond Kozlowski</author>
<author confidence="0.996135">Kathleen F McCoy</author>
<author confidence="0.996135">K</author>
<affiliation confidence="0.999711">Department of Computer and Information University of</affiliation>
<address confidence="0.535961">Newark, DE 19716,</address>
<email confidence="0.999589">kozlowsk,mccoy,vijay@cis.udel.edu</email>
<abstract confidence="0.999789933333333">Paraphrases, which stem from the variety of lexical and grammatical means of expressing meaning available in a pose challenges for a sentence generation system. In paper, we discuss the generation of paraphrases from predicate/argument structure using a simple, uniform generation methodology. Central to our approach are lexico-grammatical resources which pair elementary semantic structures with their syntactic realization and a simple but powerful mechanism for combining resources.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Bateman</author>
</authors>
<title>Enabling technology for multilingual natural language generation: the KPML development environment.</title>
<date>1997</date>
<journal>Natural Language Engineering,</journal>
<pages>3--1</pages>
<contexts>
<context position="12546" citStr="Bateman, 1997" startWordPosition="2011" endWordPosition="2012">tic rank or category of the realization of the top piece), 3) the nature of the mapping between thematic roles and syntactic positions, and 4) the grammatical alternation (e.g., there are different resources for the same verb in different alternations: the active, passive, topicalized, etc.). Because this information is contained in each lexico-grammatical resource, generation can proceed no matter what choices are specified about these in each individual resource. Our approach is fundamentally different from systems that reason directly about syntax and build realizations by syntactic rank ((Bateman, 1997), (Elhadad et al., 1997); (Nicolov et al., 1995); (Stone and Doran, 1997)). 4.1 Our algorithm Our generation algorithm is a simple, recursive, semantic-head-driven generation process, consistent with the approach described in section 2, but one driven by the semantic input and the lexico-grammatical resources. 1. given an unrealized input, find a lexicogrammatical resource that matches a portion including the top predicate and satisfies any selectional restrictions 2. recursively realize arguments, then modifiers 3. combine the realizations in step 2 with the resource in step 1, as determined </context>
</contexts>
<marker>Bateman, 1997</marker>
<rawString>John Bateman. 1997. Enabling technology for multilingual natural language generation: the KPML development environment. Natural Language Engineering, 3(1):15-55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie J Dorr</author>
</authors>
<title>Interlingual machine translation: a parametrized approach.</title>
<date>1993</date>
<journal>Artificial Intelligence,</journal>
<pages>63--1</pages>
<contexts>
<context position="5968" citStr="Dorr, 1993" startWordPosition="928" endWordPosition="929">te realizations be produced. Different placement of argument realizations Sometimes different synonyms, like the verbs enjoy and please, place argument realizations differently with respect to the head, as illustrated in (2a-2b). (2a) Amy enjoyed the interaction. (2b) The interaction pleased Amy. To handle this variety, a uniform generation methodology should not assume a fixed mapping between thematic and syntactic roles but let each lexical item determine the placement of argument realizations. Generation systems that use such a fixed mapping must override it for the divergent cases (e.g., (Dorr, 1993)). Words with overlapping meaning There are often cases of different words that realize different but overlapping semantic pieces. The easiest way to see this is in what has been termed incorporation, where a word not only realizes a predicate but also one or more of its arguments. Different words may incorporate different arguments or none at all, which may lead to paraphrases, as illustrated in (3a-3c). (3a) Charles flew across the ocean. (3b) Charles crossed the ocean by plane. (3c) Charles went across the ocean by plane. Notice that the verb fly realizes not only going but also the mode of</context>
<context position="7241" citStr="Dorr, 1993" startWordPosition="1143" endWordPosition="1144">ent realize going whose path is across the object realized AMY INTERACTION EXPERIENCER THEME by the complement, and the verb go only realizes going. For all of these verbs, the remaining arguments are realized by modifiers. Incorporation shows that a uniform generator should use the word choices to determine 1) what portion of the semantics they realize, 2) what portions are to be realized as arguments of the realized semantics, and 3) what portions remain to be realized and attached as modifiers. Generation systems that assume a one-to-one mapping between semantic and syntactic units (e.g., (Dorr, 1993)) must use special processing for cases of overlapping semantics. Different syntactic categories Predicates can often be realized by words of different syntactic categories, e.g., the verb found and the noun founding, as in (4a-4b). (4a) I know that Olds founded GM. (4b) I know about the founding of GM by Olds. Words of different syntactic categories usually have different syntactic consequences. One such consequence is the presence of additional syntactic material. Notice that (4b) contains the prepositions of and by while (4a) does not. These prepositions might be considered a syntactic cons</context>
<context position="10036" citStr="Dorr, 1993" startWordPosition="1598" endWordPosition="1599">ch sets up a syntactic context into which other constituents are fit. If the top predicate is the excelling, we have to be able to start generation not only with the verb excel but also with the adverb well and the adjective good, typically not seen as setting up an appropriate syntactic context into which the remaining arguments can be fit. Existing generation systems that handle this variety do so using special assumptions or exceptional processing, all in order to start the generation of a phrase with the syntactic head (e.g., (Stede, 1999), (Elhadad et al., 1997), (Nicolov et al., 1995), (Dorr, 1993)). Our system does not require that the semantic head map to the syntactic head. Different grammatical forms realizing semantic content Finally, we consider a case, which to our knowledge is not handled by other generation systems, where grammatical forms realize content independently of the lexical item on which they act, as in (7a-7b). (7a) Who rules Jordan? (7b) Identify the ruler of Jordan! The wh-question form, as used in (7a), realizes a request for identification by the listener (in this case, the ruler of Jordan). Likewise, the imperative structure (used in (7b)) realizes a request or </context>
</contexts>
<marker>Dorr, 1993</marker>
<rawString>Bonnie J. Dorr. 1993. Interlingual machine translation: a parametrized approach. Artificial Intelligence, 63(1):429-492.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Elhadad</author>
<author>Kathleen McKeown</author>
<author>Jacques Robin</author>
</authors>
<title>Floating constraints in lexical choice.</title>
<date>1997</date>
<journal>Computational Intelligence,</journal>
<pages>23--195</pages>
<contexts>
<context position="1592" citStr="Elhadad et al., 1997" startWordPosition="226" endWordPosition="229">arious ways using different lexical and syntactic means. These different realizations, called paraphrases, vary considerably in appropriateness based on pragmatic factors and communicative goals. If a generator is to come up with the most appropriate realization, it must be capable of generating all paraphrases that realize the input semantics. Even if it makes choices on pragmatic grounds during generation and produces a single realization, the ability to generate them all must still exist. Variety of lexical and grammatical forms of expression pose challenges to a generator ((Stede, 1999); (Elhadad et al., 1997); (Nicolov et al., 1995)). In this paper, we discuss the generation of single-sentence paraphrases realizing the same semantics in a uniform fashion using a simple sentence generation architecture. In order to handle the various ways of realizing meaning in a simple manner, we believe that the generation architecture should not be aware of the variety and not have any special mechanisms to handle the different types of realizations&apos;. Instead, we want all lexical and grammatical variety to follow automatically from the variety of the elementary building blocks of generation, lexico-grammatical </context>
<context position="9998" citStr="Elhadad et al., 1997" startWordPosition="1589" endWordPosition="1593">ically starts with the syntactic head (verb) which sets up a syntactic context into which other constituents are fit. If the top predicate is the excelling, we have to be able to start generation not only with the verb excel but also with the adverb well and the adjective good, typically not seen as setting up an appropriate syntactic context into which the remaining arguments can be fit. Existing generation systems that handle this variety do so using special assumptions or exceptional processing, all in order to start the generation of a phrase with the syntactic head (e.g., (Stede, 1999), (Elhadad et al., 1997), (Nicolov et al., 1995), (Dorr, 1993)). Our system does not require that the semantic head map to the syntactic head. Different grammatical forms realizing semantic content Finally, we consider a case, which to our knowledge is not handled by other generation systems, where grammatical forms realize content independently of the lexical item on which they act, as in (7a-7b). (7a) Who rules Jordan? (7b) Identify the ruler of Jordan! The wh-question form, as used in (7a), realizes a request for identification by the listener (in this case, the ruler of Jordan). Likewise, the imperative structure</context>
<context position="12570" citStr="Elhadad et al., 1997" startWordPosition="2013" endWordPosition="2016">ory of the realization of the top piece), 3) the nature of the mapping between thematic roles and syntactic positions, and 4) the grammatical alternation (e.g., there are different resources for the same verb in different alternations: the active, passive, topicalized, etc.). Because this information is contained in each lexico-grammatical resource, generation can proceed no matter what choices are specified about these in each individual resource. Our approach is fundamentally different from systems that reason directly about syntax and build realizations by syntactic rank ((Bateman, 1997), (Elhadad et al., 1997); (Nicolov et al., 1995); (Stone and Doran, 1997)). 4.1 Our algorithm Our generation algorithm is a simple, recursive, semantic-head-driven generation process, consistent with the approach described in section 2, but one driven by the semantic input and the lexico-grammatical resources. 1. given an unrealized input, find a lexicogrammatical resource that matches a portion including the top predicate and satisfies any selectional restrictions 2. recursively realize arguments, then modifiers 3. combine the realizations in step 2 with the resource in step 1, as determined by the resource in step </context>
</contexts>
<marker>Elhadad, McKeown, Robin, 1997</marker>
<rawString>Michael Elhadad, Kathleen McKeown, and Jacques Robin. 1997. Floating constraints in lexical choice. Computational Intelligence, 23:195{239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond Kozlowski</author>
<author>Kathleen F McCoy</author>
<author>K Vijay-Shanker</author>
</authors>
<title>Selectional restrictions in natural language sentence generation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th World Multiconference on Systemics, Cybernetics, and Informatics (SCI&apos;02).</booktitle>
<contexts>
<context position="17213" citStr="Kozlowski et al., 2002" startWordPosition="2736" endWordPosition="2739">s in our algorithm Step 1 of our algorithm requires matching the semantic side of a resource against the top of the input and testing selectional restrictions. A semantic side matches if it can be overlaid against the input. Details of this process are given in (Kozlowski, 2002a). Selectional restrictions (type restrictions on arguments) are associated with nodes on the semantic side of resources. In their evaluation, the appropriate knowledge base instance is accessed and its type is tested. More details about using selectional restrictions in generation and in our architecture are given in (Kozlowski et al., 2002). Resources for enjoy and please which match the top of the input in Fig. 1 are shown in Fig. 2. In doing the matching, the arguments AMY and INTERACTION are unified with X and Y. The dashed outlines around X and Y indicate that the resource does not realize them. Our algorithm calls for the independent recursive realization of these arguments and then putting together those realizations with the syntactic side of the resource, as indicated by the mapping. ENJOY NP VP 0 0 EXPERIENCER THEME VP1 ENJOY NP VP 0 0 EXPERIENCER THEME VP1 1 1 S X Y NP V◆ enjoy S X Y NP V◆ please VP 0 0 NP NP VP 0 0 VP</context>
</contexts>
<marker>Kozlowski, McCoy, Vijay-Shanker, 2002</marker>
<rawString>Raymond Kozlowski, Kathleen F. McCoy, and K. Vijay-Shanker. 2002. Selectional restrictions in natural language sentence generation. In Proceedings of the 6th World Multiconference on Systemics, Cybernetics, and Informatics (SCI&apos;02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond Kozlowski</author>
</authors>
<title>Driving multilingual sentence generation with lexico-grammatical resources.</title>
<date>2002</date>
<booktitle>In Proceedings of the Second International Natural Language Generation Conference (INLG&apos;02)</booktitle>
<publisher>Student Session.</publisher>
<contexts>
<context position="15243" citStr="Kozlowski, 2002" startWordPosition="2437" endWordPosition="2438">ng between semantic and syntactic constituents bears resemblance to the pairings in Synchronous TAG (Shieber and Schabes, 1990). Just like in Synchronous TAG, the mapping is Figure 2: Two different resources for ENJOY critical for combining realizations (in step 3 of our algorithm in section 4.1). There are, however, advantages that our approach has. For one, we are not constrained by the isomorphism requirement in a Synchronous TAG derivation. Also, the DSG formalism that we use affords greater flexibility, significant in our approach, as discussed later in this paper (and in more detail in (Kozlowski, 2002b)). 4.3 The grammatical formalism Both step 3 of our algorithm (putting realizations together) and the needs of lexicogrammatical resources (the encapsulation of syntactic consequences such as the position of argument realizations) place significant demands on the grammatical formalism to be used in the implementation of the architecture. One grammatical formalism that is well-suited for our purposes is the D-Tree Substitution Grammars (DSG, (Rambow et al., 2001)), a variant of Tree-Adjoining Grammars (TAG). This formalism features an extended domain of locality and flexibility in encapsulati</context>
<context position="16868" citStr="Kozlowski, 2002" startWordPosition="2688" endWordPosition="2689">sitions of both the subject and the complement are encapsulated in these elementary structures. This allows the mapping between semantic and syntactic constituents to be defined locally within the resources. Dotted lines indicate domination of length zero or more where syntactic material (e.g., modifiers) may end up. 4.4 Using resources in our algorithm Step 1 of our algorithm requires matching the semantic side of a resource against the top of the input and testing selectional restrictions. A semantic side matches if it can be overlaid against the input. Details of this process are given in (Kozlowski, 2002a). Selectional restrictions (type restrictions on arguments) are associated with nodes on the semantic side of resources. In their evaluation, the appropriate knowledge base instance is accessed and its type is tested. More details about using selectional restrictions in generation and in our architecture are given in (Kozlowski et al., 2002). Resources for enjoy and please which match the top of the input in Fig. 1 are shown in Fig. 2. In doing the matching, the arguments AMY and INTERACTION are unified with X and Y. The dashed outlines around X and Y indicate that the resource does not real</context>
</contexts>
<marker>Kozlowski, 2002</marker>
<rawString>Raymond Kozlowski. 2002a. Driving multilingual sentence generation with lexico-grammatical resources. In Proceedings of the Second International Natural Language Generation Conference (INLG&apos;02) - Student Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond Kozlowski</author>
</authors>
<title>DSG/TAG - An appropriate grammatical formalism for flexible sentence generation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Student Research Workshop at the 40th Annual Meeting of the Association for Computational Linguistics (ACL&apos;02).</booktitle>
<contexts>
<context position="15243" citStr="Kozlowski, 2002" startWordPosition="2437" endWordPosition="2438">ng between semantic and syntactic constituents bears resemblance to the pairings in Synchronous TAG (Shieber and Schabes, 1990). Just like in Synchronous TAG, the mapping is Figure 2: Two different resources for ENJOY critical for combining realizations (in step 3 of our algorithm in section 4.1). There are, however, advantages that our approach has. For one, we are not constrained by the isomorphism requirement in a Synchronous TAG derivation. Also, the DSG formalism that we use affords greater flexibility, significant in our approach, as discussed later in this paper (and in more detail in (Kozlowski, 2002b)). 4.3 The grammatical formalism Both step 3 of our algorithm (putting realizations together) and the needs of lexicogrammatical resources (the encapsulation of syntactic consequences such as the position of argument realizations) place significant demands on the grammatical formalism to be used in the implementation of the architecture. One grammatical formalism that is well-suited for our purposes is the D-Tree Substitution Grammars (DSG, (Rambow et al., 2001)), a variant of Tree-Adjoining Grammars (TAG). This formalism features an extended domain of locality and flexibility in encapsulati</context>
<context position="16868" citStr="Kozlowski, 2002" startWordPosition="2688" endWordPosition="2689">sitions of both the subject and the complement are encapsulated in these elementary structures. This allows the mapping between semantic and syntactic constituents to be defined locally within the resources. Dotted lines indicate domination of length zero or more where syntactic material (e.g., modifiers) may end up. 4.4 Using resources in our algorithm Step 1 of our algorithm requires matching the semantic side of a resource against the top of the input and testing selectional restrictions. A semantic side matches if it can be overlaid against the input. Details of this process are given in (Kozlowski, 2002a). Selectional restrictions (type restrictions on arguments) are associated with nodes on the semantic side of resources. In their evaluation, the appropriate knowledge base instance is accessed and its type is tested. More details about using selectional restrictions in generation and in our architecture are given in (Kozlowski et al., 2002). Resources for enjoy and please which match the top of the input in Fig. 1 are shown in Fig. 2. In doing the matching, the arguments AMY and INTERACTION are unified with X and Y. The dashed outlines around X and Y indicate that the resource does not real</context>
</contexts>
<marker>Kozlowski, 2002</marker>
<rawString>Raymond Kozlowski. 2002b. DSG/TAG - An appropriate grammatical formalism for flexible sentence generation. In Proceedings of the Student Research Workshop at the 40th Annual Meeting of the Association for Computational Linguistics (ACL&apos;02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicolas Nicolov</author>
<author>Chris Mellish</author>
<author>Graeme Ritchie</author>
</authors>
<title>Sentence Generation from Conceptual Graphs.</title>
<date>1995</date>
<booktitle>In Proceedings of the 3rd International Conference on Conceptual Structures (ICCS&apos;95).</booktitle>
<contexts>
<context position="1616" citStr="Nicolov et al., 1995" startWordPosition="230" endWordPosition="233">ent lexical and syntactic means. These different realizations, called paraphrases, vary considerably in appropriateness based on pragmatic factors and communicative goals. If a generator is to come up with the most appropriate realization, it must be capable of generating all paraphrases that realize the input semantics. Even if it makes choices on pragmatic grounds during generation and produces a single realization, the ability to generate them all must still exist. Variety of lexical and grammatical forms of expression pose challenges to a generator ((Stede, 1999); (Elhadad et al., 1997); (Nicolov et al., 1995)). In this paper, we discuss the generation of single-sentence paraphrases realizing the same semantics in a uniform fashion using a simple sentence generation architecture. In order to handle the various ways of realizing meaning in a simple manner, we believe that the generation architecture should not be aware of the variety and not have any special mechanisms to handle the different types of realizations&apos;. Instead, we want all lexical and grammatical variety to follow automatically from the variety of the elementary building blocks of generation, lexico-grammatical resources. We have devel</context>
<context position="10022" citStr="Nicolov et al., 1995" startWordPosition="1594" endWordPosition="1597">yntactic head (verb) which sets up a syntactic context into which other constituents are fit. If the top predicate is the excelling, we have to be able to start generation not only with the verb excel but also with the adverb well and the adjective good, typically not seen as setting up an appropriate syntactic context into which the remaining arguments can be fit. Existing generation systems that handle this variety do so using special assumptions or exceptional processing, all in order to start the generation of a phrase with the syntactic head (e.g., (Stede, 1999), (Elhadad et al., 1997), (Nicolov et al., 1995), (Dorr, 1993)). Our system does not require that the semantic head map to the syntactic head. Different grammatical forms realizing semantic content Finally, we consider a case, which to our knowledge is not handled by other generation systems, where grammatical forms realize content independently of the lexical item on which they act, as in (7a-7b). (7a) Who rules Jordan? (7b) Identify the ruler of Jordan! The wh-question form, as used in (7a), realizes a request for identification by the listener (in this case, the ruler of Jordan). Likewise, the imperative structure (used in (7b)) realizes</context>
<context position="12594" citStr="Nicolov et al., 1995" startWordPosition="2017" endWordPosition="2020">f the top piece), 3) the nature of the mapping between thematic roles and syntactic positions, and 4) the grammatical alternation (e.g., there are different resources for the same verb in different alternations: the active, passive, topicalized, etc.). Because this information is contained in each lexico-grammatical resource, generation can proceed no matter what choices are specified about these in each individual resource. Our approach is fundamentally different from systems that reason directly about syntax and build realizations by syntactic rank ((Bateman, 1997), (Elhadad et al., 1997); (Nicolov et al., 1995); (Stone and Doran, 1997)). 4.1 Our algorithm Our generation algorithm is a simple, recursive, semantic-head-driven generation process, consistent with the approach described in section 2, but one driven by the semantic input and the lexico-grammatical resources. 1. given an unrealized input, find a lexicogrammatical resource that matches a portion including the top predicate and satisfies any selectional restrictions 2. recursively realize arguments, then modifiers 3. combine the realizations in step 2 with the resource in step 1, as determined by the resource in step 1 Notice the prominence </context>
</contexts>
<marker>Nicolov, Mellish, Ritchie, 1995</marker>
<rawString>Nicolas Nicolov, Chris Mellish, and Graeme Ritchie. 1995. Sentence Generation from Conceptual Graphs. In Proceedings of the 3rd International Conference on Conceptual Structures (ICCS&apos;95).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rambow</author>
<author>K Vijay-Shanker</author>
<author>David Weir</author>
</authors>
<title>D-Tree Substitution Grammars.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<pages>27--1</pages>
<contexts>
<context position="15711" citStr="Rambow et al., 2001" startWordPosition="2505" endWordPosition="2508">formalism that we use affords greater flexibility, significant in our approach, as discussed later in this paper (and in more detail in (Kozlowski, 2002b)). 4.3 The grammatical formalism Both step 3 of our algorithm (putting realizations together) and the needs of lexicogrammatical resources (the encapsulation of syntactic consequences such as the position of argument realizations) place significant demands on the grammatical formalism to be used in the implementation of the architecture. One grammatical formalism that is well-suited for our purposes is the D-Tree Substitution Grammars (DSG, (Rambow et al., 2001)), a variant of Tree-Adjoining Grammars (TAG). This formalism features an extended domain of locality and flexibility in encapsulation of syntactic consequences, crucial in our architecture. Consider the elementary DSG structures on the right-hand-side of the resources for enjoy and please in Fig. 2. Note that nodes marked with , are substitution nodes corresponding to syntactic positions into which the realizations of Figure 3: Combining argument realizations with the resources for enjoy and please arguments will be substituted. The positions of both the subject and the complement are encapsu</context>
</contexts>
<marker>Rambow, Vijay-Shanker, Weir, 2001</marker>
<rawString>Owen Rambow, K. Vijay-Shanker, and David Weir. 2001. D-Tree Substitution Grammars. Computational Linguistics, 27(1):87-122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Yves Schabes</author>
</authors>
<title>Synchronous Tree-Adjoining Grammars.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="14755" citStr="Shieber and Schabes, 1990" startWordPosition="2356" endWordPosition="2359">predicate ENJOY and the thematic roles EXPERIENCER and THEME. The arguments filling those roles (which must be realized separately, as indicated by dashed outlines) appear as variables X and Y which will be matched against actual arguments. The syntactic sides contain the verbs enjoy and please in the active voice configuration. The mappings include links between ENJOY and its realization as well as links between the unrealized agent (X) or theme (Y) and the subject or the complement. Our mapping between semantic and syntactic constituents bears resemblance to the pairings in Synchronous TAG (Shieber and Schabes, 1990). Just like in Synchronous TAG, the mapping is Figure 2: Two different resources for ENJOY critical for combining realizations (in step 3 of our algorithm in section 4.1). There are, however, advantages that our approach has. For one, we are not constrained by the isomorphism requirement in a Synchronous TAG derivation. Also, the DSG formalism that we use affords greater flexibility, significant in our approach, as discussed later in this paper (and in more detail in (Kozlowski, 2002b)). 4.3 The grammatical formalism Both step 3 of our algorithm (putting realizations together) and the needs of</context>
</contexts>
<marker>Shieber, Schabes, 1990</marker>
<rawString>Stuart M. Shieber and Yves Schabes. 1990. Synchronous Tree-Adjoining Grammars. In Proceedings of the 13th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Gertjan van Noord</author>
<author>Fernando C N Pereira</author>
<author>Robert C Moore</author>
</authors>
<date>1990</date>
<marker>Shieber, van Noord, Pereira, Moore, 1990</marker>
<rawString>Stuart M. Shieber, Gertjan van Noord, Fernando C. N. Pereira, and Robert C. Moore. 1990.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Semantic-Head-Driven Generation</author>
</authors>
<journal>Computational Linguistics,</journal>
<pages>16--1</pages>
<marker>Generation, </marker>
<rawString>Semantic-Head-Driven Generation. Computational Linguistics, 16(1):30-42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manfred Stede</author>
</authors>
<title>Lexical semantics and knowledge representation in multilingual text generation.</title>
<date>1999</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Boston.</location>
<contexts>
<context position="1568" citStr="Stede, 1999" startWordPosition="224" endWordPosition="225"> expressed in various ways using different lexical and syntactic means. These different realizations, called paraphrases, vary considerably in appropriateness based on pragmatic factors and communicative goals. If a generator is to come up with the most appropriate realization, it must be capable of generating all paraphrases that realize the input semantics. Even if it makes choices on pragmatic grounds during generation and produces a single realization, the ability to generate them all must still exist. Variety of lexical and grammatical forms of expression pose challenges to a generator ((Stede, 1999); (Elhadad et al., 1997); (Nicolov et al., 1995)). In this paper, we discuss the generation of single-sentence paraphrases realizing the same semantics in a uniform fashion using a simple sentence generation architecture. In order to handle the various ways of realizing meaning in a simple manner, we believe that the generation architecture should not be aware of the variety and not have any special mechanisms to handle the different types of realizations&apos;. Instead, we want all lexical and grammatical variety to follow automatically from the variety of the elementary building blocks of generat</context>
<context position="9974" citStr="Stede, 1999" startWordPosition="1587" endWordPosition="1588"> (sentence) typically starts with the syntactic head (verb) which sets up a syntactic context into which other constituents are fit. If the top predicate is the excelling, we have to be able to start generation not only with the verb excel but also with the adverb well and the adjective good, typically not seen as setting up an appropriate syntactic context into which the remaining arguments can be fit. Existing generation systems that handle this variety do so using special assumptions or exceptional processing, all in order to start the generation of a phrase with the syntactic head (e.g., (Stede, 1999), (Elhadad et al., 1997), (Nicolov et al., 1995), (Dorr, 1993)). Our system does not require that the semantic head map to the syntactic head. Different grammatical forms realizing semantic content Finally, we consider a case, which to our knowledge is not handled by other generation systems, where grammatical forms realize content independently of the lexical item on which they act, as in (7a-7b). (7a) Who rules Jordan? (7b) Identify the ruler of Jordan! The wh-question form, as used in (7a), realizes a request for identification by the listener (in this case, the ruler of Jordan). Likewise, </context>
</contexts>
<marker>Stede, 1999</marker>
<rawString>Manfred Stede. 1999. Lexical semantics and knowledge representation in multilingual text generation. Kluwer Academic Publishers, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Stone</author>
<author>Christine Doran</author>
</authors>
<title>Sentence Planning as Description Using Tree Adjoining Grammar.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL&apos;9&apos;1).</booktitle>
<contexts>
<context position="12619" citStr="Stone and Doran, 1997" startWordPosition="2021" endWordPosition="2024"> nature of the mapping between thematic roles and syntactic positions, and 4) the grammatical alternation (e.g., there are different resources for the same verb in different alternations: the active, passive, topicalized, etc.). Because this information is contained in each lexico-grammatical resource, generation can proceed no matter what choices are specified about these in each individual resource. Our approach is fundamentally different from systems that reason directly about syntax and build realizations by syntactic rank ((Bateman, 1997), (Elhadad et al., 1997); (Nicolov et al., 1995); (Stone and Doran, 1997)). 4.1 Our algorithm Our generation algorithm is a simple, recursive, semantic-head-driven generation process, consistent with the approach described in section 2, but one driven by the semantic input and the lexico-grammatical resources. 1. given an unrealized input, find a lexicogrammatical resource that matches a portion including the top predicate and satisfies any selectional restrictions 2. recursively realize arguments, then modifiers 3. combine the realizations in step 2 with the resource in step 1, as determined by the resource in step 1 Notice the prominence of lexico-grammatical res</context>
</contexts>
<marker>Stone, Doran, 1997</marker>
<rawString>Matthew Stone and Christine Doran. 1997. Sentence Planning as Description Using Tree Adjoining Grammar. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL&apos;9&apos;1).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>