<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000323">
<title confidence="0.965024">
Annotating Underquantification
</title>
<author confidence="0.992291">
Aurelie Herbelot
</author>
<affiliation confidence="0.837073">
University of Cambridge
Cambridge, United Kingdom
</affiliation>
<email confidence="0.986248">
ah433@cam.ac.uk
</email>
<sectionHeader confidence="0.993625" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999683571428572">
Many noun phrases in text are ambigu-
ously quantified: syntax doesn’t explicitly
tell us whether they refer to a single en-
tity or to several, and what portion of the
set denoted by the Nbar actually takes part
in the event expressed by the verb. We
describe this ambiguity phenomenon in
terms of underspecification, or rather un-
derquantification. We attempt to validate
the underquantification hypothesis by pro-
ducing and testing an annotation scheme
for quantification resolution, the aim of
which is to associate a single quantifier
with each noun phrase in our corpus.
</bodyText>
<sectionHeader confidence="0.919185" genericHeader="keywords">
1 Quantification resolution
</sectionHeader>
<bodyText confidence="0.999730666666667">
We are concerned with ambiguously quantified
noun phrases (NPs) and their interpretation, as il-
lustrated by the following examples:
</bodyText>
<listItem confidence="0.997793571428571">
1. Cats are mammals = All cats...
2. Cats have four legs = Most cats...
3. Cats were sleeping by the fire = Some cats...
4. The beans spilt out of the bag = Most/All of
the beans...
5. Water was dripping through the ceiling =
Some water...
</listItem>
<bodyText confidence="0.9674768">
We are interested in quantification resolution,
that is, the process of giving an ambiguously quan-
tified NP a formalisation which expresses a unique
set relation appropriate to the semantics of the ut-
terance. For instance, we wish to arrive at:
</bodyText>
<listItem confidence="0.683078">
6. All cats are mammals.
|ono |= |o |where o is the set of all cats and
0 the set of all mammals.
</listItem>
<bodyText confidence="0.915365666666667">
Resolving the quantification value of NPs is im-
portant for many NLP tasks. Let us imagine an in-
formation extraction system having retrieved the
triples ‘cat – is – mammal’ and ‘cat – chase –
Ann Copestake
University of Cambridge
Cambridge, United Kingdom
aac10@cam.ac.uk
mouse’ for inclusion in a factual database about
felines. The problem with those representation-
poor triples is that they do not contain the nec-
essary information about quantification to answer
such questions as ‘Are all cats mammals?’ or ‘Do
all cats chase mice?’ Or if they attempt to answer
those queries, they give the same answer to both.
Ideally, we would like to annotate such triples with
quantifiers which have a direct mapping to proba-
bility adverbs:
7. All cats are mammals AND Tom is a cat �
Tom is definitely a mammal.
8. Some cats chase mice AND Tom is a cat �
Tom possibly chases mice.
Adequate quantification is also necessary for in-
ference based on word-level entailment: an exis-
tentially quantified NP can be replaced by a suit-
able hypernym but this is not possible in non-
existential cases: (Some) cats are in my garden
entails (Some) animals are in my garden but (All)
cats are mammals doesn’t imply that (All) animals
are mammals.
In Herbelot (to appear), we provide a formal
semantics for ambiguously quantified NPs, which
relies on the idea that those NPs exhibit an under-
specified quantifier, i.e. that for each NP in a cor-
pus, a set relation can be agreed upon. Our formal-
isation includes a placeholder for the quantifier’s
set relation. In line with inference requirements,
we assume a three-fold partitioning of the quan-
tificational space, corresponding to the natural lan-
guage quantifiers some, most and all (in addition
to one, for the description of singular, unique enti-
ties). The corresponding set relations are:
</bodyText>
<listItem confidence="0.863516666666667">
9. some(o, 0) is true iff 0 &lt; |o n o |&lt; |o − o|
10. most(o,0) is true iff |o−0 |&lt;_ |ono |&lt; |o|
11. all(o,0) is true iff |o n o |= |o|
</listItem>
<bodyText confidence="0.9995174">
This paper is an attempt to show that our for-
malisation lends itself to evaluation by human an-
notation. The labels produced will also serve as
training and test sets for an automatic quantifica-
tion resolution system.
</bodyText>
<page confidence="0.988895">
73
</page>
<note confidence="0.9718235">
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 73–81,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.905803" genericHeader="introduction">
2 Under(specified) quantification
</sectionHeader>
<bodyText confidence="0.999618714285714">
Before we present our annotation scheme, we will
spell out the essential idea behind what we call un-
derquantification.
The phenomenon of ambiguous quantification
overlaps with genericity (see Krifka et al, 1995,
for an introduction to genericity). Generic NPs
are frequently expressed syntactically as bare plu-
rals, although they occur in definite and indefinite
singulars too, as well as bare singulars. There
are many views on the semantics of generics
(e.g. Carlson, 1995; Pelletier and Asher, 1997;
Heyer, 1990; Leslie, 2008) but one of them is that
they quantify (Cohen, 1996), although, puzzlingly
enough, not always with the same quantifier:
</bodyText>
<listItem confidence="0.972739">
12. Frenchmen eat horsemeat = Some/Relatively-
many Frenchmen... (For the relatively many
reading, see Cohen, 2001.)
13. Cars have four wheels = Most cars...
14. Typhoons arise in this part of the Pacific =
Some typhoons... OR Most/All typhoons...
</listItem>
<bodyText confidence="0.999775666666667">
This behaviour has so far prevented linguists
from agreeing on a single formalisation for all
generics. The only accepted assumption is that an
operator GEN exists, which acts as a silent quan-
tifier over the restrictor (subject) and matrix (ver-
bal predicate) of the generic statement. The formal
properties of GEN are however subject to debate:
in particular, it is not clear which natural language
quantifier it would map onto (some view it as most,
but this approach requires some complex domain
restriction to deal with sentences such as 12).
In this paper, we take a different approach
which sidesteps some of the intractable prob-
lems associated with the literature on generics and
which also extends to definite plurals. Instead of
talking of ambiguous quantification, we will talk
of underspecified quantification, or underquan-
tification. By this, we mean that the bare plural,
rather than exhibiting a silent, GEN quantifier,
simply features a placeholder in the logical form
which must be filled with the appropriate quan-
tifier (e.g., uq(x, cat&apos;(x), sleep&apos;(x)), where uq is
the placeholder quantifier). This account caters
for the facts that so-called generics can so easily
be quantified via traditional quantifiers, that GEN
is silent in all known languages, and it explains
also why it is the bare form which has the high-
est productivity, and can refer to a range of quan-
tified sets, from existentials to universals. Using
the underquantification hypothesis, we can para-
phrase any generic of the form ‘X does Y’ as ‘there
is a set of things X, a certain number of which
do Y’ (note the partitive construction). Such a
paraphrase allows us to also resolve ambiguously
quantified definite plurals, which have tradition-
ally been associated with universals, outside of the
genericity phenomenon (e.g. Lyons, 1999).
Because of space constraints, we will not give
our formalisation for underquantification in this
paper (see Herbelot,to appear, for details). It in-
volves a representation of the partitive construct
exemplified above and requires knowledge of the
distributive or collective status of the verbal pred-
icate. We also argue that if generics can always be
quantified, their semantics may involve more than
quantification. So we claim that in certain cases, a
double formalisation of the NP as a quantified en-
tity and a kind is desirable. We understand kinds
in the way proposed by Chierchia (1998), that is
as the plurality of all instances denoted by a given
word in the world under consideration. Under the
kind reading, we can interpret 12 as meaning Col-
lectively, the group of all Frenchmen has the prop-
erty of eating horsemeat.
</bodyText>
<sectionHeader confidence="0.998996" genericHeader="method">
3 Motivation
</sectionHeader>
<subsectionHeader confidence="0.999811">
3.1 Linguistic motivation
</subsectionHeader>
<bodyText confidence="0.999914304347826">
It is usual to talk of ‘annotation’ generically, to
cover any process that involves humans using a set
of guidelines to mark some specific linguistic phe-
nomenon in some given text. However, we would
argue that, when considering the aims of an anno-
tation task and its relation to the existing linguistic
literature, it becomes possible to distinguish be-
tween various types of annotation. Further, we
will show that our own effort situates itself in a
little studied relation to formal semantics.
The most basic type of annotation is the
one where computational linguists mark large
amounts of textual data with well-known and well-
understood labels. The production of tree banks
like the Penn Treebank (Marcus et al, 1993) makes
use of undisputed linguistic categories such as
parts of speech. The aim is to make the computer
learn and use irrefutable bits of linguistics. (Note
that, despite agreement, the representation of those
categories may differ: see for example the range
of available parts of speech tag sets.) This type
of task mostly involves basic syntactic knowledge,
but can be taken to areas of syntax and seman-
</bodyText>
<page confidence="0.997548">
74
</page>
<bodyText confidence="0.999976761363637">
tics where the studied phenomena have a (some-
what) clear, agreed upon definition (Kingsbury et
al, 2002). We must clarify that in those cases, the
choice of a formalism may already imply a certain
theoretical position – leading to potential incom-
patibilities between formalisms. However, the cat-
egories for such annotation are themselves fixed:
there is a generally agreed broad understanding of
concepts such as noun phrases and coordination.
Another type of annotation concerns tasks
where the linguistic categories at play are not
fixed. One example is discourse annotation ac-
cording to rhetorical function (Teufel et al, 2006)
where humans are asked to differentiate between
several discursive categories such as ‘contrast’ or
‘weakness’. In such a task, the computational lin-
guist develops a theory where different states or
values are associated with various phenomena. In
order to show that the world functions according to
the model presented, experimentation is required.
This usually takes the form of an annotation task
where several human subjects are required to mark
pieces of text following guidelines inferred from
the model. The intuition behind the annotation ef-
fort is that agreement between humans support the
claims of the theory (Teufel, in press). In particu-
lar, it may confirm that the phenomena in question
indeed exist and that the values attributed to them
are clearly defined and distinguishable. The work
is mostly of a descriptive nature – it creates phe-
nomenological definitions that encompass bits of
observable language.
Our own work is similar to the latter type of
annotation in that it is trying to capture a phe-
nomenon that is still under investigation in the lin-
guistic literature. However, it is also different be-
cause the categories we use are fixed by language:
the quantifiers some, most and all exist and we as-
sume that their definition is agreed upon by speak-
ers of English. What we are trying to investigate
is whether those quantifiers should be used at all
in the context of ambiguous quantification.
The type of annotation carried out in this pa-
per can be said to have more formal aims than the
tasks usually attempted in computational linguis-
tics. In particular, it concerns itself with some of
the broad claims made by formal semantics: its
model-theoretical view and the use of generalised
quantifiers to formalise noun phrases.
In Section 1, we assumed that quantifiers de-
note relations between sets and presented the task
of quantification resolution as choosing the ‘cor-
rect’ set relation for a particular noun phrase in a
particular sentence – implying some sort of truth
value at work throughout the process: the correct
set relation produces the sentence with truth value
1 while the other set relations produce a truth value
of 0. What we declined to discuss, though, is the
way that those reference sets were selected in nat-
ural language, i.e. we didn’t make claims about
what model, or models, are used by humans when
they compute the truth value of a given quantified
statement. The annotation task may not answer
this question but it should help us ascertain to what
extent humans share a model of the world.
In Section 2, we also argued that all subject
generic noun phrases could be analysed in terms
of quantification. That is, an (underspecified) gen-
eralised quantifier is at work in sentences that con-
tain such generic NPs. It is expected that if the
annotation is feasible and shows good agreement
between annotators, the quantification hypothesis
would be confirmed. Thus, annotation may allow
us to make semantic claims such as ‘genericity
does quantify’. Note that the categories we assume
are intuitive and do not depend on a particular rep-
resentation: it is possible to reuse our annotation
with a different formalism as long as the theoreti-
cal assumption of quantification is agreed upon.
We are not aware of any annotation work in
computational linguistics that contributes to vali-
dating (or invalidating) a particular formal theory.
In that respect, the experiments presented in this
paper are of a slightly different nature than the
standard research on annotation (despite the fact
that, as we will show in the next section, they also
aim at producing data for a language analysis sys-
tem).
</bodyText>
<subsectionHeader confidence="0.993332">
3.2 Previous work on genericity annotation
</subsectionHeader>
<bodyText confidence="0.998131916666667">
The aim of our work being the production of an au-
tomatic quantification resolution system, we need
an annotated corpus to train and test our machine
learning algorithm. There is no corpus that we
know of which would give us the required data.
The closest contestants are the ACE corpus (2008)
and the GNOME corpus (Poesio, 2000) which
both focus on the phenomenon of genericity, as de-
scribed in the linguistic literature. Unfortunately,
neither of those corpora are suitable for use in a
general quantification task.
The ACE corpus only distinguishes between
</bodyText>
<page confidence="0.993305">
75
</page>
<bodyText confidence="0.999976171428571">
‘generic’ and ‘specific’ entities. The classification
proposed by the authors of the corpus is there-
fore a lot broader than the one we are attempt-
ing here and there is no direct correspondence
between their labels and natural language quanti-
fiers: we have shown in Section 2 that genericity
didn’t map to a particular division of the quantifi-
cational space. Furthermore, the ACE guidelines
contradict to some extent the literature on generic-
ity. They require for instance that a generic men-
tion be quantifiable with all, most or any. This
implies that statements such as Mosquitoes carry
malaria either refer to a kind only (i.e. they are
not quantified) or are not generic at all. Further,
despite the above reference to quantification, the
authors seem to separate genericity and universal
quantification as two antithetical phenomena, as
shown by the following quote: “Even if the au-
thor may intend to use a GEN reading, if he/she
refers to all members of the set rather than the set
itself, use the SPC tag”.
The GNOME annotation scheme is closer in
essence to the literature on genericity and much
more detailed than the ACE guidelines. However,
the scheme distinguishes only between generic
and non-generic entities, as in the ACE corpus
case, and the corpus itself is limited to three gen-
res: museum labels, pharmaceutical leaflets, and
tutorial dialogues. The guidelines are therefore
tailored to the domains under consideration; for
instance, bare noun phrases are said to be typically
generic. This restricted solution has the advantage
of providing good agreement between annotators
(Poesio, 2004 reports a Kappa value of 0.82 for
this annotation).
</bodyText>
<sectionHeader confidence="0.991066" genericHeader="method">
4 Annotation corpus
</sectionHeader>
<bodyText confidence="0.999558615384616">
We use as corpus a snapshot of the English ver-
sion of the online encyclopaedia Wikipedia.1 The
choice is motivated by the fact that Wikipedia can
be taken as a fairly balanced corpus: although it is
presented as an encyclopaedia, it contains a wide
variety of text ranging from typical encyclopaedic
descriptions to various types of narrative texts
(historical reconstructions, film ‘spoilers’, fiction
summaries) to instructional material like rules of
games. Further, each article in Wikipedia is writ-
ten and edited by many contributors, meaning that
speaker heterogeneity is high. We would also ex-
pect an encyclopaedia to contain relatively many
</bodyText>
<footnote confidence="0.981502">
1http://www.wikipedia.org
</footnote>
<bodyText confidence="0.999976964285714">
generics, allowing us to assess how our quantifi-
cational reading fares in a real annotation task. Fi-
nally, the use of an open resource means that the
corpus can be freely distributed.2
In order to create our annotation corpus, we first
isolated the first 100,000 pages in our snapshot
and parsed them into a Robust Minimal Recur-
sion Semantics (RMRS) representation (Copes-
take, 2004) using first the RASP parser (Briscoe
et al, 2006) and the RASP to RMRS converter
(Ritchie, 2004). We then extracted all construc-
tions of the type Subject-Verb-Object from the ob-
tained corpus and randomly selected 300 of those
‘triples’ to be annotated. Another 50 random
triples were selected for the purpose of annotation
training (see Section 7.1).
We show in Figure 1 an example of an anno-
tation instance produced by the parser pipeline.
The data provided by the system consists of the
triple itself, followed by the argument structure
of that triple, including the direct dependents of
its constituents, the number and tense information
for each constituent, the file from which the triple
was extracted and the original sentence in which
it appeared. The information provided to annota-
tors is directly extracted from that representation.
(Note that the examples were not hand-checked,
and some parsing errors may have remained.)
</bodyText>
<sectionHeader confidence="0.95531" genericHeader="method">
5 Evaluating the annotation
</sectionHeader>
<bodyText confidence="0.99992445">
In an annotation task, two aspects of agreement are
important when trying to prove or refute a partic-
ular linguistic model: stability and reproducibility
(Krippendorf, 1980). Reproducibility refers to the
consistency with which humans apply the scheme
guidelines, i.e. to the so-called inter-annotator
agreement. Stability relates to whether the same
annotator will consistently produce the same an-
notations at different points in time. The measure
for stability is called intra-annotator agreement.
Both measures concern the repeatability of an an-
notation experiment.
In this work, agreement is calculated for each
pair of annotators according to the Kappa mea-
sure. There are different versions of Kappa de-
pending on how multiple annotators are treated
and how the probabilities of classes are calculated
to establish the expected agreement between anno-
tators, Pr(e): we use Fleiss’ Kappa (Fleiss, 1971),
which allows us to compute agreement between
</bodyText>
<footnote confidence="0.986745">
2For access, contact the first author.
</footnote>
<page confidence="0.912832">
76
</page>
<table confidence="0.978764083333334">
digraph G211 {
&amp;quot;TRIPLE: weed include pigra&amp;quot; [shape=box];
include -&gt; weed [label=&amp;quot;ARG1 n&amp;quot;];
include -&gt; pigra [label=&amp;quot;ARG2 n&amp;quot;];
invasive -&gt; weed [label=&amp;quot;ARG1 n&amp;quot;];
compound_rel -&gt; pigra [label=&amp;quot;ARG1 n&amp;quot;];
compound_rel -&gt; mimosa [label=&amp;quot;ARG2 n&amp;quot;];
&amp;quot;DNT INFO: lemma::include() tense::present lpos::v (arg::ARG1 var::weed() num::pl pos::)
(arg::ARG2 var::pigra() num::sg pos::)&amp;quot; [shape=box];
&amp;quot;FILE: /anfs/bigtmp/newr1-50/page101655&amp;quot; [shape=box];
&amp;quot;ORIGINAL: Invasive weeds include Mimosa pigra, which covers 80,000 hectares
of the Top End, including vast areas of Kakadu. &amp;quot; [shape=box]; }
</table>
<figureCaption confidence="0.99811">
Figure 1: Example of annotation instance
</figureCaption>
<bodyText confidence="0.79139">
multiple annotators.
</bodyText>
<sectionHeader confidence="0.9865735" genericHeader="method">
6 An annotation scheme for
quantification resolution
</sectionHeader>
<subsectionHeader confidence="0.999819">
6.1 Scheme structure
</subsectionHeader>
<bodyText confidence="0.999990571428572">
Our complete annotation scheme can be found in
Herbelot (to appear). The scheme consists of five
parts. The first two present the annotation material
and the task itself. Some key definitions are given.
The following part describes the various quantifi-
cation classes to be used in the course of the an-
notation. Participants are then given detailed in-
structions for the labelling of various grammatical
constructs. Finally, in order to keep the demand
on the annotators’ cognitive load to a minimum,
the last part reiterates the annotation guidelines in
the form of diagrammatic decision trees.
In the next sections, we give a walk-through of
the guidelines and definitions provided.
</bodyText>
<subsectionHeader confidence="0.978049">
6.2 Material
</subsectionHeader>
<bodyText confidence="0.9999623125">
Our annotators are first made familiar with the ma-
terial provided to them. This material consists
of 300 entries comprising a single sentence and
a triple Subject-Verb-Object which helps the an-
notator identify which subject noun phrase in the
sentence they are requested to label (the ‘ORIG-
INAL’ and ‘TRIPLE’ lines in the parser output –
see Figure 1). No other context is provided. This
is partly to make the task shorter (letting us anno-
tate more instances) and partly to allow for some
limited comparison between human and machine
performance (by restricting the amount of infor-
mation given to our annotators, we force them – to
some extent – to use the limited information that
would be available to an automatic quantification
resolution system, e.g. syntax).
</bodyText>
<subsectionHeader confidence="0.990856">
6.3 Definitions
</subsectionHeader>
<bodyText confidence="0.999773121212121">
In our scheme, we introduce the annotators to the
concepts of quantification and kind.3
Quantification is described in simple terms, as
the process of ‘paraphrasing the noun phrase in
a particular sentence using an unambiguous term
expressing some quantity’. An example is given.
15. Europeans discovered the Tuggerah Lakes in
1796 = Some Europeans discovered the Tug-
gerah Lakes in 1796.
We only allow the three quantifiers some, most
and all. In order to keep the number of classes
to a manageable size, we introduce the additional
constraint that the process of quantification must
yield a single quantifier. We force the annotator
to choose between the three proposed options and
introduce priorities in cases of doubt: most has pri-
ority over all, some has priority over the other two
quantifiers. This ensures we keep a conservative
attitude with regard to inference (see Section 1).
Kinds are presented as denoting ‘the group in-
cluding all entities described by the noun phrase
under consideration’, that is, as a supremum. (As
mentioned in Section 2, the verbal predicate ap-
plies collectively to that supremum in the corre-
sponding formalisation.)
Quantification classes are introduced in a sep-
arate part of the scheme. We define the five la-
bels SOME, MOST, ALL, ONE and QUANT (for al-
ready quantified noun phrases) and give examples
for each one of them.
We try, as much as possible, to keep annotators
away from performing complex reference resolu-
tion. Their first task is therefore to simply attempt
</bodyText>
<footnote confidence="0.9274534">
3Distributivity and collectivity are also introduced in the
scheme because they are a necessary part of our proposed for-
malisation. However, as this paper focuses on the annotation
of quantification itself, we will not discuss this side of the
annotation task.
</footnote>
<page confidence="0.999129">
77
</page>
<bodyText confidence="0.999988272727273">
to paraphrase the existing sentence by appending
a relevant quantifier to the noun phrase to be anno-
tated. In some cases, however, this is impossible
and no quantifier yields a correct English sentence
(this often happens in collective statements). To
help our annotators make decisions in those cases,
we ask them to distinguish what the noun phrase
might refer to when they first hear it and what it
refers to at the end of the sentence, i.e., when the
verbal predicate has imposed further constraints
on the quantification of the NP.
</bodyText>
<subsectionHeader confidence="0.974728">
6.4 Guidelines
</subsectionHeader>
<bodyText confidence="0.999676666666667">
Guidelines are provided for five basic phrase
types: quantified noun phrases, proper nouns, plu-
rals, non-bare singulars and bare singulars.
</bodyText>
<subsectionHeader confidence="0.937125">
6.4.1 Quantified noun phrases
</subsectionHeader>
<bodyText confidence="0.9999855">
This is the simplest case: a noun phrase that is
already quantified such as some people, 6 million
inhabitants or most of the workers. The annotator
simply marks the noun phrase with a QUANT label.
</bodyText>
<subsectionHeader confidence="0.888016">
6.4.2 Proper nouns
</subsectionHeader>
<bodyText confidence="0.999976777777778">
Proper nouns are another simple case. But be-
cause what annotators understand as a proper noun
varies, we provide a definition. We note first that
proper nouns are often capitalised. It should how-
ever be clear that, while capitalised entities such
as Mary, Easter Island or Warner Bros refer to
singular, unique objects, others refer to groups or
instances of those groups: The Chicago Bulls, a
Roman. The latter can be quantified:
</bodyText>
<listItem confidence="0.933263">
16. The Chicago Bulls won last week. (ALL –
collective)
17. A Roman shows courage in battle. (MOST –
distributive)
</listItem>
<bodyText confidence="0.999849">
We define proper nouns as noun phrases that
‘contain capitalised words and refer to a concept
which doesn’t have instances’. All proper nouns
are annotated as ONE.
</bodyText>
<subsectionHeader confidence="0.70966">
6.4.3 Plurals
</subsectionHeader>
<bodyText confidence="0.999639857142857">
Plurals must be appropriately quantified and the
annotators must also specify whether they are
kinds or not. This last decision can simply be
made by attempting to paraphrase the sentence
with either a definite singular or an indefinite sin-
gular – potentially leading to a typical generic
statement.
</bodyText>
<subsectionHeader confidence="0.615434">
6.4.4 (Non-bare) singulars
</subsectionHeader>
<bodyText confidence="0.999920428571429">
Like plurals, singulars must be tested for a kind
reading. This is done by attempting to pluralise the
noun phrase. If pluralisation is possible, then the
kind interpretation is confirmed and quantification
is performed. If not (certain non-mass terms have
no identifiable parts), the singular refers to a single
entity and is annotated as ONE.
</bodyText>
<subsectionHeader confidence="0.992337">
6.4.5 Bare singulars
</subsectionHeader>
<bodyText confidence="0.96119375">
We regard bare singulars as essentially plural, un-
der the linguistic assumption of non-overlapping
atomic parts – for instance, water is considered a
collection of H2O molecules, rice is regarded as
a collection of grains of rice, etc (see Chierchia,
1998). In order to make this relation clear, we
ask annotators to try and paraphrase bare singulars
with an (atomic part) plural equivalent and follow,
as normal, the decision tree for plurals:
18. Free software allows users to co-operate in
enhancing and refining the programs they use
� Open source programs allow users...
When the paraphrase is impossible (as in certain
non-mass terms which have no identifiable parts),
the noun phrase is deemed a unique entity and la-
belled ONE.
</bodyText>
<sectionHeader confidence="0.877236" genericHeader="method">
7 Implementation and results
</sectionHeader>
<subsectionHeader confidence="0.994714">
7.1 Task implementation
</subsectionHeader>
<bodyText confidence="0.99994175">
Three annotators were used in our experiment.
One annotator was one of the authors; the
other two annotators were graduate students (non-
linguists), both fluent in English. The two grad-
uate students were provided with individual train-
ing sessions where they first read the annotation
guidelines, had the opportunity to ask for clarifi-
cations, and subsequently annotated, with the help
of the author, the 50 noun phrases in the train-
ing set. The actual annotation task was performed
without communication with the scheme author or
the other annotators.
</bodyText>
<subsectionHeader confidence="0.99927">
7.2 Kappa evaluation
</subsectionHeader>
<bodyText confidence="0.999644">
We made an independence assumption between
quantification value and kind value, and evaluated
agreement separately for each type of annotation.
Intra-annotator agreement was calculated over
the set of annotations produced by one of the au-
thors. The original annotation experiment was re-
produced at three months’ interval and Kappa was
</bodyText>
<page confidence="0.996459">
78
</page>
<table confidence="0.9290505">
Class Kind Quantification
Kappa 0.85 0.84
</table>
<tableCaption confidence="0.993765">
Table 1: Intra-annotator agreements for both tasks
</tableCaption>
<table confidence="0.963532">
Class Kind Quantification
Kappa 0.67 0.72
</table>
<tableCaption confidence="0.878533333333333">
Table 2: Inter-annotator agreements for both tasks
computed between the original set and the new set.
Table 1 shows results over 0.8 for both tasks, cor-
</tableCaption>
<bodyText confidence="0.953463487179487">
responding to ‘perfect agreement’ according to the
Landis and Koch classification (1977). This indi-
cates that the stability of the scheme is high.
Table 2 shows inter-annotator agreements of
over 0.6 for both tasks, which correspond to ‘sub-
stantial agreement’. This result must be taken with
caution, though. Although it shows good agree-
ment overall, it is important to ascertain in what
measure it holds for separate classes. In an ef-
fort to report such per class agreement, we cal-
culate Kappa values for each label by evaluating
each class against all others collapsed together (as
suggested by Krippendorf, 1980).
Table 3 indicates that substantial agreement is
maintained for separate classes in the kind annota-
tion task. Table 4, however, suggests that, if agree-
ment is perfect for the ONE and QUANT classes,
it is very much lower for the SOME, MOST and
ALL classes. While it is clear that the latter three
are the most complex to analyse, we can show
that the lower results attached to them are partly
due to issues related to Kappa as a measure of
agreement. Feinstein and Cicchetti (1990), fol-
lowed by Di Eugenio and Glass (2004) proved
that Kappa is subject to the effect of prevalence
and that different marginal distributions can lead
to very different Kappa values for the same ob-
served agreement. It can be shown, in particu-
lar, that an unbalanced, symmetrical distribution
of the data produces much lower figures than bal-
anced or unbalanced, asymmetrical distributions
because the expected agreement gets inflated. Our
confusion matrices indicate that our data falls into
the category of unbalanced, symmetrical distribu-
tion: the classes are not evenly distributed but an-
notators agree on the relative prevalence of each
class. Moreover, in the quantification task itself,
the ONE class covers roughly 50% of the data.
This means that, when calculating per class agree-
</bodyText>
<table confidence="0.976581">
Class KIND NOT-KIND QUANT
Kappa 0.63 0.71 0.88
</table>
<tableCaption confidence="0.8759735">
Table 3: Per class inter-annotator agreement for
the kind annotation
</tableCaption>
<table confidence="0.993589">
Class ONE SOME MOST ALL QUANT
Kappa 0.81 0.45 0.44 0.51 0.88
</table>
<tableCaption confidence="0.938288">
Table 4: Per class inter-annotator agreement for
the quantification annotation
</tableCaption>
<bodyText confidence="0.999580466666667">
ment, we get an approximately balanced distri-
bution for the ONE label and an unbalanced, but
still symmetrical, distribution for the other labels.
This leads to the expected agreement being rather
low for the ONE class and very high for the other
classes. Table 5 reproduces the per class agree-
ment figures obtained for the quantification task
but shows, in addition, the observed and expected
agreements for each label. Although the observed
agreement is consistently close to, or over, 0.9, the
Kappa values differ widely in conjunction with ex-
pected agreement. This results in relatively low re-
sults for SOME, MOST and ALL (the QUANT label
has nearly perfect agreement and therefore doesn’t
suffer from prevalence).
</bodyText>
<table confidence="0.999494166666667">
Class Kappa Pr(a) Pr(e)
ONE 0.814 0.911 0.521
SOME 0.445 0.893 0.808
MOST 0.438 0.931 0.877
ALL 0.509 0.867 0.728
QUANT 0.884 0.987 0.885
</table>
<tableCaption confidence="0.995602">
Table 5: The effect of prevalence on per class
</tableCaption>
<bodyText confidence="0.975861272727273">
agreement, quantification task. Pr(a) is the ob-
served agreement between annotators, Pr(e) the
expected agreement.
With regard to the purpose of creating a gold
standard for a quantification resolution system, we
also note that out of 300 quantification annota-
tions, there are only 14 cases in which a majority
decision cannot be found, i.e., at least two anno-
tators agreed in 95% of cases. Thus, despite some
low Kappa results, the data can adequately be used
for the production of training material.4
</bodyText>
<footnote confidence="0.99866025">
4As far as such data ever can be: Reidsma and Carletta,
2008, show that systematic disagreements between annota-
tors will produce bad machine learning, regardless of the
Kappa obtained on the data.
</footnote>
<page confidence="0.998471">
79
</page>
<bodyText confidence="0.999692666666667">
In Section 8, we introduce difficulties encoun-
tered by our subjects, as related in post-annotation
discussions. We focus on quantification.
</bodyText>
<sectionHeader confidence="0.971404" genericHeader="method">
8 Annotation issues
</sectionHeader>
<subsectionHeader confidence="0.853014">
8.1 Reference
</subsectionHeader>
<bodyText confidence="0.999506956521739">
Although we tried to make the task as simple as
possible for the annotators by asking them to para-
phrase the sentences that they were reading, they
were not free from having to work out the refer-
ent of the NP (consciously or unconsciously) and
we have evidence that they did not always pick
the same referent, leading to disagreements at the
quantification stage. Consider the following:
19. Subsequent annexations by Florence in the
area have further diminished the likelihood of
incorporation.
In the course of post-annotation discussions, it
became clear that not all annotators had chosen the
same referent when quantifying the subject NP in
the first clause. One annotator had chosen as refer-
ent subsequent annexations, leading to the reading
Some subsequent annexations, conducted by Flo-
rence in the area, have further diminished the like-
lihood of incorporation. The other two annotators
had kept the whole NP as referent, leading to the
reading All the subsequent annexations conducted
by Florence in the area have further diminished
the likelihood of incorporation.
</bodyText>
<subsectionHeader confidence="0.993152">
8.2 World knowledge
</subsectionHeader>
<bodyText confidence="0.999104615384615">
Being given only one sentence as context for the
NP to quantify, annotators sometimes lacked the
world knowledge necessary to make an informed
decision. This is illustrated by the following:
20. The undergraduate schools maintain a non-
restrictive Early Action admissions pro-
gramme.
Discussion revealed that all three annotators had
a different interpretation of what the mentioned
Early Action programme might refer to, and of
the duties of the undergraduate schools with re-
gard to it. This led to three different quantifica-
tions: SOME, MOST and ALL.
</bodyText>
<subsectionHeader confidence="0.990385">
8.3 Interaction with time
</subsectionHeader>
<bodyText confidence="0.997776638888889">
The existence of interactions between NP quantifi-
cation and what we will call temporal quantifica-
tion is not surprising: we refer to the literature on
genericity and in particular to Krifka et al (1995)
who talk of characteristic predication, or habitual-
ity, as a phenomenon encompassed by genericity.
We do not intend to argue for a unified theory of
quantification, as temporal quantification involves
complexities which are beyond the scope of this
work. However, the interactions observed between
temporality and NP quantification might explain
further disagreements in the annotation task. The
following is a sentence that contains a temporal
adverb (sometimes) and that produced some dis-
agreement amongst annotators:
21. Scottish fiddlers emulating 18th-century
playing styles sometimes use a replica of the
type of bow used in that period.
Two annotators labelled the subject of that sen-
tence as MOST, while the third one preferred
SOME. In order to understand the issue, consider
the following, related, statement:
22. Mosquitoes sometimes carry malaria.
This sentence has the possible readings: Some
mosquitoes carry malaria or Mosquitoes, from
time to time in their lives, carry malaria. The first
reading is clearly the preferred one.
The structure of (21) is identical to that of (22)
and it should therefore be taken as similarly am-
biguous: it either means that some of the Scottish
fiddlers emulating 18th-century playing styles use
a replica of the bow used in that period, or that a
Scottish fiddler who emulates 18th-century play-
ing styles, from time to time, uses a replica of such
a bow. The two readings may explain the labels
given to that sentence by the annotators.
</bodyText>
<sectionHeader confidence="0.997704" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999985153846154">
Taking prevalence effects into account, we believe
that our agreement results can be taken as evidence
that underquantification is analysable in a consis-
tent way by humans. We also consider them as
strong support for our claim that ‘genericity quan-
tifies’. Our scheme could however be refined fur-
ther. In a future version, we would add guidelines
regarding the selection of the referent of the noun
phrase, encourage the use of external resources to
obtain the context of a given sentence (or simply
provide the actual context of the sentence), and
give some pointers as to how to resolve issues or
ambiguities caused by temporal quantification.
</bodyText>
<page confidence="0.995718">
80
</page>
<sectionHeader confidence="0.995816" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999946438202247">
ACE. 2008. ACE (Automatic Content Extraction) En-
glish Annotation Guidelines for Entities, Version 6.6
2008.06.13. Linguistic Data Consortium.
Edward Briscoe, John Carroll and Rebecca Watson.
2006. ‘The Second Release of the RASP System’.
In Proceedings of the COLING/ACL 2006 Interac-
tive Presentation Sessions, Sydney, Australia, 2006.
Gregory Carlson. 1995. ‘Truth-conditions of Generics
Sentences: Two Contrasting Views’. In Gregory N.
Carlson and Francis Jeffrey Pelletier, Editors, The
Generic Book, pages 224 – 237. Chicago University
Press.
Gennaro Chierchia. 1998. ‘Reference to kinds across
languages’. Natural Language Semantics, 6:339–
405.
Ariel Cohen. 1996. Think Generic: The Meaning
and Use of Generic Sentences. Ph.D. Dissertation.
Carnegie Mellon University at Pittsburgh. Published
by CSLI, Stanford, 1999.
Ann Copestake. 2004. ‘Robust Minimal Recursion
Semantics’. www.cl.cam.ac.uk/˜aac10/
papers/rmrsdraft.pdf.
Barbara Di Eugenio and Michael Glass. 2004. ‘The
kappa statistic: a second look’. Computational Lin-
guistics, 30(1):95–101.
Alvan R. Feinstein and Domenic V. Cicchetti. 1990.
‘High agreement but low kappa: I. The problems of
two paradoxes’. Journal of Clinical Epidemiology,
43(6):543–549.
Joseph Fleiss. 1971. ‘Measuring nominal scale agree-
ment among many raters’. Psychological Bulletin,
76(5):378-382.
Aurelie Herbelot. To appear. Underspecified quantifi-
cation. Ph.D. Dissertation. Computer Laboratory,
University of Cambridge, United Kingdom.
Gerhard Heyer. 1990. ‘Semantics and Knowledge
Representation in the Analysis of Generic Descrip-
tions’. Journal of Semantics, 7(1):93–110.
Paul Kingsbury, Martha Palmer and Mitch Marcus.
2002. ‘Adding Semantic Annotation to the Penn
TreeBank’. In Proceedings of the Human Language
Technology Conference (HLT 2002), San Diego,
California, pages 252–256.
Manfred Krifka, Francis Jeffry Pelletier, Gregory N.
Carlson, Alice ter Meulen, Godehard Link and Gen-
naro Chierchia. 1995. ‘Genericity: An Introduc-
tion’. In Gregory N. Carlson and Francis Jeffry
Pelletier, Editors. The Generic Book, pages 1–125.
Chicago: Chicago University Press.
Klaus Krippendorff. 1980. Content Analysis: An In-
troduction to Its Methodology. Newbury Park, CA:
Sage.
J. Richard Landis and Gary G. Koch. 1977. ‘The
Measurement of Observer Agreement for Categor-
ical Data’. Biometrics, 33:159–174.
Sara-Jane Leslie. 2008. ‘Generics: Cognition and Ac-
quisition.’ Philosophical Review, 117(1):1–47.
Christopher Lyons. 1999. Definiteness. Cambridge
University Press, Cambridge, UK.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. ‘Building a large annotated
corpus of english: The penn treebank’. Computa-
tional Linguistics, 19(2):313–330.
Francis Jeffry Pelletier and Nicolas Asher. 1997.
‘Generics and defaults’. In: Johan van Benthem and
Alice ter Meulen, Editors, Handbook of Logic and
Language, pages 1125–1177. Amsterdam: Elsevier.
Massimo Poesio. 2000. ‘The GNOME annota-
tion scheme manual’, Fourth Version. http:
//cswww.essex.ac.uk/Research/nle/
corpora/GNOME/anno_manual_4.htm
Massimo Poesio. 2004. ‘Discourse Annotation and Se-
mantic Annotation in the GNOME Corpus’. In: Pro-
ceedings of the ACL Workshop on Discourse Anno-
tation, Barcelona, Spain.
Dennis Reidsma and Jean Carletta. 2008. ‘Reliability
measurement without limits’. Computational Lin-
guistics, 34(3), pages 319–326.
Anna Ritchie. 2004. ‘Compatible RMRS Repre-
sentations from RASP and the ERG’. http:
//www.cl.cam.ac.uk/TechReports/
UCAM-CL-TR-661.
Simone Teufel, Advaith Siddharthan, Dan Tidhar.
2006. ‘An annotation scheme for citation function’.
In: Proceedings of Sigdial-06, Sydney, Australia,
pages 80–87.
Simone Teufel. 2010. The Structure of Scientific Arti-
cles: Applications to Summarisation and Citation
Indexing. CSLI Publications. In press.
</reference>
<page confidence="0.999264">
81
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.094615">
<title confidence="0.998003">Annotating Underquantification</title>
<author confidence="0.943266">Aurelie</author>
<affiliation confidence="0.985764">University of</affiliation>
<address confidence="0.779178">Cambridge, United</address>
<email confidence="0.919335">ah433@cam.ac.uk</email>
<abstract confidence="0.998252833333333">Many noun phrases in text are ambiguously quantified: syntax doesn’t explicitly tell us whether they refer to a single entity or to several, and what portion of the set denoted by the Nbar actually takes part in the event expressed by the verb. We describe this ambiguity phenomenon in of underspecification, or rather un- We attempt to validate the underquantification hypothesis by producing and testing an annotation scheme for quantification resolution, the aim of which is to associate a single quantifier with each noun phrase in our corpus. 1 Quantification resolution are concerned with quantified noun phrases (NPs) and their interpretation, as illustrated by the following examples: Cats are mammals = Cats have four legs = Cats were sleeping by the fire = The beans spilt out of the bag = of 5. Water was dripping through the ceiling = are interested in that is, the process of giving an ambiguously quan- NP a formalisation which expresses a set relation appropriate to the semantics of the utterance. For instance, we wish to arrive at: 6. All cats are mammals. o is the set of all cats and 0 the set of all mammals. Resolving the quantification value of NPs is important for many NLP tasks. Let us imagine an information extraction system having retrieved the triples ‘cat – is – mammal’ and ‘cat – chase –</abstract>
<author confidence="0.609645">Ann</author>
<affiliation confidence="0.95354">University of</affiliation>
<address confidence="0.686705">Cambridge, United</address>
<email confidence="0.514957">aac10@cam.ac.uk</email>
<abstract confidence="0.996927270833333">mouse’ for inclusion in a factual database about felines. The problem with those representationpoor triples is that they do not contain the necessary information about quantification to answer such questions as ‘Are all cats mammals?’ or ‘Do all cats chase mice?’ Or if they attempt to answer those queries, they give the same answer to both. Ideally, we would like to annotate such triples with quantifiers which have a direct mapping to probability adverbs: All are mammals AND Tom is a cat is mammal. Some chase mice AND Tom is a cat mice. Adequate quantification is also necessary for inference based on word-level entailment: an existentially quantified NP can be replaced by a suitable hypernym but this is not possible in noncases: cats are in my garden animals are in my garden are mammals imply that animals In Herbelot (to appear), we provide a formal semantics for ambiguously quantified NPs, which relies on the idea that those NPs exhibit an underspecified quantifier, i.e. that for each NP in a corpus, a set relation can be agreed upon. Our formalisation includes a placeholder for the quantifier’s set relation. In line with inference requirements, we assume a three-fold partitioning of the quantificational space, corresponding to the natural lanquantifiers addition for the description of singular, unique entities). The corresponding set relations are: 9. true iff 10. true iff &lt;_ 11. true iff This paper is an attempt to show that our formalisation lends itself to evaluation by human annotation. The labels produced will also serve as training and test sets for an automatic quantification resolution system. 73 of the Fourth Linguistic Annotation Workshop, ACL pages Sweden, 15-16 July 2010. Association for Computational Linguistics 2 Under(specified) quantification Before we present our annotation scheme, we will out the essential idea behind what we call un- The phenomenon of ambiguous quantification</abstract>
<note confidence="0.628378">with Krifka et al, 1995,</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>ACE</author>
</authors>
<title>ACE (Automatic Content Extraction) English Annotation Guidelines for Entities, Version 6.6 2008.06.13. Linguistic Data Consortium.</title>
<date>2008</date>
<marker>ACE, 2008</marker>
<rawString>ACE. 2008. ACE (Automatic Content Extraction) English Annotation Guidelines for Entities, Version 6.6 2008.06.13. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Briscoe</author>
<author>John Carroll</author>
<author>Rebecca Watson</author>
</authors>
<title>The Second Release of the RASP System’.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions,</booktitle>
<location>Sydney, Australia,</location>
<contexts>
<context position="16174" citStr="Briscoe et al, 2006" startWordPosition="2648" endWordPosition="2651">ikipedia is written and edited by many contributors, meaning that speaker heterogeneity is high. We would also expect an encyclopaedia to contain relatively many 1http://www.wikipedia.org generics, allowing us to assess how our quantificational reading fares in a real annotation task. Finally, the use of an open resource means that the corpus can be freely distributed.2 In order to create our annotation corpus, we first isolated the first 100,000 pages in our snapshot and parsed them into a Robust Minimal Recursion Semantics (RMRS) representation (Copestake, 2004) using first the RASP parser (Briscoe et al, 2006) and the RASP to RMRS converter (Ritchie, 2004). We then extracted all constructions of the type Subject-Verb-Object from the obtained corpus and randomly selected 300 of those ‘triples’ to be annotated. Another 50 random triples were selected for the purpose of annotation training (see Section 7.1). We show in Figure 1 an example of an annotation instance produced by the parser pipeline. The data provided by the system consists of the triple itself, followed by the argument structure of that triple, including the direct dependents of its constituents, the number and tense information for each</context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>Edward Briscoe, John Carroll and Rebecca Watson. 2006. ‘The Second Release of the RASP System’. In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, Sydney, Australia, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Carlson</author>
</authors>
<title>Truth-conditions of Generics Sentences: Two Contrasting Views’. In Gregory N. Carlson and Francis Jeffrey Pelletier, Editors, The Generic Book,</title>
<date>1995</date>
<pages>224--237</pages>
<publisher>Chicago University Press.</publisher>
<contexts>
<context position="4277" citStr="Carlson, 1995" startWordPosition="713" endWordPosition="714">CL 2010, pages 73–81, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics 2 Under(specified) quantification Before we present our annotation scheme, we will spell out the essential idea behind what we call underquantification. The phenomenon of ambiguous quantification overlaps with genericity (see Krifka et al, 1995, for an introduction to genericity). Generic NPs are frequently expressed syntactically as bare plurals, although they occur in definite and indefinite singulars too, as well as bare singulars. There are many views on the semantics of generics (e.g. Carlson, 1995; Pelletier and Asher, 1997; Heyer, 1990; Leslie, 2008) but one of them is that they quantify (Cohen, 1996), although, puzzlingly enough, not always with the same quantifier: 12. Frenchmen eat horsemeat = Some/Relativelymany Frenchmen... (For the relatively many reading, see Cohen, 2001.) 13. Cars have four wheels = Most cars... 14. Typhoons arise in this part of the Pacific = Some typhoons... OR Most/All typhoons... This behaviour has so far prevented linguists from agreeing on a single formalisation for all generics. The only accepted assumption is that an operator GEN exists, which acts as </context>
</contexts>
<marker>Carlson, 1995</marker>
<rawString>Gregory Carlson. 1995. ‘Truth-conditions of Generics Sentences: Two Contrasting Views’. In Gregory N. Carlson and Francis Jeffrey Pelletier, Editors, The Generic Book, pages 224 – 237. Chicago University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gennaro Chierchia</author>
</authors>
<title>Reference to kinds across languages’. Natural Language Semantics,</title>
<date>1998</date>
<pages>6--339</pages>
<contexts>
<context position="7111" citStr="Chierchia (1998)" startWordPosition="1168" endWordPosition="1169">enon (e.g. Lyons, 1999). Because of space constraints, we will not give our formalisation for underquantification in this paper (see Herbelot,to appear, for details). It involves a representation of the partitive construct exemplified above and requires knowledge of the distributive or collective status of the verbal predicate. We also argue that if generics can always be quantified, their semantics may involve more than quantification. So we claim that in certain cases, a double formalisation of the NP as a quantified entity and a kind is desirable. We understand kinds in the way proposed by Chierchia (1998), that is as the plurality of all instances denoted by a given word in the world under consideration. Under the kind reading, we can interpret 12 as meaning Collectively, the group of all Frenchmen has the property of eating horsemeat. 3 Motivation 3.1 Linguistic motivation It is usual to talk of ‘annotation’ generically, to cover any process that involves humans using a set of guidelines to mark some specific linguistic phenomenon in some given text. However, we would argue that, when considering the aims of an annotation task and its relation to the existing linguistic literature, it becomes</context>
<context position="24656" citStr="Chierchia, 1998" startWordPosition="3990" endWordPosition="3991">lurals, singulars must be tested for a kind reading. This is done by attempting to pluralise the noun phrase. If pluralisation is possible, then the kind interpretation is confirmed and quantification is performed. If not (certain non-mass terms have no identifiable parts), the singular refers to a single entity and is annotated as ONE. 6.4.5 Bare singulars We regard bare singulars as essentially plural, under the linguistic assumption of non-overlapping atomic parts – for instance, water is considered a collection of H2O molecules, rice is regarded as a collection of grains of rice, etc (see Chierchia, 1998). In order to make this relation clear, we ask annotators to try and paraphrase bare singulars with an (atomic part) plural equivalent and follow, as normal, the decision tree for plurals: 18. Free software allows users to co-operate in enhancing and refining the programs they use � Open source programs allow users... When the paraphrase is impossible (as in certain non-mass terms which have no identifiable parts), the noun phrase is deemed a unique entity and labelled ONE. 7 Implementation and results 7.1 Task implementation Three annotators were used in our experiment. One annotator was one </context>
</contexts>
<marker>Chierchia, 1998</marker>
<rawString>Gennaro Chierchia. 1998. ‘Reference to kinds across languages’. Natural Language Semantics, 6:339– 405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariel Cohen</author>
</authors>
<title>Think Generic: The Meaning and Use of Generic Sentences.</title>
<date>1996</date>
<tech>Ph.D.</tech>
<institution>Dissertation. Carnegie Mellon University at Pittsburgh.</institution>
<note>Published by</note>
<contexts>
<context position="4384" citStr="Cohen, 1996" startWordPosition="731" endWordPosition="732">nder(specified) quantification Before we present our annotation scheme, we will spell out the essential idea behind what we call underquantification. The phenomenon of ambiguous quantification overlaps with genericity (see Krifka et al, 1995, for an introduction to genericity). Generic NPs are frequently expressed syntactically as bare plurals, although they occur in definite and indefinite singulars too, as well as bare singulars. There are many views on the semantics of generics (e.g. Carlson, 1995; Pelletier and Asher, 1997; Heyer, 1990; Leslie, 2008) but one of them is that they quantify (Cohen, 1996), although, puzzlingly enough, not always with the same quantifier: 12. Frenchmen eat horsemeat = Some/Relativelymany Frenchmen... (For the relatively many reading, see Cohen, 2001.) 13. Cars have four wheels = Most cars... 14. Typhoons arise in this part of the Pacific = Some typhoons... OR Most/All typhoons... This behaviour has so far prevented linguists from agreeing on a single formalisation for all generics. The only accepted assumption is that an operator GEN exists, which acts as a silent quantifier over the restrictor (subject) and matrix (verbal predicate) of the generic statement. T</context>
</contexts>
<marker>Cohen, 1996</marker>
<rawString>Ariel Cohen. 1996. Think Generic: The Meaning and Use of Generic Sentences. Ph.D. Dissertation. Carnegie Mellon University at Pittsburgh. Published by CSLI, Stanford, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
</authors>
<title>Robust Minimal Recursion Semantics’.</title>
<date>2004</date>
<note>www.cl.cam.ac.uk/˜aac10/ papers/rmrsdraft.pdf.</note>
<contexts>
<context position="16124" citStr="Copestake, 2004" startWordPosition="2640" endWordPosition="2642">ike rules of games. Further, each article in Wikipedia is written and edited by many contributors, meaning that speaker heterogeneity is high. We would also expect an encyclopaedia to contain relatively many 1http://www.wikipedia.org generics, allowing us to assess how our quantificational reading fares in a real annotation task. Finally, the use of an open resource means that the corpus can be freely distributed.2 In order to create our annotation corpus, we first isolated the first 100,000 pages in our snapshot and parsed them into a Robust Minimal Recursion Semantics (RMRS) representation (Copestake, 2004) using first the RASP parser (Briscoe et al, 2006) and the RASP to RMRS converter (Ritchie, 2004). We then extracted all constructions of the type Subject-Verb-Object from the obtained corpus and randomly selected 300 of those ‘triples’ to be annotated. Another 50 random triples were selected for the purpose of annotation training (see Section 7.1). We show in Figure 1 an example of an annotation instance produced by the parser pipeline. The data provided by the system consists of the triple itself, followed by the argument structure of that triple, including the direct dependents of its const</context>
</contexts>
<marker>Copestake, 2004</marker>
<rawString>Ann Copestake. 2004. ‘Robust Minimal Recursion Semantics’. www.cl.cam.ac.uk/˜aac10/ papers/rmrsdraft.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Di Eugenio</author>
<author>Michael Glass</author>
</authors>
<title>The kappa statistic: a second look’.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<marker>Di Eugenio, Glass, 2004</marker>
<rawString>Barbara Di Eugenio and Michael Glass. 2004. ‘The kappa statistic: a second look’. Computational Linguistics, 30(1):95–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alvan R Feinstein</author>
<author>Domenic V Cicchetti</author>
</authors>
<title>High agreement but low kappa: I. The problems of two paradoxes’.</title>
<date>1990</date>
<journal>Journal of Clinical Epidemiology,</journal>
<volume>43</volume>
<issue>6</issue>
<contexts>
<context position="27481" citStr="Feinstein and Cicchetti (1990)" startWordPosition="4447" endWordPosition="4450">greement, we calculate Kappa values for each label by evaluating each class against all others collapsed together (as suggested by Krippendorf, 1980). Table 3 indicates that substantial agreement is maintained for separate classes in the kind annotation task. Table 4, however, suggests that, if agreement is perfect for the ONE and QUANT classes, it is very much lower for the SOME, MOST and ALL classes. While it is clear that the latter three are the most complex to analyse, we can show that the lower results attached to them are partly due to issues related to Kappa as a measure of agreement. Feinstein and Cicchetti (1990), followed by Di Eugenio and Glass (2004) proved that Kappa is subject to the effect of prevalence and that different marginal distributions can lead to very different Kappa values for the same observed agreement. It can be shown, in particular, that an unbalanced, symmetrical distribution of the data produces much lower figures than balanced or unbalanced, asymmetrical distributions because the expected agreement gets inflated. Our confusion matrices indicate that our data falls into the category of unbalanced, symmetrical distribution: the classes are not evenly distributed but annotators ag</context>
</contexts>
<marker>Feinstein, Cicchetti, 1990</marker>
<rawString>Alvan R. Feinstein and Domenic V. Cicchetti. 1990. ‘High agreement but low kappa: I. The problems of two paradoxes’. Journal of Clinical Epidemiology, 43(6):543–549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters’.</title>
<date>1971</date>
<journal>Psychological Bulletin,</journal>
<pages>76--5</pages>
<contexts>
<context position="17991" citStr="Fleiss, 1971" startWordPosition="2930" endWordPosition="2931">-annotator agreement. Stability relates to whether the same annotator will consistently produce the same annotations at different points in time. The measure for stability is called intra-annotator agreement. Both measures concern the repeatability of an annotation experiment. In this work, agreement is calculated for each pair of annotators according to the Kappa measure. There are different versions of Kappa depending on how multiple annotators are treated and how the probabilities of classes are calculated to establish the expected agreement between annotators, Pr(e): we use Fleiss’ Kappa (Fleiss, 1971), which allows us to compute agreement between 2For access, contact the first author. 76 digraph G211 { &amp;quot;TRIPLE: weed include pigra&amp;quot; [shape=box]; include -&gt; weed [label=&amp;quot;ARG1 n&amp;quot;]; include -&gt; pigra [label=&amp;quot;ARG2 n&amp;quot;]; invasive -&gt; weed [label=&amp;quot;ARG1 n&amp;quot;]; compound_rel -&gt; pigra [label=&amp;quot;ARG1 n&amp;quot;]; compound_rel -&gt; mimosa [label=&amp;quot;ARG2 n&amp;quot;]; &amp;quot;DNT INFO: lemma::include() tense::present lpos::v (arg::ARG1 var::weed() num::pl pos::) (arg::ARG2 var::pigra() num::sg pos::)&amp;quot; [shape=box]; &amp;quot;FILE: /anfs/bigtmp/newr1-50/page101655&amp;quot; [shape=box]; &amp;quot;ORIGINAL: Invasive weeds include Mimosa pigra, which covers 80,000 hecta</context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>Joseph Fleiss. 1971. ‘Measuring nominal scale agreement among many raters’. Psychological Bulletin, 76(5):378-382.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Aurelie Herbelot</author>
</authors>
<title>To appear. Underspecified quantification.</title>
<institution>Ph.D. Dissertation. Computer Laboratory, University of Cambridge, United Kingdom.</institution>
<marker>Herbelot, </marker>
<rawString>Aurelie Herbelot. To appear. Underspecified quantification. Ph.D. Dissertation. Computer Laboratory, University of Cambridge, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerhard Heyer</author>
</authors>
<title>Semantics and Knowledge Representation in the Analysis of Generic Descriptions’.</title>
<date>1990</date>
<journal>Journal of Semantics,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="4317" citStr="Heyer, 1990" startWordPosition="719" endWordPosition="720">16 July 2010. c�2010 Association for Computational Linguistics 2 Under(specified) quantification Before we present our annotation scheme, we will spell out the essential idea behind what we call underquantification. The phenomenon of ambiguous quantification overlaps with genericity (see Krifka et al, 1995, for an introduction to genericity). Generic NPs are frequently expressed syntactically as bare plurals, although they occur in definite and indefinite singulars too, as well as bare singulars. There are many views on the semantics of generics (e.g. Carlson, 1995; Pelletier and Asher, 1997; Heyer, 1990; Leslie, 2008) but one of them is that they quantify (Cohen, 1996), although, puzzlingly enough, not always with the same quantifier: 12. Frenchmen eat horsemeat = Some/Relativelymany Frenchmen... (For the relatively many reading, see Cohen, 2001.) 13. Cars have four wheels = Most cars... 14. Typhoons arise in this part of the Pacific = Some typhoons... OR Most/All typhoons... This behaviour has so far prevented linguists from agreeing on a single formalisation for all generics. The only accepted assumption is that an operator GEN exists, which acts as a silent quantifier over the restrictor </context>
</contexts>
<marker>Heyer, 1990</marker>
<rawString>Gerhard Heyer. 1990. ‘Semantics and Knowledge Representation in the Analysis of Generic Descriptions’. Journal of Semantics, 7(1):93–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kingsbury</author>
<author>Martha Palmer</author>
<author>Mitch Marcus</author>
</authors>
<title>Adding Semantic Annotation to the Penn TreeBank’.</title>
<date>2002</date>
<booktitle>In Proceedings of the Human Language Technology Conference (HLT 2002),</booktitle>
<pages>252--256</pages>
<location>San Diego, California,</location>
<contexts>
<context position="8618" citStr="Kingsbury et al, 2002" startWordPosition="1417" endWordPosition="1420">well-known and wellunderstood labels. The production of tree banks like the Penn Treebank (Marcus et al, 1993) makes use of undisputed linguistic categories such as parts of speech. The aim is to make the computer learn and use irrefutable bits of linguistics. (Note that, despite agreement, the representation of those categories may differ: see for example the range of available parts of speech tag sets.) This type of task mostly involves basic syntactic knowledge, but can be taken to areas of syntax and seman74 tics where the studied phenomena have a (somewhat) clear, agreed upon definition (Kingsbury et al, 2002). We must clarify that in those cases, the choice of a formalism may already imply a certain theoretical position – leading to potential incompatibilities between formalisms. However, the categories for such annotation are themselves fixed: there is a generally agreed broad understanding of concepts such as noun phrases and coordination. Another type of annotation concerns tasks where the linguistic categories at play are not fixed. One example is discourse annotation according to rhetorical function (Teufel et al, 2006) where humans are asked to differentiate between several discursive catego</context>
</contexts>
<marker>Kingsbury, Palmer, Marcus, 2002</marker>
<rawString>Paul Kingsbury, Martha Palmer and Mitch Marcus. 2002. ‘Adding Semantic Annotation to the Penn TreeBank’. In Proceedings of the Human Language Technology Conference (HLT 2002), San Diego, California, pages 252–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manfred Krifka</author>
<author>Francis Jeffry Pelletier</author>
<author>Gregory N Carlson</author>
</authors>
<title>Alice ter Meulen, Godehard Link and Gennaro Chierchia.</title>
<date>1995</date>
<pages>1--125</pages>
<publisher>Chicago University Press.</publisher>
<location>Chicago:</location>
<contexts>
<context position="4013" citStr="Krifka et al, 1995" startWordPosition="670" endWordPosition="673"> is an attempt to show that our formalisation lends itself to evaluation by human annotation. The labels produced will also serve as training and test sets for an automatic quantification resolution system. 73 Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 73–81, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics 2 Under(specified) quantification Before we present our annotation scheme, we will spell out the essential idea behind what we call underquantification. The phenomenon of ambiguous quantification overlaps with genericity (see Krifka et al, 1995, for an introduction to genericity). Generic NPs are frequently expressed syntactically as bare plurals, although they occur in definite and indefinite singulars too, as well as bare singulars. There are many views on the semantics of generics (e.g. Carlson, 1995; Pelletier and Asher, 1997; Heyer, 1990; Leslie, 2008) but one of them is that they quantify (Cohen, 1996), although, puzzlingly enough, not always with the same quantifier: 12. Frenchmen eat horsemeat = Some/Relativelymany Frenchmen... (For the relatively many reading, see Cohen, 2001.) 13. Cars have four wheels = Most cars... 14. T</context>
<context position="32176" citStr="Krifka et al (1995)" startWordPosition="5200" endWordPosition="5203"> illustrated by the following: 20. The undergraduate schools maintain a nonrestrictive Early Action admissions programme. Discussion revealed that all three annotators had a different interpretation of what the mentioned Early Action programme might refer to, and of the duties of the undergraduate schools with regard to it. This led to three different quantifications: SOME, MOST and ALL. 8.3 Interaction with time The existence of interactions between NP quantification and what we will call temporal quantification is not surprising: we refer to the literature on genericity and in particular to Krifka et al (1995) who talk of characteristic predication, or habituality, as a phenomenon encompassed by genericity. We do not intend to argue for a unified theory of quantification, as temporal quantification involves complexities which are beyond the scope of this work. However, the interactions observed between temporality and NP quantification might explain further disagreements in the annotation task. The following is a sentence that contains a temporal adverb (sometimes) and that produced some disagreement amongst annotators: 21. Scottish fiddlers emulating 18th-century playing styles sometimes use a rep</context>
</contexts>
<marker>Krifka, Pelletier, Carlson, 1995</marker>
<rawString>Manfred Krifka, Francis Jeffry Pelletier, Gregory N. Carlson, Alice ter Meulen, Godehard Link and Gennaro Chierchia. 1995. ‘Genericity: An Introduction’. In Gregory N. Carlson and Francis Jeffry Pelletier, Editors. The Generic Book, pages 1–125. Chicago: Chicago University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology.</title>
<date>1980</date>
<publisher>Sage.</publisher>
<location>Newbury Park, CA:</location>
<marker>Krippendorff, 1980</marker>
<rawString>Klaus Krippendorff. 1980. Content Analysis: An Introduction to Its Methodology. Newbury Park, CA: Sage.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Richard Landis</author>
<author>Gary G Koch</author>
</authors>
<title>The Measurement of Observer Agreement for Categorical Data’.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<pages>33--159</pages>
<marker>Landis, Koch, 1977</marker>
<rawString>J. Richard Landis and Gary G. Koch. 1977. ‘The Measurement of Observer Agreement for Categorical Data’. Biometrics, 33:159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara-Jane Leslie</author>
</authors>
<date>2008</date>
<journal>Generics: Cognition and Acquisition.’ Philosophical Review,</journal>
<volume>117</volume>
<issue>1</issue>
<contexts>
<context position="4332" citStr="Leslie, 2008" startWordPosition="721" endWordPosition="722"> c�2010 Association for Computational Linguistics 2 Under(specified) quantification Before we present our annotation scheme, we will spell out the essential idea behind what we call underquantification. The phenomenon of ambiguous quantification overlaps with genericity (see Krifka et al, 1995, for an introduction to genericity). Generic NPs are frequently expressed syntactically as bare plurals, although they occur in definite and indefinite singulars too, as well as bare singulars. There are many views on the semantics of generics (e.g. Carlson, 1995; Pelletier and Asher, 1997; Heyer, 1990; Leslie, 2008) but one of them is that they quantify (Cohen, 1996), although, puzzlingly enough, not always with the same quantifier: 12. Frenchmen eat horsemeat = Some/Relativelymany Frenchmen... (For the relatively many reading, see Cohen, 2001.) 13. Cars have four wheels = Most cars... 14. Typhoons arise in this part of the Pacific = Some typhoons... OR Most/All typhoons... This behaviour has so far prevented linguists from agreeing on a single formalisation for all generics. The only accepted assumption is that an operator GEN exists, which acts as a silent quantifier over the restrictor (subject) and m</context>
</contexts>
<marker>Leslie, 2008</marker>
<rawString>Sara-Jane Leslie. 2008. ‘Generics: Cognition and Acquisition.’ Philosophical Review, 117(1):1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Lyons</author>
</authors>
<title>Definiteness.</title>
<date>1999</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="6518" citStr="Lyons, 1999" startWordPosition="1073" endWordPosition="1074">onal quantifiers, that GEN is silent in all known languages, and it explains also why it is the bare form which has the highest productivity, and can refer to a range of quantified sets, from existentials to universals. Using the underquantification hypothesis, we can paraphrase any generic of the form ‘X does Y’ as ‘there is a set of things X, a certain number of which do Y’ (note the partitive construction). Such a paraphrase allows us to also resolve ambiguously quantified definite plurals, which have traditionally been associated with universals, outside of the genericity phenomenon (e.g. Lyons, 1999). Because of space constraints, we will not give our formalisation for underquantification in this paper (see Herbelot,to appear, for details). It involves a representation of the partitive construct exemplified above and requires knowledge of the distributive or collective status of the verbal predicate. We also argue that if generics can always be quantified, their semantics may involve more than quantification. So we claim that in certain cases, a double formalisation of the NP as a quantified entity and a kind is desirable. We understand kinds in the way proposed by Chierchia (1998), that </context>
</contexts>
<marker>Lyons, 1999</marker>
<rawString>Christopher Lyons. 1999. Definiteness. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank’.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="8106" citStr="Marcus et al, 1993" startWordPosition="1332" endWordPosition="1335">t of guidelines to mark some specific linguistic phenomenon in some given text. However, we would argue that, when considering the aims of an annotation task and its relation to the existing linguistic literature, it becomes possible to distinguish between various types of annotation. Further, we will show that our own effort situates itself in a little studied relation to formal semantics. The most basic type of annotation is the one where computational linguists mark large amounts of textual data with well-known and wellunderstood labels. The production of tree banks like the Penn Treebank (Marcus et al, 1993) makes use of undisputed linguistic categories such as parts of speech. The aim is to make the computer learn and use irrefutable bits of linguistics. (Note that, despite agreement, the representation of those categories may differ: see for example the range of available parts of speech tag sets.) This type of task mostly involves basic syntactic knowledge, but can be taken to areas of syntax and seman74 tics where the studied phenomena have a (somewhat) clear, agreed upon definition (Kingsbury et al, 2002). We must clarify that in those cases, the choice of a formalism may already imply a cer</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. ‘Building a large annotated corpus of english: The penn treebank’. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francis Jeffry Pelletier</author>
<author>Nicolas Asher</author>
</authors>
<title>Generics and defaults’. In:</title>
<date>1997</date>
<booktitle>Johan van Benthem and Alice ter Meulen, Editors, Handbook of Logic and Language,</booktitle>
<pages>1125--1177</pages>
<publisher>Elsevier.</publisher>
<location>Amsterdam:</location>
<contexts>
<context position="4304" citStr="Pelletier and Asher, 1997" startWordPosition="715" endWordPosition="718">73–81, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics 2 Under(specified) quantification Before we present our annotation scheme, we will spell out the essential idea behind what we call underquantification. The phenomenon of ambiguous quantification overlaps with genericity (see Krifka et al, 1995, for an introduction to genericity). Generic NPs are frequently expressed syntactically as bare plurals, although they occur in definite and indefinite singulars too, as well as bare singulars. There are many views on the semantics of generics (e.g. Carlson, 1995; Pelletier and Asher, 1997; Heyer, 1990; Leslie, 2008) but one of them is that they quantify (Cohen, 1996), although, puzzlingly enough, not always with the same quantifier: 12. Frenchmen eat horsemeat = Some/Relativelymany Frenchmen... (For the relatively many reading, see Cohen, 2001.) 13. Cars have four wheels = Most cars... 14. Typhoons arise in this part of the Pacific = Some typhoons... OR Most/All typhoons... This behaviour has so far prevented linguists from agreeing on a single formalisation for all generics. The only accepted assumption is that an operator GEN exists, which acts as a silent quantifier over th</context>
</contexts>
<marker>Pelletier, Asher, 1997</marker>
<rawString>Francis Jeffry Pelletier and Nicolas Asher. 1997. ‘Generics and defaults’. In: Johan van Benthem and Alice ter Meulen, Editors, Handbook of Logic and Language, pages 1125–1177. Amsterdam: Elsevier.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
</authors>
<title>The GNOME annotation scheme manual’, Fourth Version.</title>
<date>2000</date>
<note>http: //cswww.essex.ac.uk/Research/nle/ corpora/GNOME/anno_manual_4.htm</note>
<contexts>
<context position="13153" citStr="Poesio, 2000" startWordPosition="2167" endWordPosition="2168">the experiments presented in this paper are of a slightly different nature than the standard research on annotation (despite the fact that, as we will show in the next section, they also aim at producing data for a language analysis system). 3.2 Previous work on genericity annotation The aim of our work being the production of an automatic quantification resolution system, we need an annotated corpus to train and test our machine learning algorithm. There is no corpus that we know of which would give us the required data. The closest contestants are the ACE corpus (2008) and the GNOME corpus (Poesio, 2000) which both focus on the phenomenon of genericity, as described in the linguistic literature. Unfortunately, neither of those corpora are suitable for use in a general quantification task. The ACE corpus only distinguishes between 75 ‘generic’ and ‘specific’ entities. The classification proposed by the authors of the corpus is therefore a lot broader than the one we are attempting here and there is no direct correspondence between their labels and natural language quantifiers: we have shown in Section 2 that genericity didn’t map to a particular division of the quantificational space. Furtherm</context>
</contexts>
<marker>Poesio, 2000</marker>
<rawString>Massimo Poesio. 2000. ‘The GNOME annotation scheme manual’, Fourth Version. http: //cswww.essex.ac.uk/Research/nle/ corpora/GNOME/anno_manual_4.htm</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
</authors>
<title>Discourse Annotation and Semantic Annotation in the GNOME Corpus’. In:</title>
<date>2004</date>
<booktitle>Proceedings of the ACL Workshop on Discourse Annotation,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="14994" citStr="Poesio, 2004" startWordPosition="2463" endWordPosition="2464">use the SPC tag”. The GNOME annotation scheme is closer in essence to the literature on genericity and much more detailed than the ACE guidelines. However, the scheme distinguishes only between generic and non-generic entities, as in the ACE corpus case, and the corpus itself is limited to three genres: museum labels, pharmaceutical leaflets, and tutorial dialogues. The guidelines are therefore tailored to the domains under consideration; for instance, bare noun phrases are said to be typically generic. This restricted solution has the advantage of providing good agreement between annotators (Poesio, 2004 reports a Kappa value of 0.82 for this annotation). 4 Annotation corpus We use as corpus a snapshot of the English version of the online encyclopaedia Wikipedia.1 The choice is motivated by the fact that Wikipedia can be taken as a fairly balanced corpus: although it is presented as an encyclopaedia, it contains a wide variety of text ranging from typical encyclopaedic descriptions to various types of narrative texts (historical reconstructions, film ‘spoilers’, fiction summaries) to instructional material like rules of games. Further, each article in Wikipedia is written and edited by many c</context>
</contexts>
<marker>Poesio, 2004</marker>
<rawString>Massimo Poesio. 2004. ‘Discourse Annotation and Semantic Annotation in the GNOME Corpus’. In: Proceedings of the ACL Workshop on Discourse Annotation, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dennis Reidsma</author>
<author>Jean Carletta</author>
</authors>
<title>Reliability measurement without limits’.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>3</issue>
<pages>319--326</pages>
<contexts>
<context position="29989" citStr="Reidsma and Carletta, 2008" startWordPosition="4854" endWordPosition="4857">.884 0.987 0.885 Table 5: The effect of prevalence on per class agreement, quantification task. Pr(a) is the observed agreement between annotators, Pr(e) the expected agreement. With regard to the purpose of creating a gold standard for a quantification resolution system, we also note that out of 300 quantification annotations, there are only 14 cases in which a majority decision cannot be found, i.e., at least two annotators agreed in 95% of cases. Thus, despite some low Kappa results, the data can adequately be used for the production of training material.4 4As far as such data ever can be: Reidsma and Carletta, 2008, show that systematic disagreements between annotators will produce bad machine learning, regardless of the Kappa obtained on the data. 79 In Section 8, we introduce difficulties encountered by our subjects, as related in post-annotation discussions. We focus on quantification. 8 Annotation issues 8.1 Reference Although we tried to make the task as simple as possible for the annotators by asking them to paraphrase the sentences that they were reading, they were not free from having to work out the referent of the NP (consciously or unconsciously) and we have evidence that they did not always </context>
</contexts>
<marker>Reidsma, Carletta, 2008</marker>
<rawString>Dennis Reidsma and Jean Carletta. 2008. ‘Reliability measurement without limits’. Computational Linguistics, 34(3), pages 319–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Ritchie</author>
</authors>
<date>2004</date>
<booktitle>Compatible RMRS Representations from RASP and the ERG’. http: //www.cl.cam.ac.uk/TechReports/</booktitle>
<pages>661</pages>
<contexts>
<context position="16221" citStr="Ritchie, 2004" startWordPosition="2658" endWordPosition="2659">meaning that speaker heterogeneity is high. We would also expect an encyclopaedia to contain relatively many 1http://www.wikipedia.org generics, allowing us to assess how our quantificational reading fares in a real annotation task. Finally, the use of an open resource means that the corpus can be freely distributed.2 In order to create our annotation corpus, we first isolated the first 100,000 pages in our snapshot and parsed them into a Robust Minimal Recursion Semantics (RMRS) representation (Copestake, 2004) using first the RASP parser (Briscoe et al, 2006) and the RASP to RMRS converter (Ritchie, 2004). We then extracted all constructions of the type Subject-Verb-Object from the obtained corpus and randomly selected 300 of those ‘triples’ to be annotated. Another 50 random triples were selected for the purpose of annotation training (see Section 7.1). We show in Figure 1 an example of an annotation instance produced by the parser pipeline. The data provided by the system consists of the triple itself, followed by the argument structure of that triple, including the direct dependents of its constituents, the number and tense information for each constituent, the file from which the triple wa</context>
</contexts>
<marker>Ritchie, 2004</marker>
<rawString>Anna Ritchie. 2004. ‘Compatible RMRS Representations from RASP and the ERG’. http: //www.cl.cam.ac.uk/TechReports/ UCAM-CL-TR-661.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Advaith Siddharthan</author>
<author>Dan Tidhar</author>
</authors>
<title>An annotation scheme for citation function’. In:</title>
<date>2006</date>
<booktitle>Proceedings of Sigdial-06,</booktitle>
<pages>80--87</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="9144" citStr="Teufel et al, 2006" startWordPosition="1498" endWordPosition="1501">the studied phenomena have a (somewhat) clear, agreed upon definition (Kingsbury et al, 2002). We must clarify that in those cases, the choice of a formalism may already imply a certain theoretical position – leading to potential incompatibilities between formalisms. However, the categories for such annotation are themselves fixed: there is a generally agreed broad understanding of concepts such as noun phrases and coordination. Another type of annotation concerns tasks where the linguistic categories at play are not fixed. One example is discourse annotation according to rhetorical function (Teufel et al, 2006) where humans are asked to differentiate between several discursive categories such as ‘contrast’ or ‘weakness’. In such a task, the computational linguist develops a theory where different states or values are associated with various phenomena. In order to show that the world functions according to the model presented, experimentation is required. This usually takes the form of an annotation task where several human subjects are required to mark pieces of text following guidelines inferred from the model. The intuition behind the annotation effort is that agreement between humans support the </context>
</contexts>
<marker>Teufel, Siddharthan, Tidhar, 2006</marker>
<rawString>Simone Teufel, Advaith Siddharthan, Dan Tidhar. 2006. ‘An annotation scheme for citation function’. In: Proceedings of Sigdial-06, Sydney, Australia, pages 80–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
</authors>
<title>The Structure of Scientific Articles: Applications to Summarisation and Citation Indexing.</title>
<date>2010</date>
<publisher>CSLI Publications. In press.</publisher>
<marker>Teufel, 2010</marker>
<rawString>Simone Teufel. 2010. The Structure of Scientific Articles: Applications to Summarisation and Citation Indexing. CSLI Publications. In press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>