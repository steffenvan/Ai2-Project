<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.035568">
<title confidence="0.9416335">
JOINT_FORCES:
Unite Competing Sentiment Classifiers with Random Forest
</title>
<author confidence="0.998566">
Oliver Dürr, Fatih Uzdilli, and Mark Cieliebak
</author>
<affiliation confidence="0.87753">
Zurich University of Applied Sciences
Winterthur, Switzerland
</affiliation>
<email confidence="0.995841">
{dueo, uzdi, ciel}@zhaw.ch
</email>
<sectionHeader confidence="0.995645" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998522583333333">
In this paper, we describe how we cre-
ated a meta-classifier to detect the mes-
sage-level sentiment of tweets. We par-
ticipated in SemEval-2014 Task 9B by
combining the results of several exist-
ing classifiers using a random forest.
The results of 5 other teams from the
competition as well as from 7 general-
purpose commercial classifiers were
used to train the algorithm. This way,
we were able to get a boost of up to
3.24 F1 score points.
</bodyText>
<sectionHeader confidence="0.99897" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996685">
The interest in sentiment analysis grows as pub-
licly available text content grows. As one of the
most used social media platforms, Twitter pro-
vides its users a unique way of expressing them-
selves. Thus, sentiment analysis of tweets has
become a hot research topic among academia
and industry.
In this paper, we describe our approach of com-
bining multiple sentiment classifiers into a meta-
classifier. The introduced system participated in
SemEval-2014 Task 9: “Sentiment Analysis in
Twitter, Subtask–B Message Polarity Classifica-
tion” (Rosenthal et al., 2014). The goal was to
classify a tweet on the message level using the
three classes positive, negative, and neutral. The
performance is measured using the macro-
averaged F1 score of the positive and negative
classes which is simply named “F1 score”
</bodyText>
<footnote confidence="0.9606335">
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.98415925">
throughout the paper. An almost identical task
was already run in 2013 (Nakov et al., 2013).
The tweets for training and development were
only provided as tweet ids. A fraction (10-15%)
of the tweets was no longer available on twitter,
which makes the results of the competition not
fully comparable. For testing, in addition to last
year’s data (tweets and SMS) new tweets and
data from a surprise domain (LiveJournal) were
provided. An overview of the provided data is
shown in Table 1.
Using additional manually labelled data for
training the algorithm was not allowed for a
“constrained” submission. Submissions using
additional data for training were marked as “un-
constrained”.
</bodyText>
<table confidence="0.999723875">
Dataset Total Pos Neg Neu
Training (Tweets) 8224 3058 1210 3956
Dev (Tweets) 1417 494 286 637
Test: Twitter2013 3813 1572 601 1640
Test: SMS2013 2093 492 394 1207
Test: Twitter2014 1853 982 202 669
Test: Twitter’14Sarcasm 86 33 40 13
Test: LiveJournal2014 1142 427 304 411
</table>
<tableCaption confidence="0.952874">
Table 1: Number of Documents we were able to
download for Training, Development and Test-
ing.
</tableCaption>
<bodyText confidence="0.9998676">
Our System. The results of 5 other teams from
the competition as well as from 7 general-
purpose commercial classifiers were used to train
our algorithm. Scientific subsystems were s_gez
(Gezici et al., 2013), s_jag (Jaggi et al., 2014),
s_mar (Marchand et al., 2013), s_fil (Filho and
Pardo, 2013), s_gun (Günther and Furrer, 2013).
They are all “constrained” and machine learning-
based, some with hybrid rule-based approaches.
Commercial subsystems were provided by
</bodyText>
<page confidence="0.996718">
366
</page>
<note confidence="0.671858">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 366–369,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.999706714285714">
Lymbix (c_lym), MLAnalyzer1 (c_mla), Seman-
tria (c_sem), Sentigem (c_snt), Syttle (c_sky),
Text-Processing.com (c_txp), and Webknox
(c_web). Subsystems c_txp and c_web are ma-
chine learning-based, c_sky is rule-based, and
m_mla is a mix (other tools unknown). All sub-
systems were designed to handle tweets and fur-
ther text types.
Our submission included a subset of all classi-
fiers including unconstrained ones, leading to an
unconstrained submission. The 2014 winning
team obtained an F1 score of 70.96 on the Twit-
ter2014 test set. Our approach was ranked on the
12th place out of the 50 participating submis-
sions, with an F1 score of 66.79. Our further
rankings were 12th on the LiveJournal data, 12th
on the SMS data, 12th on Twitter-2013, and 26th
on Twitter Sarcasm.
Improvement. Although our meta-classifier
did not reach a top position in the competition,
we were able to beat even the best single subsys-
tem it was based on for almost all test sets (ex-
cept sarcasm). In previous research we showed
that same behaviour on different systems and
data sets (Cieliebak et al., 2014). This shows that
also other systems from the competition, even
best ones, probably can be improved using our
approach.
</bodyText>
<sectionHeader confidence="0.970781" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.999798965517241">
Meta-Classifier. A meta-classifier is an ap-
proach to predict a classification given the indi-
vidual results of other classifiers by combining
them. A robust classifier, which can naturally
handle categorical input such as sentiments by
design, is the random forest classifier (Breiman,
2001). The algorithm uses the outputs of individ-
ual classifiers as features and the labels on the
training data as input for training. Afterwards, in
the test phase, the random forest makes predic-
tions using the outputs of the same individual
classifiers. We use the random forest implemen-
tation of the R-package &amp;quot;randomForest&amp;quot; and treat
the three votes (negative, neutral, positive) as
categorical input.
Training Data. To build a meta-classifier, first,
one has to train all the subsystems with a dataset.
Second, the meta-classifier has to be trained
based on the output of the subsystems with a dif-
ferent dataset than the one used for training the
subsystems. We decided to take the natural split
of the data provided by the organizers (see Table
1). For the scientific subsystems we used the
Training set to train on; for training the random
forest classifier we used the Dev set. The com-
mercial systems were used &amp;quot;as-is&amp;quot;, in particular,
we did not train them on any of the provided data
sets. Table 2 shows the performance of the indi-
vidual subsystems on the different data sets.
</bodyText>
<table confidence="0.99814075">
s_gez 32.22 31.23 30.77 28.57 51.57 50.83
s_jag 61.47 56.17 60.21 62.73 44.26 63.91
s_mar 28.95 22.94 26.68 22.86 31.01 24.47
s_fil 52.88 49.94 55.61 55.08 38.22 56.41
s_gun 63.93 61.51 65.33 65.09 48.80 68.91
c_lym 48.38 44.40 48.68 54.17 34.87 58.71
c_mla 49.79 46.41 50.17 47.74 43.16 59.02
c_sma 55.89 52.26 56.15 53.51 49.33 56.53
c_sky 56.30 52.04 54.67 56.28 40.60 54.61
c_txp 43.69 46.47 41.15 44.00 59.74 56.57
c_web 47.44 41.64 45.21 48.83 45.25 53.45
c_snt 56.86 58.42 62.17 58.35 36.08 65.74
</table>
<tableCaption confidence="0.966412">
Table 2: F1 scores of the individual systems.
Bold shows the best commercial or scientific
system per data set; grey cells indicates the over-
all maximum.
</tableCaption>
<sectionHeader confidence="0.998566" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.997916411764706">
There exist three obvious selections of subsys-
tems for our meta-classifier: all subsystems, only
scientific subsystems, and only commercial sub-
systems (called All_Subsystems, All_Scientific,
and All_Commercial, respectively). Table 3
shows performance of these selections of subsys-
tems on the data sets. For comparison, the table
shows also the performance of the overall best
individual subsystem in the first row. It turns out
that All_Subsystems is almost always better than
the best individual subsystem, while the other
two meta-classifiers are inferior.
Testing All Subsets. We performed a systematic
evaluation on how the performance depends on
the choice of a particular selection of individual
subsystems. This resembles feature selection,
which is a common task in machine learning, and
</bodyText>
<figure confidence="0.959101375">
Dev
SMS2013
Sarcasm2014
LiveJournal2014
Twitter2013
Twitter2014
ID
1 mashape.com/mlanalyzer/ml-analyzer
</figure>
<page confidence="0.988738">
367
</page>
<table confidence="0.999739125">
Dev SMS2013 Twitter2013 Twitter2014 Twitter2014 LiveJournal2014
(OOB) Sarcasm
Best Individual 63.93 61.51 65.33 65.09 48.80 68.91
All_Subsystems 63.54 64.22 67.03 67.70 46.37 71.11
All_Scientific 64.52 60.42 64.54 64.99 43.35 67.86
All_Commercial 62.11 58.34 60.70 63.86 44.85 65.57
Max_OOB_Subset 68.27 63.02 67.49 68.33 45.40 71.43
Our Submission 65.00 62.20 66.61 66.79 45.40 70.02
</table>
<tableCaption confidence="0.64568875">
Table 3: Performance (in F1 score) of meta-classifiers with different subsystems. The subset used in
our submission is composed of s_gez, s_jag, s_mar, s_fil, s_gun, c_sma, c_sky, c_snt.
“Max_OOB_Subset” is composed of s_jag, s_mar, s_gun, c_lym, c_sma, c_sky, c_txp. Bold shows
best result per data set. The first row shows results of the best individual subsystem.
</tableCaption>
<bodyText confidence="0.9996613">
As a general trend we see that the performance
increases with the number of classifiers; howev-
er, there exist certain subsets which perform bet-
ter than using all available classifiers.
Best Subset Selection. In Figure 1, we marked
for each number of subsystems the highest OOB-
F1-Score on the Dev set by a diamond. In addi-
tion, the subset with the overall highest OOB-F1-
Score, consisting of 7 classifiers, is displayed as
a filled diamond.
</bodyText>
<figureCaption confidence="0.828424">
Figure 1: Box Plot showing the F1 scores (out-of-
bag) for all subsets on the Dev set. Diamonds
mark the best combination of classifiers for the
corresponding number.
</figureCaption>
<bodyText confidence="0.9991694">
We also evaluated the performance of these
“best” subsets on other unseen test data. In Fig-
ure 2, we show the results of the test set Twit-
ter2014. The scores for the very subsets marked
in Figure 1 are displayed in the same way here.
</bodyText>
<figureCaption confidence="0.634388">
Figure 2: F1 scores of all subsets on the Twit-
ter2014 test set.
</figureCaption>
<bodyText confidence="0.99989325">
For comparison, we marked the performance
of the system with all classifiers by a straight
line. We find that all subsets that are “best” on
the Dev set perform very well on the Twit-
ter2014 set. In fact, some even beat the system
with all classifiers. Similar behaviour can be ob-
served for Twitter2013 and LiveJournal2014 (da-
ta not shown), while All_Subsets yields signifi-
cantly superior results on SMS2013 (see Figure
3). No conclusive observation is possible for
Sarcasm2014 (data not shown).
To elucidate on the question whether to use a
subset with the highest OOB-F1 on the Dev set
(called Max_OOB_Subset) or to use all available
classifiers, we show in Table 3 the performance
of these systems on all test sets in rows 2 and 5,
respectively. Since All_Systems is in 2 out of 5
cases the best classifier, and
“Max_OOB_Subset” in 3 out of 5 cases, a deci-
sive answer cannot be drawn. However, we find
</bodyText>
<page confidence="0.997148">
368
</page>
<bodyText confidence="0.970669">
that All_Systems generalizes better to foreign
types of data, while Max_OOB_Subset performs
well on similar data (in this case, tweets).
</bodyText>
<figureCaption confidence="0.917947">
Figure 3: F1 score of all subsets on the SMS2013
test set.
</figureCaption>
<sectionHeader confidence="0.998124" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999993466666667">
We have shown that a meta-classifier approach
using random forest can beat the performance of
the individual sentiment classifiers it is based on.
Typically, the more subsystems are used, the bet-
ter the performance. However, there exist selec-
tions of only few subsystems that perform com-
parable to using all subsystems. In fact, a good
selection strategy is to select the subset which
has maximum out-of-bag F1 score on the training
data. This subset performs slightly better than
All_Systems on similar data sets, and only slight-
ly worse on new types of data. Advantage of this
subset is that it requires less classifiers (7 instead
of 12 in our case), which reduces the cost
(runtime or license fees) of the meta-classifier.
</bodyText>
<sectionHeader confidence="0.998888" genericHeader="acknowledgments">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998558">
We would like to thank all system providers for
giving us the opportunity to use their systems for
this evaluation, and especially Tobias Günther
and Martin Jaggi for carefully reading the manu-
script.
</bodyText>
<sectionHeader confidence="0.989891" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999828679245283">
Leo Breiman. 2001. Random Forests. Machine Learn-
ing 45(1), 5-32.
Mark Cieliebak, Oliver Dürr, Fatih Uzdilli. 2014. Me-
ta-Classifiers Easily Improve Commercial Senti-
ment Detection Tools. In Proceedings of the 9th
edition of the Language Resources and Evaluation
Conference (LREC), pages 3100-3104, May 26-31,
2014, Reykjavik, Iceland.
Pedro P. Balage Filho, Thiago A. S. Pardo. 2013.
NILC USP: A Hybrid System for Sentiment Anal-
ysis in Twitter Messages. In Proceedings of the In-
ternational Workshop on Semantic Evaluation
(SemEval-2013), pages 568-572, June 14-15, 2013,
Atlanta, Georgia, USA.
Gizem Gezici, Rahim Dehkharghani, Berrin Yani-
koglu, Dilek Tapucu, Yucel Saygin. 2013. SU-
Sentilab: A Classification System for Sentiment
Analysis in Twitter. In Proceedings of the Interna-
tional Workshop on Semantic Evaluation
(SemEval-2013), pages 471-477, June 14-15, 2013,
Atlanta, Georgia, USA.
Tobias Günther, Lenz Furrer. 2013. GU-MLT-LT:
Sentiment Analysis of Short Messages using Lin-
guistic Features and Stochastic Gradient Descent.
In Proceedings of the International Workshop on
Semantic Evaluation (SemEval-2013), pages 328-
332, June 14-15, 2013, Atlanta, Georgia, USA.
Martin Jaggi, Fatih Uzdilli, and Mark Cieliebak.
2014. Swiss-Chocolate: Sentiment Detection using
Sparse SVMs and Part-Of-Speech n-Grams. In
Proceedings of the International Workshop on Se-
mantic Evaluation (SemEval-2014), August 23-24,
2014, Dublin, Ireland.
Morgane Marchand, Alexandru Ginsca, Romaric Be-
sançon, Olivier Mesnard. 2013. [LVIC-LIMSI]:
Using Syntactic Features and Multi-polarity Words
for Sentiment Analysis in Twitter. In Proceedings
of the International Workshop on Semantic Evalua-
tion (SemEval-2013), pages 418-424, June 14-15,
2013, Atlanta, Georgia, USA.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment Analysis in Twitter. In Proceedings of
the International Workshop on Semantic Evalua-
tion (SemEval-2014), August 23-24, 2014, Dublin,
Ireland.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, Theresa Wilson.
2013. SemEval-2013 Task 2: Sentiment Analysis
in Twitter. In Proceedings of the International
Workshop on Semantic Evaluation (SemEval-
2013), pages 312-320, June 14-15, 2013, Atlanta,
Georgia, USA.
</reference>
<page confidence="0.999161">
369
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.382614">
<title confidence="0.488802">JOINT_FORCES: Unite Competing Sentiment Classifiers with Random Forest</title>
<author confidence="0.523711">Oliver Dürr</author>
<author confidence="0.523711">Fatih Uzdilli</author>
<author confidence="0.523711">Mark</author>
<affiliation confidence="0.998808">Zurich University of Applied</affiliation>
<address confidence="0.981826">Winterthur, Switzerland</address>
<email confidence="0.991431">dueo@zhaw.ch</email>
<email confidence="0.991431">uzdi@zhaw.ch</email>
<email confidence="0.991431">ciel@zhaw.ch</email>
<abstract confidence="0.995114615384615">In this paper, we describe how we created a meta-classifier to detect the message-level sentiment of tweets. We participated in SemEval-2014 Task 9B by combining the results of several existing classifiers using a random forest. The results of 5 other teams from the competition as well as from 7 generalpurpose commercial classifiers were used to train the algorithm. This way, we were able to get a boost of up to points.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Random Forests.</title>
<date>2001</date>
<journal>Machine Learning</journal>
<volume>45</volume>
<issue>1</issue>
<pages>5--32</pages>
<contexts>
<context position="4841" citStr="Breiman, 2001" startWordPosition="764" endWordPosition="765">e best single subsystem it was based on for almost all test sets (except sarcasm). In previous research we showed that same behaviour on different systems and data sets (Cieliebak et al., 2014). This shows that also other systems from the competition, even best ones, probably can be improved using our approach. 2 Approach Meta-Classifier. A meta-classifier is an approach to predict a classification given the individual results of other classifiers by combining them. A robust classifier, which can naturally handle categorical input such as sentiments by design, is the random forest classifier (Breiman, 2001). The algorithm uses the outputs of individual classifiers as features and the labels on the training data as input for training. Afterwards, in the test phase, the random forest makes predictions using the outputs of the same individual classifiers. We use the random forest implementation of the R-package &amp;quot;randomForest&amp;quot; and treat the three votes (negative, neutral, positive) as categorical input. Training Data. To build a meta-classifier, first, one has to train all the subsystems with a dataset. Second, the meta-classifier has to be trained based on the output of the subsystems with a differ</context>
</contexts>
<marker>Breiman, 2001</marker>
<rawString>Leo Breiman. 2001. Random Forests. Machine Learning 45(1), 5-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Cieliebak</author>
<author>Oliver Dürr</author>
<author>Fatih Uzdilli</author>
</authors>
<title>Meta-Classifiers Easily Improve Commercial Sentiment Detection Tools.</title>
<date>2014</date>
<booktitle>In Proceedings of the 9th edition of the Language Resources and Evaluation Conference (LREC),</booktitle>
<pages>3100--3104</pages>
<location>Reykjavik, Iceland.</location>
<contexts>
<context position="4420" citStr="Cieliebak et al., 2014" startWordPosition="698" endWordPosition="701">winning team obtained an F1 score of 70.96 on the Twitter2014 test set. Our approach was ranked on the 12th place out of the 50 participating submissions, with an F1 score of 66.79. Our further rankings were 12th on the LiveJournal data, 12th on the SMS data, 12th on Twitter-2013, and 26th on Twitter Sarcasm. Improvement. Although our meta-classifier did not reach a top position in the competition, we were able to beat even the best single subsystem it was based on for almost all test sets (except sarcasm). In previous research we showed that same behaviour on different systems and data sets (Cieliebak et al., 2014). This shows that also other systems from the competition, even best ones, probably can be improved using our approach. 2 Approach Meta-Classifier. A meta-classifier is an approach to predict a classification given the individual results of other classifiers by combining them. A robust classifier, which can naturally handle categorical input such as sentiments by design, is the random forest classifier (Breiman, 2001). The algorithm uses the outputs of individual classifiers as features and the labels on the training data as input for training. Afterwards, in the test phase, the random forest </context>
</contexts>
<marker>Cieliebak, Dürr, Uzdilli, 2014</marker>
<rawString>Mark Cieliebak, Oliver Dürr, Fatih Uzdilli. 2014. Meta-Classifiers Easily Improve Commercial Sentiment Detection Tools. In Proceedings of the 9th edition of the Language Resources and Evaluation Conference (LREC), pages 3100-3104, May 26-31, 2014, Reykjavik, Iceland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro P Balage Filho</author>
<author>Thiago A S Pardo</author>
</authors>
<title>NILC USP: A Hybrid System for Sentiment Analysis in Twitter Messages.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation (SemEval-2013),</booktitle>
<pages>568--572</pages>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="3028" citStr="Filho and Pardo, 2013" startWordPosition="478" endWordPosition="481">ts) 8224 3058 1210 3956 Dev (Tweets) 1417 494 286 637 Test: Twitter2013 3813 1572 601 1640 Test: SMS2013 2093 492 394 1207 Test: Twitter2014 1853 982 202 669 Test: Twitter’14Sarcasm 86 33 40 13 Test: LiveJournal2014 1142 427 304 411 Table 1: Number of Documents we were able to download for Training, Development and Testing. Our System. The results of 5 other teams from the competition as well as from 7 generalpurpose commercial classifiers were used to train our algorithm. Scientific subsystems were s_gez (Gezici et al., 2013), s_jag (Jaggi et al., 2014), s_mar (Marchand et al., 2013), s_fil (Filho and Pardo, 2013), s_gun (Günther and Furrer, 2013). They are all “constrained” and machine learningbased, some with hybrid rule-based approaches. Commercial subsystems were provided by 366 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 366–369, Dublin, Ireland, August 23-24, 2014. Lymbix (c_lym), MLAnalyzer1 (c_mla), Semantria (c_sem), Sentigem (c_snt), Syttle (c_sky), Text-Processing.com (c_txp), and Webknox (c_web). Subsystems c_txp and c_web are machine learning-based, c_sky is rule-based, and m_mla is a mix (other tools unknown). All subsystems were designed to </context>
</contexts>
<marker>Filho, Pardo, 2013</marker>
<rawString>Pedro P. Balage Filho, Thiago A. S. Pardo. 2013. NILC USP: A Hybrid System for Sentiment Analysis in Twitter Messages. In Proceedings of the International Workshop on Semantic Evaluation (SemEval-2013), pages 568-572, June 14-15, 2013, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gizem Gezici</author>
<author>Rahim Dehkharghani</author>
</authors>
<title>Berrin Yanikoglu, Dilek Tapucu, Yucel Saygin.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation (SemEval-2013),</booktitle>
<pages>471--477</pages>
<location>Atlanta, Georgia, USA.</location>
<marker>Gezici, Dehkharghani, 2013</marker>
<rawString>Gizem Gezici, Rahim Dehkharghani, Berrin Yanikoglu, Dilek Tapucu, Yucel Saygin. 2013. SUSentilab: A Classification System for Sentiment Analysis in Twitter. In Proceedings of the International Workshop on Semantic Evaluation (SemEval-2013), pages 471-477, June 14-15, 2013, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tobias Günther</author>
<author>Lenz Furrer</author>
</authors>
<title>GU-MLT-LT: Sentiment Analysis of Short Messages using Linguistic Features and Stochastic Gradient Descent.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation (SemEval-2013),</booktitle>
<pages>328--332</pages>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="3062" citStr="Günther and Furrer, 2013" startWordPosition="483" endWordPosition="486">eets) 1417 494 286 637 Test: Twitter2013 3813 1572 601 1640 Test: SMS2013 2093 492 394 1207 Test: Twitter2014 1853 982 202 669 Test: Twitter’14Sarcasm 86 33 40 13 Test: LiveJournal2014 1142 427 304 411 Table 1: Number of Documents we were able to download for Training, Development and Testing. Our System. The results of 5 other teams from the competition as well as from 7 generalpurpose commercial classifiers were used to train our algorithm. Scientific subsystems were s_gez (Gezici et al., 2013), s_jag (Jaggi et al., 2014), s_mar (Marchand et al., 2013), s_fil (Filho and Pardo, 2013), s_gun (Günther and Furrer, 2013). They are all “constrained” and machine learningbased, some with hybrid rule-based approaches. Commercial subsystems were provided by 366 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 366–369, Dublin, Ireland, August 23-24, 2014. Lymbix (c_lym), MLAnalyzer1 (c_mla), Semantria (c_sem), Sentigem (c_snt), Syttle (c_sky), Text-Processing.com (c_txp), and Webknox (c_web). Subsystems c_txp and c_web are machine learning-based, c_sky is rule-based, and m_mla is a mix (other tools unknown). All subsystems were designed to handle tweets and further text typ</context>
</contexts>
<marker>Günther, Furrer, 2013</marker>
<rawString>Tobias Günther, Lenz Furrer. 2013. GU-MLT-LT: Sentiment Analysis of Short Messages using Linguistic Features and Stochastic Gradient Descent. In Proceedings of the International Workshop on Semantic Evaluation (SemEval-2013), pages 328-332, June 14-15, 2013, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Jaggi</author>
<author>Fatih Uzdilli</author>
<author>Mark Cieliebak</author>
</authors>
<title>Swiss-Chocolate: Sentiment Detection using Sparse SVMs and Part-Of-Speech n-Grams.</title>
<date>2014</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation (SemEval-2014),</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="2966" citStr="Jaggi et al., 2014" startWordPosition="468" endWordPosition="471">s “unconstrained”. Dataset Total Pos Neg Neu Training (Tweets) 8224 3058 1210 3956 Dev (Tweets) 1417 494 286 637 Test: Twitter2013 3813 1572 601 1640 Test: SMS2013 2093 492 394 1207 Test: Twitter2014 1853 982 202 669 Test: Twitter’14Sarcasm 86 33 40 13 Test: LiveJournal2014 1142 427 304 411 Table 1: Number of Documents we were able to download for Training, Development and Testing. Our System. The results of 5 other teams from the competition as well as from 7 generalpurpose commercial classifiers were used to train our algorithm. Scientific subsystems were s_gez (Gezici et al., 2013), s_jag (Jaggi et al., 2014), s_mar (Marchand et al., 2013), s_fil (Filho and Pardo, 2013), s_gun (Günther and Furrer, 2013). They are all “constrained” and machine learningbased, some with hybrid rule-based approaches. Commercial subsystems were provided by 366 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 366–369, Dublin, Ireland, August 23-24, 2014. Lymbix (c_lym), MLAnalyzer1 (c_mla), Semantria (c_sem), Sentigem (c_snt), Syttle (c_sky), Text-Processing.com (c_txp), and Webknox (c_web). Subsystems c_txp and c_web are machine learning-based, c_sky is rule-based, and m_mla is</context>
</contexts>
<marker>Jaggi, Uzdilli, Cieliebak, 2014</marker>
<rawString>Martin Jaggi, Fatih Uzdilli, and Mark Cieliebak. 2014. Swiss-Chocolate: Sentiment Detection using Sparse SVMs and Part-Of-Speech n-Grams. In Proceedings of the International Workshop on Semantic Evaluation (SemEval-2014), August 23-24, 2014, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Morgane Marchand</author>
</authors>
<title>Alexandru Ginsca, Romaric Besançon, Olivier Mesnard.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation (SemEval-2013),</booktitle>
<pages>418--424</pages>
<location>Atlanta, Georgia, USA.</location>
<marker>Marchand, 2013</marker>
<rawString>Morgane Marchand, Alexandru Ginsca, Romaric Besançon, Olivier Mesnard. 2013. [LVIC-LIMSI]: Using Syntactic Features and Multi-polarity Words for Sentiment Analysis in Twitter. In Proceedings of the International Workshop on Semantic Evaluation (SemEval-2013), pages 418-424, June 14-15, 2013, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Preslav Nakov</author>
<author>Alan Ritter</author>
<author>Veselin Stoyanov</author>
</authors>
<title>SemEval-2014 Task 9: Sentiment Analysis in Twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation (SemEval-2014),</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="1227" citStr="Rosenthal et al., 2014" startWordPosition="190" endWordPosition="193">ble to get a boost of up to 3.24 F1 score points. 1 Introduction The interest in sentiment analysis grows as publicly available text content grows. As one of the most used social media platforms, Twitter provides its users a unique way of expressing themselves. Thus, sentiment analysis of tweets has become a hot research topic among academia and industry. In this paper, we describe our approach of combining multiple sentiment classifiers into a metaclassifier. The introduced system participated in SemEval-2014 Task 9: “Sentiment Analysis in Twitter, Subtask–B Message Polarity Classification” (Rosenthal et al., 2014). The goal was to classify a tweet on the message level using the three classes positive, negative, and neutral. The performance is measured using the macroaveraged F1 score of the positive and negative classes which is simply named “F1 score” This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ throughout the paper. An almost identical task was already run in 2013 (Nakov et al., 2013). The tweets for training and development were only p</context>
</contexts>
<marker>Rosenthal, Nakov, Ritter, Stoyanov, 2014</marker>
<rawString>Sara Rosenthal, Preslav Nakov, Alan Ritter, and Veselin Stoyanov. 2014. SemEval-2014 Task 9: Sentiment Analysis in Twitter. In Proceedings of the International Workshop on Semantic Evaluation (SemEval-2014), August 23-24, 2014, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Zornitsa Kozareva</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
<author>Theresa Wilson</author>
</authors>
<title>SemEval-2013 Task 2: Sentiment Analysis in Twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation (SemEval2013),</booktitle>
<pages>312--320</pages>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="1774" citStr="Nakov et al., 2013" startWordPosition="272" endWordPosition="275">ter, Subtask–B Message Polarity Classification” (Rosenthal et al., 2014). The goal was to classify a tweet on the message level using the three classes positive, negative, and neutral. The performance is measured using the macroaveraged F1 score of the positive and negative classes which is simply named “F1 score” This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ throughout the paper. An almost identical task was already run in 2013 (Nakov et al., 2013). The tweets for training and development were only provided as tweet ids. A fraction (10-15%) of the tweets was no longer available on twitter, which makes the results of the competition not fully comparable. For testing, in addition to last year’s data (tweets and SMS) new tweets and data from a surprise domain (LiveJournal) were provided. An overview of the provided data is shown in Table 1. Using additional manually labelled data for training the algorithm was not allowed for a “constrained” submission. Submissions using additional data for training were marked as “unconstrained”. Dataset </context>
</contexts>
<marker>Nakov, Rosenthal, Kozareva, Stoyanov, Ritter, Wilson, 2013</marker>
<rawString>Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, Theresa Wilson. 2013. SemEval-2013 Task 2: Sentiment Analysis in Twitter. In Proceedings of the International Workshop on Semantic Evaluation (SemEval2013), pages 312-320, June 14-15, 2013, Atlanta, Georgia, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>