<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017090">
<title confidence="0.997514">
Task Alternation in Parallel Sentence Retrieval for Twitter Translation
</title>
<author confidence="0.998535">
Felix Hieber and Laura Jehl and Stefan Riezler
</author>
<affiliation confidence="0.991796">
Department of Computational Linguistics
Heidelberg University
</affiliation>
<address confidence="0.793647">
69120 Heidelberg, Germany
</address>
<email confidence="0.999138">
{jehl,hieber,riezler}@cl.uni-heidelberg.de
</email>
<sectionHeader confidence="0.997392" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999982461538461">
We present an approach to mine com-
parable data for parallel sentences us-
ing translation-based cross-lingual infor-
mation retrieval (CLIR). By iteratively al-
ternating between the tasks of retrieval
and translation, an initial general-domain
model is allowed to adapt to in-domain
data. Adaptation is done by training the
translation system on a few thousand sen-
tences retrieved in the step before. Our
setup is time- and memory-efficient and of
similar quality as CLIR-based adaptation
on millions of parallel sentences.
</bodyText>
<sectionHeader confidence="0.999357" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999544222222223">
Statistical Machine Translation (SMT) crucially
relies on large amounts of bilingual data (Brown et
al., 1993). Unfortunately sentence-parallel bilin-
gual data are not always available. Various ap-
proaches have been presented to remedy this prob-
lem by mining parallel sentences from comparable
data, for example by using cross-lingual informa-
tion retrieval (CLIR) techniques to retrieve a target
language sentence for a source language sentence
treated as a query. Most such approaches try to
overcome the noise inherent in automatically ex-
tracted parallel data by sheer size. However, find-
ing good quality parallel data from noisy resources
like Twitter requires sophisticated retrieval meth-
ods. Running these methods on millions of queries
and documents can take weeks.
Our method aims to achieve improvements sim-
ilar to large-scale parallel sentence extraction ap-
proaches, while requiring only a fraction of the ex-
tracted data and considerably less computing re-
sources. Our key idea is to extend a straightfor-
ward application of translation-based CLIR to an
iterative method: Instead of attempting to retrieve
in one step as many parallel sentences as possible,
we allow the retrieval model to gradually adapt to
new data by using an SMT model trained on the
freshly retrieved sentence pairs in the translation-
based retrieval step. We alternate between the
tasks of translation-based retrieval of target sen-
tences, and the task of SMT, by re-training the
SMT model on the data that were retrieved in the
previous step. This task alternation is done itera-
tively until the number of newly added pairs stabi-
lizes at a relatively small value.
In our experiments on Arabic-English Twitter
translation, we achieved improvements of over 1
BLEU point over a strong baseline that uses in-
domain data for language modeling and parameter
tuning. Compared to a CLIR-approach which ex-
tracts more than 3 million parallel sentences from
a noisy comparable corpus, our system produces
similar results in terms of BLEU using only about
40 thousand sentences for training in each of a
few iterations, thus being much more time- and
resource-efficient.
</bodyText>
<sectionHeader confidence="0.999893" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998538555555555">
In the terminology of semi-supervised learning
(Abney, 2008), our method resembles self-training
and co-training by training a learning method on
its own predictions. It is different in the aspect of
task alternation: The SMT model trained on re-
trieved sentence pairs is not used for generating
training data, but for scoring noisy parallel data
in a translation-based retrieval setup. Our method
also incorporates aspects of transductive learning
in that candidate sentences used as queries are fil-
tered for out-of-vocabulary (OOV) words and sim-
ilarity to sentences in the development set in or-
der to maximize the impact of translation-based
retrieval.
Our work most closely resembles approaches
that make use of variants of SMT to mine com-
parable corpora for parallel sentences. Recent
work uses word-based translation (Munteanu and
</bodyText>
<page confidence="0.990904">
323
</page>
<bodyText confidence="0.912453">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 323–327,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
Marcu, 2005; Munteanu and Marcu, 2006), full-
sentence translation (Abdul-Rauf and Schwenk,
2009; Uszkoreit et al., 2010), or a sophisticated
interpolation of word-based and contextual trans-
lation of full sentences (Snover et al., 2008; Jehl
et al., 2012; Ture and Lin, 2012) to project source
language sentences into the target language for re-
trieval. The novel aspect of task alternation in-
troduced in this paper can be applied to all ap-
proaches incorporating SMT for sentence retrieval
from comparable data.
For our baseline system we use in-domain lan-
guage models (Bertoldi and Federico, 2009) and
meta-parameter tuning on in-domain development
sets (Koehn and Schroeder, 2007).
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="method">
3 CLIR for Parallel Sentence Retrieval
</sectionHeader>
<subsectionHeader confidence="0.996228">
3.1 Context-Sensitive Translation for CLIR
</subsectionHeader>
<bodyText confidence="0.999323421052632">
Our CLIR model extends the translation-based re-
trieval model of Xu et al. (2001). While transla-
tion options in this approach are given by a lexical
translation table, we also select translation options
estimated from the decoder’s n-best list for trans-
lating a particular query. The central idea is to let
the language model choose fluent, context-aware
translations for each query term during decoding.
For mapping source language query terms to
target language query terms, we follow Ture et
al. (2012a; 2012). Given a source language query
Q with query terms qj, we project it into the tar-
get language by representing each source token qj
by its probabilistically weighted translations. The
score of target document D, given source language
query Q, is computed by calculating the Okapi
BM25 rank (Robertson et al., 1998) over projected
term frequency and document frequency weights
as follows:
</bodyText>
<equation confidence="0.995407">
bm25(tf(qj, D), df(qj))
tf(ti, D)P(ti|q)
df(ti)P(ti|q)
</equation>
<bodyText confidence="0.999853923076923">
where TQ = {t|P(t|q) &gt; L} is the set of trans-
lation options for query term q with probability
greater than L. Following Ture et al. (2012a;
2012) we impose a cumulative threshold C, so that
only the most probable options are added until C
is reached.
Like Ture et al. (2012a; 2012) we achieved best
retrieval performance when translation probabil-
ities are calculated as an interpolation between
(context-free) lexical translation probabilities PLex
estimated on symmetrized word alignments, and
(context-aware) translation probabilities Pnbest es-
timated on the n-best list of an SMT decoder:
</bodyText>
<equation confidence="0.999733">
P(t|q) = APnbest(t|q) + (1 − A)Plex(t|q) (1)
</equation>
<bodyText confidence="0.999053166666667">
Pnbest(t|q) is the decoder’s confidence to trans-
late q into t within the context of query Q. Let
ak(t, q) be a function indicating an alignment of
target term t to source term q in the k-th deriva-
tion of query Q. Then we can estimate Pnbest(t|q)
as follows:
</bodyText>
<equation confidence="0.77376125">
Pnbest(t |q) = Ek_1 ak(t, q)D(k, Q) (2)
Ek=1 ak(·, q)D(k, Q)
D(k, Q) is the model score of the k-th derivation
in the n-best list for query Q.
</equation>
<bodyText confidence="0.999827875">
In our work, we use hierarchical phrase-based
translation (Chiang, 2007), as implemented in the
cdec framework (Dyer et al., 2010). This allows
us to extract word alignments between source and
target text for Q from the SCFG rules used in the
derivation. The concept of self-translation is cov-
ered by the decoder’s ability to use pass-through
rules if words or phrases cannot be translated.
</bodyText>
<subsectionHeader confidence="0.999688">
3.2 Task Alternation in CLIR
</subsectionHeader>
<bodyText confidence="0.999984454545454">
The key idea of our approach is to iteratively al-
ternate between the tasks of retrieval and trans-
lation for efficient mining of parallel sentences.
We allow the initial general-domain CLIR model
to adapt to in-domain data over multiple itera-
tions. Since our set of in-domain queries was
small (see 4.2), we trained an adapted SMT model
on the concatenation of general-domain sentences
and in-domain sentences retrieved in the step be-
fore, rather than working with separate models.
Algorithm 1 shows the iterative task alternation
procedure. In terms of semi-supervised learning,
we can view algorithm 1 as non-persistent as we
do not keep labels/pairs from previous iterations.
We have tried different variations of label persis-
tency but did not find any improvements. A sim-
ilar effect of preventing the SMT model to “for-
get” general-domain knowledge across iterations
is achieved by mixing models from current and
previous iterations. This is accomplished in two
ways: First, by linearly interpolating the transla-
tion option weights P(t|q) from the current and
</bodyText>
<equation confidence="0.9984163">
|Q|
E
j=1
score(D|Q) =
� |T9|
i=1
tf(q, D) =
� |T9|
i=1
df(q) =
</equation>
<page confidence="0.992531">
324
</page>
<table confidence="0.95303475">
Algorithm 1 Task Alternation
Require: source language Tweets Qsrc, target language Tweets Dtrg, general-domain parallel sentences Sgen, general-domain
SMT model Mgen, interpolation parameter 0
procedure TASK-ALTERNATION(Qsrc, Dtrg, Sgen, Mgen, 0)
</table>
<equation confidence="0.9857415">
t + -1
while true do
Sin +- 0 &gt; Start with empty parallel in-domain sentences
if t == 1 then
M(t)
clir +-Mgen &gt; Start with general-domain SMT model for CLIR
else
M(t)
clir +- 0M(t−1)
smt + (1 — 0)M(t) &gt; Use mixture of previous and current SMT model for CLIR
smt
end if
Sin +- CLIR(Qsrc, Dtrg, M(t)
clir) &gt; Retrieve top 1 target language Tweets for each source language query
M( t1) +- TRAIN(Sgen + Sin) &gt; Train SMT model on general-domain and retrieved in-domain data
t +- t + 1
</equation>
<table confidence="0.9384195">
end while
end procedure
BLEU (test) # of in-domain sents
Standard DA 14.05 -
Full-scale CLIR 14.97 3,198,913
Task alternation 15.31 —40k
</table>
<tableCaption confidence="0.9807305">
Table 1: Standard Domain Adaptation with in-domain LM
and tuning; Full-scale CLIR yielding over 3M in-domain par-
allel sentences; Task alternation (0 = 0.1, iteration 7) using
—40k parallel sentences per iteration.
</tableCaption>
<bodyText confidence="0.9998835">
previous model with interpolation parameter θ.
Second, by always using Plex(t|q) weights esti-
mated from word alignments on Sgen.
We experimented with different ways of using
the ranked retrieval results for each query and
found that taking just the highest ranked docu-
ment yielded the best results. This returns one pair
of parallel Twitter messages per query, which are
then used as additional training data for the SMT
model in each iteration.
</bodyText>
<sectionHeader confidence="0.999716" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.966508">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999945892857143">
We trained the general domain model Mgen on
data from the NIST evaluation campaign, includ-
ing UN reports, newswire, broadcast news and
blogs. Since we were interested in relative im-
provements rather than absolute performance, we
sampled 1 million parallel sentences Sgen from the
originally over 5.8 million parallel sentences.
We used a large corpus of Twitter messages,
originally created by Jehl et al. (2012), as com-
parable in-domain data. Language identification
was carried out with an off-the-shelf tool (Lui and
Baldwin, 2012). We kept only Tweets classified
as Arabic or English with over 95% confidence.
After removing duplicates, we obtained 5.5 mil-
lion Arabic Tweets and 3.7 million English Tweets
(Dtrg). Jehl et al. (2012) also supply a set of 1,022
Arabic Tweets with 3 English translations each for
evaluation purposes, which was created by crowd-
sourcing translation on Amazon Mechanical Turk.
We randomly split the parallel sentences into 511
sentences for development and 511 sentences for
testing. All URLs and user names in Tweets were
replaced by common placeholders. Hashtags were
kept, since they might be helpful in the retrieval
step. Since the evaluation data do not contain any
hashtags, URLs or user names, we apply a post-
processing step after decoding in which we re-
move those tokens.
</bodyText>
<subsectionHeader confidence="0.977921">
4.2 Transductive Setup
</subsectionHeader>
<bodyText confidence="0.99995305">
Our method can be considered transductive in two
ways. First, all Twitter data were collected by
keyword-based crawling. Therefore, we can ex-
pect a topical similarity between development, test
and training data. Second, since our setup aims
for speed, we created a small set of queries Qsrc,
consisting of the source side of the evaluation data
and similar Tweets. Similarity was defined by
two criteria: First, we ranked all Arabic Tweets
with respect to their term overlap with the devel-
opment and test Tweets. Smoothed per-sentence
BLEU (Lin and Och, 2004) was used as a similar-
ity metric. OOV-coverage served as a second cri-
terion to remedy the problem of unknown words
in Twitter translation. We first created a general
list of all OOVs in the evaluation data under Mgen
(3,069 out of 7,641 types). For each of the top 100
BLEU-ranked Tweets, we counted OOV-coverage
with respect to the corresponding source Tweet
and the general OOV list. We only kept Tweets
</bodyText>
<page confidence="0.998834">
325
</page>
<figureCaption confidence="0.999579">
Figure 1: Learning curves for varying θ parameters. (a) BLEU scores and (b) number of new pairs added per iteration.
</figureCaption>
<figure confidence="0.998174892857143">
0 =0.0
0 =0.1
0 =0.5
0 =0.9
0 =0.0
0 =0.1
0 =0.5
0 =0.9
70000
60000
50000
# new pairs
40000
30000
20000
15.31
BLEU (test)
14.97
(a)
16.00
(b)
0 1 2 3 4 5 6 7 8
iteration
1 2 3 4 5 6 7
iteration
80
10000
14.05
</figure>
<figureCaption confidence="0.850948333333334">
containing at least one OOV term from the corre-
sponding source Tweet and two OOV terms from
the general list, resulting in 65,643 Arabic queries
covering 86% of all OOVs. Our query set Qarc
performed better (14.76 BLEU) after one iteration
than a similar-sized set of random queries (13.39).
</figureCaption>
<subsectionHeader confidence="0.999305">
4.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999475814814815">
We simulated the full-scale retrieval approach by
Jehl et al. (2012) with the CLIR model described
in section 3. It took 14 days to run 5.5M Arabic
queries on 3.7M English documents. In contrast,
our iterative approach completed a single iteration
in less than 24 hours.1
In the absence of a Twitter data set for re-
trieval, we selected the parameters A = 0.6 (eq.1),
L = 0.005 and C = 0.95 in a mate-finding
task on Wikipedia data. The n-best list size for
Pnbeat(t q) was 1000. All SMT models included
a 5-gram language model built from the English
side of the NIST data plus the English side of the
Twitter corpus Dtrg. Word alignments were cre-
ated using GIZA++ (Och and Ney, 2003). Rule
extraction and parameter tuning (MERT) was car-
ried out with cdec, using standard features. We
ran MERT 5 times per iteration, carrying over the
weights which achieved median performance on
the development set to the next iteration.
Table 1 reports median BLEU scores on test of
our standard adaptation baseline, the full-scale re-
trieval approach and the best result from our task
alternation systems. Approximate randomization
tests (Noreen, 1989; Riezler and Maxwell, 2005)
showed that improvements of full-scale retrieval
and task alternation over the baseline were statis-
</bodyText>
<footnote confidence="0.709994">
1Retrieval was done in 4 batches on a Hadoop cluster us-
ing 190 mappers at once.
</footnote>
<bodyText confidence="0.987611555555556">
tically significant. Differences between full-scale
retrieval and task alternation were not significant.2
Figure 1 illustrates the impact of 0, which con-
trols the importance of the previous model com-
pared to the current one, on median BLEU (a) and
change of Sin (b) over iterations. For all 0, few
iterations suffice to reach or surpass full-scale re-
trieval performance. Yet, no run achieved good
performance after one iteration, showing that the
transductive setup must be combined with task al-
ternation to be effective. While we see fluctuations
in BLEU for all 0-values, 0 = 0.1 achieves high
scores faster and more consistently, pointing to-
wards selecting a bolder updating strategy. This
is also supported by plot (b), which indicates that
choosing 0 = 0.1 leads to faster stabilization in
the pairs added per iteration (Sin). We used this
stabilization as a stopping criterion.
</bodyText>
<sectionHeader confidence="0.996462" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9869415">
We presented a method that makes translation-
based CLIR feasible for mining parallel sentences
from large amounts of comparable data. The key
of our approach is a translation-based high-quality
retrieval model which gradually adapts to the tar-
get domain by iteratively re-training the underly-
ing SMT model on a few thousand parallel sen-
tences retrieved in the step before. The number
of new pairs added per iteration stabilizes to a
few thousand after 7 iterations, yielding an SMT
model that improves 0.35 BLEU points over a
model trained on millions of retrieved pairs.
2Note that our full-scale results are not directly compara-
ble to those of Jehl et al. (2012) since our setup uses less than
one fifth of the NIST data, a different decoder, a new CLIR
approach, and a different development and test split.
</bodyText>
<page confidence="0.998215">
326
</page>
<sectionHeader confidence="0.996072" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999558457943926">
Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In Proceedings of the 12th Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL’09), Athens, Greece.
Steven Abney. 2008. Semisupervised Learning for
Computational Linguistics. Chapman and Hall.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation
with monolingual resources. In Proceedings of the
4th EACL Workshop on Statistical Machine Transla-
tion (WMT’09), Athens, Greece.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2).
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2).
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL 2010 System Demonstra-
tions (ACL’10), Uppsala, Sweden.
Laura Jehl, Felix Hieber, and Stefan Riezler. 2012.
Twitter translation using translation-based cross-
lingual retrieval. In Proceedings of the Sev-
enth Workshop on Statistical Machine Translation
(WMT’12), Montreal, Quebec, Canada.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine trans-
lation. In Proceedings of the Second Workshop on
Statistical Machine Translation, Prague, Czech Re-
public.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics
for machine translation. In Proceedings the 20th In-
ternational Conference on Computational Linguis-
tics (COLING’04).
Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Pro-
ceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics, Demo Session
(ACL’12), Jeju, Republic of Korea.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguis-
tics, 31(4).
Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Com-
putational Linguistics (COLING-ACL’06), Sydney,
Australia.
Eric W. Noreen. 1989. Computer Intensive Meth-
ods for Testing Hypotheses. An Introduction. Wiley,
New York.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1).
Stefan Riezler and John Maxwell. 2005. On some pit-
falls in automatic evaluation and significance testing
for MT. In Proceedings of the ACL-05 Workshop on
Intrinsic and Extrinsic Evaluation Measures for MT
and/or Summarization, Ann Arbor, MI.
Stephen E. Robertson, Steve Walker, and Micheline
Hancock-Beaulieu. 1998. Okapi at TREC-7. In
Proceedings of the Seventh Text REtrieval Confer-
ence (TREC-7), Gaithersburg, MD.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation
using comparable corpora. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP’08), Honolulu, Hawaii.
Ferhan Ture and Jimmy Lin. 2012. Why not grab a
free lunch? mining large corpora for parallel sen-
tences to improve translation modeling. In Proceed-
ings of the Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT’12),
Montreal, Canada.
Ferhan Ture, Jimmy Lin, and Douglas W. Oard.
2012. Combining statistical translation techniques
for cross-language information retrieval. In Pro-
ceedings of the International Conference on Compu-
tational Linguistics (COLING’12), Mumbai, India.
Ferhan Ture, Jimmy Lin, and Douglas W. Oard. 2012a.
Looking inside the box: Context-sensitive transla-
tion for cross-language information retrieval. In
Proceedings of the ACM SIGIR Conference on Re-
search and Development in Information Retrieval
(SIGIR’12), Portland, OR.
Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and
Moshe Dubiner. 2010. Large scale parallel doc-
ument mining for machine translation. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics (COLING’10), Beijing,
China.
Jinxi Xu, Ralph Weischedel, and Chanh Nguyen. 2001.
Evaluating a probabilistic model for cross-lingual
information retrieval. In Proceedings of the 24th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval
(SIGIR’01), New York, NY.
</reference>
<page confidence="0.998404">
327
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.935049">
<title confidence="0.999266">Task Alternation in Parallel Sentence Retrieval for Twitter Translation</title>
<author confidence="0.99937">Hieber Jehl</author>
<affiliation confidence="0.99995">Department of Computational</affiliation>
<address confidence="0.970422">Heidelberg 69120 Heidelberg,</address>
<abstract confidence="0.999653285714286">We present an approach to mine comparable data for parallel sentences using translation-based cross-lingual information retrieval (CLIR). By iteratively alternating between the tasks of retrieval and translation, an initial general-domain model is allowed to adapt to in-domain data. Adaptation is done by training the translation system on a few thousand sentences retrieved in the step before. Our setup is timeand memory-efficient and of similar quality as CLIR-based adaptation on millions of parallel sentences.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sadaf Abdul-Rauf</author>
<author>Holger Schwenk</author>
</authors>
<title>On the use of comparable corpora to improve SMT performance.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL’09),</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="4062" citStr="Abdul-Rauf and Schwenk, 2009" startWordPosition="611" endWordPosition="614">are filtered for out-of-vocabulary (OOV) words and similarity to sentences in the development set in order to maximize the impact of translation-based retrieval. Our work most closely resembles approaches that make use of variants of SMT to mine comparable corpora for parallel sentences. Recent work uses word-based translation (Munteanu and 323 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 323–327, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Marcu, 2005; Munteanu and Marcu, 2006), fullsentence translation (Abdul-Rauf and Schwenk, 2009; Uszkoreit et al., 2010), or a sophisticated interpolation of word-based and contextual translation of full sentences (Snover et al., 2008; Jehl et al., 2012; Ture and Lin, 2012) to project source language sentences into the target language for retrieval. The novel aspect of task alternation introduced in this paper can be applied to all approaches incorporating SMT for sentence retrieval from comparable data. For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007). 3 CLIR for Pa</context>
</contexts>
<marker>Abdul-Rauf, Schwenk, 2009</marker>
<rawString>Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the use of comparable corpora to improve SMT performance. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL’09), Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Semisupervised Learning for Computational Linguistics.</title>
<date>2008</date>
<publisher>Chapman and Hall.</publisher>
<contexts>
<context position="3002" citStr="Abney, 2008" startWordPosition="455" endWordPosition="456">rs stabilizes at a relatively small value. In our experiments on Arabic-English Twitter translation, we achieved improvements of over 1 BLEU point over a strong baseline that uses indomain data for language modeling and parameter tuning. Compared to a CLIR-approach which extracts more than 3 million parallel sentences from a noisy comparable corpus, our system produces similar results in terms of BLEU using only about 40 thousand sentences for training in each of a few iterations, thus being much more time- and resource-efficient. 2 Related Work In the terminology of semi-supervised learning (Abney, 2008), our method resembles self-training and co-training by training a learning method on its own predictions. It is different in the aspect of task alternation: The SMT model trained on retrieved sentence pairs is not used for generating training data, but for scoring noisy parallel data in a translation-based retrieval setup. Our method also incorporates aspects of transductive learning in that candidate sentences used as queries are filtered for out-of-vocabulary (OOV) words and similarity to sentences in the development set in order to maximize the impact of translation-based retrieval. Our wo</context>
</contexts>
<marker>Abney, 2008</marker>
<rawString>Steven Abney. 2008. Semisupervised Learning for Computational Linguistics. Chapman and Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Bertoldi</author>
<author>Marcello Federico</author>
</authors>
<title>Domain adaptation for statistical machine translation with monolingual resources.</title>
<date>2009</date>
<booktitle>In Proceedings of the 4th EACL Workshop on Statistical Machine Translation (WMT’09),</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="4563" citStr="Bertoldi and Federico, 2009" startWordPosition="692" endWordPosition="695">on for Computational Linguistics Marcu, 2005; Munteanu and Marcu, 2006), fullsentence translation (Abdul-Rauf and Schwenk, 2009; Uszkoreit et al., 2010), or a sophisticated interpolation of word-based and contextual translation of full sentences (Snover et al., 2008; Jehl et al., 2012; Ture and Lin, 2012) to project source language sentences into the target language for retrieval. The novel aspect of task alternation introduced in this paper can be applied to all approaches incorporating SMT for sentence retrieval from comparable data. For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007). 3 CLIR for Parallel Sentence Retrieval 3.1 Context-Sensitive Translation for CLIR Our CLIR model extends the translation-based retrieval model of Xu et al. (2001). While translation options in this approach are given by a lexical translation table, we also select translation options estimated from the decoder’s n-best list for translating a particular query. The central idea is to let the language model choose fluent, context-aware translations for each query term during decoding. For mapping source language </context>
</contexts>
<marker>Bertoldi, Federico, 2009</marker>
<rawString>Nicola Bertoldi and Marcello Federico. 2009. Domain adaptation for statistical machine translation with monolingual resources. In Proceedings of the 4th EACL Workshop on Statistical Machine Translation (WMT’09), Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="903" citStr="Brown et al., 1993" startWordPosition="121" endWordPosition="124"> mine comparable data for parallel sentences using translation-based cross-lingual information retrieval (CLIR). By iteratively alternating between the tasks of retrieval and translation, an initial general-domain model is allowed to adapt to in-domain data. Adaptation is done by training the translation system on a few thousand sentences retrieved in the step before. Our setup is time- and memory-efficient and of similar quality as CLIR-based adaptation on millions of parallel sentences. 1 Introduction Statistical Machine Translation (SMT) crucially relies on large amounts of bilingual data (Brown et al., 1993). Unfortunately sentence-parallel bilingual data are not always available. Various approaches have been presented to remedy this problem by mining parallel sentences from comparable data, for example by using cross-lingual information retrieval (CLIR) techniques to retrieve a target language sentence for a source language sentence treated as a query. Most such approaches try to overcome the noise inherent in automatically extracted parallel data by sheer size. However, finding good quality parallel data from noisy resources like Twitter requires sophisticated retrieval methods. Running these m</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="6794" citStr="Chiang, 2007" startWordPosition="1055" endWordPosition="1056">ontext-aware) translation probabilities Pnbest estimated on the n-best list of an SMT decoder: P(t|q) = APnbest(t|q) + (1 − A)Plex(t|q) (1) Pnbest(t|q) is the decoder’s confidence to translate q into t within the context of query Q. Let ak(t, q) be a function indicating an alignment of target term t to source term q in the k-th derivation of query Q. Then we can estimate Pnbest(t|q) as follows: Pnbest(t |q) = Ek_1 ak(t, q)D(k, Q) (2) Ek=1 ak(·, q)D(k, Q) D(k, Q) is the model score of the k-th derivation in the n-best list for query Q. In our work, we use hierarchical phrase-based translation (Chiang, 2007), as implemented in the cdec framework (Dyer et al., 2010). This allows us to extract word alignments between source and target text for Q from the SCFG rules used in the derivation. The concept of self-translation is covered by the decoder’s ability to use pass-through rules if words or phrases cannot be translated. 3.2 Task Alternation in CLIR The key idea of our approach is to iteratively alternate between the tasks of retrieval and translation for efficient mining of parallel sentences. We allow the initial general-domain CLIR model to adapt to in-domain data over multiple iterations. Sinc</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Jonathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations (ACL’10),</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="6852" citStr="Dyer et al., 2010" startWordPosition="1063" endWordPosition="1066">ted on the n-best list of an SMT decoder: P(t|q) = APnbest(t|q) + (1 − A)Plex(t|q) (1) Pnbest(t|q) is the decoder’s confidence to translate q into t within the context of query Q. Let ak(t, q) be a function indicating an alignment of target term t to source term q in the k-th derivation of query Q. Then we can estimate Pnbest(t|q) as follows: Pnbest(t |q) = Ek_1 ak(t, q)D(k, Q) (2) Ek=1 ak(·, q)D(k, Q) D(k, Q) is the model score of the k-th derivation in the n-best list for query Q. In our work, we use hierarchical phrase-based translation (Chiang, 2007), as implemented in the cdec framework (Dyer et al., 2010). This allows us to extract word alignments between source and target text for Q from the SCFG rules used in the derivation. The concept of self-translation is covered by the decoder’s ability to use pass-through rules if words or phrases cannot be translated. 3.2 Task Alternation in CLIR The key idea of our approach is to iteratively alternate between the tasks of retrieval and translation for efficient mining of parallel sentences. We allow the initial general-domain CLIR model to adapt to in-domain data over multiple iterations. Since our set of in-domain queries was small (see 4.2), we tra</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In Proceedings of the ACL 2010 System Demonstrations (ACL’10), Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Jehl</author>
<author>Felix Hieber</author>
<author>Stefan Riezler</author>
</authors>
<title>Twitter translation using translation-based crosslingual retrieval.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation (WMT’12),</booktitle>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="4220" citStr="Jehl et al., 2012" startWordPosition="636" endWordPosition="639">k most closely resembles approaches that make use of variants of SMT to mine comparable corpora for parallel sentences. Recent work uses word-based translation (Munteanu and 323 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 323–327, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Marcu, 2005; Munteanu and Marcu, 2006), fullsentence translation (Abdul-Rauf and Schwenk, 2009; Uszkoreit et al., 2010), or a sophisticated interpolation of word-based and contextual translation of full sentences (Snover et al., 2008; Jehl et al., 2012; Ture and Lin, 2012) to project source language sentences into the target language for retrieval. The novel aspect of task alternation introduced in this paper can be applied to all approaches incorporating SMT for sentence retrieval from comparable data. For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007). 3 CLIR for Parallel Sentence Retrieval 3.1 Context-Sensitive Translation for CLIR Our CLIR model extends the translation-based retrieval model of Xu et al. (2001). While t</context>
<context position="10230" citStr="Jehl et al. (2012)" startWordPosition="1617" endWordPosition="1620"> document yielded the best results. This returns one pair of parallel Twitter messages per query, which are then used as additional training data for the SMT model in each iteration. 4 Experiments 4.1 Data We trained the general domain model Mgen on data from the NIST evaluation campaign, including UN reports, newswire, broadcast news and blogs. Since we were interested in relative improvements rather than absolute performance, we sampled 1 million parallel sentences Sgen from the originally over 5.8 million parallel sentences. We used a large corpus of Twitter messages, originally created by Jehl et al. (2012), as comparable in-domain data. Language identification was carried out with an off-the-shelf tool (Lui and Baldwin, 2012). We kept only Tweets classified as Arabic or English with over 95% confidence. After removing duplicates, we obtained 5.5 million Arabic Tweets and 3.7 million English Tweets (Dtrg). Jehl et al. (2012) also supply a set of 1,022 Arabic Tweets with 3 English translations each for evaluation purposes, which was created by crowdsourcing translation on Amazon Mechanical Turk. We randomly split the parallel sentences into 511 sentences for development and 511 sentences for test</context>
<context position="12835" citStr="Jehl et al. (2012)" startWordPosition="2065" endWordPosition="2068">rs added per iteration. 0 =0.0 0 =0.1 0 =0.5 0 =0.9 0 =0.0 0 =0.1 0 =0.5 0 =0.9 70000 60000 50000 # new pairs 40000 30000 20000 15.31 BLEU (test) 14.97 (a) 16.00 (b) 0 1 2 3 4 5 6 7 8 iteration 1 2 3 4 5 6 7 iteration 80 10000 14.05 containing at least one OOV term from the corresponding source Tweet and two OOV terms from the general list, resulting in 65,643 Arabic queries covering 86% of all OOVs. Our query set Qarc performed better (14.76 BLEU) after one iteration than a similar-sized set of random queries (13.39). 4.3 Experimental Results We simulated the full-scale retrieval approach by Jehl et al. (2012) with the CLIR model described in section 3. It took 14 days to run 5.5M Arabic queries on 3.7M English documents. In contrast, our iterative approach completed a single iteration in less than 24 hours.1 In the absence of a Twitter data set for retrieval, we selected the parameters A = 0.6 (eq.1), L = 0.005 and C = 0.95 in a mate-finding task on Wikipedia data. The n-best list size for Pnbeat(t q) was 1000. All SMT models included a 5-gram language model built from the English side of the NIST data plus the English side of the Twitter corpus Dtrg. Word alignments were created using GIZA++ (Och</context>
</contexts>
<marker>Jehl, Hieber, Riezler, 2012</marker>
<rawString>Laura Jehl, Felix Hieber, and Stefan Riezler. 2012. Twitter translation using translation-based crosslingual retrieval. In Proceedings of the Seventh Workshop on Statistical Machine Translation (WMT’12), Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Josh Schroeder</author>
</authors>
<title>Experiments in domain adaptation for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="4647" citStr="Koehn and Schroeder, 2007" startWordPosition="703" endWordPosition="706">translation (Abdul-Rauf and Schwenk, 2009; Uszkoreit et al., 2010), or a sophisticated interpolation of word-based and contextual translation of full sentences (Snover et al., 2008; Jehl et al., 2012; Ture and Lin, 2012) to project source language sentences into the target language for retrieval. The novel aspect of task alternation introduced in this paper can be applied to all approaches incorporating SMT for sentence retrieval from comparable data. For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007). 3 CLIR for Parallel Sentence Retrieval 3.1 Context-Sensitive Translation for CLIR Our CLIR model extends the translation-based retrieval model of Xu et al. (2001). While translation options in this approach are given by a lexical translation table, we also select translation options estimated from the decoder’s n-best list for translating a particular query. The central idea is to let the language model choose fluent, context-aware translations for each query term during decoding. For mapping source language query terms to target language query terms, we follow Ture et al. (2012a; 2012). Giv</context>
</contexts>
<marker>Koehn, Schroeder, 2007</marker>
<rawString>Philipp Koehn and Josh Schroeder. 2007. Experiments in domain adaptation for statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Orange: a method for evaluating automatic evaluation metrics for machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings the 20th International Conference on Computational Linguistics (COLING’04).</booktitle>
<contexts>
<context position="11714" citStr="Lin and Och, 2004" startWordPosition="1857" endWordPosition="1860">ng in which we remove those tokens. 4.2 Transductive Setup Our method can be considered transductive in two ways. First, all Twitter data were collected by keyword-based crawling. Therefore, we can expect a topical similarity between development, test and training data. Second, since our setup aims for speed, we created a small set of queries Qsrc, consisting of the source side of the evaluation data and similar Tweets. Similarity was defined by two criteria: First, we ranked all Arabic Tweets with respect to their term overlap with the development and test Tweets. Smoothed per-sentence BLEU (Lin and Och, 2004) was used as a similarity metric. OOV-coverage served as a second criterion to remedy the problem of unknown words in Twitter translation. We first created a general list of all OOVs in the evaluation data under Mgen (3,069 out of 7,641 types). For each of the top 100 BLEU-ranked Tweets, we counted OOV-coverage with respect to the corresponding source Tweet and the general OOV list. We only kept Tweets 325 Figure 1: Learning curves for varying θ parameters. (a) BLEU scores and (b) number of new pairs added per iteration. 0 =0.0 0 =0.1 0 =0.5 0 =0.9 0 =0.0 0 =0.1 0 =0.5 0 =0.9 70000 60000 50000</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004. Orange: a method for evaluating automatic evaluation metrics for machine translation. In Proceedings the 20th International Conference on Computational Linguistics (COLING’04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Timothy Baldwin</author>
</authors>
<title>langid.py: An off-the-shelf language identification tool.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, Demo Session (ACL’12),</booktitle>
<location>Jeju, Republic of</location>
<contexts>
<context position="10352" citStr="Lui and Baldwin, 2012" startWordPosition="1635" endWordPosition="1638">as additional training data for the SMT model in each iteration. 4 Experiments 4.1 Data We trained the general domain model Mgen on data from the NIST evaluation campaign, including UN reports, newswire, broadcast news and blogs. Since we were interested in relative improvements rather than absolute performance, we sampled 1 million parallel sentences Sgen from the originally over 5.8 million parallel sentences. We used a large corpus of Twitter messages, originally created by Jehl et al. (2012), as comparable in-domain data. Language identification was carried out with an off-the-shelf tool (Lui and Baldwin, 2012). We kept only Tweets classified as Arabic or English with over 95% confidence. After removing duplicates, we obtained 5.5 million Arabic Tweets and 3.7 million English Tweets (Dtrg). Jehl et al. (2012) also supply a set of 1,022 Arabic Tweets with 3 English translations each for evaluation purposes, which was created by crowdsourcing translation on Amazon Mechanical Turk. We randomly split the parallel sentences into 511 sentences for development and 511 sentences for testing. All URLs and user names in Tweets were replaced by common placeholders. Hashtags were kept, since they might be helpf</context>
</contexts>
<marker>Lui, Baldwin, 2012</marker>
<rawString>Marco Lui and Timothy Baldwin. 2012. langid.py: An off-the-shelf language identification tool. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, Demo Session (ACL’12), Jeju, Republic of Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragos Stefan Munteanu</author>
<author>Daniel Marcu</author>
</authors>
<title>Improving machine translation performance by exploiting non-parallel corpora.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>4</issue>
<marker>Munteanu, Marcu, 2005</marker>
<rawString>Dragos Stefan Munteanu and Daniel Marcu. 2005. Improving machine translation performance by exploiting non-parallel corpora. Computational Linguistics, 31(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragos Stefan Munteanu</author>
<author>Daniel Marcu</author>
</authors>
<title>Extracting parallel sub-sentential fragments from nonparallel corpora.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics (COLING-ACL’06),</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="4006" citStr="Munteanu and Marcu, 2006" startWordPosition="604" endWordPosition="607">learning in that candidate sentences used as queries are filtered for out-of-vocabulary (OOV) words and similarity to sentences in the development set in order to maximize the impact of translation-based retrieval. Our work most closely resembles approaches that make use of variants of SMT to mine comparable corpora for parallel sentences. Recent work uses word-based translation (Munteanu and 323 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 323–327, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Marcu, 2005; Munteanu and Marcu, 2006), fullsentence translation (Abdul-Rauf and Schwenk, 2009; Uszkoreit et al., 2010), or a sophisticated interpolation of word-based and contextual translation of full sentences (Snover et al., 2008; Jehl et al., 2012; Ture and Lin, 2012) to project source language sentences into the target language for retrieval. The novel aspect of task alternation introduced in this paper can be applied to all approaches incorporating SMT for sentence retrieval from comparable data. For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain dev</context>
</contexts>
<marker>Munteanu, Marcu, 2006</marker>
<rawString>Dragos Stefan Munteanu and Daniel Marcu. 2006. Extracting parallel sub-sentential fragments from nonparallel corpora. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics (COLING-ACL’06), Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer Intensive Methods for Testing Hypotheses. An Introduction.</title>
<date>1989</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="13903" citStr="Noreen, 1989" startWordPosition="2251" endWordPosition="2252">el built from the English side of the NIST data plus the English side of the Twitter corpus Dtrg. Word alignments were created using GIZA++ (Och and Ney, 2003). Rule extraction and parameter tuning (MERT) was carried out with cdec, using standard features. We ran MERT 5 times per iteration, carrying over the weights which achieved median performance on the development set to the next iteration. Table 1 reports median BLEU scores on test of our standard adaptation baseline, the full-scale retrieval approach and the best result from our task alternation systems. Approximate randomization tests (Noreen, 1989; Riezler and Maxwell, 2005) showed that improvements of full-scale retrieval and task alternation over the baseline were statis1Retrieval was done in 4 batches on a Hadoop cluster using 190 mappers at once. tically significant. Differences between full-scale retrieval and task alternation were not significant.2 Figure 1 illustrates the impact of 0, which controls the importance of the previous model compared to the current one, on median BLEU (a) and change of Sin (b) over iterations. For all 0, few iterations suffice to reach or surpass full-scale retrieval performance. Yet, no run achieved </context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W. Noreen. 1989. Computer Intensive Methods for Testing Hypotheses. An Introduction. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="13450" citStr="Och and Ney, 2003" startWordPosition="2179" endWordPosition="2182">12) with the CLIR model described in section 3. It took 14 days to run 5.5M Arabic queries on 3.7M English documents. In contrast, our iterative approach completed a single iteration in less than 24 hours.1 In the absence of a Twitter data set for retrieval, we selected the parameters A = 0.6 (eq.1), L = 0.005 and C = 0.95 in a mate-finding task on Wikipedia data. The n-best list size for Pnbeat(t q) was 1000. All SMT models included a 5-gram language model built from the English side of the NIST data plus the English side of the Twitter corpus Dtrg. Word alignments were created using GIZA++ (Och and Ney, 2003). Rule extraction and parameter tuning (MERT) was carried out with cdec, using standard features. We ran MERT 5 times per iteration, carrying over the weights which achieved median performance on the development set to the next iteration. Table 1 reports median BLEU scores on test of our standard adaptation baseline, the full-scale retrieval approach and the best result from our task alternation systems. Approximate randomization tests (Noreen, 1989; Riezler and Maxwell, 2005) showed that improvements of full-scale retrieval and task alternation over the baseline were statis1Retrieval was done</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational linguistics, 29(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>John Maxwell</author>
</authors>
<title>On some pitfalls in automatic evaluation and significance testing for MT.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL-05 Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization,</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="13931" citStr="Riezler and Maxwell, 2005" startWordPosition="2253" endWordPosition="2256">the English side of the NIST data plus the English side of the Twitter corpus Dtrg. Word alignments were created using GIZA++ (Och and Ney, 2003). Rule extraction and parameter tuning (MERT) was carried out with cdec, using standard features. We ran MERT 5 times per iteration, carrying over the weights which achieved median performance on the development set to the next iteration. Table 1 reports median BLEU scores on test of our standard adaptation baseline, the full-scale retrieval approach and the best result from our task alternation systems. Approximate randomization tests (Noreen, 1989; Riezler and Maxwell, 2005) showed that improvements of full-scale retrieval and task alternation over the baseline were statis1Retrieval was done in 4 batches on a Hadoop cluster using 190 mappers at once. tically significant. Differences between full-scale retrieval and task alternation were not significant.2 Figure 1 illustrates the impact of 0, which controls the importance of the previous model compared to the current one, on median BLEU (a) and change of Sin (b) over iterations. For all 0, few iterations suffice to reach or surpass full-scale retrieval performance. Yet, no run achieved good performance after one i</context>
</contexts>
<marker>Riezler, Maxwell, 2005</marker>
<rawString>Stefan Riezler and John Maxwell. 2005. On some pitfalls in automatic evaluation and significance testing for MT. In Proceedings of the ACL-05 Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen E Robertson</author>
<author>Steve Walker</author>
<author>Micheline Hancock-Beaulieu</author>
</authors>
<title>Okapi at TREC-7.</title>
<date>1998</date>
<booktitle>In Proceedings of the Seventh Text REtrieval Conference (TREC-7),</booktitle>
<location>Gaithersburg, MD.</location>
<contexts>
<context position="5555" citStr="Robertson et al., 1998" startWordPosition="848" endWordPosition="851"> estimated from the decoder’s n-best list for translating a particular query. The central idea is to let the language model choose fluent, context-aware translations for each query term during decoding. For mapping source language query terms to target language query terms, we follow Ture et al. (2012a; 2012). Given a source language query Q with query terms qj, we project it into the target language by representing each source token qj by its probabilistically weighted translations. The score of target document D, given source language query Q, is computed by calculating the Okapi BM25 rank (Robertson et al., 1998) over projected term frequency and document frequency weights as follows: bm25(tf(qj, D), df(qj)) tf(ti, D)P(ti|q) df(ti)P(ti|q) where TQ = {t|P(t|q) &gt; L} is the set of translation options for query term q with probability greater than L. Following Ture et al. (2012a; 2012) we impose a cumulative threshold C, so that only the most probable options are added until C is reached. Like Ture et al. (2012a; 2012) we achieved best retrieval performance when translation probabilities are calculated as an interpolation between (context-free) lexical translation probabilities PLex estimated on symmetriz</context>
</contexts>
<marker>Robertson, Walker, Hancock-Beaulieu, 1998</marker>
<rawString>Stephen E. Robertson, Steve Walker, and Micheline Hancock-Beaulieu. 1998. Okapi at TREC-7. In Proceedings of the Seventh Text REtrieval Conference (TREC-7), Gaithersburg, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Language and translation model adaptation using comparable corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’08),</booktitle>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="4201" citStr="Snover et al., 2008" startWordPosition="632" endWordPosition="635">ed retrieval. Our work most closely resembles approaches that make use of variants of SMT to mine comparable corpora for parallel sentences. Recent work uses word-based translation (Munteanu and 323 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 323–327, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Marcu, 2005; Munteanu and Marcu, 2006), fullsentence translation (Abdul-Rauf and Schwenk, 2009; Uszkoreit et al., 2010), or a sophisticated interpolation of word-based and contextual translation of full sentences (Snover et al., 2008; Jehl et al., 2012; Ture and Lin, 2012) to project source language sentences into the target language for retrieval. The novel aspect of task alternation introduced in this paper can be applied to all approaches incorporating SMT for sentence retrieval from comparable data. For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007). 3 CLIR for Parallel Sentence Retrieval 3.1 Context-Sensitive Translation for CLIR Our CLIR model extends the translation-based retrieval model of Xu et </context>
</contexts>
<marker>Snover, Dorr, Schwartz, 2008</marker>
<rawString>Matthew Snover, Bonnie Dorr, and Richard Schwartz. 2008. Language and translation model adaptation using comparable corpora. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’08), Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ferhan Ture</author>
<author>Jimmy Lin</author>
</authors>
<title>Why not grab a free lunch? mining large corpora for parallel sentences to improve translation modeling.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT’12),</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="4241" citStr="Ture and Lin, 2012" startWordPosition="640" endWordPosition="643">mbles approaches that make use of variants of SMT to mine comparable corpora for parallel sentences. Recent work uses word-based translation (Munteanu and 323 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 323–327, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Marcu, 2005; Munteanu and Marcu, 2006), fullsentence translation (Abdul-Rauf and Schwenk, 2009; Uszkoreit et al., 2010), or a sophisticated interpolation of word-based and contextual translation of full sentences (Snover et al., 2008; Jehl et al., 2012; Ture and Lin, 2012) to project source language sentences into the target language for retrieval. The novel aspect of task alternation introduced in this paper can be applied to all approaches incorporating SMT for sentence retrieval from comparable data. For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007). 3 CLIR for Parallel Sentence Retrieval 3.1 Context-Sensitive Translation for CLIR Our CLIR model extends the translation-based retrieval model of Xu et al. (2001). While translation options in</context>
</contexts>
<marker>Ture, Lin, 2012</marker>
<rawString>Ferhan Ture and Jimmy Lin. 2012. Why not grab a free lunch? mining large corpora for parallel sentences to improve translation modeling. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT’12), Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ferhan Ture</author>
<author>Jimmy Lin</author>
<author>Douglas W Oard</author>
</authors>
<title>Combining statistical translation techniques for cross-language information retrieval.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING’12),</booktitle>
<location>Mumbai, India.</location>
<contexts>
<context position="5234" citStr="Ture et al. (2012" startWordPosition="795" endWordPosition="798">ts (Koehn and Schroeder, 2007). 3 CLIR for Parallel Sentence Retrieval 3.1 Context-Sensitive Translation for CLIR Our CLIR model extends the translation-based retrieval model of Xu et al. (2001). While translation options in this approach are given by a lexical translation table, we also select translation options estimated from the decoder’s n-best list for translating a particular query. The central idea is to let the language model choose fluent, context-aware translations for each query term during decoding. For mapping source language query terms to target language query terms, we follow Ture et al. (2012a; 2012). Given a source language query Q with query terms qj, we project it into the target language by representing each source token qj by its probabilistically weighted translations. The score of target document D, given source language query Q, is computed by calculating the Okapi BM25 rank (Robertson et al., 1998) over projected term frequency and document frequency weights as follows: bm25(tf(qj, D), df(qj)) tf(ti, D)P(ti|q) df(ti)P(ti|q) where TQ = {t|P(t|q) &gt; L} is the set of translation options for query term q with probability greater than L. Following Ture et al. (2012a; 2012) we i</context>
</contexts>
<marker>Ture, Lin, Oard, 2012</marker>
<rawString>Ferhan Ture, Jimmy Lin, and Douglas W. Oard. 2012. Combining statistical translation techniques for cross-language information retrieval. In Proceedings of the International Conference on Computational Linguistics (COLING’12), Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ferhan Ture</author>
<author>Jimmy Lin</author>
<author>Douglas W Oard</author>
</authors>
<title>Looking inside the box: Context-sensitive translation for cross-language information retrieval.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’12),</booktitle>
<location>Portland, OR.</location>
<contexts>
<context position="5234" citStr="Ture et al. (2012" startWordPosition="795" endWordPosition="798">ts (Koehn and Schroeder, 2007). 3 CLIR for Parallel Sentence Retrieval 3.1 Context-Sensitive Translation for CLIR Our CLIR model extends the translation-based retrieval model of Xu et al. (2001). While translation options in this approach are given by a lexical translation table, we also select translation options estimated from the decoder’s n-best list for translating a particular query. The central idea is to let the language model choose fluent, context-aware translations for each query term during decoding. For mapping source language query terms to target language query terms, we follow Ture et al. (2012a; 2012). Given a source language query Q with query terms qj, we project it into the target language by representing each source token qj by its probabilistically weighted translations. The score of target document D, given source language query Q, is computed by calculating the Okapi BM25 rank (Robertson et al., 1998) over projected term frequency and document frequency weights as follows: bm25(tf(qj, D), df(qj)) tf(ti, D)P(ti|q) df(ti)P(ti|q) where TQ = {t|P(t|q) &gt; L} is the set of translation options for query term q with probability greater than L. Following Ture et al. (2012a; 2012) we i</context>
</contexts>
<marker>Ture, Lin, Oard, 2012</marker>
<rawString>Ferhan Ture, Jimmy Lin, and Douglas W. Oard. 2012a. Looking inside the box: Context-sensitive translation for cross-language information retrieval. In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’12), Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Uszkoreit</author>
<author>Jay M Ponte</author>
<author>Ashok C Popat</author>
<author>Moshe Dubiner</author>
</authors>
<title>Large scale parallel document mining for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING’10),</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="4087" citStr="Uszkoreit et al., 2010" startWordPosition="615" endWordPosition="618">lary (OOV) words and similarity to sentences in the development set in order to maximize the impact of translation-based retrieval. Our work most closely resembles approaches that make use of variants of SMT to mine comparable corpora for parallel sentences. Recent work uses word-based translation (Munteanu and 323 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 323–327, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Marcu, 2005; Munteanu and Marcu, 2006), fullsentence translation (Abdul-Rauf and Schwenk, 2009; Uszkoreit et al., 2010), or a sophisticated interpolation of word-based and contextual translation of full sentences (Snover et al., 2008; Jehl et al., 2012; Ture and Lin, 2012) to project source language sentences into the target language for retrieval. The novel aspect of task alternation introduced in this paper can be applied to all approaches incorporating SMT for sentence retrieval from comparable data. For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007). 3 CLIR for Parallel Sentence Retrieval</context>
</contexts>
<marker>Uszkoreit, Ponte, Popat, Dubiner, 2010</marker>
<rawString>Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and Moshe Dubiner. 2010. Large scale parallel document mining for machine translation. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING’10), Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
<author>Chanh Nguyen</author>
</authors>
<title>Evaluating a probabilistic model for cross-lingual information retrieval.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’01),</booktitle>
<location>New York, NY.</location>
<contexts>
<context position="4811" citStr="Xu et al. (2001)" startWordPosition="728" endWordPosition="731">, 2008; Jehl et al., 2012; Ture and Lin, 2012) to project source language sentences into the target language for retrieval. The novel aspect of task alternation introduced in this paper can be applied to all approaches incorporating SMT for sentence retrieval from comparable data. For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007). 3 CLIR for Parallel Sentence Retrieval 3.1 Context-Sensitive Translation for CLIR Our CLIR model extends the translation-based retrieval model of Xu et al. (2001). While translation options in this approach are given by a lexical translation table, we also select translation options estimated from the decoder’s n-best list for translating a particular query. The central idea is to let the language model choose fluent, context-aware translations for each query term during decoding. For mapping source language query terms to target language query terms, we follow Ture et al. (2012a; 2012). Given a source language query Q with query terms qj, we project it into the target language by representing each source token qj by its probabilistically weighted tran</context>
</contexts>
<marker>Xu, Weischedel, Nguyen, 2001</marker>
<rawString>Jinxi Xu, Ralph Weischedel, and Chanh Nguyen. 2001. Evaluating a probabilistic model for cross-lingual information retrieval. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’01), New York, NY.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>