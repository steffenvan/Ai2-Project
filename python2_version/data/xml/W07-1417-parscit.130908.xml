<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.028872">
<title confidence="0.973246">
The Role of Sentence Structure in Recognizing Textual Entailment
</title>
<author confidence="0.994999">
Catherine Blake
</author>
<affiliation confidence="0.9989115">
School of Information and Library Science
University of North Carolina at Chapel Hill
</affiliation>
<address confidence="0.945705">
Chapel Hill, NC 27599-3360
</address>
<email confidence="0.999695">
cablake@email.unc.edu
</email>
<sectionHeader confidence="0.99565" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998906789473684">
Recent research suggests that sentence
structure can improve the accuracy of
recognizing textual entailments and
paraphrasing. Although background
knowledge such as gazetteers, WordNet
and custom built knowledge bases are
also likely to improve performance, our
goal in this paper is to characterize the
syntactic features alone that aid in
accurate entailment prediction. We
describe candidate features, the role of
machine learning, and two final decision
rules. These rules resulted in an accuracy
of 60.50 and 65.87% and average
precision of 58.97 and 60.96% in
RTE3Test and suggest that sentence
structure alone can improve entailment
accuracy by 9.25 to 14.62% over the
baseline majority class.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999972588235294">
Understanding written language is a non-trivial
task. It takes years for children to read, and
ambiguities of written communication remain long
after we learn the basics. Despite these apparent
complexities, the bag-of-words (BOW) approach,
which ignores structure both within a sentence and
within a document, continues to dominate
information retrieval, and to some extent document
summarization and paraphrasing and entailment
systems.
The rational behind the BOW approach is in part
simplicity (it is much easier and less
computationally expensive to compare terms in
one sentence with terms in another, than to
generate the sentence structure); and in part
accuracy, the BOW approach continues to achieve
similar if not improved performance than
information retrieval systems employing deep
language or logical based representations. This
performance is surprising when you consider that a
BOW approach could not distinguish between the
very different meaning conveyed by: (1)Slow
down so that you don’t hit the
riders on the road and (2)Don’t
slow down so you hit the riders on
the road. A system that employed a syntactic
representation of these sentences however, could
detect that the don’t modifier applies to hit in
first sentence and to slow second.
In contrast to information retrieval, researchers
in paraphrase and entailment detection have
increased their use of sentence structure. Fewer
than half of the submissions in the first
Recognizing Textual Entailment challenge (RTE1)
employed syntax (13/28, 46%) (Dagan, Glickman,
&amp; Magnini, 2005), but more than two-thirds (28/
41, 68%) of the second RTE challenge (RTE2)
submissions employed syntax (Bar-Haim et al.,
2006). Furthermore, for the first time, the RTE2
results showed that systems employing deep
language features, such as syntactic or logical
representations of text, could outperform the
purely semantic overlap approach typified by
BOW. Earlier findings such as (Vanderwende,
Coughlin, &amp; Dolan, 2005) also suggest that
sentence structure plays an important role in
recognizing textual entailment and paraphrasing
accurately.
Our goal in this paper is to explore the degree to
which sentence structure alone influences the
accuracy of entailment and paraphrase detection.
</bodyText>
<page confidence="0.983989">
101
</page>
<bodyText confidence="0.945256666666667">
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 101–106,
Prague, June 2007. c�2007 Association for Computational Linguistics
Other than a lexicon (which is used to identify the
base form of a term), our approach uses no
background knowledge, such as WordNet (Miller,
1995), extensive dictionaries (Litkowski, 2006) or
custom-built knowledge-bases (Hickl et al., 2006)
that have been successfully employed by other
systems. While such semantic knowledge should
improve entailment performance, we deliberately
avoid these sources to isolate the impact of
sentence structure alone.
</bodyText>
<sectionHeader confidence="0.786538" genericHeader="method">
2 System Architecture
</sectionHeader>
<subsectionHeader confidence="0.846582">
2.1 Lexical Processing
</subsectionHeader>
<bodyText confidence="0.999964">
Our approach requires an explicit representation of
structure in both the hypothesis (HSent) and test
(TSent) sentence(s). Systems in RTE challenges
employ a variety of parsers. In RTE2 the most
popular sentence structure was generated by
Minipar (Lin, 1998), perhaps because it is also one
of the fastest parsers. Our system uses the typed
dependency tree generated by the Stanford Parser
(Klein &amp; Manning, 2002). A complete set of parser
tags and the method used to map from a
constituent to a typed dependency grammar can be
found in (de Marneffe et al., 2006). Figure 1 shows
an example typed dependency grammar for pair id
355 in the RTE3Test set.
</bodyText>
<subsectionHeader confidence="0.997877">
2.2 Lexicon
</subsectionHeader>
<bodyText confidence="0.99991737037037">
Our proposed approach requires the base form of
each term. We considered two lexicons for this
purpose: WordNet (Miller, 1995) and the
SPECIALIST lexicon (National Library of
Medicine, 2000). The latter is part of the National
Library of Medicine’s (NLM) Unified Medical
Language System (UMLS) and comprises terms
drawn from medical abstracts, and dictionaries,
both medical and contemporary.
With 412,149 entries, the SPECIALIST lexicon
(version 2006AA) is substantially larger than the
5,947 entries in WordNet (Version 3.0). To
understand the level of overlap between the
lexicons we loaded both into an oracle database.
Our subsequent analysis revealed that of the
WordNet entries, 5008 (84.1%) had a
morphological base form in the SPECIALIST
lexicon. Of the 548 distinct entries that differed
between the two lexicons, 389 differed because
either the UMLS (214 terms) or WordNet (11
terms) did not have a base form. These results
suggest that although the NLM did not develop
their lexicon for news articles, the entries in the
SPECIALIST lexicon subsumes most terms found
in the more frequently used WordNet lexicon.
Thus, our system uses the base form of terms from
the SPECIALIST lexicon.
</bodyText>
<subsectionHeader confidence="0.999736">
2.3 Collapsing Preposition Paths
</subsectionHeader>
<bodyText confidence="0.999918666666667">
Previous work (Lin &amp; Pantel, 2001) suggests the
utility of collapsing paths through prepositions.
The type dependency does have a preposition tag,
prep, however, we found that the parser typically
assigns a more general tag, such as dep (see the
dep tag in Figure 1 between wrapped and by).
Instead of using the prep tag, the system collapses
paths that contain a preposition from the
SPECIALIST lexicon. For example, the system
</bodyText>
<page confidence="0.995199">
102
</page>
<figureCaption confidence="0.999705">
Figure 1. Dependency grammar tree for pair identifier 355 in the RTE3Test
</figureCaption>
<bodyText confidence="0.998452">
collapses four paths in TSentEG millions of
visitors, wrapped in 1995, wrapped by Christo,
and wrapped began before.
noun compounds and all modifiers associated with
the subject and object term.
</bodyText>
<subsectionHeader confidence="0.9901605">
2.5 Derived Sentence Features
2.4 Base Level Sentence Features
</subsectionHeader>
<bodyText confidence="0.999839">
The typed dependency grammar, such as that
shown in Figure 1, can produce many different
features that may indicate entailment. Our current
implementation uses the following four base level
features.
</bodyText>
<listItem confidence="0.988617538461538">
(1) Subject: The system identifies the subject(s)
of a sentence using heuristics and the parser
subject tags nsubjpass and nsubj.
(2) Object: The system uses the parser tag dobj to
identify the object(s) in each sentence.
(3) Verb: The system tags all terms linked with
either the subject or the object as a verb. For
example, wrapped is tagged as the verb wrap
from the link wrapped nsubjpass
Reichstag shown in Figure 1.
(4) Preposition: As described in section 2.3 the
system collapses paths that include a
preposition.
</listItem>
<bodyText confidence="0.999821777777778">
The subject feature had the most coverage of the
base level features and the system identified at
least one subject for 789 of the 800 hypotheses
sentences in RTE3Devmt. We wrote heuristics that
use the parser tags to identify the subject of the
remaining 11 sentences. The system found subjects
for seven of those eight remaining hypothesis
sentences (3 were duplicate sentences). In contrast,
the object feature had the least coverage, with the
system identifying objects for only 480 of the 800
hypotheses in the RTE3 revised development set
(RTE3Devmt).
In addition to the head noun of a subject,
modifying nouns can also be important to
recognize entailment. Consider the underlined
section of TSentEG: which was later bought by
the Russian state-owned oil company
Rosneft. This sentence would lend support to
hypotheses sentences that start with The
Baikalfinasgroup was bought by ... and end
with any of the following phrases an oil
company, a company, Rosneft, the Rosneft
Company, the Rosneft oil company, a Russian
company, a Russian Oil company, a state-
owned company etc. Our system ensures the
detection of these valid entailments by adding
We reviewed previous RTE challenges and a
subset of RTE3Devmt sentences before arriving at
the following derived features that build on the
base level features described in 2.4. The features
that use ‘opposite’ approximate the difference
between passive and active tense. For each
hypothesis sentence, the system records both the
number of matches (#match), and the percentage of
matches (%match) that are supported by the test
sentence(s).
</bodyText>
<listItem confidence="0.999630357142857">
(1) Triple: The system compares the subject-verb-
objects in HSent with the corresponding triple
in TSent.
(2) Triple Opposite: The system matches the
verbs in both HSent and TSent, but matches
the subject in HSent with the object in TSent.
(3) Triple Subject Object: This feature
approximates the triple in (1) by comparing
only the subject and the object in HSent with
TSent, but ignoring the verb.
(4) Triple Subject Object Opposite: The system
compares the objects in HSent with the
subjects in TSent.
(5) Subject Subject: In addition to the triples used
</listItem>
<bodyText confidence="0.938804">
in the derived features 1-4, the system stores
subject-verb and object-verb pairs. This
feature compares the distinct number of
subjects in HSent with those in TSent.
</bodyText>
<listItem confidence="0.996635888888889">
(6) Verb Verb: The system compares only the
verb in the subject-verb, object-verb tuples in
HSent with those in TSent.
(7) Subject Verb: The system compares the
distinct subjects in HSent with the distinct
verbs in TSent.
(8) Verb Subject: The system compares the verb
in HSent with the subject in TSent.
(9) Verb Preposition: The system compares both
the preposition and verb in HSent with those
in TSent.
(10) Subject Preposition: The system compares
both the subject and preposition in HSent with
those in TSent.
(11) Subject Word: The system compares the
distinct subjects in HSent with the distinct
words in TSent. This is the most general of all
11 derived features used in the current system
</listItem>
<page confidence="0.99672">
103
</page>
<subsectionHeader confidence="0.991911">
2.6 Combining Features
</subsectionHeader>
<bodyText confidence="0.999751884615385">
A final decision rule requires a combination of the
derived features in section 2.5. We used both
previous RTE challenges and machine learning
over the derived features to inform the final
decision rules. For the latter, we chose a decision
tree classifier because in addition to classification
accuracy, we are also interested in gaining insight
into the underlying syntactic features that produce
the highest predictive accuracy.
The decision trees shown in Figure 2 were
generated using the Oracle Data Miner 10.2.0.2.
Tree (A) suggests that if there is less than a
63.33% similarity between the number of subjects
in the hypothesis sentence and the words in any of
the test sentences (feature 11), that the hypothesis
sentence is not entailed by the test sentence(s). The
NO prediction from this rule would be correct in
71% cases, and assigning NO would apply to 42%
of sentences in the development set. A YES
prediction would be correct in 69% of sentences,
and a YES prediction would take place in 57% of
sentences in the development set. Tree (B) also
suggests that an increase in the number of matches
between the subject in the hypothesis sentence and
the words used in the test sentence(s) is indicative
of an entailment.
</bodyText>
<figureCaption confidence="0.684445">
Figure 2. Decision trees generated for the revised
RTE3Devmt set during decision rule development.
</figureCaption>
<bodyText confidence="0.999256052631579">
Although tempting to implement the decision
tree with the highest accuracy, we should first
consider the greedy search employed by this
algorithm. At each level of recursion, a decision
tree algorithm selects the single feature that best
improves performance (in this case, the purity of
the resulting leaves, i.e. so that sentences in each
leaf have all YES or all NO responses).
Now consider feature 1, where the subject, verb
and object triple in the hypothesis sentence
matches the corresponding triple in a test sentence.
Even though the predictive accuracy of this feature
is high (74.36%), it is unlikely that this feature will
provide the best purity because only a small
number of sentences (39 in RTE3Devmt) match.
Similarly, a subject-object match has the highest
predictive accuracy of any feature in RTE3Devmt
(78.79%), but again few sentences (66 in
RTE3Devm) match.
</bodyText>
<subsectionHeader confidence="0.961095">
2.7 Final Decision Rules
</subsectionHeader>
<bodyText confidence="0.999346382352941">
We submitted two different decision rules to RTE3
based on thresholds set to optimize performance in
RTE3Devmt set. The thresholds do not consider the
source of a sentence, i.e. from information
extraction, summarization, information retrieval or
question answering activities.
The first decision rule adds the proportion of
matches for each of the derived features described
in section 2.5 and assigns YES when the total
proportion is greater than or equal to a threshold
2.4. Thus, the first decision rule overly favors
sentences where the subject, verb and object match
both HSent and TSent because if a sentence pair
matches on feature 1, then the system also counts a
match for features 3, 4, 5, and 8. This lack of
feature independence is intentional, and consistent
with our intuition that feature 1 is a good indicator
of entailment.
To arrive at the second decision rule, we
considered the features proposed by decision trees
with a non-greedy search strategy that favors high
quality features even when only a small percentage
of sentences match. The second rule predicts YES
under the following conditions: when the subject,
verb, and object of HSent match those in any
TSent (feature 1), in either order (feature 2) or
when the subject and object from the HSent triple
match any TSent (feature 3), or when the TSent
subject matches &gt;= 80% of the HSent subject
terms (feature 5) or when the TSent subject and
preposition matches &gt;=70% of those in HSent
(feature 10) or when TSent word matches &gt;= 70%
of the subject terms in the HSent sentence (feature
11).
</bodyText>
<figure confidence="0.8829218">
Decision Tree (A)
If Subject-Word match &lt;= 63.3%
NO YES
Accuracy=71% Accuracy=69%
Coverage=43% Coverage= 57%
Decision Tree (B)
If Subject-Word match &lt;= 1.5
NO YES
Accuracy=58% Accuracy=64%
Coverage=52% Coverage=48%
</figure>
<page confidence="0.897256">
104
</page>
<table confidence="0.99993">
Feature RTE3pevmt RTE3Test RTE2All
Total Pos %Accy Total Pos %Accy Total Pos %Accy
1 Triple 35 26 74.29 37 24 64.86 47 35 74.47
2 Triple Opposite 4 3 75.00 9 4 44.44 2 1 50.00
3 Triple Subj Obj 66 52 78.79 76 47 61.84 102 69 67.65
4 Triple Subj Obj Opp. 9 4 44.44 16 7 43.75 10 5 50.00
5 Subject-Subject 750 397 52.93 760 404 53.16 777 391 50.32
6 Verb-Verb 330 196 59.39 345 181 52.46 395 208 52.66
7 Subject-Verb 297 178 59.93 291 168 57.73 292 154 52.74
8 Verb-Subject 348 196 56.32 369 207 56.10 398 212 53.27
9 Verb-Preposition 303 178 58.75 312 167 53.53 355 190 53.52
10 Subject-Preposition 522 306 58.62 540 310 57.41 585 303 51.79
11 Subject-Word 771 406 52.66 769 407 52.93 790 395 50.00
</table>
<tableCaption confidence="0.977808">
Table 1. Coverage and accuracy of each derived feature for RTE3 revised development collection
(RTE3Devmt), the RTE3 Test collection (RTE3Test ) and the entire RTE2 collection (RTE2All).
</tableCaption>
<table confidence="0.337489">
Accuracy (%)
</table>
<sectionHeader confidence="0.999353" genericHeader="evaluation">
3 Results
</sectionHeader>
<bodyText confidence="0.999174">
The experiments were completed using the revised
RTE3 development set (RTE3Devmt) before the
RTE3Test results were released. The remaining
RTE2 and RTE3Test analyses were then conducted.
</bodyText>
<subsectionHeader confidence="0.999955">
3.1 Accuracy of Derived Features
</subsectionHeader>
<bodyText confidence="0.9986094">
Table 1 shows the accuracy of any match between
the derived features described in section 2.5.
Complete matching triples (feature 1), and
matching subjects and objects in the triple (feature
2) provide the highest individual accuracy.
</bodyText>
<figure confidence="0.995902615384615">
100
90
80
70
60
50
40
30
20
10
0
10 20 30 40 50 60 70 80 90 100
Match (%)
</figure>
<figureCaption confidence="0.979185">
Figure 3. Correlation between accuracy and the
percentage subjects in HSent that have a
corresponding subject in TSent (feature 5).
</figureCaption>
<bodyText confidence="0.999969285714286">
The results in Table 1 do not consider the degree
of feature match. For example, only one of the
words from TSent in sentence 525’s (RTE3Devmt)
matched the eight subject terms in corresponding
HSent. If the derived features outlined in section
2.7 did capture the underlying structure of an
entailment, you would expect an increased match
would correlate with increased accuracy. We
explored the correlations for each of the derived
features. Figure 3 suggests entailment accuracy
increases with an increase in the percentage of
TSent subject terms that match HSent terms.
(feature 5) and demonstrates why we set the 80%
threshold for feature 5 in the second decision rule.
</bodyText>
<subsectionHeader confidence="0.999924">
3.2 Accuracy of Decision Rules
</subsectionHeader>
<bodyText confidence="0.999989095238096">
Of the 800 sentences in RTE3Devmt, the annotators
labeled 412 as an entailment. Thus, without any
information about HSent or TSent, the system
would assign YES (the majority class) to each
sentence, which would result in 51.50% accuracy.
The first decision rule considers the total
percentage match of all features defined in section
2.5. We arrived at a threshold of 2.4 by ranking the
development set in decreasing order the total
percentage match and identifying where the
threshold would lead to an accuracy of around
65%. Many sentences had a threshold of around
2.4, and the overall accuracy of the first decision
on the RTE3Devmt set was 62.38%, compared to
60.50% in RTE3Test. We consider the first decision
rule a baseline and the second rule is our real
submission.
The second rule uses only a sub-set of the
derived features (1, 2, 3, 5, 10, and 11) and
includes thresholds for features 5, 10 and 11. The
accuracy of the second decision rule on RTE3Devmt
</bodyText>
<page confidence="0.997617">
105
</page>
<bodyText confidence="0.999909678571429">
set was 71.50%, compared with an accuracy of
65.87 % on RTE3Test.
Our results are consistent with previous RTE2
findings (Bar-Haim et al., 2006) where task
performance varies with respect to the sentence
source. Both rules had similar (poor) performance
for information extraction (50.00 vs. 50.50%).
Both rules had moderate performance for
summarization (56.50 vs. 60.50%) and good
performance for information retrieval (70.00 vs.
75.50%). The second decision rule constantly out-
performed the first, with the largest increase of
11.5% in the question answering activity (65.50 vs.
77.00%).
Both decision rules lend themselves well to
ranking sentences in decreasing order from the
most to the least certain entailment. Average
precision is calculated using that ranking and
produces a perfect score when all sentence pairs
that are entailments (+ve) are listed before all the
sentence pairs that are not (-ve) (Voorhees &amp;
Harman., 1999). The average precision of the first
and second decision rules was 58.97% and 60.96%
respectively. The variation in precision also varied
with respect to the sentence source (IE, IR, QA and
SUM) of 48.52, 65.93, 72.38, and 56.04% for the
first decision rule and 48.32, 72.71, 78.75 and
56.69% for the second decision rule.
</bodyText>
<sectionHeader confidence="0.999685" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999984111111111">
Although most systems include both syntax and
semantics to detect entailment and paraphrasing,
our goal in this paper was to measure the impact of
sentence structure alone. We developed two
decision rules that each use features from a typed
dependency grammar representation the hypothesis
and test sentences. The first decision rule considers
all features and the second considers only a sub-set
of features, and adds thresholds to ensure that the
system does not consider dubious matches.
Thresholds for both rules were established using
sentences in RTE3Devmt only. The second rule out-
performed the first on RTE3Test, both with respect
to accuracy (60.50% vs. 65.87%) and average
precision (58.97% vs. 60.96%).
These results are particularly encouraging given
that our approach requires no background
knowledge (other than the lexicon) and that this
was the first time we participated in RTE. The
results suggest that sentence structure alone can
improve entailment prediction by between 9.25-
14.62% alone, over the majority class baseline
(51.52% in RTE3Test) and they provided additional
support to the growing body of evidence that
sentence structure will continue to play a role in
the accurate detection of textual entailments and
paraphrasing.
</bodyText>
<sectionHeader confidence="0.999192" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999630217391304">
Bar-Haim, R., Dagan, I., Dolan, B., Ferro, L.,
Giampiccolo, D., Magnini, B., et al. (2006). The
Second PASCAL Recognising Textual Entailment
Challenge. Venice, Italy.
Dagan, I., Glickman, O., &amp; Magnini, B. (2005). The
PASCAL Recognising Textual Entailment Challenge.
Southampton, U.K.
de Marneffe, M.-C., MacCartney, B., Grenager, T., Cer,
D., Rafferty, A., &amp; Manning, C. D. (2006). Learning
to distinguish valid textual entailments, In The
Second PASCAL Challenges Workshop on
Recognising Textual Entailment, Venice, Italy.
Hickl, A., Williams, J., Bensley, J., Roberts, K., Rink,
B., &amp; Shi, Y. (2006). Recognizing Textual
Entailment with LCC’s GROUNDHOG System In
The Second PASCAL Recognising Textual
Entailment Challenge, Venice, Italy.
Klein, D., &amp; Manning, C. D. (2002). Fast Exact
Inference with a Factored Model for Natural
Language Parsing. Paper presented at the Advances
in Neural Information Processing Systems.
Lin, D. (1998). Dependency-based Evaluation of
MINIPAR. In Workshop on the Evaluation of
Parsing Systems, First International Conference on
Language Resources and Evaluation, Granada,Spain.
Lin, D., &amp; Pantel, P. (2001). Induction of semantic
classes from natural language text. In The 7th
International conference on Knowledge discovery
and Data Mining, San Francisco, CA.
Litkowski, K. (2006). Componential Analysis for
Recognizing Textual Entailment. In The Second
PASCAL Challenges Workshop on Recognising
Textual Entailment, Venice, Italy.
Miller, G. (1995). WordNet: A Lexical Database for
English. Communications of the ACM, 38(11), 39-
41.
National Library of Medicine. (2000). The SPECIALIST
Lexicon, from
www.nlm.nih.gov/pubs/factsheets/umlslex.html
Vanderwende, L., Coughlin, D., &amp; Dolan, B. (2005).
What Syntax can Contribute in Entailment Task. The
PASCAL Challenges Workshop on Recognising
Textual Entailment. Southhampton, UK.
Voorhees, E. M., &amp; Harman., D. (1999). Overview of
the seventh text retrieval conference. In The Seventh
Text REtrieval Conference (TREC-7).
</reference>
<page confidence="0.99731">
106
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.958069">
<title confidence="0.998835">The Role of Sentence Structure in Recognizing Textual Entailment</title>
<author confidence="0.978863">Catherine</author>
<affiliation confidence="0.999845">School of Information and Library University of North Carolina at Chapel</affiliation>
<address confidence="0.984763">Chapel Hill, NC 27599-3360</address>
<email confidence="0.999427">cablake@email.unc.edu</email>
<abstract confidence="0.9997591">Recent research suggests that sentence structure can improve the accuracy of recognizing textual entailments and paraphrasing. Although background knowledge such as gazetteers, WordNet and custom built knowledge bases are also likely to improve performance, our goal in this paper is to characterize the syntactic features alone that aid in accurate entailment prediction. We describe candidate features, the role of machine learning, and two final decision rules. These rules resulted in an accuracy of 60.50 and 65.87% and average precision of 58.97 and 60.96% in and suggest that sentence structure alone can improve entailment accuracy by 9.25 to 14.62% over the baseline majority class.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Bar-Haim</author>
<author>I Dagan</author>
<author>B Dolan</author>
<author>L Ferro</author>
<author>D Giampiccolo</author>
<author>B Magnini</author>
</authors>
<title>The Second PASCAL Recognising Textual Entailment Challenge.</title>
<date>2006</date>
<location>Venice, Italy.</location>
<contexts>
<context position="2626" citStr="Bar-Haim et al., 2006" startWordPosition="386" endWordPosition="389">ou hit the riders on the road. A system that employed a syntactic representation of these sentences however, could detect that the don’t modifier applies to hit in first sentence and to slow second. In contrast to information retrieval, researchers in paraphrase and entailment detection have increased their use of sentence structure. Fewer than half of the submissions in the first Recognizing Textual Entailment challenge (RTE1) employed syntax (13/28, 46%) (Dagan, Glickman, &amp; Magnini, 2005), but more than two-thirds (28/ 41, 68%) of the second RTE challenge (RTE2) submissions employed syntax (Bar-Haim et al., 2006). Furthermore, for the first time, the RTE2 results showed that systems employing deep language features, such as syntactic or logical representations of text, could outperform the purely semantic overlap approach typified by BOW. Earlier findings such as (Vanderwende, Coughlin, &amp; Dolan, 2005) also suggest that sentence structure plays an important role in recognizing textual entailment and paraphrasing accurately. Our goal in this paper is to explore the degree to which sentence structure alone influences the accuracy of entailment and paraphrase detection. 101 Proceedings of the Workshop on </context>
<context position="17685" citStr="Bar-Haim et al., 2006" startWordPosition="2839" endWordPosition="2842">lead to an accuracy of around 65%. Many sentences had a threshold of around 2.4, and the overall accuracy of the first decision on the RTE3Devmt set was 62.38%, compared to 60.50% in RTE3Test. We consider the first decision rule a baseline and the second rule is our real submission. The second rule uses only a sub-set of the derived features (1, 2, 3, 5, 10, and 11) and includes thresholds for features 5, 10 and 11. The accuracy of the second decision rule on RTE3Devmt 105 set was 71.50%, compared with an accuracy of 65.87 % on RTE3Test. Our results are consistent with previous RTE2 findings (Bar-Haim et al., 2006) where task performance varies with respect to the sentence source. Both rules had similar (poor) performance for information extraction (50.00 vs. 50.50%). Both rules had moderate performance for summarization (56.50 vs. 60.50%) and good performance for information retrieval (70.00 vs. 75.50%). The second decision rule constantly outperformed the first, with the largest increase of 11.5% in the question answering activity (65.50 vs. 77.00%). Both decision rules lend themselves well to ranking sentences in decreasing order from the most to the least certain entailment. Average precision is cal</context>
</contexts>
<marker>Bar-Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini, 2006</marker>
<rawString>Bar-Haim, R., Dagan, I., Dolan, B., Ferro, L., Giampiccolo, D., Magnini, B., et al. (2006). The Second PASCAL Recognising Textual Entailment Challenge. Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>O Glickman</author>
<author>B Magnini</author>
</authors>
<title>The PASCAL Recognising Textual Entailment Challenge.</title>
<date>2005</date>
<location>Southampton, U.K.</location>
<contexts>
<context position="2498" citStr="Dagan, Glickman, &amp; Magnini, 2005" startWordPosition="365" endWordPosition="369">uish between the very different meaning conveyed by: (1)Slow down so that you don’t hit the riders on the road and (2)Don’t slow down so you hit the riders on the road. A system that employed a syntactic representation of these sentences however, could detect that the don’t modifier applies to hit in first sentence and to slow second. In contrast to information retrieval, researchers in paraphrase and entailment detection have increased their use of sentence structure. Fewer than half of the submissions in the first Recognizing Textual Entailment challenge (RTE1) employed syntax (13/28, 46%) (Dagan, Glickman, &amp; Magnini, 2005), but more than two-thirds (28/ 41, 68%) of the second RTE challenge (RTE2) submissions employed syntax (Bar-Haim et al., 2006). Furthermore, for the first time, the RTE2 results showed that systems employing deep language features, such as syntactic or logical representations of text, could outperform the purely semantic overlap approach typified by BOW. Earlier findings such as (Vanderwende, Coughlin, &amp; Dolan, 2005) also suggest that sentence structure plays an important role in recognizing textual entailment and paraphrasing accurately. Our goal in this paper is to explore the degree to wh</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Dagan, I., Glickman, O., &amp; Magnini, B. (2005). The PASCAL Recognising Textual Entailment Challenge. Southampton, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-C de Marneffe</author>
<author>B MacCartney</author>
<author>T Grenager</author>
<author>D Cer</author>
<author>A Rafferty</author>
<author>C D Manning</author>
</authors>
<title>Learning to distinguish valid textual entailments,</title>
<date>2006</date>
<booktitle>In The Second PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<location>Venice, Italy.</location>
<marker>de Marneffe, MacCartney, Grenager, Cer, Rafferty, Manning, 2006</marker>
<rawString>de Marneffe, M.-C., MacCartney, B., Grenager, T., Cer, D., Rafferty, A., &amp; Manning, C. D. (2006). Learning to distinguish valid textual entailments, In The Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hickl</author>
<author>J Williams</author>
<author>J Bensley</author>
<author>K Roberts</author>
<author>B Rink</author>
<author>Y Shi</author>
</authors>
<title>Recognizing Textual Entailment with LCC’s GROUNDHOG System In The Second PASCAL Recognising Textual Entailment Challenge,</title>
<date>2006</date>
<location>Venice, Italy.</location>
<contexts>
<context position="3588" citStr="Hickl et al., 2006" startWordPosition="524" endWordPosition="527">portant role in recognizing textual entailment and paraphrasing accurately. Our goal in this paper is to explore the degree to which sentence structure alone influences the accuracy of entailment and paraphrase detection. 101 Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 101–106, Prague, June 2007. c�2007 Association for Computational Linguistics Other than a lexicon (which is used to identify the base form of a term), our approach uses no background knowledge, such as WordNet (Miller, 1995), extensive dictionaries (Litkowski, 2006) or custom-built knowledge-bases (Hickl et al., 2006) that have been successfully employed by other systems. While such semantic knowledge should improve entailment performance, we deliberately avoid these sources to isolate the impact of sentence structure alone. 2 System Architecture 2.1 Lexical Processing Our approach requires an explicit representation of structure in both the hypothesis (HSent) and test (TSent) sentence(s). Systems in RTE challenges employ a variety of parsers. In RTE2 the most popular sentence structure was generated by Minipar (Lin, 1998), perhaps because it is also one of the fastest parsers. Our system uses the typed de</context>
</contexts>
<marker>Hickl, Williams, Bensley, Roberts, Rink, Shi, 2006</marker>
<rawString>Hickl, A., Williams, J., Bensley, J., Roberts, K., Rink, B., &amp; Shi, Y. (2006). Recognizing Textual Entailment with LCC’s GROUNDHOG System In The Second PASCAL Recognising Textual Entailment Challenge, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Fast Exact Inference with a Factored Model for Natural Language Parsing. Paper presented at the Advances in Neural Information Processing Systems.</title>
<date>2002</date>
<contexts>
<context position="4258" citStr="Klein &amp; Manning, 2002" startWordPosition="624" endWordPosition="627">ystems. While such semantic knowledge should improve entailment performance, we deliberately avoid these sources to isolate the impact of sentence structure alone. 2 System Architecture 2.1 Lexical Processing Our approach requires an explicit representation of structure in both the hypothesis (HSent) and test (TSent) sentence(s). Systems in RTE challenges employ a variety of parsers. In RTE2 the most popular sentence structure was generated by Minipar (Lin, 1998), perhaps because it is also one of the fastest parsers. Our system uses the typed dependency tree generated by the Stanford Parser (Klein &amp; Manning, 2002). A complete set of parser tags and the method used to map from a constituent to a typed dependency grammar can be found in (de Marneffe et al., 2006). Figure 1 shows an example typed dependency grammar for pair id 355 in the RTE3Test set. 2.2 Lexicon Our proposed approach requires the base form of each term. We considered two lexicons for this purpose: WordNet (Miller, 1995) and the SPECIALIST lexicon (National Library of Medicine, 2000). The latter is part of the National Library of Medicine’s (NLM) Unified Medical Language System (UMLS) and comprises terms drawn from medical abstracts, and </context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Klein, D., &amp; Manning, C. D. (2002). Fast Exact Inference with a Factored Model for Natural Language Parsing. Paper presented at the Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Dependency-based Evaluation of MINIPAR.</title>
<date>1998</date>
<booktitle>In Workshop on the Evaluation of Parsing Systems, First International Conference on Language Resources and Evaluation, Granada,Spain.</booktitle>
<contexts>
<context position="4103" citStr="Lin, 1998" startWordPosition="600" endWordPosition="601">, extensive dictionaries (Litkowski, 2006) or custom-built knowledge-bases (Hickl et al., 2006) that have been successfully employed by other systems. While such semantic knowledge should improve entailment performance, we deliberately avoid these sources to isolate the impact of sentence structure alone. 2 System Architecture 2.1 Lexical Processing Our approach requires an explicit representation of structure in both the hypothesis (HSent) and test (TSent) sentence(s). Systems in RTE challenges employ a variety of parsers. In RTE2 the most popular sentence structure was generated by Minipar (Lin, 1998), perhaps because it is also one of the fastest parsers. Our system uses the typed dependency tree generated by the Stanford Parser (Klein &amp; Manning, 2002). A complete set of parser tags and the method used to map from a constituent to a typed dependency grammar can be found in (de Marneffe et al., 2006). Figure 1 shows an example typed dependency grammar for pair id 355 in the RTE3Test set. 2.2 Lexicon Our proposed approach requires the base form of each term. We considered two lexicons for this purpose: WordNet (Miller, 1995) and the SPECIALIST lexicon (National Library of Medicine, 2000). T</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, D. (1998). Dependency-based Evaluation of MINIPAR. In Workshop on the Evaluation of Parsing Systems, First International Conference on Language Resources and Evaluation, Granada,Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>P Pantel</author>
</authors>
<title>Induction of semantic classes from natural language text.</title>
<date>2001</date>
<booktitle>In The 7th International conference on Knowledge discovery and Data Mining,</booktitle>
<location>San Francisco, CA.</location>
<contexts>
<context position="5774" citStr="Lin &amp; Pantel, 2001" startWordPosition="868" endWordPosition="871">is revealed that of the WordNet entries, 5008 (84.1%) had a morphological base form in the SPECIALIST lexicon. Of the 548 distinct entries that differed between the two lexicons, 389 differed because either the UMLS (214 terms) or WordNet (11 terms) did not have a base form. These results suggest that although the NLM did not develop their lexicon for news articles, the entries in the SPECIALIST lexicon subsumes most terms found in the more frequently used WordNet lexicon. Thus, our system uses the base form of terms from the SPECIALIST lexicon. 2.3 Collapsing Preposition Paths Previous work (Lin &amp; Pantel, 2001) suggests the utility of collapsing paths through prepositions. The type dependency does have a preposition tag, prep, however, we found that the parser typically assigns a more general tag, such as dep (see the dep tag in Figure 1 between wrapped and by). Instead of using the prep tag, the system collapses paths that contain a preposition from the SPECIALIST lexicon. For example, the system 102 Figure 1. Dependency grammar tree for pair identifier 355 in the RTE3Test collapses four paths in TSentEG millions of visitors, wrapped in 1995, wrapped by Christo, and wrapped began before. noun compo</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Lin, D., &amp; Pantel, P. (2001). Induction of semantic classes from natural language text. In The 7th International conference on Knowledge discovery and Data Mining, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Litkowski</author>
</authors>
<title>Componential Analysis for Recognizing Textual Entailment.</title>
<date>2006</date>
<booktitle>In The Second PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<location>Venice, Italy.</location>
<contexts>
<context position="3535" citStr="Litkowski, 2006" startWordPosition="519" endWordPosition="520">) also suggest that sentence structure plays an important role in recognizing textual entailment and paraphrasing accurately. Our goal in this paper is to explore the degree to which sentence structure alone influences the accuracy of entailment and paraphrase detection. 101 Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 101–106, Prague, June 2007. c�2007 Association for Computational Linguistics Other than a lexicon (which is used to identify the base form of a term), our approach uses no background knowledge, such as WordNet (Miller, 1995), extensive dictionaries (Litkowski, 2006) or custom-built knowledge-bases (Hickl et al., 2006) that have been successfully employed by other systems. While such semantic knowledge should improve entailment performance, we deliberately avoid these sources to isolate the impact of sentence structure alone. 2 System Architecture 2.1 Lexical Processing Our approach requires an explicit representation of structure in both the hypothesis (HSent) and test (TSent) sentence(s). Systems in RTE challenges employ a variety of parsers. In RTE2 the most popular sentence structure was generated by Minipar (Lin, 1998), perhaps because it is also one</context>
</contexts>
<marker>Litkowski, 2006</marker>
<rawString>Litkowski, K. (2006). Componential Analysis for Recognizing Textual Entailment. In The Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>WordNet: A Lexical Database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<pages>39--41</pages>
<contexts>
<context position="3493" citStr="Miller, 1995" startWordPosition="515" endWordPosition="516">s (Vanderwende, Coughlin, &amp; Dolan, 2005) also suggest that sentence structure plays an important role in recognizing textual entailment and paraphrasing accurately. Our goal in this paper is to explore the degree to which sentence structure alone influences the accuracy of entailment and paraphrase detection. 101 Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 101–106, Prague, June 2007. c�2007 Association for Computational Linguistics Other than a lexicon (which is used to identify the base form of a term), our approach uses no background knowledge, such as WordNet (Miller, 1995), extensive dictionaries (Litkowski, 2006) or custom-built knowledge-bases (Hickl et al., 2006) that have been successfully employed by other systems. While such semantic knowledge should improve entailment performance, we deliberately avoid these sources to isolate the impact of sentence structure alone. 2 System Architecture 2.1 Lexical Processing Our approach requires an explicit representation of structure in both the hypothesis (HSent) and test (TSent) sentence(s). Systems in RTE challenges employ a variety of parsers. In RTE2 the most popular sentence structure was generated by Minipar (</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>Miller, G. (1995). WordNet: A Lexical Database for English. Communications of the ACM, 38(11), 39-41.</rawString>
</citation>
<citation valid="false">
<date>2000</date>
<institution>National Library of Medicine.</institution>
<note>The SPECIALIST Lexicon, from www.nlm.nih.gov/pubs/factsheets/umlslex.html</note>
<marker>2000</marker>
<rawString>National Library of Medicine. (2000). The SPECIALIST Lexicon, from www.nlm.nih.gov/pubs/factsheets/umlslex.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Vanderwende</author>
<author>D Coughlin</author>
<author>B Dolan</author>
</authors>
<title>What Syntax can Contribute in Entailment Task. The PASCAL Challenges Workshop on Recognising Textual Entailment.</title>
<date>2005</date>
<publisher>Southhampton, UK.</publisher>
<contexts>
<context position="2919" citStr="Vanderwende, Coughlin, &amp; Dolan, 2005" startWordPosition="427" endWordPosition="431">etection have increased their use of sentence structure. Fewer than half of the submissions in the first Recognizing Textual Entailment challenge (RTE1) employed syntax (13/28, 46%) (Dagan, Glickman, &amp; Magnini, 2005), but more than two-thirds (28/ 41, 68%) of the second RTE challenge (RTE2) submissions employed syntax (Bar-Haim et al., 2006). Furthermore, for the first time, the RTE2 results showed that systems employing deep language features, such as syntactic or logical representations of text, could outperform the purely semantic overlap approach typified by BOW. Earlier findings such as (Vanderwende, Coughlin, &amp; Dolan, 2005) also suggest that sentence structure plays an important role in recognizing textual entailment and paraphrasing accurately. Our goal in this paper is to explore the degree to which sentence structure alone influences the accuracy of entailment and paraphrase detection. 101 Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 101–106, Prague, June 2007. c�2007 Association for Computational Linguistics Other than a lexicon (which is used to identify the base form of a term), our approach uses no background knowledge, such as WordNet (Miller, 1995), extensive dictionaries (</context>
</contexts>
<marker>Vanderwende, Coughlin, Dolan, 2005</marker>
<rawString>Vanderwende, L., Coughlin, D., &amp; Dolan, B. (2005). What Syntax can Contribute in Entailment Task. The PASCAL Challenges Workshop on Recognising Textual Entailment. Southhampton, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
<author>D Harman</author>
</authors>
<title>Overview of the seventh text retrieval conference.</title>
<date>1999</date>
<booktitle>In The Seventh Text REtrieval Conference (TREC-7).</booktitle>
<marker>Voorhees, Harman, 1999</marker>
<rawString>Voorhees, E. M., &amp; Harman., D. (1999). Overview of the seventh text retrieval conference. In The Seventh Text REtrieval Conference (TREC-7).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>