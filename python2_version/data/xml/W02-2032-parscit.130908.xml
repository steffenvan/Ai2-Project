<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002548">
<title confidence="0.983741">
Topological Field Chunking for German
</title>
<author confidence="0.917321">
Jorn Veenstra and Frank Henrik Muller and Tylman Ule
</author>
<affiliation confidence="0.4693305">
Seminar fiir Sprachwissenschaft, Universitat Tubingen
{ veenstra,fhm,ule }Asfs. uni-tuebingen. de
</affiliation>
<sectionHeader confidence="0.920155" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996135">
In this paper&apos; we compare three different ap-
proaches to the analysis of the basic structure
in German sentences: the sentence brackets
in the topological field framework in German
(Haile, 1986). The first approach is based on
hand-written Finite-State Automata (FSA); the
other two are trained on corpus data. One is
a Probabilistic Context-Free Grammar (PCFG)
approach, the other is a classification-based
Memory-Based Learning (MBL) approach. The
three approaches are evaluated on a manually
annotated corpus. We will show that the F0_,
value for this task is around 94% for all three
approaches, which suggests that this is a fruit-
ful first step for parsing and analysing German
text.
</bodyText>
<sectionHeader confidence="0.996295" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999815533333333">
Sentence structure in German is known for its
complexity. Hence, for parsing German it is
useful to first assign a basic structure to the
sentence and then base subsequent processing
on this basic structure, following the divide and
conquer strategy.
A well defined basic structure for German
sentences is the topological field structure.
Topological fields describe sections in the Ger-
man sentence with regard to the distributional
properties of the verb. Topological fields give a
handle to break up sentences into smaller, easier
to process parts. In this paper we describe three
different approaches to the detection of the ver-
bal parts of the topological fields (TopFChunk-
</bodyText>
<footnote confidence="0.986056666666667">
1-The research reported here was supported by the
German Research Council (DFG) as part of the Sonder-
forschungsbereich 441 &amp;quot;Linguistische Datenstrukturen&amp;quot;
(Linguistic Data Structures). The second author was
also supported by a grant of the graduate school &amp;quot;Inte-
griertes Linguistikstudium&amp;quot; funded by the DFG.
</footnote>
<bodyText confidence="0.989868733333333">
ing): i) hand-written rules implemented in Fi-
nite State Automata (FSA); ii) Probabilistic
Context-Free Grammar rules derived from a
corpus (PCFG); and iii) a machine learning
approach, Memory-Based Learning (MBL), in
which TopFChunking is approached as a tag-
ging task.
This paper is structured as follows. In Sec-
tion 2 we introduce the concept of Topologi-
cal Fields. In Section 3 we discuss the corpus.
In Section 4 we give a short explanation of the
three methods compared in this paper. In Sec-
tion 5 we see the results. Finally, in Section 6 we
draw our conclusions and propose some further
research.
</bodyText>
<sectionHeader confidence="0.44077" genericHeader="method">
2 The topological field model
</sectionHeader>
<subsectionHeader confidence="0.780474">
2.1 An outline of the model
</subsectionHeader>
<bodyText confidence="0.985460666666667">
The topological field model (cf. Hale (1986))
is a well-established descriptive model of the
constituent order in German and several other
Germanic languages. Topological fields describe
sections in the German sentence with regard to
the distributional properties of the verb (and
the subordinator in subclauses). There are
three different types of clauses (see also Ta-
ble 1):2 verb-last clauses (VL), verb-first clauses
(V1) and verb-second clauses (V2). VL clauses
comprise all introduced subclauses, V1 clauses
mainly comprise imperatives and yes/no ques-
tions and V2 clauses mostly comprise affirma-
tive clauses. The topological fields C/LK and
VC constitute the sentence bracket, relative to
which the other fields can be described3. The
section preceding the left part of the sentence
2Cf. Figure 1 for an illustrative example.
</bodyText>
<tableCaption confidence="0.853997">
30bligatory fields are in bold type. The fields KO-
ORD (coordination field) and LV (Linkversetzung, topi-
calization) will not be discussed in this paper. The rep-
resentation in Table 1 is slighty simplified.
Table 1: The topological field model
</tableCaption>
<table confidence="0.9856456">
clause topological
type fields
VL: KOORD LV C MF VC NF
Vi: KOORD LV LK MF VC NF
V2: KOORD LV VF LK MF VC NF
</table>
<bodyText confidence="0.9997065625">
bracket is called the Vorfeld (VF; initial field;
only in V2 clauses), the section included in the
sentence bracket is called the Mittelfeld (MF;
middle field) and the section following the right
part of the sentence bracket is called the Nach-
feld (NF; final field). While the ordering of
other constituents is relatively free in German,
the ordering of topological fields is subject to
syntactic restrictions which adhere to the un-
varying pattern outlined in Table 1. Figure 1
shows that the topological field model is also
capable of accounting for recursive structures.
Subordinate clauses may be embedded in topo-
logical fields and, with this model, it is possi-
ble to leave their attachment open until disam-
biguation in further annotation steps.
</bodyText>
<subsectionHeader confidence="0.976775">
2.2 The linguistic perspective
</subsectionHeader>
<bodyText confidence="0.999975135135135">
The topological field model is primarily a distri-
butional model. It does not give any account of
the verb-complement structure and it does not
reveal the relation of the constituents within the
topological fields, either. However, as argued
by Meurers (2002), topological field annotation
can &amp;quot;significantly help in using corpora from
the perspective of theoretical linguistics&amp;quot;. This
becomes clear when looking at Figure 1: The
structure of topological fields along with POS
tags marks the borders and the type of sub-
clauses in a sentence. This shallow annotation
thus provides the user with what one can call
the skeleton of the sentence. Without the an-
notation of topological fields, it is by no means
clear where the borders of the subclauses are
and it is not clear where the potential comple-
ments of the respective verbs are. A query for a
linguistic structure would therefore highly over-
generate, thus producing a high amount of false
matches.
The shallow annotation as shown in Figure 1
gives the user of an annotated corpus the possi-
bility to investigate various distributional phe-
nomena which have to be described relative to
the topological fields. The location of the em-
bedding of subclauses is one of them, which
has to be discussed in every linguistic theory
and which is by no means agreed upon by lin-
guists. Eisenberg (1999) is an adequate ex-
ample: &amp;quot;Der Besetzung des Nachfeldes wird
haufig eine kommunikativ-pragmatische Funk-
tion zugeschrieben, [...]&amp;quot;(p. 391)4. If linguists
want to examine such a hypothesis, they are in
need of an annotation which provides them with
topological fields. Otherwise queries to corpora
yield too many false matches.
</bodyText>
<subsectionHeader confidence="0.966422">
2.3 The computational perspective
</subsectionHeader>
<bodyText confidence="0.999948419354839">
Concerning automatic annotation, one of the
main advantages which can be acquired through
the annotation of topological fields is that they
are the skeleton of the sentence and thus consid-
erably reduce the scope of ambiguity. Without
the annotation of topological fields the scope
of complements of the verb is much wider, es-
pecially in complex sentences, in which, addi-
tionally, it is not clear which potential comple-
ments belong to which verb. Figure 1 shows how
the annotation of topological fields reduces the
scope of possible complements for the verbs by
dividing the sentence into fields and subclauses.
The subclauses can now even be dealt with as
single units, thus using a divide and conquer
strategy similar to the one outlined in Peh and
Ting (1996).
By first annotating topological fields, one can
use a strategy termed containment of ambigu-
ity by Abney (1996). This strategy proposes to
annotate higher levels first if &amp;quot;reliable markers&amp;quot;
are present because this considerably limits the
number of possible attachment sites. Annotat-
ing topological fields first and verb-complement
structure later transfers this strategy, which
was mainly used for chunking (i.e. basic phrase
recognition), to the topological fields and clause
leve1.5 That way, it is also possible to construct
a hybrid annotation system, in which every lin-
guistic phenomenon is tackled with methods es-
pecially suited for it (cf. Hinrichs et al. (2002)).
</bodyText>
<footnote confidence="0.5573615">
4&amp;quot;A communicative-pragmatic function is very often
attributed to the occupation of the Nachfeld, [...]&amp;quot; (our
translation).
5A similar strategy has already been used to prestruc-
ture sentences for an information retrieval system (cf.
Braun (1999) and Neumann et al. (2000)).
</footnote>
<figure confidence="0.810910914893617">
erst
ADV
nichts
PIS
cng:***
Katastrophenstimmung
NN
cngs:nsfw
herrscht
VVFIN
pnmt:3sis
mehr
ADV
ist
VAFIN
pnmt:3sis
wenn
KOUS
zu
PTKZU
−−
−−
−−
−−
verheimlichen
VVINF
−−
−
−
−
−
−
−
ON
VC
MOD
NX
HD HD HD − HD HD HD − HD
LK
ON
HD
HD
−
HD
VXINF
VXFIN
−
</figure>
<bodyText confidence="0.99986778125">
of manually devised rules. It acts as a trans-
ducer, providing additional structure indicating
the topological fields.
It is easy to devise an FSA for the recog-
nition of topological fields because one of the
main features of topological fields is that they
are an unvarying pattern in the German sen-
tence, i.e. they obey to absolute syntactic re-
strictions. Because the structure of topological
fields is both well-known and simple, the rules
for the FSAs can be manually constructed us-
ing linguistic knowledge extracted from gram-
mars and deduced from automatically annotat-
ing unannotated corpora and manually compar-
ing the results. The rules were improved after
evaluation of the parser against the train subset
of the corpus.
The construction of the parser is such that
it is a pipe of FSA transducers in which the
output of one level is the input to the next
level. That way, recursive structures, which
do occur in topological fields, can be cov-
ered. Ambiguities are resolved by a longest
match strategy. We use the TTT suite of tools
(Grover et al., 2000) available from the LTG Ed-
inburgh (http : //www . ltg . ed. ac .uk/) for our
purposes. The annotation of topological fields is
in fact the first step in annotation. One reason
for this is that topological fields show the out-
line of the sentence. The sentence bracket an-
notation evaluated here is only one part of the
linguistic information annotated by the parser.
</bodyText>
<subsectionHeader confidence="0.926359">
4.2 The MBL chunker
</subsectionHeader>
<bodyText confidence="0.999792146341463">
Memory-Based Learning (MBL) is a machine
learning algorithm in which the learning stage
is memory-based: train examples are stored in a
memory base without abstraction. The classifi-
cation stage is similarity-based: classification is
based on the most similar instances in the mem-
ory base. We have used the MBL implementa-
tion Timbl (Daelemans et al., 2001), which is
available from http : //ilk . kub . nl.
For TopFChunking we window over the text.
To each word we assign one of three tags to
determine whether this word is inside a chunk
(I), outside a chunk (0), or between two chunks
of the same type (B), yielding a total of 9
JOB tags, 3 for each chunk type (LK; VC; C),
cf. Ramshaw and Marcus (1995).
To find the optimal parameter setting and
feature selection for this TopFChunking task we
took 10% of the train set apart and used this as
a validation set. The other 90% of the train set
was used as a validation train set. It turned out
that working only with the POS tag features
gives the best results. For an optimal learn-
ing algorithm we found IGTree (a decision tree
variant of MBL) with IG as feature weighting
method and the number of nearest neighbours
equal to 1 (k = 1).
We have used Information Gain (IG) (Daele-
mans et al., 2001) to weight the feature rele-
vance. What we can see is that the word form
features have a low IG value, this shows that
we suffer from sparse data for features with too
many values (for word forms we have almost
20,000 different values, for the POS features
just 55). This sparse data problem is due to
the relatively small size of the TilbaD/Z cor-
pus: the train set contains no more than 100,000
words. We can expect that with a larger corpus
the word form features will gain relevance, and
with this we can expect better results for the
TopFChunking task.
</bodyText>
<subsectionHeader confidence="0.97569">
4.3 The PCFG chunker
</subsectionHeader>
<bodyText confidence="0.999989769230769">
A probabilistic context-free grammar (PCFG)
model learns context-free rules from a training
corpus, and also the probability for each non-
terminal symbol to expand into a sequence of
terminal and non-terminal symbols. The termi-
nal symbols, in our case, are POS tags. Con-
ditioning is performed only on the rules&apos; left
hand side, i.e. the PCFG is also probabilistically
context-free. It does not take into account any
lexical information, neither for terminals nor for
conditioning.
Some sub-classes of phrases receive the same
label in the TilBaD/Z annotation scheme but
show complementary distribution in different
fields, which is impossible to learn for a PCFG.
Most prominently, the C field in VL-clauses has
to contain a subordinator. While subordinating
conjunctions are usually a direct child to the C
field and therefore captured by CFG rules, rela-
tive and interrogative pronouns are contained in
a separate noun chunk and are therefore indis-
tinguishable from other noun chunks. As a con-
sequence, a CFG not distinguishing these sub-
types of NCs is not able to suppress a standard
[Nx PPOSAT NN] noun chunk in C fields, and
only to allow [Nx—c PWS ], which contains an
interrogative pronoun. The treebank was there-
fore changed to assign unique node names to all
non-terminal child nodes of C fields (see Fig-
ure 2). Another peculiarity of VL-clauses is that
their VC field contains a finite verb form, as op-
posed to the VC field for V1- and V2-clauses.
Again, all VC child nodes and also the VC nodes
themselves are systematically renamed as shown
in Figure 2 for the VC of the sentence in Fig-
ure 3. The impact on performance is quite dra-
matic for the changes in the C field, improving
from 82% to 92%. The improvement proved to
be negligable for the VC fields, however.
</bodyText>
<equation confidence="0.998476625">
SIMPX —&gt; C MF VC-F
C —&gt; NX-C
NX-C —&gt; PWS
MF NX NX
NX —&gt; PPOSAT NN
NX —&gt; ART PIDAT NN
VC-F —&gt; VXFIN
VXFIN —&gt; VVFIN
</equation>
<figureCaption confidence="0.994324">
Figure 2: Modified CFG rules for the subclause
in Figure 3
</figureCaption>
<bodyText confidence="0.99986427027027">
The lopar (Schmid, 2000) parser (download-
able from: http//www.ims.uni-stuttgart.de/) is
used to train the PCFG on the modified tree-
bank and to parse the test set with the resulting
grammar model. The task of assigning the ver-
bal frame is performed as a subtask of parsing
the whole sentence, i.e. the best parse of the
PCFG is determined for a sentence, but only
the annotation of the verbal frame is evaluated.
Another modification is applied to the tree-
bank before extracting the rules, because lopar
does not allow cyclic grammars, which may well
occur in topological field analyses. All nodes
that dominate (directly or indirectly) a node
of the same category are renamed by adding
the number of parent nodes of the same type
to their category name, removing cycles in the
resulting grammar. The nodes that do not con-
tain any nodes of the same category are not
changed, so that the unchanged node categories
refer to chunk nodes. In order to focus on the
TopFChunking task, all non-chunk nodes are re-
moved whenever they do not dominate a field
node, simplifying the structure and reducing the
number of CFG rules.
Supervised training on the treebank data
yields the results given in Table 2. Due to lim-
ited memory, lopar did not parse two sentences,
and these two sentences containing 20 TopFs
were removed from the test set for the PCFG
chunker. Lopar also supports an unsupervised
training mode. However, unsupervised training
did not increase the performance of the PCFG
model. Fo_1 dropped to 94.0% when training
on 5000 additional POS-annotated sentences,
and to 93.5% and 88.9% when training on 34269
and 90000 sentences, respectively8.
</bodyText>
<subsectionHeader confidence="0.988013">
4.4 Baseline
</subsectionHeader>
<bodyText confidence="0.9999835">
To give an idea of the complexity of the task
we have computed a baseline for both the gold
standard and the TnT POS tagged data. Per
POS tag we have taken the most frequent TopF
class, based on the frequency in the train set.
In the test set we have assigned these most fre-
quent TopF classes to each word. The baseline
is computed over this assignment.
</bodyText>
<sectionHeader confidence="0.999463" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999941363636364">
In Table 2 we give an overview of the results of
the three approaches and the baseline results.
The &amp;quot;ALL&amp;quot; column gives a weighted average
between precision and recall for all three tasks
(F0_1). The other columns give the Fo_1 re-
sult for each task separately. The gold rows
give the results with the gold standard POSs
from the corpus. This gives an idea of the influ-
ence of POS errors. The gold POS results are
trained on gold POS data, the TnT results on
TnT tagged data.
</bodyText>
<sectionHeader confidence="0.961591" genericHeader="conclusions">
6 Conclusion and Future Research
</sectionHeader>
<subsectionHeader confidence="0.764952">
6.1 Conclusion
</subsectionHeader>
<bodyText confidence="0.9987636">
In this paper we have seen that all three ap-
proaches show a Fo_1 score on the TopFChunk-
ing task around 94%. Such a high performance
gives a solid basis for parsing and other text
analysis tasks, such as information extraction
and grammatical function assignment.
We have compared an FSA chunker, an MBL
chunker and a PCFG chunker, which all have
well-known advantages. An FSA chunker is
easy to implement and is very efficient in terms
</bodyText>
<footnote confidence="0.882341">
8The first 34269 sentences of the additional training
set have 15 to 25 words, and the first 5000 sentences
have 24 to 25 words. The 90000 sentence set contains
sentences from 1 to 50 words.
</footnote>
<tableCaption confidence="0.8461065">
Table 2: Results (F0_1 in %) on the Topological
Field Chunking task
</tableCaption>
<table confidence="0.999730666666667">
ALL LK VC C
FSA TnT 94.1 96.2 92.0 93.8
FSA gold 98.4 98.8 98.3 97.5
PCFG TnT 94.4 97.0 92.2 92.3
PCFG gold 98.1 98.9 98.1 96.1
MBL TnT 93.3 96.0 90.0 91.6
MBL gold 97.2 98.0 96.7 96.6
baseline TnT 75.5 75.2 72.3 83.2
baseline gold 77.9 75.0 79.1 86.2
</table>
<bodyText confidence="0.999966844444445">
of time and space. On the other hand, rules de-
rived from annotated examples are sometimes
preferred to manually edited rules. The MBL
chunker is also efficient in terms of speed, and
it is a learning method that does not stipulate
a certain kind of grammatical structure for the
predicted events. However, its results are low-
est, while still very close to the others. The
PCFG chunker is most costly in terms of space
and execution time, but the grammar model
seems to be well suited for the task at hand.
We can see that the PCFG and the FSA chun-
ker show a better performance than the MBL
chunker. This is due to the relatively small size
of the corpus for which MBL is more sensitive,
and to the fact that in its present form the MBL
chunker uses a windowing approach with a rel-
atively small window, whereas the other two
chunkers can take long distance depencies into
account.
Now that we have found the LK, C, and VC
in the sentence it is straightforward to detect
the basic structure of the sentence. In Figure 3
we give another example of a complex sentence
with a shallow annotation. We can see that this
sentence is rather difficult to analyse without
knowledge of the topological fields. Once we
have chunked the LK, C and VC, the sentence
is much easier to parse: Since we know that
&amp;quot;macht&amp;quot; has to be the LK of the second clause
(because it is the last LK in the sentence) we
can assign MF to &amp;quot;sich entweder liicherlich oder
legendar&amp;quot; and we know where the first clause
ends. In the first clause, we know that the
MF is to be found between the C field &amp;quot;Wer&amp;quot;
and the VC field &amp;quot;prophezeit&amp;quot;. What is more,
once we know what sentence type we are deal-
ing with (V1, V2, or VL), and once we have
information about morphology and the valency
of the verb, we can expect certain constituents
in the MF, and their grammatical function. In
our example, the MF contains the Direct Ob-
ject for &amp;quot;seinem Publikum&amp;quot; and the Accusative
Object for &amp;quot;eine solche Reaktion&amp;quot;, and the C
field &amp;quot;Wee contains the Nominative Object.
</bodyText>
<subsectionHeader confidence="0.995511">
6.2 Future Research
</subsectionHeader>
<bodyText confidence="0.999945384615385">
We see two major areas for future research: a
combined classifier and a qualitative error anal-
ysis. A combination of the different TopFChun-
kers seems promising, because they seem to
have different strengths judged by the given re-
sults that differ for each field type. The three
chunkers may provide the input for a stacked
classifier which may also take other features into
account.
A combined classifier will work best if the
three chunkers make different kinds of errors.
Therefore, a qualitative analysis of the chunk-
ing errors will be useful, which will also help to
improve the individual chunkers.
Other areas of future research include further
analysis along the lines of TopFs, i.e. assign-
ing recursive sentence structure based on the
C/LK/VC fields; classification of clauses (V1,
V2 or VL) and further syntactic annotation
within fields.
Subcategorisation information is readily
available from the topological field structure
and the constituents in them. This is an im-
portant first step to finding linguistically inter-
esting phenomona; and also to information pro-
cessing, such as question-answering systems.
</bodyText>
<sectionHeader confidence="0.998407" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998098818181818">
Steven Abney. 1996. Partial Parsing via
Finite-State Cascades. In Proceedings of the
ESSLLI-96 Workshop on &amp;quot;Robust Parsing&amp;quot;,
Prague.
Thorsten Brants. 2000. TnT - a statistical
part-of-speech tagger. In Proceedings of the
6th Applied NLP Conference, ANLP-2000,
Seattle, April 29 - May 3.
Christian Braun. 1999. Flaches und robustes
Parsen deutscher Satzgefiige. Diplomarbeit,
Universitat des Saarlandes, Saarbriicken.
</reference>
<figure confidence="0.999787792207792">
Wer
PWS
seinem
PPOSAT
Publikum
NN
eine
ART
solche
PIDAT
Reaktion
NN
prophezeit
VVFIN
macht
VVFIN
sick
PRF
entweder
KON
lächerlich
ADJD
oder
KON
legendär
ADJD
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−
−−
−
−
−
VF
SIMPX
OA
MF
ON
NX
HD − HD − − HD HD HD HD
MF
−
−
−
OD
NX
OA
NX
VXFIN
VC
HD
VXFIN
HD
LK
NX
PRED
ADJX
−
− −
−
ADJX
ADJX
HD
HD
SIMPX
ON
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.194327">
<title confidence="0.964412">Topological Field Chunking for German</title>
<author confidence="0.945027">Veenstra Henrik Muller</author>
<note confidence="0.3128875">Seminar fiir Sprachwissenschaft, Universitat { veenstra,fhm,ule }Asfs. uni-tuebingen. de</note>
<abstract confidence="0.985013529411765">In this paper&apos; we compare three different approaches to the analysis of the basic structure in German sentences: the sentence brackets the field in German (Haile, 1986). The first approach is based on hand-written Finite-State Automata (FSA); the other two are trained on corpus data. One is a Probabilistic Context-Free Grammar (PCFG) approach, the other is a classification-based Memory-Based Learning (MBL) approach. The three approaches are evaluated on a manually annotated corpus. We will show that the F0_, value for this task is around 94% for all three approaches, which suggests that this is a fruitful first step for parsing and analysing German text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Partial Parsing via Finite-State Cascades.</title>
<date>1996</date>
<booktitle>In Proceedings of the ESSLLI-96 Workshop on &amp;quot;Robust Parsing&amp;quot;,</booktitle>
<location>Prague.</location>
<contexts>
<context position="7081" citStr="Abney (1996)" startWordPosition="1135" endWordPosition="1136">al fields the scope of complements of the verb is much wider, especially in complex sentences, in which, additionally, it is not clear which potential complements belong to which verb. Figure 1 shows how the annotation of topological fields reduces the scope of possible complements for the verbs by dividing the sentence into fields and subclauses. The subclauses can now even be dealt with as single units, thus using a divide and conquer strategy similar to the one outlined in Peh and Ting (1996). By first annotating topological fields, one can use a strategy termed containment of ambiguity by Abney (1996). This strategy proposes to annotate higher levels first if &amp;quot;reliable markers&amp;quot; are present because this considerably limits the number of possible attachment sites. Annotating topological fields first and verb-complement structure later transfers this strategy, which was mainly used for chunking (i.e. basic phrase recognition), to the topological fields and clause leve1.5 That way, it is also possible to construct a hybrid annotation system, in which every linguistic phenomenon is tackled with methods especially suited for it (cf. Hinrichs et al. (2002)). 4&amp;quot;A communicative-pragmatic function i</context>
</contexts>
<marker>Abney, 1996</marker>
<rawString>Steven Abney. 1996. Partial Parsing via Finite-State Cascades. In Proceedings of the ESSLLI-96 Workshop on &amp;quot;Robust Parsing&amp;quot;, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT - a statistical part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Applied NLP Conference, ANLP-2000,</booktitle>
<location>Seattle,</location>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. TnT - a statistical part-of-speech tagger. In Proceedings of the 6th Applied NLP Conference, ANLP-2000, Seattle, April 29 - May 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Braun</author>
</authors>
<date>1999</date>
<booktitle>Flaches und robustes Parsen deutscher Satzgefiige. Diplomarbeit, Universitat des Saarlandes, Saarbriicken.</booktitle>
<contexts>
<context position="7887" citStr="Braun (1999)" startWordPosition="1254" endWordPosition="1255"> first and verb-complement structure later transfers this strategy, which was mainly used for chunking (i.e. basic phrase recognition), to the topological fields and clause leve1.5 That way, it is also possible to construct a hybrid annotation system, in which every linguistic phenomenon is tackled with methods especially suited for it (cf. Hinrichs et al. (2002)). 4&amp;quot;A communicative-pragmatic function is very often attributed to the occupation of the Nachfeld, [...]&amp;quot; (our translation). 5A similar strategy has already been used to prestructure sentences for an information retrieval system (cf. Braun (1999) and Neumann et al. (2000)). erst ADV nichts PIS cng:*** Katastrophenstimmung NN cngs:nsfw herrscht VVFIN pnmt:3sis mehr ADV ist VAFIN pnmt:3sis wenn KOUS zu PTKZU −− −− −− −− verheimlichen VVINF −− − − − − − − ON VC MOD NX HD HD HD − HD HD HD − HD LK ON HD HD − HD VXINF VXFIN − of manually devised rules. It acts as a transducer, providing additional structure indicating the topological fields. It is easy to devise an FSA for the recognition of topological fields because one of the main features of topological fields is that they are an unvarying pattern in the German sentence, i.e. they obey </context>
</contexts>
<marker>Braun, 1999</marker>
<rawString>Christian Braun. 1999. Flaches und robustes Parsen deutscher Satzgefiige. Diplomarbeit, Universitat des Saarlandes, Saarbriicken.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>