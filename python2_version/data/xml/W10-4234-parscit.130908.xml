<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007549">
<title confidence="0.9655365">
The First Question Generation Shared Task Evaluation
Challenge
</title>
<author confidence="0.9626735">
Vasile Rus1, Brendan Wyse2, Paul Piwek2, Mihai Lintean1, Svetlana Stoyanchev2
and Cristian Moldovan1
</author>
<affiliation confidence="0.993407">
1 Department of Computer Science/Institute for Intelligent Systems, The University of
</affiliation>
<address confidence="0.512661">
Memphis, Memphis, TN, 38152, USA
</address>
<email confidence="0.980386">
{vrus,mclinten,cmoldova}@memphis.edu
</email>
<affiliation confidence="0.574004">
2 Centre for Research in Computing, Open University, UK
</affiliation>
<email confidence="0.698311">
bjwyse@gmail.com and {p.piwek, s.stoyanchev}@open.ac.uk
</email>
<bodyText confidence="0.8675526">
Abstract. The paper briefly describes the First Shared Task Evaluation
Challenge on Question Generation that took place in Spring 2010. The
campaign included two tasks: Task A – Question Generation from Paragraphs
and Task B – Question Generation from Sentences. An overview of each of the
tasks is provided.
</bodyText>
<keyword confidence="0.953685">
Keywords: question generation, shared task evaluation campaign.
</keyword>
<sectionHeader confidence="0.999671" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999900214285715">
Question Generation is an essential component of learning environments, help
systems, information seeking systems, multi-modal conversations between virtual
agents, and a myriad of other applications (Lauer, Peacock, and Graesser, 1992;
Piwek et al., 2007).
Question Generation has been recently defined as the task (Rus &amp; Graesser, 2009)
of automatically generating questions from some form of input. The input could vary
from information in a database to a deep semantic representation to raw text.
The first Shared Task Evaluation Challenge on Question Generation (QG-STEC)
follows a long tradition of STECs in Natural Language Processing (see the annual
tasks run by the Conference on Natural Language Learning - CoNLL). In particular,
the idea of a QG-STEC was inspired by the recent activity in the Natural Language
Generation (NLG) community to offer shared task evaluation campaigns as a
potential avenue to provide a focus for research in NLG and to increase the visibility
of NLG in the wider Natural Language Processing (NLP) community (White and
Dale, 2008). It should be noted that the QG is currently perceived as a discourse
processing task rather than a traditional NLG task (Rus &amp; Graesser, 2009).
Two core aspects of a question are the goal of the question and its importance. It is
difficult to determine whether a particular question is good without knowing the
context in which it is posed; ideally one would like to have information about what
counts as important and what the goals are in the current context. This suggests that a
STEC on QG should be tied to a particular application, e.g. tutoring systems.
However, an application-specific STEC would limit the pool of potential participants
to those interested in the target application. Therefore, the challenge was to find a
framework in which the goal and importance are intrinsic to the source of questions
and less tied to a particular context/application. One possibility was to have the
general goal of asking questions about salient items in a source of information, e.g.
core ideas in a paragraph of text. Our tasks have been defined with this concept in
mind. Adopting the basic principle of application-independence has the advantage of
escaping the problem of a limited pool of participants (to those interested in a
particular application had that application been chosen as the target for a QG STEC).
Another decision aimed at attracting as many participants as possible and
promoting a more fair comparison environment was the input for the QG tasks.
Adopting a specific representation for the input would have favored some participants
already familiar with such a representation. Therefore, we have adopted as a second
guiding principle for the first QG-STEC tasks: no representational commitment. That
is, we wanted to have as generic an input as possible. The input to both task A and B
in the first QG STEC is raw text.
The First Workshop on Question Generation (www.questiongeneration.org) has
identified four categories of QG tasks (Rus &amp; Graesser, 2009): Text-to-Question,
Tutorial Dialogue, Assessment, and Query-to-Question. The two tasks in the first QG
STEC are part of the Text-to-Question category or part of the Text-to-text Natural
Language Generation task categories (Dale &amp; White, 2007). It is important to say that
the two tasks offered in the first QG STEC were selected among 5 candidate tasks by
the members of the QG community. A preference poll was conducted and the most
preferred tasks, Question Generation from Paragraphs (Task A) and Question
Generation from Sentences (Task B), were chosen to be offered in the first QG STEC.
The other three candidate tasks were: Ranking Automatically Generated Questions
(Michael Heilman and Noah Smith), Concept Identification and Ordering (Rodney
Nielsen and Lee Becker), and Question Type Identification (Vasile Rus and Arthur
Graesser).
There is overlap between Task A and B. This was intentional with the aim of
encouraging people preferring one task to participate in the other. The overlap
consists of the specific questions in Task A which are more or less similar with the
type of questions targeted by Task B.
Overall, we had 1 submission for Task A and 4 submissions for Task B. We also
had an additional submission on development data for Task A.
</bodyText>
<sectionHeader confidence="0.983079" genericHeader="method">
2 TASK A: Question Generation from Paragraphs
</sectionHeader>
<subsectionHeader confidence="0.999064">
1.1 Task Definition
</subsectionHeader>
<bodyText confidence="0.999912333333333">
The Question Generation from Paragraphs (QGP) task challenges participants to
generate a list of 6 questions from a given input paragraph. The six questions should
be at three scope levels: 1 x broad (entire input paragraph), 2 x medium (multiple
sentences), and 3 x specific (sentence or less). The scope is defined by the portion of
the paragraph that answers the question.
The Question Generation from Paragraphs (QGP) task has been defined such that it
is application-independent. Application-independent means questions will be judged
based on content analysis of the input paragraph; questions whose answers span more
input text are ranked higher.
We show next an example paragraph together with six interesting, application-
independent questions that could be generated. We will use the paragraph and
questions to describe the judging criteria.
</bodyText>
<tableCaption confidence="0.99717">
Table 1. Example of input paragraph (from http://en.wikipedia.org/wiki/Abraham_lincoln).
</tableCaption>
<subsectionHeader confidence="0.578311">
Input Paragraph
</subsectionHeader>
<bodyText confidence="0.99826">
Abraham Lincoln (February 12, 1809 – April 15, 1865), the 16th
President of the United States, successfully led his country through
its greatest internal crisis, the American Civil War, preserving the
Union and ending slavery. As an outspoken opponent of the
expansion of slavery in the United States, Lincoln won the
Republican Party nomination in 1860 and was elected president
later that year. His tenure in office was occupied primarily with the
defeat of the secessionist Confederate States of America in the
American Civil War. He introduced measures that resulted in the
abolition of slavery, issuing his Emancipation Proclamation in 1863
and promoting the passage of the Thirteenth Amendment to the
Constitution. As the civil war was drawing to a close, Lincoln
became the first American president to be assassinated.
</bodyText>
<tableCaption confidence="0.816241">
Table 2. Examples of questions and scores for the paragraph in Table 1.
Questions Scope
</tableCaption>
<figure confidence="0.720280333333334">
Who is Abraham Lincoln? General
What major measures did President Lincoln introduce? Medium
How did President Lincoln die? Medium
When was Abraham Lincoln elected president? Specific
When was President Lincoln assassinated? Specific
What party did Abraham Lincoln belong to? Specific
</figure>
<bodyText confidence="0.9998255">
A set of five scores, one for each criterion (specificity, syntax, semantics, question
type correctness, diversity), and a composite score will be assigned to each question.
Each question at each position will be assigned a composite score ranging from 1
(first/top ranked, best) to 4 (worst rank), 1 meaning the question is at the right level of
specificity given its rank (e.g. the broadest question that the whole paragraph answers
will get a score of 1 if in the first position) and also it is syntactically and semantically
correct as well as unique/diverse from other generated questions in the set.
Ranking of questions based on scope assures a maximum score for the six
questions of 1, 2, 2, 3, 3 and 3, respectively. A top-rank score of 1 is assigned to a
broad scope question that is also syntactically and semantically correct or acceptable,
i.e. if it is semantically ineligible then a decision about its scope cannot be made and
thus a worst-rank score of 4 is assigned. A maximum score of 2 is assigned to
medium-scope questions while a maximum score of 3 is assigned to specific
questions. The best configuration of scores (1, 2, 2, 3, 3, 3) would only be possible for
paragraphs that could trigger the required number of questions at each scope level,
which may not always be the case.
</bodyText>
<subsectionHeader confidence="0.995306">
1.3 Data Sources and Annotation
</subsectionHeader>
<bodyText confidence="0.999945">
The primary source of input paragraphs were: Wikipedia, OpenLearn,
Yahoo!Answers. We collected 20 paragraphs from each of these three sources. We
collected both a development data set (65 paragraphs) and a test data set (60
paragraphs). For the development data set we manually generated and scored 6
questions per paragraph for a total of 6 x 65 = 390 questions.
Paragraphs were selected such that they are self-contained (no need for previous
context to be interpreted, e.g. will have no unresolved pronouns) and contain around
5-7 sentences for a total of 100-200 tokens (excluding punctuation). In addition, we
aimed for a diversity of topics of general interest.
We also provided discourse relations based on HILDA, a freely available
automatic discourse parser (duVerle &amp; Prendinger, 2009).
</bodyText>
<sectionHeader confidence="0.982228" genericHeader="method">
2 TASK B: Question Generation from Sentences
</sectionHeader>
<subsectionHeader confidence="0.994444">
2.1 Task Definition
</subsectionHeader>
<bodyText confidence="0.999466">
Participants were given a set of inputs, with each input consisting of:
</bodyText>
<listItem confidence="0.934795333333333">
■ a single sentence and
■ a specific target question type (e.g., WHO?, WHY?, HOW?, WHEN?; see
below for the complete list of types used in the challenge).
</listItem>
<bodyText confidence="0.998550461538462">
For each input, the task was to generate 2 questions of the specified target question
type.
Input sentences, 60 in total, were selected from OpenLearn, Wikipedia and Yahoo!
Answers (20 inputs from each source). Extremely short or long sentences were not
included. Prior to receiving the actual test data, participants were provided with a
development data set consisting of sentences from the aforementioned sources and,
for one or more target question types, examples of questions. These questions were
manually authored and cross-checked by the team organizing Task B.
The following example is taken from the development data set. Each instance has a
unique identifier and information on the source it was extracted from. The &lt;text&gt;
element contains the input sentence and the &lt;question&gt; elements contain possible
questions. The &lt;question&gt; element has the type attribute for specification of the target
question type.
</bodyText>
<figure confidence="0.928694529411765">
&lt;instance id=&amp;quot;3&amp;quot;&gt;
&lt;id&gt;OpenLearn&lt;/id&gt;
&lt;source&gt;A103_5&lt;/source&gt;
&lt;text&gt;
The poet Rudyard Kipling lost his only son
in the trenches in 1915.
&lt;/text&gt;
&lt;question type=&amp;quot;who&amp;quot;&gt;
Who lost his only son in the trenches in 1915?
&lt;/question&gt;
&lt;question type=&amp;quot;when&amp;quot;&gt;
When did Rudyard Kipling lose his son?
&lt;/question&gt;
&lt;question type=&amp;quot;how many&amp;quot;&gt;
How many sons did Rudyard Kipling have?
&lt;/question&gt;
&lt;/instance&gt;
</figure>
<bodyText confidence="0.999319285714286">
Note that input sentences were provided as raw text. Annotations were not
provided. There are a variety of NLP open-source tools available to potential
participants and the choice of tools and how these tools are used was considered a
fundamental part of the challenge.
This task was restricted to the following question types: WHO, WHERE, WHEN,
WHICH, WHAT, WHY, HOW MANY/LONG, YES/NO. Participants were provided
with this list and definitions of each of the items in it.
</bodyText>
<subsectionHeader confidence="0.988021">
2.2 Evaluation criteria for System Outputs and Human Judges
</subsectionHeader>
<bodyText confidence="0.998843136363636">
The evaluation criteria fulfilled two roles. Firstly, they were provided to the
participants as a specification of the kind of questions that their systems should aim to
generate. Secondly, they also played the role of guidelines for the judges of system
outputs in the evaluation exercise.
For this task, five criteria were identified: relevance, question type, syntactic
correctness and fluency, ambiguity, and variety. All criteria are associated with a
scale from 1 to N (where N is 2, 3 or 4), with 1 being the best score and N the worst
score.
The procedure for applying these criteria is as follows:
■ Each of the criteria is applied independently of the other criteria to each of
the generated questions (except for the stipulation provided below).
We need some specific stipulations for cases where no question is returned in
response to an input. For each target question type, two questions are expected.
Consequently, we have the following two possibilities regarding missing questions:
■ No question is returned for a particular target question type: for each of
the missing questions, the worst score is recorded for all criteria.
■ Only one question is returned: For the missing question, the worst score is
assigned on all criteria. The question that is present is scored following
the criteria, with the exception of the VARIETY criterion for which the
lowest possible score is assigned.
We compute the overall score on a specific criterion. We can also compute a score
which aggregates the overall scores for the criteria.
</bodyText>
<sectionHeader confidence="0.668368" genericHeader="conclusions">
Conclusions
</sectionHeader>
<bodyText confidence="0.999966416666667">
The submissions to the first QG STEC are now being evaluated using peer-review
mechanism in which participants blindly evaluate their peers questions. At least two
reviews per submissions are performed with the results to be made public at the 3rd
Workshop on Question Generation that will take place in June 2010.
Acknowledgments. We are grateful to a number of people who contributed to the
success of the First Shared Task Evaluation Challenge on Question Generation:
Rodney Nielsen, Amanda Stent, Arthur Graesser, Jose Otero, and James Lester. Also,
we would like to thank the National Science Foundation who partially supported this
work through grants RI-0836259 and RI-0938239 (awarded to Vasile Rus) and the
Engineering and Physical Sciences Research Council who partially supported the
effort on Task B through grant EP/G020981/1 (awarded to Paul Piwek). The views
expressed in this paper are solely the authors’.
</bodyText>
<sectionHeader confidence="0.999481" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999651375">
1. Lauer, T., Peacock, E., &amp; Graesser, A. C. (1992) (Eds.). Questions and information systems.
Hillsdale, NJ: Erlbaum.
2. Rus, V. and Graesser, A.C. (2009). Workshop Report: The Question Generation Task and
Evaluation Challenge, Institute for Intelligent Systems, Memphis, TN, ISBN: 978-0-615-
27428-7.
3. Piwek, P., H. Hernault, H. Prendinger, M. Ishizuka (2007). T2D: Generating Dialogues
between Virtual Agents Automatically from Text. In: Intelligent Virtual Agents:
Proceedings of IVA07, LNAI 4722, September 17-19, 2007, Paris, France, (Springer-
Verlag, Berlin Heidelberg) pp.161-174
4. Dale, R. &amp; M. White (2007) (Eds.). Position Papers of the Workshop on Shared Tasks and
Comparative Evaluation in Natural Language Generation.
5. duVerle, D. and Prendinger, H. (2009). A novel discourse parser based on Support Vector
Machines. Proc 47th Annual Meeting of the Association for Computational Linguistics and
the 4th Int&apos;l Joint Conf on Natural Language Processing of the Asian Federation of Natural
Language Processing (ACL-IJCNLP&apos;09), Singapore, Aug 2009 (ACL and AFNLP), pp 665-
673.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.310582">
<title confidence="0.9980505">The First Question Generation Shared Task Evaluation Challenge</title>
<author confidence="0.818143">Brendan Paul Mihai Svetlana Cristian</author>
<affiliation confidence="0.989949">1Department of Computer Science/Institute for Intelligent Systems, The University of</affiliation>
<address confidence="0.997214">Memphis, Memphis, TN, 38152, USA</address>
<email confidence="0.999323">vrus@memphis.edu</email>
<email confidence="0.999323">mclinten@memphis.edu</email>
<email confidence="0.999323">cmoldova@memphis.edu</email>
<affiliation confidence="0.989554">2Centre for Research in Computing, Open University,</affiliation>
<email confidence="0.984165">bjwyse@gmail.comand{p.piwek,s.stoyanchev}@open.ac.uk</email>
<abstract confidence="0.887072666666667">paper briefly describes the First Shared Task Evaluation Challenge on Question Generation that took place in Spring 2010. The campaign included two tasks: Task A – Question Generation from Paragraphs and Task B – Question Generation from Sentences. An overview of each of the tasks is provided. generation, shared task evaluation campaign.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Lauer</author>
<author>E Peacock</author>
<author>A C Graesser</author>
</authors>
<title>(Eds.). Questions and information systems.</title>
<date>1992</date>
<publisher>Erlbaum.</publisher>
<location>Hillsdale, NJ:</location>
<contexts>
<context position="8510" citStr="(1, 2, 2, 3, 3, 3)" startWordPosition="1330" endWordPosition="1335">s well as unique/diverse from other generated questions in the set. Ranking of questions based on scope assures a maximum score for the six questions of 1, 2, 2, 3, 3 and 3, respectively. A top-rank score of 1 is assigned to a broad scope question that is also syntactically and semantically correct or acceptable, i.e. if it is semantically ineligible then a decision about its scope cannot be made and thus a worst-rank score of 4 is assigned. A maximum score of 2 is assigned to medium-scope questions while a maximum score of 3 is assigned to specific questions. The best configuration of scores (1, 2, 2, 3, 3, 3) would only be possible for paragraphs that could trigger the required number of questions at each scope level, which may not always be the case. 1.3 Data Sources and Annotation The primary source of input paragraphs were: Wikipedia, OpenLearn, Yahoo!Answers. We collected 20 paragraphs from each of these three sources. We collected both a development data set (65 paragraphs) and a test data set (60 paragraphs). For the development data set we manually generated and scored 6 questions per paragraph for a total of 6 x 65 = 390 questions. Paragraphs were selected such that they are self-contained</context>
</contexts>
<marker>1.</marker>
<rawString>Lauer, T., Peacock, E., &amp; Graesser, A. C. (1992) (Eds.). Questions and information systems. Hillsdale, NJ: Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Rus</author>
<author>A C Graesser</author>
</authors>
<title>Workshop Report: The Question Generation Task and Evaluation Challenge, Institute for Intelligent Systems,</title>
<date>2009</date>
<pages>978--0</pages>
<location>Memphis, TN, ISBN:</location>
<contexts>
<context position="8510" citStr="(1, 2, 2, 3, 3, 3)" startWordPosition="1330" endWordPosition="1335">s well as unique/diverse from other generated questions in the set. Ranking of questions based on scope assures a maximum score for the six questions of 1, 2, 2, 3, 3 and 3, respectively. A top-rank score of 1 is assigned to a broad scope question that is also syntactically and semantically correct or acceptable, i.e. if it is semantically ineligible then a decision about its scope cannot be made and thus a worst-rank score of 4 is assigned. A maximum score of 2 is assigned to medium-scope questions while a maximum score of 3 is assigned to specific questions. The best configuration of scores (1, 2, 2, 3, 3, 3) would only be possible for paragraphs that could trigger the required number of questions at each scope level, which may not always be the case. 1.3 Data Sources and Annotation The primary source of input paragraphs were: Wikipedia, OpenLearn, Yahoo!Answers. We collected 20 paragraphs from each of these three sources. We collected both a development data set (65 paragraphs) and a test data set (60 paragraphs). For the development data set we manually generated and scored 6 questions per paragraph for a total of 6 x 65 = 390 questions. Paragraphs were selected such that they are self-contained</context>
</contexts>
<marker>2.</marker>
<rawString>Rus, V. and Graesser, A.C. (2009). Workshop Report: The Question Generation Task and Evaluation Challenge, Institute for Intelligent Systems, Memphis, TN, ISBN: 978-0-615-27428-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Piwek</author>
<author>H Hernault</author>
<author>H Prendinger</author>
<author>M</author>
</authors>
<title>Ishizuka (2007). T2D: Generating Dialogues between Virtual Agents Automatically from Text. In: Intelligent Virtual Agents:</title>
<date>2007</date>
<booktitle>Proceedings of IVA07, LNAI</booktitle>
<pages>4722</pages>
<location>Paris, France, (SpringerVerlag, Berlin Heidelberg)</location>
<contexts>
<context position="8510" citStr="(1, 2, 2, 3, 3, 3)" startWordPosition="1330" endWordPosition="1335">s well as unique/diverse from other generated questions in the set. Ranking of questions based on scope assures a maximum score for the six questions of 1, 2, 2, 3, 3 and 3, respectively. A top-rank score of 1 is assigned to a broad scope question that is also syntactically and semantically correct or acceptable, i.e. if it is semantically ineligible then a decision about its scope cannot be made and thus a worst-rank score of 4 is assigned. A maximum score of 2 is assigned to medium-scope questions while a maximum score of 3 is assigned to specific questions. The best configuration of scores (1, 2, 2, 3, 3, 3) would only be possible for paragraphs that could trigger the required number of questions at each scope level, which may not always be the case. 1.3 Data Sources and Annotation The primary source of input paragraphs were: Wikipedia, OpenLearn, Yahoo!Answers. We collected 20 paragraphs from each of these three sources. We collected both a development data set (65 paragraphs) and a test data set (60 paragraphs). For the development data set we manually generated and scored 6 questions per paragraph for a total of 6 x 65 = 390 questions. Paragraphs were selected such that they are self-contained</context>
</contexts>
<marker>3.</marker>
<rawString>Piwek, P., H. Hernault, H. Prendinger, M. Ishizuka (2007). T2D: Generating Dialogues between Virtual Agents Automatically from Text. In: Intelligent Virtual Agents: Proceedings of IVA07, LNAI 4722, September 17-19, 2007, Paris, France, (SpringerVerlag, Berlin Heidelberg) pp.161-174</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>M White</author>
</authors>
<date>2007</date>
<booktitle>(Eds.). Position Papers of the Workshop on Shared Tasks and Comparative Evaluation in Natural Language Generation.</booktitle>
<marker>4.</marker>
<rawString>Dale, R. &amp; M. White (2007) (Eds.). Position Papers of the Workshop on Shared Tasks and Comparative Evaluation in Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D duVerle</author>
<author>H Prendinger</author>
</authors>
<title>A novel discourse parser based on Support Vector Machines.</title>
<date>2009</date>
<journal>(ACL and AFNLP),</journal>
<booktitle>Proc 47th Annual Meeting of the Association for Computational Linguistics and the 4th Int&apos;l Joint Conf on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL-IJCNLP&apos;09), Singapore,</booktitle>
<pages>665</pages>
<marker>5.</marker>
<rawString>duVerle, D. and Prendinger, H. (2009). A novel discourse parser based on Support Vector Machines. Proc 47th Annual Meeting of the Association for Computational Linguistics and the 4th Int&apos;l Joint Conf on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL-IJCNLP&apos;09), Singapore, Aug 2009 (ACL and AFNLP), pp 665-</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>